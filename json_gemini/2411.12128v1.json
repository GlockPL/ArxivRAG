{"title": "The Role of Accuracy and Validation Effectiveness in Conversational Business Analytics", "authors": ["Adem Alparslan"], "abstract": "This study examines conversational business analytics, an approach that utilizes AI to address the technical competency gaps that hindered end users from effectively using traditional self-service analytics. By facilitating natural language interactions, conversational business analytics aims to enable end users to independently retrieve data and generate insights. The analysis focuses on Text-to-SQL as a representative technology for translating natural language requests into SQL statements. Using models grounded in expected utility theory, the study identifies conditions under which conversational business analytics, through partial or full support, can outperform delegation to human experts. The results indicate that partial support, which focuses solely on information generation by AI, is viable when the accuracy of AI-generated SQL queries exceeds a defined threshold. In contrast, full support includes not only information generation but also validation through explanations provided by the AI, and requires sufficiently high validation effectiveness to be reliable. However, user-based validation presents challenges, such as misjudgment and rejection of valid SQL queries, which may limit the effectiveness of conversational business analytics. These challenges underscore the need for robust validation mechanisms, including improved user support, automated processes, and methods for assessing quality independently of end users' technical competencies.", "sections": [{"title": "Introduction", "content": "Business analytics aims to generate actionable insights that support data-driven decision-making across diverse organizational contexts. Self-service analytics, a significant development in this domain, empowers end users to independently fulfill their information needs without relying on experts such as data engineers or data scientists. By providing tools for retrieving, preparing, analyzing, and visualizing data, self-service analytics enhances flexibility and agility in addressing dynamic business demands.\nDespite these advantages, self-service analytics has notable limitations. While end users are often domain experts, they frequently lack the technical skills required for advanced analytics tasks, such as navigating complex data structures, writing"}, {"title": "Traditional Self-Service Analytics", "content": "Business analytics focuses on generating actionable insights to support data-driven decision-making within business contexts [10, 17, 28, 78]. To achieve these goals, analytics ecosystems consist of components [30, 83, 63, 54] that automate four core interconnected tasks for generating insights: retrieving data, preparing data, generating information, and visualizing information (see Figure 1). In addition to these core tasks, there are complementary tasks such as ensuring data and information quality and establishing governance mechanisms [84]. Although these complementary tasks are important for achieving the goals associated with business analytics, they are not discussed further in this study.\nRetrieving data involves gathering raw data from various source systems, such as structured data from relational databases, semi-structured data like JSON or XML, and unstructured data such as text or images. The next step, preparing data, includes cleansing, transformation, and enrichment processes to ensure quality and usability. Prepared data is stored in data warehouses and data marts for structured analysis, while semi-structured or unstructured data is stored in data lakes [44] or variants, prepared when needed.\nGenerating information applies the three main methods of business analytics [16]: descriptive, predictive, and prescriptive analytics. Descriptive analytics summarizes historical and real-time data, providing insights into past and current performance using key performance indicators (KPIs), trend analysis, and target-versus-actual comparisons [17]. Predictive analytics uses statistical models and machine learning techniques to forecast future developments, such as predicting customer churn or market trends. Prescriptive analytics recommends actions based on forecasts, utilizing planning, simulation, and optimization methods to identify effective decisions. These methods are complementary, with prescriptive analytics relying on both descriptive and predictive results.\nFinally, visualizing information transforms analytical results into intuitive formats that support decision-making [69]. Effective visualizations, such as dashboards, simplify complex insights, enabling to explore trends, identify opportunities, and act quickly.\nDay-to-day information needs are typically met through predefined reports provided by analytics ecosystems. However, when new information needs arise, self-service analytics [29, 1, 46] enables end users to fulfill information needs independently, providing a viable alternative to the traditional reliance on expert (e.g. data engineers and data scientists) intervention. In this context, an end user refers to any organizational member who uses information to make decisions \u2013 whether to inform their own decisions or to prepare information for others, such as supervisors."}, {"title": "Conversational Business Analytics", "content": "Business analytics is undergoing significant development with the emergence of generative AI, giving rise to a new approach termed \u201cConversational Business Analytics\" (CBA). This new paradigm leverages natural language processing to address persistent challenges in traditional self-service analytics, particularly the technical skill gap among end users. CBA enables natural language interactions for tasks such as data processing, analysis, and insight generation, presenting the potential to enhance the accessibility and efficiency of business analytics.\nCBA shifts the focus from traditional graphical user interfaces to natural language-driven interactions, supported by advancements in large language models (LLMs) [77, 18, 7]. These models process natural language by tokenizing text and employing self-attention mechanisms to interpret contextual relationships. This architecture allows for context-aware and relevant outputs, with responses generated token by token based on probabilistic modeling. Trained on extensive text corpora, LLMs optimize billions of parameters to achieve high performance in natural language understanding and generation. As depicted in Figure 2, CBA facilitates the transformation of natural language inputs into actionable outputs, such as structured reports or visualizations.\nCBA extends the capabilities of business analytics by automating core tasks for insight generation, including data retrieval, preparation, analytics, and visualization."}, {"title": "Overview", "content": "Business analytics is undergoing significant development with the emergence of generative AI, giving rise to a new approach termed \u201cConversational Business Analytics\" (CBA). This new paradigm leverages natural language processing to address persistent challenges in traditional self-service analytics, particularly the technical skill gap among end users. CBA enables natural language interactions for tasks such as data processing, analysis, and insight generation, presenting the potential to enhance the accessibility and efficiency of business analytics.\nCBA shifts the focus from traditional graphical user interfaces to natural language-driven interactions, supported by advancements in large language models (LLMs) [77, 18, 7]. These models process natural language by tokenizing text and employing self-attention mechanisms to interpret contextual relationships. This architecture allows for context-aware and relevant outputs, with responses generated token by token based on probabilistic modeling. Trained on extensive text corpora, LLMs optimize billions of parameters to achieve high performance in natural language understanding and generation. As depicted in Figure 2, CBA facilitates the transformation of natural language inputs into actionable outputs, such as structured reports or visualizations.\nCBA extends the capabilities of business analytics by automating core tasks for insight generation, including data retrieval, preparation, analytics, and visualization."}, {"title": "Accuracy", "content": "Delegating tasks to human experts is grounded in their ability to accurately interpret and respond to information needs, as well as to build a shared understanding of the underlying goals and requirements [22, 31]. A similar dynamic exists when interacting with AI: the AI must understand the user's request to generate the desired outcome. However, while natural language is flexible and often ambiguous, SQL is highly structured and formal. For instance, the key figure \"material availability\" may be interpreted differently by the logistics and maintenance departments within the same organization due to the coexistence of multiple terminological systems [50, 85, 27]. Beyond terminological differences, ambiguities may also arise from the linguistic complexity of the request itself, such as context dependencies or vague formulations. To translate such requests into correct SQL queries, the AI must map the terms used in the request to the corresponding tables and columns within the data model (schema mapping). This requires a deep understanding of the semantics of the data model, including the specific meaning of fields and the relationships between tables. Thus, the AI must not only understand the request but also identify the relevant tables and columns that match the user's inquiry.\nThe AI's ability to interpret a natural language request and translate it into correct SQL code is referred to as its accuracy. Several distinct types of accuracy can be identified to assess different aspects of AI performance [33, 89, 74, 32]. Syntactic accuracy examines whether the SQL statements generated by the AI are executable. Execution accuracy evaluates whether the SQL query generated by the AI produces the expected result, even if the query's structure differs from a reference SQL statement (often referred to as the \"gold standard\"). In contrast, exact match accuracy is more stringent, as it requires not only the correct result but also that"}, {"title": "Validation Effectiveness", "content": "When delegating information production to human experts, there is a challenge of \"hidden action,\u201d where the actions of experts are not fully observable or assessable, potentially leading to misalignment of interests [26]. Similarly, when delegated to AI, the issue shifts also to the transparency and reliability of outputs. While human experts may act in self-interest, leading to agency costs through false information or requiring additional incentives, AI systems present the challenge of output opacity. End users cannot evaluate the correctness of SQL queries and resulting KPIs until after implementation, risking decisions based on false information with economic consequences. Ensuring the reliability of AI-generated information is crucial, particularly in high-stakes decision-making scenarios where false information can result"}, {"title": "Models of CBA", "content": "This study introduces models based on rational choice theory, with an emphasis on expected utility theory [79, 76], to evaluate the effective use of CBA. The analysis focuses on the example of Text-to-SQL. Two primary influencing factors are"}, {"title": "Basic Assumptions", "content": "This study introduces models based on rational choice theory, with an emphasis on expected utility theory [79, 76], to evaluate the effective use of CBA. The analysis focuses on the example of Text-to-SQL. Two primary influencing factors are"}, {"title": "Partial Support (PS)", "content": "The first stage involves the end user evaluating the feasibility of PS. PS addresses the competency gaps that prevent end users from utilizing traditional self-service analytics by automating the translation of natural language inputs into SQL queries."}, {"title": "Full Support (FS)", "content": "The second stage involves assessing the viability of FS, which includes an additional validation process to enhance the reliability of AI-generated SQL queries. This evaluation determines whether validation meaningfully improves the quality of the generated queries, ensuring their correctness and relevance for decision-making. By incorporating validation, FS demonstrates AI's role as an active partner in ensuring reliability and correctness. The validation process not only identifies and mitigates errors but also enhances the end user's trust in the system by providing transparent explanations. This collaboration between AI and the end user enables decision-making that aligns more closely with organizational goals, even in scenarios with imperfect accuracy.\nVarious techniques such as decomposition, visualization, and dialog-based are used to enhance the interpretability and traceability of automatically generated SQL queries. The effectiveness of these validation techniques depends on several factors, including the clarity, comprehensiveness, and contextual relevance of the explanations provided, as well as the user's technical expertise in evaluating them. High-quality explanations can significantly aid users in identifying errors and verifying the correctness of SQL queries. However, the end user's ability to critically evaluate these explanations plays an equally crucial role in determining the success of the validation process.\nValidation is inherently uncertain due to variability in user comprehension and the probabilistic nature of AI-generated explanations. These explanations are not guaranteed to be correct, as LLMs generate outputs based on probabilities rather than deterministic logic. Poor validation can amplify risks by failing to detect erroneous SQL queries or by incorrectly rejecting valid queries. This dual risk undermines decision-making and may reduce trust in the system. While user-based validation is intended to mitigate inaccuracies, its effectiveness heavily depends on the user's ability to interpret complex SQL logic and explanations. This reliance introduces variability, making validation inconsistent across users with differing expertise levels.\nTo quantify this uncertainty, validation effectiveness is modeled as a random variable, denoted as \u1e9e (where 0 < \u03b2 < 1). This probability reflects the likelihood that a user can correctly assess the SQL query based on the explanations provided. It is defined as the ratio of successful validations to the total number of validation attempts. By adopting this probabilistic framework, the model accounts for the inherent uncertainties associated with validation processes.\nThe expected value Ers(\u03b1, \u03b2) for FS is calculated as:\n $E_{FS}(\\alpha, \\beta) = \\alpha\\beta\\cdot (+1) + (1 - \\alpha)(1 - \\beta)\\cdot (-1) = \\alpha + \\beta - 1$.\nThe expected value EFS(\u03b1, \u03b2) is influenced by two key factors: accuracy and validation effectiveness. Higher values of a indicate improved accuracy in generating correct SQL queries, while higher values of \u1e9e represent enhanced effectiveness in validating the correctness of those queries. The combined improvement in these factors increases the expected value of FS, making it more likely to yield reliable and actionable KPI. This relationship highlights the complementary roles of accuracy"}, {"title": "Comparison of Full Support with Partial Support and Delegation to the Data Engineer", "content": "The end user will choose FS only if it provides a higher expected value than the profit achievable through delegation\n $E_{FS}(\\alpha, \\beta) > \\upsilon$\nand offers a better outcome than PS\n $E_{FS}(\\alpha, \\beta) > E_{PS}(\\alpha)$.\nFor FS to outperform delegation to a data engineer, the following condition, referred to as the \"AI delegation condition for FS\" must be satisfied:\n $E_{FS}(\\alpha, \\beta) > \\upsilon \\Leftrightarrow \\beta^* > (1 - \\alpha) + \\upsilon$.\nThis condition (2) requires the validation to be sufficiently effective and robust (e.g., against the shortcomings of the end user conducting the validation based on the provided explanation) to compensate for inaccuracies in AI-generated information ((1 \u2212 a)) and to deliver an overall expected value exceeding the guaranteed, but delayed profit v. If the AI delegation condition for FS (2) is satisfied, it is prioritized over delegation to the data engineer. However, this preference also depends on the accuracy of AI-generated information being sufficiently high compared to the profit v. Specifically, the \"FS feasibility condition\" must hold:\n $\\alpha \\upsilon$.\nThis indicates that the inclusion of a validation step in FS broadens the potential applicability of CBA. However, whether FS is advantageous compared to delegation to a data engineer depends on the level of validation effectiveness as defined in the AI delegation condition for FS (2).\nThe end user, acting as a rational decision-maker, will only increase the AI support level from PS to FS if the probability of successful validation exceeds the probability of successful SQL generation. This requirement is formalized as the \"validation dominance condition\":\n $E_{FS}(\\alpha, \\beta) > E_{PS}(\\beta) \\beta^{**} \u03b1$."}, {"title": "Conclusion", "content": "This study analyzes CBA, an emerging approach with considerable potential to address the competency gaps inherent in traditional self-service analytics. Through the use of LLMS, CBA facilitates natural language interactions, enabling end users to independently perform tasks such as data retrieval, preparation, and insight generation. In this context, CBA represents a new form of task delegation, shifting the responsibility for generating and interpreting information from human experts to AI, thus making advanced analytics more accessible to users without technical expertise. A focus of this study is Text-to-SQL, a well-established semantic parsing technology that has seen transformative advancements through the application of LLMs, enhancing its ability to translate natural language requests into structured SQL statements.\nThe models developed in this study examine the conditions under delegation to AI, through PS and FS, outperforms the delegation of tasks to human experts. PS is effective when the accuracy (a) exceeds a specified threshold, as outlined in condition (1). FS, however, is only beneficial when three conditions are met: the AI delegation condition ((2)), FS feasibility condition ((3)), and validation dominance condition ((4)).\nThe study highlights the critical role of validation effectiveness (\u03b2) in FS. While FS can significantly improve decision-making when validation is robust, the use of user-based validation introduces risks, particularly when users lack the necessary expertise or when explanations are unclear. These risks can undermine decision"}]}