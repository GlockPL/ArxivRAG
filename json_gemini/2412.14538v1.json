{"title": "Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities", "authors": ["Qimei Cui", "Xiaohu You", "Ni Wei", "Guoshun Nan", "Xuefei Zhang", "Jianhua Zhang", "Xinchen Lyul", "Ming Ai", "Xiaofeng Tao", "Zhiyong Feng", "Ping Zhang", "Qingqing Wu", "Meixia Tao", "Yongming Huang", "Chongwen Huang", "Guangyi Liu", "Chenghui Peng", "Zhiwen Pan", "Tao Sun", "Dusit Niyato", "Tao Chen", "Muhammad Khurram Khan", "Abbas Jamalipour", "Mohsen Guizani", "Chau Yuen"], "abstract": "With the increasing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and communication for sixth-generation (6G) network is emerging as a revolutionary architecture. This synergy deeply embeds AI into various layers of the communication network, optimizing resource allocation, enhancing network efficiency, and providing reliable performance in complex and dynamic environments. This paper presents a comprehensive overview of AI and communication for 6G networks, emphasizing their foundational principles, inherent challenges, and future research opportunities. We commence with a retrospective analysis of AI and the evolution of large-scale AI models, underscoring their pivotal roles in shaping contemporary communication technologies. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The initial stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The subsequent stage, Network for Al, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, including digital twins for AI and semantic communication. In the final stage, Al as a Service, it is anticipated that future 6G networks will innately provide AI functions as services and support application scenarios like immersive communication and intelligent industrial robots. Specifically, we have defined the quality of AI service, which refers to the measurement framework system of AI services within the network. In addition to these developmental stages, we thoroughly examine the standardization processes pertinent to AI in network contexts, highlighting key milestones and ongoing efforts. Finally, we outline promising future research opportunities that could drive the evolution and refinement of AI and communication for 6G, positioning them as a cornerstone of next-generation communication infrastructure.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid development of wireless communication technology has profoundly reshaped various aspects of our society, driving unprecedented connectivity and enabling innovative applications. Following the widespread deployment and success of the fifth-generation (5G) networks, attention has shifted towards the sixth-generation (6G) wireless communication systems. With its enhanced capabilities, it is anticipated that 6G will bring transformative changes, including ultra-low latency, significantly higher data transmission rates, increased reliability, and ubiquitous connectivity. Among these advance-ments, integrating artificial intelligence (AI) into 6G networks is expected to be a game-changer, providing new paradigms and opportunities across multiple fields.\nAI technology has advanced rapidly over the past decade, particularly in machine learning, deep learning, and natural language processing. These advancements have enabled AI to play a crucial role across various industries. In the 5G era, AI has been successfully applied in aspects of wireless networks, such as network optimization, traffic prediction, and fault detection, significantly enhancing network performance and user experience. However, to truly achieve AI-native support in 6G networks, many unresolved issues still need to be addressed.\nThe introduction of AI can enhance coding efficiency by utilizing compressed semantic information to transmit more data with less bandwidth, which helps alleviate network congestion and improve data transmission rates [1]. However, AI algorithms generate additional data that needs to be transmitted, such as model parameters, training data, and real-time feedback. This raises the question of whether the overall data volume in future networks will decrease or increase. This change in data volume will directly impact network design and architecture. Moreover, both base stations and user devices will employ AI algorithms for resource allocation to reduce energy consumption and improve resource utilization. However, the operation of AI itself may increase power consumption, leading to another question: Will energy consumption in future networks increase or decrease? In the pursuit of high efficiency, it is crucial to consider optimizing energy efficiency in AI applications to ensure the sustainable development of networks. Additionally, while networks can leverage AI algorithms to enhance transmission reliability and better respond to changing network conditions and user demands, the inherent uncertainty of AI algorithms especially in complex and dynamic network environments raises concerns about whether future networks will operate more reliably. These uncertainties may lead to decision-making errors, affecting user experience and overall network performance.\nIt is essential to consider integrating AI into the architecture, network elements, and functional pro-cesses of the new 6G system from a closer and deeper perspective to address these issues. Specifically, we need to re-examine the network's design philosophy to ensure a genuine synergy is formed between AI and the 6G network.\nThis comprehensive review explores the intricate relationship between 6G and AI, delving into the fun-damental principles, key technologies, and potential applications this integration brings forth. We discuss the critical technical enablers of 6G, including advanced wireless communication techniques, spectrum management, and network architectures. Additionally, we examine the role of AI in optimizing network operations, enhancing security, and enabling intelligent decision-making processes. The review also high-lights the open challenges to fully realizing the potential of 6G and AI integration. These challenges include data privacy, algorithmic transparency, energy efficiency, and the need for standardized frameworks to ensure interoperability and scalability. This review can be a valuable resource for researchers, practitioners, and policymakers involved in developing and deploying 6G and AI technologies by provid-ing a holistic overview of the state-of-the-art and future directions. Through this collaborative effort, we can harness the full potential of these cutting-edge technologies to build a brighter, more connected, and more sustainable future.\nThe organization of this paper is illustrated in Figure 1, and we summarize the main abbreviations used throughout this work in Table 1."}, {"title": "1.1 Requirements and Challenges in the Post-5G Era", "content": "The rapid development of information technology and networks has led to a new round of business innovation and upgrading of service demand. The networks face the dual challenges of explosive growth"}, {"title": "3 AI for Network", "content": "AI for Network is the notation of using AI to improve the performance, efficiency and user service experi-ence of the network itself. The primary research of AI for NET includes using AI to optimize traditional algorithms, optimize network functions, optimize network operation and maintenance management, etc., to improve the transmission efficiency and wireless performance of the wireless air interface and network sides. Figure 7 illustrates a schematic of AI's capabilities for the network.\nAI enables wireless networks through operation and maintenance efficiency improvement, network performance enhancement, and new business enabled. AI can allow wireless networks based on AI's capabilities, including feature extraction, Forecast, Self-adaptation, Optimization, Real-time, correlation, Scene clustering and other capabilities. These capabilities are the basis for AI to enable wireless networks. In the user's environment, AI optimizes traditional methods to improve network performance on various terminal devices. AI leverages its ability to assist in optimizing the network, and different cases reflect AI's capabilities. AI can optimize network element functions and central management in the upper-layer network to improve network operation and maintenance efficiency. At the same time, the generation of AI has also spawned a series of new services to improve and better serve users."}, {"title": "3.2 Improved O&M Efficiency", "content": "AI can analyze large amounts of data and make decisions in real time [74]. This can be particularly useful in wireless networks, where many variables and parameters must be constantly monitored and adjusted to optimize the system's performance. Table 5 lists the wireless use cases of AI.\nAI algorithms can analyze data from the wireless network to predict when maintenance is needed, allowing maintenance to be performed proactively and timely. It can dynamically allocate resources, such as bandwidth and power, in the wireless network to optimize the system's performance.\nThe air interface data is affected by the environment and has significant time variability, and the data quality is easily affected by many factors. Such sparse, high-dimensional data is very suitable for building a mapping relationship between the input and output of AI models, and the wireless data generated at all times ensures the number of samples for model training.\nComparing AI technology with traditional air interface technology, we show the unique advantages of AI itself and point out the development direction of the deep integration of wireless air interface"}, {"title": "3.2.1 AI-Based Traffic Prediction", "content": "With the wide application of 5G communication, users' demand for data traffic continues to increase, and traffic prediction is also paid more attention in communication, as shown in Figure 8. Considering the increasing network load, analysis data based on network traffic can effectively reconfigure the structure of multi-provider networks. Traffic prediction is a means [75] for various networks to realize intelligent network operations such as resource management and predictive control. A forecasting method based on historical averages can be used in traditional traffic forecasting. This approach is based on the assumption that network traffic has a cyclical nature. By averaging the historical data, it is possible to predict the traffic at the next moment.\nHowever, traffic forecasting methods based on historical averages have specific bottlenecks. Although the model of this method is simple, it lacks the use of geographic location, data change trend and other information, resulting in obvious errors. The traditional traffic prediction method makes fitting the com-plex time-varying communication traffic model challenging. In this case, introducing AI technology has become a new direction to solve this problem. At present, AI algorithms have been widely used in the field of mobile communication traffic prediction. For example, algorithm models based on machine learning in-clude Random Forest (RF), Multiple Linear Regression, Gaussian Process Regression, and Prophet time series prediction algorithm. These machine learning algorithms perform better than traditional statistical models but still struggle to capture the spatio-temporal characteristics of the data. Deep learning can effectively extract intrinsic features, such as continuity and timing of traffic information. Compared with traditional statistical models, deep learning models have strong adaptability when dealing with massive and complex communication traffic data. They can extract multi-level features and adjust themselves in time.\nExamples such as Residual Networks (ResNet), 3D Convolutional Networks (3D-CNN), GAN and other deep learning models can be used. These models can analyze user behavior patterns and device characteristics, learn the internal relationship between user data to accurately predict user demand for different services and applications, and optimize the resource allocation of the network based on this. At the same time, these models can also analyze network traffic data, identify factors that may lead to network failures, and predict the occurrence and recovery of failures, helping operators diagnose and solve problems faster and accurately and improve network reliability and stability.\nIn [76], random forest and linear regression are combined to solve the traffic prediction problem in backbone optical networks. In [77], a network traffic prediction model based on synchronous Periodic LSTM (DSP-LSTM) was proposed, which can comprehensively consider the good fitting ability of a linear model to periodic characteristic data and the nonlinear prediction characteristics of neural networks. [78] The internal mechanism of network traffic outburst is explained by extracting the time series of the newly generated number of network flows. The number of network flow-based traffic prediction models shows noticeable performance improvement over the original LSTM and TCN models. In [79], a hybrid prediction model for passive optical network traffic was proposed, which combines fully integrated empirical mode decomposition with adaptive noise with particle swarm optimization support vector regression and LSTM neural networks, showing high prediction accuracy. In [80], a network traffic prediction method based on time representation and spatial convolution was proposed. Graph convolution network was used to explore the topological features of network nodes, and LSTM was used to characterize the time features of the network to help the network learn daily and weekly long-term time features."}, {"title": "3.2.2 AI-Based BS Energy Conservation", "content": "The traditional network energy-saving algorithm usually considers the current load of the BS to control the opening and closing of the BS [81, 82]. Traditional energy-saving strategies mainly involve device-level and site-level energy conservation [83]. At the device level, hardware energy-saving schemes are primarily studied from device and hardware design, and some RF power amplifiers are turned off when the BS load"}, {"title": "3.2.3 AI-Based Network Parameter Optimization", "content": "In wireless communication systems, parameter configuration profoundly impacts network performance. One of the most important aspects is the appropriate setup of the network parameters, as these determine the design, energy consumption, and performance of the 5G network and need to be aligned with the real-time user distribution and electromagnetic environments. According to the characteristics of wireless networks, network parameters can be divided into two categories: BS and threshold. Specifically, the BS parameters involve switching essential functions, such as VONR, paging, and slicing. The threshold parameters configure thresholds for cell reselect, event decision, and neighborhood switching. Table 6 summarizes Various parameters in the current network.\nThe traditional parameter optimization mainly relies on road test collection and expert experience"}, {"title": "3.2.4 AI-Based Mobility Management", "content": "Mobility management is a critical component in modern wireless communication networks. Mobility management includes mobility in the idle state and mobility in the connected state. Due to the increasing number of mobile users and the increasing complexity of services, how to effectively manage mobility to improve network performance and user experience has become a critical issue.\nTraditional mobility management relies heavily on preset policies and rules. In the idle state, mobility management involves cell selection and switching. Cell selection is usually based on signal strength, network load, and priority. In the connected state, the device must decide whether to switch cells based on network conditions, such as network coverage, quality, and load. In addition, the device may also undergo same-frequency switching or cross-frequency switching. Frequency switching refers to the equipment at the same frequency switching between two villages; different frequency switching equipment is involved in switching between the different frequencies of the villages.\nHowever, there are some bottlenecks in the traditional mobility management approach. One is that these techniques might not manage user interactions fast enough when the number of mobile users spikes, leading to excessive network traffic and poor performance. The second is that typical mobility management techniques may be challenging in complex network situations, such as big public spaces or events, which could cause network congestion. Furthermore, users traveling at fast speeds like those in cars-may have unreliable network connections due to frequent switching.\nOver the past few decades, many schemes have been proposed to optimize mobility management [91], [92], [93], [94]. They all have a reactive nature; adjustments are often made after rather than pre-planning when dealing with mobility issues. This design can lead to delays and inefficiencies in mobility management and some unnecessary waste of resources.\nTo address these problems, AI has been introduced into mobility management. The advantage of AI is that it can process large amounts of data, learn automatically from data, and optimize strategies without human intervention. This allows AI to manage mobility more effectively, which improves network performance and user experience. Starting as early as 2015, 3GPP began to study how AI technology could be integrated into 5G networks. For example, the 3GPP SA2 and SA5 working groups' \"Enablers of Network Automation\" and \"Enhancement of Managed Data Analytics Services\" projects made impressive progress of [95].\nAI has shown significant advantages in mobility management with its superior fitting capabilities. On the one hand, using techniques such as deep learning, AI can predict user behavior, such as movement speed and direction, so that cell selection and switching can be done in advance to reduce signaling overhead. For example, in ultra-dense networks, AI can predict the next moment location of vehicles and select BSs according to the predicted location, thereby reducing the switching frequency and switching failure rate and saving the switching signaling overhead [96]. On the other hand, using techniques such as reinforcement learning, AI can dynamically optimize mobility management strategies based on the network environment and user behavior to better cope with complex network environments. Take D2D communication as an example. Traditional cellular networks require BSs to relay user communications, which helps management but adds to transmission latency. The introduction of D2D communication allows direct communication between users within a short distance, improving communication quality and reducing delays. The reuse of cell user resources can improve spectrum utilization [97]. However, this will also interfere with cellular users and increase the management burden of BSs. By utilizing deep reinforcement learning technology, a D2D user terminal functions as an agent capable of autonomously determining the best spectrum access plan without prior knowledge. This can lessen the load on the BS and enhance system efficiency.\nIn [98], a white box countermeasure attack evaluation method for the physical layer of semantic com-munication was proposed for the first time, which generates a high-concealed, data-independent and controllable physical layer attack generation method, solves the problem of 6G semantic communication air interface disturbance generation in a substantial interference wireless environment, and develops a hybrid countermeasure training method. The attack evolution method can counter a variety of typical adversarial interferences and can effectively assist wireless networks in detecting abnormal situations."}, {"title": "3.3 Air Interface Performance Enhancement", "content": "The performance of the wireless air interface will directly affect the user's communication experience. Finding a better air interface algorithm to optimize air interface performance has always been the goal of industry insiders, as shown in Figure 10. However, user demands are increasing with the continuous increase of communication services. Traditional algorithms incur complexity through complex matrix operations and reach the performance bottleneck."}, {"title": "3.3.1 \u0391\u0399-Based Channel Estimation", "content": "Channel estimation [99], [100] is one of the most critical components of wireless communication systems. The wireless environment is complex and time-variant [101] [102]; the signals are subjected to a variety of interference in the propagation process; when the signals reach the receiver, the amplitude, phase and frequency of the signals change. The role of channel estimation and channel equalization is to recover the signal as much as possible. In this sense, a good analysis and equalization algorithm is crucial to the receiver's performance, which determines the final resolution rate of the signal.\nChannel estimation algorithms based on training data sequences such as \"least square method\" [101], [103] and \"least mean square error\" [104], [105], are widely used in channel estimation. The scene of the wireless channel is complex, and the state changes in real time. When estimating the channel in real systems, the parameters in the channel estimation algorithm or the assumed model are prone to deviate from the actual channel states. As a result, the accuracy of the estimated channel can be different from the actual result. AI can remedy the mismatch between mathematical models and real scenarios in this communication [102, 106-108]. Principle: AI can learn any hidden structure and parameter to fit any complex function. Using AI to perceive wireless scenes and characterize network state space provides a new means of communication. The channel estimation process of actual scenes can be constructed into a neural network, and the algorithm's performance can be optimized by adjusting the structure and parameters of the neural network.\nConsidering the sparse mmWave channel matrix as a natural image, the authors of [106] propose a practical and accurate channel estimation framework based on the fast and flexible denoising convolutional neural network. Simulation results validate that the training speed of a quick and flexible denoising convolutional neural network is more rapid than that of other channel estimators without sacrificing normalized mean square error performance. [107] considered the time-frequency response of a fast-fading communication channel as a 2D image. This scheme considers the pilot values as low-resolution images and uses an SR network cascaded with a denoising IR network to estimate the channel. The results confirm that this pipeline can be used efficiently in channel estimation. To address channel distortion,"}, {"title": "3.3.2 \u0391\u0399-Based OFDM Receiver", "content": "The wireless communication transmitter and receiver are divided into multiple processing mods in OFDMules. Each module is responsible for specific sub-tasks, such as Quadrature amplitude modulation, an IFFT, channel estimation, signal detection, etc. The advantage of this design is that each module can be analyzed and optimized individually, resulting in a very efficient and stable system.\nHowever, the performance of the traditional modular OFDM receiver depends on the accuracy of the acquired channel information. Generally speaking, the more specific the channel considered in the algorithm design of each module, the better the receiver's performance. However, the actual system obtains the channel state information, and the parameters in the channel estimation algorithms or the models are prone to deviate from the actual channel state, resulting in the receiver's performance being much different from the results obtained.\nChannel estimation is one of the most critical technologies in the physical layer. The performance of channel estimation directly affects the quality of communication. Because the performance of the air interface is affected by many environmental factors, the channel estimated by traditional channel estima-tion methods often has a big gap with the actual channel. AI method can extract essential information in sparse and time-varying pilot frequencies. The complex mapping relationship between the pilot signal and the exact channel is effectively constructed, significantly improving channel estimation accuracy.\nThe mismatch between the mathematical model and the reality can be made up using AI as shown in Figure 11. In theory, AI can learn any hidden structure and parameter to fit any complex function. Using AI to perceive wireless scenes and characterize network state space provides a new means of communication. It can encode the information of actual scenes into neural networks and optimize the performance of algorithms by adjusting the structure and parameters of neural networks.\nRegarding an OFDM wireless receiver, some researchers have used a fully connected deep neural net-work to improve the existing modular OFDM receiver. The process of the OFDM receiver after removing the cyclic prefix is regarded as a black box. Many data samples are obtained as a training set to train the neural network, learn the parameters of each layer of the neural network, and minimize the difference between the output of the neural network and the original information. In this way, the fully connected neural network receiver, after learning, uses a large number of pilot frequency and corresponding chan-nel data as the input of multi-layer fully connected neurons and the corresponding demodulation signal as the output. Following extensive learning, the fully connected neural network receiver employs many pilot frequencies and matching channel data as the input of multi-layer fully connected neurons, with the accompanying demodulation signal serving as the output. Fitting the complex mapping relationship between the two can effectively improve the signal reception performance and demodulation accuracy and has a powerful ability to perform channel estimation and signal detection. Thus, the performance of the receiver is significantly improved.\nIn [109], a model-driven deep learning (DL) approach was proposed that combines deep learning with expert knowledge to replace existing orthogonal frequency division multiplexing receivers in wireless communications. In [110], a machine learning (ML) assisted physical layer receiver technique was proposed to demodulate the OFDM signals, subject to very high Doppler effects and corresponding distortions in the received signal. To find a balance between full-size CP and non-existent CP, the authors of [111] investigate the redundancy problem and propose a minimum redundant OFDM receiver using deep learning (DL) tools. In [112], the receivers were designed based on deep neural networks (DNNS) in deep"}, {"title": "3.3.3 AI-Based Beam Management", "content": "Compared with traditional multi-antenna technology, the large-scale multi-antenna system produces more beams and narrow widths and faces new challenges in beam alignment. In September 2022, the IMT 2030 (6G) promotion group released the Research report [116] on Very large MIMO technology, pointing out that hierarchical scanning is generally adopted to align the beam quickly. In the existing academic literature, the beam selection methods of multi-antenna systems can be divided into several categories, including beam selection based on beam scanning [117], [118], beam selection based on training signals [119], beam selection based on position prediction [120], [121], and beam selection [122], [123] based on hierarchical search.\nThe traditional beam management methods have some limitations when applied to millimeter wave Massive MIMO systems [124]. The primary cause is that the real-world wireless propagation environment often has a variety of it. After a beam is narrowed, the signal transmission is more susceptible to occlusion. The significant increase in the number of beams significantly increases the number of beam search operations in beam alignment. It is difficult to accurately select the optimal beam instantaneously.\nAI can fully explore the correlation between the beams themselves, which allows for solving the prob-lems existing in the traditional beam management method as shown in Figure 12. In the actual communi-cation scenario, the massive historical beams provide enough data for training AI models, and the search for the optimal beam based on the correlation between beams constructed by AI dramatically reduces the search space of the beam. At the same time, due to the implicit structure of AI, more complex functional relationships can be explored. Compared with the traditional beam management method received signal strength (RSS), even in a low signal-to-noise ratio environment, the optimal beam selection based on AI still has obvious advantages and robustness. As shown in Fig. 1, red and blue represent antenna arrays with two polarization directions. AI is used to intelligently select beams without beam search, effectively reducing system complexity.\nA wide beam can also be used to predict a narrow beam, reducing the beam search space and improving the accuracy of beam prediction. As the input of the AI model, the wide beam is used to predict the narrow beam without activating all antenna arrays, thus reducing the computational search cost on the BS side. Compared with the RSS method, the prediction accuracy is still high in low SNRs."}, {"title": "3.3.4 AI-Based CSI Feedback Algorithm", "content": "With the development of multi-antenna technology, CSI feedback design has become a critical point in the research of broadband wireless communication systems. In Frequency Division Duplex mode, the terminal estimates the channel through a downlink pilot to obtain downlink channel information. Since the reciprocity of upstream and downstream channels is not apparent, the terminal needs to feed CSI back to the BS through the uplink. The BS side needs to know the downlink channel to form a beam with better alignment. As the uplink resources are precious, ensuring the accuracy of channel reconstruction and reducing the CSI feedback cost as much as possible is one of the bottleneck problems to overcome in the massive MIMO system based on Frequency Division Duplex.\nTraditional CSI feedback includes codebook-based CSI feedback and compressed sensing-based CSI feedback. The standard maker designed the traditional codebook-based CSI feedback in advance and cannot adapt to all possible channel state changes. Sometimes, there is a significant feedback error [125]. CSI feedback based on compressed sensing [126] relies on the sparsity of the channel matrix to ensure performance. In other words, if the channel is not sparse enough, the accuracy of the CSI reconstructed based on compressed sensing theory will be affected. In addition, under the limited feedback overhead, either type of traditional CSI feedback requires many matrix operations, resulting in computing power overhead.\nThe massive CSI data and the inherent random characteristics of CSI make it preferable to introduce AI to design a new CSI feedback mechanism, as shown in Figure 13 compared with the traditional method. For example, the nonlinear characteristics of deep learning can be used to extract the features of CSI efficiently, and the original CSI data can be transformed into a more compact feature representation, thus reducing the communication overhead in the CSI feedback process. An example of an implementation is the autoencoder architecture, which uses neural networks to extract and compress the features of CSI channel information. The autoencoder consists of an encoder and a decoder. The encoder compiles the high-dimensional channel state information into code words. The decoder is responsible for decoding the code words into the original CSI. The internal structure of encoders and decoders of different algorithms is different, resulting in a difference in channel reconstruction performance. The authors of [127] apply deep learning to CSI feedback first. The proposed CsiNet scheme uses convolution to extract channel features, compress and reconstruct channels, and is superior to the CSI feedback scheme based on compressed sensing regarding feedback accuracy and computational complexity.\nThe studies in [128-131] increase the size of the convolutional kernel based on CsiNet to improve the perceptual field of view of the convolutional layer, which is conducive to extracting the sparsity of the channel and further enhancing the accuracy of channel reconstruction. Although a larger perceptive field of view can effectively extract the sparsity of CSI, a smaller convolution kernel can extract finer features from CSI. In [132], a Non-Local neural network is introduced based on the CsiNet network to capture a wide range of dependencies, and the accuracy of channel recovery was improved compared to CsiNet. CRNet proposed in literature [133] uses convolution kernels of different sizes for channel feature extraction and recovery, reducing the computation and improving the accuracy of channel reconstruction."}, {"title": "3.4 New Businesses Enabled", "content": "AI algorithms can dynamically allocate resources, such as bandwidth and power, to create and manage network slices in the wireless network. This can enable the creation of customized networks tailored to the needs of different applications and services. The booming development of AI has enabled the development of many new businesses, such as network slicing, mobility management, and wireless localization."}, {"title": "3.4.1 AI-Based Network Slicing", "content": "As a core component of 5G technology, network slicing divides infrastructure networks into multiple proprietary logical networks through virtualization technology to meet the customized needs of differ-ent vertical industries [134, 135]. Based on isolation principles, customization and end-to-end, network slicing can provide users with differentiated, mutually isolated, and customizable network services with functions and capacities while ensuring the realization of the end-to-end service level agreement (SLA) for services. Although network slicing technology has brought revolutionary changes to 5G networks, its implementation still faces challenges and limitations [136, 137]. Firstly, the complexity of managing and maintaining network slicing increases with the number of slices while meeting the customized needs of different industries and applications. Secondly, the dynamic network environment and changing service demands require network slicing to adapt and adjust quickly, which is often difficult to achieve under the traditional network management framework. In addition, ensuring the isolation between different network slices to prevent resource contention and performance interference is also a vital issue in implementing network slicing [138].\nAfter introducing AI technology, the management and optimization of network slicing will face changes [139] [140]. AI technology can automate the configuration, optimization, and troubleshooting of network slicing through a data-driven approach, significantly improving network flexibility and efficiency. For example, AI technology can use raw monitoring data from SDN (software-defined network) controllers to predict the impact of new slicing on the network and make the right resource management decisions to ensure efficient use of network resources while reducing operational and maintenance costs [141]. AI can also enhance the adaptive capability of network slicing, enabling it to respond in real time to changes in network conditions and the evolution of service demand, thus providing more stable and high-quality services."}, {"title": "3.4.2 \u0391\u0399-Based Wireless Localization", "content": "Wireless positioning refers to obtaining the location information of mobile users in a cellular environment. This location information is usually in the form of the mobile user's geographic coordinates relative to some reference points. Wireless location is also commonly referred to as mobile location, wireless location, and geolocation [142].\nWireless positioning technology can be divided into wide and local area positioning according to the scope of services. Wide area positioning provides users with universal positioning services. Local area positioning is mainly used in indoor scenarios. Wide area positioning primarily includes mobile commu-nication network positioning and global satellite navigation systems (e.g., the global positioning system). The GPS is a kind of high-precision radio navigation positioning system based on artificial Earth satel-lites, which can provide accurate geographical position, vehicle speed and accurate time information anywhere in the world and near-earth space. Local wireless positioning mainly includes Wi-Fi, Blue-tooth, UWB, etc. Its positioning technology can be divided into four types: ranging positioning based on signal strength (RSS), fingerprint positioning based on signal characteristics, and positioning based on signal transmission time and Angle measurement [143].\nMany applications need high-precision positioning, such as industrial AGV and asset tracking, espe-cially indoor precision positioning, but GPS cannot be used indoors. Despite having a specific positioning reference signal, LTE cannot meet the requirements for high-precision positioning because of its low po-sitioning accuracy and 100-meter BS separation (20 MHz). Bluetooth, Wi-Fi, UWB and other wireless location-based technologies have high deployment costs and are difficult to become a universal positioning technology [144].\nAI technology has been introduced to achieve high-precision positioning in the 5G era. One approach is to utilize the random forest technique, which can create a classification model to split a vast area into many small grids and then predict the grid where the user is located. It can also make a regression model to predict the user's position coordinates. In another method, multi-layer perceptron has a similar application to random forest and can establish regression and classification models to locate targets [145]. AI algorithms can also be trained to recognize the fingerprint of a wireless signal in a specific location and process and fuse data from different sensors (such as Wi-Fi and Bluetooth) to achieve high-precision positioning.\nWith the widespread use of massive MIMO technology, AI has shown more significant advantages in high-precision positioning [7]. Since systems already use CSI, there is no additional cost for position-ing. Using the location in the LOS/NLOS channel co-existence scenario based on the neural network as an example, machine learning and a significant volume of channel data can help effectively map the relationship between channel response and position coordinates, resolving the location problem in com-plex environments and increasing location accuracy. Very high positioning accuracy is achieved without negligible additional cost, even about 20 mm accuracy under indoor LOS and NLOS conditions."}, {"title": "3.4.3 AI-based Situational Awareness", "content": "Network situational awareness technology can bring real-time monitoring, prediction and response capa-bilities to mobile communication networks, including network management [87] and optimization [88], [85], [90], security monitoring, and fault detection and prediction [146]. The technology includes three levels of environmental element awareness, situation understanding and situation prediction. The net-work system can recognize and understand various network environmental factors, such as network state, network data, and user behavior patterns while making predictions and timely responses to the future trends of the network state. Figure 14 shows the AI-empowered situational awareness.\nIn the traditional situational awareness system, the rule engine is typically used to realize the perception of the network state. This sensing method is based on pre-defined rules and logic, which depends on the experience of the rule maker and cannot fully deal with the underlying information and patterns in big data. Traditional systems use data mining and statistical analysis methods to make predictions for network situation prediction models. These methods face problems such as inflexibility and difficulty dealing with complex patterns.\nApplying AI technology can significantly enhance the performance of situational awareness systems. Network awareness enabled by AI technology is a data-driven network cognition mode, which can effec-tively improve the problems existing in traditional situational awareness systems. The massive network data collected by the system under the situational element awareness model can provide data support for AI. In this model, the system can more accurately identify the state of different modes in the network, including traffic, logs, and network KPIs, through AI methods such as deep learning and predict the development trend of the network. At the same time, AI technology can use the perceptual information and the predicted state for analysis and make accurate and effective response decisions.\nSpecifically, it can be applied from the following four aspects:\n1) Application of deep learning in network situation awareness: The application of deep learning in network situation awareness mainly focuses on effectively analyzing and processing large-scale network data to predict network state and future trends accurately [147]. Traditional situational awareness systems rely on pre-defined rules and logic, and these approaches often fail to deal with complex patterns and underlying information in big data. In contrast, by building complex neural network models, deep learning can extract valuable features from massive network traffic and log data, and make efficient classification and prediction. For example, deep learning models like CNN and RNN are used to process network traffic data to identify abnormal behavior and security threats. These models can learn and adjust automatically"}, {"title": "4 Network for AI", "content": "The current network has achieved preliminary success in advancing AI4NET, developing a series of AI use cases and scenarios, and partially implementing standardization efforts. However, most of these AI4NET implementations adopt an \"add-on\" design pattern, which, while convenient, may introduce additional challenges (such as latency, data privacy concerns, etc.), hindering the network's evolution towards more efficient AI services to a certain extent. The underlying reason lies in the insufficient support the current network architecture provides for deeply integrating AI into the network. Although 5G exhibits a trend towards cloud-native, IT software-"}]}