{"title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation", "authors": ["Fabian Paischer", "Benedikt Alkin", "Lukas Hauzenberger", "Marc Peter Deisenroth", "Thomas Schmied", "Sepp Hochreiter"], "abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.", "sections": [{"title": "Introduction", "content": "Foundation models (FMs) are usually trained on large-scale data and then fine-tuned towards a particular downstream task. This training paradigm has led to significant advancements in the realm of language modeling, computer vision, and reinforcement learning. With an increasing number of model parameters, the process of fine-tuning becomes prohibitively expensive. This results in the need for efficient alternatives to fine-tuning all parameters of the pre-trained model.\nParameter-efficient fine-tuning (PEFT) approaches are commonly used as an effective alternative to full fine-tuning (FFT). PEFT methods modify the pre-trained model by introducing a small number of new trainable parameters, while the pre-trained weights remain frozen. This leads to a substantial reduction in computational cost, both in terms of time and space. A particularly successful approach, LORA , introduces new weights in the form of a low-rank decomposition for each weight matrix in the pre-trained model. After training, the new weights can be readily merged into the pre-trained weights without any additional inference latency. Recent research has explored two main avenues for enhancing LoRA: weight-driven initialization and adaptive rank allocation during training. While weight-driven initialization methods have shown promise, they typically rely on a uniform rank distribution across pre-trained weights. Further, they are constrained to the information stored in the pre-trained weights. Finally, existing adaptive rank allocation techniques initialize low-rank matrices randomly.\nWe propose a new method based on LoRA that combines adaptive rank allocation with data-driven initialization by leveraging information from the downstream task at hand. Certain activation patterns of FMs have shown to be crucial for model performance. Therefore, we leverage minibatches of activations computed on downstream data to initialize LoRA weights. To this end, we propagate minibatches of the fine-tuning data through the model and compute the singular value decomposition (SVD) on activation vectors to obtain the right-singular vectors. We then sort the right-singular vectors in descending order according to the variance they explain. Finally, we leverage the top-k components according to a given rank budget for initializing LoRA. This results in an effective initialization, that (i) is data-driven by leveraging information from the downstream task, and (ii) allocates ranks to pre-trained weights to maximize the explained variance throughout the model. We call the resulting method EVA, which is short for Explained Variance Adaptation. Importantly, this procedure can be performed within the first few minibatches of LoRA fine-tuning without significant computational overhead.\nWe demonstrate the benefits of EVA on an array of downstream tasks, namely language generation and understanding, image classification, and reinforcement learning (RL). EVA consistently improves average performance across a multitude of tasks on each domain compared to LoRA and other recently proposed initialization or rank redistribution methods. For language generation, we fine-tune 7B-9B parameter language models on math and reasoning tasks, where EVA attains the highest average performance. Further, on a set of language understanding tasks, EVA improves the average performance compared to competitors. On image classification we fine-tune a pre-trained vision transformer on a set of 19 diverse tasks. We find that EVA attains the highest average score and improves over LoRA and established extensions thereof, with most gains on in-domain data. For our RL experiments we conduct fine-tuning on continuous control tasks and find that EVA significantly exceeds performance of LoRA and even exceeds performance of full fine-tuning (FFT) when combined with DORA. Finally, we conduct ablation studies to demonstrate that the combination of direction and scale of EVA leads to the best performance."}, {"title": "Related Work", "content": "LoRA has sparked widespread interest in leveraging low-rank decompositions for fine-tuning due to its simplicity. Building on the success of LoRA, a number of other variants have been proposed."}, {"title": "Method", "content": "EVA aims at initializing LoRA weights in a data-driven manner by leveraging data from the downstream task. Since EVAbuilds on low-rank decomposition of weight matrices as in LoRA , we first briefly explain LoRA in Section 3.1. In Section 3.2, we describe how we obtain an effective initialization for the low-rank decomposition of LoRA matrices via SVD on activation vectors. This enables an adaptive assignment of ranks across all layers to maximize the explained variance throughout the pre-trained model We explain this in more detail in Section 3.3."}, {"title": "Low-Rank Adaptation (LoRA)", "content": "LORA adds new trainable weights which are computed via an outer product of low-rank matrices. This is motivated by the low intrinsic dimensionality of language models and relies on the assumption that the gradients during fine-tuning are also of low rank. In the following, we explain LORA in more detail. Let $x \\in \\mathbb{R}^{d\\times 1}$ be the input to a pre-trained weight matrix $W \\in \\mathbb{R}^{k \\times d}$. Then, LoRA introduces new weight matrices A and B as a low-rank decomposition\n$h = Wx + BAx, \\tag{1}$\nwhere $B \\in \\mathbb{R}^{k\\times r}$ and $A \\in \\mathbb{R}^{r \\times d}$. The rank r is a hyperparameter with $r \\ll k$. During fine-tuning, W remains frozen and only A and B are updated. Usually B is initialized with zeros, such that fine-tuning starts from the pre-trained model. A is usually initialized at random. Additionally, Hu et al. (2022) introduce a hyperparamter $\\alpha$ which is used to scale BAx by $\\frac{\\alpha}{r}$."}, {"title": "Data-driven Initialization of Low-Rank Adaptation", "content": "Our aim is to find an effective initialization for the low-rank matrix A in a data-driven manner to maximize performance on the downstream task. To this end, we perform SVD on batches of activation vectors $X \\in \\mathbb{R}^{b\\times d}$ to obtain the right-singular values, which constitute the directions that capture most of the variance. This procedure is done during the initial training stage where we propagate minibatches of data through the model and incrementally update the right-singular vectors. More formally, we collect batches of activations $X^i$ for N pre-trained weight matrices $W_i \\in \\{W_0, W_1, ..., W_N\\}$ that we choose to update fine-tune. Subsequently, we compute the SVD on each $X^i$ to obtain the right-singular vectors $v_j^i$ and respective singular values $\\sigma_j^i$ as\n$X^i = \\sum_{j=1}^{r} \\sigma_j^i u_j^i(v_j^i)^T. \\tag{2}$\nImportantly, we compute the SVD incrementally on each minibatch of fine-tuning data and update $v_j^i$ after each forward pass through the model. After each step we check whether $v_j^i$ has converged. To this end, we measure the column-wise cosine similarity between subsequent computations of $v_j^i$: $\\tilde{v_j^i}$: and determine convergence based on a threshold T. If the right-singular values have converged, i.e. $cossim(v_j^{i-1}, v_j^i) \\geq \\tau \\forall 1 \\leq j \\leq r$, we initialize $A = v_j^i$ and exclude the corresponding weight matrix from subsequent SVD computations. We continue this procedure until all $v_j^i$ have converged."}, {"title": "Adaptive Rank Allocation", "content": "The singular values obtained by SVD provide an estimate of the variance that is explained by their components. Leveraging this information, we can redistribute ranks across weight matrices of the pre-trained model such that the maximum amount of variance is explained. This can be done by allocating more ranks to layers that propagate more information, i.e., explain more variance. More formally, the variance explained by each component in $v_j^i$ is given by their explained variance ratio\n$\\xi_j^i = \\frac{(\\sigma_j^i)^2}{(M-1) ||\\sigma^i||_1},\\tag{3}$\nwhere $|| \\cdot ||_1$ denotes the $l_1$ norm, $\\sigma^i$ is a vector containing all r singular values, and M is the total number of samples used for the incremental SVD computation. Next, we sort the components $v_j^i$ for each weight matrix in descending order according to their explained variance ratio $\\xi_j^i$. Finally, we assign ranks to pre-traiend weights until we reach a certain rank budget.\nAdditionally, we introduce a hyperparameter $\\rho \\in [1, \\infty)$ which controls the uniformity of the rank distribution. p determines the number of ranks that we compute during SVD and increasing p allows for an increasingly heterogeneous rank distribution. That is, for each $W^i$ we compute $\\rho r$ components initially meaning we obtain $N\\rho r$ components in total. For the redistribution we only use the top Nr components according to their explained variance ratio. Thus, setting $\\rho = 1$, results in a uniform rank distribution as in LoRA, but initialized according to EVA. Therefore, p provides us with the means to change the rank distribution in a controlled manner prior to fine-tuning at the initialization stage, as opposed to learning it throughout the training process as done in prior works . In practice we found that the redistribution converges for values of $\\rho > 2$. Finally, we initialize B with zeros and perform the standard LORA fine-tuning, as recommended in . In Algorithm 1 we provide pseudocode for EVA."}, {"title": "Experiments", "content": "First, we elaborate on implementation details of EVA in Section 4.1. Then, we show results for fine-tuning large language models (LLMs) on math and reasoning tasks in Section 4.2 and language understanding tasks in Section 4.3. Further we show results for image classification in Section 4.4 and decision making tasks in Section 4.5. Finally, in Section 4.6 we demonstrate that the computational overhead induced by EVA over LoRA is negligible and that incremental SVD converges and is invariant to batch order and batch size."}, {"title": "Implementation Details", "content": "We follow the standard LoRA training procedure from Hu et al. (2022). Similar to Kalajdzievski (2023), we found LoRA training to be very sensitive to the scaling parameter a. Therefore, we set a = 1 since we found this to be the most stable setting and additionally tune the learning rate. We apply EVA to pre-trained weights only, i.e., we do not initialize newly introduced classifier heads. Following Zhang et al. (2023a), we apply LoRA to all pre-trained weight matrices except for the embedding layer. For EVA we always search over $\\rho \\in \\{1,2\\}$ to cover both uniform and non-uniform rank allocation and report the best score. All models we used are publicly available on the huggingface hub . For the implementation of baselines we leverage the widely used PEFT library. Across experiments we highlight the highest scores in boldface and underline the second-highest."}, {"title": "Language Generation", "content": "We fine-tune three different LLMs, namely Llama-2-7B , Llama-3.1-8B , and Gemma-2-9B on common sense and math reasoning benchmarks. For common sense reasoning we follow and amalgamate a training set consisting of BoolQ , PIQA , SIQA , HellaSwag , Winogrande , ARC-e and ARC-c and OpenBookQA . We apply all methods listed in Table 1 to all three models and additionally add a comparison to DoRA and EVA+DORA, which combines EVA with DORA. We train all methods with rank r = 16 and a learning rate of 5e - 4 for three random seeds. Further details on the fine-tuning settings can be found in Appendix B. We present our results in Table 2. For Llama-2-7B and Llama-3.1-8B EVA+DORA ($\\rho$ = 1) is the best performing method on average while also exhibiting the best individual scores on most tasks. For Gemma-2-9B, EVA with adaptive ranks ($\\rho$ = 2) yields the highest performance. EVA as well as EVA+DoRA are consistently among the best performing methods on all individual tasks. This highlights the effectiveness of EVA's data-driven initialization and rank allocation.\nFor the math fine-tuning experiments, we fine-tune all models on the MetaMathQA dataset for one epoch with the same hyperparameters that we used for the common sense reasoning benchmarks and report the results in Table 3. We observe that EVA attains the highest performance on the GSM8K dataset for Gemma-2-9B using $\\rho$ = 2. For Llama-2-7B and Llama-3.1-8B the best performing method is EVA+DoRA using $\\rho$ = 1 closely followed by EVA. On MATH, EVA+DORA performs best for Llama-2-7B with $\\rho$ = 1, while EVA attains the highest score for Llama-3.1-8B with $\\rho$ = 1 and Gemma-2-9B with $\\rho$ = 2. These results indicates that the performance of adaptive rank allocation depends on the selected model. We further analyze the resulting rank distributions for different values of $\\rho$ for Llama-2-7B and their effect on downstream performance in Appendix G. Finally, we provide additional results for Llama-2-7B on code fine-tuning tasks in Appendix B."}, {"title": "Language Understanding", "content": "We train ROBERTaLarge and DeBERTav3Base on the GLUE benchmark . The GLUE benchmark comprises eight downstream tasks, such as natural language inference, or sentiment analysis. Additionally to learning rate, we also search over different ranks within a maximal rank budget (r = 16). For further details about datasets, implementation, or hyperparameters, we refer the reader to Appendix C. We also add FFT as a baseline, but neglect EVA+DORA due to time constraints and report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for the remaining tasks in Table 4. For ROBERTaLarge, EVA attains the highest scores on QNLI, COLA, MRPC, RTE, and STS-B, leading to the highest average score. Interestingly, DoRA usually only slightly improves over LoRA on low resource tasks (RTE, MRPC), while performing worse in high resource tasks (MNLI, QNLI, QQP, SST2). We also compare LoRA to EVA in Table 14 in Appendix C for different rank budgets, where EVA consistently improves over LoRA. For DeBERTav3Base, EVA reaches the highest scores on SST2, RTE, and STS-B, again leading to the highest average score across all tasks. We visualize resulting rank distribution patterns of EVA for different GLUE tasks in Appendix C. More ranks are assigned to higher layers of the query, key, and value projections in the self-attention, while the remaining weights often receive a lower number of ranks. This is a consistent pattern for both, DeBERTav3Base and ROBERTaLarge."}, {"title": "Image Classification", "content": "We investigate the efficacy of EVA on the VTAB-1K benchmark, which has been widely used to evaluate PEFT methods. VTAB-1K comprises 19 image classification tasks that are divided into natural images, specialized images (medical images and remote sensing), and structured images (e.g. orientation prediction, depth estimation or object counting). We fine-tune a DINOv2-g/14 model that consists of around 1.1B parameters. For implementation details or hyperparameters we refer the reader to Appendix D. Our results are shown in Table 5 and we additionally report error bars in Table 17. EVA and EVA+DORA attain the best and second-best average accuracy across all tasks, respectively. Interestingly, EVA mainly improves over competitors on the natural tasks, i.e. in-domain datasets. LoRA performs best on the specialized tasks and full fine-tuning (FFT) performs best on the structured task. However, both LoRA and FFT perform worse on the remaining tasks, leading to a worse average score compared to EVA and EVA+DORA."}, {"title": "Decision Making", "content": "We follow the single task fine-tuning experiments in and fine-tune a Decision Transformer (DT) on the Meta-World benchmark suite . Meta-World consists of a diverse set of 50 tasks for robotic manipulation, such as object manipulation, grasping, or pushing buttons. We split Meta-World according to into 40 pre-training tasks (MT40) and 10 fine-tuning tasks (CW10). We pre-train a 12M parameter DT on MT40 and fine-tune it on the CW10 holdout tasks. We report success rates and standard errors for each task of CW10 in Table 6. We observe that EVA significantly reduces that gap between LoRA and FFT. Furthermore, DoRA performs particularly well in this experiment and exceeds FFT performance. Finally, our EVA+DoRA even improves upon DoRA and attains the best average performance across all tasks."}, {"title": "SVD Convergence Analysis", "content": "The data-driven initialization of EVA relies on incremental SVD on minibatches of activations in the initial training stage. In Figure 3, left, we show that this process converges for Llama-2-7B on MetaMathQA for different minibatch sizes. Using a minibatch size of 4 the computation for EVA's initialization lasts for approximately 80 seconds, which corresponds to around 90 minibatches. For a batch size of 32 the computation of the SVD components takes around 500 seconds. In Figure 3, right, we additionally show, that the main components obtained via SVD mostly remain consistent across different batch orders for a batch size of 4, again for Llama-2-7B on MetaMathQA. To this end, we plot cosine similarity between components obtained via incremental SVD after rank redistribution. These results indicate that these models exhibit certain activation patterns that remain consistent across different batch orders which lead to a robust initialization for EVA. We also show that the components for different batch sizes converge to mostly the same final initialization in Appendix F."}, {"title": "Ablation Studies", "content": "Finally, we conduct ablation studies on EVA to investigate important factors that contribute to its performance. Specifically, we investigate the impact of scale and directions. To this end, we use the VTAB-1K dataset because it comprises a diverse set of tasks and allows for a systematic investigation on in-domain data (natural), and out-of-distribution data (specialized and structured). We report results for our ablation studies in Table 7 and explain the different settings in the following paragraphs.\nEffect of scale. To investigate the effect of scale on the initialization, we add a setting which uses whitening (EVA-whiten). Whitening scales the initialization by the reciprocal of their eigenvalues, which alters scale, but preserves directions. We found that whitening can significantly improve performance on structured (out-of-distribution) tasks even leading to a slightly higher average score than EVA. This indicates that scale is especially important for structured data. However, EVA-whiten experiences a slight performance drop on natural and specialized tasks.\nEffect of directions. To address the importance of the directions of the components, we randomly permute its rows (EVA-perm). This preserves scale while corrupting directions and 12 norm of A. Additionally, we add a setting where we randomly rotate A (EVA-rot), which preserves l2 norm, but alters directions. We find that altering directions leads to a performance drop on the structured tasks, while changing 12 norm leads to a drop on the natural tasks. Both, EVA-perm and EVA-rot lead to worse average performance across all tasks compared to EVA."}, {"title": "Discussion and Limitations", "content": "Alternative data-driven initialization schemes. We also investigated alternative data driven initialization schemes. Such alternatives include, but are not limited to, Kernel-PCA or Linear Discriminant Analysis . While Kernel-PCA can account for non-linearities in the data, it scales with the number of datapoints. In our setting we perform SVD on minibatches of sequences, therefore, the number of datapoints grows fast, making Kernel-PCA impractical. LDA projects the data onto a subspace that maximizes linear separability between classes. Such an initialization scheme is particularly interesting for classification tasks like GLUE or VTAB-1K. However, we observed on the GLUE tasks that the LDA projection matrix never converges.\nAdditional latency of SVD. EVA leads to performance improvements over LoRA, but introduces additional latency in the beginning of training for computing the data-driven initialization. We demonstrated that this process converges quickly. In Appendix F we also show that this process is mostly invariant to the batch size, meaning that smaller batch sizes may be used for the SVD computation. This results in an additional cost in the range of 100 seconds for a Llama-2-7B model, which is negligible. Further, the SVD computation does not require backpropagation and storing of optimizer states. Hence, there is no overhead with respect to memory.\nWhat method performs well on which tasks? Throughout all of our experiments, we observed that EVA is the most stable method and consistently improves average scores across tasks for all domains compared to competitors. Interestingly, DoRA only outperformed LoRA on experiments with larger models and on RL tasks. Furthermore, FFT performed particularly well on out-of-distribution tasks in our image classification experiments, but often performs worse on in-domain or low resource tasks. Contrary, EVA consistently advances average performance on a wide range of tasks, establishing its potential as state-of-the-art fine-tuning method.\nReproducibility. We provide the source code to reproduce all our experiments (see Appendix A for more details). Further, we integrate EVA into the widely used PEFT library."}, {"title": "Conclusion and Broader Impact", "content": "We propose a novel method named Explained Variance Adaptation (EVA), extending the widely used LORA with data-driven initialization and rank re-distribution. We initialize LoRA matrices in a data-driven manner by performing SVD on minibatches of activation vectors. Further, we re-distribute ranks across weight matrices according to the amount of variance they explain. In this regard, we also introduce a hyperparameter that allows for a controlled investigation of different rank distributions. Thereby, in EVA we bind the benefits of adaptive rank allocation and data-driven initialization, resulting in one initialization to rule them all. We demonstrate performance gains of EVA over LORA and initialization schemes thereof on a variety of domains, ranging from language to vision and RL. EVA variants consistently reach the highest average performance on a wide range of tasks across all domains.\nWe believe that EVA sheds a novel view on LoRA fine-tuning, where initialization of the newly introduced weights is guided by the downstream data and can have a significant impact on future research on fine-tuning of foundation models. In the future, we aim at investigating the effect of incorporating gradient information in EVA and quantization, as well as alternative data-driven initialization schemes."}]}