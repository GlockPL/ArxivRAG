{"title": "Physics-Based Hybrid Machine Learning for Critical Heat Flux Prediction with Uncertainty Quantification", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert K. Salko", "Xu Wu"], "abstract": "Critical heat flux is a key quantity in boiling system modeling due to its impact on heat\ntransfer and component temperature and performance. This study investigates the devel-\nopment and validation of an uncertainty-aware hybrid modeling approach that combines\nmachine learning with physics-based models in the prediction of critical heat flux in nu-\nclear reactors for cases of dryout. Two empirical correlations, Biasi and Bowring, were\nemployed with three machine learning uncertainty quantification techniques: deep neural\nnetwork ensembles, Bayesian neural networks, and deep Gaussian processes. A pure ma-\nchine learning model without a base model served as a baseline for comparison. This study\nexamines the performance and uncertainty of the models under both plentiful and limited\ntraining data scenarios using parity plots, uncertainty distributions, and calibration curves.\nThe results indicate that the Biasi hybrid deep neural network ensemble achieved the\nmost favorable performance (with a mean absolute relative error of 1.846% and stable\nuncertainty estimates), particularly in the plentiful data scenario. The Bayesian neural\nnetwork models showed slightly higher error and uncertainty but superior calibration. By\ncontrast, deep Gaussian process models underperformed by most metrics. All hybrid mod-\nels outperformed pure machine learning configurations, demonstrating resistance against\ndata scarcity.", "sections": [{"title": "1. Introduction", "content": "Critical heat flux (CHF) in a boiling system occurs when a boiling regime changes from\nnucleate boiling to transition boiling; this change is marked by a sudden decrease in the\nability to transfer heat in a two-phase flow system. The physical presentation of CHF can\ngenerally be categorized into two mechanisms: departure from nucleate boiling (DNB)\nand dryout (DO). DNB occurs when steaming leads to a sustained insulating blanket of\nvapor between the heated surface and the liquid phase, inhibiting heat transfer efficiency.\nThis occurs in low-quality flows, in which the liquid phase still predominantly occupies\nthe flow channel cross section. DO is a phenomenon in high-quality flows, in which there\nis a gas/entrained droplet core and a liquid film along the walls of the channel. DO is\nindicated when the liquid film thickness reaches zero. In the case of conventional heat\nexchangers and boilers, DO can create large inefficiencies and cause tubing to rupture. In\na nuclear system, the consequences can be more catastrophic: fuel cladding failure and\nthe potential for a fuel melt. Therefore, CHF is a key safety-related quantity that requires\nrigorous consideration during the thermal-hydraulic analysis of a nuclear system.\nMuch work has been performed over the past 80 years in studying CHF. The goal\nhas been to reliably predict its occurrence in a variety of operating conditions in various\ngeometries. Most of these attempts have been in the form of tube experiments, in which\na moving fluid at a specified set of thermal-hydraulic conditions is heated until CHF is\nreached. Several empirical correlations\u2014including the Biasi, Bowring, CISE-4, and W-3\ncorrelations, among others\u2014have been developed from the resulting data [1]. Each of\nthese correlations has a unique range of validity, and some are specialized for bundles\nor DNB/DO specifically. The 2006 Groeneveld look-up table (LUT), which is widely\nused throughout the nuclear industry, was also developed by leveraging a compilation\nof data from 59 experiments [2]. This combined dataset, which was used to create\nthe Groeneveld LUT, consists of nearly 25,000 entries that describe the experimental\nconditions for uniformly heated vertical tubes using seven input variables and the resulting\nCHF values for cases of both DNB and DO. Although the empirical correlations and the\nLUT are effective at computing CHF values for a variety of different input combinations,\nthey still have substantial deviations from experimental measurements in various regions\nof the operational space [3]. As a result of this and other factors, work has not ceased in\nattempting to find a more accurate method of predicting CHF.\nSome of the most recent developments in this space have been in the use of data-driven\nmachine learning (ML) techniques, such as deep neural networks (DNNs), convolutional\nneural networks (CNNs), conditional variational autoencoders (CVAEs), support vector\nmachines (SVMs), and random forests (RFs) [4][5][6][7][8][9][10]. These approaches rely\non fitting parameters to training data and do not possess any knowledge about the physical\nworld or problem. The primary benefits of these approaches are their ability to find\nrelationships that may not be readily apparent in the data often leading to a higher overall\naccuracy in comparison with traditional methods\u2014as well as their inexpensive nature and\nquick performance post-training. In 2022, under the guidance of the Organisation for\nEconomic Co-operation and Development (OECD) Nuclear Energy Agency (NEA), the\nTask Force on Artificial Intelligence and Machine Learning for Scientific Computing in\nNuclear Engineering was formed with the objective of creating an ML benchmark for CHF\nprediction [11]. Phase one of this project focused on feature analysis and the training and\nevaluation of ML regression models using the following parameters (or some of them) as\ninputs: tube diameter (D), heated length (L), pressure (P), mass flux (G), and equilibrium\nquality (xe).\nAlthough these surrogate model approaches show greatly improved accuracy in com-\nparison with the Groeneveld LUT, one major limitation is in the interpretability of the\nmodels themselves. These data-driven methods are often described as \"black boxes\". Be-\ncause their complexity increases as the number of fitting parameters scales up; these fitting\nparameters are often on the order of thousands or even millions, depending on the problem\nand ML technique. Highly sensitive applications, such as nuclear analysis, require the\ndevelopment of a framework that emphasizes explainability and reliability in the models'\npredictions.\nData scarcity is another substantial concern in attempting to train data-driven models\nusing the limited datasets available when working with experimental data. Traditional\nML approaches, such as DNNs, require a relatively large amount of high-quality data\n(typically on the order of hundreds of thousands of data entries) to achieve acceptable\nperformance. Providing inadequate amounts of data or entries with a high degree of noise\ncan significantly degrade model performance to the point of a complete breakdown of the\ntraining process. One attempt [12] to solve this concern considered DNB in 5 \u00d7 5 bundle\ndata from the pressurized water reactor subchannel and bundle test benchmark (PSBT).\nThis study focused on using generative adversarial networks to augment a 76-point dataset\nwith synthetic values to be later used to train a DNN. Another study [13] also attempted to\nuse a generative approach to mitigate this problem in the prediction of void fractions in a\nboiling water reactor (BWR) bundle benchmark.\nAlthough these approaches have been shown to be effective at mitigating the scarcity\nconcern, they still lack explainability or a guarantee that completely nonphysical results\nwill not be produced. A recently proposed method not based on data augmentation [14]\nworks to leverage knowledge of the physical world by using a \u201cbase model\u201d in combination\nwith a data-driven ML model. This hybrid approach first uses an established model,\nsuch as an empirical thermal-hydraulic correlation, to compute an estimate for a target\noutput. The estimate value is then corrected with an ML model trained to predict the\nresidual between those outputs with known experimental values. This correction will\ncompensate for the bias and undiscovered mismatch between the domain knowledge-based\nmodel and actual observation. This arrangement is classified as a parallel \u201cgray-box\u201d\napproach [15], which is easier to interpret [16] because the bulk of physical knowledge is\nprovided by the base model, reducing the amount of inferred knowledge required by the ML\ncomponent. Structuring the model in this manner offers a larger degree of interpretability,\nperformance benefits in comparison with stand-alone ML methods, and built-in resistance\nto the deleterious effects of data limitations.\nOther studies have examined the use of these hybrid models. One such study compares\nthe use of DNNs, CNNs, and RFs as the ML technique paired with the Biasi correlation, the\nGroeneveld LUT, or the Zuber correlation as the base model [17]; this study used D, L, P,\nG, Xe, Tinlet, and Ahsub as input parameters. Uncertainty quantification (UQ) was performed\nvia input perturbation to capture data uncertainty but not uncertainty resulting from other\nsources, such as randomness in the training process, model architecture, or extrapolation,\namong others. The RF model using the LUT as the base model was shown to have the\nsmallest relative error. Another study [18] provides a comparison between the stand-alone\nLUT and the artificial neural network, SVM, and RF hybrids, each of which used the LUT\nas the base model. This study concluded that hybrid models exhibit greater accuracy in\nevery case in comparison with the stand-alone LUT and nonhybrid ML variants. The\nspecific location of CHF occurrence, in addition to its magnitude, has also been used as a\nprediction target in rectangular channels [19].\nIn a follow-up study [20] to Zhao et al. [14], this hybrid approach implemented the Biasi\nand Bowring CHF empirical correlations [1] as prior models and assessed the performance\nof these models against a purely data-driven DNN in a series of training set sizes. In all\neight of the training size cases, the hybrid models outperformed the pure DNN, and the\nmaximal improvement for the most data-scarce case. In this case, in which the training set\nhad only nine points, the pure DNN's performance decreased to 48.25% relative error, but\nboth hybrid models still outperformed the stand-alone empirical correlations with relative\nerrors below 6.40%.\nThis study builds on work previously performed using hybrid knowledge-based models\nin the prediction of CHF, specifically for cases of DO. This study is focused on new\ncontributions in three key areas: further improving the performance of the hybrid models"}, {"title": "2. Background", "content": "Exceeding the CHF can result in changes to boiling that present themselves differently\nin BWRs and pressurized water reactors. In BWRs, boiling is a part of normal operation\nand is the primary form of heat transfer from the fuel bundles. The boiling column develops\nfrom a liquid state through a series of flow regimes: bubbly, slug, and then annular, which\nconsists of a gas core containing entrained liquid droplets. As heat fluxes increase, the\nfilm layer on the heated surface decreases in depth. Once CHF is reached, DO can occur;\nthis means that the liquid film has \u201cdried out,\" leaving an uncovered surface. DO causes\na significant rise in temperature on the heated surface, and this temperature rise can cause\nfuel damage. Because of these potential consequences, a safety-related parameter known\nas the minimum critical power ratio is implemented in BWRs to ensure adequate margin\nto DO.\nIn this study, the DO state is considered exclusively to ensure that the data used are\nfrom the same underlying CHF condition. The public CHF dataset contains both DNB and\nDO experiments, which are separable by establishing a constraint on equilibrium quality\n(xe). Instances of DO have far larger equilibrium qualities in comparison with those of\nDNB, so a conservative threshold was set at 0.2 [21].\nThe NRC pubic CHF database, used to construct the 2006 Groeneveld LUT, formed\nthe source dataset for this investigation. This compilation contains both DNB and DO\nexperiments, which can effectively be separated based on equilibrium quality (xe). In\nthis study, the DO state is considered exclusively to ensure that the data used is from the\nsame underlying CHF condition. As an initial filter on the source dataset, a conservative\nequilibrium quality threshold of above 0.2 was set, since DO instances have far larger\nqualities compared to DNB [21].\nOutside the Groeneveld LUT, two widely accepted empirical correlations to predict\nCHF are the Biasi and Bowring correlations [1]. Both correlations have constraints in"}, {"title": "2.1. Critical Heat Flux", "content": "2.2. UQ of Data-Driven Machine Learning\nWithin modeling systems, two forms of uncertainty exist: aleatoric uncertainty and\nepistemic uncertainty. Aleatoric uncertainty arises from the inherent randomness present\nwithin data, from aspects such as measurement error, or from the intrinsic variability\nof the system being observed. This form of uncertainty is irreducible, even when more\ndata are provided. Conversely, epistemic uncertainty represents the limitations of the\nmodeling approach in attempting to capture the underlying target distribution. Epistemic\nuncertainty can arise from various sources, such as parameter uncertainties, differences\nin model architectures, and limitations in the training data. As an example, a model\ntrained on a small or unrepresentative dataset cannot adequately learn true patterns and\nrelationships, which results in higher epistemic uncertainty. This form of uncertainty is\nreducible with better knowledge of the problem, larger datasets, better model architectures,\nand an optimized modeling technique.\nIn conventional ML approaches (e.g., DNNs), parameter uncertainty is a key epistemic\nconsideration. Parameter uncertainty specifically arises from the randomness introduced by\nthe stochastic nature of gradient descent and other optimization processes. One example is\nthe initialization of weights in a DNN; initialization will affect the parameter space traversed\nduring training, leading to different optimized weights and predictions in comparison\nwith those of a model of a different initialization. Several techniques, such as dropout,\nBayesian approaches, and ensembling, have been developed to mitigate and understand\nthis uncertainty by averaging unique predictions or by representing weights as distributions\nrather than as single values. Understanding and managing these uncertainties can improve\nthe performance and reliability of ML models. The three methods applied in this study\nwith UQ capabilities are DNN ensembles, BNNs, and DGPs. Detailed descriptions are\nprovided in Sections 3.2, 3.3, and 3.4."}, {"title": "2.3. Quality of Uncertainty Estimates", "content": "An often overlooked aspect of UQ is verifying that the uncertainty estimates are indeed\nuseful. If these estimates are not of high quality, then they can cause misled decision-\nmaking processes and degrade the overall purpose of performing UQ. Three pieces of\nevidence can help assess uncertainty estimate quality: calibration curves, the magnitudes\nof the estimates, and their spread/distribution [23].\nIn the case of UQ, calibration curves are a graphical approach for assessing the calibra-\ntion of a model's uncertainty estimates by comparing the observed cumulative distribution\nof standard deviation-normalized residuals with the theoretical cumulative distribution of\na standard normal distribution [23]. This approach is different than the typical reliability\ndiagram, which focuses on the calibration of predictions themselves rather than focusing\non reported uncertainties. A model with well-calibrated uncertainty estimates will pro-\nduce normalized residuals that closely follow a standard normal distribution, resulting in a\ncurve that aligns with the identity line. Analyzing the calibration curve can help identify\nsystematic overconfidence or underconfidence in the model's uncertainty estimates. The\nright-hand side of the curve falling below the ideal calibration line indicates that uncer-\ntainty estimates are too low, signifying overconfidence. Conversely, the right-hand side of\nthe curve falling above the identity line indicates that uncertainty estimates are too high,\nsignifying underconfidence.\nThe magnitudes of uncertainty estimates in regression correspond to the model's con-\nfidence in its predictions. Uncertainty estimates are often quantified by the distribution of\nstandard deviations from a set of predictions when given an identical input set (interpreted\nas samples from an underlying distribution). If the magnitudes of these estimates are\ntoo large, then the models may lack sufficient data or be overly conservative. Overcon-\nfident predictions may also occur with small magnitudes, which can indicate insufficient\nrepresentation of model uncertainties.\nIn the consideration of the distribution of the prediction \u201csamples,\u201d the shape and\nspread of the uncertainty estimates across data points are useful for determining how a\nmodel captures variability. In regression, high-quality UQ provides varying uncertainty\nestimates that reflect different degrees of confidence based on the input features. This\nspread indicates how the model differentiates between high and low uncertainty regions,\nwhich can reflect regions of higher complexity or in extrapolation outside of the training\nspace. Uncertainty estimates that remain constant over all input regions are neither effective\nnor useful."}, {"title": "3. Methods", "content": "The basis of the proposed hybrid approach is the prediction of residuals calculated\nfrom a base model and experimental data values. The input parameters for this problem\nwere first selected: tube diameter (D), heated length (L), pressure (P), mass flux (G), and\ninlet subcooling (Ahsub). The two other parameters in the original CHF dataset\u2014inlet\ntemperature and outlet equilibrium quality\u2014are redundant and can be computed using the\nother input parameters. Therefore, these parameters may be readily excluded from this\ninput parameter set. A base model can now be selected to provide a \u201cfirst-guess\u201d output\nvalue, which will later be corrected by the ML component. In theory, the base model can be\nany model that provides relevant information that links the inputs to the output value. In the\ncase of this study, the base model corresponds to either the Biasi or Bowring correlations.\nOther correlations/models, such as the three-field model or even an LUT, could also be\nappropriate. The Groeneveld LUT was eliminated as an option for this experiment because\nit was constructed using the same data that were used to test and train the ML model.\nThe use of this LUT would directly leak information during the testing phase because\nthe holdout dataset would be polluted with values that the model could have potentially\n\"seen\" during training. This issue could lead to inflated performance scores and prevent\nan accurate assessment of the trained model's ability to generalize to new data.\nOnce the base\nmodel has been selected, the inputs of a given data entry i are fed into the model to compute\n\u0177i, a traditional estimate of CHF. The difference is then taken between this value and a\nknown experimental value y; to compute the residual (ri), which the ML model will be\ntrained to predict. The data-driven model is then trained using the same set of inputs,\nproducing a predicted output (fi). The final prediction (\u1ef9i) is given when the predicted\nresidual is added to the base model's output to essentially provide an adjustment to the\nbase model's first estimate, ideally closing the gap between calculated and observed values.\nThis process is the training configuration of this approach. During actual implementation,\nthe experimental data, and therefore the true residuals, are not known; the CHF predictions\nconsist of only the base models' outputs combined with the ML models' predicted residuals.\nAlthough the ML component of this method predicts a different quantity than the pure ML\nmethod does (i.e., a correlation-experiment residual vs. a direct CHF value), the true\noutput quantity of the two methods remains the same. The analysis and UQ performed in\nthis study focused on this final CHF quantity rather than the hybrids' residuals themselves.\nThree ML UQ techniques were considered in this study: DNN ensembles, BNNs, and\nDGPs. For each of these techniques, three choices of base models were compared: no base\nmodel, the Biasi correlation, and the Bowring correlation. These nine model configurations\nwere trained in two data scenarios: one with plentiful training points (7,350), and the other\nwith a severely restricted training set (9 points). Using this experimental structure, an\nML-base model combination may be identified as having the most favorable performance,\nuncertainty behavior, and resistance to performance degradation in scenarios of limited\ndata."}, {"title": "3.1. Data Preparation", "content": "The CHF dataset, filtered for instances of DO within the ranges of validity for the\nBiasi and Bowring correlations, was first shuffled to homogenize the entries and ensure\nadequate coverage of the data space. The data in the input channels and the outputs were\nthen standardized (i.e., via Z-score normalization) so that their distributions would have a\nmean of 0 and a standard deviation of 1 [24]. This standardization is necessary to avoid\nbiasing the model during training, which may attribute higher importance to input channels\nof larger magnitude. The resulting data array was partitioned into training, validation, and\ntesting partitions in an 80%/10%/10% split. The validation set is used for the optimization\nof network structure and other hyperparameters, and is separate from the testing partition,\nwhich is used to evaluate the final trained model. The use of testing data in the process\nof hyperparameter optimization may inadvertently transfer some knowledge of the holdout\ndata to the model during tuning and thereby cause inflated performance metrics [25]. These\ndata partitions and their contents were identical for every model used in this study to ensure\nan appropriate comparison.\nA second dataset was constructed by simply reducing the training partition's size to only\nnine points while maintaining the size and contents of the validation and testing partitions.\nThe same model configurations used in the plentiful data scenario would be trained on this\nlimited data scenario. The purpose of this scenario is to investigate the hybrid approach's\nresistance to performance degradation due to poor training data compared to that of a pure\nML model. The training set's size in this limited case represents 0.1% of the complete\nfiltered dataset, compared with the previous scenario's training set consisting of 80%.\nFuture development of hybrid models with limited data, for actual use, would require more\ncomprehensive qualification."}, {"title": "3.2. Deep Neural Network Ensembles", "content": "The concept of ensembling is based on the idea that a committee of unique DNN\nmodels will outperform a single model [26]. Because DNNs are purely deterministic once\ntrained, they will produce the same value every time a prediction is made on the same input\nvector. When multiple unique models are well trained with different hyperparameters and\noptimizations, they will each produce slightly different predictions with the same input,\nallowing for their treatment as samples from an underlying distribution [27]. Measures of\nthese distributions, such as the means and standard deviations for every point in the test\ndataset, can then be computed. The confidence and uncertainty in the model parameters\ncan then be quantitatively inferred from this analysis [28].\nThe simplest implementation of a DNN ensemble is using an initialization-based strat-\nergy, in which a set of models is trained using different weight/bias starting values [29].\nEach of these models subsequently navigates a different solution space to attempt to find a\nglobal minimum in the objective function's space. In this study, a set of 20 otherwise iden-\ntical models were trained with different random number generator seeds using Google's\nTensorFlow [30]. The network architecture consisted of seven densely connected hidden"}, {"title": "3.3. Bayesian Neural Networks", "content": "The second method, BNNs, extends the framework of DNNs but with built-in uncer-\ntainty estimation. Instead of using single-valued weights, BNNs use probability distri-\nbutions. This approach allows the model to directly capture uncertainty in the tunable\nparameters [32]. During training, BNNs use techniques such as variational inference to ap-\nproximate the posterior distribution over the weights, providing a structured way to quantify\nuncertainty. Variational inference transforms the problem of exact Bayesian inference into\nan optimization problem, which enables the network to learn a distribution of weights that\nbest fits the data. This allows BNNs to express uncertainty about predictions (especially\nin regions with limited data) and be more generalizable by considering multiple plausible\nmodels rather than a single set of weights.\nThe BNN was implemented using TensorFlow Probability and was constructed from\nfour densely connected hidden layers using the Flipout estimator. These layers introduce\nepistemic uncertainty because they estimate a distribution over the weights and biases\ninstead of considering them to be single values. The Flipout estimator reduces the variance\nin gradient estimates during training by performing a Monte Carlo approximation. The\nnetwork architecture outputs two parameters defining a normal distribution: the mean and\nscale (standard deviation) of the target values. During training, the negative log likelihood\nof this distribution is minimized, which also incorporates the Kullback-Leibler divergence\nbetween prior and posterior distributions. For hyperparameter optimization, the procedure\nfrom Section 3.2 was implemented.\nThe prior and posterior distributions for the weights were modeled as normal distri-\nbutions, the prior having a mean of 0 and variance of 1, and the posterior initialized with\nsmall perturbations from the prior. Variational inference was used to optimize the posterior"}, {"title": "3.4. Deep Gaussian Processes", "content": "Although DGPs also use a hierarchical network structure, they are layers of Gaussian\nprocesses (GPs). GPs are a finite collection of random variables that have a joint multi-\nvariate normal distribution. For any given input, the mean and variance are computed for\na target distribution, where the mean is considered to be the prediction. GPs use kernel\nfunctions to compute the covariance between data points, making them especially useful in\ncases of limited data. Structuring GPs with \u201cdepth\" in layers extends their ability to capture\nmore complex representations of a dataset. Coupling this hierarchical structure with GPs\nallows them to effectively extract uncertainties from complex data [33]. DGPs may also be\nconfigured to capture aleatoric uncertainty via the observation noise variance within each\nlayer, if desired. TensorFlow Probability was again used to implement the DGP models.\nThe network architecture consisted of two layers of variational Gaussian processes using\nthe Radial Basis Function kernel to define the covariance structure. This model was then\ntrained to 500 epochs."}, {"title": "4. Results", "content": "For each of the three ML methods used in this study, a purely data-driven case was run\nwithout a base model, and two hybrid cases were then run with the Biasi and Bowring base\nmodels. This yielded a total of nine unique model configurations, which were trained and\ntested in two data scenarios: plentiful training data and limited training data.\nQuantifying and comparing the performance between the three modeling approaches\n(pure ML, Biasi hybrid, and Bowring hybrid) in each case is primarily accomplished using\na set of seven metrics. The terms Herror and Maxerror correspond to the mean and maximum\nof the absolute relative error distribution across the final predictions. The relative standard\ndeviations (rStd) describe the shape of the distributions along each input predictions'\nsamples to be a measure of uncertainty for each of the final predictions. In the case of the\nDGP models, the standard deviations are directly extracted from the GP layers. Another\nerror metric, relative root-mean-square error (rRMSE), is included as a more sensitive\nmeasure that penalizes larger error values to a higher degree and is scale-invariant. The\ndefinition of rRMSE has been inconsistent over several literature entries, but this study\nuses the definition in the OECD/NEA benchmark report, as represented in Eq. (1) [11]:\nrRMSE (%) = $\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} \\frac{(\\widehat{Yi}-Yi)^2}{Yi}} \\times 100%$\n(1)\nThe metric Ferror > 10% corresponds to the percentage of absolute relative error values\nthat are larger than 10%. Finally, as the last metric, R\u00b2 is the coefficient of determination,\nwhich ranges from 0 to 1; an ideal model would attain a value of 1."}, {"title": "4.1. DNN Ensembles", "content": "After all 20 of the ensemble models were run for each of the experimental configu-\nrations, the seven metrics were computed. For the 80% training data case, all\nthree approaches obtain Herror values below 2%, and the hybrid models present slightly\nsmaller values in comparison with those of the pure ML approach. The maximum error\nvalues, however, are larger for the hybrid models in comparison with the pure ML model's\n36.03%. This difference is mild, and in assessing the fraction of these errors above 10%,\nthe hybrid models are observed to have the smallest values (0.871% of the pure ML model\nin comparison with 0.654% and 0.764% of Biasi and Bowring, respectively). All three\napproaches obtain a high R\u00b2 above 0.99, indicating strong agreement between predicted\nand measured values. In terms of uncertainty, all three ensembles exhibit a high degree of\nconfidence (all below 1.55%), with the Bowring hybrid variant having the smallest relative\nstandard (1.277%).\nIn the case of the 0.1% training size (using only nine points for training), the hybrid\nensembles have significantly more favorable metrics in comparison with the pure ML\nensemble, especially for the Biasi hybrid variant. The Herror values for all three ensembles\nare larger than those for the 80% training size case, with the pure ML value changing\nfrom 1.947% to a far larger 35.42%. The hybrids' Herror values increase by just under\n4.5 percentage points but remain below those of the stand-alone correlations (6.935% for\nBiasi and 6.778% for Bowring). Compared with the stand-alone correlations, the hybrid\nensembles attain more favorable values in every metric, even with only nine training points.\nIn the case of the pure ML ensemble, the limited training data significantly impede learning\nand the ability to generalize to new data."}, {"title": "4.2. Bayesian Neural Networks", "content": "The second set of cases uses BNNs to quantify the prediction uncertainties, again for the\npure ML and two hybrid variants in both data scenarios. For each of the input vectors, 200\n\u201csamples\u201d were predicted for the uncertainty estimation. The seven performance metrics\nwere computed. The trends noted for the DNN\nensemble in Section 4.1 are present here; all three models of the 80% training size scenario\nbehave similarly. All these values, compared with the ensemble's, are larger in error with a\nlarger degree of uncertainty. In the limited data scenario, there is the expected jump in error\nin the pure ML model's predictions but with a larger degree than that of the ensemble's\n(2.818% up to 89.58% \u03bcerror). This is also the case for the six other metrics (especially\nwith a negative R\u00b2), signaling a complete loss of generalization to new data. Both the Biasi\nand Bowring hybrid models show an expected rise in error metrics, but the Biasi hybrid\nexhibits values that are larger than those of the stand-alone Biasi correlation. This behavior\nis entirely different from the Biasi hybrid ensemble's behavior, which approached but did\nnot exceed the error of the stand-alone correlation. The Bowring hybrid BNN model,\nhowever, remains within the performance metrics of the stand-alone correlation. Despite\nthis decreased performance of the Biasi hybrid BNN, it reports the highest confidence in\nits predictions, indicating that it is not properly calibrated.\nThe collapse of the pure ML model's performance is shown. The model predicts all\nvalues about a near-horizontal axis around 2,250 kW m\u00af\u00b2. This indicates that the model\nfails entirely to generalize to the hidden testing dataset, effectively becoming useless in its\napplication. The uncertainty estimates in this case are larger for smaller predicted CHF\nvalues, with a negative gradient with increasing predicted values. The two hybrid models\nare shown with the correct trends and relatively tight grouping around the identity line;\nsome positive bias exists in the Biasi hybrid's predictions at lower true CHF values.\nTo show a more focused look at the uncertainty estimates from each of these models,\nthe relative standard deviations are provided. As observed in the performance\nmetrics, the means and shapes of the models' distributions in the plentiful training data\nscenario are similar. There are no significant deviations, except for the outlier just below\n25%, which belongs to the pure ML model. In the limited data scenario, there is a clear\ndifference between the distributions of the hybrid models and that of the pure ML model.\nThe pure ML model's rStd values are near symmetric at about the 57.79% mean value\nand are a distinct peak in comparison with the hybrid model distributions in the left-hand\ncorner. This behavior is favorable to some degree because the very large error associated\nwith this model is paired with nonconfidence in its predictions."}, {"title": "4.3. Deep Gaussian Processes", "content": "Each of the DGP models was trained and tested using the same test dataset used for\nthe other models. The standard deviations were extracted directly from the model instead\nof using a sampling approach such as the approaches used for the DNN ensembles and\nthe BNN models. The same performance metrics were then computed. In comparison with the performance of the previous two methods,\nnearly every error metric is significantly larger. Between the DGP models themselves,\nthe Biasi hybrid model outperforms both the pure ML and Bowring hybrid models by\na significant margin. Although it does not match the performance of the Biasi hybrid,\nthe Bowring hybrid model also performs with more favorable metrics in comparison with\nthe pure ML model. The exception in this case is in the Bowring hybrid's larger relative\nstandard deviation values, which indicate that more uncertainty is associated with those\npredictions.\nIn the limited training dataset case (using only nine points for training), all three of\nthe models' predictions were observed with expected deterioration in all metrics. The\ntwo hybrid models have notably smaller error metrics compared with those of the pure\nML model, except for the maximum relative standard deviation. Of the three models, the\nBowring hybrid produced the most accurate predictions with the smallest uncertainties on\naverage. Notably, as with the pure ML BNN models, the pure ML DGP predicts nearly\nthe same value for every input combination, indicating a complete inability to generalize\nto new data.\nThe parity plots with uncertainty overlays were once again generated for the limited\ntraining dataset case. These plots are nearly identical in behavior to those of the BNN,\nshowing a complete deterioration of the pure ML model's predictions with a similar relative\nstandard deviation spread. Because of this redundancy, these parity plots are omitted from\nthis section.\nThe relative standard deviations for each of the points were extracted and plotted to\nassess the behavior of this distribution. Because of the large maximum\nrStd values in the hybrid models (681.9% for Biasi and 218.6% for Bowring in the limited\ntraining data case), all outliers are removed. In the\nplentiful training dataset case, the pure ML model has the smallest spread about its mean\nrStd value of 2.008%. Although the Biasi hybrid model shows a smaller mean relative\nstandard deviation value compared with that of the pure ML model, its standard deviation\nfor this distribution is larger. The Bowring hybrid model has the largest mean, standard\ndeviation, and maximum. These models' trends are similar in the limited training data case\nbut with larger means, maximums, and standard deviations.\nThe UQ calibration curves were then constructed. These curves are provided. The Bowring hybrid model in both training data scenarios is the best calibrated compared"}, {"title": "4.4. Direct Comparison between Methods", "content": "With the results above, direct comparisons can be made across the ML techniques\n(DNN ensemble, BNN, DGP) rather than comparisons simply being made between the\npure and hybrid approaches performed within each. The first obvious metric to compare\nis the mean relative error. These values for the nine models of the 80% training set\nsize case are visually compared. Every model of the BNN and DGP has\nan increased error value compared with that of the ensemble; the DGP has the largest\nerror of the"}]}