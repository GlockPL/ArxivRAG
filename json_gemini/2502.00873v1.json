{"title": "Language Models Use Trigonometry to Do Addition", "authors": ["Subhash Kantamneni", "Max Tegmark"], "abstract": "Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the \u201cClock\" algorithm: to solve $a + b$, the helices for $a$ and $b$ are manipulated to produce the $a + b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) display surprising and significant aptitude for mathematical reasoning (Ahn et al., 2024; Satpute et al., 2024), which is increasingly seen as a benchmark for LLM capabilities (OpenAI; Glazer et al., 2024). Despite LLMs' mathematical proficiency, we have limited understanding of how LLMs process even simple mathematical tasks like addition. Understanding mathematical reasoning is valuable for ensuring LLMs' reliability, interpretability, and alignment in high-stakes applications.\nIn this study, we reverse engineer how GPT-J, Pythia-6.9B, and Llama3.1-8B compute the addition problem $a + b$ for $a, b \\in [0,99]$. Remarkably, we find that LLMs use a form of the \"Clock\" algorithm to compute addition, which was previously proposed by Nanda et al. (2023a) as a mechanistic explanation of how one layer transformers compute modular addition (and later named by Zhong et al. (2023)).\nTo compute $a + b$, all three LLMs represent $a$ and $b$ as a helix on their tokens and construct helix$(a + b)$ on the last token, which we verify with causal interventions. We then focus on how GPT-J implements the Clock algorithm by investigating MLPs, attention heads, and even specific neurons. We find that these components can be understood as either constructing the $a + b$ helix by manipulating the $a$ and $b$ helices, or using the $a + b$ helix to produce the answer in the model's logits. We visualize this procedure in Fig. 1 as rotating the dial of a clock.\nOur work is in the spirit of mechanistic interpretability (MI), which attempts to reverse engineer the functionality of machine learning models. However, most LLM MI research focuses either on identifying circuits, which are the minimal set of model components required for computations,"}, {"title": "2. Related Work", "content": "Circuits. Within mechanistic interpretability, circuits research attempts to understand the key model components (MLPs and attention heads) that are required for specific functionalities (Olah et al., 2020; Elhage et al., 2021). For example, Olsson et al. (2022) found that in-context learning is primarily driven by induction attention heads, and Wang et al. (2023) identified a sparse circuit of attention heads that GPT-2 uses to complete the indirect object of a sentence. Understanding how multilayer perceptrons (MLPs) affect model computation has been more challenging, with Nanda et al. (2023b) attempting to understand how MLPs are used in factual recall, and Hanna et al. (2023) investigating MLP outputs while studying the greater-than operation in GPT-2.\nFeatures. Another branch of MI focuses on understanding how models represent human-interpretable concepts, known as features. Most notably, the Linear Representation Hypothesis posits that LLMs store features as linear directions (Park et al., 2023; Elhage et al., 2022), culminating in the introduction of sparse autoencoders (SAEs) that decompose model activations into sparse linear combinations of features (Huben et al., 2024; Bricken et al., 2023; Templeton et al., 2024; Gao et al., 2024; Rajamanoharan et al., 2024). However, recent work from Engels et al. (2024) found that some features are represented as non-linear manifolds, for example the days of the week lie on a circle. Levy & Geva (2024) and Zhu et al. (2025) model LLMs' representations of numbers as a circle in base 10 and as a line respectively, although with limited causal results. Recent work has bridged features and circuits research, with Marks et al. (2024) constructing circuits from SAE features and Makelov et al. (2024) using attention-based SAEs to identify the features used in Wang et al. (2023)'s IOI task.\nReverse engineering addition. Liu et al. (2022) first discovered that one layer transformers generalize on the task of modular addition when they learn circular representations of numbers. Following this, Nanda et al. (2023a) introduced the \"Clock\" algorithm as a description of the underlying angular addition mechanisms these transformers use to generalize. However, Zhong et al. (2023) found the \u201cPizza\" algorithm as a rivaling explanation for some transformers, illustrating the complexity of decoding even small models."}, {"title": "3. Problem Setup", "content": "Models As in Nikankin et al. (2024), we analyze 3 LLMs: GPT-J (6B parameters) (Wang & Komatsuzaki, 2021), Pythia-6.9B (Biderman et al., 2023), and Llama3.1-8B (Grattafiori et al., 2024). All three models are autoregressive transformers which process tokens $X_0,..., X_n$ to produce probability distributions over the likely next token $X_{n+1}$ (Vaswani et al., 2017). The $i$th token is embedded as $L$ hidden state vectors (also known as the residual stream), where $L$ is the number of layers in the transformer. Each hidden state is the sum of multilayer perceptron (MLP) and attention (attn) layers.\n$h_i^l = h_i^{l-1} + a_i^l + m_i^l$,\n$a_i^l = attn(h_i^{l-1}, h_{\\lt i}^{l-1},...,h_{\\gt i}^{l-1})$,\n$m_i^l = MLP(a_i^l + h_i^{l-1})$.\nGPT-J and Pythia-6.9B use simple MLP implementations, namely $MLP(x) = \\sigma (xW_{up})W_{down}$, where $\\sigma (x)$ is the sigmoid function. Llama3.1-8B uses a gated MLP, $MLP(x) = \\sigma (xW_{gate}) (xW_{in})W_{out}$, where $\\sigma$ represents the Hadamard product (Liu et al., 2021). GPT-J tokenizes the numbers [0, 361] (with a space) as a single token, Pythia-6.9B tokenizes [0, 557] as a single token, and Llama3.1-8B tokenizes [0, 999] as a single token. We focus on the single-token regime for simplicity.\nData To ensure that answers require only a single token for all models, we construct problems $a + b$ for integers $a, b \\in [0,99]$. We evaluate all three models on these 10,000 addition problems, and find that all models can competently complete the task: GPT-J achieves 80.5% accuracy, Pythia-6.9B achieves 77.2% accuracy, and Llama3.1-8B achieves 98.0% accuracy. For the prompts used and each model's performance heatmap by $a$ and $b$, see Appendix A. Despite Llama3.1-8B's impressive performance, in the main paper we focus our analysis on GPT-J because its simple MLP allows for easier neuron interpretation. We report similar results for Pythia-6.9B and Llama3.1-8B in the Appendix."}, {"title": "4. LLMs Represent Numbers as a Helix", "content": "To generate a ground up understanding of how LLMs compute $a + b$, we first aim to understand how LLMs represent numbers. To identify representational trends, we run GPT-J on the single-token integers $a \\in [0,360]$. We do not use $a = 361$ because 360 has more integer divisors, allowing for a simpler analysis of periodic structure. We conduct analysis on $h_{360}$, which is the residual stream following layer 0 with shape [360, model_dim]. We choose to use the output of layer 0 rather than directly analyzing the embeddings because prior work has shown that processing in layer 0 is influential for numerical tasks (Nikankin et al., 2024)."}, {"title": "4.1. Investigating Numerical Structure", "content": "Linear structure. To investigate structure in numerical representations, we perform a PCA (F.R.S., 1901) on $h_{360}$ and find that the first principal component (PC1) for $a \\in [0,360]$ has a sharp discontinuity at $a = 100$ (Fig. 15, Appendix B), which implies that GPT-J uses a distinct representation for three-digit integers. Instead, in the bottom of Fig. 2, we plot PC1 for $h_{99}$ and find that it is well approximated by a line in $a$. Additionally, when plotting the Euclidean distance between $a$ and $a + \\delta a$ for $a \\in [0, 9]$ (Fig. 14, Appendix B), we see that the distance is locally linear. The existence of linear structure is unsurprising - numbers are semantically linear, and LLMs often represent concepts linearly.\nPeriodic Structure. We center and apply a Fourier transform to $h_{360}$ with respect to the number $a$ being represented and the model_dim. In Fig. 2, we average the resulting spectra across model_dim and observe a sparse Fourier domain with high-frequency components at $T = [2,5,10]$."}, {"title": "4.2. Parameterizing the Structure as a Helix", "content": "To account for both the periodic and linear structure in numbers, we propose that numbers can be modeled helically. Namely, we posit that $h_a$, the residual stream immediately preceding layer $l$ for some number $a$, can be modeled as\n$h_a = helix(a) = CB(a)^T$,\n$B(a) = [a, cos(\\frac{2\\pi}{T_1} a), sin(\\frac{2\\pi}{T_1} a), ..., cos(\\frac{2\\pi}{T_k} a) sin(\\frac{2\\pi}{T_k} a)]$.\n$C$ is a matrix applied to the basis of functions $B(a)$, where $B(a)$ uses $k$ Fourier features with periods $T = [T_1,...T_k]$. The $k = 1$ case represents a regular helix; for $k > 1$, the independent Fourier features share a single linear direction. We refer to this structure as a generalized helix, or simply a helix for brevity.\nWe identify four major Fourier features: $T = [2, 5, 10, 100]$. We use the periods $T = [2, 5, 10]$ because they have significant high frequency components in Fig. 2. We are cautious of low frequency Fourier components, and use $T = 100$ both because of its significant magnitude, and by applying the inductive bias that our number system is base 10."}, {"title": "4.3. Fitting a Helix", "content": "We fit our helical form to the residual streams on top of the $a$ token for our $a + b$ dataset. In practice, we first use PCA to project the residual stream at each layer to 100 dimensions. To ensure we do not overfit with Fourier features, we consider all combinations of $k$ Fourier features, with $k \\in [1,4]$. If we use $k$ Fourier features, the helical fit uses $2k + 1$ basis functions (one linear component, 2k periodic components). We then use linear regression to find some coefficient matrix $C_{PCA}$ of shape $100 \\times 2k + 1$ that best satisfies $PCA(h_a) = C_{PCA}B(a)^T$. Finally, we use the inverse PCA transformation to project $C_{PCA}$ back into the model's full residual stream dimensionality to find $C$.\nWe visualize the quality of our fit for layer 0 when using all $k = 4$ Fourier features with $T = [2, 5, 10, 100] in Fig. 3. To do so, we calculate $C^{{\\dagger}}h_a$, where $C^{{\\dagger}}$ is the Moore-Penrose pseudo-inverse of $C$. Thus, $C^{{\\dagger}}h_a$ represents the projection of the residual stream into the helical subspace. When analyzing the columns of $C$, we find that the Fourier features increase in magnitude with period and are mostly orthogonal (Appendix C.1)."}, {"title": "4.4. Evaluating the Quality of the Helical Fit", "content": "We want to causally demonstrate that the model actually uses the fitted helix. To do so, we employ activation patching. Activation patching isolates the contribution of specific model components towards answer tokens (Meng et al., 2022; Heimersheim & Nanda, 2024). Specifically, to evaluate the contribution of some residual stream $h_a$ on the $a$ token, we first store $h_{a,clean}$ when the model is run on a \"clean\" prompt $a + b$. We then run the model on the corrupted prompt $a' + b$ and store the model logits for the clean answer of $a + b$. Finally, we patch in the clean $h_{a,clean}$ on the corrupted prompt $a' + b$ and calculate $LD = logit_{patched}(a + b) - logit_{corrupted}(a + b)$, where $LD$ is the logit difference for $h_a$. By averaging over 100 pairs of clean and corrupted prompts, we can evaluate $h_a$'s ability to restore model behavior to the clean answer $a + b$. To reduce noise, all patching experiments only use prompts the model can successfully complete.\nTo leverage this technique, we follow Engels et al. (2024) and input our fit for $h_{a,clean}$ when patching. This allows us to causally determine if our fit preserves the information the model uses for the computation. We compare our $k$ Fourier feature helical fit with four baselines: using the actual $h_{a,clean}$ (layer patch), the first $2k + 1$ PCA components of $h_{a,clean}$ (PCA), a circular fit with $k$ Fourier components (circle), and a polynomial fit with basis terms $B(a) = [a, a^2, ...a^{2k+1}]$ (polynomial). For each value of $k$, we choose the combination of Fourier features that maximizes $\\sum_i LD_i$ as the best set of Fourier features.\nIn Fig. 4, we see that the helical fit is most performant against baselines, closely followed by the circular fit. This implies that Fourier features are predominantly used to compute addition. Surprisingly, the $k = 4$ full helical and circular fits dominate the strong PCA baseline and approach the effect of layer patching, which suggests that we have identified the correct \"variables\" of computation for addition. Additionally, we note a sharp jump between the fit for layer 0's input (the output of the embedding) and layer 1's input, aligning with evidence from Nikankin et al. (2024) that layer 0 is necessary for numerical processing.\nIn Appendix C.2, we provide evidence that Llama3.1-8B and Pythia-6.9B also use helical numerical representations. Additionally, we provide evidence that our fits are not overfitting by using a train-test split with no meaningful effect on our results. The helix functional form is not overly expressive, as a helix trained on a randomized order of $a$ is not causally relevant. We also observe continuity when values of $a$ that the helix was not trained on are projected into the"}, {"title": "4.5. Is the Helix the Full Picture?", "content": "To identify if the helix sufficiently explains the structure of numbers in LLMs, we test on five additional tasks.\n1. $a - 23$ for $a \\in [23, 99]$\n2. $a//5$ (integer division) for $a \\in [0,99]$\n3. $a * 1.5$ for even $a \\in [0,98]$\n4. $a \\mod 2$ for $a \\in [0,99]$\n5. If $x - a = 0$, what is $x$ for $a \\in [0,99]$\nFor each task, we fit full helices with $T = [2, 5, 10, 100]$ and compare against baselines. In Table 1, we describe our results on these tasks by listing $max_i LD_i$, which is the maximal causal power of each fit (full plot and additional task details in Appendix C.2). Notably, while the helix is causally relevant for all tasks, we see that it underperforms the PCA baseline on tasks 2, 3, and 5. This implies that there is potentially additional structure in numerical representations that helical fits do not capture. However, we are confident that the helix is used for addition. When ablating the helix dimensions from the residual stream (i.e. ablating $C^{{\\dagger}}$ from $h_a$), performance is affected roughly as much as ablating $h_a$ entirely (Fig. 21, Appendix C.1).\nThus, we conclude that LLMs use a helical representation of numbers to compute addition, although it is possible that additional structure is used for other tasks."}, {"title": "5. LLMs Use the Clock Algorithm to Compute Addition", "content": ""}, {"title": "5.1. Introducing the Clock Algorithm", "content": "Taking inspiration from Nanda et al. (2023a), we propose that LLMs manipulate helices to compute addition using the \"Clock\" algorithm.\nTo compute $a + b =$ , GPT-J\n1. Embeds $a$ and $b$ as helices on their own tokens.\n2. A sparse set of attention heads, mostly in layers 9-14, move the $a$ and $b$ helices to the last token.\n3. MLPs 14-18 manipulate these helices to create the helix $a + b$. A small number of attention heads help with this operation.\n4. MLPs 19-27 and a few attention heads \"read\" from the $a + b$ helix and output to model logits.\nSince we have already shown that models represent $a$ and $b$ as helices (Appendix C.2), we provide evidence for the last three steps in this section. In Fig. 5 we observe that last token hidden states are well modeled by $h = helix(a, b, a + b)$, where $helix(x, y)$ is shorthand to denote $helix(x) + helix(y)$. Remarkably, despite only using 9 parameters, at some layers helix$(a + b)$ fits last token hidden states better than a 27 dimensional PCA. The $a + b$ helix having such causal power implies it is at the heart of the computation.\nIn Appendix D, we show that other LLMs also use helix$(a + b)$. Since the crux of the Clock algorithm is computing the answer helix for $a + b$, we take this as compelling evidence that all three models use the Clock algorithm. However, we would like to understand how specific LLM components implement the algorithm. To do so, we focus on GPT-J.\nIn Fig. 6, we use activation patching to determine which last token MLP and attention layers are most influential for"}, {"title": "5.2. Investigating Attention Heads", "content": "In GPT-J, every attention layer is the sum of 16 attention heads whose outputs are concatenated. We activation and path patch each attention head on the last token and rank them by total effect (TE). To determine the minimal set of attention heads required, we activation patch $k$ attention heads at once, and find the minimum $k$ such that their combined total effect approximates patching in all attention heads. In Appendix D.1, we see that patching $k = 17$ heads achieves 80% of the effect of patching in all 448 attention heads, and we choose to round up to $k = 20$ heads (83.9% of effect).\nSince attention heads are not as influential as MLPs in Fig. 6, we hypothesize that they primarily serve two roles: 1) moving the $a, b$ helices to the last token to be processed by downstream components ($a, b$ heads) and 2) outputting the $a + b$ helix directly to logits ($a + b$ heads). Some mixed heads output all three $a, b$, and $a + b$ helices. We aim to categorize as few attention heads as mixed as possible.\nTo categorize attention heads, we turn to two metrics. $C_{a,b}$ is the confidence that a certain head is an $a, b$ head, which we quantify with $C_{a,b} = (1 - \\frac{DE_{helix(a,b)}}{TE_{helix(a,b,a+b)}}$. The first term represents the fractional indirect effect of the attention head, and the second term represents the head's total effect recoverable by just using the $a, b$ helices instead of helix$(a, b, a + b)$. Similarly, we calculate $C_{a+b}$ as the confidence the head is an $a + b$ head, using $C_{a+b} = \\frac{DE_{helix(a+b)}}{TE_{helix(a,b,a+b)}}$.\nWe sort the $k = 20$ heads by $c = max(C_{a,b}, C_{a+b})$. If a head is an $a+b$ head, we model its output using the $a+b$ helix and allow it only to output to logits (no impact on downstream components). If a head is an $a,b$ head, we restrict it to outputting helix$(a, b)$. For $m = [1,20]$, we allow $m$ heads with the lowest $c$ to be mixed heads, and categorize the rest as $a, b$ or $a + b$ heads. We find that categorizing $m = 4$ heads as mixed is sufficient to achieve almost 80% of the effect of using the actual outputs of all $k = 20$ heads. Thus, most important attention heads obey our categorization. We list some properties of each head type below.\n$\\bullet$ $a, b$ heads (11/20): In layers 9-14 (but two heads in l = 16, 18), attend to the $a, b$ tokens, and output $a, b$ helices which are used mostly by downstream MLPs.\n$\\bullet$ $a + b$ heads (5/20): In layers 24-26 (but one head in layer 19), attend to the last token, take their input from preceding MLPs, and output the $a + b$ helix to logits.\n$\\bullet$ Mixed heads (4/20): In layers 15-18, attend to the $a, b$, and $a + b$ tokens, receive input from $a, b$ attention heads and previous MLPs, and output the $a, b$, and $a + b$ helices to downstream MLPs.\nFor evidence of these properties refer to Appendix D.1. Notably, only mixed heads are potentially involved in creating the $a + b$ helix, which is the crux of the computation, justifying our conclusion from Fig. 6 that MLPs drive addition."}, {"title": "5.3. Looking at MLPs", "content": "GPT-J seems to predominantly rely on last token MLPs to compute $a + b$. To identify which MLPs are most important, we first sort MLPs by total effect, and patch in $k = [1, L = 28]$ MLPs to find the smallest $k$ such that we achieve 95% of the effect of patching in all $L$ MLPs. We use a sharper 95% threshold because MLPs dominate computation and because there are so few of them. Thus, we use $k = 11$ MLPs in our circuit, specifically MLPs 14-27, with the exception of MLPs 15, 24, and 25 (see Appendix D.2 for details).\nWe hypothesize that MLPs serve two functions: 1) reading from the $a, b$ helices to create the $a + b$ helix and 2) reading from the $a+b$ helix to output the answer in model logits. We make this distinction using two metrics: $helix(a + b)/TE$, or the total effect of the MLP recoverable from modeling its output with helix$(a + b)$, and $DE/TE$ ratio. In Fig. 7, we see that the outputs of MLPs 14-18 are progressively better modeled using helix$(a + b)$. Most of their effect is indirect and thus their output is predominantly used by downstream components. At layer 19, helix$(a + b)$ becomes a worse"}, {"title": "5.4. Zooming in on Neurons", "content": "Activation patching the 27 * 16384 neurons in GPT-J is prohibitively expensive, so we instead use the technique of attribution patching to approximate the total effect of each neuron using its gradient (see Kram\u00e1r et al. (2024)). We find that using just 1% of the neurons in GPT-J and mean ablating the rest allows for the successful completion of 80% of prompts (see Appendix D.2). Thus, we focus our analysis on this sparse set of $k = 4587$ neurons."}, {"title": "5.4.1. MODELING NEURON PREACTIVATIONS", "content": "For a prompt $a + b$, we denote the preactivation of the $n$th neuron in layer $l$ as $N^l_n(a, b)$. When we plot a heatmap of $N^l_n(a, b)$ for top neurons in Fig. 8, we see that their preactivations are periodic in $a, b$, and $a + b$. When we Fourier decompose the preactivations as a function of $a + b$, we find that the most common periods are $T = [2, 5, 10, 100]$, matching those used in our helix parameterization (Appendix D.2). This is sensible, as the $n$th neuron in a layer applies $W_{up}$ of shape (4096, ) to the residual stream, which we have effectively modeled as a helix$(a, b, a + b)$. Subsequently, we model the preactivation of each top neuron as\n$N^l_n(a,b) = \\sum_{t=a,b,a+b} \\sum_{T \\in [2,5,10,100]} c_{Tt} cos(\\frac{t-d_t}{T})$.\nFor each neuron preactivation, we fit the parameters $c$ and $d$ in Eq. 3 using gradient descent (see Appendix D.2 for details). In Fig. 8, we show the highest magnitude fit component for a selection of top neurons.\nWe evaluate our fit of the top $k$ neurons by patching them into the model, mean ablating all other neurons, and measuring the resulting accuracy of the model. In Fig. 9, we see that our neuron fits provide roughly 75% of the performance of using the actual neuron preactivations. Thus, these neurons are well modeled as reading from the helix."}, {"title": "5.4.2. UNDERSTANDING MLP INPUTS", "content": "We use our understanding of neuron preactivations to draw conclusions about MLP inputs. To do so, we first path patch each of the top $k$ neurons to find their direct effect and calculate their DE/TE ratio. For each neuron, we calculate the fraction of their fit that helix$(a + b)$ explains, which we approximate by dividing the magnitude of $c_{T,a+b}$ terms by the total magnitude of $c_{Tt}$ terms in Eq. 3. For each circuit MLP, we calculate the mean of both of these quantities across top neurons, and visualize them in Fig. 10.\nOnce again, we see a split at layer 19, where earlier neurons' preactivation fits rely on $a, b$ terms, while later neurons use $a + b$ terms and write to logits. Since the neuron preactivations represent what each MLP is \u201creading\" from, we combine this result with our evidence from Section 5.3 to summarize the role of MLPs in addition.\n$\\bullet$ MLPs 14-18 primarily read from the $a, b$ helices to create the $a + b$ helix for downstream processing.\n$\\bullet$ MLPs 19-27 primarily read from the $a + b$ helix to write the answer to model logits.\nThus, we conclude our case that LLMs use the Clock algorithm to do addition, with a deep investigation into how GPT-J implements this algorithm."}, {"title": "5.5. Limitations of Our Understanding", "content": "There are several aspects of LLM addition we still do not understand. Most notably, while we provide compelling evidence that key components create helix$(a + b)$ from helix$(a, b)$, we do not know the exact mechanism they use to do so. We hypothesize that LLMs use trigonometric identities like $cos(a + b) = cos(a) cos(b) \u2013 sin(a) sin(b)$ to create helix$(a + b)$. However, like the originator of the Clock algorithm Nanda et al. (2023a), we are unable to isolate this computation in the model. This is unsurprising, as"}, {"title": "6. Conclusion", "content": "We find that three mid-sized LLMs represent numbers as generalized helices and manipulate them using the interpretable Clock algorithm to compute addition. While LLMs could do addition linearly, we conjecture that LLMs use the Clock algorithm to improve accuracy, analogous to humans using decimal digits (which are a generalized helix with T = [10, 100, . . . ]) for addition rather than slide rules. In Appendix E, we present preliminary results that GPT-J would be considerably less accurate on \u201clinear addition\" due to noise in its linear representations. Future work could analyze if LLMs have internal error-correcting codes for addition like the grid cells presented in Zlokapa et al. (2024).\nThe use of the Clock algorithm provides striking evidence that LLMs trained on general text naturally learn to implement complex mathematical algorithms. Understanding LLM algorithms is important for safe AI and can also provide valuable insight into model errors, as shown in Appendix F. We hope that this work inspires additional investigations into LLM mathematical capabilities, especially as addition is implicit to many reasoning problems."}]}