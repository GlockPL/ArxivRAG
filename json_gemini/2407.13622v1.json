{"title": "Misspecified Q-Learning with Sparse Linear Function Approximation: Tight Bounds on Approximation Error", "authors": ["Ally Yalei Du", "Lin F. Yang", "Ruosong Wang"], "abstract": "The recent work by Dong and Yang [2023] showed for misspecified sparse linear bandits, one can obtain an $O(\\epsilon)$-optimal policy using a polynomial number of samples when the sparsity is a constant, where $\\epsilon$ is the misspecification error. This result is in sharp contrast to misspecified linear bandits without sparsity, which require an exponential number of samples to get the same guarantee. In order to study whether the analog result is possible in the reinforcement learning setting, we consider the following problem: assuming the optimal Q-function is a d-dimensional linear function with sparsity k and misspecification error $\\epsilon$, whether we can obtain an $O(\\epsilon)$-optimal policy using number of samples polynomially in the feature dimension d. We first demonstrate why the standard approach based on Bellman backup or the existing optimistic value function elimination approach such as OLIVE [Jiang et al., 2017] achieves suboptimal guarantees for this problem. We then design a novel elimination-based algorithm to show one can obtain an $O(H\\epsilon)$-optimal policy with sample complexity polynomially in the feature dimension d and planning horizon H. Lastly, we complement our upper bound with an $\\Omega(H\\epsilon)$ suboptimality lower bound, giving a complete picture of this problem.", "sections": [{"title": "Introduction", "content": "Bandit and reinforcement learning (RL) problems in real-world applications, such as autonomous driving [Kiran et al., 2021], healthcare [Esteva et al., 2019], recommendation systems [Bouneffouf et al., 2012], and advertising [Schwartz et al., 2017], face challenges due to the vast state-action space. To tackle this, function approximation frameworks, such as using linear functions or neural networks, have been introduced to approximate the value functions or policies. However, real-world complexities often mean that function approximation is agnostic; the function class captures only an approximate version of the optimal value function, and the misspecification error remains unknown. A fundamental problem is understanding the impact of agnostic misspecification errors in RL.\nPrior works show even minor misspecifications can lead to exponential (in dimension) sample complexity in the linear bandit settings [Du et al., 2020, Lattimore et al., 2020] if the goal is to learn a policy within the misspecification error. That is, finding an $O(\\epsilon)$-optimal action necessitates at least $(exp(d))$ queries (or samples) to the environment, where $\\epsilon$ is the misspecification error. Recently, Dong and Yang [2023] demonstrated that, by leveraging the sparsity structure of ground-truth parameters, one can overcome the exponential sample barrier in the linear bandit setting. They showed that with sparsity k in the ground-truth parameters, it is possible to learn an $O(\\epsilon)$-optimal action with only $O((d/\\epsilon)^k)$ samples. In particular, when k is a constant, their algorithm achieves a polynomial sample complexity guarantee."}, {"title": "1.1 Our Contributions", "content": "In this paper, we propose an RL algorithm that can handle linear function approximation with sparsity structures and misspecification errors. We also show that the suboptimality achieved by our algorithm is near-optimal, by proving information-theoretic hardness results. Here we give a more detailed description of our technical contributions.\nOur Assumption. Throughout this paper, we assume the RL algorithm has access to a feature map $\\phi$ where, for each state-action pair $(s, a)$, we have the feature $\\phi(s, a)$ with $|\\phi(s, a)| \\le 1$. We make the following assumption, which states that there exists a sequence of parameters $\\theta^* = (\\theta_0^*, ..., \\theta_{H-1}^*)$ where each $\\theta_h^* \\in \\mathbb{R}^{d-1}$ is k-sparse, that approximates the optimal Q-function up to an error of $\\epsilon$.\nAssumption 1.1. There exists $\\theta^* = (\\theta_0^*, ..., \\theta_{H-1}^*)$ where each $\\theta_h^* \\in \\mathbb{R}^{d-1}$ is k-sparse, such that\n$|\\langle \\phi(s, a), \\theta_h^* \\rangle - Q^*(s, a)| \\le \\epsilon$\nfor all $h \\in [H]$, all states s in level h, and all actions a in the action space.\nWe can approximate $\\theta^*$ using an $\\epsilon$-net of the sphere $S^{k-1}$ and the set of all k-sized subset of $[d]$. Therefore, when k is a constant, we may assume that each $\\theta_h^*$ lies in a set with size polynomial in d. Then, a natural idea is to enumerate all possible policies induced by the parameters in that finite set, and choose the one with the highest cumulative reward. However, although the number of parameter candidates in each individual level has polynomial size, the total number of induced policies would be exponential in H, and the sample complexity of such an approach would also be exponential in H.\nThe Level-by-level Approach. Note that when the horizon length $H = 1$, the problem under consideration is equivalent to a bandit problem, which can be solved by previous approaches [Dong and Yang, 2023]. For the RL setting, a natural idea is to first apply the bandit algorithm in Dong and Yang [2023] on the last level, and then apply the same bandit algorithm on the second last level based on previous results and Bellman-backups, and so on. However, we note that to employ such an approach, the bandit algorithm needs to provide a \"for-all\" guarantee, i.e., finding a parameter that approximates the rewards of all arms, instead of just finding a near-optimal arm. On the other hand, existing bandit algorithms will amplify the approximation error of the input parameters by a constant factor, in order to provide a for-all guarantee. Concretely, existing bandit algorithms can only find a parameter $\\theta$ so that $\\theta$ approximates the rewards of all arms by an error of $2\\epsilon$. As we have H levels in the RL setting, the final error would be exponential in H, and therefore, such a level-by-level approach would result in a suboptimality that is exponential in H.\nOne may ask if we can further improve existing bandit algorithms, so that we can find a parameter $\\theta$ that approximates the rewards of all arms by an error of $\\epsilon$ plus a statistical error that can be made arbitrarily small, instead of $2\\epsilon$. The following theorem shows that this is information-theoretically impossible unless one pays a sample complexity proportional to the size of the action space."}, {"title": "Theorem 1.2.", "content": "Under Assumption 1.1 with $d = k = 1$, any bandit algorithm that returns an estimate $\\hat{r}$ such that $|r(a) - \\hat{r}(a)| < 2\\epsilon$ for all arms a with probability at least 0.95 requires at least 0.9n samples, where n is the total number of arms.\nTherefore, amplifying the approximation error by a factor of 2 is not an artifact of existing bandit algorithms. Instead, it is information-theoretically impossible.\nGeometric error amplification is a common issue in the design of RL algorithm with linear function approximation [Zanette et al., 2019, Weisz et al., 2021, Wang et al., 2020a, 2021]. It is interesting (and also surprising) that such an issue arises even when the function class has sparsity structures.\nOptimistic Value Function Elimination. Another approach for the design of RL algorithm is based on optimistic value function elimination. Such an approach was proposed by Jiang et al. [2017] and was then generalized to broader settings [Sun et al., 2019, Du et al., 2021, Jin et al., 2021, Chen et al., 2022b]. At each iteration of the algorithm, we pick the value functions in the hypothesis class with maximized value. We then use the induced policy to collect a dataset, based on which we eliminate a bunch of value functions from the hypothesis class and proceed to the next iteration.\nWhen applied to our setting, existing algorithms and analysis achieve a suboptimality that depends on the size of the parameter class, which could be prohibitively large. Here, we use the result in Jiang et al. [2017] as an example. The suboptimality of their algorithm is $H\\sqrt{M}\\epsilon$, where M is Bellman rank of the problem. For our setting, we can show that there exists an MDP instance and a feature map that satisfies Assumption 1.1, whose induced Bellman rank is large."}, {"title": "Proposition 1.3.", "content": "There exists an MDP instance $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, H, P, r)$ with $|\\mathcal{A}| = 2$, $H = \\log d$, $|\\mathcal{S}| = d-1$, with d-dimensional feature map $\\phi$ satisfying Assumption 1.1 with $k=1$, such that its Bellman rank is d.\nGiven Proposition 1.3, if one na\u00efvely applies the algorithm in Jiang et al. [2017], the subop-timality would be $O(H\\sqrt{d}\\epsilon)$ in our setting, which necessitates new algorithm and analysis. In Section 4, we design a new RL algorithm whose performance is summarized in the following theorem."}, {"title": "Theorem 1.4.", "content": "Under Assumption 1.1, with probability at least $1 - \\delta$, Algorithm 1 returns a policy with suboptimality at most $(4\\epsilon_{stat}+2\\epsilon_{net}+2\\epsilon)H$ by taking $O(k d k H^{3} \\cdot ln(\\frac{dH}{\\epsilon_{net} d}) \\cdot \\frac{1}{\\epsilon_{net} \\epsilon_{stat}})$ samples.\nHere $\\epsilon_{stat}$ is the statistical error. Compared to the existing approaches, Theorem 1.4 achieves a much stronger suboptimality guarantee. Later, we will also show that such a guarantee is near-optimal.\nAlthough based on the same idea of optimistic value function elimination, our proposed algorithm differs significantly from existing approaches [Jiang et al., 2017, Sun et al., 2019, Du et al., 2021, Jin et al., 2021, Chen et al., 2022b] to exploit the sparsity structure. While existing approaches based on optimistic value function elimination try to find a sequence of parameters that maximize the value of the initial states, our new algorithm selects a parameter that maximizes the empirical roll-in distribution at all levels. Also, existing algorithms eliminate a large set of parameters in each iteration, while we only eliminate the parameters selected during the current iteration in our algorithm.\nThese two modifications are crucial for obtaining a smaller suboptimality guarantee, smaller sample complexity, and shorter running time. In existing algorithms, parameters at different levels are interdependent, i.e. the choice of parameter at level h affects the choice of parameter at level h + 1. Our new algorithm simplifies this by maintaining a parameter set for each level, so each level operates independently. Further, we can falsify and eliminate any parameter showing"}, {"title": "The Hardness Result.", "content": "one may wonder if the suboptimality guarantee can be further improved. In Section 3, we show that the suboptimality guarantee by Theorem 1.4 is near-optimal.\nWe first consider a weaker setting where the algorithm is not allowed to take samples, and the function class contains a single sequence of functions. I.e., we are given a function $Q : S \\times A \\rightarrow \\mathbb{R}$, such that $|Q(s, a) - Q^*(s, a)| \\le \\epsilon$ for all $(s, a) \\in S \\times A$.\nWe show that for this weaker setting, simply choosing the greedy policy with respect to Q, which achieves a suboptimality guarantee of $O(H\\epsilon)$, is actually optimal. To prove this, we construct a hard instance based on a binary tree. Roughly speaking, the optimal action for each level is chosen uniformly random from two actions $a_1$ and $a_2$. At all levels, the reward is $\\epsilon$ if the optimal action is chosen, and is 0 otherwise. For this instance, there exists a fixed Q that provides a good approximation to the optimal Q-function, regardless of the choice of the optimal actions. Therefore, Q reveals no information about the optimal actions, and the suboptimality of the returned policy would be at least $\\Omega(H\\epsilon)$. The formal construction and analysis and construction will be given in Section 3.1.\nWhen the algorithm is allowed to take samples, we show that in order to achieve a subopti-mality guarantee of $H/T\\epsilon$, any algorithm requires $exp(\\Omega(T))$ samples, even when Assumption 1.1 is satisfied with $d = k = 1$. Therefore, for RL algorithms with polynomial sample complexity, the suboptimality guarantee of Theorem 1.4 is tight up to log factors.\nTo prove the above claim, we still consider the setting where $d = k = 1$, i.e., a good approximation to the Q-function is given to the algorithm. We also use a more complicated binary tree instance, where we divide all the H levels into $H/T$ blocks, each containing T levels. For each block, only one state-action pair at the last level has a reward of $\\epsilon$, and all other state-action pairs in the block has a reward of 0. Therefore, the value of the optimal policy would be $H/T \\epsilon$ since there are $H/T$ blocks in total. We further assume that there is a fixed function Q, which provides a good approximation to the optimal Q-function universally for all instances under consideration.\nSince Q reveals no information about the state-action pair with $\\epsilon$ reward for all blocks, for an RL algorithm to return a policy with a non-zero value, it must search for a state-action pair with non-zero reward in a brute force manner, which inevitably incurs a sample complexity of $exp(\\Omega(T))$ since each block contains T levels and $exp(\\Omega(T))$ state-action pairs at the last level. The formal construction and analysis and construction will be given in Section 3.2."}, {"title": "1.2 Related Work", "content": "A series of studies have delved into MDPs that can be represented by linear functions of predetermined feature mappings, achieving sample complexity or regret that depends on the feature mapping's dimension. This includes linear MDPs, studied in Jin et al. [2020], Wang et al. [2019], Neu and Pike-Burke [2020], where both transition probabilities and rewards are linear functions of feature mappings on state-action pairs. Zanette et al. [2020a,b] examines MDPs with low inherent Bellman error, indicating value functions that are almost linear with respect to these mappings. Another focus is on linear mixture MDPs [Modi et al., 2020, Jia et al., 2020, Ayoub et al., 2020, Zhou et al., 2021, Cai et al., 2020], characterized by transition probabilities that combine several basis kernels linearly. While these studies often assume known feature vectors, Agarwal et al. [2020] investigates a more challenging scenario where both features and parameters of the linear model are unknown."}, {"title": "2 Preliminaries", "content": "Throughout the paper, for a given positive integer n, we use [n] to denote the set {0, 1, 2, ..., n-1}. In addition, f(n) = O(g(n)) denotes that there exists a constant c > 0 such that |f(n)| \u2264 c|g(n)|. f(n) = $\\Omega$(g(n)) denotes that there exists a constant c > 0 such that |f(n)| \u2265 c|g(n)|."}, {"title": "2.1 Reinforcement Learning", "content": "Let $\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, H, P,r}$ be a Markov Decision Process (MDP) where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $H \\in \\mathbb{Z}^+$ is the planning horizon, $P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the transition kernel which takes a state-action pair as input and returns a distribution over states, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta([0, 1])$ is the reward distribution. We assume $\\sum_{h \\in [H]} r_h \\in [0,1]$ almost surely. For simplicity, throughout this paper, we assume the initial state $s_0$ is deterministic. To streamline our analysis, for each $h \\in [H]$, we use $\\mathcal{S}_h \\subseteq \\mathcal{S}$ to denote the set of states at level h, and assume $\\mathcal{S}_h$ do not intersect with each other.\nA policy $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$ chooses an action for each state, and may induce a trajectory denoted by $(s_0, a_0, r_0, ..., s_{H-1}, a_{H-1}, r_{H-1})$, where $s_{h+1} \\sim P(s_h, a_h)$, $a_h = \\pi(s_h)$, and $r_h \\sim r(s_h, a_h)$ for all $h \\in [H]$. Given a policy $\\pi$ and $h \\in [H]$, for a state-action pair $(s, a) \\in \\mathcal{S}_h \\times \\mathcal{A}$, the Q-function and value function is defined as\n$Q^{\\pi}(s, a) = \\mathbb{E}[\\sum_{h'=h}^{H-1} r(s_{h'}, a_{h'}) | s_h = s, a_h = a, \\pi] , V^{\\pi}(s) = \\mathbb{E}[\\sum_{h'=h}^{H-1} r(s_{h'}, a_{h'}) | s_h = s, \\pi]$\nWe use $V^{\\pi}$ to denote the value of the policy $\\pi$, i.e., $V^{\\pi} = V^{\\pi}(s_0)$. We use $\\pi^*$ to denote the optimal policy. For simplicity, for a state $s \\in \\mathcal{S}$, we denote $V^*(s) = V^{\\pi^*}(s)$, and for a state-action"}, {"title": "pair (s, a) \u2208 S \u00d7 A,", "content": "we denote $Q^*(s, a) = Q^{\\pi^*}(s, a)$. The suboptimality of a policy $\\pi$ is defined as the difference between the value of $\\pi$ and that of $\\pi^*$, i.e. $V^* - V^{\\pi}$.\nFor any sequence of k-sparse parameter $\\theta = (\\theta_0, ..., \\theta_{H-1})$, we define $\\pi^{\\theta}$ to be the greedy strat-egy based on $\\theta$. In other words, for each $h \\in [H]$, a state $s \\in \\mathcal{S}_h$, $\\pi^{\\theta}(s) = \\underset{a \\in \\mathcal{A}}{\\mathrm{argmax}} \\langle \\phi(s, a), \\theta_h \\rangle$. For each $h \\in [H]$, a parameter $\\theta_h$, and a state $s \\in \\mathcal{S}_h$, we also write $V_{\\theta h}(s) = \\underset{a \\in \\mathcal{A}}{\\mathrm{max}} \\langle \\phi(s, a), \\theta_h \\rangle$.\nWe will prove lower bounds for deterministic systems, i.e., MDPs with deterministic transition P and deterministic reward r. In this setting, P and r can be regarded as functions rather than distributions. Since deterministic systems can be considered as a special case for general stochastic MDPs, our lower bounds still hold for general MDPs.\nInteracting with an MDP. An RL algorithm takes the feature function $\\phi$ and sparsity k as the input, and interacts with the underlying MDP by taking samples in the form of a trajectory. To be more specific, at each round, the RL algorithm decides a policy $\\pi$ and receives a trajectory $(s_0, a_0, r_0, ..., s_{H-1}, a_{H-1}, r_{H-1})$ as feedback. Here one trajectory corresponds to H samples. We define the total number of samples required by an RL algorithm as its sample complexity. Our goal is to design an algorithm that returns a near-optimal policy while minimizing its sample complexity.\nThe Bandits Setting. In this paper, we also consider the bandit setting, which is equivalent to an MDP with H = 1. Let $\\mathcal{A}$ be the action space, and $r : \\mathcal{A} \\rightarrow \\Delta([0,1])$ be the reward distribution. At round t, the algorithm chooses an action $a_t \\in \\mathcal{A}$ and receives a reward $r_t \\sim r(a_t)$. In this case, Assumption 1.1 asserts that there exists $\\theta^*$, such that $|\\langle \\phi(a), \\theta^* \\rangle - \\mathbb{E}[r(a)]| \\le \\epsilon$ for all $a \\in \\mathcal{A}$."}, {"title": "3 Hardness Results", "content": "We prove our hardness results. In Section 3.1, we prove that the suboptimality of any RL algorithm is $\\Omega(H\\epsilon)$ if the algorithm is not allowed to take samples. This serves as a warmup for the more complicated construction in Section 3.2, where we show that for any T satisfying $1 < 2T < H$, any RL algorithm requires $exp(\\Omega(T))$ samples in order to achieve a suboptimality of $\\Omega(H/T \\cdot \\epsilon)$."}, {"title": "3.1 Warmup: Hardness Result for RL without Samples", "content": "We prove that the suboptimality of any RL algorithm without sample is $\\Omega(H\\epsilon)$.\nTheorem 3.1. Given a MDP instance satisfying Assumption 1.1, the suboptimality of the policy returned by any RL algorithm is $\\Omega(H\\epsilon)$ with a probability of 0.99 if the algorithm is not allowed to take samples. This holds even when the dimension and sparsity satisfies $d = k = 1$ and the underlying MDP is a deterministic system.\nThe formal proof of Theorem 3.1 is given in Section A.1 in the Appendix. Below we give the construction of the hard instance together with an overview of the hardness proof.\nThe Hard Instance. Our hardness result is based on a binary tree instance. There are H levels of states, and level $h \\in [H]$ contains $2^h$ distinct states. Thus we have $2^H - 1$ states in total. We use $s_0, ..., s_{2^H-2}$ to denote all the states, where $s_0$ is the unique state at level 0, and $s_1, s_2$ are the states at level 1, etc. Equivalently, $\\mathcal{S}_h = {s_{2^{h-1}}, ..., s_{2^{h+1}-2}}$. The action space $\\mathcal{A}$ contains two actions, $a_1$ and $a_2$. For each $h \\in [H - 1]$, a state $s_i \\in \\mathcal{S}_h$, we have $P(s_i, a_1) = s_{2i+1}$ and $P(s_i, a_2) = s_{2i+2}$.\nFor each $h \\in [H]$, there exists an action $a_h^* \\in {a_1, a_2}$, such that $\\pi^*(s) = a_h^*$ for all $s \\in \\mathcal{S}_h$. Based on $a_0^*, a_1^*, ..., a_{H-1}^*$, for a state $s \\in \\mathcal{S}_h$, we define the reward function as $r(s,a) = \\epsilon$ if"}, {"title": "3.2 Hardness Result for RL with Samples", "content": "In this section, we show that for any $1 < 2T < H$, any RL algorithm requires $exp(\\Omega(T))$ samples in order to achieve a suboptimality of $\\Omega(H/T \\cdot \\epsilon)$.\nTheorem 3.2. Given a RL problem instance satisfying Assumption 1.1 and $1 < 2T < H$, any algorithm that returns a policy with suboptimality less than $H/(2T) \\cdot \\epsilon$ with probability at least 0.9 needs least $0.1 \\cdot T \\cdot 2^T$ samples.\nIn the remaining part of this section, we give an overview of the proof of Theorem 3.2. We first define the MULTI-INDEX-QUERY problem, which can be seen as a direct product version of the INDEX-QUERY problem introduced in Du et al. [2020].\nDefinition 3.3. (MULTI-INDEX-QUERY) In the m-INDQn problem, we have a sequence of m indices $(i_0^*, i_1^*, ..., i_{m-1}^*) \\in [n]^m$. In each round, the algorithm guesses a pair $(j, i) \\in [m] \\times [n]$ and queries whether $i = i_j^*$. The goal is to output $(j, i_j^*)$ for any $j \\in [m]$, using as few queries as possible.\nDefinition 3.4. ($\\delta$-correct) For $\\delta \\in (0, 1)$, we say a randomized algorithm A is $\\delta$-correct for m-INDQn if for any $i^* = {i_j^*}_{j \\in [m]}$, with probability at least $1 - \\delta$, A outputs $(j, i_j^*)$ for some j.\nWe first prove a query complexity lower bound for solving m-INDQn.\nLemma 3.5. Any 0.1-correct algorithm that solves m-INDQn requires at least 0.9n queries."}, {"title": "4 Main Algorithm", "content": "Algorithm 1: Elimination Algorithm for Finding the Optimal Hypotheses\n1: Input: feature map $\\phi$, sparsity k, approximation error $\\epsilon$, statistical error $\\epsilon_{stat}$ and $\\epsilon_{net}$, failure rate $\\delta$\n2: For each $h \\in [H]$, initialize $P_h = \\mathcal{P}_h^0 = {\\theta : \\theta_M \\in \\mathbb{N}^k, |M| = k, M \\subseteq [d]}$, where $\\mathbb{N}^k$ is the maximal $\\epsilon_{net}/2$-separated subset of the Euclidean sphere $S^k$.\n3: Calculate $m = \\frac{16 k \\ln((1 + 4/\\epsilon_{net})d) + 16 \\ln(H/\\delta)}{\\epsilon_{stat}^2}$\n4: for iteration $t = 0, 1, 2, ...$ do\n5: Choose $\\theta_0^t = \\underset{\\theta \\in P_0}{\\mathrm{argmax}} V_{\\theta_0}(s_0)$.\n6: for $h = 1, 2, ..., H - 1$ do\n7: Define a policy $\\pi_h^t$, where $\\pi_h^t(s) = \\pi_{\\theta_{h'}^t}(s)$ if $s \\in \\mathcal{S}_{h'}$ with $h' < h$, and arbitrary otherwise. Collect m trajectories following $\\pi_h^t$ as a dataset\n$\\mathcal{D}_h^t = {(s_0^i, a_0^i, r_0^i, ..., s_{H-1}^i, a_{H-1}^i, r_{H-1}^i)}_{i \\in [m]}$.\n8: Choose $\\theta_h^t = \\underset{\\theta \\in P_h}{\\mathrm{argmax}} \\frac{1}{\\mathcal{D}_h^t|}\\sum_{i \\in [\\mathcal{D}_h^t]} V_\\theta(s_h^i)$, where $s_h^i$ are from dataset $\\mathcal{D}_h^t$.\n9: end for\n10: Collect m trajectories following a policy $\\pi^t = \\pi_{\\theta^t}$ as a dataset\n$\\mathcal{D}_t = {(s_0^i, a_0^i, r_0^i, ..., s_{h-1}^i, a_{h-1}^i, r_{h-1}^i)}_{i \\in [m]}$.\n11: For each $h \\in [H]$, calculate using dataset $\\mathcal{D}_h^t$:\n$\\hat{\\epsilon} =\\begin{cases}\n\\frac{1}{|\\mathcal{D}_{h}^t|} \\sum_{i=1}^{\\mathcal{D}_{h}^t|}((\\phi(s_{h-1}^i, a_{h-1}^i), \\theta_{h-1}^t) - r_h^i - V_{\\theta_{h+1}^t}(s_{h+1}^i)), & \\text{if } h \\in [H-1] \\\\\n\\frac{1}{|\\mathcal{D}_{h}^t|} \\sum_{i=1}^{\\mathcal{D}_{h}^t|} (\\phi(s_{H-1}^i, a_{H-1}^i), \\theta_{H-1}^t) - r_{H-1}^i), & \\text{if } h = H-1.\n\\end{cases}$\n12: if $\\hat{\\epsilon}_h^t \\le 2\\epsilon + 2\\epsilon_{net} + 3\\epsilon_{stat}$ for each $h \\in [H - 1]$, and $\\hat{\\epsilon}_{H-1}^t < \\epsilon + \\epsilon_{net} + \\epsilon_{stat}$ then\n13: Terminate and output $\\pi_{\\theta^t}$.\n14: else\n15: Update $P_h = P_h \\setminus {\\theta_h^t}$, for all $h \\in [H - 1]$ satisfying $\\hat{\\epsilon}_h^t < 2\\epsilon + 2\\epsilon_{net} + 3\\epsilon_{stat}$, or h = H - 1 satisfying $\\hat{\\epsilon}_{H-1}^t < \\epsilon + \\epsilon_{net} + \\epsilon_{stat}$.\n16: end if\n17: end for\nOverview. Here we give an overview of the design of Algorithm 1.\nFirst, we approximate all candidate parameter $\\theta$ with a finite set by creating a maximal $\\epsilon_{net}/2$-separated subset of the euclidean sphere $S^{k-1}$, denoted by $\\mathbb{N}^k$, and a set of all k-sized subset of $[d]$. Then, for each $h \\in [H]$, we maintain a set of parameter candidates $P_h$. Initially, $P_h$"}, {"title": "Sample Complexity.", "content": "To bound the sample complexity of Algorithm 1, it suffices to give an upper bound on the number of iterations, since in each iteration, the number of trajectories sampled by the algorithm is simply $H^2 \\cdot m = 16H^2(k \\ln((1 + 4/\\epsilon_{net})d) + \\ln(H/\\delta))/(\\epsilon_{stat}^2)$. The following lemma gives an upper bound on the number of iterations of Algorithm 1. The proof is given in Appendix B.1.\nLemma 4.2. For any MDP instance with horizon H and satisfying Assumption 1.1 with sparsity k, Algorithm 1 runs for at most $(1 + 4/\\epsilon_{net})^{\\binom{d}{k} H}$ iterations.\nSuboptimality of the Returned Policy. We now show that with probability at least 1 - $\\delta$, the suboptimality of the returned policy is at most $(2\\epsilon + 2\\epsilon_{net} + 4\\epsilon_{stat}) H$. First, we define a high probability event E, which we will condition on in the remaining part of the analysis.\nDefinition 4.3. Define E as the event that $|\\hat{\\epsilon}_h^t - \\epsilon_h^t| \\le \\epsilon_{stat}$ and $|\\frac{1}{|D_t|}\\sum_{i \\in [D_t]} V_{\\theta_h^t}(s_h^i) - E_{s_h \\sim \\pi^t} V_{\\theta_h^t}(s_h)| \\le \\epsilon_{stat}$ (where $s_h$ is from $D_t$) for all iterations t, horizon h \u2208 [H], and parameter $\\theta \\in P_h$.\nLemma 4.4. Event E holds with probability at least 1 \u2013 \u03b4.\nTo prove Lemma 4.4, we first consider a fixed level h and iteration t. Since the empirical Bellmen error $\\hat{\\epsilon}_h^t$ is simply the empirical estimate of $E_h^t$, and $\\frac{1}{|D_t|}\\sum_{i \\in [D_t]} V_{\\theta_h^t}(s_h^i)$ is simply an empirical estimate of $E_{s_h \\sim \\pi^t} V_{\\theta_h^t}(s_h)$, applying the Chernoff-Hoeffding inequality respectively would suffice. Moreover, the number of iterations has an upper bound given by Lemma 4.2. Therefore, Lemma 4.4 follows by applying a union bound over all h \u2208 [H], t \u2208 $[(1 + 4/\\epsilon_{net})^{\\binom{d}{k} H}]$ and parameter $\\theta \\in P_h$.\nWe next show that, conditioned on event E defined above, for the sequence of parameters $\\theta^* = (\\theta_0^*, ..., \\theta_{H-1}^*)$ that satisfies Assumption 1.1, we never eliminate $\\theta_h$ from $P_h$, for all h \u2208 [H]."}, {"title": "Lemma 4.5.", "content": "Conditioned on event E defined in Definition 4.3, for a sequence of parameters $(\\theta_0^*, \\theta_1^*, ..., \\theta_{H-1}^*)$ that satisfies Assumption 1.1, and their approximations $\\hat{\\theta} = \\underset{\\theta \\in P}{\\mathrm{argmin}} ||\\theta - \\theta^*||$, during the execution of Algorithm 1, $\\theta_h$ is never eliminated from $P_h$ for all $h \\in [H", "1": "the average Bell"}]}