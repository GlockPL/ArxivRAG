{"title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation", "authors": ["Henry Hengyuan Zhao", "Difei Gao", "Mike Zheng Shou"], "abstract": "Current Graphical User Interface (GUI) agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state\u2014such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real application scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation. The code is available at https://github.com/showlab/WorldGUI", "sections": [{"title": "1. Introduction", "content": "Graphical User Interface (GUI) automation has become a prominent research area, driven by the need to enhance user productivity. This domain encompasses software usage, file management, office design, coding, and web browsing. Building upon Multimodal Large Language Models (MLLMs) such as GPT-40 (OpenAI, 2023) and Claude-3.5 (Anthropic, 2024), GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an A\u0399 assistant to help the user.\nGUI automation operates in a dynamic environment, which goes beyond the traditional computer vision tasks like image recognition (He et al., 2016) and visual question answering (Antol et al., 2015). However, current GUI benchmarks such as OSWorld (Xie et al., 2024) and WebArena (Zhou et al.) do not capture this dynamism. As shown on the left side of Fig. 1, most GUI benchmarks focus on initial and final states, measuring success rates but overlooking the changing initial conditions present in real GUI scenarios. These benchmarks often ignore situations where: (1) The software interface is not in its default state. (2) The agent might get user queries at any time. (3) Differences in agent robustness, where agents with the same low success rate (e.g., 20%) may vary in their ability to self-verify or self-correct, but these abilities are not measured in a static setting. As a result, these benchmarks fail to fully assess the capabilities of GUI agents.\nIn this paper, we take the first step toward comprehensive GUI agent testing by designing GUI tasks with various initial states. As illustrated on the right side of Fig. 1, the testing process of WorldGUI can be featured: (1) Intermediate Starting States: Real user interactions with GUI assistants do not always begin from default conditions, allowing tasks to start from intermediate states where users may seek assistance at any point (see Fig. 1 (b)). (2) Contextual Variability: In some cases, tasks may originate from"}, {"title": "2. Related Work", "content": "entirely different contexts or interfaces, requiring the agent to adapt by modifying existing steps (see Fig. 1 (c)) or introducing new steps (see Fig. 1 (d)) to ensure task progression. By incorporating these elements, WorldGUI better mirrors real-world GUI environments, enabling a more accurate and thorough assessment of GUI agent capabilities.\nSpecifically, we present WorldGUI, a new benchmark featuring 10 desktop software applications and 315 tasks, including PowerPoint, Word, Excel, VSCode, etc. For each task, we provide a user query, an instructional video, and the corresponding project file. To ensure the task quality, we engaged four trained annotators skilled in using these applications and proficient in constructing data with scripts and agents for annotation. To stimulate the dynamic testing scenarios, we demonstrate each task to obtain ground-truth (GT) plans and then conduct the augmentations for each task using pre-actions. (See details in Sec. 3.)\nIn addition, we introduce a novel GUI agent framework, GUI-Thinker, which builds upon critical thinking philosophy, an aspect less emphasized in previous GUI agents (Hong et al., 2024; Cheng et al., 2024; Lai et al., 2024; Agashe et al., 2024; Wu et al., 2024). In dynamic GUI environments, application settings may not be in default configurations. This unpredictability requires agents to have the essential ability to detect and adapt to such changes to ensure task accuracy. Through our analysis of real-world GUI scenarios, we identify three critical designs for a comprehensive agent: (1) Post-Planning Critique, (2) Pre-Execution Validation, and (3) Post-Action Evaluation. Specifically, GUI-Thinker comprises five core components: Planner, Planner-Critic, Step-Check, Actor, and Actor-Critic. We argue that these components are fundamental for effective GUI agents.\nTo summarize, our key contributions are the following: (1) We are the first to stress the dynamic testing processes in GUI automation and propose a new benchmark WorldGUI which designs the GUI tasks with various initial states to simulate the real interactions; (2) We introduce GUI-Thinker, a comprehensive GUI framework. GUI-Thinker incorporate the thinking into the overall agent design, which provides valuable insights and guidance for future development; (3) We explore the essential property of critical thinking in GUI agents and empirically show that critical thinking is extremely useful for handling complex tasks."}, {"title": "2.1. GUI Benchmarks", "content": "GUI benchmarks are essential for evaluating the performance and robustness of GUI agents. For web applications, WebShop (Yao et al., 2022), and WebArena (Zhou et al.) are two text-based GUI benchmarks, the GUI information is formatted in the text style which is limited to reflect the dynamic GUI state changes. In OS environments, OS-World (Xie et al., 2024) is a comprehensive benchmark including various operating systems with real applications. Mobile benchmarks MobileAgent (Wang et al., 2024) and AppAgent (Zhang et al., 2023) propose two GUI benchmarks of mobile applications. Windows-related benchmarks like AssistGUI (Gao et al., 2024) and WindowAgentArena (Bonatti et al., 2024) propose a list of real tasks in the Windows platform. However, these benchmarks primarily rely on a static testing process and do not adequately capture the complexity and dynamic nature of GUI environments. As a result, they are insufficient for comprehensively evaluating GUI agents. We summarize the differences between WorldGUI and other related benchmarks in Tab. 5."}, {"title": "2.2. GUI Agents", "content": "Building upon MLLMs, GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an Al assistant to help the user. CogAgent (Hong et al., 2024) is a vision language model focused on GUI understanding to facilitate GUI navigation, while SeeClick (Cheng et al., 2024) and SeeAct (Zheng et al., 2024) focus on the GUI grounding for enhancing the task performance. MobileAgent (Wang et al., 2024) and AppAgent (Zhang et al., 2023) are proposed to design the agent on mobile device. Ferret-UI (You et al., 2025) is another representative work focusing on enhancing the grounding ability in the IOS platform. AutoGLM (Liu et al., 2024) propose the GUI modes capable of learning through autonomous environmental interactions by reinforcing existing GUI models. These agents have shown their ability in GUI understanding (e.g., GUI elements grounding) or action prediction but still face limitations in handling dynamic and complicated full GUI tasks. Therefore, to enhance GUI automation in dynamic environments, we propose GUI-Thinker that improves adaptability in complex GUI settings and enables agents to effectively handle unpredictable interface changes."}, {"title": "2.3. Critical Thinking in Agents", "content": "Recent advancements in foundation models and agents, particularly in LLMs such as OpenAI-01 (OpenAI, 2024) and Deepseek-R1 (DeepSeek-AI et al., 2025), have increasingly incorporated thinking processes before providing answers to effectively handle challenging reasoning tasks. The LLM-based agents utilize verify-then-correct process to evaluate and refine intermediate reasoning steps or outputs, ensuring logical coherence and consistency. One notable LLM-based agent framework, Reflexion (Shinn et al., 2024), demonstrates the effectiveness of self-reflection in solving complex tasks. Furthermore, Critic (Gou et al., 2023) integrates external tools into the critique process, leveraging them to improve performance. Noticing the GUI task is lengthy"}, {"title": "3. WorldGUI Benchmark", "content": "and complicated, the verify-then-correct process is highly suitable for the GUI scenario. Which is not only aims to enhance the reasoning performance but also indispensable to designing the key module Actor-Critic (Konda & Tsitsiklis, 1999) to ensure task completion. A closely related work, AssistGUI (Gao et al., 2024), integrates a critical module only after the Actor module to evaluate action completion. Building upon it, we introduce two additional critical modules: Planner-Critic, applied after the Planner, and Step-Check, applied before the Actor. These two modules lead to a comprehensive and fundamental GUI agent framework GUI-Thinker which will provide insights for future GUI agent design."}, {"title": "3.1. Task Formulation", "content": "GUI Automation Definition. The GUI automation task can be considered a partially observable Markov decision process (POMDP) $(S, O, A, T, R)$ with state space S, observation O, action space A, transition function $T: S \\times A \\rightarrow S$, and reward function $R: S \\times A \\rightarrow R$. In our setting, given a natural language query q, eg., Format the slide background with gradient fill that describes a specific task in high-level, along with an instructional video v as a supplement that more detailed illustrates how to complete it, the agent first get the observation $o_t \\in O$ from the state $s_t \\in S$ in the execution environment and then generate the executable action $a_t \\in A$, resulting in a new state $s_{t+1} \\in S$ and a new observation $o_{t+1} \\in O$. The process repeats until the task is finished or failed. The reward function $R: S \\times A \\rightarrow [0, 1]$ here returns a binary integer at the final step indicating the task completion status.\nWorldGUI Task Definition. As illustrated in Fig. 2, each GUI task is paired with a user query and an instructional video. To achieve state diversity within each task, we generate various initial states that converge to the same final state, resulting in distinct ground truth (GT) plans for each case."}, {"title": "3.2. Data Collection", "content": "This is accomplished through the use of pre-actions, which consist of a sequence of executable code to initialize tasks from different initial states. With the augmentation of initial states, WorldGUI is capable of mimicking the different testing scenarios as shown in Fig. 1.\nObservation Space. The observation space $O$ indicates the information of the operating system (OS) available to the agent in each state $s_t$. In our WorldGUI settings, we follow the previous work AssistGUI (Gao et al., 2024), encompassing two types of information: metadata $m_t$ from the application and screenshot $V_t$ of current state $s_t$. The metadata mainly includes the layout of panels and pop-up windows. The screenshot $V_t$ offers holistic visual information of the current state used for planning and action generation.\nAction Space. Our action space includes all raw mouse and keyboard actions, such as left-click, right-click, double-click, drag, keystrokes, and key combinations for shortcuts, among others. Mouse-related actions also specify the target position in the pixel space of the observed screenshot. To ensure a universal and comprehensive representation of actions, we adopted the widely used Python library, PyAutoGUI, for controlling mouse and keyboard inputs. Each action is represented using the syntax action_type (arguments) as in Tab. 6."}, {"title": "3.2.1. DATA SOURCE", "content": "WorldGUI consists of a broad spectrum of desktop applications, which can be categorized into five main groups: (i) Office work, includes PowerPoint, Word, Excel, and Adobe Acrobat; (ii) Windows Usage, includes System Settings and File Management; (iii) Web Browsing, includes the configuration of Youtube and website operations; (iv) Coding, focus on the customization, configuration and editing of Visual Studio Code (VSCode); (v) Media Creation, uses the AI tool like Stable Diffusion to create or edit the image or videos"}, {"title": "3.2.2. PIPELINE OF DATA CONSTRUCTION", "content": "by following user instruction.\nFor each task 1) we manually collect the raw videos from the website, 2) then manually cut the raw lengthy videos into sub-clips, 3) ask annotators to manually write the user queries, 4) prepare the project files for each task for reproducibility, 5) generate the ground-truth plans by executing the scripts and agent, 6) conduct the data augmentation by varying the initial state for each task. As shown in Fig. 11, to achieve the above six steps, we invite four annotators and write the necessary scripts to help structure and format the data. Additionally, for the GT plan generation and pre-action generation, we build simple agents to obtain the data.\nRaw Video Collection. We collect raw videos from the YouTube website as there are a lot of high-quality tutorials for desktop applications. For each software, we ask the annotators to watch the videos and then download them using different software.\nInstruction Video Preparation. After obtaining the raw videos, we write the script codes to cut the lengthy and noisy videos into the sub-clips (30 seconds to 3 minutes) that serve as the instructional video.\nUser Query Generation. After obtaining the instructional videos, annotators are asked to manually write user queries corresponding to each video. For example, a user query for a task involving File Explorer might be: \u201cRename all the files by deleting 'class' from their names.\u201d\nProject File Preparation. Following the AssistGUI (Gao et al., 2024), we create the project file for each task to ensure reproducibility without relying on resource-intensive virtual machines (Xie et al., 2024) or Docker environments (Bonatti et al., 2024). This approach guarantees that the testing process begins from a consistent state. When combined with pre-actions, it enables augmentation of the same task with various initial states.\nGT Plan Generation. We write the script to accept user query q and instructional video v as input and generate the raw plans by agent (powered by GPT-40). Since the raw plans are not flawless, annotators are asked to watch the videos and manually execute the tasks following the raw plans. During this process, annotators edit the plans to correct any inaccurate steps or descriptions, ultimately producing the finalized GT plans.\nPre-Actions Generation. To vary the task, we propose introducing pre-actions before the task begins. These pre-actions are created by annotators and involve corresponding scripts and agents. They are written in Python code, for example:"}, {"title": "3.3. Evaluation", "content": "from pyautogui import click,\nrightClick\\n rightClick(800,400).\nThe pre-actions primarily serve two purposes: 1) Simulating Intermediate Task States: Pre-actions can complete specific steps of a task, creating a starting point from an intermediate state. This approach addresses scenarios where users may seek AI assistance because they are unable to complete a task. For example, if the task involves opening a dropdown menu, the pre-action may pre-open the menu. If the agent fails to recognize this precondition and follows its plan to click the menu again, it might inadvertently close the menu, causing task failure. 2) Introducing Diverse Initial Context States: Pre-actions can also introduce variations in the initial state, such as opening random tabs or settings. This ensures that the starting state is unconventional, challenging the agent to adapt by modifying its plan or adding new steps. See details in Fig. 8.\nWorldGUI employs an execution-oriented evaluation approach by utilizing post-processing scripts to assess task completion. Specifically, for tasks like Office work and Web Browsing, we adopt exact matching to compare the differences between the ground-truth (GT) screenshots and the final screenshots. For tasks like File Management which would produce new folders or change the locations of files, etc. We create the shell script to check the status of files."}, {"title": "3.4. Data Statistics", "content": "WorldGUI compiles GUI tasks from 10 widely used applications on the Windows platform, including productivity software such as PowerPoint, Excel, and VSCode. A total of 107 meta tasks were collected from these applications, with each task being augmented 0 to 3 times based on its specific content, resulting in 208 augmented tasks. In total, WorldGUI comprises 315 tasks. See the details in Sec. C."}, {"title": "4. GUI-Thinker: Thinking before Doing", "content": "In this section, we introduce a new comprehensive GUI framework GUI-Thinker with a core and essential mechanism: critical thinking, which is vital for designing GUI agents capable of handling dynamic environments that have been overlooked in prior GUI agents (Hong et al., 2024; Cheng et al., 2024; Lin et al., 2024b; Zhang et al., 2023; Agashe et al., 2024). The GUI-Thinker includes the five fundamental but essential components as in Fig. 3 and an Interaction reasoning loop detailed in Algorithm 1. We summarize our design principles in the following:\n\u2022 Post-Planning Critique: After the planning phase, a critique module verifies and, if necessary, self-corrects the generated plans to ensure their accuracy."}, {"title": "4.1. State-Aware Planner", "content": "\u2022 Pre-Execution Validation: Before executing each subtask, a validation module determines whether the subtask should be executed. This step is crucial, as the current GUI environment may indicate that the subtask is unnecessary or requires modification to align with the current state conditions.\n\u2022 Post-Action Evaluation: After each action execution, a mechanism evaluates whether the action was successfully completed before proceeding to the next subtask.\nThese critique mechanisms ensure the reliability and adaptability of GUI-Thinker in complex GUI environments.\nThe State-Aware Planner processes the instructional video v and user query q generates an initial plan as shown in the left of Fig. 4. We use the speech recognition model Whisper (Radford et al., 2023) to translate the video v into the subtitle and then send it to the MLLM for task planning. The task plan is hierarchically structured as $p = [P_1, P_2, \u2026, P_N]$ where $p_i$ is a text string describing the i-th milestone of the task. Under each $p_i$, there is a list of subtasks $[S^1_i, S^2_i, S^N_i]$, where $S^j_i$ is the j-th subtask in the i-th milestone. To ensure the produced plans fit the GUI environment, we propose incorporating an initial screenshot $V_0$ to represent the current state. This additional context allows the agent to output plans that align with the actual state. For example, if the instructional video suggests clicking on the \"Layout\" tab in the Word application, but the current state (as indicated by the screenshot) shows that the \u201cLayout\u201d tab is already selected, there is no need to perform this action again. By utilizing the visual information from the screenshot, the State-Aware Planner can modify the plans accordingly, rather than"}, {"title": "4.2. Planner-Critic", "content": "strictly following the guidance in the instructional video or the existing knowledge from backbone MLLMs.\nPost-Planning Critique. The goal of the Planner-Critic is to assess the correctness of the initial plans generated by the State-Aware Planner and provide corrections if needed. This module is designed to ensure the accuracy of the plans while leveraging the self-critique capabilities of MLLMs. As illustrated in Fig. 4, for each Initial Plan, the output consists of four components:\n(1) <Flag>: Indicates whether the Initial Plan is correct.\n(2) <Feedback>: Identifies the error type, categorized into one of three groups: \u201cWrong Steps,\u201d \u201cMissing Steps,\u201d or \u201cRedundant Steps.\u201d\n(3) <Correction>: Provide the corrected plans if the Flag indicates that the Initial Plan is incorrect.\n(4) <Reason>: In addition to giving the corrected plans, we force the model to give the reasons. As related works, CoT (Wei et al., 2022), GPT-01 (OpenAI, 2024), and Deepseek-R1 (DeepSeek-AI et al., 2025) demonstrate that generating reasoning steps along with the answer would enhance the performance."}, {"title": "4.3. Step-Check", "content": "Pre-Execution Validation. After the plan assessment, a navigation mechanism is crucial before sending each subtask $S_t = S^j_i$ at the time step t to the Actor module. To address this, we designed a new module called Step-Check. Through extensive investigation, we discovered that during GUI task testing, perfect execution plans are rarely feasible due to the unpredictable nature of real application environments. Most software retains user preferences (e.g., remember the last configuration of user), meaning that when executing a specific task, the plan p generated by the Planner might not align with the actual state of the software. Therefore, the model must determine whether to proceed with a subtask $S_t$ based on the current state (screenshot: $V_t$, metadata: $M_t$).\nAs illustrated in Fig. 5, we employ an MLLM to determine whether the current task has been completed or requires modification. We systematically categorize the possible outcomes into four types:"}, {"title": "4.4. Actor", "content": "(1) <Modify>: Indicates that the subtask should be modified or additional subtasks should be added.\n(2) <Pass>: Indicates that the current subtask is unnecessary and can be skipped.\n(3) <Continue>: Indicates that the subtask is valid and should be executed as planned.\n(4) <Finished>: Indicates that the subtask has already been completed and requires no further action.\nIn cases where the screenshot does not provide sufficient visual information for the MLLM to determine the output, the model outputs \"#Cannot confirm\". When this occurs, we design a Region Search module implemented by an LLM. This module takes the corresponding GUI information extracted by the GUI parser and the task description of the current subtask to identify the relevant region. It then crops the region using the generated bounding box as the center coordinate, with the maximum width and height set to half of the original screenshot dimensions (ensure the region is smaller than the origin screenshot). The cropped screenshot is subsequently sent to the Step-Check module to regenerate the decision.\nThe goal of the Actor is to translate natural language subtask $S_t$ into executable code $C_t$. Using an MLLM as the backbone model, the Actor processes metadata mt and screen-"}, {"title": "4.5. Actor-Critic", "content": "shot $V_t$ as GUI context to generate precise executable actions, such as click(100, 200). Additionally, it leverages the history of previous actions as memory to aid in generating subsequent actions. The generated actions will executed in the environment, and then the new screenshot $V_{t+1}$ and metadata $m_{t+1}$ will be captured for the next processing.\nPost-Action Evaluation. After generating an action, the Actor-Critic module evaluates subtask $S_{t-1}$ completion and makes corrections if necessary. As illustrated in Fig. 6, in the first step, the module implemented by an MLLM compares screenshots $V_{t-1}$ (before action execution) and $V_t$ (after execution) while processing each subtask $S_t$ to determine the action correctness. The model outputs a <Success> flag to indicate task completion. If the <Success> flag is true, the current state $s_t = <Next>$. If the <Success> flag is false (set $s_t = <Critic>$) and the number of trial steps is below the maximum limit, the Actor-Critic module activates the Locate GUI Elements and Actor Correction processes. We introduce the module Locate GUI Elements to identify the relevant GUI elements and regenerate actions using the Actor Correction module. The corrected actions are then executed in the environment, generating updated observations ($O_t$) that include new screenshots and metadata for the continued Actor-Critic iteration. The process repeats until the <Success> flag is true or the maximum number of trials is reached."}, {"title": "5. Experiments", "content": "Implementation Details. We implement the MLLM in our GUI-Thinker by using GPT-40 (OpenAI, 2023) (gpt-40-2024-08-06) by default. For the computer mouse and keyboard control, we use the Python library PyAutoGUI.\nFollowing the AssistGUI (Gao et al., 2024), we use the GUI parser to obtain the position information of elements, e.g., buttons, icons, and text. We use some vision foundation models such as Google OCR to extract the text. By default, we use the center coordinates to represent the location of each element. All the testing is under the same screenshot resolution (1920 \u00d7 1080). In all experiments, we set the max trials of the Actor-Critic to 3 for light interaction costs. For the total trials of each task, we set it to 2 \u00d7 N + 1, where N is the length of subtasks $S_i$ in plan p. In most cases, we set the trials to 21.\nEvaluation. Given that our WorldGUI includes 315 GUI tasks, we engaged four participants with strong coding and software backgrounds to test all tasks and document their evaluation results.\nMetric. Following the previous works of OSworld and AssistGUI, we use Success Rate (SR) as the metric.\nBaselines. We compare our GUI-Thinker with two strong approaches: AssistGUI (Gao et al., 2024) and Cluade 3.5 (Computer Use) (Anthropic, 2024). AssistGUI is a prominent agent framework designed for Desktop GUI Automation, which can plan the task and then execute the task step by step by following the query. We implement it by increasing the MLLM to GPT-40 for better performance. Cluade 3.5 (Computer Use) is the strongest MLLM model specially designed for autonomous computer use. We use the open-source implementation OOTB (Hu et al., 2024) as the codebase and then add the subtitle of instructional videos into the input prompt for a fair comparison."}, {"title": "5.1. Main Results on WorldGUI", "content": "As shown in Tab. 1, our GUI-Thinker (Claude-3.5) outperforms previous agents, achieving the highest success rate. Compared to AssistGUI (GPT-40), GUI-Thinker (GPT-"}, {"title": "5.2. Ablation Study of GUI-Thinker", "content": "40) demonstrates superior performance in Office, Web, and Coding GUI tasks. This demonstrates that the critical modules are essential for effectively handling complex tasks. However, the overall success rate (SR) remains low, indicating that GUI tasks in WorldGUI are still challenging, as reflected in the Human* SR of 87.6%. When comparing the Meta and augmented (Aug.) tasks, we observe that all agents perform better on the Meta tasks but struggle with the Aug. tasks, suggesting that current agents are not yet effective in handling tasks with varying states. Analyzing the results of Computer Use (Claude-3.5), we find that it excels in Web and Coding GUI tasks, achieving a 71.4% SR on Web tasks and 54.5% SR on Coding tasks, likely due to its design being optimized for computer usage. These results significantly surpass those of GUI-Thinker (GPT-40), which lacks the corresponding GUI automation capabilities. Additionally, by equipping GUI-Thinker with Claude-3.5 as the base model, our agent achieves comparable performance in Web and Coding tasks. For the Office task, Computer Use (Claude-3.5) struggles, likely due to the dense domain-specific icons and buttons in these applications, which pose challenges for perception. By incorporating Claude-3.5 as the MLLM in our GUI-Thinker, GUI-Thinker (Claude-3.5) achieves twice the SR (57.8% vs. 28.9%) on the Office task (Meta), demonstrating the effectiveness of our agent framework.\nImpact of Critical Modules. In Tab. 2, we investigate the effectiveness of our proposed critical modules: Planner-Critic, Step-Check, and Actor-Critic. We use GUI-Thinker (GPT-40) as the Full Model to reduce financial costs. The results show that removing any of these modules leads to a decline in performance across all task groups, highlighting their importance. Planner-Critic plays a crucial role in plan correction, as its removal leads to a relative performance drop across most tasks, particularly in Office (44.4 \u2192 31.1), Web (47.6 \u2192 38.1), and Coding (45.5 \u2192 36.4). This indicates that effective plan refinement is essential for"}, {"title": "Conclusion", "content": "successfully completing GUI tasks. Step-Check is particularly important for Web and Media tasks, where removing it results in a notable decline (Web: 47.6 33.3, Media: 57.1 \u2192 28.6). This suggests that some web-based tasks require additional step modifications to improve execution accuracy. Actor-Critic is the most critical module, as its removal results in the sharpest performance drop across all task groups (Overall SR: 36.2 \u2192 13.3). We observe that many subtasks are difficult to execute correctly on the first attempt and heavily rely on the Actor-Critic for additional action correction. The most drastic declines are seen in Coding (45.5 \u2192 0.0), Windows Usage (34.8 \u2192 4.3), and Office (44.4 \u2192 15.6), indicating that these tasks are more rely on the Actor-Critic module. Overall, these results demonstrate that all three modules contribute significantly to the performance, with Actor-Critic being the most impactful.\nImpact of Different MLLM. We also investigate the impact of using different MLLMs in our framework. As shown in Tab. 3, we compare three MLLMs across five popular applications. Claude-3.5 consistently achieves the highest performance on all five applications, particularly excelling in VSCode and Acrobat tasks, where it attains an SR of over 60%. GPT-40 demonstrates stable performance across these applications, while Gemini-2.0 underperforms compared to the other two MLLMs.\nImpact of Instructional Video. In Tab. 4, we study the impact of removing the instructional video by modifying the prompt to include only the user query for generating the initial plan. On the three Office applications, we observe a significant performance decline, as these tasks rely more heavily on additional domain knowledge for successful planning. In contrast, the MLLM performs relatively well on Win. Usage tasks, such as Settings and File Management, where it has more inherent familiarity.\nIn this paper, we take the first step toward comprehensive GUI agent evaluation by introducing a new benchmark, WorldGUI. In addition to the standard static testing processes, we incorporate dynamic testing procedures to ensure that WorldGUI effectively captures the complexity and dynamism of real-world GUI environments. Furthermore, to enhance GUI automation, we propose a novel agent framework, GUI-Thinker, built upon a critical thinking philosophy and comprising five core components. This framework en-"}, {"title": "Impact Statement", "content": "ables the agent to dynamically identify uncommon states and adjust its plans or actions accordingly. Finally, we evaluate the latest computer-using agent, Claude-3.5, using our WorldGUI benchmark, demonstrating the effectiveness of GUI-Thinker across a variety of GUI tasks.\nThis paper introduces WorldGUI and GUI-Thinker, a benchmark and agent framework designed to advance the field of GUI automation. Our work has several important societal and research implications. Firstly, WorldGUI represents a significant step toward comprehensive GUI agent evaluation by incorporating diverse initial states for each GUI task. This approach better reflects real-world conditions, inspiring future GUI-related benchmarks to emphasize realistic GUI testing scenarios in their designs. Secondly, we propose a holistic GUI agent framework, GUI-Thinker, which integrates five essential components and an iterative reasoning loop. By embedding critical thinking into its module design, GUI-Thinker enhances adaptability in complex and dynamic GUI environments, allowing agents to handle unpredictable interface changes more effectively. Our work lays a strong foundation for future research in adaptive, reasoning-driven GUI automation, with potential applications in software testing, accessibility tools, and intelligent user assistance systems."}, {"title": "A. Comparison with other benchmarks", "content": "In this section, we summarize the data statistics of other benchmarks in Tab. 5 as the comparison. We can observe that the main difference of our WorldGUI is we incorporate various initial states to stimulate the real GUI automation while other benchmarks are not considered.\nWhy Instructional Video? The rationale for using instructional videos is that, in some cases, completing a task requires extensive configuration, making it impractical to provide all necessary information within the input query. As illustrated in Fig. 7, our user query is \"Generate a photo of a girl with short brown hair with EasyNegative to improve the quality\", which does not specify how to improve the quality. The instructional video will serve as contextual information to guide the agent to complete that."}, {"title": "B. Details of Actor Space", "content": "In this section, we detail the action space used in our WorldGUI. Our action space includes all raw mouse and keyboard actions, such as left-click, right-click, double-click, drag, keystrokes, and key combinations for shortcuts, among others. Mouse-related actions also specify the target position in the pixel space of the observed screenshot. To ensure a universal and comprehensive representation of actions, we adopted the widely used Python library, PyAutoGUI, for controlling mouse and keyboard inputs. Each action is represented using the syntax action_type(arguments) as in Tab. 6."}, {"title": "C. Data", "content": "(1) Annotators. In this work, we have four annotators: A, B, C, and D. The team comprises one PhD student, one Master's student, and two undergraduate students. Prior to annotation, all annotators receive training on using the applications in WorldGUI to ensure high-quality annotations. For the 10 desktop applications, we divide the software into four parts, assigning each part to a different annotator. For the human tests presented in Tab. 1, the annotators demonstrate tasks on software that they did not annotate. As shown in Tab. 5, each annotator is responsible for different software during both the annotation and human testing phases to make the soundness of the Human Test results."}, {"title": "(2) Data Construction Pipeline.", "content": "For each task 1) we manually collect the raw videos from the website, 2) then manually cut the raw lengthy videos into sub-clips, 3) ask annotators to manually write the user queries, 4) prepare the project files for each task for reproducibility, 5) generate the ground-truth plans by executing the scripts and agent, 6) conduct the data augmentation by varying the initial state for each"}]}