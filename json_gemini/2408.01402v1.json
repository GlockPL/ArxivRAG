{"title": "Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer", "authors": ["Yu Yang", "Pan Xu"], "abstract": "Decision Transformer (DT) has emerged as a promising class of algorithms in of- fline reinforcement learning (RL) tasks, leveraging pre-collected datasets and Trans- former's capability to model long sequences. Recent works have demonstrated that using parts of trajectories from training tasks as prompts in DT enhances its per- formance on unseen tasks, giving rise to Prompt-DT methods. However, collecting data from specific environments can be both costly and unsafe in many scenar- ios, leading to suboptimal performance and limited few-shot prompt abilities due to the data-hungry nature of Transformer-based models. Additionally, the limited datasets used in pre-training make it challenging for Prompt-DT type of methods to distinguish between various RL tasks through prompts alone. To address these challenges, we introduce the Language model-initialized Prompt Decision Trans- former (LPDT), which leverages pre-trained language models for meta-RL tasks and fine-tunes the model using Low-rank Adaptation (LoRA). We further incorpo- rate prompt regularization to effectively differentiate between tasks based on prompt feature representations. Our approach integrates pre-trained language model and RL tasks seamlessly. Extensive empirical studies demonstrate that initializing with a pre-trained language model significantly enhances the performance of Prompt-DT on unseen tasks compared to baseline methods.", "sections": [{"title": "Introduction", "content": "In many sequential decision-making applications such as robotic manipulation and autonomous driving (Sinha et al., 2022; Kumar et al., 2021), it can be expensive or even unsafe for agents to learn through trial-and-error with the environment. Offline reinforcement learning (RL) methods (Levine et al., 2020) have emerged as a powerful paradigm for optimizing agent policies without directly interacting with the environment. They leverage pre-collected datasets obtained from a set of behavior policies instead of online interactions to learn an optimal policy. Among these Offline RL methods, Decision Transformer (DT) (Chen et al., 2021) has become popular for offline RL tasks due to its scalability with computation and data and stability in training. DT models a goal-conditioned policy using a Transformer network, solving a sequence-prediction problem in a supervised learning manner. More specifically, DT formulates decision-making as sequence generation over pre-collected trajectories using the powerful Transformer architecture (Vaswani et al., 2017). DT models the states, actions and return-to-go from the RL trajectory as the word tokens in Natural Language Processing (NLP) tasks and then generates the actions conditioned on the return goals.\nCompared with traditional dynamic programming-based offline RL methods (Kumar et al., 2019; Fujimoto et al., 2019; Kumar et al., 2020) that heavily rely on the Markov Decision Process (MDP) assumption of the environment, Decision Transformer can utilize entire trajectory histories to pre- dict the next action, making them more applicable in partially observable environments where all past information must be incorporated in decision-making(Kaelbling et al., 1998; Ni et al., 2024). Furthermore, the supervised learning nature of DTs enhances the stability and scalability in the"}, {"title": "", "content": "training process compared to dynamic programming algorithms based on Bellman equations (Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022). Another advantage of Transformers is their few-shot generalization ability (Brown et al., 2020; Achiam et al., 2023). Based on their remarkable few-shot generalization capability, a prompt-based framework has been proposed and proven effec- tive for adapting to new tasks in NLP (Brown et al., 2020; Li & Liang, 2021). In this paradigm, the prompt, containing useful information about the task, is inputted as a prefix to the model for identifying the environments. Previous works have demonstrated that DTs also exhibit good gener- alization ability for unseen tasks. Prompt-DT (Xu et al., 2022) leverages parts of trajectories from datasets as prompts to encapsulate task information. The method is trained on these trajectories and corresponding prompts, then tested on unseen tasks with few-shot demonstrations as prompts.\nHowever, existing Prompt-DT methods (Xu et al., 2022; Hu et al., 2023; 2024) require significant amounts of data for pre-training due to the data-hungry nature of Transformers (Brown et al., 2020; Achiam et al., 2023). Offline RL datasets are often small and insufficient to fully unleash the few- shot prompt learning capability of Transformers. Collecting large amounts of RL trajectories for pre-training powerful Decision Transformers is challenging. Inspired by the broad success of large language models in NLP, recent works (Li et al., 2022; Reid et al., 2022; Shi et al., 2023) have shown the potential of such models to provide effective initial weights for decision-making tasks. However, these works do not directly demonstrate few-shot abilities due to a lack of multi-task training and prompt guidance. Language initialization in these works provides pre-knowledge and helps alleviate the need of huge datasets. Therefore, we aim to explore the use of pre-trained language models to initialize Prompt-DT methods and reduce the dependency on large datasets for training.\nIn this work, we propose a novel framework, Language model-initialized Prompt Decision Trans- former (LPDT), that utilizes pre-trained language model initialization to improve the few-shot prompt ability of Decision Transformer. Our approach initializes the model with pre-trained lan- guage models, incorporating pre-existing knowledge that might benefit downstream RL tasks. We combine this pre-trained knowledge with domain-specific knowledge from multi-task RL by using LORA (Hu et al., 2021), a parameter-efficient fine-tuning method, to fine-tune the pre-trained model on a multi-task RL offline dataset using prompts. Furthermore, distinguishing different testing envi- ronments is vital for multi-task or meta RL. Therefore, we introduce prompt regularization methods to help the pre-trained Decision Transformer distinguish different RL tasks, guiding action gener- ation under new, unseen task-specific prompt representations. A more detailed illustration of the model structure and our training paradigm is provided in Figure 1. We conduct extensive exper- iments to assess the capability of our proposed framework in MuJoCo control environments (Fu et al., 2020) and Meta World ML1 tasks (Yu et al., 2020). Our method outperforms baselines in terms of cumulative rewards on unseen tasks. Our contributions are summarized as follows.\n\u2022 We propose a framework named LPDT to improve the few-shot prompt ability of Decision Trans- former. This framework involves leveraging the language model as the initialization of DT and imposing both supervised and unsupervised prompt regularization. LPDT demonstrates im- proved few-shot prompt capabilities with pre-trained language knowledge in multi-task RL.\n\u2022 We utilize Low-Rank Adaptation (LoRA) and an additional prompt regularization method to combine pre-trained knowledge with domain-specific RL task knowledge. LoRA allows efficient fine-tuning by adapting only a small subset of parameters, while both the supervised and un- supervised prompt regularization enhances the model's ability to distinguish task information contained in the prompts.\n\u2022 Through extensive experiments on MuJoCo control and Meta World ML1, we demonstrate the advantages of LPDT compared to baselines. Our results show that LPDT significantly outper- forms existing models in performance under full and limited datasets, highlighting its potential for real-world applications."}, {"title": "Preliminary", "content": "2.1 Offline Reinforcement Learning\nReinforcement learning problems are usually formulated as a Markov Decision Process (MDP) de- fined by a tuple ($\\mathcal{S}, \\mathcal{A}, T, d_0, R, \\gamma$), where $\\mathcal{S}$ represents the set of states $s \\in \\mathcal{S}$, $\\mathcal{A}$ represents the set of actions $a \\in \\mathcal{A}$, $T$ is the transition distribution in the form $T(s_{t+1}|s_t, a_t)$, $d_0$ is the distribution of initial states $s_0$, $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, $r_t = R(s_t, a_t)$ is the reward at timestep $t$, and $\\gamma \\in (0, 1)$ is the discount factor. The objective is to find a policy $\\pi$ that maximizes the expected cumulative rewards $J(\\pi)$:\n$J(\\pi) = \\mathbb{E}_{s_0 \\sim d_0(\\cdot), a_t \\sim \\pi(\\cdot|s_t), s_{t+1} \\sim T(\\cdot|s_t, a_t)} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)].$ (1)\nThere are many RL algorithms proposed to solve these MDP problems via online interactions with the environment. However, online RL algorithms are not feasible in some scenarios such as robotic manipulation and autonomous driving where the interactions with the environment are associated with high computational cost and risks. Offline RL methods (Levine et al., 2020) become popular in these scenarios. Different from the online setting, the agent has access to a dataset $\\mathcal{D}$ contain- ing trajectories collected by a behavior policy instead of access to the environment. The agent is expected to find the optimal policy using only the offline dataset $\\mathcal{D}$, without interacting with the"}, {"title": "", "content": "environment itself. Among various offline RL methods (Levine et al., 2020; Kumar et al., 2019; Fujimoto et al., 2019; Kumar et al., 2020), Decision Transformer (Chen et al., 2021) which leverages the Transformer (Vaswani et al., 2017) to predict the next action conditioned on the past trajectory is drawing increasing attention. Transformer is first proposed by Vaswani et al. (2017), which has been extensively used in natural language processing (NLP) and computer vision (CV). Recently, Transformer has also been increasingly studied in reinforcement learning using the sequence model- ing paradigm (Chen et al., 2021; Furuta et al., 2021; Xu et al., 2022; Janner et al., 2021). In DT, the trajectories {$s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T$} in the offline dataset $\\mathcal{D}$ are reformulated and modeled as a sequence generation problem via self-supervised learning paradigm.\n2.2 Prompt Decision Transformer\nDecision Transformer leverages the powerful Transformer model to predict the action via the se- quence generation paradigm. Recent advances in NLP (Achiam et al., 2023; Puri & Catanzaro, 2019) demonstrate that Transformer enjoys impressive few-shot learning capabilities when it is pre- trained on vast language datasets and then generates the word tokens under a specific prompt in the testing phase which is used to describe the language tasks. Similarly, Decision Transformer can also be extended to use prompt information to improve its generalization ability in unseen tasks during testing. Xu et al. (2022) introduced Prompt-DT, which leverages the Decision Transformer archi- tecture to model the RL trajectories in multi-task environments and make decisions in unseen tasks based on the prompt framework, achieving few-shot learning ability in offline RL setting. Different from the prompt-based methods in NLP tasks, Prompt-DT utilizes the sampled short trajectories from the specific task in the offline dataset as the prompt. These prompts also contain environment information, which can help the model distinguish the tasks.\nIn the offline dataset $\\mathcal{D}$, we have the trajectories $\\mathcal{T}$. The prompts and training trajectories are both from this dataset. Different from the offline RL setting, the rewards in the training trajectories of Decision Transformer are replaced by the return-to-go which are denoted as the $R_t = \\sum_{t=i}^T r_t$. The prompt is the short trajectory which can also be denoted as the tuples of state $s^*$, action $a^*$, and return-to-go $R^*$. It captures essential and concise trajectory information to aid task identification without prior knowledge of the RL tasks. During the training stage, we utilize the offline RL dataset $\\mathcal{D}$ which contains multiple RL tasks denoted as $\\mathcal{T}_i \\in \\mathcal{T}_{train}$. The input of Prompt-DT is a concatenation of the prompt and the training trajectory, denoted by $\\tau^*$ and $\\tau_i$ respectively. Specifically, we denote the prompt $\\tau^*$ as the following sequence\n$\\tau^* = (R^*_1, s^*_1, a^*_1, ..., R^*_{K^*}, s^*_{K^*}, a^*_{K^*}),$ (2)\nwhere $K^*$ is the length of the prompt. These trajectory prompts are shorter than the horizon of the tasks, thereby providing crucial guidance without enabling complete task imitation. Beyond the prompt, the training trajectories of DT can be denoted as\n$\\tau_i = (R_{i,1}, s_{i,1}, a_{i,1}, \\cdots, R_{i,K}, s_{i,K}, a_{i,K}),$ (3)\nwhere $K$ is the length of the training trajectories. Consequently, we have the input vector $\\tau^{input}$ defined as\n$\\tau^{input} = [\\tau^*, \\tau_i] = (R^*_1, s^*_1, a^*_1, ..., R^*_{K^*}, s^*_{K^*}, a^*_{K^*}, R_{i,1}, s_{i,1}, a_{i,1}, \\cdots, R_{i,K}, s_{i,K}, a_{i,K}).$ (4)\nBesides, we denote the partial trajectory from the timestep 1 to timestep t as $\\tau_{i|t}^{input}$. Then the learning objective of Prompt-DT can be formulated as the following maximum likelihood estimation:\n$\\mathcal{L}_{PDT} = \\mathbb{E}_{\\tau^{input} \\sim \\mathcal{T}_i} [\\sum_{t=1}^K - \\log M_{\\theta} (a_{i,t} | \\tau_{i|t}^{input-1}, R_{i,t}, s_{i,t})],$ (5)\nwhere $M_\\theta$ denotes the Prompt-DT model with the parameter $\\theta$. In practical implementations, we often use the mean squared error loss instead, which aims to predict the future action $a_{i,t}$ given the history trajectory and current state by minimizing the following loss function.\n$\\mathcal{L}_{PDT} = \\mathbb{E}_{\\tau^{input} \\sim \\mathcal{T}_i} [\\frac{1}{K} \\sum_{t=1}^K (a_{i,t} - \\hat{a}_{i,t})^2].$ (6)"}, {"title": "The Proposed Framework", "content": "We propose Language model-initialized Prompt Decision Transformer (LPDT), a novel and effective framework to incorporate powerful pre-trained language models into Decision Transformers to im- prove their few-shot learning abilities. We also leverage an additional prompt regularization over the prompts during fine-tuning to better identify tasks. Figure 1 illustrates the overview of our method. At a high level, LPDT incorporates several key components:\n\u2022 Language model initialization for Prompt-DT: We first use a pre-trained language model, such as DistilGPT2 (Sanh et al., 2019; Radford et al., 2019), as the initialization for our Decision Transformer. This design ensures compatibility with the Prompt-DT paradigm, where prompts from various tasks are appended to the input sequence.\n\u2022 Parameter-efficient fine-tuning on RL tasks: We adopt Low-Rank Adaptation (LoRA) (Hu et al., 2021) to fine-tune a low-rank residual matrix while keeping the original weight matrix of the language model fixed throughout the learning process. This approach significantly reduces the number of parameters compared to standard full fine-tuning of the large language models.\n\u2022 Prompt regularization with supervised and unsupervised objectives: We incorporate additional regularization over the prompt embeddings to better identify tasks. Specifically, we employ loss functions derived from both supervised and unsupervised learning techniques to fully utilize task-related information from prompts, thereby preventing the language model from overfitting specific tasks.\nWe discuss these techniques in detail in the rest of this section. At the end of this section, built on these components, we present our learning algorithm in Algorithm 1.\n3.1 Language model initialization for Prompt-DT\nThe first step in our LPDT framework is to use pre-trained language models as the initialization. Recent advances in large language models have demonstrated that these models possess strong few- shot learning abilities. With task-specific information such as prompts for translation or answering questions, language models can generate relevant outputs. We adapt these language models to RL tasks, such as MuJoCo controls, to reduce the demand for large datasets by leveraging pre-trained knowledge that may have relevance to downstream RL tasks. In this work, we use the DistilGPT2 (Sanh et al., 2019; Radford et al., 2019) as the initial weights, which is a pre-trained model with 82 million parameters and is faster and lighter than the original GPT-2 (Radford et al., 2019). The common next-token prediction objective of GPTs can be formulated as\n$\\mathcal{L}_{LLM} = \\sum_{i=1}^{t-1} - \\log(M_{\\theta^*}(w_{i+1} | w_1, ..., w_i)),$ (7)\nwhere $M_{\\theta^*}$ is the language model and $w_i$ represents the word token. To make the language model compatible with the RL sequence prediction tasks in DT, we follow previous work (Shi et al., 2023) to replace the word token embedding input and output layers with linear layers, which are trainable for specific RL tasks. Moreover, in the input layer, we also need to incorporate a prefix as the prompt from sequences chosen from trajectories rolled out in the offline dataset. The embedding layers are represented by the different color blocks in Figure 1 for the prompt and input sequence, respectively. Apart from the input and output embedding layers, the entire intermediate layers of the language model will be frozen during the training on RL tasks.\n3.2 Parameter-efficient fine-tuning on RL tasks\nTo adapt the language models to specific RL tasks, we add a low-rank adaptation of the frozen weights of the language model and update it using parameter-efficient methods like LoRA (Hu et al.,"}, {"title": "", "content": "2021). Specifically, LoRA utilizes two low-rank matrices to represent the weight matrix, significantly reducing the number of parameters. This process can be formulated as $W = W_0 + \\Delta W = W_0 + AB$, where $W \\in \\mathbb{R}^{d \\times k}$ is the weight of our model, $W_0 \\in \\mathbb{R}^{d \\times k}$ is the frozen weight inherited from the language model, and $A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times k}$ are low-rank matrices. In this way, we avoid fully fine-tuning the language model and make our method scalable to large language models, where only a small number of parameters from the low-rank matrix $\\Delta W$ need to be updated.\n3.3 Prompt regularization with supervised and unsupervised objectives\nPrevious works such as Prompt-Tuning DT (Hu et al., 2023) and Prompt Diffuser (Hu et al., 2024) aim to tune the prompt during testing on unseen tasks. These methods seek to optimize the prompt during the testing phase separately, allowing the model to distinguish the current task during testing. However, they are not always effective and can result in inferior performance when tasks are too similar. To address this challenge and achieve improved performance on testing tasks, we incorporate a task identification procedure into the training process. To effectively distinguish tasks by their prompts, we adopt additional regularization on the training loss over the prompt embeddings, termed prompt regularization.\nSince our model is built upon Prompt-DT, we use the loss function for Prompt-DT defined in (6) as the base loss function, and then incorporate a prompt regularization term. The final loss function of our method is as follows.\n$\\mathcal{L}_{total} = \\mathbb{E}_{\\tau^{input} \\sim \\mathcal{T}_i} [\\frac{1}{K} \\sum_{t=1}^K (a_{i,t} - \\hat{a}_{i,t})^2] + \\lambda \\mathcal{L}_{\\phi},$ (8)\nwhere $\\mathcal{L}_{\\phi}$ is the loss for the prompt regularization which we will specify in the rest of this section, and $\\lambda$ is the hyperparameter for prompt regularization.\nIn particular, we propose two practical implementations of prompt regularization based on supervised learning and unsupervised learning methods respectively.\nSupervised learning-based prompt regularization. In this approach, we add a classifier head to the output of the prompt encoder. We use the task ID from the dataset as the label to help the prompt encoder learn a more meaningful embedding that can easily distinguish different task environments. We adopt cross-entropy as the loss function. We formulate $\\mathcal{L}_{\\phi}$ as:\n$\\mathcal{L}_{classifier} = - \\sum_i y_i \\log(\\hat{y}_i),$ (9)\nwhere $y_i$ is the true task label which means the task ID and $\\hat{y}$ is the predicted probability for the task which comes from the prompt $\\tau^*$.\nUnsupervised learning-based prompt regularization. When task IDs are unknown, the su- pervised method may not be feasible. Therefore, we also propose an unsupervised learning method to learn the prompt representation. From an information theory perspective, the ideal prompt en- coder should aim to maximize the mutual information between the prompt representation and the tasks. We use the InfoNCE objective (Oord et al., 2018) to calculate the loss over the prompt. We formulate $\\mathcal{L}_{\\phi}$ as:\n$\\mathcal{L}_{InfoNCE} = - \\mathbb{E} [\\log \\frac{\\exp(sim(z_i, z_j) / \\tau)}{\\sum_{k=1}^N \\exp(sim(z_i, z_k) / \\tau)}],$ (10)\nwhere $z_i$ and $z_j$ are the encoded representations of the prompts $\\tau^*$ and $\\tau^*$ respectively. The term $sim(z_i, z_j)$ denotes the similarity function (e.g., cosine similarity) between $z_i$ and $z_j$.\n4 Experiments\nIn this section, we conduct experiments to evaluate the few-shot generalization ability of our proposed method LPDT. We evaluate the performance of LPDT on MuJoCo control tasks (Fu et al., 2020)"}, {"title": "", "content": "and Meta World (Yu et al., 2020) with the episode accumulated reward as the evaluation metric. We also evaluate the prompt ability of LPDT with the smaller dataset sizes. Our experiments aim to address the following questions: (1) Can LPDT with language model initialization achieve better performance compared with Prompt-DT and other baselines? (2) Does the prompt regularization help the model distinguish the tasks and improve the prompt learning ability of LPDT? (3) Does the language-initialized Transformer model contain the knowledge of the unseen RL tasks and help improve the performance on a smaller size of data.\n4.1 Implementation\nIn the empirical study, we implement our LPDT method with DistilGPT2 as the language initial- ization. The initialization language model weight comes from the Huggingface. The DistilGPT2 contains 82 million parameters which is lighter and more efficient than GPT2 with 124 million parameters. DistilGPT2 is pre-trained on the openwebtext (Puri & Catanzaro, 2019) and is dis- tilled by Sanh et al. (2019). During the fine-tuning stage, we follow the same hyperparameters for Prompt-DT (see Appendix C for detail). We also leverage the LoRA to highly reduce the parame- ters trained. For the prompt regularization, we use MLP to further encode the prompt embedding. For the supervised version of prompt regularization defined in (9), we directly use the logits from the MLP to compute the cross-entropy loss and refer to the method as LPDT-Classifier. For the unsupervised version of prompt regularization defined in (10), we calculate the similarity matrix through the cosine similarity based on the logits from the MLP and refer to it as LPDT-InfoNCE.\n4.2 Datasets and Tasks\nIn this work, we evaluate the performance of our proposed approach on MuJoCo controls and Meta World, which are commonly used in existing Prompt-DT type of methods (Xu et al., 2022; Hu et al., 2023; 2024), namely, Cheetah-dir, Cheetah-vel, Ant-dir, Meta-World reach-v2 and MW pick-place- v2. In Cheetah-dir, there are two tasks with goal directions as forward and backward, where the reward function promotes high velocity along the goal direction. The training and testing phases"}, {"title": "", "content": "both include the two tasks. Similar to Cheetah-dir, Ant-dir also segments the tasks by directions. There are 50 tasks in Ant-dir with different goal directions uniformly sampled in 2D space. The tasks are split into 45 training tasks and 5 testing tasks. The ant is also rewarded with high velocity along the goal direction. Different from segmenting the tasks by direction, Cheetah-vel penalizes the agent through the 12 errors with the target velocities sampled from the velocity interval. There are 40 tasks with different goal velocities where 35 tasks are training tasks and 5 tasks are testing tasks. Except for the MuJoCo control meta-RL tasks, we also test our approach on Meta World (Yu et al., 2020) which is an open benchmark for meta-RL and multi-task learning. In this work, we evaluate our approach on Meta-World reach-v2 and Meta-World pick-place-v2. The objective of reach-v2 is to control the robot to reach the target position in 3D positions and pick-place-v2 is to grasp the object. Each task has a different goal position.\nWe utilize the dataset and settings from the Prompt-DT paper (Xu et al., 2022). To be specific, the datasets of Cheetah-dir and Ant-dir come from the replay buffer of Soft Actor-Critic (Haarnoja et al., 2018) and the dataset of Cheetah-vel comes from TD3 (Fujimoto et al., 2018). For Meta- World reach-v2 and Meta-World pick-place-v2, we collected the dataset through the expert policies provided in the open benchmark.\n4.3 Baselines\nWe compare the few-shot generalization ability of our proposed LPDT with baseline algorithms. For each method, we compare the performance based on the accumulated reward. The baselines we choose include Prompt-DT (Xu et al., 2022), Prompt-Tuning DT (Hu et al., 2023), and Prompt Diffuser (Hu et al., 2024). Prompt-DT is the first method to utilize the prompt to guide Decision Transformer in testing with the few-shot demonstrations. Prompt-DT directly uses the prompt without any additional fine-tuning process when testing. Prompt-Tuning DT is based on Prompt- DT and utilizes prompt tuning methods when testing on the unseen task. Several prompt tuning techniques are used to tune the prompt to the specific target environment using preference ranking. Prompt Diffuser extends the prompt tuning method by leveraging diffusion models to generate high-quality prompts to improve the few-shot demonstration guidance. Beyond these baselines, Multi-Task Decision Transformer (MT-ORL) (Chen et al., 2021) is mentioned in Prompt-DT and Soft-Prompt (Lester et al., 2021) is described in Prompt Diffuser. So we do not demonstrate them in our experiments. For HDT (Xu et al., 2023b), it utilizes the adapter to adapt the pre-trained model to new tasks, which is orthogonal to the prompt-based methods. Thus we do not include their results in our comparison.\n4.4 Comparison with Prompt-DT type of methods\nTable 1: Results for MuJoCo control tasks and MW tasks. The best mean scores are highlighted in bold. For each environment, the length of the prompt is K* = 5. The dataset we utilized is the full dataset. We test all the results on unseen tasks with three random seeds. LPDT outperforms baselines on the Cheetah environment and is competitive in the Ant environment."}, {"title": "Conclusion", "content": "In this work, we proposed a novel framework for improving the few-shot prompt ability of deci- sion transformers in offline reinforcement learning, i.e., Language model-initialized Prompt Decision Transformer (LPDT). By leveraging pre-trained language models and combining them with domain- specific RL datasets, LPDT demonstrates improved few-shot prompt capabilities and outperforms or is competitive with best existing baselines in prompt based methods in terms of cumulative rewards on unseen tasks. Our approach has the potential to significantly reduce the data requirements for offline RL tasks, making it more applicable to real-world scenarios where collecting large amounts of RL trajectories is challenging. Furthermore, our results highlight the importance of using pre-trained language models as a starting point for decision-making tasks and demonstrate the effectiveness of our prompt regularization methods in enhancing the model's ability to distinguish task information contained in prompts.\nWhile LPDT has shown promising results, there are several limitations to our approach. Due to computing resource constraints, our language models are currently limited to GPT-2 and DistilGPT- 2. To fully realize the potential of pre-trained language models for decision-making tasks, we hope to extend our approach to more open-source language models and utilize more efficient fine-tuning techniques in the future. Additionally, exploring alternative architectures or incorporating multi- task learning could further enhance the performance of LPDT. Future work will focus on addressing these limitations and expanding the scope of LPDT to a broader range of pre-trained language models and decision-making tasks. As such, LPDT offers a promising direction for future research in offline RL."}, {"title": "Related Work", "content": "Decision Transformer. Decision Transformer (DT) (Chen et al., 2021) emerged as a type of algorithm for offline RL by using the powerful Transformer architecture for decision-making. DT models RL trajectories as a sequence generation problem and utilizes the next-token generation paradigm for training. Thus, DT takes the history tokens such as the return, state, and action to predict the next action, which formulates the decision-making as an action prediction or sequence generation in a supervised fashion. Since DT can fully utilize the whole trajectories and is easy to train compared with dynamic programming-based offline RL, many of the following works improved the performance under different settings. For example, Lee et al. (2022) proposed the Multi-game Decision Transformer which is trained on part of the Atari games as the multi-tasks training and fine-tuned on the remaining games to achieve efficient adaption. Hyper Decision Transformer (Xu et al., 2023a) adds an adapter into Decision Transformer and is fine-tuned on unseen tasks through the demonstration without expert actions. Xie et al. (2023) proposed to predict the action conditioned on the future trajectory embedding instead of conditioned on the return. Trajectory Transformer (Janner et al., 2021) is another research line, which is trained on sequences of state, action, and rewards and generated with the beam search.\nPrompt-DT. Prompt Decision Transformer (Xu et al., 2022) utilizes the prompt-based framework to do the meta-RL. It is trained on multi-RL tasks with offline datasets. During the training, the prompts or demonstrations which are a small part of the trajectory are combined with trajectories. During the testing on unseen tasks, the prompt can be a guide for indicating the tasks and help the model predict the action to interact with the environments. Following Prompt-DT, several works are adopting the prompt tuning method to achieve a high-quality prompt. Prompt-Tuning DT (Hu et al., 2021) uses the preference ranking function and black-box tuning method to tune the prompt when testing on unseen tasks to achieve a high-quality prompt. Moreover, Prompt Diffuser (Hu et al., 2024) leverages the diffusion model to generate high-quality prompts leading to improved performance in downstream RL tasks. Different from these works, we adopt the prompt regularization which aims to learn a high-quality prompt embedding to distinguish the different but similar RL tasks. Our method adopts this regularization during the training procedure in the prompt dataset.\nLanguage model based DT. Large language models have achieved many surprising effects in various tasks in recent years. Pre-trained on large datasets such as the corpus of the Internet, LLMs such as GPTs (Radford et al., 2019) demonstrate prompt ability which can generate the text with the guide of the task information. The success of the large language models motivates the increasing use of pre-trained language models in improving Decision Transformer to solve RL tasks (Chen et al., 2021). Several works utilize the powerful representation generalization ability of language models as policies to do the decision-making. Li et al. (2022) proposed to adopt the pre-trained language models for interactive decision-making to convert the policies to sequence data. Wik-RL (Reid et al., 2022) uses a pre-trained language model from the next-token generation paradigm as the initialization of DT for offline RL tasks. However, it suffers from inferior performance than directly using DT. To overcome these challenges and unleash the power of language models, Shi et al. (2023) proposed the LaMo algorithm which uses a pre-trained language model and parameter-efficient fine- tuning methods to improve the original DT. Zhang et al. (2024) also proposed to use LaMo in partially observable continuous control problems which demonstrates a strong generalization ability. Unlike all the above methods, our approach is fine-tuned for learning to identify different prompts for various RL tasks. And during the testing phase, just a small part of the trajectories is used in our method as the prompt without updating the model."}, {"title": "Details on the Experiment Environments", "content": "We evaluate our approach on the MuJoCo tasks and Meta-World ML1 tasks. We split the tasks in these environments into the training sets and the testing sets. The tasks in Cheetah-dir and Ant-dir are split by directions. The tasks in Cheetah-vel are split by the goal velocities. In Meta-World, the tasks are defined by different goal positions. The detailed task indexes can be found in Table 2. The experiments we conducted all followed this setting which guaranteed consistency during the evaluation."}, {"title": "Hyperparameters", "content": "In this section, we show the hyperparameter of our LPDT algorithm for experiments presented in Table 1. The hyperparameters have two parts, corresponding to the Transformer architecture and the prompt regularization respectively. We list these hyperparameters in Table 3."}, {"title": "More Experimental Results", "content": "In this section, we provide comprehensive summaries of the experimental results. These are the results of all the unseen tasks in MuJoCo control environments. This summary includes all the experiments and ablation study results on our various components. We show that our methods with prompt regularization are much better than those without regularization and with text regulariza- tion. Table 4 demonstrates the results in Cheetah-dir, Table 5 refers to the results in Cheetah-vel and Table 6 refers to the Ant-dir. These results further support our observations and conclusions drawn in the experiment section."}, {"title": "Ablation Studies", "content": "In this section, we provide ablation studies on LPDT to test the role of prompt regularization and language initialization respectively."}, {"title": "The role of prompt regularization", "content": "We first compare our proposed model with the same approach without the prompt regularization, denoted as LPDT w/o regularization. We also follow"}]}