{"title": "A Multi-Modal Deep Learning Framework for Pan-Cancer Prognosis", "authors": ["Binyu Zhang", "Shichao Li", "Junpeng Jian", "Zhu Meng", "Limei Guo", "Zhicheng Zhao"], "abstract": "Prognostic task is of great importance as it closely related to the survival analysis of patients, the optimization of treatment plans and the allocation of resources. The existing prognostic models have shown promising results on specific datasets, but there are limitations in two aspects. On the one hand, they merely explore certain types of modal data, such as patient histopathology whole slide images (WSI) and gene expression analysis, while neglecting information related to patients' basic characteristics, medical records and treatment regimens. On the other hand, they adopt the per-cancer-per-model paradigm, which means the trained models can only predict the prognostic effect of a single type of cancer, resulting in weak generalization ability.\nIn this paper, a deep-learning based model, named Unified Multi-modal Pan-cancer Survival Network (UMPSNet), is proposed. Specifically, to comprehensively understand the condition of patients, in addition to constructing encoders for histopathology images and genomic expression profiles respectively, UMPSNet further integrates four types of important meta data (demographic information, cancer type information, treatment protocols, and diagnosis results) into text templates, and then introduces a text encoder based on finetuned CLIP20 to extract textual features. In addition, the optimal transport (OT)-based attention mechanism is utilized to align and fuse features of different modalities. Furthermore, a guided soft mixture of experts (GMoE) mechanism is introduced to effectively address the issue of distribution differences among multiple cancer datasets. The code of UMPSNet is available at https://github.com/binging512/UMPSNet.\nUMPSNet conducts joint training and validating for survival prediction with five public multi-type cancer datasets from The Cancer Genome Atlas (TCGA, which contains 2831 cases and a total of 3523 WSIs), including Bladder Urothelial Carcinoma (BLCA, 373 cases), Breast Invasive Carcinoma (BRCA, 956 cases), Glioblastoma & Lower Grade Glioma (GBMLGG, 569 cases), Lung Adenocarcinoma (LUAD, 453 cases), and Uterine Corpus Endometrial Carcinoma (UCEC, 480 cases). UMPSNet achieves an average concordance index (C-index) of 0.725 on the five datasets and outperforms all state-of-the-art (SOTA) methods. This result not only demonstrates the effectiveness of our UMPSNet, but also shows the feasibility and great potential of the new learning paradigm, that is, carrying out collective prognostic analysis for multiple diseases.", "sections": [{"title": "Summary", "content": "Background\nPrognostic task is of great importance as it closely related to the survival analysis of patients, the optimization of treatment plans and the allocation of resources. The existing prognostic models have shown promising results on specific datasets, but there are limitations in two aspects. On the one hand, they merely explore certain types of modal data, such as patient histopathology whole slide images (WSI) and gene expression analysis, while neglecting information related to patients' basic characteristics, medical records and treatment regimens. On the other hand, they adopt the per-cancer-per-model paradigm, which means the trained models can only predict the prognostic effect of a single type of cancer, resulting in weak generalization ability.\nMethods\nIn this paper, a deep-learning based model, named Unified Multi-modal Pan-cancer Survival Network (UMPSNet), is proposed. Specifically, to comprehensively understand the condition of patients, in addition to constructing encoders for histopathology images and genomic expression profiles respectively, UMPSNet further integrates four types of important meta data (demographic information, cancer type information, treatment protocols, and diagnosis results) into text templates, and then introduces a text encoder based on finetuned CLIP20 to extract textual features. In addition, the optimal transport (OT)-based attention mechanism is utilized to align and fuse features of different modalities. Furthermore, a guided soft mixture of experts (GMoE) mechanism is introduced to effectively address the issue of distribution differences among multiple cancer datasets. The code of UMPSNet is available at https://github.com/binging512/UMPSNet.\nFindings\nUMPSNet conducts joint training and validating for survival prediction with five public multi-type cancer datasets from The Cancer Genome Atlas (TCGA, which contains 2831 cases and a total of 3523 WSIs), including Bladder Urothelial Carcinoma (BLCA, 373 cases), Breast Invasive Carcinoma (BRCA, 956 cases), Glioblastoma & Lower Grade Glioma (GBMLGG, 569 cases), Lung Adenocarcinoma (LUAD, 453 cases), and Uterine Corpus Endometrial Carcinoma (UCEC, 480 cases). UMPSNet achieves an average concordance index (C-index) of 0.725 on the five datasets and outperforms all state-of-the-art (SOTA) methods. This result not only demonstrates the effectiveness of our UMPSNet, but also shows the feasibility and great potential of the new learning paradigm, that is, carrying out collective prognostic analysis for multiple diseases."}, {"title": "Interpretation", "content": "By incorporating the multi-modality of patient data and joint training, a deep-learning based approach, named UMPSNet is proposed to address the problem of pan-cancer prognosis. UMPSNet outperforms all SOTA approaches, and moreover, it demonstrates the effectiveness and generalization ability of the proposed learning paradigm of a single model for multiple cancer types. This advancement highlights the potential to enhance clinical decision-making and promote precision medicine. It also provides a new solution and a more comprehensive and effective tool for personalized patient care."}, {"title": "Keywords", "content": "Multi-modal; Pan-cancer prognosis; Deep learning; Predictive model;"}, {"title": "Introduction", "content": "Deep learning approaches have revolutionized numerous medical applications in contemporary healthcare systems1 23. Patient prognosis prediction, as one of the fundamental tasks in clinical medicine, has attracted substantial interest\u2074. By systematically evaluating disease progression and patient condition, healthcare providers can generate survival estimates, facilitating optimal treatment and personalized therapeutic interventions.\nExisting deep learning-based prognostic methods rely predominantly on histopathological WSIs, which represent the gold standard for cancer staging diagnosis and demonstrate strong correlation with patient survival outcomes5 6. However, constrained by the ultra-high resolution of WSI and Graphic Process Unit (GPU) memory, researchers are forced to adopt multi-instance learning (MIL) methods to train the network, where pre-trained models are employed to extract features from WSI patches for subsequent training. Nevertheless, the regions that are most critical to patient survival may occupy only a small portion of the WSI. Therefore, the genomic expression profiles are introduced to quantify the severity of cancer7,8,9. Additionally, survival outcomes are influenced by multiple factors beyond disease severity, patients' physical condition, psychological state, cancer type, and therapeutic interventions will all have an impact. However, these factors are overlooked by the existing methods, consequently, they can only achieve suboptimal performance.\nMoreover, different types of cancers have numerous differences10, such as the distribution of histopathological image features, genomic expression profiles, and survival times, etc. Consequently, existing approaches are restricted to training and evaluation on single-cancer datasets, which leads to low generalization ability.\nTherefore, in this paper, aiming to break through the limitation of the per-cancer-per-model paradigm, we propose UMPSNet, a new and unified multi-modal network for pan-cancer prognosis. Firstly, in order to enable UMPSNet to conduct learning as comprehensively as possible, six aspects of information about patients are collected and encoded: patients' histopathological WSIs, genomic expression profiles, demographic information, cancer type information, treatment protocols, and diagnosis results. Secondly, considering the differences in modalities and information, UMPSNet employs different feature representation strategies. For example, histopathological WSIs are split into patches with the same sizes and encoded via a pre-trained visual model, while genomic expression profiles are separated into groups according to their biological roles. Regarding the meta data, as they are usually discrete numerical values, specific text templates are firstly designed to incorporate them, and then a text encoder is utilized to extract the features. Thirdly, an OT-based attention mechanism is leveraged to align and fuse features of multiple modalities. Finally, a survival classifier is trained to predict the patient survival risk, while a cancer type classifier is also learned to predict the cancer type as an agent task. Moreover, inspired by the MoE11 12, we introduce a GMoE mechanism to fuse all features, which is capable of identifying common characteristics of different cancer types while adaptively extracting the features specific to each cancer type."}, {"title": "Methods", "content": "Problem formulation\nDue to ultra-high resolution of Whole Slide Images (WSIs), the common MIL scheme is adopted, and the multi-modal data is first preprocessed to form data bags applicable to model training.\nWSI bag formulation. Considering that the slides of patients contain a large amount of background, thereby a foreground segmentation algorithm is firstly employed, and then the foreground regions are partitioned into uniform-sized patches as the WSI data bag:\n$X_i^p = \\{x_{i,n}^p\\}_{n=1}^{N_{p,i}}$\nwhere $x_{i,n}^p$ is the nth patch for ith patient, and $N_{p,i}$ is the total number of patches that belong to the patient.\nGenomic bag formulation. Given that the genes have different functions, we categorize them into six groups: (i) tumor suppressor genes, (ii) oncogenes, (iii) protein kinases, (iv) cell differentiation markers, (v) transcription factors, and (vi) cytokines and growth factors. To normalize the genomic data, the z-scores relevant to diploid samples are calculated to represent the genomic data bag:\n$X_i^g = \\{x_{i,n}^g\\}_{n=1}^{N_g}$\nwhere $x_{i,n}^g$ stands for the nth genomic group of the ith patient, and $N_g = 6$ represents the number of the genomic groups.\nText bag formulation. Inspired by the large language models (LLM)13 14 and in order to handle complex discrete four types of text data\u00b9\u00b9, four text templates are designed to standardize each piece of the original text data. Specifically, for the patient demographic, including sex, age and race, a simple template is designed as: \u201c{He/She} is a {Age}-year-old {Race} race {Man/Woman}", "This is a patient who has {Cancer}.\", will be filled with the full name of the cancer. Given that the diagnostic information includes both the overall cancer staging and the TNM staging, the entire template is designed as follows: \u201c{He/She} has {Primary_diagnosis} at {Stage}. {T_stage}, {N_stage}, {M_stage}.\". In this template, \\\"Primary_diagnosis\\\" denotes the nature of the cancer, \\\"Stage\\\" represents the overall staging phase, while the subsequent \u201cT/N/M_stage": "omponents are filled with the corresponding text descriptions. Finally, as treatment protocols represent whether radiation or pharmaceutical therapy have been utilized, \u201c{Treatments} is applied.", "as": "n$X_i^t = \\{x_{i,dem}^t, x_{i,can}^t, x_{i,dia}^t, x_{i,tre}^t\\},$\nwhere $x_{i,dem}^t$, $x_{i,can}^t$, $x_{i,dia}^t$, and $x_{i,tre}^t$ represent the generated text for patient demographic, cancer type, diagnosis and treatment protocol, respectively."}, {"title": "Network", "content": "To conduct a comprehensive analysis of multi-modal data and accomplish the patient prognosis, UMPSNet is proposed. As shown in Fig.1, it consists of three components: feature extraction, feature interaction, and the classification.\nFeature extraction. To align and embed the three different modal data, three different encoders are utilized.\n1) WSI data bag. A pretrained image encoder CTransPath is leveraged to extract the patch features $f_i^p$,\n$f_i^p = \\{F^p(x_{i,n}^p)\\}_{n=1}^{N_{p,i}},$\nwhere $F^p(.)$ represents the pretrained patch feature extractor.\n2) Genomic data. Because there are differences and missing data in genomic data among cancer datasets, we apply zero-padding to fill in the missing data and generate a mask simultaneously. In the mask, the positions that are not zero-padded are designated as 1, while the others are set as 0. Then, Transformers15 with position embedding are utilized to extract the genomic features $f_i^g$, which can be written as,\n$f_i^g = \\{F^g(x_{i,n}^g + PE, M_{i,n}^g)\\}_{n=1}^{N_g},$\nwhere $F^g(.)$ is the Transformer encoder for the nth group of genomic data, $M_{i,n}^g$ stands for the corresponding generated attention mask, and $PE$ represents the position embedding.\n$PE_{(pos,2m)} = sin(\\frac{pos}{10000^{d_{model}}})$\n$PE_{(pos,2m+1)} = cos(\\frac{pos}{10000^{d_{model}}})$\nwhere $pos$ represents the position of the gene in the genomic data, $d_{model}$ is the dimension of the embedding features. Experiments show that the above-mentioned method effectively relieves the issue of missing data while avoiding the inaccuracies caused by zero-padding.\n3) Text data. An LLM is leveraged to extract the text features. Specifically, we finetune the CLIP20 with adapters to enable the LLM to retain its generalization capability while adapting to UMPSNet. The text features $f_i^t$ can be written as,\n$f_i^t = \\{F^t(x_{i,n}^t)\\}_{n=1}^{N_t}$\nwhere $F^t(.)$ is the adapter-embedded CLIP model, $N_t = 4$ is the number of text types.\nFeature Interaction. In order to alleviate the negative impacts of modal differences on model learning, we firstly align all features into a unified latent space by means of OT-based attention mechanism. Note that OT-based attention is applied between image features and text features, as well as between genomic features and text features, thus two optimal matching flows $A^x$ can be obtained.\n$A^x = F_{OT}(f_i^x, f_i^t),$\nwhere $F_{OT}(.)$ is the OT-based attention module, and $X$ can be either $p$ or $g$, representing the image branch or the genomic branch.\nSecondly, the features from different modalities can be aligned via the matrix multiplication with the matching flow matrix. OT-based attention not only aligns different features, but also reduces the dimension of features, especially that of image features. Afterwards, text-guided Transformer layers are adopted to extract patient-aware features from each modality. Specifically, considering that the text features contain a wealth of patient-specific information, they are treated as Query in the Transformer decoder structure, while the image or genomic features are regarded as Key and Value. Finally, the fused image features $f_i^p$, genomic feature $f_i^g$, and the original text features $f_i^t$ are fed into the classification module in a combined manner.\nClassification. In this module, GMoE mechanism is firstly proposed to re-fuse the features and predict hazard scores of the patient. In addition, to enhance the model's capacity to distinguish among various cancer types, we design an agent task as an auxiliary component to jointly train the network, that is, a dedicated classifier is trained to predict the possibility of the cancer types. Note that text features are not utilized in the agent task, since they inherently contain information about cancer types."}, {"title": "GMoE architecture", "content": "As an expansible soft MoE, the GMoE is constructed to reduce the data distribution discrepancies within pan-cancer datasets. It consists of multiple expert modules, as shown in Fig. 2(a), each one has an identical structure, yet their parameters are not shared.\nThe architecture of each expert is shown in Fig. 2(b), where two Transformer decoder layers are adopted to fuse the features. Additionally, we utilize the text features as queries to guide the module to obtain the embedding related to patient survival from the image and genomic features. Finally, the embedding is fed into a survival classifier to predict the survival scores. Based on our GMoE architecture, the features of different cancer types will activate different expert modules, facilitating predictions as well as providing reasonable explanations.\nThe above prediction process can be formulated as follows as\n$S_{haz,i} = \\{F^e(f_i^p, f_i^g, f_i^t)\\}_{n=1}^{N_e} \\times F_{cw}(x_{ican}^t, x_{idia}^t)$\nwhere $S_{haz,i}$ represents the predicted hazard score of the ith patient, $F^e(.)$ is the nth expert in GMoE architecture, while $N_e$ stands for the total number of experts, $F_{cw}(\u00b7)$ denotes the linear layer for generating weights, and $x_{ican}^t$ and $x_{idia}^t$ are the cancer type and diagnosis of the patient, respectively.\nFinally, the model obtains the survival prediction results by performing a weighted summation of the outputs from all experts. The weights are generated based on the patient's cancer type and diagnostic results, thus enhancing the model's ability to distinguish among different cancer types."}, {"title": "Loss functions", "content": "To train the whole model, two loss functions are considered. Firstly, the negative log-likelihood loss ($L_{nll}$) is introduced to supervise the survival prediction task,\n$L_{nll} (S_{haz,i}, C_i, y_i) = -c_i \\cdot log (S_{surv,i}(y_i)) \u2013 (1 \u2013 c_i) \\cdot log (S_{surv,i}(y_i \u2013 1)) \u2013 (1 \u2013 c_i) \\cdot (S_{haz,i} (y_i)),$\nwhere $c_i$ denotes the censorship of the i th patient, $S_{haz,i}(y_i)$ stands for the hazard score at the $y_i$ time bin. $S_{surv,i}(y_i)$represents the cumulative survival probability at the true survival time bin $y_i$ and can be expressed as,\n$S_{surv,i}(y_i) = \\prod_{n=0}^{y_i} (1 \u2013 S_{haz,i,n}),$\nwhere $S_{haz,i,n}$ is the predicted hazard score for the ith patient at nth time bin.\nSecondly, the cross-entropy loss ($L_{ce}$) is adopted to supervise the classification task for five cancer types.\nOverall, the total loss function can be written as,\n$L_{total} = L_{ce}(\\hat{y}_{can,i}, y_{can,i}) + L_{nll}(S_{haz,i}, C_i, y_{surv,i}),$\nwhere $L_{nll}(.)$ and $L_{ce}(.)$ represent the NLLLoss and CELoss, respectively, $Y_{surv,i}$ stands for the ground truth of the patient survival bin, while $\\hat{y}_{can,i}$ and $y_{can,i}$ denote the predicted score and the actual cancer type of the patient, respectively."}, {"title": "Results", "content": "Datasets and evaluation metrics\nTo evaluate the performance of UMPSNet, five public cancer datasets in TCGA including BLCA, BRCA, GBMLGG, LUAD, UCEC, are leveraged, and their statistical information is shown in Table 1. TCGA is a large-scale, multi-institutional research initiative jointly launched by the National Cancer Institute (NCI) and the National Human Genome Research Institute (NHGRI) in the United States. This project involves the systematic collection of extensive data from numerous leading cancer research institutions and hospitals nationwide. To evaluate the model performance on pan-cancer prognosis, these five datasets are merged as a joint training dataset. Following MOTCat, a five-fold validation process is adopted. The concordance index"}, {"title": "Implementation details", "content": "In the process of WSI data bag formulation, OTSU algorithm17 is applied to segment the tissue regions, then the regions are split into non-overlapping patches, with a resolution of 256\u00d7256 pixels under a 20\u00d7 magnification18 . As for the image encoder, self-trained CTransPath19 is adopted to extract the WSI patch features for training. Note that, CTransPath specifically leverages histopathological images in the self-training stage, thus can extract more discriminative deep features. In the process of genomic data bag formulation, with the attention mask, Transformer15 is employed to encode each group of genomic data into genomic features. For the text bag, four types of text prompts are encoded via CLIP20 with adapter"}, {"title": "Experimental results", "content": "Comparison with the SOTA methods. Following the dataset partition in MOTCat , we conduct 5-fold validation experiments on the joint dataset. As shown in Table 2, comparing with existing approaches, UMPSNet not only achieves new state-of-the-art performance among the joint training methods, but also achieves competitive performance against the separately training approaches."}, {"title": "The effectiveness of each module", "content": "To further explore the effectiveness of each module within UMPSNet, the ablation experiments are conducted. As illustrated in Table 3, the performance of UMPSNet progressively improves with the incremental incorporation of various modules into the network."}, {"title": "The number of experts", "content": "To determine the optimal number of experts, we also carry out a series of experiments. As shown in Table 4, UMPSNet shows the best performance when Ne is set to 10. It demonstrates that the insufficient experts cannot meet the requirements for distinguishing multiple cancers, and meanwhile, an excessive number of experts will lead to difficulties in training optimization."}, {"title": "Interpretability", "content": "To demonstrate the interpretability of UMPSNet, GradCAM31 is utilized to generate class activation maps (CAMs) for"}, {"title": "Discussion", "content": "Prognosis aims to predict the survival risk of the patients so as to optimize the treatment plans and the resource allocation etc. Thus, a number of prognosis approaches are proposed. As the gold standard for cancer diagnosis, histopathological WSIs are widely utilized in this task. DeepAttnMISL25 proposes siamese MI-FCN to learn features from different phenotype cluster and introduces attention-based MIL pooling to perform a trainable weighted aggregation. To build hierarchical representations of morphological image patch features, Patch-GCN32 treats WSIs as 2D point clouds and formulates a graph-based structure, then leverages graph convolutional network (GCN) to aggregate the information from the patches. Surformer27 introduces a RRCA module to simultaneously detect global features and multiple pattern-specific local features, and proposes a disentangling loss constraining the local features to focus on distinct patterns. To enhance the model performance, LNPL-MIL33 and HistMIMT34 adopt agent tasks to train the model, such as cancer diagnosis, genomic expression prediction, etc. As another key characteristic of cancer, differences in genomic expression can also be leveraged to assess cancer progression. Therefore, SNN designs a module to better accommodate the characteristics of genomic data. To obtain a more comprehensive analysis for patients, several approaches explore to use multi-modal data for joint prediction of patient survival risk. Among these, the main-stream approaches involve the joint analysis of WSIs and genomic data. Pathomic35 takes WSI patches, cell spatial graph, and genomic profiles into account, and fuses the features for survival prediction. However, the fusion method lacks intermediate feature integration. Therefore, MCAT proposes a genomic-guided co-attention module, which is capable of integrating both WSI and genomic features. In addition, MOTCat embeds OT algorithm into the attention module and achieves impressive results. Moreover, to analyze the influence of individual genes on patient survival, Survpath30 introduces a transcriptomics tokenizer to generate biological pathway tokens, which leverage the existing knowledge of cellular biology. Meanwhile, a multi-level interpretability framework is proposed to enable deriving unimodal and cross-modal insights about the prediction for prognosis. To reduce the reliance on genomic data during testing while maintaining high performance, G-HANet28 trains a network using knowledge distillation technique via reconstructing genomic expression profiles from histopathological image features.\nAlthough the existing methods have achieved impressive results in prognosis task, the factors they take into account are still limited. Prognosis task differs from diagnostic task. Multiple factors, including patient's individual condition, psychological state, treatment protocols, and the healthcare environment etc., can all affect survival outcomes. Therefore, UMPSNet aims to learn more valuable features from WSIs, genomic data, and patient' textual information. Additionally, we observe that existing methods usually train and test only on individual datasets, which makes them unsuitable for pan-cancer survival prediction. Meanwhile, recent studies indicate that there are certain commonalities in the survival threats of different cancers. Consequently, UMPSNet attempts to combine multiple cancer types for training to enhance the network's ability to learn their commonalities. In the meantime, UMPSNet will activate different modules within the GMoE mechanism according to the specific cancer type, thus preserving the uniqueness of each cancer."}, {"title": "Conclusion", "content": "In this study, we introduce UMPSNet, a new multi-modal learning network for pan-cancer prognosis. Specifically, UMPSNet utilizes six kinds of patient data, including histopathological WSI, genomic expression profile, demographic information, cancer type information, treatment protocols, and diagnosis results. Three different encoders are leveraged to extract the features from different modalities, and OT-based attention is applied for feature alignment and interaction. To enhance the model's ability to distinguish between different cancer types, we propose the GMoE mechanism, which utilizes meta data to guide and fuse features. Moreover, a cancer type classifier is also learned as an agent task to improve the model"}, {"title": "Contributors", "content": "Binyu Zhang (Conceptualization: Equal; Formal analysis: Lead; Investigation: Equal; Methodology: Lead; Validation: Lead; Visualization: Equal; Writing-original draft: Lead), Shichao Li (Investigation: Equal; Methodology: Equal; Validation: Equal; Visualization: Lead) and Junpeng Jian (Methodology: Equal), Zhu Meng (Conceptualization: Lead, Investigation: Lead), Limei Guo (Conceptualization: Equal; Supervision: Equal) and Zhicheng Zhao (Project administration: Lead, Supervision: Lead, Writing-review & editing: Lead). All authors have full access to all the data, and have final responsibility for the decision to submit for publication."}]}