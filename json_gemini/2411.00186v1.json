{"title": "Self-Healing Machine Learning: A Framework for Autonomous Adaptation in Real-World Environments", "authors": ["Paulius Rauba", "Krzysztof Kacprzyk", "Nabeel Seedat", "Mihaela van der Schaar"], "abstract": "Real-world machine learning systems often encounter model performance degradation due to distributional shifts in the underlying data generating process (DGP). Existing approaches to addressing shifts, such as concept drift adaptation, are limited by their reason-agnostic nature. By choosing from a pre-defined set of actions, such methods implicitly assume that the causes of model degradation are irrelevant to what actions should be taken, limiting their ability to select appropriate adaptations. In this paper, we propose an alternative paradigm to overcome these limitations, called self-healing machine learning (SHML). Contrary to previous approaches, SHML autonomously diagnoses the reason for degradation and proposes diagnosis-based corrective actions. We formalize SHML as an optimization problem over a space of adaptation actions to minimize the expected risk under the shifted DGP. We introduce a theoretical framework for self-healing systems and build an agentic self-healing solution H-LLM which uses large language models to perform self-diagnosis by reasoning about the structure underlying the DGP, and self-adaptation by proposing and evaluating corrective actions. Empirically, we analyze different components of H-LLM to understand why and when it works, demonstrating the potential of self-healing ML.", "sections": [{"title": "1 Introduction", "content": "Consider the following scenario: You are tasked with monitoring the performance of a black-box model f deployed in production. After some time, you notice that the predictive performance of f has started to degrade. What would be the appropriate action a you should take to ensure that the model's performance returns to its prior performance levels: a\u2081: re-train the model on a subset of the data; a2: change the type of the model used; a3: remove discovered corrupted values; a4: add new covariates?\nClearly, the answer to this question is \u201cit depends\u201d. Different actions might result in different behavior of the model over time. If we could pinpoint why the performance of the model has degraded, it could help us understand what actions are most promising, since we could select an action which would directly address the root cause of the problem."}, {"title": "2 Related work", "content": "SHML is most closely related to concept drift adaptation or specialized drift handling methods. We provide an extended discussion on related work within each component in Appendix A.\nConcept drift adaptation. The field of concept drift adaptation focuses on developing algorithms to maintain the performance of machine learning models in changing environments. Such algorithms are predominantly proposed within the setting of tabular data. Most common adaptation techniques are re-training models on new data [1\u20135, 21-24], re-using stored models [1, 6\u20139] or obtaining new data altogether [25, 26]. These approaches can be implicit, like continuous retraining, or explicit, based on drift detection in data or model error [5, 23, 27]. Because these approaches do not explicitly"}, {"title": "3 Self-healing Machine Learning", "content": "This section introduces self-healing machine learning and its four components. We present the problem setting (Sec. 3.1), explain current limitations (Sec. 3.2), and outline the four stages of SHML (Sec. 3.3). NOTE: Table 1 serves as a guide for navigating the paper.\n3.1 Model degradation over time\nPreliminaries. Let X and Y denote the input and output spaces, respectively, and let Pt denote the data distribution over X \u00d7 Y at time step t\u2208 [T]. At each t, we observe a batch of data Dt = {(x(i),y(i))}ntYt =1 ~ Prt, where nt = 1 in the streaming setting and nt > 1 in the batch setting. We will drop the superscripts where clear from context.\nThe goal is to learn a sequence of functions {ft \u2208 F}Tt=1 that minimize the cumulative risk:\nR(f1,..., fT) := \\sum_{t=1}^{T}E_{(x,y)~P_t}[l(f_t(x),y)]\n(1)\nwhere F is a function class and l : Y \u00d7 Y \u2192 R\u22650 is a loss function.\nIn the time-invariant setting where Pt = P \u2200t \u2208 [T], the goal reduces to learning a single function f* e F that minimizes the risk R(f) = Ep[l(f(x), y)]. When P is unknown, f* is often approximated by minimizing the empirical risk on a training set {(x(i),y(i))}n\u1d62=1 ~ Pn. However, when Pt evolves over time, the optimal predictor ft e arg min feF Ep\u209c[l(f(x), y)] changes across time steps\u00b9. Failing to adapt ft in this time-varying setting leads to model degradation, as the learned function becomes increasingly suboptimal w.r.t. the current data distribution.\n3.2 Limitations of existing approaches in adapting to changing environments\nMaintaining stable model performance in the presence of a changing environment poses unique challenges. As the optimal predictor f evolves over time, the estimated predictor should also adapt. Ideally, we could obtain a large batch of data Dt+1 = {(x(i)(i),y(i))nt+1}+1 and minimize the empirical risk over this dataset. However, this is often impractical due to constraints such as (i) ground-truth labels not being immediately available [38]; (ii) the streaming setting, where each new batch contains only one data point [39]; (iii) gradual shifts, where past data remains relevant [40]; or (iv) the presence of corrupted data in new batches [41]."}, {"title": "3.3 The four stages of self-healing machine learning", "content": "Self-healing machine learning is a framework for autonomously detecting, diagnosing, and correcting performance degradation in deployed ML models. It aims to maintain model performance in changing environments without constant human intervention. The motto of self-healing ML is \"understanding your problem is half the solution\" (and the most important half). A SHML system is defined by a tuple (H, f), where f : X \u2192 Y is the deployed machine learning model we aim to heal (i.e., the function that makes predictions on input data), and H is a healing mechanism that interacts with the environment and acts upon the model f by proposing and implementing actions, such as selecting when to retrain a model, what data to use or how to change the input data before making predictions. Thus, H can modulate the behavior of the deployed model f.\nSelf-Healing Machine Learning in a nutshell.\nSelf-healing ML contains four components: monitoring, diagnosis, adaptation, and testing. After these steps, the best action is implemented on the ML model, illustrated in Fig. 2.\nI. Monitoring. The first step is the detection of degradation, potentially due to a shift in the data distribution. We formalize this as a monitoring component HM that takes as input the sequence of data batches {Di}ti=1, up to time t, and outputs st \u2208 [0, 1], indicating the likelihood of model degradation. Formally,\n\u041d\u043c : (X \u00d7 Y)* \u2192 [0,1],\n(4)\nwhere higher values of st indicate a greater likelihood of a shift.\nII. Diagnosis. The diagnosis component HD detects the reason of degradation. It takes data batches {Di}ti=1, up to time t, along with any available contextual information c\u2208C (e.g. background knowledge), and outputs a distribution \u03b6\u03b5\u0394(Z) over a space of possible reasons Z:\nHD : (X \u00d7 Y \u00d7 C)* \u2192 \u2206(Z).\n(5)\nZ represents the finite space of possible reasons of the shift and is a stochastic vector.\nIII. Adaptation. The adaptation component is a policy that outputs a distribution over actions. Given a diagnosis vector \u03da, actions a \u2208 A are selected from a finite space A by:"}, {"title": "4 An analysis of the properties of self-healing diagnosis", "content": "Self-healing systems have the unique property of having a diagnosis stage. But what constitutes a good diagnosis? In this section, we analyze the properties of self-healing diagnosis and establish its connection to the performance of adaptation actions.\nTo effectively use diagnosis information to guide the search for adaptation actions, we require a way to quantify the usefulness of a diagnosis. We propose three desirable properties for such a measure: (i) concentration: it should favor diagnoses that provide more information, i.e. assign higher probabilities to fewer possible reasons; (ii) sensitivity: it should be sensitive to changes in the diagnosis distribution, such that small changes in probabilities would result in small changes in the measure; (iii) maximum uncertainty: it should reach its maximum value when the diagnosis distribution is uniform, indicating no knowledge about the reason for degradation. Therefore, we propose using the entropy of the diagnosis vector as a useful proxy for quality which satisfies all three properties. Because entropy measures uncertainty, we refer to this as the certainty of the diagnosis.\nDefinition 1 (Certainty of the Diagnosis). Let Z be the finite space of possible reasons for degradation and A(Z) be the diagnosis space. The certainty of a diagnosis \u03b6 \u2208 \u2206(Z) in a self-healing machine learning system is measured by its entropy H(\u03da), defined as:\nH(\u03b6) = \u2212 \\sum_{z \\in Z} \u03b6(z) log (\u03b6(z),\n(9)\nwhere ((z) is the probability of reason z under the distribution \u03b6.\nThe link between diagnosis quality and adaptation performance highlights the importance of obtaining informative diagnoses in SHML. Building upon these concepts, we define the optimal diagnosis:\nDefinition 2 (Optimal Diagnosis). The optimal diagnosis * is defined as:\n(* = arg min Ea~\u03c0(\u00b7|\u03c2) [R(a)]\n(10)\nwhere A(Z) is the diagnosis space, \u03c0(\u00b7|\u03b6) is the conditional distribution over actions induced by diagnosis vector \u03da, and R(a) denotes the risk of fet associated with action a \u2208 A.\nThis formalizes the intuition that the best diagnosis is the one that leads to the best adaptation actions, on average. To characterize the properties of this optimal diagnosis, we introduce an assumption about the structure of the adaptation policy:\nAssumption 1 (Independent actions). We assume that \u03c0(\u00b7|\u03da) has a hierarchical structure. First, a reason z \u2208 Z is sampled according to the diagnosis \u03da: z ~ \u03da. Then, an action is sampled conditioned on this reason, a ~ \u03c0(\u00b7|z\u2020), where z\u2020 \u0454 \u2206(Z) such that z\u2020(z) = 1. \u03c0(\u00b7|\u03da) can then be described as the following mixture.\n\u03c0(\u03b1\u03c2) = \\sum_{zEZ}\u03c0(a|z\u2020)(z)\n(11)\n\u03da are the mixture weights and {\u03c0(\u00b7|z\u2020) | z \u2208 Z} are the mixture components.\nUnder this hierarchical structure, we can prove a useful property of the optimal diagnosis:\nProposition 1. Under Assumption 1, the optimal diagnosis \u03da* has a zero entropy, i.e., H(\u03da*) = 0.\nProposition 2 (Existence of Optimal Diagnosis). Suppose that the action space A is a compact subspace of R\" and R is continuous. Then there exists at least one optimal diagnosis (*."}, {"title": "5 Building self-healing systems: H-LLM", "content": "This section outlines the challenges of building SHML systems (Sec. 5.1), describes how LLMs can address these challenges (Sec. 5.2), and introduces the first self-healing system, H-LLM (Sec. 5.3).\n5.1 Unique challenges of building self-healing systems\nImplementing SHML systems (Sec. 3.3) poses unique challenges in diagnosis and adaptation.\nChallenges in diagnosis. Discovering the reasons for model degradation poses significant practical challenges because: (i) the space of possible reasons Z is often poorly defined or intractable to"}, {"title": "5.2 Language models to empower self-healing", "content": "We posit that LLMs have the potential to satisfy many of the required properties of self-healing components because of the following capabilities: (i) Hypothesis proposers. LLMs are known to be \"phenomenal hypotheses proposers\" [43] which are required to hypothesizing diagnoses of ML model performance degradation; (ii) Contextual understanding. LLMs have been pretrained with a vast corpus of information and hence have extensive prior knowledge around different contexts and settings [44, 45]; (iii) Language model agents. Language models can work as agents within a larger system [46, 47] which is required to actively interact with a deployed model, trigger and implement changes. We therefore see LLMs as capable proxies for different self-healing components."}, {"title": "5.3 Design of H-LLM", "content": "We instantiate the healing mechanism H with an LLM l, using its useful properties (Sec. 5.2) to address the practical challenges of designing SHML systems (Sec. 5.1).\nH-LLM in a nutshell.\nH-LLM is the first SHML algorithm that modulates the behavior of f following Fig. 3.\nI. Monitoring. We use statistical drift detection algorithms to monitor model degradation from k previous time points [29, 39, 48]. Diagnosis is triggered if a shift is detected.\nII. Diagnosis. Upon detection, we use a pre-defined prompt template to obtain information about the dataset before and after the diagnosis. The prompt template gives us numerical insights into how the dataset has changed and includes covariate information before and after the shift, together with other numerical details. We denote this prompt as an extractor function E : D* \u2192 De to obtain an information vector v. Using v and a chain-of-thought (CoT) module with self-reflection, H-LLM generates k candidate reasons for degradation {zi}ki=1 ~ 1(v) via Monte Carlo (MC) sampling with associated confidence scores. As before, this is obtained by following pre-defined prompt templates conditioned on the obtained information (e.g. \"Suggest {self.n} possible reasons why the model might have failed on the basis of the issues presented\"). These candidates form an empirical diagnosis vector \u015c, approximating the optimal diagnosis (*.\nIII. Adaptation. Conditioned on the empirical diagnosis distribution \u015c, H-LLM generates m candidate adaptation actions {aj}mj=1 ~ 1(\u00b7|\u011c) via CoT-based MC sampling. This approximates sampling from \u03c0(\u03b6*) (Def. 6). The actions sampled from I are textual representations, so we use an interpreter function to execute each a on f.\nIV. Testing. The sampled actions are evaluated on an empirical dataset (Def. 7), and the empirically optimal action \u00e2* = arg minje[m] R(a) is implemented. Limited access to the shifted DGP complicates evaluating R(a), but it can be approximated with empirical data Dtest by using a backtesting window, continuously incoming data, or historical data (Appendix B.4).\nGoal. This procedure aims to approximate the optimal action (Def. 8). These actions are orchestrated by an orchestrator component in H-LLM which can navigate between these steps."}, {"title": "6 Experimental viability studies", "content": "The previous sections constituted the primary contribution of our paper-establishing SHML as a framework. The goal of this section is to provide a viability study by analyzing different components of SHML. We conduct six viability studies.\u00b2\nExperimental setup. We desire to meet two properties: (i) have full control of the DGP to vary experimental parameters; and (ii) we need to benchmark against existing adaptation methods (Sec. 2) which are predominantly tabular-based. Therefore, we simulate a diabetes prediction task [49\u2013 51] based on the setup in Sec. 3.1. We predict diabetes Yt \u2208 {0,1} at each time point t for a set of n observations, generated according to a (changing) pre-specified DGP log (P(X=1)) = at + \u03a3\u03ba\u03b5\u03ba \u03b2t,kXt,k + \u20act, where K includes relevant parameters such as Age or BMI, \u1e9et,k are time-varying covariates and et ~ N(0, \u03c32) is a noise component. For evaluating H-LLM actions, we use a backtesting window-a representative sample of the shifted distribution obtained after detecting the change but before deploying the adapted model (Sec. B.4). Details provided in Appendix C.\n6.1 Viability study I: Adaptation in the presence of model degradation\nSetup. We aim to empirically demonstrate the limitations of existing approaches in adapting to changing environments (Sec. 3.2). We benchmark H-LLM against four common drift adaptation methods: (i) new model retraining on post-drift data, (ii) partially updating models with new data; (iii) Ensemble methods by re-using old models, and (iv) No retraining of the models [1]. At time t, we introduce a sudden, single intervention by changing the DGP parameters and corrupting a percentage 7 of k columns.\nDiscussion. The performance of f degrades if the corrupted columns are not handled appropriately, such as removing or inputting the corrupted data. Defaulting to standard techniques of adapting to a changed environment results in poor performance. H-LLM diagnoses issues by observing that some values have drifted too much from their original values and the DGP has changed. One of the proposed adaptation strategies is to remove samples which were estimated to be corrupted, and re-training the model on the remainder of the data. This results in superior performance.\n6.2 Viability study II: Adaptation across datasets\nSetup. We aim to empirically analyze whether SHML can provide benefits across different datasets. We cover five different datasets: Airlines [52], Poker [53], Weather [54], Electricity [55], Forest Type"}, {"title": "6.3 Viability study III: Monitoring", "content": "Setup. We use the same setup as experiment I and vary the drift detection threshold which influences the sensitivity of a detection system to changes in the DGP. Low values mean high sensitivity, and high values mean low sensitivity [57]. We measure average recovery time for H-LLM to return recover from degradation and post-intervention accuracy, H-LLMs average performance after intervention.\nDiscussion. Intuitively, one might expect earlier drift detection (lower threshold) to consistently yield faster recovery and higher accuracy. In reality, concept drift algorithms often struggle with false positives which can result in worse model performance because of unnecessary re-training [5, 58]. Self-healing ML exhibits greater robustness to these false positives, as any action will be implemented only if it outperforms doing nothing. This contrasts with traditional systems which would automat- ically trigger the selected action."}, {"title": "6.4 Viability study IV: Diagnosis", "content": "Setup. We evaluate how well self-healing systems identify the root causes of problems. We corrupt a proportion of observations (corruption coefficient) by multiplying their values by a factor (outlier factor) and see if the H-LLM detects issues related to these factors. We output a probability distribution over diagnoses of which variable is corrupted. Knowing the true corrupted variable, we measure the difference between the distributions using KL- Divergence, with lower values indicating closer matches to true corruption. A uniform diagnosis baseline represents random guessing.\nDiscussion. As the outlier factor and corruption coefficient increase, making data issues more apparent, H-LLM assigns higher probabilities to the corrupted variables. Thus, the diagnosis accuracy improves as the problem becomes more evident."}, {"title": "6.5 Viability study V: Adaptation", "content": "Setup. We study the sensitivity of SHML adaptation actions by examining how well actions perform based on (i) the number of corrupted values and (ii) the size of the backtesting dataset.\nDiscussion. As more values are corrupted, adaptation actions become more concentrated and less effective. With a larger backtesting dataset, actions are more spread out. This suggests (i) action evaluation is more reliable with non-corrupted data and (ii) larger backtesting windows help in selecting better adaptations."}, {"title": "6.6 Viability study VI: Testing", "content": "Setup. We study the importance of the testing component (Eq. 7) by evaluating H-LLM suggested actions with and without the testing phase (backtesting window) and comparing their accuracies.\nDiscussion. Having a dataset to evaluate actions significantly improves the self-healing process."}, {"title": "6.7 Other studies", "content": "We provide further experiments in Appendix D. Our framework shows strong performance with lower warm-start parameters and increasing benefit as data degradation becomes more severe (Sec. D.4). A component-wise ablation analysis (Sec. D.6) reveals each stage of SHML is essential. Extended benchmarks (Sec. D.5) and model agnostic evaluations (Sec. D.7) demonstrate consistent improvements across different adaptation approaches and ML architectures."}, {"title": "7 Discussion", "content": "Algorithms hold significant decision-making power in high-stakes applications, yet little has been done to ensure their optimal performance. This work presents a major leap towards that goal. By enabling systems to autonomously diagnose and adapt to new environments, we aim to create a wave of self-healing systems beneficial to both the ML community and society. Our theoretical framework (Sec. 3.3, 4) builds the foundation for the development of self-healing theory, such as optimal adaptation or diagnosis methods, and our viability study shows the potential benefits of SHML. Our largest contribution is formalizing this field\u2014we hope to spur new theoretical developments and encourage the adoption of such systems in critical domains like medicine [13\u201318] and finance [20].\nLimitations. SHML's success relies on accurate root cause identification and finding effective adaptation policies which could pose challenges in some complex, real-world settings (Sec. 5.1). Furthermore, the prioritization of adaptation strategies is also not trivial. Currently, H-LLM primarily looks for subgroup-level issues. We see future work tackling all areas of self-healing ML: finding better diagnosis strategies, improving adaptation selection, and enabling better testing of actions in the presence of changing environments.\nBroader impact. Because SHML could empower many positive technologies, it could also be misused to amplify the impact of more problematic systems, such as surveillance technologies."}]}