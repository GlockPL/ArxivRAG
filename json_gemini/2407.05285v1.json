{"title": "Gradient Diffusion: A Perturbation-Resilient Gradient Leakage Attack", "authors": ["Xuan Liu", "Siqi Cai", "Qihua Zhou", "Song Guo", "Ruibin Li", "Kaiwei Lin"], "abstract": "Recent years have witnessed the vulnerability of Federated Learning (FL) against\ngradient leakage attacks, where the private training data can be recovered from the\nexchanged gradients, making gradient protection a critical issue for the FL training\nprocess. Existing solutions often resort to perturbation-based mechanisms, such\nas differential privacy, where each participating client injects a specific amount\nof noise into local gradients before aggregating to the server, and the global\ndistribution variation finally conceals the gradient privacy. However, perturbation\nis not always the panacea for gradient protection since the robustness heavily\nrelies on the injected noise. This intuition raises an interesting question: is it\npossible to deactivate existing protection mechanisms by removing the perturbation\ninside the gradients? In this paper, we present the answer: yes and propose\nthe Perturbation-resilient Gradient Leakage Attack (PGLA), the first attempt to\nrecover the perturbed gradients, without additional access to the original model\nstructure or third-party data. Specifically, we leverage the inherent diffusion\nproperty of gradient perturbation protection and construct a novel diffusion-based\ndenoising model to implement PGLA. Our insight is that capturing the disturbance\nlevel of perturbation during the diffusion reverse process can release the gradient\ndenoising capability, which promotes the diffusion model to generate approximate\ngradients as the original clean version through adaptive sampling steps. Extensive\nexperiments demonstrate that PGLA effectively recovers the protected gradients\nand exposes the FL training process to the threat of gradient leakage, achieving the\nbest quality in gradient denoising and data recovery compared to existing models.\nWe hope to arouse public attention on PGLA and its defense.", "sections": [{"title": "1 Introduction", "content": "With the latest success in real-life applications of various data-based generative models, the privacy\nand information security of these models are under increasing scrutiny[2]. Federated Learning (FL), as a newly proposed privacy preservation distributed machine learning paradigm, collaboratively\ntrains a global model on the parameter server by aggregating computed updates from local clients [18], is widely used in various training scenarios[19, 21, 27], for example, Gboard and Medical Diagnosis\nSystem, where data security and privacy are highly significant[30]. Specifically, FL protects users'\nprivacy by sharing only the model parameters or gradients but not the raw data [15]. Existing FL\nprotocol designs have been demonstrated to be vulnerable to adversaries both inside and outside\nthe system, jeopardizing data privacy and system robustness [17]. Among these, gradient leakage\nattacks have shown serious privacy hazards [10, 16, 35]. To better protect the private information of\nthe clients from malicious attackers during the training and inference periods, protection of shared\ngradients becomes an almost essential step [23]. Among existing gradient protection methods,\ngradient perturbation, as an extensively used privacy-preserving method, has already been frequently\napplied in FL scenarios efficiently [20] (schematic diagram shown in Fig. 1).\nConsidering that gradient data is the result of linear transformations applied to the original data, such\nas image data, we posit that if the original data is stable and predictable, then the corresponding\ngradient data will also be stable and predictable. Moreover, we discovered that the essence of gradient\nperturbation protection is akin to the diffusion process applied to inherently structurally stable gradient\ndata. Based on these two points, we believe that compared to other models, diffusion models [12]\nhave a natural applicability for gradient perturbation denoising. In this paper, we reveal the natural\ndiffusion properties of gradient perturbation protection and propose PGLA: Perturbation-resilient\nGradient Leakage Attack, the first general gradient diffusion attack that is capable of rendering\nvarious kinds of perturbations on gradients (e.g. differential privacy [26], certain layer representation\nperturbation [25], dynamic perturbation [29]) nearly invalid. Based on the structure of obtaining\nprotected shared gradients, our approach involves constructing a surrogate downstream task model\nfor the target attack model. Without access to the original model structure or third-party datasets, we\nutilize the surrogate gradient data supply model to provide training data for our Gradient Diffusion\nModel. The trained Gradient Diffusion Model enables us to approximate the original shared gradients\nof the target clients from the perturbed privacy-preserving gradients obtained by malicious attackers.\nPGLA embeds the perturbation protection mechanism into the reverse diffusion process and regulates\nprivacy-preserving gradients denoising node as well as the diffusion forward and reverse time steps\nwith the perturbation level $M$ as an adaptable parameter. In summary, the key contributions of\nour work are:\n\u2022 We disclose the natural diffusion process in general gradient perturbation mechanisms by introducing perturbation adaptive parameter $M$.\n\u2022 We propose PGLA: Perturbation-resilient Gradient Leakage Attack, which successfully breaks the bottleneck that existing gradient leakage attacks cannot effectively leak privacy under gradient perturbation protection.\n\u2022 We present and demonstrate the vulnerability of gradient perturbation protection under diffusion denoising and the suitability of the diffusion model on gradient morphology data.\n\u2022 We design the first practical gradient diffusion attack strategy (including surrogate model construction, adjusting gradients to build diffusion training dataset, etc.), and conduct experiments under general perturbation protection FL system. Results show that PGLA achieves the best gradients denoising quality and privacy leakage ability on commonly used image datasets."}, {"title": "2 Preliminary", "content": "Federated Learning (FL) provides comparative privacy guarantees for model training which does not\nrequire the original data and the participating clients are free to enter or leave [17]. Considering a\ngeneral FL system with homogeneous architectures. Regarding the gradient protection issue in the\nFL training process, gradient perturbation is one of the most widely used mechanisms. Here, we\npresent the key preliminary definitions of gradient perturbation and gradient attacks that are pertinent\nto our work.\n2.1 Differential Privacy (DP)\nGenerally, DP is defined based on the concept of adjacent database and has been applied in various\npractical areas in Artificial Intelligence to protect privacy information through adding specific\nperturbation, e.g. Google's RAPPORT [8] and large-scale graph data publishing [4]. Tracing back to\nthe definition of Differential Privacy, in this paper we discuss both $\\varepsilon$ DP and $(\\varepsilon, \\delta)$ \u2013 DP [5, 6, 1]\nas our sample attack background. Here, we present two core definitions first.\nDefinition 1. A randomized mechanism $M: D \\rightarrow R$ with domain $D$ and range $R$ satisfies $\\varepsilon$ -\ndifferential privacy if for any two adjacent inputs $d, d' \\in D$ and for any subset of outputs $S \\subseteq R$, and\nit holds that:\n$\\Pr[M(d) \\in S] \\le e^{\\varepsilon} \\Pr[M(d') \\in S].$\n(1)\nDefinition 2. A randomized mechanism $M: D \\rightarrow R$ with domain $D$ and range $R$ satisfies $(\\varepsilon, \\delta)$ -\ndifferential privacy if for any two adjacent inputs $d, d' \\in D$ and for any subset of outputs $S \\subseteq R$, and\nit holds that:\n$\\Pr[M(d) \\in S] \\le e^{\\varepsilon} \\Pr[M(d') \\in S] + \\delta.$\n(2)\n$\\delta$-approximation is preferably smaller than $1/|d|$. Note that we usually apply Laplace perturbation for\nthe definition in Eq. (1). However, as to the definition in Eq. (2), the noise perturbation mechanism\nneeds to be Gaussian Mechanisms, which can adapt both $\\varepsilon$ and $\\delta$. According to Theorem A.1 in\nGaussian Mechanisms [7] to ensure the given Gaussian noise distribution $N(0, \\sigma_{dp}^2)$ well preserves\n$(\\varepsilon, \\delta)$ \u2013 DP, the noise scale should satisfy :\n$\\sigma_{dp} \\ge C_{dp} \\nabla_s /\\varepsilon, \\varepsilon \\in (0,1),$\n(3)\n$C_{dp} \\ge \\sqrt{2 \\ln(1.25/\\delta)},$\n(4)\nwhere $V_s = \\max_{D,D'}||S(D) - S(D')||$ and $s$ represents the real-value function. $C_{dp}$ can be\nconsidered as a hyperparameter used to define the DP boundary."}, {"title": "2.2 FL with Differential Privacy (FL-DP)", "content": "FL-DP protects the shared model parameters or gradients between clients and the server during the FL\ntraining process by applying Local Differential Privacy (LDP [33, 26]) and (or) Differential Privacy\nStochastic Gradient Descent algorithm (DPSGD [1, 34]).\nTaking the local participates site for example, the main actions for FL-DP can be divided into four\nsteps: setting DP noise mechanism, local gradient clipping, local gradient perturbation, and uploading\nthe protected parameters to the server [28]. FL involves multiple participants, making composition\ntheorem in DP necessary when applying noise. Among the composition DP methods available,\nincluding Simple Composition, Advanced Composition [7], and Moments Accountant [1], we utilize\nSimple Composition for the following discussion. In the case where $M_i$ satisfies $(\\varepsilon, \\delta)$ \u2013 DP, the\ncomposition $(M_1, M_2,..., M_k)$ satisfies $(\\sum_{i=1}^k \\varepsilon_i, \\sum_{i=1}^k \\delta_i)$. Aligning with the state-of-art FL-DP\nframework [28], We applied Simple Composition with the following noise calculation formulas for\nlocal client gradient perturbation, considering both $\\varepsilon$ DP (using the Laplace mechanism) and\n$(\\varepsilon, \\delta)$ \u2013 DP (using the Gaussian mechanism):\n$\\text{Laplace } : \\sigma_{dpc} = \\nabla_{sc} \\times 1/\\varepsilon,$\n(5)\n$\\text{Gaussian } : \\sigma_{dpc} = \\nabla_{sc} \\times \\sqrt{2 \\ln(1.25/\\delta)} / \\varepsilon,$\n(6)\nwhere $\\nabla_{se}$ represents the sensitivity and can be formulated as $\\nabla_{sc} = 2C$. Note that $C$ is the clipping\nthreshold for bounding $||\\nabla W_i|| < C'$, where $\\nabla W_i$ denotes the unperturbed gradient from the $i$ \u2013 th\nclient and $m$ is the minimum size of the local datasets.\nFrom the parameter server site, referring to noising before model aggregation FL (NbAFL) [28], the\nstandard deviation $\\sigma_{dps}$ of Gaussian noises that are added to $\\nabla W_{\\text{server}}$ (aggregated gradients at the\nserver to be broadcast to clients) in an $N$ \u2013 client FL system is as follows:\n$\\sigma_{dps} = \\begin{cases}\\frac{C_{dp} \\nabla_{ss}}{\\varepsilon} & \\text{when } T_{dps} > L_{dpc} \\sqrt{N} \\\\0 & \\text{when } T_{dps} \\le L_{dpc} \\sqrt{N}\\end{cases}$\n(7)\n$\\nabla_s = \\frac{2C}{m} \\times \\sqrt{\\frac{T_{dps}^2 - L_{dpc}^2N}{L_2N}}.$\n(8)\nThe variables $T_{dps}$ and $L_{dpc}$ represent the aggregation times and exposures of uploaded parameters\nfrom each client under the FL-DP system, respectively."}, {"title": "2.3 FL with Perturbation Protection (FL-PP)", "content": "In addition to DP, FL-PP includes a range of alternative gradient perturbation techniques, which do\nnot need to satisfy DP concepts on similarity and can employ noise to effectively obscure private\ndata. (e.g. adding random global noise to gradients, adding noise to specific layers, and dynamic\nnoise addition) [32]."}, {"title": "2.4 Gradient Inversion Attack (GradInv)", "content": "GradInv is a prevalent method in gradient leakage attacks and aims to steal the client's privacy\ninformation in the FL system. The primary idea for the majority of GradInv to recover original\ninformation is minimizing the distance between the dummy gradient $\\nabla W_{\\pounds}$ and the original gradient\n$\\nabla W$ while updating the random data $x_{\\pounds}$ and label $y_{\\pounds}$ until the optimized results $\\nabla W_{\\pounds}$ and $(x, y)$\nare close enough to the original ones. The key formulation can be described as:\n$\\min ||\\nabla W_{\\pounds} \u2013 \\nabla W|| : (x_{\\pounds}, y_{\\pounds}) \\rightarrow (x, y).$\n(9)\nPrevious works utilize Peak Signal-to-Noise Ratio (PSNR) of the recovered images to evaluate the\nperformance of GradInv, with the threshold for the success of such attacks typically hinging on\nthe degree of detail discernible to the human eye in the reconstructed private images[22, 9, 35, 16].\nTo effectively safeguard privacy information against GradInv, a plethora of gradient perturbation\nmethods have been extensively employed [35]."}, {"title": "3 Methodology", "content": "PGLA is the first Diffusion Attack Method focusing on the gradient data structure that can be applied\nto multiple kinds of gradient perturbation protection in FL through adaptive parameters setting\non both forward and reverse process of the Gradient Diffusion Model employed in PGLA. The\nthreat models targeted by PGLA encompass various forms of gradient perturbation. Meanwhile, the\nextremely similar noise mechanism on gradient perturbation protection and diffusion Markovian\nprocess provides a mathematical necessity for PGLA to realize an efficient privacy attack on gradient\nperturbation protection. Overall, our PGLA method (shown in Fig. 2) can be summarized in the\nfollowing four steps:\n(1) Get Protected Gradients. Honest-but-curious malicious attackers steal the Shared Perturbed\nGradients $\\nabla W'$ during the FL training process by hiding on the server side or waiting on the way of\nparameter sharing (Fig.1).\n(2) Construct Surrogate Model. Construct Surrogate Gradients Data Supply Model $F^s$ from the\ndata structure of Shared Perturbed Gradients $\\nabla W'$ stolen by the attacker that can output the same"}, {"title": "3. Gradient Adjustment", "content": "Gradients each with total size $L$ ($L = L_{\\nabla W_{\\text{Layer1}}} + L_{\\nabla W_{\\text{Layer2}}} + ... + L_{\\nabla W_{\\text{LayerN}}}$) are adjusted into $1 \\times g \\times g$ ($g = \\sqrt{L + P}$: $g$ = minimum integer satisfies $g^2 > L$;\n$P$ = 0-padding size) before feeding into Gradient Diffusion Model for training or inference to adapt\ngradient diffusion process. The adjustment is only related to Diffusion procedures, gradients are\nrestored to their original structure and size before Gradient-Based Attacks and Evaluations.\n[M-Adaptive Process]: Adaptive parameter $M$ is inserted into $\\nabla W'$ before inference to ensure\nthe starting time step is appropriately positioned to maximize the denoising capability of Gradient\nDiffusion Model (Eq. (16) ~ Eq. (18))."}, {"title": "3 Train Gradients Diffusion Model", "content": "Construct our Gradient Diffusion Model that takes into\naccount the level of knowledge about the attacked model (e.g., whether the level of perturbation noise\nor the type of noise is known). Conduct [Gradient Adjustment] to Clean Gradients $\\nabla W^s$ extracted\nfrom Surrogate Model $F^s$ to build training gradient dataset to train our Gradient Diffusion Model\n(Fig.2\u2462, Fig.3)."}, {"title": "4 Recover Original Gradients", "content": "The stolen Shared Perturbed Gradients $\\nabla W'$ are put into our\ntrained PGLA Gradient Diffusion Model after [Gradient Adjustment] and [M-Adaptive Process] to\ngenerate the Recovered Gradients $\\nabla W^R$ for further Gradient-Based Attack to get certain privacy\ninformation based on various demands"}, {"title": "5. M-Adaptive Process", "content": "PGLA's Gradient Diffusion Model is inspired by DDPM[12], to be specific:\nIn Step (3), refer to Algorithm 2 lines 4 to 7, we set $t \\in (1, T)$ as the time steps of Gaussian noise\naddition. $T$ is the total time step of the forward Markovian diffusion process. $\\alpha_t$ ($0 < \\alpha_t < 1$) denote\nthe adaptive variables at each iteration and $\\Upsilon_t = \\Pi_{t=0}^T \\alpha_t$. With the above settings, the forward\nprocess ($q$: with no learnable parameters) of PGLA's Gradients Diffusion Model is:\n$q(X_1 | X_0) = N(X_1; \\sqrt{\\alpha_1} X_0, (1 - \\alpha_1)I),$\n(10)\n$q(X_t | X_0) = N(X_t; \\sqrt{\\Upsilon_t} X_0, (1 - \\Upsilon_t)I).$\n(11)\nGiven $(X_0, X_t)$, $X_{t-1}$ can be modeled as:\n$q(X_{t-1} | X_0, X_t) = N(X_t; \\mu_t, \\sigma_t I).$\n(12)\nIf insert the stolen Shared Perturbed Gradients $\\nabla W'$ or constructed Surrogate Perturbed Gradients\n$\\nabla W_{\\text{perturbed}}$ as training condition then input $X_0 = (\\text{condition}, \\nabla W_{\\text{perturbed}})$.If not to train with the\ncondition, then input $X_0 = \\nabla W_j$.Through algebraic calculation, $\\mu_t$ and $\\sigma_t$ in Eq. (12) can be\nsimplified for further usage in the reverse training process[24] as:\n$\\mu_t = \\frac{\\sqrt{\\Upsilon_{t-1}} (1 - \\alpha_t)}{1 - \\Upsilon_t} X_0 + \\frac{\\alpha_t(1 - \\sqrt{\\Upsilon_{t-1}})}{1 - \\Upsilon_t} X_t,$\n(13)\n$\\sigma_t^2 = \\frac{1 - \\Upsilon_{t-1}}{1 - \\Upsilon_t} (1 - \\alpha_t).$\n(14)\nIn the reverse process ($p$) of PGLA's Gradient Diffusion training, the objective function $f_{\\theta}$ which is\ntrained to predict noise vector $\\epsilon$ is modeled as:\n$E_{X_0, \\epsilon} \\sim p(X_0) E_{\\epsilon} ||\\epsilon - f_{\\theta}(\\sqrt{\\Upsilon_t} X_0 + \\sqrt{1 - \\Upsilon_t} \\epsilon, t)||^2].$\n(15)\nIn Step (4), refer to Algorithm 3 lines 1 to 4, two different original gradient generation processes are\nchosen depending on whether or not attackers know the Noise Scale of Perturbation of the Threat\nModel. Take FL with Gaussian Differential Privacy (Eq. (6) in Preliminary) as an example, consider\nan experienced and clever attacker who may know the DP Privacy Budget $\\varepsilon$ and the probability of\ninformation leakage $\\delta$ ($\\delta$ is likely to be set as $10^{-5}$ from usual practice in DP), combining with the\nsensitivity $V_S$ (which can be estimated if the attacker has some previous information with the target\nmodel training dataset), the noise scale can be calculated or estimated to an approximate value $M$\n($M$ defined as the adaptive parameter in [M-Adaptive Process]). So, $\\nabla W'$ can be modeled as:\n$\\nabla W' = \\nabla W + M N(0, I).$\n(16)\nConsidering the forward Markovian gradient diffusion process in Step 3 Eq. (10) & Eq. (11), the\nrelation between stolen perturbed gradients $\\nabla W'$ and original gradients $\\nabla W$ can be remodeled as:\n$\\frac{1}{\\sqrt{1 + M^2}} \\nabla W' = \\frac{1}{\\sqrt{1 + M^2}} \\nabla W + \\frac{M}{\\sqrt{1 + M^2}} N(0, I).$\n(17)\nRecall that our target is to generate $\\nabla W^R$, so $\\frac{1}{\\sqrt{1 + M^2}} \\nabla W'$ should be considered as $X_t$ in the\ninverse process of Gradient Diffusion Model, while $\\nabla W$ stands for $X_0$. Correspondingly, during the\nconstruction of the recovered gradient $\\nabla W^R$, the inverse time steps can be set as $T'$. The relationship\nbetween $T'$ and $M$ is:\n$\\frac{1}{\\sqrt{1 + M^2}} = \\Pi_{t=0}^{T'} \\alpha_t.$\n(18)\nSince $\\Upsilon_t = \\Pi_{t=0}^{T'} \\alpha_t$ have been predefined and calculated during the forward Markovian diffusion\nprocess, $T'$ can be fixed in an approximate range $T' \\in (T_{0}^-, T_{0}^+)$, where $\\Pi_{t=0}^{T_{0}^-} \\alpha_t < \\frac{1}{\\sqrt{1 + M^2}} < \\frac{1}{\\sqrt{1 + M^2}} <\n\\Pi_{t=0}^{T_{0}^+} \\alpha_t$ and $T_{0}^+, T_{0}^-$ are positive integers less than the total forward noise addition time step $T$. On\nthe other hand, if $M$ can not be estimated, which means the attacker knows nothing about the noise\nscale. Since $\\frac{1}{\\sqrt{1 + M^2}} \\in (0,1)$, the input value of the inference process can simply be set as $c \\nabla W'$\nwhere $c \\in (0, 1)$ to adapt the Markovian forward process.\nAlso, if conditions allow, $e$ can be modeled and predicted by a separate Machine Learning model\naccording to the specific requirement of attackers."}, {"title": "4 Experiments", "content": "We assess the performance of the PGLA within the FL-PP framework, which encompasses the FL-DP\napproach.\n4.1 Experimental Setups.\nVariant Attack Models Three PGLA variant models: PGLA (trained with only unperturbed\nsurrogate gradients), Condition PGLA (trained with both perturbed gradients and unperturbed\nsurrogate gradients), and Non-Adaptive PGLA (without [M-Adaptive Process]: no perturbation scale\n$M$ as adaptive parameter during gradient diffusion process) are presented during experiments.\nDatasets In the subsequent experimental analyses, we employ the datasets MNIST, CIFAR100, and\nSTL10 as client privacy datasets, which also serve as the ground truth for privacy leakage evaluation.\nWe extract the unperturbed original gradients ($\\nabla W$) of the aforementioned three datasets from the\nlocal training model of the target client as the reference benchmark of gradient denoising under the\nFL-PP paradigm.\nEvaluation and Boundary In the context of evaluating Gradient Denoising (Sec. 4.2 ~ 4.3), we\nemploy the cosine similarity CosSimilar, and the Average Peak Signal-to-Noise Ratio PSNR$_g$, as\nmetrics to assess the quality of the Recovered Gradients $\\nabla W^R$ compared to the Original Gradients\n$\\nabla W$. Higher values of the two metrics indicate better accuracy in the original gradient recovery.\nTo evaluate the Privacy Leakage Capability (Sec. 4.4), we utilize the Image Average Peak Signal-\nto-Noise Ratio PSNR$_i$ and the Label Recovered Accuracy LRA. These metrics are employed to\nassess the fidelity of the recovered images and the accuracy of the recovered labels, respectively, to\nthe original images and ground truth labels. The boundary of Privacy Leakage Attack is aligned with\nprevious works (Sec. 2.4): the attack is considered successful if human visual perception can discern\nthe requisite information from the recovered images.\n4.2 Gradients Denoising under FL-DP\nAmong FL-DP, we choose the NbAFL framework [28] (Noising before model aggregation FL, details\nrefer to Sec. 2.2) as the threat model in this experiment due to its widespread adoption. To ensure a\ncomprehensive evaluation of the effectiveness of the PGLA, we train the three PGLA variant models,\nas well as non-diffusion denoising models such as NBNet [3], SS-BSN [11], and AP-BSN [14],\nusing gradients extracted from the surrogate downstream task model trained on the FashionMnist.\nFollowing that, we utilize the trained models to denoise the shared perturbed gradients intercepted\nby the attacker from the target clients. These target clients have locally trained downstream task\nmodels using privacy datasets (MNIST, CIFAR100, and STL10). The gradients are protected using\nthe NbAFL[28] before being shared with the server."}, {"title": "4.3 Gradients Denoising under FL-PP", "content": "To evaluate the generalization capability of PGLA on gradient denoising, we constructed two different\ntypes of noise (Laplace and Gaussian) randomly applied to each layer of the original gradients within\nthe FL-PP framework. The training and inference processes of PGLA variant models, and non-\ndiffusion denoising models (NBNet [3], SS-BSN [11], and AP-BSN [14]) are the same as the above\nexperiments on gradients denoising under FL-DP. In contrast to attacks specifically tailored for\nthe FL-DP framework, the gradient denoising experiments on FL-PP showcase PGLA's ability to\neffectively handle various types of perturbations and adapt to different magnitudes of perturbation.\nReferring to the experimental results illustrated in Fig. 5, it is observed that when subjected to Laplace\nperturbation, PGLA variant models exhibit an average improvement of 21.6% in PSNR and 9.2% in\nCosine Similarity. Similarly, under Gaussian perturbation, PGLA achieves an average enhancement\nof 21.3% in PSNR and 9.1% in Cosine Similarity. These findings provide compelling evidence for\nthe robustness and stability of the PGLA and its variant models."}, {"title": "4.4 Privacy Leakage Capability", "content": "The comparison of overall privacy leakage capability from perturbed gradients of PGLA and typical\nGradient Leakage Attacks (GRNN [22], IG [9], and DLG [35]) under FL-DP, compares Image\nAverage Peak Signal Noise Ratio PSNR$_i$ and Label Recovered Accuracy LRA of the recovered\nlocal clients' privacy information. Local clients' private training datasets are MNIST, CIFAR100,\nand STL10."}, {"title": "4.5 Attack Duration", "content": "To comprehensively evaluate the attack efficiency of PGLA, we compared the average inference\ntime of different denoising models under FL-PP. The training and inference procedures of the variant\nmodels of PGLA, as well as the non-diffusion denoising models, are configured to be consistent as in\nSec. 4.3.\nThe experimental results presented in Table 4 indicate that PGLA variant models exhibit a relative\nadvantage in terms of gradient denoising speed, surpassing non-diffusion methods by an average\nimprovement of 32.7% in inference time.\nLimitations: The experiments demonstrate that PGLA can reconstruct original gradients\nfrom noise-perturbed gradients without requiring knowledge of the type and magnitude of\nperturbation noise, as well as FL clients' local model. However, PGLA is not effective in attacking\nperturbations that are not based on gradient diffusion (noise perturbation) such as representation\nperturbation. In addition, the overall effectiveness of PGLA in reconstructing privacy datasets is also\nbounded by the selected subsequent Gradient Leakage Attacks."}, {"title": "5 Conclusion", "content": "This paper makes the first attempt to investigate the diffusion property of the widely-used perturbation-\nbased gradient protection in Federated Learning (FL). To reveal the potential vulnerability, we propose\na novel Perturbation-Resilient Gradient Leakage Attack (PGLA), an effective attack paradigm to\ndeactivate the perturbation protection methodology by leveraging the denoising capability of diffusion\nmodels. Based on PGLA, we wish to enhance public consciousness on the issues of perturbation\ndenoising and gradient leakage in FL applications."}]}