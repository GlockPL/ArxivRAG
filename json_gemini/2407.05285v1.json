{"title": "Gradient Diffusion: A Perturbation-Resilient Gradient Leakage Attack", "authors": ["Xuan Liu", "Siqi Cai", "Qihua Zhou", "Song Guo", "Ruibin Li", "Kaiwei Lin"], "abstract": "Recent years have witnessed the vulnerability of Federated Learning (FL) against gradient leakage attacks, where the private training data can be recovered from the exchanged gradients, making gradient protection a critical issue for the FL training process. Existing solutions often resort to perturbation-based mechanisms, such as differential privacy, where each participating client injects a specific amount of noise into local gradients before aggregating to the server, and the global distribution variation finally conceals the gradient privacy. However, perturbation is not always the panacea for gradient protection since the robustness heavily relies on the injected noise. This intuition raises an interesting question: is it possible to deactivate existing protection mechanisms by removing the perturbation inside the gradients? In this paper, we present the answer: yes and propose the Perturbation-resilient Gradient Leakage Attack (PGLA), the first attempt to recover the perturbed gradients, without additional access to the original model structure or third-party data. Specifically, we leverage the inherent diffusion property of gradient perturbation protection and construct a novel diffusion-based denoising model to implement PGLA. Our insight is that capturing the disturbance level of perturbation during the diffusion reverse process can release the gradient denoising capability, which promotes the diffusion model to generate approximate gradients as the original clean version through adaptive sampling steps. Extensive experiments demonstrate that PGLA effectively recovers the protected gradients and exposes the FL training process to the threat of gradient leakage, achieving the best quality in gradient denoising and data recovery compared to existing models. We hope to arouse public attention on PGLA and its defense.", "sections": [{"title": "1 Introduction", "content": "With the latest success in real-life applications of various data-based generative models, the privacy and information security of these models are under increasing scrutiny[2]. Federated Learning (FL), as a newly proposed privacy preservation distributed machine learning paradigm, collaboratively trains a global model on the parameter server by aggregating computed updates from local clients [18], is widely used in various training scenarios[19, 21, 27], for example, Gboard and Medical Diagnosis System, where data security and privacy are highly significant[30]. Specifically, FL protects users' privacy by sharing only the model parameters or gradients but not the raw data [15]. Existing FL protocol designs have been demonstrated to be vulnerable to adversaries both inside and outside the system, jeopardizing data privacy and system robustness [17]. Among these, gradient leakage attacks have shown serious privacy hazards [10, 16, 35]. To better protect the private information of the clients from malicious attackers during the training and inference periods, protection of shared gradients becomes an almost essential step [23]. Among existing gradient protection methods, gradient perturbation, as an extensively used privacy-preserving method, has already been frequently applied in FL scenarios efficiently [20] (schematic diagram shown in Fig. 1).\nPreprint. Under review."}, {"title": "2 Preliminary", "content": "Federated Learning (FL) provides comparative privacy guarantees for model training which does not require the original data and the participating clients are free to enter or leave [17]. Considering a general FL system with homogeneous architectures. Regarding the gradient protection issue in the FL training process, gradient perturbation is one of the most widely used mechanisms. Here, we present the key preliminary definitions of gradient perturbation and gradient attacks that are pertinent to our work."}, {"title": "2.1 Differential Privacy (DP)", "content": "Generally, DP is defined based on the concept of adjacent database and has been applied in various practical areas in Artificial Intelligence to protect privacy information through adding specific perturbation, e.g. Google's RAPPORT [8] and large-scale graph data publishing [4]. Tracing back to the definition of Differential Privacy, in this paper we discuss both \\(\\varepsilon\\) -DP and (\\(\\varepsilon, \\delta\\)) \u2013 DP [5, 6, 1] as our sample attack background. Here, we present two core definitions first.\nDefinition 1. A randomized mechanism \\(M: \\mathcal{D} \\rightarrow \\mathcal{R}\\) with domain \\(\\mathcal{D}\\) and range \\(\\mathcal{R}\\) satisfies \\(\\varepsilon\\)-differential privacy if for any two adjacent inputs \\(d, d' \\in \\mathcal{D}\\) and for any subset of outputs \\(S \\subseteq \\mathcal{R}\\), and it holds that:\n\\[Pr[M(d) \\in S] \\le e^{\\varepsilon}Pr[M(d') \\in S]. \\tag{1}\\]\nDefinition 2. A randomized mechanism \\(M: \\mathcal{D} \\rightarrow \\mathcal{R}\\) with domain \\(\\mathcal{D}\\) and range \\(\\mathcal{R}\\) satisfies (\\(\\varepsilon, \\delta\\))-differential privacy if for any two adjacent inputs \\(d, d' \\in \\mathcal{D}\\) and for any subset of outputs \\(S \\subseteq \\mathcal{R}\\), and it holds that:\n\\[Pr[M(d) \\in S] \\le e^{\\varepsilon}Pr[M(d') \\in S] + \\delta. \\tag{2}\\]\n\\(\\delta\\)-approximation is preferably smaller than 1/|\\(d\\)|. Note that we usually apply Laplace perturbation for the definition in Eq. (1). However, as to the definition in Eq. (2), the noise perturbation mechanism needs to be Gaussian Mechanisms, which can adapt both \\(\\varepsilon\\) and \\(\\delta\\). According to Theorem A.1 in Gaussian Mechanisms [7] to ensure the given Gaussian noise distribution \\(\\mathcal{N}(0, \\sigma_{dp}^{2})\\) well preserves (\\(\\varepsilon, \\delta\\)) \u2013 DP, the noise scale should satisfy :\n\\[\\sigma_{dp} \\ge C_{dp} \\frac{\\triangle s}{\\varepsilon}, \\varepsilon \\in (0,1), \\tag{3}\\]\n\\[C_{dp} \\ge \\sqrt{2ln(1.25/\\delta)}, \\tag{4}\\]\nwhere \\(\\triangle s = \\max_{D,D'}||S(D) \u2013 S(D')||\\) and \\(s\\) represents the real-value function. \\(C_{dp}\\) can be considered as a hyperparameter used to define the DP boundary."}, {"title": "2.2 FL with Differential Privacy (FL-DP)", "content": "FL-DP protects the shared model parameters or gradients between clients and the server during the FL training process by applying Local Differential Privacy (LDP [33, 26]) and (or) Differential Privacy Stochastic Gradient Descent algorithm (DPSGD [1, 34]).\nTaking the local participates site for example, the main actions for FL-DP can be divided into four steps: setting DP noise mechanism, local gradient clipping, local gradient perturbation, and uploading the protected parameters to the server [28]. FL involves multiple participants, making composition theorem in DP necessary when applying noise. Among the composition DP methods available, including Simple Composition, Advanced Composition [7], and Moments Accountant [1], we utilize Simple Composition for the following discussion. In the case where \\(M_{i}\\) satisfies (\\(\\varepsilon_{i}, \\delta_{i}\\)) \u2013 DP, the composition (\\(M_{1}, M_{2}, ..., M_{k}\\)) satisfies (\\(\\sum_{i=1}^{k} \\varepsilon_{i}, \\sum_{i=1}^{k} \\delta_{i}\\)). Aligning with the state-of-art FL-DP framework [28], We applied Simple Composition with the following noise calculation formulas for local client gradient perturbation, considering both \\(\\varepsilon\\) DP (using the Laplace mechanism) and (\\(\\varepsilon, \\delta\\)) \u2013 DP (using the Gaussian mechanism):\n\\[\\text{Laplace }: \\sigma_{dpc} = \\frac{\\triangle s_{c}}{\\varepsilon} \\times 1/\\varepsilon, \\tag{5}\\]\n\\[\\text{Gaussian }: \\sigma_{dpc} = \\frac{\\triangle s_{c} \\times \\sqrt{2ln(1.25/\\delta)}}{\\varepsilon}, \\tag{6}\\]\nwhere \\(\\triangle s_{c}\\) represents the sensitivity and can be formulated as \\(\\triangle s_{c} = 2C\\). Note that C is the clipping threshold for bounding \\(||\\nabla W_{i}|| < C\\), where \\(\\nabla W_{i}\\) denotes the unperturbed gradient from the \\(i\\) \u2013 th client and m is the minimum size of the local datasets.\nFrom the parameter server site, referring to noising before model aggregation FL (NbAFL) [28], the standard deviation \\(\\sigma_{dps}\\) of Gaussian noises that are added to \\(\\nabla W_{server}\\) (aggregated gradients at the server to be broadcast to clients) in an N \u2013 client FL system is as follows:\n\\[\\sigma_{dps} = \\begin{cases} \\frac{C_{dp} \\triangle s_{s}}{\\varepsilon} & \\text{when } T_{dps} > L_{dpc} \\sqrt{N}\\\\ 0 & \\text{when } T_{dps} \\le L_{dpc} \\sqrt{N}\\end{cases} \\tag{7}\\]\n\\[\\nabla s_{c} \\times \\sqrt{\\frac{2 C_{dp}}{\\varepsilon}} \\times \\sqrt{\\frac{T_{dps}^{2} \u2013 L_{dpc}^{2}}{L_{2}N}} \\tag{8}\\]\nThe variables \\(T_{dps}\\) and \\(L_{dpc}\\) represent the aggregation times and exposures of uploaded parameters from each client under the FL-DP system, respectively."}, {"title": "2.3 FL with Perturbation Protection (FL-PP)", "content": "In addition to DP, FL-PP includes a range of alternative gradient perturbation techniques, which do not need to satisfy DP concepts on similarity and can employ noise to effectively obscure private data. (e.g. adding random global noise to gradients, adding noise to specific layers, and dynamic noise addition) [32]."}, {"title": "2.4 Gradient Inversion Attack (GradInv)", "content": "\\(\\text{GradInv}\\) is a prevalent method in gradient leakage attacks and aims to steal the client's privacy information in the FL system. The primary idea for the majority of \\(\\text{GradInv}\\) to recover original information is minimizing the distance between the dummy gradient \\(\\nabla W_{\\xi}\\) and the original gradient \\(\\nabla W\\) while updating the random data \\(x_{\\xi}\\) and label \\(y_{\\xi}\\) until the optimized results \\(\\nabla W\\) and \\((x, y)\\) are close enough to the original ones. The key formulation can be described as:\n\\[\\min ||\\nabla W_{\\xi} \u2013 \\nabla W|| : (x_{\\xi}, y_{\\xi}) \\rightarrow (x, y). \\tag{9}\\]\nPrevious works utilize Peak Signal-to-Noise Ratio (PSNR) of the recovered images to evaluate the performance of \\(\\text{GradInv}\\), with the threshold for the success of such attacks typically hinging on the degree of detail discernible to the human eye in the reconstructed private images[22, 9, 35, 16]. To effectively safeguard privacy information against \\(\\text{GradInv}\\), a plethora of gradient perturbation methods have been extensively employed [35]."}, {"title": "3 Methodology", "content": "PGLA is the first Diffusion Attack Method focusing on the gradient data structure that can be applied to multiple kinds of gradient perturbation protection in FL through adaptive parameters setting on both forward and reverse process of the Gradient Diffusion Model employed in PGLA. The threat models targeted by PGLA encompass various forms of gradient perturbation. Meanwhile, the extremely similar noise mechanism on gradient perturbation protection and diffusion Markovian process provides a mathematical necessity for PGLA to realize an efficient privacy attack on gradient perturbation protection. Overall, our PGLA method (shown in Fig. 2) can be summarized in the following four steps:\n(1) Get Protected Gradients. Honest-but-curious malicious attackers steal the Shared Perturbed Gradients \\(\\nabla W'\\) during the FL training process by hiding on the server side or waiting on the way of parameter sharing (Fig.1).\n(2) Construct Surrogate Model. Construct Surrogate Gradients Data Supply Model \\(F^{s}\\) from the data structure of Shared Perturbed Gradients \\(\\nabla W'\\) stolen by the attacker that can output the same"}, {"title": "4 Experiments", "content": "We assess the performance of the PGLA within the FL-PP framework, which encompasses the FL-DP approach."}, {"title": "4.1 Experimental Setups.", "content": "Variant Attack Models Three PGLA variant models: PGLA (trained with only unperturbed surrogate gradients), Condition PGLA (trained with both perturbed gradients and unperturbed surrogate gradients), and Non-Adaptive PGLA (without [\\(\\text{M}\\)-Adaptive Process]: no perturbation scale \\(M\\) as adaptive parameter during gradient diffusion process) are presented during experiments.\nDatasets In the subsequent experimental analyses, we employ the datasets MNIST, CIFAR100, and STL10 as client privacy datasets, which also serve as the ground truth for privacy leakage evaluation. We extract the unperturbed original gradients (\\(\\nabla W\\)) of the aforementioned three datasets from the local training model of the target client as the reference benchmark of gradient denoising under the FL-PP paradigm.\nEvaluation and Boundary In the context of evaluating Gradient Denoising (Sec. 4.2 ~ 4.3), we employ the cosine similarity \\(Cos\\) \\(Similar\\), and the Average Peak Signal-to-Noise Ratio PSNR, as metrics to assess the quality of the Recovered Gradients \\(\\nabla W^{R}\\) compared to the Original Gradients \\(\\nabla W\\). Higher values of the two metrics indicate better accuracy in the original gradient recovery.\nTo evaluate the Privacy Leakage Capability (Sec. 4.4), we utilize the Image Average Peak Signal-to-Noise Ratio PSNR\\(_{I}\\) and the Label Recovered Accuracy LRA. These metrics are employed to assess the fidelity of the recovered images and the accuracy of the recovered labels, respectively, to the original images and ground truth labels. The boundary of Privacy Leakage Attack is aligned with previous works (Sec. 2.4): the attack is considered successful if human visual perception can discern the requisite information from the recovered images."}, {"title": "4.2 Gradients Denoising under FL-DP", "content": "Among FL-DP, we choose the NbAFL framework [28] (Noising before model aggregation FL, details refer to Sec. 2.2) as the threat model in this experiment due to its widespread adoption. To ensure a comprehensive evaluation of the effectiveness of the PGLA, we train the three PGLA variant models, as well as non-diffusion denoising models such as NBNet [3], SS-BSN [11], and AP-BSN [14], using gradients extracted from the surrogate downstream task model trained on the FashionMnist. Following that, we utilize the trained models to denoise the shared perturbed gradients intercepted by the attacker from the target clients. These target clients have locally trained downstream task models using privacy datasets (MNIST, CIFAR100, and STL10). The gradients are protected using the NbAFL[28] before being shared with the server."}, {"title": "4.3 Gradients Denoising under FL-PP", "content": "To evaluate the generalization capability of PGLA on gradient denoising, we constructed two different types of noise (Laplace and Gaussian) randomly applied to each layer of the original gradients within the FL-PP framework. The training and inference processes of PGLA variant models, and non- diffusion denoising models (NBNet [3], SS-BSN [11], and AP-BSN [14]) are the same as the above"}, {"title": "4.4 Privacy Leakage Capability", "content": "The comparison of overall privacy leakage capability from perturbed gradients of PGLA and typical Gradient Leakage Attacks (GRNN [22], IG [9], and DLG [35]) under FL-DP, compares Image Average Peak Signal Noise Ratio PSNR\\(_{I}\\) and Label Recovered Accuracy LRA of the recovered local clients' privacy information. Local clients' private training datasets are MNIST, CIFAR100, and STL10."}, {"title": "4.5 Attack Duration", "content": "To comprehensively evaluate the attack efficiency of PGLA, we compared the average inference time of different denoising models under FL-PP. The training and inference procedures of the variant models of PGLA, as well as the non-diffusion denoising models, are configured to be consistent as in Sec. 4.3.\nThe experimental results presented in Table 4 indicate that PGLA variant models exhibit a relative advantage in terms of gradient denoising speed, surpassing non-diffusion methods by an average improvement of 32.7% in inference time.\nLimitations: The experiments demonstrate that PGLA can reconstruct original gradients from noise-perturbed gradients without requiring knowledge of the type and magnitude of perturbation noise, as well as FL clients' local model. However, PGLA is not effective in attacking perturbations that are not based on gradient diffusion (noise perturbation) such as representation perturbation. In addition, the overall effectiveness of PGLA in reconstructing privacy datasets is also bounded by the selected subsequent Gradient Leakage Attacks."}, {"title": "5 Conclusion", "content": "This paper makes the first attempt to investigate the diffusion property of the widely-used perturbation- based gradient protection in Federated Learning (FL). To reveal the potential vulnerability, we propose a novel Perturbation-Resilient Gradient Leakage Attack (PGLA), an effective attack paradigm to deactivate the perturbation protection methodology by leveraging the denoising capability of diffusion models. Based on PGLA, we wish to enhance public consciousness on the issues of perturbation denoising and gradient leakage in FL applications."}]}