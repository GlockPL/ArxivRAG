{"title": "NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for Personalized Hearing Aids", "authors": ["Shafique Ahmed", "Ryandhimas E. Zezario", "Hui-Guan Yuan", "Amir Hussain", "Hsin-Min Wang", "Wei-Ho Chung", "Yu Tsao"], "abstract": "The prevalence of hearing aids is increasing. However, optimizing the amplification processes of hearing aids remains challenging due to the complexity of integrating multiple modular components in traditional methods. To address this challenge, we present NeuroAMP, a novel deep neural network designed for end-to-end, personalized amplification in hearing aids. NeuroAMP leverages both spectral features and the listener's audiogram as inputs, and we investigate four architectures: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer. We also introduce Denoising NeuroAMP, an extension that integrates noise reduction along with amplification capabilities for improved performance in real-world scenarios. To enhance generalization, a comprehensive data augmentation strategy was employed during training on diverse speech (TIMIT and TMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the Hearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index (HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the Transformer architecture within NeuroAMP achieves the best performance, with SRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI) on the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy maintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ). Furthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC approach and a two-stage baseline on the Voice-Bank+DEMAND dataset, achieving a 10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results highlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable improvements in personalized hearing aid amplification.", "sections": [{"title": "I. INTRODUCTION", "content": "HEARING loss, a pervasive global health issue, con-tinuously impacts the lives of hundreds of millions, leading to social isolation [1], depression [2], diminished quality of life [3], and even cognitive decline [4]. The World Health Organization (WHO) highlights that approximately 430 million people worldwide currently face problems related to hearing loss [5], [6]. Despite the availability of hearing aids (HAs), which can mitigate these adverse effects, only 16% of hearing-loss individuals in the United States consistently use them [7], [8]. This low adoption rate suggests a notable gap between the potential of hearing aids and their real-world utilization, emphasizing the importance of developing more effective and user-friendly solutions.\nNon-use of hearing aids by people with hearing loss can be attributed to a variety of factors, including hearing aids being ineffective in noisy environments, suboptimal sound quality, insufficient benefit, and incompatibility with the individual's specific type of hearing loss [9]. To address these challenges, various amplifier strategies for hearing aids have been devel-oped [10]\u2013[14]. These strategies primarily involve two key components: the prescription fitting formulas and compression techniques to determine the desired insertion gain, as shown in Fig. 1. The prescription formula aims to enhance the audibility of audio based on an individual's audiogram [10], [11]. Well-known prescription fitting formulas include NAL-NL1 [12], NAL-NL2 [15], NAL-R [10], and DSL m[i/o] [16].\nA common problem for people with hearing loss is a reduction in the auditory dynamic range, known as loudness recruitment. In this case, weak sounds may become inaudi-ble, while loud sounds can be uncomfortably intense. Non-linear prescription formulas like NAL-NL1, NAL-NL2, and DSL m[i/o] play a crucial role in addressing the reduced auditory dynamic range through compression. These advanced formulas, however, are primarily used in commercial hearing aids and require licensing, which may limit broader acces-sibility. In this study, we utilize the widely adopted NAL-R prescription formula and incorporate it with Wide Dynamic Range Compression (WDRC) [14] to introduce non-linearity, mimicking the behavior of more advanced systems like NAL-NL1. This approach aligns with the established practice in the open-source OpenMHA platform [17]. Moreover, while conventional amplifiers demonstrate notable performance, they often face challenges in adapting to dynamic acoustic environ-"}, {"title": "II. RELATED WORK", "content": "Prescription fitting formulas aim to amplify sound based on the user's degree of hearing loss, making speech signals audible across different frequencies while maintaining com-fort. These formulas can be broadly categorized into linear and non-linear approaches. Linear amplification, exemplified by methods like NAL-R and NAL-RP [23], provides a constant gain regardless of input volume and is generally preferred for individuals with flatter hearing loss profiles, larger dy-namic ranges, and less variability in dynamic range across"}, {"title": "III. METHODOLOGY", "content": "This section presents the architectures of the two proposed frameworks. We first introduce the NeuroAMP architecture, which outlines the core design for audio amplification using a neural network-based system. Subsequently, we introduce the Denoising NeuroAMP architecture, which is an extension of NeuroAMP that includes noise reduction capabilities."}, {"title": "A. NeuroAMP Architecture", "content": "The processing workflow of NeuroAMP involves taking an audio signal and the listener's audiogram as inputs, processing them through a neural network, and producing an amplified output signal. The detailed architecture of the NeuroAMP model is illustrated in Fig. 2. Specifically, given the input signal y, the STFT is processed to extract y into its time-frequency spectral features (Y = [y1,..., yt,..., y\u0442]). Si-multaneously, the audiogram z is transformed into a vector representation via a dense layer and then replicated into a sequence of length T (Z = [z1, . . ., zt, . . ., zT]). The two input streams are concatenated along the y-axis feature dimension and then passed to the subsequent layers of the model. In our setup, we plan to concatenate two sets of features in the latent space, leveraging its ability to capture semantic feature similarity and perform feature fusion. This approach has been shown to be effective for building personalized deep-learning models [25]. The feedforward process of NeuroAMP is defined as follows:\nY = STFT(y)\nZ = Dense(z)\nConcat = [Y | Z]\nY = NeuroAMP(Concat).\n\nIn this work, for the core NN module in NeuroAMP, we select four neural network architectures, including the Con-volutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer."}, {"title": "1) Convolutional Neural Network (CNN):", "content": "The Convolu-tional Neural Network (CNN) is integrated into the NeuroAMP model due to its ability to extract hierarchical features from spatial or spectral data [26]. CNNs are particularly adept at recognizing patterns and features in audio spectrograms from complex audio signals [27], [28]. In the NeuroAMP model, the CNN is used to process spectral features extracted by the STFT, with the goal of capturing detailed representations of the frequency components of the audio signal. The operation process of the CNN in the NeuroAMP model can be described by the following equations:\nh\u2081 = ReLU(Conv(Concat, W1) + b1)\nh2 = ReLU(Conv(h1, W2) + b2)\n:\nh\u0131 = ReLU(Conv(h\u0131\u22121, W\u0131) + bi)\n:\nY = Dense(Flatten(hL-1))\n\nwhere Wi and by are the convolutional weights and biases of the l-th layer, respectively, and L represents the number of convolutional layers. This architecture ensures a comprehen-sive feature extraction process, allowing the NeuroAMP model to more effectively distinguish between auditory elements."}, {"title": "2) Long Short-Term Memory (LSTM):", "content": "LSTM is chosen for its proficiency in modeling temporal dependencies in sequen-tial data [29], [30]. In the context of hearing aids, preserving the temporal structure of the audio signal is essential to maintaining speech intelligibility and naturalness. LSTM is effective at capturing long-range dependencies and temporal patterns [31], which is crucial for processing continuous audio signals and adapting to varying acoustic environments. The process of the LSTM in the NeuroAMP model is described as follows:\nht, ct = LSTM(Concat, ht\u22121, Ct-1)\n\u0176 = Dense(hr),\n\nwhere ht and ct are the hidden state and cell state at time step t, respectively."}, {"title": "3) Convolutional Recurrent Neural Network (CRNN):", "content": "CRNN combines the strengths of CNN and RNN, making it ideal for signal processing tasks [32], [33] that involve both spatial and temporal feature extraction. Its convolutional layers capture local spectral features, while the recurrent layers model temporal dynamics. This hybrid approach enhances the network's ability to manage audio signal complexity, thereby improving performance in tasks like speech enhancement [34], [35]. The process of the CRNN in the NeuroAMP model is described as follows:\nh = CNN(Concat)\nht, ct = LSTM(hc, ht\u22121, ct-1)\n\u0176 = Dense(hr),\n\nwhere he is the output of the convolutional layer."}, {"title": "4) Transformer:", "content": "The Transformer is included for its state-of-the-art performance in sequence modeling tasks [36]\u2013[39]. Unlike traditional RNNs, the Transformer uses a self-attention mechanism [36] to process entire sequences in parallel, al-lowing for efficient handling of long-range dependencies [40]. This capability is particularly beneficial in audio signal pro-cessing [41], where capturing the global context and dependen-cies can enhance performance. The process of the Transformer in the NeuroAMP model is described as follows:\nQ, K, V = W\u0189Concat, WKConcat, Wy Concat\nAttention = Softmax(QK\u1d40 / \u221adk)V\nh = FFN(Attention)\nY = Dense(h),\n\nwhere Q, K, and V are the query, key, and value matrices, WQ, WK, and Wy are the weight matrices for the query, key, and value, dk is the dimension of the key, and FFN is the feedforward network."}, {"title": "5) Loss function:", "content": "To optimize NeuroAMP, we use Mean Squared Error (MSE) as the loss function. This approach penalizes deviations between predicted (\u0176) and ground-truth (Y) spectral representations, guiding the network to learn spectral mappings. The loss function is calculated as:\nLNeuroAMP = 1/T \u2211t=1^T |yt - \u0177t|^2\n\nwhere yt and \u0177 are the ground-truth amplified and estimated-amplified spectral representations of the t-th frame of the training audio signal y, and T is the number of frames."}, {"title": "B. Denoising NeuroAMP Architecture", "content": "To address the challenges of noisy environments, which severely impact the effectiveness of hearing aids, we propose the Denoising NeuroAMP system. This system is built on the NeuroAMP architecture, allowing simultaneous denoising and amplification by seamlessly integrating speech enhancement within the neural network. The architecture of the Denoising NeuroAMP model is illustrated in Fig. 3.\nAs shown in Fig. 3, Denoising NeuroAMP takes a noisy audio signal (X) as input, represented as a magnitude spec-trogram. Simultaneously, the listener's audiogram (Z) is pro-"}, {"title": "IV. EXPERIMENTS", "content": "In this work, the input to NeuroAMP includes two types of speech signals (English and Taiwanese Mandarin speech data) and music data. We aim to evaluated the proposed models under diverse acoustic properties of cross-linguistic speech and music signals. The speech data contains three conditions: clean, noisy, and enhanced speech. To generate noisy speech, we added 100 types of environmental noise from [42] to clean speech at different signal-to-noise ratio (SNR) levels. Enhanced speech was then obtained by applying two deep learning-based speech enhancement models to the noisy data. The first model is based on LSTM [43], which operates on STFT features to enhance noisy speech. The second model is based on a Fully Convolutional Network (FCN) [27], which operates on the raw waveform to enhance noisy speech. For music data, we mainly consider two categories: clean music and noisy music (i.e., music with added noise). Clean music represents the original recording, while noisy music incor-porates various environmental disturbances to simulate real-world listening scenarios. To train the NeuroAMP model, we used TIMIT, TMHINT, and the MUSIC dataset provided by the Cadenza Challenge [44]. To test and evaluate the model's performance and generalization, we also included the unseen test set (the acoustic conditions and audio content are entirely different from the training conditions) from the VoiceBank and MUSDB18-HQ datasets."}, {"title": "1) Training Dataset Description:", "content": "TIMIT Dataset [45]: The TIMIT dataset is a well-known speech corpus used for tasks like speech recognition [46], [47] and speech enhancement [47], [48]. It includes recordings from 630 speakers representing eight major dialects of American English, with each speaker reading 10 phonetically rich sentences. The TIMIT training set consists of 4,620 audio files, while the test set comprises 1,690 audio files. In addition, to augment the dataset, noise signal was injected at different SNR levels(-5 dB, 0 dB, and 5 dB) into the clean audio to create noisy audio. In addition, enhanced versions of these noisy audio were generated using two SE models, which are LSTM and FCN. Following augmentation, a total of 13,700 training samples were selected from the TIMIT dataset, consisting of 3425 clean audio, 3425 noisy audio, and 6850 enhanced audio.\nTMHINT Dataset: During model training, we incor-porated an additional dataset from the Taiwan Man-darin Hearing in Noise Test (TMHINT) to improve the reliability of our models by including diverse acous-tic conditions. The TMHINT dataset has been widely used in various speech-related tasks, such as speech assessment [49] and enhancement [47]. Specifically, the TMHINT dataset consists of recordings from multiple speakers, with 16 phonemically balanced sentence sets, each containing 20 sentences. Each sentence comprises ten characters representing diverse and balanced phonetic elements of Mandarin, including four primary lexical tones and a neutral tone. The total number of utterances in"}, {"title": "V. CONCLUSION", "content": "This research introduces NeuroAMP, a novel deep learning-based system for personalized hearing aid amplification, and Denoising NeuroAMP, an extension that integrates noise re-duction for better performance in adverse environments. We investigated four neural network architectures: CNN, LSTM, CRNN, and Transformer, using spectral features and audio-grams as inputs. Extensive evaluation using HASPI, HASQI, and HAAQI, together with statistical measures like LCC, SRCC, and MSE, demonstrated the superior performance of the Transformer within the NeuroAMP framework. Notably, NeuroAMP achieved SRCC scores of 0.9810 (HASQI) and 0.9914 (HASPI) on the unseen VCTK dataset, and 0.9892 (HAAQI) on the MUSDB18-HQ dataset, validating the effec-tiveness of our data augmentation strategy. Next, Subjective listening tests confirmed that both NeuroAMP produce outputs that are perceptually similar to those of traditional methods while offering better adaptability. Furthermore, Denoising NeuroAMP outperformed conventional NAL-R+WDRC and two-stage baselines on the VoiceBank+DEMAND dataset, highlighting its potential for enhanced speech intelligibility in noisy environments. In conclusion, this work demonstrates the potential of NeuroAMP and Denoising NeuroAMP to perform a personalized, user-centric approach to amplification and noise reductions. In our future work, we will focus on refining these models, exploring real-time implementation, incorporating user feedback, and integrating other essential modules, such as a hearing loss model for compensation, to achieve a better personalized hearing assistance solution."}]}