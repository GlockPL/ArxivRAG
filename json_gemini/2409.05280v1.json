{"title": "RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation", "authors": ["Quoc-Bao Nguyen-Le", "Tuan-Hy Le", "Anh-Triet Do", "Quoc-Huy Trinh"], "abstract": "Cardiovascular disease is a major global health concern, contributing significantly to global mortality. Accurately segmenting cardiac medical imaging data is crucial for reducing fatality rates associated with these conditions. However, current state-of-the-art (SOTA) neural networks, including CNN-based and Transformer-based approaches, face challenges in capturing both inter-slice connections and intra-slice details, especially in datasets featuring intricate, long-range details along the z-axis like coronary arteries. Existing methods also struggle with differentiating non-cardiac components from the myocardium, resulting in segmentation inaccuracies and the \"spraying\" phenomenon. To address these issues, we introduce RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of intricate cardiac structures. Our approach enhances global context modeling through multiscale feature aggregation and nested skip connections in the encoder. Transformer layers facilitate capturing intra-slice interactions, while a rotatory attention mechanism handles inter-slice connectivity. A channel-wise cross-attention gate integrates multiscale information and decoder features, effectively bridging semantic gaps. Experimental results across multiple datasets demonstrate superior performance over current methods, achieving near-perfect annotation of coronary arteries and myocardium. Ablation studies confirm that our rotatory attention mechanism significantly improves segmentation accuracy by transforming embedded vectorized patches in semantic dimensional space.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical image segmentation is crucial for disease and tumor detection. While Manual segmentation remains the gold standard in delineating pathological structures, it is labor-intensive, time-consuming, and prone to human error [1]. Automated segmentation is increasingly needed to reduce reliance on expert knowledge and speed up the process. With its intricate structures and fine details, the heart presents significant challenges in this context. Previous studies primarily are binary segmentation tasks using single-labeled dataset [2], [3]. Recent studies opted for multi-class segmentation with two main datasets, MMWHS [4] and ACDC [5], in 2017. Nevertheless, these datasets only annotate basic regions in an unsophisticated way that lack significant details, such as coronary arteries and heart capillaries. More sophisticated-annotated datasets such as ImageCHD [6] with 8 labels (2021) and VHSCDD with 12 labels (2023) challenge SOTA networks. Additionally, SOTA networks, both CNN-based and Transformer-based networks, have not undergone evaluation using the same cardiac datasets, leading to the lack of fair comparison of these networks. In this paper, we conduct experiments with SOTA networks (both CNN-based and Transformer-based approaches) and propose our novel self-designed architecture that proves its superiority.\nThe content of this paper is organized as follows. In Section II, we briefly review existing methods related to our work. Then, we present our proposed solution in Section III. Experiments and result analysis are discussed in Section IV. Finally, the conclusion and implication are in Section V."}, {"title": "II. RELATED WORKS", "content": "Fully Convolutional Neural Networks (FCNs), has become the de facto standard in medical image segmentation [7], [8]. UNet [9] introduced direct skip connections joining feature maps at the same scale to mitigate detail loss in deeper layers. UNet++ [10] further improved upon UNet by incorporating nested skip connections. ResUNet [11] employs ResNet units with atrous convolutions and pyramid pooling to address the semantic gap. However, CNN-based methods struggle with capturing long-range dependencies and global contexts due to their inherited locality [5]. Attention mechanisms, such as those in U-Net Attention [12], attempt to enhance performance by focusing on relevant details and ignoring distractions. Despite these advancements, CNN-based approaches still yield weak performance, particularly with structures exhibiting significant inter-patient variability [1], [5].\nInitially designed for NLP tasks, transformers are known for their Multi-head Self-Attention (MSA) mechanism, which excels at capturing long-range interactions. In computer vision and segmentation, TransUNet [5] utilizes a Transformer encoder for global information learning and CNN decoders for spatial details extraction. Swin-Unet [13], conversely, replaces CNNs with a complete Transformer architecture, employing a shifted window mechanism for detail extraction and patch-expanding layers for upsampling. However, current Transformer-based methods focus self-attention solely on patch interactions and skip connections, processing volumetric data slice by slice and limiting inter-slice information integration. This constraint affects TransUNet's ability to achieve seamless segmentation across adjacent slices.\n3D networks like UNet 3D [14] and VNet [15] preserve inter-slice details but face limitations in GPU memory and computational demands. Thus, we introduce RotCAtt-TransUNet++, a lightweight 2.5D network, to overcome these issues."}, {"title": "III. OUR PROPOSED METHOD", "content": "A. Architecture Overview\nThe architecture diagram can be seen in Figure 1. Through meticulous experimentation and ablation studies, we observed the efficacy of the UNet++ [10] architecture coupled with nested skip connections to preserve crucial information in achieving superior segmentation results. We are also inspired by pyramid pooling at different scales of Zhao et al. [16]. Thus, instead of the conventional CNN-based feature extraction approach, such as ResNet-50 in TransUNet [5], we employ dense downsampling alongside nested skip connections, yielding four distinct feature maps $X_1, X_2, X_3, X_4$ at varying resolutions and depths.\nUnlike TransUNet and its variants, which only embed the last lowest-resolution feature maps, we employ linear embedding for multiscale feature maps. Specifically, the first three feature maps $X_1, X_2, X_3$ undergo linear embedding with a different patch size $p$ to produce different embedded vector $z \\in Z_i \\vert i \\in {1,2,3}$, which simultaneously go through transformer blocks to capture the interactions between patches and rotatory attention mechanisms to aggregate the information from adjacent slices. Within these transformer blocks, comprising $N$ transformer layers, the embedded sequence patches traverse self-attention mechanisms and multilayer perceptrons, facilitating robust intra-slice information capture and yielding new encoded image representations $E_1, E_2, E_3$.\nThe rotatory attention block, conceived to treat the batch size as multiple continuous slices, selectively processes three consecutive slices\u2014designating the first as the left, the second as the target, and the third as the right\u2014culminating in the production of 3 vectors $R_1, R_2, R_3$ encapsulating information from adjacent slices in the volumetric data. Integration of interslice and intra-slice information yields $F_1, F_2, F_3$, which are then reconstructed to their original resolution via upsampling techniques, resulting in $O_1, O_2, O_3$.\nFinally, $X_4$ undergoes concatenation with $O_3$, perpetuating this iterative process until the final segmentation map is obtained after 1 \u00d7 1 convolution.\nsubsectionFeature Extraction with Nested Skip Connections The input is structured as $(B,1, H, W)$, representing the batch size, channels, height, and width. The batch size also represents the number of adjacent slices aggregated in the rotatory attention block. This input undergoes convolution to yield $X_1$, with shape $(B, C, H, W)$, where $C = 64$. The resulting feature maps are downsampled to $X_2$, with dimensions $(B,C \\times 2, \\frac{H}{2}, \\frac{W}{2})$. Then, $X_2$ is upsampled to $(B,C \\times 2, H, W)$ and concatenated with $X_1$ along the $C$ axis, resulting in $(B,C \\times 3, H, W)$. This undergoes further convolution to produce $X^1$, which shares the same shape as $X_1$ but includes aggregated information from $X_1$. This process continues through subsequent lower-resolution images.\nIf we designate the desired number of different-resolution outputs as $D$, we have $X^i \\forall i \\in {1,...,D-1}$ and $\\forall j \\in {1,...,D-i}$, where $X_j^i$ has shape $(B,C \\times 2^{i-1}, \\frac{H}{2^{i-1}}, \\frac{W}{2^{i-1}})$. The $D$-th resolution map has a shape of $(B,C2^{D-2}, \\frac{H}{2^{D-1}}, \\frac{W}{2^{D-1}})$, and bypasses both the Transformer block and Rotatory Attention block but is instead used for the decoder. For $D = 4$, the resulting feature maps are $X_1, X_2, X_3$, simply denoted as $X_i$ for $i \\in 1,2,3$. These are linearly embedded via convolution operations $E$ to produce patches represented as embedded vectors $z \\in Z_i$ where $Z_i$ has shape $(B, n_i, d's)$ and $1 \\leq j \\leq n_i$. The sequence length and feature dimension of $Z_i$ are $n_i = \\frac{H_i}{p} \\frac{W_i}{p}$ and $d_i$, respectively. Ensuring uniformity across $n_i$ for all $i$, we establish $D-1$ patch sizes $p_i = 2^{D-i+1}$, where $i$ ranges from 1 to $D-1$, implying that $p = {2^4,2^3,2^2}$ and the smallest patch size is $2^2 = 4$, given $D = 4$"}, {"title": "B. Linear Embedding and Positional Embedding", "content": "Patch Embedding involves transforming vectorized patches $z \\in Z_i$ into a latent space of $d_i$ dimensions using a trainable linear projection. To preserve the spatial information of the patches, we incorporate position embedding specific to each patch, which is then combined with the patch embeddings.\n$Z_i = E(X_i) + E_{pos}$\n$Z_i = Z'_i + E_{pos}$\n$[z_1^P,..., z_n^P] = [P_1,..., P_n] + [e_1,...,e_n]$\nwhere $E$ is the convolution operation to perform patch embbeding on $X$ and produce $Z'_i$, while $E_{pos} \\in (B, n, d'_s)$ denotes the position embedding, $Z_i$ is the linear embedding projection after adding vectors $e \\in (B,1,d'_s)$ with positional vectors $e_j \\in (B,1,d'_s)$."}, {"title": "C. Transformer Block", "content": "The Transformer encoder consists of $N$ layers of Multihead Self-Attention (MSA) and Multi-Layer Perceptron (MLP) blocks. Therefore the output of the l-th $i \\in N$ layer can be formulated as follows:\n$Z^l = MSA(LN(Z^{l-1})) + Z^{l-1}$\n$Z^{l+1} = MLP(LN(Z^l)) + Z^l$\n$\\check{Z}^{N-1} = MSA(LN(Z^{N-1})) + Z^{N-1}$\n$Z^N = MLP(LN(Z^{N-1})) + Z^{N-1}$ where $LN(\\cdot)$ denotes the layer normalization operator and $Z^l$ is the encoded image representation at scale $i$. In each layer l-th, the encoded image representation $Z_i$ undergoes a self-attention mechanism, enabling encoded patches to learn how to attend to each other. Mathematically, the attention scores $A_l = Attention(Q_i, K_i, V_i)$ for $Z_i$ are computed using scaled dot product as follows:\n$A_l = softmax(\\frac{Q_i K_i^T}{\\sqrt{d}} V_i)$\nwhere $Q_i = W_q(Z_i), K_i = W_k(Z_i), V_i = W_v(Z_i)$ and $Q_i, K_i, V_i \\in (B, n, d/2)$. The Multi-Layer Perceptron (MLP) also contains a fully connected layer of size $d_i \\times 4$ in the middle. The resulting $E_i$ maintains the same shape as $Z_i$, which learns the intra-slice information or the relationship between patches in one 2D image slice."}, {"title": "D. Rotatory Attention Block", "content": "This technique is commonly applied in natural language processing [19], [20], which involves three main inputs: the target phrase, the previous phrase (left context), and the next phrase (right context). This method assumes that adjacent elements contribute significantly to understanding the central/target phrase. In our scenario, if we denote the current encoded input representation as $Z_i \\in (B,n,d'_s)$, we can treat this as a collection of images ${Z_1^k,...,Z_i^k,...,Z_n^k}$ where each $Z_i^k \\in (n, d')$. Thus, three consecutive encoded slices/images can be selected as ${Z_i^{k-1},Z_i^k,Z_i^{k+1} }$ or ${Z^l, Z^t, Z^r}$. For simplicity in notation, we temporarily omit the scale index i:\n$Z^l = [z^l_1,...,z^l_j,...,z^l_n] \\in R^{n \\times d'_f}$\n$Z^t = [z^t_1,...,z^t_j,...,z^t_n] \\in R^{n \\times d'_f}$\n$Z^r = [z^r_1,...,z^r_j,...,z^r_n] \\in R^{n \\times d'_f}$\nThe goal is to derive a single vector $r \\in d'_f$ and integrate it with $Z^t$ to adjust the hidden states or transform the position of each embedded patch $z^t_j$ in semantic dimensional space. $Z^t$ is represented as a single vector $r^t$, incorporating necessary information from the left and right contexts by attention mechanism to filter noise and redundant information. Firstly, a single target representation is formed by:\n$r^t = pooling(z^t_1, z^t_2, ..., z^t_n) = \\frac{1}{n} \\sum_{i=1}^n z^t_i$\nSimilar to the self-attention mechanism in Transformer layers, the key and value are extracted from the left context:\n$K^l = W_k(Z^l) = [k_1,..., k_m] \\in R^{n \\times d'_f}$\n$V^l = W_v(Z^l) = [v_1, ..., v_n] \\in R^{n \\times d'_f}$\nThe $r^t$ is now used as a query to create the context vector out of the left context. The scores are calculated with the activated general score function with tanh activation function, and the attention scores are calculated with the softmax function:\n$S = [s_1,..., s_j, ..., s_m] = tanh(K^l \\cdot r^t + b^l)$\n$a = \\frac{exp(e)}{\\sum_{l=1}^{m} exp(e)}$\nA weighted combination of patch embedding is considered as the component representation for left contexts:\n$r^l = \\sum_{l=1}^{n} a^lv_l$\nIn Figure 2, we denote the above process as Single Attention (SA), which is represented as:\n$K = W_k(Z^t), V = W_v(Z^t)$\n$a = softmax(tanh(Kr + b))$\n$SA(Z, r) = \\begin{cases}\n\\sum_{l=1}^{n} a_lV\n\\end{cases}$\nThe vector $r^l$ is then used as a query to create context out of the target context to integrate information back into the center encoded slice/image to produce $r^{l/t}/r^{r/t} = SA(Z^t, r^l)$. An analogous procedure can be performed to obtain the right-aware target representation $r^r = SA(Z^r,r^t)$ and $r^{r/t} = SA(Z^t, r^r)$. Finally, to obtain the full representation vector r, we perform concatenation: $r^k = concat([r^l, r^r, r^{l/t}, r^{r/t}])$ with $r^k \\in R^{1 \\times d'_f \\times 4}$. This r vector contains the aggregated information between 3 consecutive slices, thus we have $B-2$ vectors $r^k$ with $1 \\leq k \\leq B$. The final vector R is achieved as: $R = W_q(mean(r^k \\vert 1<k<B))$. But this is only one i-th level output; thus, we have $R_i$ output. This interslice-informational vector is added to encoded intraslice-informational $E_i$ to retrieve more optimized vectorized patch embeddings $F_i$."}, {"title": "E. Channel-wise Attention Gate for Feature Fusion", "content": "To fuse features with varied semantics between the Channel Transformer and U-Net decoder effectively, we employ a channel-wise cross-attention module, guiding channel and information filtration of Transformer features, resolving ambiguities with decoder features. Mathematically, we take the i-th level output $F_i$ after Transformer and Rotatory blocks to reconstruct or decode the encoded image representations to get $O_i \\in R^{C \\times H \\times W}$. The reconstructed $O_i$ are taken with i-th level decoder feature map $D_i \\in R^{C \\times H \\times W}$ as the inputs of Channel-wise Cross Attention. Spatial squeeze is performed by a global average pooling (GAP) layer, producing vector $G(X) \\in R^{C \\times 1 \\times 1}$ with its kth channel $G_k(X) = \\frac{1}{H W} \\sum_{i=1}^H\\sum_{j=1}^W X_k(i, j)$. We use this operation to embed the global spatial information and generate the attention mask:\n$M_i = L_1 \\cdot G(O_i) + L_2 \\cdot G(D_i)$ where $L_1 \\in R^{C \\times C}$ and $L_2 \\in R^{C \\times C}$ and weights of two Linear layers and the ReLU operator $\\delta(\\cdot)$, encoding channel-wise dependencies. Following ECA-Net [21], which emphasizes avoiding dimensionality reduction for effective channel attention, we use a single Linear layer and sigmoid function to build the channel attention map, then used to excite $O_i$ to $\\hat{O_i} = \\sigma(M_i) \\cdot O_i$. The activation $\\sigma(M_i)$ indicates the importance of channels. Finally, the masked $\\hat{O_i}$ is concatenated with the up-sampled features of the i-th level decoder."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "A. Datasets and Evaluation\nWe experimented with 5 CNN-based and 3 Transformer-based networks with our own network across 4 datasets: Multi-Modality Whole Heart Segmentation (MMWHS), Synapse multi-organ segmentation dataset, The Image Congenital Heart Diseases (ImageCHD) dataset, and Vietnamese Heart Segmentation and Cardiac Disease Detection (VHSCDD). We used an NVIDIA RTX 4090 1X GPU with 24GB memory and 81.4 TFLOPS for training and testing. We reported three metrics: Dice Coefficient Score (DSC), Intersection over Union (IoU) scores, and Hausdorff Distance (HD).\nB. Implementation Details\nWe implement the Dice score differently: it operates on logits before arg max to maximize confidence scores of predicted pixels per class. At the same time, IoU compares segmentation accuracy between ground truth G and prediction P after arg max predictions. The loss is the reverse of those metrics:\nDice Loss = $1 - \\frac{2\\sum_c \\sum_i\\sum_j P_{cij} \\times G_{cij}}{\\sum_c\\sum_i\\sum_j P_{cij} + G_{cij} + \\epsilon_C} \\forall c \\neq 0$\nIoU Loss =$1- \\frac{\\sum_c \\sum_i \\sum_j (P_{cij} \\times G_{cij})}{\\sum_i\\sum_j (P_{cij} + G_{cij} - P_{cij} \\times G_{cij})}$\\nThe exclusion of $c \\neq 0$ ensures the avoidance of unreal DSC and IoU scores from dominant background pixels. Our combined loss function is defined as:\n$L = a \\times IoU Loss + (1 - a) \\times Dice Loss$\nIn our implementation, we set a to 0.6 because we observed that the IoU loss consistently exceeds the Dice loss. Therefore, we opt to increase the penalty on the model.\nC. Results and Discussion\nThe results are summarized in Table I. The VHSCDD* dataset contains images of size 512x512, while VHSCDD and other datasets have images of size 256x256. Our model's lightweight nature is due to having only four transformer layers. The robustness of rotatory attention allows encoded vectorized patches to be effectively transformed in the semantic space, reducing the need for numerous transformer layers. As shown in Figure 3, our network demonstrates the fastest convergence time when applied to cardiac data, thanks to robust long-range interslice connectivity. However, despite RotCAtt-TransUNet++ outperforming other methods across various datasets and metrics, it is less effective on the Synapses dataset. After dataset analysis, we conclude this may be due to the discontinuous nature of organ structures in this dataset, where the model struggles to aggregate adjacent slice information or the necessary information is distant (exceeds batch size) along the z-axis. Increasing the number of transformer layers, as in TransUNet, would provide only marginal improvement while significantly increasing model parameters/complexity. Therefore, this area remains open for future improvement. We conducted an ablation study on the VHSCDD dataset to compare results with and without the attention mechanism. As shown in Table II, the DSC and IoU scores drop significantly, and in Figure 4, the \"spraying phenomenon\" occurs when no rotatory attention is applied. Our attention map analysis revealed that non-cardiac regions outside the heart exhibit high similarity to patches of the myocardium. Additionally, as illustrated in Figure 5, our method achieves near-perfect segmentation across all classes. In contrast, TransUNet (a Transformer-based approach) and UNet++ Attention (a CNN-based approach) did not perform"}, {"title": "V. CONCLUSION AND IMPLICATION", "content": "In conclusion, Transformer-based methods excel in self-attention, while CNN-based methods are strong in localization. Our study introduces RotCAtt-TransUNet++, featuring nested skip connections for multiscale feature extraction in the encoder, followed by transformer layers, and rotatory attention blocks. This architecture enhances image representation and segmentation accuracy, particularly in complex cardiac datasets. Experimental results show near-perfect annotation of critical structures like coronary arteries and myocardium, with the ablation study confirming the effectiveness of rotatory attention. Future research aims to refine the architecture, and integrate advanced techniques to improve segmentation efficiency and clinical outcomes in cardiovascular diseases."}]}