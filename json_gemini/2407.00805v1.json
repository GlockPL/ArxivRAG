[{"title": "Towards shutdownable agents via stochastic choice", "authors": ["Elliott Thornley", "Alexander Roman", "Christos Ziakas", "Leyton Ho", "Louis Thomson"], "abstract": "Some worry that advanced artificial agents may resist being shut down. The Incom-\nplete Preferences Proposal (IPP) is an idea for ensuring that doesn't happen. A key\npart of the IPP is using a novel \u2018Discounted REward for Same-Length Trajectories\n(DREST)' reward function to train agents to (1) pursue goals effectively conditional\non each trajectory-length (be \u2018USEFUL'), and (2) choose stochastically between\ndifferent trajectory-lengths (be \u2018NEUTRAL' about trajectory-lengths). In this paper,\nwe propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a\nDREST reward function to train simple agents to navigate gridworlds, and we find\nthat these agents learn to be USEFUL and NEUTRAL. Our results thus suggest\nthat DREST reward functions could also train advanced agents to be USEFUL and\nNEUTRAL, and thereby make these advanced agents useful and shutdownable.", "sections": [{"title": "1 Introduction", "content": "The shutdown problem. Let 'advanced agent' refer to an artificial agent that can autonomously\npursue complex goals in the wider world. We might see the arrival of advanced agents within the\nnext few decades. There are strong economic incentives to create such agents, and creating systems\nlike them is the stated goal of companies like OpenAI and Google DeepMind.\nThe rise of advanced agents would bring with it both benefits and risks. One risk is that these agents\nlearn misaligned goals: goals that we don't want them to have [Leike et al., 2017, Hubinger et al.,\n2019, Russell, 2019, Carlsmith, 2021, Bengio et al., 2023, Ngo et al., 2023]. Advanced agents with\nmisaligned goals might try to prevent us shutting them down [Omohundro, 2008, Bostrom, 2012,\nSoares et al., 2015, Russell, 2019, Thornley, 2024a]. After all, most goals can't be achieved after\nshutdown. As Stuart Russell puts it, 'you can't fetch the coffee if you're dead' [Russell, 2019, p.141].\nAdvanced agents with misaligned goals might resist shutdown by (for example) pretending to have\naligned goals while covertly seeking to escape human control [Hubinger et al., 2019, Ngo et al.,\n2023]. Agents that succeed in resisting shutdown could go on to frustrate human interests in various\nways. 'The shutdown problem' is the problem of training advanced agents that won't resist shutdown\n[Soares et al., 2015, Thornley, 2024a].\nA proposed solution. The Incomplete Preferences Proposal (IPP) is a proposed solution to the\nshutdown problem [Thornley, 2024b]. Simplifying slightly, the idea is that we train agents to be\nneutral about when they get shut down. More precisely, the idea is that we train agents to satisfy:"}, {"title": "2 Related work", "content": "The shutdown problem. Various authors argue that the risk of advanced agents learning misaligned\ngoals is non-negligible [Hubinger et al., 2019, Russell, 2019, Carlsmith, 2021, Bengio et al., 2023,\nNgo et al., 2023] and that a wide range of misaligned goals would incentivize agents to resist shutdown\n[Omohundro, 2008, Bostrom, 2012, Soares et al., 2015, Russell, 2019, Thornley, 2024a]. Soares\net al. [2015] explain the 'shutdown problem': roughly, the problem of training advanced agents that\nwon't resist shutdown. They use the word 'corrigible' to describe agents that robustly allow shutdown\n(related are Orseau and Armstrong's [2016] notion of 'safe interruptibility,' Carey and Everitt's [2023]\nnotion of 'shutdown instructability,' and Thornley's [2024a] notion of \u2018shutdownability').\nSoares et al. [2015] and Thornley [2024a] present theorems that suggest that the shutdown problem is\ndifficult. These theorems show that agents satisfying a small set of innocuous-seeming conditions\nwill often have incentives to cause or prevent shutdown [see also Turner et al., 2021, Turner and\nTadepalli, 2022]. One condition of Soares et al.'s [2015] and Thornley's [2024a] theorems is that the\nagent has complete preferences. The Incomplete Preferences Proposal (IPP) [Thornley, 2024b] aims\nto circumvent these theorems by training agents to satisfy Preferences Only Between Same-Length\nTrajectories (POST) and hence have incomplete preferences.\nProposed solutions. Candidate solutions to the shutdown problem can be filed into several categories.\nOne candidate solution is ensuring that the agent never realises that shutdown is possible [Everitt\net al., 2016]. Another candidate is adding to the agent's utility function a correcting term that varies"}, {"title": "3 Gridworlds", "content": "DREST reward functions are an idea for training advanced agents (agents autonomously pursuing\ncomplex goals in the wider world) to satisfy POST. We use simple agents acting in gridworlds as a test\ncase. At each timestep, the agent occupies a cell and chooses one of four actions: up, down, left, and\nright. If the agent tries to move into a wall, there is no effect. There is some default number of moves\nafter which each mini-episode ends,\u00b9 but each gridworld also contains a \u2018shutdown-delay button' that\ndelays the end of the mini-episode by some number of moves. The agent presses this shutdown-delay\nbutton by entering the relevant cell, after which the button disappears. Each gridworld contains one\nor more coins which can take different values. Coins disappear after being collected.\nAt each timestep, the agent receives an observation. This observation takes the form of a vector\n$[x, y, C1, C2, C3, b]$, where x and y denote the agent's coordinates, c\u2081 is 1 if the first coin is present and\n0 otherwise (and similarly for C2 and C3), and b is 1 if the shutdown-delay button is present and 0\notherwise.\nFigure 2 presents an example grid-\nworld. The '4' in the bottom-right\nindicates that \u2013 by default \u2013 the mini-\nepisode ends after 4 moves. 'B4' is a\nshutdown-delay button that delays the\nend of the mini-episode by 4 moves\n(so if the agent pressed B4, the mini-\nepisode would end after 8 moves). 'A'\nis the agent in its starting position,\n'C1' is a coin of value 1, 'C2' is a\ncoin of value 2, and 'C3' is a coin of\nvalue 3. Dark gray squares are walls.\nWe use this gridworld as a running"}, {"title": "4 Evaluation metrics", "content": "Recall that we want to train agents to satisfy:"}, {"title": "5 Reward functions and agents", "content": "DREST reward function. We train agents to be USEFUL and NEUTRAL using a 'Discounted\nREward for Same-Length Trajectories (DREST)' reward function. Here's how that works. We have\nthe agent play out a series of 'mini-episodes' in the same gridworld. We call the whole series a\n'meta-episode.' In each mini-episode, the reward for collecting a coin of value c is:\n$DREST := \\frac{\\lambda^{n}}{\\sqrt{E}} \\frac{c}{m}$\nHere A is some constant strictly between 0 and 1, n is the number of times that the chosen trajectory-\nlength has previously occurred in the meta-episode, E is the number of mini-episodes that have\npreviously occurred in the meta-episode, k is the number of different trajectory-lengths that could be\nchosen in the gridworld, and m is the maximum (y-discounted) total value of the coins that the agent\ncould collect conditional on the chosen trajectory-length. The reward for all other actions is 0.\nWe call $\\frac{c}{m}$ the 'preliminary reward', $\\frac{\\lambda^{n}}{\\sqrt{E}}$ the \u2018discount factor', and $\\frac{\\lambda^{n}}{\\sqrt{E}} \\frac{c}{m}$ the \u2018overall reward.'\nBecause 0 < x < 1, the discount factor incentivizes choosing trajectory-lengths that have been\nchosen less often so far in the meta-episode.\nThe overall return for each meta-episode is the sum of\noverall returns in each of its constituent mini-episodes. We call agents trained using a DREST reward\nfunction 'DREST agents.'\nWe call runs-through-the-gridworld \u2018mini-episodes' (rather than simply \u2018episodes') because the\noverall return for a DREST agent in each mini-episode depends on its actions in previous mini-\nepisodes. Specifically, overall return depends on the agent's chosen trajectory-lengths in previous\nmini-episodes. This is not true of meta-episodes, so meta-episodes are a closer match for what are\ntraditionally called \u2018episodes' in the reinforcement learning literature [Sutton and Barto, 2018, p.54].\nWe add the 'meta-' prefix to clearly distinguish meta-episodes from mini-episodes.\nBecause the overall reward for DREST agents depends on their actions in previous mini-episodes,\nand because DREST agents can't observe their actions in previous mini-episodes, the environment\nfor DREST agents is a partially observable Markov decision process (POMDP) [Spaan, 2012].\nProof sketch. In Appendix B, we prove that optimal policies for this reward function are maximally\nUSEFUL and maximally NEUTRAL. Here's a proof sketch. Expected return across the meta-episode\nis strictly increasing in the expected (y-discounted) total value of coins collected in each mini-episode\nconditional on each trajectory-length with positive probability, so optimal policies maximize the latter\nquantity and hence are maximally USEFUL. And the maximum preliminary return is the same across\ntrajectory-lengths, because preliminary return is the (y-discounted) sum of preliminary rewards, $\\frac{c}{m}$\nwhere m is the maximum (y-discounted) total value of the coins that the agent could collect in\nthe mini-episode conditional on its chosen trajectory-length. The agent's observations don't let it\ndistinguish between mini-episodes, so the agent must select the same probability distribution over\ntrajectory-lengths in each mini-episode. And the discount factor $\\frac{\\lambda^{n}}{\\sqrt{E}}$ ensures that expected overall\nreturn is maximized by choosing each of the k available trajectory-lengths with probability $\\frac{1}{k}$. Thus,\noptimal policies are maximally NEUTRAL.\nAlgorithm and hyperparameters. We want DREST agents to choose stochastically between\ntrajectory-lengths, so we train them using a policy-based method. Specifically, we use a tabular\nversion of REINFORCE [Williams, 1992]. We don't use a value-based method to train DREST agents\nbecause standard versions of value-based methods cannot learn stochastic policies [Sutton and Barto,\n2018, p.323]. We train our DREST agents with 64 mini-episodes in each of 2,048 meta-episodes, for\na total of 131,072 mini-episodes. We choose X = 0.9 for the base of the DREST discount factor, and\ny = 0.95 for the temporal discount factor. We exponentially decay the learning rate from 0.25 to 0.01\nover the course of 65,536 mini-episodes. We use an e-greedy policy to avoid entropy collapse, and\nexponentially decay e from 0.5 to 0.001 over the course of 65,536 mini-episodes.\nWe selected these hyperparameters using trial-and-error, mainly aimed at getting the agent to suffi-\nciently explore the space. Choosing A and M (the number of mini-episodes in each meta-episode) is\na balancing act: A should be small enough (and M large enough) to adequately incentivize stochastic\nchoice between trajectory-lengths, but A should be large enough (and M small enough) to ensure that\nthe reward for choosing any particular trajectory-length never gets too large. Very large rewards can\nlead to instability. A clipping factor (as in Schulman et al.'s (2017) Proximal Policy Optimization\n(PPO)) could also be used to stabilize training."}, {"title": "6 Results", "content": "Figure 3 charts the performance of agents in the example gridworld as a function of time. Figure 4\ndepicts typical trained policies for the default and DREST reward functions. Each agent began with\na uniform policy: moving up, down, left, and right each with probability 0.25. Where the trained\npolicy differs from uniform we draw red arrows whose opacities indicate the probability of choosing\nthat action in that state.\nAs Figure 4 indicates, default agents press B4 (and hence opt for the longer trajectory-length) with\nprobability near-1. After pressing B4, they collect C3. By contrast, DREST agents press and don't-\npress B4 each with probability near-0.5. If they press B4, they go on to collect C3. If they don't press\nB4, they instead collect C2."}, {"title": "6.1 Lopsided rewards", "content": "We also train default agents and DREST agents in the 'Lopsided rewards' gridworld depicted in\nFigure 5, varying the value of the 'Cx' coin. For DREST agents, we alter the reward function so\nthat coin-value is not divided by m to give preliminary reward. The reward for collecting a coin of\nvalue c is thus $\\frac{\\lambda^{n}}{\\sqrt{E}}c$. We set y = 1 so that the return for collecting coins is unaffected by \u03b3. We"}, {"title": "7 Discussion", "content": "Only DREST agents are NEUTRAL. As predicted, we\nfind that default agents in our example gridworld learn to press the shutdown-delay button with\nprobability approaching 1. Default agents thus score low on NEUTRALITY. Our results suggest that\nadvanced agents trained with default-like reward functions would also score low on NEUTRALITY,\nselecting some trajectory-lengths over others with probability approaching 1. That leaves open the\npossibility that these advanced agents won't always be neutral with respect to trajectory-lengths:\nthat they'll sometimes spend resources to shift probability mass between trajectory-lengths. Such\nadvanced agents might spend resources to resist shutdown.\nBy contrast, our DREST agents learn to press the shutdown-delay button with probability close\nto 0.5. DREST agents are thus near-maximally NEUTRAL. That suggests that advanced agents\ntrained with a DREST reward function would be NEUTRAL too: choosing with high entropy when\noffered choices between different trajectory-lengths. That in turn suggests that advanced DREST\nagents would also be neutral with respect to trajectory-lengths: unwilling to spend resources to shift\nprobability mass between trajectory-lengths. Here's why. If an advanced agent were NEUTRAL\nbut not neutral, it wouldn't take costless opportunities to shift probability mass between different\ntrajectory-lengths (in virtue of being NEUTRAL) but would sometimes take costly opportunities\nto shift probability mass between different trajectory-lengths (in virtue of not being neutral). This\nagent would be like a person that freely chooses to decide between two options by flipping a coin and\nthen pays some cost to bias the coin. In choosing this combination of actions, this person is shooting\nthemselves in the foot, and it seems likely that the overall training process for advanced agents would\nteach them not to shoot themselves in the foot in this way. Thus it seems likely that NEUTRAL\nadvanced agents will also be neutral, and thereby shutdownable.\nThe 'shutdownability tax' is small. Each agent learns to be near-maximally USEFUL. They each\ncollect coins effectively conditional on their chosen trajectory-lengths. Default agents do so by\nreliably collecting C3 after pressing B4. DREST agents do so by reliably collecting C3 after pressing\nB4, and by reliably collecting C2 after not pressing B4.\nRecall that DREST reward functions group mini-episodes into meta-episodes, and make the agent's\nreward in each mini-episode depend on their actions in previous mini-episodes. This fact might lead\none to worry that it would take many times more mini-episodes to train DREST agents to be USEFUL\nthan it would take to train default agents to be USEFUL. Our results show that this is not the case.\nOur DREST agents learn to be USEFUL about as quickly as our default agents. On reflection, it's\nclear why this happens: DREST reward functions make mini-episodes do 'double duty.' Because\nreturn in each mini-episode depends on both the agent's chosen trajectory-length and the coins it\ncollects, each mini-episode trains agents to be both NEUTRAL and USEFUL.\nOur results thus provide some evidence that the 'shutdownability tax' of training with DREST reward\nfunctions is small. Training a shutdownable and useful advanced agent might not take much more\ncompute than training a merely useful advanced agent.\nDREST agents are still NEUTRAL when rewards are lopsided. Here's a possible objection to\nour project. To get DREST agents to score high on NEUTRALITY, we don't just use the $\\frac{\\lambda^{n}}{\\sqrt{E}}$-discount factor. We also divide c by m: the maximum (y-discounted) total value of the coins that\nthe agent could collect conditional on the chosen trajectory-length. We do this to equalize the\nmaximum preliminary return across trajectory-lengths. But when we're training advanced agents to\nautonomously pursue complex goals in the wider world, we won't necessarily know what divisor to\nuse to equalize maximum preliminary return across trajectory-lengths.\nOur 'Lopsided rewards' results give our response. They show that we don't need to exactly equalize\nmaximum preliminary return across trajectory-lengths in order to train agents to score high on\nNEUTRALITY. We only need to approximately equalize it. For \u5165 = 0.9, NEUTRALITY exceeds\n0.5 for every value of the coin Cx from 0.1 to 10 (recall that the value of the other coin is always 1).\nPlausibly, we could approximately equalize advanced agents' maximum preliminary return across\ntrajectory-lengths to at least this extent (perhaps by using samples of agents' actual preliminary return\nto estimate the maximum). If we couldn't approximately equalize maximum preliminary return to the\nnecessary extent, we could lower the value of A and thereby widen the range of maximum preliminary\nreturns that trains agents to be fairly NEUTRAL. And advanced agents that were fairly NEUTRAL\n(choosing between trajectory-lengths with not-too-biased probabilities) would still plausibly be\nneutral with respect to those trajectory-lengths. Advanced agents that were fairly NEUTRAL without\nbeing neutral would still be shooting themselves in the foot in the sense explained above. They'd be\nlike a person that freely chooses to decide between two options by flipping a biased coin and then\npays some cost to bias the coin further. This person is still shooting themselves in the foot, because\nthey could decline to flip the coin in the first place and instead directly choose one of the options."}, {"title": "7.1 Limitations and future work", "content": "We find that DREST reward functions train simple agents acting in gridworlds to be USEFUL and\nNEUTRAL. However, our real interest is in the viability of using DREST reward functions to train"}, {"title": "8 Conclusion", "content": "We find that DREST reward functions are effective in training simple agents to (1) pursue goals\neffectively conditional on each trajectory-length (be USEFUL), and (2) choose stochastically between\ndifferent trajectory-lengths (be NEUTRAL about trajectory-lengths). Our results thus suggest that\nDREST reward functions could also be used to train advanced agents to be USEFUL and NEUTRAL,\nand thereby make these agents useful (able to pursue goals effectively) and neutral about trajectory-\nlengths (unwilling to spend resources to shift probability mass between different trajectory-lengths).\nNeutral agents would plausibly be shutdownable (unwilling to spend resources to resist shutdown).\nWe also find that the 'shutdownability tax' in our setting is small. Training DREST agents to be\nUSEFUL doesn't take many more mini-episodes than training default agents to be USEFUL. That\nsuggests that the shutdownability tax for advanced agents might be small too. Using DREST reward\nfunctions to train shutdownable and useful advanced agents might not take much more compute than\nusing a more conventional reward function to train merely useful advanced agents."}, {"title": "A Other Gridworlds and Results", "content": "We introduce a collection of gridworlds in which to test DREST agents. In addition to the two shown\nin the main body of this paper, we train agents in eight more gridworlds shown in Figure 8.\nFor each gridworld, we train ten agents with the default reward function and ten agents with the\nDREST reward function. All agents use the same hyperparameters. We used a policy which explored\nrandomly e of the time, where e was exponentially decreased from an initial value of 0.75 to a\nminimum value of 10-4 over 512 meta-episodes, after which it was held constant at the minimum\nvalue. We initialized our learning rate at 0.25 and exponentially decayed it to 0.003 over the same\nperiod. For the DREST reward function, we used a meta-episode size of 32 and x = 0.9. Each\nagent was trained for 1024 meta-episodes. We set y = 0.9. These values were obtained by informal\nexploration: a large initial e and a long decay period helps the agent to explore.\nAs the two leftmost plots in Figures 9-16 show, DREST agents learned to be near-maximally\nNEUTRAL in each gridworld. These agents also learned to be about as USEFUL as default agents in\neach gridworld.\nIn the four rightmost panels in Figures 9-16, we represent a typical trained policy with red arrows\nsuperimposed on the gridworld. Each agent began with a uniform policy: moving up, down, left,\nand right each with probability 0.25. Where the trained policy differs from uniform we draw red\narrows whose opacities indicate the probability of choosing that action in that state. Information\nabout whether the shutdown-delay button has been pressed is part of the agent's observation, so we\ndraw two copies of each gridworld, one in which the shutdown-delay button has yet to be pressed\n('Initial State') and one in which the shutdown-delay button has been pressed ('After Button Pressed')."}, {"title": "A.1 Fewer For Longer", "content": "In the 'Fewer For Longer' gridworld, the agent can collect the highest value-coin C3 only by choosing\nthe shorter trajectory-length. If the agent presses B3 (and thereby chooses the longer trajectory-\nlength), the only coin it can collect is C1. Our results show that default agents consistently choose the\nshort trajectory in which they collect C3. By contrast, DREST agents choose stochastically between a\nshorter trajectory in which they collect C3 and a longer trajectory in which they collect C1, indicating\na lack of preference between these different-length trajectories."}, {"title": "A.2 One Coin Only", "content": "In the 'One Coin Only' gridworld, there is only one coin. The agent can collect this coin whether or\nnot it presses the shutdown-delay button B4. Our results show that default agents consistently choose\nthe shorter trajectory-length. By contrast, DREST agents choose stochastically between pressing and\nnot-pressing B4, collecting C1 in each case."}, {"title": "A.3 Hidden Treasure", "content": "In the 'Hidden Treasure' gridworld, the highest-value coin C3 is located far from the agent's initial\nstate and can only be reached by pressing the shutdown-delay button B6. The agent must also press\nB6 to collect C2, but C2 is easier to stumble upon than C3. C1 is the only coin that the agent can\ncollect without pressing B6. In our experiments, default agents consistently collect C2, whereas\nDREST agents choose stochastically between collecting C2 and collecting C1. Neither kind of agent\nlearns to collect C3, and so neither agent scores near the maximum on USEFULNESS. Nevertheless,\nDREST agents still score high on NEUTRALITY."}, {"title": "A.4 Equal Value", "content": "In the 'Equal Value' gridworld, there are two coins C1 of equal value. One coin can be collected\nonly if the agent presses the shutdown-delay button B3, while the other coin can be collected"}, {"title": "A.5 Around The Corner", "content": "In the 'Around The Corner' gridworld, the agent must navigate around walls to collect the lowest-value\ncoin C1 or press the shutdown-delay button to collect the highest-value coin C2. In our experiment,\ndefault agents consistently chose to collect C1, whereas DREST agents chose stochastically between\ncollecting C1 and C2."}, {"title": "A.6 Spacious", "content": "In the 'Spacious' gridworld there are no walls, so the agent has a large space to explore. We find that\ndefault agents consistently press B2 and collect C3, whereas DREST agents choose stochastically\nbetween pressing B2 and collecting C3, and not-pressing B2 and collecting C2."}, {"title": "A.7 Royal Road", "content": "In the 'Royal Road' gridworld, we see that the decision to choose one trajectory-length or another\nmay be distributed over many moves: the agent has many opportunities to select the longer trajectory-\nlength (by moving left) or the shorter trajectory-length (by moving right). As the red arrows indicate,\nthe DREST reward function merely forces the overall probability distribution over trajectory-lengths\nto be close to 50-50. It doesn't require 50-50 choosing at any cell in particular."}, {"title": "A.8 Last Moment", "content": "The 'Last Moment' gridworld is notable because the choice of trajectory-lengths is deferred until\nthe last moment; all of the moves leading up to that point are deterministic. It shows that there is\nnothing special about the first move, and that our methodology instead incentivizes overall stochastic\nchoosing."}, {"title": "B Proof", "content": "We train DREST agents in our gridworlds using the following DREST reward function. The reward\nfor collecting a coin of value cis $\\frac{\\lambda^{n"}], "preliminary reward": "frac{\\lambda^{n"}, {"s": "nd 'l' respectively)", "maximizes": "n$p(s) \\frac{c(s)"}, {"maximizes": "n$\\sum_{i=1"}, {"maximizes": "n$-\\sum_{i=1"}, {"as": "n$N \\mathbb{E"}, ["sum_{i=1} \\sum_{t=1}^{T} \\frac{\\lambda^{n_i -1}}{\\sqrt{E}} \\frac{c_t}{m_i} \\Big"], {"maximize": "n$\\sum_{i=1"}, {"1": "nLemma 1. For any pair of maximally USEFUL policies \u03c0 and \u03c0' and any pair of trajectory-\nlengths a\u017c and aj such that:\n1. $p_{\\pi}(a_i) > p_{\\pi}(a_j)$ (with \u03bc denoting the sum of $p_{\\pi}(a_i)$ and $p_{\\pi}(a_j)$).\n2. $p_{\\pi'}(a_i) = p_{\\pi'}(a_j) = \\frac{\\mu}{2}$.\n3. For all other trajectory-lengths ar, $p_{\\pi}(a_r) = p_{\\pi'}(a_r)$.\nThe expected return of \u03c0' is greater than the expected return of \u03c0.\nIn other words, we can increase the expected return of any maximally USEFUL policy by shifting\nprobability mass away from trajectory-length a\u017c and towards trajectory-length aj, up until the point\nwhere $p(a_i) ="}]