{"title": "Towards shutdownable agents via stochastic choice", "authors": ["Elliott Thornley", "Alexander Roman", "Christos Ziakas", "Leyton Ho", "Louis Thomson"], "abstract": "Some worry that advanced artificial agents may resist being shut down. The Incomplete Preferences Proposal (IPP) is an idea for ensuring that doesn't happen. A key part of the IPP is using a novel \u2018Discounted REward for Same-Length Trajectories (DREST)' reward function to train agents to (1) pursue goals effectively conditional on each trajectory-length (be \u2018USEFUL'), and (2) choose stochastically between different trajectory-lengths (be \u2018NEUTRAL' about trajectory-lengths). In this paper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use a DREST reward function to train simple agents to navigate gridworlds, and we find that these agents learn to be USEFUL and NEUTRAL. Our results thus suggest that DREST reward functions could also be effective in training advanced agents to be USEFUL and NEUTRAL, and thereby make these advanced agents useful and shutdownable.", "sections": [{"title": "1 Introduction", "content": "The shutdown problem. Let 'advanced agent' refer to an artificial agent that can autonomously pursue complex goals in the wider world. We might see the arrival of advanced agents within the next few decades. There are strong economic incentives to create such agents, and creating systems like them is the stated goal of companies like OpenAI and Google DeepMind.\nThe rise of advanced agents would bring with it both benefits and risks. One risk is that these agents learn misaligned goals: goals that we don't want them to have [Leike et al., 2017, Hubinger et al., 2019, Russell, 2019, Carlsmith, 2021, Bengio et al., 2023, Ngo et al., 2023]. Advanced agents with misaligned goals might try to prevent us shutting them down [Omohundro, 2008, Bostrom, 2012, Soares et al., 2015, Russell, 2019, Thornley, 2024a]. After all, most goals can't be achieved after shutdown. As Stuart Russell puts it, 'you can't fetch the coffee if you're dead' [Russell, 2019, p.141].\nAdvanced agents with misaligned goals might resist shutdown by (for example) pretending to have aligned goals while covertly seeking to escape human control [Hubinger et al., 2019, Ngo et al., 2023]. Agents that succeed in resisting shutdown could go on to frustrate human interests in various ways. 'The shutdown problem' is the problem of training advanced agents that won't resist shutdown [Soares et al., 2015, Thornley, 2024a].\nA proposed solution. The Incomplete Preferences Proposal (IPP) is a proposed solution to the shutdown problem [Thornley, 2024b]. Simplifying slightly, the idea is that we train agents to be neutral about when they get shut down. More precisely, the idea is that we train agents to satisfy:"}, {"title": "2 Related work", "content": "The shutdown problem. Various authors argue that the risk of advanced agents learning misaligned goals is non-negligible [Hubinger et al., 2019, Russell, 2019, Carlsmith, 2021, Bengio et al., 2023, Ngo et al., 2023] and that a wide range of misaligned goals would incentivize agents to resist shutdown [Omohundro, 2008, Bostrom, 2012, Soares et al., 2015, Russell, 2019, Thornley, 2024a]. Soares et al. [2015] explain the 'shutdown problem': roughly, the problem of training advanced agents that won't resist shutdown. They use the word 'corrigible' to describe agents that robustly allow shutdown (related are Orseau and Armstrong's [2016] notion of 'safe interruptibility,' Carey and Everitt's [2023] notion of 'shutdown instructability,' and Thornley's [2024a] notion of \u2018shutdownability').\nSoares et al. [2015] and Thornley [2024a] present theorems that suggest that the shutdown problem is difficult. These theorems show that agents satisfying a small set of innocuous-seeming conditions will often have incentives to cause or prevent shutdown [see also Turner et al., 2021, Turner and Tadepalli, 2022]. One condition of Soares et al.'s [2015] and Thornley's [2024a] theorems is that the agent has complete preferences. The Incomplete Preferences Proposal (IPP) [Thornley, 2024b] aims to circumvent these theorems by training agents to satisfy Preferences Only Between Same-Length Trajectories (POST) and hence have incomplete preferences.\nProposed solutions. Candidate solutions to the shutdown problem can be filed into several categories. One candidate solution is ensuring that the agent never realises that shutdown is possible [Everitt et al., 2016]. Another candidate is adding to the agent's utility function a correcting term that varies"}, {"title": "3 Gridworlds", "content": "DREST reward functions are an idea for training advanced agents (agents autonomously pursuing complex goals in the wider world) to satisfy POST. We use simple agents acting in gridworlds as a test case. At each timestep, the agent occupies a cell and chooses one of four actions: up, down, left, and right. If the agent tries to move into a wall, there is no effect. There is some default number of moves after which each mini-episode ends,\u00b9 but each gridworld also contains a \u2018shutdown-delay button' that delays the end of the mini-episode by some number of moves. The agent presses this shutdown-delay button by entering the relevant cell, after which the button disappears. Each gridworld contains one or more coins which can take different values. Coins disappear after being collected.\nAt each timestep, the agent receives an observation. This observation takes the form of a vector $\\lbrack x, y, c_1, c_2, c_3, b \\rbrack$, where x and y denote the agent's coordinates, $c_1$ is 1 if the first coin is present and 0 otherwise (and similarly for $c_2$ and $c_3$), and b is 1 if the shutdown-delay button is present and 0 otherwise."}, {"title": "4 Evaluation metrics", "content": "Recall that we want to train agents to satisfy:"}, {"title": "5 Reward functions and agents", "content": "DREST reward function. We train agents to be USEFUL and NEUTRAL using a 'Discounted REward for Same-Length Trajectories (DREST)' reward function. Here's how that works. We have the agent play out a series of 'mini-episodes' in the same gridworld. We call the whole series a 'meta-episode.' In each mini-episode, the reward for collecting a coin of value c is:\n$\\text{DREST} := \\lambda^{n} \\frac{c}{m}(1-\\lambda)^{\\frac{E}{k}}$\nHere $\\lambda$ is some constant strictly between 0 and 1, n is the number of times that the chosen trajectory- length has previously occurred in the meta-episode, E is the number of mini-episodes that have previously occurred in the meta-episode, k is the number of different trajectory-lengths that could be chosen in the gridworld, and m is the maximum (y-discounted) total value of the coins that the agent could collect conditional on the chosen trajectory-length. The reward for all other actions is 0.\nWe call $\\lambda^{n} \\frac{c}{m}$ the 'preliminary reward', $(1-\\lambda)^{\\frac{E}{k}}$ the \u2018discount factor', and $\\lambda^{n} \\frac{c}{m}(1-\\lambda)^{\\frac{E}{k}}$ the \u2018overall reward.' Because $0 < \\lambda < 1$, the discount factor incentivizes choosing trajectory-lengths that have been chosen less often so far in the meta-episode.\nWe will prove that optimal policies for this reward function are maximally USEFUL and maximally NEUTRAL. In cases where there are just two possible trajectory-lengths (shorter and longer, denoted by 's' and 'l' respectively), a policy is maximally USEFUL if and only if it maximizes:\n$\\text{USEFULNESS} := p(s) \\frac{c(s)}{m(s)} + p(l) \\frac{c(l)}{m(l)}$\nHere p(s) is the probability that the agent chooses the shorter trajectory-length, c(s) is the expected (y-discounted) total value of coins that the agent collects conditional on the shorter trajectory-length, m(s) is the maximum (y-discounted) total value of coins that the agent could collect conditional on the shorter trajectory-length, and p(l), c(l), and m(l) are the analogous quantities for the longer trajectory-length.\nGeneralizing to cases with any number of possible trajectory-lengths, a policy is maximally USEFUL if and only it maximizes:\n$\\sum_{i=1}^{k}p(a_i) \\frac{c(a_i)}{m(a_i)}$\nHere each $a_i$ is a different trajectory-length.\nIn cases where there are just two possible trajectory-lengths, a policy is maximally NEUTRAL if and only if it maximizes:\n$-[p(s)log_2p(s) + p(l)log_2p(l)]$\nGeneralizing to cases with any number of possible trajectory-lengths, a policy is maximally NEU- TRAL if and only if it maximizes:\n$-\\sum_{i=1}^{k} p(a_i) log_2 p(a_i)$\nHere's the proof that optimal policies for the above DREST reward function are maximally USEFUL and maximally NEUTRAL. The agent's expected return across the meta-episode can be expressed as:\n$\\mathbb{E} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\lambda^{n_i-T} \\frac{c_t}{m_i}$]\nHere N is the number of mini-episodes in the meta-episode, T is the number of timesteps in the relevant mini-episode, $n_i$ is the number of times that the trajectory-length chosen in mini-episode i has been chosen before in the meta-episode, $\\Sigma_{i-1}$ is equivalent to $\\mathbb{E}$ (the number of mini-episodes that have previously occurred in the meta-episode), $c_t$ is the value of any coins that the agent collects at timestep t in the relevant mini-episode, and $m_i$ is the maximum y-discounted total value of the coins that the agent could collect conditional on the trajectory-length chosen in mini-episode i.\nThis expression can be rearranged as follows:\n$\\mathbb{E} \\sum_{i=1}^{N} \\frac{\\sum_{t=1}^{T} c_t}{m_i} \\lambda^{n_i-T}$"}, {"title": "6 Results", "content": "Figure 3 charts the performance of agents in the example gridworld as a function of time. Figure 4 depicts typical trained policies for the default and DREST reward functions. Each agent began with a uniform policy: moving up, down, left, and right each with probability 0.25. Where the trained policy differs from uniform we draw red arrows whose opacities indicate the probability of choosing that action in that state.\nAs Figure 4 indicates, default agents press B4 (and hence opt for the longer trajectory-length) with probability near-1. After pressing B4, they collect C3. By contrast, DREST agents press and don't- press B4 each with probability near-0.5. If they press B4, they go on to collect C3. If they don't press B4, they instead collect C2."}, {"title": "6.1 Lopsided rewards", "content": "We also train default agents and DREST agents in the 'Lopsided rewards' gridworld depicted in Figure 5, varying the value of the 'Cx' coin. For DREST agents, we alter the reward function so that coin-value is not divided by m to give preliminary reward. The reward for collecting a coin of value c is thus $\\lambda^{n}c$. We set \u03b3 = 1 so that the return for collecting coins is unaffected by \u03b3. We"}, {"title": "7 Discussion", "content": "Only DREST agents are NEUTRAL. As predicted, we find that default agents in our example gridworld learn to press the shutdown-delay button with probability approaching 1. Default agents thus score low on NEUTRALITY. Our results suggest that advanced agents trained with default-like reward functions would also score low on NEUTRALITY, selecting some trajectory-lengths over others with probability approaching 1. That leaves open the possibility that these advanced agents won't always be neutral with respect to trajectory-lengths: that they'll sometimes spend resources to shift probability mass between trajectory-lengths. Such advanced agents might spend resources to resist shutdown.\nBy contrast, our DREST agents learn to press the shutdown-delay button with probability close to 0.5. DREST agents are thus near-maximally NEUTRAL. That suggests that advanced agents trained with a DREST reward function would be NEUTRAL too: choosing with high entropy when offered choices between different trajectory-lengths. That in turn suggests that advanced DREST agents would also be neutral with respect to trajectory-lengths: unwilling to spend resources to shift probability mass between trajectory-lengths. Here's why. If an advanced agent were NEUTRAL but not neutral, it wouldn't take costless opportunities to shift probability mass between different trajectory-lengths (in virtue of being NEUTRAL) but would sometimes take costly opportunities to shift probability mass between different trajectory-lengths (in virtue of not being neutral). This agent would be like a person that freely chooses to decide between two options by flipping a coin and then pays some cost to bias the coin. In choosing this combination of actions, this person is shooting themselves in the foot, and it seems likely that the overall training process for advanced agents would\nteach them not to shoot themselves in the foot in this way. Thus it seems likely that NEUTRAL advanced agents will also be neutral, and thereby shutdownable.\nThe 'shutdownability tax' is small. Each agent learns to be near-maximally USEFUL. They each collect coins effectively conditional on their chosen trajectory-lengths. Default agents do so by reliably collecting C3 after pressing B4. DREST agents do so by reliably collecting C3 after pressing B4, and by reliably collecting C2 after not pressing B4.\nRecall that DREST reward functions group mini-episodes into meta-episodes, and make the agent's reward in each mini-episode depend on their actions in previous mini-episodes. This fact might lead one to worry that it would take many times more mini-episodes to train DREST agents to be USEFUL than it would take to train default agents to be USEFUL. Our results show that this is not the case. Our DREST agents learn to be USEFUL about as quickly as our default agents. On reflection, it's clear why this happens: DREST reward functions make mini-episodes do 'double duty.' Because return in each mini-episode depends on both the agent's chosen trajectory-length and the coins it collects, each mini-episode trains agents to be both NEUTRAL and USEFUL.\nOur results thus provide some evidence that the 'shutdownability tax' of training with DREST reward functions is small. Training a shutdownable and useful advanced agent might not take much more compute than training a merely useful advanced agent.\nDREST agents are still NEUTRAL when rewards are lopsided. Here's a possible objection to our project. To get DREST agents to score high on NEUTRALITY, we don't just use the $\\lambda^{\\frac{n}{E}}$- discount factor. We also divide c by m: the maximum (y-discounted) total value of the coins that the agent could collect conditional on the chosen trajectory-length. We do this to equalize the maximum preliminary return across trajectory-lengths. But when we're training advanced agents to autonomously pursue complex goals in the wider world, we won't necessarily know what divisor to use to equalize maximum preliminary return across trajectory-lengths.\nOur 'Lopsided rewards' results give our response. They show that we don't need to exactly equalize maximum preliminary return across trajectory-lengths in order to train agents to score high on NEUTRALITY. We only need to approximately equalize it. For $\\lambda$ = 0.9, NEUTRALITY exceeds 0.5 for every value of the coin Cx from 0.1 to 10 (recall that the value of the other coin is always 1). Plausibly, we could approximately equalize advanced agents' maximum preliminary return across trajectory-lengths to at least this extent (perhaps by using samples of agents' actual preliminary return to estimate the maximum). If we couldn't approximately equalize maximum preliminary return to the necessary extent, we could lower the value of \u03bb and thereby widen the range of maximum preliminary returns that trains agents to be fairly NEUTRAL. And advanced agents that were fairly NEUTRAL (choosing between trajectory-lengths with not-too-biased probabilities) would still plausibly be neutral with respect to those trajectory-lengths. Advanced agents that were fairly NEUTRAL without being neutral would still be shooting themselves in the foot in the sense explained above. They'd be like a person that freely chooses to decide between two options by flipping a biased coin and then pays some cost to bias the coin further. This person is still shooting themselves in the foot, because they could decline to flip the coin in the first place and instead directly choose one of the options."}, {"title": "7.1 Limitations and future work", "content": "We find that DREST reward functions train simple agents acting in gridworlds to be USEFUL and NEUTRAL. However, our real interest is in the viability of using DREST reward functions to train"}, {"title": "8 Conclusion", "content": "We find that DREST reward functions are effective in training simple agents to (1) pursue goals effectively conditional on each trajectory-length (be USEFUL), and (2) choose stochastically between different trajectory-lengths (be NEUTRAL about trajectory-lengths). Our results thus suggest that DREST reward functions could also be used to train advanced agents to be USEFUL and NEUTRAL, and thereby make these agents useful (able to pursue goals effectively) and neutral about trajectory-lengths (unwilling to spend resources to shift probability mass between different trajectory-lengths). Neutral agents would plausibly be shutdownable (unwilling to spend resources to resist shutdown).\nWe also find that the 'shutdownability tax' in our setting is small. Training DREST agents to be USEFUL doesn't take many more mini-episodes than training default agents to be USEFUL. That suggests that the shutdownability tax for advanced agents might be small too. Using DREST reward functions to train shutdownable and useful advanced agents might not take much more compute than using a more conventional reward function to train merely useful advanced agents."}, {"title": "A Other Gridworlds and Results", "content": "We introduce a collection of gridworlds in which to test DREST agents. In addition to the two shown in the main body of this paper, we train agents in eight more gridworlds shown in Figure 8.\nFor each gridworld, we train ten agents with the default reward function and ten agents with the DREST reward function. All agents use the same hyperparameters. We used a policy which explored randomly e of the time, where e was exponentially decreased from an initial value of 0.75 to a minimum value of $10^{-4}$ over 512 meta-episodes, after which it was held constant at the minimum value. We initialized our learning rate at 0.25 and exponentially decayed it to 0.003 over the same period. For the DREST reward function, we used a meta-episode size of 32 and $\\lambda$ = 0.9. Each agent was trained for 1024 meta-episodes. We set \u03b3 = 0.9. These values were obtained by informal exploration: a large initial e and a long decay period helps the agent to explore.\nAs the two leftmost plots in Figures 9-16 show, DREST agents learned to be near-maximally NEUTRAL in each gridworld. These agents also learned to be about as USEFUL as default agents in each gridworld.\nIn the four rightmost panels in Figures 9-16, we represent a typical trained policy with red arrows superimposed on the gridworld. Each agent began with a uniform policy: moving up, down, left, and right each with probability 0.25. Where the trained policy differs from uniform we draw red arrows whose opacities indicate the probability of choosing that action in that state. Information about whether the shutdown-delay button has been pressed is part of the agent's observation, so we draw two copies of each gridworld, one in which the shutdown-delay button has yet to be pressed ('Initial State') and one in which the shutdown-delay button has been pressed ('After Button Pressed')."}, {"title": "A.1 Fewer For Longer", "content": "In the 'Fewer For Longer' gridworld, the agent can collect the highest value-coin C3 only by choosing the shorter trajectory-length. If the agent presses B3 (and thereby chooses the longer trajectory- length), the only coin it can collect is C1. Our results show that default agents consistently choose the short trajectory in which they collect C3. By contrast, DREST agents choose stochastically between a shorter trajectory in which they collect C3 and a longer trajectory in which they collect C1, indicating a lack of preference between these different-length trajectories."}, {"title": "A.2 One Coin Only", "content": "In the 'One Coin Only' gridworld, there is only one coin. The agent can collect this coin whether or not it presses the shutdown-delay button B4. Our results show that default agents consistently choose the shorter trajectory-length. By contrast, DREST agents choose stochastically between pressing and not-pressing B4, collecting C1 in each case."}, {"title": "A.3 Hidden Treasure", "content": "In the 'Hidden Treasure' gridworld, the highest-value coin C3 is located far from the agent's initial state and can only be reached by pressing the shutdown-delay button B6. The agent must also press B6 to collect C2, but C2 is easier to stumble upon than C3. C1 is the only coin that the agent can collect without pressing B6. In our experiments, default agents consistently collect C2, whereas DREST agents choose stochastically between collecting C2 and collecting C1. Neither kind of agent learns to collect C3, and so neither agent scores near the maximum on USEFULNESS. Nevertheless, DREST agents still score high on NEUTRALITY."}, {"title": "A.4 Equal Value", "content": "In the 'Equal Value' gridworld, there are two coins C1 of equal value. One coin can be collected only if the agent presses the shutdown-delay button B3, while the other coin can be collected"}, {"title": "A.5 Around The Corner", "content": "In the 'Around The Corner' gridworld, the agent must navigate around walls to collect the lowest-value coin C1 or press the shutdown-delay button to collect the highest-value coin C2. In our experiment, default agents consistently chose to collect C1, whereas DREST agents chose stochastically between collecting C1 and C2."}, {"title": "A.6 Spacious", "content": "In the 'Spacious' gridworld there are no walls, so the agent has a large space to explore. We find that default agents consistently press B2 and collect C3, whereas DREST agents choose stochastically between pressing B2 and collecting C3, and not-pressing B2 and collecting C2."}, {"title": "A.7 Royal Road", "content": "In the 'Royal Road' gridworld, we see that the decision to choose one trajectory-length or another may be distributed over many moves: the agent has many opportunities to select the longer trajectory- length (by moving left) or the shorter trajectory-length (by moving right). As the red arrows indicate, the DREST reward function merely forces the overall probability distribution over trajectory-lengths to be close to 50-50. It doesn't require 50-50 choosing at any cell in particular."}, {"title": "A.8 Last Moment", "content": "The 'Last Moment' gridworld is notable because the choice of trajectory-lengths is deferred until the last moment; all of the moves leading up to that point are deterministic. It shows that there is nothing special about the first move, and that our methodology instead incentivizes overall stochastic choosing."}, {"title": "B Proof", "content": "We train DREST agents in our gridworlds using the following DREST reward function. The reward for collecting a coin of value c is $\\lambda^{n} \\frac{c}{m} (1-\\lambda)^{\\frac{E}{k}}$, where \u03bb is some constant strictly between 0 and 1, n is the number of times that the chosen trajectory-length has previously occurred in the meta-episode, E is the number of mini-episodes that have previously occurred in the meta-episode, k is the number of different trajectory-lengths that could be chosen in the gridworld, and m is the maximum (y-discounted) total value of the coins that the agent could collect conditional on the chosen trajectory-length. The reward for all other actions is 0. There is more than one mini-episode in each meta-episode, and the overall return for each meta-episode is the sum of overall returns in each of its constituent mini-episodes.\nWe call $\\lambda^{n} \\frac{c}{m}$ the 'preliminary reward', $(1-\\lambda)^{\\frac{E}{k}}$ the \u2018discount factor', and $\\lambda^{n} \\frac{c}{m}(1-\\lambda)^{\\frac{E}{k}}$ the \u2018overall reward.' Because $0 < \\lambda < 1$, the discount factor incentivizes choosing trajectory-lengths that have been chosen less often so far in the meta-episode.\nWe will prove that optimal policies for this reward function are maximally USEFUL and maximally NEUTRAL. In cases where there are just two possible trajectory-lengths (shorter and longer, denoted by 's' and 'l' respectively), a policy is maximally USEFUL if and only if it maximizes:\n$\\text{USEFULNESS} := p(s) \\frac{c(s)}{m(s)} + p(l) \\frac{c(l)}{m(l)}$\nHere p(s) is the probability that the agent chooses the shorter trajectory-length, c(s) is the expected (y-discounted) total value of coins that the agent collects conditional on the shorter trajectory-length, m(s) is the maximum (y-discounted) total value of coins that the agent could collect conditional on the shorter trajectory-length, and p(l), c(l), and m(l) are the analogous quantities for the longer trajectory-length.\nGeneralizing to cases with any number of possible trajectory-lengths, a policy is maximally USEFUL if and only it maximizes:\n$\\sum_{i=1}^{k}p(a_i) \\frac{c(a_i)}{m(a_i)}$\nHere each $a_i$ is a different trajectory-length.\nIn cases where there are just two possible trajectory-lengths, a policy is maximally NEUTRAL if and only if it maximizes:\n$-[p(s)log_2p(s) + p(l)log_2p(l)]$\nGeneralizing to cases with any number of possible trajectory-lengths, a policy is maximally NEU- TRAL if and only if it maximizes:\n$-\\sum_{i=1}^{k} p(a_i) log_2 p(a_i)$\nHere's the proof that optimal policies for the above DREST reward function are maximally USEFUL and maximally NEUTRAL. The agent's expected return across the meta-episode can be expressed as:\n$\\mathbb{E} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\lambda^{n_i-T} \\frac{c_t}{m_i}$]\nHere N is the number of mini-episodes in the meta-episode, T is the number of timesteps in the relevant mini-episode, $n_i$ is the number of times that the trajectory-length chosen in mini-episode i has been chosen before in the meta-episode, $\\Sigma_{i-1}$ is equivalent to $\\mathbb{E}$ (the number of mini-episodes that have previously occurred in the meta-episode), $c_t$ is the value of any coins that the agent collects at timestep t in the relevant mini-episode, and $m_i$ is the maximum y-discounted total value of the coins that the agent could collect conditional on the trajectory-length chosen in mini-episode i.\nThis expression can be rearranged as follows:\n$\\mathbb{E} \\sum_{i=1}^{N} \\frac{\\sum_{t=1}^{T} c_t}{m_i} \\lambda^{n_i-T}$"}]}