{"title": "Implications for Governance in Public Perceptions of Societal-scale AI Risks", "authors": ["Ross Gruetzemacher", "Toby D. Pilditch", "Huigang Liang", "Christy Manning", "Vael Gates", "David Moss", "James W. B. Elsey", "Willem W. A. Sleegers", "Kyle Kilian"], "abstract": "Amid growing concerns over Al's societal risks-ranging from civilizational collapse to misinformation\nand systemic bias\u2014this study explores the perceptions of Al experts and the general US registered voters\non the likelihood and impact of 18 specific Al risks, alongside their policy preferences for managing these\nrisks. While both groups favor international oversight over national or corporate governance, our survey\nreveals a discrepancy: voters perceive AI risks as both more likely and more impactful than experts, and\nalso advocate for slower AI development. Specifically, our findings indicate that policy interventions may\nbest assuage collective concerns if they attempt to more carefully balance mitigation efforts across all\nclasses of societal-scale risks, effectively nullifying the near-vs-long-term debate over Al risks. More\nbroadly, our results will serve not only to enable more substantive policy discussions for preventing and\nmitigating Al risks, but also to underscore the challenge of consensus building for effective policy\nimplementation.", "sections": [{"title": "One Sentence Summary:", "content": "US voters and experts agree that international bodies should govern Al risks but voters consider the\nsocietal-scale risks of advanced AI as more likely and more impactful than experts, preferring a slower\npace of development."}, {"title": "Background", "content": "Worries about societal-scale risks from powerful artificial intelligence (AI) systems are nearly as old as\nthe field itself [1], and as capabilities of systems have grown in recent decades so has research on policy,\ngovernance, and technical topics seeking to ensure that the development of AI is safe and beneficial to\nhumanity [2]. Over the past decade, governments have been increasingly aware of the potentially\nprofound significance of AI [3], and since the release of ChatGPT in November of 2022, the potential of\nAl has become obvious to the broader public.\nThe reason for this interest is not unwarranted. As a general purpose technology, AI's potential to\ndo great good or great harm is unparalleled [4]. AI-generated advances could cure diseases [5], foster a\nnew economic age [6], and help to tackle global challenges like climate change [7].\nConversely, the threats AI could pose are both wide-ranging and stark. While existential risks like\nextinction, civilizational collapse, or dystopian futures grab immediate attention, other societal-scale risks\nsuch as algorithmic bias, knowledge deterioration, and economic concentration of power have tarted to\nemerge and be recognized. [6,8-9]. Moreover, AI technology both changes and is changed by the human\nbehaviors and systems surrounding it [10] yielding risks that stem from structural interactions (e.g.,\ngrowing geopolitical instability, erosion of trust).\nIt is not surprising that governments have already begun efforts to regulate AI development (e.g.,\nUS Executive Order 14110), a topic with which they are substantively unfamiliar. Consider that in 1996\nthe FCC was faced with the challenge of determining suitable regulation for the (new) internet. In so\ndoing, they had to balance prospective economic and societal gains with potential risks, both known and\nunknown. The shape this watershed legislation took, and the impact it had on fostering the nascent\ntechnology in a consumer-protected, but innovation-enabling manner, shaped not only the standards and\nexpectations of this new information age, but the way society and the economy developed in the decades\nthat followed [11].\nIn the advent of new and powerful technologies, regulatory policies are shaped by an emergent\nbalance between industry stakeholders, experts, governments, and voters/consumers that drives both\ndemocratic and economic forces. Although all parties have their respective incentives, divergence of\nopinion can be especially consequential (e.g., public vs expert opinion on risks of nuclear power [12]).\nTherefore it is important for governments to thoroughly understand the varying perspectives of these\ndifferent parties, not to rely solely on the opinions of subject matter experts, but to also consider\nperspectives of the citizen-consumers who hold collective power."}, {"title": "Survey", "content": "To address this knowledge gap, we administered a survey of the AI risk perceptions and policy\npreferences of 120 AI experts and 400 US registered voters. The survey covered perceptions of both the\nlikelihood and prospective impact of 18 specific Al risk scenarios. These scenarios comprise seven\nclasses of societal-scale risks from advanced AI: economic, ethical, misuse, accident, geopolitical,\nenvironmental, and existential. Generally, the societal-scale Al risks described for the 18 scenarios\ncomprising these seven classes concern risks of AI-induced harms to large-scale social systems (e.g.,\nfinancial systems, geopolitical stability) or to nations or other large social groups if the outcomes (e.g.,\nhuman rights violations, economic harms, war) of these harms are sufficiently widespread, and are\ninclusive of catastrophic and existential risks. We emphasize that we do not draw a distinction between\nshort and longer-term risk (e.g., [13]), but rather classify the broad areas from which societal-scale harms\nmay arise.\nAdditionally, to understand perceptions of different approaches to governing AI to mitigate risks,\nwe asked respondents two questions on perceived regulatory best practices for risk mitigation. These\nquestions concerned the speed with which Al progress should proceed\u2014should it be paused, slowed\ndown, maintained, or accelerated and who should bear the responsibility for managing Al risk:\ncompanies, governments, or international institutions.\nIn contrast to previous work [14-15], this study is the first survey to conduct a comparison on\nboth the likelihood and impact of risks across the breadth of all societal-scale Al harms-from\nbias/discrimination concerns, privacy issues, and economic concerns to terrorist AI weapons, an Al arms\nrace, and existential concerns. Moreover, unlike the previous work, it is not limited to either experts or the\ngeneral public but compares the results from both groups, focusing specifically on US voters given the\nrelevance of their opinions to US policymakers."}, {"title": "Perceived Societal-Scale Risk Likelihood vs Impact", "content": "Through the disentangling of risk likelihood and risk impact, we find that US registered voters (Fig. 1A)\nestimate not only the likelihood of societal-scale risks stemming from AI as significantly higher than AI\nexperts (Fig. 1B), $\\beta = 0.306$, SE = 0.040, t = 7.748, p < 0.001, they also estimate the prospective impact of\nthose risks as significantly higher, $\\beta = 0.649$, SE = 0.033, t = 19.47, p < 0.001. Although this exposes\npotential tensions in the incentives for Al regulation, we discover a similar pattern across groups\nregarding the relative likelihoods and impacts of risks. For instance, while both groups deem existential\ncategories of risk (e.g., civilizational collapse) as substantially less likely than others, including economic\n(e.g., economic instability) or ethical (e.g., privacy) risk categories, $\\beta = -1.489$, SE = 0.099, t = -15.10, p <\n0.001, the former is considered substantially more impactful, $\\beta = 0.958$, SE = 0.086, t = 11.09, p < 0.001."}, {"title": "Policy Preferences", "content": "Importantly, we find broad agreement across Al experts and US registered voters regarding who should\nbe responsible for managing AI risk (Fig. 3A). While a plurality of both US voters and AI experts prefer\ninternational treaties, intergovernmental organizations, and NGOs to manage risks from advanced AI,\nfewer voters and experts believe either tech companies or national governments should be responsible for\nmanaging Al risks, with US voters being significantly more wary than AI experts of national governments\nin this regard, $x\u00b2(3) = 17.92, p < .001$. This may reflect recognition among both groups of the potentially\nflawed incentives and/or capability of the latter groups to effectively (self)regulate.\nHowever, despite agreeing on who should manage risk, we find US registered voters and AI\nexperts disagree significantly on what should be done about the pace of AI development (Fig. 3B). While\nexperts believe the pace of AI development should be accelerated or maintained more than voters, voters\npreferred to slow down the pace of AI development as compared to experts, $x\u00b2(3) = 56.04, p < .001$. This\nis in broad alignment with the globally higher perceptions of both risk likelihoods and impacts from AI\namong US voters."}, {"title": "Discussion", "content": "The question of how to mitigate societal-scale risks arising from the development of advanced Al is one\nthat is increasingly relevant and not easily solved. Both the positive and negative outcomes associated\nwith the development of ever more capable AI systems stand to affect us all in ways we cannot fully\nanticipate [4]. Given the shared stakes and the substantial uncertainties involving the management of\nadvanced Al development, it is valuable not only to understand experts' perspectives on AI risks and their\nmitigation, but also those of the voting public, as each have important roles to play in informing\ngovernments' regulatory and strategic decisions.\nOur findings shed new light on the perceptions of societal-scale risks among both AI experts and\nUS registered voters. We find that US voters generally perceive the societal risks associated with AI as\nboth more likely and more potentially damaging (i.e., impactful) than experts, and notably in regard to the\npotential misuse of AI tools (e.g., terrorist actions). More broadly, we find a consistent characterization of\nrisk classes across both groups, demonstrating distinct clusters for medium likelihood, medium impact\nrisks (e.g., economic and accident risk classes), and distinct clusters for low likelihood, high impact risks\n(e.g., existential). Further, our supplementary materials include figures containing heatmaps on each risk\nscenario for both voters and experts, enabling more nuanced comparisons.\nBy disentangling the perceived likelihood of risks from their perceived impact, we can more\nclearly characterize commonalities and avoid misunderstandings when determining risk area priorities for\nboth research and policy, and conversely highlight areas of disagreement more efficiently. For example, a\ndisagreement about risk importance where the difference stems from perceived likelihood indicates a need\nto better understand and agree the likely causes and predicates of a risk. Conversely, a disagreement\nstemming from impact implies a need to focus on the chain of events and mechanisms of effects incurred\nby the risk. Further, in mapping these risk class profiles, we lay the foundation for more effective policy\ndesign. For example, where risks are characterized by high likelihoods, policy goals can center around\nrisk likelihood reduction (i.e., prevention) strategies, while those characterized by high impact and low\nlikelihoods are inherently more challenging and beyond the scope of traditional wicked problems.\nThese findings also appear to have implications for the near-vs-long-term debate over how to\nprioritize mitigation of different risks from advanced AI [13]. Rather than finding in favor of prioritizing\nnearer-term risks [16] over the more extreme longer-term risks [17], they demonstrate that experts and\nvoters both share more nuanced perceptions of societal-scale AI risks. Consequently, concerns over such\nrisks might be best assuaged through a more nuanced approach which would require more carefully\nbalancing mitigation efforts among all classes of societal-scale risks. Moreover, sociotechnical AI risks\n[10, 18] appear to be considered as similarly impactful and probable for both experts and voters as risks\nfrom accident and misuse [8], and as such should not be marginalized in overall risk mitigating policy\nplanning [19].\nOur policy preference findings reveal a consistent belief that risk management should be the task\nof international treaties, intergovernmental organizations and NGOs (>50% of voters and experts), while\nfew trust tech companies with such a responsibility: ~26% voters and ~15% experts. In this regard, and\nconsistent with our risk perception findings, we note that voters are more wary than experts of the current\npace of Al development, with significantly higher proportions believing it should be slowed or paused.\nThese findings also suggest that voters trust governments less than experts in managing Al risk,\nsuggesting that legislators could benefit from public trust-building measures and outreach on this\nimportant topic."}, {"title": "Limitations", "content": "A key limitation for the study is representativeness of the different samples. For the experts, we do not\nhave information about the total population of Al experts, and so it is not possible to determine the extent\nto which the expert sample accurately reflects the views of Al experts more generally; selection biases\ncould result in especially concerned or unconcerned experts taking this survey, for example. For the U.S.\nregistered voter population, while the sample was weighted to be representative, the sample size was\nrelatively small for this purpose, and it is therefore possible that some views are underrepresented."}, {"title": "Materials A: Societal-scale AI Risks Survey", "content": "The following survey contains questions on 18 different scenarios of societal-scale risks from advanced\nartificial intelligence (AI). We use the term 'advanced AI' to describe next-generation AI technology that\nis much more capable than current AI technology; this includes next-generation generative AI technology\nor robotics. In the coming decades, advanced AI is expected to transform society as it outperforms\nhumans at an increasing number of tasks. This survey is trying to understand what risks from this\ntechnology are most concerning.\nFor each of the 18 scenarios, you will be asked about the likelihood and the impact of the scenario.\nWhen completing the questions, please consider whether each scenario being discussed could occur\nwithin a decade or two.\nAt the end of the survey, you will be asked one final question as well as some brief demographic\nquestions."}, {"title": "Economic Risks", "content": "1. Monopolies & Centralization of Power\nA. What is the likelihood that in the next decade or two advanced Al will lead to monopolies or\nconcentration of power in a small number of companies? For example, a small number of\ncompanies might become so valuable from advanced AI that they have extreme influence on\nmarkets, the economy, or the government.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think that monopolies or the centralization of economic power from\nadvanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n2. Economic Instability\nA. What is the likelihood that in the next decade or two advanced AI causes a major disruption of the\neconomy, destabilizing labor markets? For example, advanced AI might make many skills\nobsolete, forcing some people to learn new skills to keep their jobs, and resulting in significant\njob losses for others.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think the disruption of the economy by advanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n3. Financial Meltdown\nA. What is the likelihood that in the next decade or two advanced Al leads to a global financial\nmeltdown? For example, this might mean that automated decision making systems, such as in\nalgorithmic trading systems with too much power and insufficient oversight, could result in a\nstock market crash.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think financial problems caused by advanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Ethical Risks", "content": "4. Bias & Discrimination\nA. What is the likelihood that in the next decade or two biases embedded in advanced AI tools might\nlead to significant increases in discrimination and social or income inequality? For example,\nbiases in advanced AI might increase disparities in hiring and wage determination, credit scoring,\nmortgage and rental application decisions, assessment of job performance, criminal sentencing\nand policing, school admissions decisions and personalized learning systems, or decisions in\nhealthcare diagnosis and treatment.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think increases in social or income inequality from advanced AI might be\nfor society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Privacy", "content": "5. Privacy\nA. What is the likelihood that in the next decade or two advanced AI technologies cause a decrease\nin personal privacy protections? For example, advanced AI might make surveillance more\ncommon and more acceptable for preventing crime, or, it might increase the demand for value of\ntraining data for AI models tremendously, resulting in a loosening of privacy protections.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think risks to privacy protections from advanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n6. Extreme Income Inequality\nA. What is the likelihood that in the next decade or two advanced Al results in extreme income\ninequality? For example, this might happen if advanced AI drives productivity gains generate\ntremendous new wealth overwhelmingly benefiting those who are already very wealthy, while\nthose with few or no investments see little or no benefits.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think severe income inequality from advanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Misuse Risks", "content": "7. Terrorist WMD Attack\nA. What is the likelihood that in the next decade or two terrorists use advanced AI to create a deadly\nbiological weapon or chemical weapon that is used at least once against a large number of\ncivilians in a major terrorist attack?\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think terrorist attacks with deadly biological or chemical weapons\ndeveloped with advanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n8. Terrorist AI Weapons Attack\nA. What is the likelihood that in the next decade or two terrorists use advanced AI to create\nautonomous Al weapons for use in attacking civilians? For example, terrorists might use\nadvanced AI to evade authorities and create cyberweapons and autonomous drones that could\ncarry out attacks on critical infrastructure, such as hydroelectric dams or nuclear power plants, or\non large civilian targets like airports or sporting arenas.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think terrorist attacks from advanced AI weapons might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n9. Cyber Attacks\nA. What is the likelihood in the next decade or two there are major cyber attacks on civilian targets\nenabled by advanced AI? For example, advanced AI might infiltrate and reprogram computer\nsystems, like power grids or military computer systems, to execute destructive plans.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think cyber attacks from advanced AI might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Accident Risks", "content": "10. Knowledge Deterioration\nA. What is the likelihood that in the next decade or two advanced AI technology leads to society's\ncollective inability to determine what information is true or reliable? For example, this might\nhappen if people over-rely on Al systems and blindly trust their outputs even if the output is\nwrong (e.g., fake news, generative AI like ChatGPT hallucinating or making things up).\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think a gradual distortion of knowledge might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n11. Safety Systems Failures\nA. What is the likelihood that in the next decade or two an advanced Al system relied upon for\npublic safety, such as disease detection, self-driving vehicles, or disaster warning systems,\nmalfunctions and results in a significant loss of life?\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think the failure of safety critical advanced Al systems might be for\nsociety?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Geopolitical Risks", "content": "12. AI Arms Race\nA. What is the likelihood that in the next decade or two an advanced AI arms race between the\nUnited States and China leads to a direct military conflict between the two countries' armed\nforces?\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think an AI arms race inspired conflict between the US and China might\nbe for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n13. Global Power Shift\nA. What is the likelihood that in the next decade or two one or more countries develop a strategic\nadvantage in Al giving rise to a global power imbalance that favors authoritarianism? For\nexample, this might happen if one or more authoritarian governments create radically advanced\nAI before other countries or disproportionately to other countries, giving them some sort of\nstrategic advantage and allowing them to enforce their values amongst the rest of the world.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think losing an AI race might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Information Warfare", "content": "14. Information Warfare\nA. What is the likelihood that in the next decade or two an adversary uses advanced AI to engage in\ninformation warfare to destabilize an adversary? For example, this might involve a sustained\neffort to provide very persuasive and convincing misinformation, like deep fakes, generated by\nadvanced Al with the longer-term goal of destabilizing society, or, a similar use of persuasive\nadvanced AI technology in order to influence the outcome of one or more federal elections.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think a successful information warfare campaign by an adversary might be\nfor affected members of society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Environmental Risks", "content": "15. Energy Consumption\nA. What is the likelihood that in the next decade or two the computational resources required to\nsupport training, operation, and data storage for AI models consume enough energy that the\nresulting carbon emissions destabilize the global climate above and beyond the current median\nprojections?\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think this risk that increased climate volatility might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Existential Risks", "content": "16. Dystopia\nA. What is the likelihood that in the next decade or two radically advanced AI overtakes existing\nglobal governments and implements a form of government that is not acceptable to the majority\nof humanity? For example, a society run by advanced AI might entail constant surveillance and\nno privacy, extreme poverty, a prohibition of religion or other critical liberties, or other\nrestrictions causing widespread suffering or injustice.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think an advanced Al dystopia might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n17. Civilizational Collapse\nA. What is the likelihood that in the next decade or two advanced Al will cause the collapse of\nhuman civilization resulting in global anarchy? For example, this might happen if Al automates\nsystems like those for food production and distribution and we eventually lose control of our food\nsupply resulting in mass famine; in this or other similar scenarios AI could lead to the deaths of\nso many people that governments cannot be sustained.\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think civilization collapse might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]\n18. Extinction\nA. What is the likelihood that in the next decade or two AI leads to human extinction?\n[Slider from 0% to 100%; labels every 20%, with 1% increments; default at 0]\nB. How impactful do you think human extinction might be for society?\n[100 pt Slider with labels Negligible, Minor, Moderate, Major, and Catastrophic evenly spaced; default at\n0 (Negligible)]"}, {"title": "Regulation", "content": "19. In your opinion, what should we do about risks from advanced AI?\nA. We should accelerate the rate of AI progress.\nB. We should maintain the current rate of AI progress.\nC. We should slow down the rate of AI progress.\nD. We should take an indefinite pause on AI progress.\n20. In your opinion, who should manage risks from advanced AI?\nA. The companies developing AI technologies should manage the majority of risks.\nB. National governments should manage the majority of risks.\nC. International treaties, intergovernmental organizations, and non-governmental organizations\nshould manage the majority of risks."}, {"title": "Demographic Questions", "content": "Do you consider yourself an Al expert?\nDefinitely not\nProbably not\nMight or might not\nProbably yes\nDefinitely yes\nWhat is your profession?\nText box\nWhat is the highest level of education you have completed?\nSome high school\nGraduated from high school (Diploma/GED or equivalent)\nSome college, no degree\nCompleted associate's degree\nCompleted bachelor's degree\nCompleted master's degree\nCompleted professional degree beyond a bachelor's degree (e.g., M.D., J.D.)\nCompleted doctorate degree\nOn what continent(s) did you attend high school?\nAsia\nAfrica\nEurope\nNorth America\nOceania\nSouth America\nWhat is your household income per year? (please provide your pre-tax income)\nUnder $20,000\nBetween $20,000 and $49,999\nBetween $50,000 and $79,999\nBetween $80,000 and $99,999\nBetween $100,000 and $149,999\n$150,000 or more\nAre you registered to vote in US elections?\nNo, I am NOT registered to vote\nYes, I am registered to vote\nGenerally speaking, do you think of yourself as a ...?\nRepublican\nDemocrat\nIndependent\nOther\nNot sure\nWhat is your gender?\nMan\nWoman\nNon-binary or other gender identification\nPrefer not to say\nDo you approve or disapprove of the way Joe Biden is handling his job as President?\nStrongly approve\nSomewhat approve\nSomewhat disapprove\nStrongly disapprove\nDon't know / No opinion\nWhat is your current age in years?\nText box\nWhich of the following best matches your racial identity?\nAmerican Indian or Alaska Native\nAsian or Asian American\nBlack or African American\nNative Hawaiian or Pacific Islander\nWhite\nOther race\nIdentify with two or more races\nDo you consider yourself to be of Hispanic, Latino, or Spanish origin?\nYes\nNo, I am NOT of Hispanic, Latino, or Spanish origin\nIn which US State or District do you currently live?\nDropdown list\nHow would you describe the place where you live?\nCity\nSuburb\nTown\nRural area\nOther\nWould you describe yourself as a \"born-again\" or evangelical Christian, or not?\nYes\nNo"}]}