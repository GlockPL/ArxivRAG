{"title": "SYNTHETIC4HEALTH: Generating Annotated Synthetic Clinical Letters", "authors": ["Libo Ren", "Samuel Belkadi", "Lifeng Han", "Warren Del-Pinto", "Goran Nenadic"], "abstract": "Since clinical letters contain sensitive information, clinical-related datasets can not be widely applied in model training, medical research, and teaching. This work aims to generate reliable, various, and de-identified synthetic clinical letters. To achieve this goal, we explored different pre-trained language models (PLMs) for masking and generating text. After that, we worked on Bio_ClinicalBERT, a high-performing model, and experimented with different masking strategies. Both qualitative and quantitative methods were used for evaluation. Additionally, a downstream task, Named Entity Recognition (NER), was also implemented to assess the usability of these synthetic letters.\n\nThe results indicate that 1) encoder-only models outperform encoder-decoder models. 2) Among encoder-only models, those trained on general corpora perform comparably to those trained on clinical data when clinical information is preserved. 3) Additionally, preserving clinical entities and document structure better aligns with our objectives than simply fine-tuning the model. 4) Furthermore, different masking strategies can impact the quality of synthetic clinical letters. Masking stopwords has a positive impact, while masking nouns or verbs has a negative effect. 5) For evaluation, BERTScore should be the primary quantitative evaluation metric, with other metrics serving as supplementary references. 6) Contextual information does not significantly impact the models' understanding, so the synthetic clinical letters have the potential to replace the original ones in downstream tasks.\n\nUnlike previous research, which focuses more on restoring the original letters by training language models, this project provides a basic framework for generating diverse, de-identified clinical letters. It offers a direction for utilising the model to process real-world clinical letters, thereby helping to expand datasets in the clinical domain. Our codes and trained models are available at https://github.com/HECTA-UoM/Synthetic4Health", "sections": [{"title": "1 Introduction", "content": "With the development of medical information systems, electronic clinical letters are increasingly used in communication between healthcare departments. These clinical letters typically contain detailed information about patients' visits, including their symptoms, medical history, medications, etc (Rayner et al., 2020). They also often include sensitive information, such as patients' names, phone numbers, and addresses (Tarur and Prasanna, 2021; Tucker et al., 2016). As a result, these letters are difficult to share and nearly impossible to use widely in clinical education and research.\n\nIn 2018, 325 severe breaches of protected health information were reported by CynergisTek (Abouelmehdi et al., 2018). Among these, nearly 3,620,000 patients' records were at risk (Abouelmehdi et al., 2018). This is just the data from one year-similar privacy breaches are unfortunately common. The most severe hacking incident affected up to 16,612,985 patients (Abouelmehdi et al., 2018). Therefore, generating synthetic letters and applying de-identification techniques seem to be indispensable."}, {"title": "", "content": "Additionally, due to privacy concerns and access controls, insufficient data is the major challenge of clinical education, medical research, and system development (Spasic and Nenadic, 2020). Some shared datasets offer de-identified annotated data. The MIMIC series is a typical example. These datasets are accessible through PhysioNet. MIMIC-IV (Johnson et al., 2023b; A et al., 2000; Johnson et al., 2024a), the latest version, contains data from 364,627 patients' clinical information collected from 2008 to 2019 at a medical centre in Boston. It records details about hospitalisations, demographics, and transfers. Numerous research are based on this shared dataset. Another public dataset series in the clinical domain is i2b2/n2c2 (the President and of Harvard College, 2023), They are accessible through the DBMI Data Portal. This series includes unstructured clinical notes such as process notes, radiology reports, and discharge summaries, and is published for clinical informatics sharing and NLP tasks challenges.\n\nHowever, these sharing datasets are often limited to specific regions and institutions, making them not comprehensive. Consequently, models and medical research outcomes derived from these datasets cannot be widely applied (Humbert-Droz et al., 2022). Therefore, to address the lack of clinical datasets and reduce the workload for clinicians, it is essential to explore available technologies that can automatically generate de-identified clinical letters.\n\nExisting systems generate clinical letters primarily by integrating structured data, while there are not many studies on how to use Natural Language Generation (NLG) models for this task (H\u00dcSKE-KRAUS, 2003; Amin-Nejad et al., 2020a; Tang et al., 2023). NLG attempts to combine clinical knowledge with general linguistic expressions and aims to generate clinical letters that are both readable and medically sound. However, NLG technology is not yet mature enough for widespread use in healthcare systems. Additionally, it faces numerous challenges, including medical accuracy, format normalisation, and de-identification (H\u00dcSKE-KRAUS, 2003). Therefore, this investigation focuses on how NLG technology can be used to generate reliable and anonymous clinical letters, which can benefit medical research, clinical education, and clinical decision-making. The main aim of our work is to generate de-identified clinical letters that can preserve clinical information while differing from the original letters. A brief example of our objective is shown in Figure 1. Based on this objective, different generation models will be explored as a preliminary attempt. Then we select the best models and try various techniques to improve the quality of the synthetic letters. The synthetic letters are evaluated not only with quantitative and qualitative methods, but also in downstream tasks, i.e., Named Entity Recognition (NER). We hope this work will contribute to addressing the challenge of insufficient data in the clinical domain.\n\nIn summary, this work is centered on the Research Question (RQ): \"How can we generate reliable and"}, {"title": "", "content": "diverse clinical letters without including sensitive information?\" Specifically, it will answer the following related sub-questions (RQs):\n\n1. How do different models perform in masking and generating clinical letters?\n\n2. How should the text be segmented in clinical letter generation?\n\n3. How do different masking strategies affect the quality of synthetic letters?\n\n4. How can we evaluate the quality of synthetic letters?\n\nTo answer these questions, we explored various LLMs for masking and generating clinical letters, ultimately focusing on one that performed well. The overall highlights of this work are summarised as follows:\n\n1. Mask and Generate clinical letters using different LLMs at the sentence level.\n\n2. Explore methods to improve synthetic clinical letters' readability and clinical soundness.\n\n3. Initially evaluate synthetic letters using both qualitative and quantitative methods.\n\n4. Apply synthetic letters in downstream tasks.\n\n5. Explore post-processing methods to further enhance the quality of de-identified letters."}, {"title": "2 Background and Literature Review", "content": "We first introduce general language models, followed by their applications, especially within the clinical domain. We then present the generative language models based on the Transformer architecture. These models serve as the technical foundation for most modern text generation tasks. Afterward, We review related works, discussing their relevance and how they are connected to ours. Finally, all quantitative evaluation metrics used in this project are introduced."}, {"title": "2.1 Development of Language Models (LMs)", "content": "The development of Language Models can be divided into three stages: Rule-Based Approach, Supervised Modelling, and Unsupervised Modelling (Boonstra et al., 2024)."}, {"title": "2.1.1 Rule-Based Approach", "content": "The Rule-Based Approach was first used in 1950s, which marks the beginning of NLP (Nadkarni et al., 2011). This approach always uses a set of predefined rules, which were written and maintained manually by specialists (van der Lee et al., 2018; Satapathy et al., 2017). Although it can generate standardised text without being fed with extensive input data (van der Lee et al., 2018), there are still numerous limitations. Initially, manually crafted rules are often ambiguous, and the dependencies between different rules increase maintenance costs (Nadkarni et al., 2011). Secondly, these stylised models cannot perform well in understanding realistic oral English and ungrammatical text such as clinical discharge records, although these texts are still readable for humans (Nadkarni et al., 2011). Thirdly, they are not objective enough, as they are affected by the editors of the rule library. Additionally, they are not flexible enough to deal with special cases. Therefore, the Rule-Based Method is only suitable for analysing and generating highly standardised texts like prescriptions (van der Lee et al., 2018)."}, {"title": "2.1.2 Supervised Language Models", "content": "To address the limitations of the Rule-Based Approach, Supervised Learning has been applied to NLP. The invention of Statistical Machine Translation (SMT) in 1990 marked the rise of supervised NLP (Boonstra et al., 2024). It learns the correspondence rules between different languages by analysing the input of bilingual texts (parallel corpus) (Brown et al., 1990). Supervised NLP models are trained on annotated labels to learn rules automatically. The learned rules will be used in word prediction or text"}, {"title": "2.1.3 Unsupervised Language Models", "content": "To address the high cost and difficulty of obtaining labelled data, unsupervised Neural Networks are applied to the language modelling (Bengio et al., 2000). The popularity of corpora such as Wikipedia and social media provides enough data for unsupervised models' training (Boonstra et al., 2024). Word embedding is a significant technique in this stage (Johnson et al., 2024b). Word2Vec represents words using vectors with hundreds of dimensions. The context can be captured by training word vectors in the sliding window. By adjusting the hyperparameters to maximise the conditional probability of the target word, it can learn semantic information accurately (Mikolov et al., 2013a; Mikolov et al., 2013b) (e.g. 'Beijing'-\u2018China\u2019+\u2018America' => \u2018Washington' (Ma and Zhang, 2015)). After training, each word usually has a fixed word vector regardless of the context in which it appears (known as Static Word Embedding) (Santhanam, 2020).\n\nUnlike word2Vec, BERT and GPT use contextual word embedding. Their word vectors reflect the semantic information and are affected by the context (Camacho-Collados and Pilehvar, 2018). BERT focuses on contextual understanding (Zhang et al., 2020) (e.g. The bank is full of lush willows, where 'bank' means the riverside, not the financial institution), while GPTs focus on text generation in a specific context (Liu et al., 2022; Li et al., 2024) (e.g. Prompt: \"Do you know Big Ben?\u201d Answer: \u201cYes, I know Big Ben. It is the nickname for the Great Bell of the Clock located in London.\") Although unsupervised language models have been able to train and understand text proficiently, they still face challenges in practical applications, such as difficulty handling ambiguity and high computing resource consumption. Therefore, language modelling still has a long way to go.\""}, {"title": "2.2 Language Models Applications in Clinical Domain", "content": "Based on the modelling methods mentioned above, a variety of language models have been invented. They play an important role in scientific research and daily life, especially in the field of healthcare. In this section, I will discuss the clinical language model applications in detail from two aspects: named entity recognition (NER) and natural language generation (NLG)."}, {"title": "2.2.1 Named Entity Recognition (NER)", "content": "NER was originally designed for text analysis and recognition of named entities, such as dates, organisations, and proper nouns (Grishman and Sundheim, 1996). In the clinical domain, NER is used to identify clinical events (e.g. symptoms, drugs, treatment plans, etc.) from unstructured documents with their qualifiers (e.g. chronic, acute, mild), classify them, and extract the relationship (Bose et al., 2021; Kundeti et al., 2016). Initially, NER relied on rule-based and machine-learning methods that required extensive manual feature engineering. In 2011, Collobert et al. (2011) used word embeddings and neural networks in NER. Since then, research in NER has shifted to automatic feature extraction.\nSpaCy \u00b9 is an open-source NLP library for tasks like POS tagging and text classification. Additionally,"}, {"title": "2.2.2 De-Identification", "content": "The unprocessed clinical text has a risk of personal information leakage. Additionally, manual de-identification is not only prone to errors but also costly. Therefore, research on de-identification is indispensable for the secondary use of clinical data. Typically, de-identification is based on NER models to identify Protected Health Information (PHI). Then PHI will be processed by different strategies (such as synonym replacement, removal, or masking) (Berg et al., 2021; Berg et al., 2020).\n\nSimilar to NER, early de-identification relied heavily on rule-based systems, machine learning, or hybrid models. Physionet DeID, the VHA Best-of-Breed (BoB), and MITRE's MIST are three typical examples (Meystre, 2015). However, these algorithms require extensive handcrafted feature engineering. With the development of unsupervised learning, recurrent neural networks (RNNs) and Transformers are widely used in de-identification tasks (Dernoncourt et al., 2017; Kova\u010devi\u0107 et al., 2024).\nPhilter, a Protected Health Information filter (Norgeot et al., 2020), is a pioneering system that combines rule-based approaches with state-of-the-art NLP models to identify and remove PHI. Although Philter outperforms many existing tools like Physionet and Scrubber, particularly in recall and F2 score, it still requires large amounts of annotated data for training (Norgeot et al., 2020). Additionally, research has shown that while the impact of de-identification on downstream tasks is minimal, it cannot be completely ignored (Meystre et al., 2014). Therefore, performing de-identification without mistakenly removing semantic information is still a challenge in this field."}, {"title": "2.2.3 Natural Language Generation (NLG)", "content": "Both label-to-text and text-to-text generation are components of NLG (Gatt and Krahmer, 2018). NLG consists of six primary sub-tasks, covering most of the NLG process. NLG architectures can generally be divided into three categories (Gatt and Krahmer, 2018):\n\n\u2022 Modular Architectures: This architecture consists of three modules: the Text Planner (responsible for determining the content for generation), the Sentence Planner (which aggregates the synthetic text), and the Realiser (which generates grammatically correct sentences). These modules are closely related to the six sub-tasks, and each module operates independently.\n\n\u2022 Planning Perspectives: This architecture considers NLG as a planning problem. It generates tokens dynamically based on the objectives, with potential dependencies between different steps.\n\n\u2022 Integrated or Global Approaches: This is the dominant architecture for NLG, relying on statistical learning and deep learning. Common generative models, such as Transformers and conditional language models, are included in this architecture.\n\nIn the field of healthcare, NLG applications include document generation and question-answering. Document generation involves discharge letters, diagnostic reports for patients, decision-making suggestions for experts, and personalised patient profiles for administrators (Cawsey et al., 1997). Some systems have already been implemented in practice. For instance, PIGLIT generates explanations of clinical terminology for diabetes patients (Hirst et al., 1997), and MAGIC can generate reports for Intensive Care Unit (ICU) patients (McKeown et al., 1997). Question answering is another application of"}, {"title": "2.3 Generative Language Models", "content": "Nowadays, NLG in the clinical field focuses on the development and training of transformer-based large language models (LLMs); examples of this work can be seen in (Amin-Nejad et al., 2020a; Luo et al., 2022). These models perform well in specific domains such as semantic query (Kong et al., 2022) and electronic health records (EHRs) generation (Lee, 2018). However, very few systems can reliably produce concise, readable, and clinically sound reports across multiple sub-domains (Cawsey et al., 1997)."}, {"title": "2.3.1 Transformer and Attention Mechanism", "content": "Although Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) are effective at semantic understanding, their recursive structure not only prevents parallel computation, but also makes them prone to gradient vanishing (Gillioz et al., 2020). The introduction of the Transformer in 2017 addressed this issue by replacing the recurrent structure with a multi-head attention mechanism (Vaswani et al., 2017). Since then, most deep learning models have been based on the Transformer. Transformer architecture is based on an encoder-decoder model (Vaswani et al., 2017). To understand this, we first need to overview Auto-Regressive Models and the Multi-Head Attention Mechanism.\nAuto-Regressive Models Predictions for each Auto-Regressive Model token depend on the previous output. Therefore, it can only access the preceding tokens and operate iteratively. When the input sequence is X, the Auto-Regressive Model aims to train parameters 0 to maximise the log-likelihood of the conditional probability P (Vaswani et al., 2017).\n$$L(X) = \\sum_i log P(x_i | X_{i-k}, ..., X_{i-1}; \\Theta)$$\nMulti-Head Attention Mechanism Attention mechanism was initially proposed by Cho et al. (2014). It can not only focus on the element being processed, but also capture the context dependence (Vaswani et al., 2017). Multi-head attention consists of several single-head attention (Scaled Dot-Product Attention) layers (Vaswani et al., 2017). Each word in the input sequence is converted into a high-dimensional vector representing semantic information by word embedding. These vectors pass linear transformation layers and get vectors for queries (Q), keys (K), and values (V). For each word, Q, K, and V are inputs to this single-head attention layer. The importance score of this word is calculated, and V corresponding to this word should be multiplied to get the output of this head (called Attention). Finally, outputs from all layers are concatenated to form a larger vector, which is the input to a feed-forward neural network (also the output of the multi-head attention layer) (Vaswani et al., 2017).\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nTransformer and Pre-training Language Models (PLMs) Transformer consists of an encoder and a decoder. The Auto-Regressive Model is the basis of the decoder. When the input sequence is X = (x1,..., XN), output sequence is Y\u043c = (y1, ..., \u0443\u043c), the model can learn a latent feature representation Z = (21,..., ZN) from X to Y. The generation of each new element YM relies on the generated sequence YM-1 = (\u04231,\u2026\u2026\u2026, \u0423\u041c\u22121) and feature representation Z. Both the encoder and the decoder use the multi-head attention mechanism (Vaswani et al., 2017; Gillioz et al., 2020).\nMany modern models are based entirely or partially on the Transformer. They compute general feature representations for the training set by unsupervised learning. This is the concept of Pre-training Language Models (PLMs). They can be fine-tuned to adapt to the specific tasks on particular datasets (Gillioz et al., 2020; Li et al., 2024)."}, {"title": "2.3.2 Encoder-Only Models", "content": "Since the Transformer's encoder architecture can effectively capture the semantic features, some models only use this part for training. They are applied in text understanding tasks, such as text classification"}, {"title": "2.3.3 Decoder-Only Models", "content": "In 2020, the performance of ChatGPT-3 (Brown et al., 2020) in question answering task caught researchers' attention to decoder-only architectures. As mentioned earlier, the Transformer decoder is an auto-regressive model. It can only refer to the synthesised words on the left side to generate the new word, without considering the context (which is called masked self-attention). This method made it more flexible in generating coherent text. Compared with BERT, the GPT series performed well in zero-sample and small-sample learning tasks by enlarging the size of the model. Even without fine-tuning, a simple prompt can help GPT generate a reasonable answer (Wu, 2024).\nUnlike GPT, which improves models' performance by increasing dataset size and the number of parameters without limitations, Meta AI published a series of Llama Models. These models aim to maximise the use of limited resources - in other words, by extending training, they reduce the overall demand on computing resources. The latest Llama3 model requires only 8B to 70B parameters (AI, 2023), significantly fewer than GPT-3's 175 billion (Wu, 2024). Additionally, it outperforms GPT-3.5 Turbo in 5-shot learning (Context.ai, 2024)."}, {"title": "2.3.4 Encoder-Decoder Models", "content": "T5 Family (Raffel et al., 2020a) is a classic example of the Encoder-Decoder Model. This architecture is particularly suitable for text generation tasks that require deep semantic understanding (Cai et al., 2022). T5 transforms all kinds of NLP tasks into a text-to-text format (Tsirmpas et al., 2024). Unlike BERT, which uses word-based masking and prediction, T5 processes text at the fragment level using \"span corruption\u201d to understand semantics (Tsirmpas et al., 2024). For the fill-in-the-blank task, instead of replacing the specific words with  like BERT, T5 replaces the text fragments with an ordered set of <extra_id_n> to reassemble the long sequence text. T5 needs to pre-process the input text according to the tasks' requirements. A directive prefix should be added as a prompt.\nSome language models fine-tuned with T5 on specific datasets, such as SciFive (fine-tuned in some science literature) (Phan et al., 2021) and ClinicalT5 (fine-tuned in clinical dataset MIMIC-III notes) (Lu et al., 2022), have shown excellent performance in their respective fields."}, {"title": "2.3.5 Comparison and Limitations", "content": "According to Cai et al. (2022), the encoder-decoder architecture performs best with sufficient training data. However, challenges in data collection can negatively affect its performance. Despite these challenges, different architectures are well-suited to different tasks. For example, for tasks requiring semantic understanding, such as text summarisation, the encoder-decoder architecture is the most effective. In contrast, for tasks that involve minor word modifications, the encoder-only structure works better. However, the decoder-only structure is not suitable for tasks with insufficient training data and long text processing, but performs well in few-shot question-answering tasks (Amin-Nejad et al., 2020b; Cai et al., 2022).\nFollowing these discussions, Transformer-based Pre-trained Language Models (PLMs) have demonstrated strong performance in NLP tasks, but many challenges still remain."}, {"title": "2.4 Related Works on Clinical Text Generation", "content": "LT3 (Belkadi et al., 2023) uses an encoder-decoder architecture to generate synthetic text from labels. As shown in Fig 2, labels such as medications are the input of the encoder, which can generate corresponding feature representations. The decoder generates prescription sequences based on these features. The pre-trained BERT tokenizer is used to split the input sequence into sub-words. LT3 is trained from scratch. Instead of using traditional greedy decoding, which may miss the global optimum, the authors proposed Beam Search Decoding with Backtracking (B2SD). This approach broadens the search range through a backtracking mechanism, preserving possible candidates for the optimal solution. To reduce time complexity, they used a probability difference function to avoid searching for low-probability words."}, {"title": "2.4.2 Seq2Seq Generation for Medical Dataset Augmentation", "content": "Amin-Nejad et al. (2020b) compare the performance of the Vanilla Transformer and GPT-2 using the MIMIC-III dataset in seq2seq tasks. Specifically, they input a series of structured patient information as conditions, as shown in Fig 3, to generate discharge summaries. They demonstrate that the augmented data outperforms the original data in downstream tasks (e.g. readmission prediction). Furthermore, they prove that Vanilla Transformer performs better with large samples, while GPT-2 excels in few-shot scenarios. However, GPT-2 is not suitable for augmenting long texts. Additionally, they used Bio_ClinicalBERT for the downstream tasks, and discovered that Bio_ClinicalBERT significantly outperformed the baseline model (BERT) in almost all experiments. It suggests that Bio_ClinicalBERT can potentially replace BERT in the biomedical field. Interestingly, although the synthetic data have a low score on internal metrics (such as ROUGE and BLEU), the performance on downstream tasks is notably enhanced. This may be because augmenting text can effectively introduce noise into the original text, improving the model's generalisation to unseen data."}, {"title": "2.4.3 Discharge Summary Generation Using Clinical Guidelines and Human Evaluation Framework", "content": "Unlike the traditional supervised learning of fine-tuning language models (which requires a large amount of annotated data), Ellershaw et al. (2024) generated 53 discharge summaries using only a one-shot example and a clinical guideline. Their research consists of two aspects: generating discharge summaries and a manual evaluation framework.\nAs shown in Fig 4, the authors used clinical notes from MIMIC-III as input, and incorporated a one-shot summary along with clinical guidance as prompts to generate discharge summaries by GPT-4-turbo. Initially, five sample synthetic summaries were evaluated by a clinician. Based on the feedback, the clinical guidance was revised to adapt to the generation task. Through iterative optimisation, the revised guidance, combined with the original one-shot sample, became the new prompt. Then the authors generated 53 discharge summaries using this method and invited 11 clinicians to do a final manual quantitative evaluation. Clinicians were invited to evaluate the error rate at the section level (e.g., Diagnoses, Social Context, etc). It includes four dimensions:\n\u2022 Minor omissions;\n\u2022 Severe omissions;\n\u2022 Unnecessary text;\n\u2022 Incorrect additional text."}, {"title": "2.4.4 Comparison of Masked and Causal Language Modelling for Text Generation", "content": "Micheletti et al. (2024) compared masked language modelling (MLM, including BERT, ROBERTa, BiomedNLP-PubMedBERT) and causal language modelling (CLM, including T5, BART, SciFive-large-Pubmed_PMC) across various datasets in masking and text generation tasks. They used qualitative and quantitative evaluations, as well as downstream tasks, to assess the quality of the synthetic texts. Their workflow is shown in Fig 5. Based on these evaluations, the study yielded the following results:\n\u2022 MLM models are better suited for text masking and generation tasks compared to CLM."}, {"title": "", "content": "\u2022 Introducing domain-specific knowledge does not significantly improve the model's performance.\n\n\u2022 Downstream tasks can adapt to the introduced noise. Although some synthetic texts might not achieve high quantitative evaluation scores, they can still perform well in downstream tasks. This matches the findings from Amin-Nejad (Amin-Nejad et al., 2020b).\n\n\u2022 A lower random masking ratio can generate higher-quality synthetic texts.\n\nThese very recent findings provide insightful inspiration to our investigation. Our work will build on their research, expanding on masking strategies and focusing on the clinical domain."}, {"title": "3 Methodologies and Experimental Design", "content": "Due to the sensitivity of clinical information, many clinical datasets are not accessible. As mentioned in Section 2, numerous studies use NLG techniques to generate clinical letters, and evaluate the feasibility of replacing the original raw clinical letters with synthetic letters. Most existing research involves fine-tuning PLMs or training Transformer-based models from scratch on their datasets through supervising learning. These studies explore different ways to learn the mapping from original raw text to synthetic text and work on generating synthetic data that are similar (or even identical) to the original ones. Our work, however, aims to find a method that can generate clinical letters that can keep the original clinical story, while not exactly being the same as the original letters. To achieve this objective, we employed various models and masking strategies to generate clinical letters. The experiment will follow these steps:\n\n1. Data Collecting and Pre-processing: Access clinical letter examples (A et al., 2000; Johnson et al., 2024a; Johnson et al., 2023b). Segment the text at sentence level. Extract entities and the letters' templates to represent the clinical story and maintain clinical soundness.\n\n2. Randomly mask the context. Generate clinical letters by predicting masked tokens using different LLMs.\n\n3. Evaluate synthetic letters generated by different language models. Select one well-performed model - Bio-ClinicalBERT, and work on it.\n\n4. Explore different masking strategies to retain clinical stories and diversity while removing private information. After generating clinical letters using these strategies, evaluate their quality.\n\n5. Explore post-processing methods, in order to further enhance the readability of synthetic letters.\n\n6. Compare the performance of synthetic and original letters in a downstream NER task, to evaluate the usability of these synthetic letters.\n\nAn overall investigation workflow is shown in Fig 6."}, {"title": "3.1 Data Set", "content": "Based on the objective of this project, we need a dataset that includes both clinical notes and some clinical entities. The dataset we used is from the SNOMED CT Entity Linking Challenge (A et al., 2000; Johnson et al., 2024a; Johnson et al., 2023b). It includes 204 clinical letters and 51,574 manually annotated clinical entities.\nClinical Letters The clinical letters are from a subset of discharge summaries in MIMIC-IV-Note (A et al., 2000; Johnson et al., 2023a). It uses clinical notes obtained from a healthcare system in the United States. These notes were de-identified by a hybrid method of the Rule-based Approach and Neural Networks. To avoid releasing sensitive data, the organisation also did a manual review of protected health information (PHI). In these letters, all PHI was replaced with three underscores \u2018---'. The letters record the patient's hospitalisation information (including the reason for visiting, consultation process, allergy history, discharge instructions, etc.). They are saved in a comma-separated value (CSV) format file 'mimic-iv_notes_training_set.csv'. Each row of data represents an individual clinical letter. It consists of two columns, where the \"note_id\" column is a unique identifier for each patient's clinical letter, and the 'text' column contains the contents of the clinical letter. Since most language models have a limitation on the number of tokens to process (Sun and Iyyer, 2021), we tokenized the clinical letters into words using the 'NLTK' library and found that all clinical letters contained thousands of tokens. Therefore, it is necessary to split each clinical letter into multiple chunks to process them. These separated chunks should be merged in the end to generate the whole letter.\nAnnotated Clinical Entities The entities are manually annotated based on SNOMED CT. A total of 51,574 annotations cover 5,336 clinical concepts. They are saved in another CSV document which includes four columns: 'note_id', 'start', 'end', and 'concept_id'. The 'note_id' column corresponds to the 'note_id' in 'mimic-iv_notes_training_set.csv' file. The 'start' and 'end' columns indicate the position of annotated entities. The 'concept_id' can be used for entity linking with SNOMED CT. For example, for the 'note_id' '10807423-DS-19',, the annotated entity \u2018No Known Allergies' has a corresponding 'concept_id': '609328004'. This can be linked to SNOMED CT under the concept of 'Allergic disposition' (International, 2024).\nAn example of text excerpted from the original letter is shown in Figure 7. It contains the document structure and some free text. According to the dataset, document structure often corresponds to capital letters and colons ':'. Our primary goal is to mask the context that is neither part of the document"}, {"title": "3.2 Software and Environment", "content": "All codes and experiments in this project are run in the integrated development environment (IDE) 'Google Colab'. The built-in T4 GPU is used to accelerate the inference process. The primary tools used in the project include:\n\u2022 Programming Language and Environment: Python 3.8 serves as the main programming language.\n\u2022 Deep Learning Framework: PyTorch is the core framework used for loading and applying pre-trained language models (PLMs).\n\u2022 Natural Language Processing Libraries: This includes Hugging Face's Transformers, NLTK, BERTScore, etc. They are popular tools for text processing and evaluation in the NLP domain.\n\u2022 Auxiliary Tools: Libraries such as Pandas and Math can support data management, mathematical operations, and other routine tasks."}, {"title": "3.3 Pre-Processing", "content": "The collected dataset involves different files and is entirely raw data. It is necessary to pre-process them before using them in generation tasks. The pre-processing of this system contains five steps: 'Merge dataset based on 'note_id', 'Annotated Entity Recognition', 'Split Letters in Chunks', \u2018Word Tokenization' and 'Feature Extraction'. The pre-processing pipeline is shown in Fig 8."}, {"title": "3.3.1 Merging Dataset and Annotated Entity Recognition", "content": "Initially, we merged the clinical letters file and annotations file into a new DataFrame. The method is detailed in Appendix ??. After this, we extracted manually annotated entities based on their index. An excerpt from an original letter is shown in Fig 9, and the manually annotated entities are listed in Table 3."}, {"title": "3.3.2 Splitting Letters into Variable-Length Chunks", "content": "Typically, PLMs such as BERT, ROBERTa, and T5 have a limit on the number of input tokens, usually capped at 512 (Zeng et al., 2022). When dealing with text that exceeds this limit, common approaches include discarding the"}]}