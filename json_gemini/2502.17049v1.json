{"title": "TabulaTime: A Novel Multimodal Deep Learning Framework for Advancing Acute Coronary Syndrome Prediction through Environmental and Clinical Data Integration", "authors": ["Xin Zhang", "Liangxiu Han", "Stephen White", "Saad Hassan", "Philip A Kalra", "James Ritchie", "Carl Diver", "Jennie Shorley"], "abstract": "Acute Coronary Syndromes (ACS), encompassing conditions such as ST-segment elevation myocardial infarction (STEMI) and non-ST-segment elevation myocardial infarction (NSTEMI), remain a leading cause of global mortality. While traditional Cardiovascular Risk Scores (CVRS) provide valuable insights, they primarily rely on clinical data, often overlooking environmental factors like air pollution, which significantly impact cardiovascular health. Additionally, integrating complex time-series environmental data with clinical datasets poses significant challenges due to issues in alignment and fusion.\nTo address these gaps, we propose TabulaTime, a novel multimodal deep learning framework that integrates clinical risk factors with air pollution data to enhance ACS risk prediction. TabulaTime introduces three major innovations: (1) Multimodal feature integration to combine time-series air pollution data with clinical tabular data for improved predictive accuracy; (2) PatchRWKV, a new module for automatic extraction of complex temporal patterns, overcoming limitations of traditional feature engineering and maintaining linear computational complexity; and (3) Enhanced interpretability using attention mechanisms to reveal interactions between clinical and environmental factors.\nOur experimental evaluation demonstrated that TabulaTime achieves a 20.5% improvement in accuracy compared to the best-performing traditional machine learning model (CatBoost) and surpasses other models, including Random Forest and LightGBM, by 20.5% to 32.2%. The integration of air pollution data improves accuracy by 10.1%, emphasising the importance of environmental factors. The PatchRWKV module showed advanced performance in time-series analysis, outperforming the state-of-the-art methods such as Multi-Layer Perceptrons (MLPs) based methods (LightTS and Dlinera), Convolutional Neural Networks (CNNs) based methods (TimesNet), Recurrent Neural Networks (RNNs) based methods (LSTM, GRU), and Transformers based methods (Autoformer, PatchTST) across various tasks while maintaining efficiency. Additionally, feature importance analysis identified key predictors of ACS, such as previous angina, systolic blood pressure, and pollutants like PM10 and NO2, highlighting the critical role of environmental-clinical interactions in ACS risk.\nThis framework bridges the gap between traditional clinical models and environmental health insights, supporting personalised prevention strategies and informing public health policies to mitigate the cardiovascular impact of air pollution.", "sections": [{"title": "1 Introduction", "content": "Acute Coronary Syndromes (ACS), a spectrum of conditions encompassing unstable angina, ST-segment elevation myocardial infarction (STEMI), and non-ST-segment elevation myocardial infarction (NSTEMI), is the leading cause of death globally, with nearly half of these deaths due to ischaemic heart disease [1, 2]. Understanding environmental factors is crucial for developing strategies to mitigate ACS risk. Cardiovascular Risk Scores (CVRS) serve as statistical tools used to estimate a person's risk of having a cardiovascular event, such as a heart attack or stroke, usually over the next 10 years. Various risk scores have been developed, each targeting different populations, incorporating distinct risk factors, and possessing unique predictive strengths. Notable examples include the QRISK family of algorithms [3\u20135], Framingham Risk Score (FRS)[6], Reynolds Risk Score [7] and European SCORE System [8]. These tools help clinicians assess and manage patients' cardiovascular risk and target prophylactic therapy more effectively.\nHowever, there is a growing recognition that existing models may not fully capture the comprehensive range of risk factors contributing to ACS. For example, QRISK3 explains 59.6% of the variation in time to diagnosis of CVD among women and 54.8% among men, leaving over 40% of the variation unexplained by the QRISK3 score [3]. Numerous epidemiological studies have convincingly linked air pollution exposure to a heightened risk of ACS [9, 10]. Air pollutants, particularly particulate matter (PM2.5 and PM10), nitrogen oxides (NOx), sulphur dioxide (SO2), and ozone (O3), trigger inflammation, oxidative stress, and change in blood vessel function, contributing to cardiovascular issues. For instance, Rus and Morno\u015f [9] provides a broader look at the connection between environmental factors and ACS, highlighting the increased risk associated with PM2.5 exposure. Additionally, in [10], the researchers presented a large-scale study analysing the link between hourly air pollutant levels and ACS in over 1 million patients, emphasising the roles of PM2.5, NO2, SO2, and CO in triggering ACS. This gap necessitates novel approaches that leverage the power of environmental data to enhance risk prediction accuracy.\nTraditional medical data used for acute coronary syndrome (ACS) prediction typically include patient demographics and clinical data. These data points, structured in a tabular manner, allow for relatively simple analysis and prediction models. In contrast, the environmental data exists as a time series with more complex features, capturing continuous changes over time. Most current studies often utilise manual time series features, such as statistical features (e.g., maximum and minimum values, standard deviation) and temporal features (e.g., trends, event counts, anomalies). However, these methods do not fully leverage the extensive information embedded within time series data. Identifying and extracting the right features that represent the temporal dependencies within time-series data can be time-consuming and requires domain expertise [11].\nDeep learning models, such as recurrent neural networks (RNNs) [12], multi-layer perceptrons (MLPs) [13], convolutional neural networks (CNNs) [14] and transformers [15], can automatically learn complex temporal features from raw time series data, reducing the need for extensive manual engineering. Several studies have leveraged deep learning to analyse the relationship between air pollution exposure and health outcomes [16\u201319]. For instance, [20] implemented a hybrid CNN-LSTM (Long Short-Term Memory) model to analyse hourly PM2.5 concentrations in Beijing, China, and predict air quality levels for health risk assessment. Similarly, (Tsai, Zeng, and Chang 2018) used RNNs with LSTM to forecast PM2.5 concentrations, while[21] employed radial basis function (RBF) and MLP neural networks to model the temporal dynamics of air pollution and its impact on respiratory hospital admissions. Despite the success of advanced deep learning models in various fields, they can demonstrate limitations when processing multimodal data [22]. Integrating heterogeneous data types requires sophisticated architectures capable of learning and representing the intricate relationships between different modalities. Furthermore, the alignment, synchronisation, and fusion of multimodal data necessitates advanced techniques to ensure that the combined information enhances, rather than detracts from, the model's performance [23, 24].\nExisting research on ACS risk prediction primarily emphasises traditional clinical datasets, often neglecting the significant influence of environmental factors. Integrating air pollution data into deep learning models offers a promising opportunity to enhance ACS risk assessment and inform preventive cardiology strategies. This research aims to address these challenges by developing and validating a novel multimodal deep learning based ACS prediction model, TabulaTime, specifically designed to improve CVD risk prediction through the integration of time-series air pollution features and clinical tabular data. Our proposed method offers several key contributions:\n1) Multimodal feature integration and ACS prediction: We propose a novel TabulaTime deep learning framework for multimodal feature integration and ACS prediction, combining extracted time-series features from air pollution datasets with ACS risk factors obtained from clinical tabular data. This integration enables the model to conditionally learn attention maps based on the multimodal features, thereby enhancing both predictive performance and interpretability.\n2) Automatic time-series feature extraction: We introduce PatchRWKV, an advanced feature extraction module designed for automatic extraction of features from time-series data. The PatchRWKV module divides data into patches. It uses the Receptance Weighted Key Value (RWKV) technique, which combines RNN capabilities with attention mechanisms. This approach efficiently processes long sequences while maintaining linear computational complexity, making it more efficient than traditional RNNs and transformers, which have quadratic complexity. By capturing both short-term and long-term patterns, PatchRWKV provides a comprehensive representation of temporal dependencies. This capability is crucial for accurately detecting significant associations between pollution exposure patterns and ACS risk, which are often missed by traditional manual feature extraction methods.\n3) Enhanced interpretability: We employ attention mechanisms and feature importance analysis to uncover potential interactions between air pollution, clinical risk factors, and the onset of ACS. This approach ensures that the model's predictions are both accurate and interpretable, providing deeper insights into how environmental and clinical factors contribute to ACS risk.\nThis paper is structured as follows: Section 2 reviews related work, Section 3 details the proposed TabulaTime framework, and Sections 4 and 5 present experimental evaluations and discussions. The conclusion summarises findings and implications for future research."}, {"title": "2 Related Work", "content": "Acute Coronary Syndrome (ACS) risk prediction methods are essential tools for estimating an individual's likelihood of experiencing cardiovascular events. Traditional methods for predicting ACS risk such as Framingham Risk Score (FRS) [6], QRISK [3], Reynolds Risk Score [7] and European SCORE System [8] utilise a combination of demographic, clinical, lifestyle and medical history data to provide an overall risk assessment, but frequently cannot be utilised once a diagnosis of underlying cardiovascular disease has been made. The data used can be categorised as follows: 1) Demographic Information: Age, gender, and family history of cardiovascular diseases [25]. 2) Clinical History: Previous incidences of myocardial infarction, angina, hypertension, diabetes, and hypercholesterolemia. 3) Laboratory and and Clinical Tests: Blood pressure readings, cholesterol levels, and creatinine levels. 4) Electrocardiogram (ECG) Readings: Used to diagnose the type and severity of myocardial infarctions (e.g., Discrimination between STEMI and NSTEMI) [26].\nThese traditional methods systematically assess cardiovascular event risk. The FRS, Reynolds Risk Score, National Early Warning Score (NEWS) [27] and Global Registry of Acute Coronary Events (GRACE) ('Rationale and Design of the GRACE (Global Registry of Acute Coronary Events) Project: A Multinational Registry of Patients Hospitalized with Acute Coronary Syndromes' 2001) primarily relied on point-based systems incorporating traditional clinical and lifestyle factors. Each risk factor was assigned a specific number of points based on predefined scales. The QRISK algorithm employed a comprehensive multivariate approach using a wide array of data points to reflect a diverse population. The European SCORE system utilised regional risk charts to account for geographic variability in cardiovascular disease prevalence."}, {"title": "2.2 Machine Learning in ACS Prediction", "content": "While these traditional models have been instrumental in guiding clinical practice, they have notable limitations in adaptability, data integration, and predictive accuracy. Machine learning/deep learning methods offer significant advantages in these areas, providing more dynamic, accurate, and comprehensive risk assessments by leveraging advanced data analytics and continuous learning capabilities [28]. Wu et al. [29] utilised machine learning to predict in-hospital cardiac arrest in ACS patients, finding that the XGBoost model outperformed traditional risk scores such as GRACE and NEWS, achieving high accuracy and AUC. A total of 45 risk features were selected in this work from the electronic health record including age, gender, history of smoking and laboratory features, Killip classification, vital signs, mental status, etc. Similarly, Hadanny et al. [30] used Random Survival Forest (RSF) and deep neural network (DeepSurv) models to predict 1-year mortality in ACS patients, highlighting the improved performance of RSF over traditional methods. Acute Coronary Syndrome Israeli Survey (ACSIS) and the Myocardial Ischemia National Audit Project (MINAP) data were used in this work. 69 risk factors including demographics, prior medical history, prior medication, clinical presentation, basic laboratory data with admission were selected and evaluated."}, {"title": "2.3 Environmental Factors and ACS", "content": "The effects of environmental factors on ACS have also been increasingly studied. Ku\u017ama et al. [37] investigated the short-term impact of air pollution on ACS incidence in industrial versus non-industrial areas, finding significant associations between air pollution and ACS admissions. Chen et al. [10] examined hourly air pollutant concentrations and their association with ACS onset, employing a case-crossover design that revealed a strong link between short-term exposure to pollutants and increased ACS risk. Gestro et al. [38] developed models to analyse the delayed effects of air pollutants on emergency department admissions for ACS, underscoring the need for continuous air quality monitoring.\nHowever, environmental time series data have more complex temporal properties than clinic data. Current approaches often rely on manually selected features from time series datasets, including basic statistical measures such as mean, variance, and autocorrelation. These methods may not capture all relevant information, particularly complex, non-linear patterns. Recently, deep learning has emerged as a powerful tool for time series analysis, especially in examining the impact of air pollution on health outcomes. Various deep learning algorithms have been employed to analyse time series environmental data, such as Recurrent Neural Networks (RNNs) [39, 40], Multi-Layer Perceptrons (MLPs)[13], Convolutional Neural Networks (CNNs)[14], and Transformers [41].\nRNNs are specifically designed for sequence data, making them a natural fit for time series analysis. Their internal state allows them to retain information from previous inputs, which is crucial for understanding temporal dependencies. For instance, Villegas et al. [42] proposed a predictive model for COVID-19 mortality risk using RNNs with attention mechanisms to enhance interpretability. Results indicate that the RNN model outperforms traditional baselines like Support Vector Classifier and Random Forest in sensitivity and overall stability.\nMLPs, though simpler than other models, can effectively model temporal dependencies when applied along the temporal dimension. MLP methods encode these temporal dependencies into the fixed parameters of MLP layers, adopting the MLP framework along the temporal axis. This approach allows MLPs to model sequential patterns in time-series data, providing a straightforward yet powerful means to analyse complex temporal relationships. Suttaket and Kok [43] introduced Rational Multi-Layer Perceptrons (RMLP) as a novel interpretable predictive model for healthcare, addressing the limitations of deep learning's black-box nature. The model combined the strengths of weighted finite state automata and multi-layer perceptrons to process sequential data from electronic health records (EHRs). The study demonstrated that RMLP achieved strong predictive accuracy and enhanced interpretability across six clinical tasks.\nCNNs are traditionally used for image processing but have been adapted for time series analysis [44]. The key advantage of CNNs is their ability to capture local patterns and hierarchical features through convolutional operations.\nTransformers, introduced by Vaswani et al. [15], have revolutionised sequence modelling by relying on self-attention mechanisms rather than recurrent structures. This allows for parallel processing of data and better handling of long-range dependencies. Ni et al. [45] compared traditional time series models such as ARIMA and Prophet with advanced transformers based models for predicting heart rate dynamics. The study demonstrated that deep learning approaches, particularly transformer-based models like PatchTST [46], significantly outperformed traditional methods in capturing complex temporal dependencies and non-linear relationships.\nDespite their success, each of these models has specific strengths and limitations that make them suitable for different aspects of time series analysis. RNNS and LSTMs are excellent for handling temporal dependencies but are computationally intensive and complex. MLPs are simpler but less effective for complex time series compared to RNNS or LSTMs and less flexible for variable-length sequences due to requiring fixed input sizes. CNNs excel at capturing local patterns but often miss long-term dependencies. Transformers are powerful but have quadratic complexity relative to sequence length, making them computationally expensive for very long sequences.\nMoreover, to the best of our knowledge, there are a few studies [47] combining time series air pollution analysis and clinical data using deep learning algorithms for ACS prediction. Integrating heterogeneous data types remains challenging. Multimodal deep learning frameworks are required to effectively combine clinical and environmental data, addressing issues related to alignment, synchronization, and fusion of multimodal information."}, {"title": "3 The Proposed Method", "content": "In this study, we propose a multimodal deep learning framework for modelling the effect of air pollution on ACS presentation by integrating time-series data with tabular data, named TabulaTime. Figure 1 illustrates the flowchart of the TabulaTime framework, which comprises three main components:\n1) Input embedding. This component converts raw input data, such as words, images, or time series data, into dense, continuous vectors that a deep learning model can process. This transformation enables the model to interpret and utilise the data more effectively. For instance, clinical tabular data are embedded into vectors, allowing the model to learn simultaneously from both clinical and other data types. This enhances the model's ability to capture complex patterns and relationships, thereby improving its overall predictive performance.\n2) Patched RWKV module (PatchRWKV) for automatic time-series sequential data feature extraction, which includes patching and RWKV (Receptance Weighted Key Value). The patching process divides time-series data into fixed-size segments, enabling the model to focus on short-term patterns within each segment. The RWKV module serves as the backbone for feature extraction, combining the strengths of recurrent neural networks (RNNs) and attention mechanisms to efficiently process time series sequences. PatchRWKV excels at identifying both short-term spikes and long-term trends in the data, providing a comprehensive representation of temporal dependencies. This approach efficiently processes long sequences while maintaining linear computational complexity, making it more efficient than traditional RNNs and transformers, which have quadratic complexity. By capturing both short-term and long-term patterns, PatchRWKV offers a thorough representation of temporal dependencies, crucial for accurately detecting significant associations between pollution exposure patterns and ACS risk, often missed by traditional manual feature extraction methods.\n3) The multimodal feature integration and ACS prediction. In this part, we leverage attention mechanisms to integrate embedded tabular data features with extracted time-series features from the Patched RWKV module. By generating an attention map, the model assigns weights to different features, aligning data from various types and providing explanations for the model's decisions. This integration enhances the model's predictive accuracy and interpretability, ensuring that the combined information from different data sources contributes effectively to the ACS risk prediction."}, {"title": "3.2 Input Embedding", "content": "In this work, the input embedding consists of two parts: embedding clinical tabular data and embedding air pollution time series data.\nEmbedding clinical tabular data involves several crucial steps as follows: First, handling missing values is essential; we fill missing data with the mean value of the respective attribute to minimise the impact on model prediction. Next, we encode categorical variables using One-Hot encoding to convert them into binary vectors, allowing the model to process categorical information effectively. Following this, data normalisation and standardisation are applied, transforming features to have a mean of 0 and a standard deviation of 1, which improves model performance by ensuring consistency in data scale. Finally, all features are concatenated into a single feature vector, serving as the input for model integration, enabling the deep learning framework to utilise the comprehensive dataset efficiently.\nEmbedding air pollution time series data includes patching and patch embedding, which will be described in detail in a later section."}, {"title": "3.3 PatchRWKV for Time-Series Feature Extraction", "content": "The PatchRWKV module is a core component of the TabulaTime model, designed to handle and extract features from multivariate time-series data, such as air pollution data. This module allows the model to manage both short-term spikes and long-term trends, providing a comprehensive representation of temporal dependencies crucial for accurate analysis. It integrates Recurrent Neural Networks (RNNs) and attention mechanisms to efficiently process long sequence data while maintaining linear computational complexity. The PatchRWKV consists of three key components: Patching, Patch Embedding, and the RWKV encoder. The rationale behind the model design is as follows:\n1) Patching: Patching segments time-series data into fixed-size patches, allowing the model to focus on local temporal patterns within each patch and effectively capture short-term dependencies and variations. This enhances locality and semantic understanding, helping the model efficiently learn from short-term dependencies.\n2) Patch Embedding: The patch embedding component converts the patched data into a format that the model can process. This step ensures that the data is prepared for deeper analysis by the subsequent components.\n3) RWKV Encoder: Multivariate time series data consists of multiple channels, each representing different types of measurements taken over time. To extract effective feature representations, it is crucial to understand the relationships between different time periods (Time-Mixing) and between different features (Channel-Mixing). The RWKV encoder serves as the backbone architecture of the model, incorporating Time-Mixing and Channel-Mixing to enhance the model's ability to detect significant patterns and improve predictive performance. Time-Mixing involves using a linear combination of current and previous time steps along with a multi-head Receptance Weighted Key Value (RWKV) operator to capture temporal dependencies across different time patches. This ensures that the model effectively learns the relationships between data points over various time steps, which is crucial for understanding how past events influence future outcomes in a time series. Channel-Mixing is the process of using a vector of all the features from the time series and projecting it into the embedding space. This approach combines information across different channels using robust non-linear operations. Channel-Mixing helps the model learn the relationships between different types of data collected at the same time step, enhancing its ability to interpret multivariate time series data effectively.\nThe following sections detail these components."}, {"title": "3.3.1 Patching", "content": "The patching process consists of two stages: normalisation and patching. Instance normalisation has recently been shown to mitigate the distribution shift effect between training and testing data. Therefore, we first normalise each univariate time-series pollution dataset to have a zero mean and unit standard deviation before applying patching. Each input univariate time series is then tokenised through patching, with patches being either overlapping or non-overlapping, depending on the patch length (P) and stride (S). These patches are projected into a lower-dimensional space using a learnable projection matrix, reducing the number of input tokens and computational complexity. In this study, we use hourly air pollution data and aim to capture localised features of pollution for each day. Consequently, we set the patch size to 24 and do not use overlapping patches."}, {"title": "3.3.2 Patch Embedding", "content": "In patch embedding, each patch is embedded into a higher-dimensional vector by applying a linear transformation to flatten it. Then each patch is represented as a vector, which captures the essential information of that patch that can be processed by the RWKV model."}, {"title": "3.3.3 RWKV (Receptance Weighted Key Value) Encoder", "content": "The RWKV architecture is designed to learn feature representations of time-series data. It consists of stacked residual blocks, each containing a Time-Mixing and a Channel-Mixing sub-block (Figure 2 and Figure 3). These sub-blocks use recurrent structures to incorporate past information effectively."}, {"title": "Time Mixing Module", "content": "To achieve time mixing in RWKV, the model interpolates between the inputs of the current and previous time-steps. Given an input feature of $x_t$ and previous step $X_{t-1}$, a linear projection of the combination of the shifted previous step and the current step is performed using the projection matrix within the block. This process involves creating a weighted blend of $x_t$ and $x_{t-1}$ to capture temporal dependencies effectively.\n$R_t = W_r \\cdot (\\mu_\\gamma \\odot X_t + (1 - \\mu_\\gamma) \\odot X_{t-1})$\n$K_t = W_k \\cdot (\\mu_\\kappa \\odot X_t + (1 - \\mu_\\kappa) \\odot X_{t-1})$\n$V_t = W_v \\cdot (\\mu_\\upsilon \\odot X_t + (1 - \\mu_\\upsilon) \\odot X_{t-1})$\nWhere $W$ means the weight signifying the positional weight decay vector, a trainable parameter within the model. R is the receptance vector acts as the receiver of past information. K is the Key vector performs a role analogous to K in traditional attention mechanisms. V: The Value vector functions similarly to V in conventional attention processes.\nThen a Multi-head WKV Operator is used to operate the attention mechanism but with a linear time and space complexity. This recurrent behaviour in RWKV is articulated through the time-dependent update of the WKV vectors. The formula of single head WKV operator is given by.\n$wkv_t = diag(u) \\cdot kv + \\sum_{i=1}^{t-1} diag(w)^{(t-1-i)} \\cdot k \\cdot V_i$\nwhere w and u are two trainable parameters. The parameter u is a bonus that rewards the model for encountering a token for the first time, specifically the current token. This ensures the model pays more attention to the current token, preventing any potential degradation of w. Another important parameter is w, which is a channel-wise time decay vector per head. Furthermore, we transform parameter w within the range (0,1), ensuring that diag(w) represents a contraction matrix.\nLike the transformer structure, we use the multi-head WKV to enhance the model's capacity which is formally described by the following equation:\n$MHwkv_t = Concat (wkv,..., wkv)$\nWhere h is the number of heads. Finally, an output gating mechanism controls the flow of information from the recurrent unit to the next layer or the final output. This gating is implemented using the SiLU activation function and receptance. The output vector $o_t$ per head is given by:\n$O_t = (SiL U (g_t) LayerNorm (r_t \\cdot wkv_t)) W_\\circ$\nWhere Layer Norm operates on each of h heads separately. This gating ensures that only relevant information is passed forward, improving the model's ability to make accurate predictions by filtering out noise and focusing on significant data."}, {"title": "Channel Mixing Module", "content": "In the channel-mixing block, channels are mixed by strong non-linear operations as follow:\n$k_l = W \\cdot (\\mu'_\\kappa x_t + (1 - \\mu'\\kappa) \\odot X_{t-1})$\n$r_l = W \\cdot (\\mu' x_t + (1 - \\mu'\\kappa) \\odot X_{t-1})$\n$v = ReLU^2 (k) W$\n$o_l = Sigmoid (r) v$\nwhere we adopt the squared ReLU activation function to enhance the non-linearity."}, {"title": "3.4 Multimodal Feature Integration and Prediction", "content": "Feature integration and prediction in the TabulaTime model combine extracted features from time-series air pollution data with tabular clinical data for ACS risk prediction (Figure 4). This model integrates tabular data embedding feature representations of time-series data into an attention module, enabling the network to learn attention maps conditionally based on the tabular data. This integration enhances the network's ability to pinpoint what, where, and when to focus on in the data, leading to improved performance in tasks involving both time series and tabular data. Specifically, the tabular data is embedded into the same dimension as the feature representations of the time-series data and passed through shared layers. This allows the attention maps to be computed conditionally on the tabular data, facilitating a more nuanced and effective feature integration process.\nGive the input $X \\in R^C$, the feature integration can be formulated as:\n$1.Xc))cXc$\nWhere $W_1 \\in R^{L \\times C}$ and $W_2 \\in R^{C \\times L}$ are the weight matrices of the two fully connected layers. denotes the ReLU activation function. $\\sigma (W_2 \\cdot \\delta (W_1 \\cdot X_c))$ refers to the generated attention map. As a final step, a MLP is added at the end of the model to predict the ACS risk.\nThis process ensures that the model effectively integrates and leverages both the tabular and time-series data, leading to more accurate and reliable ACS risk predictions."}, {"title": "4 Experimental Evaluation", "content": "To evaluate the efficacy of the TabulaTime model, we conduct three comprehensive experiments, each designed to assess distinct aspects of its performance:\n1) Model Performance Analysis\nThis experiment evaluates the predictive accuracy of TabulaTime for Acute Coronary Syndrome (ACS) risk. We test the model using varying air pollution data durations (3, 7, 10, and 14 days) to identify the optimal time window. Subsequently, we compare its performance against traditional machine learning models, including Random Forest, LightGBM [48], and CatBoost[49], under two scenarios: (a) incorporating air pollution data, and (b) excluding air pollution data. These comparisons quantify the significance of environmental factors in enhancing predictive accuracy. Additionally, the generalizability of TabulaTime are assessed across varying time periods and air conditions.\n2) PatchRWKV Module Evaluation\nThis experiment focus on the PatchRWKV module's capability in feature extraction from time-series data. Using three publicly available datasets, we benchmark its performance in both classification and forecasting tasks, highlighting its ability to efficiently capture complex temporal patterns. We select several State-of-the-art models and referenced their results, which includes most recent and extensive empirical studies on time-series. The selected models include:\nMulti-Layer Perceptrons (MLPs) -based models: LightTS [50], LightTS is a MLPs model which is straightforward and computationally efficient, suitable for real-time and large-scale applications.\nConvolutional Neural Networks (CNNs)-based models: TimesNet [51], TimesNet is a CNN model which is efficient in capturing local patterns and hierarchical features in time-series data, providing a solid comparison for the feature extraction capabilities of the PatchRWKV module.\nTransformers-based models: Autoformer [52], PatchTST [46]. Transformer have revolutionised sequence modelling with their self-attention mechanisms, excelling in capturing long-range dependencies. Both Autoformer and PatchTST represent state-of-the-art approaches in time-series analysis with their decomposition mechanism and patching mechanism.\nRecurrent Neural Networks (RNNs) -based models: Gated Recurrent Unit (GRU) [53] and Long Short-Term Memory (LSTM) [54] are designed for sequence data and known for modelling temporal dependencies effectively. These models provide a comparison for the recurrent structures within the PatchRWKV module.\n3) Feature Importance and Model Interpretability\nThis study aim to understand the interplay between clinical and environmental factors in ACS STEMI VS NSTEMI prediction. By analyzing feature importance scores, we identified the key predictors and their relative contributions, enhancing the model's interpretability and offering valuable insights for targeted interventions. The feature importance is calculated using permutation feature importance[55]. This method assesses the increase in the model's prediction error when a feature's values are shuffled. If shuffling a feature increases the model error, the feature is considered important, as the model relies on it for predictions. Conversely, if shuffling does not affect the model error, the feature is deemed unimportant, as the model ignores it for predictions. We first calculate feature importance as the drop in the model's validation metric when a feature value is randomly shuffled. Then, we calculate step importance to show the significance of each time period."}, {"title": "4.2 Dataset Description", "content": "The study utilises two primary datasets to evaluate the performance of the TabulaTime model: 1) Salford MINAP dataset which contains clinical data from the Myocardial Ischaemia National Audit Project (MINAP) collected in Salford, UK. 2) Air Pollution Dataset which contains hourly measurements of various air pollutants collected from the Salford Eccles monitoring station in Salford, Greater Manchester, United Kingdom. 3) Additionally, to evaluate the performance of PatchRWKV, we have also used three public available time series datasets including Weather [56], Heartbeat Monitoring Dataset and Self-Regulation of Slow Cortical Potentials (SCP) Dataset."}, {"title": "4.2.1 Salford MINAP Dataset", "content": "The Myocardial Ischaemia National Audit Project (MINAP) is a domain within the National Cardiac Audit Programme (NCAP) that contains information about the care provided to patients who are admitted to hospital with acute coronary syndromes (heart attack). It is collected and analysed to illustrate the 'patient journey' from a call to the emergency services or their self-presentation at an Emergency Department, through diagnosis and treatment at the hospital, to the prescription of preventive medications on discharge. A pseudonymised dataset was obtained under ethical approval (REC reference: 22/YH/0250) after confidentiality advisory group review (CAG reference: 22/CAG/0155) for use in this study.\nIn this work, 21 risk factors including patient demographics, clinical history, medications, vital signs and diagnostic indicators are selected to predict the ACS risk and differentiate between STEMI and NSTEMI based on their relevance and significance in clinical and environmental contexts.\nFactors such as previous Acute Myocardial Infarction (AMI), angina, hypertension, hypercholesterolemia, peripheral vascular disease, cerebrovascular disease, asthma/COPD, chronic renal failure, heart failure, smoking status, diabetes, previous PCI (Percutaneous Coronary Intervention), and previous CABG (Coronary Artery Bypass Graft) are included. These factors are crucial as they provide a comprehensive view of a patient's medical history and risk factors associated with cardiovascular diseases [57, 58]. Use of ACEI or ARB (Angiotensin-Converting Enzyme Inhibitors or Angiotensin II Receptor Blockers), systolic blood pressure (BP), height, weight, BMI (Body Mass Index), family history of coronary heart disease (CHD), creatinine levels, and statin use are considered. These factors help in assessing the ongoing treatment, physiological conditions, and genetic predisposition of patients. The detailed risk factors are shown in Table 1.\nBased upon electrical heart tracings \u2013 electrocardiograms or ECGs recorded during a heart attack, patients are diagnosed as having suffered either ST-elevation myocardial infarction (STEMI) or non-ST elevation myocardial infarction (NSTEMI). In the annual NCAP report, the terms 'higher-risk' and 'lower-risk' have been used to differentiate STEMI from NSTEMI."}, {"title": "4.2.2 Air Pollution Dataset", "content": "This air pollution dataset contains hourly measurements of various air pollutants collected from the Salford Eccles monitoring station (UKA00339) in the United Kingdom. The time range covered by the data is from January 1, 2016, to December 31, 2019. The primary indicators monitored include levels of Nitrogen Oxides (NOx), Nitric Oxide (NO), Particulate Matter (PM10), and Nitrogen Dioxide (NO2), all measured in micrograms per cubic meter (\u00b5g/m\u00b3)."}, {"title": "4.2.3 Public Datasets", "content": "Weather[56], which is recorded every 10 minutes through the whole of the year 2020, contains 21 meteorological indicators, such as air temperature, humidity, etc. This dataset is used for time series forecasting. Heartbeat Monitoring Dataset[59] is used to classify and monitor heartbeats. The dataset consists of sequences of heartbeat signals, often represented as electrocardiogram (ECG) readings, which need to be classified into different categories such as normal or abnormal heartbeats. Self-Regulation of Slow Cortical Potentials (SCP) Dataset [60] is used for studying self-regulation of brain activity, specifically the ability to control slow cortical potentials (SCPs), which are slow voltage changes in the brain's electrical activity. The dataset includes sequences of brain activity measurements, which need to be classified to understand patterns related to self-regulation capabilities."}, {"title": "4.3 Performance Metrics", "content": "In this study, we evaluate the performance of our proposed TabulaTime model using several key metrics that are standard in the fields of classification and forecasting. These metrics provide a comprehensive view of the model's predictive capabilities and its effectiveness in handling both classification tasks (e.g., predicting STEMI or NSTEMI) and forecasting tasks (e.g., forecasting air pollution levels)."}, {"title": "4.3.1 Classification Metrics", "content": "For classification tasks such as distinguishing between STEMI and NSTEMI cases", "used": "nAccuracy: Accuracy is a widely used metric that represents the proportion of correctly predicted instances among the total instances.\nAccuracy = $\\frac{TP + TN"}, {"TN}$\nPrecision": "Proportion of true positive predictions out of all positive predictions made by the model", "reliability.\nRecall": "Proportion of true positive predictions out of all actual positive cases", "FP}$\nF1-Score": "Harmonic mean of Precision and Recall, balancing both metrics for a"}]}