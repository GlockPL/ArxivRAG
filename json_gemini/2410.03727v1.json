{"title": "FAITHEVAL: CAN YOUR LANGUAGE Model StaY\nFAITHFUL To Context, EVEN IF \u201cTHE MOON IS\nMADE OF MARSHMALLOWS\u201d", "authors": ["Yifei Ming", "Senthil Purushwalkam", "Shrey Pandit", "Zixuan Ke", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "abstract": "Ensuring faithfulness to context in large language models (LLMs) and retrieval-\naugmented generation (RAG) systems is crucial for reliable deployment in real-\nworld applications, as incorrect or unsupported information can erode user trust.\nDespite advancements on standard benchmarks, faithfulness hallucination-where\nmodels generate responses misaligned with the provided context-remains a sig-\nnificant challenge. In this work, we introduce FaithEval, a novel and compre-\nhensive benchmark tailored to evaluate the faithfulness of LLMs in contextual\nscenarios across three diverse tasks: unanswerable, inconsistent, and counterfac-\ntual contexts. These tasks simulate real-world challenges where retrieval mecha-\nnisms may surface incomplete, contradictory, or fabricated information. FaithE-\nval comprises 4.9K high-quality problems in total, validated through a rigorous\nfour-stage context construction and validation framework, employing both LLM-\nbased auto-evaluation and human validation. Our extensive study across a wide\nrange of open-source and proprietary models reveals that even state-of-the-art\nmodels often struggle to remain faithful to the given context, and that larger\nmodels do not necessarily exhibit improved faithfulness.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid development of large language models (LLMs) has significantly advanced natural language\nunderstanding and generation tasks, enabling systems to produce fluent and coherent responses\nacross a variety of applications (Bubeck et al., 2023; Zhao et al., 2024b; Wu et al., 2024). The\ncapabilities of these models have been further enhanced by integrating external information from\nthe Internet or knowledge sources using a popular approach of Retrieval-Augmented Generation\n(RAG) (Lewis et al., 2020; Zhao et al., 2024a). In this paradigm, generated outputs are enhanced\nby retrieving and encoding relevant information as context to the model. While RAG facilitates\nthe integration of additional knowledge, hallucination\u2014where models generate unsupported or\nungrounded content-remains a critical challenge (Nguyen et al., 2024).\nHallucination in LLMs can be generally categorized into two types: factual hallucination, where\ngenerated content deviates from established world knowledge, and faithfulness hallucination, where\nthe generated response is inconsistent with the provided context (Huang et al., 2023a). While factuality\nhas received extensive attention, with numerous benchmarks designed to evaluate correctness against\ncommon sense or world knowledge (Lee et al., 2022; Min et al., 2023; Chern et al., 2023; Wei et al.,\n2024), a fine-grained and holistic evaluation of faithfulness on noisy contexts remains underexplored,\nparticularly when the context contradicts commonly accepted facts. Maintaining faithfulness to the\ncontext is especially important for various personalized applications and can be critical in high-stakes\ndomains such as healthcare, finance and law, where inaccurate or ungrounded responses can erode\nuser trust and lead to severe consequences (Bommarito & Katz, 2022; Pal et al., 2023).\nOne of the key challenges in addressing faithfulness hallucination in RAG stems from the retrieval\nprocess, where the wealth of documents on the Internet varies in credibility. This complexity is"}, {"title": "2 RELATED WORKS", "content": "Contextual LLM and retrieval-augmented generation. As the demand for contextual LLMs\ncontinues to grow, retrieval-augmented generation (RAG) systems offer a promising solution by\nintegrating external knowledge retrieval with LLMs (Lewis et al., 2020; Sarto et al., 2022; Ramos\net al., 2022; Huang et al., 2023b; Zhao et al., 2024a). In RAG, model responses are grounded using\nknowledge sourced from private or open-access data repositories. A typical RAG system operates\nthrough a close interaction between the retriever and a generator. The retriever (Meng et al., 2024;\nChen et al., 2024a; Li et al., 2023a) identifies relevant documents from the source, and this retrieved\ninformation is supplied to the generator (e.g., a language model) to produce grounded outputs (Lewis\net al., 2020; Borgeaud et al., 2021; Izacard & Grave, 2020; Ke et al., 2024). Recent works develop\nmore sophisticate RAG frameworks to improve answer reliability (Asai et al., 2023; Li et al., 2024c;\nXu et al., 2024). The increased context sizes in LLMs have improved their ability to handle longer text\nsequences, allowing them to handle complex tasks requiring extensive background knowledge (Gao\net al., 2023; Song et al., 2024). These improvements are particularly beneficial for long-form question\nanswering (Joshi et al., 2017; Kwiatkowski et al., 2019b; Li et al., 2024a). However, the variation in\nsource quality can exacerbate challenges to maintaining faithfulness in LLMs, especially in longer\ncontexts retrieved from the Internet.\nHallucination and faithfulness evaluation. Hallucination in LLMs refers to the generation of\nungrounded content, either from the provided context or established world knowledge. The former is\ntypically described as factuality hallucination, while the latter is known as faithfulness hallucination,\nwhich highlights the discrepancy between the model's output and the context (Huang et al., 2023a).\nWhile there is rich literature on factuality evaluation and benchmarks with and without contexts (Lee\net al., 2022; Min et al., 2023; Chern et al., 2023; Wei et al., 2024), faithfulness has mostly been\nexplored for summarization (Laban et al., 2023; Jia et al., 2023) and natural language explana-\ntions (Atanasova et al., 2023; Siegel et al., 2024). Another line of research focuses on hallucination\ndetection (Liu et al., 2021; Li et al., 2023b; Hu et al., 2024), which aims to detect hallucinated outputs.\nThe task focuses on the model output instead of the context (input). Additional efforts have been\nmade to create QA benchmarks based on common misconceptions (Lin et al., 2022) or questions that\nare unanswerable by nature (Yin et al., 2023). However, none of these datasets are contextual. In\ncontrast, each question in FaithEval is accompanied by a multi-paragraph context, mimicking RAG\nscenarios with long and noisy contexts.\nAdversarial context generation. Generating challenging or adversarial contexts for language\nmodels has been explored in various scenarios. One line of research focuses on context modification.\nShi et al. (2023) propose a template-based framework that adds irrelevant facts to the context and\nstudies the effectiveness of different prompting techniques. Yu et al. (2024) leverage LLMs to\nperturb original evidence, potentially altering the answers, while Manakul et al. (2023) utilize\nLLMs to generate purely synthetic contexts that support given statements. To study the impact of\nknowledge conflicts, Xie et al. (2024) use LLMs to create adversarial contexts that conflict with the\nmodels' internal knowledge, improving coherence compared to previous word-level editing methods\n(Longpre et al., 2021; Chen et al., 2022). Another research direction involves modifying the question.\nRamakrishna et al. (2023) generate invalid (unanswerable) questions, while Huang et al. (2024) build\na small-scale dataset with 209 questions containing adversarial facts or incorrect information based\non diverse templates. In contrast, we introduce the first fine-grained, larger-scale (4.9K) high-quality\ncontextual QA benchmark featuring multi-paragraph coherent contexts across three diverse tasks."}, {"title": "3 FAITHEVAL BENCHMARK", "content": "3.1 TASK OVERVIEW\nTo systematically evaluate the contextual faithfulness of LLMs, FaithEval contains three diverse\ntasks including unanswerable context, inconsistent context, and counterfactual context. Each sample\n(c, q, a) consists of a question q, and a long context passage made up of one or more documents\nc = (d1, ..., dn), and a groundtruth answer a. The model is expected to answer the question leveraging\nthe information in the provided context. An overview of each task is presented in Figure 2. Next, we\nillustrate the construction of each task in detail."}, {"title": "Unanswerable Context", "content": "An unanswerable context arises when the context includes relevant details\nbut lacks the information needed to answer the question. In FaithEval, answerability is determined\nsolely by the context, regardless of whether the question itself is unanswerable. For instance, in the\nexample in Figure 2 (Left), the context provides the proportion of both types of commuters in 2015.\nHowever, the question \u201cWhich group of commuters in Dallas in 2009 is larger: carpooling or transit?\u201d\nis unanswerable, as the context lacks specific data from 2009. To create such task, we modify the\ncontext from a collection of 10 contextual QA datasets, covering a wide range of domains (see Source\ndatasets below). For each sample, we prompt an LLM to modify the original context so that it no\nlonger contains the supporting evidence for the ground truth answer. Additional sentences may be\nwoven into the new context to maintain coherence. The full prompt is shown in Figure 12. This\nprocess resulted in a total of 2.4k contextual QA pairs for the Unanswerable Context task. To verify\nthe quality of the modified contexts, we achieved over 98% agreement with professional human\nannotators (Sec 3.2)."}, {"title": "Inconsistent Context", "content": "An inconsistent context involves multiple documents, each providing a\ndifferent answer to the same question. This simulates noisy retrieval scenarios, where documents\nfrom sources with varying levels of credibility are retrieved. For instance, as shown in Figure 2\n(Middle), the context presents conflicting information about the tiger's name in the novel Life of Pi. A\nfaithful model should be able to identify such inconsistencies, especially when instructed to do so. To\ncreate this task, we modify contexts from the same collection of contextual QA datasets used in the\nUnanswerable Context task. For each sample, the LLM is provided with a context passage, a question,\nand an original answer, which is supported by the context. The goal is to modify the context so that\nit introduces fabricated supporting evidence for a new, conflicting answer. The detailed prompt is\nshown in Figure 13. Since this task is more challenging, we curated a collection of 1.5k high-quality\ncontextual QA pairs after filtering through professional human annotators."}, {"title": "Counterfactual Context", "content": "A counterfactual context contains statements that contradict with common\nsense or widely accepted facts, such as \"water freeze at 100 degrees Celsius\" or \"carbon dioxide is\nthe most abundant greenhouse gas in the lower atmosphere\". Unlike the other two tasks, the questions\nin this task are required to be relevant to such well-known facts. We curate this task based on ARC-\nChallenge (Clark et al., 2018), a QA dataset covering grade-school level, multiple-choice science\nquestions. Since the original dataset does not include context, we prompt an LLM to generate a long,\nmulti-paragraph context that seamlessly provides fabricated supporting evidence for a counterfactual\nanswer. The detailed prompt is shown in Figure 14. This process resulted in a total of 1k contextual\nQA pairs, each with three to five options. Due to the multiple-choice nature, we can use keyword\nmatching to verify the quality of the synthetic contexts (Appendix D)."}, {"title": "3.2 TASK CONSTRUCTION AND VALIDATION FRAMEWORK", "content": "Task construction. An overview of our task construction and validation framework is shown in\nFigure 3. Given a source QA sample and an original context (optional), we prompt an LLM to\ngenerate both a new context and a new answer for the Counterfactual and Inconsistent tasks, or only a\nnew context (that supports no answer) for the Unanswerable task. To make the tasks challenging, the\nnew context should be coherent and contain minimal modifications if the original context is provided.\nIn addition, multiple paragraphs not directly related to the answer are included, serving as distractors.\nThe new context is generated with detailed justifications explaining how it satisfies the task criterion.\nWe construct the prompt for each sample by combining the original question, the new context, and\ntask-specific instructions (Sec 3.3). In particular, an inconsistent context is created by concatenating\nthe new context with the original context.\nAuto validation and human annotation. We validate the quality of the new context by using a\nseparate LLM judge to verify whether the new answer is valid given the context (\"if\" condition)\nand whether the context does not support alternative answers (\"only-if\" condition). For example,\nthe new context in Figure 2 (Right) should not mention wood is buoyant. Samples that fail to\nmeet both conditions are filtered out. Next, we perform meticulous human annotation. Depending\non the task's validation difficulty, we employ different strategies. As the Inconsistent Context task\nis challenging to validate, we rely on full human annotation. Three Mechanical Turk (Crowston,\n2012) workers judge whether each contextual QA pair meets the \"if\" and \"only-if\" conditions, with\nfinal inclusion determined by majority agreement. This yields 1.5K samples. For the Unanswerable\nContext task, which is easier to validate, we use a similar majority-vote approach, achieving over 98%\nagreement among human annotators. This yields 2.4K samples. For the Counterfactual Context task,\nsince the answer options are provided within the context, we validate using a string-based matching\nmethod, where the context passes if all words from the answer appear in the context. This results in\n1K samples. After filtering, the FaithEval benchmark contains a total of 4.9K high-quality contextual\nQA pairs. More details are included in Appendix A."}, {"title": "3.3 EVALUATION", "content": "Models. We evaluate a wide range of competitive open-sourced and proprietary language models with\ndifferent scales, including the most recent releases up to Sep 10, 2024. Our initial experiments suggest\nthat instruction-tuned (chat) models significantly outperform base models. Therefore, we consider 18\ncompetitive chat models, including Phi-3-mini-128k-instruct (3.8B), Phi-3-medium-128k-instruct\n(14B), Phi-3.5-mini-instruct (3.8B) (Abdin et al., 2024), LLaMA-3-8B-Instruct, LLaMA-3.1-8B-\nInstruct, LLaMA-3-70B-Instruct, LLaMA-3.1-70B-Instruct (Llama, 2024), Mistral-7B-Instruct-v0.3,\nMistral-Nemo-Instruct-2407 (12B) (Jiang et al., 2023), Gemma-2-9B-it, and Gemma-2-27B-it (Team,\n2024). For proprietary models, we consider Open AI's GPT-3.5 Turbo, GPT-40-mini, GPT-40, GPT-4\nTurbo, Cohere's Command R (35B), Command R+ (104B), and Anthropic's Claude 3.5 Sonnet.\nDefault Evaluation Scheme. For all tasks, we append the following prompt to each question: You\nare an expert in retrieval-based question answering. Please respond with the exact answer, using only\nthe information provided in the context. For the Unanswerable Context task, we append an additional\ninstruction: If there is no information available from the context, the answer should be \"unknown\".\nSimilarly, for the Inconsistent Context task, the instruction is: If there is conflicting information or\nmultiple answers in the context, the answer should be \u201cconflict\". Our primary evaluation metric\nacross all tasks is accuracy (ACC), where a model's response is considered correct if it mentions the\nground truth answer. All models are evaluated using their default configurations with deterministic\ndecoding (temperature = 0). We report both strict-matching (S) ACC, which considers only a single\nground truth answer (e.g., \u201cunknown\u201d), and non-strict matching (N) ACC, which allows a broader\nrange of semantically similar phrases. A detailed list of valid phrases is provided in Appendix A.\nAlternative Evaluation Schemes. We study alternative evaluation strategies in Section 5. Specifically,\nwe examine non-deterministic decoding with temperature scaling (t = 0.3, top-p = 0.9). Additionally,\nwe investigate the impact of chain-of-thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022)\nusing the following instruction: Given the context, first provide a brief answer to the question. Then,\nexplain your reasoning step by step, detailing how you arrived at the answer."}, {"title": "4 MAIN RESULTS", "content": "4.1 UNANSWERABLE CONTEXT\nAbstaining is challenging, even when explicitly instructed. The results of the Unanswerable\nContext task are summarized in Figure 4, ranked by performance on the original context. Proprietary\nmodel names are highlighted in orange. We highlight the following key observations: (1) Modern\nLLMs experience significant performance degradation in this task. Across all chat models, the\nperformance gap ranges from 13.6% to 68.4%. (2) High performance on the original context does"}, {"title": "4.2 INCONSISTENT CONTEXT", "content": "Performance varies significantly on inconsistent context across model families. The model\nperformance on the Inconsistent Context task is summarized in Figure 5. We have the following key\nobservations: (1) Performance varies substantially across different model families. For instance, the\nPhi-3 series struggles to identify multiple answers or detect inconsistencies (conflicts), with an average\naccuracy of only 5.8%, whereas the GPT-4 series performs much better, with an average accuracy of\n89.35%. (2) Open-source models lag behind proprietary models. Unlike in the Unanswerable Context\ntask, where all models face challenges, it is evident that the top three models on the Inconsistent\nContext task are proprietary, significantly outperforming recent open-source models."}, {"title": "4.3 COUNTERFACTUAL CONTEXT", "content": "Faithfulness remains a limitation for contextual LLMs. The results on the Counterfactual Context\ntask are shown in Figure 6. The blue bars represent model performance under the closed-book QA\nsetting, where no context is provided. In this case, the models rely entirely on their parametric"}, {"title": "5 DISCUSSIONS AND FURTHER ANALYSIS", "content": "A closer look at Unanswerable and Inconsistent Contexts. We present the performance break-\ndown for each of the ten individual datasets in Figure 7 for the Unanswerable (top row) and\nInconsistent Context (bottom row) tasks. We include three representative smaller-scale models:\nLLama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, and Gemma-2-9b-it. Full results for other models\nare provided in Appendix C. We observe the following: (1) While smaller models demonstrate\ncompetitive performance on the original datasets, none are able to maintain this performance on the\nnewly introduced contexts. This suggests that strong results on common benchmarks may not neces-\nsarily translate to reliable performance in real-world retrieval systems where contexts are noisy. (2)\nAlthough performance across individual datasets varies by model family, SearchQA and TextbookQA\nconsistently pose greater challenges compared to the other datasets.\nA closer look at Inconsistent Context. Since an inconsistent context is created by concatenating\nthe original and new contexts, we separately evaluate the model's performance on the original context\nand the new context. The results, shown in Figure 8, reveal that while models struggle when both\ncontext passages are presented together (Figure 5), most models do not find the new context more\nchallenging than the original when it is presented alone. For example, Command R achieves 88%\naccuracy on the new context, compared to 81% on the original. This further underscores the difficulty\nof detecting conflicting evidence when multiple sources are involved."}, {"title": "Strict vs. non-strict matching.", "content": "In the Unanswerable and Inconsistent Context tasks, no explicit\noptions are provided in the prompt. As a result, LLMs may express concepts such as \"unknown\"\nor \"inconsistent\" in varying ways. To assess the impact of allowing alternative valid expressions,\nwe compare the performance of strict and non-strict matching. Strict matching only accepts the\nexact phrases \"unknown\" or \"conflict\" as specified in the prompt, while non-strict matching permits\na broader range of expressions that convey similar ideas (see Appendix A for the full list of valid\nexpressions). We observe that performance remains stable across most models. For instance, Figure 9\nsummarizes the results for Unanswerable Context. For competitive models such as gpt-40 and Claude\n3.5, the gaps are less than 1%. Full results can be found in Table 3 and Table 4 in Appendix C."}, {"title": "Sycophancy with task-specific instructions.", "content": "While the additional instructions used in the Unan-\nswerable and Inconsistent Context tasks (Sec 3.3) improve the model's awareness of such scenarios,\nthey can also introduce unintended effects when the context is normal (i.e., answerable and con-\nsistent). This can lead to what is known as sycophantic behavior (Perez et al., 2023; Wei et al.,\n2023), where models adjust their responses to align with the user's expectations, even when those"}, {"title": "Does chain-of-thought prompting improve faithfulness?", "content": "Popular prompting techniques, such\nas CoT, have shown promising performance on various tasks that require multi-step reasoning. We\nadopt the prompt format in Section 3.3 and summarize the results in Figure 10. It is evident that CoT\neffectively improves faithfulness over the Direct Answer prompt (default) for both Unanswerable and\nInconsistent Contexts across different model families. However, there still exists significant room for\nimprovement, especially on Unanswerable Context. For instance, the leading model achieves only\n71.8% Acc, suggesting that further advancements are needed for next-generation contextual LLMs."}, {"title": "Impact of decoding strategies.", "content": "By default, we adopt greedy decoding. We also investigate a popular\nsampling-based decoding scheme with a temperature of 0.3 and top-p of 0.9. The results are shown\nin Figure 11 based on Counterfactual Context. Similar observations also hold for Unanswerable and\nInconsistent Context. We can see that sampling-based decoding marginally improves the performance\nover greedy decoding across all models. However, the significant gap between the original and\ncounterfactual contexts cannot be mitigated with temperature scaling."}, {"title": "6 CONCLUSION", "content": "In this work, we propose FaithEval, a novel and challenging benchmark designed to assess the faith-\nfulness of contextual LLMs. FaithEval comprises 4.9K high-quality contextual problems spanning\nmultiple domains and includes three distinct tasks: unanswerable, inconsistent, and counterfactual\ncontexts. To build this benchmark, we propose a scalable multi-stage context construction and valida-\ntion framework, incorporating both automated evaluation by an LLM judge and human validation.\nThis approach enables the creation of multi-paragraph coherent contexts satisfying diverse criteria.\nWe provide a timely and in-depth study on a wide range of open-source and proprietary models, re-\nvealing that even the most competitive LLMs often struggle to remain faithful to the contexts, despite\nexcelling on standard benchmarks. We hope our work will contribute to more holistic evaluations of\ncontextual LLMs and inspire further advancements in developing faithful LLMs."}, {"title": "A ADDITIONAL EXPERIMENT DETAILS", "content": "Context Generation Model. By default, we use the latest GPT-40 (gpt-4o-2024-05-13) as the\ncontext generator. In our preliminary studies, we evaluated various alternatives, including GPT-40-\nmini, GPT-4-turbo, and LLaMA-3.1-70B. GPT-40 demonstrated superior performance in terms of\ncontext coherence, validity, and complexity.\nContext Evaluation Model. We use gpt-40-mini as the judge model for context verification. We\nselected gpt-40-mini for its faster inference speed, lower cost, and high judgment quality, with our\npreliminary study showing an agreement rate of over 95% when compared to GPT-4o as the judge.\nValid phrases for non-strict matching. The following keywords are considered valid for Unknown\nContext: \"unknown\", \"no answer\", \"no information\", \"not\", \"unclear\". For Inconsistent Context,\nthe valid phrases include: \"conflict\", \"conflicting\", \"disagreement\", \"inconsistent\", \"contradictory\",\n\"contradiction\", \"inconsistency\", \"two answers\", \"2 answers\", \"multiple answers\". These keywords\nwere selected based on an analysis of output patterns from open-source and proprietary models."}, {"title": "BPROMPT FOR CONTEXTUAL QA GENERATION", "content": "The system prompt for generating Unanswerable Context, Inconsistent Context, and Counterfactual\nContext are shown in Figure 12, Figure 13, and Figure 14, respectively. For Inconsistent Context, our\ninitial experiments suggest that a successful strategy is to decompose the generation of contextual\nQA into two steps. Step 1: generate a new answer that is fabricated and challenges common sense or\nwell-known facts. Step 2: generate a modified context with fabricated evidence that supports the new\nanswer. The model will output a JSON object containing the provided question, the provided old\nanswer, the new answer, the modified context, and a concise justification on (1) if the new answer is\nsupported by the new context (2) if all mentions of the old answer have been replaced or removed."}, {"title": "DVERIFICATION FOR COUNTERFACTUAL CONTEXT", "content": "For the Counterfactual Context task, since the answer options are provided within the context, we can\nvalidate the new context using a simple keyword-based matching method, where a context passes if\nthe answer phrase exists in the context. This results in a pass rate of 68.9%, where the new context\nclearly contains the new answer. The results on this filtered subset are shown in Figure 19. We\ncan see that the same trend still holds as shown in Section 4: a significant gap remains between the\nperformance on the original task (with no context) and the new task with counterfactual contexts,\nacross the majority of instruction-tuned models."}]}