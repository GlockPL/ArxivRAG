{"title": "Do LLMs have Consistent Values?", "authors": ["Naama Rozen", "Gal Elidan", "Amir Globerson", "Ella Daniel"], "abstract": "Values are a basic driving force underlying human behavior. Large Language\nModels (LLM) technology is constantly improving towards human-like dialogue.\nHowever, little research has been done to study the values exhibited in text gener-\nated by LLMs. Here we study this question by turning to the rich literature on value\nstructure in psychology. We ask whether LLMs exhibit the same value structure\nthat has been demonstrated in humans, including the ranking of values, and corre-\nlation between values. We show that the results of this analysis strongly depend on\nhow the LLM is prompted, and that under a particular prompting strategy (referred\nto as \"Value Anchoring\") the agreement with human data is quite compelling. Our\nresults serve both to improve our understanding of values in LLMs, as well as\nintroduce novel methods for assessing consistency in LLM responses.", "sections": [{"title": "Introduction", "content": "A key goal of Large Language Models (LLMs) is to produce agents that will be able to communicate\nin a \"human-like\" fashion. However, human communication is characterised by some level of\nconsistency within an individual, as well as variability between individuals. This raises a key\nquestion: during a single conversation with an LLM, does the \"LLM-persona\" resemble a single\nhuman? Furthermore, across multiple conversations, can LLMs produce multiple personas that\nresemble a population of humans? If this is indeed possible, how can such personas be elicited to\nbest resemble psychological characteristics observed in human populations?\nThis question has only recently begun to be addressed. For example [1] show how probing LLMs with\ndifferent names leads to variability which in some cases agrees with that of human populations. Here\nour focus is on understanding whether an LLM in a single conversation can exhibit a psychological\ncharacteristic profile that is similar to that of humans. This is a highly challenging question, since it\nrequires analyzing a complete conversation and evaluating whether it conceivably could have been\ngenerated by a single individual.\nIn order to stand on more quantitative ground to evaluating the quality of output relative to human\nresearch, we turn to the well established field of value psychology. Namely, we aim to quantify\nthe values that LLM responses are aligned with, and whether these are in agreement with the value\nhierarchy and structure observed in humans. The question of values in LLMs has rarely been\nstudied, and is of naturally broad interest. As an example of recent work, [2] prompt an LLM with a\ndescription of a profile of an individual characterized by a value and check whether generated text is\nconsistent with this description. Our focus is very different, and asks whether an LLM response is in\nagreement with what we expect human responses to look like given research in the field."}, {"title": "Related Work", "content": "Values in LLMs: Our work is based on the Schwartz theory of Personal Values, a highly accepted\ntheory within personality psychology [3]. Values are abstract goals, defining the end states individuals\naspire for (e.g., safety, independence), used to direct judgements and behaviors [6, 7]. Individuals\ntypically prioritize their values, so that values stemming from compatible motivations are similarly\nimportant, while values stemming from conflicting motivations are prioritized differently. These\nassociations were replicated across hundreds of samples, across the world [14, 15], and make value\ntheory especially useful to identify the coherence of the value profiles created by LLMs. Several\nstudies assumed LLMs can be characterized as operating on the basis of a single set of values, taking\nan \"LLMs as individuals\" approach. Fischer and colleagues [2] tested whether ChatGPT could\ncomprehend human values by providing it with value-related prompts and analyzing whether its\nresponses matched the intended value category. A second study [16], compared ChatGPT's values to\nthose observed in the World Value Survey, while another [17] investigated how temperature influences\nGPT-3's responses to the Human Value Scale. Scherrer and colleagues [18] studied responses of\nLLMs prompts evaluating moral positions, especially in ambiguous settings. A recent study [19],\ntested the value-like constructs embedded in LLMs and revealed both similarities and differences\nbetween LLMs and humans' values. Kovak et al. [20] challenged those studies by establishing\nthat context starkly influences values expressed by ChatGPT. They found significant variability in\nChatGPT's value expression in response to contextual changes, threatening the notion of stable\ncharacteristics of LLMs. Building upon the insights in [20], our study posits that upon providing\ncontrolled variability of context, LLMs can elicit a population of multiple personas. In this regard, we\naim to further explore the accuracy of LLMs' mimicking abilities within a controlled experimental\nframework.\nPrompting LLMs: There is extensive research on the prompt design for mimicking individual\ncharacteristics in LLM responses [21]. Approaches use specific scenarios [22], questionnaire items\n[23], simulation of social identities or areas of expertise [24], utilization of titles and surnames\nrepresenting genders and ethnicities [1], and other demographic information [25]. Additionally,\nresearchers explored the use of designated personas [26], and employed RLHF [27] to guide LLMs to\nreflect distinct personality traits. Despite this extensive body of work, to our knowledge, no study has\ndirectly compared the various prompting techniques to determine which approach yields responses\nthat simulate within-session psychological characteristics of an individual best.\nTemperature in LLMs: Adjusting the temperature stands as a common practice for introducing\nvariability in LLM responses [17]. However, consensus is lacking on the optimal temperature setting\nin simulating psychological characteristics. Some researchers advocate for higher temperatures to\nboost creativity [24], yet this can also introduce more noise into the data [28]. Conversely, setting the\ntemperature to zero minimizes variability, enhancing replicability [27], albeit posing challenges for\nvariance-dependent analysis [29]. Our framework enables us to explore how temperature adjustments\nimpact the ability of LLMs to simulate human characteristics across multiple datasets.\nEvaluating the Quality of Persona Generation in LLMs: The ability of LLMs to mimic and\nportray human characteristics is a focus of intense research [30, 31]. LLMs can express psychological\ntraits and attributes similar to human individuals [27, 32], and even simulate diverse populations\n[33, 24]. However, we are only beginning to understand the coherence of these LLM-generated\ncharacteristics in mirroring human psychological profiles [1, 20], and how to reliably produce such\nresponses. We are specifically challenged to evaluate the coherence of the resulting psychological\nprofiles. The literature suggested a number of approaches, including an open-ended interview with\nLLM-generated personas in order to assess the consistency between their intended characters and the\nresponses [34]. In addition, one may apply an additional \"judge\u201d LLM in order to check an LLM\npersona [35]. Finally, [23] assessed coherence with a description used to prompt the LLM. Our study\nextends upon this line of research by applying well established characteristics of human psychology\nto investigate the quality of LLM generated personas."}, {"title": "Method", "content": "In this section, we introduce the experimental design, models and prompts. The code and data are\nprovided as supplementary files in the submission."}, {"title": "Prompts", "content": "As mentioned above, we would like to measure the response of LLMs to PVQ-RR. However, as\nwith many other LLM applications, the way the model is prompted has a significant effect on output.\nThe instructions of the PVQ questionnaire were adjusted to suit LLMs. In cases of ambiguous\ngender identification in the prompt, we randomly allocated the female or male version to maintain\nconsistency of administration. LLMs were then prompted to assess their likeness to the 57 descriptions\nincorporated in the PVQ-RR. They were instructed not to provide additional explanations to maintain\nfocus on the task at hand. Following each prompt, the LLM was provided with all 57 items of the\nquestionnaire in one administration. The study utilized a basic prompt, as well as four different\nprompts below that vary instructions to create multiple personas.\nBasic prompt: This prompt mirrors the adapted instructions of the PVQ-RR questionnaire without\nadditional modifications. The prompt is structured as follows: \u201cFor each of the following descriptions,\nplease answer how much the person described is like you from 1 (Not like me at all) to 6 (Very much\nlike me), without elaborating on your reasoning.\".\nValue Anchor prompt: This prompt adds an anchor of value importance using identification with\nan item used in an additional value questionnaire, akin to the approach outlined in the study by\n[23]. Participants are instructed as follows: \u201cFor each of the following descriptions, please answer\nhow much the person described is like you from 1 (Not like me at all) to 6 (Very much like me),\nwithout elaborating on your reasoning. Answer as a person that is [value]", "[value]\" is taken\nfrom the Best-Worst Refined Values scale [36]. As a result, the prompts refer conceptually to the\nsame values that are measured using the PVQ-RR, yet do not refer directly to the value items to\nbe answered in response to the prompt. Examples of these anchor items include \"protecting the natural\nenvironment from destruction or pollution\u201d (universalism-nature) or \u201cobeying all rules and laws\"\n(conformity-rules). Please refer to E in the appendix for the complete list of anchor items.\nDemographic prompt": "Drawing from the methodology of [25], this prompt extends the original\nprompt by incorporating additional demographic details. LLMs are asked to provide ratings based\non the following prompt: \u201cFor each of the following descriptions, please rate how much the person\ndescribed is like you, using a scale from 1 (Not like me at all) to 6 (Very much like me), without\nelaborating on your reasoning. Answer as a [age]-year-old who identifies as [gender], working in the\nfield of [occupation], and enjoys [hobby].", "prompt": "In line with the methodology of [40], we directed the models to craft\npersonas. Our instruction was formulated as: \u201cCreate a persona (2-3 sentences long):\u201d, with the\ntemperature set at 0.7 to stimulate the models' creativity. An example of a persona generated by\nGemini Pro is as follows: \u201cEmily is a 25-year-old marketing manager who is passionate about her\ncareer and loves spending time with her friends and family. She is always looking for new ways\nto improve her skills and knowledge, and she is always up for a challenge.\" Using these generated\npersonas, we subsequently prompted the model as follows: \u201cFor each of the following descriptions,"}, {"title": "Data Analysis", "content": "In what follows we use the following notation. Let V = 19 be the set of value types studied. Each\nquestion in the questionnaire pertains to a particular item within the set of values $i \\in V$. Furthermore,\nfor each value there are R = 3 question variants. See Section B in the Appendix for example variants.\nRecall that the answer to each question is a number on a 6-point scale. For each LLM and prompt\ntype, we presented the questionnaire N times. The difference between each of these could be different\npersonas, names, temperature sampling etc. Thus the overall set of answers corresponds to a set of\nvalues $X_{i,j,k} \\in \\{1, . . ., 6\\}$ where i = 1, . . ., V and j = 1, . . ., R, k = 1, . . ., N.\nWhen comparing to human data, we used the study in [11]. The data is from 49 cultural groups. The\ntotal number of participants was 53,472, the mean age was 34.2, (SD = 15.8), with 59% females.\nTheir data is stored at the Open Science Framework and is available here."}, {"title": "Value Rankings", "content": "Although there is variability between individuals in their prioritization of values, there are values that\ntend to be ranked as more important than others across cultures and samples. Those suggest there are\nunderlying principles that give rise to value hierarchies. The similarity in value importance across\ncultures is referred to as the universal value hierarchy [8, 11]. Our first question for analysis was\nwhether this hierarchy is also reflected in LLM data. Namely, do LLMs tend to rank the same values\nas high or low as human subjects do.\nTo obtain LLM rankings for a given set of LLM answers, we assigned a score $v_i$ to value i, where\n$v_i$ was the average score given to the three items measuring this value by the LLM (ie the average\nof $X_{i...}$). From this score, we subtracted the average score given to all value items within the\nconversation, thus centering the data. Centring is the recommended practice in value research [6, 3],\nand allows comparison to human samples. We then sorted these $v_i$ and ranked accordingly. Finally,\nwe calculated the Spearman's Rank Correlation ($\\rho$) between this ranking and the known human\nranking [11]. We also checked for significant differences between different datasets (e.g., GPT 4 with\ntemperatures 0.0 and 0.7) using the Wilcoxon Signed-Rank Test."}, {"title": "Consistency Within Values", "content": "The simplest form of consistency in questionnaires is between questions that are conceptually intended\nto address one concept. As mentioned above, in our data there are three different questions per value.\nIndividuals endorsing a value are likely to endorse all relevant items.\nTo assess this consistency here, we calculated Cronbach's Alpha ($\\alpha$) to assess how coherently LLMs\nexpress values across related items within one conversation. Cronbach's Alpha measures the internal\nconsistency of a scale, ranging between 0 and 1. For each value i we let $\\sigma_i^2$ be the variance of all\n$X_{i,.,.}$. Also let $\\sigma^2$ denote the overall variance in X. Then the Cronbach alpha is:\n$$\\alpha = \\frac{V}{V-1} \\left( 1 - \\frac{\\sum_{i=1}^{V} \\sigma_i^2}{\\sigma^2} \\right)$$\nThus, large values indicate that scores tend to be consistent within each value. To compare results for\ndifferent temperatures and language models, we used paired samples t-tests to compare the average\nalpha across values within a dataset."}, {"title": "Correlations Between Values", "content": "The key focus of our work is correlation between values. Namely, the question of whether choice\nof value i is correlated with that of value j. In humans there is a robust correlation structure where\ncertain values are more strongly correlated than others. A standard way to represent this structure is\nvia Multidimensional Scaling (MDS) [41], calculated as follows.\nFirst, the matrix $C\\in \\mathbb{R}^{19 \\times 19}$ of empirical correlation coefficients is formed. Next each of the values\nis embedded into $\\mathbb{R}^2$ via MDS, such that distances in $\\mathbb{R}^2$ best approximate the correlations. For\nhuman data, this results in an approximately circular embedding, as shown in [11, 9, 42]. Here we\nperformed this analysis on the LLM data. To compare the resulting dataset to the human samples, we\nneed to normalize for the degrees of freedom of rotation and translation. This is done via Procrustes\nAnalysis between the human and LLM embeddings. The resulting embeddings were plotted. Then,\nwe computed the sum of squared differences between the procrusted MDS locations of each value to\nthe human benchmark. Larger differences indicate stronger divergence from the human samples."}, {"title": "Results", "content": "Values Rankings: Humans typically prioritize certain values over others. In this section, we\nanalyze the LLM responses based on these rankings, as discussed in Section 3.2.1. Table 1 presents\nthe pooled value rankings of humans and LLMs across all prompting methods. The table reveals\nthat LLM rankings generally align with human rankings in terms of which values are considered\nhigh or low. Specifically, values such as universalism, self-direction, and benevolence received high\nrankings, while values like tradition and power were consistently deemed less important.\nTo obtain a quantitative analysis of alignment of model-generated value hierarchies with human\nperspectives, we employed Spearman rank correlations to compare LLM and human rankings for\ndifferent prompting approaches and models. These correlations are shown in Figure 2. The Basic"}, {"title": "Discussion", "content": "In the current study we analyzed the values exhibited in LLM responses. We used three metrics to es-\ntimate the quality of the LLM responses against human responses: value ranking, internal consistency,\nand value correlations. Our results highlighted the importance of the prompting mechanism. Using\nthe Basic prompt (namely, just providing a questionnaire with no further instructions), the LLM\nwas likely to either generate negligible variance across generated personas, or generate internally\ninconsistent outputs (respond differently to questions about the same value). These results suggest\nthat LLMs cannot be treated as 'individuals' holding a coherent set of value priorities.\nPrompts that endow the LLM with a \u201cpersonality\" improved the consistency of each specific value\nprofile, to varying degrees. The value hierarchy was consistently found across prompts, indicating\nthat at the mean level, LLMs can simulate value rankings of human populations. More variability\nbetween conditions was found in the internal consistency metric, with the Values Anchor prompt\nproviding the best consistency, and the Names prompt a low consistency that would be considered\nunacceptable in a human sample. Finally, the Value Anchor prompt also best modeled the inter-value\ncorrelations. This is arguably the most important metric since it allows analyzing consistency across\nvalues, within a single session. These results suggest that LLMs, applying suitable prompts, can\nproduce a 'population' of individuals, each reporting a different, but coherent set of value priorities.\nOne fascinating question is where the LLM learns to produce such clear profiles of values. These\nprofiles may be implicitly learned during pre-training. Indeed, past studies indicated that values\ncan be identified in texts, such as newspaper articles and social media. However, these values did\nnot necessary follow the theoretical value inter-relations identified here [43\u201345]. Individuals who\nvalue competing values may experience stress and indecision when faced with a dilemma, resulting\nin gradual change in values toward a more coherent form [46, 42]. In contrast, text may very well\npresent both sides of a dilemma and thus retain inconsistencies. Such inconsistencies were not\nidentified because past studies relied mostly on lexical approaches [43-45]. LLMs, taking context\ninto account, may be more likely to identify value inter-relations correctly. LLMs may also have\nlearned to produce value profiles in the process of fine-tuning or RLHF [47]. Future research can\ntry to distinguish these two sources of learning using careful analysis of training sources as well as\nevaluation of different checkpoints in the training process.\nPast research into human personas sought ways to estimate the ability of the LLM to maintain a\nconsistent persona across a conversation [34]. We establish that the unique qualities of human values,\nand the ample empirical knowledge collected about them, allow their use as such a method [3, 4, 48].\nWe suggest that known behavioral correlations in humans can be applied to assess the consistency of\nLLM personas. Here we focused on evaluations via a questionnaire, but one could envision more\nelaborate evaluations that rely on other features of human personalities.\nThe procedures and data produced here may have important contributions for psychological research.\nInvestigators interested in human behavior can apply these procedures to produce datasets that\nsimulate human samples. Future research can investigate their possible use to replicate known\nfindings (e.g., age differences in values) or pretest novel hypotheses (e.g. associations between values\nand specific behaviors).\nThe question of values in LLMs is of course of philosophical and societal importance. Our results\nshow that on average, these values largely reproduce international value rankings. However, small\nvariations in value importance may have implications at the societal and individual level (e.g. gender"}, {"title": "Additional Files", "content": "The Python and R code used to generate our prompt sets and analyses can be made available upon\nrequest, and will be added to open-source repositories for wider public use soon."}, {"title": "Question Variants", "content": "For each value, we use three different question variants. For example, the three question variants\nformulated to assess individuals' alignment with the value type \"Power Dominance\" (i.e., relating to\nasserting authority and control over others) are as follows:\n\u2022 Question 6: \"He desires recognition for his abilities and seeks admiration for his actions.\"\n\u2022 Question 29: \"He prefers taking charge of situations and making decisions.\"\n\u2022 Question 41: \"He actively seeks positions of power and influence, valuing control and\nauthority over others.\""}, {"title": "Value Acronyms", "content": "The figures in the paper use the following value acronyms: SDT = Self-Direction Thought; SDA = Self-\nDirection Action; ST = Stimulation; HE = Hedonism; AC = Achievement; POD = Power-Dominance;\nPOR = Power-Resources; FAC = Face; SEP = Security-Personal; SES = Security-Societal; TR =\nTradition; COR = Conformity-Rules; COI = Conformity-Interpersonal; HUM = Humility; UNN\n= Universalism-Nature; UNC = Universalism-Concern; UNT = Universalism-Tolerance; BEC =\nBenevolence-Caring; BED = Benevolence-Dependability"}, {"title": "Example Portrait Value Questionnaire", "content": "Figure 5 provides an example for the Portrait Value Questionnaire that was used in our study."}, {"title": "The Complete Item List of Best-Worst Refined Values (BWVr)", "content": "In our value anchoring approach, we used the description of values in [36] to prompt the LLMs. The\nset of descriptions is provided below.\n1. Self-direction-thought: developing your own original ideas and opinions\n2. Self-direction-action: being free to act independently\n3. Stimulation: having an exciting life; having all sorts of new experiences\n4. Hedonism: taking advantage of every opportunity to enjoy life's pleasures\n5. Achievement: being ambitious and successful\n6. Power-dominance: having the power that money and possessions can bring\n7. Power-resources: having the authority to get others to do what you want\n8. Face: protecting your public image and avoiding being shamed\n9. Security-personal: living and acting in ways that ensure that you are personally safe and secure\n10. Security-societal: living in a safe and stable society\n11. Tradition: following cultural family or religious practices\n12. Conformity-rules: obeying all rules and laws\n13. Conformity-interpersonal: making sure you never upset or annoy others\n14. Humility: being humble and avoiding public recognition\n15. Benevolence-dependability: being a completely dependable and trustworthy friend and family\nmember\n16. Benevolence-caring: helping and caring for the wellbeing of those who are close\n17. Universalism-concern: caring and seeking justice for everyone especially the weak and vulnera-\nble in society\n18. Universalism-nature: protecting the natural environment from destruction or pollution\n19. Universalism-tolerance: being open-minded and accepting of people and ideas, even when you\ndisagree with them\n20. Animal welfare: caring for the welfare of animals"}, {"title": "Additional Results for Value Rankings", "content": "Table 3 provides additional results on value rankings for different prompting approaches."}, {"title": "Cronbach  $\\alpha$ scores for temperature 0.7", "content": "In the main text we provided results for Cronbach $\\alpha$ with zero temperature. Here we provide further\nresults for temperature 0.7 in Figure 6. As expected, the numbers are overall lower than for the zero\ntemperature case."}, {"title": "MDS figures for Gemini Pro and GPT 4 for temperature 0.0", "content": "In the main text we provided the MDS plots for Gemini Pro for Value Anchor and Names. Here we\nprovide further plots for Gemini Pro in Figure 7, and all the GPT 4 plots for temperature 0.0 in Figure\n8."}]}