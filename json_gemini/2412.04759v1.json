{"title": "REGENT: A RETRIEVAL-AUGMENTED GENERALIST AGENT THAT CAN ACT IN-CONTEXT IN NEW ENVIRONMENTS", "authors": ["Kaustubh Sridhar", "Souradeep Dutta", "Dinesh Jayaraman", "Insup Lee"], "abstract": "Building generalist agents that can rapidly adapt to new environments is a key challenge for deploying AI in the digital and real worlds. Is scaling current agent architectures the most effective way to build generalist agents? We propose a novel approach to pre-train relatively small policies on relatively small datasets and adapt them to unseen environments via in-context learning, without any fine-tuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents. Website:", "sections": [{"title": "1 INTRODUCTION", "content": "AI agents, both in the digital and real world , constantly face changing environments that require rapid or even instantaneous adaptation. True generalist agents must not only be capable of performing well on large numbers of training environments, but arguably more importantly, they must be capable of adapting rapidly to new environments. While this goal has been of considerable interest to the reinforcement learning research community, it has proven elusive. The most promising results so far have all been attributed to large policies , pre-trained on large datasets across many environments, and even these models still struggle to generalize to unseen environments without many new environment-specific demonstrations.\nIn this work, we take a different approach to the problem of constructing such generalist agents. We start by asking: Is scaling current agent architectures the most effective way to build generalist agents? Observing that retrieval offers a powerful bias for fast adaptation, we first evaluate a simple 1-nearest neighbor method: \u201cRetrieve and Play (R&P)\". To determine the action at the current state, R&P simply retrieves the closest state from a few demonstrations in the target environment and plays its corresponding action. Tested on a wide range of environments, both robotics and game-playing, R&P performs on-par or better than the state-of-the-art generalist agents. Note that these results involve no pre-training environments, and not even a neural network policy: it is clear that larger model and pre-training dataset sizes are not the only roadblock to developing generalist agents.\nHaving thus established the utility of retrieval for fast adaptation of sequential decision making agents, we proceed to incorporate it into the design of a \"Retrieval-Augmented Agent\" (REGENT). REGENT is a semi-parametric architecture: it pre-trains a transformer policy whose inputs are not only the current state and previous reward, but also retrieved tuples of (state, previous reward, action) from a set of demonstrations for each pre-training task, drawing inspiration from the recent successes of retrieval augmentation in language modeling [20]. At each \u201cquery\u201d state, REGENT is trained to prescribe an action through aggregating the action predictions of R&P and the transformer policy. By exploiting retrieval-augmentation as well as in-context learning, REGENT permits direct deployment"}, {"title": "2 RELATED WORK", "content": "Recent work in the reinforcement learning community has been aimed at building foundation models and multi-task generalist agents [38, 19, 5, 28, 7, 63, 33, 48, 24, 10, 22, 3, 17, 54, 40, 41, 55, 60].\nMany existing generalist agents struggle to adapt to new environments. Gato , a popular generalist agent trained on a variety of gameplay and robotic environments, struggles to achieve transfer to an unseen Atari game even after fine-tuning, irrespective of the pretraining data. The authors attribute this difficulty to the \"pronounced differences in the visuals, controls, and strategy\" among Atari games. They also attribute Gato's lack of in-context adaptation to the limited context length of the transformer not allowing for adequate data to be added in the context. Our method sidesteps this issue by retrieving only limited but relevant parts of demonstration trajectories to include in the context. JAT , an open-source version of Gato, faces similar problems. We compare with and significantly outperform JAT/Gato using fewer parameters and an order-of-magnitude fewer pre-training datapoints. While REGENT is a 138.6M parameter model, JAT uses 192.7M parameters. The closed-source Gato, with similar performance as the open-source JAT, reports using 1.2B parameters. JAT is also pre-trained on up to 5-10x the amount of data used by our method and yet cannot generalize to unseen environments. Even after finetuning on a few demonstrations from an unseen environment, JAT fails to meaningfully improve.\nMany recent generalist agents cannot leverage in-context learning. In-context learning capabilities enable easier and faster adaptation compared to finetuning. Robocat , which builds on the Gato model, undergoes many cycles of fine-tuning, data collection, and pre-training from scratch to adapt to new manipulation tasks. The multi-game decision transformer , an agent trained on over 50 million Atari gameplay transitions, requires another 1 million transitions for fine-tuning on a"}, {"title": "3 PROBLEM FORMULATION", "content": "We aim to pre-train a generalist agent on datasets obtained from different environments, with the goal of generalizing to new unseen environments. The agent has access to a few expert demonstrations in these new environments. In this work, the agent achieves this through in-context learning without any additional finetuning. We assume that the state and action spaces of unseen environments are known.\nWe model each environment i as a Markov Decision Process (MDP). The i-th Markov Decision Process (MDP) Mi is a tuple $(S_i, A_i, P_i, R_i, \\gamma_i, \\mathcal{I}_i)$, where Si is the set of states, Ai is the set of actions, $P_i(s' \\vert s, a)$ is the probability of transitioning from state s to s' when taking action a, $R_i(s, a)$ is the reward obtained in state s upon taking action a, $\\gamma_i \\in [0, 1)$ is the discount factor, and $\\mathcal{I}_i$ is the initial state distribution. We assume that the MDP's operate over trajectories with finite length Hi, in an episodic fashion. Given a policy $\\pi_i$ acting on Mi, the expected cumulative reward accrued over the duration of an episode (i.e., expected return) is given by $J(\\pi_i) = \\mathbb{E}_{\\pi_i}[\\sum_{t=0}^{H_i} R_i(s_t, a_t)]$.", "latex": ["$(S_i, A_i, P_i, R_i, \\gamma_i, \\mathcal{I}_i)$", "$P_i(s' \\vert s, a)$", "$J(\\pi_i) = \\mathbb{E}_{\\pi_i}[\\sum_{t=0}^{H_i} R_i(s_t, a_t)]$"]}, {"title": "4 REGENT: A RETRIEVAL-AUGMENTED GENERALIST AGENT", "content": "Simple nearest neighbor retrieval approaches have a long history in few-shot learning [50, 2, 49, 43, 9]. These works have found that, at small training dataset sizes, while parametric models might struggle to extract any signal without extensive architecture or hyperparameter tuning, nearest neighbor approaches perform about as well as the data can support. Motivated by these prior results in other domains, we first construct such an approach for an agent that can learn directly in an unseen environment with limited expert demonstrations in Section 4.1. Then, we consider how to improve this agent through access to experience in pre-training environments, so that it can transfer some knowledge to novel environments that allows it to adapt even more effectively with limited data in Section 4.2."}, {"title": "4.1 RETRIEVE AND PLAY (R&P)", "content": "This is arguably one of the simplest decision agents that leverages the retrieval toolset for adaptation. Given a state st from an environment j, let us assume that it is possible to retrieve the n-nearest states (and their corresponding previous rewards, actions) from Dj. We refer to this as the context ct \u2208 Cj. The set Cj is the set of all such contexts in environment j. We also call the state st the query state following terminology from retrieval-augmented generation for language modeling [20].\nThe R&P agent takes the state st and context ct as input, picks the nearest retrieved state s' in ct, and plays the corresponding action a'. That is, $\\pi_{R\\&P} (s_t, c_t) = a'$. We describe the retrieval process in detail later in Section 4.2. Clearly, R&P is devoid of any learning components which can transfer capabilities from pre-training to unseen environments.", "latex": ["$\\pi_{R\\&P} (s_t, c_t) = a'$"]}, {"title": "4.2 RETRIEVAL-AUGMENTED GENERALIST AGENT (REGENT)", "content": "To go beyond R&P, we posit that if an agent learns to meaningfully combine relevant context to act in a set of training environments, then this skill should be transferable to novel environments as well. We propose exactly such an agent in REGENT. We provide an overview of REGENT in Figure 3 where we depict the reinforcement learning loop with the retrieval mechanism and the REGENT transformer. In the figure, we also include the retrieved context and query inputs to the transformer and its output interpolation with the R&P action. We describe REGENT in detail below.\nREGENT consists of a deep neural network policy $\\pi_\\theta$, which takes as input the state st, previous reward rt-1, context ct, and outputs the action directly for continuous environments and the logits over the actions in discrete environments. In the context ct, the retrieved tuples of (state, previous reward, action) are placed in order of their closesness to the query state st with the closest retrieved state s' placed first. Let d(st, s') be the distance between st and s'. We perform a distance weighted interpolation between the outputs of the neural network policy and R&P as follows,\n$\\pi_{\\text{REGENT}} (s_t, r_{t-1}, c_t) = e^{-\\lambda d(s_t, s')} \\pi_{R\\&P} (s_t, c_t) + (1 - e^{-\\lambda d(s_t, s')}) \\sigma(\\pi_\\theta(s_t, r_{t-1}, c_t))\\qquad(1)$\n$\\sigma(x) = \\begin{cases}\\text{Softmax}(x), & \\text{if action space is discrete} \\\\ L \\times \\text{MixedReLU}(x), & \\text{if action space is continuous} \\end{cases}\\qquad(2)$\nwhere MixedReLU : R \u2192 [-1,1] is a tanh-like activation function from [46] and is detailed in Appendix A. Further, $L \\in \\mathbb{R}$ is a hyperparameter for scaling the output of the neural network for continuous action spaces after the MixedReLU. We simply set both L and \u03bb to 10 everywhere following a similar choice in [46]. The function $\\pi_\\theta$ is a causal transformer, which is adept at modeling relatively long sequences of contextual information and predicting the optimal action [29, 4, 8, 28]. All distances are normalized and clipped to [0, 1] as detailed in Appendix A. For discrete action spaces, where the transformer outputs a distribution over actions, we use a softened version of R&P"}, {"title": "5 SUB-OPTIMALITY BOUND FOR REGENT POLICIES", "content": "In this section, we aim to bound the sub-optimality of the REGENT policy. This is measured with respect to the expert policy $\\pi^*$, that generated the retrieval demonstrations Dj. We focus on the discrete action case here and leave the continuous action case for future work. The sub-optimality gap in (training or unseen) environment j is given by $(J(\\pi) - J(\\pi_{\\text{REGENT}}))$. Inspired by the theoretical analysis of Sridhar et al. [46], we define the \"most isolated state\" and use this definition to bound the total variation in the REGENT policy class and hence the sub-optimality gap.\nThat is, first, given Dj, we wish to obtain the maximum value of the distance term d(st, s') in Equations (1) and (3). To do so, we define the most isolated state as follows.\nDefinition 5.1 (Most Isolated State). For a given set of retrieval demonstrations Dj in environment j, we define the most isolated state $s^{\\dagger}_j := \\underset{s \\in S_j}{\\text{argmax}} (\\underset{s' \\in D_j}{\\text{min}} d(s, s'))$, and consequently the distance to\nthe most isolated state as $d_j^{\\dagger} = \\underset{s' \\in D_j}{\\text{min}} d(s^{\\dagger}_j, s')$.\nAll distances between a state in this environment and its closest retrieved state are less than the above value, which also measures state space coverage by the demonstrations available for retrieval. Using the above definition, we have the following.", "latex": ["$\\pi^*$", "$(J(\\pi) - J(\\pi_{\\text{REGENT}}))$", "$s^{\\dagger}_j := \\underset{s \\in S_j}{\\text{argmax}} (\\underset{s' \\in D_j}{\\text{min}} d(s, s'))$", "$d_j^{\\dagger} = \\underset{s' \\in D_j}{\\text{min}} d(s^{\\dagger}_j, s')$"]}, {"title": "6 EXPERIMENTAL EVALUATION", "content": "In our experiments, we aim to answer the following key questions in the two settings depicted in Figures 1 and 2. (1) How well can R&P and REGENT generalize to unseen environments? (2) How does finetuning in the new environments improve REGENT? (3) How well can REGENT generalize to variations of the training environments and perform in aggregate on training environments? (4) How does REGENT qualitatively compare with R&P in using the retrieved context?\n\u25ba Metrics: We plot the normalized return computed using the return of a random and expert agent\nin each environment as $\\frac{\\text{(total return-random return)}}{\\text{(expert return-random return)}}$. The expert and random returns for all JAT/Gato\nenvironments are made available in the original work [19]. The expert and random returns for the ProcGen environments can be found in the original ProcGen paper [11].\n\u25ba Baselines: We consider a different set of baselines for each of the two settings. In the JAT/Gato setting, we compare R&P and REGENT with two variants of JAT. The first is a JAT model trained on the same dataset as REGENT. The second is a JAT model trained on all available JAT data which consists of an order of magnitude more datapoints (in particular, 5x more data in Atari environments and 10x more data in all other environments). We label the former apples-to-apples comparison as JAT/Gato and the latter as JAT/Gato (All Data). We also compare with JAT/Gato with RAG at inference time. We use the same retrieval mechanism as REGENT for this baseline. Additional details and all hyperparameters for the baselines are in Appendix A.\nIn the ProcGen setting, we compare with MTT [37]. MTT only reports results on unseen environments and not on unseen levels in training environments. We simply take the best MTT result on each of the four unseen environments in [37], obtained when 4 demonstrations are included in the MTT context.\nFinetuning and Train-from-scratch Baselines: We fully finetune and parameter-efficient finetune (PEFT with IA3 [30]) both JAT/Gato and JAT/Gato (All Data) on various number of demos in each unseen environment in the JAT/Gato setting and compare with REGENT. We also compare", "latex": ["$\\frac{\\text{(total return-random return)}}{\\text{(expert return-random return)}}$"]}, {"title": "7 CONCLUSIONS AND FUTURE WORK", "content": "In this work, we demonstrated that retrieval offers generalist agents a powerful bias for rapid adaptation to new environments, even without large models and vast datasets. We showed that a simple retrieval-based 1-nearest neighbor agent, Retrieve and Play (R&P), is a strong baseline that matches or exceeds the performance of today's state-of-the-art generalist agents in unseen environments. Building on this, we proposed a semi-parametric generalist agent, REGENT, that pre-trains a transformer-based policy on sequences of query state, reward, and retrieved context. REGENT exploits retrieval-augmentation and in-context learning for direct deployment in unseen environments with only a few demonstrations. Even after pre-training on an order of magnitude fewer datapoints than other generalist agents and with fewer parameters, REGENT outperforms them across unseen environments and surpasses them even after they have been finetuned on demonstrations from those unseen environments. REGENT, itself, further improves with finetuning on even a small number of demonstrations. We note that REGENT faces a couple of limitations: adapting to new embodiments and long-horizon environments. In future work, we believe a larger diversity of embodiments in the training dataset and improved retrieval from longer demonstrations can help REGENT overcome these challenges. We conclude with the conviction that retrieval in general and REGENT in particular redefines the possibilities for developing highly adaptive and efficient generalist agents, even with limited resources."}, {"title": "APPENDIX", "content": "A ADDITIONAL DETAILS FOR REGENT AND VARIOUS BASELINES\nAdditional Details for REGENT\nMixedReLU activation: MixedReLU is a tanh-like activation function from [46] given by\nMixedReLU(x) = (2(ReLU(x+1) \u2212ReLU(x\u22121)) \u2212 1) which simplifies to \u22121 for x < -1, x for-1 < x < 1, and 1 for x > 1.\nNormalizing distances to [0, 1]: We compute the 95th percentile of all distances d(s, s') between any (retrieved or query) state s and the first (closest) retrieved state s'. This value is computed from the demonstrations Dj and is used to normalize all distances in that environment. This value is calculated for all (training or unseen) environments during the preprocessing stage. If after normalization, a distance value is greater than 1, we simply clip it to 1.\nSoftening \\(T_{R&P}\\) logits for discrete action environments: Let us assume that the discrete action space consists of Nact different actions. We have the following."}, {"title": "D ADDITIONAL PROCGEN RESULTS", "content": "Normalized returns in unseen levels in all 12 ProcGen training environments against the number of demonstration trajectories the agent can retrieve from. Each value is an average across 10 levels with 5 rollouts each with psticky = 0.1.", "latex": []}, {"title": "E ADDITIONAL RELATED WORK", "content": "In addition to our related work section, we discuss other relevant papers where a retrieval bias has been recognized as a useful component here [61, 57, 59, 23, 25, 39].\nRAEA [61] performs behavior cloning in each new environment with access to a \"policy memory bank\" and focuses on improving performance in a training task. In RAEA's appendix, the authors demonstrate very initial signs of generalization to new tasks only after training on a few demonstrations from the new tasks. REGENT on the other hand pretrains a generalist policy that can generalize without any finetuning to completely new environments with different observations, dynamics, and rewards. Re-ViLM [57] trains a retrieval-augmented image captioning model, which while demonstrating the usefulness of a retrieval bias beyond robotics and game-playing tasks, is not applicable to our settings.\nExpel [59] and RAP [23] build LLM agents for high-level planning on top of frozen LLMs. Although off-the-shelf LLMs are not yet helpful with low-level fine-grained continuous control, these methods could be key in high-level semantic text-based action spaces.\nGLAs [25] and RADT [39] are recent in-context reinforcement learning (RL) methods. We briefly discussed earlier in-context RL methods in our related work section. GLAs attempt to generalize to new mujoco embodiments via in-context RL but only show minor improvements over random agents. RADT emphasizes that they are unable to generalize via in-context RL to unseen procgen, metaworld, or DMControl environments. REGENT, on the other hand, is an in-context imitation learning method that generalizes to unseen metaworld, atari, and procgen environments via retrieval augmentation (from a retrieval buffer of a few expert demonstrations) and in-context learning. REGENT also struggles with generalization to new mujoco embodiments like GLAs."}]}