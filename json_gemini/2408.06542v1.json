{"title": "VALUE OF INFORMATION AND REWARD SPECIFICATION IN\nACTIVE INFERENCE AND POMDPS", "authors": ["Ran Wei"], "abstract": "Expected free energy (EFE) is a central quantity in active inference which has recently gained pop-\nularity due to its intuitive decomposition of the expected value of control into a pragmatic and an\nepistemic component. While numerous conjectures have been made to justify EFE as a decision\nmaking objective function, the most widely accepted is still its intuitiveness and resemblance to\nvariational free energy in approximate Bayesian inference. In this work, we take a bottom up ap-\nproach and ask: taking EFE as given, what's the resulting agent's optimality gap compared with a\nreward-driven reinforcement learning (RL) agent, which is well understood? By casting EFE under\na particular class of belief MDP and using analysis tools from RL theory, we show that EFE approxi-\nmates the Bayes optimal RL policy via information value. We discuss the implications for objective\nspecification of active inference agents.", "sections": [{"title": "1 Introduction", "content": "Active inference (Parr et al., 2022) is an agent modeling framework derived from the free energy principle, which\nroughly states that all cognitive behavior of an agent can be described as minimizing free energy, an information\ntheoretic measure of the \"fit\" between the environment and the agent's internal model thereof (Friston, 2010). In\nrecent years, active inference has seen increased popularity in various fields including but not limited to cognitive\nand neural science, machine learning, and robotics (Smith et al., 2021; Mazzaglia et al., 2022; Lanillos et al., 2021).\nOne common application of active inference across these fields is in modeling decision making behavior, often taking\nplace in partially observable Markov decision processes (POMDP). This offers active inference as complementary, a\npotential alternative to, or a possible generalization of optimal control and reinforcement learning (RL).\nThe central difference between active inference and RL is that instead of choosing actions that maximize expected\nreward or utility, active inference agents are mandated to minimize expected free energy (EFE), which in its most\ncommon form is written as (Da Costa et al., 2020):\n$EFE(a) = -E_{Q(o|a)}[\\log P(o)] \u2013 E_{Q(o|a)}[KL[Q(s|o, a)||Q(s|a)]]$\nPragmatic value\nEpistemic value\nHere, a is a sequence of actions to be evaluated, $Q(s|a)$ and $Q(o|a)$ are the agent's prediction of future states s and\nobservations o, $Q(s|o, a)$ is the future updated beliefs about states given future observations, $P(o)$ is a distribution\nencoding the agent's preferred observations and KL denotes Kullback-Leibler (KL) divergence, a measure of distance\nbetween two distributions.\nOne can obtain an intuitive understanding of the EFE objective by analyzing the two terms separately. The first term is\nthe negative expected log likelihood of predicted future observations under the preference or target distribution, which\nis equivalent to the cross entropy between the predicted and preferred observation distributions. Minimizing this term\nencourages the agent to take actions that lead to preferred observations. It is thus usually referred to as the \"pragmatic\nvalue\" or \"expected value\". The second term is the expected KL divergence between the predicted future states and\nupdated beliefs about future states given future observations, which quantifies the belief update amount. This term is\nusually referred to as \"epistemic value\" or \"expected information gain\" because it encourages the agent to take actions\nthat lead to a higher amount of belief update \u2013 an implicit resolution of uncertainty."}, {"title": "2 Background", "content": "In this section, we introduce notations for Markov decision process, partially observable Markov decision process, and\nthe belief MDP view of POMDPs. We then introduce active inference and the EFE objective."}, {"title": "2.1 Markov Decision Process", "content": "A discrete time infinite-horizon discounted Markov decision process (MDP; Sutton and Barto, 2018) is defined by a\ntuple $M = (S, A, P, R, \u03bc, \u03b3)$, where S is a set of states, A a set of actions, $P : S \u00d7 A \u2192 \u25b3(S)$ a state transition\nprobability distribution (also called transition dynamics), $R : S \u00d7 A \u2192 R$ a reward function, $\u03bc : \u2206(S)$ the initial state\ndistribution, and $\u03b3\u2208 (0,1)$ a discount factor. In this work, we consider planning as opposed to learning, where the\nMDP tuple M is known to the agent rather than having to be estimated from samples obtained by interacting with the\nenvironment defined by M. We use $\u03c0 : S \u2192 A(A)$ to denote a time-homogeneous Markovian policy which maps a\nstate to a distribution over actions. Rolling out a policy in the environment for a finite number of time steps T induces\na sequence of states and actions $T = (s_{0:T}, a_{0:\u0442})$ (also known as a trajectory) which is distributed according to:\n$P(T) = \\prod_{t=0}^{T} P(s_t|s_{t-1}, a_{t\u22121})\u03c0(a_t|s_t),$\nwhere $P(s_0|s_{\u22121}, a_{\u22121}) = \u03bc(s_0)$. We use $\u03c1\u03c0(s, a) = E[\\sum_{t=0}^{100}\u03b3^t Pr(s_t = s, a_t = a)]$ to denote the state-action\noccupancy measure of policy \u03c0 in environment with dynamics P, where the expectation is taken w.r.t. the interaction\nprocess (2) for $T \u2192 \u221e$. We denote the normalized occupancy measure, also called the marginal state distribution or\nstate marginal, as $d\u03c0(s, a) = (1 \u2013 \u03b3)\u03c1\u03c0(s, a)$.\nSolving a MDP refers to finding a policy \u03c0 which maximizes the expected cumulative discounted reward in the envi-\nronment J(\u03c0) defined as:\n$J(\u03c0) = E[\\sum_{t=0}^{\\infty} \u03b3^t R(s_t, a_t)]$\nThe process of finding an optimal policy is sometimes referred to as reinforcement learning and it is a well-known\nresult that there exists at least one time-homogeneous Markovian policy which is optimal w.r.t. (3) (Sutton and Barto,\n2018). This significantly simplifies our analysis later compared to finite horizon un-discounted MDPs for which the\noptimal policy is time-dependent. The quantity $\u03b3^{100}$ has a similar notion to planning horizon, because it represents\nthe time step at which discounting is effectively zero. The optimal policy \u03c0* is characterized by the Bellman optimality\nequation:\n$Q(s,a) = R(s, a) + E_{s\u2032\u223cP(\u00b7|s,a)} [V(s')], V(s) = \\max_{a} Q(s, a),$\nfrom which it can be obtained by taking the action which maximizes the action value function Q for each state as\n$\u03c0^*(a|s) = \u03b4(\u03b1 \u2212 arg max_{\u0101} Q(s, \u0101))$, where $\u03b4(a \u2013 b)$ is the dirac delta distribution which has probability 1 if a = b\nand probability 0 elsewhere. The advantage function $A(s, a) = Q(s, a) \u2013 V(s) \u2264 0$ quantifies the suboptimality of\nan action. We will omit the * notation in most cases. When needed, we denote the value and advantage functions\nassociated with policy \u03c0 and MDP M as $Q^M_\u03c0 , V^M_\u03c0 , A^M_\u03c0$"}, {"title": "2.2 Partially Observable Markov Decision Process", "content": "A discrete time infinite-horizon discounted partially observable MDP (POMDP; Kaelbling et al., 1998) is character-\nized by a tuple $M = (S, A, O, P, R, \u03bc, \u03b3)$, where the newly introduced symbol O is a set of observations, and the\nnew transition dynamics P consists of the state transition probability distribution $P(s_{t+1}|s_t, a_t)$ and an observation\nemission distribution $P(o_t|s_t)$. In a POMDP environment, the agent only has access to observations emitted from the\nenvironment state but not the state itself. It is thus generally not sufficient to consider Markovian policies but policies\nthat depend on the history of observation-action sequences, i.e., $\u03c0(a_t|h_t)$ where $h_t = (o_{0:t}, a_{0:t\u22121})$.\nIt is a well-known result that the Bayesian belief distribution $b_t = P(s_t|h_t)$ is a sufficient statistic for the interaction\nhistory (Kaelbling et al., 1998). The history dependent value functions and policy in POMDP can thus be written in\nterms of beliefs:\n$Q(b,a) = \\sum_{S}b(s)R(s, a) + \\sum_{o'}P(o'|b,a)V(b'(o', a, b)), V(b) = \\max_{a} Q(b,a),$\nwhere $P(o'|b, a) = \\sum_{s,s'} P(o'|s')P(s'|s, a)b(s)$ and $b'(o', a, b)$ denotes the belief update function from prior b(s) to\nthe posterior:\n$b'(o', a, b) := b'(s'|o', a,b) = \\frac{P(o'|s')\\sum_{s} P(s'|s,a)b(s)}{\\sum_{s'} P(o'|s')\\sum_{s} P(s'|s, a)b(s)}.$\nThe optimal policy derived from the above value functions is sometimes referred to as the Bayes optimal policy (Duff,\n2002).\nThe belief value functions in (5) imply a special class of MDPs known as belief MDPs (Kaelbling et al., 1998) where\nthe reward and dynamics are defined on the belief state as:\n$R(b,a) = \\sum_{S}b(s)R(s,a), P(b'|b,a) = \\sum_{o'}P(o'|b,a)\u03b4(b' \u2013 b'(o', a, b)).$\nThe stochasticity in the belief dynamics is entirely due to the stochasticity of the next observation; the belief updating\nprocess itself is deterministic.\nIn this work, we generalize the notion of belief MDP to refer to any MDP defined on the space of beliefs. However,\nnot all belief MDPs could yield the optimal policies for some POMDPs."}, {"title": "2.3 Active Inference", "content": "Active inference is an application of the variational principle to perception and action, where intractable Bayesian\nbelief updates (i.e., (6)) are approximated by variational inference (Da Costa et al., 2020). At every time step t, vari-\national inference searches for an approximate posterior $Q(s_t)$ which maximizes the evidence lower bound of data\nmarginal log likelihood, or equivalently minimizes the variational free energy F:\n$F(Q) = E_{Q(s_t)} [\\log Q(s_t) \u2013 \\log P(o_t, s_t)],$\nwhere $P(o_t, s_t) = P(o_t|s_t)P(s_t)$. In the context of POMDPs, the prior is given by $P(s_t) =\nE_{st-1} P(s_t|s_{t\u22121}, a_{t\u22121})Q(s_{t\u22121})$. It is well-known that the optimal variational approximation under appropriately\nchosen family of posterior distributions equals to the exact posterior in (6) (Blei et al., 2017). We will thus assume\nappropriate choices of variational family and omit suboptimal belief updating in subsequent analyses.\nCentral to the current discussion is the policy selection objective functions used in active inference, which is its main\ndifference from classic POMDPs. In particular, active inference introduces an objective function called expected free\nenergy (EFE) which, given an initial belief $Q_0(s_0)$ and a finite sequence of actions $a_{0:T\u22121}$, is defined as (Friston et al.,\n2017):\n$EFE(o_{0:T\u22121}, Q_0) = E_{Q(o_{1:T},s_{1:T}|o_{0:T-1})} [\\log Q(s_{1:T}|o_{0:T\u22121}) \u2013 \\log P(o_{1:T}, s_{1:T})],$\nwhere $Q(s_{1:T}|o_{0:T-1})$ is defined as the product of the marginal state distributions along the action sequence (we show\nhow this can be approximately obtained as a result of variational inference and discuss the implication of defining this\ninstead as the joint distribution in the appendix, which also contains all derivations and proofs):\n$Q(s_{1:T}|o_{1:T-1}) = \\prod_{t=1}^{T} Q(s_t|Qt-1, a_{t-1}),$\n$Q(s_t|Qt-1, a_{t-1}) := \\sum_{s_{t-1}} P(s_t|s_{t-1}, a_{t-1})Q(s_{t-1}|Qt-2, a_{t-2}),$\nand $Q(o_{1:T}, s_{1:T}|a_{1:T\u22121}) = \\prod_{t=1}^{T}P(o_t|s_t)Q(s_t|Qt\u22121, a_{t\u22121})$. These distributions represent the agent's posterior\npredictive beliefs about states and observations in the future. Notice (9) is different from (1), but it is used here\nbecause it is more general (Champion et al., 2024).\nThe distribution $P(o_{1:T}, s_{1:T})$ is interpreted as a \"preference\" distribution under which preferred observations and\nstates have higher probabilities. While there are multiple ways to specify P in the literature, we will focus on the most\npopular specification:\n$P(o_{0:T}, s_{0:T}) = \\prod_{t=0}^{T} P(o_t)P(s_t|o_t),$\nwhere $P(s_t|o_t)$ is an arbitrary distribution. This specification allows us to factorize EFE over time and construct the\nfollowing approximation:\n$EFE(o_{0:T-1}, Q_0) \u2248 \\sum_{t=1}^{T} - E_{Q (o_t|a_{0:T-1})} [\\log P(o_t)] \u2013 E_{Q(o_t|a_{0:T-1})} [KL[Q(s_t|o_t, a_{0:T\u22121})||Q(s_t|a_{0:T\u22121})]],$\nPragmatic value\nEpistemic value\nwhere KL denotes Kullback-Leiblier divergence, $Q(o_t|a_{0:T\u22121}) = \\sum_{s_t} P(o_t|s_t)Q(s_t|Qt\u22121,A_{t\u22121})$ is the posterior\npredictive over observations, and $Q(s_t|o_t, a_{0:T\u22121}) XP(o_t|s_t)Q(s_t|Qt\u22121, a_{t\u22121})$ is the future posterior given posterior\npredictive of future states as prior and future observations. We discuss this approximation and optimal choice of $P(s|o)$\nfurther in the appendix.\nAs preempted in the introduction, in (12), the first term \"pragmatic value\" scores the quality of predicted observa-\ntions under the preferred distribution. The second term \"epistemic value\" measures the distance between future prior\n$Q(s_t|a_{0:T-1})$ and posterior beliefs $Q(s_t|o_t, a_{0:T\u22121})$, which corresponds to the amount of expected \"information gain\"\nfrom future observations. The epistemic value term is an especially salient difference between active inference and\nclassic POMDPS."}, {"title": "3 Unifying Active Inference and RL Under Belief MDPS", "content": "The use of EFE vs. reward and the search for action sequences (i.e., plans) vs. policies are the main contentions\nbetween active inference and RL. In this section, we show that active inference can be equally represented using\nreward and policy in a special class of belief MDPs. The key is to show that the EFE objective can be characterized\nusing a recursive equation akin to the Bellman equation. This can be achieved immediately by expressing the predictive\ndistribution at each step using the predictive distribution at the previous step:\n$EFE(o_{0:T-1}, Q_0) = \\sum_{t=1}^{T} -E_{Q(o_t|a_{0:T-1})} [\\log P(o_t)] \u2013 E_{Q(o_t|a_{0:T-1})} [KL[Q(s_t|o_t, a_{0:T\u22121})||Q(s_t|a_{0:T\u22121})]]$\n$= \\sum_{t=0}^{T-1} -E_{Q(o_{t+1}|Qt,a_t)} [\\log P(o_{t+1})] \u2013 E_{Q(o_{t+1}|Qt,a_t)}[KL[Q(s_{t+1}|o_{t+1},Qt, a_t)||Q(s_{t+1}|Qt, a_t)]]$\n$=-E_{Q(o_1|Q_0,a_0)} [\\log P(o_1)] \u2013 E_{Q(o_1|Q_0,ao)} [KL[Q(s_1|o_1, Q_0, a_0)||Q(s_1|Q_0, a_0)]] + EFE(a_{1:T\u22121}, Q_1).$\nThe recursive equation implies a transition dynamics over the state marginal $Q_t$ which only depends on the previous\nstate marginal $Q_{t-1}$, i.e., the transition is Markovian. The per-time step EFE only depends on the current state marginal.\nUsing the equivalence between the optimal $Q_t$ and $b_t$, we can write the reward and transition dynamics of the belief\nMDP implied by EFE as follows:\n$R^{EFE}(b,a) = E_{P(o'|b,a)} [\\log P(o')] + E_{P(o'|b,a)} [KL[b(s'|o', b,a)||b(s'|b,a)]]$\n$:= R(b,a) + IG(b,a),$\n$P^{open} (b'|b, a) = \u03b4(b' \u2013 b'(a, b)), where b'(a, b) := b' (s'|b, a) = \\sum_{S}P(s'|s, a)b(s).$\nBy constructing the above belief MDP, the search for optimal action sequences can be equally represented as the search\nfor optimal belief-action policies.\nProposition 3.1. (Active inference policy) The EFE achieved by the optimal action sequence can be equivalently\nachieved by a time-indexed belief-action policy $\u03c0(a_t|b_t)$.\nProof. The proof is due to the above belief MDP characterization. An alternative proof is given in the appendix.\nThese identities enable us to define infinite-horizon discounted belief MDPs using the EFE reward and dynamics in\n(14) and restrict our search to time-homogeneous Markovian belief-action policies. A similar result was presented\nrecently by Malekzadeh and Plataniotis (2022). However, rather than focusing on policy optimization algorithms, our\ngoal here is to clarify the belief MDP implied by EFE.\nHowever, notice a few differences between the EFE belief MDP and the Bayes optimal belief MDP. First, the belief\ndynamics in (14c) does not contain observation o; rather it is the marginal prediction of the next state given the previous\nbelief. Such a belief dynamics has been referred to as open-loop in the literature (Flaspohler et al., 2020) in the sense\nthat it does not take into account the possibility of updating beliefs based on future observations, akin to open-loop\ncontrols. In contrast to the POMDP belief dynamics in (7), the open-loop belief dynamics is deterministic given a.\nSecond, the EFE reward function contains an information gain term which corresponds to epistemic value. The first\nterm pragmatic value is defined as the expected log likelihood of the next observation. This does not introduce much\ndifference from the POMDP reward function because we can define the active inference preference distribution as a\nBoltzmann distribution parameterized by a reward function $P(o) x exp(\u0158(o))$ and assume that $R(o)$ self-normalizes\nso that the partition function equals 1. The resulting reward can still be written as a linear combination of state-action\nreward:\n$R(b,a) = E_{P(o'|b,a)} [\\log P(o')]$\n$= \\sum_{S}b(s) \\sum_{s'} P(s'|s, a) \\sum_{o'}P(o'|s')\u0158(o')$$\n$ = \\sum_{S}b(s)\u0158(s,a).$"}, {"title": "4 Analyzing Policies in MDPS", "content": "The belief MDP characterizations of both the EFE policy and the Bayes optimal policy enable us to use MDP analysis\ntools for POMDPs. The main analysis tools we use in this paper are recent extensions of the performance difference\nlemma (Kakade and Langford, 2002) and simulation lemma (Kearns and Singh, 2002) which are well-known results\nin RL theory that quantify the performance difference between different policies or the same policy in different en-\nvironments. To compare active inference with RL, we are interested in the setting where two policies are optimal\nw.r.t. both different rewards and different dynamics, however, the evaluation reward and dynamics are equivalent to\nonly one of the policies (here referred to as the expert policy). The following lemma, which extends lemma 4.1 in\n(Vemula et al., 2023) to the setting of different rewards, gives the performance gap (also known as the regret) between\nthe two policies:\nLemma 4.1. (Performance difference in mismatched MDPs) Let \u03c0 and \u03c0' be two policies which are optimal w.r.t.\ntwo MDPs M and M'. The two MDPs share the same initial state distribution and discount factor but have different\nrewards R, R' and dynamics P, P'. Denote $\u2206R(s,a) = R'(s,a) \u2013 R(s, a)$. The performance difference between \u03c0\nand \u03c0' when both are evaluated in M is given by:\n$J_M(\u03c0) \u2013 J_M(\u03c0') = \\frac{1}{(1 \u2013 \u03b3)}E_{(s,a)\u223cd_\u03c0}[A_\u03c0^{M}(s,a)]$\nAdvantage under expert distribution\n$+ \\frac{1}{(1 \u2013 \u03b3)}E_{(s,a)\u223cd_\u03c0}[\u2206R(s, a) + (E_{s'\u223cP'(\u00b7|s,a)} [V_{\u03c0'}^{M'} (s')] \u2013 E_{s'\u223cP(\u00b7|s,a)} [V_{\u03c0'}^{M'} (s')])]$\nReward-model advantage under own distribution\n$+ \\frac{1}{(1 \u2013 \u03b3)}E_{(s,a)\u223cd_\u03c0'}[-\u2206R(s, a) + (E_{s'\u223cP(\u00b7|s,a)} [V_{\u03c0'}^{M'} (s')] \u2013 E_{s'\u223cP'(\u00b7|s,a)} [V_{\u03c0'}^{M'} (s')])]$\nReward-model disadvantage under expert distribution\nLemma 4.1 decomposes the performance gap in MDP M between policy \u03c0 (the expert) and \u03c0' into three terms. The\nfirst term is the advantage value of \u03c0' under the expert's state-action marginal distribution. The second term is the\ndifference in reward between MDP M' and M and the difference in the value $V_{\u03c0'}^{M'}$ of \u03c0' in M' due to the difference\nin dynamics expected under the state-action marginal distribution of \u03c0'. This term quantifies the \"advantage\" of being\nevaluated in one MDP vs another. The last term is the opposite of reward-model advantage, i.e., disadvantage, expected\nunder the expert policy \u03c0's state-action marginal distribution.\nOne can obtain an intuitive understanding of (17) by attempting to minimize the performance gap via optimizing\nR', P', given we require \u03c0' to be the optimal policy w.r.t. some R', P'. First, it holds that when R', P' are respectively\nequal to R, P, the reward and model advantages are zeros, and the policy advantage is zero as a result. This means one\ncan read policy, reward, and model advantage as a measure of error from the expert MDP and policy. When such error\nis nonzero, R', P' are optimized to increase reward-model advantage under the expert distribution and decrease reward-\nmodel advantage under the policy's own distribution. This encourages \u03c0' to choose actions that lead to state-actions"}, {"title": "5 Value of Information in POMDPS", "content": "Given the primary difference between active inference and RL is the definition of epistemic value and open-loop belief\ndynamics, we ask whether it could be seen as an approximation to the Bayes optimal policy, specifically the epistemic\naspect thereof? To this end, we first analyze the \"value of information\" in the Bayes optimal policy. We then show that\nepistemic value closes the gap to the Bayes optimal policy by making up for the loss of information value."}, {"title": "5.1 Value of Information in Bayes Optimal RL Policy", "content": "It's colloquially accepted that the Bayes optimal policy characterized by the value functions in (5) optimally trades off\nexploration and exploitation. However, it's not immediately obvious what is being traded off, the comparison is made\nagainst which alternative action or policy, and how large is the performance gap. In this paper, we adopt the view that\nwhat's being traded off is the value of information, which we try to quantify in an action or policy. In (Howard, 1966),\nthe value of information for a single step decision making problem is defined as the reward a decision maker is willing\nto give away if they could have their uncertainty resolved (e.g., by a clairvoyant). Formally, the expected value of\nperfect information (EVPI) is defined as the difference between the expected value given perfect information (EV|PI)\nand the expected value without perfect information (EV).\nIn the POMDP setting, the agent cannot in general obtain perfect information about the hidden state, but an observation\nthat is usually correlated with the state. It turns out that this corresponds to an extension of Howard's definition in\nthe single step decision making setting called the value of imperfect information (Raiffa and Schlaifer, 2000). For\nconsistency in notation, we will label it as the expected value of perfect observation (EVPO) and define it as:\n$EV PO = EV|PO \u2013 EV,$\n$EV = \\max_{a} \\sum_{S}b(s) R(s,a),$\n$EV|PO = \\sum_{o} \\sum_{S}P(o|s)b(s) \\max_{a} R(b(s|o), a) .$\nSimilar to EVPI, EVPO is non-negative because an optimal decision maker cannot gain information and do worse\n(Howard, 1966).\nExtending this definition for the multi-stage sequential decision making setting, we have the following corollary of EV\nand EV PO for POMDPs:\n$EV: Q^{open} (b,a) = \\sum_{S}b(s)R(s, a) + \u03b3V^{open} (b'(a, b)),$\n$EV|PO: Q(b,a) = \\sum_{S}b(s)R(s, a) + \\sum_{o'}P(o'|b, a) V (b'(o', a, b)) .$"}, {"title": "5.2 Main Result: EFE Approximates Bayes Optimal RL Policy", "content": "The main insight of this work is that EFE closes the optimality gap between open and closed-loop policies by aug-\nmenting the reward of the open-loop policy with the epistemic value term. Given the pragmatic value is linear in the\nbelief (15), we will use it as the shared reward between active inference and RL agents, i.e., $R(s,a) = \u0158(s,a)$.\nProposition 5.2 shows that the advantage of closed-loop belief transition is proportional to the information gain pro-\nvided by the next observation. While the agent cannot change either belief transition distributions, it can change its\nreward to alter the reward-model advantage and the marginal distribution under which it is evaluated. An obvious\nchoice for the reward advantage is to set it to the information gain in order to cancel with the information disadvantage\nof open-loop belief dynamics. To ensure the agent does not get distracted by gaining information and still focus on\ntask relevant behavior, we make the following assumption on preference distribution specification:\nAssumption 5.3. (Preference specification) The preference distribution or reward is specified such that the gain in\npragmatic value after receiving a new observation is higher than the loss in epistemic value in expectation under the\nBayes optimal policy \u03c0 in closed-loop belief dynamics P:\n$\\sum_s (b(s|o) \u2013 b(s)) \u0158(s,a) \u2265 E_{(b,a)\u223cd_\u03c0} [IG(b(s), a) \u2013 IG(b(s|o), a)].$\nThis assumption also ensures that the advantage of closed-loop belief dynamics under the EFE value function is non-\nnegative. In practice, since the Bayes optimal policy behavior can be difficult to know a priori, we can approximate\nthe above by setting a reward function such that the reward difference is sufficiently high. In the appendix, we prove\nthat the advantage upper bound given this assumption is the same as that evaluated under the open-loop belief MDP in\nproposition 5.2. To facilitate the comparison between open-loop and EFE policy, we introduce two more assumptions:\nAssumption 5.4. (Policy behavior) We make the following assumptions on the behavior of the evaluated policies:\n1. The absolute advantage of the EFE policy $\u03c0^{EFE}$ expected under the Bayes optimal policy's marginal\ndistribution is no worse than that of the open-loop policy $\u03c0^{open}$: $E_{\u03c0^{EFE}} = E_{(b,a)\u223cd_\u03c0}[|A_\u03c0^{open}(b,a)|] \u2265\n$E_{(b,a)\u223cd_\u03c0}[|A_\u03c0^{open}(b,a)|].$\n2. For both the open-loop policy $\u03c0^{open}$ and EFE policy $\u03c0^{EFE}$, it always holds that $IG(b,a) \u2265 2$ for any b, a\nsampled from either their own or the expert policy's marginal distribution.\nNote that both assumptions are conservative but they will enable us to focus the comparison of both policies on their\ninformation seeking behavior. Assumption 1 is reasonable because we expect the EFE policy to be more similar to the\nexpert than the open-loop policy given the information gain reward encourages information seeking behavior. This\nenables us to remove policy advantage from the comparison. Assumption 2 is partly numerically motivated because it\nallows us to further upper bound the closed-loop model advantage in proposition 5.2 via $\\sqrt{2KL} < KL$ so that the IG\nreward bonus in EFE can be directly compared with closed-loop model advantage and subtracted from it. In practice,\nmany POMDP environments are much more benign in that partial observability, and thus the value of information,\ndecreases to zero in a small number of time steps (Liu et al., 2022). In that case, the difference between open-loop,\nEFE, and Bayes optimal policies become very small. Thus, the setting we consider is harder or more pessimistic.\nThe following theorem, which is the main result, gives the performance gap of both policies compared to the Bayes\noptimal policy:\nTheorem 5.5. Let all policies be deployed in POMDP M and all are allowed to update their beliefs according to\n$b'(o',a,b)$. Let $\u20ac_{IG} = E_{(b,a)\u223cd_\u03c0}[IG(b,a)]$ denotes the expected information gain under the Bayes optimal policy's\nbelief-action marginal distribution and let the belief-action marginal induced by both open-loop and EFE policies\nhave bounded density ratio with the Bayes optimal policy $||\\frac{d^{\u03c0^{open}}(b,a)}{d_\u03c0(b,a)}|| < C$. Under assumptions 5.3 and 5.4, the\nperformance gap of the open-loop and EFE policies from the optimal policy are bounded as:\n$J_M(\u03c0) \u2013 J_M(\u03c0^{open}) \u2264 \\frac{1}{1\u2212\u03b3}\u03b5_\u03c0^{open}+ \\frac{(C+1)\u03b3R_{max}}{(1 \u2013 \u03b3)^2}\u20ac_{IG},$\n$J_M(\u03c0) \u2013 J_M(\u03c0^{EFE}) \u2264 \\frac{1}{1\u2212\u03b3}\u03b5_\u03c0^{EFE}+ \\frac{(C+1)\u03b3R_{max}}{(1 \u2013 \u03b3)^2}\u20ac_{IG}.$"}, {"title": "6 Discussions", "content": "Our results highlight the nuanced relationship between active inference and the classic approach to POMDPs. In this\nsection, we provide a few complementary perspectives on related POMDP approximation and extensions from the RL\nliterature and discuss objective specification in active inference informed by our results."}, {"title": "6.1 POMDP Approximation and Extensions", "content": "In the POMDP planning literature, there is a suite of approximation techniques to overcome the intractability of exact\nbelief update and value function representation. The simplest ones are the maximum likelihood heuristic and QMDP\nheuristic which first compute the underlying MDP value function and then obtain the belief value function using either\nthe most likely state under the current belief or a belief-weighted average (Littman et al., 1995). These approximations\nleverage the fact that MDP value functions (in discrete space) are easy to compute, but they can be overly optimistic\nsince they implicitly assume the state in the next time step will be fully observed ("}]}