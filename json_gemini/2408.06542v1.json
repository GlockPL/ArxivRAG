{"title": "VALUE OF INFORMATION AND REWARD SPECIFICATION IN ACTIVE INFERENCE AND POMDPS", "authors": ["Ran Wei"], "abstract": "Expected free energy (EFE) is a central quantity in active inference which has recently gained pop- ularity due to its intuitive decomposition of the expected value of control into a pragmatic and an epistemic component. While numerous conjectures have been made to justify EFE as a decision making objective function, the most widely accepted is still its intuitiveness and resemblance to variational free energy in approximate Bayesian inference. In this work, we take a bottom up ap- proach and ask: taking EFE as given, what's the resulting agent's optimality gap compared with a reward-driven reinforcement learning (RL) agent, which is well understood? By casting EFE under a particular class of belief MDP and using analysis tools from RL theory, we show that EFE approxi- mates the Bayes optimal RL policy via information value. We discuss the implications for objective specification of active inference agents.", "sections": [{"title": "1 Introduction", "content": "Active inference (Parr et al., 2022) is an agent modeling framework derived from the free energy principle, which roughly states that all cognitive behavior of an agent can be described as minimizing free energy, an information theoretic measure of the \"fit\" between the environment and the agent's internal model thereof (Friston, 2010). In recent years, active inference has seen increased popularity in various fields including but not limited to cognitive and neural science, machine learning, and robotics (Smith et al., 2021; Mazzaglia et al., 2022; Lanillos et al., 2021). One common application of active inference across these fields is in modeling decision making behavior, often taking place in partially observable Markov decision processes (POMDP). This offers active inference as complementary, a potential alternative to, or a possible generalization of optimal control and reinforcement learning (RL). The central difference between active inference and RL is that instead of choosing actions that maximize expected reward or utility, active inference agents are mandated to minimize expected free energy (EFE), which in its most common form is written as (Da Costa et al., 2020):\nEFE(a) = EQ(ola) [log P(o)] \u2013 EQ(ola) [KL[Q(s|o, a)||Q(sa)]]\nPragmatic value Epistemic value\n(1)\nHere, a is a sequence of actions to be evaluated, Q(sa) and Q(o|a) are the agent's prediction of future states s and observations o, Q(s|o, a) is the future updated beliefs about states given future observations, P(o) is a distribution encoding the agent's preferred observations and KL denotes Kullback-Leibler (KL) divergence, a measure of distance between two distributions.\nOne can obtain an intuitive understanding of the EFE objective by analyzing the two terms separately. The first term is the negative expected log likelihood of predicted future observations under the preference or target distribution, which is equivalent to the cross entropy between the predicted and preferred observation distributions. Minimizing this term encourages the agent to take actions that lead to preferred observations. It is thus usually referred to as the \"pragmatic value\" or \"expected value\". The second term is the expected KL divergence between the predicted future states and updated beliefs about future states given future observations, which quantifies the belief update amount. This term is usually referred to as \"epistemic value\" or \"expected information gain\" because it encourages the agent to take actions that lead to a higher amount of belief update \u2013 an implicit resolution of uncertainty."}, {"title": "2 Background", "content": "In this section, we introduce notations for Markov decision process, partially observable Markov decision process, and the belief MDP view of POMDPs. We then introduce active inference and the EFE objective."}, {"title": "2.1 Markov Decision Process", "content": "A discrete time infinite-horizon discounted Markov decision process (MDP; Sutton and Barto, 2018) is defined by a tuple M = (S, A, P, R, \u03bc, \u03b3), where S is a set of states, A a set of actions, P : S \u00d7 A \u2192 \u25b3(S) a state transition probability distribution (also called transition dynamics), R : S \u00d7 A \u2192 R a reward function, \u03bc : \u2206(S) the initial state distribution, and \u03b3\u2208 (0,1) a discount factor. In this work, we consider planning as opposed to learning, where the MDP tuple M is known to the agent rather than having to be estimated from samples obtained by interacting with the environment defined by M. We use \u03c0 : S \u2192 A(A) to denote a time-homogeneous Markovian policy which maps a state to a distribution over actions. Rolling out a policy in the environment for a finite number of time steps T induces a sequence of states and actions T = (80:T, 00:\u0442) (also known as a trajectory) which is distributed according to:\nP(T) = \\prod_{t=0}^{T} P(S_{t} | S_{t-1}, A_{t-1})(a_{t} | S_{t}),\n(2)\nwhere P(so|s\u22121,a\u22121) = \u03bc(so). We use \u03c1\u03c0(s, a) = \u0395[\u03a3\u221et=0\u03b3t Pr(st = s,at = a)] to denote the state-action occupancy measure of policy \u03c0 in environment with dynamics P, where the expectation is taken w.r.t. the interaction process (2) for T \u2192 \u221e. We denote the normalized occupancy measure, also called the marginal state distribution or state marginal, as d\u03c0p(s, a) = (1 \u2013 \u03b3)\u03c1\u03c0(s, a).\nSolving a MDP refers to finding a policy \u03c0 which maximizes the expected cumulative discounted reward in the environment J(\u03c0) defined as:\nJ(\u03c0) = E[\\sum_{t=0}^{\\infty} \\gamma^{t} R(S_{t}, a_{t})],\n(3)\nThe process of finding an optimal policy is sometimes referred to as reinforcement learning and it is a well-known result that there exists at least one time-homogeneous Markovian policy which is optimal w.r.t. (3) (Sutton and Barto, 2018). This significantly simplifies our analysis later compared to finite horizon un-discounted MDPs for which the optimal policy is time-dependent. The quantity has a similar notion to planning horizon, because it represents the time step at which discounting is effectively zero. The optimal policy \u03c0* is characterized by the Bellman optimality equation:\nQ(s,a) = R(s, a) + E_{s'~P(.\\s,a)} [V(s')], V(s) = \\max_{a} Q(s, a),\n(4)\nfrom which it can be obtained by taking the action which maximizes the action value function Q for each state as \u03c0*(as) = \u03b4(\u03b1 arg max\u0101 Q(s, \u0101)), where \u03b4(a \u2013 b) is the dirac delta distribution which has probability 1 if a = b and probability 0 elsewhere. The advantage function A(s, a) = Q(s, a) \u2013 V(s) \u2264 0 quantifies the suboptimality of an action. We will omit the * notation in most cases. When needed, we denote the value and advantage functions associated with policy \u03c0 and MDP Mas QM, VM, AM"}, {"title": "2.2 Partially Observable Markov Decision Process", "content": "A discrete time infinite-horizon discounted partially observable MDP (POMDP; Kaelbling et al., 1998) is character- ized by a tuple M = (S, A, O, P, R, \u03bc, \u03b3), where the newly introduced symbol O is a set of observations, and the new transition dynamics P consists of the state transition probability distribution P(st+1|st, at) and an observation emission distribution P(ot|st). In a POMDP environment, the agent only has access to observations emitted from the environment state but not the state itself. It is thus generally not sufficient to consider Markovian policies but policies that depend on the history of observation-action sequences, i.e., \u03c0(at|ht) where ht = (00:t, a0:t\u22121).\nIt is a well-known result that the Bayesian belief distribution bt = P(st|ht) is a sufficient statistic for the interaction history (Kaelbling et al., 1998). The history dependent value functions and policy in POMDP can thus be written in terms of beliefs:\nQ(b,a) = \\sum_{S}b(s)R(s, a) + \\sum_{o'} P(o'|b,a)V(b' (o', a, b)), V(b) = \\max_{a} Q(b,a),\n(5)\nwhere P(o'|b, a) = \u2211s,s, P(o'|s')P(s'|s, a)b(s) and b'(o', a, b) denotes the belief update function from prior b(s) to the posterior:\nb'(o', a, b) := b'(s'|o', a,b) = \\frac{P(o'|s') \\sum_{s} P(s'|s,a)b(s)}{\\sum_{s'}P(o'|s') \\sum_{s} P(s'|s, a)b(s)}.\n(6)\nThe optimal policy derived from the above value functions is sometimes referred to as the Bayes optimal policy (Duff, 2002).\nThe belief value functions in (5) imply a special class of MDPs known as belief MDPs (Kaelbling et al., 1998) where the reward and dynamics are defined on the belief state as:\nR(b,a) = \\sum_{S}b(s)R(s,a), P(b'|b,a) = P(o'|b,a)\\delta(b' \u2013 b'(o', a, b)).\n(7)\nThe stochasticity in the belief dynamics is entirely due to the stochasticity of the next observation; the belief updating process itself is deterministic.\nIn this work, we generalize the notion of belief MDP to refer to any MDP defined on the space of beliefs. However, not all belief MDPs could yield the optimal policies for some POMDPs."}, {"title": "2.3 Active Inference", "content": "Active inference is an application of the variational principle to perception and action, where intractable Bayesian belief updates (i.e., (6)) are approximated by variational inference (Da Costa et al., 2020). At every time step t, vari- ational inference searches for an approximate posterior Q(st) which maximizes the evidence lower bound of data marginal log likelihood, or equivalently minimizes the variational free energy F:\nF(Q) = E_{Q(s_{t})} [log Q(s_{t}) \u2013 log P(o_{t}, s_{t})],\n(8)\nwhere P(ot, St) = P(Ot St)P(St). In the context of POMDPs, the prior is given by P(St) = \u2211st\u22121 P(St St\u22121, at\u22121)Q(st\u22121). It is well-known that the optimal variational approximation under appropriately chosen family of posterior distributions equals to the exact posterior in (6) (Blei et al., 2017). We will thus assume appropriate choices of variational family and omit suboptimal belief updating in subsequent analyses.\nCentral to the current discussion is the policy selection objective functions used in active inference, which is its main difference from classic POMDPs. In particular, active inference introduces an objective function called expected free energy (EFE) which, given an initial belief Q0(80) and a finite sequence of actions a0:T\u22121, is defined as (Friston et al., 2017):\nEFE(00:T\u22121, Q0) = E_{Q(01:T,S1:T|00:T-1)} [log Q(S1:T|@0:T\u22121) \u2013 log P(01:T, 81:T)],\n(9)\nwhere Q(S1:T00:T-1) is defined as the product of the marginal state distributions along the action sequence (we show how this can be approximately obtained as a result of variational inference and discuss the implication of defining this instead as the joint distribution in the appendix, which also contains all derivations and proofs):\nQ(S1:T01:T-1) = \\prod_{t=1}^{T}Q(S_{t}|Q_{t-1}, a_{t-1}),\nQ(S_{t}|Q_{t-1}, a_{t-1}) := \\sum_{S_{t-1}} P(S_{t} | S_{t-1}, a_{t-1})Q(S_{t-1}|Q_{t-2}, a_{t-2}),\n(10)\nand Q(01:T, 81:T|A1:T\u22121) = \u03a0=1P(Ot|St)Q(St|Qt\u22121, at\u22121). These distributions represent the agent's posterior predictive beliefs about states and observations in the future. Notice (9) is different from (1), but it is used here because it is more general (Champion et al., 2024).\nThe distribution P(01:T, 81:T) is interpreted as a \"preference\" distribution under which preferred observations and states have higher probabilities. While there are multiple ways to specify P in the literature, we will focus on the most popular specification:\nP(00:T, 50:T) = \\prod_{t=0}^{T}P(o_{t})P(S_{t}|o_{t}),\n(11)\nwhere P(stot) is an arbitrary distribution. This specification allows us to factorize EFE over time and construct the following approximation:\nEFE(00:T-1, Q0) \u2248 \\sum_{t=1}^{T} - E_{Q(0t|00:T-1)} [log P(o_{t})] \u2013 E_{Q(0t|90:T-1)} [KL[Q(S_{t}|o_{t}, 00:T\u22121)||Q(S_{t}|ao:T\u22121)]], (12)\nPragmatic value\nEpistemic value\nwhere KL denotes Kullback-Leiblier divergence, Q(0t|a0:T\u22121) = \u2211s\u0142 P(ot|St)Q(St|Qt\u22121,At\u22121) is the posterior predictive over observations, and Q(st|ot, a0:T\u22121) XP(Ot|St)Q(St|Qt\u22121, at\u22121) is the future posterior given posterior predictive of future states as prior and future observations. We discuss this approximation and optimal choice of P(s|0) further in the appendix.\nAs preempted in the introduction, in (12), the first term \"pragmatic value\" scores the quality of predicted observa- tions under the preferred distribution. The second term \"epistemic value\" measures the distance between future prior Q(stao:T-1) and posterior beliefs Q(st Ot, ao:T\u22121), which corresponds to the amount of expected \"information gain\" from future observations. The epistemic value term is an especially salient difference between active inference and classic POMDPS."}, {"title": "3 Unifying Active Inference and RL Under Belief MDPS", "content": "The use of EFE vs. reward and the search for action sequences (i.e., plans) vs. policies are the main contentions between active inference and RL. In this section, we show that active inference can be equally represented using reward and policy in a special class of belief MDPs. The key is to show that the EFE objective can be characterized using a recursive equation akin to the Bellman equation. This can be achieved immediately by expressing the predictive distribution at each step using the predictive distribution at the previous step:\nEFE(00:T-1, Q0)\n= \\sum_{t=1}^{T} - E_{Q(ot|ao:T-1)} [log P(o_{t})] \u2013 E_{Q(ot|ao:T-1)} [KL[Q(S_{t}|o_{t}, a0:T\u22121)||Q(S_{t}|ao:T\u22121)]]\n= \\sum_{t=0}^{T-1} - E_{Q(Ot+1|Qt,at)} [log P(o_{t+1})] \u2013 E_{Q(ot+1|Qt,at)}[KL[Q(S_{t+1}|Ot+1,Qt, a_{t})||Q(S_{t+1}|Qt, a_{t})]]\n= -E_{Q(01|Q0,40)} [log P(01)] \u2013 E_{Q(01|Q0,ao)} [KL[Q(S_{1}|01, Qo, ao)||Q(s_{1}|Qo, ao)]] + EFE(a1:T\u22121, Q1).\n(13)\nThe recursive equation implies a transition dynamics over the state marginal Qt which only depends on the previous state marginal Qt-1, i.e., the transition is Markovian. The per-time step EFE only depends on the current state marginal. Using the equivalence between the optimal Qt and bt, we can write the reward and transition dynamics of the belief MDP implied by EFE as follows:\nREFE(b,a) = EP(o'lb,a) [log P(o')] + EP(o'lb,a) [KL[b(s'|o', b,a)||b(s'|b,a)]]\n:= R(b,a) + IG(b,a),\nPopen (b'|b, a) = \\delta(b' \u2013 b'(a, b)), where b' (a, b) := b' (s'|b, a) = \\sum_{S}P(s'|s, a)b(s).\n(14a)\n(14b)\n(14c)\nBy constructing the above belief MDP, the search for optimal action sequences can be equally represented as the search for optimal belief-action policies.\nProposition 3.1. (Active inference policy) The EFE achieved by the optimal action sequence can be equivalently achieved by a time-indexed belief-action policy \u03c0(at|bt).\nProof. The proof is due to the above belief MDP characterization. An alternative proof is given in the appendix.\nThese identities enable us to define infinite-horizon discounted belief MDPs using the EFE reward and dynamics in (14) and restrict our search to time-homogeneous Markovian belief-action policies. A similar result was presented recently by Malekzadeh and Plataniotis (2022). However, rather than focusing on policy optimization algorithms, our goal here is to clarify the belief MDP implied by EFE.\nHowever, notice a few differences between the EFE belief MDP and the Bayes optimal belief MDP. First, the belief dynamics in (14c) does not contain observation o; rather it is the marginal prediction of the next state given the previous belief. Such a belief dynamics has been referred to as open-loop in the literature (Flaspohler et al., 2020) in the sense that it does not take into account the possibility of updating beliefs based on future observations, akin to open-loop controls. In contrast to the POMDP belief dynamics in (7), the open-loop belief dynamics is deterministic given a.\nSecond, the EFE reward function contains an information gain term which corresponds to epistemic value. The first term pragmatic value is defined as the expected log likelihood of the next observation. This does not introduce much difference from the POMDP reward function because we can define the active inference preference distribution as a Boltzmann distribution parameterized by a reward function P(o) x exp(x\u0158(0)) and assume that R(o) self-normalizes so that the partition function equals 1. The resulting reward can still be written as a linear combination of state-action reward:\nR(b,a) = E_{P(o'lb,a)} [log P(o')]\n= \\sum_{S}b(s) \\sum_{s'} P(s'|s, a) \\sum_{o'}P(o'|s')\\~{R}(o')\n= \\sum_{S}b(s)\\~{R}(s,a).\n(15)"}, {"title": "4 Analyzing Policies in MDPS", "content": "The belief MDP characterizations of both the EFE policy and the Bayes optimal policy enable us to use MDP analysis tools for POMDPs. The main analysis tools we use in this paper are recent extensions of the performance difference lemma (Kakade and Langford, 2002) and simulation lemma (Kearns and Singh, 2002) which are well-known results in RL theory that quantify the performance difference between different policies or the same policy in different en- vironments. To compare active inference with RL, we are interested in the setting where two policies are optimal w.r.t. both different rewards and different dynamics, however, the evaluation reward and dynamics are equivalent to only one of the policies (here referred to as the expert policy). The following lemma, which extends lemma 4.1 in (Vemula et al., 2023) to the setting of different rewards, gives the performance gap (also known as the regret) between the two policies:\nLemma 4.1. (Performance difference in mismatched MDPs) Let \u03c0 and \u03c0' be two policies which are optimal w.r.t. two MDPs M and M'. The two MDPs share the same initial state distribution and discount factor but have different rewards R, R' and dynamics P, P'. Denote \u2206R(s,a) = R'(s,a) \u2013 R(s, a). The performance difference between \u03c0 and \u03c0' when both are evaluated in M is given by:\nJ\u043c(\u043f) \u2013 J\u043c(\u03c0')\n=\n1\n(1 \u2013 \u03b3)\nAdvantage under expert distribution\n+\n+\nModel advantage under own distribution\nReward-model disadvantage under expert distribution\n(17)\nLemma 4.1 decomposes the performance gap in MDP M between policy \u03c0 (the expert) and \u03c0' into three terms. The first term is the advantage value of \u03c0' under the expert's state-action marginal distribution. The second term is the difference in reward between MDP M' and M and the difference in the value VM', of \u03c0' in M' due to the difference in dynamics expected under the state-action marginal distribution of \u03c0'. This term quantifies the \"advantage\" of being evaluated in one MDP vs another. The last term is the opposite of reward-model advantage, i.e., disadvantage, expected under the expert policy \u03c0's state-action marginal distribution.\nOne can obtain an intuitive understanding of (17) by attempting to minimize the performance gap via optimizing R', P', given we require \u03c0' to be the optimal policy w.r.t. some R', P'. First, it holds that when R', P' are respectively equal to R, P, the reward and model advantages are zeros, and the policy advantage is zero as a result. This means one can read policy, reward, and model advantage as a measure of error from the expert MDP and policy. When such error is nonzero, R', P' are optimized to increase reward-model advantage under the expert distribution and decrease reward- model advantage under the policy's own distribution. This encourages \u03c0' to choose actions that lead to state-actions"}, {"title": "5 Value of Information in POMDPS", "content": "Given the primary difference between active inference and RL is the definition of epistemic value and open-loop belief dynamics, we ask whether it could be seen as an approximation to the Bayes optimal policy, specifically the epistemic aspect thereof? To this end, we first analyze the \"value of information\" in the Bayes optimal policy. We then show that epistemic value closes the gap to the Bayes optimal policy by making up for the loss of information value."}, {"title": "5.1 Value of Information in Bayes Optimal RL Policy", "content": "It's colloquially accepted that the Bayes optimal policy characterized by the value functions in (5) optimally trades off exploration and exploitation. However, it's not immediately obvious what is being traded off, the comparison is made against which alternative action or policy, and how large is the performance gap. In this paper, we adopt the view that what's being traded off is the value of information, which we try to quantify in an action or policy. In (Howard, 1966), the value of information for a single step decision making problem is defined as the reward a decision maker is willing to give away if they could have their uncertainty resolved (e.g., by a clairvoyant). Formally, the expected value of perfect information (EVPI) is defined as the difference between the expected value given perfect information (EV|PI) and the expected value without perfect information (EV).\nIn the POMDP setting, the agent cannot in general obtain perfect information about the hidden state, but an observation that is usually correlated with the state. It turns out that this corresponds to an extension of Howard's definition in the single step decision making setting called the value of imperfect information (Raiffa and Schlaifer, 2000). For consistency in notation, we will label it as the expected value of perfect observation (EVPO) and define it as:\nEV PO = EV|PO \u2013 EV,\nEV = \\max_{a} \\sum_{S}b(s) R(s,a),\nEV|PO = \\sum_{o} \\sum_{S}P(os)b(s) \\max_{a} R(b(s|o), a) .\n(19)\nSimilar to EVPI, EVPO is non-negative because an optimal decision maker cannot gain information and do worse (Howard, 1966).\nExtending this definition for the multi-stage sequential decision making setting, we have the following corollary of EV and EV PO for POMDPs:\nEV: Q^{open}(b,a) = \\sum_{S}b(s)R(s, a) + \\gamma V^{open} (b'(a, b)),\nEV\\PO : Q(b,a) = \\sum_{S}b(s)R(s, a) + \\gamma \\sum_{o'} P(o'|b, a) V (b'(o', a, b)) .\n(20a)\n(20b)"}, {"title": "5.2 Main Result: EFE Approximates Bayes Optimal RL Policy", "content": "The main insight of this work is that EFE closes the optimality gap between open and closed-loop policies by aug- menting the reward of the open-loop policy with the epistemic value term. Given the pragmatic value is linear in the belief (15), we will use it as the shared reward between active inference and RL agents, i.e., R(s,a) = \u0154(s,a).\nProposition 5.2 shows that the advantage of closed-loop belief transition is proportional to the information gain pro- vided by the next observation. While the agent cannot change either belief transition distributions, it can change its reward to alter the reward-model advantage and the marginal distribution under which it is evaluated. An obvious choice for the reward advantage is to set it to the information gain in order to cancel with the information disadvantage of open-loop belief dynamics. To ensure the agent does not get distracted by gaining information and still focus on task relevant behavior, we make the following assumption on preference distribution specification:\nAssumption 5.3. (Preference specification) The preference distribution or reward is specified such that the gain in pragmatic value after receiving a new observation is higher than the loss in epistemic value in expectation under the Bayes optimal policy \u03c0 in closed-loop belief dynamics P:\n\\sum_{S} (b(s|o) \u2013 b(s)) R(s,a) \\geq E_{(b,a)~d^{\\pi}} [IG(b(s), a) \u2013 IG(b(s|o), a)] .\n(23)\nThis assumption also ensures that the advantage of closed-loop belief dynamics under the EFE value function is non- negative. In practice, since the Bayes optimal policy behavior can be difficult to know a priori, we can approximate the above by setting a reward function such that the reward difference is sufficiently high. In the appendix, we prove that the advantage upper bound given this assumption is the same as that evaluated under the open-loop belief MDP in proposition 5.2. To facilitate the comparison between open-loop and EFE policy, we introduce two more assumptions:\nAssumption 5.4. (Policy behavior) We make the following assumptions on the behavior of the evaluated policies:\n1.  The absolute advantage of the EFE policy \u03c0EFE expected under the Bayes optimal policy's marginal distribution is no worse than that of the open-loop policy \u03c0open: E_{EFE} = E_{(b,a)~d^{\\pi}} [|A_{\\pi^{EFE}}(b,a)|] \\geq E_{(b,a)~d^{\\pi}} [|A_{\\pi^{open}}(b,a)|]."}, {"title": "6 Discussions", "content": "Our results highlight the nuanced relationship between active inference and the classic approach to POMDPs. In this section, we provide a few complementary perspectives on related POMDP approximation and extensions from the RL literature and discuss objective specification in active inference informed by our results."}, {"title": "6.1 POMDP Approximation and Extensions", "content": "In the POMDP planning literature, there is a suite of approximation techniques to overcome the intractability of exact belief update and value function representation. The simplest ones are the maximum likelihood heuristic and QMDP heuristic which first compute the underlying MDP value function and then obtain the belief value function using either the most likely state under the current belief or a belief-weighted average (Littman et al., 1995). These approximations leverage the fact that MDP value functions (in discrete space) are easy to compute, but they can be overly optimistic since they implicitly assume the state in the next time step will be fully observed (Hauskrecht, 2000). As a result, the agent does not take information gathering actions.\nTo address this shortcoming, there is a special set of heuristics dedicated to inducing information gathering actions (Roy et al., 2005). These information gathering heuristics typically operate in a \"dual-mode\" fashion where exploita- tion and exploration are arbitrated by some criterion. For example, in Cassandra et al. (1996), the exploitation mode chooses actions based on the underlying MDP whereas the exploration mode chooses actions to minimize belief en- tropy in the next time step. These two modes are arbitrated by the entropy of the current belief. Complementary to dual-mode execution, Flaspohler et al. (2020) propose to interleave open-loop with closed-loop belief dynamics when the value of information is low to speed up value function computation. In doing so, these methods alleviate the expensive belief updating operation during planning. While EFE resembles these heuristics and thus amenable to efficiency gain, it introduces an information gain term in the reward, which could be an expensive operation in itself (Belghazi et al., 2018)."}, {"title": "6.2 Objective Specification in Active Inference", "content": "The objective functions in active inference have been subject to various interpretations since its inception in the late 2000's and have only slowed down relatively recently (Gottwald and Braun, 2020). The EFE objective, which first appeared in the literature as early as 2015 in (Friston et al., 2015), was initially motivated by an intuitive argument that \"free energy minimizing agents should choose actions to minimize (expected) free energy\". However, far from being heuristic, the EFE objective is rooted in the free energy principle which adopts a physics and information geometric perspective, rather than a decision theoretic perspective, on agent behavior (Friston et al., 2023b,a; Barp et al., 2022), in which case open-loop belief dynamics is the natural outcome. It should be mentioned, however, that the information geometric derivation of EFE relies on a \"precise agent\" assumption on the environment in which future actions and observations are assumed to have matching entropy (Barp et al., 2022; Da Costa et al., 2024). It remains open whether this assumption is satisfied in real environments.\nRecently, Friston et al. (2021) introduced a \"sophisticated\" version of EFE as an improved planning objective for active inference agents, where instead of evaluating EFE based on future state marginals, EFE is evaluated based on future posterior beliefs Q(st Ot, ao:t-1). This means that the belief MDP underlying the sophisticated EFE uses the closed- loop belief dynamics rather than the open-loop belief dynamics in the vanilla EFE, however, the information gain term is still used in the reward function. This means that we can no longer view sophisticated EFE as an approximation to the Bayes optimal policy. Rather, the combination of pragmatic value and closed-loop belief dynamics renders parts of sophisticated EFE exactly equal to the Bayes optimal belief MDP, until the equivalence is \"broken\" again by the additional information gain term. Does this mean the agent may be motivated to acquire too much information while compromising task performance? A simple manipulation shows that if we define the preference distribution as the exponentiated reward multiplied by a negative temperature parameter \u5165 P(o) x exp(x\u0158(0)), then the EFE reward becomes proportional to a weighted combination of reward and information gain:\nR(s, a) \\propto \\sum_{s'}P(s'|s, a) \\sum_{o'}P(o'|s')\\~{R}(o')\n= \u03bb\u0158(s,a),\n\\widehat{R}^{EFE}(b,a) \\propto \\frac{\u03bb}{\\frac{1}{2}} \\sum_{S}b(s)\\~{R}(s,a) + IG(b,a),\n(26)\nwhere choosing a high \u2192 \u221e corresponds to purely optimizing reward. However, this does mean that when A is not sufficiently high, in which case the objective highly resembles active sensing, the agent may be distracted. But whether"}, {"title": "7 Conclusion", "content": "In this paper, we study the theoretical connection between active inference and reinforcement learning and show that the epistemic value in the EFE objective of active inference can be seen as an approximation to the Bayes optimal RL policy in POMDPs, achieving a linear improvement in regret compared to a naive policy which doesn't take into account the value of information. The results also suggest that, from the perspective of RL, the specification of EFE needs to balance reward with information gain in the environment, via an appropriate temperature parameter (\u03bb). Conversely, from the perspective of active inference, an EFE minimizing agent will pursue a Bayes optimal RL policy, under a suitable temperature parameter. This conclusion might have been anticipated by one reading of the complete class theorem (Wald, 1947; Brown, 1981); namely, for any pair of reward function and choices, there exists some prior beliefs that render the choices Bayes optimal, in a decision theoretic sense (Berger, 2013)."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Proofs for Section 2.3", "content": null}, {"title": "Derivation of Q(S1:T00:T-1) in (10) from variational inference", "content": "We aim to obtain a predictive distribution over future states S1:T given an action sequence a0:T-1 using variational inference. Typically", "Q(S1": "T|ao:T\u22121) = \u03a0=1Q(st|ao:T-1). Since there is no observation and thus no likelihood term", "as": "nF(Q) = E_{Q(S1:T|ao:T\u22121)"}, []]}