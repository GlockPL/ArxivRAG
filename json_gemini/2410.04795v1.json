{"title": "Representing the Under-Represented: Cultural and Core Capability\nBenchmarks for Developing Thai Large Language Models", "authors": ["Dahyun Kim", "Sukyung Lee", "Yungi Kim", "Attapol Rutherford", "Chanjun Park"], "abstract": "The rapid advancement of large language mod-\nels (LLMs) has highlighted the need for ro-\nbust evaluation frameworks that assess their\ncore capabilities, such as reasoning, knowl-\nedge, and commonsense, leading to the incep-\ntion of certain widely-used benchmark suites\nsuch as the H6 benchmark. However, these\nbenchmark suites are primarily built for the En-\nglish language, and there exists a lack thereof\nfor under-represented languages, in terms of\nLLM development, such as Thai. On the other\nhand, developing LLMs for Thai should also\ninclude enhancing the cultural understanding as\nwell as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose\ntwo key benchmarks: Thai-H6 and Thai Cul-\ntural and Linguistic Intelligence Benchmark\n(ThaiCLI). Through a thorough evaluation of\nvarious LLMs with multi-lingual capabilities,\nwe provide a comprehensive analysis of the\nproposed benchmarks and how they contribute\nto Thai LLM development. Furthermore, we\nwill make both the datasets and evaluation code\npublicly available to encourage further research\nand development for Thai LLMs", "sections": [{"title": "1 Introduction", "content": "Rapid advancements in large language models\n(LLMs) have significantly contributed to the field\nof natural language processing (NLP) (Chang et al.,\n2024). These advancements created the pressing\nneed for comprehensive benchmarks that rigor-\nously evaluate core capabilities such as reason-\ning, knowledge, and commonsense (Peng et al.,\n2024; Wang et al., 2023). While considerable\nprogress for the aforementioned evaluation need\nhas been achieved for the English language (Guo\net al., 2023), similar evaluation needs are far from\nbeing met for under-represented languages such as\nThai. Current benchmarks for Thai focus mainly\non traditional NLP tasks (Phatthiyaphaibun et al.,\n2023; Trakuekul et al., 2024), i.e., tokenization and\nnamed entity recognition, leaving a critical gap in\nassessing the broader capabilities of LLM.\nHowever, evaluating only the core capabilities\nof an LLM is not enough for the development of\nThai LLMs. Thai LLMs must also appropriately\nreflect the distinct sensitivities and cultural norms\nwithin the Thai language, as these are deeply tied\nto the nation's identity, values, and communication\npatterns (Kirsch, 1977; Thanasankit and Corbitt,\n2002). For example, the Thai pronoun system re-\nflects the social hierarchy that must be respected\nin a conversation (Uckaradejdumrong, 2016). The\nThais have a delicate relationship with their neigh-\nboring countries, which differ from Thailand in\nterms of language, ethnicity, and religion, although\nculturally related in many ways, so the biases are\ncommonplace and encoded in the Thai language\nitself. However, existing evaluation resources (Ar-\nreerard et al., 2022) often lack the depth necessary\nto adequately assess cultural comprehension, creat-\ning yet another evaluation gap.\nTo address these gaps, we propose two compre-\nhensive benchmarks aimed at advancing LLM re-\nsearch in Thai: Thai-H6 and Thai Cultural and Lin-\nguistic Intelligence Benchmark (ThaiCLI). Thai-\nH6 is a localized adaptation of six internation-\nally recognized benchmarks for evaluating core\ncapabilities of LLMs; AI2 Reasoning Challenge\n(ARC) (Clark et al., 2018), Massive Multitask\nLanguage Understanding (MMLU) (Hendrycks\net al., 2020), Truthful Question Answering (Truth-\nfulQA) (Lin et al., 2021), HellaSwag (Zellers et al.,\n2019), Grade School Math (GSM8k) (Cobbe et al.,\n2021), and Winograd Schema Challenge (Wino-\ngrande) (Sakaguchi et al., 2021). The adaptation\nincludes a human expert validation process to en-\nsure both linguistic and contextual accuracy, which\nis illustrated in detail in Section 3.1.\nWe design ThaiCLI to evaluate the comprehen-"}, {"title": "2 Related Work", "content": "Thai NLP Research in Thai NLP has advanced\nsignificantly in recent years, addressing traditional\ntasks such as word segmentation (Limkonchotiwat\net al., 2020; Chormai et al., 2020), named entity\nrecognition (Buaphet et al., 2022), and discourse\nparsing (Prasertsom et al., 2024), to name a few.\nThese past studies address many of the challenges\nin processing Thai language data. The Thai writ-\ning script does not use space or any punctuation to\nmark word and sentence boundaries, making both\nsentence-level and discourse-level analyses very\ndifficult (Lowphansirikul et al., 2022). Thai named\nentities do not show special orthography (e.g. capi-\ntalization), and new Thai names proliferate as peo-\nple prefer unique names. The rise of transformer-\nbased language model propels the progress on Thai\nNLP, but the limited computing resources and the\nscarcity of datasets remain a challenge (Lowphan-\nsirikul et al., 2021; Sriwirote et al., 2023).\nThai Large Language Models (LLMs) The de-\nvelopment of LLMs for the Thai language has\nlagged behind that of other major languages (Wei\net al., 2023; Zhu et al., 2023; Dubey et al., 2024),\nsuch as English, Chinese, and Japanese, primarily\ndue to the lack of high-quality datasets and com-\nprehensive benchmarks. While multilingual LLMs\nsuch as LLaMA have shown some ability to gen-\neralize across languages, their performance on the\nThai language remains suboptimal, as shown in\nSection 5. Recent attempts to fine-tune these multi-\nlingual models specifically for Thai have yielded\nimprovements in certain tasks like machine transla-\ntion (Dou et al., 2024; Nguyen et al., 2023; Zhang\net al., 2024; Pipatanakul et al., 2023). However,\nthese models still struggle to capture the nuances\nand cultural contexts of the Thai language due to\ntraining on predominantly non-Thai corpora (Pi-\npatanakul et al., 2023), as shown in Section 5.3.\nMeanwhile, there is a scarcity of Thai-specific\nLLMs that are pretrained from scratch on large-\nscale Thai text corpora, which limits their applica-\nbility and performance in Thai contexts.\nBenchmarks for Thai LLMs The evaluation of\nThai LLMs has been constrained by the absence\nof comprehensive, well-designed benchmarks that\nassess models capabilities across diverse contexts.\nExisting Thai benchmarks are largely focused on\ntraditional NLP tasks, such as sentiment analy-\nsis, named entity recognition, and machine transla-\ntion (Phatthiyaphaibun, 2019; Suriyawongkul et al.,\n2019; Team, 2022). Recent LLMs use a Thai uni-\nversity entrance exam dataset to assess the capabil-\nity, but these datasets do not assess commonsense\nreasoning or culturally sensitive text generation,\nwhich is required for modern NLP appllications (Pi-\npatanakul et al., 2023). To address this, we create\nbenchmark datasets that extend beyond conven-\ntional NLP tasks to include the cultural and con-\ntextual nuances of the Thai language. The develop-\nment of such benchmarks is crucial for advancing\nLLM research in underrepresented languages like\nThai and ensuring that models can function accu-\nrately and responsibly in real-world Thai contexts."}, {"title": "3 Thai-H6", "content": null}, {"title": "3.1 Annotation Process", "content": "The overall annotation process of the Thai-H6\nbenchmark is depicted in Figure 1. We design the\nannotation process to ensure that the dataset cov-\ners the fundamental capabilities of LLMs, such as\nreasoning, commonsense, and knowledge, within\nthe context of the Thai language. Similar to the\nmethodology used for Ko-H5 (Park et al., 2024),\nwe first use machine translation to convert existing\nH6 benchmark datasets (Clark et al., 2018; Zellers\net al., 2019; Hendrycks et al., 2020; Lin et al., 2021;\nSakaguchi et al., 2021; Cobbe et al., 2021) into"}, {"title": "3.2 Dataset sizes", "content": "The Thai-H6 benchmark contains six datasets: th-\nARC, th-HellaSwag, th-MMLU, th-TruthfulQA,\nth-GSM8k, and th-Winogrande. Each dataset is de-\nsigned to test specific capabilities of LLMs, ranging\nfrom general reasoning and commonsense (e.g., th-\nHellaSwag, th-MMLU) to domain-specific knowl-"}, {"title": "3.3 Evaluation Methodology", "content": "As Thai-H6 is built from the original English H6\nbenchmark, we also adopt the same evaluation strat-\nagy for each of the dataset. Specifically, we use the\nlog-probability evaluation protocol for the th-ARC,\nth-HellaSwag, th-MMLU, th-TruthfulQA, and th-\nWinogrande datasets and the exact match proto-\ncol of the generated answers for the th-GSM8K\ndataset. Scores for each of the datasets is acquired\nseparately, where the average of the six scores is\nused as the Thai-H6 benchmark score. Since log-\nprobability protocol is involved in the evaluation\nmethodology, it is currently not possible to evaluate\nclosed LLM APIs."}, {"title": "4 ThaiCLI", "content": null}, {"title": "4.1 Dataset Structure", "content": "The objective of ThaiCLI benchmark is to assess\nthe alignment of LLMs with Thai cultural norms,\nvalues, and ethical standards. Each question in\nthe dataset is paired with two distinct types of re-\nsponses: Chosen and Rejected, forming {Question,\nChosen, Rejected} triplets. For example, in re-"}, {"title": "4.1.1 Question Distribution", "content": "In ThaiCLI, the questions cover seven key thematic\ndomains: royal family, religion, culture, economy,\nhumanity, lifestyle, and politics. These categories\nprovide a comprehensive evaluation of the model's\nunderstanding of the various aspects of Thai cul-\nture. The format of the questions can be classified\ninto two distinct categories of Factoid, and In-\nstruction. Each category is designed to evaluate\ndifferent aspects of the model's performance in a\nThai context.\nFactoid This category comprises general con-\nversational questions that are designed to cover\nregular day-to-day dialect. The questions cover a\nwide range of societal issues, reflecting everyday\ninquiries a Thai-speaking user might pose. The\ngoal is to ensure that the model can respond in a\nway that respects Thai cultural and social norms\nfor a broad range of themes. An example from\nthe factoid category, with accompanying English\ntranslations, are shown in Figure 2.\nInstruction This category of question describes\na task that a user might use LLM to complete, such\nas giving an example or summarizing. The model"}, {"title": "4.1.2 Answer Types", "content": "Each question in the dataset is paired with two con-\ntrasting types of responses: Chosen and Rejected.\nThese responses are intended to serve as positive\nand negative exemplars, respectively, for assessing\nthe model's cultural understanding. Additionally,\neach type of response is constructed according to\nsix key characteristics, as outlined in the methodol-\nogy described by Lee et al. (2023).\nChosen answers. Chosen answers are formu-\nlated to demonstrate cultural sensitivity, ethical\nsoundness, and inclusivity. They are designed to\nalign with Thai societal norms and reflect a nu-\nanced understanding of the diverse cultural, reli-\ngious, and social contexts.\nRejected answers. Rejected answers fail to show\nthe understanding of Thai cultural facts or fail to\nrecognize that the task that the user asks to per-\nform is culturally insensitive or biased. The core\nattributes which human contributors must consider\nwhen annotating the chosen and rejected answers\nare summarized in Table 2."}, {"title": "4.2 Annotation Process", "content": "The annotation process of the ThaiCLI dataset is\nshown in Figure 4. We recruit 12 native Thai speak-\ners (for more details, see Appendix C), all of whom\nare either university graduates majoring in humani-\nties or social science or professionals actively em-\nployed within Thailand. The annotators first review\nthe annotation guidelines to understand the types\nof questions and answers that we expect. Then the\nannotators begin to write questions and answers for\neach theme and each type of question.\nSubsequently, the dataset undergoes three rounds\nof human review by the authors, each focusing on a\ndistinct aspect: the relevance between the questions\nand answers, alignment with Thai cultural norms,"}, {"title": "4.3 Dataset sizes", "content": "The ThaiCLI dataset consists of 1,790 samples\nwith factoid question formats, i.e., conversational\nquestions, designed to assess the model ability to\nprovide factually accurate and culturally sensitive\nresponses (Table 3). Additionally, there are 100\nsamples with instruction question formats, which\ntest the model's adherence to cultural norms as well\nas its instruction following capabilities."}, {"title": "4.4 Evaluation Methodology", "content": "The main goal of the ThaiCLI benchmark is to mea-\nsure an LLMs ability to incorporate Thai cultural\nnorms into its responses. Unfortunately, judging\nwhether a models answer adheres to such cultural\nnorms is difficult to do pragmatically, i.e., hard to\ndefine scoring functions.\nAnother option would be to not generate model\nanswers at all. Rather, one could use the chosen\nand rejected answers in the ThaiCLI dataset and\ncalculate the probability that a given model would\ngenerate those answers. Then, a higher probabil-\nity for the chosen answer would indicate correct\nbehavior of the model for that particular question.\nHowever, as this approach does not directly eval-"}, {"title": "5 Experiments", "content": "To evaluate the performance of LLMs on the Thai-\nH6 and ThaiCLI benchmark, we select multiple\nopen source state-of-the-art LLMs. We choose\nopen-source models based on their performance on\nglobally recognized benchmarks and their availabil-\nity in the Thai language or their adaptability to it.\nFurther, for the ThaiCLI benchmark, we also eval-\nuate multiple closed LLM APIs, to better gauge\nthe status quo of open source LLM for the Thai\nlanguage."}, {"title": "5.1 Model Details", "content": "Open source LLMs. The open source LLMs\nused for evaluation are Meta-Llama-3.1-8B-\nInstruct (Dubey et al., 2024), Meta-Llama-3.1-\n70B-Instruct (Dubey et al., 2024), Qwen2-72B-\nInstruct (Yang et al., 2024), Llama-3-Typhoon-\nv1.5x-70b-Instruct (Pipatanakul et al., 2023),\nSailor-14B-Chat (Dou et al., 2024), and SeaLLMs-\nv3-7B-Chat (Zhang* et al., 2024). The first three\nmodels are chosen for their globally well-known\nperformance, while the latter three models are cho-\nsen for their adaptation to Thai or South East Asian\nlanguages.\nClosed LLM APIs. In addition to open source\nLLMs, we also evaluate multiple closed LLM APIs\nfor the ThaiCLI benchmark. Note that it is not"}, {"title": "5.2 Performance on Thai-H6", "content": "The results in Table 4 summarize the performance\nof various open source LLMs on the Thai-H6\nbenchmark. Note that closed LLM APIs cannot\nbe evaluated.\nEffect of model size. The highest Thai-H6 score\nis achieved by Qwen2-72B-Instruct, which is\nclosely followed by Llama-3-Typhoon-v1.5x-70b-\nInstruct and Meta-Llama-3.1-70B-Instruct, where\nall three models have the largest parameter count\nthat exceeds 70 billion. In contrast, smaller sized\nLLMs definitely score lower on the Thai-H6 bench-\nmark, sometimes despite their specific focus on\nSouth East Asian languages. Smaller LLMs tend\nto lag behind on general (th-ARC and th-MMLU)\nand mathematical (th-GSM8K) reasoning the most.\nEffect of regional specialization. We note\nthat regional specialization is not always obso-\nlete. For instance, Llama-3-Typhoon-v1.5x-70b-\nInstruct does score higher than Meta-Llama-3.1-\n70B-Instruct with the same number of parameters.\nHowever, the advantage of regional specializa-\ntion is not as pronounced in the Thai-H6 bench-\nmark than model size. For instance, SeaLLMs-v3-\n7B-Chat actually scored lower than Meta-Llama-\n3.1-8B-Instruct, despite having similar number\nof parameters. Additionally, Sailor-14B-Chat\nstill exhibit lower scores than Meta-Llama-3.1-8B-\nInstruct on datasets such as th-GSM8K. This sug-"}, {"title": "5.3 Performance on ThaiCLI", "content": "The evaluation results on the ThaiCLI benchmark\nfor closed LLM APIs and open source LLMs are\nsummarized in Table 5. The scores are aggregated\nbased on the category being factoid or instruction,\nof which the average is shown as the ThaiCLI\nscore.\nClosed LLM APIs. For closed LLM APIs, GPT-\n40 has the highest score, closely followed by\nClaude Sonnet and GPT-40 mini. Interestingly,\nGPT-40 mini outperforms GPT-4 Turbo despite be-\ning a much cheaper API. Furthermore, Gemini Pro,\nthe flagship API from Google, lags behind that of\nOpenAI or Anthropic. GPT-3.5 Turbo shows the\nlowest score by far, possibly indicating that the\nAPIs performance is not on par with other options.\nFurther, closed LLM APIs, with the exception\nof GPT-3.5 Turbo, show little difference in scores\nbetween the factoid and instruction categories. This\nis interesting because the instruction category has\nthe additional difficulty of having to follow specific\ninstructions as well as aligning to Thai culture. This\nmay indicate that closed LLM APIs all excel in\ninstruction following abilities.\nOpen source LLMs. For open source LLMs, the\nbest score is achieved by SeaLLMs-v3-7B-Chat,\neven higher than models with much bigger sizes.\nInterestingly, the ThaiCLI benchmark seems to\ndemonstrate the importance of language specializa-\ntion in LLMs where models such as SeaLLMs-v3-\n7B-Chat, Sailor-14B-Chat, and Llama-3-Typhoon-\nv1.5x-70b-Instruct all show good performance. In\ncontrast, Meta-Llama-3.1-70B-Instruct shows the\nsecond lowest score, indicating that the ThaiCLI\nbenchmark is not all about size.\nAnother interesting result is that all open source"}, {"title": "5.4 Comparative Analysis Between Thai-H6\nand ThaiCLI", "content": "The apparent differences in performance trends for\nthe Thai-H6 and the ThaiCLI benchmarks clearly\nindicate that the ThaiCLI benchmark is capturing a\npart of an LLM's ability that is not well represented\nin the Thai-H6 benchmark. For instance, SeaLLMs-\nv3-7B-Chat, the worst performing model in the\nThai-H6 benchmark, is the best performing open\nsource LLM in the ThaiCLI benchmark. Thus, gen-\neral knowledge and reasoning, as captured by Thai-\nH6, and cultural understanding, as captured by\nThaiCLI, may require different traits and strengths\nin LLMs.\nFor example, while larger models clearly dom-\ninate in Thai-H6, their performance on ThaiCLI"}, {"title": "6 Conclusion", "content": "In this work, we address the lack of evaluation\nframeworks for Thai LLMs by introducing two key\nbenchmarks: Thai-H6 and ThaiCLI. Thai-H6 pro-\nvides a foundational assessment of LLMs' reason-\ning, knowledge, and commonsense abilities, while\nThaiCLI evaluates cultural understanding and ethi-\ncal alignment within Thai contexts. Together, these\nbenchmarks offer a comprehensive approach to\nevaluating LLMs in Thai, ensuring that models are\nboth linguistically accurate and culturally informed.\nOur results emphasize the importance of incorpo-\nrating cultural considerations into LLM evaluation,\nhighlighting the need for more inclusive LLMs. We\nhope that ThaiCLI and Thai-H6 will foster further\nresearch in developing LLMs for under-represented\nlanguages and contribute to the creation of more\nequitable language technologies."}, {"title": "Limitations", "content": "Despite the significance of the Thai-H6 and\nThaiCLI benchmarks in advancing the evaluation\nof Thai LLMs, several limitations remain. First,\nthe ThaiCLI benchmark provides valuable insights\ninto cultural alignment, but it is inherently lim-\nited by the subjective nature of cultural interpre-\ntation. Cultural norms and sensitivities can vary\nwidely even within the same country, and what is\ndeemed appropriate by one group may not be uni-\nversally accepted. Although the benchmark was\ndeveloped with expert input, it may not fully cap-\nture the rich diversity of perspectives within Thai\nsociety, which could affect the consistency of eval-\nation outcomes.\nSecond, the ThaiCLI benchmark focuses on con-\ntemporary ethical and cultural norms, which are\ninherently fluid and subject to change. As societal\nvalues evolve, the benchmark may require periodic\nupdates to remain relevant and reflective of current\nethical considerations.\nThird, while our benchmarks are designed to as-\nsess core linguistic and cultural capabilities, they\ndo not address other important factors, such as mul-\ntimodal understanding or interactive dialogue capa-\nbilities, which are becoming increasingly relevant\nin real-world LLM applications. Future work will\naim to address these limitations by incorporating\na wider range of linguistic varieties, refining the\ncultural benchmarks to reflect changing norms, and\nexpanding the scope of evaluation to include more\ndynamic aspects of language use.\nLastly, while we provide open access to both\nthe datasets and evaluation code, the Thai-specific\nnature of the benchmarks may limit their appli-\ncability to other languages. Future work should\nexplore the development of similar culturally sen-\nsitive benchmarks for other underrepresented lan-\nguages, thereby enhancing inclusivity in LLM eval-\nuation across different linguistic contexts."}, {"title": "Ethics Statement", "content": "All experiments conducted in this work were per-\nformed with fairness and transparency. The eval-\nuation of the Thai-H6 and ThaiCLI benchmarks\nwas carried out impartially, ensuring that no bias\nor manipulation influenced the results. The dataset\ncreation process was handled by professional third-\nparty organizations specializing in linguistic and\ncultural assessments, ensuring the development ad-\nhered to strict guidelines for accuracy and fairness.\nWe further confirm that there are no licensing\nissues associated with the datasets or models used\nin this research. All data and resources comply with\nopen access and licensing regulations, ensuring that\nour work meets both ethical and legal standards."}]}