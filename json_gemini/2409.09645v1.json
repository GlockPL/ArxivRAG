{"title": "COSCO: A Sharpness-Aware Training Framework for Few-shot Multivariate Time Series Classification", "authors": ["Jesus Barreda", "Ashley Gomez", "Ruben Puga", "Kaixiong Zhou", "Li Zhang"], "abstract": "Multivariate time series classification is an important task with widespread domains of applications. Recently, deep neural networks (DNN) have achieved state-of-the-art performance in time series classification. However, they often require large expert-labeled training datasets which can be infeasible in practice. In few-shot settings, i.e. only a limited number of samples per class are available in training data, DNNs show a significant drop in testing accuracy and poor generalization ability. In this paper, we propose to address these problems from an optimization and a loss function perspective. Specifically, we propose a new learning framework named COSCO consisting of a sharpness-aware minimization (SAM) optimization and a Prototypical loss function to improve the generalization ability of DNN for multivariate time series classification problems under few-shot setting. Our experiments demonstrate our proposed method outperforms the existing baseline methods. Our source code is available at: https://github.com/JRB9/COSCO.", "sections": [{"title": "1 Introduction", "content": "Multivariate time series classification has attracted significant research interest in the last two decades due to its wide applications [2, 15, 18, 19, 22, 35]. Recently, deep neural networks (DNN) have surpassed classical distance-based methods (e.g. 1NN-DTW) [7, 14] and achieved state-of-the-art performance. However, their success often relies on large numbers of labels [11, 29, 30, 32] from domain experts which are challenging to obtain in time series data [26, 28, 34]. Under a more realistic few-shot setting, where each class has only a limited number of expert-labeled training data [30, 32], DNNs models show poor generalization ability to new data as illustrated by Figure 1. For example, the testing accuracy of ResNet on the SpokenArabicDigits dataset from UCR Time Series Classification Archive drops sharply from 87.0% under the 30-shot setting to 70.0% under the 10-shot setting and falls dramatically to 36.2% under the 1-shot setting. It has been noted [12, 21, 25] that commonly used loss functions such as cross-entropy, often exhibit a complex landscape and thus might not be not be sufficient to achieve satisfactory generalization [12]. In particular, without sufficient labeled data or prior knowledge regularization, the large scale of trainable parameters often fall into sharp local minima during model training [1, 10], where the loss value increases rapidly with any small perturbation to the model weights. It has been observed that the sharp local minima are correlated with poor model generalization in various domains. In addition, time series signals are known to be very complex and noisy. Under few-shot learning setting, any outlier samples could severely affect the decision boundary learnt by the model and significantly hurt the model generalization [9, 24]. To address this issue, we propose a new learning framework named Centroid Oriented Sharpness-Controlled Optimization (COSCO) for few-shot multivariate time series classification. Specifically, COSCO proposes to incorporate sharpness-aware minimization (SAM) [12] optimization technique with a perturb-then-update operation based on the neighborhood of loss values to combat complex landscapes in commonly used loss functions. In addition, COSCO utilizes a prototypical loss to replace the conventional cross-entropy loss to further improve the robustness against outlier samples in data scarcity situations. In summary, our main contributions is as follows: 1) propose a new learning framework designed for few-shot multivariate time series classification; 2) conduct comprehensive experimental results to demonstrate that COSCO outperforms baselines; 3) conduct ablation test to show the effectiveness of the modules."}, {"title": "2 Related Work", "content": "Multivariate time series classification has been one of the most extensively studied topics in the time series data mining domain over the past two decades [2, 15, 20, 35]. Recent deep learning methods [7, 8, 13, 35] have achieved state-of-the-art performance, surpassing classical time series data mining methods such as 1NN-DTW [6]. However, most existing work relies on fully labeled datasets and requires extensive supervision. In few-shot settings, where only a limited amount of samples are available, their performance often experiences a significant decline. Existing research efforts have focused on semi-supervised learning and assume unlabeled training data are available. TapNet [35] utilizes a prototypical network to perform semi-supervised learning. SimTSC [32] integrates pre-computed Dynamic Time Warping (DTW) [23] and performs batch Graph Convolution Network (GCN) [17] to similarly perform semi-supervised learning. SimTSC requires pairwise DTW computation for all training, unlabeled, and testing data, which can be inefficient for large datasets. Meanwhile, LB-SimTSC [29] uses LB-Keogh bound [16] instead of exact DTW distance, achieving comparable performance in semi-supervised learning. All above methods rely on the availability of unlabeled or testing data during training time, making them unsuitable for the more challenging situations encountered in few-shot time series classification problems. We differ from Tapnet [35] in two major ways. Firstly, our proposed prototypical loss is a loss function, rather than a network structure, and is specifically designed for few-shot learning without relying on testing data. In addition, Tapnet [35] is used in semi-supervised settings where test data is available during training. Recently, Sharpness-aware minimization techniques have demonstrated promising performance in domains such as image classification [10, 12], natural language processing [3], and graph machine learning [27, 31, 36, 37]. However, to the best of our knowledge, there is currently no research exploring how to improve the generalization ability of deep learning models through optimization techniques in multivariate time series classification problems."}, {"title": "3 Preliminaries", "content": "We begin by discussing the background of time series and then formulate the problem setting, followed by an introduction to deep learning models for time series.\nDefinition 3.1. A univariate time series is denoted as x = $[x_1, x_2,\\cdots, x_T] \\in R^1$, where T is time length.\nDefinition 3.2. A multivariate time series X consists of M co-evolving univariate time series, i.e. X = $[X_1, X_2, \\cdots, X_M] \\in R^{M\\times T}$."}, {"title": "3.1 Problem Setting", "content": "We formulate the multivariate time series classification (MTSC) problem. Let X = $[X_1,X_2,\\cdots,X_N] \\in R^{N\\times M \\times T}$ denote a collection of multivariate time series (MTS), where $X_i \\in R^{M\\times T}$ is the i-th sample. Each instance $X_i$ is associated with a label $y_i$ from classes $Y\\epsilon$ {1,\u2026, C}. Given a training dataset $X_{train}$ = $[X_1,X_2,\\cdots, X_N]$ and the corresponding training labels $Y_{train}$ = $[y_1,\\cdots, Y_N, ]$, the goal of MTSC problem is to learn a mapping function $f : X \\rightarrow Y$ and apply it to predict the sample labels at testing set $X_{test}$.\nIn this work, we consider a challenging setting of few-shot learning, where the training data consists of a limited number of multivariate time series instances and labels. Particularly, there are only k instances per class for training and k is small.\nIt is widely observed that deep learning models in various domains, such as computer vision and graph machine learning, are prone to converging to sharp local minima, where the loss value increases rapidly in the neighborhood around model weights [1, 5, 10, 12, 33]. The phenomenon of sharp local minima becomes more grievous in the few-shot setting, leading to poor generalization for classification for many test samples. However, no existing research discusses the connection between time-series deep learning models and their generalization performance under few-shot settings. We aim to bridge this gap by developing a model f that can accurately predict on test data despite only training on a small number of samples."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Sharpness-aware Minimization", "content": "SAM [12] has been recently proposed to smooth sharp local minima in large models and improve their generalization in realistic deployments. Specifically, SAM involves two successive steps to update the model: first, it generates perturbation gradients to adversarially shift the model weights to the neighbors associated with the worst loss; second, it explicitly updates the model to minimize this loss value. Through this perturb-then-update operation, SAM softens the loss landscape and improves generalization in domains such as visual and linguistic processing [4], graph data mining [27, 31], Specifically, at each step, SAM trains deep neural networks by solving the following min-max optimization problem:\n$\\min_{\\theta} \\max_{\\epsilon:||\\epsilon||_2 \\leq \\rho} L(\\theta + \\epsilon)$,                                                                                                (1)\nwhere \u03b8 denotes trainable parameters, \u03b5 denotes perturbation gradient, \u03c1 is the maximum size of perturbation gradient, and L is the loss function such as cross-entropy loss for a classification problem. According to the above equation, SAM requires two gradient computations at each training step: i) The inner maximization problem seeks to obtain the worst-case adversarial gradient, termed perturbing gradient. The perturbed model with $\\hat{\\epsilon}$ has the highest loss value centered around model weight \u03b8. ii) The outer minimization problem obtains an updating gradient used to finally improve the model. Via minimizing the worst-case loss, the deep learning model will converge to the optimal weights whose entire neighborhoods have lower loss values and lower curvature. In other words, the sharp local minima are flattened, which may result in enhanced generalization performance. In particular, considering training step t + 1, SAM solves Problem 1 involving the following process:\n$\\epsilon_t = \\nabla L(\\theta_t),  \\hat{\\epsilon}_t = \\rho \\cdot \\frac{\\epsilon_t}{||\\epsilon_t||_2}; \\\\\n@t = \\nabla L(\\theta + \\epsilon_t),  \\theta_{t+1} = \\theta_t - \\eta_t \\cdot @_t$.                                                     (2)\n$\\theta_t$ and $\\theta_{t+1}$ denote the model weights at training steps t and t + 1, respectively. To obtain the intermediate perturbation gradient, SAM requires two complete forward and backward propagations, which obtain the desired generalization at the cost of time efficiency."}, {"title": "4.2 Prototypical Loss", "content": "We introduce the prototypical loss used in COSCO. Instead of using commonly used cross-entropy and fully connected neural networks, we use prototypical loss which can integrate similarity between embeddings and utilize embedding centroid in each class to improve model generalizations. Given a neural network $f : R^{M\\times T} \\rightarrow R^E$ where E is the embedding size, Prototypical loss first computes an average embedding class centroid via:\n$k_j = \\frac{1}{|Y = j|} \\sum_{Y_i=j} f(x_i)$,                                                     (3)\nwhere |Y = j| denotes total number of samples belong to class j. Next, we compute the distance between each instance embedding and their corresponding class embedding centroid $k_j$:\n$D_{i,j} = ||(f(X_i) \u2013 K_j ||_2$,                                                     (4)\nwhere $|| ||_2$ denote L2 norm. The prototypical loss is then defined as follows:\n$L(X_i, y_i) = log\\frac{exp(-D_{i,j})}{\\sum_{i=1}^N exp(-D_{i,j})}$.                                                    (5)\nBy using prototypical loss, we avoid the last fully connected neural network in typical neural networks to save the number of parameters and resist potential outlier instances in few-shot multivariate time series classification problems."}, {"title": "5 Experiment", "content": "In this section, we will describe our experiment setting and evaluate the performance of the model."}, {"title": "5.1 Experiment Setting", "content": "5.1.1 Dataset. In our experiments, we use all multivariate datasets that contain sufficient samples to be adapted into 10-shot settings (at least 10 samples per class) from the famous UEA time series classification repository [2] to generate our few-shot data. We excluded datasets that took more than 4 hours to process.\nTable 1 shows the information about the remaining 21 datasets we used in our experiment."}, {"title": "5.2 Baseline models", "content": "For COSCO, we use the same ResNet architecture from [32] as our backbone. We set the neighborhood parameter \u03c1 = 0.1 for SAM across all 1-shot and 10-shot data in this experiment. We use ResNet as one of the baselines. To maintain consistency, we apply the same learning rate and train for 100 epochs as with the ResNet baseline. We use the implementation from AEON\u00b9 for 1-nearest-neighbor with Dynamic Time Warping (1NN-DTW), Euclidean distance methods (1NN-ED), and TapNet [35]. For 1NN-DTW, we set the warp window size to 0.1 following [6]. For TapNet, we use the default parameters and train for 100 epochs, consistent with our ResNet configuration. TapNet utilizes the Adam optimizer with a learning rate of 0.01. For both ResNet [13] and our COSCO models, we use the architecture from [32] with Stochastic Gradient Descent (SGD) optimizer"}, {"title": "5.3 Experiment 1: Compare with Baselines", "content": "We first conduct experiments with baseline methods. Table 2 shows the testing accuracy for all methods. For accuracy, COSCO ties the highest average performance in 1-shot setting with 1NN-DTW, and achieves the highest average accuracy in 10-shot setting. For ranking, COSCO achieves the best average rank compared with all other methods. Compare with ResNet, COSCO consistently performed better than its backbone ResNet in both average accuracy and ranking, showing the effectiveness of our proposed learning framework. COSCO achieved the highest accuracy in the PEMS-SF 10-shot dataset, surpassing the second-place model by over 6%. In the RacketSports dataset (HAR), COSCO excelled in the 1-shot setting, outperforming 1NN-DTW by over 17%. The experiments demonstrate the ability of COSCO to handle a diverse category of data while improving average performance in few-shot settings."}, {"title": "5.4 Experiment 2: Ablation Test of COSCO", "content": "We conduct an ablation test to demonstrate the effectiveness of both the Prototypical Loss function and SAM optimization technique. We step-wisely remove prototypical loss (PL) and SAM optimization and report the average rank performance on all the 1-shot and 10-shot datasets, as shown in Table 3. In the 1-shot data setting, COSCO surpasses vanilla ResNet but falls short against SAM. In the 10-shot setting, COSCO consistently achieves 1st place (1.762) across all 21 datasets. Overall, COSCO shows the best performance out of three (1.881) and the vanilla ResNet model, which is COSCO without SAM and PL performs the worst (2.024). This shows our proposed modules are effective in general."}, {"title": "6 Conclusion", "content": "In this paper, we propose to address these problems from an optimization and a loss function perspective. Specifically, we propose a new learning framework named COSCO consisting of a sharpness-aware minimization (SAM) optimization and a Prototypical loss function to improve the generalization ability of DNN for multivariate time series classification problems under few-shot learning setting. Our experiments demonstrate our proposed method outperforms the existing baseline methods."}]}