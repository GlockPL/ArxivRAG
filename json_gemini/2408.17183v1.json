{"title": "Causal Reasoning in Software Quality Assurance: A Systematic Review", "authors": ["Luca Giamattei", "Antonio Guerriero", "Roberto Pietrantuono", "Stefano Russo"], "abstract": "Context: Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders\nthat software products work as expected after release in operation. Machine Learning (ML) has proven to be able to\nboost SQA activities and contribute to the development of quality software systems. In this context, Causal Reasoning\nis gaining increasing interest as a methodology to solve some of the current ML limitations. It aims to go beyond a\npurely data-driven approach by exploiting the use of causality for more effective SQA strategies.\nObjective: Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support\nresearchers to access this research field, identifying room for application, main challenges and research opportunities.\nMethods: A systematic literature review of causal reasoning in the SQA research area. Scientific papers have been\nsearched, classified, and analyzed according to established guidelines for software engineering secondary studies.\nResults: Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant\nmethodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where\ncausal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing\nare rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl's graphical\nformulation of causality being preferred, likely due to its intuitiveness. Tools to favour their application are appearing\nat a fast pace most of them after 2021.\nConclusions: The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple\nquality attributes, especially during V&V, evolution and maintenance to ensure reliability, while it is not yet fully\nexploited for phases like requirements engineering and design. We give a picture of the current landscape, pointing\nout exciting possibilities for future research.", "sections": [{"title": "1. Introduction", "content": "Software engineering is an intellectually demanding and creative activity involving complex interdependent tasks\naimed at building software products and ensuring their quality. The advancement of Machine Learning (ML) fostered\na human-machine co-design view to develop highly dependable systems: ML algorithms are able to search for signif-\nicant patterns in large historical datasets gathered throughout the system life cycle, thus supporting several software\nengineering tasks, aimed for instance at fault avoidance (e.g., testing), fault removal (e.g., debugging) and prediction.\nWhile recognizing patterns in data is a fundamental tool for decision-making, well supported by ML, engineers do\nmuch more when building and validating a system. They tend to infer cause-effect relationships among the involved\nvariables, and, based on that, infer hypotheses, simulate possible actions, and derive explanations to then support\ndecisions - in other words, they reason on what have learned. Researchers have been trying to explain causality using\nstatistical and ML methods for years. However, these methods are able to identify connections between variables like\ncorrelation and regression but fall short in detecting causality. Techniques that merely recognize patterns in data only\nget us to what Pearl and Mackenzie [1] call the first rung of the causation ladder (i.e., \"association\"). At this level, we\nare limited to reason about what has been observed."}, {"title": "2. Background", "content": "This Section sets out the background concepts and the terminology used throughout the paper, with respect to the\ntwo drivers of this work: causal reasoning and software quality assurance."}, {"title": "2.1. Causal Reasoning (CR)", "content": "Causal Reasoning (CR) has its roots in the 20th century and has had its first applications in a number of domains,\nsuch as epidemiology, economics, social sciences, and psychology. It had a significant impact on better understanding\nand explaining complex phenomena, making predictions, and managing decision-making processes. CR is supposed\nto mimic something natural in human logic, namely the identification of causes and effects and the answering of \"what\nif\" (causal) questions. Pearl and Mackenzie [1] have conceptualized a framework describing three levels of under-\nstanding and reasoning about causation, called the \"ladder of causality\". The first level is association (correlation),\nbased only on what we observe. It is a basic understanding that two variables tend to co-occur, making no claims\nabout causation. On the other hand, CR allows to stair up to the second and third rung of the ladder, by performing,\nrespectively, \"interventions\u201d (i.e., evaluating the impact of actively manipulating one variable to observe the effect\non another) and \u201ccounterfactuals\" (i.e., explore hypothetical scenarios that did not occur but help us understand what\nmight have happened under different conditions).\nThrough the years a number of frameworks have been proposed in order to allow machines to reason with causality,\nby modeling cause-effect relations and quantifying the effects of causes through what is called \"causal inference\"."}, {"title": "2.1.1. Causal frameworks", "content": "Causal frameworks are used to learn and estimate causal effects between involved variables of a certain system.\nOne of the first conceptualized framework is the \"potential outcome\" by Neyman [5, 6] and Rubin [7] (also called\nNeyman-Rubin Causal Model). It is intended to provide a language for estimating counterfactuals. It allows estimation\nof the causal effect of a treatment on an individual by comparing what happened (observed outcome) to what would\nhave happened in the absence of that treatment (potential outcome). The formal definitions follow [8, 2, 9].\nA potential outcome is fundamentally distinct from an actual outcome. The first is a hypothetical outcome that,\ndifferently from the second, has not been observed. For instance, in case of a binary treatment T = {0, 1}, if we observe\nan outcome YT=1(u), we cannot observe YT=0(u) at the same time - e.g., if an individual took a medicine (T = 1), we\ncannot observe the case in which (s)he did not take it (T = 0)). This is a potential outcome (also called counterfactual\noutcome), and one retrospectively reasons about \u201cwhat would have happened if\u201d, as opposed to the observed outcome\n(or factual outcome).\nThe PO framework, defined at the level of an individual, aims to estimate a potential outcome and then compute\nthe treatment effect. An example metric is the Individual Treatment Effect (ITE):\n$ITE(u) = Y_{T=1}(\u0438) \u2013 Y_{T=0}(\u0438)$\n(1)\nThe impossibility of simultaneously observing both potential outcomes and consequently observing the effect of\na treatment t on an individual u is the fundamental problem of causal inference [10]. However, statistical solutions\nallow us to replace this impossible problem of observing the causal effect of t on a specific individual with the feasible\nestimation of the average causal effect of t over a population of individuals. At the population level, we define the\ntreated group, including all the treated individuals (T = 1), and the control group (T = 0). For simplicity, we use\na binary treatment; otherwise multiple treated groups are formed. The ITE computed at population level is called\nAverage Treatment Effect (ATE):\n$ATE = E[Y_{T=1} - Y_{T=0}]$\n(2)\nA number of other metrics exist, such as Average Treatment effect on the Treated group (ATT), and Conditional\nAverage Treatment Effect (CATE)."}, {"title": "2.1.2. Causal Inference (CI)", "content": "Given a causal framework, the fundamental objective of inference is to quantify the effect the assignment of a\nvalue t to the treatment variable T has, or would have had, on another variable Y (i.e., P(Y|do(T = t)) ).\nReaching this objective involves several steps. The first step is the identification of the estimand, which consists\nin finding the mathematical formula that generates the answer to a causal query. In other words, an estimand is a\n\"statistical quantity to be estimated from the data that, once estimated, can legitimately represent the answer to the\nquery\" [1]. However, it may not always be possible to identify the estimand from the model: for example imagine a\nnot-measurable variable Z, that has an effect on both X and Y, and with X having an effect on Y too; then the query\nP(Y|do(X)) cannot be easily answered. In this case, model refining or further assumptions (e.g., consider negligible the\neffect of Z) are needed. In the case of answerable queries and graphical causal models, the do-calculus provides a set\nof rules that exploits graphical properties of a causal graph to transform an interventional query into an associational\nestimand. Furthermore, a number of methods can be exploited to control for confounding bias (e.g., the back-door,\nfront-door, mediators, instrumental variables [1, 21, 2]).\nOnce the estimand is identified, a variety of statistical methods can be used to estimate the model's parameters from\nobserved data [8], including propensity-based stratification [22], propensity score matching [23], inverse propensity\nweighting [24], regression discontinuity [25], two-stage least square [26], and generalized linear models [8]. These\nmethods give the estimate for the identified estimand.\nFor our investigation, we consider the classification given by Guo et al. [9] on methods to estimate causal effects\nstarting from observational data\u00b2, shown in Figure 2. Depending on the assumptions of data, CI methods are separated\ninto two categories: observational data with unconfoundness and with no unconfoundness (i.e, without and with\nunobserved confounders).\nMethods without unobserved confounders include three categories of adjustment: regression adjustment, propen-\nsity score-based methods, and covariate balancing. Adjustments eliminate the confounding bias based on a set of\nobserved features. In essence, these methods isolate the true causal effect of the treatment by statistically controlling\nthe influence of confounding variables, assuming that all confounder variables are among the observed ones.\nMethods with unobserved confounders relax the assumption of unconfoundedness, making them more related\nto real-world scenarios where unobserved confounders may play a role. In terms of SCM, the assumption on un-\nconfoundness implies that conditioning on a subset of observed variables blocks all back-door paths. Without this"}, {"title": "2.1.3. Causal Discovery (CD)", "content": "CI assumes the availability of a causal framework describing the cause-effect relationships between variables. It\ncan be built in three main ways: by randomized controlled experiments, manually with domain knowledge, or by\nemploying Causal Discovery (CD) algorithms [29]. The focus of this section is on the latter, that aims at extracting a\ngraphical causal structure from observational data, avoiding controlled experiments, which may be too expensive or\neven technically infeasible [30]. To achieve this, CD algorithms assume that causality can be derived from statistical\ndependencies. These algorithms rely (to a different extent) on various subsets of assumptions, the main ones being\nthe Causal Markov, Faithfulness, and Sufficiency assumptions [31, 8, 32, 29], to derive correspondences between the\n(conditional) independence in the probability distribution and the causal connectivity relationships in the generated\nDAG [31]. The first two conditions are the most important ones and respectively state that: i) every vertex X in the\ngraph G is probabilistically independent of its non-descendants given its parents; ii) if a variable X is independent of\nY given a conditioning set Z in the probability distribution, then X is d-separated from Y given Z in the DAG (in\nother words, the statistical dependence between variables estimated from the data does not violate the independence\ndefined by any causal graph that generates the data [9]). Markov and faithfulness conditions are sufficient to define an\nequivalence structure over directed acyclic graphs, where graphs that are in the same Markov equivalence class have\nthe same (conditional) independence structure. Sufficiency requires that, for a pair of observed variables, all their\ncommon causes must also be observed in the data."}, {"title": "2.2. Software Quality Assurance", "content": "Software quality is the \u201ccapability of a software product to conform to requirements\" [49, 50]. Software Quality\nAssurance (SQA) activities aim to support a justified confidence that the software product conforms to its established\nrequirements [51]. The SQA process refers to \"a set of activities that assess adherence to, and the adequacy of the\nsoftware processes used to develop and modify software products. SQA also determines the degree to which the\ndesired results from software quality control are being obtained\" [50].\nA failure of a system is a deviation from its intended service. It is due to the presence of faults or defects which,\nonce activated, corrupt the system state and may provoke a failure if they reach the interface. Since in the wide\nliterature about software quality some terms have different meanings in different contexts, we hereafter report the\nmeaning of the basic terms as used throughout this paper. We refer to the terminology adopted by the IFIP WG\n10.4 on Dependable Computing and Fault Tolerance \u2013 due to the foundational work on dependability by Avizienis,\nLaprie, et al. [52] - as it better reflects the one adopted in the surveyed papers.\nA Failure is a deviation of the delivered service from correct service.\nAn Error is that part of the system state that may cause a subsequent failure: a failure occurs when an error reaches\nthe service interface, making the service deviate from correct service.\nA Fault is the adjudged or hypothesized cause of an error. An active fault produces an error; a fault not activated\nis said to be dormant. Throughout this paper, with fault we implicitly refer to software faults if not differently\nspecified. In addition, a fault in the code, i.e., a fault introduced by a human mistake at some point in the life\ncycle and that slipped through the code, is also called a software defect [54] or a bug.\nThe SQA process entails activities spanning the whole life cycle, concerning both functional and non-functional\nrequirements. We categorize activities and quality attributes by borrowing again the taxonomy of dependability and\nsecurity by Avizienis, Laprie, et al. [52], extending it with tasks and attributes not related to dependability and security\nas reported in Figure 4.\nAs for the means for assuring quality related to software faults, we refer to the following categories:\nFault avoidance/prevention, to limit the introduction of faults. This is typically achieved by good engineering\npractices, such as requirements engineering, design modularity, encapsulation and information hiding, reuse."}, {"title": "3. Methodology", "content": "This study aims to answer the following research questions (RQ) about causal reasoning in the context of soft-\nware quality assurance.\nWith this RQ, we aim to characterize the activities for which researchers find causal reasoning more useful, and\nwhat quality attribute is targeted. Thus, we investigate what are the quality-related tasks targeted by the use of\ncausal reasoning (inference and/or discovery), what is the phase of the software development life cycle in which\nthe proposed technique, method or tool is used.\nThis RQ aims to characterize how researchers adopted causal reasoning for quality improvement and assess-\nment. We investigate what causal task is performed (i.e., CD and/or CI), the causal framework, the CD and CI\nmethodologies, and the tools used.\nWe finally aim to check the maturity of the proposal; being it a relatively new field of application, we aim to\ncheck how many solutions are experimentally validated and, if any, how many solutions are being experimented\nin industrial environments, and in what domain."}, {"title": "3.2. Search and selection", "content": "To address the above questions, we conducted the research according to the protocol shown in Figure 5. The\nsearch and selection process started from the formulation of the query shown in Listing 1. We executed the query\non four of the most popular digital libraries - namely, Scopus, IEEEXplore, ACM, and Web of Science (step \u2460).\nThen, we merged the results (step) and removed duplicates, obtaining an initial list of 770 papers.\nOn this list of 770 papers, we applied the following inclusion and exclusion criteria.\nIC1: the paper's primary goal is software quality assurance/assessment (namely, activities to assure/assess\nquality related to software faults according to the categories defined in Section 2.2) with the use of causal\nreasoning.\nIC2: the target of the software quality assurance/assessment activity is a software application or a software-\nenabled component (e.g., a machine learning model like a Deep Neural Network) or system where the\npaper's proposal is meant to assure/assess the quality of software, namely against software faults.\nEC1: the paper is a thesis, book, or review.\nEC2: the paper is not subject to peer review, e.g., it is a technical report, a white paper, a project report,\ngovernment document (i.e., grey literature) or it is a preprint.\nEC3: the paper presents a causal inference or discovery method, algorithm, library or tool not used for an\nSQA task.\nEC4: the paper is a secondary or tertiary study.\nEC5: the paper is not written in English.\nTwo researchers were involved in applying the criteria to the entire set of papers, with the support of a senior author\nas arbiter. Since we only have two raters evaluating identical items, a good fit is to use the Cohen's kappa to assess\nthe level of agreement. According to the common interpretation in literature [56], the results (k = 0.778) indicate\nsubstantial agreement. This inclusion/exclusion process allowed us to exclude 693 papers while including 76, listed\nin Table 1. Most of the papers were excluded due to violation of IC1 (a few IC2), as they focus on causal reasoning\ntechniques applied in healthcare and economics, not SQA. A subset of papers were excluded due to EC3 and EC4.\nAdditionally, we compiled a list of papers we knew and believed should be part of the list, creating a reference test\nset. We then compared the selected papers with this reference set, following established guidelines for secondary\nstudies, as recommended by Petersen et al. [57], Kitchenham and Brereton [58]. The reference test set was composed\nby 21 papers. 17/21 papers are selected from a set of 25 papers reviewed in a similar previous survey by Siebert\n[3], specifically, by taking those that match also our inclusion/exclusion criteria (P1, P8, P11, P15-17, P20-21, P24,\nP26-28, P30, P32, P44, P48, P86 - Table 1). To this, we added 4 further papers that we expected to be in the search\n(P31, P33, P34, and P42 - Table 1). 19/21 papers were included in the list of papers selected by our query, ensuring\nrepresentativeness of the selection \u2013 the remaining two papers (i.e., P17 and P86) did not appear in the query results\nsince they do not contain the words included in the last two \"AND\" of the query (i.e., \"software\u201d and \u201ccaus*\") in their\ntitle, abstract, or keywords. We considered these terms pivotal for the scope of the search, and decided to not further\nrefine the query to avoid side effects, e.g., explosion of false positives. Thus, considering the list of 76 papers resulting"}, {"title": "3.3. Data extraction, classification, and analysis", "content": "The data extraction step \u2462 provides key information about each paper, useful for classification. Starting from the\nRQs, we defined an initial RQs scheme (Figure 5). Then, to refine and agree on the scheme, we proceeded iteratively,\nbeginning with a horizontal classification. We divided the list of papers into equal subsets and assigned them to three\nauthors, who independently classified them according to all dimensions of the scheme. With the classified papers\n(i.e., the filled scheme), we validated the classification in plenary meetings. The validation phase was useful to decide\nwhether refining the scheme was necessary (e.g., adding, removing, or modifying dimensions). In such cases, a refined\nscheme was agreed upon in phase 5. This iterative process concluded with the agreement on the final scheme with\n17 dimensions, as reported in Table 2. Finally, to ensure homogeneous classification, the papers were re-classified\nvertically: each author was assigned a subset of sub-dimensions and classified all 86 papers for only the assigned\ndimensions. This favored the detection of inconsistent classifications performed during the horizontal classification\nphase. Dedicated meetings solved such disagreements.\nOn the classified papers, we present in Sections 4-6 the analysis (step \u2465) we performed to answer the RQs. We\nhighlight the main findings, implications, and open challenges derived by the analysis in Section 7."}, {"title": "3.4. Results overview", "content": "As shown in Figure 5, the search in the digital libraries produced a total of 1,086 papers. Specifically, 429\npapers were retrieved from Scopus, 342 from IEEExplore, 45 from ACM DL, and 270 from Web of Science. After\nremoving duplicates, we obtained a list of 770 unique papers, on which we applied the inclusion/exclusion criteria.\nFollowing this step, 78 papers were included, and an additional 8 were added through the snowballing phase, resulting\nin a total of 86 papers for classification. Figure 6 shows the distribution of the 86 included papers by publication venue.\nThe majority of papers (56) are conference papers, followed by journal papers (24), and workshop papers (6)."}, {"title": "4. RQ1: Quality assurance activities and quality attributes", "content": "This question investigates three dimensions of the scheme defined in Table 2: Task, Phase, and Quality attribute.\nConsidering the above-defined quality assurance means (Section 2), we hereafter list the specific tasks we have found\nafter classifying the surveyed papers, categorized by quality assurance means:\nFault avoidance/prevention:\nThreats modeling. An approach used to identify, quantify, and address potential security threats and\nvulnerabilities in a system.\nFault removal:\nTesting and analysis. Complementary activities aimed to expose failures through code execution (that is\ntesting) and to verify properties (e.g., correctness, security) on (a model of) the source code (that is static\nanalysis) or on execution traces (dynamic analysis) [144].\nFault localization. This is the task of locating the cause that originated the failure. It is referred to as\ndiagnosis in Avizienis, Laprie, et al. [52]; its objective can be twofold: i) the identification of the line(s)\nof code containing the fault and of the correction action; in this case, we tag the task as debugging, clearly\na fault removal means; ii) the identification of a faulty unit in a running system (e.g., the failure culprit\nmicroservice in a service-based system), to design fault tolerance means (referred to as fault handling in\nAvizienis, Laprie, et al. [52]) or to support post mortem analysis and repair; in this case, we tag the task as\nroot cause analysis, the common term used in these works to spot the failure-causing system components.\nTherefore, this fault localization category is also repeated under the fault tolerance means.\nFault tolerance:\nFault localization, for root cause analysis. The process of identifying the specific location or component\nwithin a system where a defect or malfunction has occurred.\nAnomaly detection. The dependability classification of [52] considers error detection in the fault toler-\nance means; anomaly detection is here referred to the attempt of performing error detection.\n(Fault) forecasting:\nFault prediction, also referred to as defect/bug prediction. The process using statistical and machine\nlearning techniques to predict the location and occurrence of defects in software."}, {"title": "4.2. Results", "content": "Figure 8 shows the percentage of papers using CR per task, namely QA activity (a paper can be related to more\ntasks). The detail about which paper falls in which category are reported in the online material. The Figure shows\nthat most of the papers use CR for fault localization, either for debugging or for root cause analysis. This is probably\ndue to how CR is perceived, namely as a method to explain the reasons for an event, and, as such, is particularly prone\nto represent the causal chain that leads from a fault to a failure. The employment of CR for other tasks is a more recent\napplication. More details are in the following subsections.\nAn activity can be performed in different phases of the software life cycle. Figure 9 depicts the main phases in\nwhich CR solutions are adopted, allowing researchers from different software engineering areas to get if and how\ncausal reasoning can be useful for their field. As result of our classification, we have found papers in the following\nphases: Requirements, Coding, V&V, Operation, Evolution & Maintenance.\nFigure 10 shows the count by life cycle phases. The most considered phases for CR application are V&V (39\npapers) and Evolution & Maintenance (39). This is in line with the results on the activity, as i) fault localization\nhappens in the V&V phase and during maintenance; ii) several other activities are carried out in the post-coding\nphases, such as testing, anomaly detection, KPI prediction based on field data, which can rely on the information"}, {"title": "4.2.1. CR for retrospective analysis", "content": "The intrinsic nature of CR makes it suitable to perform tasks aiming to trace causal relationships among software\nsystem components to find the \u201creasons\u201d behind certain undesired behaviors. In the following, we discuss the main\ntasks performed when something wrong happens in the software systems.\nFault localization & Anomaly detection. For fault localization, we mean all the techniques, methods, and tools to lo-\ncalize issues (namely, software faults or failures' root causes) causing undesired behaviors of a target software system.\nAs mentioned, fault localization is at the base of debugging and of root cause analysis processes. Anomaly detection\naims to highlight working conditions potentially yielding to failures or to degradation of the observed software system.\nAs shown in Figure 8, these two categories together cover more than 50% of the papers and are spread across all\nthe phases of the life cycle successive to the coding phase.\nThe possibility to model the software system structure and/or development process by casual models, such as to\nmodel program dependencies (P24) or the connection between dataflow architectures and causal graphs in Flow-based\nprogramming (P19), has enabled a plethora of CR solutions for the localization of causes of unexpected behavior.\nHigh effectiveness has been achieved in studies where the causal model is extracted from existing (white-box)\nknowledge, like for delta debugging of Software Defined Networks (P6, P12) or for faulty services localization (P33).\nAlthough fault localization is mainly related to finding reliability issues, CR is also used to detect causes of\nperformance (P4), availability (P5), security (P5, P51), and safety (P18, P51) issues.\nWe classify fault localization in two categories: debugging and Root Cause Analysis (RCA). The count by these\ntwo categories is highlighted in Figure 8. Many fault localization techniques (in 24 papers) are aimed at code de-\nbugging actions (P24, P47). The main applications are Automatic Program Repair (P40), also concerning Neural\nNetworks repair (P51), delta debugging (P6, P7, P12), evolution by using casual difference graph (P81), and Statis-\ntical Fault Localization (SFL) (P8, P11, P15, P16, P17, P21, P26, P27, P28, P30, P48, P63). Just one of the paper"}, {"title": "4.2.2. CR for forecasting", "content": "In the previous paragraphs, CR was considered for its ability to understand the causes of undesired behaviors of\nthe software system, as well as for activities supporting fault removal such as testing. In this case, we consider CR\nto forecast the behaviors of software systems before they are deployed in the operational environment. Forecasting is\nmainly related to software maintenance and evolution activities.\nTesting and analysis. The ability of CR to determine the causal relationships between variables makes it particularly\nsuited to run testing and analysis activities during the V&V phase of software systems life cycle. Exploiting causal\nrelations can help execute tests more prone to spot defects of the system under test, with reference to both traditional\nsoftware systems (P78) and learning-enabled ones (P42, P54, P65), also Autonomous Driving Systems (P52, P53,\nP62, P64). This aspect has been underlined by a big company like Netflix, which included CR in their science-centric\nexperimentation platform (P9).\nThe main task CR is used for is test generation. It has been adopted to generate mutants and metamorphic relations\nfor mutation (P1) and metamorphic (P33) testing. Authors in P34 envision a framework for test generation, in which\nthe causal model is used as a surrogate to search the input space efficiently, with the aim of maximizing a given testing"}, {"title": "5. RQ2: How is causal reasoning used?", "content": "This question explores the dimensions outlined in the scheme defined in Table 2, covering the CR task (inference\nand/or discovery), the causal framework employed, the CD methodology (how causal relationships are defined), the\nCI methodology (how authors estimate causal effects and the metrics used), and the tools/libraries (which tools are\nutilized for CD and/or CI)."}, {"title": "5.2. Results", "content": "Figure 12 illustrates the distribution of papers performing CD, CI, or a combination of both. The majority of papers\n(55.8% - 48 out of 86 papers) focus solely on inference mechanisms on a causal model, be it manually constructed\nor obtained from existing knowledge. Among these papers, 4 (i.e., P23, P44, P52, P53) involve manual model\nconstruction by domain experts, while the remaining derive the causal structure from a variety of sources (e.g., control\nflow graph, service dependency graph, message tracing, design artifacts).\nA subset of papers (26.7% - 23 out of 86 papers) employ CD algorithms to derive a causal structure but do\nnot apply traditional CI methodologies. For root cause analysis, papers P25, P35, P38, P46, P50, and P69 utilize"}, {"title": "5.2.2. Causal framework", "content": "We identified three primary causal frameworks used in the literature (see Section 2): PO, SCM, and CausalDAG.\nTo classify the papers, we rely on what the authors explicitly state to use in their study, or, if not stated, on our\nunderstanding of the proposal.\nWhile most papers refer to the three primary models, we identified a few papers that employ alternative causal\nmodels. These include Fuzzy Cognitive Maps, Evidential Networks, Difference-in-Differences, and Causal Trees. We\nhave grouped these models into an \"Other\" category.\nFigure 14 shows the number of papers by models. It is important to note that when a paper employs multiple\nmodels, each one is counted separately, leading to a cumulative count that may exceed the total number of selected\npapers. The only exception is in the case of SCM and CausalDAG (since an SCM necessarily uses a CausalDAG): the\nCausalDAG category refers to the papers using only a CausalDAG but not an SCM."}, {"title": "5.2.3. CD methodology", "content": "In this section, we focus on the CD algorithms used to automatically building a causal structure from observa-\ntional data. As shown in Figure 12, 44.1% of the collected papers (38 papers) utilize CD algorithms. Figure 16\nshows the number of papers by CD algorithm. Some papers (e.g., P35, P36, and P64) use more than one algorithm:\neach is counted independently. Additionally, we consider extensions of PC and LiNGAM (e.g., PC-kernel and direc-t/mixedLiNGAM, respectively) as one.\n26.5% of papers opt for constraint-based CD algorithms. For instance, P18, P20, P34 use FCI, while P25, P31,\nP35, P36, P50, P57, and P83 use PC, P64 use both in addition to RFCI. Score-based CD algorithms such as GFCI\n(P3, P64, and P79), GES/FGES (P35 and P64), BLIP (P45), and DiBS (P54) are common choices too (16.3%). The\nauthors' rationale for choosing FCI (and GFCI) lies in the ability to discover latent confounders based on conditional\ndependence relationships between measured variables, as well as in the compatibility with various data types. P50"}, {"title": "5.2.4. CI methodology", "content": "Out of 63 papers that perform CI, 39 explicitly give information about the inference methodology. The remaining\neither do not give details or perform other forms of inference (i.e., not associated with any in the classification scheme);\namong them, P3 uses expectation-maximization [145] to build a conditional probability table, P7 performs differential\nchecking between buggy and correct programs, P10 and P29 compute the ATE with the Difference-in-Difference\n(DiD) model, P19 propose a novel algorithm that use Shapley values to compute the contribution estimate of a change,\nP53 use structural equation modeling and path analysis for latent hazard notification for automated driving systems,\nand P61 perform actual causality analysis with the HP2SAT tool proposed by Ibrahim et al. [146].\nFigure 18 shows the number of papers per methodology. The most used methodology is the regression adjustment,\nwith 27 out of 39 papers. Ensemble, simulation, and neural networks methods see four papers each: the first method\ncomprehend P14 using causal trees, P21 and P26 random forest, and P44 causal forest; in the second method, P34,"}, {"title": "5.2.5. Tools/libraries", "content": "Figure 20 shows the number of papers explicitly mentioning the usage of third party tools. It can be seen that\nmany papers, especially before 2021, implemented their code to perform causal tasks. However, starting from 2021,\nan increasing number of papers used available third party tools and libraries, which we collected and reported. Table\n3 shows the tools and libraries we found, together with their implementation language, papers using them, and their\nreference. Notably, the most used ones are causal-learn and dowhy for CD and CI, respectively. causal-learn\nis a Python wrapper of Tetrad (second most"}]}