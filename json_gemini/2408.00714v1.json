{"title": "SAM 2: Segment Anything in Images and Videos", "authors": ["Nikhila Ravi", "Valentin Gabeur", "Yuan-Ting Hu", "Ronghang Hu", "Chaitanya Ryali", "Tengyu Ma", "Haitham Khedr", "Roman R\u00e4dle", "Chloe Rolland", "Laura Gustafson", "Eric Mintun", "Junting Pan", "Kalyan Vasudev Alwala", "Nicolas Carion", "Chao-Yuan Wu", "Ross Girshick", "Piotr Doll\u00e1r", "Christoph Feichtenhofer"], "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3\u00d7 fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6\u00d7 faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.", "sections": [{"title": "1 Introduction", "content": "Segment Anything (SA) introduced a foundation model for promptable segmentation in images (Kirillov et al., 2023). However an image is only a static snapshot of the real world in which visual segments can exhibit complex motion, and with the rapid growth of multimedia content, a significant portion is now recorded with a temporal dimension, particularly in video data. Many important applications in AR/VR, robotics, autonomous vehicles, and video editing require temporal localization beyond image-level segmentation. We believe a universal visual segmentation system should be applicable to both images and videos.\nSegmentation in video aims to determine the spatio-temporal extent of entities, which presents unique challenges beyond those in images. Entities can undergo significant changes in appearance due to motion, deformation, occlusion, lighting changes, and other factors. Videos often have lower quality than images due to camera motion, blur, and lower resolution. Further, efficient processing of a large number of frames is a key challenge. While SA successfully addresses segmentation in images, existing video segmentation models and datasets fall short in providing a comparable capability to \"segment anything in videos\".\nWe introduce the Segment Anything Model 2 (SAM 2), a unified model for video and image segmentation (we consider an image as a single-frame video). Our work includes a task, model, and dataset (see Fig. 1).\nWe focus on the Promptable Visual Segmentation (PVS) task that generalizes image segmentation to the video domain. The task takes as input points, boxes, or masks on any frame of the video to define a segment of interest for which the spatio-temporal mask (i.e., a \u2018masklet') is to be predicted. Once a masklet is predicted, it can be iteratively refined by providing prompts in additional frames.\nOur model (\u00a74) produces segmentation masks of the object of interest, in single images and across video frames. SAM 2 is equipped with a memory that stores information about the object and previous interactions, which allows it to generate masklet predictions throughout the video, and also effectively correct these based on the stored memory context of the object from previously observed frames. Our streaming architecture is a natural generalization of SAM to the video domain, processing video frames one at a time, equipped with a memory attention module to attend to the previous memories of the target object. When applied to images, the memory is empty and the model behaves like SAM."}, {"title": "2 Related work", "content": "Image segmentation. Segment Anything (Kirillov et al., 2023) introduces a promptable image segmentation task where the goal is to output a valid segmentation mask given an input prompt such as a bounding box or a point that refers to the object of interest. SAM trained on the SA-1B dataset allows for zero-shot segmentation with flexible prompting, which enabled its adoption to a wide range of downstream applications. Recent work has extended SAM by improving its quality. For example, HQ-SAM (Ke et al., 2024) enhances SAM by introducing a High-Quality output token and training the model on fine-grained masks. Another line of work focuses on SAM's efficiency to enable wider use in real-world and mobile applications, such as EfficientSAM (Xiong et al., 2023), MobileSAM (Zhang et al., 2023a), and FastSAM (Zhao et al., 2023). The success of SAM led to its adoption in a wide range of applications, such as medical imaging (Ma et al., 2024;\nDeng et al., 2023; Mazurowski et al., 2023; Wu et al., 2023a), remote sensing (Chen et al., 2024; Ren et al., 2024), motion segmentation (Xie et al., 2024), and camouflaged object detection (Tang et al., 2023).\nInteractive Video Object Segmentation (iVOS). Interactive video object segmentation has emerged as a crucial task to efficiently obtain object segmentations in videos (masklets) with user guidance, often in the form of scribbles, clicks, or bounding boxes. A few early approaches (Wang et al., 2005; Bai & Sapiro, 2007; Fan et al., 2015) deploy graph-based optimization to guide the segmentation annotation process. More recent approaches (Heo et al., 2020; Cheng et al., 2021b; Delatolas et al., 2024) often adopt a modular design, converting the user inputs into a mask representation on a single frame and then propagating it to other frames. Our work shares a similar goal to these works to segment objects across videos with a good interactive experience, and we build a strong model along with a large and diverse dataset in pursuit of this goal.\nIn particular, the DAVIS interactive benchmark (Caelles et al., 2018) allows interactively segmenting an object via scribble inputs on multiple frames. Inspired by the DAVIS interactive benchmark, we also adopt an interactive evaluation setting for the promptable video segmentation task in \u00a76.1.\nClick-based input is easier to collect (Homayounfar et al., 2021) for interactive video segmentation. Recent works have used a combination of SAM on images with video trackers based on masks (Cheng et al., 2023b;\nYang et al., 2023; Cheng et al., 2023c) or points (Raji\u010d et al., 2023). However, these approaches have limitations: the tracker may not work for all objects, SAM may not perform well for image frames from videos, and there is no mechanism to interactively refine a model's mistakes, other than re-annotating using SAM from scratch on the erroneous frame and restarting the tracking from there.\nSemi-supervised Video Object Segmentation (VOS). Semi-supervised VOS usually begins with an object mask as input in the first frame, which must be accurately tracked throughout the video (Pont-Tuset et al., 2017). It is called \"semi-supervised\" since the input mask can be seen as a supervision signal of the object appearance that is available only for the first frame. This task has drawn significant attention due to its relevance in various applications, including video editing, robotics, and automatic background removal.\nEarly neural network-based approaches have often used online fine-tuning on the first video frame (Caelles et al., 2016; Perazzi et al., 2016; Yoon et al., 2017; Maninis et al., 2017; Hu et al., 2018a; Bhat et al., 2020;\nRobinson et al., 2020) or on all frames (Voigtlaender & Leibe, 2017) to adapt the model to the target object. Faster inference has been achieved with offline-trained models, conditioned either only on the first frame (Hu et al., 2018b; Chen et al., 2018), or also integrating the previous frame (Oh et al., 2018; Yang et al., 2018, 2020). This multi-conditioning has been extended to all frames with RNNs (Xu et al., 2018a) and cross-attention (Oh et al., 2019; Cheng et al., 2021a; Li et al., 2022a; Yang et al., 2021b, 2024; Cheng & Schwing, 2022; Yang\n& Yang, 2022; Wang et al., 2022; Cheng et al., 2023a; Goyal et al., 2023). Recent approaches (Zhang et al., 2023b; Wu et al., 2023b) extend a single vision transformer to jointly process the current frame along with all previous frames and associated predictions, resulting in a simple architecture but at a prohibitive inference cost. Semi-supervised VOS can be seen as a special case of our Promptable Visual Segmentation (PVS) task, as it is equivalent to only providing a mask prompt in the first video frame. Nevertheless, annotating the required high-quality object mask in the first frame is practically challenging and time-consuming.\nVideo segmentation datasets. Many datasets have been proposed to support the VOS task. Early VOS datasets (Prest et al., 2012; Li et al., 2013; Ochs et al., 2014; Fan et al., 2015), such as DAVIS (Pont-Tuset\net al., 2017; Caelles et al., 2019), include high-quality annotations but their limited size does not allow training deep-learning based approaches. Covering 94 object categories over 4 thousand videos, YouTube-VOS (Xu et al., 2018b) is the first large-scale dataset for the VOS task. As algorithms became better and benchmark performance started to saturate, researchers have looked at increasing the difficulty of the VOS task by specifically focusing on occlusions (Qi et al., 2022; Ding et al., 2023), long videos (Hong et al., 2023, 2024), extreme transformations (Tokmakov et al., 2022), object diversity (Wang et al., 2021b, 2023) or scene diversity (Athar et al., 2022).\nWe find that current video segmentation datasets lack sufficient coverage to achieve the capability of \"segmenting anything in videos\". Their annotations typically cover entire objects (not parts) and datasets are often centered around specific object classes, such as people, vehicles, and animals. In comparison to these datasets, our released SA-V dataset not only focuses on whole objects but also extensively covers object parts and contains over an order of magnitude more masks."}, {"title": "3 Task: promptable visual segmentation", "content": "The PVS task allows providing prompts to the model on any frame of a video. Prompts can be positive/negative clicks, bounding boxes, or masks, either to define an object to segment or to refine a model-predicted one. To provide an interactive experience, upon receiving a prompt on a specific frame, the model should immediately respond with a valid segmentation mask of the object on this frame. After receiving initial (one or multiple) prompts (either on the same frame or different frames), the model should propagate these prompts to obtain the masklet of the object across the entire video, which contains the segmentation mask of the target object on every video frame. Additional prompts can be provided to the model on any frame to refine the segment throughout the video (example in Fig. 2). For details on the task, see \u00a7A.\nSAM 2, introduced in the next section (\u00a74), is applied as a data collection tool to the PVS task for building our SA-V dataset (\u00a75). The model is evaluated (\u00a76) in an online and offline setting by simulating interactive video segmentation scenarios involving annotations across multiple frames, in the conventional semi-supervised VOS setting where annotations are limited to the first frame, and for image segmentation on the SA benchmarks."}, {"title": "4 Model", "content": "Our model can be seen as a generalization of SAM to the video (and image) domain. SAM 2 (Fig. 3) supports point, box, and mask prompts on individual frames to define the spatial extent of the object to be segmented across the video. For image input, the model behaves similarly to SAM. A promptable and light-weight mask decoder accepts a frame embedding and prompts (if any) on the current frame and outputs a segmentation mask for the frame. Prompts can be iteratively added on a frame in order to refine the masks.\nUnlike SAM, the frame embedding used by the SAM 2 decoder is not directly from an image encoder and is instead conditioned on memories of past predictions and prompted frames. It is possible for prompted frames to also come \"from the future\" relative to the current frame. Memories of frames are created by the memory encoder based on the current prediction and placed in a memory bank for use in subsequent frames. The memory attention operation takes the per-frame embedding from the image encoder and conditions it on the memory bank to produce an embedding that is then passed to the mask decoder.\nWe describe individual components and training below and provide more details in Appendix C."}, {"title": "5 Data", "content": "To develop the capability to \"segment anything\" in video, we built a data engine to collect a large and diverse video segmentation dataset. We employ an interactive model in the loop setup with human annotators. Similar to Kirillov et al. (2023), we do not impose semantic constraints on the annotated masklets, and focus on both whole objects (e.g., a person) and parts (e.g., a person's hat). Our data engine went through three phases, each categorized based on the level of model assistance provided to annotators. Next, we describe each data engine phase and our SA-V dataset.\n5.1 Data engine\nPhase 1: SAM per frame. The initial phase used the image-based interactive SAM (Kirillov et al., 2023) to assist human annotation. Annotators are tasked with annotating the mask of a target object in every frame of the video at 6 frames per second (FPS) using SAM, and pixel-precise manual editing tools such as a \u201cbrush\u201d and \"eraser\". There is no tracking model involved to assist with the temporal propagation of masks to other frames. As this is a per-frame method, and all frames require mask annotation from scratch, the process is slow, with an average annotation time of 37.8 seconds per frame in our experiment. However, this yields high-quality spatial annotations per frame. In this phase, we collected 16K masklets across 1.4K videos. We further use this approach to annotate our SA-V val and test sets to mitigate potential biases of SAM 2 during evaluation.\nPhase 2: SAM + SAM 2 Mask. The second phase added SAM 2 into the loop, where SAM 2 only accepted masks as prompts. We refer to this version as SAM 2 Mask. Annotators used SAM and other tools as in Phase 1 to generate spatial masks in the first frame, and then use SAM 2 Mask to temporally propagate the annotated mask to other frames to get the full spatio-temporal masklets. At any subsequent video frame, annotators can spatially modify the predictions made by SAM 2 Mask by annotating a mask from scratch with SAM, a \u201cbrush\u201d and/or \"eraser\", and re-propagate with SAM 2 Mask, repeating this process until the masklet is correct. SAM 2 Mask was initially trained on the Phase 1 data and publicly available datasets. During Phase 2, we re-trained and updated SAM 2 Mask in the annotation loop twice using the collected data. In Phase 2, we collected 63.5K masklets. The annotation time went down to 7.4 s/frame, a ~5.1x speed up over Phase 1.\nDespite an improvement in annotation time, this decoupled approach requires annotating masks in intermediate frames from scratch, without previous memory. We then advanced to develop the fully-featured SAM 2, capable of performing both interactive image segmentation and mask propagation in a unified model.\nPhase 3: SAM 2. In the final phase, we utilize the fully-featured SAM 2, which accepts various types of prompts, including points and masks. SAM 2 benefits from memories of objects across the temporal dimension to generate mask predictions. This means annotators only need to provide occasional refinement clicks to SAM 2 to edit the predicted masklets in intermediate frames, as opposed to annotating from scratch with a spatial SAM which has no such memory context. During Phase 3, we re-trained and updated SAM 2 using the collected annotations five times. With SAM 2 in the loop, the annotation time per frame went down to 4.5 seconds, a ~8.4x speed up over Phase 1."}, {"title": "6 Zero-shot experiments", "content": "Here, we compare SAM 2 with previous work on zero-shot video tasks (\u00a76.1) and image tasks (\u00a76.2). We report the standard J&F metric (Pont-Tuset et al., 2017) for video and mIoU metric for image tasks. Unless otherwise mentioned, the results reported in this section follow our default setup using Hiera-B+ image encoder with a resolution of 1024 and trained on the full combination of datasets, i.e., SAM 2 (Hiera-B+) in Table 7 (see also \u00a7C.2 for details).\n6.1 Video tasks\n6.1.1 Promptable video segmentation\nWe first evaluate promptable video segmentation, which involves simulating an interactive setting that resembles the user experience. We have two settings, offline evaluation, where multiple passes are made through a video to select frames to interact with based on the largest model error, and online evaluation, where the frames are annotated in a single forward pass through the video. These evaluations are conducted on 9 densely annotated zero-shot video datasets using Nclick = 3 clicks per frame (see \u00a7E.1 for details).\nWe create two strong baselines, SAM+XMem++ and SAM+Cutie, based on two state-of-the-art models for video object segmentation, XMem++ (Bekuzarov et al., 2023) and Cutie (Cheng et al., 2023a). We use XMem++ to generate a video segmentation based on mask inputs on one or multiple frames. SAM is used to provide an initial mask or to refine an output (by feeding the current segmentation as a mask prompt to SAM). For the SAM+Cutie baseline, we modify Cutie to allow taking mask inputs on multiple frames.\nIn Fig. 6, we report the average J&F accuracy over Nframe = 1,..., 8 interacted frames. SAM 2 outperforms SAM+XMem++ and SAM+Cutie for both offline and online evaluation settings. Across all 9 datasets (see per-dataset results in \u00a7E.1), SAM 2 dominates both methods, confirming that SAM 2 is able to generate high-quality video segmentation from a few clicks while also allowing continued refinement of the results with further prompts. Overall, SAM 2 can generate better segmentation accuracy, with >3\u00d7 fewer interactions.\n6.1.2 Semi-supervised video object segmentation\nWe next evaluate the semi-supervised video object segmentation (VOS) setting (Pont-Tuset et al., 2017) with click, box, or mask prompts only on the first frame of the video. When using click prompts, we interactively sample either 1, 3 or 5 clicks on the first video frame, and then track the object based on these clicks.\nSimilar to the interactive setting in \u00a76.1.1, we compare to XMem++ and Cutie, using SAM for click and box prompts, and in their default setting when using mask prompts. We report the standard J&F accuracy (Pont-Tuset et al., 2017), except for on VOST (Tokmakov et al., 2022), where we report the I metric following its protocol. The results are in Table 4. SAM 2 outperforms both baselines on the 17 datasets, using various input prompts. The results underline that SAM 2 also excels at the conventional, non-interactive VOS task with mask input, for which these other works are specifically designed. More details are in \u00a7E.1.3.\n6.1.3 Fairness evaluation\nWe evaluate SAM 2 for fairness across demographic groups. We collect annotations for the people category in the Ego-Exo4D (Grauman et al., 2023) dataset, which contains self-reported demographic information supplied by the subject of the video. We employ the same annotation setup as for SA-V val and test sets and apply this to 20-second clips from the third-person (exo) videos. We evaluate SAM 2 on this data using 1-, 3-clicks, and ground-truth mask on the first frame.\nTable 5 shows the comparison in J&F accuracy of SAM 2 for segmenting people across gender and age. At 3 clicks and with ground-truth mask prompts there is minimal discrepancy. We manually inspect 1 click predictions, and find the model frequently predicts the mask for a part instead of the person. When limiting the comparison to clips where the person is correctly segmented, the gap in 1 click shrinks substantially (J&F male 94.3, female 92.7), suggesting the discrepancy can be partially attributed to ambiguity in the prompt.\nIn Appendix G, we provide model, data and annotation cards for SA-V.\n6.2 Image tasks\nWe evaluate SAM 2 on the Segment Anything task across 37 zero-shot datasets, including 23 datasets previously used by SAM for evaluation. 1-click and 5-click mIoUs are reported in Table 6 and we show the average mIoU by dataset domain and model speed in frames per second (FPS) on a single A100 GPU.\nThe first column (SA-23 All) shows accuracy on the 23 datasets from SAM. SAM 2 achieves higher accuracy (58.9 mIoU with 1 click) than SAM (58.1 mIoU with 1 click), without using any extra data and while being 6x faster. This can be mainly attributed to the smaller but more effective Hiera image encoder in SAM 2.\nThe bottom row shows how training on our SA-1B and video data mix can further improve accuracy to 61.4% on average on the 23 datasets. We also see exceptional gains on the video benchmarks from SA-23 (video datasets are evaluated as images, identical to Kirillov et al. (2023)), and the 14 new video datasets we added.\nOverall, the findings underscore SAM 2's dual capability in interactive video and image segmentation, a strength derived from our diverse training data that encompasses videos and static images across visual domains. More detailed results including a breakdown by dataset are in \u00a7E.3.\n7 Comparison to state-of-the-art in semi-supervised VOS\nOur primary focus is on the general, interactive PVS task, but we also address the specific semi-supervised VOS setting (where the prompt is a ground-truth mask on the first frame), as it is a historically common protocol. We evaluate two versions of SAM 2 with varying image encoder sizes (Hiera-B+/-L) with different speed-vs-accuracy tradeoffs. We measure framesper second (FPS) on a single A100 GPU using a batch-size of one. SAM 2 based on Hiera-B+ and Hiera-L runs at real-time speeds of 43.8 and 30.2 FPS, respectively.\nWe present a comparison with existing state-of-the-art in Table 7, reporting accuracy using standard protocols. SAM 2 shows significant improvement over the best existing methods. We observe that using a larger image encoder brings significant accuracy gains across the board."}, {"title": "8 Data and model ablations", "content": "This section presents ablations that informed the design decisions for SAM 2. We evaluate on our MOSE development set (\"MOSE dev\") which contains 200 randomly-sampled videos from the MOSE training split and excluded from the training data in our ablations, SA-V val, and the average over 9 zero-shot video datasets. As the metric for comparison, we report J&F under 3-click input on the first frame as a balance between the 1-click regime and the VOS-style mask prompts. Additionally, we report the average 1-click mIoU on the 23-dataset benchmark used by SAM for the SA task on images. Unless otherwise specified, we run our ablations at 512 resolution and with SA-V manual and a 10% subset of SA-1B. Additional details are in \u00a7C.2.\n8.1 Data ablations\nData mix ablation. In Table 8, we compare the accuracy of SAM-2 when trained on different data mixtures. We pre-train on SA-1B and then train a separate model for each setting. We fix the number of iterations (200k) and batch size (128) with only the training data changing between experiments. We report accuracy on our SA-V val set, MOSE, 9 zero-shot video benchmarks, and the SA-23 tasks (\u00a76.2). Row 1 shows that a model purely trained on VOS datasets (Davis, MOSE, YouTubeVOS) performs well on the in-domain MOSE dev, but poorly on all the others including the 9 zero-shot VOS datasets (59.7 J&F).\nWe observe tremendous benefit from adding our data engine data into the training mix, including +12.1% average performance improvement on 9 zero-shot datasets (row 11 vs 1). This can be attributed to the limited coverage and size of VOS datasets. Adding SA-1B images improves the performance on the image segmentation task (rows 3 vs 4, 5 vs 6, 9 vs 10, 11 vs 12) without degrading the VOS capability. Training only on SA-V and SA-1B (row 4) is enough to obtain strong performance on all benchmarks except for MOSE. Overall, we obtain the best results when mixing all datasets: VOS, SA-1B, and our data engine data (row 12).\nData quantity ablation. Next, we study the effect of scaling training data. SAM 2 is pre-trained on SA-1B before training on varying sizes of SA-V. We report average J&F score (when prompted with 3 clicks in the"}, {"title": "9 Conclusion", "content": "We present a natural evolution of Segment Anything into the video domain, based on three key aspects: (i) extending the promptable segmentation task to video, (ii) equipping the SAM architecture to use memory when applied to video, and (iii) the diverse SA-V dataset for training and benchmarking video segmentation. We believe SAM 2 marks a significant advancement in visual perception, positioning our contributions as milestones that will propel further research and applications in the field."}, {"title": "10 Acknowledgements", "content": "We thank Alexander Kirillov and Jitendra Malik for discussions on project direction. Thanks to Andrew Huang, Sahir Gomez, Miguel Martin, Devansh Kukreja, and Somya Jain for work on the demo, and to Aohan Lin and Meng Wang for creating the dataset visualizer. We thank Shoubhik Debnath and Sagar Vaze for their work on dataset preparation. Thanks also to William Ngan and Sasha Mitts for their design expertise and to Grant Gardner and George Orlin for leading product management. We are grateful to Joelle Pineau, Daniel Bolya, Kate Saenko, Pengchuan Zhang, and Christopher Chedeau, for valuable discussions. Thanks to Rene Martinez Doehner and Baishan Guo for data support, and to our annotation engineering and management partners: Robert Kuo, Rishi Godugu, Bob Kamma, Ida Cheng, Claudette Ward, Kai Brown, Jake Kinney, Jenny Truong, and Karen Bergan. Thanks to Vispi Cassod, Parth Malani, Shiva Koduvayur, Alexander Miller, and Caleb Ho for their support with compute and infra. Finally, we thank Azita Shokrpour, Mallika Malhotra, Rodrick Shepard, Jonathan Torres, Luc Dahlin, David Soofian, Alex Bosenberg, and Amanda Kallet for project-level support."}, {"title": "Appendix", "content": "Table of contents:\n\u2022 \u00a7A: Task Details\n\u2022 \u00a7B: Limitations\n\u2022 \u00a7C: Model Details\n\u2022 \u00a7D: Dataset Details\n\u2022 \u00a7E: Zero-shot Experiments Details\n\u2022 \u00a7G: Dataset, Annotation, and Model Cards\n\u2022 \u00a7D.2.1: Annotation Guidelines"}, {"title": "A Details on the PVS Task", "content": "The Promptable Visual Segmentation (PVS) task can be seen as an extension of the Segment Anything (SA) task from static images to videos. In the PVS setting, given an input video, the model can be interactively prompted with different types of inputs (including clicks, boxes, or masks) on any frame in the video, with the goal of segmenting (and tracking) a valid object throughout the video. When interacting with a video, the model provides an instant response on the frame being prompted (similar to the interactive experience of SAM on images), and also returns the segmentation of the object throughout the entire video in near real-time. Similar to SAM the focus is on valid objects which have a clearly defined boundary, and we do not consider regions without visual boundaries (e.g. Bekuzarov et al. (2023)). Fig. 8 illustrates the task.\nPVS is related to several tasks in both the static image and video domains. On images, the SA task can be considered a subset of PVS with the video reduced to a single frame. Similarly, traditional semi-supervised and interactive VOS (Pont-Tuset et al., 2017) tasks are special cases of PVS, limited to mask prompts provided only on the first frame and scribbles on multiple frames to segment objects throughout a video, respectively. In PVS, prompts can either be clicks, masks, or boxes, and the focus is on enhancing the interactive experience, enabling easy refinement of an object's segmentation with minimal interaction."}, {"title": "B Limitations", "content": "SAM 2 demonstrates strong performance in both static image and video domains, yet it encounters difficulties in certain scenarios. The model may fail to segment objects across shot changes and can lose track of or confuse objects in crowded scenes, after long occlusions or in extended videos. To alleviate this issue, we"}, {"title": "C SAM 2 details", "content": "C.1 Architecture\nHere we discuss further architecture details, expanding on the model description in \u00a74.\nImage encoder. We use a feature pyramid network (Lin et al., 2017) to fuse the stride 16 and 32 features from Stages 3 and 4 of the Hiera image encoder respectively to produce the image embeddings for each frame. In addition, the stride 4 and 8 features from Stages 1 and 2 are not used in the memory attention but are added to the upsampling layers in the mask decoder as shown in Figure 9, which helps produce high-resolution segmentation details. We follow Bolya et al. (2023) in using windowed absolute positional embeddings in the Hiera image encoder. In Bolya et al. (2023), RPB provided positional information spanning across windows in the image encoder, in lieu of which we adopt a simpler approach of interpolating the global positional embedding instead to span across windows. We do not use any relative positional encoding. We train models with varying image encoder sizes \u2013 T, S, B+ and L. We follow Li et al. (2022b) and use global attention in only a subset of the image encoder layers (see Table 13).\nMemory attention. In addition to sinusoidal absolute positional embeddings, we use 2d spatial Rotary Positional Embedding (ROPE) (Su et al., 2021; Heo et al., 2024) in self-attention and cross-attention layers. The object pointer tokens are excluded from ROPE as they do not have specific spatial correspondence. By default, the memory attention uses L = 4 layers.\nPrompt encoder and mask decoder. The prompt encoder design follows SAM, and we next discuss additional details on design changes in the mask decoder. We use the mask token corresponding to the output mask as the object pointer token for the frame, which is placed in the memory bank. As discussed in \u00a74, we also introduce an occlusion prediction head. This is accomplished by including an additional token along with the mask and IoU output tokens. An additional MLP head is applied to this new token to produce a score indicating the likelihood of the object of interest being visible in the current frame (as shown in Figure 9)."}, {"title": "C.2 Training", "content": "C.2.1 Pre-training\nWe first pre-train SAM 2 on static images on the SA-1B dataset (Kirillov et al., 2023). Table 13a details the settings used during pre-training on SA-1B - other settings not mentioned here follow Kirillov et al. (2023). The image encoder is initialized from MAE pre-trained Hiera (Ryali et al., 2023). Similar to SAM, we filter masks covering more than 90% of the image and restricted training to 64 randomly sampled masks per image.\nUnlike SAM, we found it beneficial to use an $l_1$ loss to more aggressively supervise the IoU predictions and to apply a sigmoid activation to the IoU logits to restrict the output into the range between 0 and 1. For multi-mask predictions (on the first click), we supervise the IoU predictions of all masks to encourage better learning of when a mask might be bad, but only supervise the mask logits with the lowest segmentation loss (linear combination of focal and dice loss). In SAM, during iterative sampling of points, two iterations were inserted with no additional prompts (only feeding the previous mask logits) - we do not add such iterations during our training and use 7 correction clicks (instead of 8 in SAM). We also employ horizontal flip augmentation during training and resize the image to a square size of 1024x1024.\nWe use AdamW (Loshchilov & Hutter, 2019) and apply layer decay (Clark et al., 2020) on the image encoder and follow a reciprocal square-root schedule (Zhai et al., 2022). See Table 13 (a) for the hyperparameters in our pre-training stage.\nC.2.2 Full training\nAfter pre-training, we train SAM 2 on our introduced datasets SA-V + Internal (section \u00a75.2), a 10% subset of SA-1B, and a mixture of open-source video datasets including DAVIS (Pont-Tuset et al., 2017; Caelles et al., 2019), MOSE (Ding et al., 2023), and YouTubeVOS (Xu et al., 2018b). Our released model is trained on SA-V manual + Internal and SA-1B.\nSAM 2 is designed for two tasks; the PVS task (on videos) and the SA task (on images). Training is done jointly on image and video data. To optimize our data usage and computational resources during training, we adopt an alternating training strategy between video data (multiple frames) and static images (one single frame). Specifically, in each training iteration, we sample a full batch either from the image or video dataset, with their sampling probabilities proportional to the size of each data source. This approach allows for a balanced exposure to both tasks and a different batch size for each data source to maximize compute utilization. Settings not explicitly mentioned here for the image task follow settings from the pre-training phase. See Table 13 (b) for the hyperparameters in our full training stage. The training data mixture consists of ~"}]}