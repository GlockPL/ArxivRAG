{"title": "A Survey on Data Markets", "authors": ["JIAYAO ZHANG", "YURAN BI", "MENGYE CHENG", "JINFEI LIU", "KUI REN", "QIHENG SUN", "YI-HANG WU", "YANG CAO", "RAUL CASTRO FERNANDEZ", "HAIFENG XU", "RUOXI JIA", "YONGCHAN KWON", "JIAN PEI", "JIACHEN T. WANG", "HAOCHENG XIA", "LI XIONG", "XIAOHUI YU", "JAMES ZOU"], "abstract": "Data is the new oil of the 21st century. The growing trend of trading data for greater welfare has led to the emergence of data markets. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. It serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets.", "sections": [{"title": "1 INTRODUCTION", "content": "Data is considered an invaluable resource in the digital economy. The last decades have witnessed the explosive growth of data. As raw material for acquiring knowledge and developing products, data generates value in an indirect way. After remodeling the commercial perspective of data, data is directly monetized like other material commodities nowadays. Individuals and organizations extensively trade datasets and derived data products. In this new vision, data is no longer the enabler of products, but also the product itself. Governments around the world are seizing this new opportunity. For example, the Chinese government unveiled a guideline to improve the market-based allocation of data factors, which is the first to list data as a production factor following land, labor, capital, and entrepreneurship [146]. The United States created the Federal Data Strategy Action Plan aimed at leveraging data as a strategic asset [462].\nDriven by the tides of data monetization, data markets have emerged. Data markets, a nascent interdiscipline of computer science and economics, are growing rapidly and evolving in myriad research directions. The history of data markets can be traced back to 1986. A seminal work by Admati and Pfleiderer [48] studies a market where traders purchase information from a monopolistic seller. The information they trade is the data endowed or produced by individual agents. To the best of our knowledge, the term \u201cdata market\" is put forward by Keenan [298] in 2008 for the first time in the literature. They propose to exchange spatial data collected by geographic information systems in the market. In 2011, Balazinska et al. [70] present a vision of a more general data market where commodities are derivative data products. They outline key challenges in a relational cloud data market for the database research community. Since then, data markets have experienced rapid development. Koutris et al. [308] design the first query-based data market; Deep and Koutris [162] propose a scalable and flexible pricing framework for relational queries; Agarwal et al. [51] design the first two-sided marketplace for trading training data directly; Chen et al. [132] introduce the first model-based data market; and more recently Liu et al. [342] propose the first end-to-end (model-based) data market involving the interactions among sellers, brokers, and buyers.\nWith the growing demand for data transactions, many data marketplaces have sprung up, such as AWS Data Exchange [2], Dawex [161], BDEX [3], Factual [8], and Snowflake [23]. Data marketplaces are online transaction locations or exchanges that facilitate the buying and selling of data products. They are authorized to host data products and conduct data transactions for the benefit of stakeholders.\nWe propose a definition of the data market in this survey as follows.\nA data market is any mechanism whereby the exchange of data products including datasets and data\nderivatives (such as query results and trained models) takes place as a result of data buyers and data\nsellers being in contact with one another, either directly or through mediating agents.\nThe data market serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In data markets, the life chain of data covers the process of data search, productization, monetization in pricing and transaction, and finally destruction. Trading data products naturally raises privacy, security, and trust concerns, and faces regulatory barriers to achieving compliance and traceability. Whether in academia or industry, there are rich explorations on designing data markets, where different data markets vary from each other in terms of data products, underlying functions, and market mechanisms.\nIn this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets."}, {"title": "1.1 Related Surveys", "content": "The existing surveys on data markets can be generally categorized based on the scope: (1) surveys on academic research [40, 176, 335, 483], (2) surveys on industry status [64, 299, 329, 445], and (3) surveys on data pricing [145, 153, 205, 371, 377, 415, 559, 561].\nSurveys on academic research. Efforts [40, 176, 335, 483] have been made to survey academic research for data markets within the whole lifecycle. Thomas and Leiponen [483] provide managers with a literature review on the commercialization of big data. From a managerial and commercial perspective, they introduce six business models in the data ecosystem, led by data suppliers, data managers, data custodians, application developers, service providers, and data aggregators. Based on this taxonomy, they discuss the characteristics of the data ecosystem and conclude with the challenges faced by managers and corresponding guidelines in trading big data including pricing and privacy concerns which we reinforce in this paper. Abbas et al. [40] examine 133 academic articles using a Service-Technology-Organization-Finance (STOF) model. They find that the existing literature on data marketplaces is primarily dominated by technology studies. Driessen et al. [176] present a statistical analysis of works related to data markets up until 2021, discuss practical application areas for data markets, categorize the problems of designing data markets, and find corresponding solutions in the literature. Liang et al. [335] use 4V (Volume, Velocity, Variety, and Value) to define big data and survey the lifecycle of trading big data, including data pricing, data trading, and data protection, for each of which they review corresponding issues and models. The above works [40, 176, 335, 483] mainly focus on techniques for trading data, while our survey covers state-of-the-art literature for trading general data products, including raw data and its derivatives such as queries, statistical inferences, and machine learning models. Moreover, our survey comprehensively covers key issues in main procedures in data markets from data search to data destruction.\nSurveys on industry status. There are four works [64, 299, 329, 445] conducting industry surveys on data marketplaces. Schomm et al. [445] present an initial survey of data marketplaces and data vendors by investigating 46 data suppliers from twelve dimensions (type, time frame, domain, data origin, pricing model, data access, data output, language, target audience, trustworthiness, size of vendor, and maturity) up until Summer 2012. Li et al. [329] introduce policies of China for developing data markets and discuss concerns and research opportunities including preprocessing, pricing, security, privacy, and verifiability. Azcoitia and Laoutaris [64] investigate 180 entities which trade data on the Internet, summarize different business models, and discuss open challenges. Kennedy et al. [299] introduce different types of data marketplaces and describe data transaction lifecycle from the perspective of buyers and sellers. They also interview buyers and sellers to understand the current status and challenges of online data marketplaces in 2022. The above works [64, 299, 329, 445] provides an understanding of data marketplaces through practical investigations of entities and marketplaces. In contrast, our survey not only examines mainstream data marketplaces worldwide but also provides a list of government policies.\nSurveys on data pricing. There have been several surveys [153, 205, 371, 377, 415, 559, 561] specializing in data pricing, a subtopic that receives the most attention in data markets. Muschalle et al. [377] investigate seven established vendors for their potential market situations, pricing approaches, and trends. Fricker and Maksimov [205] report a literature survey of 18 papers regarding several research questions, including the maturity and targets of pricing models, types of data products, and pricing mechanisms. Zhang and Beltr\u00e1n [559] review novel data pricing studies and categorize data pricing methods based on data granularity and privacy. Pei [415] starts with the economics of data pricing and reviews pricing models based on a set of fundamental principles. He also discusses the differences between digital products and data products, and the corresponding pricing methods. Very recently, Cong et al. [153] survey data pricing methods in machine learning pipelines, including pricing raw data sets, pricing data labels, and pricing in collaborative machine learning models. Zhang et al. [561] categorize and review pricing methods for queries from the aspects of market structure, privacy notion, query type, and pricing method. Miao et al. [371] classify data pricing techniques into three strategies and analyze thirteen pricing models. Chi et al. [145] outline the fundamental concepts of data pricing, categorize data pricing strategies into query-based and privacy-based approaches, and offer an overview of data pricing from a data science standpoint. In addition to covering other data market functions, our survey includes a comprehensive analysis of data pricing that examines both revenue allocation for allocating compensations to data sellers and data product pricing for pricing data products to data buyers and their interactions. Furthermore, it systematically reviews emerging game-theoretic approaches to data pricing for the first time.\nIn summary, while existing surveys approach data markets from either academic or industry perspective, our survey provides a comprehensive and general review of data markets covering both academic research and industry status including government policies across representative countries and domains. We also discuss the differences between data and other production factors and the corresponding impact on the design of data markets. While existing surveys investigate a few significant challenges in data markets, we study the interaction between key entities, summarize important desiderata for designing a well-functioning data market, and review techniques regarding data search, productization approaches, pricing mechanisms, data transactions, privacy concerns, etc, based on a formal framework as in Figure 2."}, {"title": "Contributions.", "content": "We present a comprehensive survey of data markets in both academia and industry. The purpose of this survey is to delve into subtopics of data markets in terms of computer science while covering mechanisms, regulations, and challenges in economics, law, and governance. The main contributions of this survey are summarized as follows.\n\u2022 Identify the unique properties of data and discuss the difference between data markets and other markets\nfor the four production factors (land, labor, capital, and entrepreneurship).\n\u2022 Introduce the framework of data markets, formalize the abilities and restrictions of key roles, and illustrate\nthe main procedures in the operations of data markets.\n\u2022 Present important desiderata for well-functioning data markets.\n\u2022 Summarize various methods of data search for various purposes, including crowdsourced dataset collection,\ndataset discovery in databases, data discovery in machine learning, and general dataset search.\n\u2022 Introduce various approaches to data productization based on versioning and data market categories.\n\u2022 Outline advertising strategies for data sellers and data purchase methods for data buyers in data transactions.\n\u2022 Review different approaches for revenue allocation and data product pricing, along with game-theoretic\npricing methods.\n\u2022 Describe possible attacks on privacy preservation, fairness, profitability, and traceability from dishonest\nentities and corresponding solutions.\n\u2022 Present guidelines and regulations and investigate actual data marketplaces in representative countries\nand domains.\n\u2022 Discuss various open challenges and emerging directions for future research."}, {"title": "1.2 Structure of The Survey", "content": "Figure 1 shows the structure of the survey. Section 2 first presents a diagram that shows different problems affecting data markets and their relationship. Section 3 summarizes important desiderata for building well-functioning data markets. Section 4 reviews methods of data search including crowdsourced dataset collection, dataset discovery in databases, dataset discovery in machine learning, and general dataset search. Section 5 describes techniques for data productization. Section 6 overviews strategies of both data buyers and data sellers in data transactions. Section 7 reviews studies investigating data product pricing and game-theoretic pricing. Section 8 reviews studies investigating revenue allocation. Section 9 deals with issues related to dishonest participants in untrusted data markets. Section 10 provides government policies and industry status in representative countries and regions. Section 11 draws a conclusion and discusses open challenges and opportunities for future work."}, {"title": "2 FRAMEWORK OF DATA MARKETS", "content": "In this section, we first show the differences between data and other production factors in Section 2.1. We then introduce key entities in data markets and their interactions in Section 2.2. Finally, we describe seven main procedures to illustrate how data markets operate in Section 2.3."}, {"title": "2.1 The Differences Between Data and Other Production Factors", "content": "Production factors, also known as factors of production, are the essential inputs required to produce goods and services. These factors play a key role in the production process by contributing to the creation of value. The classic economic classification of production factors includes land, labor, capital, and entrepreneurship [446, 457]. Differences in various properties between data and other production factors (land, labor, capital, and entrepreneurship) are listed in Table 1, which provides insights into the burgeoning data markets. Land refers to the natural resources used to create a good or service, e.g., forests and oil. Labor is the work done by the people in a workforce. Capital refers to capital goods or man-made resources such as tools and infrastructure while entrepreneurship represents innovation and technology used in the production of a good or service. The characteristics of data make data markets more complicated than other markets.\nConcerning replication. One of the most significant properties of data is the replicability. Data, once obtained by someone, can be easily replicated almost without cost, which is the same as entrepreneurship, while land, labor, and capital cannot be reproduced unlimitedly. Research on the challenges brought by the replicability of data are summarized in Section 9.2, along with the corresponding techniques including data replication attacks and robust-to-replication data pricing. The replicability of data leads to another property, non-rivalry, which means that multiple parties can share the usage of the same data at the same time. The replicability of data makes the externality especially prominent in data markets [253] while externality exists in other factors as well. The externality of data suggests an indirect impact on the data value for a data user when other users have the same data. The externality can be either positive or negative. A positive externality arises in cooperation because data users specializing in distinctive fields can benefit from each other due to the insights from data, e.g., co-workers in the same product line. A negative externality arises in competition because the use of data confers on a given user a competitive advantage that hinders the performance of its competitors. The externality of data has been taken into account in data pricing as shown in Section 7.\nConcerning utilization. In terms of utilization, several similarities and differences appear between data and other production factors. The composability implies data can be combined for different uses which is in line with other factors, while the divisibility differs data from labor, capital, and entrepreneurship, indicating the data can be easily divided for independent uses. In terms of value, the value of data extraordinarily varies in different tasks, making the heterogeneity much more prominent for data than other factors. Data is also persistent, which indicates that data can be reused and cannot be consumed, while labor and capital can be considered expendables, i.e., once used, never recycled. Besides persistence, data can even grow without artificial effort, e.g., weather data can be gathered naturally as time goes by, while entrepreneurship, e.g., technology and management expertise, must be created by human beings with mental labor. In other words, data can be constantly updated, which raises concerns about how to measure, acquire, and value data updates. This natural growth property of data brings a greater challenge for data pricing and data acquisition, which will be further discussed in Sections 6 and 7. While the properties of physical object and tradability can be easily understood, the lack of high liquidity of data implies that currently data can hardly be turned into cash easily since there is no mature market system as shown in the review of industrial development of data markets in Section 10, giving data low liquidity."}, {"title": "2.2 Key Entities", "content": "In this section, we introduce key entities in data markets and how they interact with others. Well-functioning data markets must bring together key entities with clear benefits for all. The framework is illustrated in Figure 2. An entity (an individual or organization) can play multiple roles\u2014data owner, data buyer, data seller, and data broker\u2014depending on the specific context and transaction they are involved in.\nData owners. Data owners are individuals or organizations that have the ownership of data. They may sell their data, making them data sellers, or buy data from others, making them data buyers.\nData sellers. Data sellers are individuals or organizations that sell data products in a data transaction.\nData buyers. Data buyers are individuals or organizations that purchase data products in a data transaction.\nData brokers. Data brokers are individuals or organizations that arrange data transactions between data sellers and data buyers. Data brokers act as data buyers when interacting with data sellers and as data sellers when interacting with data buyers. It cannot be ignored that data brokers serve as agents, managing and negotiating data transactions on behalf of both data sellers and data buyers. A data broker is not necessary for a data transaction, as data sellers and buyers trade data products directly.\nData marketplaces. Data marketplaces are platforms that connect data buyers with data sellers and provide free or paid services for data transactions, such as data product searches and negotiation areas. In addition to providing fast and centralized market information, data marketplaces usually need to operate as a data management platform to ensure that the data trading process is trustworthy, fair, profitable, secure, traceable, and efficient. Data brokers are practitioners in data marketplaces and may retain a portion of transaction proceeds as profits.\nPolicymakers and regulators. Policymakers propose and establish guidelines and regulations for data transac- tions, while regulators oversee and enforce them. The two authorities play effective roles in ensuring that entities comply with norms in data markets."}, {"title": "2.3 Main Procedures", "content": "In this section, we introduce seven main data market procedures, which generally follow a sequence: data search, data transaction, data compliance, data pricing, data traceability, and data destruction. It is important to note that not all of these procedures will occur, nor do they necessarily follow a fixed order, and they may overlap or interact with each other. The data market begins with data search, where data sellers gather valuable data from multiple sources. The collected data is further productized to generate various data products. Data products that are identified as compliant with data laws will be priced and traded in marketplaces. Reaching from data search to data transaction, the source and transformation of data will be tracked and recorded for traceability. Finally, when the ownership of data is revoked, meaning the data seller chooses to withdraw the control over the data from the data buyer, the data buyer is required to destroy the corresponding data.\nData Search. Data search is the initial phase of data markets where data sellers search and gather data from a wide range of sources, integrate them, and ensure they are valuable and accurate to meet the needs of data buyers. Given the data proliferation and the heterogeneous value of data as shown in Section 2.1, how to identify valuable data from plentiful data sources remains an important and challenging problem. Chapman et al. [124] survey techniques and implementations of dataset search, including information retrieval, databases, entity-centric, and tabular search. Roh et al. [430] survey subproblems of data aggregation for machine learning, including data acquisition, data labeling, and improving existing data. di Vimercati et al. [166] discuss the issues, possible existing techniques, and challenges of enabling individuals to trade data using a data marketplace platform while maintaining control over it. While various types of data vendors offer integrated data sources [422, 445], inaccuracies in data attributes sold by data brokers have been identified. A study by Venkatadri et al. [496] investigating Facebook advertising system and its partnership with six data brokers shows that more than 40% of attributes sold by data brokers are not accurate. Data search methods are summarized from the data usage perspective in Section 4.\nData Productization. Data productization is the process by which data sellers analyze the possible needs of data buyers, and then create sellable, standardized, repeatable, and comprehendible data products [242]. It can be sold independently or integrated to achieve higher value (property \u201ccomposability\u201d in Table 1). Raw data can be sold directly or taken as materials to create new data products with more value. Thus, data productization offers an extensive variety of commodities derived from raw data such as web interfaces, query results, and machine learning models. In general, data sellers develop data products by actualizing a product differentiation strategy. They conduct market research and release different versions of data products tailored to current and potential data buyers with different needs and sold at different prices (versioning) [449]. Yet it presents technical challenges to versioning strategy and data pricing, such as maximizing revenue [234, 342], no-arbitrage pricing [308, 310], and fine-grained pricing [330]. We provide a detailed discussion of data productization and versioning strategies in Section 5.\nData Transaction. A data transaction is an agreement between a data seller and a data buyer to exchange data products in return for payment. The interactions between data sellers and buyers are central to achieving a data transaction. Since data buyers do not have exact prior value of data, data sellers can take strategic actions to shape the beliefs of data buyers for greater sales. Data sellers send signals of data products to promote the sale, which can include descriptions, schema, and pricing details. Data buyers choose cost-efficient data products according to their needs. Fernandez et al. [201] propose a research agenda of data market platforms to tackle the problems of data sharing, discovery, and integration in the data transaction. The challenges of data transactions lie in marketing, selling, and purchasing data products, as well as addressing issues related to discovery and integration, and the tradeoffs between monetary costs and quality. In contrast, the challenges of data search lie in integrating raw data from diverse sources with data quality assurance. While both processes involve discovery and integration, the fundamental difference lies in the nature of the data being processed, i.e., data products versus the raw data. Detailed analysis of problems in the data transaction is provided in Section 6.\nData Pricing. Data pricing is to set prices for data products, which are influenced by interactions among all entities. The price and cost of a product are critical factors in calculating profitability [242]. A well-designed data pricing system can widely facilitate data transactions. In markets selling tangible products, one can determine the minimum price once the costs are understood, where the costs can be easily divided into material costs, processing costs, etc. Nevertheless, the costs of data products are hard to decompose and clearly estimate. The manufacturing cost of a data product includes human expertise, computing resources, and other factors, which are challenging to evaluate in practice according to any theoretical model, while its replication cost quickly comes closer to zero. In other words, data products may have high fixed costs with low marginal costs. The traditional cost-based pricing and competition methods tend to drive prices of data products to the level of marginal costs, leaving data sellers unable to recover their up-front investments [449]. Moreover, data products are task-oriented, making their prices application-specific. Customer-perceived value differs for the same data product. To tackle these problems, many researchers have proposed data pricing strategies that are more value-based and personalized by incorporating economics or game theory, such as Shapley value [51, 278, 341, 342, 450, 578] and auctions [52, 82, 141, 206, 380, 432]. See details in Section 7.\nData Traceability. Data traceability describes where data comes from and how it is transformed, which is a way to guarantee the truthfulness of data search, data productization, data pricing, and data transactions. Data buyers face a pressing problem of how to verify whether the data sellers have truthfully provided data products. An opportunistic way for data sellers to reduce the expenditure for data acquisition is to mingle some falsified data into the raw dataset without notifying data buyers [395]. In addition, malicious entities are likely to copy and transmit data products at almost zero cost, when the data market lacks a sound notion of authorship [64]. Therefore, it is necessary to ensure data traceability, which can also mitigate the data quality problem and the revenue allocation problem. Detailed analysis of data traceability is provided in Section 9.4.\nData Compliance. Data compliance refers to the standards and regulations that all entities must adhere to in order to prevent sensitive data leakage, misuse, destruction, etc. Different types of data security laws have been laid out around the world, such as HIPAA [460], GDPR [490], \u0421\u0421\u0420\u0410 [109], PCD-DSS [152], and SOX [439]. The actors in data markets have to comply with these laws or face steep fines. See Sections 10.1 and 10.2 for more guidelines and regulations. A rich body of studies delve into techniques for using data in compliance with data security laws and avoiding privacy violations, such as data desensitization [117], differential privacy [180\u2013182], multi-party secure computation [210, 547], federated learning [288], and machine unlearning [113]. See details in Section 9.1.\nData Destruction. Data destruction requires data buyers to destroy data as soon as it is no longer used or the data seller is no longer willing to share the data. It mainly includes the physical destruction of storage media or overwriting of storage contents. Recently, regulated by data privacy regulations (e.g., GDPR [490], \u0421\u0421\u0420\u0410 [109], and PIPL [147]), the right to be forgotten becomes a part of the personal data protection standard. A data subject has the right to have their personal data erased from the controller under several specific circumstances. Data itself can exist persistently as pointed out in Section 2.1, but data usage should be restricted. A new research direction coined \u201cmachine unlearning\u201d focuses on making machine learning models comply with the right to be forgotten."}, {"title": "3 IMPORTANT DESIDERATA IN DATA MARKETS", "content": "In this section, we show the important desiderata that well-functioning data markets should enjoy. These include truthfulness, fairness, privacy preservation, and efficiency in all procedures, as well as profitability for data pricing, which should be free from arbitrage, dynamic, revenue maximization and cost minimization, and emphasizing social welfare optimization. Additionally, it is important to have traceability in data transactions to ensure transparency and accountability."}, {"title": "3.1 Truthfulness", "content": "The most important concern in data markets is truthfulness, which is also referred to as incentive compatibility (IC) [432] in economic theory and mechanism design. A data market is considered to be truthful when entities act truthfully and reveal their real thoughts to gain their maximum utility while selfish entities cannot benefit from lying. The desired property of truthfulness is supposed to hold in the interactions among all entities of data markets. It can convince data sellers that data brokers have truthfully used their data and convince data buyers that data brokers have truthfully produced and managed data products. Specifically in data pricing, truthfulness indicates that entities truthfully reveal their data valuations and bids. Researchers seek to design mechanisms to motivate entity behaviors to be truthful. Examples are shown as follows.\n\u2022 Truthful report: Chen et al. [137] propose a pricing mechanism based on peer prediction where reporting true data is the only optimal way for the data sellers to maximize the expected payment. Zheng et al. [574] use pointwise mutual information to guarantee that data providers always maximize their expected score by truthfully reporting their observed data.\n\u2022 Truthful bidding: The Vickrey-Clarke-Groves (VCG) auction [432] incentivize bidders to bid their true val- uations for multiple items by ensuring that truthfulness is an optimal strategy for each bidder, regardless of what other bidders do (a strong truthfulness guarantee known as dominant strategy incentive compatibility). The VCG auction has been extensively applied to data markets [220, 229, 521]. See details in Section 7.2.\n\u2022 Truthful storage: A blockchain [437] is a distributed data structure used to store immutable records of information, featuring decentralization, verifiability, and integrity. It has been applied to data markets [68, 141, 150] to prevent fraudulent behaviors and enhance truthfulness."}, {"title": "3.2 Fairness", "content": "Fairness is one of the fundamental requirements for dividing resources or allocating revenue among participants. The universal notion of fairness can be tricky to formalize since it is natural that different phenomena call for different notions of fairness. A typical data market requires some of them. In this section, we review different definitions of fairness in two areas: resource division in Section 3.2.1 and revenue allocation in Section 3.2.2."}, {"title": "3.2.1 Resource Division", "content": "Fair resource division distributes data products to data buyers in a fair manner, typically in auction-style data transactions. There is a rich body of studies in economics, sociology, and mathematics on the problem of fair division, which could date back to Steinhaus [463] in 1948. We review several multifaceted notions of fairness in the fair division. Given a set of entities $N = \\{1, ..., n\\}$ and a set of resources $O = \\{o_1, ..., o_m\\}$, an allocation $\\Pi = (\\pi_1, . . ., \\pi_n)$ is a $n$-partition of the set of resources $O$, where $\\pi_i \\subseteq O$ is a bundle of resources allocated to entity $i$. Utility measures the satisfaction or value that an entity derives from consuming or owning the resource. Let $u_i(\\pi_j) \\in \\mathbb{R}_{\\geq o}$ be the utility of entity $i$ toward $\\pi_j$ for the allocation $\\Pi = (\\pi_1, . . ., \\pi_n)$.\n\u2022 Envy freeness [202]: An allocation $\\Pi$ is envy-free iff $\\forall i, j \\in N, u_i(\\pi_i) \\geq u_i(\\pi_j)$. That is, each entity prefers its own allocation over that of any other entity.\n\u2022 Pareto efficiency [410]: An allocation $\\Pi$ is Pareto-efficient iff there is no allocation $\\Pi' = (\\pi'_1, . . ., \\pi'_n)$ such that $\\forall i \\in N, u_i(\\pi'_i) \\geq u_i(\\pi_i)$ and $\\exists j \\in N, u_j(\\pi'_j) > u_j(\\pi_j)$. That is, no entity can be better off without making any other entity worse off or without any loss. An envy-free allocation is proven to be Pareto efficiency when the number of entities equals the number of indivisible resources and the utilities are quasi-linear [475].\n\u2022 Max-min fairness (also known as the Santa Claus problem) [454]: An allocation $\\Pi$ is max-min fair iff $\\Pi = \\max_{\\Pi} \\min_i u_i(\\pi_i)$. Since it is somehow difficult to provide an exact notion of fairness or to find a mechanism that satisfies all desirable notions of fairness, max-min fairness is proposed in this case, which is defined as making the least-happy entity as happy as possible [454]."}, {"title": "3.2.2 Revenue Allocation", "content": "Fair revenue allocation distributes the jointly-produced revenue to the data sellers in the process of data pricing in a fair manner. Consider a set of $n$ players $N = \\{z_1", "342": ".", "544": ".", "72": "The Banzhaf value of player $z_i$ is defined as $BV_i = \\frac{1"}, {"U(S)": ".", "183": "Let an allocation $x : N \\to R_+$ represent the payoff distributed to the players in $N$ with $x(N) = U(N)$. The excess is defined by $e(S", "443": "The allocation $x$ is called an imputation if $x_i \\geq U(\\{z_i\\})$ holds for all"}]}