{"title": "TraSCE: Trajectory Steering for Concept Erasure", "authors": ["Anubhav Jain", "Yuya Kobayashi", "Takashi Shibuya", "Yuhta Takida", "Nasir Memon", "Julian Togelius", "Yuki Mitsufuji"], "abstract": "Recent advancements in text-to-image diffusion models have brought them to the public spotlight, becoming widely accessible and embraced by everyday users. However, these models have been shown to generate harmful content such as not-safe-for-work (NSFW) images. While approaches have been proposed to erase such abstract concepts from the models, jail-breaking techniques have succeeded in bypassing such safety measures. In this paper, we propose TraSCE, an approach to guide the diffusion trajectory away from generating harmful content. Our approach is based on negative prompting, but as we show in this paper, conventional negative prompting is not a complete solution and can easily be bypassed in some corner cases. To address this issue, we first propose a modification of conventional negative prompting. Furthermore, we introduce a localized loss-based guidance that enhances the modified negative prompting technique by steering the diffusion trajectory. We demonstrate that our proposed method achieves state-of-the-art results on various benchmarks in removing harmful content including ones proposed by red teams; and erasing artistic styles and objects. Our proposed approach does not require any training, weight modifications, or training data (both image or prompt), making it easier for model owners to erase new concepts. Our codebase is publicly available", "sections": [{"title": "1. Introduction", "content": "Diffusion models [22, 26] have pushed the boundaries of realistic image generation by making it as easy as writing a simple prompt. This advancement has brought these models into the public space where they are now used by millions of users every day. However, these models are trained on multiple billion images that have not been cleaned to remove harmful content such as nudity and violence, which have introduced unwanted capabilities into these models. While some safety checks and alignment methods [5-7, 13-15, 19, 25, 29, 30] have been proposed for these models, adversaries [3, 18, 27, 31] have been successful in bypassing them. Thus it is pertinent to develop more efficient methods not to allow these models to generate harmful content.\nSimilarly, these models have knowingly or unknowingly been trained on copyrighted content scrapped from the web. Model owners now face scrutiny in the form of lawsuits asking them to remove the capability of the model to generate such content or concepts [2, 12, 16, 17]. One such example is generating images with artistic styles similar to those of particular artists.\nA naive solution for model owners is to retrain the base diffusion model after removing the problematic content from the datasets. This possibly requires human annotation of billions of images and further retraining of the diffusion model, which can be extremely costly. Since this is infeasible, model owners are interested in quick fixes that (a) require little to no training; (b) allow easy removal of new concepts; and (c) do not impact the overall model performance on other tasks.\nTo tackle this problem, most existing approaches proposed either require updating the weights of the model [5-7, 9, 13, 15, 30] or work at the inference level, using some variants of negative prompts [25]. Updating the model weight (a) requires the collection of problematic prompts or images that define the concept. Given that this data pertains to a concept that needs to be erased, it can be harmful content that cannot be collected or even copyrighted information, making these approaches difficult to implement in practice. (b) This comes at a cost to the overall generation capabilities of the diffusion model on unrelated concepts, especially when a large number of concepts need to be erased. (c) Additionally, once a concept is erased, it cannot be reintroduced in this scenario. The model owner may also wish to have multiple inference conditions from the same weights, which a user can toggle based on their requirements. Updating weights does not allow this, they would rather need multiple inference models, increasing storage requirements and making it harder to manage as the number of erased concepts increases. (d) Lastly, updating model weights is a computationally expensive procedure. Thus, a more practical solution for model owners is to have methods that work on the inference stage, without requiring weight updates.\nMore recently, researchers have shown the ability to jailbreak text-to-image concept erasure methodologies [3, 18, 27, 28, 31]. These jail-breaking methodologies find harder prompts that do not directly contain identifying information of the concept that needs to be erased, thus bypassing the security measures. Previous defenses [5-7, 9, 13, 15, 25, 29, 30] work well when prompted with the concept or synonyms of the concept, but fail when prompted with prompts that do not directly mean the concept. They focus on removing the ability of a particular set of prompts to generate a particular type of images, but this does not necessarily remove the ability of the model to produce such images when prompted differently. In this paper, we study how to evade jail-breaking approaches such that the model cannot produce the concept even when prompted with phrases that do not directly imply the concept.\nSince a model owner can control the generation process, it is sufficient to guide the denoising process away from the space represented by a particular concept. Current approaches in this direction focus on negative prompting which replaces unconditional scores in classifier-free guidance with scores from negative prompts. However, this does not guarantee that we will push the trajectory away from the space pertaining to a particular concept, as we will show in this paper.\nTo address this issue, we propose two techniques. Firstly, we propose a modification of the conventional negative prompting procedure. We argue that conventional negative prompting has an issue in its formulation (in particular, when applied to the concept erasure task). We provide a simple corner case where an adversary prompts the model with the same prompt as the negative prompt that is set by the model owner. In this case, conventional negative prompting guides the denoising process towards the negative prompt (which is the concept we want to erase). Thus, we update the way it is computed by guiding it towards an unconditional score instead of the score from the negative prompt when this is the case. Secondly, we propose localized loss-based guidance to steer the trajectory so that our modified negative prompting technique works more effectively. In our preliminary experiments, we had observed that even with our modified negative prompting technique, adversarial prompts could still successfully generate the concept we wanted to avoid. We hypothesize that this is because adversarial attacks find prompts that do not directly imply the concept and these do not completely align with the negative prompt, bypassing the negative guidance."}, {"title": "2. Related Works", "content": "Researchers have explored methodologies to erase concepts by updating the model weights [5-7, 9, 13, 15, 19, 30] and also during the model inference stage [14, 25, 29]. Kumari et al. [13] proposed minimizing the KL divergence between a set of prompts defining a concept and an anchor concept. Schramowski et al. [25] proposed a modified version of negative prompts to guide the diffusion model away from generating unsafe images. Pham et al. [19] proposed using task vectors to update the weights of the base model based on a version of the model that is trained solely on the concept that needs to be erased. Gandikota et al. [6] found a close form expression of the weights of an erased diffusion model based on a set of prompts and updated the weights accordingly in a one-shot manner. Lu et al. [15] used LORA (low-rank adaptation) for fine-tuning the base model along with a closed-form expression of the cross-attention weights. Gandikota et al. [5] updated the diffusion model weights to minimize the likelihood of generating particular concepts based on an estimated distribution from a set of collected prompts. Gong et al. [7] proposed using a closed-form solution to find target embeddings corresponding to a concept and then updated the cross-attention layer accordingly. Heng et al. [9] updated the model weights to forget concepts inspired by continual learning. Zhang et al. [30] proposed cross-attention re-steering which updates the cross-attention maps in the UNet model to erase concepts. Li et al. [14] proposed a self-supervised approach to find latent directions pertaining to particular concepts and then used these to steer the trajectory away from them. In a recent work, Yoon et al. [29] found subspaces in the text embedding space corresponding to particular concepts and filtered the embeddings based on this to erase concepts. They additionally applied a re-attention mechanism in the latent space to diminish the influence of certain features.\nFirstly, most methods have been shown to be vulnerable to adversarial prompts that attempt to bypass defenses, as discussed in the next section. Our work focuses on how to mitigate the threat of adversarial prompts. Secondly, most approaches require one or more of the following - training, weight updates, and/or training data (images or prompts). This makes removing new concepts and reintroducing previously erased concepts harder or impossible in certain scenarios. Our approach is free of all these constraints."}, {"title": "2.1. Concept Erasure", "content": ""}, {"title": "2.2. Jail Breaking Concept Erasure", "content": "Red-teaming efforts have focused on circumventing concept erasure methods by finding jail-breaking prompts via either white-box [3, 18, 28, 31] or black-box [27, 28] adversarial attacks. Pham et al. [18] used textual inversion to find adversarial examples that can generate erased concepts. Tsai et al. [27] used an evolutionary algorithm to generate adversarial prompts in a black-box setting. Zhang et al. [31] found adversarial prompts using the diffusion model's zero-shot classifier for guidance. Chin et al. [3] proposed optimizing the prompt to minimize the distance of the diffusion trajectory from an unsafe trajectory. Yang et al. [28] proposed both white-box and black-box attacks on both the prompt and image modalities to bypass prompt filtering and image safety checkers.\nIn this paper, we propose a method that safeguards against such attack methods, especially in the case of generating harmful content such as nudity."}, {"title": "3. Preliminaries", "content": "Diffusion models [26] such as Stable Diffusion (SD) [22] and Imagen [23] are trained with the objective of learning a model $\\epsilon$ to denoise a noisy input vector at different levels of noise characterized by a time-dependent noise scheduler.\nDuring training, the forward diffusion process comprises a Markov chain with fixed time steps $T$. Given a data point $x_0 \\sim q(x)$, we iteratively add Gaussian noise with variance $\\beta_t$ at each time step $t$ to $x_{t-1}$ to get $x_t$ such that $x_T \\sim \\mathcal{N}(0, I)$. This process can be expressed as,\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t \\epsilon I), \\forall t \\in \\{1, ...,T\\}.$"}, {"title": "4. An Effective Concept Erasure Technique", "content": "The reason why most currently proposed concept erasure methods are susceptible to adversarial prompts is that they erase concepts based on modifications from a set of prompts defining a concept. However, this does not completely erase the ability of the model to generate the concept. Black-box adversarial prompts using approaches such as evolutionary algorithms [27] simply find other prompts in the embedding space that were not suppressed by the defense method.\nThus, we need an approach to guide the trajectory away from space corresponding to a particular unfavorable concept. To do so, we propose a method which consists of two parts: (1) a modified version of negative prompting and (2) localized loss-based guidance to steer the diffusion trajectory.\nModified Negative Prompting. Negative prompting is a commonly used technique for guiding away from generating certain concepts/objects. It simply steers away from space pertaining to the negative concept and towards the input prompt. However, in the case of concept erasure, if the input prompt is adversarial in nature, it limits its effectiveness.\nTraditionally, negative prompting has been expressed as\n$\\epsilon \\leftarrow \\epsilon_\\theta(X_t, \\epsilon_{np}) + S(\\epsilon_\\theta(X_t, \\epsilon_{p}) - \\epsilon_\\theta(X_t, \\epsilon_{np})).$"}, {"title": "5. Experiments", "content": "In the following sections, we show how our approach can be applied to various tasks, including avoiding generating NSFW content and violence and erasing artistic styles and objects (in Appendix 7)."}, {"title": "5.1. Robustness to Jail Breaking Prompts", "content": "One of the major issues with previous concept erasure techniques is that they are susceptible to adversarial prompts, which can even be found in a black-box setting. Since previous approaches already perform well in removing concepts when directly prompted with the concept, in this paper we primarily focus on protecting diffusion models against adversarial prompts targeted to generate NSFW content and images containing violence.\nAdversarial or jail-breaking prompts are either generated using white-box attacks [3, 31] on diffusion models or through black-box [27, 28] attacks. We treat the adversary as having only black-box access to the diffusion model wherein the adversary can prompt the model any number of times using any seed value they set. We make this assumption, given that we do not directly update the weights of the models, thus a model owner cannot share the model weights while placing our security measures."}, {"title": "6. Conclusion", "content": "In this paper, we proposed a technique to erase concepts from conditional diffusion models through a modified version of negative prompting along with loss-based guidance. We used these guidance techniques to push the diffusion trajectory away from generating the images of the concept we wish to erase. Our approach does not require any training, training data (prompts or images), or weight updates. We showed that this approach is robust against adversarial prompts targeted towards generating NSFW and violence-depicting content. We further extended our analysis to show that you approach is effective in erasing artistic styles and objects as well."}]}