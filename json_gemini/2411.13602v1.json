{"title": "Large-scale cross-modality pretrained model enhances cardiovascular state estimation and cardiomyopathy detection from electrocardiograms: An AI system development and multi-center validation study", "authors": ["Zhengyao Ding", "Yujian Hu", "Youyao Xu", "Chengchen Zhao", "Ziyu Li", "Yiheng Mao", "Haitao Li", "Qian Li", "Jing Wang", "Yue Chen", "Mengjia Chen", "Longbo Wang", "Xuesen Chu", "Weichao Pan", "Ziyi Liu", "Fei Wu", "Hongkun Zhang", "Ting Chen", "Zhengxing Huang"], "abstract": "Cardiovascular diseases (CVDs) present significant challenges for early and accurate diagnosis. While cardiac magnetic resonance imaging (CMR) is the gold standard for assessing cardiac function and diagnosing CVDs, its high cost and technical complexity limit accessibility. In contrast, electrocardiography (ECG) offers promise for large-scale early screening. This study introduces CardiacNets, an innovative model that enhances ECG analysis by leveraging the diagnostic strengths of CMR through cross-modal contrastive learning and generative pretraining. CardiacNets serves two primary functions: (1) it evaluates detailed cardiac function indicators and screens for potential CVDs, including coronary artery disease, cardiomyopathy, pericarditis, heart failure and pulmonary hypertension, using ECG input; and (2) it enhances interpretability by generating high-quality CMR images from ECG data. We train and validate the proposed CardiacNets on two large-scale public datasets (the UK Biobank with 41,519 individuals and the MIMIC-IV-ECG comprising 501,172 samples) as well as three private datasets (FAHZU with 410 individuals, SAHZU with 464 individuals, and QPH with 338 individuals), and the findings demonstrate that CardiacNets consistently outperforms traditional ECG-only models, substantially improving screening accuracy. Furthermore, the generated CMR images provide valuable diagnostic support for physicians of all experience levels. This proof-of-concept study highlights how ECG can facilitate cross-modal insights into cardiac function assessment, paving the way for enhanced CVD screening and diagnosis at a population level.", "sections": [{"title": "Main", "content": "Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, with their share of global deaths increasing significantly in recent years\u00b9-3. Notably, the mortality rate associated with cardiomyopathy has risen by 7.6% during this period, contributing to an escalating burden of disease4. The multifaceted nature of CVD presentations often results in missed or misdiagnosed cases in clinical practice, leading to delays in treatment and inadequate patient care. To effectively assess cardiovascular health, clinicians utilize a variety of diagnostic modalities, including electrocardiography (ECG) 7 and cardiovascular magnetic resonance imaging (CMR). ECG measures the electrical potential differences within the heart and is widely employed to detect cardiac abnormalities such as myocardial infarction and arrhythmias. Its non-invasive and cost-effective nature has facilitated its adoption across healthcare institutions, making it accessible at all levels. In contrast, CMR is a comprehensive imaging modality that excels in evaluating cardiac morphology, function, myocardial perfusion, and tissue characterization, providing a more detailed assessment of cardiac status10-13. As such, CMR is regarded a common and effective diagnostic tool for CVDs\u00b94, particularly structural heart conditions like cardiomyopathy 15-17. However, the high costs and operational complexities associated with CMR limit its accessibility\u00b98, especially in developing countries and rural areas. Given this context, there is an urgent need to explore methodologies that can leverage ECG as a viable alternative to CMR for assessing cardiac health and conducting preliminary screenings for cardiovascular diseases. By harnessing the advantages of ECG, we can potentially enhance diagnostic capabilities and improve patient outcomes in settings where advanced imaging techniques are not readily available.\nIn response to urgent clinical need, the recent emergence of large-scale multimodal datasets, such as the UK Biobank (UKB) and MIMIC19,20, combined with rapid advancements in deep learning technologies, presents a transformative opportunity to address existing challenges in healthcare. Cross-modal contrastive learning approaches, exemplified by CLIP (Contrastive Language-Image Pre-training)21, have demonstrated remarkable potential in aligning latent representations from diverse modalities through the creation of a shared embedding space. This innovative capability enables high-precision tasks, including zero-shot classification, which empowers models to make accurate predictions without the necessity for task-specific training. Moreover, generative models like Stable Diffusion model22 leverage the text encoder from CLIP as a conditional guide, facilitating the generation of highly realistic images that closely correspond to textual descriptions. These advancements underscore the power of cross-modal relationships in enhancing the capabilities of individual modalities and advancing artificial intelligence systems in healthcare. By integrating these cutting-edge technologies, researchers can develop more effective disease screening and diagnostic tools, as well as novel treatment strategies, ultimately improving patient outcomes and transforming the landscape of clinical practice.\nBuilding on the foundation of cross-modal approaches and previous studies23-25, we propose a novel approach for cross-modal alignment and generative pretraining, named CardiacNets. Unlike traditional methods that focus on aligning text-image pairs with similar informational content\u00b2\u00b9, our model emphasizes the relationship between a robust modality-one that conveys rich, comprehensive information and a weaker modality, aiming to enhance the latter's capabilities through insights gained from the former. While earlier approaches have jointly analyzed ECG and CMR data23, CardiacNets is specifically tailored for cardiac disease screening, demonstrating significant improvements in both diagnostic performance and the quality of CMR generation. The proposed solution consists of two primary components. First, we implement contrastive learning to align ECG data with their corresponding CMR images, enabling a more nuanced understanding of cardiac conditions. By leveraging the strengths of each modality, CardiacNets aims to not only improve the accuracy of CVD screening, but also enhance the generation of clinically relevant images, ultimately contributing to more effective patient care. Second, the ECG encoder, trained through contrastive learning, is subsequently frozen and employed as a conditional encoder within a diffusion model. This aligned ECG representation effectively guides a latent video diffusion model26\u2014a state-of-the-art approach in the realm of computer-generated video in producing high-quality CMR sequences that correspond to the ECG inputs. The generated CMR sequences significantly enhance the interpretability of ECG-based predictions, offering clinicians a more profound understanding of their patients' cardiovascular status.\nThis study aims to develop and validate a deep learning solution that utilizes ECG as a surrogate for CMR in assessing cardiac status and downstream CVD screening. Our approach is structured into two phases, as illustrated in Fig.1: (1) cross-modal contrastive learning utilizing ECG and CMR data and ECG-based CMR Image Generation Model, (2) comprehensive evaluation of downstream tasks. In the first phase, cross-modal pre-training enables ECG to capture corresponding CMR information and generate CMR images. The second phase demonstrates the superiority of the cross-modal pretrained ECG model over traditional single-modality supervised learning models that rely solely on ECG data across all evaluated downstream tasks. Notably, we observed a remarkable 21.4% improvement in cardiac indicators assessment using the UK Biobank dataset and an 8.7% enhancement in pericarditis screening on the MIMIC dataset. In several downstream tasks, the performance of the pretrained model approached that of CMR-based supervised learning models. Moreover, high-fidelity CMR images generated by the diffusion model, highlight the capabilities of our proposed model in visualization and interpretability. Finally, a reader study revealed that our model significantly enhances clinicians' ability to screen for cardiomyopathy, showcasing its practical utility in real-world clinical settings."}, {"title": "Results", "content": "Screening for Cardiovascular diseases and cardiac phenotype prediction\nWe utilized two public datasets (UKB and MIMIC) and three private datasets (FAHZU, SAHZU, and QPH) to evaluate the performance of the fine-tuned model in predicting cardiac structural phenotypes and various CVD conditions (Fig. 2). In the UKB dataset (Fig. 2a), which includes paired CMR images, we compared the performance of CardiacNets against standalone ECG-based supervised learning and standalone CMR-based supervised learning. Across all diseases, CardiacNets demonstrated significant improvement over the ECG-only models (p < 0.05), particularly in cardiomyopathy, achieving an AUC of 87.65% compared to 72.56% for the ECG-only model. Furthermore, when comparing CardiacNets with CMR-based supervised learning, no significant difference was observed for most diseases (p > 0.05), except for CAD. This suggests that CardiacNets can nearly match the performance of CMR-based models in most cases.\nFor cardiac phenotype prediction, we incorporated 82 cardiac phenotypes published by the UKB, including left and right atrial ejection fractions and end-diastolic/systolic volumes for both atria. These indicators provide valuable insights into the detailed structure and overall state of the heart. The average Pearson correlation coefficients for these 82 indicators were as follows: CardiacNets achieved an average of 0.426, while ECG-based and CMR-based models had averages of 0.351 and 0.612, respectively. Regression plots for each indicator can be found in Supplementary Fig. 1. Although CardiacNets demonstrated significant improvement over the ECG-only model, there remains a notable gap compared to the CMR-based model. In the MIMIC dataset (Fig. 2b), where paired CMR data was unavailable, we compared CardiacNets with the ECG-only supervised learning model. As in the UKB dataset, CardiacNets significantly outperformed the ECG-only model (p < 0.0001).\nWe also validated the model's performance on three private datasets for cardiomyopathy. Using models trained on the MIMIC dataset (including both ECG-CMR pre-trained models and those trained solely on ECG), we applied them directly to the three private datasets to assess generalization. The results consistently showed that the pre-trained models outperformed the ECG-only models, with significant differences observed in the FAHZU dataset (p < 0.0001), SAHZU dataset (p < 0.01), and QPH dataset (p<0.05). Moreover, the results across all three private datasets were comparable to or exceeded those from the MIMIC dataset, further emphasizing the robustness and generalizability of the models. These findings highlight the efficiency and effectiveness of CardiacNets across a wide range of settings. Additional quantitative results can be found in Supplementary Tables 1-3.\nLabel efficiency for disease screening\nLabel efficiency refers to the amount of training data and labels required to achieve a target performance level for a specific downstream task, highlighting the annotation workload for medical experts. CardiacNets demonstrated remarkable label efficiency across various disease screening tasks in the MIMIC dataset (Fig. 2c). Notably, CardiacNets outperformed standalone ECG-based models while using only 10% of the training data, underscoring the potential to address data scarcity. This capability is crucial for real-world applications, as it alleviates the annotation burden on clinical experts, making the model more applicable across diverse healthcare settings."}, {"title": "Discussion", "content": "In this study, we propose CardiacNets, a cross-modal pretraining model that employs a pretraining-finetuning paradigm to efficiently adapt to the screening of a broad spectrum of CVDs. Additionally, the visualizations generated by the model bolster the interpretability of the results. CardiacNets demonstrates substantial potential for utilizing ECG in the screening of cardiovascular diseases, particularly in economically underdeveloped regions where access to advanced diagnostic tools may be limited.\nBuilding on previous work24, we acknowledge that while focusing on a specific task and obtaining CMR model weights through supervised learning can enhance the ECG model's performance using contrastive loss training, this approach has a significant limitation: it does not leverage the inherent paired information between ECG and CMR. As a result, each time a new task arises, we must retrain both the CMR supervised model and the ECG alignment model specifically for that task. To address this limitation, we developed a model that introduces a critical shift from prior approaches: we utilize a self-supervised27 method to comprehensively learn the structural information of CMR. This encapsulates a holistic representation of cardiac status that serves as a foundation for analyzing downstream tasks. In our self-supervised pretraining model, we freeze the parameters of the CMR model and update the ECG model by minimizing contrastive loss. This allows the ECG model to extract essential features that represent the overall cardiac state inherent to CMR, independent of specific tasks. These features exhibit strong discriminative capabilities across various downstream tasks. For CMR, we implement a masked self-supervised learning approach28 using the GreenMIM29 framework based on the Swin Transformer30. Unlike conventional Vision Transformers (ViTs)\u00b3\u00b9, the unique shifted window mechanism of the Swin Transformer creates a receptive field akin to that of Convolutional Neural Networks (CNNs)32, effectively addressing the limitations of ViTs in capturing local details in images\u00b3\u00b3\u2014an essential aspect of medical image analysis. For ECG, which is inherently a periodic time-series signal, we employ a modified ViT for encoding. This enables attention computation across independent blocks, allowing for a more comprehensive focus on information compared to traditional time-series models such as Recurrent Neural Networks (RNNs)34.\nThe results of CardiacNets on the UKB and MIMIC datasets demonstrate that the model significantly enhances the performance of ECG, particularly in smaller datasets, approaching the upper limits achieved through supervised learning with CMR. In contrast, this improvement is less pronounced in larger datasets, which is understandable: deep learning methods rely on large sample sizes to accurately approximate true distributions; therefore, the more data available, the better the model's fitting capacity35. However, even within the MIMIC dataset, where the number of samples for CAD and HF exceeds two to three hundred thousand, models trained using ECG-CMR paired learning consistently outperform those based solely on supervised learning with ECG. This observation indicates that there is an upper limit to what can be achieved with ECG alone, primarily due to the lack of cross-modal structural information that CMR provides. This underscores the effectiveness of our cross-modal solution.\nInterpretability is essential in clinical settings36, and CardiacNets introduces an innovative video diffusion model conditioned on ECG to generate temporal CMR images. This approach allows for the creation of CMR images even in the absence of actual CMR data, serving as a valuable tool for both explainability and visualization. While existing video diffusion models, such as Sora\u00b3\u201d, have made significant advances in areas like artistic creation, their use in medical imaging is still limited. This"}, {"title": "Methods", "content": "Dataset for Model Development and Evaluation\nWe utilized the UK Biobank\u00b99, the largest publicly available dataset containing paired ECG and cine CMR imaging data, to advance our study. The UKB encompasses a diverse cohort of 500,000 volunteers aged 40 to 69, recruited across the UK since 2006, with ongoing data collection efforts. For our study, we focused on ECG-CMR pairs collected during participants' initial imaging visits, yielding a total of 41,519 paired datasets. The ECG data comprises 12-lead recordings, each lasting 10 seconds and sampled at 500 Hz, while the cine CMR data includes both short-axis and four-chamber long-axis views, each capturing a single cardiac cycle with 50 frames. The dataset exhibits a gender distribution of 48.3% male and 51.7% female participants. In terms of ethnicity, the majority of participants are White (96.6%), with smaller representations of Mixed (0.5%), Asian or Asian British (1.1%), Black or Black British (0.7%), Chinese (0.3%), and other ethnic groups (0.5%). Additionally, 0.3% of participants preferred not to disclose their ethnicity or were uncertain. The ethnicity classifications adhere to the UKB Data-Coding system (1001). Leveraging the UKB data, we developed our Contrastive ECG-CMR Pre-training model and the ECG2CMR Diffusion Model. The dataset was divided into training, validation, and test sets using a 7:1:2 ratio, ensuring a robust solution for model evaluation and development.\nThe FAHZU dataset is divided into three distinct subsets: the first subset includes only ECG data for a three-class classification of cardiomyopathy, encompassing 5,518 cases of dilated cardiomyopathy, 2,546 cases of hypertrophic cardiomyopathy, and 824 cases of restrictive cardiomyopathy. The second subset, also based solely on ECG data, is intended for external validation of cardiomyopathy, comprising 222 patients diagnosed with cardiomyopathy and 188 individuals without the condition. The third subset consists of ECG-CMR paired data for a reader study, including 77 patients with cardiomyopathy and 34 individuals without the condition. The SAHZU dataset includes 264 patients diagnosed with cardiomyopathy and 200 individuals without the condition, while the QPH dataset contains 236 cardiomyopathy patients and 102 non-cardiomyopathy individuals. Both of these private datasets are employed for external validation of cardiomyopathy. Importantly, all datasets utilized ECG formats consistent with that of the UKB dataset. Detailed data distributions for all datasets are provided in Supplementary Table 7. Except for the datasets designated for external validation, all remaining datasets were partitioned into training, validation, and testing sets in a 70:10:20 ratio.\nData processing and augmentation\nWe implemented a series of signal pre-processing techniques on all collected ECG data to enhance the quality and reliability of the signals for subsequent analysis. Initially, baseline drift was addressed using seasonal decomposition, allowing us to isolate and correct the trend component effectively. Following this, we applied wavelet transform denoising using the \"db6\" wavelet along with soft thresholding. This approach effectively reduced noise while preserving critical signal characteristics, ensuring that the integrity of the ECG waveform was maintained. Finally, we employed a Savitzky-Golay filter 51 to smooth the signal. This filter utilizes polynomial fitting within a sliding window to refine the ECG waveform further. Collectively, these pre-processing steps ensured the generation of clean and reliable data, facilitating robust analysis in our study.\nThe original data for short-axis cardiac magnetic resonance (CMR) images is structured in four dimensions: length, width, slice, and time. Typically, the slice dimension comprises 3 to 10 slices. To streamline the model input, we selected the middle basoapical slice, reformulating the data into a three-dimensional structure, where the time dimension consists of 50 frames. To focus the model on the cardiac region, we employed a pretrained segmentation model 52 to derive a heart region mask. The images were then cropped to 80\u00d780 pixels, centering on the minimal bounding box encompassing the non-zero region of the mask. For long-axis four-chamber CMR images, the original data is three-dimensional, consisting of length, width, and time dimensions, with the time dimension also comprising 50 frames. Using the same segmentation methodology applied to the short-axis images, we cropped the long-axis images to 96\u00d796 pixels. This adjustment was necessary due to the larger heart region captured in this view, ensuring that the entire heart was included within the cropped area.\nIn the Contrastive ECG-CMR pre-training and downstream tasks, we employed several data augmentation techniques. For the ECG data, we implemented crop resizing, time reversal (TimeFlip), and sign inversion (SignFlip). These transformations were followed by Min-Max scaling within a range of -1 to 1 on a channel-wise basis. During validation and testing, only Min-Max scaling was applied. For the CMR data, we utilized random rotation (up to 30 degrees), random horizontal and vertical flips, and random resized cropping with a scale range of 0.8 to 1.0 and an aspect ratio of 0.9 to 1.1. This was succeeded by resizing the images to 256\u00d7256 pixels using bilinear interpolation. Normalization was applied across all 50 frames of both short-axis and long-axis images, using a mean of 0.5 and a standard deviation of 0.5. During validation and testing, only normalization and resizing were performed to ensure consistency in the evaluation process.\nDuring the training phase of the diffusion model, we focused on preserving critical data features, including the waveform characteristics of the ECG and the orientation of the heart in the CMR images. As such, we applied only Min-Max scaling to the ECG data and resizing to the CMR images.\nSelf-supervised method for CMR\nWe initially employed self-supervised learning on the CMR images to obtain robust representations for subsequent Contrastive ECG-CMR pre-training, as shown in Fig. 6a. The Swin Transformer base served as the backbone, utilizing GreenMIM 29 for masked self-supervised training. The encoder patch size was set to 4\u00d74, with a window size of 7. For both long-axis and short-axis CMR images, the temporal dimension (50 frames) was treated as the number of channels in the 2D images, with masking applied separately to each channel. In addition to the reconstruction loss aimed at restoring the original images, we introduced contrastive learning losses post-encoder for both long-axis and short-axis images to align and enhance the information between the two. Qualitative results can be found in Extended Fig. 2. The masking ratio was set to 0.75 during model training on an 80GB A800 GPU, utilizing a batch size of 32 over 400 training epochs. The first 40 epochs focused on learning rate warm-up, gradually increasing the learning rate from 0 to 1\u00d710-4. The final model weights were saved as checkpoints for downstream tasks.\nContrastive ECG-CMR Pre-training\nWe employed a ViT-Large\u00b3\u00b9 model as the ECG encoder and utilized a frozen, self-supervised Swin Transformer as the CMR encoder, as shown in Fig. 6b. The ECG encoder processes data in the format [b, 12, 5000], where 12 corresponds to the 12 ECG leads, and 5000 represents 10 seconds of data at 500 Hz. This data is treated as a 1-channel image with dimensions 12\u00d75000, and we applied a ViT model with a patch size of (1,100), resulting in 600 patches. Standard ViT processing is subsequently applied to these patches. For the CMR model, we utilize the encoder from the aforementioned self-supervised model, while keeping its parameters frozen. During training, the features encoded from the ECG are compared with those from both long-axis and short-axis CMR images in a contrastive learning setup, with the total loss being the sum of these two comparisons. The model was trained on an 80G A800 GPU with a batch size of 8, with the first 40 epochs dedicated to learning rate warm-up (from 0 to 1\u00d710-4). The model checkpoint with the lowest validation loss was saved for downstream tasks.\nECG2CMR Diffusion Model\nWe adapted the architecture of a text-to-video model 26 to develop our ECG2CMR model,, as shown in Fig. 6c. It's a latent diffusion model comprising three main components. The first component is an autoencoder that transforms CMR data into a low-dimensional latent space and subsequently decodes it back to pixel space. For this purpose, we utilized a pretrained model on natural images, keeping its parameters frozen. The second component is a denoising U-Net, which exhibits slight variations from the standard U-Net used in text-to-image models. It comprises four key structures: the initial block, down-sampling blocks, spatiotemporal blocks, and up-sampling blocks. The initial block projects the input into the embedding space, while the down-sampling and up-sampling blocks adjust the spatial resolution of the feature maps. The spatiotemporal block is crucial for capturing complex spatial and temporal dependencies in the latent space, thereby enhancing the quality of the synthesized CMR images.\nThis U-Net was initialized using pre-trained weights from a text-to-video diffusion model. The third component, which distinguishes our approach from typical text-to-video models, is the conditional encoder. We employed the frozen ECG encoder from the previously pre-trained ECG model as the conditional encoder, utilizing cross-attention to guide the training of the diffusion model. Due to the significant differences between long-axis and short-axis CMR images, we trained two separate models to generate each type of CMR image.\nIn the forward process of diffusion model training, when a CMR image xo~p(x) is input, it first passes through the latent encoder E, and gets the latent features zo = E(xo). Assuming zo ~ q(z), the diffusion process adds Gaussian noise incrementally over T timesteps, yielding latent vectors Zo, Z\u2081,.\u2026\u2026,ZT, governed by the following distributions:\nq(ZtZt-1) = N(zt; \u221a1 - BtZt-1, BtI)\nq(zt|zo) = N(zt; \u221a\u0101tzo, (1 \u2013 \u0101t)I)\nwhere at = 1 - \u03b2t, \u1fb6t = \u03a0\u1f31=1 \u03b1\u00a1, and \u1e9et is the noise schedule, linearly increasing from 0.0001 to 0.02. Ast \u2192 \u221e, zt approaches pure Gaussian noise. The reverse process aims to recover zo from zy. Using Bayes' rule, the reverse distribution is:\nq(Zt-1|2t, zo) = N(zt\u22121; \u03bc(zt, zo), \u03b2\u03b5I)\nwhere (zt, zo) is predicted by the neural network U\u016f (zt, t, c), and c represents the conditional features from the ECG conditional encoder. Finally, the latent vector Zo is decoded back to pixel space Xo via the latent decoder. The training objective is:\nLLDM = E\u025b(x),y,\u20ac~N(0,1),t []\u20ac \u2013 \u20ac\u0473 (Zt, t, T\u0473(y))[2]\nwhere x is the CMR images, y is the ECG signal, & is the latent encoder, and \u2208e is the noise predicted by the Unet. The training was conducted on a single 80G A800 GPU, with a batch size of 4 and a learning rate maintained at 3e-6. After each epoch, the model was evaluated on the validation set, and the weights corresponding to the lowest loss were saved as model checkpoints for CMR generation. During inference, CMR images were generated by sampling from Gaussian noise and iteratively denoising under the guidance of the ECG, utilizing the DDIM sampling method\u00b3, which required around 100 steps to produce a single image.\nDownstream tasks\nIn adapting to downstream tasks, we utilized only the ECG encoder from the Contrastive ECG-CMR Pre-training model, excluding the CMR encoder, as shown in Fig. 6d. The ECG encoder generates high-level features from ECG signals that embed CMR information. Additionally, we incorporated demographic and lifestyle factors specific to each dataset through feature modulation for joint training. For the UKB dataset, these factors included age at assessment center visit, frequency of alcohol intake, body mass index (BMI), days per week engaging in moderate-intensity physical activity lasting at least 10 minutes, sex, sleep duration, smoking status, standing height, stress levels/high stress, weight, intake of raw vegetables, days per week walking at least 10 minutes, pork intake, insomnia, and average heart rate. For other datasets, the factors used were sex, age, and average heart rate. For binary classification tasks, we followed the methodology outlined in references50, maintaining a balanced ratio of approximately 1:1 for positive and negative samples. For the CMR generation model, we employed the pre-trained diffusion model (Fig. 6c) to generate high-fidelity CMR images containing authentic cardiac information, offering clinicians valuable visual and interpretive insights.\nIn our experiments, we fine-tuned the entire ECG encoder to accommodate the varying data distributions of each task, except for the external validation datasets where the trained model was directly used for inference. The training objective was to generate classification outputs that aligned with CVD labels or regression outputs that closely approximated the ground truth values of 82 cardiac structural indicators. We set the batch size to 10 and allowed for a maximum of 400 training epochs. To prevent unnecessary training, we employed early stopping, dedicating the first 10 epochs to a learning rate warm-up from 0 to 1\u00d710-4, followed by cosine annealing to 0. After each epoch, the model was evaluated on the validation set, and the weights corresponding to the highest AUROC on the validation set were saved as checkpoints for internal evaluation across datasets.\nQuantitative assessment and statistical analysis\nFor classification tasks, we employed several evaluation metrics: area under the curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). In our three-class classification, we binarized the labels using a one-vs-rest approach prior to evaluation. For regression tasks related to cardiac indices, we used the Pearson correlation coefficient as the primary metric. To compute 95% confidence intervals, we applied the Wilson Score Interval 54 for classification metrics and the bootstrap method 55 for regression metrics. For method comparisons, we utilized the DeLong test for AUC, while other metrics were assessed using two-sided z-tests.\nSoftware for data process and model development\nWe utilized several libraries for data processing and model development, including numpy (version 1.25.2)56, sklearn (version 1.1.1)57, scipy (version 1.11.2)58, simpleITK (version 2.3.1)59 and pandas (version 2.2.1). For model development, we employed pytorch (version 1.11.0)."}]}