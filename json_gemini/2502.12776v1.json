{"title": "Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models", "authors": ["Daiki Chijiwa", "Taku Hasegawa", "Kyosuke Nishida", "Kuniko Saito", "Susumu Takeuchi"], "abstract": "While foundation models have been exploited for various expert tasks through fine-tuning, any foundation model will become outdated due to its old knowledge or limited capability. Thus the underlying foundation model should be eventually replaced by new ones, which leads to repeated cost of fine-tuning these new models. Existing work addresses this problem by inference-time tuning, i.e., modifying the output probabilities from the new foundation model with the outputs from the old foundation model and its fine-tuned model, which involves an additional overhead in inference by the latter two models. In this paper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT), that reduces the inference overhead by its nature, based on the reformulation of fine-tuning as the reward maximization. Specifically, instead of fine-tuning parameters of the foundation models, PRT trains the reward model explicitly through the same loss function as in fine-tuning. During inference, the reward model can be used with any foundation model (with the same set of vocabularies or labels) through the formulation of reward maximization. Experimental results, covering both vision and language models, demonstrate that the PRT-trained model can achieve comparable accuracy to the existing work of inference-time tuning, with less inference cost.", "sections": [{"title": "1. Introduction", "content": "Foundation models, or simply pretrained models, play a central role in the recent development of artificial intelligence. They are typically large-scale neural networks pretrained on massive amounts of data from the Internet, which makes them generalizable to vari-"}, {"title": "2. Background", "content": "In this section, we summarize the background of this work. In Sec. 2.1, we review the basics of KL-regularized Reward Maximization from the literature of Decision-Making Theory and Reinforcement Learning. In Sec. 2.2, we briefly explain previous work on inference-time tuning with its formulation based on reward maximization."}, {"title": "2.1. KL-Regularized Reward Maximization", "content": "Let S be a state space, A be an action space, and a policy model $\\pi(a|s)$ be the probability that the action $a \\in A$ is chosen by some probabilistic mapping $S \\rightarrow A$ given the state $s \\in S$. Furthermore, we consider a reward function $r(s,a): S \\times A \\rightarrow \\mathbb{R}$ and a reference model $\\pi_{ref}(a|s)$. Then the goal of KL-regularized reward maximization is to maximize the expected reward $\\mathbb{E}_{a \\sim \\pi(a|s)}[r(s,a)]$ with a soft constraint that keeps $\\pi(a|s)$ close to the reference model $\\pi_{ref}(a|s)$, in the sense of KL divergence, as follows:\n$\\max_{\\pi(\\cdot|s)} \\mathbb{E}_{a \\sim \\pi(a|s)}[r(s, a)] - \\lambda D_{K L}(\\pi(\\cdot|s) || \\pi_{r e f}(\\cdot|s)),$ (1)\nwhere $D_{K L}(\\pi(\\cdot|s) || \\pi_{r e f}(\\cdot|s)):= \\sum_{a} \\pi(a|s) \\log(\\pi(a|s)/ \\pi_{r e f}(a|s))$. The closed-form solution of this problem is well-known as an extension of maximum entropy principle , given by\n$\\pi(a|s)=\\frac{1}{Z(s)} \\pi_{r e f}(a|s) \\exp \\left(\\frac{r(s, a)}{\\lambda}\\right),$ (2)"}, {"title": "2.2. Inference-Time Tuning", "content": "Let $\\pi_{\\theta}(y|x)$ be a classification model defined as $\\pi_{\\theta}(y|x):=\\operatorname{softmax}(f(x; \\theta))$, where $x \\in \\mathcal{X}$ is an input, $y \\in \\mathcal{Y}=\\{c_1, ..., c_L\\}$ a classification label, $f(x;\\theta) \\in \\mathbb{R}^{L}$ a neural network parameterized by $\\theta \\in \\mathbb{R}^{N}$, and the softmax function\u00b9. Let $\\pi_{p t}(y|x):=\\pi_{\\theta_{p t}}(y|x)$ be a pretrained model, and $\\pi_{f t}(y|x):=\\pi_{\\theta_{f t}}(y|x)$ its fine-tuned model on some specific task. Typical examples include (i) in image classification, $x$ is an image, $y$ is its corresponding label and $f(x; \\theta)$ is a CNN or Vision Transformer ; and (ii) in language generation, $x$ is a sequence of tokens $t_1 \\ldots t_k$, $y$ is the next token $t_{k+1}$ and $f_{\\theta}(x)$ is a decoder-only Transformer.\nBased on the theory of KL-regularized reward maximization, Mitchell et al. proposed an emulated fine-tuning (EFT) which views the fine-tuned model $\\pi_{f t}(y|x)$ as the solution of the following problem:\n$\\max _{\\pi(\\cdot | x)} \\mathbb{E}_{y \\sim \\pi(y/x)} \\left[\\log \\left(\\frac{\\pi_{f t}(y | x)}{\\pi_{p t}(y | x)}\\right)\\right]-\\lambda D_{K L}(\\pi(\\cdot | x) || \\pi_{p t}(\\cdot | x)),$ (3)\nwhere $\\log (\\pi_{f t}(y|x) / \\pi_{p t}(y|x))$ is called an implicit reward, expected to be the reward function that reflects the task-specific preference for $y \\in \\mathcal{Y}$. Indeed, by applying (2), the closed-form solution $\\pi(y|x)$ of (3) is given by\n$\\pi(y | x)=\\frac{1}{Z(x)} \\pi_{p t}(y | x) \\exp \\left(\\frac{1}{\\lambda} \\log \\left(\\frac{\\pi_{f t}(y | x)}{\\pi_{p t}(y | x)}\\right)\\right)=\\pi_{f t}(y | x)$ (4)\nBuilding on this fact, Mitchell et al. also proposed what they call scale decoupling, which replaces the pretrained model in the KL constraint by different pretrained model from the one appeared in the implicit reward. In other words, they consider the following problem:\n$\\max _{\\pi(\\cdot | x)} \\mathbb{E}_{y \\sim \\pi(y/x)} \\left[\\log \\left(\\frac{\\pi_{f t}(y | x)}{\\pi_{p t}(y | x)}\\right)\\right]-\\lambda D_{K L}(\\pi(\\cdot | x) || \\tilde{\\pi}_{p t}(\\cdot | x)),$ (4)\nwhere $\\tilde{\\pi}_{p t}(\\cdot | x)$ is another pretrained model that is different from $\\pi_{p t}(\\cdot | x)$, possibly with different network architecture. The closed-form solution of (4) can be considered as the emulation result of fine-tuning the new pretrained model $\\tilde{\\pi}_{p t}(y|x)$ through the implicit reward on the specific task. Also, Liu et al. proposed almost the same approach called proxy-tuning.\n\u00b9softmax$\\left(y_{1}, \\ldots, y_{L}\\right):=\\left(e^{y_{1}} / \\sum_{i} e^{y_{i}}, \\ldots, e^{y_{L}} / \\sum_{i} e^{y_{i}}\\right)$"}, {"title": "3. Portable Reward Tuning", "content": "In this section, we develop a new fine-tuning framework, called portable reward tuning (PRT), with both training and inference algorithms based on KL-regularized reward maximization. Throughout this section, we follow the same classification setting as in Sec. 2.2, which includes both image classification and language generation tasks."}, {"title": "3.1. Training of PRT", "content": "Setup. Let $r(x; \\theta)=\\left(r_{1}(x; \\theta), \\ldots, r_{L}(x; \\theta)\\right) \\in \\mathbb{R}^{L}$ be a neural network with $L$-dimensional outputs. We refer to the $i$-th component of $r(x; \\theta)$ as the reward value, denoting $r_{\\theta}(x, c_i):=r_{i}(x; \\theta)$ for an input $x$ and the $i$-th label $c_{i} \\in \\mathcal{Y}$. We also assume that a pretrained model $\\pi_{p t}$ and a dataset of input-label pairs $\\mathcal{S}=\\{(x_{1}, y_{1}), \\ldots, (x_{|\\mathcal{S}|}, y_{|\\mathcal{S}|})\\}$ for some specific task are given.\nFormulation. In our PRT framework, we optimize the reward model $r_{\\theta}(x, y)$, instead of directly optimizing the given pretrained model $\\pi_{p t}(y|x)$. For a little while, assume that we already have the learned reward model $r_{\\theta}(x, y)$ for the given specific task. Then, the desired classification model $\\pi_{\\theta}(y|x)$ is defined as the solution of reward maximization with KL-constraint to the pretrained model:\n$\\max _{\\pi(\\cdot | x)} \\mathbb{E}_{y \\sim \\pi(y/x)}[r_{\\theta}(x, y)]-\\lambda D_{K L}(\\pi(\\cdot | x) || \\pi_{p t}(\\cdot | x)).$ (5)\nAs we already discussed in Sec. 2.1, the closed-form solution for this maximization problem is provided by\n$\\pi_{\\theta}(y | x)=\\frac{1}{Z_{\\theta}(x)} \\pi_{p t}(y | x) \\exp \\left(\\frac{r_{\\theta}(x, y)}{\\lambda}\\right),$ (6)\nwhere $Z_{\\theta}(x):=\\sum_{y} \\pi_{p t}(y | x) \\exp (r_{\\theta}(x, y) / \\lambda)$ is the normalization factor. We call this $\\pi_{\\theta}(y|x)$ a PRT model (for training) with the reward $r_{\\theta}(x, y)$ and the pretrained model $\\pi_{p t}(y|x)$.\nAlthough it is somewhat obvious, the following proposition guarantees that the above PRT models are equivalent to standard fine-tuned models in terms of their expressiveness:\nProposition 3.1. There is a one-to-one correspondence between fine-tuned models and rewards, which preserves their accuracy:\n$\\left\\{\\pi_{f t}(y | x): \\text { fine-tuned models }\\right\\} \\rightarrow\\left\\{r(y | x): \\text { rewards satisfying } \\mathbb{E}_{y \\sim \\pi_{p t}(y|x)}[r(x, y)]=1\\right\\}$\nwhere $\\pi_{f t}(y | x)$ is mapped to the implicit reward $\\log (\\pi_{f t}(y|x) / \\pi_{p t}(y|x))$.\nProof. The mapping preserves accuracy since the PRT model (6) with the implicit reward recovers the given model"}, {"title": "3.2. Inference of PRT", "content": "Let $r_{\\theta}(x, y)$ and $\\pi_{p t}(y|x)$ be the reward and pretrained model introduced in Section 3.1. Let $\\tilde{\\pi}_{p t}(y|x)$ be another pretrained model whose label space $\\mathcal{Y}$ (or vocabularies for language models) is the same as the one for $\\pi_{p t}(y|x)$. Examples of $\\tilde{\\pi}_{p t}(y|x)$ include a model pretrained on larger or more recent dataset than that of $\\pi_{p t}(y|x)$, and a pretrained model with more parameters.\nThe inference model $\\pi_{\\theta}(y|x)$ for the reward $r_{\\theta}(x, y)$ and the specified pretrained model $\\tilde{\\pi}_{p t}(y|x)$ can be derived by replacing $\\pi_{p t}(y|x)$ in (6) with $\\tilde{\\pi}_{p t}(y|x)$. Specifically, given an input $x \\in \\mathcal{X}$, the prediction for its label $y$ is performed by the following model that maximizes the reward $r_{\\theta}(x, y)$ while minimizing the deviation from the specified pretrained model $\\tilde{\\pi}_{p t}(y|x)$:\n$\\tilde{\\pi}_{\\theta}[y | x]:=\\arg \\max _{\\pi(\\cdot | x)} \\mathbb{E}_{y \\sim \\pi(y/x)}[r_{\\theta}(x, y)]-\\lambda D_{K L}(\\pi(\\cdot | x) || \\tilde{\\pi}_{p t}(\\cdot | x))=\\frac{1}{\\tilde{Z}_{\\theta}(x)} \\tilde{\\pi}_{p t}(y | x) \\exp \\left(\\frac{r_{\\theta}(x, y)}{\\lambda}\\right),$ (10)\nwhere $\\tilde{Z}_{\\theta}(x):=\\sum_{y} \\tilde{\\pi}_{p t}(y | x) \\exp (r_{\\theta}(x, y) / \\lambda)$. The implementation of inference by this PRT model (10) is straightforward as described in Algorithm 2, with $\\lambda=1$ as in training.\nNow the following question naturally arises: How does the choice of $\\tilde{\\pi}_{p t}(y|x)$ affect the behavior of the inference model $\\pi_{\\theta}(y|x)$? Intuitively, if $\\tilde{\\pi}_{p t}(y|x)$ does not deviate from the original $\\pi_{p t}(y|x)$, the inference model $\\pi_{\\theta}(y|x)$"}, {"title": "3.3. A PAC-Bayesian Perspective", "content": "We can suppose that the pretrained models for both training and inference, i.e., $\\pi_{p t}(y|x)$ and $\\tilde{\\pi}_{p t}(y|x)$, are chosen from some distribution $P$ over the set of pretrained models. Then the PRT models for inference, $\\pi_{\\theta}(y|x)$ combined with the sampled pretrained model $\\tilde{\\pi}_{p t}(y|x) \\sim P$, form a new distribution $Q_{\\theta}$.\nThis formulation of PRT models naturally fits into the PAC-Bayes framework established in McAllester , which enables us to analyze the generalization error of posterior distributions over classifiers, in comparison to a fixed prior distribution. Specifically in our setting, the pretrained distribution $P$ can be seen as a prior distribution, and the PRT distribution $Q_{\\theta}$ as a posterior distribution.\nLet $l(x, \\pi)$ be a finitely bounded loss function, e.g., one that returns the error rate, for a given input $x$ and classifier $\\pi$. Assume that the input $x$ follows some distribution $\\mathcal{D}$. The following generalization bound can be obtained as a direct consequence of Theorem 1 in McAllester:\nProposition 3.3. Let $\\mathcal{S}=\\left(x_{1}, \\ldots, x_{m}\\right) \\sim \\mathcal{D}^{m}$ be i.i.d. $m$ training samples from the data distribution $\\mathcal{D}$. Then, with probability at least $1-\\delta$, we have\n$\\mathbb{E}_{\\pi \\sim Q} \\mathbb{E}_{x \\sim \\mathcal{D}}[l(x, \\pi)] \\leq \\mathbb{E}_{\\pi \\sim Q_{\\theta}}\\left[\\frac{1}{m} \\sum_{i=1}^{m} l\\left(x_{i}, \\pi\\right)\\right]+\\sqrt{\\frac{D_{K L}\\left(Q_{\\theta} || P\\right)+\\log (\\frac{1}{\\delta})+\\log m}{2 m}},$ (11)\nfor the posterior distribution $Q_{\\theta}$ with the reward $r_{\\theta}$ trained on $\\mathcal{S}$.\nThe KL divergence $D_{K L}\\left(Q_{\\theta} || P\\right)$ is not computationally tractable because the pretrained distribution $P$ itself is not tractable and also the underlying space of pretrained models is too vast. Nevertheless, the result implies that the generalization capability of PRT models can be captured by the closeness of the PRT model $\\pi_{\\theta}(y|x)$ compared to the underlying pretrained model $\\tilde{\\pi}_{p t}(y|x)$."}, {"title": "4. Experiments", "content": "In this section, we evaluate the performance of portable reward tuning (PRT) for inference-time tuning, with various pretrained models including both vision and language models. The main comparison is between PRT and its corresponding baseline, emulated fine-tuning (EFT; Mitchell et al. ), which differ only in whether they maximize an explicit or implicit reward during inference-time tuning."}, {"title": "4.1. Results", "content": "Figures 2, 3 and 4 show the results of inference-time tuning using PRT and EFT, from a source pretrained model, i.e., the one used in tuning the (either explicit or implicit) reward, to a target pretrained model, i.e., the one never used in tuning the reward. Here PRT refers to the vanilla one without regularization for fair comparison. We also compare them"}, {"title": "4.2. Qualitative Analysis", "content": "To analyze the behavior of PRT in more detail, we examine the tokens generated by the model. In this analysis, we used Llama3-8B as the target model, while Llama-3.2-1B was"}, {"title": "4.3. Effects of Entropy Regularization", "content": "Here we employ Entropy Maximization (EM) regularization in PRT training, with its coefficient $\\alpha \\in \\mathbb{R}_{>0}$. In Section 3.3, the PAC-Bayesian analysis indicated that the generalization capability of PRT models is affected by the entropy of the reward distribution $p_{\\theta}(y|x)$ defined by the reward model $r_{\\theta}(x, y)$. In Figure 6 and 7, we empirically analyze its effect by varying $\\alpha$. First of all, we observed that the EM regularization generally suppresses the accuracy of the reward itself (Fig. 6a) as $\\alpha$ increases, which can be naturally expected. However, interestingly, the performance of PRT (Fig. 6b, 7) is not degraded even with such rewards, but also sometimes boosted regardless of the accuracy of rewards themselves. The regularization itself may not be yet practical since the optimal $\\alpha$ tends to be dependent on tasks and pretrained"}, {"title": "4.4. Memory and Speed Analysis", "content": "Since PRT introduces an auxiliary model as the reward for both training and inference, we investigate how the memory usage and speed increase or decrease, compared to standard fine-tuning in training and EFT in inference. For training, Table 1 in Appendix shows that, while training time slightly increases due to the auxiliary model, the increase in memory usage is relatively negligible because the pretrained model in PRT does not require back-propagation. For inference, Tables 2 and 3 show that PRT successfully reduces both inference speed and memory usage compared to EFT, which highlights the benefit of employing explicit reward models."}, {"title": "5. Related Work", "content": "Tuning by Refining Predictions. In this paper we focused on inference-time tuning accomplished by refining predictions from the underlying pretrained model. In particular, our baseline is the emulated fine-tuning (EFT; Mitchell et al. ) which established the interpretation of inference-time tuning based on KL-regularized reward maximization. Parallel work by Liu et al. also proposed an essentially same method called proxy-tuning. While these previous work focused on inference-time tuning with pretrained models that only differ in their model scale, we examined a more general setting that the pretrained models may differ even in their architectures or pretraining datasets. Also, the previous work assumed the fine-tuned model was prepared in advance, and explored how to exploit it for a new pretrained model. In contrast, we reexamined the assumption and explored an alternative to fine-tuning that is more suitable for inference-time tuning. As a consequence, at the cost of a little overhead in training, our approach successfully halves the overhead in inference-time tuning.\nThe literature of controlled text generation is also related to our work, but they have explored specific methods for attribute-conditioned text generation, which requires a classifier for some attributes, rather than fine-tuning for general tasks as in our work. In particular, Deng & Raffel proposed to control language models with reward models similarly to our work, but which are trained with a manually-designed loss specific to text classification tasks, while our method employs standard loss for fine-tuning and thus can be naturally applied to broader domains and tasks such as vision classification and instruction tuning.\nTuning by Editing Parameters or Activations. Another possible approach for inference-time tuning would be di-"}, {"title": "6. Conclusion", "content": "In this paper, we introduced a new fine-tuning principle called portable reward tuning (PRT) as an alternative to"}]}