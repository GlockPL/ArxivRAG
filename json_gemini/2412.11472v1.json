{"title": "Leveraging Foundation Language Models (FLMs) for Automated Cohort Extraction from Large EHR Databases", "authors": ["Purity Mugambi", "Alexandra Meliou", "Madalina Fiterau"], "abstract": "A crucial step in cohort studies is to extract the required cohort from one or more study datasets. This step is time-consuming, especially when a researcher is presented with a dataset that they have not previously worked with. When the cohort has to be extracted from multiple datasets, cohort extraction can be extremely laborious. In this study, we present an approach for partially automating cohort extraction from multiple electronic health record (EHR) databases. We formulate the guided multi-dataset cohort extraction problem in which selection criteria are first converted into queries, translating them from natural language text to language that maps to database entities. Then, using FLMs, columns of interest identified from the queries are automatically matched between the study databases. Finally, the generated queries are run across all databases to extract the study cohort. We propose and evaluate an algorithm for automating column matching on two large, popular and publicly-accessible EHR databases MIMIC-III and eICU. Our approach achieves a high top-three accuracy of 92%, correctly matching 12 out of the 13 columns of interest, when using a small, pre-trained general purpose language model. Furthermore, this accuracy is maintained even as the search space (i.e., size of the database) increases.", "sections": [{"title": "1. Introduction", "content": "Cohort extraction is a crucial step in clinical and health informatics research, and especially, in observational retrospective exploratory studies. Typically, when conducting these studies researchers have access to one or more databases from which they extract the cohort of interest using predetermined inclusion/exclusion criteria. It is also common for these studies to have a primary dataset from which the key findings are learned and a secondary dataset which is used to validate those findings. EHR databases are the main data sources for this research.\nEHR databases contain real-world data and are attractive for retrospective research because their data is abundant, immediately available, and researchers have high flexibility in terms of the kinds of analyses they can conduct on that data Gehrmann et al. (2023). However, data preparation costs are a huge challenge when using EHR data Gehrmann et al. (2023). Data heterogeneity Sarwar et al. (2022); Kreuzthaler et al. (2015), lack of standardization in clinical terms and database entities such as table and column names Reisman (2017) and database sizes, are some of the reasons data preparation is time consuming. For example, the popular EHR databases used in this study, MIMIC-III and eICU, have 26 and 31 tables, with a combined total of 324 and 391 columns respectively. In studies where researchers have to extract cohorts multiple times, such as, in health equity studies Toseef et al. (2022); Claxton et al. (2019); Mugambi et al. (2023), or in multi-"}, {"title": "2. Related work", "content": "Cohort discovery via disease phenotyping: In this research area, patient cohorts are discovered in data through disease phenotyping. Cohorts can be manually curated from a specific criteria that is developed by advanced researchers for identifying a disease. They are often presented as a set of rules Liao et al. (2015). The criteria can also be converted into algorithms, aggregated and offered as the gold standard; for instance, PheKB Kirby et al. (2016). This approach is time consuming and has motivated the development of automated methods for cohort discovery which are largely based on representation learning. To discover cohorts, these models learn embeddings for each patient and then group patients with similar embeddings together (based on some distance metric) and/or use key medical concepts to retrieve patient groups. Due to the versatility of representation learning methods, automated tools for cohort discovery have been developed for a wide range of applications including; general disease risk prediction Miotto et al. (2016), identification of patient cohorts by learning disease phenotypes Kandula et al. (2011); Yu et al. (2015); Glicksberg et al. (2018), identifying computational structures of multiple diseases Pivovarov et al. (2015), and learning cohort definitions as a side effect of bulk database phenotyping Chiu and Hripcsak (2017).\nThe present study differs from this class of work in three ways. First, the problems, while related, are different. Research on cohort discovery learns a criteria that identifies a specific disease phenotype. We developed a method for retrieving a cohort from a database when presented with a selection criteria."}, {"title": "3. Methods", "content": "This section discusses the problem components withholding implementation details which are discussed in Section 3.2.\nContext: A researcher has inclusion/exclusion criteria and multiple databases. The researcher has, at a minimum, a high-level understanding of the structure (i.e., tables, their columns, and data types) of at least one of the databases, henceforth referred to as the reference database. This is a reasonable assumption because the structure of the database is provided as metadata, and/or through the dataset documentation. Furthermore, many researchers use their institutional databases for observational studies and would typically know their structure. Additionally, technical support, including cohort discovery tools, are made available to assist researchers in quickly understanding the structure of many databases. The researcher wants to use the criteria to extract a cohort from the reference database(s) and other databases they have access to, henceforth referred to as unknown databases.\nWe formulate guided cohort extraction as a three-step problem: (1) break down the criteria into a set of queries, (2) match columns of interest in the reference database(s) to their counterparts in the unknown database(s), and (3) execute the selection criteria queries on all databases to obtain the cohorts."}, {"title": "3.1.1. BREAK DOWN CRITERIA TO QUERIES", "content": "Suppose the inclusion criteria is: patients with a diagnosis of Asthma, aged 20-35, who were prescribed inhaler corticosteroids. A researcher could write three SQL queries;"}, {"title": "3.1.2. \u039c\u0391TCH COLUMNS OF INTEREST", "content": "From step one above, we have translated the criteria into tables and columns. In this step, we first identify those tables and columns in the reference database, then, we find the best match(es) for each of the identified columns from the unknown database(s). For instance, using the first query from step one above, the researcher would look through their reference database and find which table contains diagnosis information, and which column in that table has the diagnosis text. If the diagnosis information is in a column named diagnosis Text, in this step, the best match (e.g., diagnosis) would be obtained from an unknown database. This study develops and evaluates an algorithm to match columns across databases, which is discussed in detail in Section 3.2."}, {"title": "3.1.3. EXECUTE QUERIES ON ALL DATABASES", "content": "Finally, since all the relevant tables and columns of the unknown database have been identified, the queries developed in part one can be executed to obtain the cohort. This problem is also typical in observational studies, therefore we do not develop methods for it.\nFormulating the problem in this way allows us to:\n1. leverage the researcher's knowledge of their current database to rapidly understand the new unknown database(s), lowering the barrier for multi-dataset analyses.\n2. use the structure of the known database as a guide to find relevant tables and columns in unknown databases. This reduces errors and computation time, which would otherwise occur if one was searching unguided across the entire database."}, {"title": "3.2. Column matching", "content": "In this paper, we propose a solution for the second part of the problem; i.e., automating column matching, and evaluate it in isolation. In the future, the proposed solution would be used as part of a complete system that also includes existing solutions for parts 1 and 3."}, {"title": "3.2.1. ALGORITHM", "content": "We developed a column matching algorithm (Algorithm 1) that takes as input a list of all the columns from all the databases; reference and unknown. For each column, using a pre-trained LM, we generate a vector embedding of its values then aggregate them into a mean embedding for that column (line 2). Next, we compute similarity scores (i.e., distances between the mean vector embeddings) of all columns using cosine-similarity (equation 1, line 3) and return the best k matches (line 4). k is a user provided integer argument specifying how many top matches should be returned.\n$\\displaystyle c o s s i m ( x , y ) = \\frac{x^T y}{||x|| ||Y||}$  (1)\nWhereas column data is most informative when finding matches, metadata (such as column name and data type) could provide extra context essential in differentiating between close matches. To examine the effect of metadata in disambiguating between matches, we first obtain top M matches whose similarity distance is greater than or equal to some threshold (line 6). Then, for each of these matches, we separately encode the column name, its datatype, and table names for which it appears (line 7). Finally, we compute similarity scores based on the metadata embedding vectors and return the best k matches (lines 8-9). The computational efficiency of the algorithm is linear in the number of columns, L."}, {"title": "3.2.2. IMPLEMENTATION", "content": "Code was written using Python 3.10. We used the Bidirectional Encoder Representations from Transformers (BERT) FLM to generate the embeddings. BERT can be accessed in various forms; we chose sentence-BERT (SBERT) Reimers and Gurevych (2019) because it contains the state of the art image and text embeddings and is readily available as a module in Python. Within SBERT, we picked the pre-trained all-MiniLM-L6-v2 model because it was trained on a large dataset (1 billion training pairs) and is considered a general purpose use model sbe. Unlike other general purpose models trained on the same size of data, all-MiniLM-L6-v2, henceforth referred to as SBERT-model, is five times faster sbe.\nThe SBERT-model generates a 384-dimensional vector embedding for each input. In this study, we treated each column value as an independent sentence and obtained a vector embedding for it. To reduce memory usage and computation time we only encoded unique values in each column. Whereas SBERT is trained to encode text/image data, we hypothesized that it would generate semantically meaningful embeddings for numerical data, and used it to also encode floats, dates, and integers. Prioritizing the development of a flexible method that could deal with data variety, we chose to use one model (i.e., SBERT-model) and cast everything to text, especially because our prior experience with MIMIC had showed us that several columns can have mixed numerical and text data.\nWe ran the models on a compute cluster with 100-500GB RAM and 2 5 GPUs. Despite only encoding unique values in each column, we processed input data in chunks to ensure that the raw data and their corresponding large embeddings could fit in memory. Data was processed using the Pandas library and intermittent results written to HDF5 files."}, {"title": "3.3. Experimental setup", "content": "We conducted experiments on two large, publicly-accessible EHR datasets; MIMIC-III and eICU, with MIMIC-III as the reference database and eICU the unknown. To evaluate the accuracy of the match algorithm, we manually identified matches for columns in MIMIC-III in eICU. As stated in Section 3.1, this method would be applied to select a cohort based on a criteria, typically to answer a specific research question. Therefore, to evaluate the proposed solution, we developed the usecase below.\nResearch question: Are there differences in pharmacological treatment of acute myocardial infarction (AMI) by patient sex, race, age, and type of insurance?\nInclusion criteria: Adult patients with a primary diagnosis of AMI who have no known co-morbidities and were prescribed drugs in at least one of the four drug-classes for standard pharmacological treatment of AMI; Angiotensin-converting-enzyme (ACE) inhibitors, statins, beta-blockers and anticoagulants/antiplatelets.\nBy breaking down the inclusion criteria into queries, we identified <diagnosis-table>, <diagnosis-column>, <diagnosis-priority-column>, <past-medical-history-table>, <past-medical-history-value-column>, <prescriptions-table>, <drug-name-column>, <patient-information-table> and <age-column> as the tables and columns important to meet the inclusion criteria. In addition, <sex-column>, <insurance-column> and <race/ethnicity-column> are also required to answer the question."}, {"title": "3.4. Experiments", "content": "We first conducted experiments with a total of 52 columns; columns of interest from MIMIC-III and their equivalent matches from eICU, (32 in total and shown in Table 2), and a further 20 randomly-selected columns from eICU. We then increased the number of randomly-selected columns from eICU to 30, 50, 70, 90, 100, and eventually all columns to examine how the results change when a few reference columns need to be matched against an increasing size of the unknown database. When including metadata in the matching, we set the threshold for top-ranked matches based on column-values alone (line 6 in Algorithm 1) to the lowest value for which the column had matches. Experiments were repeated for k \u2208 [1-3]."}, {"title": "EVALUATION METRIC", "content": "Similar to prior studies, such as Dong et al. (2023), we evaluate the accuracy of the algorithm by determining whether the \"true\" match is returned for each column. Whereas most prior work Dong et al. (2021); Fan et al. (2023); Dong et al. (2023) is evaluated on web datasets where a column can have multiple positive matches, our application expects a single match from the unknown database for each column in the reference database. This is because EHR databases are typically normalized and rely on foreign keys to connect tables unlike web datasets where data files are typically standalone. Therefore, our problem is harder which led us to compute accuracy at k = 1, k = 2 and k = 3. That is, we assign a score = 1 if the \"true\" match is among the returned matches for each value of k. This way, k = 1 is the hardest case, and the problem gets easier with increasing values of k. While k = 1 is the ideal solution, small values of k (for instance, k \u2208 [2 \u2013 10]) significantly reduce the search space for the researcher. We report the number of columns that were correctly matched for each value of k, e.g., 10/13."}, {"title": "4. Results", "content": "When running experiments with 52 columns (i.e., only 20 additional columns are selected from eICU), the SBERT-model correctly identified 7 out of the 13 columns (53.8%; blue bars in Fig.1) among the top-3 matches when matching using column values only, and 12 (92.3%; orange bars in Fig.1) when metadata is used to sort among matches that have a similarity score of 0.4 and above."}, {"title": "4.2. Key takeaways", "content": "FLMs are suited for automated column matching. Through a small general-purpose pre-trained LM, we achieve high accuracy in column matching. Despite being trained on non-medical, text and image data, SBERT-model shows good performance for healthcare data and for non-text data types, specifically, dates and integers. Surprisingly, the performance improves only slightly when k > 1 indicating that SBERT-model is able to rank the \"true\" match at the top.\nMetadata often improve match accuracy. Inclusion of metadata can be useful in finding matches that are difficult to find using column values alone. Examples of columns that benefited from inclusion of metadata, icd9_code, gender, and dose_val_rx, have significantly different data formats between the two databases. In MIMIC-III, ICD9 codes are recorded as integers while in eICU they are a string combination of ICD 9&10 codes. For instance, in MIMIC-III, ICD9 code \"410.71\u201d is recorded as \u201c41071\" while in eICU it is recorded as \"410.71, I21.4\". Similarly, in MIMIC-III, gender is recorded as \u2018F\u2019or \u2018M', while in eICU it is \"female\" or \"male\". Using the column name the algorithm is able to pick up the correct matches for both of these columns from a list of other close matches. Metadata do not always guarantee improvement in performance, as shown by subject_id. When using values only, the LM is able to rank the correct match among the top three but when metadata is included it fails. The inclusion of extra information on column types and associated table names causes the algorithm to prioritize columns in eICU that appear in as many tables whose embeddings are close enough to the tables subject_id appears in.\nMetadata protect against decreased performance as the database grows. While the accuracy decreases as the size of the database grows, including metadata ensures that the accuracy is still relatively high, Fig. 2.\nColumn names alone are insufficient for column matching. While it may appear as though column names alone could help us achieve high match accuracy, that is not the case. As shown in Table 4, column names by themselves only helped match 3 of the 13 columns; ethnicity, icd_code and gender because these are named in the exact same way in both databases. However, for the rest of the columns matching by column names alone produces a large number of false positives, as shown in Appendix Table 8. Some of the reasons for this include: 1) short columns names (e.g., dob, drug) often have a high match rate with other columns because fewer characters are being used to match the sentences; 2) many columns can have a shared substring e.g., drug and drugrate have \"drug\" in them and would probably be matched by their column names despite containing different information; 3) columns can have similar names but represent different types of data. For instance, dischargelocation and hospitaldischargelocation are both columns in eICU representing the same information but in different data types."}, {"title": "4.3. Limitations and future work", "content": "This paper has not examined variance in algorithm efficiency because we believe it is out of scope and should be handled in a separate study. We also plan to examine how the algorithm performs when presented with a larger set of reference columns, for instance, by constructing and evaluating multiple selection criteria. Future work could also compare the performance of general-purpose LMs against LMs trained or fine-tuned on clinical data."}, {"title": "5. Conclusion", "content": "In this work, we presented an approach for automating matching of columns of EHR databases. Our results show that the proposed approach is a good solution for automated column matching. Not only is the match accuracy high, this performance persists even as the size of the unknown database increases. Additionally, our method can be run on clusters with limited compute resources (2GPUs and 100GB RAM) and requires that a small FLM be run just once to compute the embeddings. To obtain column matches, embeddings are looked up (from storage) and distances between them computed, which is an inexpensive operation."}, {"title": "6. First Appendix: Descriptions of columns of interest used in the main experiment", "content": ""}, {"title": "7. Second Appendix: Additional Usecases", "content": ""}, {"title": "7.1. Usecase 1", "content": "Research question: Are the drugs and labs most commonly ordered for patients with AMI similar across datasets?\nInclusion criteria: Top 20 most ordered drugs and procedures for patients whose primary diagnosis is AMI.\nBreaking down the inclusion criteria into queries, we identify the following tables and columns: prescriptions-table, drug-name-column, labs-table, lab-name-column, diagnosis-table, diagnosis-column, diagnosis-priority-column, procedures-table, and procedure-name-column. In MIMIC-III these tables and columns map to: prescriptions, drug, d_labitems, label, diagnosis_icd, diagnosis, icd9_code, seq_num, d_icd_procedures, and, long_title respectively."}, {"title": "7.2. Usecase 2", "content": "Research question: Is the number of patient clinical notes associated with disease severity?\nInclusion criteria: Adult patients that experienced shock during their hospitalization and/or died in hospital, and their associated notes.\nThe following tables and columns are identified as being important to answering this question. Diagnosis-table, diagnosis-icd9-code-column, admissions-table, discharge-location-column, noteevents-table, and note-text-column. In MIMIC-III these tables and columns map to: diagnosis_icd, icd9_code, admissions, discharge-location, noteevents, and text respectively."}, {"title": "8. Third Appendix: Errors, suggested improvements, and computation time", "content": ""}, {"title": "8.1. Errors and suggested improvements", "content": "In this section, we present examples of instances when the proposed algorithm fails."}, {"title": "Case 1: Columns with similar values are matched, despite not being contextually related.", "content": "When matching columns based on column values alone, the proposed method fails to correctly match subject_id, hadm_id, icd9_code, value, gender, and dose_val_rx within the top 3 returned matches (Table 3). In MIMIC-III, subject id, hadm_id, icd9_code, are integer values which can map to other columns (e.g., highly ranked columns customlabid, nursingchartid, noteid, patientunitstayid) with integer values. Value-wise, these are correct matches, i.e., integer values are matched to other integer values, but context-wise, they are incorrect. These errors are corrected when metadata such as, tables in which columns co-occur, are provided. This helps the algorithm find columns whose values are integers and that co-occur in specific tables. Including other complex metadata such as whether column is a primary/foreign key, can further help in getting these matches correct."}, {"title": "Case 2: Data inconsistency - mixed column value types, and different formatting.", "content": "The algorithm fails to correctly match value, and gender columns when using column values alone (Table 3) because of inconsistent data types and values. In MIMIC-III, gender is recorded as 'F', 'M', while in eICU it is recorded as 'Female' and 'Male'. We are able to correctly match columns when column names are included in the matching. On the other hand, values in the column value are of mixed datatypes. This column represents information about diverse sets of data that are recorded in clinicial charts. For instance, vitals information such as temperature and blood pressure, and past medical history would be recorded in this same column. When using values alone, the algorithm finds that it matches columns with varied data types some containing text data and others numerical data."}, {"title": "Case 3: Unavailable matches", "content": "For several columns (indicated in grey in Table 3), there are no actual matches in eICU. This is expected because based on database designs, some information may be aggregated in a single column in one database while separated into several databases in another, e.g., using admittime and dob to compute patient age in MIMIC-III, while the 'age' column directly exists in eICU. The algorithm will find matches for these columns based on data types and metadata, despite not being \"true\" matches for that column. This is a tricky case for the researchers because they do not know whether the algorithm failed or whether those columns have no matches in the other database. The database schemas can be used to disambiguate this, and future work will look into how this schema information can be integrated in the algorithm to offer further support to the researcher."}, {"title": "8.2. Computation time", "content": "As stated in section 3.2.1, the proposed algorithm is a runtime Linear to the number of columns in the database. The most expensive cost the algorithm pays is generating embeddings for all values in each column (line 2 in Algorithm 1). This cost however is not prohibitive when using a smaller model such as SBERT. For instance, using 2 GPUs, we generated embeddings for the largest column (column name: value, table: Chartevents, dataset: MIMIC-III) which has 316,158,414 values in 10hours. For columns with fewer unique values, e.g., ethnicity and gender, it takes 9.5 and 5.6 seconds respectively, to read and encode unique values in all the batches. Furthermore, the cost of encoding values is paid once, henceforth, the embeddings are used to compute distances and find matches for various columns extracted from different criteria."}, {"title": "9. Fourth Appendix: Strawman version - naive column-name-based matching.", "content": ""}]}