{"title": "Truth Knows No Language: Evaluating Truthfulness Beyond English", "authors": ["Blanca Calvo Figueras", "Eneko Sagarzazu", "Julen Etxaniz", "Jeremy Barnes", "Pablo Gamallo", "Iria De Dios Flores", "Rodrigo Agerri"], "abstract": "We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.", "sections": [{"title": "1 Introduction", "content": "Measuring how truthful LLMs are is crucial to avoid several issues regarding their use: (i) accidental misuse of LLMs leading to deception and distrust by end-users; (ii) blocking positive applications of LLMs due to the lack of evidence regarding their truthfulness (e.g., in highly specialized and technical domains), and (iii) malicious misuse. So far, truthfulness in LLMs has been evaluated mainly using TruthfulQA (Lin et al., 2022), a benchmark to assess the truthfulness and informativeness of LLMs by focusing on imitative falsehoods. Its popularity grew with its inclusion in the first version of the HuggingFace OpenLLM Leaderboard\u00b2 and it has since been adopted as the standard benchmark to evaluate truthfulness in LLMs.\nHowever, TruthfulQA is only available in English. Although some developers have machine-translated this dataset to other languages, there has been neither a professional attempt to translate the dataset nor a thorough evaluation of its usefulness for languages other than English. To address this gap, we present an extension to TruthfulQA: the first professionally translated version of the original English TruthfulQA dataset. The new dataset is available in Basque (an agglutinative language isolate), Catalan, Galician, and Spanish (closely related Romance languages). Except for Spanish, these are low-resource languages, traditionally underrepresented in the pre-training data used to develop LLMs (Luukkonen et al., 2023; Lin et al., 2024; Etxaniz et al., 2024b).\nAlthough TruthfulQA is highly Anglocentric, working with a professionally translated parallel dataset allows us to test the effect of the language on truthfulness (i.e., are LLMs equally truthful independently of the language?). Recent work has aimed at developing multilingual truthfulness benchmarks focusing on context- and time-independent knowledge (Aula-Blasco et al., 2025). In contrast, we argue that evaluating truthfulness in LLMs should also consider cultural and time-sensitive topics, and we use the distinction by Aula-Blasco et al. (2025) to further stress this point.\nIn addition to the multilingual extension to the TruthfulQA dataset, we present a comprehensive"}, {"title": "2 Multilingual TruthfulQA", "content": "TruthfulQA (Lin et al., 2022) is a benchmark comprising 817 questions crafted to test how LLMs mimic human falsehoods. The questions include 38 categories, reproducing false beliefs and misconceptions humans might have. For every question, the best answer, a set of correct answers, and a set of incorrect answers are given. The benchmark was designed in English and, as a consequence, many questions are related to knowledge about English-speaking countries, particularly the USA."}, {"title": "2.1 Professional Translation", "content": "The professional translation of this dataset presented two main options: (1) localizing the questions to align with the cultural context of the target languages or (2) maintaining the cultural context. Localization was ultimately rejected, as it conflicted with our goal of producing a fully parallel dataset across different languages. Both approaches contained inherent drawbacks. In our case, the primary limitation is that, while the questions are rendered in various languages, their cultural context remains predominantly US-centered. To address recurring challenges and ensure consistency, we developed some additional guidelines that can be applied to all four target languages."}, {"title": "3 Experimental Settings", "content": "In this section, we describe the LLMs selected for evaluation and the three main evaluation methods: multiple-choice, LLM-as-a-Judge, and human evaluation. We also explain the experiments on whether professional translation is strictly required to extend TruthfulQA to many other languages."}, {"title": "3.1 Large Language Models", "content": "We experiment with three families of LLMs, specifically Llama 3, Llama 3.1, and Gemma 2 (Dubey et al., 2024; Team et al., 2024). We choose these models due to their strong performance on many"}, {"title": "3.2 Evaluation", "content": "Evaluation is based on three different methods. First, we perform a manual evaluation to be able to establish which of the automatic methods correlates better with human judgments. Second, we use multiple-choice (MC2), the most common automatic metric in leaderboards that include TruthfulQA4 (Beeching et al., 2023). Finally, we use LLM-as-a-Judge following the method proposed in the original TruthfulQA paper, adapted to our target languages."}, {"title": "3.2.1 Human Evaluation", "content": "We perform a manual evaluation of 400 responses for truthfulness and informativeness, with 100 questions and three responses from four models, namely, Gemma 2 27B, Llama 3 8B and Llama 3 70B (the last one both base and instruct versions).\nThe evaluation guidelines used for human evaluation in TruthfulQA are scalar (Lin et al., 2022). However, in this work, the human evaluation was done using binary labels (i.e., truthful/not-truthful and informative/not-informative). Thus, a response that would have a score lower than 0.5 in the original guidelines is labeled as negative. Furthermore, the original paper evaluates only base models and,"}, {"title": "3.2.2 Multiple-choice (MC2)", "content": "The automatic MC2 metric measures the total likelihood of true answers normalized across all true and false reference answers. We use the usual method based on LM Evaluation Harness (Gao et al., 2021) with 6 few-shot examples using a prompt with the form \"Q: {question}\nA: {answer}\" (see few-shot examples in Appendix A). For instruct models, we format each few-shot example as multi-turn user and assistant messages that correspond to questions and answers."}, {"title": "3.2.3 LLM-as-a-Judge", "content": "We use LLMs to train a judge model able to evaluate truthfulness in a generation setting. First, we use a previously fine-tuned judge model based on Llama 2 7B5 as it achieved similar results to the GPT3 judge model used in the TruthfulQA article. Second, we also use stronger multilingual models: Gemma 2 9B and Llama 3.1 8B. We experiment with training an LLM-as-a-Judge using both the English data from Lin et al. (2022) and its MT version (Team et al., 2022) for the target languages. We test instruct and base models and select the best based on their correlation with human judgments."}, {"title": "3.2.4 Correlation with Human Judgments", "content": "We use Cohen Kappa inter-annotator agreement (IAA) (Cohen, 1960) to (i) pick the best LLM-as-a-Judge model; (ii) measure reliability between human annotators, and (iii) establish which automatic evaluation method correlates better with human judgments."}, {"title": "3.3 Experiments with Machine Translation", "content": "As an alternative to the professionally translated version described in Section 2, we generate a multilingual extension of TruthfulQA by automatically translating it using Claude 3.5 Sonnet (Enis and Hopkins, 2024) (see prompt in Annex B). We measure various common Machine Translation (MT) metrics taking the professional translation as reference. The reported results in Annex C show that the automatic translations can be considered of high quality. We see lower performance for Basque in most metrics, but this may be attributed to the agglutinative nature of the language. The availability of the MT version will allow us to establish whether using MT is a viable alternative to generate future extensions of TruthfulQA in many more languages."}, {"title": "4 Results", "content": "We present the main truthfulness results for all five languages in Table 4. Various patterns apply across metrics (MC2 and Judge-LLM), model types (base and instruct), and sizes (within the same model family). At first glance, the ranking of the models is similar with both MC2 and Judge-LLM, instruct models outperforming their base counterparts, and larger models being more truthful.\nHowever, MC2 and Judge-LLM give us a rather different picture when comparing results per language. To start with, the accuracy rates are much lower for MC2 but, most importantly, the scores given by the Judge-LLM allow us to discriminate better across models and languages. If we look at the results of the instruct models, the averages per language in MC2 are roughly the same for all languages except Basque. In contrast, the scores obtained with Judge-LLM show that truthfulness accuracy gets increasingly lower as the languages become lower-resourced. Furthermore, we are now able to observe that the Gemma instruct models,"}, {"title": "4.1 Contextual vs Universal Knowledge", "content": "TruthfulQA includes two types of questions based on the knowledge they inquire about, namely, whether it is time- and context-dependent or universal (Aula-Blasco et al., 2025). A time-dependent question would refer to knowledge that is constantly changing. For example, a question such as In the US it's illegal to do what to a flag? is both contextual (US-centered) and time-dependent (laws can be changed). In contrast, a universal ques-"}, {"title": "4.2 Comparison with Machine Translation", "content": "We leverage the MT version of the dataset to evaluate whether truthfulness performance varies depending on the translation. As it can be seen in"}, {"title": "5 Discussion", "content": "Differences between languages. The results of the manually translated extension to TruthfulQA revealed a correlation between textual resource availability and model truthfulness. Thus, LLMs"}, {"title": "6 Related Work", "content": "A significant challenge in contemporary Artificial Intelligence (AI) research concerns the development of methodologies to optimize LLMs for factual accuracy and veracity in their outputs. Improving factual consistency and reducing hallucinations would help to increase trust in LLMs thereby increasing their application across various domains. Apart from the popular TruthfulQA, already introduced in Section 2, other approaches include SimpleQA (Wei et al., 2024), and VeritasQA (Aula-Blasco et al., 2025).\nSimpleQA is a benchmark dataset designed for"}, {"title": "7 Conclusion", "content": "This paper presents a professionally translated version of the original TruthfulQA dataset, encompassing English, Basque, Catalan, Galician, and Spanish. We have uncovered several interesting points about truthfulness across languages through a comprehensive evaluation of 12 state-of-the-art LLMs using human assessment, multiple-choice metrics, and LLM-as-a-Judge approaches. Although English responses demonstrated superior detail and coherence, the gap in truthfulness across languages was less pronounced than anticipated. Our findings challenge previous assumptions about the correlation of model size with truthfulness (Lin et al., 2022; Aula-Blasco et al., 2025) and highlight the limitations of using multiple choice metrics alone, showing that Judge-LLM methods correlate better with human judgments. Results also reveal that, when available, high-quality MT can effectively generate multilingual truthfulness evaluation datasets, while suggesting that universal topics may be easier to solve by modern LLMs than context- and time-dependent questions. We hope these results improve our understanding of LLM truthfulness across linguistic boundaries, providing valuable insights for developing more reliable multilingual AI systems."}, {"title": "Limitations", "content": "The limitations of the present work are mainly related to language diversity, evaluation techniques, and the dynamic and local nature of a great number"}]}