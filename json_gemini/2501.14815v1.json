{"title": "A VM-HDL Co-Simulation Framework\nfor Systems with PCIe-Connected FPGAs", "authors": ["Shenghsun Cho", "Mrunal Patel", "Basavaraj Kaladagi", "Han Chen", "Tapti Palit", "Michael Ferdman", "Peter Milder"], "abstract": "PCIe-connected FPGAs are gaining popularity as an\naccelerator technology in data centers. However, it is challenging\nto jointly develop and debug host software and FPGA hardware.\nChanges to the hardware design require a time-consuming FPGA\nsynthesis process, and modification to the software, especially\nthe operating system and device drivers, can frequently cause\nthe system to hang, without providing enough information for\ndebugging. The combination of these problems results in long\ndebug iterations and a slow development process. To overcome\nthese problems, we designed a VM-HDL co-simulation frame-\nwork, which is capable of running the same software, operating\nsystem, and hardware designs as the target physical system, while\nproviding full visibility and significantly shorter debug iterations.", "sections": [{"title": "I. INTRODUCTION", "content": "FPGAs are gaining popularity as an accelerator technology\nto offload complex computation and data flows. The combina-\ntion of programmability, high degree of parallelism, and low\npower consumption make FPGAs suitable for environments\nwith rapidly changing workloads and strict power consumption\nlimits, such as data centers. To put FPGAs into existing\nsystems, PCIe has become the most common connection\nchoice, due to its wide availability in server systems. Today,\nthe majority of FPGAs in data centers are communicating with\nthe host system through PCIe [1], [2].\nUnfortunately, developing applications for FPGAs requires\nthe time-consuming FPGA compilation processes, including\nsynthesis, place, and route. Moreover, it is challenging to\ndevelop and debug the host software and the FPGA hardware\ndesigns at the same time. The hardware designs running on the\nFPGAs provide little to no visibility, and even small changes\nto the hardware may need hours to go through the FPGA\ncompilation process. The development process becomes even\nmore difficult when taking operating systems into account.\nChanges to the operating system kernel, the loadable kernel\nmodules, and the application software and hardware can fre-\nquently hang the system without providing enough information\nfor debug, forcing a tedious reboot process. The combination\nof these problems results in long debug iterations and a slow\ndevelopment process.\nThe traditional way to test and debug hardware designs\nwithout running them on real FPGAs is by writing testbenches\nfor simulation. The main drawback of this approach is that\nthe hardware cannot be tested together with the software\nand operating system. While some vendors provide hardware-\nsoftware co-simulation environments, they still lack the ability\nto cover the development of operating system and device driver\ncode. There are also frameworks that connect an instruction-set\nsimulator to an HDL simulator to perform full-system simu-\nlation. However, these frameworks target system-on-chips\u2014\ntypically, ASICs with ARM cores\u2014and thus cannot simulate\nthe PCIe-connected FPGAs used in data center servers.\nWe observe that, although there is no readily available\nenvironment for full-system simulation of servers with PCIe-\nconnected FPGAs, we can extend existing tools to build a\nco-simulation framework for debugging such systems. Virtual\nmachines (VMs) are widely used to run services in data\ncenters. The capability of emulating a full system, including\nCPUs, disks, memory, and peripherals such as PCIe devices,\nmakes VMs a natural fit to emulate the server system in a\ndevelopment environment. On the other hand, FPGA vendors\nare providing sophisticated software for developers to generate\nFPGA platforms with PCIe interfaces for real hardware and for\nHDL simulators with testbenches. The key missing component\nis a link between a VM's virtual PCIe device and the PCIe\nblock in an FPGA HDL simulation platform.\nIn this work, we developed a co-simulation framework\nusing communication channels between a VM and an HDL\nsimulator. On the VM side, we created a PCIe FPGA pseudo\ndevice to represent the FPGA board. The operating system and\nsoftware running inside the VM see the same PCIe device\nas if they were running in a real system with an FPGA\nboard plugged in. On the HDL side, we developed a PCIe\nsimulation bridge to talk to the VM. The PCIe simulation\nbridge is pin-compatible with the PCIe block used in the\nphysical FPGA hardware. The rest of the FPGA platform sees\nthe same interface toward PCIe and requires no modification.\nTo the FPGA development tools, the PCIe simulation bridge\nappears as a regular hardware block and has no impact"}, {"title": "II. THE VM-HDL CO-SIMULATION FRAMEWORK", "content": "The key component in our co-simulation framework is the\nlink between the virtual machine monitor (VMM) and the\nHDL simulator. The most important requirement for this link\nis to expose exactly the same interface and functionality as\nits counterpart in the real system, to allow a smooth transition\nbetween the development and debugging of the system in the\nco-simulation framework and its deployment on real hardware.\nAll parts of the system, including the FPGA platform, the\noperating system driver, and the software, must be able to\nrun in simulation and on the target hardware without any\nmodifications. Another requirement is that the cut-off point of\nthe link and the rest of the system should be generic and well-\ndefined, to reduce the effort of developing and using the link.\nWith these considerations in mind, we developed a VM-HDL\nlink with three parts: a PCIe FPGA pseudo device, a PCIe\nsimulation bridge, and message passing channels between\nthem. The architecture of the co-simulation framework is\nshown in Figure 1.\nWe created a PCIe FPGA pseudo device in the VMM to\nrepresent the PCIe FPGA board. The structure of the VMM's\nemulated PCIe devices is generic and well defined, enabling\nus to create the PCIe FPGA pseudo device by modifying\nan existing device and customizing it with the target FPGA\nboard's PCIe characteristics, such as the number and size\nof the Base Address Register (BAR) regions and Message\nSignaled Interrupt (MSI) capabilities. The benefit of using a\nPCIe pseudo device is that it interacts with the guest operating\nsystem through Memory-Mapped Input-Output (MMIO), thus\nallowing us to avoid the low-level PCIe protocol details.\nMMIO read and write requests to the BAR regions are handled\nusing callback functions and translated into messages that are\nsent to the HDL simulator. The PCIe FPGA pseudo device\nalso configures the VMM to listen to memory accesses and\ninterrupts from the HDL side by registering the file descriptors\nof the communication channels with the VMM's main loop,\nenabling the VMM subsystem to respond to the memory read,\nwrite, and interrupt requests from the HDL simulator.\nOn the HDL side, we developed a PCIe simulation bridge\nto replace the hardware PCIe bridge in the FPGA platform.\nTo avoid implementing the low-level PCIe protocol, we rely\non an industry-standard on-chip bus protocol, the Advanced\neXtensible Interface (AXI). AXI serves as the bridge's in-\nterface to the rest of the FPGA platform. A slave interface\nmonitors the AXI bus signals for memory access requests\nto the simulation bridge, which triggers the corresponding\nfunctions, implemented using SystemVerilog DPI, to send\nthese requests to the VMM. The simulation bridge also listens\nto requests and reads responses from the VMM, calling the\ncorresponding HDL tasks to either send MMIO read and\nwrite requests to the FPGA platform through the AXI master\ninterface, or to send back read responses to the FPGA platform\nthrough the AXI slave interface. An interrupt pin on the\nsimulation bridge's interface allows the FPGA platform to also\nsend requests that generate MSI interrupts in the VM.\nThe VMM's PCIe FPGA pseudo device and the PCIe simu-\nlation bridge communicate through two pairs of unidirectional\nchannels, one for HDL to VMM accesses and the other for"}, {"title": "III. IMPLEMENTATION AND EVALUATION", "content": "To demonstrate our VM-HDL co-simulation framework,\nwe developed an FPGA-based sorting offload platform. The\nsorting unit we used in the platform is automatically generated\nby the Spiral Sorting Network IP Generator [3]. The sorting\nunit takes a stream of input data and produces the output result\nstream after a fixed number of cycles. The sorting unit is fully\npipelined and able to consume back-to-back input streams.\nWe build the FPGA platform using Xilinx Vivado 2016.2,\ntargeting the NetFPGA SUME PCIe board. The sorting unit in"}, {"title": "IV. RESULTS", "content": "The primary advantage of our co-simulation framework is\nthe improvement in the debug iteration time, the time needed\nto make a change to the software or hardware description and\nobserve its results. For physical systems, this is particularly\nnoticeable when the system \u201changs\u201d due to a bug and requires\na tedious reboot or when changes to the FPGA platform hard-\nware require the time-consuming FPGA compilation process.\nEven worse, the physical system does not provide sufficient\nvisibility into the hardware or software when an error occurs,\nrequiring developers to go through many debug iterations to\nfind and fix each bug. To contrast the two debug approaches,\nTable II shows the breakdown of the debug iteration time\non a physical system and on our co-simulation framework.\nCompared to a physical system, which requires the FPGA\ncompilation process, the co-simulation framework is 25x faster\nin our test case. Changes in the FPGA platform can run in the\nHDL simulator with full visibility in just a few minutes."}, {"title": "A. Debug Iteration Time", "content": "The primary advantage of our co-simulation framework is\nthe improvement in the debug iteration time, the time needed\nto make a change to the software or hardware description and\nobserve its results. For physical systems, this is particularly\nnoticeable when the system \u201changs\u201d due to a bug and requires\na tedious reboot or when changes to the FPGA platform hard-\nware require the time-consuming FPGA compilation process.\nEven worse, the physical system does not provide sufficient\nvisibility into the hardware or software when an error occurs,\nrequiring developers to go through many debug iterations to\nfind and fix each bug. To contrast the two debug approaches,\nTable II shows the breakdown of the debug iteration time\non a physical system and on our co-simulation framework.\nCompared to a physical system, which requires the FPGA\ncompilation process, the co-simulation framework is 25x faster\nin our test case. Changes in the FPGA platform can run in the\nHDL simulator with full visibility in just a few minutes."}, {"title": "B. Application Execution Time", "content": "Although the debug iteration time is drastically reduced, the\nsimulation platform runs slower than the physical system, as\nshown in Table II. This is expected because the co-simulation\nframework performs cycle-accurate HDL simulation and, on\nevery cycle, the simulator performs additional work to poll the"}, {"title": "C. Simulated Time", "content": "Our co-simulation framework targets functional level cor-\nrectness. Although the HDL simulator is cycle-accurate, the\nPCIe simulation bridge and the QEMU VM are not, which\ncauses the co-simulated time to differ from the real system run\ntime. Table III presents a comparison between simulated time\nin the co-simulation framework and actual time of running the\ndesign on the physical system. Although the gap is significant\nand precludes performance evaluation using the co-simulation\nframework, the difference is acceptable for the purpose of\ndebugging system correctness."}, {"title": "V. RELATED WORK", "content": "Among prior work, the vpcie project [5] is the most similar\nto our co-simulation framework. Vpcie links QEMU with a\nVHDL simulator and is capable of full-system simulation.\nHowever, a key difference from our system is that vpcie links\nQEMU and HDL at a lower level. On the QEMU side, vpcie\nforwards low-level PCIe messages that require extra software\nto process, whereas our platform forwards high-level memory\naccess and interrupt requests directly. Similarly, on the HDL\nside, vpcie exposes a non-standard interface with the PCIe\nBAR information to the FPGA platform, whereas our system\nuses an industry standard memory-mapped interface, reducing\nthe complexity and improving the adaptability of our co-\nsimulation framework.\nFPGA companies and FPGA cloud vendors provide co-\nsimulation software-HDL environments for their products\nand services, such as Intel OpenCL for FPGA [6], Xilinx\nSDAccel [7], and Amazon F1 [8]. Unlike our co-simulation\nframework that enables the development and debugging of the\noperating system and device drivers, the environments these\nvendors provide are limited to executing application software.\nSeveral works use QEMU as an instruction-set simulator\nand connect it to virtual platforms built in SystemC for full-\nsystem co-simulation [9], [10], [11], [12]. Some of these plat-\nforms have the ability to run HDL simulations, making them"}, {"title": "VI. CONCLUSIONS", "content": "In this work, we described a VM-HDL co-simulation frame-\nwork for software-hardware co-design on systems with PCIe-\nconnected FPGA boards. Our framework enables developers to\nhave the same software, operating system, and FPGA platform\nrunning in the co-simulation environment as in the physical\nsystem, while providing much greater visibility into both the\nsoftware and the hardware, and drastically reducing the debug\niteration time."}]}