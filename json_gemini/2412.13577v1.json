{"title": "Bridge then Begin Anew: Generating Target-relevant Intermediate Model for Source-free Visual Emotion Adaptation", "authors": ["Jiankun Zhu", "Sicheng Zhao", "Jing Jiang", "Wenbo Tang", "Zhaopan Xu", "Tingting Han", "Pengfei Xu", "Hongxun Yao"], "abstract": "Visual emotion recognition (VER), which aims at understanding humans' emotional reactions toward different visual stimuli, has attracted increasing attention. Given the subjective and ambiguous characteristics of emotion, annotating a reliable large-scale dataset is hard. For reducing reliance on data labeling, domain adaptation offers an alternative solution by adapting models trained on labeled source data to unlabeled target data. Conventional domain adaptation methods require access to source data. However, due to privacy concerns, source emotional data may be inaccessible. To address this issue, we propose an unexplored task: source-free domain adaptation (SFDA) for VER, which does not have access to source data during the adaptation process. To achieve this, we propose a novel framework termed Bridge then Begin Anew (BBA), which consists of two steps: domain-bridged model generation (DMG) and target-related model adaptation (TMA). First, the DMG bridges cross-domain gaps by generating an intermediate model, avoiding direct alignment between two VER datasets with significant differences. Then, the TMA begins training the target model anew to fit the target structure, avoiding the influence of source-specific knowledge. Extensive experiments are conducted on six SFDA settings for VER. The results demonstrate the effectiveness of BBA, which achieves remarkable performance gains compared with state-of-the-art SFDA methods and outperforms representative unsupervised domain adaptation approaches.", "sections": [{"title": "Introduction", "content": "With the rapid development of social networks, people have become used to posting images to express their feelings (Zhao et al. 2017). Visual emotion recognition (VER) aims to identify human emotions elicited by images (Zhao et al. 2022b), attracting increasing attention and playing an essential role in various applications, such as depression detection (Bokolo and Liu 2023) and opinion mining (Razali et al. 2021). Advanced technologies based on supervised deep neural networks have been proposed to improve the VER performance (Deng et al. 2024; Cen et al. 2024). Existing methods mainly follow a supervised pipeline requiring sufficient emotion annotations. However, labeling reliable large-scale datasets for VER tasks is challenging in practical applications because of the intrinsic properties of emotions, such as subjectivity, complexity, and ambiguity (Zhao et al. 2023). To reduce the annotation burden, much attention has been devoted to unsupervised domain adaptation (UDA) for VER, which enables the models trained on labeled source data to generalize well on the unlabeled target data (Zhao et al. 2019b). However, existing UDA methods require access to the source data during adaptation (Li, Guo, and Ge 2023; Zhao et al. 2024; Jiang et al. 2024). Regarding privacy and security concerns, source-free domain adaptation (SFDA) has attracted much interest in dealing with the situation where labeled source data is unavailable (Li et al. 2024).\nOn the one hand, although prior SFDA methods are effective for standard classification tasks, they face specific challenges and performance decreases when directly applied to VER. This is because of the large affective gap (Zhao et al. 2014) between VER datasets, which arises from annotator variations and the scope of data collection. The distribution gap significantly affects the feature alignment, which in turn reduces the confidence of pseudo-labels. As shown in Figure 1 (a), we use SHOT (Liang, Hu, and Feng 2020), a typical SFDA method, to generate pseudo-labels for VER datasets EmoSet \u2192 FI and standard classification datasets Real World \u2192 Clipart, respectively. The results show that the classification scores for emotional pseudo-labels are relatively low, which increases the risk of misalignment between features and labels, and can lead to a confirmation bias due to the accumulation of errors (Ding et al. 2023). To address this issue, bridging the inter-domain variation to generate reliable pseudo-labels is necessary.\nOn the other hand, traditional SFDA methods are fine-tuned on the source model. However, the unclear class features of the VER dataset make the source model lack robustness. For a better illustration, as shown in Figure 1 (b), we train two ResNet-101 models (He et al. 2016) with the same number of categories on EmoSet and Clipart from the Office-Home (Venkateswara et al. 2017), respectively. The results indicate that the VER dataset has lower interclass distinction and higher intraclass variability. Consequently, the source model will overfit on noise, which is unrelated to the discriminative class features. As a result, when directly adapted to the target dataset, i.e., EmoSet \u2192 FI and Real World \u2192 Clipart, without further improvement, standard classification model tends to outperform VER model by a large margin. This suggests that the VER dataset lacks explicit class characterization, making the source domain model prone to overfitting. To eliminate the effects of source domain noise, it is necessary for the target model to learn from the target data itself, instead of fine-tuning from the source model.\nWe summarize the challenges above: the large affective gap leads to incorrect pseudo-labels, and lack of clarity in the distribution of the VER dataset can lead to overfitting the source domain model. To address these two challenges, we propose a novel SFDA framework for VER, termed Bridge then Begin Anew (BBA). BBA contains two steps: Domain-bridged model generation (DMG) and target-related model adaptation (TMA). To improve the confidence of pseudo-labels, DMG generates a bridge model to align emotional data across various domains, thus avoiding the challenges associated with directly fine-tuning emotional data. Moreover, we introduce a clustering-based pseudo-label post-processing and masking strategy to constrain the pseudo-label distribution and explore richer semantic contexts. Then, to eliminate the effects of overfitting the source model, TMA starts training anew, which allows the target model to learn feature relations independently, thus extending the exploration of the target-specific features. Furthermore, in order to better learn about the emotional features, we introduce polarity constraints to enhance the target model's discriminative ability for emotion categories.\nThe main contributions of our work are outlined below:\n\u2022 We propose a new task, i.e., source-free domain adaptation for visual emotion recognition (SFDA-VER), which focuses on the cross-domain transfer of VER without accessing source data during adaptation.\n\u2022 We propose a two-step framework, comprising DMG and TMA, to improve the reliability of pseudo-labels and eliminate the effects of source model overfitting.\n\u2022 We conduct experiments on six SFDA-VER settings, and the results show that our approach outperforms existing state-of-the-art methods by an average of +3.03."}, {"title": "Related Works", "content": "Deep learning and CNNs have revolutionized VER (Zhao et al. 2023). The field evolved from global feature extraction (You et al. 2015; Zhu et al. 2017; Yang et al. 2018a; Rao, Li, and Xu 2020) to focusing on emotion-rich local regions and their relationships (Yang et al. 2018b; Zhao et al. 2019a; Rao et al. 2019; Yao et al. 2019; Zhang et al. 2020). Recently, CLIP-based approaches have emerged (Deng et al. 2022, 2024; Cen et al. 2024). However, all these methods require large amounts of accurately labeled emotional data to train the network. Therefore, domain adaptation methods are needed to alleviate the requirement for a large number of annotations during the training process.\nDue to the significant domain bias in VER datasets, Zhao et al. (2019b) introduced CycleEmotionGAN, an unsupervised approach for cross-domain emotion adaptation using emotional consistency loss. Zhao et al. (2022a) enhanced this with multi-scale similarity and improved emotional consistency. Considering the privacy constraints that limit direct access to source data for emotion analysis, our study presents a new source-free domain adaptation for VER tasks. It is required to adapt the source emotion recognition model to the target domain without accessing source data."}, {"title": "Source Free Domain Adaptation", "content": "The SFDA setting represents a more complex but realistic UDA scenario where source data are unavailable (Li et al. 2024). The following are some standard methods: Pseudo-labeling is to label the unlabeled target samples based on the predictions of the source model (Liang et al. 2021; Huang et al. 2022; Xie et al. 2022; Wang et al. 2022; Ahmed, Morerio, and Murino 2022; Liang et al. 2022). Entropy minimization is a constraint which can be inversely employed to guide the optimization of the model (Liang, Hu, and Feng 2020; Jeon, Lee, and Kang 2021; Mao et al. 2024; Kothandaraman, Chandra, and Manocha 2021). In virtual source methods (Du et al. 2023; Hao, Guo, and Yang 2021), the variants of Generative Adversarial Networks (GANs) (Li et al. 2020; Kurmi, Subramanian, and Namboodiri 2021) are common approaches to construct virtual source data. However, the above methods are insufficient to address the task of VER due to low domain relationships between emotional domains. Unlike existing work, we focus on mining the inherent structure of the target domain."}, {"title": "Problem Definition", "content": "In this paper, we focus on Source-Free Domain Adaptation for Visual Emotion Recognition (SFDA-VER), adapting from a labeled source domain to an unlabeled target domain. Our source dataset contains $N_s$ labeled images ${x^i}_{i=1}^{N_s}$ with corresponding emotion labels ${y^i}_{i=1}^{N_s}$, where $x \\in X_s$, $y \\in Y_s$. The target dataset contains $N_T$ unlabeled images $T = {x^i}_{i=1}^{N_T}$, where $x \\in X_t$. The feature spaces $X$ and $Y$ differ across domains. For simplicity, the superscript $i$ is henceforth omitted. In particular, during the adaptation process, the data from the source domain is inaccessible; only the source model can be used for adaptation."}, {"title": "Overview", "content": "Figure 2 summarizes our framework, Bridge then Begin Anew (BBA), which consists of two steps: domain-bridged model generation (DMG) and target-related model adaptation (TMA). Unlike most previous SFDA methods that utilize the source model to initialize the target model, DMG introduces a bridge model to enhance the consistency between domains and produce reliable pseudo-labels. To reduce source model overfitting due to category noise, TMA starts by training the target model anew, learning from the target structure itself independently."}, {"title": "Domain-bridged Model Generation", "content": "It is still a challenge for SFDA to generate high-confident pseudo-labels, especially for tasks with large inter-domain variations like VER. In traditional methods, the source model is directly used to generate pseudo-labels on the target data. These approaches make the confidence of pseudo-labels highly dependent on the generalization ability of the source model. Due to category noise in VER data, the source model lacks robustness, resulting in low-confidence pseudo-labels. Hence, we propose a bridge model to minimize domain gaps and generate high-confidence pseudo-labels.\nIn order to improve interclass distinction and reduce intraclass variability in VER features, we consider mitigating the pseudo-label mismatch problem caused by domain differences. Inspired by k-means, we propose a fused distance clustering method as a label optimization strategy, which imposes additional constraints on the model outputs. Specifically, we apply weighted k-means clustering to compute the centroid of each class within the target domain:"}, {"title": "", "content": "$c_k^{(0)} = \\frac{\\sum_{x_t \\in X_t} \\sigma_k(\\phi_s(x_t))g_s(x_t)}{\\sum_{x_t \\in X_t} \\sigma_k(\\phi_s(x_t))}$ (1)\nwhere $\\sigma_k(a) = \\frac{exp(a_k)}{\\sum_i exp(a_i)}$ denotes the k-th element in the softmax output of a K-dimensional vector a, $\\phi_s = f_s \\circ g_s$ indicates source model, $g_s$ is the source feature extractor. $X_t = {x_t}, \\hat{y_t} = arg \\max \\sigma(\\phi_s(x_t)), \\sigma$ is the softmax operation, and $1$ is the indicator function. After calculating the centroid of each class, we assign each sample to the nearest class as follows:\n$\\hat{y_t} = arg \\min D_f(g_s(x_t), C_k^{(0)})$, (2)\nwhere $D_f(P, Q)$ is the distance measure used to calculate the distance between each sample and centroid. Considering different distance metrics focus on various parts, we adopt a fused distance metric, i.e., we calculate Euclidean ($D_{eu}$), Cosine ($D_{cos}$), and Manhattan ($D_{man}$) distances between two features:\n$D_f(P, Q) = D_{eu}(P, Q) + D_{cos}(P, Q) + D_{man}(P, Q)$. (3)\nThen, we update the category centroids according to:\n$C_k^{(1)} = \\frac{\\sum_{x_t \\in X_t} 1(\\hat{y_t} = k)g_s(x_t)}{\\sum_{x_t \\in X_t} 1(\\hat{y_t} = k)}$ (4)\nTo reduce intraclass variability, we consider the difference between VER images and traditional classification images: Unlike standard classification tasks, where different blocks in an image correspond to different semantics, different blocks in VER images are usually associated with a unified emotion label. As such, we propose a masking-enhanced framework to leverage emotional consistency across different contexts. As shown in Figure 3, models are encouraged to focus on local features to more effectively extract emotionally consistent features for better classification. Our method uses the random mask technique. Specifically, given a 2D image I of size H \u00d7 W, and the number of blocks to be masked n, which is randomly selected from a given range or distribution, the mask M is defined as:\n$M = \\{M_{ij}\\}_{i=1,j=1}^{H,W},\\text{with}\\sum_{i=1}^{H}\\sum_{j=1}^{W} m_{ij} = nb^2$, (5)\nwhere $m_{ij} \\in \\{0, 1\\}$ is the mask value at position (i, j), and if (i, j) \u2208 randomly selected blocks, $m_{ij} = 1$. Each block has a size of b x b. The masked image $x_t^M$ is obtained by the element-wise multiplication of M and $x_t$, denoted as:\n$x_t^M = M x_t$. (6)\nAfter masking, the classifier is forced to use sparse residual cues, to learn local and global affective information. Moreover, to provide a consistent and stable guidance for bridge model, our source model adopts the exponential moving average (EMA) strategy for parameter updates (Tarvainen and Valpola 2017). The bridge model $\\phi_b = f_b \\circ g_b$ inherits parameters from the pre-trained source model $\\phi_s$. To better generalize to the target domain, the feature extractor $g_b$ is updated while the classifier $f_b$ is fixed. Both $g_b$ and $g_s$ are updated at this step, $g_b$ is updated by backpropagation as described in Eq. (10), and $g_s$ is updated by EMA:\n$\\theta^{t+1} \\leftarrow \\alpha_e \\theta^t + (1 - \\alpha_e) \\theta$, (7)\nwhere t is the training step, $\\alpha_e$ is the smoothing factor, $\\theta, \\theta'$ are the parameters of $g_s$, $g_b$. Then, we train the masked image together with the original target domain image to participate in the computation of the self-labeling loss and the knowledge distillation loss:\n$L_{sl} = - \\frac{1}{N_t}\\sum_{x_t \\in X_t}\\sum_{k=1}^{K} 1(\\hat{y_t} = k) \\log \\sigma(\\phi_b(x_t))$,\n$- \\frac{1}{N_t}\\sum_{x_t \\in X_t}\\sum_{k=1}^{K} 1(\\hat{y_t} = k) \\log \\sigma(\\phi_b(x_t^M))$, (8)\n$L_{kd} = -E_{x_t \\in X_t} \\hat{y_t} \\log \\phi_b(x_t) - E_{x_t \\in X_t} \\hat{y_t} \\log \\phi_b(x_t^M)$, (9)\nwhere $\\hat{y_t} = D_f(g_s(x_t), C_k)$ denotes the soft pseudo output of the teacher feature extractor $g_s$ after clustering, and $C_k$ denotes the final centroid of each class. The total loss function for the DMG is expressed as:\n$L_{dmg} = L_{kd} + \\lambda L_{sl}$, (10)\nwhere $\\lambda$ is the weight for $L_{sl}$, controlling the importance of $L_{sl}$ during DMG step."}, {"title": "Target-related Model Adaptation", "content": "As stated earlier, noise in the source data that is unrelated to the discriminative features can cause the source model to overfit, potentially constraining its representation of target features. To overcome it, we argue that the target model should be given sufficient latitude to explore the target feature space from scratch, enabling it to identify novel features. Following recent advances in self-distillation (Kim et al. 2021; Laine and Aila 2016; Liang et al. 2022), we train the target model $\\phi_t$ from scratch using target data alone, guided by the bridge model $\\phi_b$, which produces more reliable pseudo-labels for improved supervision:\n$L_{align} = E_{x_t \\in X_t} D(\\phi_b(x_t), \\phi_t(x_t))$, (11)\nwhere $D(\\cdot, \\cdot)$ is a function that measures the distance between two emotion samples. In this paper, $D(\\cdot, \\cdot)$ represents the KL divergence. $\\phi_b$ updates pseudo-labels by mixing $\\phi_t$ outputs, where $\\alpha_t$ is a momentum hyper-parameter:\n$\\phi_b(x_t) \\leftarrow \\alpha \\phi_b(x_t) + (1 - \\alpha_t) \\phi_t(x_t), \\forall x_t \\in X_t$. (12)\nIn addition, we note that emotions naturally contain polarized information; categories with matching emotion polarities (positive or negative) show closer associations. To explore global and hierarchical polarity features when learning the target structure, we combine emotion polarity with the loss of self-labeling (SL) and the loss of information maximization (IM). Specifically, the distribution of emotion polarity is detailed as follows:\n$P_{pos} = \\sum_{k \\in C_{pos}} \\sigma(\\phi_t(x_t))_k, P_{neg} = \\sum_{k \\in C_{neg}} \\sigma(\\phi_t(x_t))_k$, (13)\nwhere $C_{pos}$ and $C_{neg}$ represents the positive and negative emotion categories. $P_{pos}$ and $P_{neg}$ symbolize the aggregate predicted likelihood across all positive and negative emotion categories for each sample, and they are collectively referred to $p_e$. Then, we propose a polarized IM loss that incorporates the emotion polarity distribution:\n$L_{im} = -E_{x_t \\in X_t} (L_e + L_d)$, (14)\n$L_e = -E_{k \\in K} [p_k \\log(p_k)] - E_{e \\in \\{pos, neg\\}} [p_e \\log(p_e)]$, (15)\n$L_d = -E_{k \\in K} [p_k \\log E_k(p_k)] - E_{e \\in \\{pos, neg\\}} [p_e \\log E_e(p_e)]$, (16)\nwhere $L_e$ and $L_d$ denote the entropy and diversity loss respectively. $p_k$ represents the predicted probability of the $k^{th}$ class, represented by $\\sigma(\\phi_t(x_t))_k$. Moreover, we also incorporate the emotion polarity distribution into the SL loss, resulting in the following polarized SL loss:\n$L_{sl} = -E_{x_t \\in X_t} \\sum_{k=1}^{K} \\hat{y_t} \\log p_k - E_{x_t \\in X_t} \\sum_{e \\in \\{pos, neg\\}} \\hat{y_e} \\log p_e$, (17)\nwhere $\\hat{y_e} = arg \\max p_e$ denotes the polarity pseudo-label. To illustrate the necessity and effectiveness of the proposed emotion polarity loss, we cite a simple example in Figure 4. We can see that the standard IM loss $L_{im}$ gives the same loss value for different outputs $\\phi_t(x_A)$ and $\\phi_t(x_B)$, which are both 0.93. This is because the standard IM loss only focuses on the entropy of the output vector and overlooks the category relations. However, the polarity loss, which has values of 0.34 and 1.01, imposes an extra constraint on the model output. Furthermore, for the standard SL loss $L_{sl}$, it is unchanged between the outputs $\\phi_t(x_A), \\phi_b(x_t)$ and $\\phi_t(x_B), \\phi_b(x_t)$, remaining at 2.07. After including the polarity loss, which are 0.10 and 1.42, the total loss differs. This highlights how the polarity loss considers an additional correlation across categories.\nOverall, the total loss for the TMA phase can be expressed as:\n$L_{tma} = L_{align} + L_{pol}$, (18)\nwhere $L_{pol} = \\gamma L_{sl} + \\delta L_{im}$, $\\gamma$ and $\\delta$ are the weights for $L_{sl}$ and $L_{im}$."}, {"title": "BBA Learning", "content": "For training, in the DMG step, we use clustered post-processed pseudo-labels to guide the training of both the target and masked target data. This approach aims to improve interclass separability and intraclass compactness, thereby generating more reliable pseudo-labels. The loss function is $L_{dmg}$ in Eq. (10). In the TMA step, to avoid the effects of source domain model overfitting, the target model is randomly initialized and guided by DMG-corrected pseudo-labels to learn the target structure. The loss function is $L_{tma}$ in Eq. (18). For inference, we only use the target model, and the model size is maintained the same as the source model."}, {"title": "Experiments", "content": "Extensive experiments are conducted on four VER datasets: FI (You et al. 2016), EmoSet (Yang et al. 2023), and ArtPhoto (Machajdik and Hanbury 2010) are categorized into Mikels' eight emotion categories (Zhao et al. 2016); In contrast, Emotion6 is classified according to the Ekman model's six emotion categories (Peng et al. 2015). We employ six SFDA-VER settings for experiments: FI \u2192 EmoSet, FI \u2194 ArtPhoto, and FI \u2192 Emotion6, with the latter considered as binary classification tasks due to their distinct categorical frameworks.\nWe compare BBA with the following baselines: 1) Source only, which refers to a basic approach where the model is trained on the source domain and directly applied to the target domain. 2) SFDA methods, which include SHOT (Liang, Hu, and Feng 2020), SHOT++(Liang et al. 2021), G-SFDA(Yang et al. 2021), AaD (Yang et al. 2022), DaC (Zhang et al. 2022b), DINE (Liang et al. 2022), and TPDS (Tang et al. 2024). 3) UDA methods, which can utilize source domain data during the adaptation process, including MCC (Jin et al. 2020), ELS (Zhang et al. 2022a), and MIC (Hoyer et al. 2023). 4) Oracle, which is the ideal scenario where the model is trained and tested within the target domain, representing the upper bound of performance.\nIn BBA, the EMA parameter $\\alpha_e$ of the source network is set to 0.999. The loss weights $\\lambda$, $\\gamma$, and $\\delta$ are set to 0.9, 1, and 0.3 for all six settings, respectively. More information about evaluation metrics and implementation details can be found in the supplementary material."}, {"title": "Comparisons with State-of-the-art Methods", "content": "The results on FI \u2194 EmoSet and FI \u2194 Emotion6 are comprehensively detailed in Table 1 and Table 2, respectively. More results for the other two settings, consisting of the small ArtPhoto dataset, can be found in the supplementary material. Compared to the prior state-of-the-art SFDA methods, BBA achieves average improvements ranging from +2.26 to +7.17 on FI \u2192 EmoSet and from +3.81 to +11.3 on FI \u2192 Emotion6. The increase is due to our 'bridge then begin anew' training framework, which improves the pseudo-labeling accuracy while learning from the target data distribution. Despite the theoretical advantage of the UDA method in achieving better results by having access to the source domain data during the adaptation process, BBA outperforms the UDA method in accuracy, with average improvements of +1.57 to +2.78 on FI \u2192 EmoSet and +3.00 to +7.10 on FI \u2192 Emotion6, due to its targeted solutions to the challenges inherent in emotion data."}, {"title": "Ablation Study", "content": "In our ablation study, shown in Table 3, we analyze the effectiveness of each component. We reveal that all modules are effective, with clustering and masking strategies bridging the domain gaps, enhancing feature discrimination, and generating more reliable pseudo-labels. Furthermore, $L_{align}$ and $L_{pol}$ enable the model to train directly on target domain data and explore the hierarchical features inherent in emotional images. Panels (a), (b), and (c) of Figure 5 provide a sensitivity analysis for the loss weights $\\lambda$, $\\gamma$, and $\\delta$. They show that the performance of BBA is relatively stable across different hyper-parameters. Panel (d) details the performance gains of the model during training, with different colors representing different stages of training. Panels (e) and (f) visualize the ablation results with different evaluation metrics on FI \u2192 EmoSet, further illustrating the effectiveness of the components proposed in BBA."}, {"title": "Visualization", "content": "Figure 6 visually analyzes the results before and after adaptation on FI \u2192 EmoSet. Panel (a) and Panel (f) illustrate the feature distribution on EmoSet for models trained on FI and EmoSet, respectively, showing the initial discrepancy between the source and target domains. Panels (b) and (c) show the feature distributions obtained by applying the SFDA methods SHOT and its extension SHOT++. Both methods retain substantial source domain knowledge due to their fundamental reliance on the source domain model.\nPanel (d) visualizes the results after our DMG step. Compared with panels (b) and (c), DMG increases intra-class compactness and inter-class separability, making the feature distribution clearer and enabling the generation of high-confidence pseudo-labels. Panel (e) visualizes the results after our TMA step, highlighting its ability to effectively learn emotion feature representations from the target domain. This is achieved by learning the target domain model from scratch, closely mirroring the Oracle results shown in Panel (f). It demonstrates the superior ability of our method to capture the nuanced emotional features of the target domain, thereby setting a new state-of-the-art in SFDA-VER."}, {"title": "Conclusion", "content": "This paper introduces a new task termed source-free domain adaptation for visual emotion recognition (SFDA-VER). To address this task, we propose a novel method called BBA, which consists of two steps: domain-bridged model generation (DMG) and target-related model adaptation (TMA). First, the DMG step bridges the source and target domains. The clustering post-processing strategy enhances inter-class separability, while the masking strategy increases intra-class compactness. These improvements bolster the separability of categories and generate more reliable pseudo-labels. Subsequently, the TMA step focuses on the target structure to begin training anew. The emotion polarity loss enhances the model's capabilities in emotion recognition. In summary, BBA addresses the unique challenges of the SFDA task within the VER dataset."}]}