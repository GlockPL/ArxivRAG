{"title": "Test-time regression: a unifying framework for designing sequence models with associative memory", "authors": ["Ke Alexander Wang", "Jiaxin Shi", "Emily B. Fox"], "abstract": "Sequences provide a remarkably general way to represent and process information. This powerful\nabstraction has placed sequence modeling at the center of modern deep learning applications, inspiring\nnumerous architectures from transformers to recurrent networks. While this fragmented development\nhas yielded powerful models, it has left us without a unified framework to understand their fundamental\nsimilarities and explain their effectiveness. We present a unifying framework motivated by an empiri-\ncal observation: effective sequence models must be able to perform associative recall. Our key insight\nis that memorizing input tokens through an associative memory is equivalent to performing regres-\nsion at test-time. This regression-memory correspondence provides a framework for deriving sequence\nmodels that can perform associative recall, offering a systematic lens to understand seemingly ad-hoc\narchitectural choices. We show numerous recent architectures including linear attention models,\ntheir gated variants, state-space models, online learners, and softmax attention emerge naturally as\nspecific approaches to test-time regression. Each architecture corresponds to three design choices: the\nrelative importance of each association, the regressor function class, and the optimization algorithm.\nThis connection leads to new understanding: we provide theoretical justification for QKNorm in soft-\nmax attention, and we motivate higher-order generalizations of softmax attention. Beyond unification,\nour work unlocks decades of rich statistical tools that can guide future development of more powerful\nyet principled sequence models.", "sections": [{"title": "Introduction", "content": "Sequences play a vital role in modern machine learning by providing a powerful abstraction: any compu-\ntational task can be viewed as transforming one sequence into another (Sutskever et al., 2014). This se-\nquential perspective has spread across diverse domains, including natural language processing (Sutskever\net al., 2014; Devlin et al., 2019; Brown et al., 2020), computer vision (Dosovitskiy et al., 2021; Bertasius\net al., 2021), time series analysis (Salinas et al., 2020; Gruver et al., 2023; Ansari et al., 2024), and compu-\ntational biology (Jumper et al., 2021; Zhou and Troyanskaya, 2015; Nguyen et al., 2024), highlighting the\nimportance of building generically applicable sequence layers (Vaswani et al., 2017).\nThis development has produced a diversity of architectures, each with its own unique characteris-\ntics and performance trade-offs. While these architectures have achieved considerable success, they have\nlargely emerged through separate lines of investigation. This fragmented and often empirically-driven ap-\nproach to model development limits our ability to systematically understand and improve design choices.\nMoreover, the idiosyncratic notations of each architecture obscures their underlying connections (Rush,"}, {"title": "Outline of our paper", "content": "In Section 2, we introduce our test-time regression framework and formalize the connection between asso-\nciative memory and regression. We show how this perspective provides a systematic approach to sequence\nmodel design through three key choices: regression objective, function class, and optimization algorithm.\nSection 3 demonstrates the broad applicability of our framework by deriving state-of-the-art sequence\narchitectures from first principles. In the process, we explain the effectiveness of QKNorm in softmax\nattention, and we derive higher-order generalizations of softmax attention, among other contributions.\nSection 4 examines how to construct effective key-value pairs for associative recall. Finally, Section 5"}, {"title": "Test-time regression as a framework for model design", "content": "The goal of sequence modeling is to transform a sequence of input tokens x1, . . ., x\u012b into output tokens\n\u04231,..., \u0443\u0442 (Sutskever et al., 2014). Although many sequence layers exist, past works have shown that\nassociative recall ability is crucial to this task by enabling in-context learning (Olsson et al., 2022). Though\nmany sequence layers exist, it is not a priori clear which ones are able to perform associative recall without\nempirical evaluations (Arora et al., 2023). A common design choice is to transform the input tokens into\nkey-value pairs (k1, v1), . . ., (kT, vT) and queries q1, . . ., q\u012b before mixing them, following the success\nof transformers (Vaswani et al., 2017). While transforming inputs into query-key-value tokens has proven\nempirically effective, the field has lacked theoretical principles for designing sequence layers with associa-\ntive memory capabilities. We address this gap through our test-time regression framework, which provides\na principled recipe for deriving sequence architectures with mathematically grounded recall abilities."}, {"title": "Associative memory as regression.", "content": "From our everyday experience, we already have an intuition for\nwhat an associative memory should behave like; for example, hearing a friend's name should trigger a\nmental impression of that friend. This pairing is called a \u201ccue\u201d and a \u201cresponse\u201d. To reflect terminology\nin deep learning (Vaswani et al., 2017), we will refer to cues as \u201ckeys\u201d and responses as \u201cvalues\u201d. Given a\nset of associations (k1, v\u2081), . . ., (kT, v\u012b), an associative memory system is a system that performs associa-\ntive recall: return vi when given k\u012f. Such a mapping between different sets of related objects is known\nas a hetero-associative memory in classic signal processing and neurocomputing literature (Hinton and\nAnderson, 1989).\nWith this intuition, we can now define a mathematical model for associative memory. Given key-value"}, {"title": "A recipe for designing your own architecture.", "content": "Although Equation 1 succinctly summarizes our\nframework, it leaves many details unspecified. For example, over which class of regressors M should\nwe consider? Additionally, which associations should we prioritize memorizing? Finally, which optimiza-\ntion procedure should we use to minimize the loss? The regression-memory correspondence thus allows\nfor a vast design space. We partition this wide landscape into three simple design choices:\n1. the relative importance of each association, specified through weights in Equation 1,\n2. the function class M over which we search,\n3. the minimization algorithm,\ngiving us a formulaic \"recipe\u201d for deriving sequence layers that can perform associative recall."}, {"title": "Deriving existing architectures from regression", "content": "In this section, we consider causal sequence models that construct a new m\u2081 at each timestep using as-\nsociations At = {(k1, V1), . . ., (kt, vt)}. The case for non-causal sequence models easily generalizes,\nfollowing our previous discussion."}, {"title": "Notation.", "content": "We first explain the mathematical notation we will use in our derivations. We use bold lower-\ncase letters to refer to vectors (e.g. x, k, v) and bold uppercase letters to refer to matrices (e.g. M, K, V).\nAssume we have a sequence of T inputs x1, ..., x\u012b which have already transformed into a sequence of\nkeys and values (k\u2081, v\u2081), . . ., (k\u0442, \u0475\u0442). We will use these keys and values as the dataset for our test-time\nregression problem. Let Kt = [k1... kt] T \u2208 RtxDk and Vt = [V1... Vt] T \u2208 RtxDv be matrices that\ncontain t keys and t values respectively, for t = 1, . . ., T. Each key/value vector is a row of the key/value\nmatrix. To minimize clutter, we will use the shorthand K and V instead of Kt and Vt when the timestep\nt is clear from context."}, {"title": "Vignette 1: Linear attention is (suboptimal) linear least squares regression", "content": "We start by deriving an architecture from linear least squares, the most classic regression method. Linear\nleast squares corresponds to the following choices in our recipe.\n1. Choice of weights: assign equal weight to each association,\nMLINEAR, t = argmin1MEMLINEAR2\u2211ti=1||vi \u2013 m(ki)||\n\n2. Choice of function class: linear functions MLINEAR = {m | m(k) = Mk, M\u2208 RDv\u00d7Dk}.\n\n3. Choice of minimization algorithm: analytical solution equivalent to one step of Newton's method\u00b9.\nSince MLINEAR, t is a linear function, it is equivalent to a linear map M\u2081 \u2208 RDv\u00d7Dk. Minimizing the linear\nleast squares objective results in the solution:\nMt = argmin1M\u2211ti=1||vi Vi - Mki ||2=\u2265 Dk,VTK(KTK)\u22121, overdetermined, e.g. tVT(KKT)\u2212\u00b9K, underdetermined, e.g. t < Dk,\n\nwhere we choose the min-norm solution in the underdetermined case. The overdetermined case corre-\nsponds to K having linearly-independent columns, which is not possible if t < Dk, while the underde-\ntermined case corresponds to K having linearly-independent rows, which is not possible if t > Dk. We\nrestrict our discussion to the case when t > Dk, which is the case for typical applications. Applying Equa-\ntion 3 as a layer was previously explored by von Oswald et al. (2023) (who called it a mesa-layer) in the\ncontext of understanding deep transformers and by Garnelo and Czarnecki (2023) (who called an intention\nlayer) on the expressivity of self-attention, but here we derive it as mathematical consequence of requiring\nthe sequence model to be able to perform associative recall."}, {"title": "Relationship to linear attention.", "content": "Although MLINEAR, t is the optimal linear memory with respect to\nour objective function, it can be expensive to compute, requiring us to invert a Dk \u00d7 Dk matrix at every\ntimestep. One crude but hardware-efficient simplification would be to assume that K\u00afK = I, avoiding\nmatrix inversion completely, at the cost of some recall ability. After making this rough approximation, we\narrive exactly at the equations for linear attention: yt = Mtqt \u2248 VKqt = \u2211ti=1 viki qt.\n\nOur derivation clarifies exactly how linear attention fails as an associative memory. When the keys\nare zero mean, KTK is proportional to the empirical covariance matrix of the keys. Thus linear attention\nis a crude associative memory that ignores the covariance between the dimensions of the key vectors. In fact,\nit is an optimal linear memory only when t \u2264 Dk and the keys are orthonormal ki kj = \u03b4\u03af\u03bb.\nFrom a training stability perspective, the inverse covariance factor helps prevent the output yt from\nbecoming unbounded. We can bound the norm of the output of a least squares layer (Equation 3) by\n||yt||2 = ||VTK(KTK)\u00af\u00b9qt||2 \u2264 ||qt||2 (\u2211ti=1||Vi||2||ki||2) /Amin (KTK) where Amin (KTK) is the min-\nimum eigenvalue of KK. See Appendix A for the derivation. When we ignore the covariance matrix,\napproximating it with the identity, the denominator disappears and the output yt can grow arbitrarily\nlarge with the sequence length, leading to training instability. This instability indeed shows up in practice\nfor linear attention, and Qin et al. (2022) proposed output normalization to stabilize training. Our deriva-\ntion shows that output normalization works by approximately restoring the self-normalizing property of\nEquation 3 that linear attention loses. We will see in Section 4 that ignoring the covariance causes linear\nattention to perform worse at associative recall."}, {"title": "Efficient computation.", "content": "Conveniently, we can compute Mt in Equation 3 for t = 1, . . . T in parallel or\nsequentially. To compute them in parallel, we solve a batch of T independent linear systems. To compute\nthem sequentially, we note that (KtTK+)\u22121 = (Kt_1Kt + ktkt\u00af)\u22121, allowing us to autoregressively\ncompute the inverses via the Woodbury matrix identity. The sequential form is known as online linear re-\ngression or recursive least squares (RLS) (Haykin, 2014). Thus during training we can use the parallel form,\nand during testing we can unroll the recurrence into a recurrent neural network. In prior deep learning\nliterature, this kind of unweighted linear regression layer has been called an intention layer (Garnelo and\nCzarnecki, 2023) and a mesa-layer (von Oswald et al., 2023), though not motivated by associative memory."}, {"title": "Nonlinear associative memory.", "content": "So far we have only considered linear associative memory imple-\nmented by linear functions MLINEAR which have limited expressiveness. We can derive nonlinear memo-\nries by using a feature map & : RDk \u2192 RD, replacing each key k\u2081 with $(k\u2081). Then our function class\nis defined by M\u2081 = {m | m(k) = M\u00f3(k), M \u2208 RDv\u00d7D$} and we solve the least-squares objective\nargminm \u2211ti=1||vi \u2013 M\u00f3(ki)||2/2. Solving this least squares objective for M results in an optimal asso-\nciative memory that is nonlinear with respect to the input keys. Parameterizing associative memory with\na nonlinear feature map has a long history, dating back to Poggio (1975) which found polynomial feature\nmaps to be highly effective, consistent with more recent works (Zhang et al., 2023; Arora et al., 2024).\nIf we make the same crude approximation as linear attention of disregarding the covariance among\n{$(ki)}=1, we arrive at a suboptimal nonlinear memory. Thus, recent works that apply linear attention\nwith a nonlinear feature map, such as Schlag et al. (2021); Kasai et al. (2021); Zhang et al. (2023); Chen et al.\n(2024), and others, can be understood within our framework as suboptimal nonlinear memory as well."}, {"title": "Vignette 2: Gated linear attention and state-space models are (suboptimal) weighted linear least squares regression", "content": "Classic regression theory tells us that unweighted linear regression works best when the data is stationary\n(Hastie et al., 2009; Haykin, 2014). However, time-series data and text data are typically non-stationary,\nand recent tokens are more informative of predicting the next token. Thus a natural requirement is to"}, {"title": "Vignette 3: Fast weight programmers and online learners are first-order methods for solving streaming least-squares", "content": "Up until now we have derived linear associative memory architectures through direct analytical solution,\nequivalent to one step of Newton's method, a second-order optimization method. However, second-order\niterative methods are computationally expensive, requiring matrix inversions at each step. On the other\nhand, disregarding the matrix inversion as done by linear attention leads to poor performance, since it\nignores the covariance structure of the data.\nA computationally cheaper alternative is to apply a first-order gradient-descent method, corresponding\nto the following design choices:\n1. Choice of weights: assign equal weight to each association,\nMLINEAR, t = argmin1MEMLINEAR2\u2211ti=1||vi - m(ki)||2.\n\n2. Choice of function class: linear functions MLINEAR = {m | m(k) = Mk, M \u2208 RDv\u00d7Dk}.\n\n3. Choice of minimization algorithm: gradient descent, possibly with adaptive step sizes, momentum,\nor weight decay (L2 regularization).\nWe can now derive the test-time regression layer corresponding to a first-order iterative method.\nSince we consider linear functions, solving Equation 7 is equivalent to minimizing the objective\nt\u2211i=1Li(M) = t\u2211i=112||vi - Mki||2,\n\nusing its gradient\nVL(M) = t\u2211i=1VL(M) = t\u2211i=1(Mki - viki = (MKT \u2013 VT)K.\n\nto find Mt. Let the initialization be at the origin M\u2070) = 0. Iteratively solving this objective with full-batch\ngradient descent with step size B\u2081 at each iteration results in\nM(2) = M(-1) \u2013 BL(M(-1))\n\n= M-1) \u2013 B\u2211tj=1VLj(M-1))\n\n= M(-1) \u2013 \u03b2t\u2211tj=1 (\u039c(1) kj - jkjT,\n\n= M(-1) (1 \u2013 \u2211tj=1\u03b2kjkjT+ \u03b2 \u03a3tj=1vjkjT.\n\nAfter I iterations, we produce the linear memory represented by Mt = M(I)."}, {"title": "Preconditioning gradient descent.", "content": "Consider the I = 1 case with step size \u03b2\u2081 = 1. Since we initialize\nat the origin, we obtain the equation for linear attention\nM(1) = M(0) \u2013 VL(M(0)) = VTK,\n\na result also shown by (Sun et al., 2024; Irie et al., 2022).\nA standard technique to accelerate the convergence of gradient descent is to precondition the gradient\n(Boyd and Vandenberghe, 2004). The optimal preconditioner in this case, due to the quadratic objective, is\nthe Jacobian of Lt, J\u00a3\u2084(M) = K\u00afK. This preconditioner allows gradient descent to converge in a single\nupdate\nM(1) = M(0) - VL(M(0) J\u00a3\u2081 (M(0)) -1 = VTK(KTK)-1,\n\nreproducing the analytic solution from Equation 3. The preconditioned recurrence is equivalent to one\nstep of Newton's method, a second order iterative method.\nBy taking this iterative optimization perspective, we see that linear attention and Equation 3 lie on\ntwo extremes: batch gradient descent with no preconditioning versus batch gradient descent with perfect\npreconditioning. The difference in performance comes from whether the optimizer accounts for the cur-\nvature of the objective, governed by the covariance of the keys (Boyd and Vandenberghe, 2004). Exploring\nother preconditioners, e.g. quasi-Newton methods, that interpolates between the two extremes may be an\ninteresting direction for future research."}, {"title": "Online learners as stochastic gradient descent.", "content": "An alternative to full-batch gradient descent is stochas-\ntic gradient descent (SGD) where we use a subset of the samples to compute the gradient at each step. This\nis particularly convenient in causal sequence modeling where keys and values typically are presented one\nat a time, rather than all at once. In this streaming setting, we can instead perform single-example SGD to\niteratively minimize Equation 1. In the case of linear memories MLINEAR, Equation 11 with a single-example\nbecomes\nMt = Mt\u22121 - \u1e9et\u2207Lt(Mt\u22121)\n\n= Mt\u22121 + Bt(vt \u2013 Mt\u22121kt)ktT\n\n= Mt-1(I - BtktktT+ BtvtktT,\n\nwhere Mt is the matrix representation of mt. This is exactly the recurrence of DeltaNet (Schlag et al., 2021;\nYang et al., 2024c). As we perform stochastic gradient descent, we produce an online sequence of memories\n{M}_1, each of which is an associative memory mi for key-value pairs up to (ki, vi). The stochastic\ngradient approach to streaming inputs has the advantage that mt focuses more on recent key-value pairs,\nrather than uniformly over all timesteps as in the case of unweighted RLS and unweighted linear attention.\nThis recency bias allows online learners to handle non-stationary data such as text and time-series.\nEquation 18 is known by many names, such as the delta rule or the Widrow-Hoff algorithm (Widrow\nand Hoff, 1988). Within the adaptive filtering literature this algorithm is known as Least-Mean Squares\n(LMS) (Haykin, 2014); there it is one of the most common algorithms for handling non-stationary signals.\nClassic neurocomputing literature (Kohonen, 1989, Equation 6.33) has also previously studied using gradi-\nent descent with a single example to update an associative memory map. The iterates M\u0165 are also known as\n\"fast weights\", which were originally proposed by Schmidhuber (1992) as an internal learning/optimization\nprocedure executed at test time, in contrast to the \"slow weights\" of a neural network that are updated\nonly at training time. It is also possible to apply Equation 16 to iteratively produce nonlinear regressors\n(Sun et al., 2024)."}, {"title": "Adaptive step sizes.", "content": "Adaptive step sizes is an intuitive improvement to our gradient-descent-based\nrecurrent architecture. A common way to derive adaptive step sizes in general is by leveraging the re-\nlationship between gradient descent and online learning (Cesa-Bianchi et al., 2004; Zinkevich, 2003), a\nconnection that has produced many successful algorithms in the past, such as Adagrad (Duchi et al., 2011)\nand Adam (Kingma and Ba, 2015). Recently, Liu et al. (2024) derived an effective sequence layer called\nLonghorn from an online learning perspective (Kulis and Bartlett, 2010), solving\nMt = argminM12||M-Mt-1||2+12|vt - Mkt,\n\nresulting in the recurrence\nMt = Mt-11+\u03b4\u03b5||kt||2\u2212 ktktT+1+\u03b4\u03b5||kt||21svtkt.\n\nComparing the Longhorn update in Equation 20 to the SGD update in Equation 18, we see that Longhorn's\nlayer is equivalent to SGD with an adaptive step size of \u1e9et = 1/(1+dt||kt||3). This alternative perspective\nreflects the fact that online learning is essentially equivalent to stochastic optimization (Cesa-Bianchi et al.,\n2004; Duchi et al., 2011; Hazan, 2022).\nIn Equation 19, \u03b4t controls how well Mt memorizes the latest association (kt, vt). When dt \u2192 \u221e,\nEquation 19 is equivalent to a constrained optimization problem\nMt = argmin1M||M-Mt-1|| subject to Mikt = vt.\n\nSolving this constrained optimization problem results in a step size \u1e9et = 1/||kt||3 with update\nMt = Mt-11||kt||3ktktT+1||kt||2vtkt.\n\nThis special case is also known as the normalized LMS (NLMS) update in adaptive filtering (Castoldi and\nde Campos, 2009) and known as the projection method (Tanabe, 1971) or Kaczmarz's method (Kaczmarz,\n1937) in optimization theory.\nAlthough here we discuss test-time regressors with adaptive step sizes that only depend on the current\ntimestep, more sophisticated step sizes with long term dependencies, such as Adam (Kingma and Ba, 2015),\nhas been explored by Clark et al. (2022) but are more challenging to parallelize. Given that adaptive opti-\nmizers are now a staple of modern deep learning, having come a long way from naive gradient descent, an\ninteresting future line of research would be to integrate better adaptive step sizes into test-time regressors."}, {"title": "L2 regularization and momentum.", "content": "In addition to adaptive step sizes, we can add regularization and\nmomentum. We focus on L2 regularization/weight decay here, but concurrent work by Behrouz et al.\n(2024) derives the case with momentum. Consider L2 regularizing Equation 8 to produce the objective\nfunction\nLreg,t(M) = \u2211ti=1Li(M) +At2|IM||2\n\nwhere \u03bb\u0165 \u2208 [0, 1] is the regularization strength. When we minimize this regularized objective with single-\nexample stochastic gradient descent, we obtain the recurrence\nMt = Mt\u22121 - \u03b2t [\u2207Lt(Mt\u22121) + AtMt\u22121]\n\n= (1 - \u03b2txt)Mt\u22121 - \u1e9et\u2207Lt(Mt-1)\n\n= (1 \u2013 \u03b2tt)Mt-1 + Bt(vt \u2013 Mt\u22121kt)ktT"}, {"title": "Vignette 4: Softmax attention is nonparametric regression", "content": "Having exhaustively derived multiple associative memory architectures via linear and nonlinear para-\nmetric regression, we can now derive self-attention-like architectures using the tools of nonparametric\nregression."}, {"title": "Unnormalized softmax attention is (suboptimal) kernel regression.", "content": "We start first from kernel re-\ngression as most common nonparametric regression method (Sch\u00f6lkopf and Smola, 2001), applying the\nkernel trick to our feature-mapped linear memory. Let k : RDk\u00d7Dk \u2192 R be a Mercer kernel such that\nk(ki, kj) = (\u03a6\u03a6)ij where \u03a6 = [(k\u2081)...(kt)] \u0f0b \u2208 Rt\u00d7D\u00a2 and \u00a2 : RDk \u2192 RD is an infinite-\ndimensional feature map. Following our recipe for model design, we make the following design choices:\n1. Choice of weights: assign equal weight to each association\nmk, t = argmin1MEMk2\u2211ti=1||vi - M(ki)||2,\n\nwhere & is an infinite-dimensional feature map.\n2. Choice of function class: linear functions Mk = {m | m(k) = M\u00f3(k), M\u2208 RDvxDk}.\n3. Choice of minimization algorithm: analytical solution."}, {"title": "Softmax attention with QKNorm is a locally-constant nonparametric regressor.", "content": "Local polyno-\nmial regressors is another classic class of nonparametric estimators (Wasserman, 2006), which have been\nhistorically used as nonlinear smoothing functions in data analysis (Fan, 2018). Given a query q \u2208 RDk at\ntime t that we would like to use to retrieve memory/make a prediction, let our function class be Ma, the\nset of all vector-valued functions that are locally a degree p polynomial around points similar to q, as mea-\nsured by some similarity function s that only depends on the distance to q, i.e. s(k, q) = s(k-q). A classic\nexample is the exponential smoothing kernel Sexp, B(k, q) = exp(-||k \u2013 q||3/B) where B is a bandwidth\nparameter controlling the local neighborhood size around q. Typically local polynomial regression is done\nfor 1-dimensional data; here we generalize it to the multivariate case.\nNonparametric regression with local polynomial estimators corresponds to the following design choices:\n1. Choice of weights: assign weights to associations based on similarity to q,\nm(p,q), t = argmin1MEM\u2211ti=1 Villvi \u2013 m(ki)||2\n\nwith weights given by Yi = s(ki, q) = s(ki - q). We choose weights that are monotonic in distance\nto ensure that m(p,q), t is an accurate approximation within a local neighborhood of q defined by s.\n2. Choice of function class: Ma, the class of polynomials around q, such that any m\u2208 M can be\nwritten as:\nm(ki) = M(0) + M(1) (ki - q) + M(2) (ki \u2013 q, ki \u2013 q) + . . . + M(P) (ki - q, ..., ki q)\np arguments\n\nwhere M(i) is an order j multilinear map\u00b2. This can be thought of as an order p Taylor expansion\napproximation of the true key-value mapping around q.\n3. Choice of minimization algorithm: analytical solution. Solving for the minimum is equivalent to\nsolving for the set of tensors (M(0), . . ., M(p)) that minimize the least squares objective. Our pre-\ndiction on q is then m(p,q), t(q) = M(0).\nUnfortunately, solving Equation 33 is computationally expensive for large p. Instead, we can restrict our-\nselves to the simplest case of p = 0, corresponding to the class of locally constant functions around q,"}, {"title": "Higher-order attention.", "content": "Equation 33 suggests a path to higher-order generalizations of self-attention\nby considering p > 0 at the cost of more computation. By reducing Equation 33 to a weighted least squares"}, {"title": "Constructing effective key-value pairs", "content": "Having shown that models derived via regression are able to perform associative recall, we now discuss the\nimportance of constructing test-time key-value pairs that are pertinent to the task at hand. This echoes\nan unchanging principle in machine learning: a well-designed model is only as effective as the data it\nprocesses. Historically, query-key-value sequences were constructed via a linear projection on the cor-\nresponding timestep: qt = WQxt, kt = WKxt, vt = Wyxt (Vaswani et al., 2017). Our framework\nnaturally generalizes to the multihead case (Shazeer, 2019; Ainslie et al., 2023), corresponding to setting\nup parallel test-time regression problems, possibly with shared inputs or outputs.\nRecent works on recurrent models, starting with Fu et al. (2022), have identified the importance of\napplying a \u201cshort convolution\u201d to construct the query-key-value sequences, which can express the \u201cshift\u201d\noperation that induction heads learn (Olsson et al., 2022; von Oswald et al., 2023; Fu et al., 2022). The short\nconvolution is a crucial component of purely recurrent language models, resulting in severe performance\ndrops if removed (Yang et al., 2024b,c,a; Sun et al., 2024). In the case of transformers or model that combine\nrecurrence with softmax attention (De et al., 2024), an explicit short convolution is not needed since earlier\nsoftmax attention layers can learn the behavior of a short convolution as empirically observed by von\nOswald et al. (2023). Here we show that this short convolution is crucial to associative recall, creating\nbigram-like key value pairs."}, {"title": "One short conv is all you need for associative recall.", "content": "Consider an abstracted version of multi-query\nassociative recall (MQAR), a standard associative recall task introduced by Arora et al. (2023), in which\na model must be able to recall multiple key-value pairings. In this task, there are P pairs of unique and\nconsistent cue-response tokens {(uj, vj)}j\u2208[P] with each cue u; mapping one-to-one to its response\nvj. The model is given a contextual sequence of randomly drawn, possibly repeated, cue-response pairs\nfollowed by a previously seen cue: (x1, X2, ..., XT\u22121, XT, XT+1) = (Ui1, Vi1,..., UiT/2, ViT/2, Uij). Uij is\na cue that appeared earlier (the jth pair) and the model is expected to output the paired response vij at\ntimestep T + 1.\nWe now show that a single test-time regression layer (e.g. as simple as linear attention) with one short\nconvolution is sufficient to solve MQAR, without any parameters other than the embeddings, as long as it is\nprovided with the appropriate key-value pairs to regress. To solve this task, it suffices to test-time memorize\nall the bigram pairing {(uj, vj)}. Given x1, . . . , Xxy, we causally construct our test-time regression dataset\nas follows. Let the keys be constructed via a short convolution kt = W0Xt + W1Xt-1 = xt\u22121 where\nwo = 0, w\u2081 = 1. Let the queries and values be the same as the inputs: qt = vt = xt. At timestep T + 1,\nwe have qT+1 = XT+1 = ui;. The output of a linear attention layer is then yT+1 =\u2211T+1t=1-1 xtxt-1ui; When the embedding space is large enough, we can set the embeddings such that all tokens are orthonormal. Then xt-1 ui; is nonzero only when xt-1 = u\u2081\u2081, simplifying the output to a sum of only tokens that are followed by uij. Since the cue-response map is one-to-one, the remaining\ntokens are all vij, producing the output YT+1 X Vij, solving the task."}, {"title": "Memory capacity, not sequence length, limits model performance.", "content": "Typical evaluations of models\non the MQAR task look at model performance with respect to the length of the sequence, possibly varying\nthe model capacity. A few examples include Figure 2 of Arora et al. (2023), Figure 8 of Dao and Gu (2024),\nFigure 2 of Yang et al. (2024c), and likely others. However, our above construction shows that the sequence\nlength doesn't matter for memorizers with a large enough memory capacity; once the memory is large\nenough, the difficulty of MQAR is independent of the sequence length T, which may be counter to the"}, {"title": "Conclusion and discussion", "content": "We have presented a unifying framework that views sequence models with associative memory as test-\ntime regressors. Each architecture emerges naturally by specifying three aspects: the relative importance\nof each association, the function class, and the optimization algorithm. Through this lens, we derived linear"}, {"title": "Bounding the norm of a linear regression layer", "content": "Let yt = VTK(KTK)\u00af\u00b9qt and let || A||2 indicate the spectral norm of a matrix A, a matrix norm induced\nby the L2 norm on vectors. Then\n||yt||2 = ||VTK(KK)\u00af\u00b9qt||2\n\n<||qt||2 sup ||VTK(KK)\u00af\u00b9q||2\n||9||2=1\n\n= ||qt||2||VTK(KK)\u00af\u00b9||2\n\n<||qt||2||VK||2||(KTK)-1||2\n\n= ||qt||2||Viki ||2||(KK)-1||2\n\n<||qt||2\u2211ti=1||Vi||2||Ki ||2(KTK)-1||\n\n=||qt||2\u2211ti=1||Vi||2||ki||2 max ((KK"}]}