{"title": "BanglaDialecto: An End-to-End AI-Powered Regional Speech Standardization", "authors": ["Md. Nazmus Sadat Samin", "Jawad Ibn Ahad", "Tanjila Ahmed Medha", "Fuad Rahman", "Mohammad Ruhul Amin", "Nabeel Mohammed", "Shafin Rahman"], "abstract": "This study focuses on recognizing Bangladeshi dialects and converting diverse Bengali accents into standardized formal Bengali speech. Dialects, often referred to as regional languages, are distinctive variations of a language spoken in a particular location and are identified by their phonetics, pronunciations, and lexicon. Subtle changes in pronunciation and intonation are also influenced by geographic location, educational attainment, and socioeconomic status. Dialect standardization is needed to ensure effective communication, educational consistency, access to technology, economic opportunities, and the preservation of linguistic resources while respecting cultural diversity. Being the fifth most spoken language with around 55 distinct dialects spoken by 160 million people, addressing Bangla dialects is crucial for developing inclusive communication tools. However, limited research exists due to a lack of comprehensive datasets and the challenges of handling diverse dialects. With the advancement in multilingual Large Language Models (mLLMs), emerging possibilities have been created to address the challenges of dialectal Automated Speech Recognition (ASR) and Machine Translation (MT). This study presents an end-to-end pipeline for converting dialectal Noakhali speech to standard Bangla speech. This investigation includes constructing a large-scale diverse dataset with dialectal speech signals that tailored the fine-tuning process in ASR and LLM for transcribing the dialect speech to dialect text and translating the dialect text to standard Bangla text. Our experiments demonstrated that fine-tuning the Whisper ASR model achieved a CER of 0.8% and WER of 1.5%, while the BanglaT5 model attained a BLEU score of 41.6% for dialect-to-standard text translation. We completed our end-to-end pipeline for dialect standardization by utilizing AlignTTS, a text-to-speech (TTS) model. With potential applications across different dialects, this research lays the groundwork for future research into Bangla dialect standardization.", "sections": [{"title": "I. INTRODUCTION", "content": "A dialect, also referred to as a regional language, is a distinct version of a language spoken in a specific geographical area, characterized by unique phonetics, pronunciation, and vocabulary [7]. Children naturally acquire it as their first language from their caregivers, without formal grammar instruction. Societal factors such as socio-economic background, education levels, and geographic location further contribute to subtle variations in pronunciation and intonation [8]. Bangla, the language spoken by approximately 160 million people across 64 districts in Bangladesh, is the fifth most spoken language in the world [9]. Interestingly, there are around 55 distinct dialects of Bangla [10]. Addressing these language variations is crucial for creating inclusive and effective communication tools for Bengali speakers. This framework could potentially be expanded to accommodate dialects from other regions globally. Despite the cultural significance of these dialects, research on Bangla regional speech remains limited, primarily due to the absence of a comprehensive database and the complexities associated with handling such extensive and diverse datasets. Dialectal differences often hinder people's access to essential services, effective communication, medical treatment, and education and impede their social and professional interactions. The advancement of automatic speech recognition (ASR) technology is imperative in bridging the gap between human language and machine comprehension [11]. This study aims to tackle the challenges posed by Bengali regional accents in voice recognition, particularly focusing on addressing the complexities of mining big data for this purpose. The project endeavors to remove barriers to effective communication and improve the accuracy and inclusivity of automated speech-to-text systems by developing a robust system capable of recognizing and processing a wide range of regional accents. A reliable ASR system is essential for local communities in regions with diverse accents. Traditionally, researchers have heavily relied on conventional voice recognition methods. (a) DNN-based ASR: Many studies on Bangla ASR have employed the deep neural network (DNN) approach [12]. Researchers have attempted to enhance ASR performance"}, {"title": "II. RELATED WORKS", "content": "Dialect Speech Recognition: Over the past decade, Bangladesh has made significant strides in speech-related studies. The field of ASR research took off in the early 1990s, yielding approximately 75 papers across various Bangla speech-related tasks [18]. With the increasing relevance of Natural Language Processing (NLP), dialect recognition has emerged as a prominent area of interest in computer science. For instance, roughly 230 studies have been conducted on Chinese dialect recognition [19]. Similarly, machine learning and deep learning techniques have been utilized to develop ASR systems for diverse Arabic dialects [20]. Notably, for low-resource languages like the Sudanese dialect, a recognition system achieved a label error rate (LER) of 73.67% using a CNN model and a dataset of 7 hours and 50 minutes [21]. Furthermore, an innovative approach using DeepSpeech [22] was employed to create an ASR system for the Tunisian dialect, achieving a word error rate (WER) of 24.4% and a character error rate (CER) of 18.7% with the TunSpeech dataset [23]. A similar approach was taken by Pan et al. [24] to develop an end-to-end ASR system for the Lhasa dialect in Tibet, addressing limited data challenges with transformer-based models and unique acoustic modeling techniques. In Bangladesh, research on dialect recognition systems is gaining momentum. In 2023, Noor et al. [25] developed a speech-to-text system using a word-matching algorithm for the Noakhali and Chittagong dialects, facing hurdles due to regional pronunciation variations and limited training data. Additionally, Sultana and Hossain [26] utilized deep learning methods to classify Bangla dialects and gender across seven local languages. Recognizing the limitations of Bangla dialect datasets, in this investigation, we have constructed a Bangla dialect dataset and leveraged OpenAI's Whisper model dialect speech recognition. Bangla Language MT: The field of multilingual machine translation (MT) has significantly improved cross-language communication. Notably, recent efforts have involved translating Bangla to English using a sequence-to-sequence (seq2seq) model with an attention-based recurrent neural network (RNN) [27]. While it is uncommon to translate regional dialects into standard Bangla [10] successfully implemented the BanglaT5 [28] and mT5 [29] models, achieving a BLEU score of 69.06% and a word error rate of 15.48% for the Mymensingha dialect, as well as a 47.38% BLEU score for the Noakhali dialect with 2500 samples. Milon et al. [30] focused on converting the Chittagonian dialect using a word-to-word mapping technique and a multilingual lexicon. Given the minimal availability of the Noakhali dialect dataset in recent research, we incorporated a more extensive vocabulary from the Noakhali regions to develop a more robust translation model. Bangla Text-To-Speech System: The recent years have witnessed significant advancements in developing an efficient Bangla TTS system. Initially, Alam introduced a Bangla TTS system using the open-source Festival toolkit [31]. Subsequently, in 2019, students from Shahjalal University of Science and Technology developed a Bangla TTS based on a Deep Neural Network, leveraging a 40 hours speech dataset comprising 12,500 utterances, which outperformed other existing systems at that time [32]. Building upon this progress, the same authors introduced the first end-to-end Bangla TTS system two years later, trained on 20 hours of speech data, which led to further performance enhancements [33]. While much attention has been given to the development of Bangla TTS systems, only a few researchers have focused on integrating TTS into dialect recognition systems in Bangla. For instance, Begum et al. [34] developed a TTS synthesis system for the Mymensingha dialect using the Festival toolkit, albeit encountering issues with rhythm in long sentences and a small dataset. The main limitations in current Bengali dialect speech recognition, translation, and TTS studies are the scarcity of available dialect-specific databases and the dependence on traditional methods, hindering comprehensive research in this field. In this study we constructed a large-scale Bengali dialect dataset, particularly focusing on the Noakhali dialect."}, {"title": "III. METHODOLOGY", "content": "In this study, we intended to develop an end-to-end pipeline for converting Bengali dialect speech into the standardized Bangla speech signal. Prior research was done in this field separately focusing on either ASR or MT. We aim to integrate ASR, MT, and TTS for generating standard Bangla speech from dialectal Bangla speech signals. Problem Formulation: Consider there are n number of dialect speech signals, where $n \\in N$. For each dialect speech signal, there is a corresponding dialect text, and considering the semantic structure of the Bengali dialect and standard form, for each dialect text, there is a unique standard text. Specifically, for the ith sample, the dialect speech signal is $s^i$, the corresponding dialect text is $t_d^i$, and the standard text is $t_s^i$. We aim to build model $F_1$ to transcript $s^i$ into corresponding $t_d^i$ and model $F_2$ to predict and generate $t_s^i$ from $t_d^i$. The objective is to build a two-stage model to convert dialect speech into standard text. This study focuses on accurately generating the standard text $t_s^i$ from the dialect speech signal $s^i$ leveraging LLMs. Then a TTS model will be used to complete the end-to-end pipeline through text-to-speech generation. In this study, we focused on addressing two key challenges. (a) Managing large-scale speech signals: Traditionally, Bangla ASR has relied on deep learning approaches. However, the emergence of LLMs in natural language processing has shown their potential in speech recognition for low-resource languages, as indicated by Liu et al. [16] Although researchers are working to enhance the performance of low-resource ASR by leveraging transformer-based models, their application in Bangla speech-related tasks remains largely unexplored. A significant obstacle is the dearth of extensive and diverse audio datasets necessary for effectively fine-tuning ASR models for these tasks. The use of very small datasets has hindered research on the Bangla dialect, leading to less accurate performance [25]. Our primary objective is to address these limitations by utilizing a large audio dataset that has not yet been utilized for Bangla dialect recognition or translation purposes. Through extensive fine-tuning of ASR and LLMs on speech datasets, our goal is to develop models capable of capturing linguistic variations. (b) Improving performance in dialect translation: LLMs are pretrained in standard Bangla language, making it challenging to capture dialectal vocabulary variations without extensive fine-tuning [35]. Modifying these models with large datasets covering"}, {"title": "A. NDD: Noakhali Dialect Dataset", "content": "Understanding the various regional dialects of Bangladesh presents significant challenges, leading to a lack of available datasets. Data collection was therefore a critical and difficult aspect of this project. According to the literature, no speech signal dataset for the Noakhali dialect has been available to date. So, we manually gathered accented audio samples from diverse sources, including YouTube videos and Facebook groups shared among native speakers, and conducted interviews with residents in Noakhali, Bangladesh. Participants were asked to read a standard paragraph in their native accent, and these recordings were used to develop our dataset. In total, we collected 10 hours of Noakhali regional speech data, denoted as $s^i$. The Noakhali Dialect Dataset, NDD comprises three columns: Noakhali regional speech data, $s^i$; each speech signal's $s^i$ annotated dialect text version, $t_d^i$; and standard Bangla translation form $t_s^i$ of each dialectal text. The annotation is conducted by highly qualified human evaluators from Noakhali. We gathered data from 24 humans, 18 of whom were men and 6 of whom were women. Mostly, they are aged between 18 and 28. Demographic details are shown in table I. This constructed NDD dataset then undergoes preprocessing to handle big data management challenges. The following preprocessing NDD dataset is utilized for fine-tuning the ASR model and LLM for the dialect speech-to-dialect text transcription and dialect text-to-standard text translation tasks."}, {"title": "B. Preprocessing: Denoising and Splitting Voice Data", "content": "Before preparing for fine-tuning ASR, each dialect speech signal, $S^i$ was denoised to remove background noise and improve clarity, ensuring higher-quality inputs for the model. This step is essential for accurate transcription and translation. Managing Long Speech Data: To manage long speech signals, each denoised signal $s^i$ was split into smaller, 5-second segments. Given a speech signal $s^i$ of total duration $T^i$, it was divided into $N^i$ segments:\n$N^i = \\frac{T^i}{5}$\nEach segment $s^i_k$ (where $k = 1, 2, ..., N^i$) represents a 5- second portion of $s^i$, defined as: $s_k^i = s^i [(k - 1) \\cdot 5 : j \\cdot 5]$ If the total duration $T^i$ is not a multiple of 5, the remaining portion of the signal, $s_{remainder} \\in S$, is truncated to fit the architecture of transformer models. This ensures that all input segments are of equal length, aligning with the fixed-length input requirement of transformers. Text Splitting Mechanism: After splitting the dialect speech signal $s^i$ into 5-second segments, the corresponding dialect text $t_d^i$ and standard text $t_s^i$ were also split to align with the speech segments. The text splitting follows the same segmentation process to ensure that each chunk of $s^i$ can be fine-tuned with its corresponding text segment. For the $i^{th}$ data sample, the dialect text $t_d^i$ and the standard text $t_s^i$ are split into $N^i$ segments:\n$t_d^i = \\{t_{d, 1}^i, t_{d, 2}^i, ..., t_{d, N^i}^i\\}$\n$t_s^i = \\{t_{s, 1}^i, t_{s, 2}^i, ..., t_{s, N^i}^i\\}$\nEach segment $t_{d,k}^i$ and $t_{s,k}^i$ corresponds to the dialect and standard text that matches the speech segment $s_k^i$, ensuring synchronization. This alignment allows each 5-second chunk of the speech signal $s_k^i$ to be fine-tuned with the corresponding chunk of dialect text $t_{d,k}^i$ during the first-stage fine-tuning, and with the standard text $t_{s,k}^i$ during the second stage. This process ensures coherent and accurate training across both the speech-to-text and text-to-text transformations, matching the segmented inputs."}, {"title": "C. Employing ASR and LLM in dialect standardization", "content": "We employ ASR and LLM in two scenarios: (a) converting dialect speech to dialect text and (b) generating standard text from dialect text. (a) Convert Dialect Speech to Dialect Text: Fine-tuning is essential for adapting pre-trained models to specialized tasks, improving relevance and accuracy. In this study, we fine-tuned models for both speech-to-text conversion and translation. Pre-trained models often struggle with dialect recognition and nuanced language variations. By fine-tuning, we customized the models to capture the linguistic features of the Noakhali dialect for improved speech-to-text accuracy. Whisper ASR models\u2014base (74M), small (244M), medium (769M), and large (1550M) were fine-tuned on the NDD dataset. Each speech segment $s_k^i$ was paired with its corresponding dialect text $t_{d,k}^i$, forming the input-output pair ($s_k^i$, $t_{d,k}^i$). The objective of fine-tuning is to generate accurate $t_{d,k}^i$ from $s_k^i$ through LLMs. (b) Text standardizing from dialect: To generate standard Bangla text from dialect text, we fine-tuned several models: XLM-ProphetNet (616M parameters), mBART50 (611M), IndicBART (244M), mT5 (base, 582M), and BanglaT5 (247M). Fine-tuning these models on the NDD dataset enabled them to adapt to dialect-specific linguistic features, ensuring more accurate translation into standard Bangla $t_{s,k}^i$. The dialect text $t_{d,k}^i$ served as the input, and the corresponding standard Bangla text $t_{s,k}^i$ as the target output, improving the models' ability to standardize text while preserving linguistic coherence. Model Architecture: We utilized three different models for our study. (a) Some studies have already been done in low-resource languages, and authors got better performance by fine-tuning the whisper ASR model. They encourage us to use this whisper ASR model in our study to see how whisper fine-tuning works in the Bangla dialect. Whisper [46] is a powerful speech recognition system created by OpenAi. It utilized an encoder-decoder transformer architecture to transcribe and translate speech in many languages effectively. Whisper, trained on a large dataset of 680,000 hours of multilingual audio, can generalize across varied linguistic contexts and acoustic surroundings. It has 5 versions, tiny with the lowest parameters of 39M and large with 1550M parameters."}, {"title": "IV. EXPERIMENT", "content": "A. Setup Dataset: We fine-tuned the ASR and LLMs using our constructed dataset, NDD. After applying the pre-processing strategy on big data $s^i$, $t_d^i$ and $t_s^i$, and the data samples turned to manageable $s_k^i$, $t_{d,k}^i$; and $t_{s,k}^i$ segments. After segmentation, the NDD dataset comprises 7200 sample data points. We partitioned this dataset into training, validation, and testing sets. The training set consists of 6270 samples, the validation set contains 810 samples, and the test set includes 120 samples. This division ensures that the models are trained on a substantial portion of the data, with a smaller portion reserved for validation and testing, which is a standard and thereby enables rigorous evaluation of model performance. Implementation Details 1 : Before proceeding towards fine-tuning ASR, (a) we used noisereduce library 2 to reduce the noise in wav form audio signal. Then pydub library 3 was employed to partition the big audio data $s^i$ into smaller manageable segments, $s^i_k$. Annotation was done manually on these dialect audio signals, $s^i_k$, with dialect transcription $t_{d, k}^i$, and then, in a similar way, dialect text $t_{d,k}^i$ was translated to standard Bangla text $t_{s,k}^i$.\nEvaluation metrics: In the evaluation process, two steps are involved: one for assessing the performance of transcript dialect voice to dialect text and another for translating dialect text to standard text. For evaluation purposes, the following metrics are utilized: (a) Character error rate(CER): CER [50] is an evaluation metric used to assess the accuracy of speech recognition and translation systems by quantifying the number of character-level errors, including insertions, deletions, and substitutions in the machine-generated text as compared to the reference text. A lower percentage indicates better performance in the generated text. (b) Word error rate(WER):WER [51] is a widely used metric to gauge the model performance on speech recognition or machine translation systems. It computes the disparity between the generated output and the reference text in terms of words. It evaluates the performance of the generated text by dividing the errors, such as substitutions, deletions, and insertions, by the total number of words in the reference text. A lower WER signifies that the generated output is closer to the reference text. (c) Bilingual Evaluation Understudy(BLEU): BLEU [52], introduced by IBM in 2001, was one of the pioneering metrics for assessing the performance of machine-generated text, particularly in machine translation tasks. It utilizes the n-grams approach to evaluate the quality of the output text by analyzing the similarity of the generated text to the reference text."}, {"title": "B. Results and Analysis", "content": "In this section, we present the results of our novel approach for converting Noakhali dialect speech into standard Bangla using an integrated, end-to-end framework. Our system combines ASR with OpenAI's fine-tuned Whisper models, MT using BanglaT5, and text-to-speech (TTS) via AlignTTS. To the best of our knowledge, no previous work has integrated these three distinct speech-related tasks into a unified framework. Furthermore, we utilized a large-scale audio and text dataset for fine-tuning the multilingual ASR and LLMs, which is uncommon for Bangla dialects. For comparison, Table II presents evaluation results from previous studies, while Table III illustrates the performance improvements observed in our study. Bangla Dialect Speech Recognition: At the very beginning, we tried different audio models to check the performance of our custom audio dataset. Nowadays LLMs are becoming a source of speech-related tasks [53]. Finally, We have seen the best output is provided by Open Al's whisper large model. We fine-tuned all of the available Whisper models with our constructed NDD dataset. The comparative analysis between pre-trained and fine-tuned Whisper models reveals significant performance improvements in the fine-tuned variants, which is illustrated in Table- III. Specifically, for the Dialect Speech-to-Text (STT) task, the Whisper-large V2 model, when fine-tuned, demonstrates a substantial reduction in Character Error Rate (CER) and Word Error Rate (WER) compared to its pre-trained counterpart. The pre-trained Whisper-large V2 model exhibits a CER of 135.2% and a WER of 167.5%, while the fine-tuned version achieves a remarkable CER of 0.8% and a WER of 1.5%. This reduction underscores the efficacy of fine-tuning in enhancing the model's accuracy and efficiency in transcribing dialectal speech, highlighting its superior capability to handle complex linguistic variations with greater precision. In contrast, the pre-trained Whisper models, including Whisper-base, Whisper-small, and Whisper-medium, exhibit higher CER and WER values, indicating their limited effectiveness in addressing dialectal speech nuances without fine-tuning. The improvement in fine-tuned Whisper models demonstrates their enhanced adaptability and performance in specialized tasks, making them significantly more effective for practical applications in speech-to-text technology. The fine-tuning process not only improves the accuracy of these models but also emphasizes their potential for achieving higher efficiency and impact in real-world scenarios. Performance Analysis of Translation Models: In the Dialect Text-to-Translation (DTT) task, fine-tuning notably enhances model performance. The pretrained models, such as mBART50, IndicBART, and mT5 (base), exhibit comparatively lower BLEU scores, with mBART50 achieving a BLEU score of 0.45% and IndicBART and mT5 (base) scoring 11.9% and 5.6%, respectively. In contrast, the fine-tuned BanglaT5 model achieves a BLEU score of 41.6%, the highest among the evaluated models. This significant improvement demonstrates the efficacy of fine-tuning in refining translation capabilities, particularly for dialect-specific tasks. The fine-tuned BanglaT5 model's superior BLEU score highlights its enhanced accuracy and fluency in translation, making it more effective for handling nuanced dialectal variations and providing more reliable translations, indicating in Table IV. The results underscore the impact of fine-tuning in optimizing model performance and achieving higher translation quality. Our observations include (a) fine-tuning Whisper multilingual models and MT models on a large-scale speech signal dataset, NDD allows them to adapt the complexity of both dialect speech to dialect text and dialect text to standard text tasks, (2) fine-tuned models excel in handling dialectal variations, delivering superior results in both speech-to-text and translation tasks, Whisper-large V2's CER and WER drop dramatically, and BanglaT5 achieves the highest BLEU score among models, and (c) after completing the end-to-end pipeline, the AlignTTS handle the standardized speech,"}, {"title": "V. CONCLUSION", "content": "The project mainly focuses on the concept of Bangladeshi dialect recognition along with converting various Bengali accents into a standardized, formal Bengali voice. The technology recognizes regional sound differences in spoken Bengali and converts them into a clear, neutral form using powerful speech processing technologies. The main goal of this project is to enhance communication in formal contexts, such as medical purposes, media, and education, where a formal language is favored. This project will encourage native people from different regions to participate and experience new things previously difficult for them due to language barriers. This study successfully demonstrates the construction of an end-to-end pipeline for converting Noakhali dialect speech into standard Bangla speech using fine-tuned LLMs. The investigation introduces a large-scale dialectal Noakhali speech signal dataset, NDD. By leveraging Whisper for speech recognition, BanglaT5 for translation, and AlignTTS for speech synthesis, we achieved significant improvements in transcription and translation accuracy. Whisper-large V2 reduced CER and WER, while BanglaT5 delivered high BLEU scores, highlighting the effectiveness of fine-tuning in handling dialectal variations. Regional voice to standard Bangla voice can be a great prospect for our country. People from different regional districts can use it in every aspect of their life. Although focused on the Noakhali dialect, this approach lays the foundation for future research into Bangla dialect standardization, with potential applications across various dialects. Future works: Future improvements should include increasing dataset diversity by incorporating more regional accents and expanding the system's multilingual capabilities to support communication across different languages. Enhancing contextual understanding of dialects can further refine dialect conversion processes, potentially benefiting educational, entertainment, and medical applications for local communities."}, {"title": "ETHICS STATEMENT", "content": "This study was conducted with a firm commitment to ethical standards, particularly in the development and assessment of dialect speech recognition using LLMs within a scientific framework. We reached out to a total of 24 participants from diverse regions of Noakhali, ensuring that participation was voluntary and based on informed consent. The majority of participants were students, many of whom had recently graduated. In order to safeguard the safety and well-being of all individuals involved, significant measures were implemented to minimize any potential physical or psychological risks during the evaluation process. Recognizing the substantial impacts that even minor errors in scientific research can have, we implemented stringent procedures to ensure that all content produced adhered to the highest ethical standards. Our methodology was designed to avoid language or findings that could potentially perpetuate harm or inequality based on race, gender, or other social determinants of health. By upholding these principles, our study was conducted with the utmost ethical integrity, fostering a respectful environment for collaboration between our work and the community at large."}]}