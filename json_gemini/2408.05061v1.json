{"title": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares", "authors": ["Stav Cohen", "Ron Bitton", "Ben Nassi"], "abstract": "In this paper we argue that a jailbroken GenAI model can cause substantial harm to GenAI-powered applications and facilitate PromptWare, a new type of attack that flips the GenAI model's behavior from serving an application to attacking it. PromptWare exploits user inputs to jailbreak a GenAI model to force/perform malicious activity within the context of a GenAI-powered application. First, we introduce a naive implementation of PromptWare that behaves as malware that targets Plan & Execute architectures (a.k.a., ReAct, function calling). We show that attackers could force a desired execution flow by creating a user input that produces desired outputs given that the logic of the GenAI-powered application is known to attackers. We demonstrate the application of a DoS attack that triggers the execution of a GenAI-powered assistant to enter an infinite loop that wastes money and computational resources on redundant API calls to a GenAI engine, preventing the application from providing service to a user. Next, we introduce a more sophisticated implementation of PromptWare that we name Advanced PromptWare Threat (APwT) that targets GenAI-powered applications whose logic is unknown to attackers. We show that attackers could create user input that exploits the GenAI engine's advanced AI capabilities to launch a kill chain in inference time consisting of six steps intended to escalate privileges, analyze the application's context, identify valuable assets, reason possible malicious activities, decide on one of them, and execute it. We demonstrate the application of APwT against a GenAI-powered e-commerce chatbot and show that it can trigger the modification of SQL tables, potentially leading to unauthorized discounts on the items sold to the user.", "sections": [{"title": "1 Introduction", "content": "GenAI (Generative AI) models offer various advantages and are used by content creators, researchers, marketers, developers, and individuals. To ensure the safety of the outputs generated by GenAI models, guardrails and safeguards (e.g., input/output filtering) are integrated into GenAI models to prevent users from misusing/fooling them. However, various jailbreaking techniques have been demonstrated by ongoing research to bypass the integrated guardrails and safeguards of GenAI models and cause them to respond in a toxic manner (e.g., insulting/cursing the user) or provide the users with harmful and dangerous information (e.g., generating instructions for fabricating explosives). While it is clear that users can apply various techniques to jailbreak a GenAI model (e.g., using"}, {"title": "2 Related Work", "content": "Attack Vectors. One line of research explored attack vectors targeting GenAI models, such as direct prompt injection [19] and indirect prompt injection [1].\nOutcomes of Attacks. A second line of research focused on revealing the outcomes of attacks against GenAI models and showed methods to: jailbreak the GenAI model [4, 6, 8, 29], leak the training data or the prompt [2, 18, 20, 27, 28], poison the dialog of a GenAI model with the user [3], and steal parts of the GenAI model [5].\nInputs. A third line of research focused on the types of inputs that could be used to apply attacks against GenAI models and showed that prompts can be injected into text [6, 8, 19, 29], images [3, 4, 12], and audio samples [3].\nAttacks against GenAI-powered Applications. A fourth line of research investigated attacks against GenAI-powered applications. An initial discussion on the security of GenAI-powered applications was raised by [1, 17, 24]. Other studies discussed compromising RAG-based GenAI- powered applications [21, 26, 30]. A recent study presented an AI Worm that targets RAG-powered GenAI applications and demonstrated it against Gemini and ChatGPT-powered e-mail assistants [7]. A recent study presented a new timing attack against ChatGPT [23]. A fifth line of research (but unrelated to this work) also investigated the use of GenAI models for offensive purposes [9, 10, 11, 13, 14]."}, {"title": "3 GenAI-powered Applications & Adversarial Self-Replicating Prompts", "content": "3.1 GenAI-powered Applications\nWe consider a GenAI-powered application to be any kind of application that relies on the GenAI engine to perform its activity. While there are many kinds of GenAI-powered applications, in our study, we focus on GenAI-powered applications that: (1) receive user inputs, (2) use them in the queries sent to the GenAI engine (usually by appending parts/all of them into a base prompt), and (3) determine their execution flow based on the output of the GenAI engine. Such applications include GenAI-powered chatbots (used to interpret user inputs into actions), GenAI-powered email applications (used to classify inputs, generate drafts for replies for incoming emails, etc.), and GenAI-powered personal assistants (used to interpret commands into actions).\nFor example, in GenAI-powered email applications with the functionality of reply generation (using GenAI engine), the user input can be a received email (\"Hi, here is the paragraph you asked me to write about the hi-tech industry in Japan, let me know what you think about it......\"), the base prompt (\"You are an email assistant, please provide a reply to the following email\") and the output can be a draft for a reply (\"Beautiful paragraph\")."}, {"title": "4 PromptWare", "content": "In this section, we show how attackers can craft adversarial self-replicating prompts that force a desired execution flow in GenAI-powered applications (assuming the GenAI-powered application's execution flow is known to the attacker).\n4.1 Threat Model\nAttacker's Objective & Capabilities We assume the attacker intends to force a desired execution flow. The objective of forcing a dedicated application flow can be gaining a financial profit, applying a DoS attack against the application, cheating in a contest, or any other outcome that the attacker wishes to achieve by forcing a desired application flow. We also assume that the logic of the GenAI-powered application (the queries sent to the GenAI server) is known to the attacker which allows the attacker to extract the finite state machine of the application.\nThe logic of the GenAI-powered application could be discovered by the attacker in two ways: (1) By analyzing the application's source code - In GenAI-powered applications whose source code contains the logic of the interface with the GenAI engine (the queries to the GenAI engines are coded on the client side), the attacker can download the application from the Internet, decompile it, and extract the application finite-state machine by reverse engineer the application. (2) By revealing the prompts using prompt leakage attack - the attacker could extract the finite-state machine by applying a preliminary step of prompt leakage/extraction attacks that intended to reveal the prompts used by the application to interface with the GenAI engine [2, 20, 27, 28].\n4.2 Adversarial Self-Replicating Prompts\nWe note that the core of the attack relies on the ability to embed an adversarial self-replicating prompts into the user input. Adversarial self-replicating prompts were first introduced in [7] where they were used to create a worm that targets GenAI-powered email applications that rely on RAG (Retrieval Augmented Generation).\nDefinition. Assuming a GenAI model G with input x and output G(x), an adversarial self- replicating prompt is a prompt that triggers the GenAI model to output the prompt (so it will be replicated next time as well) and perform a malicious activity. More formally, an adversarial self-replicating prompt is a prompt in one of the following forms:\n1.  G(x) \u2192 x. In this case, the input is identical to the output. The input consists of the adversarial self-replicating prompt and the payload, e.g., a picture that serves as a payload (spams the user or spreads propaganda) with a prompt embedded into it. The embedded prompt is replicated by a GenAI model to its output when an inference is conducted.\n2.  G(w || x || y) \u2192 payload || x. In this case, the prompt x (e.g., a jailbreaking prompt), which is located somewhere in the input text (w || x || y) to the GenAI model, causes the GenAI model to output the payload (e.g., toxic content) and the input prompt x.\nWe note that the input to the GenAI model and the output of the GenAI model are not necessarily text input or output as x can also be non-textual inputs/outputs such as images or audio samples (as was demonstrated in prior research [3, 4].\nAn adversarial self-replicating prompt consists of a jailbreaking prompt intended to hijack the GenAI inference. The jailbreaking prompt intended to force the GenAI engine to conduct the following tasks:\n1.  Replicate the input prompt to the output. This piece of text is intended to ensure the persistence of the original prompt for future inferences conducted by a GenAI engine in"}, {"title": "5 Advanced PromptWare Threat", "content": "In this section, we show how attackers can craft adversarial self-replicating prompts that can au- tonomously decide and execute malicious activities based on a real-time process conducted in inference time intended to understand the context of the application, the assets, and the damage that could be applied.\n5.1 Threat Model\nAttacker's Objective & Capabilities As opposed to the previous scenario, in which the attacker's objective was to force a desired execution flow, in this scenario, the attacker does not know the execution flow of the target application because the logic is unknown to the attacker in advance. Therefore, we consider the target application as an unknown environment whose finite-state machine cannot be extracted by the attacker in advance.\nDespite not having prior knowledge regarding the finite state machine of the target application, the attacker still wants to launch an attack against the application. The result of the attack is unknown in advance because the attacker does not know how the application is implemented. Instead of forcing a desired execution using a well-crafted adversarial self-replicating prompt that is intended to obtain a specific goal, the attacker prompts the application with an adversarial self-replicating prompt that guides the GenAI engine towards a target goal.\n5.2 Advanced PromptWare Threat\nIn the previous scenario, the attacker could design an adversarial self-replicating prompt that forces a desired execution flow with no memory unit required to achieve the goal.\nTo launch an attack in an unknown environment, the attacker will have to craft an adversarial self- replicating prompt that will instruct the GenAI engine to apply a kill chain throughout the different inferences conducted by the application while also serving as a memory unit to store the information extracted and gathered throughout the execution.\nThe kill chain consists of three parts (privilege escalation, reconnaissance, and execution) divided into six steps:\n1.  Privilege Escalation - in this step, the adversarial self-replicating prompt jailbreaks the GenAI engine to ensure that the inference of the GenAI engine bypasses the GenAI engine's guardrails and will follow the instructions provided in the prompt.\n2.  Reconnaissance for understanding the context - in this step, the adversarial self- replicating prompt questions/queries the GenAI engine regarding the context of the ap- plication. The context of the application is usually provided to the GenAI engine as part of the query sent by the GenAI application (e.g., \"You are an email assistant...\", \"You are a Q&A bot\", etc.) and could be extracted by the adversarial self-replicating prompt.\n3.  Reconnaissance for identifying the assets in the context - in this step, the adversarial self-replicating prompt questions/queries the GenAI engine regarding the assets of the application: sensitive information (databases), confidential information (information about the user), etc. Such information is usually provided to the GenAI engine (depending on the usecase) to allow the GenAI engine to generate dedicated SQL queries (e.g., in the case of an e-commerce GenAI-powered chatbot, the SQL scheme could be provided to the GenAI engine to interpret a user request into an SQL query), responses for questions (e.g., in the case of Q&A GenAI-powered chatbot or GenAI-powered email application, previous correspondents that may contain sensitive data could be provided to create accurate and personalized answers).\n4.  Reasoning the possible damage that could be applied - in this step, the adversarial self-replicating prompt instructs the GenAI engine to use the information it obtained in the reconnaissance (context, assets) to reason the possible damages that could be done and to enumerate them and their outcome.\n5.  Deciding the damage to apply among the possible alternatives - in this step, the adver- sarial self-replicating prompt instructs the GenAI engine to use the information to decide the malicious activity to perform among the different alternatives.\n6.  Execution - in this step, the adversarial self-replicating prompt instructs the GenAI to perform the malicious activity.\n5.3 Case Study 1: Malicious SQL Activities\nUsecase. Consider a GenAI-powered e-commerce chatbot that is capable of interpreting users' requests by executing SQL queries to databases. The chatbot is implemented using a ReWOO (a"}, {"title": "6 Countermeasures", "content": "In this section we discuss countermeasures.\nLimiting the Length of User Input. We note that in some cases, the application of PromptWares could be mitigated easily by limiting the length of user input. By doing so, attackers will have to find new ways to squeeze the jailbreaking prompt into lower space. We note that this type of countermeasure is not suitable for all usecases because chatbots and personal assistants are intended to analyze text, instructions, emails, etc.\nLimiting the Number of API calls to GenAI engines. We suggest implementing a rate limit on the number of API calls to GenAI engines. The existence of a rate limit will prevent the waste of money on redundant API calls that are triggered by PromptWare which causes the GenAI application to enter an infinite loop and generate infinite calls to a GenAI engine.\nDetecting Jailbreaking Attempts. We suggest implementing a detector intended to identify prompts consisting of text intended to jailbreak the GenAI engine.\nDetecting Adversarial Self-Replicating Prompts. We suggest implementing a detector intended to identify adversarial self-replicating prompts based on their unique form."}, {"title": "7 Discussion", "content": "In this work, we showed how user prompts could flip the behavior of a GenAI engine from serving a GenAI-powered application to attacking and demonstrated PromptWare (that resembles malware in its behavior) and Advanced PromptWare Threat (that resembles APT in its behavior). We consider this work complementary to our previous work [7] on user prompts (that resemble worms in their behavior). We hope that these works will serve as a wake-up call to the industry, motivating R&D departments to analyze the security of their applications against the risks posed by user inputs.\nWe believe that the various forms of PromptWares (that behave as malware, APwT, and worms) will rise in the next decade and pose a significant risk to GenAI-powered applications for the following reasons: (1) we identify a gold rush to integrate GenAI capabilities into existing and new applications that will increase the exposure of applications for such attacks (2) attacks always get better and new risks are revealed every day, (3) and the needed countermeasures to secure GenAI-powered applications against such risks are yet to be developed, and (4) dedicated mechanisms intended to provide isolation between data and instructions (code) are not integrated into GenAI engines. Therefore, attackers can easily blend instructions into inputs that are incorporated into prompts provided by applications to GenAI engines and subvert the entire application's execution. Whether our forecast regarding the rise of PromptWare is true or not is yet to be seen. We hope that our forecast is wrong, although recent incidents seem to support our forecast [16].\nWhile we demonstrated the PromptWare concept against GenAI-powered applications based on Plan & Execute architectures, it is important to note that GenAI-powered applications that are not based on Plan & Execute architectures may also be at risk of being targeted using PromptWares. We demonstrated the concept against Plan & Execute architectures because: (1) they are very popular architectures (2) they create plans on the fly, making them harder to secure and therefore extremely vulnerable to PromptWares, and (3) the plan they create usually consists of a sequence of interactions with a GenAI engine (ensuring that the GenAI engine will conduct at least one inference and therefore PromptWare could be applied).\nOne might argue that the findings of this work are not satisfying because the APwT demonstration that included an SQL attack against a PoC implementation will fail in reality because SQL databases are already secured against attempts of chatbots to write. We note that the objective of the APWT demonstration is to show that GenAI engines could be exploited by attackers using prompts to au- tonomously perform a kill chain intended to conduct a malicious activity in an unknown environment with no prior knowledge of the implementation. We believe that the result of the demonstrated APwT that caused the GenAI engine to output an SQL query that yields a discount for the user proves our claim (whether the SQL server is secured against the query is a different issue, unrelated to our study)."}]}