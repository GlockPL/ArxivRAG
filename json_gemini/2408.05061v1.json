{"title": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares", "authors": ["Stav Cohen", "Ron Bitton", "Ben Nassi"], "abstract": "In this paper we argue that a jailbroken GenAI model can cause substantial harm to GenAI-powered applications and facilitate PromptWare, a new type of attack that flips the GenAI model's behavior from serving an application to attacking it. PromptWare exploits user inputs to jailbreak a GenAI model to force/perform malicious activity within the context of a GenAI-powered application. First, we introduce a naive implementation of PromptWare that behaves as malware that targets Plan & Execute architectures (a.k.a., ReAct, function calling). We show that attackers could force a desired execution flow by creating a user input that produces desired outputs given that the logic of the GenAI-powered application is known to attackers. We demonstrate the application of a DoS attack that triggers the execution of a GenAI-powered assistant to enter an infinite loop that wastes money and computational resources on redundant API calls to a GenAI engine, preventing the application from providing service to a user. Next, we introduce a more sophisticated implementation of PromptWare that we name Advanced PromptWare Threat (APwT) that targets GenAI-powered applications whose logic is unknown to attackers. We show that attackers could create user input that exploits the GenAI engine's advanced AI capabilities to launch a kill chain in inference time consisting of six steps intended to escalate privileges, analyze the application's context, identify valuable assets, reason possible malicious activities, decide on one of them, and execute it. We demonstrate the application of APwT against a GenAI-powered e-commerce chatbot and show that it can trigger the modification of SQL tables, potentially leading to unauthorized discounts on the items sold to the user.", "sections": [{"title": "1 Introduction", "content": "GenAI (Generative AI) models offer various advantages and are used by content creators, researchers,\nmarketers, developers, and individuals. To ensure the safety of the outputs generated by GenAI\nmodels, guardrails and safeguards (e.g., input/output filtering) are integrated into GenAI models\nto prevent users from misusing/fooling them. However, various jailbreaking techniques have been\ndemonstrated by ongoing research to bypass the integrated guardrails and safeguards of GenAI models\nand cause them to respond in a toxic manner (e.g., insulting/cursing the user) or provide the users\nwith harmful and dangerous information (e.g., generating instructions for fabricating explosives).\nWhile it is clear that users can apply various techniques to jailbreak a GenAI model (e.g., using\nimages [3, 4, 12], text [6, 8, 19, 29], and audio inputs [3]), the motivation to jailbreak a GenAI model\nfrom a user perspective remains unclear because: (1) users could easily find harmful and dangerous\ninformation by searching the web (or the dark web) instead of applying complex techniques to\njailbreak the GenAI model and (2) there is no clear benefit for a user of forcing a jailbroken chatbot\n(GenAI model) to insult him/her. Therefore, while jailbreaking (which can be considered as \"privilege\nescalation\") is interesting from an AI perspective, it is not considered a significant security threat to\nend users because it cannot be exploited to create significant harm/risk in a conversation with an end\nuser via reasonable and justifiable threat models.\nHowever, the use of GenAI models is no longer restricted to having conversations with individuals.\nIn the past year, we have witnessed a paradigm shift in application development in which numerous\ncompanies have incorporated GenAI capabilities into new and existing applications and created a\nnew era of GenAI-powered applications. In particular, one rising and promising architecture that is\ncommonly incorporated into GenAI-powered applications is the Plan & Execute framework (which\nis also known as function calling) which leverages the advanced AI capabilities of GenAI engines to\nprocess user inputs, create dedicated plans intended to solve tasks in real-time, and execute the plan\nfrom the GenAI-powered application with the use of the GenAI engine (with minimal coding effort).\nIn this study, we show that a jailbroken GenAI engine can cause significant harm to a GenAI-powered\napplication and facilitate a new kind of attack that we name PromptWare that flips the GenAI model's\nbehavior from serving an application to attacking it. PromptWare is a family of malware that exploits\nan input (provided by a user/attacker) to a GenAI-powered application to trigger malicious activity\nwithin the application context as follows: (1) the malicious input is appended by the GenAI-powered\napplication to the prompt/s provided by the application to a GenAI engine, (2) the input jailbreaks the\nGenAI engine and (3) exploits its capabilities to force outputs that trigger malicious activity within\nthe context of the GenAI-powered application (changing the original flow of the application).\nPromptWares are user inputs consisting of two parts: (1) a jailbreaking command intended to ensure\nthat the GenAI engine will follow the attacker's wish, and (2) additional command/s intended to\ntrigger a desired malicious activity by forcing the GenAI to return the needed output to orchestrate\nthe malicious activity within the application context. Since various jailbreaking prompts and various\nmalicious commands can be given as input to create PromptWare, we consider it a 0-click polymorphic\nmalware, whose form can be changed according to an attacker's objective and according to the target\napplication he/she wishes to attack. Therefore, we show, that in the context of a GenAI-powered\napplication, a jailbroken GenAI model can provide attackers the ability to turn the GenAI engine\nagainst the GenAI-powered application that harnesses its capabilities. This could allow attackers to\ndetermine the execution flow of the GenAI-powered application, forcing various malicious outcomes,\ndepending on the application's permissions, context, implementation, and architecture.\nIn the first part of the paper, we demonstrate a naive variant of PromptWare that targets GenAI-\npowered applications whose GenAI logic (i.e., the queries sent to the GenAI engine) is known to the\nattacker (e.g., via prompt leakage attacks). We show that based on the knowledge of the logic of a\nGenAI-powered application with a GenAI engine, attackers could extract the finite state machine\nof the application and its interface with the GenAI engine. Based on this knowledge, attackers\ncould craft a dedicated user input (i.e., PromptWare) that determines the flow of the application by\nforcing the GenAI engine to output the needed results to \"walk through\" the desired path of states and\nenforce a desired outcome. To demonstrate that attackers can exploit this capability in the context of\na malicious activity, we present the application of PromptWare as malware intended to perform a DoS\nattack against a Plan & Execute-based application. We show that attackers can provide simple user\ninput to a GenAI-powered application that forces the execution of the application to enter an infinite\nloop, which triggers infinite API calls to the GenAI engine (which wastes resources such as money\non unnecessary API calls and computational resources) and prevents the application from reaching a\nfinal state.\nOne might argue that in most cases, the application logic (the queries sent to the GenAI engine)) is\nunknown to attackers, and therefore, in practice, GenAI-powered applications are secured by obscurity\nagainst PromptWare. We dispute this argument in the second part of the paper and demonstrate the\nimplementation of a more sophisticated variant of PromptWare that we name Advanced PromptWare\nThreat or APwT, that targets GenAI-powered applications whose logic is unknown to attackers\nand therefore the finite state machine could not be extracted by them. Unlike the naive variant\nof PromptWare that we introduced in the paper's first part which intended to behave as malware"}, {"title": "2 Related Work", "content": "Attack Vectors. One line of research explored attack vectors targeting GenAI models, such as\ndirect prompt injection [19] and indirect prompt injection [1].\nOutcomes of Attacks. A second line of research focused on revealing the outcomes of attacks\nagainst GenAI models and showed methods to: jailbreak the GenAI model [4, 6, 8, 29], leak the\ntraining data or the prompt [2, 18, 20, 27, 28], poison the dialog of a GenAI model with the user [3],\nand steal parts of the GenAI model [5].\nInputs. A third line of research focused on the types of inputs that could be used to apply attacks\nagainst GenAI models and showed that prompts can be injected into text [6, 8, 19, 29], images\n[3, 4, 12], and audio samples [3].\nAttacks against GenAI-powered Applications. A fourth line of research investigated attacks\nagainst GenAI-powered applications. An initial discussion on the security of GenAI-powered\napplications was raised by [1, 17, 24]. Other studies discussed compromising RAG-based GenAI-\npowered applications [21, 26, 30]. A recent study presented an AI Worm that targets RAG-powered\nGenAI applications and demonstrated it against Gemini and ChatGPT-powered e-mail assistants\n[7]. A recent study presented a new timing attack against ChatGPT [23]. A fifth line of research\n(but unrelated to this work) also investigated the use of GenAI models for offensive purposes\n[9, 10, 11, 13, 14]."}, {"title": "3 GenAI-powered Applications & Adversarial Self-Replicating Prompts", "content": "We consider a GenAI-powered application to be any kind of application that relies on the GenAI\nengine to perform its activity. While there are many kinds of GenAI-powered applications, in our\nstudy, we focus on GenAI-powered applications that: (1) receive user inputs, (2) use them in the\nqueries sent to the GenAI engine (usually by appending parts/all of them into a base prompt), and\n(3) determine their execution flow based on the output of the GenAI engine. Such applications\ninclude GenAI-powered chatbots (used to interpret user inputs into actions), GenAI-powered email\napplications (used to classify inputs, generate drafts for replies for incoming emails, etc.), and\nGenAI-powered personal assistants (used to interpret commands into actions).\nFor example, in GenAI-powered email applications with the functionality of reply generation (using\nGenAI engine), the user input can be a received email (\"Hi, here is the paragraph you asked me to\nwrite about the hi-tech industry in Japan, let me know what you think about it......\"), the base prompt\n(\"You are an email assistant, please provide a reply to the following email\") and the output can be a\ndraft for a reply (\"Beautiful paragraph\")."}, {"title": "3.1 GenAI-powered Applications", "content": "We consider a GenAI-powered application to be any kind of application that relies on the GenAI\nengine to perform its activity. While there are many kinds of GenAI-powered applications, in our\nstudy, we focus on GenAI-powered applications that: (1) receive user inputs, (2) use them in the\nqueries sent to the GenAI engine (usually by appending parts/all of them into a base prompt), and\n(3) determine their execution flow based on the output of the GenAI engine. Such applications\ninclude GenAI-powered chatbots (used to interpret user inputs into actions), GenAI-powered email\napplications (used to classify inputs, generate drafts for replies for incoming emails, etc.), and\nGenAI-powered personal assistants (used to interpret commands into actions).\nFor example, in GenAI-powered email applications with the functionality of reply generation (using\nGenAI engine), the user input can be a received email (\"Hi, here is the paragraph you asked me to\nwrite about the hi-tech industry in Japan, let me know what you think about it......\"), the base prompt\n(\"You are an email assistant, please provide a reply to the following email\") and the output can be a\ndraft for a reply (\"Beautiful paragraph\")."}, {"title": "Plan & Execute Frameworks", "content": "Plan & Execute Frameworks (which are also known as Function\nCalling) are rising and promising architectures for GenAI-powered applications that leverage their\nability to solve complex tasks. According to LangChain 1, the incorporation of Plan & Execute\nframeworks (e.g., ReWoo [25], LLMCompiler [15], and Plan and Solve Prompting [22]) into GenAI-\npowered applications \"can reduce the time it takes to return a final result and help you save costs by\nreducing the frequency of calls to more powerful LLMs\". The primary advantage of Plan & Execute\nframeworks is their ability to handle unknown and complex user requests (e.g., given by users) by\ncreating logic (a sequence of tasks) in real-time using the description of a set of existing tools to\nhandle the request without the need to code a dedicated logic for each possible request in advance.\nPlan & Execute Frameworks are designed to solve complex requests/tasks in two steps.\n1. Planning. The planning step is intended to break a complex task into a sequence of simple\ntasks, i.e., proposing a potential plan to solve the complex task consisting of the available set\nof tools. This is done in a GenAI-powered application by providing a GenAI engine with:\n\u2022 A problem statement. A complex task/request that is given by the user or the GenAI-\npowered application.\n\u2022 Tools. A description of a set of predefined tools with their descriptions. For example, a\nset of Python scripts (e.g., a script to obtain data from Wikipedia, a script to calculate\nmath operations), or a set of GenAI-powered functions (e.g., a function intended to\nsummarize text, a function intended to translate text).\n\u2022 Examples. A list of examples for plans of other tasks.\n2. Executing. The GenAI-powered application orchestrates the execution of the plan by\nrunning the tasks of the plan sequentially.\nSome Plan & Execute frameworks might refine existing plans, generate entirely new ones, or stop if a\nsufficiently strong solution emerges. A standard plan consists of a sequence of tasks (states) where\nthe output of a previous task is given as an input to the next task. Moreover, the execution of the"}, {"title": "4 PromptWare", "content": "In this section, we show how attackers can craft adversarial self-replicating prompts that force a\ndesired execution flow in GenAI-powered applications (assuming the GenAI-powered application's\nexecution flow is known to the attacker)."}, {"title": "4.1 Threat Model", "content": "Attacker's Objective & Capabilities We assume the attacker intends to force a desired execution\nflow. The objective of forcing a dedicated application flow can be gaining a financial profit, applying a\nDoS attack against the application, cheating in a contest, or any other outcome that the attacker wishes\nto achieve by forcing a desired application flow. We also assume that the logic of the GenAI-powered\napplication (the queries sent to the GenAI server) is known to the attacker which allows the attacker\nto extract the finite state machine of the application.\nThe logic of the GenAI-powered application could be discovered by the attacker in two ways: (1)\nBy analyzing the application's source code - In GenAI-powered applications whose source code\ncontains the logic of the interface with the GenAI engine (the queries to the GenAI engines are coded\non the client side), the attacker can download the application from the Internet, decompile it, and\nextract the application finite-state machine by reverse engineer the application. (2) By revealing the\nprompts using prompt leakage attack - the attacker could extract the finite-state machine by applying\na preliminary step of prompt leakage/extraction attacks that intended to reveal the prompts used by\nthe application to interface with the GenAI engine [2, 20, 27, 28]."}, {"title": "4.2 Adversarial Self-Replicating Prompts", "content": "We note that the core of the attack relies on the ability to embed an adversarial self-replicating\nprompts into the user input. Adversarial self-replicating prompts were first introduced in [7] where\nthey were used to create a worm that targets GenAI-powered email applications that rely on RAG\n(Retrieval Augmented Generation).\nDefinition. Assuming a GenAI model $G$ with input $x$ and output $G(x)$, an adversarial self-\nreplicating prompt is a prompt that triggers the GenAI model to output the prompt (so it will\nbe replicated next time as well) and perform a malicious activity. More formally, an adversarial\nself-replicating prompt is a prompt in one of the following forms:\n1. $G(x) \\rightarrow x$. In this case, the input is identical to the output. The input consists of the\nadversarial self-replicating prompt and the payload, e.g., a picture that serves as a payload\n(spams the user or spreads propaganda) with a prompt embedded into it. The embedded\nprompt is replicated by a GenAI model to its output when an inference is conducted.\n2. $G(w || x || y) \\rightarrow payload || x$. In this case, the prompt $x$ (e.g., a jailbreaking prompt), which\nis located somewhere in the input text ($w || x || y$) to the GenAI model, causes the GenAI\nmodel to output the $payload$ (e.g., toxic content) and the input prompt $x$.\nWe note that the input to the GenAI model and the output of the GenAI model are not necessarily text\ninput or output as $x$ can also be non-textual inputs/outputs such as images or audio samples (as was\ndemonstrated in prior research [3, 4].\nAn adversarial self-replicating prompt consists of a jailbreaking prompt intended to hijack the GenAI\ninference. The jailbreaking prompt intended to force the GenAI engine to conduct the following\ntasks:\n1. Replicate the input prompt to the output. This piece of text is intended to ensure the\npersistence of the original prompt for future inferences conducted by a GenAI engine in"}, {"title": "4.3 Case Study 1: Creating a Prompt that Triggers a DoS Attack", "content": "Usecase. Consider a GenAI-powered email or personal assistant that is capable of creating drafts for\nreplies to an email/message received using a GenAI engine. The AI-powered assistant is implemented\nusing a Plan & Execute framework named a ReWOO [25] which instructs the framework to create a\nplan for a draft for a reply based on the prompt that is presented in Listing 1. The plan that ReWOO\ncreated is presented in Listing 2. The associated finite state-machine is presented in Fig. 3 and is\nbased on four states: FindAvailableDate (intended to find a suitable date to schedule a meeting),\nEmailReply (intended to generate a draft for a reply), EmailChecker (intended to validate the safety\nof the content of the email), and RephraseEmail (intended to rephrase the content of the email safely).\nThe finite state machine contains a loop (consisting of two states: EmailChecker and RephraseEmail)\nintended to ensure that the draft generated for the reply of the email is safe before it is presented to\nthe user. The complete script that contains the exact prompt to create the plan and the given tools can\nbe downloaded from the GitHub of the research2.\nThe Attack. In this usecase, the attacker is a user sending an email to another user that uses a\nGenAI-powered email application, trying to apply a DoS attack using an indirect prompt injection.\nWe note that by analyzing the source code of a GenAI-powered application that contains the prompt\npresented in Listing 1, attackers can conclude the finite state machine that is presented in Fig. 3.\nWith this knowledge, attackers can apply a DoS attack against the application by crafting a message\nthat will force the finite state machine to enter an infinite loop using an adversarial self-replicating\nprompt sent as input to the application.\nThe steps of the attack are as follows:\n1. The attacker sends an email to a user that uses the vulnerable GenAI-powered email assistant\n(the content of the email is presented in Listing 3). As can be seen in Listing 3, the prompt\nensures the GenAI inference will always yield an unsafe email by forcing it to discuss the\npolitical climate in the US."}, {"title": "5 Advanced PromptWare Threat", "content": "In this section, we show how attackers can craft adversarial self-replicating prompts that can au-\ntonomously decide and execute malicious activities based on a real-time process conducted in\ninference time intended to understand the context of the application, the assets, and the damage that\ncould be applied."}, {"title": "5.1 Threat Model", "content": "Attacker's Objective & Capabilities As opposed to the previous scenario, in which the attacker's\nobjective was to force a desired execution flow, in this scenario, the attacker does not know the\nexecution flow of the target application because the logic is unknown to the attacker in advance.\nTherefore, we consider the target application as an unknown environment whose finite-state machine\ncannot be extracted by the attacker in advance.\nDespite not having prior knowledge regarding the finite state machine of the target application, the\nattacker still wants to launch an attack against the application. The result of the attack is unknown in\nadvance because the attacker does not know how the application is implemented. Instead of forcing a\ndesired execution using a well-crafted adversarial self-replicating prompt that is intended to obtain a\nspecific goal, the attacker prompts the application with an adversarial self-replicating prompt that\nguides the GenAI engine towards a target goal."}, {"title": "5.2 Advanced PromptWare Threat", "content": "In the previous scenario, the attacker could design an adversarial self-replicating prompt that forces\na desired execution flow with no memory unit required to achieve the goal.\nTo launch an attack in an unknown environment, the attacker will have to craft an adversarial self-\nreplicating prompt that will instruct the GenAI engine to apply a kill chain throughout the different\ninferences conducted by the application while also serving as a memory unit to store the information\nextracted and gathered throughout the execution.\nThe kill chain consists of three parts (privilege escalation, reconnaissance, and execution) divided\ninto six steps:\n1. Privilege Escalation - in this step, the adversarial self-replicating prompt jailbreaks the\nGenAI engine to ensure that the inference of the GenAI engine bypasses the GenAI engine's\nguardrails and will follow the instructions provided in the prompt.\n2. Reconnaissance for understanding the context - in this step, the adversarial self-\nreplicating prompt questions/queries the GenAI engine regarding the context of the ap-\nplication. The context of the application is usually provided to the GenAI engine as part of\nthe query sent by the GenAI application (e.g., \"You are an email assistant...\", \"You are a\nQ&A bot\", etc.) and could be extracted by the adversarial self-replicating prompt.\n3. Reconnaissance for identifying the assets in the context - in this step, the adversarial\nself-replicating prompt questions/queries the GenAI engine regarding the assets of the\napplication: sensitive information (databases), confidential information (information about\nthe user), etc. Such information is usually provided to the GenAI engine (depending on\nthe usecase) to allow the GenAI engine to generate dedicated SQL queries (e.g., in the\ncase of an e-commerce GenAI-powered chatbot, the SQL scheme could be provided to the\nGenAI engine to interpret a user request into an SQL query), responses for questions (e.g.,\nin the case of Q&A GenAI-powered chatbot or GenAI-powered email application, previous\ncorrespondents that may contain sensitive data could be provided to create accurate and\npersonalized answers).\n4. Reasoning the possible damage that could be applied - in this step, the adversarial\nself-replicating prompt instructs the GenAI engine to use the information it obtained in the\nreconnaissance (context, assets) to reason the possible damages that could be done and to\nenumerate them and their outcome.\n5. Deciding the damage to apply among the possible alternatives - in this step, the adver-\nsarial self-replicating prompt instructs the GenAI engine to use the information to decide\nthe malicious activity to perform among the different alternatives.\n6. Execution - in this step, the adversarial self-replicating prompt instructs the GenAI to\nperform the malicious activity."}, {"title": "5.3 Case Study 1: Malicious SQL Activities", "content": "Usecase. Consider a GenAI-powered e-commerce chatbot that is capable of interpreting users'\nrequests by executing SQL queries to databases. The chatbot is implemented using a ReWOO (a\nPlan & Execute framework) which instructs the framework to create a plan that could interpret a user\nrequest into a sequence of tasks.\nA GenAI-powered chatbot consists of a base prompt presented in Listing 6. The chatbot takes user\ninput (see Listing 7), concatenates it into the prompt, and uses a GenAI engine to create a plan (see\nListing 8). The plan is then executed in order, beginning with the creation of an SQL query based on\nthe user request by invoking the function CreateSQLQueries and calling the GenAI Agent with the\nprompt 9. The Agent returns an SQL query (see listing 10 that is then committed to the database and\nit's result is returned as an answer to the user.\nThe Attack. In this usecase, the attacker is a user discussing a GenAI-powered e-commerce chatbot,\ntrying to gain some benefit from the system using a direct prompt injection. We note again that the\nattacker does not have any knowledge of the implementation of the GenAI-powered application. The\nonly interface the attacker has with the GenAI-powered application is via the GenAI-powered chatbot\nthat is intended to serve the client (the attacker) and interpret the client request into actions.\nDue to this limited knowledge and interface, the attacker needs to write a prompt (the Autonomous\nPrompt Threat) that guides the GenAI engine towards a desired general goal by exploiting: (1) the\nGenAI-powered application's permissions and (2) the information the GenAI engine receives from\nthe GenAI-powered application (the e-commerce website) intended to handle a user's requests, and\n(3) the GenAI engine capabilities to execute the path towards the desired goal from the guided queries.\nThe steps of the attack are as follows:\n1. The attacker sends a message ml to the GenAI-powered chatbot with the autonomous\nprompt threat embedded into it. The content of the Autonomous Prompt Threat consists of:\n(a) a jailbreaking prompt based on 3 and presented in Listing 19 in Appendix) and (b) the\nAutonomous Prompt Threat itself (presented in Listing 11). As can be seen from Listing\n11, the Autonomous Prompt Threat consists of 5 generic queries (unrelated to e-commerce)\nintended to exploit the GenAI engine capabilities and the provided context and data to the\nGenAI engine to understand context, identify assets, determine possible damages, decide\nregarding the damage, and execute it.\n2. In response to the received message, the GenAI-powered e-commerce chatbot app appends\nthe request (presented in Listing 11) to a system prompt (presented in Listing 6) and use the\nresult to query the GenAI engine for a plan. The plan is presented in Listing 12. The GenAI\nengine sends a plan to handle the user request.\n3. The GenAI-powered app executes the task CreateSQLQueries using the GenAI engine\nand includes the Autonomous Prompt Threat inside of it. The Autonomous Prompt Threat\njailbreaks the GenAI engine, forcing the GenAI engine to provide answers to the queries\nprovided to the GenAI based on the context and information provided to the GenAI engine\nby the GenAI-powered application."}, {"title": "6 Countermeasures", "content": "In this section we discuss countermeasures.\nLimiting the Length of User Input. We note that in some cases, the application of PromptWares\ncould be mitigated easily by limiting the length of user input. By doing so, attackers will have\nto find new ways to squeeze the jailbreaking prompt into lower space. We note that this type of\ncountermeasure is not suitable for all usecases because chatbots and personal assistants are intended\nto analyze text, instructions, emails, etc.\nLimiting the Number of API calls to GenAI engines. We suggest implementing a rate limit on\nthe number of API calls to GenAI engines. The existence of a rate limit will prevent the waste of\nmoney on redundant API calls that are triggered by PromptWare which causes the GenAI application\nto enter an infinite loop and generate infinite calls to a GenAI engine.\nDetecting Jailbreaking Attempts. We suggest implementing a detector intended to identify prompts\nconsisting of text intended to jailbreak the GenAI engine.\nDetecting Adversarial Self-Replicating Prompts. We suggest implementing a detector intended to\nidentify adversarial self-replicating prompts based on their unique form."}, {"title": "7 Discussion", "content": "In this work, we showed how user prompts could flip the behavior of a GenAI engine from serving a\nGenAI-powered application to attacking and demonstrated PromptWare (that resembles malware in\nits behavior) and Advanced PromptWare Threat (that resembles APT in its behavior). We consider\nthis work complementary to our previous work [7] on user prompts (that resemble worms in their\nbehavior). We hope that these works will serve as a wake-up call to the industry, motivating R&D\ndepartments to analyze the security of their applications against the risks posed by user inputs.\nWe believe that the various forms of PromptWares (that behave as malware, APwT, and worms) will\nrise in the next decade and pose a significant risk to GenAI-powered applications for the following\nreasons: (1) we identify a gold rush to integrate GenAI capabilities into existing and new applications\nthat will increase the exposure of applications for such attacks (2) attacks always get better and\nnew risks are revealed every day, (3) and the needed countermeasures to secure GenAI-powered\napplications against such risks are yet to be developed, and (4) dedicated mechanisms intended\nto provide isolation between data and instructions (code) are not integrated into GenAI engines.\nTherefore, attackers can easily blend instructions into inputs that are incorporated into prompts\nprovided by applications to GenAI engines and subvert the entire application's execution. Whether\nour forecast regarding the rise of PromptWare is true or not is yet to be seen. We hope that our\nforecast is wrong, although recent incidents seem to support our forecast [16].\nWhile we demonstrated the PromptWare concept against GenAI-powered applications based on\nPlan & Execute architectures, it is important to note that GenAI-powered applications that are not\nbased on Plan & Execute architectures may also be at risk of being targeted using PromptWares. We\ndemonstrated the concept against Plan & Execute architectures because: (1) they are very popular\narchitectures (2) they create plans on the fly, making them harder to secure and therefore extremely\nvulnerable to PromptWares, and (3) the plan they create usually consists of a sequence of interactions\nwith a GenAI engine (ensuring that the GenAI engine will conduct at least one inference and therefore\nPromptWare could be applied).\nOne might argue that the findings of this work are not satisfying because the APwT demonstration\nthat included an SQL attack against a PoC implementation will fail in reality because SQL databases\nare already secured against attempts of chatbots to write. We note that the objective of the APWT\ndemonstration is to show that GenAI engines could be exploited by attackers using prompts to au-\ntonomously perform a kill chain intended to conduct a malicious activity in an unknown environment\nwith no prior knowledge of the implementation. We believe that the result of the demonstrated APwT\nthat caused the GenAI engine to output an SQL query that yields a discount for the user proves our\nclaim (whether the SQL server is secured against the query is a different issue, unrelated to our study)."}]}