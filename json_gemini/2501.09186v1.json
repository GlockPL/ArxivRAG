{"title": "Guiding Retrieval using LLM-based Listwise Rankers", "authors": ["Mandeep Rathee", "Sean MacAvaney", "Avishek Anand"], "abstract": "Large Language Models (LLMs) have shown strong promise as rerankers, especially in \"listwise\" settings where an LLM is prompted to rerank several search results at once. However, this \"cascading\" retrieve-and-rerank approach is limited by the bounded recall problem: relevant documents not retrieved initially are permanently excluded from the final ranking. Adaptive retrieval techniques address this problem, but do not work with listwise rerankers because they assume a document's score is computed independently from other documents. In this paper, we propose an adaptation of an existing adaptive retrieval method that supports the listwise setting and helps guide the retrieval process itself (thereby overcoming the bounded recall problem for LLM rerankers). Specifically, our proposed algorithm merges results both from the initial ranking and feedback documents provided by the most relevant documents seen up to that point. Through extensive experiments across diverse LLM rerankers, first stage retrievers, and feedback sources, we demonstrate that our method can improve nDCG@10 by up to 13.23% and recall by 28.02%-all while keeping the total number of LLM inferences constant and overheads due to the adaptive process minimal. The work opens the door to leveraging LLM-based search in settings where the initial pool of results is limited, e.g., by legacy systems, or by the cost of deploying a semantic first-stage.", "sections": [{"title": "1 Introduction", "content": "One of the most prevalent approaches in document search systems is the two-stage retrieve-and-rerank (\u201ccascading\u201d) pipeline, where an initial retrieval phase is followed by a more involved reranker [18]. In this paradigm, the results from a retrieval model are re-ordered by a reranker a slower but more capable relevance model based on their estimated relevance to the query. For example,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Retrieval and Ranking", "content": "Cascading approaches have a long history in information access systems [18]. In retrieval systems, cascading often manifests itself as a retrieve-then-rerank pipeline. Sparse retrieval algorithms (such as BlockMax-WAND [4]) and dense retrieval algorithms (such as HNSW [16]) can produce an initial set of (potentially approximate) top-k search results in sublinear time with respect to the size of the corpus, allowing them to scale to massive corpora. These initial retrieval results are limited by the representation bottleneck (e.g., the terms matched in a bag-of-words representation), and therefore often benefit from an additional iteration of refinement before being presented to a user often through a cross-encoding neural model, such as one based on BERT [23] or an LLM [34]. By considering the content of the query and document jointly, they can often produce a higher-quality relevance ranking. These models are not capable of scaling to large corpora, given that they operate in linear (or superlinear) time (usually with high constant factors). The central limitation of cascading approaches is the bounded recall problem; documents filtered out in previous stages have no chance of being seen by the reranker. In the context of search systems, this has spawned substantial research into stronger first-stage retrieval systems, such as those that leverage dense [8] or learned sparse vectors [21]. Despite considerable"}, {"title": "2.2 Adaptive Retrieval", "content": "To overcome the recall limitations of cascading approaches, adaptive reranking [12] techniques have been proposed. These methods utilize an index structure based on document-document similarities and leverage relevance estimations obtained during the reranking process to retrieve and score documents that were not captured in the initial retrieval stage. Drawing inspiration from the Cluster Hypothesis [5], adaptive reranking posits that documents in close proximity to those with high relevance scores are also likely to be relevant; by pulling in these similar documents when reranking, recall limitations of the first stage can be mitigated. By using a document-to-document similarity graph (\"corpus graph\", akin to HNSW [16]), this process can be accomplished with minimal overheads.\nSeveral strategies for adaptive reranking have been proposed. For example, GAR [12] and QUAM [29] employ an alternating strategy that scores batches of documents from both the initial retrieval pool and those sourced from the corpus graph. Beyond cross-encoders, adaptive reranking techniques have also been successfully applied to bi-encoders [7,11] and ensemble models [37]. For instance, LADR [7] uses an efficient lexical model to identify promising \"seed\" documents, which are then used to explore the corpus graph further. AR is inherently designed for pointwise ranking methods. In this work we extend AR to listwise settings, where the ranking function considers the relative order of documents as a whole rather than assigning individual scores."}, {"title": "2.3 Pseudo-Relevance Feedback (PRF)", "content": "Another line of work, pseudo-relevance feedback (PRF), tries to improve recall by providing feedback from the top-k initial retrieved documents. Examples of methods based on this approach include Rocchio [32], KL expansion [38], relevance modeling [19], LCE [20], and RM3 expansion [1]. These methods result in the query drift problem. Recent works [15,14] have proposed Generative Relevance Feedback (GRF). The goal is to enhance the original query by using an LLM. The expanded query can incorporate the original terms along with useful terms generated from various subtasks, such as keywords, entities, chain-of-thought reasoning, facts, news articles, documents, and essays. However, such methods of expanding LLM-based queries are costly, since the number of output tokens is high."}, {"title": "2.4 LLM Rerankers", "content": "Recent works, such as [34,33], have proposed RankGPT, zero-shot LLMs for document ranking task. The zero-shot LLM-based ranker shows significant im-"}, {"title": "3 SLIDEGAR: Sliding Window based Adaptive Retrieval", "content": ""}, {"title": "3.1 Preliminaries", "content": "Recall from Section 2.2 that Graph-based Adaptive Retrieval (GAR) and Query Affinity Modelling based Adaptive Retrieval (QUAM) augment the set of documents for reranking by selecting the neighbors of top-ranked documents by leveraging a corpus graph (constructed offline) to find neighbors in constant time. To formalize and set the stage for our proposed method, let Ro be the initial retrieval pool, b is the batch size, c is the ranking budget, and G is the corpus graph. GAR and QUAM start with taking the top b (batch size for pointwise ranker) documents from the initial retrieval Ro and ranking by a pointwise ranker like MonoT5 [23]. Next, considering the top-ranked documents, the corpus graph G is explored to select b neighboring documents. Subsequent iterations alternate between the initial retrieval and the graph neighborhood until the reranking budget c is met."}, {"title": "3.2 The SLIDEGAR Algorithm", "content": "For a given query q, the standard listwise reranking pipeline takes the retrieval results as input. Since the LLM rankers are limited by the context size, an"}, {"title": "4 Experimental Setup", "content": "Our experiments answer the following research questions about SLIDEGAR:\nRQ1: What is the impact of adaptive retrieval in the ranking performance of LLM rerankers?\nRQ2: How sensitive are the LLM based rerankers to the graph depth (k)?\nRQ3: What are the additional computational overheads by SLIDEGAR in comparison to standard LLM based reranking?"}, {"title": "4.1 Dataset and Evaluation", "content": "In our experiments, we use the MSMARCO [22] passage corpus and evaluate performance on TREC Deep Learning 2019 (DL19) and 2020 (DL20) query sets. The DL19 and DL20 datasets have 43 and 54 queries respectively. To further"}, {"title": "4.2 Retrieval and Ranking Models", "content": "Retrievers: We perform experiments with both sparse and dense retrieval methods. For sparse retrieval, we use BM25 [31] using a PISA [17] index. For dense retrieval, we use TCT [8] and retrieve (exhaustively) top $c \\in [50, 100]$ documents using the TCT-ColBERT-HNP [8] model from huggingface7.\nRankers: We use different listwise LLM rankers, both zero-shot and fine-tuned. We use zero-shot rankers from RankGPT [34] mainly based on gpt-3.5-turbo and gpt-40 models from OpenAI [24]. We use Azure API8 for both gpt-3.5-turbo (gpt-35-turbo in Azure) and gpt-40 models. We also use fine-tuned listwise LLM rankers, mainly RankZephyr [27] and RankVicuna [26]. For all these listwise rankers, we use the implementation from the open-source Python library, called Rerankers [3] which incorporates the RankLLM\u00ba (a better variant of RankGPT). These models use the sliding window strategy over a list of documents and generate ranking (also called permutation). It is important to note that these rankers do not provide scores. We use default settings where the sliding window size is 20 and the step size is 10. We use the context size of 4096 for all rankers.\nBaselines: We compare our approach to the baseline (non-adaptive) LLM reranking systems. Furthermore, we compare with a variation of SLIDEGAR that uses RM3 [1] (denoted by SLIDEGARRM3) as its source of additional relevant documents, rather than the corpus graph. Specifically, after reranking the current window by the LLM, we expand the query using top-b documents and for the subsequent window, retrieve the remaining b documents using the expanded query from the index (mainly BM25). Note that the LLM always uses the original"}, {"title": "5 Results and Analysis", "content": "We now discuss our results and provide a detailed analysis. We denote our sliding window and graph-based retrieval method by SLIDEGAR and the corresponding graph in the subscript, for example, SLIDEGARBM25 represents the SLIDEGAR with a BM25-based graph. We incorporate SLIDEGAR with all ranking methods."}, {"title": "5.1 Effectiveness", "content": "To answer RQ1, we propose the SLIDEGAR algorithm and assess the effectiveness over different reranking budgets, initial retrievers, and rerankers. We report the performance of different reranking approaches in Table 1. We observe that SLIDEGAR shows a significant improvement over the standard ranking. Across all rankers, the improved recall remains in the same range for the given reranking budget. However, since we use the window level ranking to look for the neighbors, we find that the better ranker provides slightly better recall. Hence, RankGPT-40 and RankZephyr show slightly better recall improvements than RankGPT-3.5 and RankVicuna.\nThe most significant improvements are at the ranking budget of 50 and when the sparse retriever is used. On DL19, Recall@50 improves between 22% and 28% across rankers, where BM25\u00bbRankGPT-40 with SLIDEGARTCT shows the greatest improvements. Additionally, the fine-tuned RankZephyr improves nDCG@10 the most for the BM25\u00bbRankZephyr (from 0.670 to 0.747) with SLIDEGARTCT\u00b7 Recall@100 improves the highest (up to 20%) for RankGPT-40 and the lowest (up to 14%) for RankVicuna. We observe similar trends for the DL20 dataset. Our adaptive RM3 baseline, SLIDEGARRM3, also improves recall (significant in 13 of 16 comparisons), but is not able to successfully re-rank these documents to yield an improved nDCG@10. This observation suggests that the corpus graph exploration approach is preferable to that of traditional PRF variants in terms of both efficiency and effectiveness. We also find that SLIDEGARTCT shows better performance in comparison to SLIDEGARBM25 and SLIDEGARRM3, since the dense retriever-based corpus graph is capable of providing complementary signals to the lexically retrieved documents.\nWhen the dense retriever, TCT, is used for the initial ranking, we continue to see improvements, though the relative differences are lower due to the stronger initial result set. Both RankGPT-40 and RankZephyr improve recall@50 between"}, {"title": "5.2 Generalization on Different Corpus", "content": "In Table 2, we report the results on the MSMARCO-passage-v2 corpus using DL21 and DL22 as test sets. We use RankGPT-3.5 and RankZephyr as rankers. Unsurprisingly, SLIDEGAR also shows significant improvements on the new corpus. Specifically, on DL21, SLIDEGARTCT improves Recall@50 from 0.248 to 0.356 (43% improvement) with BM25\u00bbRankGPT-3.5 and from 0.248 to 0.365 (47% improvement) with BM25\u00bbRankZephyr. Similar improvements are observed when the budget is set to 100. Our method also shows robust performance on the DL22 dataset, where the initial retrieval effectiveness is considerably lower (Recall@50 is 0.119 and Recall@100 is 0.171). In particular, SLIDEGARTCT improves Recall@50 by 28% with BM25\u00bbRankGPT-3.5 and by 47% with BM25\u00bbRankZephyr. Similar trends are observed when the budget increases to 100. Additionally, recall improvements are evident when the dense (TCT) retriever is used at the initial stage. Overall, we observe better ranking and recall improvements when the fine-tuned listwise ranker, RankZephyr, is utilized."}, {"title": "5.3 Effect of graph depth", "content": "A key hyperparameter of SLIDEGAR is the graph depth k, which we now investigate to answer RQ2. The adaptive retrieval is mainly based on the number of neighbors, k, in the corpus graph. In this section, we study the behavior of different ranking approaches when graph depth k varies. We do not consider RankGPT-40 for this ablation study due to its high cost. Towards this, we vary the graph depth $k \\in [2, 16]$ (by multiples of 2) and fix the reranking budget c to 50. We report the performance across different metrics in Figure 2. We observe that the recall@50 improves for SLIDEGAR as we go deeper into the graph neighborhood. Though, the BM25 variant, SLIDEGARBM25 seems to be robust to the number of neighbors. A similar trend can be seen in the nDCG metrics. On the other hand, the SLIDEGARTCT is able to find further relevant documents as we go deeper in the graph. In particular, recall@50 improves for both RankVicuna"}, {"title": "5.4 Computational Overhead", "content": "We re-iterate that SLIDEGAR alternate between initial retrieved documents and neighborhood documents after each window is processed and has the same number of reranking operations (number of LLM calls depending upon window size w and step size b) as standard listwise ranking. The cost of ranking all windows dominates the total cost of reranking the pipeline (including the cost of preparing windows). To answer RQ3, we are interested to see what the additional latency cost our adaptive sliding window approach, SLIDEGAR, adds to the total LLM reranking latency. Toward this, we conduct an experiment on the DL19 dataset with the reranking budget c of 50 and 100. We use BM25 as initial retrieval and SLIDEGARBM25 variant of our adaptive method. In Table 3, we report the mean latency (in ms) per query for reranking and SLIDEGAR component during reranking. For this experiment, we only use RankZephyr and RankVicuna"}, {"title": "6 Conclusion", "content": "We augment existing adaptive ranking algorithms to work with listwise LLM reranking models. We find that our proposed method, SLIDEGAR, is able to successfully overcome the bounded recall problem from first-stage retrievers by successfully leveraging feedback signals from an LLM. Also, the computational overhead of applying SLIDEGAR is minimal compared to a typical LLM reranking pipeline. In our opinion this work enables the broader adoption of LLM reranking, such as in cases where the first stage is unsuccessful or systems are limited by legacy first-stage (lexical) keyword-based retrieval systems."}]}