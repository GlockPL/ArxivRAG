{"title": "BMFT: Achieving Fairness via Bias-based\nWeight Masking Fine-tuning", "authors": ["Yuyang Xue", "Junyu Yan", "Raman Dutt", "Fasih Haider", "Jingshuai Liu", "Steven McDonagh", "Sotirios A. Tsaftaris"], "abstract": "Developing models with robust group fairness properties is\nparamount, particularly in ethically sensitive domains such as medical\ndiagnosis. Recent approaches to achieving fairness in machine learning\nrequire a substantial amount of training data and depend on model re-\ntraining, which may not be practical in real-world scenarios. To mitigate\nthese challenges, we propose Bias-based Weight Masking Fine-Tuning\n(BMFT), a novel post-processing method that enhances the fairness of a\ntrained model in significantly fewer epochs without requiring access to\nthe original training data. BMFT produces a mask over model parame-\nters, which efficiently identifies the weights contributing the most towards\nbiased predictions. Furthermore, we propose a two-step debiasing strat-\negy, wherein the feature extractor undergoes initial fine-tuning on the\nidentified bias-influenced weights, succeeded by a fine-tuning phase on\na reinitialised classification layer to uphold discriminative performance.\nExtensive experiments across four dermatological datasets and two sen-\nsitive attributes demonstrate that BMFT outperforms existing state-\nof-the-art (SOTA) techniques in both diagnostic accuracy and fairness\nmetrics. Our findings underscore the efficacy and robustness of BMFT\nin advancing fairness across various out-of-distribution (OOD) settings.", "sections": [{"title": "1 Introduction", "content": "Machine learning is known to exhibit biases from various sources, such as human\njudgement, inherent algorithmic predispositions, and representation bias [22].\nThe latter emerges when data from minority ethnicities, genders, and age groups\nare under-represented, leading to unfair predictions by machine learning systems.\nThis can result in serious issues, including direct or indirect discrimination [24].\nIt is important to ensure an equitable and responsible application of AI to not\nundermine the trustworthiness and acceptance of AI solutions by end users [32]."}, {"title": "2 Method", "content": "Bias in model prediction typically originates from two key sources: (1) the en-\ntanglement between noisy, harmful (spurious or irrelevant) features and, useful,\ncore features within the feature extractor; and (2) the incorrect composition of\nrepresentation in the classification layer, leading to the core features information\nloss [20]. The main idea of BMFT is to start with debiasing the feature ex-\ntractor, by fine-tuning only the targeted weights that are disproportionately in-\nstigating bias. Subsequently, as the first process may affect how the core features\nare integrated by the classification layer for prediction, BMFT then fine-tunes\na reinitialised classification layer."}, {"title": "2.1 Preliminaries", "content": "Consider an external dataset of N subjects, \\(D_e = \\{x_i,y_i,a_i\\}_{i=1}^N\\), where \\(x_i\\),\n\\(y_i\\), and \\(a_i\\) are the input image, the corresponding label, and the sensitive at-\ntribute, respectively. For simplicity, we assume binary targets and attributes\n(i.e. \\(y_i, a_i \\in \\{0,1\\}\\)). We will assume a compositional construction of a biased\nmodel \\(f_\\theta(\\cdot)\\), consisting of a feature extractor \\(E(\\cdot)\\) and a classification head \\(C(\\cdot)\\),\npre-trained by the original training data \\(D_{org}\\). Our goal is to debias the model\n\\(f_\\theta(\\cdot)\\) w.r.t. fairness metrics such as Demographic Parity (DP) [9] or Equalised\nOdds (EOdds) [13] by fine-tuning with significantly fewer epochs, using \\(D_e\\); and\nfurther enhance the generalisability of the model on various OOD test datasets\n\\(D_t\\), which share the same sensitive attribute \\(a_i\\). To avoid attribute bias from the\nexternal dataset \\(D_e\\), we sample a group-balanced subset \\(D_r\\) from \\(D_e\\). This ensures\nthat each attribute group a has the same amount of data. The proportion of\npositive to negative labelled images is adjusted to be the same in each a group,\nwhich prevents bias due to different label distributions. Implementing these two"}, {"title": "2.2 Bias-Importance-Based Mask Generation", "content": "To identify the effective parameters for updating, we evaluated the importance\nof each parameter with respect to the designed bias influence function and loss\nfunction. The top K parameters, that are highly correlated with bias and yet\nprovide only minimal contribution to prediction, were selected as the masking\nweights. We adopt weighted Binary Cross Entropy (WBCE) as loss function\nsince the dermoscopic dataset contains label imbalance. Together with the bias\ninfluence function B, the differentiable proxy of EOdds [13], is given by:\n\n\\(\\mathcal{L}_{WBCE} = \\sum_{i=1}^i ( \\frac{N_n}{N_n + N_p} -y_i log(p_i) - \\frac{N_p}{N_n + N_p}(1- p_i) )\\)  (1)\n\n\\(B(f_\\theta, x_i, y_i, a_i) = \\frac{\\sum_{i=1}^N f_\\theta(x_i)a_iy_i}{\\sum_{i=1}^N a_iy_i} - \\frac{\\sum_{i=1}^N f_\\theta(x_i) (1 - a_iy_i)}{\\sum_{i=1}^N(1 - a)y_i}\\),  (2)\n\nwhere \\(p_i\\) is the prediction probability, \\(N_p\\) is the number of images with positive\nlabel, \\(N_n\\) the number of images with negative label. For a pre-trained model \\(f_\\theta(\\cdot)\\)\nwith effectively learnt parameters \\(\\theta^*\\), the sensitivity for each parameter \\(\\theta\\) can be\nobtained by the second-order derivative of loss close to the minimum [10]. This\nvalue can be interpreted as the importance of the parameter for the prediction.\nThe diagonal of the Fisher Information Matrix (FIM) is equivalent to the second-\norder derivative of the loss [26], which provides an efficient computation through\nfirst-order derivatives. The first-order expression for the FIM is provided by:\n\n\\(I(f_\\theta, D_e) = E [(\\frac{alog f_\\theta(D_e|\\theta)}{d \\theta}) (\\frac{alog f_\\theta(D_e|\\theta)}{d \\theta})^T]\\)  (3)\n\nWe employ Eq. 3 to quantify the importance of each weight that contributes\nto the prediction. The importance of each weight for the bias can be computed\nusing the second derivative of the bias influence function (that is, Eq. 2). Noting\nthat the bias influence function is a linear function of the prediction, and thus\nreplaces the loss function with the bias influence function during backpropaga-\ntion, the importance of a parameter w.r.t. the bias can also be derived by Eq. 3.\nWe denote the weight mask as \\(M_i\\), where i is the weight index.\nTo ensure that the weight importance for bias or prediction is not skewed in\nfavour of a particular subgroup, in practice, we use the group-balanced dataset\n\\(D_r\\) to calculate the importance. To select top K% weights in the mask \\(M_i\\), we\nuse the bias importance \\(I_{i,b}\\) and the loss importance \\(I_{i,l}\\) of each parameter as:\n\n\\(M_i = \\begin{cases} 1, & \\text{if } Z_{i,b}/I_{i,l} > a \\\\ 0, & \\text{otherwise} \\end{cases}\\), (4)"}, {"title": "2.3 Impair-Repair: Two-Step Fine-Tuning", "content": "To disentangle core features from biased features, whilst preserving predictive\ncapability, a two-stage fine-tuning process is structured in an \"impair-repair\"\nmanner. Firstly, we fine-tune the selected masked parameters within the feature\nextractor \\(E(\\cdot)\\) using the group-balanced dataset \\(D_r\\), guided by an objective func-\ntion (Eq. 2). Next, we fine-tune the entire reinitialised classification layer \\(C(\\cdot)\\).\nThe weights identified by the mask in the feature extractor are closely linked to\nattribute bias and play a smaller role in predicting the class. This relationship\nhelps us remove the unfavourable influence of bias contained in the model, and\nyet crucially also retain information embedded in core features.\nWe highlight that the pre-trained model has the capability to sufficiently\nextract core features. Further, eliminating the influence of (only) bias-effected\nweights suggests moving the model decision boundary away from bias features.\nMoreover, fine-tuning is prone to overfitting on a small dataset. These observa-\ntions suggest that small epochs are sufficient to realise the proposed debiasing\nfine-tuning strategy.\nAn unbiased model, trained with a balanced dataset, will have the majority\nof its classification layer weights being zero, due to minimisation of irrelevant fea-\nture impact on class prediction [17]. To this end, we first reinitialise the weights\nand biases in the classification layer \\(C(\\cdot)\\) to zero to enable fast convergence to\nthe optimal unbiased state and then fine-tune the layer on \\(D_r\\) using a loss which\ncombines the WBCE loss \\(\\mathcal{L}_{WBCE}\\) and fairness constraints (i.e. Eq. 2), as:\n\n\\(\\mathcal{L} = \\mathcal{L}_{WBCE}(f_\\theta) + \\beta B(f_\\theta),\\) (5)\n\nwhere the fairness constraint \\(B(f_\\theta)\\) modulates the emphasis of the features, and\nthe hyperparameter \\(\\beta\\) establishes an efficiently identified core trade-off between\nclass prediction accuracy and fairness. The improved feature extractor retains\ncore features without being influenced by attribute bias, allowing the model to\nefficiently identify and recover its predictive performance in limited epochs. To\nbalance bias reduction with efficiency, an equal number of epochs is used for\nboth the \"impair\" and \"repair\" stages. Our empirical findings indicate that only\n10% of the total original training epochs are adequate for successful fine-tuning."}, {"title": "3 Experiments and Results", "content": ""}, {"title": "3.1 Dataset and Data-processing", "content": "ISIC Challenge Training Dataset. The International Skin Imaging Collabo-\nration (ISIC) challenge dataset is a collection of dermoscopic images of melanoma"}, {"title": "3.2 Implementation", "content": "We conducted experiments in PyTorch [25] using NVIDIA A100 40GB GPUs. We\nadopt different variants of ImageNet pretrained ResNet architecture (ResNet-\n18, 34, 50, and 101) [14]. The learning rates for impair and repair processes\nwere set at 0.001 and 0.003 respectively, using Adam optimiser [16], with a\nbatch size of 64. We perform a hyperparameters sweep and selected K = 50,\nand \\(\\beta\\) was selected as 0.02 for the ResNet50 model (details on hyperparameter\ntuning are shown in supplementary materials). To address class-imbalance, we\nadopt the WBCE loss function \\(\\mathcal{L}_{WBCE}\\). Furthermore, we use accuracy (ACC),\narea under the receiver operating characteristic curve (AUC) as primary per-\nformance metrics, and equalised odds (EOdds [13]) as fairness metric, as done\npreviously [7,12,33]. Our post fine-tuning method requires just an additional\nfive epochs, which is 10% of the initial 50-epoch training duration. Code will be\nreleased soon."}, {"title": "3.3 Results", "content": "We compare with baselines and SOTA models which we describe below. We\npre-trained the ResNet model on the training dataset \\(D_{org}\\) as a Baseline. Per-\nformance is evaluated using established metrics. Full fine-tuning (Full FT), a\nwidely adopted fine-tuning practice, adjusts all weights on the external dataset\n\\(D_e\\) with the loss function specified in Eq. 5. FairPrune [33] improves fairness\nby pruning parameters and prioritises subgroup accuracy; however, the opti-\nmal hyperparameter configuration is specific to model and dataset. LLFT [20]\nfine-tunes only the last layer of a deep classification model to promote fairness.\nSimilarly, Diff-Bias [21] fine-tunes on an external dataset, using a bias-aware\nloss function to steer network optimisation. We highlight that a good fairness\nscore does not constitute the sole criterion for success and that a model should"}, {"title": "4 Conclusion", "content": "Our study advances bias mitigation in discriminative models for dermatologi-\ncal disease. The proposed BMFT distinguishes the core features from biased\nfeatures, enhancing fairness without sacrificing classification performance. This\ntwo-step impair-repair fine-tuning approach can effectively reduce bias within\nminimal epochs, with only 10% of the original training effort, providing an effi-\ncient solution when computing resources or data access are limited. Our method\nis agnostic to the choice of pre-trained models. Our future work will explore a"}]}