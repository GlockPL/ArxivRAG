{"title": "BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning", "authors": ["Yuyang Xue", "Junyu Yan", "Raman Dutt", "Fasih Haider", "Jingshuai Liu", "Steven McDonagh", "Sotirios A. Tsaftaris"], "abstract": "Developing models with robust group fairness properties is paramount, particularly in ethically sensitive domains such as medical diagnosis. Recent approaches to achieving fairness in machine learning require a substantial amount of training data and depend on model re-training, which may not be practical in real-world scenarios. To mitigate these challenges, we propose Bias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method that enhances the fairness of a trained model in significantly fewer epochs without requiring access to the original training data. BMFT produces a mask over model parameters, which efficiently identifies the weights contributing the most towards biased predictions. Furthermore, we propose a two-step debiasing strategy, wherein the feature extractor undergoes initial fine-tuning on the identified bias-influenced weights, succeeded by a fine-tuning phase on a reinitialised classification layer to uphold discriminative performance. Extensive experiments across four dermatological datasets and two sensitive attributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA) techniques in both diagnostic accuracy and fairness metrics. Our findings underscore the efficacy and robustness of BMFT in advancing fairness across various out-of-distribution (OOD) settings.", "sections": [{"title": "1 Introduction", "content": "Machine learning is known to exhibit biases from various sources, such as human judgement, inherent algorithmic predispositions, and representation bias [22]. The latter emerges when data from minority ethnicities, genders, and age groups are under-represented, leading to unfair predictions by machine learning systems. This can result in serious issues, including direct or indirect discrimination [24]. It is important to ensure an equitable and responsible application of AI to not undermine the trustworthiness and acceptance of AI solutions by end users [32]."}, {"title": "2 Method", "content": "Bias in model prediction typically originates from two key sources: (1) the entanglement between noisy, harmful (spurious or irrelevant) features and, useful, core features within the feature extractor; and (2) the incorrect composition of representation in the classification layer, leading to the core features information loss [20]. The main idea of BMFT is to start with debiasing the feature extractor, by fine-tuning only the targeted weights that are disproportionately in-stigating bias. Subsequently, as the first process may affect how the core features are integrated by the classification layer for prediction, BMFT then fine-tunes a reinitialised classification layer."}, {"title": "2.1 Preliminaries", "content": "Consider an external dataset of N subjects, $D_e = {x_i,y_i,a_i}_{i=1}^N$, where $x_i$, $y_i$, and $a_i$ are the input image, the corresponding label, and the sensitive attribute, respectively. For simplicity, we assume binary targets and attributes (i.e. $y_i, a_i \u2208 {0,1}$). We will assume a compositional construction of a biased model $f_0(\u00b7)$, consisting of a feature extractor $E(\u00b7)$ and a classification head $C(\u00b7)$, pre-trained by the original training data $D_{org}$. Our goal is to debias the model $f_0(\u00b7)$ w.r.t. fairness metrics such as Demographic Parity (DP) [9] or Equalised Odds (EOdds) [13] by fine-tuning with significantly fewer epochs, using $D_e$; and further enhance the generalisability of the model on various OOD test datasets $D_t$, which share the same sensitive attribute $a_i$. To avoid attribute bias from the external dataset $D_e$, we sample a group-balanced subset $D_r$ from $D_e$. This ensures that each attribute group $a$ has the same amount of data. The proportion of positive to negative labelled images is adjusted to be the same in each $a$ group, which prevents bias due to different label distributions. Implementing these two"}, {"title": "2.2 Bias-Importance-Based Mask Generation", "content": "To identify the effective parameters for updating, we evaluated the importance of each parameter with respect to the designed bias influence function and loss function. The top K parameters, that are highly correlated with bias and yet provide only minimal contribution to prediction, were selected as the masking weights. We adopt weighted Binary Cross Entropy (WBCE) as loss function since the dermoscopic dataset contains label imbalance. Together with the bias influence function B, the differentiable proxy of EOdds [13], is given by:\n$L_{WBCE} = \\sum_{i} ( \\frac{N_n}{N_n + N_p} - y_i \\log(p_i) + \\frac{N_p}{N_n + N_p} -(1-y_i) \\log(1 - p_i) )$\t(1)\n$B(f_0, x_i, y_i, a_i) = \\frac{\\sum_{i=1}^{N}f_0(x_i)a_iy_i}{\\sum_{i=1}^{N} a_iy_i}, \\frac{\\sum_{i=1}^{N} f_0(x_i) (1 \u2013 a_iy_i}{\\sum_{i=1}^{N}(1 \u2013 a)y_i}$\t(2)\nwhere $p_i$ is the prediction probability, $N_p$ is the number of images with positive label, $N_n$ the number of images with negative label. For a pre-trained model $f_0(\u00b7)$ with effectively learnt parameters $\\theta^*$, the sensitivity for each parameter $\\theta$ can be obtained by the second-order derivative of loss close to the minimum [10]. This value can be interpreted as the importance of the parameter for the prediction. The diagonal of the Fisher Information Matrix (FIM) is equivalent to the second-order derivative of the loss [26], which provides an efficient computation through first-order derivatives. The first-order expression for the FIM is provided by:\n$I(f_0, D_e) = E[\\frac{\\partial log f_0(D_e|\\theta)}{\\partial \\theta} (\\frac{\\partial log f_0(D_e|\\theta)}{\\partial \\theta})^T]$\t(3)\nWe employ Eq. 3 to quantify the importance of each weight that contributes to the prediction. The importance of each weight for the bias can be computed using the second derivative of the bias influence function (that is, Eq. 2). Noting that the bias influence function is a linear function of the prediction, and thus replaces the loss function with the bias influence function during backpropaga-tion, the importance of a parameter w.r.t. the bias can also be derived by Eq. 3. We denote the weight mask as $M_i$, where i is the weight index.\nTo ensure that the weight importance for bias or prediction is not skewed in favour of a particular subgroup, in practice, we use the group-balanced dataset $D_r$ to calculate the importance. To select top K% weights in the mask $M_i$, we use the bias importance $I_{i,b}$ and the loss importance $I_{i,l}$ of each parameter as:\n$M_i = \\begin{cases} 1, & \\text{if } Z_{i,b}/I_{i,l} > \\alpha \\\\ 0, & \\text{otherwise} \\end{cases}$\t(4)"}, {"title": "2.3 Impair-Repair: Two-Step Fine-Tuning", "content": "To disentangle core features from biased features, whilst preserving predictive capability, a two-stage fine-tuning process is structured in an \"impair-repair\" manner. Firstly, we fine-tune the selected masked parameters within the feature extractor E(\u00b7) using the group-balanced dataset $D_r$, guided by an objective function (Eq. 2). Next, we fine-tune the entire reinitialised classification layer C(\u00b7). The weights identified by the mask in the feature extractor are closely linked to attribute bias and play a smaller role in predicting the class. This relationship helps us remove the unfavourable influence of bias contained in the model, and yet crucially also retain information embedded in core features.\nWe highlight that the pre-trained model has the capability to sufficiently extract core features. Further, eliminating the influence of (only) bias-effected weights suggests moving the model decision boundary away from bias features. Moreover, fine-tuning is prone to overfitting on a small dataset. These observations suggest that small epochs are sufficient to realise the proposed debiasing fine-tuning strategy.\nAn unbiased model, trained with a balanced dataset, will have the majority of its classification layer weights being zero, due to minimisation of irrelevant feature impact on class prediction [17]. To this end, we first reinitialise the weights and biases in the classification layer C(\u00b7) to zero to enable fast convergence to the optimal unbiased state and then fine-tune the layer on $D_r$ using a loss which combines the WBCE loss $L_{WBCE}$ and fairness constraints (i.e. Eq. 2), as:\n$L = L_{WBCE}(f_0) + \\beta B(f_0)$,\t(5)\nwhere the fairness constraint $B(f_0)$ modulates the emphasis of the features, and the hyperparameter $\\beta$ establishes an efficiently identified core trade-off between class prediction accuracy and fairness. The improved feature extractor retains core features without being influenced by attribute bias, allowing the model to efficiently identify and recover its predictive performance in limited epochs. To balance bias reduction with efficiency, an equal number of epochs is used for both the \"impair\" and \"repair\" stages. Our empirical findings indicate that only 10% of the total original training epochs are adequate for successful fine-tuning."}, {"title": "3 Experiments and Results", "content": ""}, {"title": "3.1 Dataset and Data-processing", "content": "ISIC Challenge Training Dataset. The International Skin Imaging Collabo-ration (ISIC) challenge dataset is a collection of dermoscopic images of melanoma"}, {"title": "3.2 Implementation", "content": "We conducted experiments in PyTorch [25] using NVIDIA A100 40GB GPUs. We adopt different variants of ImageNet pretrained ResNet architecture (ResNet-18, 34, 50, and 101) [14]. The learning rates for impair and repair processes were set at 0.001 and 0.003 respectively, using Adam optimiser [16], with a batch size of 64. We perform a hyperparameters sweep and selected K = 50, and $\u03b2$ was selected as 0.02 for the ResNet50 model (details on hyperparameter tuning are shown in supplementary materials). To address class-imbalance, we adopt the WBCE loss function $L_{WBCE}$. Furthermore, we use accuracy (ACC), area under the receiver operating characteristic curve (AUC) as primary performance metrics, and equalised odds (EOdds [13]) as fairness metric, as done previously [7,12,33]. Our post fine-tuning method requires just an additional five epochs, which is 10% of the initial 50-epoch training duration. Code will be released soon."}, {"title": "3.3 Results", "content": "We compare with baselines and SOTA models which we describe below. We pre-trained the ResNet model on the training dataset $D_{org}$ as a Baseline. Performance is evaluated using established metrics. Full fine-tuning (Full FT), a widely adopted fine-tuning practice, adjusts all weights on the external dataset $D_e$ with the loss function specified in Eq. 5. FairPrune [33] improves fairness by pruning parameters and prioritises subgroup accuracy; however, the optimal hyperparameter configuration is specific to model and dataset. LLFT [20] fine-tunes only the last layer of a deep classification model to promote fairness. Similarly, Diff-Bias [21] fine-tunes on an external dataset, using a bias-aware loss function to steer network optimisation. We highlight that a good fairness score does not constitute the sole criterion for success and that a model should"}, {"title": "4 Conclusion", "content": "Our study advances bias mitigation in discriminative models for dermatologi-cal disease. The proposed BMFT distinguishes the core features from biased features, enhancing fairness without sacrificing classification performance. This two-step impair-repair fine-tuning approach can effectively reduce bias within minimal epochs, with only 10% of the original training effort, providing an effi-cient solution when computing resources or data access are limited. Our method is agnostic to the choice of pre-trained models. Our future work will explore a"}]}