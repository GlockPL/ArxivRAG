{"title": "Optimal Classification Trees for Continuous Feature Data Using Dynamic Programming with Branch-and-Bound", "authors": ["Catalin E. Brita", "Jacobus G. M. van der Linden", "Emir Demirovi\u0107"], "abstract": "Computing an optimal classification tree that provably maximizes training performance within a given size limit, is NP-hard, and in practice, most state-of-the-art methods do not scale beyond computing optimal trees of depth three. Therefore, most methods rely on a coarse binarization of continuous features to maintain scalability. We propose a novel algorithm that optimizes trees directly on the continuous feature data using dynamic programming with branch-and-bound. We develop new pruning techniques that eliminate many sub-optimal splits in the search when similar to previously computed splits and we provide an efficient subroutine for computing optimal depth-two trees. Our experiments demonstrate that these techniques improve runtime by one or more orders of magnitude over state-of-the-art optimal methods and improve test accuracy by 5% over greedy heuristics.", "sections": [{"title": "1 Introduction", "content": "Decision trees combine human comprehensibility and accuracy and can capture complex non-linear relationships. As such, decision trees are well-suited models for explainable AI (Rudin 2019; Arrieta et al. 2020). Despite their straightforwardness, constructing an optimal decision tree (ODT, a tree with the smallest training error) of limited size is NP-hard (Hyafil and Rivest 1976). Therefore, greedy heuristics, such as CART (Breiman et al. 1984) and C4.5 (Quinlan 1993), have been widely used. These methods attain scalability by locally optimizing an information gain metric at each decision node, but yield on average less accurate and larger trees than optimal (Van der Linden et al. 2024).\nTo compute ODTs, some employ general-purpose solvers such as mixed-integer programming (MIP) (Bertsimas and Dunn 2017), constraint programming (Verhaeghe et al. 2020), or Boolean satisfiability (SAT) solvers (Narodytska et al. 2018). However, these approaches struggle to scale with the number of observations and features.\nBetter scalability is obtained by specialized algorithms using dynamic programming (DP) and branch and bound (BnB) (Aglin, Nijssen, and Schaus 2020a; Demirovi\u0107 et al. 2022). However, most of these algorithms cannot directly deal with numeric data which are frequently present in"}, {"title": "2 Related Work", "content": "Heuristics Traditionally, decision trees are trained using top-down induction heuristics such as CART (Breiman et al. 1984) and C4.5 (Quinlan 1993) because of their scalability. These heuristics recursively divide the data based on local criteria such as information gain or Gini impurity. On average this yields trees that are larger than the optimal tree (Murthy and Salzberg 1995) or, if constrained by a fixed depth, have lower out-of-sample accuracy than optimal trees under the same size limit (Van der Linden et al. 2024).\nOptimal Optimal decision trees globally optimize an objective (e.g., minimize the number of misclassifications) within a given size limit on the training data. Computing ODTs, however, is NP-hard (Hyafil and Rivest 1976), and thus scalability is challenging. To address this challenge, many different approaches have been proposed.\nGeneral-purpose solvers The first MIP approaches were proposed by Bertsimas and Dunn (2017) and Verwer and Zhang (2017). Many other formulations followed, typically using binarization of the continuous feature data to improve the scalability (Verwer and Zhang 2019; G\u00fcnl\u00fck et al. 2021; Aghaei, G\u00f3mez, and Vayanos 2024; Liu et al. 2024). Hua, Ren, and Cao (2022) instead use a multi-stage MIP model with novel lower bounds. Al\u00e8s, Hur\u00e9, and Lambert (2024) obtain stronger linear relaxations from a novel quadratic formulation. Narodytska et al. (2018), Janota and Morgado (2020), Avellaneda (2020), Hu et al. (2020) and Al\u00f2s, Ans\u00f3tegui, and Torres (2023) propose SAT models that also require binarization of continuous feature data. As far as we know, Shati, Cohen, and McIlraith (2021, 2023) propose the only SAT-based algorithm that can directly process continuous and categorical features. Finally, Verhaeghe et al. (2020) propose a constraint programming approach that also requires binary data. However, despite improvements, these MIP, SAT, and CP methods still face problems scaling beyond a few thousand data instances and trees of depth three.\nSpecialized algorithms Nijssen and Fromont (2007, 2010) introduced DL8, an early DP approach. Aglin, Nijssen, and Schaus (2020a,b) improved it to DL8.5 with branch-and-bound and extended caching techniques. Hu, Rudin, and Seltzer (2019) and Lin et al. (2020) contribute new lower bounds including a subproblem similarity bound. Demirovi\u0107 et al. (2022) introduce a specialized subroutine for trees of depth two and constraints for limiting the number of branching nodes. Kiossou et al. (2022) and Demirovi\u0107, Hebrard, and Jean (2023) improve the anytime performance of the search. Van der Linden, De Weerdt, and Demirovi\u0107 (2023) generalize previous DP approaches beyond classification. Because of all these algorithmic advances, a recent survey considers the DP approach currently the best in terms of scalability (Costa and Pedreira 2023). However, all of these methods require binarized input data.\nContinuous features Quant-BnB (Mazumder, Meng, and Wang 2022) is the only specialized algorithm for ODTS that directly optimizes datasets with continuous features. It employs branch-and-bound by splitting on quantiles of the feature distribution. Although Quant-BnB can handle much larger datasets than the MIP and SAT approaches, it also struggles to scale beyond trees of depth three.\nSummary Scalability advances for ODT search mostly require binarization. When operating directly on the numeric data, scalability is still challenging."}, {"title": "3 Preliminaries", "content": "In this section, we introduce notation, formally define the problem, and describe the lower-bounding technique that provides the basis for ConTree's pruning."}, {"title": "4 The ConTree Algorithm", "content": "We present the ConTree algorithm (CT) which constructs an ODT by recursively performing splits on every branching node within a full tree of pre-defined depth. Subproblems are identified by the dataset D and the remaining depth limit d. This results in the following recursive DP formulation:\n$$CT(D, d) = \\begin{cases}\n\\min_{\\hat{y}\\in Y} \\sum_{(x,y) \\in D} 1(\\hat{y} \\neq y) \\text{ if } d = 0 \\\\\n\\min_{f \\in F} Branch(D, d, f) \\text{ if } d > 0\n\\end{cases}$$\nLeaf nodes assign the label with the least misclassifications. Branching nodes find the feature f with the best misclassification score from the subtrees by calling the subprocedure Branch which iterates over all possible split thresholds \u03c4:\n$$Branch(D, d, f) = \\min_{\\tau \\in S_f} Split(D, d, f, \\tau) .$$\nEvery split results in two subproblems:\n$$Split(D, d, f, \\tau) = CT (D(f < \\tau), d \u2013 1) + CT (D(f > \\tau), d \u2212 1) .$$\nGiven a splitting feature f, computing the misclassification score $\u03b8_\u03c4$ for all possible split points $\u03c4 \u2208 S_f$ is computationally expensive since each split point considered requires solving two (potentially large) subproblems. Therefore, we provide the following runtime improvements (each of which preserves optimality):\nLower-bound pruning three novel pruning techniques specifically designed for continuous features to speed up the computation without losing optimality;\nDepth-two subroutine a subroutine for depth-two trees that iterates over sorted feature data to update class occurrences and efficiently solves the two depth-one sub-problems in Eq. (6) simultaneously;\nCaching the same dataset caching technique as Demirovi\u0107 et al. (2022): ConTree reuses cached solutions to subproblems (defined by D and d).\nTo control the trade-off between training time and accuracy, we also provide a max-gap parameter to set the maximum permissible gap to the optimal solution."}, {"title": "Pruning Techniques", "content": "Based on the similarity-based lower bound presented in Eq. (3), we present three novel pruning techniques to reduce the number of split points that need to be considered in Eq. (5) without losing optimality. The key idea is that if the feature data is sorted, the solution of any Split call with threshold \u03c4 provides a lower bound for all next calls with a different threshold \u03c4' since we can easily count how many observations shifted from the left to the right subtree by subtracting their indices in the sorted data: |z(\u03c4) \u2013 z(\u03c4')|.\nTheorem 1. Let UB be the best solution so far or the score needed to obtain a better solution. Let $\u03b8_\u03c4=Split(D, d, f, \u03c4)$ be the optimal misclassification score for the subtree when branching on f with threshold \u03c4. Then any other threshold \u03c4' with $|z(\u03c4) \u2212 z(\u03c4')| \u2264 \u03b8_\u03c4 \u2013 UB$ cannot yield an improving solution.\nProof. This follows directly from Eq. (3). If \u03c4' > \u03c4, then |D(f \u2264 \u03c4')\\D(f < \u03c4)| = z(\u03c4') \u2013 z(\u03c4) and if \u03c4' < \u03c4, then |D(f > \u03c4')\\D(f > \u03c4)| = z(\u03c4) \u2212 z(\u03c4'). Therefore, the SLB for a split at \u03c4' is: $\u03b8_{\u03c4'} \u2265 \u03b8_\u03c4 \u2212 |z(\u03c4) \u2212 z(\u03c4')| \u2265 UB$.\n$\\Box$\nCorollary 1. Let u be a split index with its corresponding solution value $\u03b8_u$ and index z(u) within $D_f$. Let \u2206 be the difference between $\u03b8_u$ and UB and at least one: $\u2206 = max(1, \u03b8_u \u2013 UB)$. Any improving split must have a threshold smaller than the value in $D_f$ at index $z(u) \u2013 \u2206$ or larger than the value at index z(u) + \u0394.\nPer Branch call, ConTree keeps track of the set of threshold indices that may yield a split that improves the current best tree. This set is represented as a set of index intervals Q. Initially, Q contains one interval of all indices: Q = {[1..m]}. After each Split call, ConTree updates the set Q by using three pruning functions P that return a list of pruned intervals from the current interval [i..j], the current UB, and one or more subproblem solutions \u03b8. Next, we explain these three pruning functions: neighborhood pruning, interval shrinking, and sub-interval pruning.\nNeighborhood pruning (NB) After the misclassification score $\u03b8_u$ for a split point $u \u2208 [i..j]$ is computed, neighborhood pruning uses the SLB to remove similar split points from consideration. A simplified illustration of this pruning technique can be seen in Fig. 1a. Using Cor. 1, we define two functions A and A that return the closest thresholds from u that could still improve on UB.\n$\\underline{A}(u, \u2206) = max \\{ u' \u2208 [m] \\mid z(u') < z(u) - \u2206 \\}$\n$\\overline{A}(u, \u2206) = min \\{ u' \u2208 [m] \\mid z(u') > z(u) + \u2206 \\}$\nThe functions A and A can be implemented using binary search with time complexity O(log(m)). Using these, we define $P_{NB}$ which yields two new intervals:\n$$P_{NB} ([i..j], u, \u2206) = \\{ [i..\\underline{A}(u, \u2206)], [\\overline{A}(u, \u2206)..j] \\}$$"}, {"title": "Interval shrinking (IS)", "content": "Interval shrinking is an extension of neighborhood pruning and acts whenever UB is updated. Given an interval [i..j] \u2208 Q, IS searches for the largest threshold index u \u2208 V smaller than i and the smallest threshold index v \u2208 V larger than j, with V the set of split indices for which the misclassification score $\u03b8_u$ and $\u03b8_v$ are already computed. Using Cor. 1, IS then prunes the interval [i..j] as illustrated in Fig. 1b. To search for these indices u and v, we define the function B that uses binary search over V with time complexity O(log(m)):\n$$B([i..j], V) = \\begin{cases}\nmax \\{ u \\in V \\mid u<i \\} \\\\\nmin \\{ v \\in V \\mid v>j \\}\n\\end{cases}$$\nAdditionally, IS uses the following theorem:\nTheorem 2. Let w by any split point with a solution $\u03b8_w$ with a left subtree misclassification score $\u03b8_{w,L}$ of zero. Then any split point u < w will yield $\u03b8_u \u2265 \u03b8_w$. Similarly, if the right subtree misclassification score $\u03b8_{w,R}$ is zero, then for all v > w also $\u03b8_v \u2265 \u03b8_w$.\nProof. W.l.o.g., consider the left case. Since u < w, it holds that D(f \u2264 $S_u$) \u2282 D(f \u2264 $S_w$) and D(f > $S_u$) \u2283 D(f > $S_w$). Eq. (3) then implies that $\u03b8_{u,L} \u2264 \u03b8_{w,L} = 0$ and $\u03b8_{u,R} \u2265 \u03b8_{w, R}$. Thus $\u03b8_u = \u03b8_{u,R}$, $\u03b8_w = \u03b8_{w,R}$, and $\u03b8_u \u2265 \u03b8_w$.\n$\\Box$\nLet $M_L$ and $M_R$ represent the right-most (left-most) index with a zero left (right) misclassification score found so far, incremented (decremented) by one. By combining Cor. 1 and Theorem 2, we define:\n$$P_{IS}([i..j], u, v, \u2206_u, \u2206_v, M_L, M_R) = [max(i, M_L, \\underline{A}(u, \u2206_u)), min(j, M_R, \\overline{A}(v, \u2206_v))]$$\nwhere $\u0394_u = \u03b8_u \u2013 UB$ and $\u0394_v = \u03b8_v \u2013 UB$.\nSub-interval pruning (SP) Sub-interval pruning can prune an entire interval [i..j] using the following theorem:\nTheorem 3. Let [i..j] be any threshold index interval. Let $\u03b8_u$ and $\u03b8_v$ be optimal solutions for previously computed split points u < i and v > j, with the corresponding left and right misclassification scores $\u03b8_{u,L}$, $\u03b8_{u,R}$, $\u03b8_{v,L}$, and $\u03b8_{v,R}$. Then, if $\u03b8_{u,L} + \u03b8_{v,R} > UB$, any split point w \u2208 [i..j] cannot improve on UB.\nProof. w > i > u and w < j < v and thus $\u03b8_{w,L} \u2265 \u03b8_{u,L}$ and $\u03b8_{w,R} \u2265 \u03b8_{v,R}$. Therefore, $\u03b8_w = \u03b8_{w,L} + \u03b8_{w,R} >UB$.\n$\\Box$\nFrom Theorem 3, we define:\n$$P_{SP}([i..j],UB, \u03b8_{u,L},\u03b8_{v,R}) = \\begin{cases}\n\\varnothing, & \\text{if } \u03b8_{u,L} + \u03b8_{v,R} > UB \\\\\n [i..j], & \\text{otherwise} .\n\\end{cases}$$\nGeneral Recursive Case\nTo eliminate the exploration of unnecessary splits in the recursion of Eq. (4), we use the pruning mechanisms presented above and keep track of upper bounds."}, {"title": "Depth-Two Subroutine", "content": "To improve runtime, Demirovi\u0107 et al. (2022) introduced a specialized subroutine for trees of depth-two that is more efficient than doing recursive calls. However, it requires a quadratic amount of memory in terms of the number of binary features, which is problematic if we consider a binary feature for every possible threshold on continuous data.\nInstead, we provide a specialized subroutine that simultaneously finds an optimal left and right subtree of a depth-two split and does not have this quadratic memory consumption by exploiting the fact that we can sort the observations by their feature values. Since splitting the data preserves the order, we sort the dataset once in the beginning. Then for the sorted data, Alg. 2 shows how an optimal depth-two tree can be found in O(|D||F|) for a given split point w on feature $f_1$ for the root node of the subtree. The core idea is that we first count with the variables $FQ_y^L$ and $FQ_y^R$ how many observations of class y go to the left and right by splitting at point w. Then, when deciding on the second splitting feature $f_2$ (of either the left or right subtree), we traverse all observations sorted by $f_2$ and incrementally keep track of the current counts per label $C_y^L$ and $C_y^R$. Based on these two label counts, the label counts for all splitting thresholds on $f_2$ for all four leaf nodes of a depth-two tree can be determined as the dataset is traversed:\n$$C_{LL}= C_y^L, \\\\\\ C_{RL} = C_y^R\\\\\\  C_{LR} = FQ_y^L - C_y^L, \\\\\\ C_{RR} = FQ_y^R - C_y^R.$$\nAlg. 2 shows how the minimal misclassifications in the subtrees $\u03b8_{LL}$ and $\u03b8_{RR}$ can be computed based on these values.\nOptimality Gap\nWe add a max-gap parameter that determines how far from optimal the final solution is allowed to be. This increases the pruning strength at the expense of optimality. For example, for NB, the distance \u2206 to a possibly improving split is now computed as $\u2206 = max(1, \u03b8_u-(UB-max-gap))$. To use the gap parameter across multiple depths of the search, we set the max-gap for the current depth to half of the total. The other half is distributed evenly over the two subproblems. This allows a trade-off between training time and accuracy.\nComparison to Previous Bounding Methods\nMazumder, Meng, and Wang (2022) propose three lower bounds that require splitting the data into quantiles for a given feature. Their first lower bound is similar to our sub-interval pruning, but our definition is independent of the remaining depth budget. Their other lower bounds are tighter, but also more expensive to compute. Future work could investigate using such or similar lower bounds in ConTree.\nThe similarity-based lower bound (SLB) was proposed in previous work (Hu, Rudin, and Seltzer 2019; Lin et al. 2020; Demirovi\u0107 et al. 2022), but our application of the bound is more efficient. Lin et al. (2020) point out that the similar support bound in OSDT (Hu, Rudin, and Seltzer 2019) is too expensive to compute frequently. Therefore, they propose to use hash trees to identify similar subtrees but provide no further details. The implementation of their method, GOSDT, computes the difference between two subproblems by computing the xor of bit-vectors that represent the dataset corresponding to the subproblems. Demirovi\u0107 et al. (2022) loop once over the two sorted lists of identifiers of the datasets to count the differences. Both of these approaches require O(n) operations to compute the difference. ConTree, on the other hand, exploits the properties of sorted numeric feature data and computes the bound in (1) by computing the difference between the two split indices. It applies the bound using the novel pruning techniques in O(log(m))."}, {"title": "5 Experiments", "content": "Our experiments aim to answer the following questions: 1) what is the effect of the pruning techniques and the depth-two subroutine; 2) how does ConTree's runtime compare to state-of-the-art ODT algorithms; and 3) what is ConTree's anytime performance? In the appendices B, C, and D, we additionally answer the following questions: 4) how does"}, {"title": "A Data Preprocessing", "content": "All datasets used in the experiments are from the UCI machine learning repository (Dua and Graff 2017). We apply the same data preprocessing techniques as Mazumder, Meng, and Wang (2022): all features that do not assist prediction are removed (e.g., timestamps or unique identifiers). For the experiments that measure the runtime and memory usage, we use the train split from their repository. For the out-of-sample experiments in Appendix D, we use the full dataset. The preprocessing per dataset is as follows:\nAvila (Stefano et al. 2018) All features are kept as is.\nBank (Lohweg 2013) We keep all the features from the Banknote Authentication dataset.\nBean (Koklu and Ozkan 2020) We keep all the features from the Dry Bean dataset.\nBidding (eBay 2020) We remove the features Record_ID, Auction_ID, and Bidder_ID from the Shill Bidding dataset.\nEeg (Roesler 2013) We keep all the features from the EEG Eye State dataset.\nFault (Buscema, Terzi, and Tastle 2010) We keep all the features from the Steel Plates Faults dataset.\nHtru (Lyon et al. 2016) We keep all the features from the HTRU2 dataset.\nMagic (Bock 2007) We keep all the features from the MAGIC Gamma Telescope dataset.\nOccupancy (Candanedo and Feldheim 2016) We remove the id and date features from the Occupancy Detection dataset.\nPage (Malerba 1995) We keep all the features from the Page Blocks Classification dataset.\nRaisin (\u00c7\u0131nar, Koklu, and Ta\u015fdemir 2020) All features are kept as is.\nRice (Cinar and Koklu 2019) We keep all the features from the Rice (Cammeo and Osmancik) dataset.\nRoom (Singh et al. 2018) We remove the Date and Time features from the Room Occupancy Estimation dataset.\nSegment (Brodley 1990) We remove the REGION-PIXEL-COUNT feature from the Image Segmentation dataset because it has only one unique value.\nSkin (Bhatt and Dhall 2012) We keep all the features from the Skin Segmentation dataset.\nWilt (Johnson 2014) All features are kept as is.\nTo determine all unique values per feature in a dataset, we sort the instances and check if consecutive values differ more than a small value \u025b. In this paper, we set \u025b = 1\u00d710-7."}, {"title": "B Results for Larger Depth Limits", "content": "Table 3 and 4 show the results for training optimal decision trees of depths five and six. The results are averaged over five runs. ConTree finds and proves optimal trees for eight and seven datasets respectively."}, {"title": "C Memory Usage Results", "content": "Table 5 reports the memory usage of the SAT-Shati, Quant-BnB, and ConTree for computing trees of depth two (all methods), three (Quant-BnB and ConTree), and four (only ConTree). For depth two, memory poses no problem for all three methods. For depth three, Quant-BnB uses 15GB of memory for Eeg and Htru and even 25GB for Fault. In contrast, ConTree's memory usage for these datasets is 18MB, 15MB, and 7MB respectively. At depth four, ConTree's maximum memory usage is for the Skin dataset, where it uses 123MB of memory. This highlights ConTree's memory efficiency in comparison to previous methods.\nTable 3 and Table 4 further show the maximum memory usage when computing optimal trees of depths five and six. For all datasets, except Skin, the memory consumption stays within 320MB. The Skin dataset is the only dataset where ConTree uses a lot of memory when optimizing deeper trees. The reason for this is the size of the dataset which is ten times larger than the next largest dataset in our benchmark. If memory becomes an issue, ConTree's caching mechanism could be changed from dataset caching to branch caching to lower memory consumption (Demirovi\u0107 et al. 2022)."}, {"title": "D Out-of-Sample Results", "content": "To test the out-of-sample performance of ConTree, we compare it to the CART heuristic, and to optimal decision trees trained on binarized data. We test two binarization approaches: one based on quantiles and the other based on threshold guessing using a reference ensemble (McTavish et al. 2022). For the quantile approach, we binarize each of the numeric features using thresholds on ten quantiles of the feature distribution. For the threshold guessing approach, we follow McTavish et al. (2022) and train a gradient boosting classifier with the number of estimators set to ten times the number of continuous features and the maximum depth to two. After training, the least important features are removed iteratively until the performance of the ensemble drops."}]}