{"title": "GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model", "authors": ["Yongjie Fu", "Yunlong Li", "Xuan Di"], "abstract": "Autonomous driving training requires a diverse range of datasets encompassing various traffic conditions, weather scenarios, and road types. Traditional data augmentation methods often struggle to generate datasets that represent rare occurrences. To address this challenge, we propose GenDDS, a novel approach for generating driving scenarios generation by leveraging the capabilities of Stable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology involves the use of descriptive prompts to guide the synthesis process, aimed at producing realistic and diverse driving scenarios. With the power of the latest computer vision techniques, such as ControlNet and Hotshot-XL, we have built a complete pipeline for video generation together with SDXL. We employ the KITTI dataset, which includes real-world driving videos, to train the model. Through a series of experiments, we demonstrate that our model can generate high-quality driving videos that closely replicate the complexity and variability of real-world driving scenarios. This research contributes to the development of sophisticated training data for autonomous driving systems and opens new avenues for creating virtual environments for simulation and validation purposes.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving landscape of artificial intelligence, generative models have emerged as a cornerstone technology, significantly impacting various industries, including autonomous driving and smart city [1], [2]. These advanced models, capable of synthesizing highly realistic and diverse data, are revolutionizing the way we approach the development and testing of autonomous vehicles. By generating intricate driving scenarios, they offer a scalable solution to the challenge of training and validating autonomous systems under a wide array of conditions. This capacity not only accelerates the advancement of autonomous driving technologies but also enhances their safety and reliability, marking a pivotal shift in our journey toward fully autonomous transportation systems.\nThe past year has witnessed significant advancements in deep generative models, spanning multiple data areas, including natural language, audio, and visual content. The domain of self-driving technology increasingly requires training data of superior quality that is well-annotated. The methods for obtaining autonomous driving datasets often rely on simulators or image generation models to expand the dataset. However, capturing datasets under specific environmental conditions or unique scenarios can be challenging using these methods. Data generated by simulators is not as realistic compared to the real-world data. As a result, to create a more straightforward and realistic generator for autonomous driving datasets, we propose GenDDS, which utilizes diffusion and text-to-video models for this task."}, {"title": "A. Related Work", "content": "Video generation models utilize neural networks to generate video samples. GAN-based (generative adversarial network [3], [4]), autoregressive-based, VAE-based (variational autoencoder [5]), normalizing flow [6], and diffusion-based generators are widely used in data generation. Diffusion probabilistic models (DM) have emerged as a state-of-the-art technique across several domains, notably in image and video generation applications. For images, diffusion models have been successfully applied to tasks such as generation, editing, and image-to-image translation [7]. In the realm of video, recent advancements in diffusion models have significantly improved video quality and realism. Applications include general video generation [8], as well as the generation of video from text prompts [9], demonstrating enhanced capabilities in creating dynamic and realistic video sequences.\nText-to-image models are a popular kind of controllable generation model, that can generate high-resolution, multi-styled images. Various strategies have been explored to extend the quality and efficiency of generation. The transformer architecture is widely utilized across various tasks [10], [11]. Low-Rank Adaptation (LoRA) [12] for the transformer architecture is a useful fine-tuning strategy for the diffusion model.\nExisting research has employed diffusion-based models within the realms of autonomous driving and transportation. Specifically, the guided conditional diffusion model has been leveraged to create controllable traffic simulations, allowing users to dictate the desired properties of trajectories [13]. Li introduces DrivingDiffusion [14] as a technique for generating multi-view videos of autonomous driving scenes, featuring precise layout control. Harvey [8] introduces a framework based on Diffusion Probabilistic Models (DDPMS) that is capable of generating extended video sequences with realistic and coherent scene completion.\nIn this study, we propose GenDDS to generate contextually relevant driving videos based on textual descriptions."}, {"title": "II. PRELIMINORY", "content": "Diffusion Models (LDM) [15] represent an approach in the field of generative modeling, particularly in the generation of a data distribution p(x), by operating within a latent space rather than the original data space. The core concept behind LDMs lies in their ability to learn a distribution over a compressed, or latent, representation of data, which significantly reduces computational costs and improves model efficiency. A key equation that underlies the LDM framework is the diffusion process, which is modeled as a Markov chain of Length T that gradually adds noise to the data over a sequence of steps until it reaches a Gaussian distribution.\nLDM can be interpreted as an equally weighted sequence of demonizing autoencoders e\u04e9(xt,t);t = 1...T, which are trained to predict a denoised variant of their input x\u2081, where xt is a noisy version of the input x. The corresponding objective can be simplified to:\nLDM = Ex\u2081,e~N(0,1),1 [|| E \u2013 eo(x,t)||2] (1)\nwith t uniformly sampled from {1,...,T}.\nThe Stable Diffusion model is a more flexible, conditional image generator that utilizes a U-Net backbone and cross-attention mechanisms. It is effective for learning models that leverage attention across various input modalities. Diffusion models are able to model conditional distributions of the form p(zy), which can be implemented with a conditional demonizing autoencoder to control the synthesis process through condition y such as text prompts.\nDiffusion models are enhanced to become more versa-tile generators of conditional images by augmenting their foundational UNet structure, which is known for its efficacy with the cross-attention mechanism cited in the work. This mechanism is adept at learning from models that base attention on a range of input types, as indicated in references. In order to process the input y from diverse sources, such as verbal instructions, a domain-specific encoder \u03c4\u03bf is utilized to map y into a preliminary representation Te (y) \u2208 RM\u00d7dr. This is subsequently applied to the intermediate layers of the UNet by employing a cross-attention layer. The cross-attention layer is defined as:\nAttention(Q, K, V) = softmax (OKT / \u221ad )V (2)\nWhere the query Q, key K, and value V matrices are produced as follows:\nQ=W\u00ba\u00b7 \u03a6\u03af(zt),\nK = W\u00b7 \u03c4\u03bf (y),\nV = WV \u00b7 \u03c4\u04e9 (y).\nWhere i denotes a representation of UNet implementing and W,w,w are learnable projection matrices."}, {"title": "B. Stable Diffusion XL (SDXL)", "content": "Stable Diffusion XL [16] is a latent diffusion model designed for the task of generating images conditioned on text. The distinction of SDXL lies in its three times larger U-Net backbone, which provides more attention blocks and a larger cross-attention context. SDXL shows improved performance compared to the stable diffusion model.\nSDXL utilizes a heterogeneous distribution of transformer blocks within the U-Net. Additionally, it opts for a more powerful pre-trained text encoder (OpenCLIP ViT-BigG in combination with CLIP ViT-L [17]) for text conditioning. This modification results in a U-Net model size of 2.6 billion parameters, compared to the stable diffusion model's 865 million parameters."}, {"title": "C. Low-Rank Adaptation (LORA)", "content": "LORA [12] locks the weights of the pre-trained model and introduces trainable matrices based on rank decomposition into every layer of the transformer structure. This technique significantly lowers the count of parameters that need to be trained for subsequent tasks, which can work with any diffusion model.\nThe updates of the weights have a low \u201cintrinsic rank [18]\u201d during adaptation. For an initially established weight matrix Wo \u2208 Rd\u00d7k, its modification is limited through a low-rank factorization, expressed as Wo+AW = Wo + BA. Here, B\u2208 Rd\u00d7r and A \u2208 Rr\u00d7k, where the rank r is significantly smaller than the smaller dimension between d and k. In the training process, Wo remains static and does not undergo gradient updates, while A and B are adjustable parameters. It's important to note that both Wo and AW = BA interact with the identical input, and their output vectors are added together element-wise. The updated forward pass formula is then given by h = Wox, indicating:\nh = Wox+AWx = Wox+BAx (3)"}, {"title": "D. ControlNet", "content": "ControlNet is an innovative neural network architecture designed to inject additional conditional controls into diffusion models. ControlNet will inject additional conditions into the neural network, which can give control to a large pre-trained diffusion model. The ControlNet is applied to each of the encoder levels of the U-Net. A trainable copy of the encoding blocks and the middle block of the diffusion model is created. The ControlNet is computationally efficient since the locked copy parameters are frozen, no gradient computation is required in the originally locked encoder for the fine-tuning. ControlNet is computationally efficient because its locked copy parameters are frozen, eliminating the need for gradient computation in the originally locked encoder during fine-tuning."}, {"title": "E. Hotshot-XL", "content": "Hotshot-XL [19] is a text-to-GIF generation model that can work together with SDXL. We are able to feed any fine-tuned SDXL model into Hotshot-XL. Loading the fine-tuned SDXL-based LoRAs is easier than fine-tuning Hotshot-XL. The architecture of Hotshot-XL is illustrated in Figure 2. Hotshot-XL consists of multiple SDXL modules and a Hotshot-XL temporal transformer module, which are connected sequentially to take a prompt as input and output the GIFs. The temporal transformer architecture is detailed on the right side of Figure 2, consisting of normalization, self-attention, and feed-forward neural networks. The Hoishot-XL can generate high-resolution GIFs at 8 frames per second and supports various image sizes."}, {"title": "III. PROPOSED PIPLINE", "content": "We aim to address the challenge of generating realistic driving videos straightforwardly using a generative model. Consequently, we propose a pipeline that leverages the capabilities of SDXL to generate a driving video dataset.\nFigure. 3 illustrates how GenDDS integrates the SDXL, LORA, ControlNet auto-tagger process, and Hotshot-XL. We extract raw images from open datasets and use Tagger to generate tags for each frame. These prepared datasets are then used to train the LoRA layers in SDXL. Once the fine-tuned SDXL model is ready, we feed it into Hotshot-XL, which acts as a module in the sequence. Concurrently, we use ControlNet to enhance the spatial relationships generated by Hotshot-XL. After completing the entire pipeline, we can run inference with Hotshot-XL to generate driving videos. We will illustrate the details of each module in the experiment section."}, {"title": "IV. EXPERIMENTS", "content": "The main dataset used in our experiments is the KITTI dataset [20], a set of benchmarks for computer vision algorithms mainly used in autonomous driving scenarios. The KITTI dataset is created by the Karlsruhe Institute of Technology and the Toyota Technological Institute in Chicago. Ranging from stereo images to optical flow, from visual odometry to 3D object detection and tracking. In our project, we utilize the high-resolution images and sequences from this dataset to train our model to generate realistic driving scenarios under various environmental conditions."}, {"title": "B. Experiment Setup", "content": "Our experiments utilize the functionality of the SDXL base model [16], which is known for its ability to generate high-resolution images through deep learning techniques. We enhanced this model by integrating Low-Rank Adaptation (LoRA), a modification designed for efficient parameter updating. This adaptive technique facilitates specialized training of the large-scale model for specific tasks, thus greatly reducing the need for full retraining.\nA component of the data preparation was the use of the WD 1.4 MOAT Tagger V2 [21] tagging mechanism. This tagging system works by labeling each frame with the data that the image contains, such as \"outdoors, road, scenery, sky...\". The automated tagging is followed by a careful manual review to ensure the accuracy of the tagging and the distinction between different images, including traffic conditions and other keywords such as \"light traffic\" or \"heavy traffic\". This labeling strategy improves model training efficiency by providing highly accurate annotations. These tags help train the model to recognize and generate nuances in driving scenarios based on different environments and traffic conditions.\nThe computational backbone of our training infrastructure is equipped with NVIDIA 3090Ti GPUs. Our training process employs a selected set of hyperparameters to optimize the balance between model accuracy and computational resource utilization. Model optimization was performed with the AdamW8bit optimizer, employing a fine-grained learning rate strategy starting at 0.0001. This approach is complemented by applying different learning rates to the U-Net and text encoder portions of the model, thus enabling customized learning trajectories within the model architecture.\nOne aspect of our model configuration is the selection of training parameters to improve efficiency and effectiveness. We set the model to run 32 iteration steps per image, a decision aimed at achieving a balance between image quality and computational demands. We used the DPM++ 2M Karras sampler and set the CFG ratio to 7.5, which improves the model's ability to generate images that are consistent with the textual descriptions and input conditions, thus maintaining the thematic integrity of the generated scenes."}, {"title": "C. Experiment Process", "content": "In our experimental framework, we employ real-world driving footage as ControlNet [22]. This innovative approach utilizes the realism of actual road conditions captured on video to guide the image synthesis process. By integrating SDXL models enhanced with LoRA technology into the Hotshot XL environment [19], we set the stage for generating video content. This approach allowed us to generate continuous frame sequences that effectively simulate dynamic driving scenarios.\nIn ControlNet, depth information is estimated from real traveling video inputs by depth detection methods. This depth information plays a role throughout the image generation process, guiding the model to maintain accurate spatial relationships and perspectives in the generated image. The integration of depth estimation ensures that the synthetic enhancements applied during the generation process are not only visually realistic, but also contextually appropriate and reflective of real driving scenarios.\nThe core of this process revolved around using the real driving videos as a baseline from which the model could generate enhanced or varied scenarios under controlled con-ditions. The Hotshot XL facilitated this by providing the architecture to process the SDXL-LORA model efficiently. Through this setup, we could manipulate the original video frames, applying synthetic alterations that represent different weather conditions, traffic densities, and other environmental variables, all while maintaining the original video's structural integrity."}, {"title": "D. Results", "content": "The experiment results for GenDDS are visualized in a series of graphs that illustrate the model's ability to synthesize highly realistic, contextually coherent video sequences under a variety of conditions. From contextual coherence to environmental adaptability, each diagram emphasizes a different aspect of the model's capabilities.\nIn Figure. 5, our model is able to generate video sequences under different weather conditions, including sunny, cloudy, foggy, and rainy days, thus highlighting the versatility of the model. This variety of outputs demonstrates the model's broad understanding of environmental factors and its ability to modify the scene accordingly. From changing lighting conditions to introducing elements such as rain or fog, each weather condition presents a set of challenges that our model handles with ease, giving us a glimpse of its great adaptability.\nFigure. 4 provides a snapshot of our model's capability to maintain contextual coherence across sequential frames. It showcases a series of four continuous frames where the model has successfully captured the dynamic nature of the driving scenario. Notably, the forward progression of vehicles in the scene is realistically rendered, with a particular focus on a vehicle that gradually moves forward across the frames. This illustrates the model's adeptness at understanding and applying the temporal continuity essential for video generation, ensuring that each frame is a logical progression from the last.\nBuilding upon the rain scenario, Figure. 6 explores the model's ability to generate videos with different traffic densities. This feature is particularly noteworthy as it demonstrates the model's capacity for fine-grained control over the elements within the scene. By adjusting the number of vehicles, our model can simulate a range of traffic conditions from light to heavy, offering insights into its potential for creating varied driving scenarios that are crucial for training autonomous driving systems.\nWhile the initial series of images focused on demonstrating variability in consistent streetscapes for comparison purposes, Figure. 7 expands the scope to demonstrate the model's ability to generate different streetscapes. This illustrates the scalability and adaptability of our approach, confirming the ability of our model to simulate driving scenarios with different street layouts in addition to varying weather conditions and traffic density. The diversity of streetscapes highlights the potential of the model to generate a variety of realistic driving environments.\nIn Figure. 8, we demonstrate our model's capability to produce driving videos under rare conditions, which do not usually happen in the real world, such as \"Heavy Traffic on a Snowy Day in Rural Areas\" and \"Heavy Traffic on a Foggy Day on the Highway.\" The videos are both realistic and high-resolution. The outcomes are promising across various conditions involving traffic density, weather, and types of streetscapes."}, {"title": "V. CONCLUSIONS", "content": "This paper introduces GenDDS, a novel pipeline that leverages a stable diffusion-based model to generate driving videos under diverse conditions. Key features of GenDDS include the use of an auto-tagger mechanism for dataset preparation. Additionally, we employ LoRa to fine-tune the model and integrate ControlNet to inject additional conditions, enhancing the relevance and coherence of the generated videos.\nThe evaluation results conducted on the KITTI dataset provide compelling indications that our proposed methodology is capable of generating high-resolution and realistic driving videos under different conditions or condition combinations. This work delves into the frontiers of the autonomous driving domain, focusing specifically on methods for driving video generation. With GenDDS, we can easily generate driving videos based on different conditions and settings.\nTo further advance the research, the generation length and complexity could be increased. Moreover, the conditions for generation could extend beyond textual descriptions of the video to include factors related to driver behavior and vehicle dynamics. Generative models can further expand the capabilities of autonomous driving technologies."}]}