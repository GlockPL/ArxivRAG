{"title": "GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model", "authors": ["Yongjie Fu", "Yunlong Li", "Xuan Di"], "abstract": "Autonomous driving training requires a diverse range of datasets encompassing various traffic conditions, weather scenarios, and road types. Traditional data augmentation methods often struggle to generate datasets that represent rare occurrences. To address this challenge, we propose GenDDS, a novel approach for generating driving scenarios generation by leveraging the capabilities of Stable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology involves the use of descriptive prompts to guide the synthesis process, aimed at producing realistic and diverse driving scenarios. With the power of the latest computer vision techniques, such as ControlNet and Hotshot-XL, we have built a complete pipeline for video generation together with SDXL. We employ the KITTI dataset, which includes real-world driving videos, to train the model. Through a series of experiments, we demonstrate that our model can generate high-quality driving videos that closely replicate the complexity and variability of real-world driving scenarios. This research contributes to the development of sophisticated training data for autonomous driving systems and opens new avenues for creating virtual environments for simulation and validation purposes.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving landscape of artificial intelligence, generative models have emerged as a cornerstone tech- nology, significantly impacting various industries, including autonomous driving and smart city [1], [2]. These advanced models, capable of synthesizing highly realistic and diverse data, are revolutionizing the way we approach the devel- opment and testing of autonomous vehicles. By generating intricate driving scenarios, they offer a scalable solution to the challenge of training and validating autonomous systems under a wide array of conditions. This capacity not only accelerates the advancement of autonomous driving tech- nologies but also enhances their safety and reliability, mark- ing a pivotal shift in our journey toward fully autonomous transportation systems.\nThe past year has witnessed significant advancements in deep generative models, spanning multiple data areas, includ- ing natural language, audio, and visual content. The domain of self-driving technology increasingly requires training data of superior quality that is well-annotated. The methods for obtaining autonomous driving datasets often rely on sim- ulators or image generation models to expand the dataset. However, capturing datasets under specific environmental conditions or unique scenarios can be challenging using these methods. Data generated by simulators is not as realistic compared to the real-world data. As a result, to create a more straightforward and realistic generator for autonomous driv- ing datasets, we propose GenDDS, which utilizes diffusion and text-to-video models for this task."}, {"title": "A. Related Work", "content": "Video generation models utilize neural networks to gen- erate video samples. GAN-based (generative adversarial net- work [3], [4]), autoregressive-based, VAE-based (variational autoencoder [5]), normalizing flow [6], and diffusion-based generators are widely used in data generation. Diffusion probabilistic models (DM) have emerged as a state-of-the-art technique across several domains, notably in image and video generation applications. For images, diffusion models have been successfully applied to tasks such as generation, editing, and image-to-image translation [7]. In the realm of video, recent advancements in diffusion models have significantly improved video quality and realism. Applications include general video generation [8], as well as the generation of video from text prompts [9], demonstrating enhanced capa- bilities in creating dynamic and realistic video sequences.\nText-to-image models are a popular kind of controllable generation model, that can generate high-resolution, multi- styled images. Various strategies have been explored to ex- tend the quality and efficiency of generation. The transformer architecture is widely utilized across various tasks [10], [11]. Low-Rank Adaptation (LoRA) [12] for the transformer architecture is a useful fine-tuning strategy for the diffusion model.\nExisting research has employed diffusion-based models within the realms of autonomous driving and transportation. Specifically, the guided conditional diffusion model has been leveraged to create controllable traffic simulations, allowing users to dictate the desired properties of trajectories [13]. Li introduces DrivingDiffusion [14] as a technique for generat- ing multi-view videos of autonomous driving scenes, featur- ing precise layout control. Harvey [8] introduces a frame- work based on Diffusion Probabilistic Models (DDPMS) that is capable of generating extended video sequences with realistic and coherent scene completion.\nIn this study, we propose GenDDS to generate contextu- ally relevant driving videos based on textual descriptions."}, {"title": "II. PRELIMINORY", "content": "Our methodology leverages the KITTI driving dataset, a comprehensive collection of real-world automotive driving scenarios, to train a LoRA (Low-Rank Adaptation) layer on top of the pre-existing SDXL model. This adaptation allows for the efficient incorporation of dynamic, text-based requirements into the video generation process.\nFollowing the initial training phase, the enhanced SDXL model, equipped with the trained LoRA layer, is employed within the Hotshot XL framework for the generation of images. To enrich the model's capability in handling di- verse driving scenarios and improve its generalization, we introduce an additional component known as ControlNet. ControlNet utilizes auxiliary driving videos, beyond those found in the KITTI dataset, to provide contextual grounding and control signals during the image synthesis process.\nThe highlight of GenDDS is a sophisticated video genera- tion model capable of producing high-quality driving videos that are not only visually compelling but also accurately reflect the nuances of the provided textual descriptions. This approach demonstrates a significant advancement in the field of conditional video synthesis, particularly in the domain of autonomous driving simulation and training environments."}, {"title": "B. Contributions of this work", "content": "We propose GenDDS, a driving video generation method. The main contributions of the framework can be summarized as follows:\n1) Utilize the generative model API to create text prompts from video inputs together with manual creation, streamlining the process of preparing training data.\n2) Fine-tune the LoRA-based Stable Diffusion XL (SDXL) model on a real-world driving dataset.\n3) Load the SDXL model in Hotshot-XL and generate driving scenarios given different text prompt conditions together with ControlNet.\nThe rest of the paper is organized as follows. Sec. II introduces the preliminary knowledge used in this paper. Sec. III illustrates the solution approach. Sec. IV introduces the setting and results of the experiments. And Sec.V concludes this study."}, {"title": "A. Stable Diffusion", "content": "Diffusion Models (LDM) [15] represent an approach in the field of generative modeling, particularly in the generation of a data distribution p(x), by operating within a latent space rather than the original data space. The core concept behind LDMs lies in their ability to learn a distribution over a compressed, or latent, representation of data, which sig- nificantly reduces computational costs and improves model efficiency. A key equation that underlies the LDM framework is the diffusion process, which is modeled as a Markov chain of Length T that gradually adds noise to the data over a sequence of steps until it reaches a Gaussian distribution.\nLDM can be interpreted as an equally weighted sequence of demonizing autoencoders e\u04e9(xt,t);t = 1...T, which are trained to predict a denoised variant of their input x\u2081, where xt is a noisy version of the input x. The corresponding objective can be simplified to:\n$L_{DM} = E_{x_0, \\epsilon \\sim \\mathcal{N}(0,1), t}[||\\epsilon - e_\\theta(x_t, t)||^2]$\\nwith t uniformly sampled from {1,...,T}.\nThe Stable Diffusion model is a more flexible, conditional image generator that utilizes a U-Net backbone and cross- attention mechanisms. It is effective for learning models that leverage attention across various input modalities. Diffusion models are able to model conditional distributions of the form p(zy), which can be implemented with a conditional demonizing autoencoder to control the synthesis process through condition y such as text prompts.\nDiffusion models are enhanced to become more versa- tile generators of conditional images by augmenting their foundational UNet structure, which is known for its efficacy with the cross-attention mechanism cited in the work. This mechanism is adept at learning from models that base attention on a range of input types, as indicated in references. In order to process the input y from diverse sources, such as verbal instructions, a domain-specific encoder \u03c4\u03b8 is utilized to map y into a preliminary representation \u03c4\u03b8(y) \u2208 RM\u00d7dr. This is subsequently applied to the intermediate layers of the UNet by employing a cross-attention layer. The cross- attention layer is defined as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$\\nWhere the query Q, key K, and value V matrices are produced as follows:\n$Q = W^Q \\cdot \\phi_i(z_t),$\n$K = W^K \\cdot \\tau_\\theta(y),$\n$V = W^V \\cdot \\tau_\\theta(y).$\nWhere \u03c6i denotes a representation of UNet implementing and WQ,WK,WV are learnable projection matrices."}, {"title": "B. Stable Diffusion XL (SDXL)", "content": "Stable Diffusion XL [16] is a latent diffusion model de- signed for the task of generating images conditioned on text. The distinction of SDXL lies in its three times larger U-Net"}, {"title": "C. Low-Rank Adaptation (LORA)", "content": "LORA [12] locks the weights of the pre-trained model and introduces trainable matrices based on rank decomposition into every layer of the transformer structure. This technique significantly lowers the count of parameters that need to be trained for subsequent tasks, which can work with any diffusion model.\nThe updates of the weights have a low \u201cintrinsic rank [18]\u201d during adaptation. For an initially established weight matrix Wo \u2208 Rd\u00d7k, its modification is limited through a low- rank factorization, expressed as Wo+\u0394W = Wo + BA. Here, B\u2208 Rd\u00d7r and A \u2208 Rr\u00d7k, where the rank r is significantly smaller than the smaller dimension between d and k. In the training process, Wo remains static and does not undergo gradient updates, while A and B are adjustable parameters. It's important to note that both Wo and \u0394W = BA interact with the identical input, and their output vectors are added together element-wise. The updated forward pass formula is then given by h = Wox, indicating:\n$h = W_0x + \\Delta Wx = W_0x + BAx$"}, {"title": "D. ControlNet", "content": "ControlNet is an innovative neural network architecture designed to inject additional conditional controls into dif- fusion models. ControlNet will inject additional conditions into the neural network, which can give control to a large pre-trained diffusion model. The ControlNet is applied to each of the encoder levels of the U-Net. A trainable copy of the encoding blocks and the middle block of the diffusion model is created. The ControlNet is computationally efficient since the locked copy parameters are frozen, no gradient computation is required in the originally locked encoder for the fine-tuning. ControlNet is computationally efficient because its locked copy parameters are frozen, eliminating the need for gradient computation in the originally locked encoder during fine-tuning."}, {"title": "E. Hotshot-XL", "content": "Hotshot-XL [19] is a text-to-GIF generation model that can work together with SDXL. We are able to feed any fine-tuned SDXL model into Hotshot-XL. Loading the fine-tuned SDXL-based LoRAs is easier than fine-tuning Hotshot-XL. The architecture of Hotshot-XL is illustrated in Figure 2. Hotshot-XL consists of multiple SDXL modules and a Hotshot-XL temporal transformer module, which are con- nected sequentially to take a prompt as input and output the GIFs. The temporal transformer architecture is detailed on the right side of Figure 2, consisting of normalization, self- attention, and feed-forward neural networks. The Hoishot- XL can generate high-resolution GIFs at 8 frames per second and supports various image sizes."}, {"title": "III. PROPOSED PIPLINE", "content": "We aim to address the challenge of generating realistic driving videos straightforwardly using a generative model. Consequently, we propose a pipeline that leverages the capabilities of SDXL to generate a driving video dataset.\nFigure. 3 illustrates how GenDDS integrates the SDXL, LORA, ControlNet auto-tagger process, and Hotshot-XL. We extract raw images from open datasets and use Tagger to generate tags for each frame. These prepared datasets are then used to train the LoRA layers in SDXL. Once the fine-tuned SDXL model is ready, we feed it into Hotshot-XL, which acts as a module in the sequence. Concurrently, we use ControlNet to enhance the spatial relationships generated by Hotshot-XL. After completing the entire pipeline, we can run inference with Hotshot-XL to generate driving videos. We will illustrate the details of each module in the experiment section."}, {"title": "IV. EXPERIMENTS", "content": "The main dataset used in our experiments is the KITTI dataset [20], a set of benchmarks for computer vision algorithms mainly used in autonomous driving scenarios. The KITTI dataset is created by the Karlsruhe Institute of Technology and the Toyota Technological Institute in Chicago. Ranging from stereo images to optical flow, from visual odometry to 3D object detection and tracking. In our project, we utilize the high-resolution images and sequences from this dataset to train our model to generate realistic driving scenarios under various environmental conditions."}, {"title": "B. Experiment Setup", "content": "Our experiments utilize the functionality of the SDXL base model [16], which is known for its ability to generate high-resolution images through deep learning techniques. We enhanced this model by integrating Low-Rank Adaptation (LoRA), a modification designed for efficient parameter up- dating. This adaptive technique facilitates specialized train- ing of the large-scale model for specific tasks, thus greatly reducing the need for full retraining.\nA component of the data preparation was the use of the WD 1.4 MOAT Tagger V2 [21] tagging mechanism. This tagging system works by labeling each frame with the data that the image contains, such as \"outdoors, road, scenery, sky...\". The automated tagging is followed by a careful manual review to ensure the accuracy of the tag- ging and the distinction between different images, including traffic conditions and other keywords such as \"light traffic\" or \"heavy traffic\". This labeling strategy improves model training efficiency by providing highly accurate annotations. These tags help train the model to recognize and generate nuances in driving scenarios based on different environments and traffic conditions.\nThe computational backbone of our training infrastructure is equipped with NVIDIA 3090Ti GPUs. Our training pro- cess employs a selected set of hyperparameters to optimize the balance between model accuracy and computational resource utilization. Model optimization was performed with the AdamW8bit optimizer, employing a fine-grained learning rate strategy starting at 0.0001. This approach is comple- mented by applying different learning rates to the U-Net and text encoder portions of the model, thus enabling customized learning trajectories within the model architecture.\nOne aspect of our model configuration is the selection of training parameters to improve efficiency and effectiveness. We set the model to run 32 iteration steps per image, a decision aimed at achieving a balance between image quality and computational demands. We used the DPM++ 2M Karras sampler and set the CFG ratio to 7.5, which improves the model's ability to generate images that are consistent with the textual descriptions and input conditions, thus maintaining the thematic integrity of the generated scenes."}, {"title": "C. Experiment Process", "content": "In our experimental framework, we employ real-world driving footage as ControlNet [22]. This innovative approach utilizes the realism of actual road conditions captured on video to guide the image synthesis process. By integrating SDXL models enhanced with LoRA technology into the Hotshot XL environment [19], we set the stage for generating video content. This approach allowed us to generate con- tinuous frame sequences that effectively simulate dynamic driving scenarios.\nIn ControlNet, depth information is estimated from real traveling video inputs by depth detection methods. This depth information plays a role throughout the image generation process, guiding the model to maintain accurate spatial relationships and perspectives in the generated image. The integration of depth estimation ensures that the synthetic enhancements applied during the generation process are not only visually realistic, but also contextually appropriate and reflective of real driving scenarios.\nThe core of this process revolved around using the real driving videos as a baseline from which the model could generate enhanced or varied scenarios under controlled con- ditions. The Hotshot XL facilitated this by providing the architecture to process the SDXL-LORA model efficiently. Through this setup, we could manipulate the original video"}, {"title": "D. Results", "content": "frames, applying synthetic alterations that represent different weather conditions, traffic densities, and other environmental variables, all while maintaining the original video's structural integrity.\nThe experiment results for GenDDS are visualized in a series of graphs that illustrate the model's ability to synthe- size highly realistic, contextually coherent video sequences under a variety of conditions. From contextual coherence to environmental adaptability, each diagram emphasizes a different aspect of the model's capabilities.\nFigure. 4 provides a snapshot of our model's capability to maintain contextual coherence across sequential frames. It showcases a series of four continuous frames where the model has successfully captured the dynamic nature of the driving scenario. Notably, the forward progression of vehicles in the scene is realistically rendered, with a particular focus on a vehicle that gradually moves forward across the frames. This illustrates the model's adeptness at understanding and applying the temporal continuity essential for video generation, ensuring that each frame is a logical progression from the last.\nIn Figure. 5, our model is able to generate video sequences under different weather conditions, including sunny, cloudy, foggy, and rainy days, thus highlighting the versatility of the model. This variety of outputs demonstrates the model's broad understanding of environmental factors and its ability to modify the scene accordingly. From changing lighting conditions to introducing elements such as rain or fog, each weather condition presents a set of challenges that our model handles with ease, giving us a glimpse of its great adaptability.\nBuilding upon the rain scenario, Figure. 6 explores the model's ability to generate videos with different traffic den- sities. This feature is particularly noteworthy as it demon- strates the model's capacity for fine-grained control over the elements within the scene. By adjusting the number of vehicles, our model can simulate a range of traffic conditions from light to heavy, offering insights into its potential for creating varied driving scenarios that are crucial for training autonomous driving systems.\nWhile the initial series of images focused on demon- strating variability in consistent streetscapes for comparison purposes, Figure. 7 expands the scope to demonstrate the model's ability to generate different streetscapes. This illus- trates the scalability and adaptability of our approach, con-"}, {"title": "V. CONCLUSIONS", "content": "This paper introduces GenDDS, a novel pipeline that leverages a stable diffusion-based model to generate driving videos under diverse conditions. Key features of GenDDS include the use of an auto-tagger mechanism for dataset preparation. Additionally, we employ LoRa to fine-tune the model and integrate ControlNet to inject additional conditions, enhancing the relevance and coherence of the generated videos.\nThe evaluation results conducted on the KITTI dataset pro- vide compelling indications that our proposed methodology is capable of generating high-resolution and realistic driving videos under different conditions or condition combinations. This work delves into the frontiers of the autonomous driving domain, focusing specifically on methods for driving video generation. With GenDDS, we can easily generate driving videos based on different conditions and settings.\nTo further advance the research, the generation length and complexity could be increased. Moreover, the conditions for generation could extend beyond textual descriptions of the video to include factors related to driver behavior and vehicle dynamics. Generative models can further expand the capabilities of autonomous driving technologies."}]}