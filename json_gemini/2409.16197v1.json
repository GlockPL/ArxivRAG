{"title": "Second Order Bounds for Contextual Bandits with Function\nApproximation", "authors": ["Aldo Pacchiano"], "abstract": "Many works have developed algorithms no-regret algorithms for contextual bandits with function\napproximation, where the mean rewards over context-action pairs belongs to a function class F. Although\nthere are many approaches to this problem, one that has gained in importance is the use of algorithms\nbased on the optimism principle such as optimistic least squares. It can be shown the regret of this\nalgorithm scales as \\tilde{O} (\\sqrt{deluder (F) log(F)T}) where deluder (F) is a statistical measure of the complexity\nof the function class F known as eluder dimension. Unfortunately, even if the variance of the measurement\nnoise of the rewards at time t equals \\sigma_t^2 and these are close to zero, the optimistic least squares algorithm's\nregret scales with \\sqrt{T}. In this work we are the first to develop algorithms that satisfy regret bounds\ndeluder(F).log(|F|)}) for contextual bandits with function\nof the form \\tilde{O} (deluder(F)\\sqrt{log(F) \\sum_{t=1}^T \\sigma_t^2 + deluder(F) \\cdot log(|F|)}\napproximation and unknown variances. These bounds generalize existing techniques for deriving second\norder bounds in contextual linear problems.", "sections": [{"title": "Introduction", "content": "Modern decision-making algorithms have achieved impressive success in many important problem domains,\nincluding robotics [21, 28], games [29, 35], dialogue systems [26], and online personalization [3, 37]. Problems\nin these domains are characterized by the interactive nature of the data collection process. For example,\nto train a robotic agent to perform a desired behavior in an unseen environment, the agent is required\nto interact with the environment in a way that empowers it to learn about the world, while at the same\ntime learning how to best achieve its objectives. Many models of sequential interaction have been proposed\nin the literature to capture scenarios such as this. Perhaps the most basic one is the multi-armed bandit\nmodel [38, 22, 5, 24], where it is assumed a learner has access to K\\in N arms (actions), such that when\nplaying any of these results in a random reward. Typically, the learner's objective is to select actions, and\nobserve rewards in order to learn which arm produces the highest mean reward value. Algorithms for the\nmulti-armed bandit model can be used to solve problems such as selecting a treatment that in expectation\nover the population achieves the best expected success.\nDeploying an algorithm designed for the multi-armed bandit setting may be suboptimal for applications\nwhere personalized policies are desirable, for example, when we would like to design a treatment regime that\nmaximizes the expected success rate conditioned on an individual's characteristics. This situation arises in\nmany different scenarios, from medical trials [39, 8], to education [13], and recommendation systems [27] . In\nmany of these decision-making scenarios, it is often advantageous to consider contextual information when\nmaking decisions. This recognition has sparked a growing interest in studying adaptive learning algorithms\nin the setting of contextual bandits [23, 27, 4] and reinforcement learning (RL) [36].\nIn the contextual bandit model, a learner interacts with the world in a sequential manner. At the start\nof round t\\in N the learner receives a context x_t \\in X, for example in the form of user or patient features.\nThe learner then selects an action to play a_t \\in A, representing for example a medical treatment, and then"}, {"title": "Problem Definition", "content": "In this section we consider the scenario of contextual bandits, where at time t the learner receives a context\nx_t \\in X belonging to a context set X, decides to take an action a_t \\in A and observes a reward of the form\nr_t such that E_t[r_t] = f_*(x_t, a_t) where it is assumed that f_* \\in F for F a known class of functions with\ndomain X \\times A (see Assumption 2.1). Throughout this section we use the notation r_t = f_*(x_t, a_t) + \\varepsilon_t so\nthat the conditional expectation of \\varepsilon_t satisfies E_t [\\varepsilon_t] = 0. Throughout this work we will use the notation\n\\sigma_t^2 = Var_t(\\varepsilon_t) to denote the time t conditional variance of the noise. We'll assume the random variables r_t\nare bounded by a known parameter B > 0 with probability one.\nThe objetive of this work is to design algorithms with sublinear regret. Regret is a measure of performance\ndefined in the realizable contextual scenario studied in this work as the cumulative difference between the\nbest expected reward the learner may have achieved at each of the contexts it interacted with and the\nexpected reward of the actions played.\nRegret(T) = \\sum_{t=1}^T \\max_{a \\in A} f_*(x_t, a) - f_*(x_t, a_t)\nThe objective is to design algorithms with regret scaling sublinearly with the time horizon T.\nAssumption 2.1 (Realizability). There exists a (known) function class F : X \\times A \\rightarrow \\mathbb{R} such that E_t[r_t] =\nf_*(x_t, a_t) for all t \\in \\mathbb{N}.\nAssumption 2.2 (Boundedness). There exists a (known) constant B > 0 such that |r_t|, |\\varepsilon_t| \\leq B and\n\\max_{x \\in X, a \\in A} |f(x,a)| \\leq B and \\max_{x \\in X, a \\in A}|f(x,a) - f'(x, a)| \\leq B for all f, f' \\in F and all l\\in \\mathbb{N}.\nThe sample complexity analysis of our algorithms will rely on a combinatorial notion of statistical com-\nplexity of a scalar function class known as Eluder Dimension [34]. We reproduce the necessary definitions\nhere for completeness.\nDefinition 2.1. (\\epsilon-dependence) Let G be a scalar function class with domain Z and \\epsilon > 0. An ele-\nment z \\in Z is \\epsilon-dependent on \\{z_1,\\dots, z_n\\} \\subseteq Z w.r.t. G if any pair of functions g,g' \\in G satisfy-\nFurthermore, z \\in Z is \\epsilon-independent of\n\\{z_1,\\dots, z_n\\} w.r.t. G if it is not \\epsilon-dependent on \\{z_1,\\dots, z_n\\}.\nDefinition 2.2. (\\epsilon-eluder) The \\epsilon-non monotone eluder dimension deluder(G,\\epsilon) of G is the length of the\nlongest sequence of elements in Z such that every element is \\epsilon-independent of its predecessors. Moreover,\nwe define the \\epsilon-eluder dimension deluder(G,\\epsilon) as deluder(G, \\epsilon) = \\max_{\\epsilon' > \\epsilon} deluder(G, \\epsilon').\nIn order to introduce our methods we require some notation. The uncertainty radius function is a mapping\n\\omega : X \\times X \\times \\mathcal{P}(F) \\rightarrow \\mathbb{R} is defined as,\n\\omega(x, a, G) = \\max_{f,f'\\in G} |f(x, a) - f'(x, a)|\nfor x \\in X, a \\in A, G \\subseteq F. The quantity \\omega(x, a, G) equals the maximum fluctuations in value for the function\nclass G when evaluated in context x \\in X and action a \\in A. Throughout this work we will use the notation\n\\Sigma(\\mathcal{A}, \\mathcal{B},\\dots, \\mathcal{C}) to denote the sigma algebra generated by the random variables \\mathcal{A}, \\mathcal{B},\\dots,\\mathcal{C}.\nIn this work we design the first algorithm for contextual bandits with function approximation that\nsatisfies a variance dependent regret bound. In this work we extend the optimistic least squares algorithm\nfor contextual bandits with function approximation [34]. Our main result (simplified) states that,"}, {"title": "Optimistic Least Squares", "content": "The algorithms we propose in this work are based on the optimism principle. This simple yet powerful\nalgorithmic idea is the basis of a celebrated algorithm for contextual bandits with function approximation\nknown as Optimistic Least Squares. Algorithm 1 presents the pseudo-code of the Optimistic Least Squares\nalgorithm. In what follows,\nTo derive a bound for the optimistic least squares algorithm, we require guarantees for the confidence\nsets. This is captured by the following Lemma.\nLet \\delta\\in (0,1), \\{x_t, a_t\\}_{t=1}^T be a sequence of context-action pairs and and \\{r_t\\}_{t=1}^T\nbe a sequence of reward values satisfying r_t = f_*(x_t, a_t) + \\varepsilon_t where f_* \\in F and \\varepsilon_t is conditionally zero mean.\nLet \\hat{f}_t = \\arg \\min_{f \\in F} \\sum_{l=1}^{t-1} (f(x_l, a_l) - r_l)^2 be the least squares fit. If Assumption 2.2 holds then there is a\nconstant C > 0 such that,\n\\sum_{l=1}^{t-1} (\\hat{f}_t(x_l, a_l) - f_*(x_l, a_l))^2 \\leq \\beta_t(\\delta)\nwith probability at least 1 - \\delta for all t \\in \\mathbb{N} where \\beta_t(\\delta) = 4CB^2 \\log(t\\cdot |F|/\\delta).\nThe proof of Lemma 3.1 can be found in Section B. This result allows provides us with the tools to justify\nthe choice of confidence sets in Algorithm 1. A simple corollary is,\nLet \\delta\\in (0,1) and \\beta_t(\\delta) = 4CB^2 \\log(t\\cdot|F|/\\delta) as defined in Lemma 3.1. The confidence sets\nG_t satisfy f_* \\in G_t for all t \\in \\mathbb{N} simultaneously with probability at least 1 - \\delta. Moreover, this property holds,\nU_t(x, a_t) \\geq \\max_{a\\in A} f_*(x_t, a) for all te N."}, {"title": "Second Order Optimistic Least Squares with Known Variance", "content": "In this section we introduce an algorithm that satisfies second order regret bounds. Algorithm 2 takes as\ninput a variance upper bound \\sigma^2 such that \\sigma_t^2 < \\sigma^2, and achieves a regret bound of order\nRegret(T) < 0 (\\sigma \\sqrt{ deluder(F, B/T)Tlog(T|F|/\\delta)} + deluder \\cdot log(T|F|/\\delta))\nThis algorithm is based on an uncertainty filtered least squares procedure that satisfies sharper bounds than\nthe unfiltered ordinary least squares guarantees from Lemma 3.1. Since \\sigma^2 < B this bound could be much\nsmaller than the regret bound for Algorithm 1 described in Theorem 3.4. In this section we work under the\nfollowing assumption,\nAssumption 4.1 (Known Variance Upper Bound). There exists a (known) constant \\sigma > 0 such that \\sigma_t \\leq \\sigma^2\nfor all t \\in \\mathbb{N}.\nGiven a data stream \\{(x_l, a_l,r_l)\\}_{l\\in N} where r_l = f_*(x_l,a_l) + \\varepsilon_l for f_* \\in F such that \\mathcal{G}_l is conditionally\nzero mean, a sequence of subsets of G_t \\subseteq \\dots G_2 = G_1 = F such that G_t is a function of \\{(x_l, a_l,r_l)\\}_{l=1}^t),\nand f_* \\in G_t for all t \\in \\mathbb{N}. Given \\tau > 0 we define an uncertainty filtered least squares objective that\ntakes a filtering parameter \\tau > 0 and defines a least squares regression function computed only over datapoints\nwhose uncertainty radius is smaller than \\tau,\n\\hat{f}_t = \\arg \\min_{f \\in G_{t-1}} \\sum_{l=1}^{t-1} (f(x_l, a_l) - r_l)^2 \\mathbb{I}(\\omega(x_l, a_l, G_l) < \\tau)\\n(2)\nThe uncertainty filtering procedure will allow us to prove a least squares guarantee with dependence on\nthe variance and also on a vanishing low order term that scales with \\frac{T}{B}. We'll use the notation\n\\beta_t(\\tau, \\delta, \\tilde{\\sigma}^2) = (4\\min(\\tau+B, B^2) + 16\\tilde{\\sigma}^2) \\log(t|F|/\\delta)\nto denote the confidence radius function, in this case a function of \\tau,\\delta and  Algorithm 2 shows the\npseudo-code for our Second Order Optimistic Algorithm.\nNotice that by definition in Algorithm 2 the confidence sets satisfy G_l \\subseteq G_{l'} for all l > l'. In order to\nstate our results we'll define a sequence of events \\{E_l\\}_{l=1}^{t-1} such that E_l corresponds to the event that f_* \\in G_{l-1}\nand therefore f_* \\in G_{l'} for all l' < l - 1. The following proposition characterizes the error of the filtered least\nsquares estimator \\hat{f}_t when E_t holds.\nLet t \\in \\mathbb{N},\\tau > 0 and \\tilde{\\sigma} > 0. If \\sigma_l^2 < \\tilde{\\sigma}^2 for all\nl\\leq t-1 and E_t holds then\n(f\\hat{f}l \\mathbb{I}(\\omega(x_l, a_l, G_l) < \\tau) \\leq \\beta_t(\\tau, \\delta, \\tilde{\\sigma}^2), \\forall \\mathcal{G}_{l})\n\\mathbb{P}(E_t) - \\delta.\n(4)\nThe proof of Proposition 4.1 can be found in Appendix B. It follows the structure of the least squares\nresult from Proposition 4.1. For a given \\tau > 0, estimator \\hat{f}_t achieves a sharper bound than the ordinary\nleast squares estimator because the low order term in the portion of the analysis that requires the use of\nFreedman's inequality (see Lemma A.1) that has a magnitude scaling with the error of \\hat{f}_t on historial points"}, {"title": "Contextual Bandits with Unknown Variance", "content": "In the case where the variance is not known our contextual bandit algorithms work by estimating the\ncumulative variance up to constant multiplicative accuracy and use this estimator to build confidence sets\nas in Algorithms 1 and 2. In section 5.1 we describe how to successfully estimate the cumulative variance in\ncontextual bandit problems and finally in section 5.3 we introduce Algorithm 3 that satisfy a regret guarantee\nwhose dominating term scales with \\sqrt{\\sum_{t=1}^T\\sigma_t^2}, and the low order term with deluder \\cdot log(|F|).\nIn this section we discuss methods for estimating the variance in contextual bandit problems. Our estimator\nis the cumulative least squares error of a sequence of (biased) estimators. The motivation behind this choice\nis the following observation,\nVar(X) = \\min_{u} E [(X - u)^2]\nwhere the minimizer \\arg \\min_{u} E [(X - u)^2] = E[X]. Inspired by this we propose the following estimation\nprocedure. Given context-action pairs and reward information \\{(x_l, a_l,r_l)\\}_{l=1}^{t-1} and a filtering process b_t =\n\\{b_l\\}_{l=1}^t of bernoulli random variables b_l \\in \\{0, 1\\} such that b_l is \\Sigma(x_1, a_1, b_1, r_1, \\dots, x_{l-1}, a_{l-1}, b_{l-1}, r_{l-1}, x_l, a_l)-\nmeasurable. Let \\hat{f}_{b_t} be the \"filtered\" least squares estimator:\n\\hat{f}_{b_t} = \\arg \\min_{f \\in F} \\sum_{l=1}^{t-1} b_l\\cdot (f(x_l, a_l) - r_l)^2\nA filtered least squares estimator satisfies a least squares bound similar to Lemma 3.1,"}, {"title": "Conclusion", "content": "In this work we have introduced second order bounds for contextual bandits with function approximation.\nThese bounds improve on existing results such as [41] because they only require a realizability assumption\non the mean reward values of each context-action pair. We introduce two types of algorithm, one that\nachieves what we believe is sharp dependence on the complexity of the underlying reward class measured by\nthe eluder dimension when all the measurement noise variances are the same and unknown, and a second\none that in the case of changing noise variances achieves a bound that scales with the square root of the\nsum of these variances but scales linearly in the eluder dimension. In a future version of this writeup we\nwill strive to update our results to achieve a sharper dependence on the eluder dimension scaling with its\nsquare root. We hope the techniques we have developed in this manuscript can be easily used to develop\nsecond order algorithms with function approximation in other related learning models such as reinforcement\nlearning. These techniques distill, simplify and present in a didactic manner many of the ideas developed\nfor the variance aware literature in linear contextual bandit problems in works such as [20, 45, 19, 44, 42]\nand transports them to the setting of function approximation. Although we did not cover this in our work,\nan interesting avenue of future research remains to understand when can we design second order bounds for\nalgorithms based on the inverse gap weighting technique that forms the basis of the SquareCB algorithm\nfrom [14]."}]}