{"title": "Predicting change in time production - A machine learning approach to time perception", "authors": ["Amrapali Pednekar", "Alvaro Garrido", "Yara Khaluf", "Pieter Simoens"], "abstract": "Time perception research has advanced significantly over the years. However, some areas remain largely unexplored. This study addresses two such under-explored areas in timing research: (1) A quantitative analysis of time perception at an individual level, and (2) Time perception in an ecological setting. In this context, we trained a machine learning model to predict the direction of change in an individual's time production. The model's training data was collected using an ecologically valid setup. We moved closer to an ecological setting by conducting an online experiment with 995 participants performing a time production task that used naturalistic videos (no audio) as stimuli. The model achieved an accuracy of 61%. This was 10 percentage points higher than the baseline models derived from cognitive theories of timing. The model performed equally well on new data from a second experiment, providing evidence of its generalization capabilities. The model's output analysis revealed that it also contained information about the magnitude of change in time production. The predictions were further analysed at both population and individual level. It was found that a participant's previous timing performance played a significant role in determining the direction of change in time production. By integrating attentional-gate theories from timing research with feature importance techniques from machine learning, we explained model predictions using cognitive theories of timing. The model and findings from this study have potential applications in systems involving human-computer interactions where understanding and predicting changes in user's time perception can enable better user experience and task performance.", "sections": [{"title": "I. INTRODUCTION", "content": "TIME perception, or the subjective experience of time in humans, has been extensively studied from both psycho- logical and neuroscientific standpoints [1]. This research has been able to offer scientific explanations to common phrases like \"time flies when we are having fun\" or \"a watched pot never boils\" [2]. Thus, research in time perception has shed light on the underlying mechanisms responsible for familiar phenomena, such as the accelerated sense of time during enjoyable activities and the slow passage of time during antic- ipation. In addition to this, timing research has helped unravel the different brain regions involved in temporal processing and the factors that can influence the subjective experience of time.\nThe cognitive and neural theories of time perception have advanced timing research into the realm of computational modelling. Early models of time perception such as the pacemaker-accumulator model [3], attentional-gate model [4] and beat frequency models [5] serve as foundational frame- works for explaining timing behaviour in humans. Building on these frameworks, researchers have integrated timing com- ponents into widely-used cognitive architectures like adaptive control of thought-rational (ACT-R) [6]\u2013[9]. These studies have facilitated a better understanding of the interplay between timing and other cognitive processes in the brain. Additionally, well-documented timing biases like regression to the mean and time-order errors, have been explained through a Bayesian perspective [10]\u2013[12]. These studies have suggested that time perception can be thought of as a probabilistic inference process in the brain, similar to other sensory perceptions. Such advances in modelling time perception have enabled the integration of timing research into application-oriented fields like robotics and human-computer interactions [13]-[15].\nWhile research in time perception has advanced significantly over the years, several aspects remain largely unexplored. A quantitative analysis of timing studies at an individual or sub- group level, is one such under-explored area [16]. As noted by Matthews [16], most timing research studies aggregate the data across participants, reporting findings at a population level but ignoring individual differences in timing behaviour. Thus, well-established timing effects observed at the population level may not hold for specific individuals or sub-groups. This lim- itation extends to the computational models of timing. These models are often evaluated for their ability to qualitatively replicate human timing behaviour at a population level and usually lack output analysis at an individual or sub-group level. In addition to analysing individual level data, a quantitative approach can enable the use of predictive modelling techniques like machine learning on time perception data. Thus, it can open new avenues to not only analyse individual differences in timing behaviour but also predict time perception of an individual given a set of conditions.\nAnother under-examined area in timing research is, time perception in an ecological (or naturalistic) setting [16], [17]. A majority of timing studies rely on simple stimuli like flashes, tones and images, with a small number of participants performing synthetic tasks that do not resemble real-world scenarios. As argued by several authors [16], [17], findings and models derived from such experiments may not be applicable in real-world settings. Thus, there is a need to move towards more complex stimuli and have a larger pool of participants perform slightly more realistic tasks. Such an approach to studying time perception in naturalistic settings would in turn reinforce the need to analyse individual and sub-group differ- ences, as discussed above. This is because, naturalistic settings introduce greater variability in environmental and individual"}, {"title": null, "content": "characteristics, potentially amplifying individual deviations in timing behaviour from the population mean. Consequently, an ecological approach to time perception goes alongside accounting for quantitative differences at an individual or sub- group level [17].\nAddressing these gaps in timing research is important be- cause, in addition to understanding the timing mechanisms in humans, time perception research is gaining importance from an application standpoint in fields like robotics and human robot interactions [14]. For example, ChronoPilot [18] is a system that uses extended reality technologies to engage with a user performing a task. The aim of this system is to enable well-being and better task performance by modulating the subjective time experience of a user. Another example, as described in [19], involves robots assisting multiple users. The overall user experience of such a system is improved by the robot prioritizing users with a faster perceived time, thus optimizing the perceived wait times of its users. Such applications highlight the importance of understanding an individual's time perception in naturalistic settings. Therefore, a quantitative and ecologically valid approach to timing re- search can open new avenues for both theoretical research and practical applications.\nOne potential obstacle for an application-oriented approach is maintaining a balance between model performance and model explainability (especially, within the framework of existing theories of time perception). Models grounded in cognitive architectures provide explanations consistent with cognitive theories of timing [6]-[8]. However, these mod- els often show poor quantitative performance, especially in naturalistic settings, because their parameters are based on heuristics rather than real-world data distributions [20], [21]. Conversely, data-driven models from machine learning tend to have a superior performance because they are trained on real-world data. However, they often function as black-box systems that give only limited explanations for their predictions [22]. This suggests a need for hybrid approaches, either by incorpo- rating statistical estimations into cognitive model parameters, or by adding cognitive explanations to machine learning model predictions.\nThis study takes a small step towards an application- oriented approach to time perception. We trained a machine learning model to predict the direction of change in subjective time production of an individual in the upcoming trial of an experiment. Training data for the model was collected through an online experiment designed to study the prospective time production of participants watching a video (without audio). An online setup ensured fewer controlled variables compared to an offline experiment and allowed to collect data from a large number of participants. Additionally, the videos were simulated driving scenarios from a front-seat passenger's perspective. They were designed so as to incorporate multiple time-influencing stimuli like magnitude, salience and oddball (each of which have been mostly studied independently in laboratory settings [23], [24]). Hence, by recruiting a large number of participants (995), having fewer controlled vari- ables, using a first-person perspective [25] and combining multiple time-influencing stimuli in one dynamic stimuli like a video [26], [27], we move towards a more ecological setting. An ecological setting also ensures wider applications of the model. Consequently, the machine learning model was tested on unseen data from a second experiment where it performed equally well, providing evidence for its broader applicability. An analysis of the model's output revealed that it contained information about the magnitude of change in time production in addition to direction of change. Furthermore, the features used by the machine learning model were connected to the different components of the attentional-gate model [4], a cognitive framework of time perception. This enabled us to provide cognitive explanations to the predictions and features used by the model. Thus, this study makes the following contributions:\n1) An ecologically valid dataset of time perception: The training data for the machine learning model consists of time production data from a large number of partic- ipants (995). This data was collected using an online experiment which ensured fewer controlled variables. The videos used in the experiment were simulated driving scenarios from a first-person perspective that combined multiple time-influencing visual stimuli. They were designed to closely resemble the experience of a front-seat passenger observing scenes from a moving car.\n2) A machine learning model that predicts change in time production: We built a machine learning model that predicts the direction of change in subjective time production of a participant between consecutive trials. The model performance is quantitatively evaluated on a held-out test set and its generalization is demonstrated on unseen data from a second slightly different exper- iment. Furthermore, the model output is quantitatively evaluated for it's ability to carry information about magnitude of change in time production in addition to direction of change. The model predictions have also been analysed at both the population and individual level using SHAP values [28], a technique in explainable AI. Thus, the model and its analysis contribute towards a quantitative approach to time perception at an individual level.\n3) Explanation of model predictions using a cognitive framework of time perception: We connect features used by the machine learning model with different components of the attentional-gate model. Thus, by combining the attentional-gate theory of time perception with feature importance techniques of machine learning, we aim to provide a cognitive explanation to the ma- chine learning model predictions. Thus, we were able to formulate hypotheses about reasons for change in time production for a particular participant."}, {"title": "II. RELATED WORK", "content": "In this section we discuss some previous efforts at addressing the gaps mentioned in this study.\nTime perception at an individual level has been studied using physiological and neural activation data. In [29], the authors train a machine learning model that uses physiological biomarkers to predict whether time is passing fast or slow based on timing errors of a given individual. While in [30], the authors study the relation between physiological data and time perception in a more naturalistic environment. They train a ma- chine learning model to predict the perceived passage of time of air traffic controllers under various workload conditions using physiological data. Individual differences in timing have also been studied using functional magnetic resonance imaging (fMRI). In [31], the authors study fMRI's of participants performing a colour and temporal discrimination task. They conclude that different regions in the brain are involved in timing, and individual differences may arise due to different levels of activities in these regions. In [32], the authors study the relation between difference in beat perception and differ- ence in time perception among individuals. Using fMRI, they show that activities in auditory and motor areas in the brain is correlated with individual differences in beat perception. They further studied the relationship between the differences in activities due to varying beat perception and individual difference in time perception.\nAttempts have been made to move to a more ecological setting by using complex and dynamic stimuli in a laboratory setting. In [25], the authors studied interval timing of drivers using videos generated by a driving simulator. The videos were from the drivers or co-drivers perspective with the car moving at different speeds. By using dynamic stimuli like a video with a first person perspective, the authors attempted to move closer to a real-world setting. The results confirmed existing qualitative findings from simpler settings. Namely, videos depicting faster speeds or faster moving cars are perceived to be longer compared to videos depicting slower speeds or slower moving cars. In [27], the authors used images and videos to study the impact of naturalistic stimuli on age related differences in time perception. They conclude that due to larger information processing required for naturalistic stimuli the timing performance of younger adults is found to be better than older adults. In [26], the authors used naturalistic video recordings that combined multiple simpler visual stimuli. They studied the effect of salience on time perception and qualitatively replicated these effects using activations from different layers of a convolutional neural network.\nApart from using dynamic stimuli in a laboratory, efforts have been made to take the experiment setup outside labo- ratories into the real-world where participants perform more natural timing tasks. In [33], the authors assessed the role of attention in time perception of players of an online multiplayer real-time strategy game. They found that distractions during the interval have lesser influence on time perception as com- pared to distractions just before the required response. They present qualitative results showing that, rather than an attention gate slowing down the cognitive counter, the variance in timing due to distraction of attention away from time is because the distraction prevents participants from checking their cognitive counter. In [34], the authors study longer timing interval (from 12 minutes to 58 minutes) of video game players in a gaming centre. In this study, the authors compare prospective and retrospective timing of players. The prospective estimates were found to be longer than the retrospective ones and thus the results confirm the classical distinction found between these paradigms in a naturalistic setting. In [35], the authors study the effect of cognitive load on time perception of air-traffic controller who were actively engaging in real flight control sessions. The results showed that higher cognitive load leads to overproduction, again confirming earlier insights of laboratory studies about the effect of cognitive load on time perception. These studies highlight the need for ecological validity in an experimental setup, to both confirm existing laboratory findings and uncover additional timing behaviours that may arise when moving from the laboratory to the real-world.\nComputational modelling in time perception can be broadly divided into two types, models based on cognitive architecture and models based on Bayesian framework. Most efforts with cognitive architectures are focused around integrating the pacemaker based internal clock into the ACT-R architecture. For example, in [6], the clock rate of the internal clock was decayed over time to ensure scalar property of time estimation [36]. While in [7], clock rate was based on attentional-gate theory and thus had a fixed mean rate. On the other hand, [8] added a timing module into ACT-R for retrospective timing instead of prospective timing. These models however have only been tested for their ability to qualitatively replicate tendencies and biases found in human timing data. They do not involve individual level analysis or predictions. Most efforts with Bayesian modelling are focused on explaining the popular timing biases like regression to the mean and time-order errors using a Bayesian framework [12]. Parameters of the Bayesian observer model are either based on heuristics or a combination of heuristics and human experiment data [11], [10]. These studies again focus on population level timing biases and do not discuss model predictions at in individual level. To the best of our knowledge, no studies have focused on predicting change in time perception of an individual.\nAttempts to draw parallels between computational models of timing and existing theories of time perception are very few. In [37], the authors discuss how the different components of a pacemaker accumulator model can be linked to the different components of the Bayesian framework. Specifically, they relate the reference memory of the pacemaker model to the prior in a Bayesian model, the clock stage to the likelihood, the working memory to the posterior and the comparator stage of the pacemaker model to the loss minimization stage in a Bayesian model. Such a comparison helps to draw parallels between a model trained on data and the existing theories of time perception. In this study, we aim to do something similar by aligning the features used by a machine learning model trained on ecologically valid timing data with the different components of the attentional-gate model of time perception [4]."}, {"title": "III. METHODS", "content": "In this section, we first define the methods used to move closer to an ecological setting by describing the naturalistic video stimuli and the main experiment used to collect training data for the machine learning model. Next, we outline the experiment data points used as features for the machine learning model and provide a description of the model itself. The attentional-gate model is then briefly described, with a focus on aligning the machine learning model features with different components of the attentional-gate model. This is followed by an overview of the baseline models used in the study and a description of the second experiment used to test the machine learning model's generalization capabilities."}, {"title": "A. Naturalistic video stimuli", "content": "The video stimuli used in the main experiment consists of different simulated driving scenes from a front-seat passen- ger's perspective. These videos are considered naturalistic for two reasons. First, they represent a first-person perspective [25], which can be considered a close resemblance to the experience of a front-passenger observing scenes from a moving car. Second, they simultaneously incorporate multiple time-influencing visual stimuli like magnitude, salience and oddball in a less controlled manner [26], [27].\n\"Visual engagement\u201d is used as an umbrella term encom- passing one or more time-influencing visual stimuli. It defines the degree to which a given video captures a viewer's attention. For simplicity, videos were grouped into three levels of visual engagement (Figure 1 shows screenshots of videos under each engagement level):\nLow Engagement level: A desert scene with sand and canyons along a straight, empty road with no other ac- tivity or objects in sight. These scenes have low salience and high predictability. Hence, they are assumed to have a low engagement level.\nMedium Engagement level: A city scene with buildings along the two sides of a straight road, other cars moving on the road and pedestrians walking on the sidewalk. These scenes have moderate salience and predictability. Hence, they are assumed to have a medium engagement level.\nHigh Engagement level: A city scene similar to the one described in medium engagement level, but with additional unusual elements. For example, a Godzilla and other animated characters from online games appear every 10 or 15 seconds along the road. These scenes have the highest salience, and induce unpredictability. Viewers may become curious about what happens next. Hence, they are assumed to have a high engagement level.\nThe videos were generated by a professional content creator using Unreal engine [38], giving us flexibility in adding or removing objects from different locations within each scene. There were a total of three videos under each type of engagement level. Links to the videos are included in the supplementary material."}, {"title": "B. Main experiment design", "content": "The main experiment was set up to collect training data for the machine learning model. The aim of this experiment was to capture the change in prospective time perception of a participant experiencing a pure visual input (no audio or haptic).\nThe experiment setup is shown in Figure 2. A video followed by a questionnaire, is defined as one trial. In a trial, participants were asked to watch the video attentively and stop it when they thought 30 seconds had passed. To ensure that participants did not count, they were instructed to pay attention to the video contents in order to complete a quiz after the video. The questionnaire following each video, contained questions about the video content (attention checks), a self-evaluation of timing performance, and questions related to visual perception and passage of time. Thus, the two trials involved a prospective time production task and a non- temporal task, both of equal importance. As the target interval (30 seconds) was the same in the two trials, a change in time production in the second trial as compared to the first, was assumed to be related to change in prospective time perception.\nThe video series, or the two videos shown to each partici- pant could belong to either the same or different engagement levels. Thus, there were a total of nine permutations of the three engagement levels for the two trials. Out of these, one was selected randomly for each participant. It is important to note that each engagement level comprised of three videos. Hence, we made sure even if a participant watched the same engagement level consecutively, she would be watching a different video each time.\nThis experiment was conducted on Prolific [39], [40], an online experiment platform. The online setting enabled us to move closer to an ecological setting due to fewer controlled variables and a large number of participants. A total of 995 participants were recruited. Out of these, nine participants were excluded either because they failed the attention checks or reported technical problems during the experiment. There was no explicit criteria for gender, age or ethnicity of the participants. However, they were expected to have a good knowledge of English. The participants were paid at a rate of 8 pounds per hour (the median duration of the experiment"}, {"title": "C. Feature selection", "content": "Features for the machine learning model were extracted from different data points of the experiment, such as, timing per- formance and questionnaire responses. Using feature elimi- nation techniques like permutation feature importance (see supplementary information for details), the five most predictive features were selected. These features have been assigned to broader descriptive categories to enhance readability and facilitate generalization to other settings. Table I shows the feature categories and values for the selected features.\nPrior timing performance: The feature TIRelError, captures information about a participant's relative time production error. It is defined (similar to the relative time estimation error described in [29]), using the time produced in trial one, T1ProducedTime, and the target interval, TargetInterval (30 seconds) as follows:\n$$TIRelError = \\frac{T1ProducedTime - TargetInterval}{TargetInterval} * 100$$    \nSelf-evaluation of timing performance: In the question- naire following each video, participants were asked to categorize their produced time as either higher or lower than 30 seconds. The binary feature, TILowerThan30, represents a participant's corresponding response from the trial one questionnaire.\nParticipant sensitivity: The feature, HighVisualSensitiv- ity, captures information about extreme discrepancies in reported subjective engagement level for low engagement videos. A participant is marked as sensitive, if she reports a low engagement video to be highly engaging. It is important to note that only participants who watched a low engagement video in trial one were assessed for sensitivity. All others, who watched either a medium or high engagement video in trial one, were assumed to not be sensitive by default since we could not capture an extreme discrepancy between perceived and objective engagement level for such participants. A corresponding feature that captured extreme discrepancies for high en- gagement videos was not included in the most predictive features. (See feature selection section in supplementary information for more information).\nEnvironmental characteristics: The feature V2EngagementLevel, represents the objective engagement level of the video presented in trial two. And, the feature ChangeInEngagementLevel, represents the change in engagement level between the video shown in trial two and the video shown in trial one. Both features are fully under control of the experimenter and are independent of the participant's subjective sensitivity or timing characteristics."}, {"title": "D. Prediction model", "content": "The prediction model is a machine learning model that uses the selected features to predict direction of change in a partic- ipant's production time. Specifically, given a set of feature values generated by a participant in the first trial and the engagement level of the upcoming trial's video, the model predicted the probability of decrease in time production in the second trial as compared to the first. Thus, the model output (probability of decrease) carried information about the direction of change in time production. When the probability was more than 0.5, the direction of change was classified as decrease in time production. When it was less than 0.5, the direction of change was classified as an increase in time production. Furthermore, the model output was also found to contain some information about magnitude of change in production time. The Results section contains a detailed evaluation of the model for it's ability to convey information about both the direction and magnitude of change in time production.\nAnalysis of the experiment data revealed that 36% of par- ticipants decrease time production in second trial as compared to the first. In order to handle this class imbalance, data was sampled using random undersampling to have equal number of increase and decrease cases. Due to this, the final training dataset had 706 participants.\nA number of classical machine learning algorithms covering different model families like tree based, probabilistic and neural networks, were tested for the prediction problem. All models had similar performances on a held-out test set. The logistic regression model was selected for its simplicity and implicit probability calibration. The supplementary informa- tion section contains further information about performance of each model and the model selection process. All algorithms were implemented using the scikit-learn library in Python. The following equation represents the logistic regression function used to calculate the probability of decrease,\n$$Pr(Y = Decrease | X) = \\frac{1}{1+ exp(-X)}$$\nWhere X for the trained model was,\n$$X = 0.016 +0.662 * (T1RelError)\n- 0.191 * (TILowerThan30)\n- 0.241 * (HighVisualSensitivity)\n- 0.187 * (V2EngagementLevel)\n+0.177 * (ChangeInEngagementLevel)$$\nThus, each participant was represented as a set of five features and the model predicted the probability of decrease in time production. Since all feature values were scaled using scikit-learn's Standard scalar function, the coefficients in Equation (3) represents feature importance. A detailed analysis about the effect of different features on the predicted probability is conducted in the Results section."}, {"title": "E. Attentional-gate model", "content": "The attentional-gate model [4] is a popular cognitive frame- work that explains time perception as an interplay between different components. As seen in Figure 4(a), it consists of a pacemaker that emits ticks at a given rate, referred to as clock speed. Clock speed can change due to different levels of arousal [41] caused by a stimuli or the environment. The ticks from the pacemaker pass through an attention-gate. When attention is focused on time, this gate is open wide and lets all the ticks pass through it. Conversely, when attention is diverted away from time, the gate is narrow and lets fewer ticks pass through it. The ticks that pass through the attention- gate are then accumulated in a cognitive counter. The refer- ence memory contains information about the number of ticks needed in the cognitive counter to reach a target duration. The decision phase involves a comparator that compares the ticks in the cognitive counter with that in the reference memory, at different points in time. If the accumulated ticks are less than the reference memory ticks, then no action is taken. Otherwise, an action associated with marking the end of an interval may be taken. The clock speed, attention-gate width and reference memory depend on a number of factors, they differ between participants and can also change for a given participant in different situations.\nIn this study, the attentional-gate model is used to formulate hypotheses about reasons for change in time perception. To simplify the problem, it is assumed that change in production time is a result of either a change in the cognitive counter or the reference memory or both (the two components imme- diately preceding the decision phase). Refer to Figure 4 (b) for a graphical representation. Using this simplification, we formulate a hypothesis about which features can indicate a change in a particular component of the attentional-gate model. Reference memory is affected by contextual calibrations like regression to the mean and prior timing experiences [37]. Hence, the time performance features, namely, prior-timing performance and self-evaluation of timing performance are assumed to indicate a change in reference memory. On the other hand, cognitive counter is affected by changes in atten- tion to time or due to different levels of arousal caused by the stimuli or environment. Thus, the features under environ- mental characteristics and participant sensitivity are assumed to indicate a change in the cognitive counter. For simplicity, we ignore any other factors that can cause a change within the two components of the attentional-gate model.\nThus, the above assumptions enable aligning the selected features of the machine learning model with the different components of the attentional-gate model. Consequently, by combining these assumptions with feature importance analysis"}, {"title": null, "content": "of the machine learning model, we can formulate a hypothesis about reasons for change in time production and provide a cognitive explanation for the machine learning model's predictions. Specifically, when prior timing performance or self-evaluation of timing performance are found to have a significant contribution to the prediction, the change in time production is hypothesized to be caused by a change in reference memory of the participant. On the other hand, when environmental characteristics or participant sensitivity have a significant contribution to the prediction, the change in produced time is hypothesized to be caused by a change in the cognitive counter. It is important to note that the reasons for reference memory change, namely contextual changes, may not reflect an actual change in perceived time. Instead, it could be related to participants implicitly (or explicitly) adjusting their timing performance. Conversely, changes in cognitive counter can reflect an actual change in time perception because it involves changes in clock speed or changes in attention to time. In the Results section, we combine the discussions and assumptions made in this section with individual and population level feature importance results obtained using SHAP values, to provide a cognitive explanation for the machine learning model's predictions."}, {"title": "F. Baseline models", "content": "The prediction model's performance is compared against two baseline models. These models are derived from hypotheses formulated using the attentional-gate model [4].\nAttention based model: This model is based on the as- sumption that, a high engagement video distracts attention away from time towards the visual content. As a result, the cognitive counter would collect fewer ticks in a high engagement video trial as compared to a low engagement video trial. Hence, for the same objective time, the com- parator would wait longer in case of a high engagement video as compared to a low engagement video, to match the reference memory ticks. Thus, according to this model there would be an increase in production for a transition from lower to higher engagement level and conversely a decrease in time production for a transition from higher to lower engagement level.\nArousal based model: This model assumes that a high engagement video increases clock speed due to high arousal [3] as compared to a low engagement video. As a result, the cognitive counter would collect more ticks in a high engagement video trial as compared to a low engagement video trial. Here, we will see an opposite effect as compared to the attention based model described above. Thus, according to this model there would be a decrease in production for a transition from lower to higher engagement level and conversely a increase in time production for a transition from higher to lower engagement level."}, {"title": "G. Second experiment to test generalization", "content": "In order to check whether the model generalizes well to other scenarios, it was tested on data collected from an experiment with a slightly different setting. The details of this experiment are described in [42]. This was an offline experiment where participants were immersed in a driving simulator environment. The task was to drive through different scenes while maintaining a constant speed of 70 km/h and press a button when they thought 30 seconds had passed. After each trial, the participants were asked questions about the task difficulty, boredom and time perception, similar to the main experiment above. However, participants had to perform six trials in contrast to only two trials in the main experiment. Thus, while the second experiment was similar to the main experiment in terms of producing 30 seconds, it differed in terms of the non-timing task of maintaining a constant speed of 70 km/h (as opposed to watching the video with attention in the main experiment) and number of trials. Additionally, this was an offline setting while the main experiment was performed in an online setting.\nIn order to adjust for six trials instead of two, we predict the time in the next trial given the information from the immediately previous trial. This additionally helped ensure that the model predictions are not limited to the first two trials but can generalize to any consecutive trials throughout the experiment.\nOne more implementation difference between the two ex- periments was that in the second experiment, the different driving scenes did not have explicit labels of high, medium, or low engagement. Nevertheless, each scene projected a different level of difficulty in doing the non-timing task of maintaining a constant speed. This involved adding fog to the environment, a tail-gating car that could be seen from the rear view mirror, and a car in front that would break unexpectedly (stop-and- go). This difference was resolved by labelling a scene based on the performance in the non-timing task. The assumption was that if the non-timing task performance in a scene was poor on average, it was more engaging and diverted attention away from time. The stop-and-go scene had the worst performance and hence was labelled as a high engagement scene. In contrast, the experiment started with a scene called \u201cdaylight\u201d, which did not involve stimuli or distractions, serving as a familiarization task for the participants. This was classified as a low engagement scene. All other cases, which fell between these two extremes in terms of performance, were classified as medium engagement scenes. Other features were derived from the data in the same way as in the last experiment."}, {"title": "IV. RESULTS", "content": "Direction of change prediction"}, {"title": "A. Direction of change prediction", "content": "This section contains an evaluation of the logistic regression model for its ability to predict the direction of change in time production. When the predicted probability of decrease is more than 0.5, the prediction is classified as a decrease. Likewise, when the predicted probability is less than 0.5, the prediction is classified as an increase.\nThe model was tested on both the main experiment data and data collected from the second experiment. For the main experiment, the model was evaluated using Leave-One-Out cross-validation (LOOCV). In this approach, the model is trained on all except one participant and tested on the left out participant. This process is repeated until each participant is considered as a test data point. Thus, the test data of the main experiment had 706 data points in total. For the second experiment, the test set contained 25 participants (29 participants performed the experiment, out of which, 4 were excluded for incomplete questionnaire). With each participant performing six trials, there were 125 data points in the second test set (last trial was excluded for all participants since there was no next trial to predict for).\n\nThe logistic regression model had an accuracy of 61%. The precision of the model reveals that, out of all predictions suggesting a decrease in time production, 62% were correct. Correspondingly, the recall reveals that, out of all participants that actually decreased their time production in the next trial, the model was able to predict 59% of them correctly. Numerically the results may not seem very impressive. However, when compared with the rule-based baseline models that were derived from existing timing research, the model's accuracy is 10 percentage points higher. Furthermore, the model gen- eralized fairly well on the second experiment data with an accuracy of 66%. Since the two experiments had some crucial experimental differences (e.g. number of trials performed by a single participant), we should not compare the evaluation metrics for the two experiments. However, we can conclude that, on the second experiment data, the model performs better than a random model (which would have an accuracy of 50% on a balanced dataset like the one used for model training and evaluation)."}, {"title": "B. Magnitude of change", "content": "This section contains an evaluation of the logistic regres- sion model's output probabilities to determine the amount of information they carry about magnitude of change in time production. For simplicity, the magnitude of change is categorized into three levels: \u201chigh decrease\u201d, \u201chigh increase\u201d and \"small change\". We assume that for highly informative probabilities, a high probability of decrease should correspond to a high decrease in production time, a low probability of decrease should correspond to a high increase in production time, and a probability close to 0.5 which indicates uncertainty in model prediction should correspond to a small change.\nThus, the probabilities of the trained logistic regression model convey additional information which a rule-based model cannot achieve. This also means that we do not have any baselines (derived from timing research) to compare these results against. Nevertheless, by looking at Figure 5 and tables III and IV, the model probabilities can be considered reliable because of low numbers of extreme incorrect cases. The results are promising also because the model was not explicitly trained for the three magnitude levels. Instead, it was only trained on the binary task of predicting the direction of change. The resulting probabilities happen to carry some information about the magnitude of change. Thus, an approx- imate inference about how long a participant will wait before stopping the video can be made using the probabilities of the logistic regression model."}, {"title": "C. Cognitive interpretation of the prediction model", "content": "This section highlights the most important features and il- lustrates how the different feature values affect the model's predictions at a population level. We first explain the working of the prediction model at a population level using aggregated SHAP values [28"}]}