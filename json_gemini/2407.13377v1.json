{"title": "Linear-Complexity Self-Supervised Learning for Speech Processing", "authors": ["Shucong Zhang", "Titouan Parcollet", "Rogier van Dalen", "Sourav Bhattacharya"], "abstract": "Self-supervised learning (SSL) models usually require weeks of pre-training with dozens of high-end GPUs. These models typically have a multi-headed self-attention (MHSA) context encoder. However, MHSA takes quadratic time and space in the input length, contributing to the high pre-training cost. Linearcomplexity alternatives to MHSA have been proposed. For instance, in supervised training, the SummaryMixing model is the first to outperform MHSA across multiple speech processing tasks. However, these cheaper alternatives have not been explored for SSL yet. This paper studies a linear-complexity context encoder for SSL for the first time. With better or equivalent performance for the downstream tasks of the MP3S benchmark, SummaryMixing reduces the pre-training time and peak VRAM of wav2vec 2.0 model by $18 \\%$ and by $23 \\%$, respectively, leading to the pre-training of a 155 M wav2vec 2.0 model finished within one week with 4 Tesla A100 GPUs. Code ${ }^{1}$ is available.", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning (SSL) models have demonstrated state-of-the-art (SOTA) performance for speech processing tasks [1, 2, 3, 4]. SSL models are pre-trained on unlabeled data to learn hidden features of the input audio. Since the pre-training does not require human transcription, SSL can leverage a huge amount of unlabeled data. The large amount of pre-training data is one of the keys to the success of SOTA SSL models. SSL models also benefit from the large model size. The typical model size varies from millions to billions parameters. However, training large models with huge amount of data leads to extremely big training cost, resulting in an exceedingly high barrier for the research of SSL models, as well as extensive carbon footprints. For example, the pre-training of a 330M wav2vec 2.0 model [1] with 3 k hours data requires 32 Tesla V100 GPUs running for two weeks, consuming 1.818 MWh of energy, while the pre-training of a 965 M wav2vec 2.0 model with 14 k hours data requires 32 Tesla A100 GPUs running for two weeks, consuming 16.511 MWh of energy [5]. In this paper, we address the pre-training inefficiency of SSL models from the efficient architecture aspect. SOTA SSL models typically consistent of a feature extractor and a context encoder. The feature extractor extracts features from the raw wave input, and the context encoder generates further hidden representations. Efficient feature extractors have been proposed $[3,6,7,8]$. Nevertheless, to the best of our knowledge, the context encoder has not been studied from the efficiency angle yet. Thus, this paper improves the pre-training efficiency from the aspect of context encoder. The context encoder of the SOTA SSL models is usually a multi-headed self-attention (MHSA) Transformer [9] or Conformer [10] encoder. However, MHSA has a quadratic time and space complexity in the input sequence length, slowing down pre-training and increasing VRAM consumption. Methods of developing sub-quadratic complexity alternatives to MHSA include designing priors for the attention patterns [11, 12], lowrank approximation [13], kernelization [14], and linearization [15]. Unfortunately, compared to MHSA, these methods usually lead to inferior results for speech processing tasks [16, 17]. Aggressive downsampling is also commonly used to reduce the training and inference time and VRAM consumption of MHSA based speech processing models [18, 19, 20]. Nevertheless, this approach does not reduce the quadratic complexity of MHSA. However, a recently-developed linear-complexity model, SummaryMixing [17], is promising for developing linearcomplexity SSL models, since it is the first linear-comlexity model that surpasses SOTA MHSA models for automatic speech recognition and spoken language understanding under supervised training. SummaryMixing has two branches: a local branch uses a point-wise feed-forward network to capture the local information, and a summary branch which uses the average vector of the input frames to capture the global information. The output of the two branches are merged to form the hidden representations of the input. Although this efficient model performs well in supervised learning, it is unknown yet whether this simple design is flexible enough to capture all necessary features at different levels for different downstream speech processing tasks. In this paper, we equip wav2vec 2.0 using a Conformer context encoder with SummaryMixing. We show that compared to MHSA Conformer wav2vec 2.0 model, our proposed model gives better or equivalent results for the downstream automatic speech recognition, intent classification, emotion recognition, and automatic speaker verification tasks of the MP3S benchmark [21]. The numerical experimental results and our analysis of downstream tasks demonstrate that SummaryMixing captures different levels of speech representations (i.e. content, semantic, paralinguistics, and speaker features) well through SSL pretraining. For the efficiency aspect, SummaryMixing reduces the pre-training time and the peak VRAM by $18 \\%$ and $23 \\%$, respectively, making the pre-training of a 155 M wav2vec 2.0 model finished within 7 days with 4 Tesla A100 GPUs. To the best of our knowledge, this paper is the first to present a linear-complexity SSL model with no performance drop on downstream tasks. We release the necessary recipes of reproducing SummaryMixing Conformer wav2vec 2.0 with the opensource toolkit SpeechBrain [22]."}, {"title": "1.1. Previous works for efficient SSL", "content": "Previous works have addressed the inefficiency of the pretraining of SSL models from different angles. [3, 23] improve the pre-training procedure. [24, 4] enhance the efficiency by crafting the pre-training objective. From the efficient model architecture aspect, the feature extractor have been studied. [25] reduces the number of channels and the kernel size of the deep CNN feature extractor. A more effective approach is to replace the deep CNN feature extractor with Mel filterbanks or a combination of Mel filterbanks and a shallow CNN [26, 3, 6, 7, 8]. Our proposed SummaryMixing context encoder addresses the inefficiency problem from a different angle, and it is compatible with existing methods. For example, Section 2 will combine an efficient feature extractor with the SummaryMixing context encoder."}, {"title": "2. SummaryMixing for wav2vec 2.0", "content": "This section first introduces SummaryMixing [17]. Then, it proposes SummaryMixing wav2vec 2.0 model. SummaryMixing is a linear-complexity alternative to MHSA. It transforms the input sequence $\\mathbf{X} \\in \\mathbb{R}^{T \times D}=\\left\\{\\mathbf{x}_{0}, \\ldots, \\mathbf{x}_{T}\right\\}$ of $T$ feature vectors $\\mathbf{x}_{t}$ of length $D$ to a sequence of hidden representations $\\mathbf{H} \\in \\mathbb{R}^{T \times D^{\\prime}}=\\left\\{\\mathbf{h}_{0}, \\ldots, \\mathbf{h}_{T}\right\\}$. It has been found that MHSA in trained Transformer and Conformer layers can behave as feed-forward networks [27, 28]. For Branchformer which has one convolutional branch and one MHSA branch, the MHSA tends to merely implement an average [16]. SummaryMixing therefore explicitly computes an average, but at linear cost. As shown in Figure 1a, it has a branch that generates a single vector summarizing the global information of the input sequence by averaging the non-linear transformation $s\\left(\\mathbf{x}_{t}\right)$ of each input vector over time $\\left(\\frac{1}{T} \\sum\right)$. The local branch uses a non-linear transformation $f(\\cdot)$ to extract the local information for each input vector $\\mathbf{x}_{t}$. Then, the single global vector and the local vector $f\\left(\\mathbf{x}_{t}\right)$ are combined through a non-linear combination function $c(\\cdot)$ to produce the hidden representation $\\mathbf{h}_{t}$ for each time step. Figure 1a illustrates the architecture of SummaryMixing. Mathematically, it can be described as \n\n$$\\overline{\\mathbf{s}}=\\frac{1}{T} \\sum_{t=1}^{T} s\\left(\\mathbf{x}_{t}\right) ; \\quad \\mathbf{h}_{t}=c\\left(\\left[f\\left(\\mathbf{x}_{t}\right), \\overline{\\mathbf{s}}\right]\right)$$\n\nwhere $s: \\mathbb{R}^{D} \rightarrow \\mathbb{R}^{D^{\\prime \\prime}}, f: \\mathbb{R}^{D} \rightarrow \\mathbb{R}^{D^{\\prime \\prime}}$, and $c: \\mathbb{R}^{2 D^{\\prime \\prime}} \rightarrow \\mathbb{R}^{D^{\\prime}}$. $s(\\cdot), f(\\cdot)$, and $c(\\cdot)$ are neural networks with one hidden layer. The averaging $\\frac{1}{T} \\sum$ and the non-liner functions $s(\\cdot), f(\\cdot)$, and $c(\\cdot)$ have a linear time and space complexity with respect to the input sequence length, making SummaryMixing a linearcomplexity model. SummaryMixing can be used inside a Conformer or a Branchformer [17]. Figure 1b shows the architecture of the Conformer. The standard self-attention block inside it can simply be replaced with SummaryMixing. Both Conformer and Branchformer can be equipped with SummaryMixing and reach SOTA performance for automatic speech recognition and spoken language understanding tasks [17]."}, {"title": "2.2. SummaryMixing for wav2vec 2.0", "content": "We use wav2vec 2.0 (w2v2) [1] as the SSL model in this paper, since it is a well-established model and popular for SSL research. ![img-0.jpeg](img-0.jpeg) (a) The SummaryMixing block, which can be inserted into the Conformer in Figure 1b. The function $\\frac{1}{T} \\sum$ is executed only once and the average is fed back to each time step $t$, leading to a linear complexity. ![img-1.jpeg](img-1.jpeg) (b) The Conformer, which can be equipped (the green block) with either self-attention or SummaryMixing. Figure 1: Architectures of SummaryMixing and the Conformer. Since w2v2 is implemented in widely used toolkits such as SpeechBrain [22] and FairSeq [29], results are straightforward to reproduce. Compared to more recently proposed SSL models, w2v2 is still competitive for downstream speech processing tasks [21]. Section 3 will show that our SummaryMixing w2v2 model, which will be proposed in this section, outperforms HuBERT [2] and data2vec [24] for a variety of tasks. To improve the pre-training efficiency of w2v2 model, we equip the context encoder of w2v2 with SummaryMixing. SummaryMixing has been used inside both of Branchformers and Conformers for supervised training. However, the Conformer is more typically used for SSL [26, 6, 30]. Thus, we will use the Conformer with SummaryMixing rather than the Branchformer with SummaryMixing as the context encoder. In addition, we do not consider the Transformer context encoder in the original w2v2 model [1], since Conformer-based architectures have demonstrated better performance for both of SSL models $[26,6,30]$ and supervised trained SummaryMixing models [17]. To make the pre-training more efficient, we replace the original w2v2 deep CNN feature extractor with a combination of Mel filterbanks and a shallow 1D CNN as in [7]. This replacement leads to equivalent performance for downstream tasks in the previous work [7]. Other model components and the training objective follows the original w2v2 model [1]."}, {"title": "3. Experiments", "content": "As discussed in Section 2, we build a w2v2 model with a Conformer context encoder. For simplicity, we use \"MHSA w2v2\" and \"SummaryMixing w2v2\" to denote w2v2 model with MHSA and SummaryMixing Conformer context encoder, respectively. Model Architecture and training objective. For both of MHSA w2v2 and SummaryMixing w2v2, the Conformer context encoder has 12 layers. Each Conformer layer has a hidden dimension of 768 for the MHSA or SummaryMixing block, and a hidden dimension of 3072 for the MLP block. The kernel size and the stride is 31 and 1 for the Convolution block, respectively. Each MHSA or SummaryMixing block has 8 heads (multi-headed SummaryMixing is defined in [17]). To further enhance the pre-training efficiency, following [7], we use the Mel filterbanks with a 1D CNN (FBank-CNN1D) as the feature extractor for the w2v2 models in this paper. The FBank-CNN1D feature extractor consist of 80 channel filterbank with 25 ms windows and a hop length of 10 ms , and a 512-channel two-layered 1D CNN with kernel sizes and strides of $(3,3),(2,1)$ respectively. All other components and the training objective follow the original w2v2 [1]. All models are implemented and pre-trained with SpeechBrain V1.0 [22]. Pre-training details. We use the Libri-Light Medium subset as the pre-training data [31]. We use the voice activation detection tool in [31] to clip the audio recordings. For each audio recording, we discard clips longer than 60 s. Then, we concatenate adjacent clips to sequences with a maximum 30 s length. If a clip is between 30 s and 60 s, it will not be concatenated to any other clip. For example, if a recording is clipped into 10 s, 10 s, and 35 s clips, this recording will give a 20 s long and a 35 s long training sequence. In this way, the total amount pre-training data is 4.3 k hours. We use 4 Tesla A100 GPUs for pre-training. Each GPU has a batch size 360s. A gradient accumulation of 4 is used, resulting in a 1.6 h total batch size. The pre-training takes 300k steps. We use the Noam [9] learning rate scheduler with 30 k warmup steps and a peak learning rate $5 \\cdot 10^{-4}$. All other setups follow the SpeechBrain V1.0 w2v2 pre-training configuration. Pre-training Efficiency. Table 1 shows the pre-training time and peak VRAM for both of the MHSA and SummaryMixing w2v2 models. Compared to MHSA, SummaryMixing reduces the pre-training time by $18 \\%$, relatively. It worth noting that the pre-training of the 155M SummaryMixing w2v2 model can be finished within 7 days with 4 Tesla A100 GPUs. With the same infrastructure, batch size, and number of steps, the training time of the original 95 M w2v2 base model is more than 10 days [7]. For the memory efficiency, compared to MHSA, SummaryMixing also reduces the peak VRAM by $23 \\%$, relatively. Moreover, since each SummaryMixing block is more efficient compared to each MHSA block for each Conformer layer, SummaryMixing will lead to larger efficiency gain when scaling up the models. We demonstrate this by efficiency experiments of doubling the number of Conformer context encoder layers, leading to a 308M SummaryMixing w2v2 model and a 329M MHSA w2v2 model. Same as the 12-layered 155M SummaryMixing and 165M MHSA w2v2 models, the 24-layered 308M SummaryMixing model can be trained with a 360s batch size on each GPU. As Table 1 shows, doubling the layers of SummaryMixing context encoder increases the peak VRAM from 51GB to 76GB. The peak VRAM of the 24-layered 308M SummaryMixing model is almost the same as the 12-layered 165 MHSA model. Training the 329M MHSA w2v2 with a 360 s batch size per GPU will lead to out of memory issues. To fit the 329M MHSA w2v2 within the 80G VRAM limitation of a Tesla A100 GPU, the batch size per GPU needs to be reduced from 360 s to at most 250 s. We estimate the pre-training time of the scaled-up models by pre-training each model for one epoch. Table 1 shows the scaling-up will increase the pre-training time of SummaryMixing w2v2 from 7 days to 15 days, and will increase the pre-training time of MHSA w2v2 from 9 days to more than 24 days. Thus, for the scaled up models, the relative pre-training time reduction from SummaryMixing encoder is increased from $18 \\%$ to $35 \\%$. We do not fully pre-train 24-layered w2v2 models in this paper since the efficiency benefit is already demonstrated even with the smaller models, and the pre-training of the 329M MHSA w2v2 is too time-consuming for our infrastructure."}, {"title": "3.2. Downstream tasks", "content": "MP3S benchmark. We test the downstream tasks performance of the w2v2 models through SpeechBrain MP3S benchmark [21]. Similar to other widely used SSL benchmarks such as SUPERB [32], the pre-trained SSL models are frozen. The weighted sum of the input to the context encoder and the hidden representations from all the context encoder layers, with respect to a set of trainable weights, is used as the speech representation for the downstream models. However, differently from SUPERB, MP3S does not limit each task to a single downstream model. The tasks in MP3S include automatic speech recognition (ASR), intent classification (IC), emotion recognition (ER), and automatic speaker verification (ASV). Experimental setups. For ASR, following MP3S, we considered three datasets: LibriSpeech [33] train-clean-100 split for English ASR, CommonVoice 11.0 [34] Welsh (Cymraeg) and Basque (Euskera) datasets for low-resource ASR. The Welsh and Basque dataset contain 15.8 and 11 hours of training data, respectively. We use the long short-term memory (LSTM) [35] or ContextNet [36] with Connectionist Temporal Classification (CTC) [37] objective function as the downstream model for LibriSpeech, and LSTM with CTC for Welsh and Basque. We do not use the Buckeye corpus [38] as in MP3S due to licence issues. For IC, we use the SLURP dataset [39] with a LSTM downstream model. For ER, we use the IEMOCAP dataset [40] with an ECAPA [41] downstream model. For ASV, we use the VoxCeleb1 corpus [42] with an ECAPA downstream model. All the model and training configurations follow [21]. Experimental results. Table 2 shows the performance of downstream tasks. For ASR, our SummaryMixing w2v2 model outperforms the MHSA w2v2 model for English LibriSpeech dataset with 100 h training data. For low-resource ASR with limited training data, SummaryMixing w2v2 surpasses MHSA w2v2 for both of Welsh 15.8 h and Basque 11 h . For all ASR tasks, on average, SummaryMixing reduces the WERs by $7.8 \\%$, relatively. The w2v2 base model outperforms trained w2v2 Conformer models on LibriSpeech, but this is due to the unfair advantage that w2v2 base is also pre-trained with LibriSpeech. For the low-resource ASR where the ASR datasets are strictly different from the pretraining datasets, SummaryMixing w2v2 outperforms the w2v2 base model by a large margin - $14.7 \\%$ relative WERs reduction on average. More impressively, for low-resource ASR, our SummaryMixing w2v2 surpasses the HuBERT large model with a $7.3 \\%$ average relative WERs reduction. These results demonstrate the effectiveness of the SummaryMixing SSL model for ASR under difference scenarios. Not included in the table is an intriguing initial result when the w2v2 model itself is finetuned as well, on LibriSpeech train-clean-100, using a two-layered neural network on top of it with the CTC objective function. In this experiment, with MHSA the LibriSpeech dev-clean WER is 5.1 , which surprisingly is better than SummaryMixing, at 5.4 (w2v2 base gives a 6.1 WER [1]). This is unexpected since when trained in a fully supervised fashion, SummaryMixing outperforms MHSA for a variety of ASR tasks [17], and the frozen SummaryMixing w2v2 also gives better downstream ASR results as shown in Table 2. We leave the investigation of this unexpected result as future work. Table 2 also shows SummaryMixing w2v2 outperforms MHSA w2v2 for intent classification (IC) and automatic speaker verification (ASV), with a $2.0 \\%$ and $7.7 \\%$ relative performance gain, respectively. It is worth noting that for IC and ASV, our SummaryMixing w2v2 model surpasses w2v2 large, Hubert large, and data2vec large, even though these large models have about $2 \times$ more parameters and are pre-trained with $15 \times$ more data. For emotion recognition (ER), SummaryMixing is slightly behind MHSA ( $0.7 \\%$ relative). However, for ER, the w2v2 large model is also $6.5 \\%$ relatively worse than the w2v2 small model. This indicates it may be difficult to produce a universal optimal w2v2 model for all tasks. Therefore, in summary, SummaryMixing gives better or equivalent results for the downstream ASR, IC, ER and ASV tasks compared to MHSA. Figure 2 shows the learned weights for each context encoder layer for all the downstream tasks. For ASR, IC, and ER, both the SummaryMixing and MHSA models have most weights assigned to the higher layers, which is consistent with previous findings that the higher layers of trained w2v2 models tend to capture linguistic features such as phonetic information and word meaning [43]. For ASV, the downstream model tend to use low-level features, which is also consistent with the literature [43]. The distribution of the learned weights and the numerical experimental results indicate that SSL pre-training with the linear model SummaryMixing well captures different levels of representations, i.e., content (ASR), semantic (IC), paralinguistic (ER) and speaker (ASV) features."}, {"title": "4. Conclusion", "content": "In this paper, we investigate a linear-complexity model, a Conformer with SummaryMixing, as the context encoder for wav2vec 2.0 model. Compared to self-attention based Conformer context encoder, SummaryMixing improves the pre-training speed by $18 \\%$ and reduces the peak VRAM by $23 \\%$. Also, when used as a feature extractor, with SummaryMixing the model yields better or the same level of performance for downstream speech processing tasks compared to wav2vec 2.0 with selfattention. Future works include building other SSL models using SummaryMixing in a Conformer context encoder, as well as exploring full fine-tuning for SummaryMixing SSL models."}]}