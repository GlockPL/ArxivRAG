{"title": "Linear-Complexity Self-Supervised Learning for Speech Processing", "authors": ["Shucong Zhang", "Titouan Parcollet", "Rogier van Dalen", "Sourav Bhattacharya"], "abstract": "Self-supervised learning (SSL) models usually require weeks\nof pre-training with dozens of high-end GPUs. These models\ntypically have a multi-headed self-attention (MHSA) context\nencoder. However, MHSA takes quadratic time and space in the\ninput length, contributing to the high pre-training cost. Linear-\ncomplexity alternatives to MHSA have been proposed. For\ninstance, in supervised training, the SummaryMixing model is\nthe first to outperform MHSA across multiple speech process-\ning tasks. However, these cheaper alternatives have not been\nexplored for SSL yet. This paper studies a linear-complexity con-\ntext encoder for SSL for the first time. With better or equivalent\nperformance for the downstream tasks of the MP3S benchmark,\nSummaryMixing reduces the pre-training time and peak VRAM\nof wav2vec 2.0 model by 18% and by 23%, respectively, leading\nto the pre-training of a 155M wav2vec 2.0 model finished within\none week with 4 Tesla A100 GPUs. Code\u00b9 is available.\nIndex Terms: self-supervised learning, efficient models", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning (SSL) models have demonstrated state-\nof-the-art (SOTA) performance for speech processing tasks [1,\n2, 3, 4]. SSL models are pre-trained on unlabeled data to learn\nhidden features of the input audio. Since the pre-training does\nnot require human transcription, SSL can leverage a huge amount\nof unlabeled data. The large amount of pre-training data is one\nof the keys to the success of SOTA SSL models. SSL models\nalso benefit from the large model size. The typical model size\nvaries from millions to billions parameters.\nHowever, training large models with huge amount of data\nleads to extremely big training cost, resulting in an exceedingly\nhigh barrier for the research of SSL models, as well as extensive\ncarbon footprints. For example, the pre-training of a 330M\nwav2vec 2.0 model [1] with 3k hours data requires 32 Tesla\nV100 GPUs running for two weeks, consuming 1.818 MWh of\nenergy, while the pre-training of a 965M wav2vec 2.0 model\nwith 14k hours data requires 32 Tesla A100 GPUs running for\ntwo weeks, consuming 16.511 MWh of energy [5].\nIn this paper, we address the pre-training inefficiency of\nSSL models from the efficient architecture aspect. SOTA SSL\nmodels typically consistent of a feature extractor and a context\nencoder. The feature extractor extracts features from the raw\nwave input, and the context encoder generates further hidden\nrepresentations. Efficient feature extractors have been proposed\n[3, 6, 7, 8]. Nevertheless, to the best of our knowledge, the\ncontext encoder has not been studied from the efficiency angle\nyet. Thus, this paper improves the pre-training efficiency from\nthe aspect of context encoder.\nThe context encoder of the SOTA SSL models is usually a\nmulti-headed self-attention (MHSA) Transformer [9] or Con-\nformer [10] encoder. However, MHSA has a quadratic time and\nspace complexity in the input sequence length, slowing down\npre-training and increasing VRAM consumption. Methods of\ndeveloping sub-quadratic complexity alternatives to MHSA in-\nclude designing priors for the attention patterns [11, 12], low-\nrank approximation [13], kernelization [14], and linearization\n15]. Unfortunately, compared to MHSA, these methods usu-\nally lead to inferior results for speech processing tasks [16, 17].\nAggressive downsampling is also commonly used to reduce the\ntraining and inference time and VRAM consumption of MHSA\nbased speech processing models [18, 19, 20]. Nevertheless, this\napproach does not reduce the quadratic complexity of MHSA.\nHowever, a recently-developed linear-complexity model,\nSummaryMixing [17], is promising for developing linear-\ncomplexity SSL models, since it is the first linear-comlexity\nmodel that surpasses SOTA MHSA models for automatic speech\nrecognition and spoken language understanding under super-\nvised training. SummaryMixing has two branches: a local branch\nuses a point-wise feed-forward network to capture the local infor-\nmation, and a summary branch which uses the average vector of\nthe input frames to capture the global information. The output of\nthe two branches are merged to form the hidden representations\nof the input. Although this efficient model performs well in su-\npervised learning, it is unknown yet whether this simple design\nis flexible enough to capture all necessary features at different\nlevels for different downstream speech processing tasks.\nIn this paper, we equip wav2vec 2.0 using a Conformer\ncontext encoder with SummaryMixing. We show that compared\nto MHSA Conformer wav2vec 2.0 model, our proposed model\ngives better or equivalent results for the downstream automatic\nspeech recognition, intent classification, emotion recognition,\nand automatic speaker verification tasks of the MP3S benchmark\n[21]. The numerical experimental results and our analysis of\ndownstream tasks demonstrate that SummaryMixing captures\ndifferent levels of speech representations (i.e. content, semantic,\nparalinguistics, and speaker features) well through SSL pre-\ntraining. For the efficiency aspect, SummaryMixing reduces\nthe pre-training time and the peak VRAM by 18% and 23%,\nrespectively, making the pre-training of a 155M wav2vec 2.0\nmodel finished within 7 days with 4 Tesla A100 GPUs.\nTo the best of our knowledge, this paper is the first to present\na linear-complexity SSL model with no performance drop on\ndownstream tasks. We release the necessary recipes of repro-\nducing SummaryMixing Conformer wav2vec 2.0 with the open-\nsource toolkit SpeechBrain [22]."}, {"title": "1.1. Previous works for efficient SSL", "content": "Previous works have addressed the inefficiency of the pre-\ntraining of SSL models from different angles. [3, 23] improve\nthe pre-training procedure. [24, 4] enhance the efficiency by\ncrafting the pre-training objective. From the efficient model\narchitecture aspect, the feature extractor have been studied. [25]\nreduces the number of channels and the kernel size of the deep\nCNN feature extractor. A more effective approach is to replace\nthe deep CNN feature extractor with Mel filterbanks or a com-\nbination of Mel filterbanks and a shallow CNN [26, 3, 6, 7, 8].\nOur proposed SummaryMixing context encoder addresses the\ninefficiency problem from a different angle, and it is compatible\nwith existing methods. For example, Section 2 will combine\nan efficient feature extractor with the SummaryMixing context\nencoder."}, {"title": "2. SummaryMixing for wav2vec 2.0", "content": "This section first introduces SummaryMixing [17]. Then, it\nproposes SummaryMixing wav2vec 2.0 model."}, {"title": "2.1. SummaryMixing", "content": "SummaryMixing is a linear-complexity alternative to MHSA. It\ntransforms the input sequence $X \\in \\mathbb{R}^{T \\times D} = \\{x_0,..., x_T\\}$\nof T feature vectors $x_t$ of length D to a sequence of hidden repre-\nsentations $H \\in \\mathbb{R}^{T \\times D'} = \\{h_0, ..., h_T\\}$. It has been found that\nMHSA in trained Transformer and Conformer layers can behave\nas feed-forward networks [27, 28]. For Branchformer which has\none convolutional branch and one MHSA branch, the MHSA\ntends to merely implement an average [16]. SummaryMixing\ntherefore explicitly computes an average, but at linear cost. As\nshown in Figure 1a, it has a branch that generates a single vector\nsummarizing the global information of the input sequence by av-\neraging the non-linear transformation $s(x_t)$ of each input vector\nover time ($\\sum$). The local branch uses a non-linear transfor-\nmation $f(.)$ to extract the local information for each input vector\n$x_t$. Then, the single global vector and the local vector $f(x_t)$ are\ncombined through a non-linear combination function $c(.)$ to pro-\nduce the hidden representation $h_t$ for each time step. Figure 1a\nillustrates the architecture of SummaryMixing. Mathematically,\nit can be described as\n$s = \\frac{1}{T} \\sum_{t=1}^T s(x_t);$\n$h_t = c([f(x_t), s]).$\nwhere $s : \\mathbb{R}^D \\rightarrow \\mathbb{R}^{D''}$, $f : \\mathbb{R}^D \\rightarrow \\mathbb{R}^{D''}$, and $c : \\mathbb{R}^{2D''} \\rightarrow \\mathbb{R}^{D'}$.\n$s(.)$, $f(.)$, and $c(.)$ are neural networks with one hidden layer.\nThe averaging $\\sum$ and the non-liner functions $s(\\cdot)$, $f(\\cdot)$, and\n$c(\\cdot)$ have a linear time and space complexity with respect to\nthe input sequence length, making SummaryMixing a linear-\ncomplexity model.\nSummaryMixing can be used inside a Conformer or a\nBranchformer [17]. Figure 1b shows the architecture of the\nConformer. The standard self-attention block inside it can sim-\nply be replaced with SummaryMixing. Both Conformer and\nBranchformer can be equipped with SummaryMixing and reach\nSOTA performance for automatic speech recognition and spoken\nlanguage understanding tasks [17]."}, {"title": "2.2. SummaryMixing for wav2vec 2.0", "content": "We use wav2vec 2.0 (w2v2) [1] as the SSL model in this paper,\nsince it is a well-established model and popular for SSL research.\nSince w2v2 is implemented in widely used toolkits such as\nSpeechBrain [22] and FairSeq [29], results are straightforward\nto reproduce. Compared to more recently proposed SSL models,\nw2v2 is still competitive for downstream speech processing tasks\n[21]. Section 3 will show that our SummaryMixing w2v2 model,\nwhich will be proposed in this section, outperforms HuBERT [2]\nand data2vec [24] for a variety of tasks.\nTo improve the pre-training efficiency of w2v2 model, we\nequip the context encoder of w2v2 with SummaryMixing. Sum-\nmaryMixing has been used inside both of Branchformers and\nConformers for supervised training. However, the Conformer\nis more typically used for SSL [26, 6, 30]. Thus, we will use\nthe Conformer with SummaryMixing rather than the Branch-\nformer with SummaryMixing as the context encoder. In addi-\ntion, we do not consider the Transformer context encoder in the\noriginal w2v2 model [1], since Conformer-based architectures\nhave demonstrated better performance for both of SSL models\n[26, 6, 30] and supervised trained SummaryMixing models [17].\nTo make the pre-training more efficient, we replace the origi-\nnal w2v2 deep CNN feature extractor with a combination of Mel\nfilterbanks and a shallow 1D CNN as in [7]. This replacement\nleads to equivalent performance for downstream tasks in the\nprevious work [7]. Other model components and the training\nobjective follows the original w2v2 model [1]."}, {"title": "3. Experiments", "content": "As discussed in Section 2, we build a w2v2 model with a Con-\nformer context encoder. For simplicity, we use \"MHSA w2v2\"\nand \"SummaryMixing w2v2\" to denote w2v2 model with MHSA\nand SummaryMixing Conformer context encoder, respectively."}, {"title": "3.1. SSL pre-training", "content": "Model Architecture and training objective. For both of\nMHSA w2v2 and SummaryMixing w2v2, the Conformer\ncontext encoder has 12 layers. Each Conformer layer has a"}, {"title": "3.2. Downstream tasks", "content": "MP3S benchmark. We test the downstream tasks performance\nof the w2v2 models through SpeechBrain MP3S benchmark\n[21]. Similar to other widely used SSL benchmarks such as\nSUPERB [32], the pre-trained SSL models are frozen. The\nweighted sum of the input to the context encoder and the\nhidden representations from all the context encoder layers,\nwith respect to a set of trainable weights, is used as the speech\nrepresentation for the downstream models. However, differently\nfrom SUPERB, MP3S does not limit each task to a single\ndownstream model. The tasks in MP3S include automatic\nspeech recognition (ASR), intent classification (IC), emotion\nrecognition (ER), and automatic speaker verification (ASV).\nExperimental setups. For ASR, following MP3S, we\nconsidered three datasets: LibriSpeech [33] train-clean-100 split\nfor English ASR, Common Voice 11.0 [34] Welsh (Cymraeg)\nand Basque (Euskera) datasets for low-resource ASR. The Welsh\nand Basque dataset contain 15.8 and 11 hours of training data,\nrespectively. We use the long short-term memory (LSTM) [35]\nor ContextNet [36] with Connectionist Temporal Classification\n(CTC) [37] objective function as the downstream model for\nLibriSpeech, and LSTM with CTC for Welsh and Basque. We\ndo not use the Buckeye corpus [38] as in MP3S due to licence\nissues. For IC, we use the SLURP dataset [39] with a LSTM\ndownstream model. For ER, we use the IEMOCAP dataset [40]\nwith an ECAPA [41] downstream model. For ASV, we use the\nVoxCeleb1 corpus [42] with an ECAPA downstream model. All\nthe model and training configurations follow [21].\nExperimental results. Table 2 shows the performance of down-\nstream tasks. For ASR, our SummaryMixing w2v2 model outper-\nforms the MHSA w2v2 model for English LibriSpeech dataset\nwith 100h training data. For low-resource ASR with limited train-\ning data, SummaryMixing w2v2 surpasses MHSA w2v2 for both\nof Welsh 15.8h and Basque 11h. For all ASR tasks, on average,\nSummaryMixing reduces the WERs by 7.8%, relatively. The\nw2v2 base model outperforms trained w2v2 Conformer models\non LibriSpeech, but this is due to the unfair advantage that w2v2\nbase is also pre-trained with LibriSpeech. For the low-resource"}, {"title": "4. Conclusion", "content": "In this paper, we investigate a linear-complexity model, a\nConformer with SummaryMixing, as the context encoder for\nwav2vec 2.0 model. Compared to self-attention based Conformer\ncontext encoder, SummaryMixing improves the pre-training\nspeed by 18% and reduces the peak VRAM by 23%. Also, when\nused as a feature extractor, with SummaryMixing the model\nyields better or the same level of performance for downstream\nspeech processing tasks compared to wav2vec 2.0 with self-\nattention. Future works include building other SSL models using\nSummaryMixing in a Conformer context encoder, as well as\nexploring full fine-tuning for SummaryMixing SSL models."}]}