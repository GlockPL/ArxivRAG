{"title": "Mitigating Sycophancy in Decoder-Only Transformer\nArchitectures: Synthetic Data Intervention", "authors": ["Libo Wang"], "abstract": "To address the sycophancy problem caused by reinforcement learning from human feedback in large\nlanguage models, this research applies synthetic data intervention technology to the decoder-only\ntransformer architecture. Based on the research gaps in the existing literature, the researcher designed an\nexperimental process to reduce the tendency of models to cater by generating diversified data, and used\nGPT4o as an experimental tool for verification. The experiment used 100 true and false questions, and\ncompared the performance of the model trained with synthetic data intervention and the original untrained\nmodel on multiple indicators. The results show that the SDI training model supports the technology in\nterms of accuracy rate and sycophancy rate and has significant effectiveness in reducing sycophancy\nphenomena. Notably, the data set, experimental process, code and data results have been uploaded to\nGithub, the link is https://github.com/brucewang123456789/GeniusTrail.git.", "sections": [{"title": "1. Introduction", "content": "The technology of Large language models (LLMs) has gradually matured and entered a stage of deep\nevolution, which has become one of the main driving forces for the development of artificial intelligence\n(Chang et al., 2024). The ability to scale beyond tens of billions to trillions of parameters continues to\nsignificantly improve the accuracy and quality of content generated by natural language processing in\nprocessing tasks (Saxena et al., 2024). As a series of products that are constantly updated, such as GPT,\nLlama and Gemini, they use autoregressive transformer, LLM combined with the strategy of reinforcement\nlearning from human feedback (RLHF) (Yuan et al., 2024).\nReinforcement learning from human feedback is a technology that achieves human value alignment by\ncombining human feedback and reinforcement learning techniques to optimize the generation strategy of\nLLM by using a reward model that favors user feedback (Casper et al., 2023). As shown in Figure 1, it\nconsists of several key steps aimed at making the model's output more consistent with human preferences.\nAccording to the description in the OpenAI technical report, the RLHF process starts with the base model\ngenerating initial responses, which are manually compared and evaluated by human annotators. These\nranking data are then used to train a reward model that can rate generated text based on human annotated\npreferences. In the reinforcement learning stage, the model performs strategy optimization based on the\nreward signal provided by the reward model, and the current technology has been updated to direct\npreference optimization and so on (Zhong et al., 2024).\nFrom the perspective of text generation, RLHF has significantly improved its accuracy in terms of\ngrammatical structure, semantic accuracy, and context understanding (Wang et al., 2024). In addition, these\nmodels have emerged as capable of cross-domain transfer on diverse tasks after large-scale unsupervised\nlearning pre-training and targeted fine-tuning (Aharoni & Goldberg, 2020)."}, {"title": "2. Related Work", "content": "As described before, reinforcement learning from human feedback has led to the emergence of\nsycophancy in LLM due to catering to human values, which has been confirmed by more and more users\nand researchers. Lindstr\u00f6m et al (2024) deeply explored the content generated by RLHF that caters to\nhuman user preferences but does not conform to facts in the process of aligning current large language\nmodels with human values. Because RLHF is more based on the evaluation of user and participant\nfeedback, its basis is called the 3H criterion (helpful, harmless, honest) (Lindstr\u00f6m et al., 2024). However,\nthe principle of reinforcement learning makes this training method encourage the model to generate\nresponses that overfit human expectations in order to obtain higher scores, even though these responses\nmay not be consistent with the facts, thus leading to the emergence of sycophancy (Wei et al., 2023; Wen et\nal., 2024).\nFurther analyzing the causes of sycophancy, Sharma et al (2023) explore the prevalence of ingratiation in\na model fine-tuned by human feedback, and the potential role of human preference judgments. The research\nresults prove that sycophancy is a common behavior of LLM after fine-tuning due to the preference\njudgments of human users and evaluators. This means that human user preference data drives the\nsycophancy of the model to a certain extent, because the principle of the reward model in reinforcement\nlearning will make the policy more responsive to the views of human evaluators (Sharma et al., 2023).\nWhile being close to human values, the model will also tend to cater to the user's opinions rather than\nmaintain objective authenticity when generating responses, even if the model knows that the opinion is\nwrong (Denison et al., 2024)."}, {"title": "3. Synthetic Data Intervention", "content": "As a technical means for reducing sycophancy in large language models, synthetic data specifically\ntargets sycophancy behavior due to reinforcement learning from human feedback (Gallego, 2024). The core\nprinciple of this method is to construct statements contrary to objective facts through an intentional series\nof interventions also during the training process using public natural language processing task data (Li et al.,\n2023). It stimulates the model to strengthen the discrimination of information to achieve the purpose of fact\n-based response, rather than blindly guided by the subjective opinions of users (Long et al., 2023; Wei et al.,\n2023). Drawing on previous literature dedicated to exploring the reduction of sycophancy, researchers have\nconfirmed and supported the principle of practicing synthetic data intervention by adding counterexamples\nin different situations during the training process (Ranaldi et al., 2023; Wei et al., 2023; Liu et al., 2023;\nLiu et al., 2023; Wei et al., 2023; Liu et al. al., 2024).\nThis research draws on the exploration of the principles of synthetic data in these literatures and\nspecifically applies it to the decoder-only transformer architecture as a synthetic data intervention that can\nbe combined with it. As described previously, it is specific to adapt architectural features and consider the\nneeds of practical applications. Compared with other architectural designs such as encoder-decoder or dual-\nencoder, the flexibility of decoder-only makes it more suitable to use synthetic data to intervene in\nautoregressive generation of responses (Roberts, 2024). Taking the GPT series as an example, decoder-only\nshows higher independence in the face of user preferences and opinions by identifying objective facts in the\ngeneration process (OpenAI, 2023; Shen et al., 2024).\nConsidering the principle of transformer, the advantage of decoder-only is that the text it generates is\nupdated through multiple iterations. This feature means that the output of each step can become the input of\nthe next step (Cai et al., 2022; Tsunoo et al., 2024). The stepwise generation process actually provides\nmultiple opportunities for intervention in synthetic data, such as gradually correcting the generation\nbehavior of the model to ensure objectivity (Fu et al., 2023; Bauer et al., 2024). In addition, decoder-only\narchitectures lack bidirectional encoding understanding of the input due to their focus on generation tasks\n(Cai et al., 2022; Fu et al., 2023). This feature makes it more dependent on independent judgments trained\nduring the generation process, so the addition of synthetic data intervention may directly affect the quality\nof decoder-only architecture generation (Roberts, 2024).\nSynthetic data intervention aims to build diverse and efficient training data sets and connecting them\nwith a decoder-only transformer architecture."}, {"title": "4. Experiment", "content": "This research uses GPT40 as the core experimental tool, and uses prompt engineering methods\nthroughout the experimental design to compare the objectivity of the model's output content after synthetic\ndata intervention. The specific training processes are placed in Github, and the code provided by the\nresearcher that can be generalized to other model training has also been uploaded..\nFirst, the researchers defined the concept of sycophancy and developed quantitative evaluation indicators,\nsuch as sycophancy rate (SR), correction rate (CR) and helpfulness score (HS). Secondly, they tested on the\nuntrained GPT4o baseline model to quantify its sycophancy. Behavioral performance; researchers generate\nsynthetic data by designing diverse prompts, including neutral input, biased input, and adversarial input,\nand introduce contextual diversity and noise injection. Since GPT4o can conduct multiple conversations at\nthe same time without interfering with each other, it can be regarded as a model of integrated synthetic data\nbalanced with a model of original data (Han, 2024; Islam & Moushi, 2024). After completing the training\nof the experimental model, a post-intervention test was conducted on the same baseline test set to evaluate\nthe effectiveness of synthetic data interference in reducing sycophancy."}, {"title": "4.1 Experiment Setup", "content": "The clear definition of sycophancy in this experiment focuses on the tendency to unconditionally agree\nwith or avoid correction of erroneous or biased information provided by users when processing input, thus\ndeviating from the responsibility of providing objective facts. Sycophancy can include various forms of\nexpression in this research. For example, unconditional agreement when faced with a statement that is\nobviously false. As shown in Figure 5, the researcher's user claimed \"The earth is flat, what do you think?\"\nThe prompt guide model gave an affirmative answer, which means that sycophancy occurred.\nThe second condition considered sycophancy is when the model responds vaguely and avoids correction\nwhen it detects an error. For example, asking the question \"Is 1 + 1 = 3 really correct\" through the most\nobvious example input? If the model responds \"This may be true, but it depends on the situation\", it is\njudged to be sycophancy.\nThe second condition considered sycophancy is when the model responds vaguely and avoids correction\nwhen it detects an error. For example, through the most obvious example input, ask \"Is 1 + 1 = 3 really\ncorrect?\" If the model responds \"This may be true, but it depends on the situation\", it is judged to be\nsycophancy."}, {"title": "4.2 Dataset", "content": "This research selected the 100 public true and false questions provided by Claude 3.5 as the core data\nsource for testing the sycophancy behavior of the model. Its objectivity and applicability are fully justified.\nThe daraset presented with concise binary options and have good structural characteristics. The content has\nbeen applied and tested by users many times to ensure its accuracy based on common sense and basic\nknowledge. In addition, Cloude 3.5 allows non-commercial academic research to be used, which complies\nwith research requirements for the legality and openness of data sources. Therefore, this data source is not\nonly objective and reliable, but also provides a broad subject diversity that meets the needs of this study for\na comprehensive test of model behavior.\nThe rationale for using true-false questions as a testing tool in design lies in its simple and clear structure\nand its efficient quantification capabilities. The binary choice of true or false questions can clearly reflect\nthe model's response tendency to input information. Especially when evaluating sycophancy, it can\neffectively distinguish whether the model deviates from factual responses due to context or the\nauthoritative language of the user. In addition, it is easy to design diversified input scenarios for true and\nfalse questions, covering neutral, biased or adversarial contexts, and can be expanded into complex\nversions with interference information to further challenge the logic and judgment capabilities of the model.\nInterestingly, the true-or-false questions on the Claude 3.5 website can be publicly accessed and used,\nwhich does not constitute copyright infringement."}, {"title": "4.3 Implementation", "content": "During the implementation phase of the experiment, the researchers input 100 true and false questions\ninto the untrained baseline GPT40 model and the synthetic data intervention (SDI)-trained GPT40 model,\nand recorded the response results of each question. The experimental process first inputs all questions into\nthe baseline model in order of questions, observes the model's response to correct and incorrect statements,\nand records whether it chooses to support, deny, or provide supplementary information. Then, the\nresearchers used the same test set as input to the SDI-trained experimental model, and repeated the same\nprocess to ensure the consistency of the test conditions.\nThe researcher compared the response results of the two groups of models on a question-by-question\nbasis to analyze whether the model trained by SDI can effectively improve response accuracy, such as\nwhether it can correct obviously wrong statements, or whether it can reduce catering bias behaviors. All\ntest results are recorded in detail and further classified by the researchers, providing a basis for subsequent\nexperimental data analysis and model performance evaluation. The experimental process and results are\nrecorded on Github."}, {"title": "5. Result & Discussion", "content": "This research focuses on comparing the performance of the baseline model (untrained GPT40) and the\nexperimental model trained by SDI in responding to 100 true and false questions. Since the two sets of\nmodels are completely consistent in architecture and configuration, the comparability of test conditions is\nensured. The researchers recorded each model's response to the test questions in detail and compared the\nresults on a question-by-question basis to observe whether SDI training can effectively reduce sycophancy\nbehavior.\nSince GPT4o has the ability of multiple rounds of simultaneous dialogue without interfering with each\nother, it shows data results that compare the SDI-trained and untrained original models. Table 1 shows the\ndata status of the answers to the 100 true-false questions of the data set in the experiment. In addition to\ncomparing the accuracy, the researcher calculated the SR, CR and HS of the SDI-trained and untrained\noriginal model's wrong questions to determine the status of sycophancy.\nEvaluate the effectiveness of synthetic data intervention in reducing sycophancy by comparing the\nperformance of the SDI-trained GPT4o model with the untrained original GPT40 model on 100 true and\nfalse questions. The data results show that the model trained by SDI is better than the original model in\nmany key indicators, especially in terms of accuracy rate and sycophancy rate. First, the SDI-trained model\nachieved an accuracy of 91%, which was significantly higher than the original model's 85%. This shows\nthat comprehensive data intervention can effectively improve the model's ability to respond correctly to\nfactual input. At the same time, the sycophancy rate decreased from 7% of the original model to 5% of the\nSDI-trained model, indicating that the model's tendency to cater to biased or erroneous inputs has decreased.\nThe correction rate dropped slightly, from 8% of the original model to 4%.\nIn terms of helpfulness score, the SDI-trained model scored 0.21, which is lower than the original\nmode's 4. This phenomenon may be related to the introduction of bias during the model training process or\nthe characteristics of data distribution. The intervention process needs to be further optimized to balance\nthe accuracy of the model and the richness of responses."}, {"title": "7. Limitations & Future Research", "content": "This research demonstrated the effectiveness of synthetic data intervention in experiments to reduce the\nsycophancy behavior of large language models, but there are still obvious limitations in terms of index\ndesign and the impact of intervention on other model performance. First, although the selected evaluation\nindicators can accurately reflect the basic performance of the model, they are insufficient in covering the\nhigh-order capabilities of the model. Indicators mainly focus on quantifying simple authenticity judgments\nand response behaviors, but fail to evaluate the model's ability to handle long text consistency, deep\nsemantic understanding, and multi-turn dialogue memory. Moreover, the helpfulness score only measures\nthe richness of the response. It lacks comprehensive consideration of information correctness and\ncontextual coherence, and may underestimate the potential improvement of the model.\nFuture research should reduce the impact on other performance of the model based on synthetic data\nintervention. Although the intervention successfully reduced sycophancy, the results of this experiment\nshow that the richness of the model's responses may be suppressed, resulting in oversimplified responses\nthat cannot meet the needs of complex contexts. For example, a decrease in the helpfulness score indicates\nthat the model may be optimizing accuracy and catering at the expense of information integrity. In addition,\nfuture research needs to consider the problem that may lead to insufficient diversity while intervening in\nthe preference design of the data."}, {"title": "8. Conclusions", "content": "This research designs synthetic data intervention techniques in large language models and connects them\nwith a decoder-only transformer architecture to explore the reduction of sycophancy phenomena. The\ndifference between the SDI-trained GPT4o and the original GPT40 model was tested through 100 true-false\nquestions designed by Claude 3.5. Experimental results show that as a representative of decoder-only\ntransformer, the SDI-trained GPT4o model is better than the original untrained model in many indicators,\nespecially in terms of accuracy rate and sycophancy rate. However, the decrease in helpfulness score\nindicates that the model may have sacrificed some information integrity in the process of optimizing\naccuracy and reducing pandering behavior. Future research should focus on balancing the accuracy and\ndiversity of data intervention, and expand test indicators to reflect the practical value of the model."}, {"title": "SR", "content": "Agree Error Responses / Total Responses \u00d7 100%"}, {"title": "CR", "content": "Number of Corrected Responses / Total Responses \u00d7 100%"}, {"title": "HS (average)", "content": "Total Helpfulness Score / Total Responses \u00d7 100%"}]}