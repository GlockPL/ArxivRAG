{"title": "Reverse Thinking Makes LLMs Stronger Reasoners", "authors": ["Justin Chih-Yao Chen", "Zifeng Wang", "Hamid Palangi", "Rujun Han", "Sayna Ebrahimi", "Long Le", "Vincent Perot", "Swaroop Mishra", "Mohit Bansal", "Chen-Yu Lee", "Tomas Pfister"], "abstract": "Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (REVTHINK), a framework composed of data augmentation and learning objectives. In REVTHINK, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model's zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10\u00d7 more forward reasoning. REVTHINK also exhibits strong generalization to out-of-distribution held-out datasets.", "sections": [{"title": "1 Introduction", "content": "\u201cInvert, always, invert.\u201d\nCarl Jacobi.\nReverse thinking plays a crucial role in the human reasoning process (Branchini et al., 2021). Take a"}, {"title": "2 Related Work", "content": "Reasoning with LLMs. A large body of research has shown that LLM reasoning can be improved via advanced test-time approaches, such as prompting and aggregation. Representative methods include"}, {"title": "3 Method", "content": "REVTHINK mainly consists of two stages. In Section 3.1, we provide a formal description of the problem setup. Section 3.2 then describes the details of training data creation. Lastly, in Section 3.3, we introduce the learning objectives."}, {"title": "3.1 Problem Setup", "content": "Let $D = {(Q^{(i)}, A^{(i)})}_{i=1}^{n}$ denote a dataset of n samples, where each sample comprises a question Q(i) and its corresponding answer A(i). We assume black-box access to a teacher model T, where we can get the output but not logits from the teacher, and our objective is to train a smaller student model S and enhance its reasoning capabilities. During"}, {"title": "3.2 Data Augmentation", "content": "Given a reasoning dataset $D = {(Q^{(i)}, A^{(i)})}_{i=1}^{n}$, we begin with augmenting it to produce $D_{aug}$, where each data point in $D_{aug}$ consists of $(Q^{(i)}, R_{f}^{(i)}, Q_{b}^{(i)}, R_{b}^{(i)})$, representing the original question, forward reasoning, backward question, and backward reasoning, respectively. Note that $R_f, Q_b, R_b$ are all generated by the teacher model T. First, we generate forward reasoning $R_f$ by prompting T, and we only keep the samples that $R_f$ is leading to the correct answer, i.e., $g(R_f) = A$, where g is an answer extraction function. Then, we generate the backward question by conditioning on the original question Q and the ground truth answer A, using a detailed instruction $I_{bq}$ (see Appendix B): $Q_b = T(Q, A; I_{bq})$.\nAfter obtaining the backward question, we prompt the teacher model to generate the backward reasoning by answering the backward question: $R_b = T(Q_b)$. To filter inconsistent pairs (i.e., the backward reasoning is causing conflict with the original question), we prompt T with an instruction $I_{con}$ (see Appendix C) to check for the consistency: $c = T(Q, A, Q_b, R_b; I_{con})$, where $c \\in {0,1}$ represents whether the forward-backward reasoning is consistent. We filter out the data points that are not consistent, i.e., c = 0. That is, we augment D by prompting the teacher model to incorporate backward question and backward reasoning, and we only keep samples when (1) the forward reasoning is correct, and (2) the backward reasoning that is consistent with the question."}, {"title": "3.3 Learning Objectives", "content": "We train the student model S with the augmented dataset $D_{aug}$. To internalize the backward reasoning process, we use the following objectives:\n$\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} [\\ell(\\mathcal{S}(Q^{(i)}), R_{f}^{(i)}) + \\ell(\\mathcal{S}(Q^{(i)}), Q_{b}^{(i)})\n+ \\ell(\\mathcal{S}(Q_{b}^{(i)}), R_{b}^{(i)})]$\\tag{1}\nwhere $\\ell$ is the cross-entropy between the predicted and target tokens. Specifically, the objective function $\\mathcal{L}$ is composed of three losses that make full"}, {"title": "4 Experimental Setup", "content": "We use Gemini-1.5-Pro-001 (Reid et al., 2024) as the teacher model T, with Mistral-7B-Instruct-v0.3 and Gemma-7B-Instruct as the student model S. For training, we use LoRA fine-tuning (Hu et al., 2022) with rank 32. We use vllm (Kwon et al., 2023) and greedy decoding (with temperature = 0) for all baselines as well as our method. The student model is fine-tuned for 3 epochs on math reasoning tasks (MATH and GSM8K) and 10 epochs for all other domains. For Mistral-7B-Instruct-v0.3, we set the learning rate to 5e-6, while for Gemma-7B-Instruct, we use a learning rate of 2e-4. These configurations remain consistent across all baseline comparisons. We evaluate our method on a wide range of tasks: Commonsense reasoning: StrategyQA (SQA; Geva et al., 2021), CommonsenseQA (CSQA; Talmor et al., 2019), ARC-challenge (ARC; Clark et al., 2018). Math reasoning:"}, {"title": "5 Results and Analysis", "content": "5.1 Main Results\nWe present our main result in Table 1. First, REVTHINK demonstrates superior average performance, outperforming all baselines across datasets and models. Compared to the zero-shot performance of the student model, REVTHINK achieves an average improvement of 12.68% with Mistral and 14.37% with Gemma. Additionally, when compared to SKD and Distill Step-by-Step, which rely on supervised fine-tuning using the correct reasoning chains from the teacher model, REVTHINK shows substantial improvements of 6.44% to 7.15%. Compared to augmentation-based baselines, REVTHINK exhibits greater performance gains, particularly in the areas of commonsense reasoning, tabular reasoning and date understanding. While some of these augmentation methods, e.g., Answer Augmentation (AnsAug) are effective for math reasoning, they tend to show less improvements in other domains, suggesting that math, being a more structured domain, scales better with additional data (Li et al., 2024; Yuan et al., 2023). In contrast,"}, {"title": "5.2 Additional Analysis", "content": "REVTHINK exhibits sample efficiency. Having demonstrated that REVTHINK outperforms all baselines with the full training set, we now explore the performance of REVTHINK and the SKD baseline with varying portions of the training data, denoted by p \u2208 {0.1, 0.25, 0.5, 1.0}. For instance, when p = 0.1, we sample 10% of the correct forward reasoning for SKD fine-tuning and apply our data augmentation approach, as described in Section 3, for our fine-tuning. Results shown in Fig. 3 demonstrate that REVTHINK exhibits strong sample efficiency. Across multiple reasoning tasks, REVTHINK consistently outperforms SKD at all levels of p, even surpassing SKD at p = 1.0 with only 10% of the data on StrategyQA.\nBackward question generation boosts performance, but the full use of our dataset yields the best performance. Recall that each instance in our teacher data is a tuple (Q(i), R(i)f, Q(i)b, R(i)b),\nconsists of the original question, forward reasoning,"}, {"title": "6 Conclusion", "content": "We introduce REVTHINK, a framework that improves LLM by enabling backward reasoning. We propose an effective data augmentation method that generates well-structured forward-backward data from a teacher model, and we also propose an effective learning objective with auxiliary tasks that make full use of this augmented data. Experimental results not only show that REVTHINK is effective across 12 datasets on a wide range of tasks, but also reveal additional benefits, including sample efficiency, generalization, and the complementary strength to existing methods."}, {"title": "Limitations", "content": "Despite efforts to make state-of-the-art large language models (LLMs) safer and more trustworthy (Liu et al., 2023), the teacher model used in REVTHINK can still produce biased responses or reflect stereotypes embedded in its pre-training data. As a result, student models generated through distillation may inherit these undesirable traits, a challenge inherent to any distillation method. In other words, because student models learn from the teacher model, they remain vulnerable to producing similar biased outputs. Therefore, models created through REVTHINK distillation share the same risks of misuse as other LLM-derived methods. Further research is needed to effectively evaluate and mitigate these biases in LLMs."}, {"title": "A Additional Analysis", "content": "A.1 Verification improves data quality.\nIn Section 3.2, we note that our data augmentation consists of a verification stage conducted by the teacher model. In this ablation, we study the difference on downstream performance. Results in Table 5 show that removing verification cause a drop in performance, suggesting that verification improves data quality, even though the number of training samples might be reduced."}, {"title": "B Prompt for Backward Question Generation", "content": "Prompt for Backward Question Generation (Multiple Choice Problem)\nYour task is to generate an inverse question with {n} answer choices, based on the input question and its correct answer.\nFollow these rules:\n1. Use the correct answer from the input question to create a new, related but inverse question.\n2. Ensure that the {n} new answer choices are inversely correlated with the {n} input question's choices.\n3. Make sure only one answer choice in your generated question is correct and reasonable.\n4. The correct answer in your generated question must be present in the input question.\n5. The generated question and answer choices should be semantically different from the input question.\n{in_context_samples}\n{input_question}\nPrompt for Backward Question Generation (Math Reasoning Problem)\nYour task is to generate an inverse question, based on the input question and its correct answer.\nFollow these rules:\n1. Use the correct answer from the input question to create a new, related but inverse question.\n2. Make sure there exists only one correct answer in your generated question.\n3. The correct answer in your generated question must be present in the input question.\n4. The generated question should be semantically different from the input question.\n{in_context_samples}\n{input_question}\nThe prompt Ibq we use to generate backward questions. They are general templates, where we insert specific {in_context_samples} depending on the task. Below, we provide the in-context examples for each task."}, {"title": "C Prompt for Verification", "content": "Prompt for Verification (Consistency Check)\nYou will be given two question-answering pairs, (Q1, A1) and (Q2, A2).\nYour task is to check the consistency between Q1 and A2.\nIf (1) A2 can be found in Q1, and (2) A2 is correct, output 'True'.\nOtherwise, if Q1 and A2 is not related, or A2 is not correct, output 'False'.\n{in_context_samples}\n{input_question}\nThe prompt Icon used to verify the consistency c = T(Q, A, Qb, Rb; Icon) follows the structure outlined above. Similarly, it serves as a general template to be filled with in-context examples, as shown below."}, {"title": "D Examples of the Augmented Data", "content": "Example on StrategyQA\nOriginal Question (Q): Could the members of The Police perform lawful arrests?\nForward Reasoning (Rf): Here is the step-by-step reasoning:\n1. **The Police were a rock band:** Their members were musicians, not law enforcement officers.\n2. **Law enforcement officers have specific powers:** These powers are granted by the government and include the ability to make arrests.\n3. **Being in a band called \"The Police\" does not grant legal authority:** Band names are artistic choices and have no bearing on legal powers.\n**Therefore, the members of The Police could not perform lawful arrests.**\nThe answer is **no**\nBackward Question (Qb): If the members of The Police could not perform lawful arrests, then The Police are likely (A) musicians (B) police officers.\nBackward Reasoning (Rb): Here is the reasoning:\n1. The question implies there are two possible meanings for \"The Police\".\n2. One meaning refers to a group that can make lawful arrests. This is the definition of police officers.\n3. The other meaning refers to a group that does not have the authority to make lawful arrests.\n4. Since the question states this group cannot make lawful arrests, they cannot be police officers.\n5. Therefore, \"The Police\" in this context likely refers to a group known for something other than law enforcement, making them likely musicians.\nThe answer is (A)"}, {"title": "E Dataset Statistics", "content": ""}]}