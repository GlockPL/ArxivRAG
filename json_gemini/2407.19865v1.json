{"title": "Imitation Learning for Intra-Day Power Grid Operation through Topology Actions", "authors": ["Matthijs de Jong", "Jan Viebahn", "Yuliya Shapovalova"], "abstract": "Power grid operation is becoming increasingly complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. In this paper we study the performance of imitation learning for day-ahead power grid operation through topology actions. In particular, we consider two rule-based expert agents: a greedy agent and a N-1 agent. While the latter is more computationally expensive since it takes N-1 safety considerations into account, it exhibits a much higher operational performance. We train a fully-connected neural network (FCNN) on expert state-action pairs and evaluate it in two ways. First, we find that classification accuracy is limited despite extensive hyperparameter tuning, due to class imbalance and class overlap. Second, as a power system agent, the FCNN performs only slightly worse than expert agents. Furthermore, hybrid agents, which incorporate minimal additional simulations, match expert agents' performance with significantly lower computational cost. Consequently, imitation learning shows promise for developing fast, high-performing power grid agents, motivating its further exploration in future L2RPN studies.", "sections": [{"title": "Introduction", "content": "The energy transition is a crucial development for ensuring the sustainability and future security of society. Transmission system operators (TSOs) play a crucial role in this transition and are consequently faced with new challenges. Consumption pattern changes, such as those caused by electrical vehicles, are rapidly increasing the energy demand. In the Netherlands, the energy consumption is expected to grow thirty percent between 2024 and 2033; the projections are similar for other European countries [15]. Simultaneously, renewable energy comes from sources that are often uncontrollable and more variable. The percentage of energy from controllable sources is expected to fall by approximately forty by 2033 in the Netherlands [15].\nCongestion management is one of these challenges [17]. Exploiting the flexibility in the network topology presents an opportunity for remedying grid congestion [16]. However, the topology space is combinatorially large, and hence too difficult to explore by humans and too computationally expensive for traditional computational methods [17].\nRecent advancements in machine learning (ML) present new opportunities for grid topology control [17]. By being exposed to a vast number of grid states and topologies, machine learning models can learn successful combinations between them. Moreover, machine learning provide greater computational efficiency compared to traditional algorithmic paradigms, which is essential for solving complex problems like topology control. Operating the power grid is a sequential decision making problem, for which the two main ML paradigms are imitation learning (IL) and reinforcement learning (RL). With IL, a model learns from the state-action pairs from an expert agent, whereas with RL, the agent learns from directly interacting with the environment [2]. So far, most research has focused on RL methods for grid topology control [18,11,19,14]. RL methods have the benefit that they can learn behavior beyond what the expert agents exhibit. IL methods have only been applied shallowly for pre-training models for RL, which helps RL agents learn complex behavior [4,1,6]. No research has investigated IL as the main method for developing topology control agents, despite the fact that it has certain benefits over RL. Training with IL is more simple and efficient, and because the model learns to imitate expert agents, there is less uncertainty about the model's actions.\nIn this work, we investigate rule-based expert agents and ML agents that are based on IL. We consider two rule-based agents: a greedy agent and a N-1 agent that aims to satisfy N-1 redundancy. We specifically look at grid control in an intra-day setting, instead of the more commonly used but less realistic intra-month period. We also consider various regimes of line outages. We perform an extensive hyperparameter tuning and error analysis to the ML models, and apply them to the grid directly or as hybrid agents. This research aims to investigate the merit that imitation learning and hybrid agents have at operating the grid.\nThe remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 describes the power grid setup and experimental setting. Section 4 describes the rule based agents as well as the results relating to them. Section 5 describes the IL agents and the related results. Section 6 provides discussion and a summary, and section 7 directions for future research. Additionally, Table 1 lists the symbols and notation used throughout the paper. The code for this project is shared on Github 3."}, {"title": "Power Grid Setup", "content": "We employ the IEEE 14-bus system using the rte_case14_realistic environment of the Python package Grid2Op5. The power grid is shown in Figure 1 and includes fourteen substations, five generators, eleven loads, and twenty power lines. The generators consist of one solar, one nuclear, one wind-based, and two thermal generators. The grid is divided into two sides: the high-voltage transmission side containing substations 0 to 4, and the low-voltage distribution side containing substations 5 to 13. Lines 15 to 19 model the transformers connecting these sides. We adjust the thermal limits of the lines to make differences between the transmission and distribution parts more pronounced and realistic, as described by Subramanian et al. [14].\nElectric current flows through power lines. Each line has a thermal limit, which the load cannot exceed for an extended period of time, lest the line fails and is disabled. The ratio of the current to the thermal limit of a line is called its loading, denoted by \\( \\rho \\) (see Table 1). Electricity previously flowing through a failed line finds a new path through the network, increasing the load of the power lines on that path. That increases the risk of those lines failing, potentially causing a cascade of failing lines. Cascading failures can render parts of or the entire power grid nonfunctional. The aim of topology control is to avoid such failure by redirecting current flow. Power networks are also designed to avoid cascading failures by satisfying N-1 redundancy: the property that the network remains stable in the presence of the failure of any single line. We investigate the ability of agents to prepare for, and control during, line outages. To that end, we also consider variations of the network with a single line disabled, which we call N-1 networks."}, {"title": "Action space", "content": "In this study, we focus on topological remedial actions. Topology control is possible because each substation contains two busbars to which grid objects (these are generators, lines, and line endpoints) can be attached. Electricity flows between objects attached to the same busbar and between the two endpoints of a power line, but not between objects on different busbars within the same substation. As such, reconfiguring the object-busbar attachments allows control over the flow of electricity in the network. In Grid2Op, the topology vector variable indicates at which busbar of its substation the object is attached. Values of 1, 2, and -1 represent attachment to the first and second busbar, and disablement, respectively.\nIn Grid2Op, topology control actions correspond to changing object-busbar attachments through either absolute set-actions or relative switch-actions. Grid2Op introduces real-world constraints by allowing object-busbar changes at only a single substation per timestep. Certain configurations of object-busbar attachments at substations are either invalid or redundant. For instance, actions that isolate generators or loads are considered invalid in Grid2op. Similarly, configurations mirrored w.r.t. busbar attachments are redundant. We use the approach by Subramanian et al. [14] to calculate the space of valid, unique set-actions (i.e., substation configurations). Note that as set-actions are represented absolutely, set-actions might set the busbar configurations to states they are currently in, effectively being do-nothing actions. We call these implicit do-nothing actions, in contrast to the explicit do-nothing actions described in \u00a74.1."}, {"title": "Intra-day Scope & Regimes", "content": "The environment features one thousand scenarios, each consisting of 8064 timesteps separated by intervals of five minutes, i.e. 28 days. Each scenario features different generation and load profiles. Because of the intra-day setup, we split the scenarios into individual days. Between days, the topology is reset to the default topology, in which all enabled objects are attached to the first busbar. Besides realism, this has two benefits: (1) shortening the duration of runs increases the amount of usable data for imitation learning, as data from runs where the expert agent fails cannot be used, and (2) this emulates topology reversal, which has been found to be beneficial for operating power grids [6]. A game-over occurs when the power grid fails to transport sufficient power from generators to loads due to line failures. To avoid circular analysis, we split the scenarios into 70/10/20% train/validation/test sets. We excluded three scenarios where Grid2Op failed to converge the power flow without agent misoperation. These scenarios have been omitted throughout the research.\nWe consider three different regimes which represent different difficulty levels for grid operation. The full-network regime considers the full network, i.e. without outages. In the planned-outage regime, a single line is disabled for the entire day. The unplanned-outage regime introduces an opponent that disables lines randomly. In specifying the opponent we follow Blazej et al. [7]. The opponent disables a uniformly sampled line for four hours twice a day. We use lines 0, 2, 4, 5, 6, and 12 as the lines that the opponent can disable to match the disabled lines in the dataset (read \u00a75.1). We include a cooldown period, so that the outages are separated by minimally an hour."}, {"title": "Rule-based Expert Agents", "content": "The greedy agent simulates each action and selects the action that minimizes the maximum loading \\( \\rho^{max} \\). To avoid hyperactive behavior, we introduce an activity threshold \\( \\eta \\) [14], which we set to 0.97. The agent selects an explicit do-nothing action if the maximum loading does not exceed this threshold. Algorithm 1 presents the pseudocode for the greedy agent."}, {"title": "N-1 Agent", "content": "The second rule-based agent aims to achieve grid configurations that satisfy N-1 redundancy, i.e. grid configurations robust to the disablement of single lines. This agent quantifies the N-1 risk of the resulting topologies and selects the action that minimizes this risk.\nN-1 risk is quantified by the maximum N-1 loading, denoted \\( \\rho^{N-1} \\). This is calculated as the maximum of the maximum loadings, \\( \\rho^{max} \\), among the N-1 networks. Evaluation of the greedy agent on the N-1 networks showed that the network became almost entirely inoperable when certain lines were disabled (see \u00a74.3). Therefore, we only consider the disablement of a subset of lines, \\( L^{disable} \\subset L \\). The excluded lines are annotated with red or pink in Figure 1. Thus, the maximum N-1 loading for the topology resulting from simulating a particular action a is given by\n\\[\\rho^{N-1}(a) = max_{l \\in L^{disable}}(\\rho^{max|a, l})\\]\nwhere \\( (\\rho^{max|a, l}) \\) is the maximum loading resulting from simulating action a and the disablement of line l. In case of a game-over, we consider \\( \\rho^{max} = \\infty \\). Therefore, if disabling one of the lines leads to an immediate game-over, \\( \\rho^{N-1} = \\infty \\). Algorithm 2 presents the pseudocode for the N-1 agent. The policy of the N-1 agent consists of three steps:\n1. Activity check: Similarly to the greedy agent, if the maximum loading does not exceed the inactivity threshold (\\( \\rho^{max} < \\eta \\)), an explicit do-nothing action is selected.\n2. Minimizing N-1 risk: Above that threshold, the agent simulates each action and considers the subset of actions \\( A^{secure} \\subset A \\) that do not put the simulated network at risk:\n\\[A^{secure} = \\{a \\in A | (\\rho^{max|a}) < \\theta\\},\\]\nwhere \\( \\theta = 1.0 \\) is the risk threshold parameter. Among \\( A^{secure} \\), the action with the lowest maximum N-1 loading \\( \\rho^{N-1} \\) is found:\n\\[a^{best} = argmin_{a \\in A^{secure}} \\rho^{N-1}(a).\\]\nIf the lowest finite maximum N-1 loading is finite, i.e. \\( \\rho^{N-1}(a^{best}) < \\infty \\), then the corresponding action is selected.\n3. Fallback to greedy behavior: If either \\( A^{secure} \\) is empty or the lowest maximum N-1 loading is non-finite, then an action is taken greedily instead.\nOur algorithm differs from the one used by Lehna et al. [6] through the addition of the pre-selection for secure actions and the greedy-fallback mechanism. In the unplanned-outage regime, the N-1 agent is replaced by the Greedy agent during unplanned outages."}, {"title": "Results of the Rule-based Agents", "content": "Full-Network Regime. For the generation of data, both rule-based agents were applied to all scenarios of the full-network regime."}, {"title": "Imitation Learning Agents", "content": "We create a dataset with the data from the N-1 agent (from the full-network regime) and data from the Greedy agent in the N-1 networks with lines 0, 2, 4, 5, 6, and 12 disabled. We selected the lines that belong to the first cluster of N-1 networks as the N-1 agent performed well on these (see \u00a74.3). We combine the data from the Greedy agent and the N-1 agent with the aim to train a ML agent that is capable of performing well in both the full and N-1 networks. Explicit do-nothing data points and data points from failed days are excluded. The data is split into train/validation/test sets based on the scenario they belong to (see \u00a73.2). The sets have 196,477/26,228/59,150 data points each.\nEach data point contains the features per object, the topology vector, and the expert action. The features per object type are listed in Table 3. Object features are normalized, flattened into a vector, and concatenated with the topology vector. Features of the endpoints of disabled lines are zero-imputed. The corresponding value in the topology vector is -1. Actions are transformed from a set-action format into a switch-action format. As such, actions are represented by a vector with a length of the topology vectors, whose elements are either value zero or one, indicating whether the corresponding object is switched between busbars. This produces a multi-label binary classification task."}, {"title": "Imitation Learning", "content": "We use a simple fully-connected neural network (FCNN) as the ML model. The FCNN consists of an input layer, several hidden layers, and an output layer. The output layer uses a sigmoid activation function to ensure the output is in the (0, 1) range. The hidden layers use the leaky ReLU activation function. The weights are initialized according to a normal distribution, with the standard deviation as a tuned hyperparameter. We use the Adam optimizer to minimize a label-weighted binary cross-entropy loss, defined as:\n\\[L=mean(-w(y log(p) + (1 \u2013 y) log(1 \u2013 p))),\\]\nwhere p, y and w represent the prediction, target, and the label weight vectors respectively. We introduced label weights because the sparsity of labels with value 1 (since only a few objects switch per action) caused the predictions to collapse to only zeros. The label weights assign lower weights to labels that do not belong to the target or predicted substation:\n\\[w_i = \\begin{cases} 1 & \\text{object i belongs to the target substation} \\\\  1 & \\text{object i belongs to the predicted substation} \\ w_i \\in W \\\\  \\alpha & \\text{otherwise} \\end{cases}\\]\nwhere \\( \\alpha \\) is the label weight hyperparameter. Note that there is potentially no target or no predicted substation, due to do-nothing actions. The target substation, if any, depends on the data point's action. There is no target substation if the action was an implicit do-nothing action. The predicted action is considered a do-nothing action, without a predicted substation, if all predictions \\( p_i \\in \\textbf{p} \\) do not exceed 0.5. Otherwise, the predicted substation is the substation where the predictions \\( p_i \\), at that substation \\( s \\in S \\) maximize\n\\[\\Sigma_{p_i \\in \\textbf{p}_s} max(p_i \u2013 0.5, 0).\\]\nEach value in the output vector represents whether to switch the corresponding object. However, not every output vector corresponds to a valid action. We apply a postprocessing step where the model prediction p is replaced by the valid switch-action nearest to p. This postprocessing step is applied during validation, testing, and inference, but not during training.\nHyperparameter tuning was performed through multiple iterations of hyperparameter sweeps. Each sweep narrowed down the hyperparameter ranges and trained with more epochs and less strict early stopping. The hyperparameter sweeps used random search with Hyperband early termination (distinct from early stopping), which terminates unpromising runs early. Table 4 displays the hyperparameter ranges in the final sweep and the selected final values. The weight decay was set to zero after observing that the best runs had values near the lower limit; subsequent runs with a weight decay of zero performed even better. We experimented with various values of the label weight \\( \\alpha \\) when we introduced label weighting, but did not include it in the hyperparameter sweeps. The LReLU negative slope parameter was not tuned.\nFive final models were trained, with different weight initializations. Each training run lasted one hundred epochs, unless it was stopped early. Runs were stopped early if the maximum validation accuracy had not increased the last twenty evaluations. The validation accuracy was calculated every 250,000 iterations. The training curves are displayed in Figure 4."}, {"title": "Imitation Learning Agents", "content": "The obtained FCNN models can be applied to environment directly, but we are also interested in the performance of agents that combine ML and simulation capabilities. We evaluate four agents that use the ML model.\nThe Naive agent executes the predicted action directly. We observe that the Naive agent occasionally failed days by predicting a singular bad action. The Verify agent addresses this by verifying predicted action with simulation. If the simulated maximum loading of the action exceeds the current max loading and the risk threshold parameter \\( \\theta = 1 \\), the action is replaced by a do-nothing action. We also consider hybrid agents. The Verify+Greedy agent functions as the verify agent as long as the maximum loading is below the risk threshold and as the Greedy agent otherwise. The Verify+N-1 agent functions identically, except with N-1 agent instead of the Greedy agent. Each of these agents uses the activity threshold \\( \\eta = 0.97 \\) for active behavior."}, {"title": "Results", "content": "For the five models, the accuracies on the training, validation, and test sets were 79.1% \u00b11.5, 78.6% \u00b10.6, and 76.4% \u00b10.6, respectively. The action postprocessing step had a sizeable effect on accuracy: the validation accuracy without this step decreased from 78.6% \u00b10.6 to 76.2% \u00b10.7 (see Figure 4).\nThe accuracies are limited to approximately 80%. The class imbalance seems to contribute to the limited accuracy, as indicated by the negative relation between class rank and accuracy shown in Figure 2a. Figure 2b indicates that the model predicts infrequent classes disprortionally infrequently. However, the class imbalance alone seems insufficient to explain the limited accuracy, as shown by the high variance in accuracy for actions with similar frequencies, in Figure 2a. Additionally, the accuracy is limited even on frequent classes: the accuracy on the fourth and fifth most frequent classes is 82.3% and 61.1% respectively.\nClass overlap also appears to contribute to the limited accuracy. Per (N-1) network, we found the most frequently confused pairs of classes and performed a principal components analysis on the features. We also transformed the actions back from a switch-action format to a set-action format. In Figure 5, we plot the data points of the confused classes projected on the first two principal components, highlighting which data points are confused. As is visible, the confusions occur exactly where the classes overlap (red points occur at the intersection of the blue and green points). The pattern is consistent across (N-1) networks and pairs of confused classes. An inspection of the nearest neighbors supports the notion that the class overlap causes the low accuracy. For a subset of 2500 validation data points, the accuracy over the data points whose nearest neighbors is in the same class is 92.97%, but only 44.44% for those whose nearest neighbor is in another class.\nTable 2 shows the resulting performances of the agents on the different environmental regimes (explained in \u00a73.2). A partial ordering holds true in all settings: Do-nothing < Naive < Verify < Verify + Greedy < Verify + N-1. Among the ML agents, performance increases with more simulation in every setting (rows 5 to 8). In every setting, the Verify agent improves upon the Naive agent by one to three percent (rows 5 and 6), whereas the improvements by the hybrid agents are more variable (rows 6, 7, and 8). In the full-topology regime, the hybrid agents approach the performance of the N-1 agent (column 2). Moreover, in the planned-outage and unplanned-outage regimes, hybrid agents fully achieve the performance of both rule-based agents (columns 3 to 6).\nFinally, we investigated the inference speed of the different agents. Figure 6 plots the performance and inference duration of each agent (values listed in Table 2, column 6). The mean inference speed of the IL agents is orders of magnitude smaller than those of the rule-based agents, even for the hybrid agents. Consequently, the hybrid agents can achieve performance similar to the rule-based agents at a much higher inference speed."}, {"title": "Summary & Discussion", "content": "Topology control is a underutilized opportunity for managing congestion in power grids. Imitation learning is a promising technique for topology control, but its application has not been studied in detail. In this research, we train FCNN models on the state-action pairs of two rule-based expert agents, and consequently apply these model to the power grid as agents.\nWe find that the rule-based agents, particularly the N-1 agent, can effectively control the grid in an intra-day setting without outages. The performance of both agents decreases in the unplanned-outage regime, but far more so for the greedy agent. We also find that the N-1 agent performs longer action sequences.\nWe successfully trained FCNN models with imitation learning. We introduced an action post-processing step that contributed to a small increase in accuracy. However, the accuracies remain limited due to an imbalanced class distribution and class overlap. We hypothesize that the class overlap results from usage of the Grid2Op simulate function by the agents. The simulate function simulates the next timestep with generator/load forecasts predetermined by the scenario. It's possible that (near-) identical network states have diverging forecast that lead to the selection of different actions.\nThe purely ML agents are adept as operating the grids in the different regimes, although the hybrid agents do outperform them. This is in line with the findings of the L2RPN 2020 submission analysis [9]. The most unexpected result is that the hybrid agents outperform the rule-based agents in the planned-outage and unplanned-outage regimes, which cannot be explained through an IL lens. We hypothesize that the hybrid models can find more robust topologies than the rule-based models due to model bias against rare actions. As discussed in section \u00a74.3, the action distribution is highly imbalanced. The few dominant actions are selected often by the rule-based agents, indicating that these actions result in optimal topologies across many grid states. Their optimality in many grid states indicates their robustness. In contrast, rare actions are optimal in only few grid states, potentially making them over-specialized. Figure 2b shows that the model predicts rare classes disproportionally infrequent. This bias towards more frequent actions might consequently bias the model towards more robust, and consequently, more beneficial, topologies.\nThe ML and hybrid agents have good combinations of performance and inference speed. All ML and hybrid agents represent a different tradeoff between performance and speed. In practice, the optimal agent choice depends on the available resources and performance requirements."}, {"title": "Future Work", "content": "Various directions stand out as promising for future research. There is as yet little knowledge about the best practices for creating a dataset or training a model with IL for topology control. Future research might shed light on such best practices. The class overlap present in our dataset might be avoided by including the forecasted production and consumption in the features. The developed IL methods must be compared against RL methods.\nSimple behavioral cloning, as researched here, can result in distribution shift: a compounding divergence between the behavior of the expert and student agent [12]. This might have been exasperated in the unplanned-outage setting, where agents were evaluated on a regime not included in the dataset. It seems promising to apply more advanced imitation learning frameworks. DAgger, for instance, mitigates the aforementioned distribution shift problem [12]. An in-depth analysis of the behavior of rule-based, purely ML, and hybrid agents would be interesting, as it might clarify what behavior leads to good performance.\nThere remains a large divergence between environments studied in L2RPN competitions and real power grids. Real grids are larger, more complex (e.g. including other grid objects and varying number of busbars between substations), and permit other types of actions. They are also frequently subject to changes, which means that models should generalize to unseen networks. Research is required to integrate these complications with existing methods. Graph neural networks seem promising w.r.t. generalizability."}]}