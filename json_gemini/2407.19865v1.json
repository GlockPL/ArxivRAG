{"title": "Imitation Learning for Intra-Day Power Grid Operation through Topology Actions", "authors": ["Matthijs de Jong", "Jan Viebahn", "Yuliya Shapovalova"], "abstract": "Power grid operation is becoming increasingly complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. In this paper we study the performance of imitation learning for day-ahead power grid operation through topology actions. In particular, we consider two rule-based expert agents: a greedy agent and a N-1 agent. While the latter is more computationally expensive since it takes N-1 safety considerations into account, it exhibits a much higher operational performance. We train a fully-connected neural network (FCNN) on expert state-action pairs and evaluate it in two ways. First, we find that classification accuracy is limited despite extensive hyperparameter tuning, due to class imbalance and class overlap. Second, as a power system agent, the FCNN performs only slightly worse than expert agents. Furthermore, hybrid agents, which incorporate minimal additional simulations, match expert agents' performance with significantly lower computational cost. Consequently, imitation learning shows promise for developing fast, high-performing power grid agents, motivating its further exploration in future L2RPN studies.", "sections": [{"title": "Introduction", "content": "The energy transition is a crucial development for ensuring the sustainability and future security of society. Transmission system operators (TSOs) play a crucial role in this transition and are consequently faced with new challenges. Consumption pattern changes, such as those caused by electrical vehicles, are rapidly increasing the energy demand. In the Netherlands, the energy consumption is expected to grow thirty percent between 2024 and 2033; the projections are similar for other European countries [15]. Simultaneously, renewable energy comes from sources that are often uncontrollable and more variable. The percentage of energy from controllable sources is expected to fall by approximately forty by 2033 in the Netherlands [15].\nCongestion management is one of these challenges [17]. Exploiting the flexibility in the network topology presents an opportunity for remedying grid congestion [16]. However, the topology space is combinatorially large, and hence too difficult to explore by humans and too computationally expensive for traditional computational methods [17].\nRecent advancements in machine learning (ML) present new opportunities for grid topology control [17]. By being exposed to a vast number of grid states and topologies, machine learning models can learn successful combinations between them. Moreover, machine learning provide greater computational efficiency compared to traditional algorithmic paradigms, which is essential for solving complex problems like topology control. Operating the power grid is a sequential decision making problem, for which the two main ML paradigms are imitation learning (IL) and reinforcement learning (RL). With IL, a model learns from the state-action pairs from an expert agent, whereas with RL, the agent learns from directly interacting with the environment [2]. So far, most research has focused on RL methods for grid topology control [18,11,19,14]. RL methods have the benefit that they can learn behavior beyond what the expert agents exhibit. IL methods have only been applied shallowly for pre-training models for RL, which helps RL agents learn complex behavior [4,1,6]. No research has investigated IL as the main method for developing topology control agents, despite the fact that it has certain benefits over RL. Training with IL is more simple and efficient, and because the model learns to imitate expert agents, there is less uncertainty about the model's actions.\nIn this work, we investigate rule-based expert agents and ML agents that are based on IL. We consider two rule-based agents: a greedy agent and a N-1 agent that aims to satisfy N-1 redundancy. We specifically look at grid control in an intra-day setting, instead of the more commonly used but less realistic intra-month period. We also consider various regimes of line outages. We perform an extensive hyperparameter tuning and error analysis to the ML models, and apply them to the grid directly or as hybrid agents. This research aims to investigate the merit that imitation learning and hybrid agents have at operating the grid.\nThe remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 describes the power grid setup and experimental setting. Section 4 describes the rule based agents as well as the results relating to them. Section 5 describes the IL agents and the related results. Section 6 provides discussion and a summary, and section 7 directions for future research. Additionally, Table 1 lists the symbols and notation used throughout the paper. The code for this project is shared on Github 3."}, {"title": "Power Grid Setup", "content": "We employ the IEEE 14-bus system using the rte_case14_realistic environment of the Python package Grid2Op5. The power grid is shown in Figure 1 and includes fourteen substations, five generators, eleven loads, and twenty power lines. The generators consist of one solar, one nuclear, one wind-based, and two thermal generators. The grid is divided into two sides: the high-voltage transmission side containing substations 0 to 4, and the low-voltage distribution side containing substations 5 to 13. Lines 15 to 19 model the transformers connecting these sides. We adjust the thermal limits of the lines to make differences between the transmission and distribution parts more pronounced and realistic, as described by Subramanian et al. [14].\nElectric current flows through power lines. Each line has a thermal limit, which the load cannot exceed for an extended period of time, lest the line fails and is disabled. The ratio of the current to the thermal limit of a line is called its loading, denoted by $\\rho$ (see Table 1). Electricity previously flowing through a failed line finds a new path through the network, increasing the load of the power lines on that path. That increases the risk of those lines failing, potentially causing a cascade of failing lines. Cascading failures can render parts of or the entire power grid nonfunctional. The aim of topology control is to avoid such failure by redirecting current flow. Power networks are also designed to avoid cascading failures by satisfying N-1 redundancy: the property that the network remains stable in the presence of the failure of any single line. We investigate the ability of agents to prepare for, and control during, line outages. To that end, we also consider variations of the network with a single line disabled, which we call N-1 networks."}, {"title": "Action space", "content": "In this study, we focus on topological remedial actions. Topology control is possible because each substation contains two busbars to which grid objects (these are generators, lines, and line endpoints) can be attached. Electricity flows between objects attached to the same busbar and between the two endpoints of a power line, but not between objects on different busbars within the same substation. As such, reconfiguring the object-busbar attachments allows control over the flow of electricity in the network. In Grid2Op, the topology vector variable indicates at which busbar of its substation the object is attached. Values of 1, 2, and -1 represent attachment to the first and second busbar, and disablement, respectively.\nIn Grid2Op, topology control actions correspond to changing object-busbar attachments through either absolute set-actions or relative switch-actions. Grid2Op introduces real-world constraints by allowing object-busbar changes at only a single substation per timestep. Certain configurations of object-busbar attachments at substations are either invalid or redundant. For instance, actions that isolate generators or loads are considered invalid in Grid2op. Similarly, configurations mirrored w.r.t. busbar attachments are redundant. We use the approach by Subramanian et al. [14] to calculate the space of valid, unique set-actions (i.e., substation configurations). Note that as set-actions are represented absolutely, set-actions might set the busbar configurations to states they are currently in, effectively being do-nothing actions. We call these implicit do-nothing actions, in contrast to the explicit do-nothing actions described in \u00a74.1."}, {"title": "Intra-day Scope & Regimes", "content": "The environment features one thousand scenarios, each consisting of 8064 timesteps separated by intervals of five minutes, i.e. 28 days. Each scenario features different generation and load profiles. Because of the intra-day setup, we split the scenarios into individual days. Between days, the topology is reset to the default topology, in which all enabled objects are attached to the first busbar. Besides realism, this has two benefits: (1) shortening the duration of runs increases the amount of usable data for imitation learning, as data from runs where the expert agent fails cannot be used, and (2) this emulates topology reversal, which has been found to be beneficial for operating power grids [6]. A game-over occurs when the power grid fails to transport sufficient power from generators to loads due to line failures. To avoid circular analysis, we split the scenarios into 70/10/20% train/validation/test sets. We excluded three scenarios where Grid2Op failed to converge the power flow without agent misoperation. These scenarios have been omitted throughout the research.\nWe consider three different regimes which represent different difficulty levels for grid operation. The full-network regime considers the full network, i.e. without outages. In the planned-outage regime, a single line is disabled for the entire day. The unplanned-outage regime introduces an opponent that disables lines randomly. In specifying the opponent we follow Blazej et al. [7]. The opponent disables a uniformly sampled line for four hours twice a day. We use lines 0, 2, 4, 5, 6, and 12 as the lines that the opponent can disable to match the disabled lines in the dataset (read \u00a75.1). We include a cooldown period, so that the outages are separated by minimally an hour."}, {"title": "Rule-based Expert Agents", "content": "The greedy agent simulates each action and selects the action that minimizes the maximum loading $\\rho^{max}$. To avoid hyperactive behavior, we introduce an activity threshold $\\eta$ [14], which we set to 0.97. The agent selects an explicit do-nothing action if the maximum loading does not exceed this threshold. Algorithm 1 presents the pseudocode for the greedy agent."}, {"title": "N-1 Agent", "content": "The second rule-based agent aims to achieve grid configurations that satisfy N-1 redundancy, i.e. grid configurations robust to the disablement of single lines. This agent quantifies the N-1 risk of the resulting topologies and selects the action that minimizes this risk.\nN-1 risk is quantified by the maximum N-1 loading, denoted $\\rho^{N-1}$. This is calculated as the maximum of the maximum loadings, $\\rho^{max}$, among the N-1 networks. Evaluation of the greedy agent on the N-1 networks showed that the network became almost entirely inoperable when certain lines were disabled (see \u00a74.3. Therefore, we only consider the disablement of a subset of lines, $\\mathcal{L}_{disable} \\subset \\mathcal{L}$. The excluded lines are annotated with red or pink in Figure 1. Thus, the maximum N-1 loading for the topology resulting from simulating a particular action $a$ is given by\n$\\rho^{N-1}(a) = \\max_{l \\in \\mathcal{L}_{disable}} (\\rho^{max}|a, l)$\nwhere $(\\rho^{max}|a, l)$ is the maximum loading resulting from simulating action $a$ and the disablement of line $l$. In case of a game-over, we consider $\\rho^{max} = \\infty$. Therefore, if disabling one of the lines leads to an immediate game-over, $\\rho^{N-1} = \\infty$. Algorithm 2 presents the pseudocode for the N-1 agent. The policy of the N-1 agent consists of three steps:\n1. Activity check: Similarly to the greedy agent, if the maximum loading does not exceed the inactivity threshold ($\\rho^{max} < \\eta$), an explicit do-nothing action is selected.\n2. Minimizing N-1 risk: Above that threshold, the agent simulates each action and considers the subset of actions $\\mathcal{A}_{secure} \\subset \\mathcal{A}$ that do not put the simulated network at risk:\n$\\mathcal{A}_{secure} = \\{a \\in \\mathcal{A}|(\\rho^{max}|a) < \\theta\\}$,\nwhere $\\theta = 1.0$ is the risk threshold parameter. Among $\\mathcal{A}_{secure}$, the action with the lowest maximum N-1 loading $\\rho^{N-1}$ is found:\n$a_{best} = \\arg\\min_{a \\in \\mathcal{A}_{secure}} \\rho^{N-1}(a)$.\nIf the lowest finite maximum N-1 loading is finite, i.e. $\\rho^{N-1}(a_{best}) < \\infty$, then the corresponding action is selected.\n3. Fallback to greedy behavior: If either $\\mathcal{A}_{secure}$ is empty or the lowest maximum N-1 loading is non-finite, then an action is taken greedily instead.\nOur algorithm differs from the one used by Lehna et al. [6] through the addition of the pre-selection for secure actions and the greedy-fallback mechanism. In the unplanned-outage regime, the N-1 agent is replaced by the Greedy agent during unplanned outages."}, {"title": "Results of the Rule-based Agents", "content": "Full-Network Regime. For the generation of data, both rule-based agents were applied to all scenarios of the full-network regime. planned-Outage Regime. In the planned-outage regime, only the Greedy agent was applied, as the N-1 agent was designed to operate without outages present. For data generation, the Greedy agent was applied to all scenarios of the planned-outage regime.\nThe performance of the Greedy agent depends greatly on which line is disabled, as described in Figure 1. Two clusters emerge in the N-1 networks. This is apparent in a comparison of the different action distributions in different N-1 networks. The first cluster consists of the N-1 networks with lines disabled in the transmission side of the grid and can be operated well by the Greedy agent (>96% of days completed).\nThe second cluster concerns the lines that act as transformers and line 13, and is more difficult to operate by the Greedy agent (68-97% of days completed).\nUnplanned-Outage Regime. As expected, the N-1 agent performs much better (92.39% of days completed) than the greedy agent (81.79% completed)."}, {"title": "Imitation Learning Agents", "content": "We create a dataset with the data from the N-1 agent (from the full-network regime) and data from the Greedy agent in the N-1 networks with lines 0, 2, 4, 5, 6, and 12 disabled. We selected the lines that belong to the first cluster of N-1 networks as the N-1 agent performed well on these (see \u00a74.3). We combine the data from the Greedy agent and the N-1 agent with the aim to train a ML agent that is capable of performing well in both the full and N-1 networks. Explicit do-nothing data points and data points from failed days are excluded. The data is split into train/validation/test sets based on the scenario they belong to (see \u00a73.2). The sets have 196,477/26,228/59,150 data points each.\nEach data point contains the features per object, the topology vector, and the expert action. Actions are transformed from a set-action format into a switch-action format. As such, actions are represented by a vector with a length of the topology vectors, whose elements are either value zero or one, indicating whether the corresponding object is switched between busbars. This produces a multi-label binary classification task."}, {"title": "Imitation Learning", "content": "Setup. We use a simple fully-connected neural network (FCNN) as the ML model. The FCNN consists of an input layer, several hidden layers, and an output layer. The output layer uses a sigmoid activation function to ensure the output is in the (0, 1) range. The hidden layers use the leaky ReLU activation function. The weights are initialized according to a normal distribution, with the standard deviation as a tuned hyperparameter. We use the Adam optimizer to minimize a label-weighted binary cross-entropy loss, defined as:\n$\\mathcal{L}=\\text{mean}(-w(y \\log(p) + (1 - y) \\log(1 \u2013 p)))$,\nwhere $p$, $y$ and $w$ represent the prediction, target, and the label weight vectors respectively. We introduced label weights because the sparsity of labels with value 1 (since only a few objects switch per action) caused the predictions to collapse to only zeros. The label weights assign lower weights to labels that do not belong to the target or predicted substation:\n$w_i = \\begin{cases} 1 & \\text{object } i \\text{ belongs to the target substation} \\\\ 1 & \\text{object } i \\text{ belongs to the predicted substation} \\\\ \\alpha & \\text{otherwise} \\end{cases} \\qquad w \\in W$\nwhere $\\alpha$ is the label weight hyperparameter. Note that there is potentially no target or no predicted substation, due to do-nothing actions. The target substation, if any, depends on the data point's action."}, {"title": "Results", "content": "IL Model. For the five models, the accuracies on the training, validation, and test sets were 79.1% \u00b11.5, 78.6% \u00b10.6, and 76.4% \u00b10.6, respectively. The action postprocessing step had a sizeable effect on accuracy: the validation accuracy without this step decreased from 78.6% \u00b10.6 to 76.2% \u00b10.7.\nThe accuracies are limited to approximately 80%. The class imbalance seems to contribute to the limited accuracy. However, the class imbalance alone seems insufficient to explain the limited accuracy. Additionally, the accuracy is limited even on frequent classes.\nClass overlap also appears to contribute to the limited accuracy.\nAnalysis of IL Agents. Table 2 shows the resulting performances of the agents on the different environmental regimes (explained in \u00a73.2). A partial ordering holds true in all settings: Do-nothing < Naive < Verify < Verify + Greedy < Verify + N-1. Among the ML agents, performance increases with more simulation in every setting . Moreover, in the planned-outage and unplanned-outage regimes, hybrid agents fully achieve the performance of both rule-based agents.\nFinally, we investigated the inference speed of the different agents . Consequently, the hybrid agents can achieve performance similar to the rule-based agents at a much higher inference speed."}, {"title": "Summary & Discussion", "content": "Topology control is a underutilized opportunity for managing congestion in power grids. Imitation learning is a promising technique for topology control, but its application has not been studied in detail. In this research, we train FCNN models on the state-action pairs of two rule-based expert agents, and consequently apply these model to the power grid as agents.\nWe find that the rule-based agents, particularly the N-1 agent, can effectively control the grid in an intra-day setting without outages. We also find that the N-1 agent performs longer action sequences.\nWe successfully trained FCNN models with imitation learning. We introduced an action post-processing step that contributed to a small increase in accuracy. However, the accuracies remain limited due to an imbalanced class distribution and class overlap.\nThe purely ML agents are adept as operating the grids in the different regimes, although the hybrid agents do outperform them. The most unexpected result is that the hybrid agents outperform the rule-based agents in the planned-outage and unplanned-outage regimes, which cannot be explained through an IL lens.\nWe hypothesize that the hybrid models can find more robust topologies than the rule-based models due to model bias against rare actions. The few dominant actions are selected often by the rule-based agents, indicating that these actions result in optimal topologies across many grid states. Their optimality in many grid states indicates their robustness. This bias towards more frequent actions might consequently bias the model towards more robust, and consequently, more beneficial, topologies.\nThe ML and hybrid agents have good combinations of performance and inference speed. All ML and hybrid agents represent a different tradeoff between performance and speed. In practice, the optimal agent choice depends on the available resources and performance requirements."}, {"title": "Future Work", "content": "Various directions stand out as promising for future research. There is as yet little knowledge about the best practices for creating a dataset or training a model with IL for topology control. Future research might shed light on such best practices. The class overlap present in our dataset might be avoided by including the forecasted production and consumption in the features. The developed IL methods must be compared against RL methods.\nSimple behavioral cloning, as researched here, can result in distribution shift: a compounding divergence between the behavior of the expert and student agent [12]. It seems promising to apply more advanced imitation learning frameworks. There remains a large divergence between environments studied in L2RPN competitions and real power grids. Graph neural networks seem promising w.r.t. generalizability."}]}