{"title": "Revisiting the Integration of Convolution and Attention for Vision Backbone", "authors": ["Lei Zhu", "Xinjiang Wang", "Wayne Zhang", "Rynson Laut"], "abstract": "Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel at different granularity levels instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus enabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named GLMix: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (e.g., 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at https://github.com/rayleizhu/GLMix.", "sections": [{"title": "1 Introduction", "content": "Since the renaissance of deep learning over a decade ago, CNNs had dominated image analysis, until recently when transformers become popular in vision tasks. CNNs and transformers differ in how they model spatial feature interactions: CNNs use convolutions (Convs), while transformers use multi-head self-attentions (MHSAs). Both have their own advantages and limitations. For example, Convs have an inductive bias of translation equivariance, which matches the image property and enables decent performances with less data. They also have a linear complexity w.r.t. pixel number, making them scalable to high-resolution input. However, they have a limited receptive field, which cannot be remedied simply by stacking more layers together [39]. In contrast, MHSAs can model long-range dependency flexibly, but suffer from a quadratic complexity w.r.t. input resolution and require more data to compensate for the lack of inductive bias. Besides, some discussions [41] also point out that MHSAs play the role of low-pass filters, while Convs play the role high-pass ones. Hence, they are complementary to each other.\nThere are indeed some works that use both Convs and MHSAs to build vision backbones. Some of them alternate Convs and MHSAs across different stages/blocks [29, 47], forming a loose collaboration. Others [40, 14, 5] integrate Convs and MHSAs tightly in each block. Specifically, they apply Convs and MHSAs in parallel at the same granularity level and fuse their outputs for further processing, as shown in Figure 1(top). With Convs responsible for fine-grained feature extraction, we ask if we still need to apply the heavy MHSAs at the pixel level. Meanwhile, recent vision-language models [1, 63] have shown that an image can be described as a fixed number of visual tokens regardless of its resolution, possibly stemming from the low-rank property of natural signals. Inspired by these works, we propose a global-local mixing (GLMix) block, which uses Convs and MHSAS at different granularities for different roles: while Convs focus on extracting local features, MHSAS focus on learning global inter-object relations. Specifically, in each block, we represent an image as both a fine-grained regular grid and a coarse-grained set of semantic slots, and then apply Convs to the grid and MHSAs to the slots in parallel. To enable local-global feature fusion, we introduce a pair of conjugated soft clustering and dispatching modules to bridge the grid and set representations. In this way, we achieve highly efficient local-global modeling by using lightweight Convs to extract high-resolution features and heavy MSHAs to process a fixed number of semantic slots.\nTo verify the performance of the proposed integration scheme for Convs and MHSAs, referred to as GLMix, we start by building a Swin-Tiny-Layout model, referred to as GLNet-STL, based on the GLMix blocks. GLNet-STL achieves 82.5% top-1 accuracy on ImageNet-1k. It surpasses Swin-T (81.3% top-1 accuracy) significantly by 1.2%. Besides, we note that the macro architectural designs are also important factors for the performance of vision backbones. For example, PoolFormer [64] and ConvNext [36] reveal that with a deeper architecture, vision backbones can still achieve strong performances with simple token mixers such as average pooling and depth-wise convolution. Hence, we further adopt several macro designs from recent state-of-the-art vision backbones [34, 43] and scale the model up to derive a family of 3 models: GLNet-4G/9G/16G. As a result, the GLNet-4G/9G/16G models achieve 83.7%/84.5%/85.0% top-1 accuracy, while being more efficient than recent state-of-the-art works (as shown in Figure 2). Evaluations on downstream dense prediction tasks such as object detection, instance segmentation, and semantic segmentation demonstrate the strengths of GLNet consistently. We also observe that a meaningful semantic grouping effect has emerged in the soft clustering module, even with only image-level classification supervision.\nHere, we refer to the pixels on the feature maps instead of the input image."}, {"title": "2 Related Works", "content": "Efficient Attention Mechanisms. Vanilla MHSAs [48] have a quadratic complexity w.r.t. the number of input tokens, causing huge computation burden and heavy memory footprints, especially in vision applications where the feature maps are in high-resolution. A large volume of works have been conducted to develop efficient variants to overcome such a limitation. These works can be roughly categorized as sparse approximations [6, 35, 70], low-rank approximations [50, 51], and kernel-based methods [27, 7, 19]. The global branch in the proposed GLMix block, which is a combination of soft clustering and dispatching modules and an MHSA, can be used independently as a low-rank attention approximation with not only key-value pairs but also queries being down-sampled. However, according to our experiments, such a usage produces poor performance (Table 6), possibly due to losing too many details and the lack of inductive bias. We find that using MSHAs and Convs in a complementary way is crucial to the success of our proposed GLNet family.\nHybrid Vision Backbones. Many works indicate that hybrid vision backbones, which use both Convs and MHSAs, can achieve better performances than pure transformers and CNNs. Among these works, some of them use Convs and MHSAs alternatively across different blocks or stages [34, 29, 47, 59, 12], forming a loose collaboration between the two operators. Another approach adopted by several recent state-of-the-art works [14, 43, 19, 18, 55] is to integrate Convs and MHSAs in each block tightly. Different from these works that apply Convs and MHSAs at the same granularity level, we find that by offloading the burden of extracting fine-grained and location-preserving features to lightweight depth-wise Convs, MHSAs can be applied aggressively on coarse semantic slots while achieving compelling performances with higher efficiency.\nClustering for Representation Learning. Clustering is a type of unsupervised learning method used to find meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. Existing works, such as [58, 66, 30, 17, 49], have explored clustering for representation learning in deep neural networks. However, unlike ClusTR [58] and TCFormer [66], which use DPC-KNN [16] for the clustering, the soft clustering module in our work is fully learnable and does not rely on predefined rules. In comparison with ClusterFormer [30] and PaCaViT [17], which perform cross-attention between the feature grid and cluster representations/slots, our work performs self-attention over the slots (i.e., queries and key-value pairs are both from the slots), making the attention even more lightweight. Besides, our soft clustering module is hardware-efficient because it is designed to be non-iterative and mainly involves a dense matrix multiplication."}, {"title": "3 Methodology", "content": "Modern vision backbones are usually built by alternating spatial modeling modules (e.g., Convs, MHSAs, spatial MLPs) and per-location feed-forward networks (FFNs, i.e., embedding MLPs). Much research has been dedicated to developing spatial modeling modules, which is also the primary focus of this work. Specifically, we seek an integration scheme for Convs and MHSAs, which can utilize the strengths of both and scale to high-resolution inputs. Without modifying the standard design of the two basic operators, our key idea is to represent input features twice as both a regular feature grid and a set of semantic slots, and then process the feature grid with Convs and the semantic slots with MHSAs (Sec. 3.1). A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the two representations, enabling local-global fusion (Sec. 3.2). Based on such an integration scheme we propose a new family of vision backbones named GLNet (Sec. 3.3)."}, {"title": "3.1 Convs and MHSAs at Different Granularities", "content": "Image features are usually organized as a regular grid in vision backbones. Such a representation preserves the spatial correspondence between features and the input image, which is necessary for downstream dense prediction tasks (e.g., semantic segmentation). Besides, extracting local features with the grid representation is convenient and efficient.\nIn addition to the grid representation, we also create an intermediate set representation composed of a fixed number of semantic slots to enable efficient global context modeling. The reason is that although global interactions are usually expensive to compute, it is feasible to use a small amount (e.g., 64 in our experiment) of semantic slots to summarize an image [1, 63], as images are natural signals with heavy spatial redundancy [22]. Notably, the set of semantic slots that we use here is different from the sequence of visual tokens in plain ViTs [15, 46]. While each visual token corresponds to a hard-divided regular patch (e.g., 16 \u00d7 16 pixels), semantic slots are an abstraction of some \"soft\" irregular semantic regions, as shown in Figure 3.\nWe apply Convs to the grid representation to extract local features as they are lightweight and thus efficient in processing the fine-grained feature grid. To model global context, we apply MHSAs on semantic slots. This is a natural choice as MHSAs are permutation-equivariant operators, thus naturally suitable for the set representation. The scalability issue w.r.t. input resolution is avoided, as we have only a small number of semantic slots. Hence, the drawback of MHSAs is overcome.\nNext, we illustrate how the set and grid representations are bridged by a pair of soft clustering and dispatching modules, so that local and global features can be fused."}, {"title": "3.2 Bridging The Set and Grid Representations", "content": "To establish a connection between coarse-grained semantic slots and fine-grained feature grids, we need to create a correspondence between them. Although the classical k-means clustering is applicable for this purpose [49, 66], it is suboptimal for two reasons. First, it is an iterative algorithm, which is inefficient on GPUs. Second, it is a heuristic approach, which cannot be end-to-end optimized. Hence, we introduce a simplified and fully differentiable clustering module and the conjugated dispatching module to address these two problems, as shown in Figure 4. We illustrate the process below.\nClustering (feature grid \u2192 semantic slots). Given an input feature grid, $X \\in R^{C \\times H \\times W}$, where C is the number of channels, H is the height, and W is the width, we first initialize M semantic slots, $S_{init} \\in R^{M \\times C}$, via average pooling followed by shape flattening, as:\n$S_{init} = Flatten(AvgPool(X)).$\nWe then compute the correspondence logits, $A \\in R^{M \\times HW}$, as scaled cosine similarity between the initial semantic slots $S_{init}$ and the flattened input features $X \\in R^{HW \\times C'}$, as:\n$A = CosineSimilarity(S_{init}, X)/\\sigma.$\nHere, the learnable scale factor $\\sigma$ smooths the distribution of A, preventing dominance by salient entrances. With the correspondence logits, we perform a 1-step update to derive refined semantic slots, $S \\in R^{M \\times C}$, as the weighted sum of flattened features X, as:\n$S = Softmax(A)X.$\nThe refined semantic slots S are then fed to MHSAs as input.\nDispatching (semantic slots \u2192 feature grid). After transforming S to propagate global context with an MHSA module, the transformed semantic slots S' are dispatched to spatial locations for fusion with local features. Specifically, the dispatched features, $G \\in R^{C \\times H \\times W}$, are computed as:\n$G = Unflatten(Softmax(A^T)S').$\nG can be readily fused with the feature grid processed by Convs due to shape compatibility. We follow the Feature Pyramid Network [32] to use additive fusion, as it is simple/lightweight and provides a regularization that aligns the global and local features in the same semantic space.\nDiscussion. The soft clustering and dispatching operations are highly efficient as the main computations lie in hardware-friendly dense matrix multiplications, and we perform only 1-step instead of iterative updates of the semantic slots. They are fully differentiable as we do not use hard assignments like k-means. The combination is similar to soft-routing in SoftMoE [42], which aims to build large mixture-of-expert models. However, as we target a better balance between cost/performance, our design has several differences: (1) slots are initialized with a different strategy (i.e., per-image average pooling instead of learned parameters shared by all images); (2) the clustering module is placed at a different position (i.e., in pair with token mixers instead of FFNs); and (3) significantly fewer slots are used (i.e., 64, instead of 4096 which is even more than the number of tokens for an image)."}, {"title": "3.3 GLNet", "content": "To verify the performance of the GLMix block proposed above, we start by creating a Swin-Tiny-Layout architecture, named GLNet-STL, which follows the macro architectural designs of the Swin-T [35] model but with the spatial mixing modules (window attention and shift-window attention) replaced. Specifically, we use the GLMix block in the first three stages of GLNet-STL, where the feature maps are in high-resolution ($\\frac{1}{4}$, $\\frac{1}{8}$, $\\frac{1}{16}$ of the input resolution). At the 4th stage, which is $\\frac{1}{32}$ of the input resolution, we use full attention because this is affordable, and beneficial for the performance [29, 41]. As shown in Table 1, our GLNet-STL achieves competitive 82.5% Top-1 accuracy at the highest throughput of 835.9 im/s among several comparable architectures.\nThe compelling performance of GLNet-STL encourages us to build stronger vision backbones based on it. We therefore investigate several recent state-of-the-arts [14, 47, 70, 43, 34], and incorporate the following advanced architectural designs adopted by them: (1) Overlapped patch embedding: use overlapped convolutions (3 \u00d7 3 Conv with stride 2) for image/feature down-sampling, instead of non-overlapped ones (2 \u00d7 2 Conv with stride 2) as in Swin-Transformer; (2) Hybrid stage 3: alternate full MHSAs and GLMix in consecutive blocks of stage 3; (3) Convolutional position encoding: add a 3 \u00d7 3 residual depth-wise convolution prior to each spatial mixing block; (4) Deeper layout: increase the depth ([2, 2, 6, 2] \u2192 [4, 4, 18, 4]) while reducing the width (base channel 96 \u2192 64 and FFN expansion ratio 4 \u2192 3); and (5) Convlutional FFN: add a 3 \u00d7 3 residual depth-wise convolution between the two linear projections of FFN.\nNote that all designs above are widely used in the vision transformers. In addition, as this work is mainly to propose an effective as well as efficient integration scheme for MHSAs and Convs, we do not further incorporate some possibly useful designs, such as the squeeze-and-excitation (SE) block [23] used by MaxViT [47] and the gated linear unit (GLU) [44] used by SMT [34]. An ablation study for the incorporated architecture designs can be found in the Supplemental. After applying these modifications sequentially to GLNet-STL, we derive GLNet-4G, a model with 4.5G FLOPs. We scale it up to GLNet-9G and GLNet-16G FLOPs by increasing the number of base channels (96 for GLNet-9G and 128 for GLNet-16G). The model specifications are summarized in Table 2."}, {"title": "4 Experiments", "content": "We first empirically evaluate our proposed GLNet on a series of computer vision tasks, including ImageNet-1k [13] image classification (Sec. 4.1), COCO [31] object detection and instance segmentation (Sec. 4.2), and ADE20k [69] semantic segmentation (Sec. 4.3). Following existing works, we first train the models for image classification from scratch and then use the trained weights for model initialization when performing downstream dense prediction tasks. Note that for dense prediction tasks which take high-resolution inputs, we keep the number of semantic slots to 64, which is consistent with that of image classification. We have found that 64 slots are sufficient to achieve state-of-the-art performances while increasing the number does not help. We then visualize the semantic slots to demonstrate that a meaningful semantic grouping effect emerges in the proposed soft clustering module (Sec. 4.4). Finally, we conduct an ablation study on the design choices of the GLMix integration scheme, which is the core of GLNet (Sec. 4.5)."}, {"title": "4.1 Image Classification on ImageNet-1k", "content": "Settings. For a fair comparison with existing works, we conduct image classification experiments on the ImageNet-1k dataset [13], using the standard training recipe provided by Swin-Transformer [35] and the advanced distillation recipe provided by LV-ViT [26]. The training details can be found in Appendix B. We then evaluate the models for classification accuracy and benchmark their throughputs with the script provided by the timm library [54], following the same hardware (a single Tesla V100 32G GPU) and batch size (128) configurations used in Swin-Transformer [35].\nResults. In Table 3, we compare GLNet with several closely related methods and/or recent state-of-the-arts. Under the setting of standard supervised training, our GLNet-4G/9G/16G consistently show comparable or superior performances to existing best-performing models across different model scales. With dense distillation supervision, the potential of GLNets is further unleashed compared to standard supervised training. For example, the accuracy of the GLNet-4G model increases from 83.7% to 84.4%, a significant performance improvement of 0.7%. Both GLNet-4G and GLNet-9G provide a more competitive performance-FLOPs trade-off than other distilled models. As FLOPs is an indirect metric for practical inference speed and does not consider the memory access cost, we also plot the performance-throughput curve in Figure 2. The improvements become more pronounced when viewed w.r.t. throughputs. Interestingly, several of the latest vision backbones (i.e., SG-Former [43],"}, {"title": "4.2 Object Detection and Instance Segmentation", "content": "Settings. We evaluate the backbones for object detection and instance segmentation on COCO 2017 [31]. All experiments are conducted using the MMDetection [4] toolbox to ensure a fair comparison with existing works. The RetinaNet [33] framework is used for object detection, and the Mask R-CNN [21] framework is used for instance segmentation. During training, we initialize the backbone with weights trained on ImageNet-1K while leaving all other layers randomly initialized. Input images are resized by fixing the shorter side to 800 pixels while restricting the longer side to no more than 1,333 pixels. We train the RetinaNet and Mask R-CNN detectors with 1\u00d7 schedule (12 epochs) and 3\u00d7 schedule (36 epochs) provided by MMDetection. More training details are provided in Appendix B. We report the widely used average precision (AP) metric family, such as mean average precision (mAP), average precision at different thresholds (AP75 and AP50), and average precision for objects of different sizes (APS, APM and APL). Details of these metrics can be found in MMDetection [4].\nResults. We show the results for object detection and instance segmentation in Table 4. Our method achieves the best performances among the compared methods across all metrics and the two model sizes in both cases. These results indicate that local-global modeling with the GLMix block benefits object/instance-level tasks."}, {"title": "4.3 Semantic Segmentation on ADE20K", "content": "Settings. Our semantic segmentation experiments are conducted on the ADE20K dataset using the MMSegmentation [10] toolbox. We evaluate our approach using two frameworks - Semantic FPN [28] and UperNet [57]. In both cases, the backbone is initialized with ImageNet-1k weights, while the other layers are randomly initialized. For a fair comparison, we follow the same setting as PVT [51] to train the model 80k steps in our Semantic FPN experiments. On the other hand, for our UperNet experiments, we follow the settings used in Swin Transformer [35] and train the model for 160k iterations. More training details are provided in Appendix B. We report the mean intersection over union (mIoU) metric with no test-time augmentation.\nResults. Table 5 shows the results of the two different frameworks. Our GLNet-4G/16G achieve 49.6/51.3 mIoU with the Semantic FPN framework, improving the previous best SG-Former-S/M by 0.6/1.2 mIoU. A similar performance gain for the UperNet framework is also observed. The enhancements demonstrate the benefits of utilizing GLNet for high-resolution pixel-wise predictions."}, {"title": "4.4 Visualization of Semantic Slots", "content": "As mentioned in Sec. 3.2, the conjugated clustering and dispatching modules construct a correspondence between the semantic slots and the feature grid. Such a formulation allows us to visualize which regions the semantic slots correspond to. Specifically, we extract the clustering weights in Eq. 3 and split them into M scalar maps of shape H \u00d7 W. These scalar maps are then pseudo-colored for visualization. In addition, we use the k-medoids algorithm to select four representative slots for a closer look automatically. We find that a meaningful semantic grouping effect emerges in the first block of stage 3, as shown in Figure 5. Note that we use ImageNet-1k trained GLNet-STL for visualization. Hence, the model receives no dense supervision. Visualization for more samples, more blocks and at different epochs can be found in Appendix C."}, {"title": "4.5 Ablation Study", "content": "We ablate our GLMix integration scheme using the GLNet-STL model. By default, we use a global branch with 64 semantic slots and a local branch with 5 \u00d7 5 depth-wise conv in parallel in the GLMix blocks, as shown in Figure 4. With this default setting, we investigate the effect of (a) local-global collaboration, (b) the clustering strategy, (c) Conv kernel size in the local branch, and (d) number of slots in the global branch. Table 6 shows the experimental results. We summarize our findings below.\nLocal-global collaboration. First, using both local and global branches together is crucial. With the global/local branch removed, the model has a significantly degraded accuracy of 81.8%/78.0%, indicating that both coarse-grained inter-object relationship and fine-grained per-pixel local context are important. Second, using global and local branches in parallel instead of sequentially is important. A possible explanation is that the parallel layout provides a regularization for the global branch from the local branch. Otherwise, the global branch is difficult to optimize due to the lack of inductive bias. Finally, using Convs in the local branch is better than window MHSAs, as the latter are heavier and significantly decrease the throughput from 835.9 im/s to 660.9 im/s. This may be because Convs can implicitly bring position information via padding [25, 9] while window MHSAs cannot.\nClustering strategy. The soft clustering approach is an important component of the GLMix block. Using the k-means clustering results do not only produce a significantly lower throughput (835.9 im/s \u2192 440.6 im/s) but also incurs unstable training. This can be attributed to the fact that k-means is an iterative, non-differentiable algorithm, as mentioned in Sec. 3.2. We also observe that initializing the semantic slots as learnable parameters decreases the accuracy from 82.5% to 82.1%. This implies that per-image adaptive initialization is better than static initialization. Possibly, there are difficulties to learn diverse contexts for each image with shared parameters as the slot initialization, according to visualizations in Appendix C.\nConvolution kernel size in the local branch. The model is robust to the convolution kernel size in the local branch. Using a kernel size of 3 or 7 produces a similar accuracy (82.4%) to the kernel size of 5 (82.5%). This is because the global branch has provided a sufficient large receptive field."}, {"title": "5 Conclusion", "content": "In this paper, we have revisited the existing integration approaches for Convs and MHSAs, and proposed to apply the two operators at different granularity levels. We discover that by offloading the task of extracting fine-grained features to the lightweight Convs, the heavy MHSAs can be aggressively applied to a few semantic slots. Such an integration scheme, named GLMix, enables highly efficient local-global modeling to build high-performance vision backbones. A key component of GLMix is a pair of conjugated soft clustering and dispatching modules for bridging the feature grid and the set of semantic slots. Meaningful semantic grouping effects, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches, are observed in the clustering process.\nCurrently, we only consider using a static number of semantic slots (i.e., 64 in our experiments) for all images. This may cause many redundant slots representing the same content, as shown in Figure 5. It may be interesting to design a dynamic slot pruning mechanism for more efficient computation and end-to-end weakly-supervised segmentation. Another drawback of GLNet is that it still incorporates many hardware-inefficient depth-wise convolutions with low arithmetic intensity. Seeking more hardware-friendly alternatives will further improve its throughputs on modern hardware."}, {"title": "A Effect of Advanced Architecture Designs", "content": "As mentioned in Sec. 3.3, we incorporate several advanced architecture designs adopted by recent vision backbones in our GLNet family to achieve state-of-the-art performance. Here, we list the effects of these designs in Table 7. All these designs improve the accuracy. However, they also decrease the throughput."}, {"title": "B Training Details", "content": "This section provides more training details for ImageNet-1k image classification, COCO object detection and instance segmentation, and ADE20K semantic segmentation.\nImage classification. For the standard supervised training recipe, training details are in Table 8. When training with the advanced distillation recipe [26], we add an extra distillation head to the GLNet-4G/9G model and use the NFNet-F6 [2] to generate distillation targets; other training details are shown in Table 9. Experiments are run on 16 Tesla V100 SXM2 (32GB) GPUs. Each experiment takes 2-4 days, depending on model size.\nObject detection and instance segmentation. For COCO experiments, all models are trained using the AdamW [38] optimizer with a batch size of 16. We use a linear schedule with 500 warm-up iterations and set the peak learning rate as le - 4. The weight decay is 0.05 for Mask R-CNN [21] and 0.001 for RetinaNet [33]. Experiments are run on 8 or 16 Tesla V100 SXM2 (32GB) GPUs. Each experiment takes 1-2 days, depending on model size.\nSemantic segmentation. For the ADE20K semantic segmentation task, we apply the AdamW optimizer with a batch size of 32. In Semantic FPN [28] experiments, we use the cosine annealing learning rate schedule with 1000 warm-up iterations and a peak learning rate of 2e-4. The weight decay is 1e - 4. In UperNet [57] experiments, a polynomial learning rate schedule is employed with a linear warm-up phase of 1500 iterations. We set the learning rate as 6e - 4 and weight decay as 1e - 2. Experiments are run on 8 Tesla V100 SXM2 (32GB) GPUs. Each experiment takes 1-2 days, depending on model size."}, {"title": "C More Visualization Results", "content": "In this section, we provide more visualization results, including (1) visualization of semantic slots of blocks at different depths (Figure 6), (2) visualization of slot evolution over training epochs (Figure 7), and (3) visualization of slots using learned parameters as clustering initialization (Figure 8). We summarize the main observations as below:\n\u2022 The semantic slots at the lower block (2nd block) tends to group pixels according to color cues. At the middle block (5th block), an object-level grouping effect has emerged. The upper block (10th block) pays attention to discriminative local regions.\n\u2022 During the training, we found that at the end of the 1st epoch we can already distinguish the foreground objects and the backgrounds, although the grouping has not very concentrated patterns, this is possibly due to the fact that even a random projection can preserve distances/similarities well. At the end of the 5th epoch, the semantic grouping becomes more concentrated and similar to that of the final stage."}]}