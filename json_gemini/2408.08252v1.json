{"title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding", "authors": ["Xiner Li", "Yulai Zhao", "Chenyu Wang", "Gabriele Scalia", "Gokcen Eraslan", "Surag Nair", "Tommaso Biancalani", "Aviv Regev", "Sergey Levine", "Masatoshi Uehara"], "abstract": "Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely -generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require \u201cdifferentiable\u201d proxy models (e.g., classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at https://github.com/masa-ue/SVDD.", "sections": [{"title": "1 Introduction", "content": "Diffusion models have gained popularity as powerful generative models. Their applications extend beyond image generation to include natural language generation (Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2023), molecule generation (Jo et al., 2022; Vignac et al., 2022), and biological (DNA, RNA, protein) sequence generation (Avdeyev et al., 2023; Stark et al., 2024), and in each of these domains diffusion models have been shown to be very effective at capturing complex natural distributions. However, in practice, we might not only want to generate realistic samples, but to produce samples that optimize specific downstream reward functions while preserving naturalness by leveraging pre-trained models. For example, in computer vision, we might aim to generate natural images with high aesthetic and alignment scores. In drug discovery, we may seek to generate valid molecules with high QED/SA/docking scores (Lee et al., 2023; Jin et al., 2018) or natural RNAs (such as mRNA vaccines (Cheng et al., 2023)) with high translational efficiency and stability (Castillo-Hair and Seelig, 2021; Asrani et al., 2018), and natural DNAs with high cell-specificity (Gosai et al., 2023; Taskiran et al., 2024; Lal et al., 2024).\nThe optimization of downstream reward functions using pre-trained diffusion models has been approached in various ways. In our work, we focus on non-fine-tuning-based methods because fine- tuning generative models (e.g., when using classifier-free guidance (Ho et al., 2020) or RL-based fine-"}, {"title": "2 Related Works", "content": "Here, we summarize related work. We first outline methods relevant to our goal, categorizing them based on whether they involve fine-tuning. We then discuss related directions, such as discrete diffusion models, where our method excels, and decoding in autoregressive models.\nNon-fine-tuning methods. There are primarily two methods for optimizing downstream functions in diffusion models without fine-tuning.\n\u2022 Classifier guidance (Dhariwal and Nichol, 2021; Song et al., 2020): It has been widely used to condition pre-trained diffusion models without fine-tuning. Although these methods do not originally focus on optimizing reward functions, they can be applied for this purpose (Uehara et al., 2024, Section 6.2). In this approach, an additional derivative of a certain value function is incorporated into the drift term (mean) of pre-trained diffusion models during inference. Subsequent variants (e.g., Chung et al. (2022); Ho et al. (2022); Bansal et al. (2023); Guo et al. (2024); Wang et al. (2022); Yu et al. (2023)) have been proposed to simplify the learning of value functions. However, these methods require constructing differentiable models, which limits their applicability to non-differentiable features/reward feedbacks commonly encountered in scientific domains as mentioned in Section 1. Additionally, this approach cannot be directly extended to discrete diffusion models in a principle way.\n\u2022 Best-of-N: The naive way is to generate multiple samples and select the top samples based on the reward functions, known as best-of-K in the literature on (autoregressive) LLMs (Stiennon et al., 2020; Nakano et al., 2021; Touvron et al., 2023; Beirami et al., 2024; Gao et al., 2023). This approach is significantly less efficient than ours, as our method uses soft-value functions that predict how intermediate noisy samples lead to high rewards in the future. We validate this experimentally in Section 6.\nFine-tuning of diffusion models. Several methods exist for fine-tuning generative models to optimize downstream reward functions, such as classifier-free guidance (Ho and Salimans, 2022) and RL-based fine-tuning (Fan et al., 2023; Black et al., 2023)/its variants (Dong et al., 2023). However, these approaches often come with caveats, including high computational costs and the risk of easily forgetting pre-trained models. In our work, we propose an inference-time technique that eliminates the need for fine-tuning generative models, similar to classifier guidance.\nDiscrete diffusion models. Based on seminal works Austin et al. (2021); Campbell et al. (2022), recent work on masked diffusion models (Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024) has demonstrated their strong performance in natural language generation. Additionally, they have been applied to biological sequence generation (e.g., DNA, protein sequences in Campbell et al. (2024); Sarkar et al. (2024)). In these cases, the use of diffusion models over autoregressive models is particularly apt, given that many biological sequences ultimately adopt complex three-dimensional structures. We also note that ESM3 (Hayes et al., 2024), a widely recognized foundational model in protein sequence generation, bears similarities to masked diffusion models.\nDespite its significance, it cannot be integrated with standard classifier guidance because adding a continuous gradient to a discrete objective is not inherently valid. Unlike standard classifier guidance, our algorithm can be seamlessly applied to discrete diffusion models.\nDecoding in autoregressive models with rewards. The decoding strategy, which dictates how sentences are generated from the model, is a critical component of text generation in autoregressive language models (Wu et al., 2016; Chorowski and Jaitly, 2016; Leblond et al., 2021). Recent studies have explored inference-time techniques for optimizing downstream reward functions Dathathri et al. (2019); Yang and Klein (2021); Qin et al. (2022); Mudgal et al. (2023); Zhao et al. (2024); Han et al. (2024). While there are similarities between these works and ours, to the best of our knowledge, no prior work has extended such methodologies to diffusion models. Furthermore, our approach leverages characteristics unique to diffusion models that are not present in autoregressive models such as SVDD."}, {"title": "3 Preliminaries and Goal", "content": "In this section, we describe the standard method for training diffusion models and outline the objective of our work: optimizing downstream reward functions given pre-trained diffusion models."}, {"title": "3.1 Diffusion Models", "content": "In diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), our goal is to learn a sampler $p(x_c) \\in [C \\to \\Delta(X)]$ given data consisting of $(x, c) \\in X \\times C$. For instance, in text-to-image diffusion models, $c$ is a text, and $x$ is an image. In protein inverse folding, $c$ is a backbone protein structure, and $x$ is a protein sequence.\nThe training process for a standard diffusion model is summarized as follows. First, we introduce a (fixed) forward process $q_t: X \\to \\Delta(X)$. Now, after introducing the forward process, we aim to learn a backward process: ${p_t}$ where each $p_t$ is $X \\times C \\to \\Delta(X)$ so that the induced distributions induced by the forward process and backward process match marginally. For this purpose, by parametrizing the backward processes with $\\theta \\in \\mathbb{R}^d$, we typically use the following loss function:\n$E_{x_{1},...,x_{T} \\sim q(x_{0})}\\left[-\\log P_{\\theta}(x_{0}|x_{1}) + \\sum_{t=1}^{T}KL(q_t(\\cdot | X_{t-1}) ||P_t(\\cdot | X_{t+1}, C; \\theta)) + KL(q_T(\\cdot)||P_T(\\cdot))\\right]$,\nwhich is derived from the variational lower bound of the negative likelihood (i.e., ELBO).\nHere are two examples of concrete parameterizations. Let $\\alpha_t \\in \\mathbb{R}$ be a noise schedule.\nExample 1 (Continuous space). When $X$ is Euclidean space, we typically use the Gaussian distribution $q_t(\\cdot | x) = N(\\sqrt{\\alpha_t}x, (1 - \\alpha_t))$. Then, the backward process is parameterized as\n$N(\\frac{\\sqrt{\\alpha_t(1 - \\bar{\\alpha}_{t+1})}x_t + \\frac{\\sqrt{\\alpha_{t-1}(1 - \\alpha_t)}x_{\\theta}(x_t, c; \\theta)}{(1 - \\bar{\\alpha}_t)}}{\\sqrt{\\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}}}, \\frac{(1 - \\alpha_{t+1})(1 - \\alpha_t)}{1 - \\bar{\\alpha}_t}),$\nwhere $\\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i$. Here, $x_{\\theta}(x_t, c; \\theta)$ is a neural network that predicts $x_0$ from $x_t$ (i.e., $E_q[x_0 | x_t, c]$).\nExample 2 (Discrete space in Sahoo et al. (2024); Shi et al. (2024)). Let $X$ be a space of one-hot column vectors ${x \\in {0,1}^K : \\sum_{i=1}^{K} X_i = 1}$, and $Cat(\\pi)$ be the categorical distribution over $K$ classes with probabilities given by $\\pi \\in \\Delta^{K}$ where $\\triangle^{K}$ denotes the K-simplex. A typical choice is $q_t(x) = Cat(\\alpha_t + x + (1 - \\alpha_t)m)$ where $m = [0,......, 0, Mask]$. Then, the backward process is parameterized as\n$\\left\\{\n\\begin{array}{ll}\nCat(x_t), & \\text{if } x_t \\neq m \\\\\nCat(\\frac{(1-\\alpha_{t-1})m+(\\alpha_{t-1}-\\bar{\\alpha}_t)x_{\\theta}(x_t,c;\\theta)}{1-\\bar{\\alpha}_t}), & \\text{if } x_t = m.\n\\end{array}\n\\right.$\nHere, $x_{\\theta}(x_t, c; \\theta)$ is a neural network that predicts $x_0$ from $x_t$. Note that when considering a sequence of L tokens ($x_{1:L}$), we use the direct product: $p_t(x_{1:L}^{t}|x_{t+1}^{1:L}, c) = \\Pi_{l=1}^{L} p_t(x_{l}^{t}|x_{t+1}^{l}, c)$.\nAfter learning the backward process, we can sample from a distribution that emulates training data distribution (i.e., $p(x|c)$) by sequentially sampling ${p_t}_{t=T}^0$ from t = T to t = 0.\nNotation. The notation $\\delta_{\\alpha}$ denotes a Dirac delta distribution centered at $\\alpha$. The notation $x \\propto$ indicates that the distribution is equal up to a normalizing constant. With slight abuse of notation, we often denote $p_T(\\cdot | x_T, c)$ by $p_T(\\cdot)$."}, {"title": "3.2 Objective: Generating Samples with High Rewards While Preserving Naturalness", "content": "We consider a scenario where we have a pre-trained diffusion model, which is trained using the loss function explained in Section 3.1. These pre-trained models are typically designed to excel at characterizing the natural design space (e.g., image space, biological space, or chemical space) by emulating the extensive training dataset. Our work focuses on obtaining samples that also optimize downstream reward functions $r: X \\to \\mathbb{R}$ (e.g., QED and SA in molecule generation) while maintaining the naturalness by leveraging pre-trained diffusion models. We formalize this goal as follows."}, {"title": "4 Soft Value-Based Decoding in Diffusion Models", "content": "In this section, we first present the motivation behind developing our new algorithm. We then introduce our algorithm, which satisfies desired properties, i.e., the lack of need for fine-tuning or constructing differentiable models."}, {"title": "4.1 Key Observation", "content": "We introduce several key concepts. First, for $t \\in [T + 1,\\cdots, 1]$, we define the soft value function:\n$v_{t-1}(x_t) := \\alpha \\log E_{X_{t-1} \\sim p^{pre}_{t}\\lbrack x_{t} = x_t]}\\left[\\exp(\\frac{r(x_0)}{\\alpha}) | X_{t-1} = x_t\\right]$,\nwhere $E_{{p^{pre}}}[\\cdot]$ is induced by ${p^{pre}_{t}(x_{t-1} | x_{t+1}, c)}_{t=T}^{t=1}$. This value function represents the expected future reward at $t = 0$ from the intermediate noisy state at $t - 1$. Next, we define the following soft optimal policy (denoising process) $p_t^*: X \\times C \\to \\Delta(X)$ weighted by value functions $v_{t-1}: X \\to \\mathbb{R}$:\n$p_t^{\\alpha}(x_{t-1} | x_t, c) = \\frac{p^{pre}_t(x_{t-1} | x_t, c) \\exp(v_{t-1}(x_{t-1})/\\alpha)}{\\int p^{pre}_t(x | x_t, c) \\exp(v_{t-1}(x)/\\alpha)dx}.$\nHere, we refer to $v_t$ as soft value functions, $p_t^{\\alpha}$ as soft optimal policies, respectively, because they literally correspond to soft value functions and soft optimal policies where we embed diffusion models into entropy-regularized MDPs, as demonstrated in Uehara et al. (2024).\nWith this preparation in mind, we utilize the following key observation:\nTheorem 1 (From Theorem 1 in Uehara et al. (2024)). The distribution induced by ${p_t^{\\alpha}(x_{t+1}, c)}_{t=T}^{1}$ is the target distribution $p^{(\\alpha)}(x|c)$, i.e.,\n$p^{(\\alpha)}(x_0 | c) = \\int \\left\\{\\prod_{t=1}^{T+1} p_t^{\\alpha}(x_{t-1} | x_t, c)\\right\\} dx_{1:T}.$\nWhile Uehara et al. (2024) presents this theorem, they use it primarily to interpret their RL-based fine-tuning introduced in Fan et al. (2023); Black et al. (2023). In contrast, our work explores how to convert this into a new fine-tuning-free optimization algorithm.\nOur motivation for a new algorithm. Theorem 1 states that if we can hypothetically sample from ${p_t^{\\alpha}}_{t=T}^{1}$, we can sample from the target distribution $p^{(\\alpha)}(x)$. However, there are two challenges in sampling from each $p_t^{\\alpha}$:\n1. The soft-value function $v_{t-1}$ in $p_t^{\\alpha}$ is unknown.\n2. It is unnormalized (i.e., calculating the normalizing constant is often hard)."}, {"title": "4.2 Inference-Time Algorithm", "content": "Algorithm 1 SVDD (Soft Value-Based Decoding in Diffusion Models)\n1: Require: Estimated soft value function ${\\hat{v}_t}_{t=T}^{1}$ (refer to Algorithm 2 or Algorithm 3), pre- trained diffusion models ${p^{pre}_t}_{t=T}^{1}$, hyperparameter $\\alpha \\in \\mathbb{R}$\n2: for t \u2208 [T + 1,\uff65\uff65\uff65, 1] do\n3: Get M samples from pre-trained polices ${x_{t-1}^{(m)}}_{m=1}^{M}$ ~$ p_t^{pre}(x_t)$, and for each m, calculate\n$w_{t-1}^{(m)} := \\exp(v_{t-1}(x_{t-1}^{(m)}/\\alpha)$\n4: $x_{t-1} \\leftarrow x_{t-1}^{(s_{t-1})}$ after selecting an index: $s_{t-1} \\sim Cat(\\frac{\\{w_{t-1}^{(m)}\\}_{m=1}^{M}}{\\sum_{j=1}^{M} w_{t-1}^{(j)}})$\n5: end for\n6: Output: $x_0$\nNow, we introduce our algorithm, which leverages the observation, in Algorithm 1. Our algorithm is an iterative sampling method that integrates soft value functions into the standard inference procedure of pre-trained diffusion models. Each step is designed to approximately sample from a value-weighted policy ${p_t^{\\alpha}}_{t=T}^{1}$.\nWe have several important things to note.\n\u2022 When $\\alpha = 0$, Line 4 corresponds to $(x_{t-1} = argmax_{m \\in [1,\\cdots,M]} \\hat{v}_{t-1}(x_t^{(m)}))$. In practice, we often recommend this choice.\n\u2022 A typical choice we recommend is M = [5,\u2026\u2026\u2026, 20]. The performance with varying M values will be discussed in Section 6.\n\u2022 Line 3 can be computed in parallel at the expense of additional memory.\n\u2022 When the normalizing constant can be calculated relatively easily (e.g., in discrete diffusion with small K, L), we can directly sample from ${p_t^{\\alpha}}_{t=T}^{1}$.\nThe remaining question is how to obtain the soft value function, which we address in the next section."}, {"title": "4.3 Learning Soft Value Functions", "content": "Next, we describe how to learn soft value functions $v_t(x)$ in practice. We propose two main approaches: the Monte Carlo regression approach and the posterior mean approximation approach.\nMonte Carlo regression. Here, we use the following approximation $v_{t'}$ as $v_t$ where\n$\\hat{v}_t(.) := E_{x_0 \\sim p^{pre}(x_0 | x_t)}[r(x_0) | X_t = .]$"}, {"title": "5 Advantages, Limitations, and Extensions of SVDD", "content": "So far, we have detailed our algorithm, SVDD. In this section, we discuss its advantages, limitations, and extensions."}, {"title": "5.1 Advantages", "content": "No fine-tuning (or no training in SVDD-PM). Unlike classifier-free guidance or RL-based fine- tuning, SVDD does not require any fine-tuning of the generative models. In particular, when using SVDD-PM, no additional training is needed as long as we have r.\nNo need for constructing differentiable models. Unlike classifier guidance, SVDD does not require differentiable proxy models, as there is no need for derivative computations. For example, if r is non-differentiable feedback (e.g., physically-based simulations, QED, SA in molecule generation), our method SVDD-PM can directly utilize such feedback without constructing differentiable proxy models. In cases where non-differentiable feedback is costly to obtain, proxy reward models may still be required, but they do not need to be differentiable; thus, non-differentiable features or non- differentiable models based on scientific knowledge (e.g., molecule fingerprints, GNNs) can be leveraged. Similarly, when using SVDD-MC, while a value function model is required, it does not need to be differentiable, unlike classifier guidance.\nAdditionally, compared to approaches that involve derivatives (like classifier guidance or DPS), our algorithm may be more memory efficient at inference time, particularly when M is moderate, and can be directly applied to discrete diffusion models mentioned in Example 2.\nProximity to pre-trained models (robust to reward over-optimization). Since samples are consistently generated from pre-trained diffusion policies at each step, they are ensured to remain within the natural space defined by pre-trained diffusion models.\nThis is especially advantageous when rewards to be optimized by SVDD are learned from offline data. In such cases, learned reward functions may be inaccurate for out-of-distribution samples (Uehara et al., 2024). Consequently, conventional fine-tuning methods often suffer from over-optimization (reward hacking) by exploiting these out-of-distribution regions (Fan et al., 2023; Clark et al., 2023). Given that non-natural design spaces typically encompass a significant portion of out-of-distribution regions of the offline data, maintaining proximity to pre-trained models acts as a regularization mechanism against these regions."}, {"title": "5.2 Potential Limitations", "content": "Memory and computational complexity in inference time. Our approach requires more compu- tational resources (if not parallelized) or memory (if parallelized), specifically M times more than standard inference methods.\nProximity to pre-trained models. The proximity to pre-trained models might be a disadvantage if significant changes to the pre-trained models are desired. We acknowledge that RL-based fine-tuning could be more effective for this purpose than our algorithm."}, {"title": "5.3 Extensions", "content": "Using a likelihood/classifier as a reward. While we primarily consider scenarios where reward models are regression models, by adopting a similar strategy in (Zhao et al., 2024), they can be readily replaced with classifiers or likelihood functions in the context of solving inverse problems (Chung et al., 2022; Bansal et al., 2023).\nCombination with sequential Monte Carlo. SVDD iterates IS and resampling at each team step locally, but our algorithm can be combined with the more sophisticated global resampling method known as particle filter method (i.e., sequential Monte Carlo) (Del Moral and Doucet, 2014; Kitagawa, 1993; Doucet et al., 2009). Although we do not recommend this approach in practice due to its difficulty in parallelization, we discuss its extension in Section A.\nApplication to fine-tuning. Our SVDD, can also be naturally extended to fine-tuning by generating samples with SVDD, and then using these samples for supervised fine-tuning."}, {"title": "6 Experiments", "content": "We conduct experiments to assess the performance of our algorithm relative to baselines and its sensitivity to various hyperparameters. We start by outlining the experimental setup, including baselines and models, and then present the results."}, {"title": "6.1 Settings", "content": "Methods to compare. We compare the following methods.\n\u2022 Pre-trained models: We generate samples using pre-trained models.\n\u2022 Best-of-N (Nakano et al., 2021): We generate samples from pre-trained models and select the top 1/N samples. This selection is made to ensure that the computational time during inference is approximately equivalent to that of our proposal.\n\u2022 DPS (Chung et al., 2022): DPS is a widely used enhancement of classifier guidance. Although the original work was not designed for discrete diffusion, we employ specific heuristics to adapt it for this purpose (Section C).\n\u2022 SVDD (Ours): We implement SVDD-MC and SVDD-PM. We generally set M 20 for images and M = 10 for other domains, additionally we set \u03b1 = 0.\nDatasets and reward models. We provide details on the pre-trained diffusion models and downstream reward functions used. For further information, refer to Section C.\n\u2022 Images: We use Stable Diffusion v1.5 as the pre-trained diffusion model (T = 50). For downstream reward functions, we use compressibility and aesthetic scores (LAION Aesthetic Predictor V2 in Schuhmann (2022)), as employed in Black et al. (2023); Fan et al. (2023). Note that compressibility is non-differentiable reward feedback.\n\u2022 Molecules: We use GDSS (Jo et al., 2022), trained on ZINC-250k (Irwin and Shoichet, 2005), as the pre-trained diffusion model (T = 1000). For downstream reward functions, we use QED and SA calculated by RDKit, which are non-differentiable feedback. Here, we renormalize SA to (10 - SA)/9 so that a higher value indicates better performance.\n\u2022 DNAs (Enhancers): We use the discrete diffusion model (Sahoo et al., 2024), trained on datasets from Gosai et al. (2023), as our pre-trained diffusion model (T = 128). For the downstream reward function, we use an Enformer model (Avsec et al., 2021) to predict activity in the HepG2 cell line.\n\u2022 RNAs (5'UTRs): We use the discrete diffusion model (Sahoo et al., 2024) as the pre-trained diffusion model (T = 128). For downstream reward functions, we employ a reward model that predicts the mean ribosomal load (MRL) measured by polysome profiling (trained on datasets from Sample et al. (2019)) and stability (trained on datasets from (Agarwal and Kelley, 2022))."}, {"title": "6.2 Results", "content": "Overall, our proposal outperforms the baseline methods (Best-of-N and DPS), as evidenced by higher rewards for samples generated in large quantities. More formally, in Section C, we compare the top 10 and 20 quantiles from each algorithm and confirm that SVDD always outperforms the baselines. This indicates that our algorithm can generate high-reward samples that Best-of-N and DPS cannot.\n\u2022 Compared to Best-of-N, although the rewards for generating samples in smaller quantities could be lower with our algorithm, this is expected because our algorithm generates samples with high likelihood $p^{pre}(x|c)$ in pre-trained diffusion models, but with possibly lower rewards.\n\u2022 Compared to DPS, our algorithm consistently outperforms. Notably, in molecular generation, DPS is highly ineffective due to the non-differentiable nature of the original feedback.\nThe superiority of our two proposals (SVDD-MC or SVDD-PM) appears to be domain-dependent. Generally, SVDD-PM may be more robust since it does not require additional learning (i.e., it directly utilizes reward feedback). The performance of SVDD-MC depends on the success of value function learning, which is discussed in Section C."}, {"title": "7 Conclusion", "content": "We propose a novel inference-time algorithm, SVDD, for optimizing downstream reward functions in pre-trained diffusion models that eliminate the need to construct differentiable proxy models. In future work, we plan to conduct experiments in other domains, such as protein sequence optimization (Gruver et al., 2023; Alamdari et al., 2023; Watson et al., 2023) or controllable 3D molecule generation (Xu et al., 2023)."}, {"title": "A Soft Value-Based Decoding with Particle Filter", "content": "In this section, we explain soft-value decoding incorporating particle filtering (Doucet et al., 2009). However, we generally do not recommend this approach in practice due to its difficulty in paralleliza- tion.\nThe complete algorithm is summarized in Algorithm 4. Here, we provide a brief overview. It consists of two steps. Since our algorithm is iterative, at time point t, consider we have N samples (particles) ${x^{[i]}_t}^{i = 1N}$.\nIS step (line 3). We generate a set of samples ${x^{[i]}_{t-1}}^{i = 1}_{N}$ following a policy from a pre-trained model $p^{pre}(.)$. In other words,\n$\\forall i \\in [1,\u2026, N]; x^{[i]}_{t-1} \\sim p^{pre}_{t}(x_t)$.\nNow, we denote the importance weight for the next particle $x_{t\u22121}$ given the current particle $x_t$ as $w(x_{t-1}, x_t)$, expressed as\n$w(x_{t-1}, x_t) := \\frac{exp(v_{t-1}(x_{t-1})/\\alpha)}{\\int exp(v_{t-1}(x_{t-1})/\\alpha)p^{pre}_{t-1}(x_{t-1}|x_t)dx_{t-1}} = \\frac{exp(v_{t-1}(x_{t-1})/\\alpha)}{exp(v_t(x_t)/\\alpha)},$\nand define\n$\\forall i \\in [1,\u2026\u2026, N]; w^{[i]} := w(x^{[i]}_{t-1}, x_t)$.\nNote here we have used the soft Bellman equation:\n$exp(v_t(x_t)/\\alpha) = \\int exp(v_{t-1}(x_{t-1})/\\alpha)p^{pre}_{t}(x_{t-1}|x_t)dx_{t-1}.$\nHence, by denoting the target marginal distribution at t - 1, we have the following approximation:\n$p^{tar}_{t-1} \\approx \\sum_{i=1 j=1}^{N} \\frac{w^{[i]}}{N} \\delta_{x^{[i]}_{t-1}} \\sum_{i=1 j=1}^{N} \\frac{w^{[i]}}{N}$\nSelection step (line 5). Finally, we consider a resampling step. The resampling indices are determined by the following:\n$\\left\\{\n\\begin{array}{ll}\n{s_t}^{[i]}\n\\end{array}\n\\right\\} \\sim Cat( \\frac{{w^{[i]}}_{t-1}}{\\sum^{N}_{j=1}{{w}_{t-1}}^{[j]}} )$\nTo summarize, we conduct\n$p^{tar}_{t-1} \\approx \\sum_{i=1 j=1}^{N} \\frac{w^{[i]}}{N} \\delta_{x^{[i]}_{t-1}} \\underrightarrow{\\text{Resampling}} \\frac{1}{N} \\sum_{i=1}^{N} \\delta_{x_{t-1}^{[i]}}$\nRemark 4 (Works in autoregressive models). We note that in the context of autoregressive (language) models, Zhao et al. (2024); Lew et al. (2023) proposed a similar algorithm."}, {"title": "B Soft Q-learning", "content": "In this section, we explain soft value iteration to estimate soft value functions, which serves as an alternative to Monte Carlo regression.\nSoft Bellman equation. Here, we use the soft Bellman equation:\n$exp(v_t(x_t)/\\alpha) = \\int exp(v_{t-1}(x_{t-1})/\\alpha)p^{pre}_{t-1}(x_{t-1}|x_t)dx_{t-1},$\nas proved in Section 4.1 in (Uehara et al., 2024). In other words,\n$v_t(x_t) = \\alpha log\\{E_{x_{t-1} \\sim p^{pre}(.|x_t)}[exp(v_{t-1}(x_{t-1})/\\alpha)|x_t]\\}.$\nAlgorithm. Based on the above, we can estimate soft value functions recursively by regressing $v_{t-1}(x_{t-1})$ onto $x_t$. This approach is often referred to as soft Q-learning in the reinforcement learning literature (Haarnoja et al., 2017; Levine, 2018).\nIn our context, due to the concern of scaling of a, as we have done in Algorithm 2, we had better use\n$v_t(x_t) = E_{x_{t-1} \\sim p^{pre}(.|x_t)}[v_{t-1}(x_{t-1})|x_t].$\nWith the above recursive equation, we can estimate soft value functions as in Algorithm 5."}, {"title": "C Additional Experimental Details", "content": "We further add additional experimental details."}, {"title": "C.1 Additional Setups for Experiments", "content": "Images.\n\u2022 DPS: We require differentiable models that map images to compressibility. For this task", "DPS": "Following the implementation in Lee et al. (2023)", "SVDD-MC": "We use a Graph Isomorphism Network (GIN) model (Xu et al."}, {"DPS": "Although DPS was originally proposed in the continuous space, we have adapted it for our use by incorporating the gradient of the value function model at each step and representing each sequence"}]}