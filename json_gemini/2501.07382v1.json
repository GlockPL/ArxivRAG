{"title": "Information-Theoretic Dual Memory System for Continual Learning", "authors": ["RunQing Wu", "KaiHui Huang", "HanYi Zhang", "QiHe Liu", "GuoJin Yu", "JingSong Deng", "Fei Ye"], "abstract": "Continuously acquiring new knowledge from a dynamic environment is a fundamental capability for animals, facilitating their survival and ability to address various challenges. This capability is referred to as continual learning, which focuses on the ability to learn a sequence of tasks without the detriment of previous knowledge. A prevalent strategy to tackle continual learning involves selecting and storing numerous essential data samples from prior tasks within a fixed-size memory buffer. However, the majority of current memory-based techniques typically utilize a single memory buffer, which poses challenges in concurrently managing newly acquired and previously learned samples. Drawing inspiration from the Complementary Learning Systems (CLS) theory, which defines rapid and gradual learning mechanisms for processing information, we propose an innovative dual memory system called the Information-Theoretic Dual Memory System (ITDMS). This system comprises a fast memory buffer designed to retain temporary and novel samples, alongside a slow memory buffer dedicated to preserving critical and informative samples. The fast memory buffer is optimized employing an efficient reservoir sampling process. Furthermore, we introduce a novel information-theoretic memory optimization strategy that selectively identifies and retains diverse and informative data samples for the slow memory buffer. Additionally, we propose a novel balanced sample selection procedure that automatically identifies and eliminates redundant memorized samples, thus freeing up memory capacity for new data", "sections": [{"title": "1. Introduction", "content": "In practical scenarios and applications, data samples are typically provided in a sequential manner, precluding the possibility of accessing all data simultaneously. This form of learning framework within computer science is identified as continual learning [1]. While contemporary deep learning models have demonstrated remarkable effectiveness in a single, static dataset, their application within a continual learning framework poses significant challenges due to the potential loss of previously acquired knowledge when learning new tasks. Such a phenomenon of performance degeneration is referred to as catastrophic forgetting [1].\nRecent studies in the field of continual learning have introduced five distinct methodologies to mitigate catastrophic forgetting, namely: regularization-based strategies [2], memory or experience replay techniques [3], optimization-based methods, representation-based methods and architecture-based strategies [4]. Among these methodologies, the memory replay technique stands out as both straightforward and widely adopted in continual learning. It operates via a compact memory buffer that retains a limited number of past data samples [3]. When engaging in new task learning, these retained samples are integrated with new data for model training, thereby the model's performance heavily relies on the quality of the memorized samples. The regularisation-based approach, conversely, seeks to regulate the optimization process of the model to attain optimal performance across both prior and current tasks [5]. This technique incorporates an additional regularization term into the main objective function, aiming to penalize significant alterations in many critical network parameters. Regularisation strategies are well-equipped to tackle traditional continual learning challenges and are also applicable to online continual learning scenarios [6] by combining them with memory-based approaches. Another category of continual learning methods is denoted as dynamic expansion models [7], which inherently enhance model capacity by adding new hidden layers and nodes within the existing network architecture. In contrast to static network models [8], dynamic expansion models present several advantages, including scalability and robust generalization performance for previously learned tasks. Nonetheless, a significant drawback of this approach is the increased computational complexity and storage requirements, posing challenges in real-world contexts where devices and machines have severe resource constraints.\nThe Complementary Learning Systems (CLS) theory elucidates an important biological mechanism within the brain, in which the information is processed through two interdependent systems: a gradual learning mechanism designed to retain novel skills and experiences from a dynamically evolving environment, and a rapid learning mechanism aimed at efficiently distilling essential information from prior experiences to reinforce acquired knowledge [9]. Inspired by the results from the CLS framework, many studies have developed several innovative continual learning systems [10, 11]. Nonetheless, these approaches either necessitate the incorporation of an auxiliary neural network"}, {"title": "2. Background and Related Work", "content": "Catastrophic forgetting is a significant challenge in continual learning (CL), where models would lose all previously learnt knowledge when training on a new task [1]. To address this, various continual learning technologies have been proposed, including regularization-based methods [12, 13, 14, 15], memory replay-based methods[16, 17, 18, 19, 20, 21], optimization-based methods [22, 23, 24, 25, 26, 27, 28, 29], representation-based methods[30, 10, 31, 32], and architecture-based methods [33, 34, 4, 35, 36, 7]. In this section, we firstly review these approaches focusing on their mathematical foundations and contributions to mitigating forgetting. We will then briefly introduce current approaches that leverage information-theoretic methods in CL. \u201cWe summarize the representative methods listed above in Table. 1.\"\n2.1. Regularization-Based Methods\nRegularization-based methods mitigate catastrophic forgetting by introducing explicit regularization terms into the loss function, balancing the learning between old and new tasks[60, 38, 37]. These methods often require storing a reference copy of the previous model to guide the regularization process. Regularization-based approaches can be broadly categorized into weight regularization and function regularization.\nWeight regularization focuses on constraining parameter updates to preserve knowledge from previous tasks. For instance, Elastic Weight Consolidation (EWC) [2] introduces a regularization term $\\sum F_i(\\theta_i - \\theta_i^*)^2$, where A is a hyperparameter controlling the regularization effects, $F_i$ is derived from the Fisher Information Matrix, and $\\theta_i^*$ represents the optimal parameters from previous tasks. This approach minimizes significant changes in critical parameters during the learning of new tasks, effectively stabilizing key parameters. Synaptic Intelligence (SI) [37] is another method that estimates parameter importance by tracking their contributions to the total loss over time, adjusting updates to preserve crucial knowledge. Function regularization, on the other hand, aims"}, {"title": "2.2. Replay-Based Methods", "content": "Besides regularization techniques, replay-based methods also play a crucial role in mitigating catastrophic forgetting by incorporating data from previous tasks during the learning of new ones[40, 61]. This is typically achieved by combining the loss functions of old and new tasks to form a total loss that balances the importance of both. Replay-based methods are primarily divided into experience replay and generative replay.\nExperience replay involves maintaining a buffer of selected samples from previous tasks, which are replayed during the training of new tasks. The challenge lies in optimizing the selection and storage of these samples due to limited memory capacity. Techniques like Reservoir Sampling[42] provide basic solutions by selecting representative samples, while more advanced methods dynamically adjust the buffer content to capture the"}, {"title": "2.3. Optimization-Based Methods", "content": "Optimization-driven techniques alleviate the issue of forgetting by meticulously modifying the optimization framework during training to safeguard knowledge from antecedent tasks. These techniques frequently entail adjustments to gradient updates to avert interference from recently introduced tasks. A prevalent strategy is gradient projection, which aligns the present gradient updates with those of preceding tasks or projects them onto orthogonal subspaces to minimize conflict. Gradient Episodic Memory (GEM)[16] is a classic optimization approach, which ensures that the gradient associated with a new task $\\nabla L_{new} (\\theta)$ remains congruent with earlier gradients $\\nabla L_{old}(\\theta)$, thereby upholding the criterion $\\nabla L_{new} (\\theta) \\cdot \\nabla L_{old} (\\theta) \\geq 0$. This approach aids in averting the deterioration of previously acquired knowledge. Additional methodologies, such as Orthogonal Gradient Descent (OGD)[44] and Gradient Projection Memory (GPM)[45], enhance this by projecting the gradient updates onto subspaces that are orthogonal to the significant gradient vectors of past tasks, thus preserving crucial information while facilitating new learning tasks.\nMeta-learning, often referred to as \"learning to learn\" represents a pivotal concept in the machine learning field. The main goal of using the meta-learning is to improve the model's responsiveness to novel tasks by leveraging previous experiences. Model-Agnostic Meta-Learning (MAML)[46] focuses on optimizing parameters that enable swift fine-tuning for emerging tasks while minimizing the risk of knowledge retention loss. Methodologies such as OML[47] and La-MAML[48] harmoniously merge meta-learning with continual learning, blending gradient modifications with experience replay to strike a balance between the preservation of established knowledge and the assimilation of new information.\nIn conclusion, optimization-based techniques provide a resilient framework for continual learning by regulating gradient adjustments to maintain previously acquired knowledge or by utilizing meta-learning to improve flexibility. Nevertheless, these approaches may demand significant computational resources, which makes them particularly applicable in small-size machines and devices."}, {"title": "2.4. Representation-Based Methods", "content": "Representation-based approaches mitigate the issue of forgetting by cultivating common representations that are transferable across various tasks, which in turn minimizes interference and safeguards knowledge retention. Generally, the lower layers of the network are responsible for acquiring these shared features h(x), whereas the specialized classifiers $f_t(h(x))$ situated in the upper layers are tasked with executing predictions for each individual task t.\nRecent innovations have integrated self-supervised learning (SSL) and extensive pre-training to significantly improve representation learning. SSL methodologies, such as those utilized in LUMP[49] and Co2L[62], leverage contrastive loss to establish robust representations that are resilient against forgetting. Furthermore, dual-network frameworks such as DualNet [10] and CL-SLAM[63] synergize supervised and self-supervised learning to achieve a balance between generalization and stability. The efficacy of large-scale pre-training is also notable, as models that are trained on comprehensive datasets tend to exhibit greater resilience to forgetting and demonstrate superior knowledge transfer capabilities to novel tasks. Nonetheless, a prominent challenge persists in the adaptation of these pre-trained representations to new tasks while preserving their wide applicability. This issue is navigated through various strategies that depend on whether the pre-trained features remain static or are dynamically adjusted during new task learning. Additionally, continual pre-training and meta-training methodologies allow models to incrementally refine their representations as fresh data is introduced. For example, merging techniques like Barlow Twins with Elastic Weight Consolidation (EWC)[64] facilitates learning from incremental data, thereby enhancing the model's aptitude for accommodating new tasks, whereas methodologies such as IncCLIP[50] perpetually update multi-modal models through the replay of generated samples.\nIn conclusion, representation-based approaches offer a strong framework for continual learning through the creation of shared representations that effectively generalize across various tasks. Although they are efficient, these methods necessitate meticulous oversight of the equilibrium between stability and adaptability, particularly when addressing a wide range of diverse tasks."}, {"title": "2.5. Architecture-Based Methods", "content": "Architecture-driven techniques tackle the forgetting problem in continual learning by adaptively altering the model's structure when learning new tasks. This methodology often consists of augmenting the network with additional layers or modules tailored to each task, thereby safeguarding the newly acquired knowledge. Mathematically, this is realized by dynamically building the task-specific parameters $\\theta_t$ for learning a new task. The overall model's parameters can be formulated as the union of task-specific parameter sets $\\Theta = \\bigcup_{t=1}^T \\Theta_t$, in which each $\\Theta_t$ preserves the information for a specific task. A most popular architecture-based approach is Progressive Neural Networks (PNNs)[51], where new network channels are added for each task learning. Such a mechanism prevents catastrophic forgetting and also enables learning a growing number of tasks. However, the PNNS can lead to considerable computational complexity and memory costs when learning a long sequence of tasks. Dynamically Expandable Networks (DEN)[52] and PathNet[53] offer more refined solutions by selectively growing"}, {"title": "2.6. Information-Theoretic Approaches in Continual Learning", "content": "Recent studies have integrated information-theoretic principles to improve memory rehearsal strategies in continual learning[56, 57, 58, 59, 65]. By employing criteria such as \"surprise\" and \"learnability,\" they aim to retain diverse and relevant information while balancing computational efficiency[56, 57]. These methods leverage online selection strategies, Bayesian models, and contrastive learning to maintain a representative subset of data, which helps mitigate catastrophic forgetting and enhances model robustness in imbalanced or dynamic learning environments[58, 59]. Entropy, a measure that assesses the uncertainty or randomness of a dataset, is crucial for ensuring diversity during sample selection. Moreover, mutual information evaluates the shared information between variables, which is vital for identifying and maintaining relevant features while minimizing redundancy. Although information-theoretic methods have been extensively utilized in domains such as feature selection, clustering, and anomaly detection, their potential in continual learning still have rooms to improve. By leveraging these principles, the effectiveness of memory buffers in rehearsal-based strategies can be significantly enhanced, leading to a comprehensive and representative data subset for continual learning applications.\nTo summarize, although each of these methodologies presents distinct benefits in tackling the forgetting problem in continual learning, they are accompanied by trade-offs relating to computational resources, memory demands, and model intricacy. The approach we propose is grounded in these principles, incorporating information-theoretic concepts into both data subset selection and memory buffer optimization, with the goal of improving the model's capacity for addressing forgetting in continual learning.\nBy conducting thorough experimental assessments, we corroborate the effectiveness of our methodology in enhancing the performance of continual learning models, especially in scenarios characterized by imbalanced data. Our findings introduce a comprehensive framework for refining memory buffer strategies and strengthening the resilience of CL models against the phenomenon of catastrophic forgetting."}, {"title": "3. Methodology", "content": "3.1. Problem definition\nIn continual learning, a model can not access the whole training dataset at one time and is learnt on a dynamically changing data stream. Let us define ${T_1, T_2, ..., T_N}$ as a series of N tasks, where each task $T_i$ contains a labelled training dataset $D_i = \\{(x_j^t,y_j^t)\\}_{j=1}^{n_t}$ and a testing dataset $D_i^t = \\{(x_j^t,y_j^t)\\}_{j=1}^{n^{t_i}}$, where $n^i$ and $n^{t_i}$ denote the total"}, {"title": "3.2. Information-Theoretic Dual Memory System", "content": "Most existing memory-based methods usually consider managing a single memory system to preserve many critical data samples [66], which would not easily capture both long- and short-term knowledge during the whole learning procedure. In addition, using a single memory buffer would implement the memory optimization strategy by evaluating all memorized samples, leading to considerable computational costs. In this paper, we introduce a novel memory approach for continual learning, consisting of a"}, {"title": "3.3. The Optimization Strategy for the Slow Memory Buffer", "content": "In this section, we present a novel strategy for optimizing memory usage in the slow memory buffer. The objective is to choose a fixed number of samples from the dataset $D_i$, which can accurately represent its underlying data structure. To determine which samples should be incorporated into the slow memory buffer, we propose assigning a selection weight (represented as a binary variable) to each sample. A weight of \"1\" signifies that the corresponding sample has a high likelihood of being included in the slow memory buffer. To facilitate the selection of the most pertinent data samples, we first define an information cost function that assigns a high score to the most important samples, as expressed by the following equation :\n$J(X^i) = \\lambda_{H_2} \\cdot H_2(X^i) + \\lambda_{CS} \\cdot D_{CS} (X^i, \\tilde{X}^i),$ (7)\nwhere $\\lambda_{H_2} \\in \\mathbb{R}^+$ and $\\lambda_{CS} \\in \\mathbb{R}^+$ are two hyperparameters controlling the importance of the second-order Renyi entropy term and Cauchy-Schwar divergence term, respectively. The term $H_2(X^i)$ in Eq. (7) aims to estimate the diversity of the data samples and is useful to address imbalanced data scenarios. The term $D_{CS}(X^i, \\tilde{X}^i)$ in Eq. (7) is used to quantify the discrepancy between the selected subset and the real training dataset. Let w be a selection weight vector, where each $w_j \\in (0,1)$ denotes the j-th dimension of w and is a selection probability for the j-th sample $\\{x_j,y_j\\}$ from the dataset $D_i$. The sample selection process can be formulated as an optimization procedure :\n$\\min L_{info} (X^i),$ (8)\n$L_{info} (X^i) = \\lambda_{H_2} H_2 (X^i_w) + \\lambda_{CS}D_{CS} (X^i, \\tilde{X}^i_w) + r (w).$\nCompared to Eq. (7), Eq. (8) involves a regularization term r(w) that is defined as :\nr (w) = \\lambda_{ksp}|s_n - \\sum_{j=0}^{n_i} w_j | + \\lambda_{L_1} \\sum_{i=0}^{n_i} w_j\n- \\lambda_{H} \\sum_{j=0}^{n_i} (w_j \\cdot log w_j + (1 \u2013 w_j) \\cdot log (1 \u2013 w_j)) .$ (9)\nWe have found through empirical analysis that omitting the regularization term r(w) in Eq. (8) leads each sample weight to gravitate towards 1 throughout the optimization process. Consequently, this prevents us from selecting a suitable subset from the"}, {"title": "3.4. Memory Allocation Mechanism via A Balanced Sample Selection Approach", "content": "To optimize the utilization of the proposed dual memory system, we ensure that both the fast and slow memory buffers are employed to retain as many data samples as feasible until the total of stored samples, represented as $|M_i^{slow}| + |M_i^{fast}|$, reaches the predetermined maximum memory capacity, $M_{max}$. Here, $M_i^{slow}$ and $M_i^{fast}$ refer to the slow and fast memory components updated at the i-th task learning, respectively. The quantities $|M_i^{slow}|$ and $|M_i^{fast}|$ denote the total number of samples retained in each memory buffer. After each task switch, it is necessary for the slow memory buffer to purge a portion of the stored samples to create sufficient memory capacity for the integration of new data from the current task. Consequently, the slow memory buffer can effectively gather essential data samples across all tasks over time.\nThe central challenge associated with the removal of memorized samples from $M_i^{slow}$ lies in the fact that the slow memory buffer would eliminate varying quantities of samples across different categories, potentially resulting in a data imbalance issue. To overcome this challenge, we introduce an innovative Balanced Sample Selection (BSS) approach designed to appropriately remove samples from each category, thereby ensuring a more"}, {"title": "3.5. The Algorithm Framework", "content": "Given that the proposed dual memory architecture can seamlessly integrate with current continual learning techniques with minimal alterations, we propose to establish our memory system based on a widely recognized and fundamental baseline known as DER++ [18]. This baseline utilizes a singular memory buffer and employs reservoir sampling [67] for memory updates. In this study, we substitute the singular memory buffer of DER++ with the proposed dual memory framework, and we define the"}, {"title": "4. Experiment", "content": "We conduct experiments in continual learning, focusing on three principal scenarios : Task Incremental Learning (Task-IL), Class Incremental Learning (Class-IL), and Domain Incremental Learning (Domain-IL).\nIn the Task-IL scenario, each training task operates with an independent label space. Conversely, in the Class-IL scenario, the tasks share a common label space. During evaluation, the model in the Task-IL framework receives the label space corresponding to the current task, while in the Class-IL approach, the model remains unaware of the specific task to which the sample belongs. In practice, datasets such as CIFAR-10 and Tiny ImageNet are segmented into 5 and 10 tasks, respectively, introducing 2 and 20 new classes per task in a consistent and fixed sequence across various iterations.\nDomain-IL entails learning across multiple domains that share the same label space, yet exhibit different data distributions. The central challenge in these scenarios is ensuring that the model continues to learn while retaining previously acquired knowledge, amidst the ongoing evolution of data distributions. For this purpose, we employ two established protocols derived from the MNIST dataset: Permuted MNIST, which involves random pixel shuffling, and Rotated MNIST, which rotates images by a random angle within [0, \u03c0). The total number of tasks in both protocols is 20.\n4.1. Evaluation Framework\nArchitecture: In the context of the MNIST configurations, a multilayer perceptron (MLP) consisting of two hidden layers, each with 400 neurons, along with 2,000 memory"}, {"title": "4.2. Experiment Results on the Balanced Data Stream", "content": "In this section, we assess our model's efficacy utilizing balanced datasets and compare it against several established methodologies, including regularization-based techniques (such as oEWC and SI), knowledge distillation methods (e.g., iCaRL and LwF), a"}, {"title": "4.3. Experiment Results on the Imbalanced Data Stream", "content": "In Section 4.2, we evaluate the effectiveness of our proposed ITDMS framework under balanced data stream conditions by comparing its continual learning performance"}, {"title": "4.4. Analysis Results", "content": "In the first, we construct an empirical experiment to show the property of the proposed memory system. We consider creating a Gaussian mixture distribution dataset, which consists of 1,000 data points categorized into four classes, each following a mixture Gaussian distribution with relatively fixed spatial locations. This dataset can simulate the distributional differences between the classes for a general dataset. We report the results in Fig. 5. The dispersion of the original data points represents the overall diversity of the dataset, while the density between different groups indicates latent information characteristics. Our goal is for the model to identify high-value samples that are both diverse and representative of the overall dataset distribution by optimizing the sample weights.\nIn the simulation results, Fig. 6 effectively illustrates the changes in sample weight distribution during the optimization process. Assuming that we want the model to select 100 high-value sample points, the sample weights are initially set to the same constant at the beginning of each optimization process, ensuring equal importance for all samples and preventing the algorithm from converging to suboptimal solutions due to random initial weight assignments. As the optimization progresses, the sample weights are gradually clustered into several parts then back to a concentrated cluster. This indicates that the model optimizes the weights to identify an optimal subset of samples, in which the weights for most of the unimportant samples are near zero while the weights for the remaining samples approximate 1.\nTo further explain the internal optimization phase of the proposed memory system, we"}, {"title": "4.5. Ablation Experiment", "content": "To achieve a more nuanced comprehension of the model's internal workings while improving its interpretability and transparency, we consider constructing an ablation study aimed at investigating and comparing the impact of the proposed memory optimization approach on the model's performance. First, we consider creating a baseline model that employs the ITDMS memory system but replaces the proposed memory optimization approach using the reservoir sampling, namely ITDMS-reservoir. In addition, we also do not use the proposed BBS strategy to maintain balanced samples for the ITDMS-reservoir. As a result, the ITDMS-reservoir employs two memory buffers, which are based on reservoir sampling. Furthermore, we employ the same hyperparameter configuration for both ITDMS and ITDMS-reservoir."}, {"title": "5. Conclusion and Future Works", "content": "The results derived from the CLS theory framework suggest that information is processed through both rapid and gradual learning mechanisms. Building on this foundation, we propose the implementation of these mechanisms via the Information-Theoretic Dual Memory System (ITDMS), which comprises both a slow and a fast memory buffer. The fast memory buffer utilizes a reservoir sampling technique to dynamically replace outdated memorized samples with new data. Additionally, this paper introduces an information-theoretic memory optimization strategy that assesses the quality of each sample according to an information cost function, offering a systematic approach for selecting and retaining the most critical data in the slow memory buffer. Moreover, we present a novel balanced sample selection method that allows the memory system to flexibly allocate memory capacity for the storage of new samples. Empirical results from a series of experiments illustrate that the proposed approach, when integrated into existing continual learning models, can significantly enhance their performance.\nThe fundamental constraint of the proposed method lies in its inability to manage an unbounded array of tasks due to the limitations inherent in both the model and memory capacity. To mitigate this issue, one viable approach is to introduce an innovative dynamic memory expansion strategy that progressively enhances the slow memory buffer's"}]}