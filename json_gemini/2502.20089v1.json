{"title": "RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning", "authors": ["Adib Karimi", "Mohammad Mehdi Ebadzadeh"], "abstract": "We introduce a novel Inverse Reinforcement Learning (IRL) approach that overcomes limitations of fixed reward assignments and constrained flexibility in implicit reward regularization. By extending the Maximum Entropy IRL framework with a squared temporal-difference (TD) regularizer and adaptive targets, dynamically adjusted during training, our method indirectly optimizes a reward function while incorporating reinforcement learning principles. Furthermore, we integrate distributional RL to capture richer return information. Our approach achieves state-of-the-art performance on challenging MuJoCo tasks, demonstrating expert-level results on the Humanoid task with only 3 demonstrations. Extensive experiments and ablation studies validate the effectiveness of our method, providing insights into adaptive targets and reward dynamics in imitation learning.", "sections": [{"title": "Introduction", "content": "Inverse Reinforcement Learning (IRL) [Abbeel and Ng, 2004] is a foundational paradigm in artificial intelligence, enabling agents to acquire complex behaviors by observing expert demonstrations. This approach has catalyzed progress in diverse domains such as robotics [Osa et al., 2018], autonomous driving [Knox et al., 2023], and drug discovery [Ai et al., 2024]. By eliminating the need for explicitly defined reward functions, IRL provides a practical framework for training agents in environments where designing such functions is infeasible.\nA prominent framework in IRL is Maximum Entropy (MaxEnt) IRL [Ziebart, 2010], which underpins many state-of-the-art (SOTA) IL methods. Prior works have combined MaxEnt IRL with adversarial training [Ho and Ermon, 2016, Fu et al., 2018] to minimize divergences between agent and expert distributions. However, these adversarial methods often suffer from instability during training. To address this, recent research has introduced implicit reward regularization, which indirectly represents rewards via Q-value functions by inverting the Bellman equation. For instance, IQ-Learn [Garg et al., 2021] unifies reward and policy representations using Q-functions with an L2-norm regularization on rewards, while LSIQ [Al-Hafez et al., 2023] minimizes the chi-squared divergence between expert and mixture distributions, resulting in a squared temporal difference (TD) error objective analogous to SQIL [Reddy et al., 2020]. Despite its effectiveness, this method has limitations: LSIQ assigns fixed rewards (e.g., +1 for expert samples and -1 for agent samples), which constrains flexibility by treating all tasks and state-action pairs uniformly, limiting performance and requiring additional gradient steps for convergence.\nWe present an extension of implicit reward regularization under the MaxEnt IRL framework through two methodological innovations: (1) adaptive target rewards ($\\lambda^{\\pi_E}, \\lambda^{\\pi}$) that dynamically adjust during training to replace static targets, enabling context-sensitive alignment of expert and policy data, and (2) distributional RL integration, where value distributions $Z^{\\pi}(s,a)$ [Bellemare et al., 2017] are trained to capture richer return information while preserving theoretical consistency with Q-values, as the expectation of the value distribution is used for policy and value function optimization. By unifying adaptive target regularization with distributional insights [Zhou et al., 2023], our framework addresses prior rigidity in reward learning and empirically outperforms online IL baselines on MuJoCo benchmarks [Todorov et al., 2012], marking a novel integration of distributional RL into non-adversarial IRL.\nOur contributions are as follow:\n\u2022 We introduce adaptive target rewards for implicit reward regularization to enable dynamic adjustments during training.\n\u2022 We leverage value distributions instead of classical Q-functions to capture richer insights from return distributions.\n\u2022 We demonstrate the effectiveness of our approach through extensive experiments on MuJoCo tasks, providing detailed analyses of implicit rewards and adaptive target mechanisms."}, {"title": "Related Works", "content": "Imitation learning (IL) and inverse reinforcement learning (IRL) [Watson et al., 2023] are foundational paradigms for training agents to mimic expert behavior from demonstrations. Behavioral Cloning (BC) [Pomerleau, 1991], the simplest IL approach, treats imitation as a supervised learning problem by directly mapping states to expert actions. While computationally efficient, BC is prone to compounding errors [Ross and Bagnell, 2011] due to covariate shift during deployment. The Maximum Entropy IRL framework [Ziebart, 2010] addresses this limitation by probabilistically modeling expert behavior as reward maximization under an entropy regularization constraint, establishing a theoretical foundation for modern IRL methods.\nThe advent of adversarial training marked a pivotal shift in IL methodologies. Ho and Ermon [2016] introduced Generative Adversarial Imitation Learning (GAIL), which formulates imitation learning as a generative adversarial game [Goodfellow et al., 2014] where an agent learns a policy indistinguishable from the expert's by minimizing the Jensen-Shannon divergence between their state-action distributions. This framework was generalized by Ghasemipour et al. [2019] in f-GAIL, which replaces the Jensen-Shannon divergence with arbitrary f-divergences to broaden applicability. Concurrently, Kostrikov et al. [2019] proposed Discriminator Actor-Critic (DAC), improving sample efficiency via off-policy updates and terminal-state reward modeling while mitigating reward bias.\nRecent advances have shifted toward methods that bypass explicit reward function estimation."}, {"title": "Background", "content": "We consider a Markov Decision Process (MDP) [Puterman, 2014] to model policy learning in Rein-forcement Learning (RL). The MDP framework is defined by the tuple (S, A, po, P, R, y), where S denotes the state space, A the action space, po the initial state distribution, $P : S \\times A \\times S \\rightarrow [0,1]$ the transition kernel with $P(\\cdot | s, a)$ specifying the likelihood of transitioning from state s given action a, $R : S \\times A \\rightarrow \\mathbb{R}$ the reward function, and \u03b3 \u2208 [0,1] the discount factor which tempers future rewards. A stationary policy \u03c0\u2208 \u03a0is characterized as a mapping from states s \u2208 S to distributions over actions a \u2208 A. The primary objective in RL [Sutton and Barto, 2018] is to maximize the expected sum of discounted rewards, expressed as $E_{\\pi} [\\sum_{i=0}^{\\infty} \\gamma^i R(s_t, a_t)]$. Furthermore, the occupancy measure $\\rho_{\\pi}(s,a)$ for a policy \u03c0\u2208 \u03a0 is given by $(1 - \\gamma) \\pi(a | s) \\Sigma_t \\gamma^t P(s_t = s | \\pi)$. The corresponding measure for an expert policy, $\u03c0_E$, is similarly denoted by $\u03c1_E$. In Imitation Learning (IL), it is posited that the expert policy $\u03c0_\u0118$ remains unknown, and access is restricted to a finite collection of expert demonstrations, represented as (s,a,s')."}, {"title": "Distributional Reinforcement Learning", "content": "Maximum Entropy (MaxEnt) RL [Haarnoja et al., 2018] focuses on addressing the stochastic nature of action selection by maximizing the entropy of the policy, while Distributional RL [Bellemare et al., 2017] emphasizes capturing the inherent randomness in returns. Combining these perspectives, the distributional soft value function $Z^{\\pi} : S \\times A \\rightarrow \\mathcal{Z}$ [Ma et al., 2020] for a policy \u03c0\u2208 \u03a0 encapsulates uncertainty in both rewards and actions. It is formally defined as:\n$Z^{\\pi} (s,a) = \\sum_{t=0}^{\\infty} \\gamma^t[R(s_t,a_t) + \\alpha H(\\pi(\\cdot | s_t)],$\\nwhere $\\mathcal{Z}$ represents the space of action-value distributions, $H(\\pi) = E_{\\pi}[- log \\pi(a | s)]$ denotes the entropy of the policy, and \u03b1 > 0 balances entropy with reward.\nThe distributional soft Bellman operator $B^{\\pi} : \\mathcal{Z} \\rightarrow \\mathcal{Z}$ for a given policy \u03c0 is introduced as"}, {"title": "Inverse Reinforcement Learning", "content": "Given expert trajectory data, Maximum Entropy (MaxEnt) Inverse RL [Ziebart, 2010] aims to infer a reward function R(s, a) from the family R = RS\u00d7A. Instead of assuming a deterministic expert policy, this method optimizes for stochastic policies \u03c0\u2208 \u03a0 that maximize R while matching expert behavior. Ho and Ermon [2016] extend this framework by introducing a convex reward regularizer $\\psi : \\mathbb{R}^{S \\times A} \\rightarrow \\mathbb{R}$, leading to the adversarial objective:\n$\\max_{R \\in \\mathcal{R}} \\min_{\\pi \\in \\Pi} L(\\pi, R) = E_{\\rho_E} [R(s, a)] \u2013 E_{\\rho_\\pi}[R(s,a)] \u2013 H(\\pi) \u2013 \\psi(R)$\nIQ-Learn [Garg et al., 2021] departs from adversarial training by implicitly representing rewards through Q-functions $Q \\in \\Omega$ [Piot et al., 2014]. It leverages the inverse soft Bellman operator $T^{\\pi}$, defined as:\n$(T^{\\pi}Q)(s,a) = Q(s,a) \u2013 \\gamma E_{s'~P(\u00b7\\s,a),a'~\\pi(\\cdot\\s')} [Q(s', a') \u2013 \\alpha log \\pi(a'|s')]$\nFor a fixed policy \u03c0, $T^{\\pi}$ is bijective, ensuring a one-to-one correspondence between Q-values and rewards: $T^{\\pi}Q = R$ and $Q = (T^{\\pi})^{-1}R$. This allows reframing the MaxEnt IRL objective in Q-policy space as $\\max_{Q \\in \\Omega} \\min_{\\pi \\in \\Pi} \\mathcal{I}(\\pi, Q)$. IQ-Learn simplifies the problem by defining the implicit reward $R_Q(s,a) = T^{\\pi}Q(s, a)$ and applying an L2 regularizer $\\psi(R_Q)$. The final objective becomes:\n$J(\\pi, Q) = E_{\\rho_E} [R_Q(s,a)] \u2013 E_{\\rho_\\pi}[R_Q(s,a)] \u2013 H(\\pi) \u2013 c (E_{\\rho_E} [R_Q^2] + E_{\\rho_\\pi}[R_Q^2])$."}, {"title": "Methodology", "content": "In this section, we present a systematic framework for developing our proposed approach. We begin by integrating Distributional RL with Imitation Learning (IL). Next, we introduce a novel regularization technique that enhances existing Implicit Reward methods through an adaptive reward mechanism. Finally, we propose our algorithm, which leverages the strengths of both Distributional RL and IL to achieve superior performance in complex environments."}, {"title": "Distributional Value Function", "content": "Our method diverges from traditional imitation learning by modeling value distributions as critics in an actor-critic framework [Zhou et al., 2023]. We propose that learning the full return distribution $Z^{\\pi} (s, a)$\u2014rather than point estimates like $Q^{\\pi} (s, a)$\u2014enables better decision-making in complex environments. By optimizing policies using the expectation of $Z^{\\pi}(s, a)$, we derive a more robust learning signal. This robustness stems from explicitly capturing environmental stochasticity through distributional modeling, avoiding the limitations of Q-networks that collapse variability into deterministic estimates. In Distributional RL, the action-value function is defined as $Q^{\\pi}(s,a) = E[Z^{\\pi}(s,a)]$, preserving the mathematical properties required by the IQ-Learn framework, ensuring theoretical consistency (see Lemma A.1 for more details)."}, {"title": "Implicit Reward Regularization", "content": "In this section, we propose a novel regularizer for Inverse Reinforcement Learning (IRL) that refines existing implicit reward formulations [Garg et al., 2021]. We define the implicit reward as $R_Q(s, a) = Q_\\theta(s,a) \u2013 \\gamma V^{\\pi}(s')$, where $V^{\\pi} (s') = Q_\\theta(s', a') \u2013 \\alpha log \\pi(a'|s')$. Prior work often regularizes implicit rewards using L2-norms, treating them as squared Temporal Difference (TD) errors between rewards and fixed targets [Reddy et al., 2020, Al-Hafez et al., 2023]. While adopting a similar squared TD framework [Mnih et al., 2015], we introduce adaptive targets $\\lambda^{\\pi_E}$ (for expert policy $\u03c0_E$) and $\\lambda^{\\pi}$ (for imitation policy \u03c0) to derive our convex regularizer $\\Gamma : \\mathbb{R}^{S \\times A} \\rightarrow \\mathbb{R}$:\n$\\Gamma(R_Q, \\Lambda) = E_{\\rho_E} [(R_Q \u2013 \\lambda^{\\pi_E})^2] + E_{\\rho_{\\pi}} [(R_Q \u2013 \\lambda^{\\pi})^2]$.\nLemma 4.1. The implicit reward structure $R_Q$ and regularizer \u0393 guarantee:\n\u2022 Bounded Rewards: $R_Q$ converges to a convex combination of adaptive targets $\\lambda^{\\pi_E}$ and $\\lambda^{\\pi}$.\n\u2022 Temporal Consistency: Ensures $|Q(s, a) \u2013 \\gamma Q(s',a')| \\leq |\\lambda| + |\\epsilon| + \\gamma \\alpha|log \\pi(a'|s')|$,\nwhere e is the optimization tolerance.\nThis framework stabilizes learning by tethering rewards $R_Q$ to adaptive targets that evolve with the policy, unlike static methods (e.g., LSIQ/SQIL) that use fixed rewards (\u00b11). By co-optimizing $R_Q$ and A, we establish dynamic equilibrium\u2014targets adapt to the current policy to avoid brittle regular-ization while maintaining bounded Q-values. The temporal consistency property ensures smooth value function updates, which directly translates to robust policy updates\u2014reducing variance and improving the likelihood of convergence to the optimal policy. This dual adaptability distinguishes our approach from prior rigid regularization schemes."}, {"title": "Algorithm", "content": "We introduce RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning, an algorithm enhancing implicit reward IRL through a convex regularizer inspired by TD error minimization. Unlike standard RL that uses environmental rewards, RIZE employs self-updating adaptable targets, creating a somewhat reinforcement paradigm where rewards automatically align with their moving targets through regularization (see Algorithm 1)."}, {"title": "Critic Updates", "content": "Let $Z_{\\phi}(s, a)$ denote the return distribution and $\u03c0_\\theta(a|s)$ the policy. We compute Q-values as:\n$Q^{\\pi}(s,a) = \\sum_{i=0}^{N-1} (\\tau_{i+1} \u2013 \\tau_i)Z_{\\phi,\\tau_i} (s,a)$\nwhere \u03c4\u1d62 are quantile fractions and $Z_{\\phi,\\tau} (s, a)$ represents quantile-specific return distributions.\nThe core objective integrates a squared-error regularizer \u0393(RQ, \u03bb) to stabilize target alignment:\n$\\mathcal{L}(\\pi, Q) =E_{\\rho_E}[R_Q] \u2013 E_{\\rho_{\\pi}}[R_Q] \u2013 H(\\pi)$\n$- c [E_{\\rho_E} [(R_Q \u2212 \\lambda^{\\pi_E})^2] + E_{\\rho_{\\pi}}[(R_Q \u2212 \\lambda^{\\pi})^2]]$\nwhere implicit rewards RQ derive from: $R_Q(s,a) = Q^{\\pi} (s,a) \u2013 \\gamma [Q^{\\pi} (s', a') \u2013 \\alpha log \\pi(a'|s')]$"}, {"title": "Adaptive Target Updates", "content": "Targets $\\lambda^{\\pi_E}$ and \u5165\u2122 self-update via the following objectives, creating a feedback loop where reward estimates continuously adapt to match their moving targets.\n$\\min_{\\lambda^{\\pi_E}} E_{\\rho_E} [R_Q \u2013 \\lambda^{\\pi_E}]^2$, $\\min_{\\lambda^{\\pi}} E_{\\rho_{\\pi}} [R_Q \u2013 \\lambda^{\\pi}]^2$"}, {"title": "Experiments", "content": "We evaluate our algorithm on four MuJoCo [Todorov et al., 2012] benchmarks (HalfCheetah-v2, Walker2d-v2, Ant-v2, Humanoid-v2) against state-of-the-art imitation learning methods: IQ-Learn [Garg et al., 2021], LSIQ [Al-Hafez et al., 2023], and SQIL [Reddy et al., 2020]. All experiments are conducted with five seeds for statistical significance [Henderson et al., 2018]. To assess sample efficiency, we test each method using three and ten expert trajectories. For ten trajectories, we retain baseline hyperparameters; for three trajectories (not reported in prior works), we adapt configurations from single-demonstration settings. IQ-Learn and LSIQ use their official implementations, while SQIL is evaluated using the LSIQ codebase. Results report mean performance across five seeds, with half a standard deviation to indicate uncertainty, and lines are smoothed for better readability. We normalize episode returns based on expert performance."}, {"title": "Main Results", "content": "Our method outperforms LSIQ and SQIL across tasks, with IQ-Learn as the only competitive baseline. Notably, in the most complex environment, Humanoid-v2, our approach is the sole method achieving expert-level performance, while all baselines fail (see Figure 1). This demonstrates our algorithm's scalability to high-dimensional control problems. Additionally, our method shows superior sample efficiency, requiring fewer gradient steps to match expert performance compared to SOTA algorithms. These results highlight the robustness and efficiency of our approach in tackling complex tasks (see Figure 2)."}, {"title": "Reward Dynamics Analysis", "content": "Our implicit reward regularizer \u0393(RQ, \u03bb) ensures proximity between the learned Ro and target rewards \u03bb. As illustrated in Figure 3, when provided with sufficient expert trajectories (ten demonstrations), the rewards for both the expert and the policy stabilize around consistent values, as observed in environments like HalfCheetah and Ant. While our method is not Adversarial IL, it extends MaxEnt IRL principles. This alignment is evident as the policy increasingly mirrors expert behavior, causing the discriminator to converge and provide similar reward signals for both the expert and policy samples. This convergence reflects an equilibrium between policy optimization and reward learning, demonstrating the stability and effectiveness of our approach."}, {"title": "Ablation: Regularization Strategies", "content": "We investigate the effect of squared TD error regularization on the Ant-v2 task using three distinct formulations: Expert-focused TD: $E_{\\rho_E}[R_Q \u2013 \\lambda^{\\pi_E}]^2 + E_{\\rho_{\\pi}}[R_Q]^2$, Policy-focused TD: $E_{\\rho_E} [R_Q]^2 + E_{\\rho_{\\pi}} [R_Q \u2013 \\lambda^{\\pi}]^2$, Baseline L2: $E_{\\rho_E}[R_Q]^2 + E_{\\rho_{\\pi}}[R_Q]^2$."}, {"title": "Conclusion", "content": "We present a novel IRL framework that overcomes the rigidity of fixed reward mechanisms through dynamic reward adaptation via context-sensitive regularization and probabilistic return estimation with distributional RL. By introducing adaptive target rewards that evolve during training\u2014replacing static assignments\u2014our method enables nuanced alignment of expert and agent trajectories, while value distribution integration captures richer return dynamics without sacrificing theoretical consistency. Empirical evaluations on MuJoCo benchmarks demonstrate state-of-the-art performance, achieving expert-level proficiency on the Humanoid task with only three demonstrations, alongside ablation studies confirming the necessity of applying our regularization mechanism. This work advances imitation learning by unifying flexible reward learning with probabilistic return representations, offering a scalable, sample-efficient paradigm for complex decision making. Future directions include extending these principles to offline IL and risk-sensitive robotics, where adaptive rewards and uncertainty-aware distributions could further enhance robustness and generalization."}]}