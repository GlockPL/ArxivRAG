{"title": "RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning", "authors": ["Adib Karimi", "Mohammad Mehdi Ebadzadeh"], "abstract": "We introduce a novel Inverse Reinforcement Learning (IRL) approach that overcomes limitations of fixed reward assignments and constrained flexibility in implicit reward regularization. By extending the Maximum Entropy IRL framework with a squared temporal-difference (TD) regularizer and adaptive targets, dynamically adjusted during training, our method indirectly optimizes a reward function while incorporating reinforcement learning principles. Furthermore, we integrate distributional RL to capture richer return information. Our approach achieves state-of-the-art performance on challenging MuJoCo tasks, demonstrating expert-level results on the Humanoid task with only 3 demonstrations. Extensive experiments and ablation studies validate the effectiveness of our method, providing insights into adaptive targets and reward dynamics in imitation learning.", "sections": [{"title": "Introduction", "content": "Inverse Reinforcement Learning (IRL) [Abbeel and Ng, 2004] is a foundational paradigm in artificial intelligence, enabling agents to acquire complex behaviors by observing expert demonstrations. This approach has catalyzed progress in diverse domains such as robotics [Osa et al., 2018], autonomous driving [Knox et al., 2023], and drug discovery [Ai et al., 2024]. By eliminating the need for explicitly defined reward functions, IRL provides a practical framework for training agents in environments where designing such functions is infeasible.\nA prominent framework in IRL is Maximum Entropy (MaxEnt) IRL [Ziebart, 2010], which underpins many state-of-the-art (SOTA) IL methods. Prior works have combined MaxEnt IRL with adversarial training [Ho and Ermon, 2016, Fu et al., 2018] to minimize divergences between agent and expert distributions. However, these adversarial methods often suffer from instability during training. To address this, recent research has introduced implicit reward regularization, which indirectly represents rewards via Q-value functions by inverting the Bellman equation. For instance, IQ-Learn [Garg et al., 2021] unifies reward and policy representations using Q-functions with an L2-norm regularization on rewards, while LSIQ [Al-Hafez et al., 2023] minimizes the chi-squared divergence between expert and mixture distributions, resulting in a squared temporal difference (TD) error objective analogous to SQIL [Reddy et al., 2020]. Despite its effectiveness, this method has limitations: LSIQ assigns fixed"}, {"title": "Related Works", "content": "Imitation learning (IL) and inverse reinforcement learning (IRL) [Watson et al., 2023] are foundational paradigms for training agents to mimic expert behavior from demonstrations. Behavioral Cloning (BC) [Pomerleau, 1991], the simplest IL approach, treats imitation as a supervised learning problem by directly mapping states to expert actions. While computationally efficient, BC is prone to compounding errors [Ross and Bagnell, 2011] due to covariate shift during deployment. The Maximum Entropy IRL framework [Ziebart, 2010] addresses this limitation by probabilistically modeling expert behavior as reward maximization under an entropy regularization constraint, establishing a theoretical foundation for modern IRL methods.\nThe advent of adversarial training marked a pivotal shift in IL methodologies. Ho and Ermon [2016] introduced Generative Adversarial Imitation Learning (GAIL), which formulates imitation learning as a generative adversarial game [Goodfellow et al., 2014] where an agent learns a policy indistinguishable from the expert's by minimizing the Jensen-Shannon divergence between their state-action distributions. This framework was generalized by Ghasemipour et al. [2019] in f-GAIL, which replaces the Jensen-Shannon divergence with arbitrary f-divergences to broaden applicability. Concurrently, Kostrikov et al. [2019] proposed Discriminator Actor-Critic (DAC), improving sample efficiency via off-policy updates and terminal-state reward modeling while mitigating reward bias.\nRecent advances have shifted toward methods that bypass explicit reward function estimation."}, {"title": "Background", "content": "We consider a Markov Decision Process (MDP) [Puterman, 2014] to model policy learning in Reinforcement Learning (RL). The MDP framework is defined by the tuple (S, A, po, P, R, y), where S denotes the state space, A the action space, po the initial state distribution, P : S \u00d7 A \u00d7 S \u2192 [0,1] the transition kernel with P(\u00b7 | s, a) specifying the likelihood of transitioning from state s given action a, R : S \u00d7 A \u2192 R the reward function, and y \u2208 [0,1] the discount factor which tempers future rewards.\nA stationary policy \u03c0\u2208 \u03a0is characterized as a mapping from states s \u2208 S to distributions over actions a \u2208 A. The primary objective in RL [Sutton and Barto, 2018] is to maximize the expected sum of discounted rewards, expressed as \\(E_\\pi [\\sum_{i=0}^\\infty \\gamma^iR(s_t, a_t)]\\). Furthermore, the occupancy measure \u03c1\u03c0(s,a) for a policy \u03c0\u2208 \u03a0 is given by \\((1 - \\gamma)\\pi(a | s) \\Sigma_{t=0}^\\infty \\gamma^tP(s_t = s | \\pi)\\). The corresponding measure for an expert policy, \u03c0E, is similarly denoted by PE. In Imitation Learning (IL), it is posited that the expert policy \u03c0\u0118 remains unknown, and access is restricted to a finite collection of expert demonstrations, represented as (s,a,s')."}, {"title": "Distributional Reinforcement Learning", "content": "Maximum Entropy (MaxEnt) RL [Haarnoja et al., 2018] focuses on addressing the stochastic nature of action selection by maximizing the entropy of the policy, while Distributional RL [Bellemare et al., 2017] emphasizes capturing the inherent randomness in returns. Combining these perspectives, the distributional soft value function Z\u03c0 : S \u00d7 A \u2192 Z [Ma et al., 2020] for a policy \u03c0\u2208 \u03a0 encapsulates uncertainty in both rewards and actions. It is formally defined as:\n\\[\nZ^\\pi(s,a) = \\sum_{t=0}^\\infty \\gamma^t[R(s_t,a_t) + \\alpha H(\\pi(\\cdot | s_t)],\n\\tag{1}\n\\]\nwhere Z represents the space of action-value distributions, \\(H(\\pi) = E_{\\pi}[-\\log \\pi(a | s)]\\) denotes the entropy of the policy, and a > 0 balances entropy with reward.\nThe distributional soft Bellman operator B\u03c0 : Z \u2192 Z for a given policy \u03c0 is introduced as"}, {"title": "Inverse Reinforcement Learning", "content": "Given expert trajectory data, Maximum Entropy (MaxEnt) Inverse RL [Ziebart, 2010] aims to infer a reward function R(s, a) from the family R = RS\u00d7A. Instead of assuming a deterministic expert policy, this method optimizes for stochastic policies \u03c0\u2208 \u03a0 that maximize R while matching expert behavior. Ho and Ermon [2016] extend this framework by introducing a convex reward regularizer 4 : IRS\u00d7A \u2192 I\u0158, leading to the adversarial objective:\n\\[\n\\max_{R \\in \\mathcal{R}} \\min_{\\pi \\in \\Pi} L(\\pi, R) = E_{\\rho_E} [R(s, a)] \u2013 E_{\\rho_\\pi}[R(s,a)] \u2013 H(\\pi) \u2013 \\psi(R)\n\\tag{2}\n\\]\nIQ-Learn [Garg et al., 2021] departs from adversarial training by implicitly representing rewards through Q-functions Q \u2208 \u03a9 [Piot et al., 2014]. It leverages the inverse soft Bellman operator T\u03c0, defined as:\n\\[\n(T\\pi Q)(s,a) = Q(s,a) \u2013 \\gamma E_{s'~P(:\\s,a),a'~\\pi(\\cdot\\s')} [Q(s', a') \u2013 a log \\pi(a'|s')]\n\\tag{3}\n\\]\nFor a fixed policy \u03c0, T\u03c0 is bijective, ensuring a one-to-one correspondence between Q-values and rewards: TQ = R and Q = (T\u03c0)-1R. This allows reframing the MaxEnt IRL objective in Q-policy space as maxQ\u2208n min\u03c0\u03b5\u03a0 I (\u03c0, Q). IQ-Learn simplifies the problem by defining the implicit reward Ro(s,a) = TQ(s, a) and applying an L2 regularizer \u03c8(RQ). The final objective becomes:\n\\[\nJ(\\pi, Q) = E_{\\rho_E} [R_Q(s,a)] \u2013 E_{\\rho_\\pi}[R_Q(s,a)] \u2013 H(\\pi) \u2212 c (E_{\\rho_E} [R_Q^2] + E_{\\rho_\\pi}[R_Q^2]).\n\\tag{4}\n\\]"}, {"title": "Methodology", "content": "In this section, we present a systematic framework for developing our proposed approach. We begin by integrating Distributional RL with Imitation Learning (IL). Next, we introduce a novel regularization technique that enhances existing Implicit Reward methods through an adaptive reward mechanism. Finally, we propose our algorithm, which leverages the strengths of both Distributional RL and IL to achieve superior performance in complex environments."}, {"title": "Distributional Value Function", "content": "Our method diverges from traditional imitation learning by modeling value distributions as critics in an actor-critic framework [Zhou et al., 2023]. We propose that learning the full return distribution Z\u03c0 (s, a) rather than point estimates like Q\u03c0 (s, a)\u2014enables better decision-making in complex environments. By optimizing policies using the expectation of Z\u03c0(s, a), we derive a more robust learning signal. This robustness stems from explicitly capturing environmental stochasticity through distributional modeling, avoiding the limitations of Q-networks that collapse variability into deterministic estimates. In Distributional RL, the action-value function is defined as Q\u03c0(s,a) = E[Z\u03c0(s,a)], preserving the mathematical properties required by the IQ-Learn framework, ensuring theoretical consistency (see Lemma A.1 for more details)."}, {"title": "Implicit Reward Regularization", "content": "In this section, we propose a novel regularizer for Inverse Reinforcement Learning (IRL) that refines existing implicit reward formulations [Garg et al., 2021]. We define the implicit reward as Ro(s, a) = Qt(s,a) \u2013 \u03b3Vt(s'), where Vt (s') = Qt(s', a') \u2013 a log \u03c0(a'|s'). Prior work often regularizes implicit rewards using L2-norms, treating them as squared Temporal Difference (TD) errors between rewards and fixed targets [Reddy et al., 2020, Al-Hafez et al., 2023]. While adopting a similar squared TD framework [Mnih et al., 2015], we introduce adaptive targets \u03bb\u03c0E (for expert policy \u03c0\u314c) and \u03bb\u03c0 (for imitation policy \u03c0) to derive our convex regularizer \u0413 : RS\u00d7A \u2192 I\u0158:\n\\[\n\\Gamma(R_Q, \\lambda) = E_{\\rho_E} [(R_Q \u2013 \\lambda^{\\pi_E})^2] + E_{\\rho_\\pi} [(R_Q \u2013 \\lambda^\\pi)^2] .\n\\tag{5}\n\\]\nLemma 4.1. The implicit reward structure Ro and regularizer \u0393 (Equation 5) guarantee:\n\u2022 Bounded Rewards: Ro converges to a convex combination of adaptive targets \u03bb\u03c0E and \u03bb\u03c0.\n\u2022 Temporal Consistency: Ensures |Q(s, a) \u2013 yQ(s',a')| \u2264 |\u03bb| + |e| + \u03b3\u03b1|log \u03c0(a'|s')|,\nwhere e is the optimization tolerance.\nThis framework stabilizes learning by tethering rewards RQ to adaptive targets that evolve with the policy, unlike static methods (e.g., LSIQ/SQIL) that use fixed rewards (\u00b11). By co-optimizing Ro and A, we establish dynamic equilibrium\u2014targets adapt to the current policy to avoid brittle regularization while maintaining bounded Q-values. The temporal consistency property ensures smooth value function updates, which directly translates to robust policy updates\u2014reducing variance and improving the likelihood of convergence to the optimal policy. This dual adaptability distinguishes our approach from prior rigid regularization schemes."}, {"title": "Algorithm", "content": "We introduce RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning, an algorithm enhancing implicit reward IRL through a convex regularizer inspired by TD error minimization. Unlike standard RL that uses environmental rewards, RIZE employs self-updating"}, {"title": "Experiments", "content": "We evaluate our algorithm on four MuJoCo [Todorov et al., 2012] benchmarks (HalfCheetah-v2, Walker2d-v2, Ant-v2, Humanoid-v2) against state-of-the-art imitation learning methods: IQ-Learn [Garg et al., 2021], LSIQ [Al-Hafez et al., 2023], and SQIL [Reddy et al., 2020]. All experiments are conducted with five seeds for statistical significance [Henderson et al., 2018]. To assess sample efficiency, we test each method using three and ten expert trajectories. For ten trajectories, we retain baseline hyperparameters; for three trajectories (not reported in prior works), we adapt configurations from single-demonstration settings. IQ-Learn and LSIQ use their official implementations, while SQIL is evaluated using the LSIQ codebase. Results report mean performance across five seeds, with half a standard deviation to indicate uncertainty, and lines are smoothed for better readability. We normalize episode returns based on expert performance."}, {"title": "Main Results", "content": "Our method outperforms LSIQ and SQIL across tasks, with IQ-Learn as the only competitive baseline. Notably, in the most complex environment, Humanoid-v2, our approach is the sole method achieving expert-level performance, while all baselines fail (see Figure 1). This demonstrates our algorithm's scalability to high-dimensional control problems. Additionally, our method shows superior sample efficiency, requiring fewer gradient steps to match expert performance compared to SOTA algorithms. These results highlight the robustness and efficiency of our approach in tackling complex tasks (see Figure 2)."}, {"title": "Reward Dynamics Analysis", "content": "Our implicit reward regularizer \u0393(RQ, \u03bb) (Equation 5), ensures proximity between the learned Ro and target rewards \u03bb. As illustrated in Figure 3, when provided with sufficient expert trajectories (ten demonstrations), the rewards for both the expert and the policy stabilize around consistent values, as observed in environments like HalfCheetah and Ant. While our method is not Adversarial IL, it extends MaxEnt IRL principles. This alignment is evident as the policy increasingly mirrors expert behavior, causing the discriminator to converge and provide similar reward signals for both the expert and policy samples. This convergence reflects an equilibrium between policy optimization and reward learning, demonstrating the stability and effectiveness of our approach."}, {"title": "Ablation: Regularization Strategies", "content": "We investigate the effect of squared TD error regularization on the Ant-v2 task using three distinct formulations: Expert-focused TD: E\u03c1\u03b5[RQ \u2013 \u03bb\u03c0E]2 + \u0388\u03c1\u03c0[RQ]\u00b2, Policy-focused TD: Ep\u025b [RQ]\u00b2 + \u0395\u03c1\u03c0 [RQ\n\u03bb\u03c0]2, Baseline L2: \u0388\u03c1\u03b5[RQ]\u00b2 + E\u03c1\u03c0[RO]\u00b2."}, {"title": "Conclusion", "content": "We present a novel IRL framework that overcomes the rigidity of fixed reward mechanisms through dynamic reward adaptation via context-sensitive regularization and probabilistic return estimation with distributional RL. By introducing adaptive target rewards that evolve during training\u2014replacing static assignments\u2014our method enables nuanced alignment of expert and agent trajectories, while value distribution integration captures richer return dynamics without sacrificing theoretical consistency. Empirical evaluations on MuJoCo benchmarks demonstrate state-of-the-art performance, achieving expert-level proficiency on the Humanoid task with only three demonstrations, alongside ablation studies confirming the necessity of applying our regularization mechanism. This work advances imitation learning by unifying flexible reward learning with probabilistic return representations, offering a scalable, sample-efficient paradigm for complex decision making. Future directions include extending these principles to offline IL and risk-sensitive robotics, where adaptive rewards and uncertainty-aware distributions could further enhance robustness and generalization."}, {"title": "Proofs", "content": "Lemma A.1. Consider an MDP with discount factor y \u2208 [0,1) and rewards bounded by some constant C > 0\nsuch that |R(s,a)| \u2264 C. Define the return distribution as\n\\[\nZ(s,a) = \\sum_{t=0}^\\infty \\gamma^tR(s_t, a_t),\n\\]\nwhere the trajectory {(st,at)} is generated by following policy n under dynamics P. Then the classical action-value function is given by\n\\[\nQ^\\pi (s,a) = E_{\\pi,\\rho}[Z(s,a)].\n\\]\nMoreover, if we approximate Z(s, a) by a learned distribution Zo(s, a) with expectation Q(s,a) = E[Ze(s,a)], then under the conditions of the Dominated Convergence Theorem (DCT), Q(s, a) converges to Qt (s,a).\nProof of Lemma A.1.\nExistence of the Expectation: Since the rewards are bounded, we have |R(st,at)| \u2264 C, and the discounted series satisfies\n\\[\nE \\bigg[\\sum_{t=0}^\\infty \\gamma^tR(s_t, a_t)\\bigg] < \\sum_{t=0}^\\infty \\gamma^tC = \\frac{C}{1-\\gamma} < \\infty.\n\\]\nThis absolute convergence allows us to apply DCT:\n\\[\nE[Z(s,a)] = E \\bigg[\\sum_{t=0}^\\infty \\gamma^tR(s_t, a_t)\\bigg] = \\sum_{t=0}^\\infty \\gamma^tE[R(s_t, a_t)].\n\\]\nEquivalence to Q-value: By definition,\n\\[\nQ^\\pi (s, a) = E_{\\pi,\\rho} \\bigg[\\sum_{t=0}^\\infty \\gamma^tR(s_t, a_t) \\bigg] = E[Z(s,a)].\n\\]\nConvergence of the Learned Distribution: In distributional reinforcement learning, if we represent the return distribution using Ze(s, a), then its mean is given by\n\\[\n\\hat{Q}(s,a) = E[Z_{\\theta}(s,a)].\n\\]\nUnder DCT conditions, we have\n\\[\n\\hat{Q}(s,a) \\rightarrow Q^\\pi (s,a).\n\\]"}, {"title": "Bounded Rewards", "content": "The expert and policy reward targets \u03bb\u03c0E, \u03bb\u03c0 are learned via:\n\\[\n\\lambda^{\\pi_E} \\leftarrow E_{\\rho_E} [R_Q], \\quad \\lambda^{\\pi} \\leftarrow E_{\\rho_\\pi}[R_Q].\n\\]\nThis ensures \u03bb\u03c0E and \u03bb\u03c0 track the empirical mean of RQ.\nFixing \u03bb\u03c0E and \u03bb\u2122, solving \u2207R\u2116\u0413 = 0 yields:\n\\[\nR_Q = \\frac{\\rho_E \\lambda^{\\pi_E} + \\rho_\\pi \\lambda^{\\pi}}{\\rho_E + \\rho_\\pi}\n\\]\nThis is a convex combination of \u03bb\u03c0E and \u03bb\u03c0. Since \u03bb\u03c0E, \u03bb\u03c0 are empirical means, |Ro| \u2264 max(|\u03bb\u03c0E|,|\u03bb\u03c0|), ensuring boundedness."}, {"title": "Temporal Consistency", "content": "The reward Ro (for either expert or policy samples) is defined via the soft Q-function:\n\\[\nR_Q = Q(s,a) \u2013 \\gamma E_{s'} [Q(s', a') \u2013 a log \\pi(a'|s')] .\n\\]\nRearranging gives:\n\\[\nQ(s,a) = R_Q + \\gamma E_{s'} [Q(s', a') \u2013 a log \\pi(a'|s')] + \\epsilon,\n\\]\nwhere e is the Bellman optimization error (due to approximation or finite samples). Substituting\nRo = \u03bb:\n\\[\nQ(s, a) \u2013 \\gamma Q(s', a') = \\lambda + \\epsilon \u2212 \\gamma a \\log \\pi(a'|s').\n\\]\nTaking absolute values:\n\\[\n|Q(s,a) \u2013 \\gamma Q(s', a')| \\le |\\lambda| + |\\epsilon| + \\gamma a |\\log \\pi(a'|s')|.\n\\]\nThis bounds successive Q-values, ensuring temporal consistency."}, {"title": "Experiments", "content": "We evaluate our imitation learning approach, RIZE, on four benchmark Gym [Brockman et al., 2016] MuJoCo [Todorov et al., 2012] environments: HalfCheetah-v2, Walker2d-v2, Ant-v2, and Humanoid-v2. Expert trajectories are generated using a pretrained Soft Actor-Critic (SAC) [Haarnoja et al., 2018] agent, with each trajectory consisting of 1,000 state-action transitions. To facilitate performance comparisons, episode returns are normalized relative to expert performance: HalfCheetah (5100), Walker2d (5200), Ant (4700), and Humanoid (5300)."}, {"title": "Implementation Details", "content": "Our architecture integrates components from Distributional SAC (DSAC)2 [Ma et al., 2020] and IQ-Learn\u00b3 [Garg et al., 2021], with hyperparameters tuned through systematic search and ablation studies. Key configurations for experiments involving three demonstrations are summarized in Table 1, while Table 2 provides settings for scenarios with ten demonstrations. Our implementation can be found at https://github.com/adibka/RIZE.\nDistributional SAC Components: The critic network is implemented as a three-layer multilayer perceptron (MLP) with 256 units per layer, trained using a learning rate of 3 \u00d7 10-4. The policy network is a four-layer MLP, also with 256 units per layer. Environment-specific learning rates are applied: 1 \u00d7 10-5 for the Humanoid environment and 5 \u00d7 10\u20135 for all others. To enhance training stability, we employ a target policy\u2014a delayed version of the online policy\u2014and sample next-state actions from this module. For value distribution training Z\u03c6,\u03c4, we adopt the Implicit Quantile Networks (IQN) [Dabney et al., 2018a] approach by sampling quantile fractions \u03c4 uniformly from U(0,1). Additionally, dual critic networks with delayed updates are used, which empirically improve training stability.\nIQ-Learn Adaptations: Key adaptations from IQ-Learn include adjustments to the regularizer coefficient c and entropy coefficient a. Specifically, for the regularizer coefficient c, we find that c = 0.5 yields robust performance on the Humanoid task, while c = 0.1 works better for other tasks. For the entropy coefficient a, smaller values lead to more stable training. Unlike RL, where exploration is crucial, imitation learning relies less on entropy due to the availability of expert data. Across all tasks, we set initial target reward parameters as \u03bb\u03c0E = 10 and \u03bb\u03c0 = 5. Furthermore, we observe that lower learning rates for target rewards improve overall learning performance.\nPrevious implicit reward methods such as IQLearn, ValueDICE, and LSIQ\u2074 have employed distinct modifications to the loss function. In our setup, two main loss variants are defined:"}, {"title": "Value loss:", "content": "\\[\nL(\\pi, Q) = E_{\\rho_E} [Q^\\pi(s,a) \u2013 \\gamma V^\\pi(s')] \u2013 E_{\\rho}[V^\\pi(s) \u2013 \\gamma V^\\pi (s')] \u2013 c\\Gamma(R_Q, \\lambda)\n\\tag{9}\n\\]"}, {"title": "v0 loss:", "content": "\\[\nL(\\pi, Q) = E_{\\rho_E} [Q^\\pi(s,a) \u2013 \\gamma V^\\pi(s')] \u2013 (1 \u2013 \\gamma)E_{\\rho_0} [V^\\pi(s_0)] \u2013 c\\Gamma(R_Q, \\lambda)\n\\tag{10}\n\\]"}, {"title": "Extended Main Results", "content": "In this section, we showcase the performance of our method, RIZE, using both ten and three expert demonstrations. RIZE outshines LSIQ and SQIL in various tasks, with IQ-Learn as its only competitor. Remarkably, it reaches expert-level performance in the challenging Humanoid-v2 environment with just three trajectories, proving its scalability. Plus, it requires fewer training steps than other algorithms, emphasizing its efficiency and robustness in complex situations. Our findings demonstrate that using fewer expert samples does not negatively impact our imitation learning approach (see Figure 5)."}, {"title": "Extended Recovered Rewards", "content": "Here, we investigate the implicit reward dynamics of our method using both three and ten expert demonstrations. As illustrated in Figure 6, with ample expert trajectories (n=10), the rewards for both"}, {"title": "Hyperparameter Fine-Tuning", "content": "We present our analysis and comparison of important hyperparameters utilized in our algorithm. As before, all experiments use five seeds, and we show mean and half a standard deviation of all the seeds. As a challenging task among MuJoCo environments, we only experiment with Ant-v2."}]}