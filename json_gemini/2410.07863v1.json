{"title": "Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games", "authors": ["Fanqi Kong", "Yizhe Huang", "Song-Chun Zhu", "Siyuan Qi", "Xue Feng"], "abstract": "Real-world multi-agent scenarios often involve mixed motives, demanding altruistic agents capable of self-protection against potential exploitation. However, existing approaches often struggle to achieve both objectives. In this paper, based on that empathic responses are modulated by inferred social relationships between agents, we propose LASE (Learning to balance Altruism and Self-interest based on Empathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games. LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship\u2014a metric evaluating the friendliness of co-players estimated by counterfactual reasoning. In particular, social relationship measures each co-player by comparing the estimated Q-function of current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking module. Comprehensive experiments are performed in spatially and temporally extended mixed-motive games, demonstrating LASE's ability to promote group collaboration without compromising fairness and its capacity to adapt policies to various types of interactive co-players.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) has exhibited impressive performance in numerous collaborative tasks and zero-sum games such as MPE, StarCraft, and Google Research Football [19, 25, 38]. These environments involve a predefined competitive or cooperative relationship between agents. Besides, mixed-motive games are prevalent, in which the relationships between agents are non-deterministic and dynamic. That is, agents could cooperate with some co-players and simultaneously compete with someone else. Furthermore, along with interactions, friends may turn into foes, and vice versa. In such games, to maximize self-interest, agents need to cooperate altruistically in some relationships while keep self-interested to avoid being exploited in some others. Consequently, in mixed-motive environments, the ability to balance altruism and self-interest according to social relationships is crucial for agent performance.\n\nThe commonly used CTDE (Centralized Training and Decentralized Execution) methods [31,29] in MARL focus on global optimization goals and necessitate individual information sharing with centralized controllers, which is impractical for self-interest agents in mixed-motive games. On the other hand, simply training self-interest agents in a decentralized way may converge to local"}, {"title": "2 Related work", "content": "In multi-agent learning, the game-theoretic notion of social dilemmas has been generalized from the classic two-player matrix-form game Tab. 1 to sequential social dilemmas, a spatial-temporally-extended complex behavior learning setting [15, 4]. Various approaches have been proposed to foster cooperative behavior among agents to advance societal welfare. One approach incorporates the rewards of others as intrinsic rewards into one's own optimization objectives, with the ratio of intrinsic to extrinsic rewards determined by different methods, such as pre-defined social value orientations like altruistic or prosocial [21, 23], introducing the concept of inequity aversion [11], or learning in a model-free way [33]. However, this approach depends on direct access to others' reward functions, which may not be feasible in realistic mixed-motive games. Another line of work dispenses with this assumption, allowing agents to model others and influence them through their actions [6, 17, 13, 8, 10]. Here, we employ a more direct form of opponent shaping named gifting."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Partially observable Markov games", "content": "We consider an N-player partially observable Markov game (POMG) [27, 18], M =(N, S, {O}, {A}, P, {R}), where N represents the number of agents, $s \\in S$ represents the state of the environment. In the partially observable setting, agent i only obtains the local observation $o^i \\in O^i$ based on the current state s. Each agent i learns an independent policy $\\pi^i (a^i|o^i)$ to select actions and form a joint action $a = (a^1, ..., a^N) \\in A^1 \\times \\cdot\\cdot\\cdot \\times A^N$, resulting in the state change from s to s' according to the transition function $P : S \\times A^1 \\times \\cdot\\cdot\\cdot \\times A^N \\rightarrow \\Delta(S)$, where $\\Delta(S)$ represents the set of probability distributions over the set S. Agent i receives an individual extrinsic reward $r^i = R^i(s,a^1,\\dots, a^n)$ and tries to maximize a long-term return:\n\n$V_\\pi^i(s_0) = E_{a_t,s_{t+1}\\sim P(s_t,a_t)}[\\sum_{t=0}^{\\infty} \\gamma^t R^i(s_t, a_t)],$ (1)\n\nwhere the variables in bold represent the joint information of all agents, and $\\gamma$ is the discount factor."}, {"title": "3.2 Policy Gradient Learning", "content": "In decentralized MARL, each self-interest agent i learns an independent policy $\\pi^i$ parameterized by $\\theta^i$. The optimization objective is to maximize the expected return in Eq. 1. We use the policy-based Actor-Critic method as the learning algorithm for our agents. The gradient for actor is\n$\\$\\nabla_{\\theta} J(\\theta) = E_{\\pi_{\\theta}}[\\sum_{t=0}^T \\gamma^t \\nabla_{\\theta} log \\pi_{\\theta}(a_t|o_t)], where $V_t$ represents the critic's evaluation of the actor. The most popular form of $V_t$ is TD-error: $V_t = r_t + \\gamma V^{\\pi_{\\theta}}(S_{t+1}) - V^{\\pi_{\\theta}}(S_t)."}, {"title": "4 Methodology", "content": "To balance altruism and self-interest in mixed-motive games, we propose a distributed MARL algorithm LASE which empathically shares rewards with co-players based on inferred social relationships. The architecture of LASE is illustrated in Fig. 1, composed of two main modules: Social Relationship Inference (SRI) and Gifting. SRI conducts counterfactual reasoning to get the social relationships with co-players, which reflects the impact of co-players' actions on LASE's return. SRI compares the Q-value (estimated by the SR value network) of the current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking (PT) module. PT is provided to address the challenge of predicting co-players' policies in partially observable and decentralized environments. In particular, PT consists of an observation conversion network, simulating the co-player j's observation $\\hat{o}^j$ from the local observation $o^i$, and an SR policy network, learning a function from $\\hat{o}$ to inferred j's policy.\n\nThe Gifting module, according to the inferred social relationships, determines the amount of reward sharing with others. Meanwhile, agents receive others' gifts and get the final reward $r^{i,tot}$, which"}, {"title": "4.1 Zero-Sum Gifting", "content": "Here, we use the zero-sum gifting mechanism which adheres to the principle of \"what I give is what I lose\" [20], indicating that the overall rewards for the group remain constant. Notably, gifting is an autonomous decision, wherein any agent i holds a gifting weight vector at time step t: $w_t^i = [w_{ij}^i]_{j=1}^N$, where $w_{ij}^i \\in [0,1]$ and $\\sum_{j=1}^N w_{ij}^i = 1$. $w_{ij}^i$ is the fraction of agent i's reward to gift to j. It is exactly the social relationship computed as Eq. 3. For an N-player group using the zero-sum gifting mechanism, $r_t$ denotes the extrinsic rewards vector obtained through interactions with the environment at timestep t. Agent i's total reward is computed by\n\n$r_t^{i,tot}(w_t, r_t) = \\sum_{j=1}^N w_{ji}^j r_t^j,$ (2)\n\nThe policy $\\pi^i(a^i|o^i)$ is trained to maxmize $E_{\\pi^i}[\\sum_{t=0}^T r_t^{i,tot}]$ using the TD-error introduced in Section 3.2 by replacing $r_t$ with $r_t^{i,tot}$."}, {"title": "4.2 Social Relationships Inference", "content": "The social relationship $w^{ij}$, modeled as a continuous variable, measures the impact of co-player j's action on i's Q-value. Based on counterfactual reasoning, $w^{ij}$ is inferred as:\n\n$w^{ij} = \\frac{Q^i(o, a) - \\sum_{a^j} \\pi^j(a^j |o^i)Q^i(o, (a^{-j}, a^j))}{M},$ (3)\n\nwhere $Q^i(o, a)$, estimated by the SR value network, is i's Q-value of its local observation and the joint action. Eq. 3 compares $Q^i(o, a)$ with a counterfactual baseline, which is the weighted sum of Q-values, with j taking all possible actions $a^j$ while the other agents' actions $a^{-j}$ fixed. The weight $\\pi^j(a^j |o^i)$ is j's action probability distribution. The denominator is for normalization: $M =(N - 1)(max_{a^j} Q^i(o, (a^{-j}, a^j)) - min_{a^j} Q^i(o, (a^{-j}, a^j)))$, where $(N - 1)$ ensures $w^{ij} < \\frac{1}{N-1}$.\n\nAfter gifting, LASE keeps the remaining reward for itself: $w^{ii} = 1 - \\sum_{j=1, j\\neq i}^N w^{ij}$.\n\nDue to partial observability and decentralized learning, i is unable to accurately obtain j's policy $\\pi^j$ and its observation $o^j$. So we utilize PT module to get the prediction: $\\pi^j(a^{j'} |o) \\approx \\pi_{SR}^{\\mu^i}(a^{j'} |\\hat{o}^j)$. $\\pi_{SR}^{\\mu^i}$ is the SR policy network parameterized by $\\mu^i$ which predicts j's actions conditioned on the simulated obervation $\\hat{o}^j$ learned by the observation conversion network. The observation conversion network, parameterized by $\\eta^i$, enables LASE to adopt the perspective of others and generate a simulated observation of them. At timestep t, LASE processes its own observation $o_t^i$ along with another agent"}, {"title": "4.3 Analysis in Iterated Matrix Game", "content": "We use iterated matrix games to theoretically analyze LASE's learning process. The iterative matrix game is to play multiple rounds of a single game with the payoff matrix shown in Tab. 1, where both players get a payoff of R by mutual cooperation (C) and P by mutual defection (D). If one player defects and the other cooperates, the defector receives a reward of T, while the cooperator receives a reward of S. We normalize R to 1 and S to 0, and let $0 < T < 2, \u22121 < S \u2264 1$ which is shown to be sufficient to characterize the three kinds of dilemmas in Tab. 3 [26].\n\nWe carry out a closed-from gradient descent analysis on LASE in the two-player iterated matrix games and derive the policy update rule Eq. 15 and Eq. 16, where each agent i optimizes the reward after gifting $r^{i,tot}$. The detailed deduction is provided in Appendix A. Then we simulate the policy update iteratively with random initial value and plot the LASE's cooperation probability after convergence under various game parameters as illustrated in Fig. 2.\n\nThe results demonstrate that LASE effectively enhances the cooperation probability of the two players, achieving convergence to a domain that surpasses the random policy (choosing cooperation with a probability of 0.5), thereby promoting the improvement in the cooperation probability of different kinds of dilemmas."}, {"title": "5 Experimental setup", "content": ""}, {"title": "5.1 Environments", "content": "Iterated Prisoner's Dilemma (IPD). Here, we use iterated prisoner's dilemma (IPD) as an illustration to validate the theoretical analysis of LASE conducted in Section 4.3 and Appendix A. The specific game"}, {"title": "5.2 Implementations", "content": "We employ fully decentralized training and execution for the agents, where all network parameters are independent. The policy structure of the agent comprises 2 convolutional layers for encoding observations, an LSTM layer to capture temporal information, and several fully connected layers activated by ReLU. The input is a multi-channel binary tensor, with the specific number of channels determined by the characteristics of different environments. For example, in Cleanup, 7 channels are incorporated, wherein the first four channels denote the positions of the four agents, the fifth and sixth"}, {"title": "5.3 Baselines", "content": "Independent advantage actor-critic labeled A2C [22] is a classical gradient-based RL algorithm. LIO [36] learns an incentive function through the learning updates of reward recipients. LOLA [6] considers the learning process of other agents when updating its own policy parameters. IA [11, 32] modifies the individual reward function by introducing inequity aversion. SI [13, 32] achieves coordination by rewarding agents for having causal influence over other agents' actions. We also show the approximate upper bound on performance by training the group optimal (GO) agents to maximize the collective reward. And we conduct ablation experiments LASE w/o, by removing the observation conversion network and replacing $\\pi^j(a^{j'}|o)$ in Eq. 3 with $1/|A|$."}, {"title": "6 Results", "content": ""}, {"title": "6.1 LASE promotes cooperation in social dilemmas", "content": "In IPD, as shown in Fig. 4, LASE successfully escapes the dilemma of non-efficient Nash Equilibrium (D, D). Both LASE agents converge to cooperate with a high probability, around 0.93 (see Fig. 4a), accompanied by a high collective reward. This is consistent with the theoretical results shown in Fig. 2, validating the effectiveness of LASE in dealing with social dilemmas. LASE is better at convergence speed and stability than LOLA, which also achieves a high collective reward. Unsurprisingly, GO, aiming to maximize group reward, reaches the upper bound. On the other hand, A2C, optimizing for one's own return, easily falls into the Nash equilibrium, where everyone defects and the group reward reduces to the minimum."}, {"title": "6.2 LASE promotes fairness", "content": "Considering the fact that the division of labor in Cleanup is more pronounced than in other environments and that cooperation in Cleanup is purely altruistic, making the dilemma more challenging, we take Cleanup as an example to show LASE's ability to promote fairness. Fig. 6a and Fig. 6b show the extrinsic rewards of each agent and the amount of waste cleaned by each in Cleanup. We find Agent 4 is the only one to clean and does not receive any extrinsic reward.\n\nFig. 6c illustrates the gifting weights each agent received from the other three agents. Agent 4 gets the most gifts, indicating that its cleaning contribution to the team is recognized and rewarded by the other three agents. Fig. 6d shows reward curves after gifting, where the reward gap between Agent 4 and the other three agents shrinks, implying fairness within the group is improved."}, {"title": "6.3 LASE distinguishes co-players and responds adaptively", "content": "To evaluate LASE's adaptation ability to interact with various types of agents, we conduct an experiment in which a focal LASE agent interacts with three rule-based agents: cooperator (always clean up waste), defector (always try to collect apples), and a random agent. The gifting weights of LASE to the other agents are shown in Fig. 8. LASE can explicitly distinguish between different types of co-players. Moreover, it responds in a manner that aligns with human values: preferring to share rewards with cooperators rather than defectors.\n\nTo study how LASE responds dynamically and how it affects collective behavior, we conduct an experiment where one focal LASE agent interacts with three A2C agents (Background agents, Bgs). A GO agent is trained in the same way for comparison with LASE. Fig. 9a displays each agent's rewards after training for 30k episodes, whereas the LASE group shows the reward after gifting.\n\nLASE and GO improve the group reward to a similar level, while four A2C agents will converge to the equilibrium of defection and gain almost no reward (see Fig. 5d). With gifting, LASE incentives Bg3 to clean as shown in Fig. 9b. Thus, LASE and the other two Bgs can get rewards by gathering apples. But for GO, the focal agent sacrifices itself to undertake all the cleaning tasks and cannot get any reward.\n\nGO and LASE represent two different methods to foster cooperation. GO sacrifices its own interest to promote collective reward. LASE attempts to alleviate social dilemmas by incentivizing others to cooperate. When such an incentive mechanism fails, LASE will no longer gift to those agents who constantly exploit it, like the defector in Fig. 8. Overall, we believe that LASE is a more efficient and secure policy for SSDs, as it can promote cooperation as well as avoid potential exploitation by others."}, {"title": "7 Conclusion", "content": "We introduce LASE, a decentralized MARL algorithm that fosters cooperation through gifting while safeguarding individual interests in mixed-motive games. LASE uses counterfactual reasoning to infer the social relationships with others which captures the influences of others' actions on LASE and modulates the gifting strategy empathetically. In particular, to empower LASE with the ability to infer others' policies in partially observable and decentralized environments, we establish a perspective taking module for LASE. Both theoretical analyses in matrix-form games and experimental results across diverse SSDs show that LASE can effectively promote cooperative behavior while ensuring relative fairness within the group. Furthermore, LASE is also able to recognize various types of co-players and adjust its gifting strategy adaptively to avoid being exploited. Whilst LASE exhibits"}, {"title": "8.1 Validating the environments", "content": "In this section, we will show that our four environments are all sequential social dilemmas defined in [11]: An N-player sequential social dilemma is a tuple (\u041c, \u041f = \u041f\u0109 \u3129 \u041f\u0430\u0105) of a Markov game and two disjoint sets of policies, said to implement cooperation and defection respectively, satisfying the following properties. Consider the strategy profile (\u03c0,..., \u03c0\u03bf, \u03c0\u03b1,..., \u03c0\u03b7) \u2208 \u03a0 \u00d7 \u03a0 with l + m = N. We shall denote the average payoff for the cooperating policies by Re(l) and for the defecting policies by Ra(l). (M, II) is a sequential social dilemma iff the following hold:\n\nMutual cooperation is preferred over mutual defection: R(N) > Ra(0).\nMutual cooperation is preferred to being exploited by defectors: Rc(N) > Rc(0).\nEither the fear property, the greed property, or both:\nFear: mutual defection is preferred to being exploited. Ra(i) > Re(i) for sufficiently small i.\nGreed: exploiting a cooperator is preferred to mutual cooperation. Ra(i) > Rc(i) for sufficiently large i.\n\nA Schelling diagram is a game representation that highlights interdependencies between agents, showing how the choices of others shape one's own incentives. It plots the curves Re(l + 1) and Ra(l) as shown in Fig. 10. All the environments satisfy the first two properties of sequential social dilemmas: Re(N) > Rd(0) and Rc(N) > Ra(0). In SSH, fear promotes defection: Ra(0) > Re(0). In Cleanup and SSG, the problem is greed: Ra(3) > Rc(3). Coingame suffers from both temptations to defect. This indicates that our experimental environments include all three different types of sequential social dilemmas corresponding to Tab. 3."}, {"title": "B.2 Environment details", "content": "IPD. Each agent makes decisions based on the actions of the two players in the previous step, so this is a fully observable environment unlike other environments. We trained for 10k episodes, each with 100 steps.\n\nCoingame. Map size is 5 \u00d7 5. Agent's action space is A = {up, down, left, right}. The state is represented as a 5 \u00d7 5 \u00d7 5 binary tensor. The five channels are {blue agent, red agent, blue coin, red coin, mask}, where the first two channels encode the location of each agent, the last channel distinguishes between parts within and beyond the boundary, and the other two channels encode the location of the coin if any exist. If two agents walk on the coin at the same time, one of them is randomly selected to successfully pick it up. Cooperative agents will only try to collect coins with their own color, while self-interested agents tend to greetingly collect all coins. Each episode lasts for 100 steps.\n\nCleanup. Map size is 8 \u00d7 8. A = {up, down, left, right, stay, clean, pick}, where the last two actions require the agent to be in the same position as the waste or the apple. The seven channels are {agent 1, ..., agent 4, waste, apple, mask}, The parameters with the same meaning as the open-source implementation about Cleanup [32] are shown in Tab. 4. To achieve cooperation, agents need to take the initiative to undertake part of the cleaning task to help improve the group's revenue. A defecting agent will just keep waiting for the apple to grow and gather it. Each episode lasts for 100 steps.\n\nSSH. Map size is 8 \u00d7 8. A = {up, down, left, right, stay, hunt hare, hunt stag}. Agents must be in the same position as the prey to hunt and the prey doesn't respawn after being hunted. The seven channels are {agent 1, ..., agent 4, hare, stag, mask}. Cooperative agents are happy to hunt deer with others, while defectors only hunt rabbits to avoid the risk of not getting a payoff. Each episode lasts for 30 steps.\n\nSSG. Map size is 8 \u00d7 8. A = {up, down, left, right, stay, remove snowdrift}. Agents must be in the same position as the snowdrift to remove it and the removed snowdrift doesn't respawn. The six channels are {agent 1, ..., agent 4, snowdrift, mask}. Cooperators will proactively remove snowdrifts to bring high rewards to the team, while defectors will just wait for others to remove them. Each episode lasts for 50 steps."}, {"title": "C Implementation", "content": "The pseudocode for the LASE algorithm is shown in Algorithm 1:\n\nSSDs. The actor-critic model interacting with environments utilizes two cascaded CNNs to process input data, with a kernel size of 3, stride of size 1 and 16/32 output channels. This is connected to one fully connected layer of size 128 activated by ReLU, and an LSTM with 128 cells. The actor head and critic head are two separate fully-connected layers that output the softmax normalized action policy and a scalar value respectively. The implementation of the intrinsic policy network and value network in OM is basically the same, but due to the need to further judge the actions of others, joint action space is added to the value network input dimension. In the observation transformation network, the input data is passed through a CNN with a kernel of size 3, stride of size 1 and 16 output channels, and is concatenated with a one-hot vector representing the agent index to input in two FC"}, {"title": "D Additional Results", "content": ""}, {"title": "D.1 Estimate the uncertainty of their social relationship", "content": "We select the $w_{ji}$ data from the last $10^6$ timesteps of training to calculate their mean value and standard deviation, which estimates the uncertainty of social relationships. The calculation method is as follows:\n\n$\\overline{w_{ji}} = \\frac{\\sum_{t=T_{max} - 10^6}^{T_{max}} w_{ji}^t}{1e^6} ,  \\overline{\\overline{w_{i}}} = \\frac{\\sum_{j,j\\neq i} \\overline{w_{ji}}}{n(n-1)} , s = \\sqrt{\\frac{\\sum_{t=T_{max} - 10^6}^{T_{max}} (w_{ji}^t - \\overline{w_{ji}})^2}{1e^6 - 1}}$"}, {"title": "D.2 Compare the Equality with other baselines", "content": "As an evaluation metric, fairness should be evaluated alongside reward to measure algorithm performance effectively. Some algorithms may fail to address decision-making issues in mixed-motive games where each agent receives a small reward, but the reward disparity between agents is minimal, resulting in high fairness. Clearly, these methods are not effective. An effective method should both maximize group reward and ensure intra-group equity. Here, we include the fairness results of other baselines:"}, {"title": "E Broader Impact", "content": "The rapid advancement of AI technology has brought about an explosion in the number of agents, making it unfeasible to rely on a centralized controller to achieve coordinated collective behavior. The question of how to design independent agents that excel in specific tasks and can demonstrate adequate social behavior when interacting with humans or other agents remains an open challenge. We take a tentative step towards this problem by introducing the mechanism of gifting and the theory of empathy in human society. We believe our work will have a positive impact on the interaction of multi-agent systems, the alignment of AI with human values, and the development of AI safety."}, {"title": "A Analysis in Iterated Matrix Games", "content": "For a general iterated matrix game whose payoff matrix is computed according to Tab. 1 at each step, we can let \u03b8' for i \u2208 {1,2} denote each agent's probability of taking the cooperative action, and let \u00d4i for j \u2208 {2, 1} denote each agent's prediction of the other's policy. To avoid the difficulties caused by the coupled update of reinforcement learning for the theoretical analysis, we make the simplifying assumption that the three networks in SRI module have been fully trained. We further assume that this two-player game is fully observable, and we can make the following approximation to Eq. 3:\n\n$w^{ij} = \\frac{Q^i(o, a) - \\sum_{a^j} \\pi^j(a^j |\\hat{o}^i)Q^i(o, (a^{-j}, a^j))}{N - 1 max_{a^j} Q^i(o, (a^{-j}, a^j)) - min_{a^j} Q^i(o, (a^{-j}, a^j))}  \\approx \\frac{1}{N-1} \\frac{ri (a^i, a^j) - (\\hat{\\theta}^jri (a^i, C) + (1 - \\hat{\\theta}^j)ri (a^i, D))}{ri(a^i, C) - ri (a^i, D)},$ (6)\n\nHere, $r^i(a^i, a^j)$ represents player i's reward determined by the payoff matrix which only relies on the two players' actions without state or observation. The update rule Eq. 5 states that it is feasible to replace Q\u00b2 by r\u00b2. Then we can calculate the gifting weights under the four combinations of actions: CC, CD, DC, and DD. We take $w^{12}$ for example:\n\n$w_{CC}^{12} = \\frac{r^1 (C, C) - (\\hat{\\theta}^2r^1 (C, C) + (1 - \\hat{\\theta}^2)r^1 (C, D))}{r^1 (C, C) - r^1(C, D)} = \\frac{R - (\\hat{\\theta}^2 \\cdot R + (1 - \\hat{\\theta}^2 \\cdot S))}{R - S} = \\frac{1 - \\hat{\\theta}^2}{\\hat{R - S}},$ (7)\n\n$w_{CD}^{12} = \\frac{r^1 (C, D) - (\\hat{\\theta}^2r^1 (C, C) + (1 - \\hat{\\theta}^2)r^1(C, D))}{r^1 (C, C) - r^1(C, D)} = \\frac{S - (\\hat{\\theta}^2 \\cdot R + (1 - \\hat{\\theta}^2 \\cdot S))}{R - S} = \\frac{-\\hat{\\theta}^2}{R - S},$ (8)\n\n$w_{DC}^{12} = \\frac{r^1(D, C) - (\\hat{\\theta}^2r^1(D, C) + (1 - \\hat{\\theta}^2)r^1 (D, D))}{r^1(D, C) - r^1(D, D)} = \\frac{T - (\\hat{\\theta}^2 \\cdot T + (1 - \\hat{\\theta}^2 \\cdot P))}{T - P} = \\frac{1 - \\hat{\\theta}^2}{\\hat{T - P}},$ (9)\n\n$w_{DD}^{12} = \\frac{r^1(D, D) - (\\hat{\\theta}^2r^1(D, C) + (1 - \\hat{\\theta}^2)r^1(D, D))}{r^1(D, C) - r^1(D, D)} = \\frac{P - (\\hat{\\theta}^2 \\cdot T + (1 - \\hat{\\theta}^2 \\cdot P))}{T - P} = \\frac{-\\hat{\\theta}^2}{T - P},$ (10)\n\nSimilarly as Section 4, we set $w^{12}$ less than 0 to 0. So agent 1's reward distribution scheme is:\n\n$\\overline{r_{CC}}^{11} = w_{CC}^{12} = (1 - \\hat{\\theta}^2)R, \\overline{r_{CC}} = R - \\overline{r_{CC}} = \\hat{\\theta}^2 R,$\n\n$\\overline{r_{CD}}^{11} = w_{CD}^{12} = (1 - \\hat{\\theta}^2)T, \\overline{r_{CD}} = T - \\overline{r_{CD}} = \\hat{\\theta}^2T,$\n\n$\\overline{r_{DC}}^{11} = \\overline{r_{DD}}^{11} = 0, \\overline{r_{DC}} = S, \\overline{r_{DD}} = P$ (11)\n\nAgent 2's computation is symmetric. The total reward received by each agent is\n\n$r^{1,tot} = [\\overline{ \\hat{\\theta}^2R + (1 - \\hat{\\theta}^1)R}, S + (1 - \\hat{\\theta}^1)T, \\hat{\\theta}^2, P]$ (12)\n\n$r^{2,tot} = [\\overline{ \\hat{\\theta}^1 R + (1 - \\hat{\\theta}^2)R}, \\hat{\\theta}^1, S + (1 - \\hat{\\theta}^2)T, P]$ (13)\n\nThe value function for each agent is defined by\n\n$V^i (\\theta^1,\\theta^2) = \\sum_{t=0}^{\\infty} \\gamma^t p r^{i,tot},$ (14)\n\nwhere $p = [\\theta^1 \\theta^2, \\theta^1 (1 - \\theta^2), (1 - \\theta^1) \\theta^2, (1 - \\theta^1)(1 - \\theta^2)]$.\n\nAgent 2 updates its policy by\n\n$\\theta^2 = \\theta^2 + \\alpha \\nabla_{\\theta} V^2(\\theta^1, \\theta^2)$\n\n$= \\theta^2 + \\frac{\\alpha}{1-\\gamma} \\nabla_{\\theta} [\\hat{\\theta}^1 \\hat{\\theta}^2R + (1 - \\hat{\\theta}^1)r]$\n\n$+ \\theta^1 (1 - \\theta^2)\\hat{\\theta}^2 + \\theta^2 (1 - \\theta^1) [S + (1 - \\hat{\\theta}^2)T] + (1-\\theta^1) (1 - \\theta^2)P]$"}, {"title": "B Environments", "content": ""}]}