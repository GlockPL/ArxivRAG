{"title": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher Education", "authors": ["Calvin Yeung", "Jeff Yu", "King Chau Cheung", "Tat Wing Wong", "Chun Man Chan", "Kin Chi Wong", "Keisuke Fujii"], "abstract": "Automated grading has become an essential tool in education technology due to its ability to efficiently assess large volumes of student work, provide consistent and unbiased evaluations, and deliver immediate feedback to enhance learning. However, current systems face significant limitations, including the need for large datasets in few-shot learning methods, a lack of personalized and actionable feedback, and an overemphasis on benchmark performance rather than student experience. To address these challenges, we propose a Zero-Shot Large Language Model (LLM)-Based Automated Assignment Grading (AAG) system. This framework leverages prompt engineering to evaluate both computational and explanatory student responses without requiring additional training or fine-tuning. The AAG system delivers tailored feedback that highlights individual strengths and areas for improvement, thereby enhancing student learning outcomes. Our study demonstrates the system's effectiveness through comprehensive evaluations, including survey responses from higher education students that indicate significant improvements in motivation, understanding, and preparedness compared to traditional grading methods. The results validate the AAG system's potential to transform educational assessment by prioritizing learning experiences and providing scalable, high-quality feedback.", "sections": [{"title": "1 Introduction", "content": "In the field of education technology, automated grading has long been a key objective due to its ability to efficiently assess large volumes of work [4], deliver consistent and unbiased evaluations [15], and provide immediate feedback to enhance student learning [18]. It also allows educators to focus on developing engaging learning experiences and leverage data-driven insights for improved teaching strategies.\nAutomated grading systems have evolved significantly since their inception in the 1960s, initially focusing on programming and essay evaluation [19,29]. Traditional methods, typically compare student submissions with reference answers using text similarity and predefined rules [17,24,26,35]. Recent developments in Large Language Models (LLMs) have introduced new opportunities for automated grading. LLMs such as BERT [11], GPT-3 [3], and GPT-4 [1] have shown remarkable capabilities in tasks like question answering and summarization. Where few-shot learning approaches (see Section 2 for more details) have become common in automated grading systems.\nDespite significant advancements, current automated grading systems still face notable challenges. Firstly, few-shot learning methods demand substantial datasets, which are difficult to acquire for dynamic or specialized course content. Building extensive labeled datasets for each possible assignment or course content is time-consuming and often impractical, especially in fast-evolving subjects. Zero-shot learning, in contrast, doesn't require such extensive datasets and can be generalized across many tasks without additional task-specific training. Therefore zero-shot approaches were more scalable and adaptable, particularly for grading diverse assignments or subjects without the need for retraining. Additionally, previous efforts [22] to implement zero-shot learning in grading have shown limited effectiveness, though there is potential for improvement through prompt engineering, an area that remains underexplored. Secondly, these systems often lack a strong emphasis on providing personalized, actionable feedback to students. Lastly, evaluations tend to prioritize benchmark performance over student experience and the practical value delivered to learners.\nTo address these challenges, we propose a Zero-Shot LLM-Based Automated Assignment Grading (AAG) system, illustrated in Fig. 1, which leverages prompt engineering to evaluate student submissions without requiring additional training or fine-tuning. This system is capable of evaluating both computational and explanatory responses while providing personalized feedback. The feedback not only highlights mistakes but also provides tailored and actionable suggestions for improvement. Our evaluation approach goes beyond simply assessing grading accuracy, focusing on how the approach enhances learning experiences for both undergraduate and graduate students, based on their survey responses. The contributions of this paper are as follows:\n1. Zero-shot LLM framework for AAG: The proposed framework demonstrates effective assignment evaluation via prompt engineering without requiring additional training or fine-tuning on specific datasets.\n2. Enhanced Learning through Tailored Feedback: By delivering actionable, personalized feedback, the system improves individual learning outcomes, helping students identify strengths and address weaknesses.\n3. Impact Validated via Student-Centered Metrics: Through surveys and evaluations, the framework's significant role in enhancing student engagement, understanding, and preparedness is demonstrated, providing clear evidence of its practical educational value."}, {"title": "2 Related Work", "content": "Automated Grading Systems. Automated grading tools were first introduced in the 1960s for programming [19] and essay evaluation [29]. Since then, numerous methods have been developed, including unit testing [26], rule-based approaches [24,35], and techniques based on stacking and domain adaptation [17]. These methods typically compare student submissions with reference answers using text similarity, measurable text characteristics (e.g., sentence length, essay length, number of prepositions, and punctuation), and predefined rules. However, with the rapid advancement of deep learning, machine learning, and neural network-based methods have proven more effective than traditional non-neural approaches [31]. Consequently, various neural models have been applied to automated grading, including Long Short-Term Memory (LSTM) networks [28,31], Convolutional Neural Networks (CNNs) [32], and Prototypical Neural Networks [39]. Additionally, additive model-based methods [9] have been introduced to enhance model explainability.\nLarge Language Models in Automated Grading Systems. In recent years, the significant growth of LLMs in natural language processing has led to their adoption in automated grading systems for evaluation using prompts and predefined criteria (e.g., marking schemes). Compared to traditional neural models, LLMs offer the advantage of handling tasks such as question answering, reading comprehension, and summarization without requiring fine-tuning (i.e., training on task-specific datasets through supervised learning) [30]. However, in automated grading, few-shot learning supervised learning with limited training epochs-has become the standard approach. This method has been applied to state-of-the-art LLMs, including BERT [7,33], GPT-2 [30], GPT-3 [3,5,27], LaMDA [36], and GPT-4 [8,21,37]. These studies demonstrate the effectiveness of LLMs in grading through benchmarking on grading datasets and evaluations conducted by educators. While there have been attempts [22] to apply zero-shot learning with simple prompts on higher education courses using LLMs (i.e., without further supervised learning), it nevertheless proved inadequately effective for comprehensive assessments, such as examinations.\nStudent Perceptions and Adaptive Feedback. While LLMs have shown effectiveness in grading and supporting student learning, understanding students' perceptions on automated grading systems is equally important. Although automated grading systems have been implemented in classroom settings [6,12,23], few studies have examined how students perceive these systems. Concerns may arise regarding the accuracy of automated grading, misunderstandings of system operations resulting in suboptimal responses and difficulties in adapting answers to align with evaluation criteria [2,14,20]. Moreover, effectively identifying student mistakes remains critical, as emphasized by the LLM-based feedback tool introduced in [25].\nLimitations. Although previous studies offer valuable insights into Automated Grading Systems, several limitations may hinder their further development and effectiveness:\n1. Dataset Requirements. Few-shot learning methods still require substantial datasets, especially considering the scale of LLMs. Acquiring large-scale datasets can be costly and may not be feasible for introductory courses or ad-hoc topics, where content frequently changes.\n2. Limited Focus on Feedback. Automated Grading Systems often prioritize grading accuracy while neglecting additional functions such as providing meaningful feedback and comments to students.\n3. Benchmark-Oriented Evaluation. These systems are typically evaluated based on their performance on grading datasets, overlooking the student experience and feedback.\nTo address these challenges, we propose a zero-shot LLM-based Automated Assignment Grading (AAG) System capable of evaluating answers that include both calculations and natural language explanations through prompt engineering. Additionally, the system provides constructive feedback to students by identifying mistakes and suggesting ways to improve. While extensive research has examined the grading effectiveness of LLMs, this study focuses on evaluating the system from students' perspectives through survey responses after grading their actual homework and delivering personalized feedback an area that, to the best of our knowledge, remains underexplored."}, {"title": "3 Zero-Shot LLM-Based Automated Assignment Grading System", "content": "This section introduces the Zero-Shot LLM-Based Automated Assignment Grading (AAG) system. An overview of the system is illustrated in Fig. 1. By automatically providing feedback to both teachers and students, the AAG system could significantly reduce the manual grading workload while offering comprehensive and detailed insights to support continuous improvement. A demonstration of the AAG system is provided using a question (see Table 1) from the STAT1011 Introduction to Statistics course assignment at The Chinese University of Hong Kong.\n3.1 Adaptive Input Processing and Marking Scheme Refinement\nThe AAG system processes three key inputs: the student's assignment submission, the assignment questions, and optionally a reference solution and/or marking scheme (indicated by the dashed box labeled \"non-complementary input\" in Fig. 1). While the reference solution and marking scheme can offer more direct guidance to the LLM, they are not essential for the grading process.\nThe AAG system is designed to generate or refine scoring guidelines based on the assignment questions. This feature is particularly beneficial for introductory courses like STAT1011 Introduction to Statistics, where the relevancy of assignments often depends on timely topics and current events, such as presidential elections or census surveys. This flexibility allows instructors to create more diverse and engaging questions without the need to constantly develop detailed marking schemes.\n3.2 Leveraging LLMs for Automated Grading\nThe AAG system utilizes GPT-4 [1] for solution refinement, student submission evaluation, and summarizing student issues. Among the state-of-the-art LLMs-including Llama-3 [13], Qwen-2 [38], Claude-3, and Gemini-1.5 [34]\u2014in which GPT-4 was identified as the most effective model. This conclusion was based on a comparative evaluation using identical prompts, with performance assessed by a group of university lecturers.\nThe student submission, assignment questions, and marking scheme are integrated into the evaluation prompt, as illustrated in Fig. 2. The prompt begins by outlining the evaluation task and specifying the course context. It then provides the assignment background, question, marking scheme, student response, and correct answer (see Table 1 and Fig. 3). Finally, the prompt includes instructions on which specific question (or subquestion) to focus on especially important for sequential questions along with the scoring scale (1 to 10) and the type of feedback required for students. With a carefully engineered prompt and refined marking scheme, the LLM could provide satisfactory feedback without fine-tuning.\n3.3 Personalized Feedback for Students\nStudents receive tailored feedback that includes detailed comments on their work, along with a score based on the refined marking scheme (see Fig. 3 for an example). The feedback highlights the errors made in the student's responses, explains why those answers are incorrect, and offers suggestions for improvement. These suggestions address both the student's understanding of the material and their approach to answering questions. This type of feedback provides more actionable insights compared to traditional grading by teaching assistants, which typically only provides marks and short comments without further explanation.\n3.4 Performance Overview for Teachers\nFor teachers, the system generates a \"Performance Summary\", providing an overall view of how the class or an individual student performed relative to the assignment objectives. An example is shown in Fig. 4. Fig. 4A presents summary statistics for the assignment, giving teachers an overview of submission and student performance. Fig. 4B offers a breakdown of each question in the assignment, while Fig. 4C illustrates an example of student issues in Question 1 (see Table 1), it summarizes common problems based on the feedback provided to students through the LLM. This summary equips teachers with valuable insights to guide their instructional decisions and support student improvement.\n3.5 Limitations of the AAG System\nWhile the AAG system offers significant benefits, it faces several limitations. First, the AAG system relies on prompt engineering and marking scheme quality to ensure grading consistency and diversity. Implementing adaptive marking scheme generation could streamline the process, though human validation would still be necessary. Nevertheless, this approach would be far more efficient than traditional human grading. Second, the lack of iterative feedback restricts opportunities for student learning and growth. Introducing a multi-stage feedback mechanism in the future would enable students to revise and resubmit their work, fostering continuous improvement. Finally, limited integration with collaborative tools and resources could limit student engagement. Expanding support for widely-used collaborative platforms could enhance interaction, making the system more adaptable and fostering greater student involvement."}, {"title": "4 Experiment", "content": "This section evaluates the effectiveness of the AAG system by comparing its grading with that of human teaching assistants (TAs) on open-ended questions from the STAT1011 course. The alignment of scores between the AAG system and TAs is assessed, along with the system's impact on grading consistency and feedback quality. Additionally, student feedback from a voluntary survey is included to evaluate the system's perceived usefulness. The appendix, all data, and code for the statistical results are available at https://github.com/calvinyeungck/Automated_Assignment_Grading.\n4.1 AAG Grading Evaluation\nWhile there was substantial evidence demonstrating the effectiveness of LLMs in grading (see Section 2), a limitation of LLM-based grading was the distributional differences between AI scorers and human [16]. To address this issue, we compared the scores assigned by TAs and the AAG system using two open-ended questions from the STAT1011 Introduction to Statistics course. Open-ended questions were selected due to their greater flexibility in student responses compared to calculation-based questions. These two questions are summarized in Tables 1, and 2, respectively. For the comparison, both the TA and the AAG system independently graded 150 student assignments.\nFigure 5 presents the grading distributions of Question 1 (left) and Question 2 (right) by both human graders and the AAG system. The Pearson correlation coefficients between human and AAG scores for Questions 1 and 2 were 0.75 and 0.82, respectively. These strong correlations indicate that the AAG system produced grading results closely aligned with human evaluations, indirectly supporting the grading accuracy of the AAG system (direct validation against ground truth was not possible due to the absence of reference grading data, which would serve as the ground truth)."}, {"title": "A Hypothesis Testing for Survey Responses", "content": "This section outlines the hypothesis testing performed on the survey responses, using statistical tests to evaluate the significance of the responses for each question.\n1. Wilcoxon Signed-Rank Test (for Q1 to Q9)\nNull Hypothesis ($H_0$): The median response for each question (Q1 to Q9) is less then or equal to 3.\nAlternative Hypothesis ($H_1$): The median response for each question (Q1 to Q9) is greater than 3.\n2.Binomial Test (for Q10)\nNull Hypothesis ($H_0$): The proportion of binary responses is less than or equal to 0.5.\nAlternative Hypothesis ($H_1$): The proportion of binary responses greater than 0.5.\nThe hypotheses are determined based on the following conditions:\nIf the p-value is less than 0.05, the null hypothesis will be rejected, indicating a significant difference.\nIf the p-value is greater than 0.05, the null hypothesis cannot be rejected, suggesting no significant difference.\nThe results of the hypothesis tests for questions Q1 to Q9 (Wilcoxon Signed-Rank Test) and Q10 (Binomial Test) are presented in Table 3."}, {"title": "B Hypothesis Testing for Between-Group Comparison", "content": "This section describes the use of the Mann-Whitney U Test (for Q1 to Q10) for comparing the survey responses between two groups of students: weak students (Group 1) and strong students (Group 2). For the comparison between weak and strong students, the hypotheses are as follows:\nNull Hypothesis ($H_0$): There is no significant difference between the weak students (Group 1) and the strong students (Group 2) with respect to the survey responses, i.e., the distribution of survey responses for weak students is less than or equal to that of strong students.\nAlternative Hypothesis ($H_1$): The distribution of survey responses for weak students is greater than that of strong students.\nThe hypotheses are determined based on the following conditions:\nIf the p-value is less than 0.05, the null hypothesis will be rejected, indicating a significant difference.\nIf the p-value is greater than 0.05, the null hypothesis cannot be rejected, suggesting no significant difference.\nThe results of the Mann-Whitney U test comparing weak and strong students (Groups 1 and 2) for each survey question are summarized in Table 4. The interpretation of each result follows the decision rule outlined above."}, {"title": "C Survey Questions", "content": "This section includes the set of survey questions designed to gather valuable insights into various aspects of the AAG system. The goal is to understand students' perspectives, experiences, and feedback, which will inform future development and improvements."}]}