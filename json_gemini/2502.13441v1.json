{"title": "The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Ruochen Xu", "Zilun Zhang", "Jianwei Yin"], "abstract": "Self-improving large language models (LLMs) - i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself - is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents CRESCENT a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. CRESCENT first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that CRESCENT sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, CRESCENT-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distil LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) such as GPT-40 (Hurst et al., 2024), Gemini (Anil et al., 2023), Llama (Touvron et al., 2023a), and DeepSeek-R1 (Guo et al., 2025) have demonstrated remarkable capabilities, revolutionizing natural language processing and various other tasks. The success of these models can be attributed to the scaling laws (Kaplan et al., 2020), which dictate the relationship between model parameters, computational resources, and training data size. For instance, the prominent performance of Llama-3.1 with 405B parameters (Dubey et al., 2024) roots in, amongst others, the massive, high-quality datasets for pre- and post-training. However, as models continue to scale, the available real-world (public) data quickly becomes exhausted; meanwhile, manually crafting high-quality data is time- and labor-intensive. Thus, data volume has become a key limiting factor for the effective scaling of new-generation models.\nIn response to this challenge, synthetic data generation and data augmentation have emerged as key methods to further improve the performance of LLMs while avoiding extensive supervision. These methods leverage the ability of LLMs to mirror real-world distributions and generate high-quality, pseudo-realistic data (Zhang et al., 2023). Following this line of research, the problem of self-improvement naturally arises: Can we improve the performance of an LLM by fine-tuning it with synthetic data generated by itself? This problem has triggered a recent surge of research results (Wang et al., 2024). These methods, however, rely heavily on external seed datasets for augmentation (e.g., (Huang et al., 2023; Wang et al., 2023b)) and/or stronger third-party models as classifiers or reward agents (e.g., (Le et al., 2022; Xin et al.,"}, {"title": "2 The CRESCENT Approach", "content": "This section presents CRESCENT a framework for controlled QA self-generation via diversification and consensus enhancement. CRESCENT suffices to generate high-quality domain-specific QA pairs leveraging only the model itself, with zero external data, nor assistance from third-party models.\nFig. 2 sketches the general workflow of CRESCENT, which consists of three main steps: (I) Bait prompting: We use a bait prompt to instruct the original, aligned LLM to produce a set of raw questions within a specific domain; (II) Diversification: The raw questions may be semantically analogous to each other (as per some similarity metric), and thus we employ a rejection sampling mechanism to attain a diverse pool of representative questions through self-deduplication; (III) Consensus enhancement: We treat the generated questions as query prompts and feed them back to the LLM. Then, by majority vote, we obtain the final set of synthetic QA pairs. We show that such QA pairs are of high quality in the sense that they suffice to improve the domain-specific capabilities (mathematical reasoning, in our case) by fine-tuning the original LLM with these QA pairs while preserving its general capabilities."}, {"title": "2.1 Question Generation (Steps (I) and (II))", "content": "We begin by utilizing a simple bait prompt to elicit the LLM to generate a bunch of domain-specific questions, such as math word problems illustrated in Fig. 2, denoted as raw questions. As some of them may be semantically analogous to each other, we optimize diversity of the questions in an iterative manner: Each generated question is vectorized and compared against the (embeddings of) other questions. If there exists a question that is deemed sufficiently similar (i.e., the similarity score is below a prescribed threshold), we apply the following deduplication prompt to modify it:\n{question} is very similar to {question}, please modify the latter to make it different.\nThis iterative process ensures that the question pool remains diverse and representative across the specific domain through redundancy-aware selection.\nFormally, the question-generation phase can be described as follows: Let $Q = {q_1,q_2,..., q_n}$ be the set of raw questions generated by the LLM per the bait prompt. For each question $q_i$, we embed it as a real-valued vector $v_i$ and compare it against the vector representations ${v_1, v_2, ..., v_{i-1}}$ of the previously generated questions. The similarity between the two questions is determined by the distance between their respective vector embeddings in the inner product space, e.g., the $L^2$ distance. If the distance is below a given threshold $\\theta$, then $q_i$ with (i > j) is considered as a duplicate and thus"}, {"title": "needs to be modified via the deduplication prompt, i.e.,", "content": "If $d(v_i, v_j) < \\theta$ then $q_i^* = Deduplicate(q_i)$. Such similarity-based deduplication incorporates the maximal marginal relevance (MMR) criterion (Carbonell and Goldstein, 1998) to minimize repetition while preserving content relevance. Moreover, the iterative refining process falls into the paradigm of rejection sampling (cf. e.g., (Liu and Liu, 2001)), which ultimately yields a diversified question pool featuring relevance and representativeness w.r.t. the target domain with negligible redundancy; see Section 2.3."}, {"title": "2.2 Answer Generation (Step (III))", "content": "Let $Q^* = {q_1,q_2,\u2026\u2026\u2026,q_n}$ be the deduplicated set of questions generated through the previous step. The phase of answer generation aims to synthesize the corresponding high-quality answers w.r.t. each $q_i \\in Q^*$. We achieve this by means of consensus enhancement, namely, we feed each question $q_i$ back to the LLM and collect $m$ independently produced answers, denoted by the set $A_i = {a_1,a_2,...,a_m}$, where each $a_j$ contains integrated chain-of-thought (CoT) processes (Wei et al., 2022) generated for question $q_i$. We then select the final answer $a_i$ for question $q_i$ using majority voting (Wang et al., 2023a). That is, we first identify the set $\\bar{A_i}$ of most frequent answers:\n$\\bar{A_i} = \\{a_j \\in A_i \\| f(a_j) = \\underset{a_k \\in A_i}{max} f(a_k)\\} $,\nwhere $f(a_j)$ denotes the frequency (i.e., the number of occurrences) of answer $a_j$ in $A_i$. Then, we"}, {"title": "2.3 Rationale for Self-Improvement", "content": "Next, we provide the intuition on why self-generated QA pairs using the CRESCENT framework can be used to improve the capabilities of the underlying LLM. This observation will be further justified by extensive experiments in Section 3. The intuition is three-fold (see Fig. 3):\n(i) Relevance by bait prompting: The initial bait prompt restricts the considered space of questions and answers to a specific domain and hence all the generated QA pairs within the CRESCENT scope are pertinent to this domain.\n(ii) Diversity by rejection sampling-based deduplication: Our diversification step explores the question space while maintaining a minimal pair-wise distance to alleviate redundancy. This is achieved by a rejection sampling loop where question samples violating the distance criterion per (\u2020) are modified and, therefore, the generated questions exhibit a scattered distribution stretching over the space.\n(iii) Accuracy by majority voting: Based on the observation that a complex reasoning problem typically admits multiple distinct ways of thinking yielding its unique correct answer (Wang et al., 2023a), our consensus enhancement step selects, for each question, the most frequent answer that may coincide with the correct one with high likelihood.\nAs a consequence, fine-tuning the original LLM with the so-obtained QA pairs will strengthen its domain-specific capabilities by enforcing a reduction in the variance of answer generation for a diverse set of domain-relevant questions."}, {"title": "3 Experiments", "content": "3.1 Experimental Setups\nBenchmarks. We adopt three benchmarks on math word problems (MWPs): (i) GSM8K (Cobbe et al., 2021): 8.5K grade school math problems with step-by-step solutions; (ii) ASDiv (Miao et al., 2020): 2,305 diverse MWPs covering multiple difficulty levels; and (iii) GSM-Plus (Li et al., 2024): an enhanced version of GSM8K with 12K problems incorporating robustness checks. In order to accelerate the evaluation, we use GSM-Plus-mini - a subset of GSM-Plus containing 2,400 questions. It should be noted that the GSM-Plus-mini and GSM8K datasets do not overlap.\nBaseline Models. We conduct self-improvement experiments with two different LLM models: (i) Llama3-8B-Instruct: the instruction-tuned version of Llama3-8B (Dubey et al., 2024); and (ii) Llama2-7B-Chat (Touvron et al., 2023b): a instruction-tuned version of Llama2-7B.\nGeneration Configurations. For each model, we generate MWP QA pairs following these settings:\nQuestion Generation: Bait prompt: \u201cGenerate a diverse math word problem requiring multi-step reasoning\u201d. We generate 50K candidate questions for Llama2-7B-Chat and 75k for Llama3-8B-Instruct, both with temperature T = 0.95. Diversification: We use sentence embeddings generated by the all-MiniLM-L6-v2 model from the Sentence-BERT (Reimers and Gurevych, 2019) family; we eliminate semantically similar questions using the L2 distance with threshold $\\theta$ = 0.25. We employ FAISS (Douze et al., 2024) to accelerate vector computation and comparisons.\nAnswer Generation: For each question, sample 5 answers with temperature T = 0.95, then select the most frequent answer as the final answer. We use the same answer generation settings for both models. We use the vLLM (Kwon et al., 2023) inference framework for both generation stages.\nGPU hours: It took 30.0 GPU hours to generate 75k QA pairs with Llama3-8B-Instruct and 42.9 GPU hours for the 50k pairs with Llama2-7B-Chat."}, {"title": "SFT Implementation.", "content": "Our SFT procedure uses single-epoch training with max sequence length of 2,048 tokens. Optimization is performed using AdamW (Loshchilov and Hutter, 2019) (\u03b2\u2081 = 0.9, \u03b22 = 0.95) under a linear learning rate schedule (initial LR = 1e-5, 3% warm-up), and the batch size is set to 128 through 8-way parallelization on NVIDIA A100-80GB GPUs with 16-step gradient accumulation. We use DeepSpeed Stage3 (Rasley et al., 2020) and bfloat16 for mitigating memory constraints, and FlashAttention-2 (Dao, 2024) for efficient attention computation."}, {"title": "Evaluation Protocol.", "content": "We use LM-Evaluation-Harness (Gao et al., 2024) library; all datasets are evaluated under 0-shot and 5-shot settings. Few-shot examples are randomly selected from training sets, excluding test samples. We use two answer extractors: one identifies the number appearing after \"####\" and the other extracts the last number in the output. An answer is considered correct if either of the extractors retrieves the correct answer."}, {"title": "3.2 Main Results", "content": "The experimental results shown in Table 1 validate our core hypothesis: self-generated reasoning QA pairs \u2013 boosted through diversification and consensus enhancement \u2013 enable model improvement without external supervision signals. For GSM8K, Llama2-7B-Chat shows improvements of +4.4%\u2191 (0-shot) and +2.1%\u2191 (5-shot), while Llama3-8B-Instruct achieves noticeable gains of +28.8%\u2191 (0-shot) and +1.8%\u2191 (5-shot). Similar observations apply consistently to ASDiv and GSM-Plus-mini featuring different QA distributions.\nIt is noteworthy that CRESCENT leads to substantial improvements in the 0-shot setting across all three datasets, with performance on certain datasets surpassing even the 5-shot counterparts for the original models. This observation highlights the potential of 0-shot learning in reducing dependency on task-specific examples, thus indicating better generalization to real-world unseen problem types."}, {"title": "3.3 Ablation Study", "content": "To justify the pivotality of CRESCENT's core components, we conduct comprehensive ablation experiments over Llama3-8B-Instruct under 5-shot GSM8K evaluation. As depicted in Fig. 4, (i) full method of CRESCENT achieves accuracy of 77.6%, outperforming all ablated variants and the baseline; (ii) removing consensus enhancement (w/o CE) reduces performance to 73.0% (-4.6%); (iii) excluding diversification (w/o DV) yields a more severe drop to 71.1% (-6.53%); (iv) using only bait prompting (BP only) results in 70.6% (-7.0%). The results demonstrate the significance of both diversification and consensus enhancement.\nNotably, CRESCENT surpasses the Magpie variants by substantial margins: (i) +5.6% over Magpie-Common (Magpie-C) (72.0%); (ii) +11.0% over Magpie-Math (Magpie-M) (66.6%).\nTo investigate the discrepancy between CRESCENT and Magpie-Math, we conduct a sampling analysis on the mathematical questions generated by CRESCENT, CRESCENT w/o DV, and Magpie-Math: For each method, we randomly sample 1,500 questions; Each question is then classified by difficulty using GPT-40 (Hurst et al., 2024), vectorized with the all-MiniLM-L6-v2 embedding model, and projected into a two-dimensional plane using t-SNE (Van der Maaten and Hinton, 2008). The visualization in Fig. 5 suggests that, even without diversification, CRESCENT can still generate high-quality mathematical questions, albeit with reduced diversity and difficulty (Fig. 5b). In contrast, the vectors for Magpie-Math problems (Section 3.3) feature (i) a more agglomerate form exhibiting significantly low coverage than CRESCENT; and (ii) numerous gray points signifying non-mathematical problems; they are merely instructions related to the mathematics topic, e.g., \u201cCould you tell me what type of mathematics you like?\". The latter aligns with the observation in (Xu et al., 2024,\""}, {"title": "4 Detailed Analysis of CRESCENT", "content": "4.1 General-Capability Preservation\nWill CRESCENT incur catastrophic forgetting of general capabilities? We address this problem by evaluating Llama3-8B-Instruct before and after CRESCENT on five non-mathematical benchmarks covering commonsense reasoning (ARC-C (Clark et al., 2018), HellaSwag (Zellers et al., 2019)), general knowledge preserving (MMLU (Hendrycks et al., 2021)), instruction following (IFEval (Zhou et al., 2023)), and graduate-level question answering (GPQA (Rein et al., 2023)). We use the CRESCENT checkpoint directly from Section 3.2.\nTable 2 shows that the CRESCENT-enhanced model exhibits performance comparable to that of the original model in all five tasks. This observation reveals that domain-specific self-enhancement through CRESCENT does not compromise general capabilities, a critical advantage over fine-tuning approaches using external data, which often exhibit significant capability trade-offs (Luo et al., 2023)."}, {"title": "4.2 Analysis of Corrected Questions", "content": "Our results show significant improvements in the 0-shot setting. However, does this improvement reflect better generalization, or is it due to the lack of formatting constraints in GSM8K's 0-shot evaluation, which can lead to incorrect answer extraction?\nTo investigate, we analyze Llama3-8B-Instruct's 0-"}, {"title": "4.3 Comparison with Prompt Engineering", "content": "Can prompt techniques achieve a similar performance with CRESCENT? We address this question by comparing CRESCENT-trained LLaMA3-8B-Instruct against five prompting methods: (i) Standard prompt from Llama3 official repository;\u00b9 (ii) Standard prompt with self-consistency (SC, aka majority voting) following the settings in (Wang et al., 2023a); (iii) Random rephrased utilizes GPT-40 to randomly rephrase the standard prompt five times (where we select the best evaluation result). Considering the answer-extractor failures discussed in Section 4.2, we carefully craft each instruction to control the output format, such as requesting the answer to be placed after \"####\" or at the end of the output, ensuring that the prompt includes relevant formatting information compatible with our answer extractor when rephrased by GPT-40; (iv) CoT prompt following the settings in (Wei et al., 2022); (v) Optimized prompt by integrating CoT, the best candidate from random rephrased, and the SC process.\nThe comparison results are reported in Table 3. Overall, 0-shot outcomes demonstrate higher sensitivity to prompt variations compared to 5-shot configurations. For the original model, the optimized prompt achieves optimal performance, improving 0-shot accuracy by 10.6% over standard prompts while exhibiting comparable 5-shot results. However, this result remains substantially inferior (-18.2%) to CRESCENT using only standard prompts. Notably, when employing the same optimized prompts, the CRESCENT-enhanced model further improves 0-shot performance by 6.5%.\nThe observed performance gap substantiates that the improvements achieved by CRESCENT can-"}, {"title": "4.4 Data Efficiency and Training Dynamics", "content": "Next, we investigate the effect of self-improvement in terms of the volume of synthetic data and the number of training epochs.\nData Volume: We perform one epoch of SFT using Llama3-8B-Instruct on CRESCENT data with data volumes of 25k, 50k, 75k, 100k, and 150k; we use the standard prompt for evaluation. As shown in Fig. 7, the model's performance improves consistently from 25k to 75k, but stabilizes between 75k and 150k, suggesting an upper limit to the improvement gained from increasing data volume.\nTraining Epochs: We perform SFT with Llama3-8B-Instruct on 50k CRESCENT data for 4 epochs. The evaluation is conducted using the standard prompt. Table 5 shows that, in both settings of 0-shot and 5-shot, the model exhibits a steady performance as the number of epochs increases."}, {"title": "4.5 CRESCENT for Model Distillation", "content": "Next, we explore the potential of using the CRESCENT-generated data to distil the knowledge of an"}, {"title": "5 Related Work", "content": "Synthetic Data from Scratch: Recent efforts to reduce reliance on external seed data have led to the exploration of generating data from scratch for fine-tuning LLMs. UltraChat (Ding et al., 2023) shows how to generate diverse, high-quality multi-turn conversations without human queries. Magpie (Xu et al., 2024) introduces a self-synthesis method to generate large-scale alignment data by utilizing only pre-defined chat templates. GenQA (Chen et al., 2024a) aims to generate large instruction datasets with minimal human oversight by prompting LLMs to create diverse instruction examples. Note note that these methods primarily focus on creating alignment data to train the instruction-following capabilities of base models.\nLLM Self-Improvement: Recent methods exploring self-improvement demonstrate the potential of enhancing LLMs' capabilities through self-generated feedback. (Huang et al., 2023) demonstrates that LLMs can improve by sampling high-confidence answers from existing high-quality question sets. Similarly, CodeRL (Le et al., 2022) introduces reinforcement learning to program synthesis, where the model receives feedback from unit tests and critic scores from other models, aiming to optimize performance on unseen coding tasks. StaR (Zelikman et al., 2022) leverages small amounts of rationale examples and iteratively refines the reasoning ability through self-generated rationales. SPIN (Chen et al., 2024b) proposes a self-play fine-tuning method, where a model generates its training data from previous iterations."}, {"title": "6 Conclusion", "content": "We presented CRESCENT as a simple yet effective framework \u2013 leveraging techniques of bait prompting, diversification, and consensus enhancement \u2013 for exploring the self-improvement problem of LLMs. We show that CRESCENT suffices to improve the mathematical reasoning capabilities of an LLM with zero supervision signals while preserving its general performance. Moreover, it facilitates more effective and efficient LLM knowledge distillation than existing approaches based on seed-dataset augmentation."}, {"title": "Limitations", "content": "We observe the following limitations of this work:\nDomain scalability. Although CRESCENT can generate a variety of domain-specific datasets, the experiments in this paper are confined to evaluating its effectiveness in improving math reasoning capabilities. Further extensions to other domains are subject to future work.\nAligned model restriction. CRESCENT is designed for aligned chat models. In this paper, we did not investigate whether the same approach can be used to generate high-quality, domain-specific data for base models without instruction tuning."}]}