{"title": "PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method", "authors": ["Runtao Ren", "Jian Maa"], "abstract": "As humanity stands on the brink of a new era of technological innovation, the ability to rapidly transform creative ideas into protected intellectual property (IP) is more crucial than ever. However, the conventional processes for patent drafting are fraught with challenges, demanding a nuanced understanding of advanced field knowledge and technical concepts. Existing large language models (LLMs), while powerful, often fall short in this IP creation domain due to their lack of specialized knowledge and context-awareness necessary for generating technically accurate patent documents. To bridge this critical gap, we propose a groundbreaking framework for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow Al with the ability to autonomously mine, understand, and apply domain-specific knowledge. Our model, PatentGPT leverages a unique combination of knowledge graph-based pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Through extensive evaluation, PatentGPT has demonstrated outstanding performance, scoring up to approximately 400% higher in patent related benchmark tests compared to state-of-the-art models. By KFT method the model's capability to not only assist but also augment human creativity and innovation, our approach sets a new standard for AI-driven intellectual property generation, paving the way for more efficient and effective invention processes.", "sections": [{"title": "1 Introduction", "content": "In today's era of rapid scientific development, the ability to transform knowledge into intellectual property (IP) is crucial for every organization and economy. IP is not only a legal guarantee, but also plays a vital role in transforming groundbreaking scientific discoveries into tangible innovations that can drive economic and technological progress [1]. WIPO's World Intellectual Property Report has analyzed nearly 40 million patent applications, more than 70 million scientific papers, and economic activities worth more than 300 trillion US dollars in goods and services exports [2]. In the past 20 years, the top eight countries in terms of the number of patents accounted for 50% of global exports, 60% of scientific publications, and 80% of international patent grants [2]. It can be seen that industrial entity that use a diverse innovation ecosystem of intellectual property and develop deep capabilities are more likely to win in the innovation race. E.g., Nokia sold its mobile phone business to Microsoft in 2013, but as long as there is a 5G mobile phone in the world that uses Nokia patents, the mobile phone manufacturer needs to pay Nokia a patent fee of 3 euros [3]. In the following ten years, it not only did not withdraw from the market, but remained active in the communications industry. Its 2022 annual report showed that its profit was 4.259 billion euros, of which patent licensing reached 1.596 billion euros. For enterprises in the industrial circle, intellectual property is also a magic weapon that can consolidate their market position. Therefore, it is crucial to convert knowledge or ideas into IP.\nWith the rapid development of AI for Science, especially the discovery and application of large language models (LLMs) in industrial informatics, more and more attention has been focused on their potential [4-5]. Top journals such as Nature have reported breakthroughs in these models in multiple scientific fields using the industrial information, e.g., chemical molecule design, and drug discovery [6-7]. Moreover, LLMs have been used to generate material property databases such as the critical cooling rate of metallic glasses, demonstrating its ability to handle highly specialized and technical content [8]. Similarly, in the biomedical field, LLMs have played an important role in predicting drug-target interactions and identifying potential therapeutic compounds, greatly shortening the time from discovery to clinical application [9]. These advances highlight the broader significance of artificial intelligence in scientific discovery, where LLMs not only simplify the process of extracting and synthesizing information, but also open up new innovative avenues by identifying potential patterns in massive data sets.\nHowever, the drafting of traditional patents requires initial ideas and professional knowledge. Due to the complexity of patent documents and the strictness of legal requirements, even experienced patent attorneys may face"}, {"title": "2 Related Work", "content": "In the field of automated text generation, although there have been studies attempting to use LLM to handle document generation tasks in specific domains, these methods often show shortcomings in the highly structured and legally constrained field of patent writing. E.g., existing knowledge-based systems heavily rely on inherent knowledge graph networks for retrieval and generation, and cannot generalize to a deep understanding of complex technical content and innovative expressions [14]. In order to effectively address the challenges of automatic text generation, especially in vertical fields, large language models need to have prior knowledge in related fields [15]. Because LLM itself may not have knowledge of a certain vertical field, the importance of prior knowledge is more prominent.\nPrevious literature has also explored various fine-tuning methods to enhance the capabilities of LLM in specific domain tasks. For these fine-tuning-based Q&A systems, the method usually adopted is to select a base model and then construct a dialogue dataset for SFT. SFT is another approach for fine-tuning models on domain-specific labeled datasets. While SFT can help models learn the specific style and requirements of patent writing, it typically requires a large amount of annotated data and still struggles to generalize beyond the examples it was trained on. E.g., Zhang et al. selected Baichuan-7B as the base model and used the real data of doctors and the distilled data of ChatGPT to realize multi-round dialogues in the diagnosis process through mixed feedback reinforcement learning [16]. This fine-tuning method aligns with the real scene, but its disadvantage is that a large amount of real data needs to be artificially constructed for training. In the field of legal consultation, Wang designed a Q&A system that supports structured extraction, taking into account the structure of legal texts, and used different legal cases as the corpus of the LLM for fine-tuning [17]. However, these methods have limitations, such as heavy reliance on field-specific textual data, which may fail to capture the broader knowledge context required for patent writing."}, {"title": "3 Proposed Method", "content": "Our framework systematically integrates LLM with knowledge graphs for finetuning. The approach consists of four main stages (as shown in figure 2) including knowledge extracting, knowledge injecting, knowledge learning, knowledge feedbacking."}, {"title": "3.1 Knowledge Extracting", "content": "In the initial stage, we focus the various patent textual data $D ={ d_1, d_2, , d_n }$ from USPTO and use the LLM $\\pi_\\theta$ to automatically extract entities and relationships based on Luan et al. scientific knowledge graph construction method for constructing the comprehensive knowledge graph with triples (h, r, t) as below [20]."}, {"title": "3.2 Knowledge Injecting", "content": "Following the extraction of knowledge, each extracted triple is transformed into natural language sentences $s_i$ termed knowledge corpus S. This approach enables large language models to learn structured knowledge in an understandable way. The model undergoes continued pre-training using both knowledge corpus S and general corpus G, facilitating full-parameter fine-tuning and enhancing the model's comprehension of domain-specific knowledge."}, {"title": "3.3 Knowledge Learning", "content": "After obtaining the model with knowledge injection by continued pre-training, the supervised fine-tuning (SFT) is used to align the outputs of a language model with specific tasks by training it on dialogue datasets $Q = { (q_1, a_1), (q_2, a_2), ..., (q_n, a_n) }$. To convert a general large model into a large model with domain knowledge, it is not enough to simply inject background knowledge into the model. It is also necessary to learn how to let the model use knowledge according to specific scenarios. Therefore, this step is called the knowledge learning process. This procedure reduces the difference between the anticipated outputs and the ground truth labels, which aids the model in producing more accurate and contextually relevant replies in the concrete field."}, {"title": "3.4 Knowledge Feedbacking", "content": "In this phase, the model is improved through reinforcement learning from human feedback (RLHF). Specifically, we collect user feedback when deploying the model, test the user's preference for the same question, and mark the answer as human preference (i.e., consistent with human knowledge recognition), so this process is called knowledge feedbacking. Then these labeled human feedback datasets $H ={ (q_1, p_1, n_1), (q_n, p_n, n_n) }$ will then be used to train the reward model. To stabilize reinforcement learning training, we also used proximal policy optimization (PPO) with a reward signal provided by the RM score."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiment setup", "content": "In the process of building PatentGPT, we chose Qwen2-1.5b as the base model to conducting knowledge finetuning. The reason for choosing Qwen2-1.5b as the base model is due to its moderate parameter size (1.5B parameters) and good general performance, which achieves an ideal balance between computing resources and performance. The fine-tuning and inference processes are conducted on NVIDIA 4090 GPUs. During fine-tuning, we use a LoRA, batch size of 1, learning rate of 5e-5, and training epochs of 3. For inference, we set the temperature to 0.7 and the maximum generation tokens to 1,024 on the LLaMA-Factory and Im-evaluation-harness framework [21-22]."}, {"title": "4.2 Data", "content": "In this study, we used three main datasets to train and optimize our large language model (LLM) for automated patent writing. These datasets include a patent text dataset from the USPTO, a knowledge graph dataset built specifically for patents, and a question-answering dataset combined with human feedback.\nUSPTO Patent Text Dataset. This dataset contains a wide range of patent documents published by the United States Patent and Trademark Office (USPTO), covering the complete text from preliminary applications to granted patents [23]. These documents provide rich technical and legal information and are a valuable resource for understanding patent writing style and structure. We use these texts for preliminary model pre-training, with the goal of enabling the model to learn and imitate professional patent writing methods.\nPatent Knowledge Graph Dataset. In order to improve the knowledge understanding ability of the model, we built a patent knowledge graph, which contains patent-related entities (e.g., technical fields, patent classifications, etc.) and their relationships. The purpose of this dataset is to integrate structured patent information into the continuous pre-training of the model through knowledge injection, thereby improving the model's expertise and semantic understanding ability in the patent field.\nSupervised fine-tuning dialogue dataset. We also created a conversational dataset consisting of patent application conversations to further fine-tune (i.e., SFT) the model's interactive capabilities. This dataset specifically focuses on how the text content of patents (such as titles, abstracts, and claims) is actually written.\nMixed Dataset with Human Feedback. In order to improve the quality and legal accuracy of the model's responses, we used a human feedback question-answering dataset containing interactions between patent attorneys and applicants. This dataset was used in the RLHF of the model to optimize the model's answer strategy through real user feedback."}, {"title": "4.3 Baseline", "content": "In order to comprehensively evaluate the performance of PatentGPT, we selected a variety of open source models of different scales and architectures for comparison, including Qwen2-1.5b [24], Gemma-2-2b [25], phi-2 [26] and phi-1.5 [27]. These models range from small parameter models to large parameter models. We also compared it with the most advanced large-scale model in the field of IP MOZIP-7b to demonstrate the reliability of our model in this field [19]. The purpose is to verify whether the small parameter model can outperform the large parameter model in the patent writing task after knowledge fine-tuning."}, {"title": "4.4 Metrics", "content": "Automated evaluation. In order to comprehensively evaluate the performance of the automated patent writing, we used a variety of language generation quality evaluation indicators. These indicators include ROUGE-1, ROUGE-2, ROUGE-L, BLEU-4 and BERTScore, which together help us measure the quality and accuracy of the text generated by the model from different dimensions.\nIPQulz. It is sourced from a public dataset on Hugging Face designed to evaluate a model's understanding of IP concepts and regulations [19]. This benchmark consists of multiple-choice questions collected from publicly accessible websites in various languages worldwide. For each question, the model is required to select the correct answer from a list of candidates. The number of correct selections indicates the model's proficiency in comprehending IP-related concepts and regulations. A higher number of correct answers reflects a stronger grasp of IP knowledge and regulatory understanding.\nPatentMatch. This benchmark also comes from the public benchmark test set on Hugging face and is used to evaluate whether the model truly understands the invention described in the patent document and whether it can accurately distinguish different patents [19].\nPatentExam. This is a custom benchmark derived from multiple-choice question-answering content used in the U.S. Patent Bar Examination. By requiring the model to navigate complex legal scenarios and answer questions that mirror those faced by aspiring patent attorneys, the IPexam benchmark serves as a comprehensive measure of the model's proficiency in applying IP knowledge in practical, legally precise contexts. A higher performance on this benchmark indicates a model's capability to not only understand but also accurately apply IP law principles.\nMMLU. MMLU (Massive Multitask Language Understanding) is a widely recognized benchmark designed to assess a model's performance across a diverse set of tasks, encompassing a broad range of subjects from the humanities, sciences, and professional disciplines [28]. In the process of drafting or reviewing a patent specification, a deep understanding of scientific and technological common sense across areas such as physics, chemistry, engineering, as well as patent law, is crucial. To evaluate the scientific, technological, and legal common sense of our PatentGPT models, we utilized the MMLU benchmark. By testing the model on this comprehensive dataset, we can measure its ability to generalize beyond domain-specific knowledge, ensuring that"}, {"title": "5 Benchmark testing", "content": null}, {"title": "5.1 Patent Writing", "content": "The patent writing benchmark is designed to evaluate the ability of large language models to write patent text. It involves writing titles, abstracts, and claims based on ideas. The test compares the text generated by the large model with real text, and the total number of instances in the test set is 1000. The results are summarized in Table II. As shown, PatentGPT significantly outperforms other models across all metrics. Specifically, PatentGPT achieves the highest Bert Score of 90.0036, indicating a strong semantic alignment with the reference texts. It also excels in BLEU-4 with a score of 45.3602, and consistently higher ROUGE scores across all variants, demonstrating its ability to accurately and fluently generate text that closely mirrors the stylistic and structural elements of real patent documents."}, {"title": "5.2 Comparison Results", "content": "In this experiment, we compare the effectiveness of different training methodologies on model performance, particularly focusing on the distinction between traditional pre-training (PT) and knowledge-injected pre-training (KPT). Specifically, we assess models trained with PT, KPT, and various combinations with SFT and RLHF. The findings demonstrate that models trained with KPT exhibit significant improvements in downstream tasks compared to those trained with PT alone.\nKPT vs. PT: The results show that while models trained with only KPT and PT (without additional fine-tuning) perform similarly in terms of BERT Score and BLUE-4, the real advantage of KPT becomes evident when these models undergo further task-specific training. For example, the KPT model achieves a BERT Score of 82.9026 and a BLUE-4 score of 22.0156, which are relatively close to those of the PT model, which has a BERT Score of"}, {"title": "5.3 Ablation Study", "content": "The ablation study presented provides a comprehensive evaluation of the impact of different components within the PatentGPT training pipeline. The table examines the performance of PatentGPT by sequentially removing key training stages RLHF, SFT, and KPT to assess their contribution to the overall model performance. When RLHF is omitted, the model's performance shows a notable decrease across all evaluation metrics, this drop indicates the significant contribution of RLHF in refining the model's outputs to better align with human expectations and enhancing its understanding of nuanced legal language and complex expressions in patent writing. Excluding both SFT and RLHF results in a more pronounced decline in performance, the substantial decrease highlights the critical role of SFT in equipping the model with task-specific knowledge and the ability to generate structurally and contextually appropriate patent documents. The model's performance further deteriorates when all three stages KPT, SFT, and RLHF are removed, these results demonstrate the foundational importance of KPT in setting the model up for success in subsequent stages. Without KPT, the model lacks the domain-specific knowledge essential for effective patent text generation. Hence, the ablation study confirms that each component of the training process KPT, SFT, and RLHF plays a vital role in enhancing the performance of PatentGPT. KPT provides the necessary foundation of domain knowledge, SFT sharpens the model's task-specific capabilities, and RLHF further refines the outputs to meet high standards of quality and relevance. The removal of any of these components results in a marked degradation in performance, underscoring their collective importance in the overall training pipeline."}, {"title": "5.3 Patent Bench", "content": "The Patent Bench results highlight the performance of different models, including PatentGPT, MOZIP, Qwen2-1.5b, Gemma-2-2b, phi-2, and phi-1.5, across four distinct evaluation tasks: IP Quiz, IP Exam, IP Match, and MMLU. Each task is designed to assess various competencies relevant to intellectual property (IP) management and language understanding."}, {"title": "5.4 Case Study", "content": "In this case study, we assess the performance of several large language models in generating a patent abstract for a hybrid vehicle drive system. Each model was tasked with the prompt for producing a concise and informative abstract, capturing the invention's scope, technical details, and advantages. The Qwen2-1.5b model provides a detailed and structured abstract, effectively covering various components of the hybrid vehicle drive system. Phi-2 provided a moderately detailed response, balancing technical detail with clarity, but lacked the specificity needed for a patent document. Llama-3-8b's response is quite generic and follows a template-based approach. It describes the system in broad terms without diving into specific technical details or unique aspects of the hybrid drive system. This highlights the model's limitations in generating specific, high-fidelity technical content. Gemma-2-2b and Phi-1.5 performed poorly, with outputs that were either incomplete or irrelevant, highlighting significant gaps in their ability to handle domain-specific tasks.\nFor the comparison of large language models in specialized IP domains, MOZI-7b offers a response that mixes some technical details with a more conversational style. It attempts to balance technical accuracy with clarity, but the lack of specificity in the details and the informal tone might not be suitable for a patent abstract, where precision and formal language are essential. PatentGPT delivers a comprehensive and well-structured abstract, closely aligning with the requirements for a patent document. It details the system's components, including the battery, internal combustion engine, motor, transmission, and axle. The description is precise, highlighting the technical aspects and the invention's advantages, which reflects the model's specialized training for generating patent-related content. The response demonstrates the model's capability to understand and articulate complex technical concepts effectively, making it the most suitable for patent writing among the models tested."}, {"title": "6 Summary", "content": "In conclusion, as we stand on the verge of a new technological frontier, the role of artificial intelligence in augmenting human creativity and streamlining complex processes like patent drafting has never been more significant. The development of PatentGPT, through the innovative Knowledge Fine-Tuning (KFT) framework, addresses a critical gap in the capabilities of current large language models by equipping them with specialized knowledge and contextual understanding essential for generating precise and legally robust patent documents. Our approach, which integrates knowledge graph-based pre-training with domain-specific supervised fine-tuning and reinforcement learning from human feedback, significantly enhances the model's ability to comprehend and articulate complex technical concepts. The empirical results demonstrate that PatentGPT not only surpasses existing models on patent-related benchmarks but also sets a new benchmark for AI-driven intellectual property creation. This advancement not only facilitates more efficient and accurate patent drafting but also represents a"}]}