{"title": "Imit Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning", "authors": ["Yuhang Dong", "Haizhou Ge", "Yupei Zeng", "Jiangning Zhang", "Beiwen Tian", "Guanzhong Tian", "Hongrui Zhu", "Yufei Jia", "Ruixiang Wang", "Ran Yi", "Guyue Zhoui", "Longhua Ma"], "abstract": "Visuomotor imitation learning enables embodied agents to effectively acquire manipulation skills from video demonstrations and robot proprioception. However, as scene complexity and visual distractions increase, existing methods that perform well in simple scenes tend to degrade in performance. To address this challenge, we introduce Imit Diff, a semanstic guided diffusion transformer with dual resolution fusion for imitation learning. Our approach leverages prior knowledge from vision language foundation models to translate high-level semantic instruction into pixel-level visual localization. This information is explicitly integrated into a multi-scale visual enhancement framework, constructed with a dual resolution encoder. Additionally, we introduce an implementation of Consistency Policy within the diffusion transformer architecture to improve both real-time performance and motion smoothness in embodied agent control. We evaluate Imit Diff on several challenging real-world tasks. Due to its task-oriented visual localization and fine-grained scene perception, it significantly outperforms state-of-the-art methods, especially in complex scenes with visual distractions, including zero-shot experiments focused on visual distraction and category generalization. The code will be made publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "The performance robustness and generalization capabilities of embodied agents in complex manipulation scenarios have long been a focus of significant research interest [17, 47]. Visuomotor imitation learning is one of the mainstream paradigms of robot manipulation policy [6, 40, 48, 9, 13]. This approach enables agents to derive state estimation and decision-making capabilities from expert demonstrations that incorporate high-dimensional visual observations and robot proprioception [49].\nHowever, as scene complexity and visual distractions increase, the performance of decision models that excel in simpler environments tends to degrade [53, 24]. Not only do simple imitation learning policies face challenges, but even advanced multimodal foundation models, such as GPT-40 [16] or vision language action models (VLA) [26, 3, 4, 30, 18, 45], struggle to accurately focus on specific details within se- mantically complex images. In fact, in robot control and embodied multimodal foundation models, the focus is often on action prediction, observation mapping, or multimodal alignment. Therefore, intuitive visual perception enhancement is typically lacking. Models can only acquire task-oriented semantic localization knowledge from relevant visual regions either implicitly or when guided by high-level text instructions [37].\nTo tackle this challenge problem, we introduce Imit Diff, a diffusion transformer imitation learning framework with dual resolution enhancement guided by fine-grained semantics information. Specifically, our work has three key components:\n1) Semanstic injection. Imit Diff transforms task-oriented semantic information and high-level textual guidance into explicit pixel-level visual localization labels through the pretrain knowledge of vision language models (VLM) and vision foundation models, and injects them into the policy observation.\n2) Dual resolution (dual res) fusion. We develop a dual res image observation stream and employed a dual res vision encoder to extract global and fine-grained visual features. The extracted multi-scale visual information is subsequently fused within an attention block, integrating fine-grained details into the global visual feature. This approach enhances scene understanding while maintain- ing computational efficiency.\n3) Consistency policy on diffusion transformer (DiT). Diffusion-based imitation policies often suffer from inef- ficiencies due to the required denoising steps. To address this, we design a DiT [33] action head incorporating a consistency policy [42], enabling the decision layer to achieve high-frequency system responses through single- step denoising. Furthermore, leveraging faster inference times, we introduce temperal ensemble to enhance the smoothness of predicted actions.\nWe design four real-world tasks with challenging manip- ulation precision to evaluate Imit Diff and test the model's scene understanding capabilities by introducing increased scene complexity and visual distractions. Additionally, we conducted zero-shot experiments on visual distraction and category generalization to assess the benefits of the dual res enhancement framework and fine-grained semantic injection. Experimental results demonstrate that Imit Diff significantly outperforms existing strong baselines.\nIn summary, the contributions of our work are three-fold:\n1) We propose Imit Diff, a DiT architecture imitation learning framework with dual res enhancement guied by fine-grained semantics information.\n2) We developed an open-set vision foundation model pipeline to generate explicit visual masks. This approach effectively addresses challenges such as motion blur,"}, {"title": "II. RELATED WORK", "content": "A. Visuomotor Imitation Learning\nImitation learning offers an effective approach for robots to acquire human-like skills through expert demonstrations [44, 1, 12, 38, 10]. Policies based on visual observations have become dominant in the field [32, 11]. These methods focus on mapping or conditioning visual observations to an action space [51, 21]. Recent advancements in diffusion based visuomotor policies have shown significant promise in learning complex manipulation tasks by effectively integrating visual data with high-dimensional, multimodal action distributions [6, 7, 52, 49]. However, these methods focus on the con- struction of the policy decision layer and lack attention to the visual perception layer in robot manipulation scenarios. In our work, we attempt to propose a dual vision workflow as an enhancement framework for the visual perception layer and inject it with fined-grained pixel-level visual semantic information.\nB. Open Vocabulary Vision Foundation Models in Robotics\nMultimodal models that integrate vision, language, and action are crucial for developing embodied agents. While end-to-end approaches are commonly used for offline tasks, learning directly from language-annotated data presents signif- icant challenges, particularly when aligning vision, language observations, and robot sensor data in a shared space [2, 43]. Open vocabulary vision foundation models, including vision language models (VLMs), enable natural language descrip- tions to guide visual understanding through vision-language joint learning [16, 23]. These models demonstrate strong transferability across various downstream tasks, making them valuable tools in robotics for defining complex objects, serving as semantic anchors for multimodal representations, and pro- viding an intermediate foundation for planning and reasoning. Existing methods, such as VoxPoser [14] and ReKep [15], utilize open vocabulary vision foundation models to acquire high-level text instructions or operational constraints. In our work, we leverage the prior knowledge from these models to align high-level semantic information with fine-grained, pixel-level labels that are the same modality with visual observations.\nC. Diffusion Model Acceleration Strategy in Robotics\nDiffusion models typically suffer from long inference times due to their iterative sampling process, which presents a key challenge for improving the real-time performance of robot control. Approaches such as Denoising Diffusion Implicit Models (DDiM) [41] and Elucidated Diffusion Models (EDM) [29] interpret the process as a deterministic ordinary differen- tial equation (ODE), reducing inference time by minimizing denoising steps during prediction. However, this variable step size approach reduces the number of denoising steps, which can degrade sampling quality. Other methods, like Picard iteration, accelerate diffusion models through parallel sam-"}, {"title": "III. METHOD", "content": "Imit Diff is a diffusion transformer imitation learning frame- work that injects fine-grained semantic information into dual- vision workflows, which extract multi-scale visual features. It also ensures real time control of the robot. To this end, we introduce Imit Diff with three critical components: (a) Semantics Injection. Imit Diff injects fine-grained semanstic information into policy learning through vision foundation models. (b) Dual Res Fusion. The semanstic information is then injected into multi-scale vision features extracted by dual res fusion module. (c) Consistency Policy with DiT Architecture. Imit Diff contructs a DiT based action head with consistenvy policy implementation to ensure real time control. An overview of Imit Diff is in Figure 2. We will detail each part in the following sections.\nA. Semanstic Injection\nAs shown in Figure 2 (a) Semanstic Injection Module, given a manipulation task T (e.g., cover the white lid on the red glue), we use GPT-40 to analyze the semantic description of the task, considering specified constraints based on user prompts, and filter out objects relevant to the task. We then use the vision foundation model GroundingDINO [25] to add visual cues to the initial frame Io. The tracker MixFormerV2 [8] ensures these visual cues are maintained during robot motion, including handling motion blur and occlusion. To balance real-time performance with computational efficiency, we employ Mobile SAM [50] to generate fine-grained visual semantic masks from these visual cues. To maintain alignment, the visual semanstic masks are resized to match the low res- olution observations in the dual visual streams (which will be described in detail in Section III-B), and the same pre-trained visual encoder is used for domain fine-tuning through different projectors. We integrate a semanstic injection block Fs with the transformer decoder architecture to inject visual semantic features Is into the visual observation features Iv, where I'v are treated as Q, and I's as K and V. We do not directly use the visual semantic features as separate observations because we aim for the policy perception layer to learn the relationship between foreground objects and the robot state at the image- level through attention mechanisms.\nB. Dual Res Fusion\nImit Diff employs a dual res visual workflow to concurrently extract global features and multi-scale fine-grained features. Figure 3 illustrates this process. The high resolution image IH \u2208 RC\u00d7H\u00d7W is first downsampled to a lower resolution IL \u2208 RC\u00d7H/2\u00d7W/2. For the low resolution image, the global"}, {"title": "C. Consistency Policy with DiT Architecture", "content": "The iterative sampling process of the diffusion model and the pipeline for obtaining visual semantic masks make real time control of the robot a challenge. Inspired by the excellent performance of consistency policy in image generation, we designed a DiT action head based on consistency strategies and integrated it into an asynchronous framework with visual observation acquisition and action prediction. We first train a teacher model G within the EDM framework. We then concatenate time t and observation condition o to form the keys (K) and values (V) in the transformer decoder, replacing the FiLM module used in U-Net and input the current action Xt. The derivative of the Probability Flow ODE (PFODE) trajectory is estimated using the attention mechanism:\n$\t dxt/dt = -(xt \u2013 G\u00a2(xt,t; o))/t$\nWe use an optimized Denoising Score Matching (DSM) loss to train the EDM model. The DSM is sampled along the PFODE trajectory(xt,t) and is trained to predict the initial action ground truth xo.\n$LDSM(0) = Et,x0,xt|xo [d(x0, G\u00a2(xt, t; 0))]$\nd is an optimized Huber loss:\n$d(x, y) = \\sqrt{||x \u2212 y||2 + c2} \u2212 c$\nFor the student model go(xt,t,s;o), we concatenate the stop time s with the token sequence of time t and observation condition o, and then denoise the sampled points (xt1,t1) and (Xt2,t2) along the same PFODE trajectory back to the same stop time s. We denote the denoised outputs as (1) and xt2), corresponding to go (xt\u2081, t1, s; o) and go(xt2,t2, s; o), respectively. These are then denoised back to the initial time step t = 0, resulting in go(x(t1), s, o; o) and go(x+2), s, o; o). The consistency loss is then calculated in the fully denoised action space.\n$L\u0441\u0442\u043c = d (go(x(t1), 5, 0; 0), 90 (x(12), 8, 10; 0))$\nThe final loss calculation combines DSM loss and CTM loss.\n$LCP = QLCTM + BLDSM$"}, {"title": "IV. EXPERIMENTS", "content": "We present experiments to evaluate the performance of Imit Diff on fine manipulation tasks. Four real-world tasks are designed to assess Imit Diff's effectiveness in complex scenes with visual distractions. Ablation experiments highlight the contribution of each component of the policy. Furthermore, zero-shot experiments on visual distraction and appearance / category generalization are conducted to demonstrate that Imit Diff benefits from the dual visual enhancement framework, which injects fine-grained explicit semantic information.\nA. Environment Setup\nRobot Setup: Imit Diff is evaluated across 4 real-world tasks on Airbot Play 6-DoF robot arms, including a teacher and a gripper for demonstration and inference. We use simple RGB web cameras to obtain real-world visual observations from global view and wrist view. Our real-world setup and everyday objects used in our tasks are shown in Figure 4.\nTraning and Inference Setup: For the four tasks, we use 8 \u00d7 A100 with 80 GB of VRAM for training all experiments. We use a desktop containing a single 4060 Ti GPU with 16 GB of VRAM for inference."}, {"title": "B. Tasks and Metrics", "content": "Tasks Setup: We design four real-world tasks with vary- ing manipulation properties and task requirements. Figure 5 illustrates all tasks and we provide detailed explanations of each task's specific content and the rationale behind our design in the following paragraphs. For all four tasks, we collect 100 demonstrations per task using Airbot Play teleoperation: 50 sets with no visual distraction, 30 sets with easy visual distraction, and 20 sets with hard visual distraction. The different levels of visual distraction are illustrated in Figure 6. A summary of the task setup for Imit Diff is provided in Table I.\n1) Stack blocks in order (Stack Blocks). In this task, the Airbot Play Arm is required to stack three blocks of different colors in a specific order. The stacking operations demand precise alignment to ensure that the blocks do not fall off. This task is designed to assess Imit Diff's performance on stack manipulation and long- horizon tasks, where fine-grained visual information is crucial for maintaining stability during the manipulation process.\n2) Cover white lid on red glue (Cover Glue). In this task, the Airbot Play Arm is required to first grab the white lid and then place it onto the red glue. This task is designed to evaluate Imit Diff's performance in insertion-based manipulation, where precise alignment and control are essential for successfully completing the task.\n3) Place jujube in bowl (Jujube Place). In this task, the Airbot Play Arm is required to grab a jujube and place it in a bowl. The task has fine-grained requirements for the grasping position of the jujube to ensure proper placement. This task evaluates Imit Diff's performance in place-and-grasp manipulation, where precision in both the grasping and placement stages is critical.\n4) Place block in drawer (Drawer Place). In this task, the Airbot Play Arm is required to first open the drawer, then grab the block and place it inside. This task is designed to evaluate Imit Diff's performance on fine manipulation tasks involving the operation of hinges.\nMetrics: For each trained policy, we report average success rates on the policy checkpoint (selected using action mean-squared error). For success rates, we average over 25 trials per situation. Starting positions are rondomized between trials for each task.\nC. Baselines and Experiment Results\nBaselines: We benchmark Imit Diff against state-of-the-art imitation learning policies that have demonstrated significant success in complex robot tasks. Specifically, we use Action Chunking Transformer (ACT) as the baseline model for auto-regression and CVAE, and Diffusion Policy (DP) as the base- line model for diffusion. We use the same action prediction"}, {"title": "D. Ablation Study", "content": "We choose Cover Glue as the reference task for the ablation study. The experimental results are presented in Table III. The ablation study demonstrates the effectiveness of each component in Imit Diff. We utilize two different pretrained"}, {"title": "E. Zero-shot Generalization Experiment", "content": "Distraction Zero-shot Experiment. We collected 100 episodes of data without visual distraction for the Drawer Place task. The goal is to evaluate the robustness of Imit Diff to visual distraction in a zero-shot setting. Table IV presents the comparison of Imit Diff with other baseline methods, demonstrating its superior robustness in handling visual distraction in the zero-shot case.\nAppearance & Category Generalization Zero-shot Ex- periment. Similar to the zero-shot visual distraction exper- iment, we collected 100 episodes of data without visual distraction for the Drawer Place task. this time we altered the appearance and category of the task objects, as shown in Fig- ure 7. The experimental results are presented in Table V. Imit Diff leverages the explicit fine-grained semantic observations provided by the vision foundation models and the dual res visual enhancement framework, enabling it to achieve superior object appearance and category generalization capabilities."}, {"title": "V. CONCLUSION", "content": "In this work, we present Imit Diff, a fine-grained semantic-guided diffusion transformer imitation learning framework featuring dual res fusion. The core contribution of Imit Diff lies in its innovative utilization of prior knowledge from vision foundation models. It transforms traditionally challenging-to-align high-level semantic supervision into fine-grained semantic masks that share the same modality as visual observations. This pixel-level semanstic information is then effectively integrated into the multi-scale visual information extracted by the dual res fusion module. Furthermore, the DiT action head, based on a consistency policy, significantly reduces the denoising time of the diffusion transformer in robot control tasks. Imit Diff demonstrates high accuracy and robustness in fine manipulation tasks, even in complex scenes with visual distractions. Additionally, we showcase the generalization capability of Imit Diff through its performance in zero-shot everyday tasks.\nLimitations. Although we have developed an effective vi- suomotor imitation learning policy, several limitations remain in this work. The open vocabulary vision foundation models pipeline achieves 15-20 FPS on a 4060 Ti GPU, which leaves room for optimization. Future work could explore integrating more advanced and lightweight modules, such as SAM2 [36] or Efficient Track Anything [46], to enhance computational efficiency. Additionally, while the consistency policy improves inference efficiency, its training demands substantial compu-tational resources which introduces instability into training process. Diffusion models based on flow matching may remain for future exploration [22]."}]}