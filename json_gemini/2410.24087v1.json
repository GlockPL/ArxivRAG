{"title": "In-Context Fine-Tuning for Time-Series Foundation Models", "authors": ["Abhimanyu Das", "Matthew Fawt", "Rajat Sen", "Yichen Zhou"], "abstract": "Motivated by the recent success of time-series foundation models for zero-shot forecasting, we present a methodology for in-context fine-tuning of a time-series foundation model. In particular, we design a pretrained foundation model that can be prompted (at inference time) with multiple time-series examples, in order to forecast a target time-series into the future. Our foundation model is specifically trained to utilize examples from multiple related time-series in its context window (in addition to the history of the target time-series) to help it adapt to the specific distribution of the target domain at inference time. We show that such a foundation model that uses in-context examples at inference time can obtain much better performance on popular forecasting benchmarks compared to supervised deep learning methods, statistical models, as well as other time-series foundation models. Interestingly, our in-context fine-tuning approach even rivals the performance of a foundation model that is explicitly fine-tuned on the target domain.", "sections": [{"title": "Introduction", "content": "Time-series data is ubiquitous in several domains such as retail, finance, manufacturing, healthcare, and natural sciences. In many of these domains, time-series forecasting, i.e., predicting time-series into the future, is a critical problem - for example, in applications like retail forecasting, climate and weather predictions, and traffic forecasting. In the last decade, deep learning approaches [SFGJ20; OCCB19; SYD19] have become popular in forecasting, often outperforming statistical approaches like ARIMA [BJ68]. However, until recently, deep learning approaches for forecasting have adhered to the traditional supervised machine learning framework of having to train a forecasting model on task-specific training data, before being able to perform forecasting for that task. On the other hand, in Natural Language Processing (NLP), Large Language Models (LLMs) [RWCLAS+19; Bro+20] have shown the promise of foundation models: a single pretrained model can perform well and adapt to tasks like translation, code generation, text summarization during inference time in a zero-shot or few-shot manner.\nMotivated by the success in NLP, there has been significant work in recent years on time-series foundation models for forecasting, ranging from re-purposing LLMs directly for forecasting [GFQW23] to fine-tuning pretrained LLMs on time-series data [ZNWSJ23; CPC23] to pre-training time-series foundation models from scratch [DKSZ24; GSCCLD24; WLKXSS24; Ans+24; GM23]. The last approach in particular has been shown to obtain strong zero-shot accuracy, rivaling the best supervised models trained specifically for the target datasets."}, {"title": "Related Work", "content": "As mentioned previously, there has been a spurt of recent work on time-series foundation models for forecasting. These approaches can be broadly divided into three categories. (i) Prompting LLMs like GPT-4 to directly predict the future of a numerical series encoded as text. This was investigated in LLMTime [GFQW23]; despite the initial promise subsequent works have shown that such approaches can be lacking in accuracy [WLKXSS24; DKSZ24]. (ii) Fine-tuning pretrained LLMs like GPT2 on time-series data with adapter layers [ZNWSJ23; CPC23]. These approaches have mostly been shown to work well in the dataset-to-dataset transfer learning setting (rather than in the zero-shot setting), and they are also disadvantaged from having to use excessively large models due to their LLM backbones. (iii) Pretraining transformer based models from scratch on huge volumes of time-series data, which seems to be the most promising approach towards time-series foundation models [DKSZ24; GM23; Ans+24; WLKXSS24; GSCCLD24]. Indeed, some of these models have shown superior zero-shot accuracy when compared to supervised deep forecasters and statistical methods even on datasets that are outside of their pretraining set.\nSome of the above papers, e.g., [Ans+24; GSCCLD24], have additionally shown that their pretrained models' performance can be further improved by fine-tuning the model on examples from the target dataset. While this supervised fine-tuning results in improved per-task accuracy, this practice breaks the zero-shot paradigm in terms of requiring extra training on the target dataset.\nIn the NLP domain, a defining property of a foundation LLM is its ability to be further adapted to domain-specific tasks through either fine-tuning or prompting. In particular, LLMs have been shown to perform in-context learning on a variety of downstream NLP tasks by prompting them with a natural language instruction [RWCLAS+19] and a few demonstrations or examples of the task. This phenomenon is also referred to as few-shot learning [Bro+20]. Subsequent works [MLZH22; CZZKH22] have proposed fine-tuning a pretrained LLM to obtain better performance on few-shot learning prompts. Other papers [Min+22; Wei+23] have empirically investigated how few-shot learning works in LLMs. More recently, Shi et al. [Shi+23] explored a similar idea for in-context pretraining, where they pretrain an LLM on sequences of related documents. This in-context"}, {"title": "Problem Definition", "content": "Time-series foundation models aim to build a general purpose forecaster that can take in a past history of a target forecasting task, $y_{1:L} = {Y_1, Y_2,\u2026\u2026Y_L}$, where we look back $L$ time-steps and map them to a forecast $\\hat{y}_{L+1:L+H}$, for a horizon length of $H$. The aim is to have $\\hat{y}_{L+1:L+H}$ as close as possible to the unseen future $Y_{L+1:L+H}$ according to some well defined error metric. Such"}, {"title": "Model Architecture", "content": "Motivated by the strong zero-shot performance achieved by stacked transformer models in decoder-only mode for time-series forecasting, we propose to adapt a base TimesFM model [DKSZ24] to leverage the additional information available via in-context examples. In particular, we pretrain TimesFM in its original fashion to obtain a base checkpoint. We then modify the model architecture and continue pretraining from the base checkpoint using training data with in-context examples (we call this phase continued pretraining) to obtain a new pretrained foundation model TimesFM-ICF. The base TimesFM checkpoint that we start from will be referred to as TimesFM (base).\nAdapting their model architecture to make use of the in-context examples is somewhat delicate, and requires modifications to the original model. A depiction of our proposed model architecture is given in Figure 4. As in their model, our model partitions each example into non-overlapping input patches, and uses a shared input residual block (a one-hidden layer perceptron with skip connection, see Das et al. [DKLMSY23]), to embed each patch as a token before feeding the tokens into the stacked transformers in a decoder-only fashion. The output embeddings are mapped to the next output patches via another shared output residual block.\nTo teach the model to use the new in-context examples, we adapt the original TimesFM archi-tecture to better handle (1) the in-context example separators, (2) the cross-example attention, and (3) the positional encoding. Despite these changes, we are still able to leverage the TimesFM (base) checkpoint, which was pretrained for forecasting given just the history of the target time-series. We describe the key details of our architecture design below."}, {"title": "Separators for In-context examples", "content": "Our context window contains in-context examples from different time-series. Hence the model needs to be able to separate these, since na\u00efve concatenation can confuse the model. Consider the example in Figure 3. If we na\u00efvely concatenate multiple in-context examples (e.g., linear trends, Figure 3a) together, then the combination of these trends may appear to the model as an entirely different time-series (e.g., a triangle wave, Figure 3b). Therefore, we choose to insert a common learnable separator token after each in-context example. We visually depict these separators as the dashed lines in Figure 3a. When feeding examples to the decoder, we sequentially pass each tokenized patch of each time-series example to the model, followed by the separator token at the end of an example. This process is depicted in Figure 4."}, {"title": "Cross-example Attention", "content": "In order to allow our model to distinguish between different in-context examples, we allow the transformer to attend (causally) to all previous patches including the separator tokens. Note that, if the model did not attend to the separator tokens, then we could never hope to distinguish between the two scenarios from Figure 3a and Figure 3b. By attending to the previous separator tokens, the model can potentially distinguish how many in-context examples have been processed so far.\nAlthough at the input to the stacked transformer we use a common separator token to separate the examples, the output tokens corresponding to the positions of these separator tokens can play a much more nuanced role as we proceed through the subsequent transformer layers. As the output tokens corresponding to these separator tokens causally attend to all previous tokens, after several transformer layers these tokens can, for instance, potentially summarize the information in all the patches corresponding to their example and/or convey the separation boundaries of the different in-context examples to the model."}, {"title": "Positional Encoding", "content": "Based on the findings in Haviv et al. [HRPIL22], we create the pretrained TimesFM (base) checkpoint with No Positional Encodings (NoPE), in contrast to the absolute positional encod-ings [Vas+17] used in the original TimesFM model. We note that we can achieve the same accu-racy reported in the original TimesFM paper without using any positional encodings. Indeed it has been hypothesized in Haviv et al. [HRPIL22] that the presence of causal attention itself can encode positional information when there are more than one stacked transformer layers.\nThe advantages of NoPE for our continued pretraining are two fold: (i) NoPE models usually have better length generalization, which is particularly important here since we increase the prompt length by adding in-context examples to the context. (ii) If we use the original absolute positional encodings used in [DKSZ24], the meaning of these positional encodings in the base model would be different from their meaning during the continued pretraining with in-context examples. This could cause problems for the continued pretraining phase."}, {"title": "Overall Model", "content": "Since our model builds upon the TimesFM architecture [DKSZ24], we introduce a similar notation style for ease of exposition. The model processes in-context examples in the following fashion. Starting with an example input ${y^{(1)}_{1:T_1},...,y^{(n)}_{1:T_n}}$, each example $y^{(i)}$ is partitioned into input patches of length $p$:\n$\\tilde{y}^{(i)}_j = y^{(i)}_{p(j-1)+1:pj}$ $\\forall j \\in [[T_i/p]]$ and $i \\in [n]$.\nAs in [DKSZ24], our model takes an additional padding mask $m^{(i)}_j$, to ensure that it makes good predictions on time-series which are not a multiple of the patch length $p$. Analogously to the partitioning of the example inputs, we partition the padding masks as:\n$m^{(i)}_j = m^{(i)}_{p(j-1)+1:pj}$ $\\forall j \\in [[T_i/p]]$ and $i \\in [n]$.\nGiven these patches and masks, we feed each patch $\\tilde{y}^{(i)}_j$ through a common MLP embedding layer to obtain tokens:\nt^{(i)}_j = InputResidualLayer(\\tilde{y}^{(i)}_j) (1 \u2013 m^{(i)}_j) \\forall j \\in [[T_i/p]]$ and $i \\in [n]$.\nWe will slightly abuse notation by denoting the separator token $\\sigma$ as $t^{(i)}_{[[T_i/p]] + 1} = \\sigma$, and let the mask for the separator token $m^{(i)}_{[[T_i/p]] + 1} = 0$ (i.e., the separator tokens are never masked). After tokenizing the input patches, we feed the tokens, together with a learnable separator token $\\sigma$, autoregressively to the stacked transformer layers in decoder-only mode. We take $m^{(i)}_{1:j}$ to be the last entry of $m^{(i)}_{1:j}$, and denote the sequence of token/mask pairs corresponding to example $i$ as\nt^{(i)}_{1:j} = ((t^{(i)}_1, m^{(i)}_1),..., (t^{(i)}_{[[T_i/p]]}, m^{(i)}_{[[T_i/p]]})) \\forall j \\in [[T_i/p] + 1]$ and $i \\in [n]$.\nThen, the output of the stacked transformer layer for token $t^{(i)}_j$ can be described as:\no^{(i)}_j = StackedTransformer(t^{(i-1)}_{[[T_i/p]] + 1},...,t^{(i-1)}_{[[T_i/p]] + 1}, t^{(i)}_{1:j},m^{(i)}_{1:j}) $\\forall j \\in [[T_i/p] + 1]$ and $i \\in [n]$."}, {"title": "Positional Encoding", "content": "We emphasize the output $o^{(i)}_j$ for token $t^{(i)}_j$ defined above depends on (i) all previous (unmasked) tokens $t^{(i')}_{j'}$, $i' < i$ and $j' \\in [[T_{i'}/p]]$, (ii) the $i \u2212 1$ separator tokens $t^{(i')}_{[[T_{i'}/p]] + 1} = \\sigma$ for $i' < i$, and (iii) the tokens $t^{(i)}_{1:j}$ for the current example.\nFinally, we feed the outputs $o^{(i)}_j$ from the stacked transformer through a residual block to obtain the predicted time-series:\n$\\hat{y}^{(i)}_{pj+1:pj+h} = OutputResidualLayer(o^{(i)}_j) \\forall j \\in [[T_i/p]]$ and $i \\in [n]$.\nThis corresponds to the model\u2019s prediction of the next $h$ steps (output patch length) of $y^{(i)}_{pj+1:pj+h}$"}, {"title": "Loss Function", "content": "Similar to [DKSZ24], we use Mean Squared Error (MSE) as our point forecast loss:\nTrainLossPerContext = $\\frac{1}{\\sum_{i=1}^n [[T_i/p]]} \\sum_{i=1}^n \\sum_{j=1}^{[[T_i/p]]} ||\\hat{y}^{(i)}_{pj+1:pj+h} - y^{(i)}_{pj+1:pj+h}||^2$."}, {"title": "Pretraining Data", "content": "As mentioned before, we start with TimesFM (base) which was pretrained on a diverse corpus of about 400B time-points. Please see Table 1 in Appendix A.1 and Das et al. [DKSZ24] for more details on the datasets. We then continue pretraining it on training data containing in-context examples.\nContext Generation. We convert individual datasets to generate contexts with in-context examples that the model sees during the continued pretraining. Recall that the original TimesFM model is trained up to a maximum history length of $L_{max} = 512$. During the training of TimesFM (base) a time-series of length $T = L_{max} + h$ is loaded for back propagation where $h = 128$ is the output patch length. Therefore, we choose $T$ as the maximum length of our $n$ in-context examples. For any time-series in a particular dataset, we use windowing with a shift of 1 to generate examples of length $T$ i.e. for a time-series $y_{1:M}$ the possibles examples are ${y_{1:T}, y_{2:T+1},\u2026\u2026y_{M-T+1:M}}$. For time-series that are less than $T$ in length, we generate padded examples as detailed in Appendix A.1. Now these examples are packed in groups of $n$ to form the context. We consider two kinds of grouping:\n1. Times-series level: For a long time-series, we can split the original time-series into shorter time-series examples, each of length $T$, then select $n$ of those shorter examples to form the context${y^{(i)}_{1:T}}_{i=1}^n$ for the original time-series.\n2. Dataset level: For each dataset, we can group any $n$ segments of length $T$ from any of the time-series in that dataset, to form a context. For instance, a set of $n$ segments from any of the time-series from the Electricity dataset could be grouped to form a context ${y^{(i)}_{1:T}}_{i=1}^n$.\nBoth time-series level and dataset level groupings guarantee that the grouped examples have similar patterns to borrow from each other.\nDataset Mixture. We choose all datasets in Table 1 other than the four Wiki datasets to generate in-context examples for continued training. The Wiki datasets contain millions of time-series that correspond to a wide variety of articles, which need not be related to each other. In fact"}, {"title": "Experimental Results", "content": "Following prior time-series foundation model papers like [DKSZ24; GFQW23], we compare the zero-shot performance of our proposal with that of supervised models, statistical models trained per dataset as well as other zero-shot models. Similar to prior works, we report our results on a subset of Monash datasets [GBWHM21] and the ETT datasets [Zho+21] that have not been seen by our model or the TimesFM (base) model."}, {"title": "Out-of-domain Forecasting on Monash", "content": "Monash archive [GBWHM21] is a collection of 30 datasets of different training and prediction lengths that covers granularities ranging from minutes to years and domains including finance, de-mand forecasting, weather and traffic. The archive reports four official metrics for several statistical"}, {"title": "Out-of-domain Forecasting on ETT", "content": "A group of long horizon datasets have been commonly used for benchmarking (mainly) transformer based deep learning algorithms starting from [Zho+21]. Some of the datasets in these benchmarks are in our pretraining datasets (like Electricity and Traffic). Therefore, for the interest of zero-shot evaluation we use the 4 Electricity Transformer Temperature (ETT) datasets, specifically ETTh1, ETTh2 (hourly) and ETTm1, ETTm2 (15 min).\nIn terms of baselines, following [DKSZ24], we compare against Informer [Zho+21] and sub-sequent works like Pyraformer [Liu+21], FEDFormer [ZMWWSJ22], PatchTST [NNSK22]. We also compare with N-HiTS [COOGMD23] which yields an improvement over N-BEATS [OCCB19] for these datasets. Similar to Das et al. [DKSZ24], we focus on the task of predicting horizon lengths 96, 192 given a history of 512 time-steps. We provide rolling validation numbers for the test time-period which consists the last 1/5-th of the time-points. This is standard for these bench-marks [NNSK22], where the datasets are split into train:validation:test in the ratio 7:1:2.\nWe present the MAE obtained for horizon lengths 96 and 192 averaged over the 4 datasets in Figure 5b. Note that since the MAE is computed on scaled datasets in this benchmark [Zho+21], we can directly report the arithmetic mean across datasets. We see that TimesFM-ICF yields a marked improvement of more than 25% on mean MAE over the nearest baseline. PatchTST, N-HiTS and TimesFM (base) perform similarly and are much better than the other baselines. In this case, all the datasets have in-context examples with enough time-points to cover T time-steps, unlike in Monash where 9 out of 18 datasets have time-series of length less than 512 time-steps. Therefore, we can see more value from in-context fine-tuning. We provide a more fine-grained analysis with the number of in-context examples on ETTh datasets in Sections 6.4.1 and 6.4.2."}, {"title": "Comparison with Fine-tuning per dataset", "content": "One of the main motivations of this work was to see whether we can recover the gains from fine-tuning foundation models on the target domain without doing any gradient updates. Therefore, in this section, we compare against a very strong baseline: for every dataset in our Monash benchmark"}, {"title": "Number of examples", "content": "The number of in-context examples is an important consideration that dictates the performance of our model. We perform an ablation where we vary the number of in-context examples from 1 to the maximum during our training i.e. n = 50. The corresponding results are reported on the ETTh test set in Figure 7. We can see a monotonic increase in performance with more in-context examples. We chose to perform this ablation on the ETT datasets since, unlike the Monash datasets, all time-series are big enough to provide complete in-context examples of length T, which makes it easier to perform this experiment."}, {"title": "Longer History", "content": "In this section, we compare the performance of TimesFM-ICF with a version of TimesFM (base) trained with a longer history $L = 2048$ which we will refer to as TimesFM (LH). We provide the aggregate scaled MAE on Monash datasets in Figure 6b where we include two versions of TimesFM-ICF, one with 4 in-context examples (TimesFM-ICF-4ex) and one with 50 in-context examples (TimesFM-ICF-50ex). We can see that TimesFM (LH) yields a modest 1% improvement over TimesFM (base) (which has a maximum history of 512) while TimesFM-ICF-50ex yields a 7% improvement. Even TimesFM-ICF-4ex which uses the same total context length for all in-context examples as TimesFM (LH) is 3% better than the baseline.\nThis shows that our technique of in-context fine-tuning can be more effective than training a longer history model, especially when there is a mix of short-history and long-history time-series. This is because, for in-context fine-tuning, many short time-series can be packed as in-context examples inside the context, while for the case of usual long history training such time-series will just be padded and most of the context is wasted. As shown in the detailed results in Appendix A.2, the long history model performs better on longer datasets like australian electricity demand, but degrades on shorter datasets like cif and tourism yearly."}, {"title": "Conclusion", "content": "In this paper, we introduce and study a methodology for in-context fine-tuning of a time-series foundation model for forecasting. In particular, we start with a base foundation model and adapt it to be able to effectively utilize, at inference time, not just the history of the target time-series for"}, {"title": "Appendix", "content": "A.1 More Details about our Model and Baselines\nMonash Baselines. For the results on Monash datasets, we borrow the official numbers from [GB-WHM21]. For LLMTime [GFQW23] we use the pre-computed outputs supplied by the original authors.\nWe also add the PatchTST [NNSK22] as a baseline for this benchmark because it is the best performing baseline (only worse than our models) in the ETT datasets. For this model we use the hyperparameters used by original paper for the ETTh datasets 5.\nETT Baselines. On the ETT datasets, the baseline numbers (except TimesFM (base)) are borrowed from the official numbers reported in Table 2 of [DKLMSY23]. We evaluate the base model, TimesFM (base) as well as our method in a rolling validation manner on the test splits to obtain the corresponding metrics.\nTimesFM (base). Following Das et al. [DKSZ24], we train a 200M model with 16 attention heads, 20 layers, a input patch length of 32 and output patch length of 128. The model dimension is set to 1280. We use the learning rate schedule in [Vas+17] with peak learning rate of 5e \u2013 4. The hidden dims of both the residual block and the FFN in the transformer layers are set as the same as model dimensions. We keep layer norm in transformer layers but not in the residual blocks. The only difference between the model in Das et al. [DKSZ24] and our base model is that we use NOPE instead of teh absolute positional encoding [Vas+17]. As we have mentioned before, this leads to no loss in accuracy while being easier to extend to our in-context fine-tuning setting.\nFine-tuning Per Dataset. On the Monash benchmark, we also compare with TimesFM (base) fine-tuned on the train set for every dataset and the forecasting on the corresponding test set. For all our fine-tuning runs, we use a batch size of 16 and a maximum of 10k iterations. Note that this means that the fine-tuned model will see many more training examples than the in-context examples given to our model. For the fine-tuning runs, we use the same decoder only loss function that was used in the original pretraining of TimesFM (base), the only difference is that the training is not restricted to the training set of one dataset. We do two kinds of fine tuning:\n\u2022 Full: All weights in the model are updated during fine-tuning.\n\u2022 Linear Probing (LP): We hold the transformer weights fixed and only update the parameters in the input and output residual blocks.\nTimesFM-ICF. We continue to train TimesFM-ICF model from TimesFM (base). Therefore, most of the parameters in the model remain the same. Here, are the key training details that are unique to TimesFM-ICF:\n\u2022 Separator Token: We have a trainable separator token that is also updated during the con-tinued pretraining. The token is nothing but a learnt embedding whose dimension is equal to the model dimension i.e. 1280 in our case.\n\u2022 Number of Examples: We use a maximum of n = 50 in-context examples for each context during training.\n\u2022 Padding: In short datasets like M4 yearly and quarterly, each time-series might have number of time-points much less than T = 640. Sometimes the number of time-points are even less than our input patch length p = 32. For such cases, a whole time-series can fit into one of the n examples and they are preprocessed in the following manner:"}, {"title": "Positional Encoding", "content": "If the length of the time-series l is less than p, we left pad with k padding time-points such that $p < k + l < 2p$. This is because we want the decoder only model to predict something meaningful for the second patch after seeing the first patch and if not, is penalized by the loss on the second patch. If the $l > p$, we do not need to perform this left padding.\nLastly, we right pad such that the length of the total padded example is $T = 640$.\nNote that the last patch in such examples would be padded from the right, i.e., they will have real time-series values for the first few points and padding for the rest. We make sure that such incomplete from the right patches are not attended by subsequent tokens belonging to examples coming after.\nThe pretraining datasets are detailed in Table 1."}, {"title": "Detailed Metrics on Monash and ETT", "content": "A.2.1 Monash\nTable 2 presents the per-dataset MAE numbers of TimesFM-ICF against other supervised and zero-shot methods on Monash.\nA.2.2 ETT\nTable 3 presents the MAE numbers of TimesFM-ICF against other methods on ETTh1, ETTh2, ETTm1 and ETTm2 respectively, with forecasting horizons of 96 and 192 respectively."}, {"title": "varying the number of in-context examples", "content": "Table 4 and 5 shows the accuracy metric numbers of TimesFM-ICF on ETT and Monash respec-tively when different numbers of in-context examples are used."}, {"title": "Long History", "content": "Table 6 and 7 show respectively the aggregated (geometric mean of scaled MAE) and the raw MAE numbers on Monash of different TimesFM models, with the focus on the comparison between TimesFM-ICF and TimesFM (LH) which is a long-2048-history TimesFM model. We compare TimesFM-ICFin two different modes: (i) 50ex, in which the model has access to 50 in-context examples, and (ii) 4ex, in which the model has access to only 4 in-context examples. In mode (ii), the aggregate length of all in-context examples is the same as the length of the history used by TimesFM (LH)."}, {"title": "Fine-tuning per Dataset", "content": "Table 8, 9 and 10 present the detailed accuracy and timing metrics to compare TimesFM-ICF and FT-TimesFM on Monash. While TimesFM-ICF is more accurate, it is also significantly faster than"}, {"title": "Illustrative Examples", "content": "We illustrate visually in Figure 8 how in-context examples can help disambiguate the prediction tasks, by plotting the actual forecasts from TimesFM-ICF with and without the in-context exam-ples. In the left two figures, the history is not sufficiently informative for the model to make an accurate prediction. By providing in-context examples together with this short history (see the right two figures), however, the model is able to make a more accurate forecast."}]}