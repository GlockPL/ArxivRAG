{"title": "FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation", "authors": ["Peng Ling", "Wenxiao Xiong"], "abstract": "Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training. In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear in-stance segmentation problem. In this framework, we construct a fourier guidance mod-ule to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear. Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision. In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at https://github.com/LQY404/FrGNet.", "sections": [{"title": "1. Introduction", "content": "Pathological slide analysis is widely regarded as the gold standard for cancer diagnosis, treatment, and prevention. Nuclear instance segmentation is a critical step in this process, because the nuclear features such as average size, density and nucleus-to-cytoplasm ratio are related to the clinical diagnosis and management of cancer. In general, some of the major nuclear segmentation methods are based on fully-supervised designs"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Nuclear instance segmentation", "content": "Before the emergence of deep learning, instance segmentation methods predominantly relied on classical machine learning algorithms and image processing techniques. These included statistical features, thresholding-based methods and morphological features for contour and shape identification. Energy-based solutions gained prominence during the 1990s. Marker-based approaches such as the Watershed algorithm proved beneficial for nuclear instance segmentation, as demonstrated by Cheng et al. (2008). However, these techniques faced limitations in accurately segmenting nuclear due to their diverse structures, textures, intensities, leading to unreliable outcomes. Moreover, the effectiveness of these systems heavily depended on manual parameter tuning, including thresholds and weights, rendering them unsuitable for widespread application due to their inherent unreliability.\nWith the rapid development of deep learning, Ronneberger et al. (2015) proposed the UNet model, which has since become one of the most fundamental models in medical image segmentation. Raza et al. (2019) introduced Micro-Net, achieving robustness to large internal and external variances in nuclear size by utilizing multi-resolution and weighted loss functions.\nQu et al. (2019) developed a full-resolution convolutional neural network (FullNet), which enhances localization accuracy by eliminating downsampling operations in the network structure.\nHe et al. (2021b) presented a hybrid attention nested U-shaped network (Han-Net) to extract effective feature information from multiple layers.\nTo leverage contour information for distinguishing contact/overlapping nuclear, Chen et al. (2017) initially proposed incorporating contour information into a multi-level fully convolutional network (FCN) to create a deep contour-aware network for nuclear instance segmentation. Subsequently, Zhou et al. (2019) introduced the contour-aware information aggregation network, which combines spatial and textural features between nuclear and contours. Additionally, some models have approached nuclear instance segmentation as an object detection task, such as contour proposal networks (CPN) which use a sparse list of contour representations to define a nuclear instance.\nSeveral works have introduced distance maps to separate contact/overlapping nuclear. Naylor et al. (2018) addressed the issue of segmenting touching nuclear by formulating the segmentation task as a regression task of intra-nuclear distance maps. Graham et al. (2019) proposed Hover-Net, a network for simultaneous nuclear segmentation and classification, which uses the vertical and horizontal distances between a nuclear pixel and its center of mass to separate clusters of nuclear. Moreover, He et al. (2021a) introduced a centripetal directional network (CDNet) for nuclear instance segmentation, incorporating directional information into the network. He et al. (2023a) take topological information into consideration to further split overlapping nuclear instance. These works mainly are proposed for full-supervised nuclear instance segmentation, and don't consider the characteristic of nuclear itself. In this work, we make full use of the characteristic of nuclear instance (we name it as nuclear guidance), introduce a new framework with guidance, not only can solve full-supervised but also weak-supervised nuclear instance segmentation efficiently."}, {"title": "2.2. Weakly-supervised segmentation", "content": "Weakly supervised approaches offer the advantage of reducing manual annotation effort compared to fully supervised methods. In natural image segmentation, Papandreou et al. (2015) proposed an Expectation-Maximization (EM) method for training with image-level or bounding-box annotations. Pathak et al. (2015) added a set of linear constraints on the output space in the loss function to leverage information from image-level labels.\nIn contrast to image-level annotations, point annotations provide more precise location information for each object. Bearman et al. (2016) incorporated an objectness prior in the loss function to guide the training of a CNN, aiding in the separation of objects from the background. Scribble annotations, which require at least one scribble per object, are a more informative type of weak label. Lin et al. (2016) used scribble annotations to train a graphical model that propagates information from the scribbles to the unmarked pixels.\nBounding boxes are the most widely used form of weak annotation, applied in both natural images and medical images.\nKervadec et al. (2019) utilized a small fraction of full labels and imposed a size constraint in their loss function, achieving good performance, though this method is not applicable to scenarios involving multiple objects of the same class. Another method proposed a two-stage approach that uses only a small fraction of nuclear locations.\nIn this work, we propose a novel end-to-end framework to solve both fully-supervised and weakly-supervised nuclear instance segmentation tasks. We start from the characteristics of the nuclear image itself, and use the information of this feature as the a priori information to build the corresponding module to guide the model for training. With only a small amount of labeled data, our model is able to approach fully-supervised results."}, {"title": "2.3. Contrastive learning", "content": "Contrastive learning is a highly regarded technique for learning representations from unlabeled features these days. It aims to enhance representation learning by contrasting similar features (positive pairs) against dissimilar features (negative pairs). A key innovation in contrastive learning lies in the selection of positive and negative pairs. Additionally, the use of a memory bank to store more negative samples has been adopted, as this can lead to improved performance.\nIn the field of segmentation, numerous works leverage contrastive learning for the pre-training of models Recently, Wang et al. (2021a) demonstrated the advantages of applying contrastive learning in a cross-image pixel-wise manner for supervised segmentation. The CAC approach shows improvement in semi-supervised segmentation by performing directional contrastive learning pixel-to-pixel, aligning lower-quality features towards their high-quality counterparts.\nUnlike these works, we construct the guide-based insatnce level contrastive (GILC) module from the image characteristics of the nuclear, which relies on the automatically generated guide mask to further enhance the feature representation of the nuclear, and is able to be applied to both fully-supervised and weakly-supervised nuclear instance segmentation tasks."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "As illustrated in Fig. 2, our framework consist of three parts: feature extraction module, fourier guidance module, and guide-based instance level contrastive module. For an input nuclear pathology image, we first extract the image features using the feature extraction module to obtain multi-level image features, with the layers being smaller at higher resolutions. Secondly, for the multilevel image features, we uniformly input them into the fourier guidance module for processing.\nIn the fourier guidance module, we select the feature map of layer 0 as the input to the fourier guide head to get the predicted guide mask, and use the pre-generated guide mask ground truth for supervision. Subsequently, for the predicted guide mask, we use guide attention residual unit (GARU) to filter and reinforce all the feature maps with features to produce new feature maps. Finally, in order to get the nuclear segmentation instances, we extract the layer 1 from the new feature maps as the input to the instance head to get the predicted instance results, supervision only for labeled instances.\nIn addition, in order to further enhance the feature representation of nuclear, we use the proposed instance-level comparison module to make full use of the bootstrap a priori information to strengthen the nuclear features and enhance the model performance.\nTo make the manuscript self-contained, we briefly describe the necessary technical details of the original Contour proposal networks (CPN) first, and then introduce how we extend it with proposed fourier guide module and guide-based instance level contrastive module for nuclear instance segmentation task."}, {"title": "3.2. Contour proposal networks", "content": "Contour proposal networks (CPN) is proposed by Upschulte et al. (2022), which models nuclear instances using explicit contour expressions. CPN consists of five parts: feature extraction backbone, classification head, contour regression head, contour refinement regression head, and post-processing. For the input nuclear image, feature extraction is first performed using backbone (e.g., ResNet ). Then, layer 2 feature map is extracted for instance classification and instance contour regression to obtain instance proposals. For these instance proposals, they are sampled according to the ground truth, and only the proposals that correspond to the nuclear of the cell according to the ground truth are retained. These retained proposals are then fine-tuned to all contours using the results generated by the contour refinement regression head, so that each predicted contour more closely matches its corresponding ground truth. Finally, overlapping instances are removed using Non-Maximum Suppression (NMS) to get the final output instance results."}, {"title": "3.3. Fourier guidance module", "content": "As shown in Fig. 1, we find that the fourier transform can be used to obtain the example segmentation results of the nuclear in the pathology image of the nuclear directly, using the image characteristics of the nuclear, without any training at all. However, such segmentation results are rough and cannot handle more complex cases. Therefore, we inject this image characteristics information of the nuclear itself into the model training process as a kind of a priori information to guide the segmentation of the model.\nTo this end, we first design the automatic generation strategy of guide mask as shown in Fig. 4. Specifically, for an input image of a nuclear pathology, we first use the fourier transformation on it to obtain the corresponding fourier spectrogram. Second, this spectrogram is low-pass filtered using a circle with radius R (we set it as 1 during experiments) to obtain the corresponding high-frequency fourier spectrogram. Again, for the high-frequency fourier spectrogram, we use the inverse fourier transformation to convert it back to the spatial domain image to obtain the initial guide mask. At this time, the guide mask has more noise, in order to facilitate the processing, we carry out the normalization operation on it, so that its pixel value is normalized to between 0-1, obtaining the normalized guide mask. Finally, we add the binary instance ground truth mask on the basis of the normalized guide mask to get the final guide mask trained with the guide model.\nAfter obtaining the guide mask through the above process, we designed the fourier guidance module to further handle the integration of the guide mask with the model training process.\nAs shown in Fig. 2, the input of the fourier guidance module is multi-layered feature maps. For this feature maps, we take the layer 0 feature map Fo and construct the fourier guide head, using Fo as the input to get the predicted guide mask, which is supervised using Loss guide mask\u00b7\nMeanwhile, we utilize the predicted guide mask to filter all feature maps using the constructed GARU. Specifically, for each feature map Fi, we first resize the predicted guide mask to the same size as Fi, and then use the GARU to bootstrap the update of Fi to strengthen the features at the corresponding locations of the nuclear, and weaken the features of the non-nuclear regions to obtain the updated feature map. For this updated feature map, we then use instance head to predict the segmentation of instances to get the final segmentation result."}, {"title": "3.4. Guide-based instance level contrastive module", "content": "In nuclear pathology images, there is a more pronounced difference between the nuclear and the background region, and a higher degree of similarity between the nuclear and the nuclear. Therefore, the similarity between the features of different nuclear should be as similar as possible. In our framework, the input of instance head is the feature map F\u2081 of layer 1, with size D\u2081 x h\u2081 x w\u2081. During processing, we treat each position on F\u2081 as an instance-level feature. Therefore, at most h\u2081*w\u2081 instance features may exist at this point. In the actual training process, the model is able to gradually learn the nuclear instance feature representation due to the presence of ground truth. In order to further promote the feature representation of nuclear, we utilize the guidance information provided by the guide mask to further strengthen the model's feature representation of nuclear.\nSpecifically, for each nuclear feature Ci, a vector of dimension D\u2081, we first adopt a similar projector as SimLRV2 which maps C; into a hidden space of dimension P to obtain nuclear feature embedding. Subsequently, we resize the guide mask to h\u2081 x W\u2081 dimensions. Finally, based on the resized guide mask, all the nuclear feature embedding are searched for their corresponding positive and negative samples, and supervised training is performed using the contrast"}, {"title": "3.5. Objective functions", "content": "To optimise our FrGNet, our loss function consists of three parts: instance, guide mask, and contrastive:\n$\\Loss = Loss_{instance} + LOSS_{guide\\_mask} + LOSS_{contrastive}$ (1)\nFor instance loss, since we use contour to represents each instance, the problem of segmentation of instances is solved by transforming it into a regression of contour coordinates. In the process of implementation we use the contour expression form in CPN and the instance loss construction, please see the original article for details. The guide mask is generated by the fourier guide head, whose ground truth is a mask between 0 and 1. Therefore, we use binary cross entropy loss for supervision:\n$\\LoSS_{guide\\_mask}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i log(\\hat{y}_i) + (1 - y_i) log(1 \u2013 \\hat{y}_i)]$ (2)\nWhere N is the number of samples, $y_i$ is the true guide mask of the i-th sample, and $\\hat{y}_i$ is the predicted obtained guide mask. As for Loss contrastive, we use infoNCE loss (Oord et al., 2018) to construct, defined as below:\n$\\LOSS_{contrastive} = \\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp(sim(z_i, z)/T)}{\\sum_{j=1}^{N} exp(sim(z_i, z_j)/T)}$ (3)\nwhere N is the number of samples, zi is the i-th instance embedding, z is the positive instance embedding of zi. sim() represents the similarity operation, we use dot product during experiments. The 7 denotes temperature coefficient, used to adjust the distribution of similarity scores, we keep it as 0.1."}, {"title": "3.6. Inference stage", "content": "The workflow of inference stage is shown in Fig. 3. Firstly, for an input image, after processed by FrGNet, the instance head generates original predicted instances, the instances are generated in the form of contour. Secondly, the fourier head generates the predictions of the guide mask. Finally, for each of the predicted instances, we perform post-processing operations in conjunction with the predicted guide mask. Specifically, for a predicted instance, we extract the region of the guide mask where the instance is located from the predicted guide mask. If the maximum value of the pixel values in the region's guide mask is less than a set filtering threshold, the instance is discarded, otherwise it is retained. Such a filtering operation is performed until the last predicted instance, and the retained instances are the final output refined instances."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental settings", "content": "Dataset: 1) MoNuSeg (Kumar et al., 2017) contains 51 H&E stained pathological images of size 1000\u00d71000 from seven organs, including a total of 21,623 nuclei labeled without distinction between various categories. In the original dataset, there are 37 for training, and 14 for testing (combining the same organ and different organ test sets); 2) CPM17 (Vu et al., 2019) contains 64 H&E stained histopathology images with 7,570 annotated nuclear boundaries, where 32 for training and 32 for testing. It is from the MICCAI 2017 Digital Pathology Challenge (Vu et al., 2019) and images on two different scales: 500\u00d7500 and 600\u00d7600. This is also a dataset for single class segmentation task."}, {"title": "Evaluation metrics", "content": "We use four instance-level evaluation metrics to measure the instance segmentation performance of the comparison models, which are: Aggregated Jaccard Index (AJI) mean Detection Quality (DQ), mean Segmentation Quality (SQ), and mean Panoptic Quality (PQ) (Kirillov et al., 2019). AJI computes the correlation between intersection and union pixel counts. It addresses the issue of over-penalization by employing ground truth with maximal intersection over union:\n$AJI = \\frac{\\sum_{i=1}^{M}|G_i \\cap P_i|}{\\sum_{i=1}^{M}G_i \\cup P_i + \\sum_{F\\in U} |P_F|}$ (4)\nwhere n denotes the total number of nuclei, $P$ represents the connected regions responsible for generating mask. $G_i$ represents the connected area where $P_i$ has the largest intersection with actual data. U denotes connected regions that are not intersecting with actual data, and $P_F$ denotes element F inside set U. DQ, SQ and PQ defined as Eq. (5).\n$PQ = \\frac{\\sum_{(x,y)\\in Tp} IoU(x, y)}{|TP|} = \\frac{TP}{|TP| + \\frac{1}{2}|FP| + \\frac{1}{2}|FN|} * \\frac{TP}{|TP| + \\frac{1}{2}|FP| + \\frac{1}{2}|FN|} $segmentation quality (SQ) detection quality (DQ)(5)\nwhere x is the ground truth values, y are the prediction values. IOU denotes intersection over union and each (x, y) is established as a distinct and different set. TP, FP, FN represent true positive, false positive and false negative, respectively."}, {"title": "4.2. Implementation details", "content": "We proposed FrGNet is trained and tested using the open-source software library Pytorch 1.13.1 on 2 NVIDIA GeForce 3090 with CUDA 11.7.\nConsidering the size of the MoNuSeg and CPM17 datasets, we crop patches from the original histopathologic images using fixed-size boxes. For the cropped instances, if the area of a cropped instance is less than 10% of the initial instance, the cropped instance is removed. Additionally, we remove small objects with an area of less than 20 pixels to avoid unnecessary foreground caused by incorrect pixel predictions. We obtain 481 patches and 416 patches from MoNuSeg and CPM17 dataset respectively.\nFor full-supervised setting, we use all instance annotations to train proposed FrGNet model. We train for 50 epoches with a batch size of 16 in MoNuSeg dataset. AdamW is adopted as the optimizer, with an initial learning rate 0.0004. As for CPM17 dataset, We train for 30 epoches with a batch size of 16. Adam is adopted as the optimizer, with an initial learning rate 0.001. Both training use warm-up and multi-step decay strategy to control the change of learning rate. Additionally, we use replace standard batch normalization with synchronized batch normalization during training.\nFor weak-supervised setting, we first random keep 20%, 30%, 40%, 60% and 80% instance annotations in each image to execute training. Then, we use full annotations to test model that trained in weak-supervised setting. Except for using hard guide, other training settings are consistent with full supervised. During inference, we set the threshold of post process as 0.5."}, {"title": "4.3. Performance comparisons in full-supervised", "content": "We compare our weakly-supervised method with fully-supervised methods that are trained with the completely-annotated nuclei instances, such as U-Net, Mask-RCNN DCAN DIST Micro-Net Full-Net Hover-Net, PFF-Net CDNet CPN and TopoSeg The quantitative results are shown in Table 1. As can be seen from the table, our proposed method FrGNet possesses a significant performance advantage over all current SOTAs on the Monuseg dataset, with at least 4.8% and 2% improvement in the PQ and AJI metrics, respectively, to achieve the new SOTA results. Correspondingly, our method also has some advantages on the CPM17 dataset, proving the effectiveness of our method.\nTo further demonstrate the effectiveness of our framework, we performed a comparison from a visualisation point of view, and the comparison results are shown in Fig. 6. Our FrGNet not only handles the case of dense segmentation of nuclear well (first row), but also can substantially suppress the emergence of false positive instances when the distribution of nuclear is more sparse (the second and the third rows), which is attributed to the design of our Fourier-guided model architecture."}, {"title": "4.4. Performance comparisons in weak-supervised", "content": "The quantitative results of the method proposed in this paper and the existing methods under weakly supervised design are shown in Table 2. Under weak supervision, we randomly remove 20%, 40%, 60%, 70%, and 80% of the instance annotations in each image, followed by training, and testing with the same test set as full supervision. As can be seen from the table, the quantitative metrics of baseline show a linear decrease as the instance annotations are reduced, while our method always maintains a performance that is not too different from that of full supervision. Structurally, the shape of the nucleus is basically similar, so the model can learn the shape characteristics of the nucleus with little data. The localisation of nucleus instances can be done by our Fourier guidance module, and thus the method proposed in this paper still has good performance even with a rather small amount of annotations. The visualization results are shown in Fig. 7."}, {"title": "4.5. Ablation study", "content": "We performed ablation studies on the CPM17 and MoNuSeg datasets to validate the efficacy of the proposed FrGNet method. All the following experiments were conducted under the setting of fully supervised tasks.\nEffectiveness of FG and GILC modules. Our proposed FG module provides guiding information about the location of the nucleus for the whole framework, and the GILC module enhances the feature representation of the nucleus using instance-level information. Table 3 shows the changes in the performance metrics of the model when the FG and GILC modules are used or not. From the table, it can be seen that when both FG and GILC modules are not used (#0), the model does not introduce Fourier bootstrap information at this time, and thus has the worst performance. After the FG module is used (#1), the model's performance is improved to some extent thanks to the introduction of the a priori bootstrap information. And when the FG and GILC modules are used at the same time (#2), the two modules promote each other and work together to provide the model with optimisation information, which promotes the model to further learn further about the feature representation of the nucleus as well as the distribution of the nuclear location, in which case the model performance is highest.\nType of guide mask. By generating a fourier guidance mask (as shown in Fig. 4) is not a binary mask, but a floating-point form of mask. In concert with the commonly used and effective label smoothing trick (M\u00fcller et al., 2019), we further explored the impact of the supervised form of the guide mask on model performance. Specifically, the guide mask generated by Fig. 4 is the soft form of guide mask, and the hard guide mask is obtained by binarising the soft guide mask. After obtaining the soft guide mask and the hard guide mask, we explored the effect of the type of guide mask on the performance of the model, and the results are shown in Table 4. From this table, it can be seen that when no guide mask is used, the model performs poorly (#0) due to the missing guide information. When the guide mask is used, the model performance is improved (#1&2). At the same time, the use of soft form of guide mask has the greatest improvement in the performance of the model, this is because soft form of guide mask not only brings a priori guidance information, but also brings feature information to the model that cannot be brought by the hard form, and these feature information makes the model more robust."}, {"title": "4.6. Generalization", "content": "As shown in Table 2, our proposed method is able to keep stable performance when data annotations decreasing. In order to further explore the generalization ability of FrGNet, we use a private dataset. This dataset consist of 3100 histopathologic images, without any instance annotations. Specially, we initial our FrGNet model with the model weight that training on the MoNuSeg dataset in full-supervised setting. Then, we fine-tune this model 5 epoches, with freeze backbone and other heads"}, {"title": "5. Limitations and discussion", "content": "In this work, we present a framework for segmenting nuclear instances based on fourier guidance. The framework incorporates the feature information of the nuclear image itself, and injects the a priori information of the nuclear location into the model by using a constructed fourier guidance (FG) module to guide the model for training. To further improve the model performance, we propose the guide-based instance level contrastive (GILC) module, which provides instance-level feature guidance for the model and enhances the feature representation of the nuclear instances. Through comparison and ablation experiments, the effectiveness of our framework and the proposed modules are demonstrated to reach a new SOTA in performance. Meanwhile, it is demonstrated through experiments that thanks to the construction of fourier guidance information, our framework is able to solve both fully supervised and weakly supervised nuclear instance segmentation problems. Besides, our model possesses good generalisation. By working on a private nuclear pathology dataset without any supervised annotations, our model is still able to perform the task of instance segmentation of nuclear well in data that has not been seen at the time of training.\nNuclear in pathology images are generally difficult to annotate. How to construct a completely unsupervised framework for segmenting nuclear instances is a task of its own interest, and solving the problem will largely contribute to the development of nuclear segmentation in the field of pathology. The framework proposed in this paper, although possessing a certain degree of generalisation, still needs to rely on a pre-trained model on fully supervised dataset, and is unable to accomplish the task of unsupervised nuclear instance segmentation in the true sense. Recently the field of unsupervised segmentation has gained some development (Sun et al., 2024; Niu et al., 2024), which is of some guidance for unsupervised nuclear instance segmentation."}]}