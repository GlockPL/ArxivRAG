{"title": "Average-Reward Reinforcement Learning with Entropy Regularization", "authors": ["Jacob Adamczyk", "Volodymyr Makarenko", "Stas Tiomkin", "Rahul V. Kulkarni"], "abstract": "The average-reward formulation of reinforcement learning (RL) has drawn increased interest in recent years due to its ability to solve temporally-extended problems without discounting. Independently, RL algorithms have benefited from entropy-regularization: an approach used to make the optimal policy stochastic, thereby more robust to noise. Despite the distinct benefits of the two approaches, the combination of entropy regularization with an average-reward objective is not well-studied in the literature and there has been limited development of algorithms for this setting. To address this gap in the field, we develop algorithms for solving entropy-regularized average-reward RL problems with function approximation. We experimentally validate our method, comparing it with existing algorithms on standard benchmarks for RL.", "sections": [{"title": "Introduction", "content": "A successful reinforcement learning (RL) agent learns from interacting with its surroundings to achieve desired behaviors, as encoded in a reward function. Thus, in \"continuing\" tasks, where the amount of interactions is potentially unlimited, the total sum of rewards received by the agent is unbounded. To avoid this divergence, the most popular technique is to discount future rewards relative to current rewards. The framework of discounted RL enjoys convergence properties (Sutton and Barto 2018; Kakade 2003; Bertsekas 2012), practical benefits (Schulman et al. 2016; Andrychowicz et al. 2020), and a plethora of useful algorithms (Mnih et al. 2015; Schulman et al. 2015, 2017; Hessel et al. 2018; Haarnoja et al. 2018b) making the discounted objective an obvious choice for the RL practitioner. Despite these benefits, the use of discounting introduces a (typically unphysical) hyperparameter y which must be tuned for optimal performance. The difficulty in properly tuning y is illustrated in our motivating example in Figure 1. Furthermore, agents solving the discounted RL problem will fail in optimizing long-term behaviors requiring timescales longer than those allowed by discounting, and it has been argued that the discounted objective is not even a well-defined optimization problem (Naik et al. 2019). Moreover, despite most state-of-the-art algorithms using this discounted framework, their metric for performance is often the total or average reward over trajectories, as opposed to the discounted sum.\nTo address these issues, another criterion for solving continuing tasks has been studied (Schwartz 1993; Mahadevan 1996): the average-reward objective. Although it is arguably a more natural choice, it has less obvious convergence properties. Importantly, there are limited average-reward algorithms beyond the tabular setting. Current algorithms beyond tabular settings use policy-gradient methods to develop actor-based models: (Zhang and Ross 2021; Ma et al. 2021; Saxena et al. 2023). While these advancements represent a positive step for average-reward RL algorithms, there remains a need for alternative approaches to develop the field of average-reward deep RL.\nIn both the discounted and average-reward scenarios, the optimal policy is known to be deterministic (Mahadevan 1996; Sutton and Barto 2018). However, due to observational and control noise, a deterministic policy can fail in im-"}, {"title": "Preliminaries", "content": "In this section, we focus on the development of theory for discrete state-action spaces for clarity. Let \u2206(X) denote the probability simplex over the space X. A Markov Decision Process (MDP) is modeled by a state space S, action space A, reward function  r : S \u00d7 A \u2192 R, transition dynamics p : S \u00d7 A \u2192 \u25b3(S) and initial state distribution \u03bc\u03b5 \u0394(S). The state space describes the set of possible configurations in which the agent (and environment) may exist. (This can be juxtaposed with the \u201cobservation\u201d which encodes the state information accessible to the agent. We will consider fully observable MDPs where state and observation are synonymous.) The action space is the set of controls available to the agent. Enacting control, the agent may alter its state in the environment. This change is dictated by the (generally stochastic) transition dynamics, p.\nAt each discrete timestep, an action is taken and the agent receives a reward r(s, a) \u2208 R from the environment.\nWe will make some of the usual assumptions for average-reward MDPS:\nAssumption 1. The Markov chain induced by any stationary policy is irreducible and aperiodic.\nAssumption 2. The reward function is bounded.\nIn solving an average-reward MDP, one seeks a control policy which maximizes the expected reward-rate, denoted p\u03c0. In the average-reward framework, such an objective reads\n$\\rho^{\\pi} = \\lim_{N \\to \\infty} \\frac{1}{N} E_{\\tau \\sim \\rho,\\pi,\\mu} \\left[ \\sum_{t=0}^{N-1} r(s_t, a_t) \\right],$ (1)\nwhere the expectation is taken over trajectories generated by the dynamics p, control policy \u03c0, and initial state distribution \u03bc.\nThe non-scalar (that is, (s, a)-dependent) contribution to the action-value function is called the average-reward differential bias function. Because of its analogy to the Q function in discounted RL, we follow recent work (Zhang and Ross 2021) and similarly denote it as:\n$Q^{\\pi}(s, a) = E_{\\tau \\sim \\rho,\\pi} \\left[ \\sum_{t=0}^{\\infty} (r(s_t, a_t) - \\rho^{\\pi}) | s_0 = s, a_0 = a \\right],$ (2)\nWe will now introduce a variation of this MDP framework which includes an entropy regularization term. For notational convenience we refer to entropy-regularized average-reward MDPS as ERAR MDPs. The ERAR MDP constitutes the same ingredients as an average-reward MDP stated above, in addition to a pre-specified prior policy2  \u03c00 : S \u2192 \u0394(A) and \u201cinverse temperature\u201d, \u03b2. The modified objective function for an ERAR MDP now includes a regularization term based on the relative entropy (Kullback-Leibler divergence),\nso that the agent now aims to optimize the expected entropy-regularized reward-rate, denoted \u03b8:\n$\\theta = \\lim_{N \\to \\infty} \\frac{1}{N} E_{\\tau \\sim \\rho,\\pi,\\mu} \\left[ \\sum_{t=0}^{N-1} r(s_t, a_t) - \\frac{1}{\\beta} \\log \\frac{\\pi(a_t | s_t)}{\\pi_0(a_t | s_t)} \\right],$ (3)\n$\\pi^*(a | s) = \\underset{\\pi}{\\operatorname{argmax}} \\theta^{\\pi}.$ (4)\nBeyond the mathematical generalization from the MaxEnt formulation, the KL divergence term has also found use in behavior-regularized RL tasks, especially in the offline setting (Wu, Tucker, and Nachum 2019; Zhang and Tan 2024). Because of Assumption 1, the expression in Equation (3) is independent of the initial state-action. From hereon, we will simply write \u03b8\u03c0\u2217 for the optimal entropy-regularized reward-rate for brevity. Comparing to Equation (1), this rate is seen to include an additional entropic contribution, the relative entropy between the control \u03c0 and prior \u03c00."}, {"title": "Soft Actor-Critic for the Average-Reward Objective", "content": "We begin with a discussion of the extension of soft actor-critic (SAC), for which we derive new theoretical results and provide the corresponding average-reward algorithm. First, we draw inspiration from SAC (Haarnoja et al. 2018b) which relies on iteratively calculating a value (critic) of a policy (actor) and improving the actor through soft policy improvement (PI). In the discounted problem formulation, soft PI states that a Boltzmann-form for the policy (\u03c0\u2032) derived from the value function of a previous policy (\u03c0) such that: \u03c0\u2032 \u221d exp \u03b2Q\u03c0(s, a), is guaranteed to outperform the previous policy in the Q-value sense:  Q\u03c0\u2032(s, a) > Q\u03c0(s, a) for all s, a (cf. Lemma 2 of (Haarnoja et al. 2018b) for details). We will first show that an analogous result for policy improvement holds in the ERAR setting.\nSince the value of a policy is now encoded in the entropy-regularized average reward rate \u03b8\u03c0 and not in the differential value, the analogue to policy improvement is to establish the bound \u03b8\u03c0\u2032 > \u03b8\u03c0 for some construction of \u03c0\u2032 from \u03c0. Indeed, as we show, the same Boltzmann form over the differential value leads to soft PI in the ERAR objective. After establishing PI and the related theory in this setting we present our algorithm, denoted \u201cASAC\u201d (for average-reward SAC, and following the naming convention of APO (Ma et al. 2021) and ATRPO (Zhang and Ross 2021)).\nAs in the discounted case, it can be shown that the Q function for a fixed policy \u03c0 satisfies a recursive Bellman backup equation4:\nProposition 1 ((Wu, Ke, and Wu 2024)). Let an ERAR MDP be given with reward function r(s, a), fixed evaluation policy \u03c0 and prior policy \u03c00. Then the differential value of \u03c0, Q\u03c0(s, a), satisfies\n$Q^{\\pi}(s, a) = r(s, a) - \\theta^{\\pi} + E_{s' \\sim p} V_g^{\\pi}(s'),$ (6)\nwith the entropy-regularized definition of state-value function\n$V_g^{\\pi}(s) = \\frac{1}{\\beta} \\log E_{a \\sim \\pi} \\frac{\\pi(a | s)}{\\pi_0(a | s)}.$ (7)\nIn the average reward formulation, the metric of interest is the reward-rate (\u03b8\u03c0). Our policy improvement result therefore focuses on increases in \u03b8\u03c0, generalizing the recent work of (Zhang and Ross 2021) to the current setting. We find that the gap between any two entropy-regularized reward-rates can be expressed in the following manner."}, {"title": "Soft Q-Learning for the Average-Reward Objective", "content": "In this section, we present an extension of the SQL algorithm to the average-reward setting, which we denote ASQL. For simplicity we focus on the case of discrete actions, in which case the necessary action integrals can be calculated exactly.\nIn soft Q-learning (Haarnoja et al. 2017), one refines estimates of the optimal (soft) Q function by iterating the Bellman optimality operator which is defined as  T (Q) = r(s, a) + \u03b3\u03b2\u22121 Es\u2032 log Ea\u2032 exp \u03b2 (\u00b7). Since \u03b3 \u2208 (0, 1), this operator is a contraction, and hence converges to its (unique) fixed point, the optimal Q-function: T\u221e(Q0) = T(Q\u2217) = Q\u2217. In the ASQL case, the lack of discount factor poses a challenge in terms of ensuring convergence. However, for the case of deterministic dynamics, it can be shown by mapping to exponential space that the corresponding undiscounted equation (Equation (13)) is simply an eigenvector equation (Arriojas et al. 2023b), which has a unique non-trivial solution corresponding to the entropy-regularized average-reward rate (eigenvalue) and differential Q function (eigenvector). Additional details about this connection are provided in the Appendix.\nFurthermore, we obtain the optimality equation for the entropy-regularized differential value, allowing us to solve the ERAR MDP through Q-learning iterations:\nLemma 3. The optimal ERAR Q-function satisfies the following Bellman equation:\n$Q(s, a) = r(s, a) - \\theta + \\frac{1}{\\beta} E_{s'\\sim p} \\log E_{a' \\sim \\pi_0} e^{\\beta Q(s', a')},$ (13)\nwhere \u03b8 is the maximum ERAR rate, \u03b8 = max\u03c0 \u03b8\u03c0 (with \u03b8\u03c0 given in Equation (3).\nThe proof follows from the limit of policy improvement (given in Appendix A). In accordance with SQL's temporal difference update, we propose to iterate Equation (13) until convergence.\nTo implement this procedure in the spirit of SQL, we use a mean squared error loss between an online network and corresponding estimate of Equation (13) calculated through a target network,\n$\\mathcal{L}(\\psi) = \\frac{1}{2} E_{s,a,r,s' \\sim D} (Q_{\\psi}(s, a) - \\hat{y})^2,$ (14)\nwhere the target is\n$\\hat{y} = r - \\theta + \\beta^{-1} \\log E_{a' \\sim \\pi_0} e^{\\beta Q_{\\hat{\\psi}}(s', a')},$ (15)\nand where D denotes a replay buffer based on past experience. Note that the temporal difference (TD) target is calculated from a target network (lagging weights relative to the online network), denoted \u03c8\u02c6. Appendix B contains further implementation details. The algorithm's pseudocode is shown in Algorithm 1, with the main differences from SQL highlighted in red. The value of \u03b8 must be updated online as well, and we use the same Equation (12) as discussed in the previous section to update its value. The value of \u03b8 is updated after averaging its new value over the \u201cgradient steps\u201d loop in Algorithm 1.\nThe algorithm implements three key components present in many value-based deep RL methods: (1) an estimate of the value function parameterized by two deep neural nets (inspired by (Van Hasselt, Guez, and Silver 2016)),"}, {"title": "Discussion", "content": "The motivation for developing novel algorithms for average rewards RL arises from the problems generally associated with discounting. When the RL problem is posed in the discounted framework, a discount factor y is a required input parameter. However, there is often no principled approach for choosing the value of y corresponding to the specific problem being addressed. Thus, the experimenter must treat y as a hyperparameter. This reduces the choice of y to a trade-off between large values to capture long-term rewards and small values to capture computational efficiency which typically scales polynomially with the horizon, (1 \u2013 \u03b3)\u22121 (Kakade 2003). The horizon introduces a natural timescale to the problem, but this timescale may not be well-aligned with another timescale corresponding to the optimal dynamics: the mixing time of the induced Markov chain. For the discounted approach to accurately estimate the optimal policy, the discounting timescale (horizon) must be larger than the mixing time; however the estimation of the mixing time for the optimal dynamics can be challenging to obtain in the general case, even when the transition dynamics are known. Therefore, an arbitrary \"sufficiently large\" choice of y is often made without knowledge of the relevant problem-dependent timescale. This can be problematic from a computational standpoint as evidenced by recent work (Jiang et al. 2015; Schulman et al. 2017; Andrychowicz et al. 2020). These points are illustrated in Figure 1 which shows the performance of SAC for the Swimmer-v4 environment for different choices of y. For the widely used choice \u03b3 = 0.99 the evaluation rewards are low relative to the optimal case, whereas the average rewards algorithms perform well (cf. Appendix), highlighting the benefits of using the average-rewards framework.\nIn this work, we have developed a framework for combining the benefits of the average rewards approach with entropy regularization. In particular, we have focused on extensions of the discounted algorithms SAC and SQL to the average rewards domain. By leveraging the connection of the ERAR objective to the soft discounted framework, we have presented the first solution to ERAR MDPs in continuous state and action spaces by use of function approximation. Our experiments suggest that ASQL and ASAC compare favorably in several respects to their discounted counterparts. Our algorithm leverages existing codebases allowing for a straightforward and easily extendable implementation for solving the ERAR objective."}, {"title": "Future Work", "content": "The current work suggests multiple extensions which we plan to explore in future research. Following the line of work in (Arriojas et al. 2023b), recent work by the same authors shows a method for the more general case of stochastic transition dynamics, by iteratively learning biases for the dynamics and rewards (Arriojas et al. 2023a). With a model-based algorithm, this seems to be a promising avenue for future exploration for an alternative approach to ASQL in stochastic environments. As a value-based technique, other ideas from the literature such as TD(n), REDQ (Chen et al. 2021), DrQ (Kostrikov, Yarats, and Fergus 2020), combating estimation bias (Hussing et al. 2024), or dueling architectures (Wang et al. 2016) may be included. From the perspective of sampling, the calculation of \u03b8 can likely benefit from more complex replay sampling, e.g. PER (Schaul et al. 2015). An important contribution for future work is studying the sample complexity and convergence properties of the proposed algorithms. We believe that the average-reward objective with entropy regularization is a fruitful direction for further research and real-world application, with this work addressing a gap in the existing literature."}, {"title": "A Proofs", "content": "Lemma 1 (ERAR Backup Equation). Let an ERAR MDP be given with reward function r(s, a), fixed evaluation policy \ud835\udf0b and prior policy \ud835\udf0b0. Then the differential value of \ud835\udf0b, \ud835\udc44\ud835\udf0b(\ud835\udc60, \ud835\udc4e), satisfies\n$Q(s, a) = r(s, a) - \\theta^{\\pi} + E_{s'\\sim p} V_g^{\\pi}(s'),$ (16)\nwith the entropy-regularized definition of state-value function\n$V_g^{\\pi}(s) = \\frac{1}{\\beta} \\operatorname{Bax} \\left[ Q^{\\pi}(s, a) - \\log \\frac{\\pi(a | s)}{\\pi_0(a | s)} \\right].$ (17)\nProof. We begin with the definitions\n$Q(s, a) = r(s, a) - \\theta^{\\pi} + E\\left[ \\sum_{t=1}^{\\infty} (r(s_t, a_t) - \\theta^{\\pi}) \\right],$ (18)\nfor the current state-action and\n$Q(s', a') = r(s', a') - \\theta^{\\pi} + E\\left[ \\sum_{t=2}^{\\infty} (r(s_t, a_t) - \\theta^{\\pi}) | s_t = s', a_t = a' \\right],$ (19)\nfor the next state-action.\nRe-writing \ud835\udc44(\ud835\udc60, \ud835\udc4e) and highlighting the terms of \ud835\udc44(\ud835\udc60\u2032, \ud835\udc4e\u2032) in blue,\n$Q(s, a) = r(s, a) - \\theta^{\\pi} + E\\left[ (r(s_1, a_1) - \\theta^{\\pi}) + E_{s'\\sim p} \\left[ \\sum_{t=2}^{\\infty} (r(s_t, a_t) - \\theta^{\\pi}) | s_t = s', a_t = a' \\right] \\right],$ (20)\n$Q(s, a) = r(s, a) - \\theta^{\\pi} + E_{s'\\sim p} [r(s', a') - \\theta^{\\pi} + Q^{\\pi}(s', a')],$ (21)\nand identifying the entropy-regularized state value function as  \ud835\udc49\ud835\udf0b(\ud835\udc60) = Ex\u223c\ud835\udf0b [\ud835\udc44(\ud835\udc60, \ud835\udc4e) \u2212  log \nLemma 1 (ERAR Rate Gap). Consider two policies \ud835\udf0b, \ud835\udf0b\u2032 absolutely continuous w.r.t. \ud835\udf0b0. Then the gap between their corresponding entropy-regularized reward rates is:\n$\\theta^{\\pi'} - \\theta^{\\pi} = E_{s\\sim d_\\pi, d_\\pi'} \\left( A(s, a) - \\frac{1}{\\beta} \\log \\frac{\\pi'(a | s)}{\\pi_0(a | s)} \\right),$ (22)\nwhere A\u03c0(s, a) = Q(s, a) \u2212 V\u03c0(s) is the advantage function of policy \ud835\udf0b and \ud835\udc51 is the steady-state distribution induced by\n\ud835\udf0b\u2032.\nProof. Working from the right-hand side of the equation,\n$E_{s\\sim d_\\pi, d_\\pi'} \\left( A(s, a) - \\frac{1}{\\beta} \\log \\frac{\\pi'(a | s)}{\\pi_0(a | s)} \\right) = E_{s\\sim d_\\pi, d_\\pi'} \\left( (Q^{\\pi}(s, a) - V(s) \\frac{1}{\\beta} \\log \\frac{\\pi'(a | s)}{\\pi_0(a | s)} \\right),$\n$= E_{s\\sim d_\\pi, d_\\pi'} \\left( E_{s'\\sim p}V_g^{\\pi}(s') - V_g^{\\pi}(s) - \\frac{1}{\\beta} \\log \\frac{\\pi'(a | s)}{\\pi_0(a | s)} \\right)$,\n$= \\theta^{\\pi'} - \\theta^{\\pi} - E_{s\\sim d_\\pi, d_\\pi'} E_{s'\\sim p} V_g^{\\pi}(s') - V_g^{\\pi}(s)]$,\n$= \\theta^{\\pi'} - \\theta^{\\pi}$. (23)\nwhere we have used the definition\n$\\theta^{\\pi'} = E_{s\\sim d_\\pi, d_\\pi'} \\left( r(s, a) - \\frac{1}{\\beta} \\log \\frac{\\pi'(a | s)}{\\pi_0(a | s)} \\right),$ (24)\nand\n$E_{s'\\sim p} E_{s\\sim d_\\pi, d_\\pi'} V_g^{\\pi}(s') = E_{s\\sim d_\\pi, d_\\pi'} V_g^{\\pi}(s),$ (25)\nwhich follows given that \ud835\udc51 is the stationary distribution. In other words, \ud835\udc51 is an eigenvector of the transition operator p(s\u2032|s, a) \u00b7 \ud835\udf0b\u2032(a\u2032|s\u2032)."}, {"title": "A Soft Q-Learning Proofs", "content": "Lemma 4. The optimal ERAR Q function satisfies\n$Q(s,a) = r(s, a) - \\theta + \\frac{1}{\\beta} E_{s'\\sim p} \\log E_{a' \\sim \\pi_0} e^{\\beta Q(s',a')},$ (28)\nProof. Policy iteration monotonically improves the ERAR rate until \ud835\udf03\u2032 = 0 at convergence. So by the case of equality seen in the previous proof, we have a relation between the optimal state and action value functions:\n$V(s) = \\frac{1}{\\beta} \\log E_{a \\sim \\pi_0} e^{\\beta Q(s,a)}$ (29)\nfrom which the desired result follows immediately via Lemma 1."}, {"title": "A Eigenvector Equation", "content": "This update equation is in fact the same (in exponential space) as derived by Arriojas et al. (2023b) for deterministic dynamics. Since in the exponential space, this is an eigenvalue equation for a primitive matrix, the unique solution (eigenvector) exists and can be obtained e.g. by the power method. The eigenvector given by (Arriojas et al. 2023b) is\n$u(s,a) = e^{\\beta (r(s,a)-\\theta)} \\sum_{s'\\sim p, a' \\sim \\pi_0} u(s', a'),$ (30)\nwhich in the case of deterministic dynamics can be written in log-space as:\n$\\log u(s, a) = \\beta (r(s, a) - \\theta) + \\sum_{s'} E_{s' \\sim p} \\log \\sum_{a' \\sim \\pi_0} u(s', a'),$ (31)\nUpon dividing by \ud835\udefd and defining \ud835\udc5e = \ud835\udefd\u22121 log \ud835\udc62:\n$q(s,a) = r(s,a) - \\theta + \\frac{1}{\\beta} \\sum_{s'} E_{s' \\sim p} \\log \\sum_{a' \\sim \\pi_0} e^{\\beta q(s',a')},$ (32)\nwhich is identical to our Equation (13).\nFor the case of stochastic dynamics, (Arriojas et al. 2023a) has shown that the average-reward RL problem can be mapped onto another eigenvector equation with a different (learnable) choice of dynamics and reward function. So for both cases (i.e. for deterministic dynamics and for stochastic dynamics) the update equation can be mapped on to an eigenvalue equation (in exponential space)."}, {"title": "B Implementation Details", "content": null}]}