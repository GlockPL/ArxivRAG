{"title": "From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning", "authors": ["Chao Feng", "Yuanzhe Gao", "Alberto Huertas Celdr\u00e1n", "G\u00e9r\u00f4me Bovet", "Burkhard Stiller"], "abstract": "Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange. However, model training inevitably leaves exploitable traces that can be used to infer sensitive information. In Decentralized FL (DFL), the overlay topology significantly influences its models' convergence, robustness, and security. This study explores the feasibility of inferring the overlay topology of DFL systems based solely on model behavior, introducing a novel Topology Inference Attack. A taxonomy of topology inference attacks is proposed, categorizing them by the attacker's capabilities and knowledge. Practical attack strategies are developed for different scenarios, and quantitative experiments are conducted to identify key factors influencing the attack effectiveness. Experimental results demonstrate that analyzing only the public models of individual nodes can accurately infer the DFL topology, underscoring the risk of sensitive information leakage in DFL systems. This finding offers valuable insights for improving privacy preservation in decentralized learning environments.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has emerged as a novel framework for enabling privacy-preserving Machine Learning (ML), facilitating collaborative model training among distributed clients without the necessity of sharing raw data [McMahan et al., 2017]. Conventional FL systems are predicated on a centralized architecture, referred to as Centralized FL (CFL), wherein a central entity is responsible for collecting, aggregating, and redistributing models to the clients. Nevertheless, this centralized architecture presents several challenges, such as processing bottlenecks and the single points of failure risk [Beltr\u00e1n et al., 2023]. In response to these limitations, Decentralized FL (DFL) has been introduced, wherein model training and aggregation occur locally, and communication between nodes relies on P2P networks peer-to-peer (P2P) network. This approach eliminates dependency on a central server, mitigating risks associated with single points of failure [Beltr\u00e1n et al., 2024].\nAlthough FL, both centralized and decentralized, protect raw data privacy through its unique model-sharing mechanism, the training process inevitably leaves traces in the models. These traces may be susceptible to exploitation by malicious actors, potentially leading to inference attacks, including membership inference attacks, property inference attacks, and attribute inference attacks [Hu et al., 2022]. Previous studies have illustrated that such attacks pose a considerable risk to privacy within FL systems by elucidating sensitive information based on the behavior of the models [Dayal et al., 2023]. While much of the existing work focuses on the CFL, a notable research gap exists in exploring information leakage risks associated with DFL.\nOverlay network topology defines how participants are interconnected in an FL system. In CFL, the client-server architecture enforces a fixed, star-shaped topology. However, DFL leverages the P2P network, enabling flexible node connections to form diverse topologies. Existing studies demonstrate that topology significantly influences DFL models' robustness and privacy-preserving capabilities [Feng et al., 2024]. Thus, topology should be considered sensitive data and a critical asset within DFL systems. However, theoretical analyses on how network topology affects model convergence remain sparse. Moreover, there is limited research investigating potential information leakage risks for this sensitive overlay topology data and strategies to safeguard this critical asset.\nThis paper addresses this research gap by proposing a novel overlay topology inference attack targeting DFL systems. The proposed attack leverages the behavioral traces generated by models to uncover sensitive overlay topologies. The main contributions of this work are as follows: (i) the formulation of a taxonomy of topology inference attacks, classifying these attacks based on the attacker's capabilities and knowledge; (ii) the development of practical strategies tailored to various attack types, accompanied by a quantitative analysis of critical factors that influence the efficiency of these attacks; (iii) rigorous experimental evaluations conducted across a range of datasets and topology configurations to empirically validate the proposed attack strategies; and (iv) the provision of insights aimed at informing the design of effective defensive mechanisms to safeguard sensitive information and preserve privacy within DFL systems."}, {"title": "2 Background and Related Work", "content": "This section overviews inference attacks, encompassing their classifications and potential attack surfaces. Since there is a lack of research directly related to topology inference attacks on DFL, this paper examines topology inference studies conducted in other domains, including communication systems and social networks."}, {"title": "2.1 Inference Attacks", "content": "Inference attacks exploit ML models to extract sensitive information without direct access to the underlying data [Salem et al., 2018]. By analyzing model behaviors or outputs, adversaries can infer properties of the data, model parameters, or system structure. As FL grows in adoption for its privacy-preserving capabilities, it also faces increased vulnerability to such attacks [Yin et al., 2021]. Inference attacks vary in objectives and strategies, with common types including:\n\u2022 Membership Inference Attack: Determines whether a specific data sample was part of the training dataset, often using shadow models or prediction confidence scores to differentiate between training and unseen data [Shokri et al., 2017]. This is critical in sensitive domains like healthcare or finance.\n\u2022 Model Inversion Attack: Reconstructs input data or infers its properties by iteratively optimizing inputs to match observed model outputs, posing risks in applications involving sensitive information like facial recognition or medical data [Fredrikson et al., 2015].\n\u2022 Property Inference Attack: Infers characteristics of the training data, such as demographic distributions, by analyzing model updates or outputs. In FL, this can involve monitoring node-specific behaviors to deduce shared properties of local datasets [Ganju et al., 2018].\n\u2022 Attribute Inference Attack: Targets specific attributes of training data, often by injecting malicious samples to influence the model's learning process and make it vulnerable to data leakage [Gong and Liu, 2018].\nThese attacks not only compromise users' sensitive information and undermine data security but also erode trust in FL's privacy protection mechanisms. As a result, users may hesitate to participate in the training process, undermining the overall effectiveness of the FL system.\nHowever, existing research on inference attacks in FL primarily focuses on CFL, with limited exploration of privacy leakage in DFL. Moreover, the critical role of DFL's overlay topology has received insufficient attention in these studies."}, {"title": "2.2 Topology Inference", "content": "Topology inference is explored in various research domains, such as identifying source-destination paths in communication systems and uncovering interconnections among nodes in social networks.\nCommunication Systems\nIn communication systems, most protocols are designed to avoid ring formation, resulting in tree-like overlay topologies. Moreover, many topology inference studies commonly assume a single path between nodes [Castro et al., 2004]. A widely used approach involves leveraging ICMP-based protocols, such as the traceroute, to map network topologies effectively [Jin et al., 2006]. Tomography-based metrics, like packet loss rates, further aid in deducing a system's topology: by observing loss rates from a source node to a target node, one can estimate the likelihood of a direct connection and thereby reconstruct the network [Coates et al., 2002]. Additionally, by measuring the timestamps of sent and received packets, it is possible to determine the number of forwarding hops, offering deeper insight into the overall network configuration [Hou et al., 2020].\nSocial Networks\nSocial networks generally refer to the social structure formed by individuals in society connected by a specific relationship. In social networks, user interactions (e.g., likes, retweets) serve as valuable signals for topology modeling and inference by capturing how information diffuses across the network [Gomez-Rodriguez et al., 2012]. However, this behavioral data is often incomplete, especially in large-scale settings, necessitating methods like noisy sparse subspace clustering to infer network structures from partial observations [Wang et al., 2016]. Meanwhile, social networks are dynamic and time-sensitive, and they frequently exhibit small-world properties.[Gong et al., 2012] incorporate these distributional patterns as prior knowledge in evolutionary models to better capture network changes over time.\nCommunication networks that often assume tree topologies and social networks are typically modeled as small-world networks; however, these assumptions do not hold for DFL network topologies. Moreover, common inference tools and metrics, such as traceroute, packet loss rate, or user interaction data, are not readily applicable in DFL contexts, creating the need for specialized inference metrics and strategies tailored to the particularities of DFL systems."}, {"title": "3 Overlay Topology in DFL", "content": "The overlay topology in DFL significantly affects model convergence. The overlay topology is modeled as an undirected graph G = (V, E), where V is the set of nodes and E represents the edges indicating direct communication links between nodes. The adjacency matrix A encodes the graph structure, where:\n$$A_{ij} =\n\\begin{cases}\n1 & \\text{if nodes } i \\text{ and } j \\text{ are connected,}\\\\\n0 & \\text{otherwise.}\n\\end{cases}$$\n(1)\nThe degree matrix D is a diagonal matrix where $D_{ii}$ = degree(i) + 1. The aggregation process is governed by the normalized aggregation matrix:\n$$P = D^{-1}(A + I),$$\n(2)\nwhere I is the identity matrix. This formulation ensures P is row-stochastic ($\\sum_{j} P_{ij} = 1$), enabling consistent scaling during aggregation.\nDFL proceeds iteratively over T rounds, comprising local training and model aggregation steps. Let $M^{t} = [\\theta_{1}^{t}, \\theta_{2}^{t},....\\theta_{|N|}^{t}]$ represent the models of all nodes at round t, where $\\theta_{i}^{t}$ denotes the parameters of node i."}, {"title": "4 Problem Statement", "content": "Theoretical analysis of the DFL learning process reveals that overlay topology is crucial in determining system performance, robustness, and privacy. This section introduces the problem of topology inference and categorizes potential attack types based on the attacker's knowledge and capabilities.\nConsider a DFL system represented by an undirected graph G = (V, E), where V is the set of participating nodes, and E is the set of edges representing direct communication links between nodes. The adjacency matrix A of size |V| x |V| encodes the network topology, as defined in Equation (1).\nAn attacker aims to infer a predicted adjacency matrix A' that approximates the ground truth A as closely as possible, thereby revealing sensitive topology information.\nThe effectiveness of topology inference attacks depends on the attacker's knowledge and capabilities. The following assumptions are made. (i) Internal Adversary: The attacker is an internal participant in the DFL system and can identify a subset of nodes. (ii) Decoupled Information: Node-level information (e.g., models and datasets) and network-level information (e.g., connections) are treated as separate assets. For instance, an attacker may know a node's model but not its connections.\nLet V' \u2286 V represent a set of nodes known to the attacker and E' \u2286 E represent the subset of edges known to the attacker, where \u00d8 \u2286 E' \u2286 E to represent partial knowledge. The attacker's goal is to infer the entire edge set E based on partial knowledge of V' and E'.\nFor each node i \u2208 V, the local model is represented as Mi, and the local dataset is represented as Di. This paper assumes that models and data within a node are distinct assets. Thus, the attacker's capabilities are defined as follows:\n\u2022 Model Knowledge: The attacker can access models of known nodes V', forming the set: M' = {Mi | i \u2208 V'}.\n\u2022 Dataset Knowledge: The attacker may not have access to the datasets Di of these nodes. The datasets known to the attacker are denoted by: D' \u2286 {Di | i \u2208 V'} or D' = \u00d8.\nBased on the attacker's knowledge and capabilities, the topology inference attack is classified into five scenarios:\n\u2022 Scenario 1: The attacker knows all node models and all the datasets, and only partial edge information (M' = M, D' = D, \u00d8 \u2286 E' \u2286 E).\n\u2022 Scenario 2: The attacker knows all node models and none of the datasets, and only partial edge information (M' = M, D' = \u00d8, \u00d8 \u2286 E' \u2286 E).\n\u2022 Scenario 3: The attacker knows all node models and all the datasets, but no edge information (M' = M, D' = D, E' = \u00d8).\n\u2022 Scenario 4: The attacker knows all node models and none of the datasets, but no edge information (M' = M, D' = \u00d8, E' = \u00d8).\n\u2022 Scenario 5: The attacker knows partial models and none of the datasets, but has no knowledge of the edges (\u00d8 \u2282 M' \u2282 M, D' = \u00d8, E' = \u00d8).\nFor Scenario 1, since the attacker can control all nodes and knows partial information about the edges, it can be classified as a white-box attack. Scenarios 2, 3, and 4 can be categorized as gray-box attacks. For attack scenario 5, no information is available regarding the total number of nodes or edges, making it infeasible to fully reconstruct the network topology with the approaches considered. As a result, this work only considers the attack scenarios 1, 2, 3, and 4."}, {"title": "5 Topology Inference Attack", "content": "This section first explores the metrics that attackers can exploit to carry out topology inference attacks, and then designs attack strategies for various attack scenarios."}, {"title": "5.1 Attack Metrics", "content": "In a DFL system, attackers may leverage the local data and local models available at each known node. Additionally, DFL's decentralized communication mechanism allows attackers to access models transmitted from neighboring nodes. Thus, this paper proposes four attack metrics that attackers can exploit: relative loss, relative entropy, cosine similarity, and Euclidean similarity.\n\u2022 Relative Loss. This metric evaluates how well a model trained on one node's data generalizes to another node's dataset. Consider a model fi trained on node i's dataset Di, and let Dj be the dataset of another node j. The relative loss Relative Lossi,j measures the performance of fi on Dj. A lower relative loss indicates better transfer-ability and generalization across nodes:\n$$Relative \\text{Loss}_{i,j} = L(f_i, D_j)$$\n(8)\nwhere L(f, D) is the loss function used by the model f on dataset D.\n\u2022 Relative Entropy. This metric measures the uncertainty of fi's predictions on Dj. While relative loss focuses on correctness, relative entropy focuses on confidence. A model may be confident (low entropy) but still perform poorly if it is incorrect.\n$$Relative \\text{Entropy}_{i,j} = \\frac{1}{|D_j|} \\sum_{x \\in D_j} \\sum_k Y_k(x) \\log(f_{i,k}(x))$$\n(9)\nwhere $y_k(x)$ is the true label distribution for sample x and $f_{i,k}(x)$ is the predicted probability of class k under model fi.\n\u2022 Cosine Similarity. This metric compares the direction of two parameter vectors a and b derived from different models. If both vectors point in similar directions, their cosine similarity is high. In DFL, a high cosine similarity suggests that the two nodes frequently aggregate their parameters, resulting in more closely aligned models:\n$$Cosine \\text{Similarity}(a, b) = \\frac{a \\cdot b}{||a|| ||b||}$$\n(10)\nwhere a \u00b7 b is the dot product, and ||a||, ||b|| are the L2 norms of the vectors a and b, respectively.\n\u2022 Euclidean Similarity. It quantifies how similar two vectors are based on the Euclidean distance between them. A higher Euclidean similarity indicates that the two models are numerically closer, while a smaller distance suggests they differ significantly:\n$$Euclidean \\text{Similarity}(a, b) = \\frac{1}{1 + \\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^2}}$$\n(11)\nwhere ai and bi are the i-th elements of a and b.\nRelative loss and entropy require the attacker to use both the local model and the local dataset, which are suitable for attack scenarios 1 and 3. In contrast, cosine similarity and Euclidean similarity depend solely on the models and do not require access to the datasets, which are suitable for attack scenarios 2 and 4."}, {"title": "5.2 Attack Strategies", "content": "The attacker's objective in a topology inference attack is to construct a predicted adjacency matrix A' of size |V| x |V| that closely approximates the ground-truth matrix. This can be viewed as a binary classification problem, determining whether each element Aij in A' equals one or zero.\nSupervised Attack Strategy: In Attack Scenarios 1 and 2, since the attacker has knowledge of the true labels for some edges, a supervised classification approach is employed in this work to perform the attack. Given partial edges and their labels, these can be leveraged to train the attack model M. After training, M predicts the unknown edges to generate the predicted adjacency matrix A', as shown in Algorithm 1.\nUnsupervised Attack Strategy: For scenarios 3 and 4, where true labels are unavailable, this paper proposes an attack strategy based on unsupervised clustering, as shown in Algorithm 2. Since edges are known to fall into two categories, edge and non-edge group, the clustering algorithm M is configured for two clusters (K = 2). The attack metrics X are then clustered by M, producing a predicted adjacency matrix A'.\nThese attack algorithms enable the implementation of diverse strategies, tailored to varying levels of attacker knowledge and capabilities, to effectively infer the topology of the DFL."}, {"title": "6 Evaluation", "content": "This work empirically validates the effectiveness of the proposed topology inference attack. First, it identifies which metrics yield stronger attack performance. Building on these results, it then thoroughly evaluates the attack strategy under various conditions, including different datasets, topologies, node counts, and data preprocessing methods. This comprehensive assessment demonstrates the robustness and applicability of the proposed topology inference attack."}, {"title": "6.1 Experimental Setups", "content": "This section describes the configurations employed in the experiments.\nA. Dataset and Model: All experiments are conducted on three datasets: MNIST, Fashion-MNIST, and CIFAR10, widely used in ML and inference attack benchmarking.\n\u2022 MNIST [LeCun and Cortes, 2010]: Contains 60,000 grayscale training images and 10,000 test images of handwritten digits (0-9) at 28\u00d728 resolution. A two-hidden-layer MLP (256 and 128 neurons) trained with Adam (lr=1e-3) is used.\n\u2022 FMNIST [Xiao et al., 2017]: Similar size and format to MNIST, but with 10 categories of clothing items. A CNN with two convolutional layers (32 and 64 filters, kernel size 3\u00d73), ReLU, and max-pooling is employed.\n\u2022 CIFAR10 [Krizhevsky et al., 2009]: Contains 60,000 32\u00d732 RGB images from 10 categories. The training set is only treated with basic image normalization. The image pixel values are normalized to keep the original image content unchanged. A MobileNet [Sandler et al., 2019] is used for the model training.\nB. Topology Setting: All experiments employ a DFL network that includes five topologies: a ring, a star, and three Erd\u0151s-R\u00e9nyi (ER) graphs with probabilities p = 0.3, p = 0.5, and p = 0.7 [Erd\u0151s et al., 2012]. These configurations cover a spectrum of network densities, ranging from sparse to dense.\nC. Federation Setting: All experiments employ the following DFL configurations:\n\u2022 Number of Nodes: The network size is set to 10, 20, or 30 nodes, with training data randomly assigned to each node in an Independent and Identically Distributed (IID) manner, ensuring no overlap between nodes.\n\u2022 Total Rounds: The total number of rounds is 10, 30, and 40 for networks with 10, 20, and 30 nodes, respectively.\n\u2022 Local Epochs: Two scenarios (3 and 10 local epochs) are considered to evaluate how varying levels of local overfitting affect the attack's effectiveness.\nD. Attack Models: The topology inference attack models are categorized by the attack strategy. For the supervised attack strategy, the models used are Logistic Regression [Wright, 1995], Support Vector Machine (SVM) [Hearst et al., 1998], and Random Forest (RF) [Breiman, 2001]. For the unsupervised attack strategy, the models include K-Means [Lloyd, 1982], Gaussian Mixture Model (GMM) [Reynolds and others, 2009], and Spectral Clustering [Von Luxburg, 2007].\nE. Evaluation Metric: Since the topology inference attack is formulated as a binary classification problem, the F1-Score set is used as the evaluation metric."}, {"title": "6.2 Selection of Attack Metrics", "content": "Selecting appropriate attack metrics is critical to the success of inference attacks. An effective attack metric should distinguish between connected and non-connected nodes as clearly as possible. To this end, the first part of this experiment examines the effectiveness of the four proposed attack metrics by analyzing their respective distributions. Figure 1 illustrates the distributions of these metrics for a 10-node ring topology DFL system trained on the MNIST dataset, comparing directly connected nodes (the edge group) with those not (the non-edge group). To facilitate direct comparison, all values have been normalized. The results indicate that the relative loss for connected nodes is notably lower than that for non-connected nodes, with minimal overlap between the respective distributions. By contrast, the relative entropy metric exhibits substantially greater overlap. Since computing the relative loss requires access to local models and data, it has been employed in attack scenarios 1 and 3.\nRegarding similarity-based metrics, the Euclidean similarity between connected models is distinctly higher than that between non-connected models, displaying no overlap between the distributions. The cosine similarity, on the other hand, shows a considerably more significant degree of overlap. Thus, the Euclidean similarity has been adopted as the attack metric in scenarios where only local models are available (attack scenarios 2 and 4)."}, {"title": "6.3 Selection of Attack Models", "content": "Building on the attacker's knowledge and capabilities, the topology inference attack is formulated either as a supervised learning problem (attack scenarios 1 and 2) or an unsupervised learning problem (attack scenarios 3 and 4). Corresponding attack algorithms are developed for each scenario.\nUsing the established attack metrics, experiments are conducted on three datasets under a DFL system with an ER 0.5 topology, aiming to select the effective attack model.\nFigure 2 compares the F1-Score of three supervised algorithms across the three datasets. The results demonstrate that the Logit algorithm consistently achieves strong and stable performance in both attack scenarios and on all three datasets. Therefore, the Logit algorithm is chosen for supervised attacks (attack scenarios 1 and 2).\nFigure 3 presents the F1-Score of three unsupervised algorithms evaluated on the three datasets for attack scenarios 3 and 4. Among these methods, the K-Means algorithm delivers the best and most stable results, making it the algorithm of choice for unsupervised attacks (attack scenarios 3 and 4)."}, {"title": "6.4 Attack Performance", "content": "After establishing the attack metrics and selecting the appropriate attack models, the proposed topology inference attacks were evaluated on three datasets, encompassing four attack scenarios and five types of topologies. The results in Table 1 indicate that the proposed methodology is highly effective. In a 10-node network, all four attack scenarios achieve an F1-Score exceeding 0.8, demonstrating that even with access restricted to public models, there is a high probability of successfully inferring the sensitive topology of a DFL system.\nGenerally, attack scenario 1 yields the highest attack success rate when the number of nodes equals 20 and 30 due to its access to the most information, including local models, data, and partial edge knowledge. Meanwhile, the performance in attack scenario 2 declines when local data are unavailable, underscoring the critical importance of local data for accurate topology inference. Notably, attack scenario 3 also achieved a high attack success rate despite lacking edge information. This finding suggests that, compared to edge knowledge, access to local data contributes more to the successful inference of the topology in DFL. The complete set of experimental results is available in the supplementary materials."}, {"title": "A. Effect of Topology Density", "content": "To investigate the influence of topological characteristics on topology inference attacks, Figure 4 presents the F1-Scores achieved by attack scenario 1 and attack scenario 3 under five different topologies with 20 nodes(complete experimental results are provided in the supplementary materials). Among these, the star topology consistently achieves a high inference success rate. This is because each leaf node is connected solely to the central node in the star topology, thereby amplifying the differences in model parameters between leaf nodes and making the topology particularly easy to infer.\nAdditionally, under attack scenario 1, the inference performance negatively correlates with network density. In denser networks (such as ER 0.7), frequent information exchange between nodes reduces model heterogeneity, limiting the supervised attack model's generalization and inference effectiveness. In contrast, network density does not adversely affect the unsupervised model."}, {"title": "B. Effect of Network Size", "content": "To evaluate the scalability of the proposed topology inference attacks, experiments were extended to networks comprising 20 and 30 nodes. The results, summarized in Table 1, indicate that increasing the number of nodes generally makes the inference task more challenging, leading to a decline in attack performance. Nevertheless, under attack scenarios 1 and 2, F1-Scores consistently remain above 0.8 across all three datasets, demonstrating robust scalability. In contrast, attack scenario 4 exhibits a more pronounced decrease in performance as the network size increases."}, {"title": "7 Mitigation", "content": "The effectiveness of the proposed topology inference attacks indicates that topology in DFL systems can be discerned solely by analyzing model behaviors. This observation highlights the security and privacy concerns of DFL systems. This section proposes defense strategies to mitigate these vulnerabilities within DFL environments."}, {"title": "7.1 Reduce Overfitting", "content": "Similar to other inference attacks, the success of topology inference attacks also relies on the overfitting of the target model. To validate this, an experiment was conducted on a 10-node CIFAR10 network by increasing the number of local training epochs to induce greater overfitting. As shown in Figure 5, raising the local training epochs from 3 to 10 enhances the attack success rate under both attack scenario 1 and attack scenario 3 across all examined topologies. This result indicates that overfitting intensifies the risk of information leakage in DFL systems and suggests that mitigating overfitting can effectively mitigate topology inference attacks.\nIn line with this, data augmentation techniques were employed in the CIFAR10 dataset to mitigate overfitting. Specifically, each training image was randomly cropped by 4 pixels and padded. As depicted in Figure 5, introducing data augmentation substantially decreases the F1-Scores of the attacks across various topologies. Notably, for attack scenario 3 in the ER 0.5 network, the F1-score drops by as much as 50%. These results demonstrate that data augmentation not only enhances model generalization but also mitigates the risk of information leakage."}, {"title": "7.2 Data Heterogeneity", "content": "The distribution of data across nodes significantly influences the DFL model performance. Previous experiments have been conducted under the IID condition, where data is uniformly allocated across nodes. To explore the effects of data heterogeneity, specifically non-IID distributions, a Dirichlet distribution with a parameter of a = 0.1 was utilized to simulate these non-IID conditions. Experiments were carried out using a CIFAR10 dataset distributed across 10 nodes, with the findings illustrated in Figure 5. The results indicate that an increase in data heterogeneity, achieved by setting the non-IID parameter a = 0.1, significantly diminishes the effectiveness of topology inference attacks across all tested attack scenarios and network topologies. This outcome implies that enhancing data heterogeneity may serve as an effective strategy for reducing the risk of sensitive information exposure in DFL systems."}, {"title": "8 Conclusion and Future Work", "content": "This work introduces a novel topology inference attack against DFL, exposing critical vulnerabilities related to privacy and information leakage. By analyzing local models, attackers can accurately infer the overlay topology, one of DFL's most sensitive assets, highlighting the system's susceptibility to privacy breaches. The study explores various attack scenarios and develops tailored metrics, models, and algorithms, with experiments confirming the feasibility and effectiveness of these attacks.\nFurthermore, factors like network size, density, model overfitting, and data heterogeneity significantly influence attack success. Mitigation strategies, such as data augmentation and increasing data heterogeneity, can enhance model generalization and reduce the risk of information leakage.\nThe present work focuses on attack scenarios 1 through 4 and does not yet address the most challenging attack scenario 5. Future research plans include exploring feasible strategies for Scenario 5, as well as extending the evaluations to a wider range of datasets."}]}