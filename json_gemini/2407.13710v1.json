{"title": "OxonFair: A Flexible Toolkit for Algorithmic Fairness", "authors": ["Eoin Delaney", "Sandra Wachter", "Brent Mittelstadt", "Zihao Fu", "Chris Russell"], "abstract": "We present OxonFair, a new open source toolkit for enforcing fairness in binary\nclassification. Compared to existing toolkits: (i) We support NLP and Com-\nputer Vision classification as well as standard tabular problems. (ii) We support\nenforcing fairness on validation data, making us robust to a wide range of over-\nfitting challenges. (iii) Our approach can optimize any measure based on True\nPositives, False Positive, False Negatives, and True Negatives. This makes it\neasily extendable and much more expressive than existing toolkits. It supports\n9/9 and 10/10 of the decision-based group metrics of two popular review pa-\npers. (iv) We jointly optimize a performance objective. This not only minimizes\ndegradation while enforcing fairness, but can improve the performance of oth-\nerwise inadequately tuned unfair baselines. OxonFair is compatible with stan-\ndard ML toolkits including sklearn, Autogluon, and PyTorch and is available at\nhttps://github.com/oxfordinternetinstitute/oxonfair.", "sections": [{"title": "Introduction", "content": "The deployment of machine learning systems that make decisions about people offers an opportunity\nto create systems that work for everyone. However, such systems can lock in existing prejudices.\nLimited data for underrepresented groups can result in ML systems that do not work for them, while\nthe use of training labels based on historical data can result in ML systems copying previous biases.\nAs such, it is unsurprising that AI systems have repeatedly exhibited unwanted biases towards certain\ndemographic groups in a wide range of domains including medicine [1, 2], finance [3, 4], and policing\n[5]. Such groups are typically identified with respect to legally protected attributes, such as ethnicity\nor gender [6, 7, 3]. The field of algorithmic fairness has sprung up in response to these biases.\nContributions to algorithmic fairness can broadly be split into methodological and policy-based\napproaches. While much methodological work focuses on measuring and enforcing (un)fairness, a\ncommon criticism from the policy side is that this work can occur 'in isolation from policy and civil\nsocietal contexts and lacks serious engagement with philosophical, political, legal and economic\ntheories of equality and distributive justice' [8].\nIn response to these criticisms, we have developed OxonFair, a more expressive toolkit for algorithmic\nfairness. We acknowledge that people designing algorithms are not always the right people to decide\non policy, and as such we have chosen to create as flexible a toolkit as possible to allow policymakers\nand data scientists with domain knowledge to identify relevant harms and directly alter the system\nbehaviour to address them. Unlike existing Fairness toolkits such as AIF360 [9], which take a\nmethod-driven approach, and provide access to a wide-range of methods but with limited control over\ntheir behaviour, we take a measure-based approach and provide one fairness method that is extremely\ncustomizable, and can optimize user-provided objectives and group fairness constraints."}, {"title": "Related Work", "content": "Bias mitigation strategies for classification have been broadly categorized into three categories\n[6, 22\u201324]; pre-processing, in-processing and post-processing."}, {"title": "Toolkit interface", "content": "The interface of OxonFair decomposes into three parts: (i) evaluation of fairness and performance\nfor generic classifier outputs. (ii) evaluating and enforcing fairness for particular classifiers. (iii)\nspecialist code for evaluating and enforcing fairness for deep networks."}, {"title": "Inference", "content": "To make decisions, we assign thresholds to groups. We write $f(x)$ for the response of a classifier\nf, on datapoint x, t for the vector corresponding to the ordered set of thresholds, and $G(x)$ for the\none-hot encoding of group membership. We make a positive decision if\n$f(x) - t. G(x) \\geq 0$\n(1)\nTo optimize arbitrary measures we perform a grid search over the choices of threshold, t.\nEfficient grid sampling We make use of a common trick for efficiently computing measures such\nas precision and recall for a range of thresholds. This trick is widely used without discussion for\nefficient computation of the area under ROC curves, and we have had trouble tracking down an\noriginal reference for it. As one example, it is used by scikit-learn [18].\nThe trick is as follows: sort the datapoints by classifier response, then generate a cumulative sum of\nthe number of positive datapoints and the number of negatives, going from greatest response to least.\nWhen picking a threshold between points i and i + 1, TP is given by the cumulative sum of positives\nin the decreasing direction up to and including i; FN is the sum of negatives in the same direction; FP\nis the total sum of positives minus TP, and TN is the total sum of negatives minus TN.\nWe perform this trick per group, and efficiently extract the TP, FN, FP and TN for different thresholds.\nThese are combinatorially combined across the groups and the measures computed. This two stage\ndecoupling offers a substantial speed-up. If we write T for the number of thresholds, k for the\nnumber of groups, and n for the total number of datapoints, our procedure is upper-bounded by\nO(Tk + n log n), while the na\u00efve approach is $O(nTk)$. No other fairness method makes use of this,\nand in particular, all the threshold-based methods offered by AIF360 make use of a na\u00efve grid search.\nFrom the grid sampling, we extract a Pareto frontier with respect to the two measures\u00b2. The thresholds\nthat best optimize the objective while satisfying the constraint are returned as a solution. If no such\nthreshold exists, we return the thresholds closest to satisfying the constraint."}, {"title": "Inferred characteristics", "content": "When using inferred characteristics, we offer two pathways for handling estimated group membership.\nThe first pathway we consider makes a hard assignment of individuals to groups, based on a classifier\nresponse. The second pathway explicitly uses the classifier confidence as part of a per-datapoint\nthreshold. In practice, we find little difference between the two approaches, but the hard assignment\nto groups is substantially more efficient and therefore allows for a finer grid search and generally\nbetter performance. However, the soft assignment remains useful for the integration of our method\nwith neural networks, where we explicitly merge two heads (a classifier and a group predictor) of a\nneural network to arrive at a single fair model. For details of the two pathways see Appendix A."}, {"title": "Fairness for Deep Networks", "content": "We use the method proposed in [21] (N.B., they used it only for demographic parity). Consider a\nnetwork with two heads f, and g, comprised of single linear layers, and trained to optimize two\ntasks on a common backbone B. Let f be a standard classifier trained to maximize some notion of\nperformance such as log-loss and g is a classifier trained to minimize the squared loss\u00b3 with respect to\na vector that is a one hot-encoding of group membership. Any decision $f(x) \u2212 t \u00b7 g(x) \\geq 0$ can now\nbe optimized for given criteria by tuning weights w using the process outlined in the slow pathway.\nAs both f and g are linear layers on top of a common backbone, we can write them as:\n$f(x) = w_f \u00b7 B(x) + b_f, g(x) = w_g \u00b7 B(x) + b_g$\n(2)\nnote that as $f(x)$ is a real number, and $g(x)$ is a vector, $w_f$ is a vector and $b_f$ a real number, while\n$w_g$ is a matrix and $b_g$ a vector."}, {"title": "Toolkit expressiveness", "content": "Out of the box, OxonFair supports all 9 of the decision-based group fairness measures defined by\nVerma and Rudin [59] and all 10 of the fairness measures from Sagemaker Clarify [60]. OxonFair\nsupports any fairness measure (including conditional fairness measures) that can be expressed per\ngroup as a weighted sum of True Positives, False Positives, True Negatives and False Negatives.\nOxonFair does not support notions of individual fairness such as fairness through awareness [61].\nSee Appendix B for a discussion of how metrics are implemented and comparison with two review\npapers. Appendix C contains details of non-standard fairness metrics, including utility optimization\n[58]; minimax fairness [57, 62, 63]; minimum rate constraints [8], and Conditional Demographic\nParity [64]. This also includes a variant of Bias Amplification [65, 66]."}, {"title": "Experimental Analysis", "content": "For tabular data, we compare with all group fairness methods offered by AIF360, and the reductions\napproach of Fairlearn. OxonFair is compatible with any learner with an implementation of the method\npredict_proba consistent with scikit-learn [18] including AutoGluon [67] and XGBoost [19]. A\ncomparison with Fairlearn and the group methods from AIF360 on the adult dataset can be seen in\nfigures 1 and 6 using random forests. This follows the setup of [9]: we enforce fairness with respect\nto race and binarize the attribute to white vs everyone else (this is required to compare with AIF360),\n50% train data, 20% validation, and 30% test, and a minimum leaf size of 20. With this large leaf\nsize, all errors on train, validation, and test are broadly comparable, but our approach of directly\noptimizing an objective and a fairness measure leads us to outperform others.\nFigure 1 top right shows the importance of being able to use a validation set to balance errors. Using\nsklearn's default parameters we overfit to adult, and as the classifier is perfect on the training set,\nall fairness metrics that match error rates are trivially satisfied [68, 17]. The same behavior can be\nobserved using XGBoost on the medical dataset [69] when enforcing equal opportunity with respect\nto sex. In general, tabular methods need not overfit, and tuning parameters carefully can allow users\nto get relatively good performance while maintaining error rates between training and test.\nFigure 3 left shows Equal Opportunity on the COMPAS dataset. To show that OxonFair can also\nwork in low-data regimes where we have insufficient data for validation, we enforce fairness on the\ntraining set. As before, we binarize race to allow the use of AIF360. We drop race from the training\ndata, and use inferred protected attributes to enforce fairness. Here OxonFair generates a frontier that\nis comparable or better than results from existing toolkits, and OxonFair+ (see section A), further\nimproves on these results. See Figure 3 right for a comparison with Fairlearn varying the groups."}, {"title": "Computer Vision and CelebA", "content": "CelebA [75]: We use the standard aligned & cropped partitions frequently used in fairness evaluation\n[17, 21, 28, 41]. Following Ramaswamy et al. [28], we consider the 26 gender-independent, gender-\ndependent and inconsistently labelled attributes as the target attributes for our evaluations (see\nTable 10 for details). Male is treated as the protected attribute.\nImplementation Details We follow Wang et al.'s setup [41]. We use a Resnet-50 backbone [76]\ntrained on ImageNet [77]. A multitask classification model is trained, replacing the final fully-\nconnected layer of the backbone with a separate fully-connected head that performs binary prediction\nfor all attributes. Dropout [78] (p = 0.5) is applied. For all models are trained with a batch size of"}, {"title": "NLP and Toxic Content", "content": "We conducted experiments on hate speech detection and toxicity classification using two datasets: the\nmultilingual Twitter corpus [80] and Jigsaw [81]. Experiments were performed across five languages\n(English, Polish, Spanish, Portuguese, and Italian) and five demographic factors (age, country, gender,\nrace/ethnicity, and religion) were treated as the protected groups. For details, see Appendix E.1.\nWe compare OxonFair with the following approaches. Base reports results of the standard BERT\nmodel [82]. CDA (Counterfactual Data Augmentation) [29, 83\u201386] rebalances a corpus by swapping"}, {"title": "Conclusion", "content": "The key contributions of our toolkit lie in being more expressive than other approaches, and supporting\nNLP and computer vision. Despite this, most of the experiments focus on the standard definitions\nof Demographic Parity and Equal Opportunity. This is not because we agree that they are the right\nmeasures, but because we believe that the best way to show that OxonFair works is to compete with\nother methods in what they do best. On low-dimensional tabular data, when optimizing accuracy and\na standard fairness measure, it is largely comparable with Fairlearn, but if overfitting or non-standard\nperformance criteria and fairness metrics are a concern, then OxonFair has obvious advantages. For\nNLP, and computer vision, our approach clearly improves on existing state-of-the-art. In no small\npart, this is due to the observation of [17], that methods for estimating or enforcing error-based\nfairness metrics on high-capacity models that do not use held-out validation data can not work.\nWe hope that OxonFair will free policy-makers and domain experts to directly specify fairness\nmeasures and objectives that are a better match for the harms that they face. In particular, we want to\ncall out the measures in fig. 8 as relevant to medical ML. The question of how much accuracy can we\nretain, while guaranteeing that test sensitivity (AKA recall) is above k% for every group, captures\nnotions of fairness and clinical relevance in a way that standard fairness notions do not [8].\nLimitations: We have chosen to optimize as broad a set of formulations as possible. As a result, for\ncertain metrics (particularly equalized odds [3]) the solutions found are known to be suboptimal; and\nfor others [12] the exponential search is unneeded. Techniques targeting particular formulations may\nbe needed to address this. A major driver of unfairness is a lack of data regarding particular groups.\nHowever, this very absence of data makes it hard for any toolkit to detect or rectify unfairness.\nBroader Impact: OxonFair is a tool for altering the decisions made by ML systems that are frequently\ntrained on biased data. Care must be taken that fair ML is used as a final step after correcting for bias\nand errors in data collation, and not as a sticking plaster to mask problems [88]. Indeed, inappropriate"}, {"title": "Acknowledgements", "content": "This work has been supported through research funding provided by the Wellcome Trust (grant nr\n223765/Z/21/Z), Sloan Foundation (grant nr G-2021-16779), Department of Health and Social Care,\nEPSRC (grant nr EP/Y019393/1), and Luminate Group. Their funding supports the Trustworthiness\nAuditing for AI project and Governance of Emerging Technologies research programme at the Oxford\nInternet Institute, University of Oxford.\nAn early prototype version of the same toolkit, for tabular data, was developed while CR was\nworking at AWS and is available online as autogluon.fair https://github.com/autogluon/\nautogluon-fair/. CR is grateful to Nick Erickson, and Weisu Yin for code reviews of the prototype.\nThe authors thank Kaivalya Rawal for feedback on the manuscript and testing the codebase."}, {"title": "Inferred characteristics", "content": "In many situations, protected attributes are not available at test time. In this case, we simply use\ninferred characteristics to assign per-group thresholds and adjust these thresholds to guarantee fairness\nwith respect to the true (i.e. uninferred) groups.\nWhen using inferred characteristics, we offer two pathways for handling estimated group membership.\nThe first pathway we consider makes a hard assignment of individuals to groups, based on a classifier\nresponse. The second pathway explicitly uses the classifier confidence as part of a per-datapoint\nthreshold. In practice, we find little difference between the two approaches, but the hard assignment\nto groups is substantially more efficient and therefore allows for a finer grid search and generally\nbetter performance. However, the soft assignment remains useful for the integration of our method\nwith neural networks, where we explicitly merge two heads of a neural network to arrive at a single\nfair model."}, {"title": "Fast pathway", "content": "The fast pathway closely follows the efficient grid search for known characteristics. We partition the\ndataset by inferred characteristics, and then repeat the trick. However, as the inferred characteristics\ndo not need to perfectly align with the true characteristics, we also keep track of the true group\ndatapoints belongs to, i.e., for all datapoints assigned to a particular inferred group, we compute the\ncumulative sum of positives and negatives that truly belong to each group. This allows us to vary\nthe thresholds with respect to inferred groups while computing group measures with respect to the"}, {"title": "Slow pathway", "content": "The slow pathway tunes t to optimize the decision process $f(x) - t.g(x) \\geq 0$, where g is a real\nvector valued function. Given the lack of assumptions, no obvious speed-up was possible and we\nperform a two stage na\u00efve grid-search, first coarsely to extract an approximate Pareto frontier, and\nthen a finer search over the range of thresholds found in the first stage. This is then followed by a final\ninterpolation that checks for candidates around pairs of adjacent candidates currently in the frontier.\nIn situations where $g(x)$ is the output of a classifier and $G'(x)$ its binarization, it is reasonable to\nsuspect that loss of information from binarization might lead to a drop in performance when we\ncompare the slow pathway with the fast. In practice, we never found a significant change, and in a\nlike-with-like comparison over a similar number of thresholds the fast pathway was as likely to be\nfractionally better as it was to be worse. Moreover, for more than 3 groups the slow pathway becomes\npunitively slow, and to keep the runtime acceptable requires decreasing the grid size in a way that\nharms performance.\nDespite this, we kept the slow pathway as it is directly applicable to deep networks as we describe in\nthe next section. In practice, when working with deep networks we make use of a hybrid approach,\nand perform the fast and slow grid searches before fusing them into a single frontier and then\nperforming interpolation. This allows us to benefit from the better solutions found by a fine grid\nsearch when the output of the second head is near binary (see Figure 2), and robustly carry over to\nthe slower pathway where its binarization is a bad approximation of the network output."}, {"title": "Implementation of Performance and Fairness Measures", "content": "To make OxonFair as extensible as possible, we create a custom class to implement all performance\nand fairness measures. This means when OxonFair doesn't support a particular measure, both the\nobjectives and constraints can be readily extended by the end user.\nMeasures used by OxonFair are defined as instances of a python class GroupMetrics. Each group\nmeasure is specified by a function that takes the number of True Positives, False Positives, False\nNegatives, and True Negatives and returns a score; A string specifying the name of the measure; and\noptionally a bool indicating if greater values are better than smaller ones.\nFor example, accuracy is defined as:\naccuracy = gm.GroupMetric(lambda TP, FP, FN, TN: (TP + TN) / (TP + FP + FN\n+ TN), 'Accuracy')\nFor efficiency, our approach relies on broadcast semantics and all operations in the function must be\napplicable to numpy arrays. Having defined a GroupMetric it can be called in two ways. Either:\naccuracy(target_labels, predictions, groups)\nHere target_labels and predictions are binary vectors corresponding to either the target ground-\ntruth values, or the predictions made by a classifier, with 1 representing the positive label and 0\notherwise. groups is simply a vector of values where each unique value is assumed to correspond to\na distinct group.\nThe other way it can be called is by passing it a single 3D array of dimension 4 by number of groups\nby k, where k is the number of candidate classifiers that the measure should be computed over."}, {"title": "Additional Metrics", "content": "Minimax Fairness [57, 62, 63] refers to the family of methods which minimize the loss of the group\nwhere the algorithm performs worst, i.e., they minimize the maximal loss. [90] observed that"}, {"title": "Utility Optimization", "content": "OxonFair supports the utility-based approach of Bakalar et al. [58], whereby different thresholds can\nbe selected per group to optimize a utility based objective. Utility functions can be defined in one\nline. In the following example, we consider a scenario where an ML system identifies issues that\nmay require interventions. In this example, every intervention has a cost of 1, regardless of if it was\nneeded, but a missed intervention that was needed has a cost of 5. Finally, not making an intervention\nwhen one was not needed has a cost of 0."}, {"title": "Levelling up", "content": "One criticism of many methods of algorithmic fairness is that enforcing equality of recall rates (as\nin equal opportunity) or selection rates (as in demographic parity) will decrease the recall/selection\nrate for some groups while increasing it for others. This behavior is an artifact of trying to maximize\naccuracy [8] and occurs despite fairness methods altering the overall selection rate [91]. As an\nalternative, OxonFair supports levelling up where harms are reduced to, at most, a given level per\ngroup [8]. For example, if we believe that black patients are being disproportionately harmed by a\nhigh number of false negatives in cancer detection (i.e., low recall), instead of enforcing that these\nproperties be equalized across groups, we can instead require that every group of patients has, at\nleast, a minimum recall score. Depending on the use case, similar constraints can be imposed in with\nrespect to per-group minimal selection rates, or minimal precision. These constraints can be enforced\nby a single call, for example, enforcing that the precision is above 70% while otherwise maximizing\naccuracy can be enforced by calling: .fit(gm.accuracy, gm.precision.min, 0.7). See also\nfig. 4."}, {"title": "Fairness under constrained capacity", "content": "When deploying fairness in practice, we may be capacity limited. For example, as in fig. 8 we may\nuse the output of a classifier for detecting cancer to schedule follow-up appointments. In such a\ncase, you might wish that the recall is high for each demographic group, but be constrained by the\nnumber of available appointments. Calling .fit(gm.recall.min, gm.pos_pred_rate, k,\ngreater_is_better_const = False) will maximize the recall on the worst-off group subject to a\nrequirement that no more than k% of cases are scheduled follow-up appointments.\nIn general, maximizing the group minimum of any measure that is monotone with respect to the\nselection rate, while enforcing a hard limit on the selection rate will enforce equality with respect to\nthat measure (e.g. optimizing gm.recall.min will result in equal recall a.k.a. equal opportunity,\nwhile maximizing gm.pos_pred_rate.min will result in demographic parity), while also enforcing\nthe selection rate constraints. See [91] for proof and a discussion of the issues arising, and [93] for an\nalternate approach.\nAs such, calling .fit(gm.recall.min, gm.pos_pred_rate, k, greater_is_better_const =\nFalse) will enforce equal opportunity at k% selection rate, and .fit(gm.pos_pred_rate.min,\ngm.pos_pred_rate, 0.4, greater_is_better_const = False) will enforce demographic\nparity at k% selection rate."}, {"title": "Conditional Metrics", "content": "A key challenge of using fairness in practice is that often some sources of bias are known, and\nthe practitioner is expected to determine if additional biases exist and to correct for them. For\nexample, someone's salary affects which loans they are eligible for, but salary has a distinctly\ndifferent distribution for different ethnicities and genders. [94]. Identifying and correcting fairness\nhere rapidly becomes challenging, when considering the intersection of attributes, many small groups\narise and purely by chance some form of unfairness may be observed [95, 64]suggested the use of\na technique from descriptive statistics that [96] had previously applied to the problem of schools\nadmissions at Berkley [97]. In this famous example, every school in Berkley showed little gender\nbias, but due to different genders applying at different rates to different schools, and the schools\nthemselves having substantially different acceptance rate, a strong overall gender bias was apparent.\n[96] observed that you could correct for this bias by computing the per school selection-rate, and\nthen taking a weighted average, where the weights are given by the total number of people applying\nto the school. The resulting selection rates are equivalent to a weighted selection-rate over the\n#individuals in school\nwhole population, where the weight wi for an individual i in a particular group and applying to a\nwi =\nparticular school is\n. To enforce this form of conditional demographic\n#individuals in group and school\nparity in OxonFair, we simply replace the sum of true positives etc. in Section 3, with the weighted\nsum. We support a range of related fairness metrics, including conditional difference in accuracy;\nand conditional equal opportunity (note that for equal opportunity we replace the numbers used to\ncompute wi with the same counts but only taking into account those that have positive ground-truth\nlabels). As such metrics can level down (Appendix C.3), we also support conditional minimum\nselection rates, and conditional minimum recall."}, {"title": "Directional Bias Amplification Metric Derivation for OxonFair", "content": "We also support a variant of Bias Amplification, as defined by Wang et al. [66].\nClosely following the notation of Wang et al. [66], let A be the set of protected demographic groups:\nfor example, A = {male, female}. Aa for a \u2208 A is the binary random variable corresponding to the\npresence of the group a; thus P(Awoman = 1) can be empirically estimated as the fraction of images\nin the dataset containing women. Let T\u2081 with t \u2208 T similarly correspond to binary target tasks. Let\n\u00c2a and T\u2081 denote model predictions for the protected group a and the target task t, respectively.\n$BiasAmp=\\frac{1}{|A||T|}\\sum_{a \\in A, t \\in T}Y_{at}\\Delta_{at} + (1 - Y_{at})(-\\Delta_{at})$\nwhere Yat = 1 [P(Aa = 1, T\u2081 = 1) > P(A\u2081 = 1)P(T\u2081 = 1)]\n\u2206at =\nP(\u00cet = 1|Aa = 1) \u2212 P(T\u2081 = 1|Aa = 1)\nif measuring Attribute \u2192 Task Bias\n(6)\nP(\u00c2a = 1|T\u2081 = 1) \u2212 P(Aa = 1|T\u2081 = 1)\nif measuring Task \u2192 Attribute Bias\nOf which, the Attribute \u2192 Task Bias is relevant here.\nEach component can be written as a function of the global True Positives, False Positives etc., and\nthe per group True Positives, and as such it can be optimized by our framework, albeit, not by using a\nstandard group metrics. However, this metric is gamable, and consistently underestimating labels in\ngroups where they're over-represented and vice versa would be optimal, but undesirable behavior\nthat leads to a negative score.\nInstead, we consider the absolute BiasAmp:\n|BiasAmp\u2192 =$\\frac{1}{|A||T|}\\sum_{a \\in A, t \\in T}|Y_{at}\\Delta_{at} + (1 - Y_{at})(-\\Delta_{at})|$\n(7)\n$\\frac{1}{|A||T|}\\sum_{a \\in A, t \\in T}|\\Delta_{at}|$\nWe can decompose |\u2206at | into the appropriate form for a GroupMetric (see appendix B) as follows:"}, {"title": "Computer Vision Experiments", "content": "We extensively used the codebase of Wang et. al [41] to conduct comparative experiments\u2079.\n\u2022 Empirical Risk Minimization (ERM) [98]: Acts as a baseline in our experiments where\nthe goal is to minimize the average error across the dataset without explicitly considering\nthe sensitive attributes.\n\u2022 Adversarial Training with Uniform Confusion [70]: The goal is to learn an embedding\nthat maximizes accuracy whilst minimizing any classifier's ability to recognize the protected\nclass. The uniform confusion loss from Alvi et al. [70] is used following the implementation\nof [41].\n\u2022 Domain-Discriminative Training [41]: Domain information is explicitly encoded and then\nthe correlation between domains and class labels is removed during inference.\n\u2022 Domain-Independent Training [41]: Trains a different classifier for each attribute where\nthe classifiers do not see examples from other domains.\n\u2022 OxonFair + Multi-Head [21]: Described in Section 4.2. N - 1 heads are trained to\nminimize the logistic loss over the target variable, where N is the total number of at-\ntributes. A separate head minimizes the squared loss over the protected attribute. Fair-\nness is enforced on validation data with two separate optimization criteria. OxonFair-\nDEO calls fpredictor.fit(gm.accuracy, gm.equal_opportunity, 0.01) to en-\nforce Equal Opportunity. OxonFair-MGA calls fpredictor.fit(gm.accuracy,\ngm.min.accuracy.min, 0)."}, {"title": "NLP Experiments", "content": "We employ a BERT-based model architecture [82], augmented with an additional head to simul-\ntaneously predict demographic factors (see Section 4.2. During training, we utilize the standard\ncross-entropy loss for the primary prediction task and a mean squared error loss for the demographic\npredictions, aggregating these to compute the overall loss. We ensure data consistency by excluding\nentries with missing demographic information. To facilitate easy comparison with different models,\nwe select the Polish language for the multilingual Twitter corpus, noted for its high DEO score, to\ndemonstrate how various models can reduce this score. We also conducted our experiment on the\nJigsaw data. Unlike the multilingual Twitter corpus, the Jigsaw religion dataset contains three groups:\nChristian, Muslim, and others. The entire model, including the BERT backbone, is fine-tuned for 10\nepochs using an initial learning rate of 2 \u00d7 10-5, following the original BERT training setup. All\nexperiments are conducted on an NVIDIA A100 80GB GPU.\nWe follow the methodology outlined in [80] to conduct the hate speech detection task using our tool.\nVariables such as age and country in the multilingual Twitter corpus are binarized using the same\nmethod as described in [80]. The data splits for training, development, and testing are shown in\nTable 14.\nMultilingual Experiment. To demonstrate the capability of our proposed tool in handling multilin-\ngual scenarios, we conduct experiments across five languages: English, Polish, Spanish, Portuguese,\nand Italian and the results are shown in Figure 11. Observations from the results indicate that: 1) Our\nmodel improves equal opportunity performance with minimal sacrifice to the main task performance.\n2) The datasets in Polish and Portuguese show higher equal opportunity, indicating more severe bias\ncompared to other languages, yet our proposed method effectively enhances performance in these\nconditions."}, {"title": "Comparison Table Information", "content": "In this section, we provide further details on the information from Figure 1. While all approaches have\nmany fairness definitions that can be computed, very few can be enforced via bias mitigation. As a\nminimum, OxonFair supports enforcing the methods from tables 5 and 6 (eliminating duplicates give\nthe number 14 in the table). In addition to this, it supports a wide range of metrics that aren't used in\nthe literature, for example minimizing the difference in balanced accuracy, F1 or Matthews correlation\ncoefficient (MCC) between groups, e.g., by using balanced_accuracy.diff as a constraint. It also\nsupports the definitions set out in Appendix C, including minimax notions; absolute bias amplification;\nand enforcing for minimum rates per group in recall, or precision, or sensitivity actively promoting\nlevelling-up [8", "31": ".", "3": "nis supported. The adversarial approach of [32", "11": "LFR [99", "Pre-processing\n[26": "Reweighting [25", "100": ".", "AdversarialDebiasing\n[32": "PrejudiceRemover [101"}, {"31": ".", "44": "EqOddsPostprocessing\n[3", "25": ".", "88": "."}]}