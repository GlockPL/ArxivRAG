{"title": "OxonFair: A Flexible Toolkit for Algorithmic Fairness", "authors": ["Eoin Delaney", "Zihao Fu", "Sandra Wachter", "Chris Russell", "Brent Mittelstadt"], "abstract": "We present OxonFair, a new open source toolkit for enforcing fairness in binary\nclassification. Compared to existing toolkits: (i) We support NLP and Com-\nputer Vision classification as well as standard tabular problems. (ii) We support\nenforcing fairness on validation data, making us robust to a wide range of over-\nfitting challenges. (iii) Our approach can optimize any measure based on True\nPositives, False Positive, False Negatives, and True Negatives. This makes it\neasily extendable and much more expressive than existing toolkits. It supports\n9/9 and 10/10 of the decision-based group metrics of two popular review pa-\npers. (iv) We jointly optimize a performance objective. This not only minimizes\ndegradation while enforcing fairness, but can improve the performance of oth-\nerwise inadequately tuned unfair baselines. OxonFair is compatible with stan-\ndard ML toolkits including sklearn, Autogluon, and PyTorch and is available at\nhttps://github.com/oxfordinternetinstitute/oxonfair.", "sections": [{"title": "Introduction", "content": "The deployment of machine learning systems that make decisions about people offers an opportunity\nto create systems that work for everyone. However, such systems can lock in existing prejudices.\nLimited data for underrepresented groups can result in ML systems that do not work for them, while\nthe use of training labels based on historical data can result in ML systems copying previous biases.\nAs such, it is unsurprising that AI systems have repeatedly exhibited unwanted biases towards certain\ndemographic groups in a wide range of domains including medicine [1, 2], finance [3, 4], and policing\n[5]. Such groups are typically identified with respect to legally protected attributes, such as ethnicity\nor gender [6, 7, 3]. The field of algorithmic fairness has sprung up in response to these biases.\nContributions to algorithmic fairness can broadly be split into methodological and policy-based\napproaches. While much methodological work focuses on measuring and enforcing (un)fairness, a\ncommon criticism from the policy side is that this work can occur 'in isolation from policy and civil\nsocietal contexts and lacks serious engagement with philosophical, political, legal and economic\ntheories of equality and distributive justice' [8].\nIn response to these criticisms, we have developed OxonFair, a more expressive toolkit for algorithmic\nfairness. We acknowledge that people designing algorithms are not always the right people to decide\non policy, and as such we have chosen to create as flexible a toolkit as possible to allow policymakers\nand data scientists with domain knowledge to identify relevant harms and directly alter the system\nbehaviour to address them. Unlike existing Fairness toolkits such as AIF360 [9], which take a\nmethod-driven approach, and provide access to a wide-range of methods but with limited control over\ntheir behaviour, we take a measure-based approach and provide one fairness method that is extremely\ncustomizable, and can optimize user-provided objectives and group fairness constraints."}, {"title": "Related Work", "content": "Bias mitigation strategies for classification have been broadly categorized into three categories\n[6, 22\u201324]; pre-processing, in-processing and post-processing.\nPre-processing algorithms improve fairness by altering the dataset in an attempt to remove biases\nsuch as disparate impact [11] before learning a model itself. Popular pre-processing approaches\ninclude simply re-weighting samples in the training data to enhance fairness [25], optimizing this\nprocess by learning probabilistic transformations [26], or by generating synthetic data [27\u201329].\nIn-processing / In-training methods mitigate bias by adjusting the training procedure. Augmenting\nthe loss with fair regularizers [23, 30] is common for logistic regression and neural networks.\nAgarwal et al. [31] iteratively alter the cost for different datapoints to enforce fairness on the\ntrain set. Approaches based on adversarial training typically learn an embedding that reduces\nan adversary's ability to recover protected groups whilst maximizing predictive performance [32\u2013\n35]. Other popular approaches include Disentanglement [36, 37], Domain Generalization [38\u201340],\nDomain-independence [41] and simple approaches such as up-sampling or reweighing minority\ngroups during training. Notably, in the case of high-capacity models in medical computer-vision\ntasks, a recent benchmark paper by Zong et al. [42] showed state-of-the-art in-processing methods do\nnot significantly improve outcomes over training without considering fairness at all. A comprehensive\nbenchmark study of in-processing methods in other domains is provided by Han et al. [43].\nPost-processing methods aim to enforce fairness by using thresholds or randomization to adjust the\npredictions of a trained model based on the protected attributes [3, 44]. Post-processing methods are\ntypically model-agnostic and can be applied to any model that returns confidence scores.\nEnforcing Fairness on Validation Data is useful for avoiding the misestimation of error rates due to\noverfitting. It has shown particular promise in computer vision through Neural Architecture Search\n[45], adjusting decision boundaries [30], reweighting [46] and data augmentation [17]."}, {"title": "Fairness Toolkits", "content": "Most toolkits such as Fairness Measures [47], TensorFlow Fairness Indicators [48], and FAT Foren-\nsics [49] focus on measuring bias and do not support enforcing fairness through bias mitigation.\nFairML[50] audits fairness by quantifying the importance of different attributes in prediction. This is\nbest suited for tabular data where features are well-defined. FairTest [51] investigates the associations\nbetween application outcomes (e.g., insurance premiums) and sensitive attributes such as age to\nhighlight and debug bias in deployed systems. Aequitas [52] provides examples of when different\nmeasures are (in)appropriate with support for some bias mitigation methods in binary classification.\nThemis-ML [53] supports the deployment of several simple bias mitigation methods such as rela-\nbelling [25], but focuses on linear models. Friedler et al. [22] introduce the more complete Fairness\nComparison toolkit where four bias mitigation strategies are compared across five tabular datasets\nand multiple models (Decision trees, Gaussian Na\u00efve Bayes, SVM, and Logistic Regression).\nThere are two fairness toolkits that support sklearn[18] like OxonFair. These are the two most\npopular toolkits: Microsoft Fairlearn [20] (1.9k GitHub Stargazers as of June 2024) and IBM AIF360\n[9] (2.4k Stargazers). AIF360 offers a diverse selection of bias measures and pre-processing, in-\nprocessing and post-processing bias mitigation strategies on binary classification tabular datasets. For\nmitigation, Fairlearn primarily offers implementations of [31, 3] avoiding the use of the term bias,\ninstead considering fairness through the lens of fairness-related harms [54] where the goal is to \u201chelp\npractitioners assess fairness-related harms, review the impacts of different mitigation strategies and\nmake trade-offs appropriate to their scenario\". Lee & Singh [55] recognized Fairlearn as one of the\nmost user-friendly fairness toolkits, and critiqued AIF360 as being the least user-friendly toolkit.\nBoth AIF360 and Fairlearn contain post-processing methods that select per-group thresholds. Unlike\nOxonFair, neither method uses the fast optimization we propose; both methods require group\ninformation at test time; AIF360 only supports two groups, but does use cross-validation to avoid\noverfitting; Fairlearn does not support the use of validation data, but does support more than two\ngroups. According to their documentation, neither toolkit can be applied to NLP or computer vision."}, {"title": "Toolkit interface", "content": "The interface of OxonFair decomposes into three parts: (i) evaluation of fairness and performance\nfor generic classifier outputs. (ii) evaluating and enforcing fairness for particular classifiers. (iii)\nspecialist code for evaluating and enforcing fairness for deep networks."}, {"title": "Inference", "content": "To make decisions, we assign thresholds to groups. We write $f(x)$ for the response of a classifier\nf, on datapoint x, t for the vector corresponding to the ordered set of thresholds, and $G(x)$ for the\none-hot encoding of group membership. We make a positive decision if\n$f(x) - t. G(x) \\geq 0$\n(1)\nTo optimize arbitrary measures we perform a grid search over the choices of threshold, t.\nEfficient grid sampling We make use of a common trick for efficiently computing measures such\nas precision and recall for a range of thresholds. This trick is widely used without discussion for\nefficient computation of the area under ROC curves, and we have had trouble tracking down an\noriginal reference for it. As one example, it is used by scikit-learn [18].\nThe trick is as follows: sort the datapoints by classifier response, then generate a cumulative sum of\nthe number of positive datapoints and the number of negatives, going from greatest response to least.\nWhen picking a threshold between points i and i + 1, TP is given by the cumulative sum of positives\nin the decreasing direction up to and including i; FN is the sum of negatives in the same direction; FP\nis the total sum of positives minus TP, and TN is the total sum of negatives minus TN.\nWe perform this trick per group, and efficiently extract the TP, FN, FP and TN for different thresholds.\nThese are combinatorially combined across the groups and the measures computed. This two stage\ndecoupling offers a substantial speed-up. If we write T for the number of thresholds, k for the\nnumber of groups, and n for the total number of datapoints, our procedure is upper-bounded by\n$O(Tk + n log n)$, while the na\u00efve approach is $O(nTk)$. No other fairness method makes use of this,\nand in particular, all the threshold-based methods offered by AIF360 make use of a na\u00efve grid search.\nFrom the grid sampling, we extract a Pareto frontier with respect to the two measures. The thresholds\nthat best optimize the objective while satisfying the constraint are returned as a solution. If no such\nthreshold exists, we return the thresholds closest to satisfying the constraint."}, {"title": "Inferred characteristics", "content": "When using inferred characteristics, we offer two pathways for handling estimated group membership.\nThe first pathway we consider makes a hard assignment of individuals to groups, based on a classifier\nresponse. The second pathway explicitly uses the classifier confidence as part of a per-datapoint\nthreshold. In practice, we find little difference between the two approaches, but the hard assignment\nto groups is substantially more efficient and therefore allows for a finer grid search and generally\nbetter performance. However, the soft assignment remains useful for the integration of our method\nwith neural networks, where we explicitly merge two heads (a classifier and a group predictor) of a\nneural network to arrive at a single fair model. For details of the two pathways see Appendix A."}, {"title": "Fairness for Deep Networks", "content": "We use the method proposed in [21] (N.B., they used it only for demographic parity). Consider a\nnetwork with two heads f, and g, comprised of single linear layers, and trained to optimize two\ntasks on a common backbone B. Let f be a standard classifier trained to maximize some notion of\nperformance such as log-loss and g is a classifier trained to minimize the squared loss with respect to\na vector that is a one hot-encoding of group membership. Any decision $f(x) \u2212 t \u00b7 g(x) \u2265 0$ can now\nbe optimized for given criteria by tuning weights w using the process outlined in the slow pathway.\nAs both f and g are linear layers on top of a common backbone, we can write them as:\n$f(x) = w_f \u00b7 B(x) + b_f, g(x) = w_g \u00b7 B(x) + b_g$\n(2)\nnote that as f(x) is a real number, and g(x) is a vector, $w_f$ is a vector and $b_f$ a real number, while\n$w_g$ is a matrix and $b_g$ a vector.\nThis means that the decision function $f(x) - t.g(x) \\geq 0$ can be rewritten using the identity:\n$f(x) \u2212 t \u00b7 g(x) = w_f \u00b7 B(x) + b_f - t \u00b7 w_g \u00b7 B(x) - t.b_g$\n(3)\n$= (w_f - t . w_g) \u00b7 B(x) + (b_f - t.b_g)$\n(4)\nThis gives a 3 stage process for enforcing any of these decision/fairness criteria for deep networks.\n1. Train a multitask neural network as described above.\n2. Compute the optimal thresholds t on held-out validation data as described in Appendix A.\n3. Replace the multitask head with a neuron with weights $(w_f - t w_g)$ and bias $(b_f - t.b_g)$.\nTo maximize performance, the training set should be augmented following best practices, while the\nvalidation set typically should not. The resulting network f* will have the same architecture as the\noriginal non-multitask network, while satisfying chosen criteria.\nOxonFair has a distinct interface for deep learning5. Training and evaluating NLP and vision\nfrequently involves complex pipelines. To maximize applicability, we assume that the user has\ntrained a two-headed network as described above, and evaluated on a validation set. Our constructor\nDeepFairPredictor takes as an input: the output of the two-headed network over the validation\nset; the ground-truth labels; and the groups. fit and the evaluation functionality can then be called\nin the same way. Once a solution is found, the method extract_coefficients can be called to\nextract the thresholds t from 4, so that the user can use to merge the network heads."}, {"title": "Toolkit expressiveness", "content": "Out of the box, OxonFair supports all 9 of the decision-based group fairness measures defined by\nVerma and Rudin [59] and all 10 of the fairness measures from Sagemaker Clarify [60]. OxonFair\nsupports any fairness measure (including conditional fairness measures) that can be expressed per\ngroup as a weighted sum of True Positives, False Positives, True Negatives and False Negatives.\nOxonFair does not support notions of individual fairness such as fairness through awareness [61].\nSee Appendix B for a discussion of how metrics are implemented and comparison with two review\npapers. Appendix C contains details of non-standard fairness metrics, including utility optimization\n[58]; minimax fairness [57, 62, 63]; minimum rate constraints [8], and Conditional Demographic\nParity [64]. This also includes a variant of Bias Amplification [65, 66]."}, {"title": "Inferred characteristics", "content": "In many situations, protected attributes are not available at test time. In this case, we simply use\ninferred characteristics to assign per-group thresholds and adjust these thresholds to guarantee fairness\nwith respect to the true (i.e. uninferred) groups.\nWhen using inferred characteristics, we offer two pathways for handling estimated group membership.\nThe first pathway we consider makes a hard assignment of individuals to groups, based on a classifier\nresponse. The second pathway explicitly uses the classifier confidence as part of a per-datapoint\nthreshold. In practice, we find little difference between the two approaches, but the hard assignment\nto groups is substantially more efficient and therefore allows for a finer grid search and generally\nbetter performance. However, the soft assignment remains useful for the integration of our method\nwith neural networks, where we explicitly merge two heads of a neural network to arrive at a single\nfair model."}, {"title": "Fast pathway", "content": "The fast pathway closely follows the efficient grid search for known characteristics. We partition the\ndataset by inferred characteristics, and then repeat the trick. However, as the inferred characteristics\ndo not need to perfectly align with the true characteristics, we also keep track of the true group\ndatapoints belongs to, i.e., for all datapoints assigned to a particular inferred group, we compute the\ncumulative sum of positives and negatives that truly belong to each group. This allows us to vary\nthe thresholds with respect to inferred groups while computing group measures with respect to the"}, {"title": "Slow pathway", "content": "The slow pathway tunes t to optimize the decision process $f(x) - t.g(x) \\geq 0$, where g is a real\nvector valued function. Given the lack of assumptions, no obvious speed-up was possible and we\nperform a two stage na\u00efve grid-search, first coarsely to extract an approximate Pareto frontier, and\nthen a finer search over the range of thresholds found in the first stage. This is then followed by a final\ninterpolation that checks for candidates around pairs of adjacent candidates currently in the frontier.\nIn situations where g(x) is the output of a classifier and G'(x) its binarization, it is reasonable to\nsuspect that loss of information from binarization might lead to a drop in performance when we\ncompare the slow pathway with the fast. In practice, we never found a significant change, and in a\nlike-with-like comparison over a similar number of thresholds the fast pathway was as likely to be\nfractionally better as it was to be worse. Moreover, for more than 3 groups the slow pathway becomes\npunitively slow, and to keep the runtime acceptable requires decreasing the grid size in a way that\nharms performance.\nDespite this, we kept the slow pathway as it is directly applicable to deep networks as we describe in\nthe next section. In practice, when working with deep networks we make use of a hybrid approach,\nand perform the fast and slow grid searches before fusing them into a single frontier and then\nperforming interpolation. This allows us to benefit from the better solutions found by a fine grid\nsearch when the output of the second head is near binary (see Figure 2), and robustly carry over to\nthe slower pathway where its binarization is a bad approximation of the network output."}, {"title": "Implementation of Performance and Fairness Measures", "content": "To make OxonFair as extensible as possible, we create a custom class to implement all performance\nand fairness measures. This means when OxonFair doesn't support a particular measure, both the\nobjectives and constraints can be readily extended by the end user.\nMeasures used by OxonFair are defined as instances of a python class GroupMetrics. Each group\nmeasure is specified by a function that takes the number of True Positives, False Positives, False\nNegatives, and True Negatives and returns a score; A string specifying the name of the measure; and\noptionally a bool indicating if greater values are better than smaller ones.\nFor example, accuracy is defined as:\n`accuracy = gm.GroupMetric(lambda TP, FP, FN, TN: (TP + TN) / (TP + FP + FN\n+ TN), 'Accuracy')`\nFor efficiency, our approach relies on broadcast semantics and all operations in the function must be\napplicable to numpy arrays. Having defined a GroupMetric it can be called in two ways. Either:\n`accuracy(target_labels, predictions, groups)`\nHere target_labels and predictions are binary vectors corresponding to either the target ground-\ntruth values, or the predictions made by a classifier, with 1 representing the positive label and 0\notherwise. groups is simply a vector of values where each unique value is assumed to correspond to\na distinct group.\nThe other way it can be called is by passing it a single 3D array of dimension 4 by number of groups\nby k, where k is the number of candidate classifiers that the measure should be computed over."}, {"title": "Fairness Measures", "content": "Table 5: The fairness measures in the review of [59]. All 9 group metrics that concern the decisions\nmade by a classifier are supported by OxonFair.\nAs a convenience, GroupMetrics automatically implements a range of functionality as sub-objects.\nHaving defined a metric as above, we have a range of different objects:\n* metric.diff reports the average absolute difference of the method between groups.\n* metric.average reports the average of the method taken over all groups.\n* metric.max_diff reports the maximum difference of the method between any pair of\ngroups.\n* metric.max reports the maximum value for any group.\n* metric.min reports the minimum value for any group.\n* metric.overall reports the overall value for all groups combined, and is the same as\ncalling metric directly\n* metric.ratio reports the average over distinct pairs of groups of the smallest value divided\nby the largest\n* metric.per_group reports the value for every group.\nAll of these can be passed directly to fit, or to the evaluation functions we provide.\nThe vast majority of fairness metrics are implemented as a . diff of a standard performance measure,\nand by placing a .min after any measure such as recall or precision it is possible to add constraints\nthat enforce that the precision or recall is above a particular value for every group.\nThese classes make it easy to extend OxonFair. To demonstrate the OxonFair's versatility, Tables 5\nand 6 show the metrics of two reviews and how many can are implemented out of the box by our\napproach. An example showing how all clarify metrics can be enforced using inferred groups, and\nthree group labels on compas can be seen in Table 7."}, {"title": "Additional Metrics", "content": "Minimax fairness [57, 62, 63] refers to the family of methods which minimize the loss of the group\nwhere the algorithm performs worst, i.e., they minimize the maximal loss. [90] observed that"}, {"title": "Post-training Metrics", "content": "Table 6: The post-training fairness measures in the review of [89]. All measures are supported by\nOxonFair.\nHere, we compare OxonFair against minimax fairness. To do this, we define a new performance\nmeasure corresponding to the lowest accuracy over the positive or negative labelled datapoints.\n`min accuracy = min(TP / TP + FP, TN / FN + TN)`\n(5)\nMartinez et al [57] argued that we should seek a Pareto optimal solution that has the highest possible\noverall accuracy, subject to the requirement it maximizes the lowest per group accuracy. We can do\nthis in OxonFair by calling fpredictor.fit(gm.min_accuracy.min, gm.accuracy, 0) Here\nmin_accuracy.min corresponds to the lowest min accuracy of any group. We use accuracy > 0 as\nthe constraint, as we do not want an active constraint from preventing us from finding the element of\nthe Pareto frontier (see [57] for frontier details) with the highest minimum accuracy. Note that the\ngroups used by OxonFair with this loss correspond to the true groups, such as ethnicity or gender,"}, {"title": "Utility Optimization", "content": "OxonFair supports the utility-based approach of Bakalar et al. [58], whereby different thresholds can\nbe selected per group to optimize a utility based objective. Utility functions can be defined in one\nline. In the following example, we consider a scenario where an ML system identifies issues that\nmay require interventions. In this example, every intervention has a cost of 1, regardless of if it was\nneeded, but a missed intervention that was needed has a cost of 5. Finally, not making an intervention\nwhen one was not needed has a cost of 0."}, {"title": "Levelling up", "content": "One criticism of many methods of algorithmic fairness is that enforcing equality of recall rates (as\nin equal opportunity) or selection rates (as in demographic parity) will decrease the recall/selection\nrate for some groups while increasing it for others. This behavior is an artifact of trying to maximize\naccuracy [8] and occurs despite fairness methods altering the overall selection rate [91]. As an\nalternative, OxonFair supports levelling up where harms are reduced to, at most, a given level per\ngroup [8]. For example, if we believe that black patients are being disproportionately harmed by a\nhigh number of false negatives in cancer detection (i.e., low recall), instead of enforcing that these\nproperties be equalized across groups, we can instead require that every group of patients has, at\nleast, a minimum recall score. Depending on the use case, similar constraints can be imposed in with\nrespect to per-group minimal selection rates, or minimal precision. These constraints can be enforced\nby a single call, for example, enforcing that the precision is above 70% while otherwise maximizing\naccuracy can be enforced by calling: .fit(gm.accuracy, gm.precision.min, 0.7). See also\nfig. 4."}, {"title": "Conditional Metrics", "content": "A key challenge of using fairness in practice is that often some sources of bias are known, and\nthe practitioner is expected to determine if additional biases exist and to correct for them. For\nexample, someone's salary affects which loans they are eligible for, but salary has a distinctly\ndifferent distribution for different ethnicities and genders. [94]. Identifying and correcting fairness\nhere rapidly becomes challenging, when considering the intersection of attributes, many small groups\narise and purely by chance some form of unfairness may be observed [95, 64]suggested the use of\na technique from descriptive statistics that [96] had previously applied to the problem of schools\nadmissions at Berkley [97]. In this famous example, every school in Berkley showed little gender\nbias, but due to different genders applying at different rates to different schools, and the schools\nthemselves having substantially different acceptance rate, a strong overall gender bias was apparent.\n[96] observed that you could correct for this bias by computing the per school selection-rate, and\nthen taking a weighted average, where the weights are given by the total number of people applying\nto the school. The resulting selection rates are equivalent to a weighted selection-rate over the\nwhole population, where the weight wi for an individual i in a particular group and applying to a\n#individuals in school\nparticular school is wi = `. To enforce this form of conditional demographic\n#individuals in group and school`\nparity in OxonFair, we simply replace the sum of true positives etc. in Section 3, with the weighted\nsum. We support a range of related fairness metrics, including conditional difference in accuracy;\nand conditional equal opportunity (note that for equal opportunity we replace the numbers used to\ncompute wi with the same counts but only taking into account those that have positive ground-truth\nlabels). As such metrics can level down (Appendix C.3), we also support conditional minimum\nselection rates, and conditional minimum recall."}, {"title": "Directional Bias Amplification Metric Derivation for OxonFair", "content": "We also support a variant of Bias Amplification, as defined by Wang et al. [66].\nClosely following the notation of Wang et al. [66], let A be the set of protected demographic groups:\nfor example, A = {male, female}. Aa for a \u2208 A is the binary random variable corresponding to the\npresence of the group a; thus P(Awoman = 1) can be empirically estimated as the fraction of images\nin the dataset containing women. Let T\u2081 with t \u2208 T similarly correspond to binary target tasks. Let\n\u00c2a and T\u2081 denote model predictions for the protected group a and the target task t, respectively.\n$BiasAmp_{\u2192} = 1 / |A||T| \u03a3_{a \u2208 A, t \u2208 T} \u03b3_{at} \u00b7 \u2206_{at}$ where\n$\u03b3_{at} = 1[P(Aa = 1, Tt = 1) > P(Aa = 1)P(Tt = 1)]$\n\u2206at = {P(\u00cet = 1|Aa = 1) \u2013 P(Tt = 1|Aa = 1), if measuring Attribute \u2192 Task Bias\nP(\u00c2a = 1|Tt = 1) \u2013 P(Aa = 1|Tt = 1), if measuring Task \u2192 Attribute Bias}\n(6)\nOf which, the Attribute \u2192 Task Bias is relevant here.\nEach component can be written as a function of the global True Positives, False Positives etc., and\nthe per group True Positives, and as such it can be optimized by our framework, albeit, not by using a\nstandard group metrics. However, this metric is gamable, and consistently underestimating labels in\ngroups where they're over-represented and vice versa would be optimal, but undesirable behavior\nthat leads to a negative score.\nInstead, we consider the absolute BiasAmp:\n|BiasAmp\u2192 = 1 / |A||T| \u03a3_{a \u2208 A, t \u2208 T} \u03b3_{at} \u00b7 |\u2206_{at}|\n(7)\nWe can decompose |\u2206at | into the appropriate form for a GroupMetric (see appendix B) as follows:"}, {"title": "Vision Experiments", "content": "We extensively used the codebase of Wang et. al [41] to conduct comparative experiments\n* Empirical Risk Minimization (ERM) [98]: Acts as a baseline in our experiments where\nthe goal is to minimize the average error across the dataset without explicitly considering\nthe sensitive attributes.\n* Adversarial Training with Uniform Confusion [70]: The goal is to learn an embedding\nthat maximizes accuracy whilst minimizing any classifier's ability to recognize the protected\nclass. The uniform confusion loss from Alvi et al. [70] is used following the implementation\nof [41].\n* Domain-Discriminative Training [41]: Domain information is explicitly encoded and then\nthe correlation between domains and class labels is removed during inference.\n* Domain-Independent Training [41]: Trains a different classifier for each attribute where\nthe classifiers do not see examples from other domains.\n* OxonFair + Multi-Head [21]: Described in Section 4.2. N 1 heads are trained to\nminimize the logistic loss over the target variable, where N is the total number of at-\ntributes. A separate head minimizes the squared loss over the protected attribute. Fair-\nness is enforced on validation data with two separate optimization criteria. OxonFair-\nDEO calls fpredictor.fit(gm.accuracy, gm.equal_opportunity, 0.01) to en-\nforce Equal Opportunity. OxonFair-MGA calls fpredictor.fit(gm.accuracy,\ngm.min.accuracy.min, 0)."}, {"title": "Details", "content": "Computer vision experiments were conducted using a NVIDIA RTX 3500 Ada GPU with 12GB of\nRAM."}, {"title": "Experiments", "content": "We employ a BERT-based model architecture [82], augmented with an additional head to simul-\ntaneously predict demographic factors (see Section 4.2. During training, we utilize the standard\ncross-entropy loss for the primary prediction task and a mean squared error loss for the demographic\npredictions, aggregating these to compute the overall loss. We ensure data consistency by excluding\nentries with missing demographic information. To facilitate easy comparison with different models,\nwe select the Polish language for the multilingual Twitter corpus, noted for its high DEO score, to\ndemonstrate how various models can reduce this score. We also conducted our experiment on the\nJigsaw data. Unlike the multilingual Twitter corpus, the Jigsaw religion dataset contains three groups:\nChristian, Muslim, and others. The entire model, including the BERT backbone, is fine-tuned for 10\nepochs using an initial learning rate of 2 \u00d7 10-5, following the original BERT training setup. All\nexperiments are conducted on an NVIDIA A100 80GB GPU."}, {"title": "Hate Speech Detection Task", "content": "We follow the methodology outlined in [80] to conduct the hate speech detection task using our tool.\nVariables such as age and country in the multilingual Twitter corpus are binarized using the same\nmethod as described in [80]. The data splits for training, development, and testing are shown in\nTable 14.\nMultilingual Experiment. To demonstrate the capability of our proposed tool in handling multilin-\ngual scenarios, we conduct experiments across five languages: English, Polish, Spanish, Portuguese,\nand Italian and the results are shown in Figure 11. Observations from the results indicate that: 1) Our\nmodel improves equal opportunity performance with minimal sacrifice to the main task performance.\n2) The datasets in Polish and Portuguese show higher equal opportunity, indicating more severe bias\ncompared to other languages, yet our proposed method effectively enhances performance in these\nconditions."}, {"title": "Demographic Experiments", "content": "To demonstrate our tool's ability to address various demographic\nfactors in text, we conducted experiments focusing on age, country, gender, and ethnicity, with results\ndetailed in Figure 12 and Figure 13. The outcomes reveal that our tool effectively improves equal\nopportunities across all demographic factors, underscoring its capability to handle general debiasing\nscenarios."}, {"title": "Toxicity Classification Task", "content": "We also evaluate toxicity classification using the Jigsaw toxic comment dataset [81], which has been\ntransformed into a Kaggle challenge. To demonstrate the ability of OxonFair to handle multiple\nprotected groups, we consider religion as the protected attribute and evaluate performance across\nthree groups: Christian, Muslim, and Other. Owing to the limitted dataset size, all samples labelled as\na religion that was neither Christian nor Muslim were merged into Other and unlabeled samples were\ndiscarded. The statistics for this dataset are shown in Table 15, where each cell displays the count of\nnegative and positive samples, respectively. The experimental results are discussed in the main paper.\nFor the Jigsaw dataset, we follow the setup of [74], selecting race as the protected attribute. We focus\non the subset of comments identified as Black or Asian, as these two groups exhibit the largest gap in\nthe probability of being associated with toxic comments. The data statistics are shown in Table 16\nwhere each cell displays the count of negative and positive samples, respectively. The experimental\nresults, presented in Table 17, demonstrate that our proposed tool outperforms all other models."}, {"title": "Comparison Table Information", "content": "In this section, we provide further details on the information from Figure 1. While all approaches have\nmany fairness definitions that can be computed, very few can be enforced via bias mitigation. As a\nminimum, OxonFair supports enforcing the methods from tables 5 and 6 (eliminating duplicates give\nthe number 14 in the table). In addition to this, it supports a wide range of metrics that aren't used in\nthe literature, for example minimizing the difference in balanced accuracy, F1 or Matthews correlation\ncoefficient (MCC) between groups, e.g., by using balanced_accuracy.diff```json"}]}