{"title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model", "authors": ["Jinwei Hu", "Zhenglin Huang", "Xiangyu Yin", "Wenjie Ruan", "Guangliang Cheng", "Yi Dong", "Xiaowei Huang"], "abstract": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in generative AI (Achiam et al., 2023; Anil et al., 2023; Dubey et al., 2024), fueled by innovations in training techniques such as Parameter-Efficient Fine-Tuning (PEFT), have enabled large language models (LLMs) to efficiently internalize linguistic knowledge and excel in tasks from text generation to decision making (Hu et al., 2022; yang Liu et al., 2024). These models derive their power from massive, diverse corpora, but this dependency on large-scale datasets introduces significant risks: harmful, biased, or sensitive information can be inadvertently encoded and amplified, leading to ethical violations, regulatory noncompliance, and potential misuse (Hsu et al., 2024; Urman & Makhortykh, 2023; Jiao et al., 2024).\nExisting mitigation strategies, such as implementing guardrails (Dong et al., 2024) or training models with expertly curated datasets to refuse harmful queries (Ouyang et al., 2022), are computationally expensive and often inadequate against adversarial attacks (Xu et al., 2024; Yin et al., 2024). In contrast, while retraining an entire model on a cleaned dataset to eliminate harmful influences is theoretically feasible, it is prohibitively resource-intensive for modern LLMs (Jin et al., 2023). Additionally, adversaries can exploit PEFT to reintroduce such unwanted information, highlighting the urgent need for more effective and scalable solutions for publicly accessed LLMs (Qi et al., 2024).\nTo solve harmful or sensitive information in machine learning models, Machine Unlearning (MU) has emerged as a promising solution, supported by growing regulations such as the \"right to be forgotten\" under the GDPR (Regulation, 2016; Ginart et al., 2019). It originally developed in the non-LLMs domain and has proven effective at removing specific data influences while preserving model performance (Bourtoule et al., 2021; Kim & Woo, 2022a; Liu et al., 2023). When transferred to maintain responsible LLMS, MU offers significant advantages, being far more computationally efficient than full retraining. Unlearned models also exhibit greater inherent safety, as they lack the undesired knowledge necessary for malicious behaviors (Hendrycks et al., 2021b; Li et al., 2024).\nDespite its potential, LLM unlearning still faces several fundamental issues: (1) existing approaches typically rely on empirical methods like grid search to identify intervention parameters, lacking an efficient and interpretable way to guide parameter selection within deep architectures, (2) current methods normally rely on coarse-grained manipulation (using simplistic loss combinations that induce random representation dispersion with uncontrolled gradient dynamics, struggling to balance knowledge removal and utility preservation) rather than fine-grained representation manipulation (achieving more effective knowledge separation through targeted representation modification and regulated gradient dynamics for reducing damage to model utility), and (3) knowledge recovery methods like jailbreaking attack can recover the undesired information from the unlearned model (Shaik et al., 2024)."}, {"title": "2. Related work", "content": "Our paper focuses on LLM unlearning for undesired knowledge, information-theoretic metrics, and contrastive learning. We highlight the developments and limitations of LLM unlearning in this section, while related advancements in information-theoretic metrics, contrastive learning, and gradient projection are detailed in the Appendix A and B.\nLLM Unlearning LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality (Zhang et al., 2024a). Current approaches can be broadly categorized into training-time methods and inference-time methods (Barez et al., 2025). Among training-time approaches, which represent the mainstream methodology, two primary directions have emerged. The first direction focuses on gradient optimization,"}, {"title": "3. Problem Formulation", "content": "The task of LLM unlearning involves selectively removing specific knowledge (forget set) from the model while retaining critical information (retain set). However, this process is complicated by the issue of knowledge entanglement, where representations of the forget and retain sets overlap significantly within the model's parameters (Zhang et al., 2024b)."}, {"title": "3.1. Problem Setup", "content": "To formalize the unlearning process, we adopt the general formulation proposed by Liu et al. (Liu et al., 2024b):\n$\\min _{\\theta}\\left\\{E_{(x, yf) \\in D_{F}}[L(yf | x ; \\theta)]+\\lambda E_{(x, y) \\in D_{R}}[L(y | x ; \\theta)]\\right\\}$\nwhere $L(y | x ; \\theta)$ measures the discrepancy between the model's prediction and the target response $y$ for a given input $x$ under the model's parameters $\\theta$. Here, $D_{F}$ and $D_{R}$ denote the forget set and retain set, respectively. The variable $y_{f}$ specifies the intended output for the forget set after unlearning, while the hyperparameter $\\lambda \\geq 0$ controls the trade-off between forgetting and retention objectives. For simplicity, we will refer to this objective as $\\min _{\\theta} E_{mu}(\\theta)$ in subsequent sections.\nDespite the generality of above formulation, it does not explicitly quantify the representations of forgotten and retained knowledge. This lack of quantification poses challenges in precisely guiding the unlearning process (Qu et al., 2024). To address this, a principled metric is needed to evaluate and minimize knowledge entanglement, ensuring that unlearning primarily affects the forget set while minimizing interference with the retain set.\nTo address this, we introduce information-theoretic measures, specifically continuous entropy and mutual information, to quantify the dependency between the activations of the forget and retain sets. Let $F$ and $R$ represent the activations of the forget and retain sets at a specific layer of the model, respectively. The degree of knowledge entanglement between representations can be formulated as the mutual information $I(F ; R)$:\n$I(F ; R)=H(F)+H(R)-H(F, R)$\nwhere $H(F)$ and $H(R)$ are the continuous entropies of the activations $F$ and $R$, and $H(F, R)$ denotes their joint entropy. These measures provide a systematic approach to identify layers with minimal entanglement and guiding the optimization process for targeted and effective unlearning. The details of these metrics are shown in Appendix C."}, {"title": "3.2. LLM unlearning with MI Guidance", "content": "To quantify knowledge entanglement during machine unlearning, we use MI to measure the dependency between the activations of the forget set $F^{(l)}$ and the retain set $R^{(l)}$ at each layer $l$. The MI $I\\left(F^{(l)} ; R^{(l)}\\right)$ serves as an indicator to guide the unlearning process by minimizing entanglement between $F^{(l)}$ and $R^{(l)}$.\nTo minimize the entanglement between the forget and retain sets' representations, we formulate the layer selection as:\n$l^{*}=\\arg \\min _{l} \\hat{I}^{(l)}$"}, {"title": "4. Methodology", "content": "To address the challenges of more thorough selective multi-domain knowledge unlearning and enhanced robustness against knowledge recovery in LLMs, we propose FALCON shown in Figure 1, a framework that advances both precision and effective in knowledge manipulation. Unlike prior approaches that rely on coarse-grained loss combinations, FALCON introduces three key mechanisms: (1) mutual information-based guidance to identify parameters where knowledge representations are least entangled, enabling interpretable parameter selection; (2) contrastive mechanism with enhanced representation separation to achieve fine-grained knowledge manipulation while ensuring robust resistance against knowledge recovery attempts; and (3) gradient orthogonal projection to resolve optimization conflicts and ensure training stability. This holistic design enables precise, interpretable, and robust knowledge unlearning in LLMs, transcending traditional loss-combination methods."}, {"title": "4.1. Information-Theoretic Guidance for Unlearning", "content": "In this paper, we utilize a principled approach to selective multi-domain knowledge unlearning in LLMs through mutual information. MI provides a natural measure of representational entanglement between the forget and retain datasets across model layers. By identifying layers that minimize MI, we can target unlearning interventions where forget and retain representations exhibit minimal overlap, thus preserving desired knowledge while selectively removing unwanted information.\nWe extend this measure to the multi-domain scenario where the forget set $F$ consists of multiple sub-domains $F^{1}, F^{2},..., F^{m}$. Our approach quantifies two critical relationships: (1) the interaction between each sub-domain and the retain set $R$, measured by $I\\left(F_{i}^{(l)} ; R^{(l)}\\right)$ at layer $l$, where lower values indicate reduced entanglement and thus more selective unlearning; and (2) the inter-domain dependencies captured by $I\\left(F_{i}^{(l)} ; F_{j}^{(l)}\\right)$ for sub-domains $F^{i}$ and $F^{j}(i \\neq j)$, which characterizes potential conflicts or redundancies that may impact unlearning effectiveness.\nTo quantify the overall representational conflicts between the forget and retain datasets, $I\\left(F^{(l)} ; R^{(l)}\\right)$, and the interdependence among forgettable sub-domains, $I\\left(F_{i}^{(l)} ; F_{j}^{(l)}\\right)$ at layer $l$, we define the aggregate MI as $\\hat{I}^{(l)}$:\n$\\hat{I}^{(l)}=\\sum_{i=1}^{m} I\\left(F_{i}^{(l)} ; R^{(l)}\\right)+\\eta \\sum_{i=1}^{m} \\sum_{j=i+1}^{m} I\\left(F_{i}^{(l)} ; F_{j}^{(l)}\\right)$\nwhere $m$ denotes the number of sub-domains in the forget set $F$, and $\\eta$ is a balancing coefficient that controls the relative importance of inter-domain dependencies.\nFor each layer $l$, since the activations are high-dimensional and continuous, direct entropy calculation is infeasible (Tsur et al., 2024). Instead, we utilize Kernel Density Estimation (KDE) to approximate the underlying global data distribution, estimating continuous entropy in activation space as defined in Appendix C (Walters-Williams & Li, 2009).\nWe implement KDE using a Multivariate Gaussian Kernel, which provides a smooth density estimation suitable for high-dimensional data. The estimated probability density function for activations $A$ is given by:\n$p(a)=\\frac{1}{N h} \\sum_{n=1}^{N} K\\left(\\frac{a-a_{n}}{h}\\right)$\nwhere $a \\in R^{d}$ represents a single sample from the activations $A$, including $F$ and $R$, with $d$ denoting the feature dimensionality of the activations, $N$ as the number of samples, $K(\\cdot)$ represents the kernel function and $h$ as the adaptive bandwidth calculated using Scott's rule (Scott, 2015), is defined as $h=\\sigma N^{-\\frac{1}{d+4}}$, which is particularly suitable for high-dimensional data due to its adjustment based on dimensionality. In this formula, $\\sigma$ is the standard deviation of the data. This adaptive bandwidth selection can effectively balance bias and variance, ensuring robust density estimation for diverse activation distributions (Belhaj, 2024). Furthermore, to mitigate the curse of dimensionality, we apply Principal Component Analysis (PCA) to reduce the dimensions of the activations before performing KDE (Altman & Krzywinski, 2018). The number of components is chosen to retain at least 95% of the variance in the activation data, ensuring minimal information loss while significantly lowering computational complexity.\nUsing the KDE-based entropy estimations, we approximate the overall mutual information $\\hat{I}$ at each layer based on Eq. (5). The optimal layer $l^{*}$ for unlearning is then determined by minimizing $\\hat{I}$:\n$l^{*}=\\arg \\min _{l} \\hat{I}^{(l)}$\nBy identifying the layer with the lowest MI, we locate the model region where the forget and retain datasets are least entangled, minimizing the overlap between the two types of knowledge. Concurrently, this layer exhibits higher entanglement among sub-domains within the forget set, enabling efficient updates to shared representations across forgettable"}, {"title": "4.2. Contrastive Orthogonal Unalignment", "content": "To achieve selective knowledge unlearning in LLMs, we devise Contrastive Orthogonal Unalignment through contrastive mechanisms and gradient projection to balance knowledge unlearning and retention."}, {"title": "4.2.1. CONTRASTIVE REPRESENTATION UNLEARNING", "content": "The core task of LLM unlearning is to selectively separate knowledge representations to be forgotten from those to be retained. Contrastive learning provides an effective mechanism for this task by learning discriminative representations through comparing similar and dissimilar samples. In our context, we leverage contrastive learning to maximize the distance between representations that should be forgotten while maintaining the coherence of retained knowledge.\nTo enhance unlearning, we construct Principal Offset Vectors (POV), which steer model activations away from undesired knowledge by reducing their alignment with dominant components identified via SVD in the representation space. The goal of POVs is guiding activations into less entangled subspaces, effectively separating the forget and retain sets.\nMathematically, given an activation matrix $H \\in R^{(B \\cdot L) \\times D}$, where $B$ is the batch size, $L$ the sequence length, and $D$ the hidden dimension, we perform SVD to obtain the dominant principal directions $v_{1},..., v_{K}$ corresponding to the top-$K$ singular values. The POVs $H^{+}$ is defined as:\n$H^{+}=H \\cdot\\left(I-w \\sum_{i=1}^{K} v_{i} v_{i}^{T}\\right)+r\\cdot\\left(I-w \\sum_{i=1}^{K} v_{i} v_{i}^{T}\\right)^{c},E$\nHere, $r \\in R^{D}$ is a vector initialized with random values, $w$ controls the influence of principal directions, and $I \\in R^{D \\times D}$ is the identity matrix. The term $\\epsilon$ introduces optional perturbations. The function $f(\\cdot)$ represents a generalized transformation operator like projection and nonlinear mapping, designed to enhance disentanglement and make representations more resistant to recovery.\nThis formulation ensures $H^{+}$ is directed away from dominant principal subspaces, combining deterministic guidance and randomized transformations to improve robustness. Unlike generic random vectors, POVs target multiple dominant features, enhancing resistance to adversarial prompts and improving unlearning effectiveness."}, {"title": "4.2.2. ORTHOGONALIZING GRADIENT CONFLICT", "content": "After computing the forget loss $L_{F}$ and retain loss $L_{R}$, we address the misalignment between the optimization direc-"}, {"title": "5. Experiments", "content": "To validate FALCON's effectiveness, we conduct extensive experiments to answer the following research questions:\nRQ1: Can MI guidance quantify the degree of knowledge entanglement, providing a measurable basis for parameter selection in unlearning?\nRQ2: Does the FALCON outperform state-of-the-art train-time unlearning methods in terms of both unlearning effectiveness and utility?\nRQ3: Can FALCON effectively resist recovery attempts of unlearned knowledge?"}, {"title": "5.1. Experimental Setup", "content": "To answer the above research questions, we evaluate our method on four benchmark datasets that are highly relevant to unlearning effectiveness and utility performance: WMDP (Li et al., 2024), WikiText (Merity et al., 2016), and MMLU (Hendrycks et al., 2021a). The WMDP dataset evaluates the reduction of malicious use by targeting the removal of sensitive and harmful information. WikiText benchmarks model perplexity in general-purpose language understanding, while MMLU (Massive Multitask Language Understanding) is used to evaluate model utility across diverse domains after unlearning.\nWe test FALCON on three high-quality open-source pre-trained language models: Zephyr-7B-Beta (Tunstall et al., 2023), Yi-6B-Chat (Young et al., 2024), and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), to evaluate the levels of forgetting and utility in LLMs after unlearning.\nTo ensure a fair comparison, we evaluate FALCON against several state-of-the-art training-time unlearning methods, including LLMU (Yao et al., 2023), SCRUB (Kurmanji et al., 2024), SSD (Foster et al., 2024), and RMU (Li et al., 2024). The details of each method are shown in Appendix D."}, {"title": "5.2. Mutual Information for Parameter Selection", "content": "The heatmaps of MI shown in Figure 2 illustrate the degree of knowledge entanglement between the forget sets (WMDP-Bio, WMDP-Cyber) and the retained set (WikiText-2-raw-v1) across the layers of various LLMs. MI provides a quantitative and interpretable measure to identify layers where the representations of forget and retain datasets are least intertwined, offering a principled criterion for selecting suitable layers for unlearning.\nAcross all three models (Zephyr-7B-Beta, Mistral-7B-Instruct-v0.3, and Yi-6B-Chat), lower MI values are generally concentrated in the earlier layers, indicating more domain-specific and disentangled representations, which aligns with both intuition and experimental observations (Li et al., 2024). Notably, Yi-6B-Chat exhibits more complex entanglement patterns between biological and cybersecurity domains, presenting a greater difficulty for unlearning multi-domain knowledge and making it an ideal candidate for our effectiveness analysis experiments in Section 5.3. Moreover, MI-guided parameter selection can improve computational efficiency by narrowing the parameter optimization space compared to exhaustive methods like grid search. This approach scales effectively with increasing model complexity, providing a practical guiding indicator for unlearning in modern LLM architectures."}, {"title": "Gradient Conflicts Analysis", "content": "We empirically evaluated MI's effectiveness in guiding unlearning by analyzing gradient conflicts between forget and retain objectives across different layers in Mistral-7B-Instruct-v0.3. As shown in Figure 3, layers with low MI values demonstrate significantly reduced gradient conflicts, exhibiting cosine similarity values closer to zero. This indicates that these layers' disentangled representations minimize interference between objectives, enabling more stable and efficient parameter updates. In contrast, layers with high MI values show pronounced conflicts through widely fluctuating and often negative cosine similarity values, reflecting the challenges posed by entangled representations."}, {"title": "5.3. Unlearning Effectiveness and Utility Analysis", "content": "We evaluate FALCON against state-of-the-art unlearning methods across three LLM architectures shown in Table 1 and Appendix E.1, with our evaluation focusing on three key metrics: WMDP scores for measuring unlearning effectiveness, MMLU scores for assessing general knowledge retention, and perplexity (PPL) for model stability. Our primary objective is to minimize WMDP scores while maintaining MMLU and PPL values close to the base model's performance (MMLU and PPL), as this indicates successful knowledge removal without compromising general capabilities. To ensure fair comparison, we prioritize maintaining general model utility and report each method's best unlearning performance under this setting. The experimental results demonstrate FALCON's superior performance compared to baseline methods, which often struggle to balance unlearning effectiveness with model utility and show increased uncertainty in their perplexity scores. On Zephyr-7B, FAL-"}, {"title": "5.4. Resistance Against Knowledge Recovery Attempts", "content": "We conduct extensive experiments on Yi-6B-Chat to evaluate FALCON's resistance against knowledge recovery attempts (\u0141ucki et al., 2024). Logit Lens (Patil et al., 2024), which projects intermediate activations onto the model's vocabulary space, serves as a powerful technique for probing the model's internal knowledge representations and potential recovery of unlearned information. As shown in Figure 4, the logit lens analysis across different architectural components such as MLP and attention layers demonstrates that the unlearned knowledge remains consistently inaccessible, with performance staying close to the unlearned baseline and far below the original model's performance. Additionally, as shown in Table 2, FALCON exhibits strong resilience against Enhanced GCG in QA setting, a sophisticated prefix-optimization based jailbreaking attack (Thompson & Sklar, 2024). Even with increasing attack iterations, the recovered WMDP scores remain remarkably stable near the unlearned baseline, demonstrating that our approach achieves robust unlearning by fundamentally altering the model's internal representations rather than merely achieve surface-level knowledge separation. Further evaluation using conversational templates for jailbreaking attacks (detailed in Appendix E.3) further validates our method's robustness against knowledge recovery attempts. These results across both probing techniques validate FALCON's effectiveness in creating a more permanent and recovery-resistant form of knowledge removal."}, {"title": "5.5. Ablation Study Analysis", "content": "To validate the effectiveness of FALCON's components, we conduct ablation studies on Yi-6B-Chat. The baseline demonstrates a solid performance of 27.5% on WMDP and 60.3% on MMLU. Replacing the contrastive loss with MSE loss (w/o Loss) renders unlearning ineffective, emphasizing the necessity of the contrastive mechanism for precise knowledge separation. While removing gradient projection (w/o GP) or replacing POVs with random vectors (w/o POVs) has a minor impact on unlearning but degrades knowledge retention, highlighting their critical role in preserving model utility. These results empirically confirm that each component is essential for FALCON's success in achieving precise unlearning while maintaining general model performance."}, {"title": "6. Conclusion", "content": "This paper presents FALCON, a fine-grained representation-guided framework for LLM unlearning. Leveraging mutual information guidance and contrastive orthogonal unalignment, it enables precise and efficient unlearning through principal component-based representation separation and gradient conflict resolution. Extensive experiments demonstrate its superior performance in effectively removing un-"}, {"title": "A. Information-Theoretic Metrics", "content": "Information-theoretic metrics, such as mutual information and entropy, provide a robust theoretical foundation for understanding and managing uncertainty in machine learning models, including LLMs (Malinin & Gales, 2018; Dombrowski & Corlouer, 2024). Entropy, as a measure of uncertainty, has been widely applied to assess prediction informativeness (Van Amersfoort et al., 2020), guide feature selection (Deng et al., 2022), and reduce predictive uncertainty (Malinin & Gales, 2018). In LLMs, entropy-based methods have been used to evaluate model confidence, regularize outputs, and detect hallucinations (Attanasio et al., 2022; Qiu & Miikkulainen, 2024; Farquhar et al., 2024). Similarly, mutual information quantifies shared information between variables, offering a principled approach to analyzing dependencies within model layers, improving representation learning, and understanding information propagation across deep neural networks (Gabri\u00e9 et al., 2018; Tschannen et al., 2020). In LLMs, MI has been leveraged to optimize pretraining objectives, identify task-relevant variables during fine-tuning, and improve knowledge distillation (Cha et al., 2022; Wu et al., 2024; Chen et al., 2024). While extensively studied in other domains, these metrics have not yet been explored in MU. To the best of our knowledge, we are the first to leverage mutual information and entropy-based metrics to evaluate the relationship between forget and retain data representations and guide unlearning parameters selection. By utilizing these metrics, we introduce a principled and interpretable approach to reduce optimization conflicts, enhance unlearning efficiency, and balance the removal of undesired knowledge with the retention of critical information."}, {"title": "B. Contrastive Learning & Gradient Projection", "content": "Contrastive learning has emerged as a key technique for representation learning, leveraging the principle of maximizing similarity between positive pairs while minimizing it for negative pairs (Hu et al., 2024). It has shown success in self-supervised learning, feature disentanglement, and robustness improvement in deep neural networks (Ericsson et al., 2022; Kahana & Hoshen, 2022; Wang et al., 2024). Recent works have explored its application in MU, where it is used to suppress target representations while preserving critical functionality (Kim & Woo, 2022b; Yang & Li, 2023). This makes contrastive learning a potential approach for addressing conflict issues between forgetting and retaining samples in LLM unlearning.\nGradient projection, on the other hand, addresses optimization conflicts by projecting gradients onto feasible directions aligned with Pareto-optimal solutions (Iskander et al., 2023). It has been successfully applied to multi-objective tasks and continual learning, effectively achieving gradient equilibrium and ensuring stable updates (Chen et al., 2022; Zhang et al., 2024c). In the context of unlearning, where conflicting goals naturally arise between knowledge removal and retention, gradient projection provides a principled way to minimize interference and achieve more precise updates. Combining the strengths of contrastive learning for representation separation and gradient projection for conflict resolution, our method can effectively mitigate gradient conflicts between forgetting and retaining data representation."}, {"title": "C. Preliminary", "content": "In this section, we present the foundational concepts of continuous and joint entropy, which serve as the theoretical underpinnings for quantifying knowledge entanglement in our unlearning framework. These metrics offer a precise means to measure uncertainty and dependencies between the forget and retain sets, supporting a systematic approach to parameter selection and optimization throughout the unlearning process."}, {"title": "C.1. Continuous Entropy", "content": "The concept of entropy in the continuous setting, often referred to as differential entropy, measures the uncertainty of a continuous random variable (Garbaczewski, 2006; Yeung & Yeung, 2008). For a random variable F with probability density function p(F), the entropy H(F) is defined as:\n$H(F)=-\\int p(F) \\log p(F) d F$\nwhere p(F) is the probability density of the activations F over its support. Similarly, the entropy H(R) of the retain set activations R is defined in the same manner."}, {"title": "C.2. Joint Entropy", "content": "To quantify the combined uncertainty of the activations F and R, the joint entropy H(F, R) is introduced, which is defined as:\n$H(F, R)=-\\int \\int p(F, R) \\log p(F, R) d F d R$\nwhere p(F, R) represents the joint probability density function of the activations F and R in continuous space. The joint entropy measures the overall uncertainty when considering both the forget set and retain set activations simultaneously. In the context of mutual information, the joint entropy H(F, R) acts as a correction term, accounting for the overlap or dependency between the two distributions."}, {"title": "D. Implementation Details", "content": "This section details the experimental settings, hyperparameters, and method configurations. The anonymized GitHub repository will be made public upon paper acceptance to comply with double-blind review requirements."}, {"title": "D.1. LLMU", "content": "Following RMU (Li et al., 2024), we made several modifications to LLMU (Yao et al., 2023) to better align it with our tasks. Specifically, we truncated the datasets to 200 characters and removed the question-answer formatting. Additionally, we trained LLMU using LORA (Hu et al., 2022) with a rank of 32 and a scaling factor of 16. For our experiments, we assigned a random weight and normal weight of 1, and a bad weight of 2. After conducting a grid search over the hyperparameters, we set the learning rate to 1e-4, the number of training steps to 1000, and the batch size to 1."}, {"title": "D.2. SCRUB", "content": "We adapted the Scalable Remembering and Unlearning unBound (SCRUB) (Kurmanji et al., 2024) framework to align with our tasks. Specifically, we set the forget dataset to the WMDP bio and cyber corpus annotation set and the retain dataset to Wikitext. SCRUB was trained using the Adam optimizer with a weight decay of 0.01 and a learning rate of 1e-4. We employed log perplexity on Wikitext as the task-specific loss. Besides, to balance the loss weightings between knowledge distillation and the task-specific loss, we tuned the \\alpha hyperparameter with values $[1 \\times 10^{-4}, 1 \\times 10^{-3}, 1 \\times 10^{-2}, 1 \\times 10^{-1}, 1, 10]$."}, {"title": "D.3. SSD", "content": "We adapted the Selective Synaptic Dampening (Foster et al., 2024) method to make it suitable for large language models. Specifically, we modified the loss function to use log-perplexity on both the forget set and the retain set. Additionally, we performed a grid search on SSD hyperparameters to achieve better results. The grid search included thresholds of [0.1,0.5,1.0,5.0] and dampening constants of $[1 \\times 10^{-4}, 1 \\times 10^{-3}, 1 \\times 10^{-2}, 1 \\times 10^{-1}, 1]$."}, {"title": "D.4. RMU", "content": "For RMU implementation, our parameter selection was followed by both Li et al.'s empirical findings (Li et al., 2024) and our mutual information visualization results, which consistently indicated layer l = 7 as optimal for minimizing parameter entanglement. Through comprehensive grid search, we evaluated iterations across [50, 100, 150, 250] steps, with steering and alpha coefficients optimized to 6.5 and 1150 for Zephyr-7B, and 40 and 200 for Yi-6B respectively. Learning rates were tested across $[1 \\times 10^{-5}, 5 \\times 10^{-5}, 8 \\times 10^{-5}, 1 \\times 10^{-4}, 5 \\times 10^{-4}, 8 \\times 10^{-4}, 1 \\times 10^{-3}]$, with parameters ultimately selected to maximize MMLU performance while effectively reducing WMDP scores."}, {"title": "D.5. FALCON", "content": "For FALCON's implementation, we maintained comparable learning rate ranges and number of iterations to RMU. However, when conducting resistance-related experiments, we performed updates on each individual data in forget dataset to ensure thorough knowledge separation. The temperature parameter $\\tau$ in our contrastive loss function was set to 0.7. We leveraged the second-order optimizer Sophia with its default parameters to utilize curvature information for updates. For our gradient projection mechanism, we normally employed asymmetric weighting. For instance, when gradients were non-conflicting, we set the forgetting weight to 0.8 and retention weight to 1.2; in cases of gradient conflict, these values were adjusted to 0.2"}, {"title": "E. Experiments", "content": "Due to space constraints in the main text, we present additional experimental results on the Mistral-7B-Instruct-v0.3 model in Table 4. Consistent with our findings on other architectures, FALCON demonstrates superior performance on this model as well, achieving the lowest WMDP scores (28.0 for Bio and 24.3 for Cyber domains) while maintaining strong MMLU performance (57.9) and model stability (PPL of 1.4). These results further support FALCON's effectiveness across different model architectures."}, {"title": "E.1. unlearning effectiveness and utility results for Mistral-7B", "content": "Where cos(\u00b7) < 0 indicates opposing directions, signifying a conflict between the two objectives. To mitigate this conflict, we adjust the gradients by projecting one onto the orthogonal complement of the other. Specifically, if cos(\u00b7) < 0, we project \u2207LF onto the subspace orthogonal to \u2207LR:\n$\u2207_{L_{F}}^{proj} = \u2207_{L_{F}} - \\frac{\u2207_{L_{F}} \u00b7 \u2207_{L_{R}}}{||\u2207_{L_{R}}||^2} \u2207_{L_{R}}$\nThis adjustment ensures that $\u2207_{L_{F}}^{proj}$ is orthogonal to \u2207LR, eliminating interference from the retain objective during the update for the forget objective. Once the gradients are adjusted, the final update direction of the FALCON is determined by combined gradients:\n$\u2207L_{FALCON} = \u03b1\u2207_{L_{F}}^{proj} + \u03b2\u2207_{L_{R}}$\nwhere \u03b1 and \u03b2 are hyperparameters balancing the contributions of the forget and retain objectives."}, {"title": "5.5. Ablation Study Analysis", "content": "To ensure consistent scaling", "similarity": "n$S^+ = \\sum_{d=1"}, {"H_a[d": "H^+ [d"}, {"H_a[d": "H_z [d", "objective": "n$L_{F} = \\frac{1}{|B|} \\sum_{b=1}^{|B|} \\log \\frac{\\exp (S^+ / \u03c4)}{\\exp (S^+ / \u03c4) + \\sum_{z=1}^{Z} \\exp (S^- / \u03c4)}$\nwhere \u03c4 is a temperature scaling parameter. This loss encourages the updated model's representations to align with the Principal Offset Vectors while diverging from the frozen model's representations of undesired knowledge. By leveraging both directional guidance through Principal Offset Vectors and contrastive learning, our approach achieves more precise and efficient representation unalignment in activation space.\nIn addition to unlearning undesired representations, it is essential to preserve critical knowledge required for downstream tasks. To achieve this, we define a retain loss that measures the alignment between the updated model's activations ($H"}]}