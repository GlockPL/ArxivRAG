{"title": "A Walsh Hadamard Derived Linear Vector Symbolic Architecture", "authors": ["Mohammad Mahmudul Alam", "Alexander Oberle", "Edward Raff", "Stella Biderman", "Tim Oates", "James Holt"], "abstract": "Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in Rd are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems. Code is available at https://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding.", "sections": [{"title": "Introduction", "content": "Vector Symbolic Architectures (VSAs) are a unique approach to performing symbolic style AI. Such methods use a binding operation B : Rd \u00d7 Rd \u2192 Rd, where B(x, y) = z denotes that two concepts/vectors x and y are connected to each other. In VSA, any arbitrary concept is assigned to vectors in Rd (usually randomly). For example, the sentence \u201cthe fat cat and happy dog\" would be represented as B(fat, cat) + B(happy, dog) = S. One can then ask, \u201cwhat was happy\u201d by unbinding the vector for happy, which will return a noisy version of the vector bound to happy. The unbinding operation is denoted B* (x, y), and so applying B*(S, happy) \u2248 dog.\nBecause VSAs are applied over vectors, they offer an attractive platform for neuro-symbolic methods by having natural symbolic AI-style manipulations via differentiable operations. However, current VSA methods have largely been derived for classical AI tasks or cognitive science-inspired work. Many such VSAs have shown issues in numerical stability, computational complexity, or otherwise lower-than-desired performance in the context of a differentiable system.\nAs noted in [39], most VSAs can be viewed as a linear operation where B(a,b) = a Gb and B*(a,b) = a Fb, where G and F are d \u00d7 d matrices. Hypothetically, these matrices could be learned via gradient descent, but would not necessarily maintain the neuro-symbolic properties of VSAs without additional constraints. Still, the framework is useful as all popular VSAs we are aware fit within this framework. By choosing G and F with specified structure, we can change the computational complexity from O(d\u00b2), down to O(d) for a diagonal matrix.\nIn this work, we derive a new VSA that has multiple desirable properties for both classical VSA tasks, and in deep-learning applications. Our method will have only O(d) complexity for the binding step, is numerically stable, and equals or improves upon previous VSAs on multiple recent deep"}, {"title": "Related Work", "content": "Smolensky [38] started the VSA approach with the Tensor Product Representation (TPR), where d dimensional vectors (each representing some concept) were bound by computing an outer product. Showing distributivity (B(x, y + z) = B(x, y) + B(x, z)) and associativity, this allowed specifying logical statements/structures [13]. However, for p total items to be bound together, it was impractical due to the O(dp) complexity. [36, 25, 24] have surveyed many of the VSAs available today, but our work will focus on three specific alternatives, as outlined in Table 1. The Vector-Derived Transformation Binding (VTB) will be a primary comparison because it is one of the most recently developed VSAs, which has shown improvements in what we will call \"classic\" tasks, where the VSA's symbolic like properties are used to manually construct a series of binding/unbinding operations that accomplish a desired task. Note, that the VTB is unique in it is non-symmetric (B(x,y) \u2260 B(y, x)). Ours, and most others, are symmetric.\nNext is the Holographic Reduced Representation (HRR) [32], which can be defined via the Fourier transform F(.). One derives the inverse operation of the HRR by defining the one vector 1 as the identity vector and then solving F(a*)\u00bfF(a); = 1. We will use a similar approach to deriving HLB but replacing the Fourier Transform with the Hadamard transform, making the HRR a key baseline. Last, the Multiply Add Permute (MAP) [10] is derived by taking only the diagonal of the tensor product from [38]'s TPR. This results in a surprisingly simple representation of using element-wise multiplication for both binding/unbinding, making it a key baseline. The MAP binding is also notable for its continuous (MAP-C) and binary (MAP-B) forms, which will help elucidate the importance of the difference in our unbinding step compared to the initialization avoiding values near zero. HLB differs in devising for the unbinding step, and we will later show an additional corrective term that HLB employs for p different items bound together, that dramatically improve performance.\nOur motivation for using the Hadamard Transform comes from its parallels to the Fourier Transform (FT) used to derive the HRR and the HRR's relatively high performance. The Hadamard matrix has a simple recursive structure, making analysis tractable, and its transpose is its own inverse, which simplifies the design of the inverse function B*. Like the FT, WHT can be computed in log-linear time, though in our case, the derivation results in linear complexity as an added benefit. The WHT is already associative and distributive, making less work to obtain the desired properties. Finally, the WHT involves only {-1, 1} values, avoiding numerical instability that can occur with the HRR/FT."}, {"title": "Methodology", "content": "First, we will briefly review the definition of the Hadamard matrix H and its important properties that make it a strong candidate from which to derive a VSA. With these properties established, we will begin by deriving a VSA we term HLB where binding and unbinding are the same operation in the same manner as which the original HRR can be derived [32]. Any VSA must introduce noise when vectors are bound together, and we will derive the form of the noise term as \u03b7\u00b0. Unsatisfied with the magnitude of this term, we then define a projection step for the Hadamard matrix in a similar spirit to '[9]'s complex-unit magnitude projection to support the HRR and derive an improved operation with a new and smaller noise term \u03b7\". This will give us the HLB bind/unbind steps as noted in Table 1.\nHadamard Ha is a square matrix of size d \u00d7 d of orthogonal rows consisting of only +1 and -1s given in Equation 1 where d = 2n \u2200n \u2208N: n \u2265 0. Bearing in mind that Hadamard or Walsh-Hadamard Transformation (WHT) can be equivalent to discrete multi-dimensional Fourier Transform (FT) when applied to a d dimensional vector [26], it has additional advantages over Discrete Fourier Transform (DFT). Unlike DFT, which operates on complex C numbers and requires irrational multiplications, WHT only performs calculations on real R numbers with addition and subtraction operators and does not require any irrational multiplication.\nH\u2081 = \\begin{bmatrix}1\\end{bmatrix} H2 = \\begin{bmatrix}1 & 1\\\\ 1 & -1\\end{bmatrix} H2n = \\begin{bmatrix}H2n-1 & H2n-1\\\\ H2n-1 & -H2n-1\\end{bmatrix} (1)\nVector symbolic architectures (VSA), for instance, Holographic Reduced Representations (HRR) em- ploys circular convolution to represent compositional structure which is computed using Fast Fourier Transform (FFT) [32]. However, it can be numerically unstable due to irrational multiplications of complex numbers. Prior work [9] devised a projection step to mitigate the numerical instability of the FFT and it's inverse, but we instead ask if re-deriving the binding/unbinding operations may yield favorable results if we use the favorable properties of the Hadamard transform as given in Lemma 3.1.\nThe bound composition of two vectors into a single vector space is referred to as BINDING. The knowledge retrieval from a bound representation is known as UNBINDING. We define the binding function by replacing the Fourier transform in circular convolution with the Hadamard transform given in Definition 3.1. We will denote the binding function four our specific method by B and the unbinding function by B*.\nDefinition 3.1 (Binding and Unbinding). The binding of vectors x, y \u2208 Rd in Hadamard domain is defined in Equation 2 where \u00a9 is the elementwise multiplication. The unbinding function is defined in a similar fashion, i.e., B = B*. In the context of binding, B(x, y) combines the vectors x and y, whereas in the context of unbinding B* (x, y) refers to the retrieval of the vector associated with y from x.\nB(x,y) = \\frac{1}{d} \\cdot H(Hx\u00a9 Hy) (2)"}, {"title": "Definition", "content": "Now, we will discuss the binding of p different representations, which will become important later in our analysis but is discussed here for adjacency to the binding definition. Composite representation in vector symbolic architectures is defined by the summation of the bound vectors. We define a parameter \u03c1\u03b5 \u039d : \u03c1 \u2265 1 that denotes the number of vector pairs bundled in a composite representation. Given vectors xi, Yi \u2208 Rd and \u2200i\u2208N:1 \u2264 i \u2264 p, we can define the composite representation X as\nX = B(x1,y1) X = B(x1,y1) + B(x2,y2) X\u03c1 = \u2211 \u0392(Xi, Yi) (3)\n\u03c1=1 \u03c1=2 i=1\nNext, we require the unbinding operation, which is defined via an inverse function via the following theorem. This will give a symbolic form of our unbinding step that retrieves the original component x being searched for, as well as a necessary noise component n\u00b0, which must exist whenever p \u2265 2 items are bound together without expanding the dimension d.\nTheorem 3.1 (Inverse Theorem). Given the identity function Hx \u2022 Hx\u2020 = 1 where x\u2020 is the inverse of x in Hadamard domain, then B* (B(x1, y1) + \u00b7\u00b7\u00b7 + B(x, y), y) = \\begin{cases} xi & \\text{if} p = 1\\\\ xi + ni & \\text{else} p > 1\\end{cases} where xi, Yi \u2208 Rd and n; is the noise component.\nProof of Theorem 3.1. We start from the identity function Hx \u2022 Hx\u2020 = 1 and thus Hx\u2020 = \\frac{1}{Hx}. Now using Equation 2 we get,\nB* (B(x1,y1) + \u00b7\u00b7\u00b7 + B(xp, Yp),y) = \\frac{1}{d} H((Hx1 \u00a9 Hy1+\u06f0\u06f0\u06f0 + Hx, \u00a9 Hyp) \u00a9 \\frac{1}{Hyi} ) = \\frac{1}{d} H(Hxi \u00a9 Hyi + \u2211(HxHyj))Lemma 3.1 \\frac{1}{Hyi} = \\begin{cases} xi & \\text{if} p = 1\\\\ xi + ni & \\text{else} p > 1\\end{cases} .\nj=1\nj\u2260i\nTo reduce the noise component and improve the retrieval accuracy, [9, 32] proposes a projection step to the input vectors by normalizing them by the absolute value in the Fourier domain. While such identical normalization is not useful in the Hadamard domain since it will only transform the elements to +1 and -1s, we will define a projection step with only the Hadamard transformation without normalization given in Definition 3.2.\nDefinition 3.2 (Projection). The projection function of x is defined by \u03c0(x) = \\frac{1}{d} \u2022 Hx.\nIf we apply the Definition 3.2 to the inputs in Theorem 3.1 then we get\n\u0392* (\u0392(\u03c0(x1), \u03c0(Y1)) + \u00b7\u00b7\u00b7 + B(\u03c0(x\u03c1), \u03c0(\u03b3\u03c1)), \u03c0(yi)\u2020) = \u0392* \\frac{1}{d}H(x1 \u00a9 y1 + ...xp \u00a9yp)),\\frac{1}{Yi} = \\frac{1}{d}H((x1 \u00a9 y1 + ... \u00a9 xp \u00a9 yp)) (4)\nThe retrieved value would be projected onto the Hadamard domain as well and to get back the original data we apply the reverse projection. Since the inverse of the Hadamard matrix is the Hadamard matrix itself, in the reverse projection step we just apply the Hadamard transformation again which derives the output to\n\\frac{1}{d} H*\\frac{1}{d} H((X1 \u00a9 Y1 + \u06f0\u06f0\u06f0 x, \u00a9 yp)) = \\begin{cases} xi & \\text{if} p = 1\\\\ xi +  & \\text{else} p > 1\\end{cases}\nj=1, j\u2260i\\frac{\u2211pj=1, j\u2260iXjYj}{Yi}(5)\nwhere n is the noise component due to the projection step. In expectation, \u03b7 < n (see Appendix A). Thus, the projection step diminishes the accumulated noise. More interestingly, the retrieved output"}, {"title": "Initialization of HLB", "content": "term does not contain any Hadamard matrix. Therefore, we can recast the initial binding definition by multiplying the query vector y\u2081 to the output of Equation 5 which makes the binding function as the sum of the element-wise product of the vector pairs and the compositional structure a linear time O(n) representation. Thus, the redefinition of the binding function is B'(x, y) = x \u2299 y and p bundle of the vector pairs is xp = \u2211i=1(xi \u2299 yi). Consequently, the unbinding would be a simple element-wise division of the bound representation by the query vector, i.e, B*\u2032(x, y) = \\frac{x}{y} where x and y are the bound and query vector, respectively.\nFor the binding and unbinding operations to work, vectors need to have an expected value of zero. However, since we would divide the bound vector with query during unbinding, values close to zero would destabilize the noise component and create numerical instability. Thus, we define a Mixture of Normal Distribution (MiND) with an expected value of zero but an absolute mean greater than zero given in Equation 6 where U is the Uniform distribution. Considering half of the elements are sampled for a normal distribution of mean \u2013\u00b5 and the rest of the half with a mean of \u00b5, the resulting vector has a zero mean with an absolute mean of \u00b5. The properties of the vectors sampled from a MiND distribution are given in Properties 3.1.\n\u03a9(\u03bc, 1/d) = \\begin{cases}N(\u2212\u03bc, 1/d) & \\text{if} U(0, 1) > 0.5\\\\(N(\u03bc, 1/d) & \\text{else} U(0, 1) \u2264 0.5\\end{cases} (6)\nProperties 3.1 (Initialization Properties). Let x \u2208 Rd sampled from \u03a9(\u03bc, 1/d) holds the following properties. E[x] = 0, E[|x|] = \u03bc, and ||x||2 = \u221a\u00b5\u00b2d"}, {"title": "Similarity Augmentation", "content": "In VSAs, it is common to measure the similarity with an extracted embedding 2 with some other vector x using the cosine similarity. For our HLB, we devise a correction term when it is known that p items have been bound together to extract \u00ee\u00e2, i.e., B*(xp, z) = x. Then if x is the noisy version of the true bound term \u00e6, we want cossim(x, x) = 1, and cossim(x, y) = 0, \u2200y \u2260 x. We achieve this by instead computing cossim(x, x) \u00b7 \u221ap, and the derivation of this corrective term is given by Theorem 3.2.\nGiven xi, Yi ~ \u03a9(\u03bc, 1/d) Vi\u2208N:1\u2264 i \u2264 p, the cosine Theorem 3.2 (\u03a6 \u2013 p Relationship).\nsimilarity \u03a6 between the original x\u2081 and retrieved vector \u00ee\u00bf is approximately equal to the inverse square root of the number of vector pairs in a composite representation p given by \u03a6 \u2248 \\frac{1}{\u221aP}.\nProof of Theorem 3.2. We start with the definition of cosine similarity and insert the value of x. The step-by-step breakdown is shown in Equation 7.\n\u03a6 = \\frac{\u2211dixi2i}{||Xi||2. ||Xi + \u2211j=1, j\u2260i xjYj}|| = \\frac{\u2211dixi(xi + \u2211j=1, j\u2260i xjYj)}{||Xi||2. ||Xi + \u2211j=1, j\u2260i xjYj||} = \\frac{\u2211dixi2 + \u2211dixi(\u2211j=1, j\u2260i xjYj)}{||Xi||2. ||Xi + \u2211j=1, j\u2260i xjYj||}(7)\nEmploying Properties 3.1 we can derive that ||xi||2 = \u221a2xi\u00b7xi = \u221a\u00b5\u00b2d and || \u2211j=1, j\u2260i xjYj|| = \u221a\u00b5\u00b2d.\nThus, the square of the ||x + \u2211j=1, j\u2260i \\frac{xjYj}{Yi}||2 can be expressed as\n||xi + \u2211j=1, j\u2260i \\frac{xjYj}{Yi}||2 = \u2211 \u2211 + \u2211 + 2\u2211xixi = \u03bc\u00b2d + (p \u2212 1) \u00b7 \u03bc\u00b2d + 2\u03b1 + 2\u03b2 = \u03c1\u00b7 \u03bc\u00b2d + 2\u03b1 + 2\u03b2 (8)\nj=1, j\u2260i j=1 l=1\\frac{\u2211XjYjX1Y1 }{YiYi j\u2260i l\u2260j}\nTherefore, using Equation 7 and Equation 8 we can write that\nE[\u03a6] = \\frac{\u03bc\u00b2d + \u03b1}{\u221a\u03bc\u00b2d\u00b7 \u221a\u03c1\u00b7 \u03bc\u00b2d + 2\u03b1 + 2\u03b2} \u22481 \u221a\u03c1 (9)"}, {"title": "Classical VSA Tasks", "content": "A common VSA task is, given a bundle (addition) of p pairs of bound vectors s = \u2211=1B(xi, Yi), given a query xq \u2208 s, c can the corresponding vector yq be correctly retrieved from the bundle. To test this, we perform an experiment similar to one in [37]. We first create a pool P of N = 1000 random vectors, then sample (with replacement) p pairs of vectors for p \u2208 {1,2,\u2026\u2026,25}. The pairs are bound together and added to create a composite representation s. Then, we iterate through all left pairs xq in the composite representation and attempt to retrieve the corresponding yq, \u2200q\u2208 [1,p]. A retrieval is considered correct if B* (s, xq)Tyq > B* (s,xq)Tyj,\u2200j \u2260 q. The total accuracy score for the bundle is recorded, and the experiment is repeated for 50 trials. Experiments are performed to"}, {"title": "Deep Learning with Hadamard-derived Linear Binding", "content": "Two recent methods that integrate HRR with deep learning are tested to further validate our approach and briefly summarized in the two sub-sections below. In each case, we run all four VSAs and see that HLB either matches or exceeds the performance of other VSAs. In every experiment, the standard method of sampling vectors from each VSA is followed as outlined in Table 1. All the experiments are performed on a single NVIDIA TESLA PH402 GPU with 32GB memory."}, {"title": "Connectionist Symbolic Pseudo Secrets", "content": "When running on low-power computing environments, it is often desirable to offload the computation to a third-party cloud environment to get the answer faster and use fewer local resources. However, this may be problematic if one does not fully trust the available cloud environments. Homomorphic encryption (HE) is the ideal means to alleviate this problem, providing cryptography for computations. HE is currently more expensive to perform than running a neural network itself [11], defeating its own utility in this scenario. Connectionist Symbolic Pseudo Secrets (CSPS) [3] provides a heuristic means of obscuring the nature of the input (content), and output (number of classes/prediction), while also reducing the total local compute required.\nCSPS mimics a \u201cone-time-pad\u201d by taking a random VSA vector s as the secret and binding it to the input \u00e6. The value B(s, \u00e6) obscures the original \u00e6, and the third-party runs the bulk of the network on their platform. A result \u1ef9 is returned, and a small local network computes the final answer after unbinding with the secret B* (\u1ef9, s). Other than changing the VSA used, we follow the same training, testing, architecture size, and validation procedure of [3].\nCSPS experimented with 5 datasets MNIST, SVHN, CIFAR-10 (CR10), CIFAR-100 (CR100), and Mini-ImageNet (MIN). First, we look at the accuracy of each method, which is lower due to the noise of the random vector s added at test time since no secret VSA is ever reused. The results are shown in Table 2, where HLB outperforms all prior methods significantly. Notably, the MAP VSA is second best despite being one of the older VSAs, indicating its similarity to HLB in using a simple binding procedure, and thus simple gradient may be an important factor in this scenario."}, {"title": "Xtreme Multi-Label Classification", "content": "Extreme Multi-label (XML) is the scenario where, given a single input of size d, C >> d classes are used to predict. This is common in e-commerce applications where new products need to be tagged, and an input on the order of d \u2248 5000 is relatively small compared to C \u2265 100,000 or more classes. This imposes unique computational constraints due to the output space being larger than the input space and is generally only solvable because the output space is sparse often less than 100 relevant classes will be positive for any one input. VSAs have been applied to XML by exploiting the low positive class occurrence rate to represent the problem symbolically [9].\nWhile many prior works focus on innovative strategies to cluster/make hierarchies/compress the penultimate layer[19, 20, 31, 18, 44, 22], a neuro-symbolic approach was proposed by [9]. Given K total possible classes, they assigned each class a vector ck to be each class's representation, and the set of all classes a = k=1 Ck\nK\nThe VSA trick used by [9] was to define an additional \"present\" class p and a \u201cmissing\" class m. Then, the target output of the network f(\u00b7) is itself a vector composed of two parts added together. First, B(p, \u03a3k ck) represents all present classes, and so the sum is over a finite smaller set. Then the absent classes compute the missing representing B(m, a \u2212 \u2211k ck), which again only needs to compute over the finite set of present classes, yet represents the set of all non-present classes by exploiting the symbolic properties of the VSA.\nFor XML classification, we have a set of K classes that will be present for a given input, where K\u2248 10 is the norm. Yet, there will be L total possible classes where L \u2265 100, 000 is quite common. Forming a normal linear layer to produce L outputs is the majority of computational work and memory use in standard XML models, and thus the target for reduction. A VSA can be used to side-step this cost, as shown by [9], by leveraging the symbolic manipulation of the outputs. First, consider the target label as a vector s \u2208 Rd such that d \u00ab L. By defining a VSA vector to represent \"present\" and \"missing\u201d classes as p and m, where each class is given its own vector C1,...,L, we can shift the computational complexity form O(L) to O(K) by manipulating the \u201cmissing\u201d classes as the compliment of the present classes as shown in Equation 10."}, {"title": "Conclusion", "content": "In this paper, a novel linear vector symbolic architecture named HLB is presented derived from Hadamard transform. Along with an initialization condition named MiND distribution is proposed for which we proved the cosine similarity \u03a6 is approximately equal to the inverse square root of the no. of bundled vector pairs p which matches with the experimental results. The proposed HLB showed superior performance in classical VSA tasks and deep learning compared to other VSAs such as HRR, VTB, and MAP. In learning tasks, HLB is applied to CSPS and XML classification tasks. In both of the tasks, HLB has achieved the best results in terms of respective metrics in all the datasets showing a diverse potential of HLB in Neuro-symbolic AI."}, {"title": "Noise Decomposition", "content": "When a single vector pair is combined, one of the vector pairs can be exactly retrieved with the help of the other component and the inverse function, recalling the retrieved output does not contain any noise component for a single pair of vectors, i.e., p = 1. However, when more than one vector pairs are bundled, noise starts to accumulate. In this section, we will uncover the noise components accumulated with and without the projection to the inputs and analyze their impact on expectation. We first start with the noise component without the projection step n.\nni = \\frac{1}{d} Hyi \u2211(HxHyj) (12)\nj=1\nj\u2260i\nLet, set the value of n to be 1 thus, d = 2n = 2 and the number of vector pairs p = 2, i.e., \u03a7\u03c1=2 = B(x1, y1) + B(X2,Y2). We want to retrieve x\u2081 using the query y\u2081, thereby, the expression of n is uncovered step by step for p = 2 shown in Equation 13.\nni = \\frac{1}{d}(Hy1 (Hx2 Hy2)) = \\frac{1}{\u221ad. H (\u221ax1 \u00d72 \u00d72 y2 )) = 1 \\frac{1}{\u221ad\u221ax1 \u00d72 + \u221a \u221ay2 )} d \\frac{1}{d} \u00d72 \u2212 \u00d72 y2 + \u00d72 y2 ) = \\frac{1}{d} \u00d72 \u00d72 13\nHere, Ok \u2200 k \u2208 N : 1 \u2264 k \u2264 d are the polynomials comprises of (x2, y2), and the query vector y1.\nP is the vector of polynomials consisting of 4k. From the noise expression, we can observe that the numerator is a polynomial and the denominator is the product of all the elements of the Hadamard transformation of the query vector. This is true for any value of n and p. Thus, in general, for any query y\u2081 we can express ni as shown in Equation 14.\nni = \\frac{P(X, Y, Yi)}{\u220fj=1, j\u2260i(Hy)k} k (14)\nThe noise accumulated after applying the projection to the inputs is quite straightforward as given in Equation 15.\nni = \u2211(()() )Yi Yj Yi Yi (Y)\n(15)\nAlthough the vectors xi, Yi Vi\u2208N: 1\u2264 i \u2264 pare sampled from a MiND with an expected value of 0 given in Equation 6, the sample mean of xi or yi would be \u00fb \u2248 0 but \u00fb \u2260 0. Both the numerator of n and n are the polynomials thus the expected value would be very close to 0. However, the expected value of the denominator of n would be \u0395[\u220f\u00b2=1(Hyi)k] = \u041f\u0430=1E[(Hyi)k] = \u00fbd whereas the expected value of the denominator of \u03b7 is E[yi] = \u03bc. Since, \u00fbd < \u00fb, hence, in"}, {"title": "Norm Relation", "content": "Theorem B.1 (\u03a7\u03c1 \u2013 \u03c1 Relationship). Given xi, Yi ~ \u03a9(\u03bc, 1/d) \u2208 Rd \u2200i\u2208N : 1 \u2264 i \u2264 p, the norm of the composite representation Xp is proportional to \u221ap and approximately equal to the \u00b5\u00b2\u221a\u03c1\u00b7 d.\nGiven Xp is the composite representation of the bound vectors, i.e., the Proof of Theorem B.1.\nsummation of p no. of individual bound terms. First, let's compute the norm of the single bound term as shown in Equation 16.\n||B(xi, Yi)||2 = ||Xi\u00b7Yi||2 = \u221a(x(1)y(1))2 + (x 2) y(2))2 + ... + (x(d) y(d))2 = \u221a(\u00b1\u03bc\u00b2)2 + (\u00b1\u03bc\u00b2)2 + \u00b7\u00b7\u00b7 + (\u00b1\u03bc2)2 [E[x(1)] \u00b7 E[y(1)] = \u00b1\u03bc\u00b7 \u00b0\u03bc = \u00b1\u03bc\u00b2] = \u221a \u03bc\u03b1 (16)\nNow, let's expand and compute the square norm of the composite representation given in Equation 17.\n||\u03a7\u03c1||2 = ||B(X1,Y1) + B(X2,Y2) + \u00b7\u00b7\u00b7 + B(xp, Yp) ||2 = ||B(x1,y1) ||2 + ||B(x2, y2) ||12 + \u00b7\u00b7\u00b7 + ||B(x\u03c1,Yp) ||2 + \u03be where & is the rest of the terms of square expansion. = \u03bc\u2074d + \u03bc\u2074d + \u00b7\u00b7\u00b7 + \u03bc\u2260d + \u03be = \u03c1\u00b7 \u03bc\u2074d + \u03be ||\u03a7\u03c1||2 = \u221a \u03c1\u00b7 \u03bc\u00b2d + \u03be \u2248\u221a\u03c1\u00b7\u03bcd [gis the noise term and discarded to make an approximation ] (17)"}, {"title": "Cosine Relation", "content": "Theorem 3.2 shows how the cosine similarity \u03a6 between the original x\u2081 and retrieved vector xi is approximately equal to the inverse square root of the number of vector pairs in a composite representation p. In this section, we will perform an empirical analysis of the theorem and compare it with the theoretical results. For p = {1,2,\u2026\u2026,50}, similarity score \u03a6 is calculated for vector dimension d = 512. Additionally, the theoretical cosine similarity score is also calculated using the value of \u03a6 following the theorem."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: The paper claims to present a new Hadamard-derived linear vector symbolic architecture in the abstract and introduction which accurately reflects the contribution and scope of the paper.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: Limitations are described in the paper.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?"}, {"title": "Guidelines", "content": "Answer: [Yes", "nJustification": "All the theoretical results and proofs are provided in the Methodology section.\n(See section 3)\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems", "Reproducibility\nQuestion": "Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes"}, {"nJustification": "The experimental setup is based on previous papers. All the self-used parameters are discussed in the paper.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments", "reviewers": "Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n(c) If the contribution is a new model (e"}]}