{"title": "AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants", "authors": ["Pascal J. Sager", "Benjamin Meyer", "Rebekka von Wartburg-Kottler", "Peng Yan", "Layan Etaiwi", "Aref Enayati", "Gabriel Nobel", "Ahmed Abdulkadir", "Benjamin F. Grewe", "Thilo Stadelmann"], "abstract": "Instruction-based computer control agents (CCAs) execute complex action sequences on personal computers or mobile devices to fulfill tasks using the same graphical user interfaces as a human user would, provided instructions in natu-ral language. This review offers a comprehensive overview of the emerging field of instruction-based computer control, examining available agents their tax-onomy, development, and respective resources and emphasizing the shift from manually designed, specialized agents to leveraging foundation models such as large language models (LLMs) and vision-language models (VLMs). We formal-ize the problem and establish a taxonomy of the field to analyze agents from three perspectives: (a) the environment perspective, analyzing computer envi-ronments; (b) the interaction perspective, describing observations spaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard actions, exe-cutable code); and (c) the agent perspective, focusing on the core principle of how an agent acts and learns to act. Our framework encompasses both special-ized and foundation agents, facilitating their comparative analysis and revealing how prior solutions in specialized agents, such as an environment learning step, can guide the development of more capable foundation agents. Additionally, we review current CCA datasets and CCA evaluation methods and outline the chal-lenges to deploying such agents in a productive setting. In total, we review and classify 86 CCAs and 33 related datasets. By highlighting trends, limitations, and future research directions, this work presents a comprehensive foundation to obtain a broad understanding of the field and push its future development.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning (Schmidhuber, 2015) has surpassed the point of enabling useful AI agents (Wei et al, 2022b; Zhuge et al, 2023) in several domains. Unlike other deep learning systems (LeCun et al, 2015; Stadelmann et al, 2019; Simmler et al, 2021), AI agents move beyond mere predictive functions to act within a certain environment (van Otterlo and Wiering, 2012; Humphreys et al, 2022). One important such envi-ronment is represented by computer systems (desktop or mobile) and the applications running on them. Consider the wealth of tasks humans today accomplish using their computing devices, and imagine the benefit if the same tasks could be approached by AI agents working through the same interfaces on the same kind of devices, just by being instructed to do so by a user in natural language. The opportunities are immense, and we witness now what will become known as the early days of AI agents for com-puter use with first commercial prototypes becoming available (e.g., Anthropic, 2024; Google Deepmind, 2024; David, 2025). This review gives a comprehensive overview of the research landscape and approaches behind such agents.\nSpecifically, instruction-based computer control agents (CCAs) receive instructions from a user, which they fulfill by using computers through their graphical user inter-faces (GUIs). CCAs access screen information analogously to a human user, e.g., visually, and act through the same interfaces, i.e., a keyboard, mouse, or touchscreen. For instance, a user could instruct a smartphone agent to propose meeting dates via email. Unlike many other autonomous agents, CCAs are not limited to purely simulated environments, getting exposure to the com-plex dynamics of real-world applications and access to growing collections of sample trajectories by observing users operate the devices they are installed on. This makes them a particularly interesting form of AI agent both for research and commercial exploitation.\nEarly CCA research primarily explored reinforcement learning (RL) techniques (e.g., Branavan et al, 2009; Jia et al, 2019; Humphreys et al, 2022) that were successful in simplified scenarios (e.g., MiniWoB, Shi et al, 2017). The progress in more realistic scenarios (for instance Mind2Web, Deng et al, 2023) accelerated in 2023. This was mainly driven by the integration of foundation models in the decision-making process through leveraging their emerging properties (Wei et al, 2022a) for computer control (Kim et al, 2023). This fueled and facilitated research on foundation model-based CCAs, leading to a rapid increase in publications in the field. \nThis review organizes and analyzes the growing body of CCA research, providing an overview of the field. Therefore, it introduces a taxonomy to structure the landscape of the CCA field in an efficient way, effective to gain a deeper understanding of the following aspects of agent design: (i) What fundamental building blocks constitute computer domains like smartphones, personal computers, or the Web, and what are the conceptual simi-larities? For example, various computer domains provide an alternative textual screen representation, such as HTML (Web) or the Android View Hierarchy (Android). (ii) How do these building blocks shape the interaction between a CCA and the computer?"}, {"title": "1.1 Relation to Other Surveys", "content": "In contrast to existing surveys, our review examines the field of instruction-based computer control from a technology-agnostic perspective and introduces a unifying framework that bridges diverse domains, methodologies, and technologies. This allows us to summarize insights from a broader range of approaches, including different types of computer domains, such as personal computers and Android, different types of technologies, such as reinforcement learning and foundation models, and different kinds of modalities, such as text and vision-based input. This broad scope allows us to introduce a novel, unifying taxonomy for instruction-based computer control that is compatible across a wide range of agent types - something previous work could not realize due to their limited scope. Specifically, existing surveys have the following limitations:\nLimited scope within computer control: Zhang et al (2024a) and Wang et al (2024c) review computer control only for foundation-model-based agents, not discussing other learning frameworks such as reinforcement learning as the core principle of design. Wu et al (2024a) discuss only mobile agents, neglecting other computer domains. While these surveys provide a comprehensive review of some aspects of computer control, they focus on a specific sub-part of the field. To have a unified taxonomy and to discuss future research directions comprehensively, it is important to analyze the field as a whole.\nLack of computer control specificity Some surveys (e.g., Arulkumaran et al, 2017; Moerland et al, 2023) focus on general, reinforcement learning-based agents. Other surveys (e.g., Wang et al, 2024b; Li et al, 2024a) review general, foundation-model-based agents. While these reviews provide a comprehensive overview of agents based on a specific technology, they do not focus on the domain of computer control and all its intricacies.\nAdjacent research areas with limited relevance: Another set of surveys (e.g., Yu et al, 2023; Li, 2023) concentrate on related topics, such as GUI testing, but do not cover agent-based interactions. Other reviews (e.g., Syed et al, 2020; Chakraborti et al, 2020) focus on robotic process automation using software robots (agents) to automate predefined workflows.\nIn contrast, our review provides a technology-agnostic review that connects these disparate computer control subfields and technologies. This allows us to highlight synergies and introduce a unified taxonomy for instruction-based computer con-trol, incorporating insights from reinforcement learning (RL), large language models (LLMs), vision language models (VLMs), and beyond. While Gao et al (2024b) pro-vides a valuable overview with similar scope, our review goes deeper into key aspects, offering a more comprehensive analysis and novel insights, culminating in a taxonomy built upon existing intelligent agent theory."}, {"title": "1.2 Survey Methodology", "content": "The CCA field is fragmented and a unified terminology is not yet established. This prevents a classic systematic survey, and we proceeded as follows instead:\nInitial collection: Using prior knowledge and combinations of keyword searches, we selected a preliminary list of publication candidates.\nPublication selection: We selected publication candidates to be included in the survey after carefully reviewing their titles, abstracts, or additional parts of their content for fit using the criteria catalog described below.\nExtension: We extended our selection by manually analyzing each selected publi-cation's related work section and bibliography for additional candidates, following the same selection process. We repeated this phase for every selected publication. We ended the initial and extended collection phase in October 2024.\nThe selection criteria for the collection process of papers on both agents and datasets are defined as follows:\nDeep learning focus: We only selected agents applying deep learning techniques to computer control, excluding traditional control algorithms.\nComputer control focus: We exclude instruction-based agents (chatbots) that access external tools but do not control the computer through user interfaces (e.g., Yang et al, 2023b; Tang et al, 2023; Li et al, 2024f; Guo et al, 2024b; Qin et al,"}, {"title": "1.3 Survey Structure", "content": "This review is structured in a way to provide a unified introduction to the field of instruction-based computer control. Due to the developing nature of this field, indi-vidual CCAs that stand for important strands do not yet stand out; rather, many agents only employ certain aspects of what contributes to the full picture of a C\u0421\u0410. Hence, most subsequent chapters of this review put individual elements of the tax-onomy at the center rather than individual CCAs, giving representative exemplary (indicated by citations prefixed by \u201ce.g.\u201d) or specific CCAs (no \u201ce.g.\u201d before citations) as references for each aspect. A notable exception will be Section 5.2, where indi-vidual agents are most prominently portrayed, as it discusses their core development paradigm. Otherwise, a structuring of the field by agents can be found in the tables in the Appendix A.\nSpecifically, in Section 2, we formalize the problem of instruction-based computer control agents and introduce respective terminology as a precursor to introducing the perspectives of the proposed taxonomy. Then, we look into each perspective in detail in the three subsequent chapters: in Section 3, we discuss the composition of commonly used domains (environment perspective); in Section 4, we analyze the interaction between the agent and the environment through the observation and action space (interaction perspective); in Section 5, we dissect the components of an agent, how an agent acts, and how an agent learns to act (agent perspective). Then, in Section 6, we summarize existing datasets used to train or evaluate agents, and we examine metrics and methodologies used to evaluate an agent's performance in Section 7. Finally, we outline challenges for deploying these agents in a production environment in Section 8 before we conclude by summarizing our findings and providing directions for future research in Section 9."}, {"title": "2 The Field of Instruction-based Computer Control", "content": "Human users instruct CCAs through a text-based instruction i, which must be achieved by the CCA through interacting with a computer environment. The nature of these environments is discussed in the environment perspective chapter (see Section 3)."}, {"title": "2.1 Definitions", "content": "Human users instruct CCAs through a text-based instruction i, which must be achieved by the CCA through interacting with a computer environment. The nature of these environments is discussed in the environment perspective chapter (see Section 3)."}, {"title": "2.2 A Comprehensive Taxonomy", "content": "We introduce a taxonomy for CCAs that distinguishes three perspectives: the envi-ronment perspective, the interaction perspective, and the agent perspective. Each perspective discusses CCAs with a different focus and classifies them according to the features visualized in . We provide a brief overview of each perspective here, with detailed discussions in the subsequent sections.\nEnvironment perspective: Here we discuss properties of computer environments and identify observation and action types shared across computer domains. This addresses question (i) from Section 1."}, {"title": "3 Environment Perspective", "content": "This perspective discusses the common properties of computer environments and shared concepts across computer domains."}, {"title": "3.1 The Nature of Computer Environments", "content": "In , we classify computer environments according to the framework established by Russell et al (2022, Chapter 2.3). Computer environments are typically partially observable and single-agent. In the literature, they are mostly assumed to be deter-ministic, meaning that for a state \\(s_t\\) and action \\(a_t\\) only one possible outcome \\(s_{t+1}\\) exists. While this assumption holds in many cases, real-world environments can have stochastic elements such as a shuffle button in a music app. Additionally, the liter-ature assumes that computer environments are episodic, meaning that each episode is independent of the previous ones. In reality, however, computer environments are sequential: The environment is not reset after each trajectory, and prior actions can influence future states across episodes. Another simplifying assumption is that the environment is static, meaning that the environment's state \\(s_t\\) only changes in response"}, {"title": "3.2 Domains", "content": "In the existing literature, we identify the Web, Android, and personal computers as the most commonly utilized domains. Each domain provides a unique set of possible observations and actions, yet we establish shared types of observation and action types across these domains.\nObservation types shared across domains:\nImage screen representation: A screenshot capturing the current screen, parts of the screen, or an extended view of the screen as a pixel image (e.g., Niu et al, 2024; Song et al, 2024a; Zhang et al, 2024b).\nTextual screen representation: A textual representation of the screen, such as HTML in the Web domain (e.g., Kim et al, 2023; Wen et al, 2024a; Zhang et al, 2024b).\nIndirect: Indirect observations that do not describe the current screen but infor-mation of the computer state \\(s_t \\in S\\), for example, by accessing stored files (e.g., Song et al, 2023b; Wu et al, 2024c; Guo et al, 2024a).\nAction types shared across domains:\nMouse/touch and keyboard: Screen coordinate-based actions like mouse clicks or touch taps and keyboard actions for typing text into a previously selected element (e.g., Humphreys et al, 2022; Wang et al, 2024a; Rahman et al, 2024).\nDirect UI access: UI element-based actions like clicking a specific HTML ele-ment based on its id (e.g., Gur et al, 2023; Zhang et al, 2023; Branavan et al, 2009).\nTask-tailored actions: Task- or domain-specific actions to solve a sequence of steps in a single action (e.g., Nakano et al, 2022; Bonatti et al, 2024; Wang et al, 2024d).\nExecutable code: Allowing the agent to generate executable code to interact with the environment through a programmatic interface (e.g., Sun et al, 2023; Gur et al, 2024; Deng et al, 2024a)."}, {"title": "4 Interaction Perspective (Agent \u2194 Environment)", "content": "This section discusses the interaction between the agent and the environment through the observation and action types established in Section 3."}, {"title": "4.1 Observation Spaces", "content": "In the previous section, we introduced three observation types: image screen repre-sentation, textual screen representation, and indirect. Most computer environments contain only observations of one type in their observation space O. However, some utilize both image and textual observations, termed bi-modal screen representation. reflects the increased attention CCAs received after the rise of large language models (LLM) , which are text-based. However, as we will discuss, each observation type has its own merits, and while textual screen representations are the most commonly used, they are not inherently superior to other types.\nImage Screen Representation\nAgents processing image screen representations (screenshots) have been successfully employed in the Web (e.g., Zheng et al, 2024a), Android (e.g., Zhang et al, 2023) and personal computer (e.g., Gao et al, 2024a) domains. An agent using screenshots closely aligns with human visual perception, differing primarily in capturing one static image \\(O_t\\) at a single moment t instead of having a continuous input stream. Also, an agent's image representation may only show parts of the screen like the foreground application (Gao et al, 2024a) or extend beyond the typical field of view observed by humans. For example, Chen et al (2024b) render the entire website as an image, while humans need scrolling to perceive the whole page. The alignment with human perception makes screenshots widely applicable, as most applications provide a visual user interface.\nDue to practical concerns, screenshot observations of are typically simplified \\(o_t \\rightarrow o'_t\\) by downsampling their resolution (e.g., Toyama et al, 2021; Chen et al, 2024b). Rahman et al (2024) even combine a high resolution 1120 \u00d7 1120 and low resolution 224 \u00d7 224 screenshot to have a compact view but still access image details if needed. To feed the text-based instruction i into vision-only agents, the instruction is either encoded separately and added in the embedding space (e.g., Baechler et al, 2024) or visually rendered atop of each screenshot (e.g., Shaw et al, 2023).\nTextual Screen Representation\nAgents processing textual screen representations have been successfully employed in the Web domain using HTML (e.g., Kim et al, 2023), the Android domain using the Android Hierarchy View (e.g., Shvo et al, 2021), and the personal computer domain using the Windows UI Automation Tree (e.g., Zhang et al, 2024b)."}, {"title": "4.2 Action Space", "content": "In Section 3, we introduced four action types: mouse/touch and key-board actions, direct UI access actions, task-tailored actions, and executable code. Computer environments typically provide only actions of one type in their action space A.\nMouse / Touch and Keyboard\nMouse, touch, and keyboard actions are general-purpose actions, aligning with human (inter-)actions, making them straightforward to collect and use as training data for CCAs (Humphreys et al, 2022). Both mouse actions, such as click(x,y), and touch actions, such as tap(x,y), require absolute screen coordinates (x,y), making them conceptually identical for CCAs. illustrates various approaches for predict-ing screen coordinates. Some methods make discrete predictions, such as predicting a position on a low-resolution coordinate grid (e.g., Shi et al, 2017; Toyama et al, 2021), predicting two interdependent discrete values for the x and y coordinates (e.g.,"}, {"title": "Direct UI Access", "content": "Direct UI access actions such as click(e) or type(e, text) involve interacting directly with a UI element e observed by the agent. To apply such actions, the appli-cation domain must provide an accessible interface, and the agent must be able to identify referenceable UI elements. There are two primary approaches for referencing an element e. The first approach allows the agent to directly reference the element by predicting a unique identifier, such as the element's id attribute (e.g., Li et al, 2023). For instance, clicking a perceived element <button id=\"search\"> is done by pre-dicting the action click(id=search). Similarly, Kim et al (2023) utilizes the element's XPath  as a unique identifier instead of the id attribute. The second approach involves the agent scoring each element and selecting the one with the high-est score (e.g., Jia et al, 2019). This can include predicting a probability score for each element (e.g., Li et al, 2024d), enabling the agent to select the most relevant UI ele-ment. To simplify the action space, the set of referenceable elements can be reduced. A common strategy is to focus only on leaf nodes in the user interface tree (e.g., Liu et al, 2018). Alternatively, an auxiliary model can preselect potential candidate ele-ments (e.g., Deng et al, 2023). For specific tasks such as web navigation, Zaheer et al (2022) and Chen et al (2024b) limit the referenceable set to only hyperlink elements."}, {"title": "Task-Tailored Actions", "content": "Task-tailored actions are typically not general purpose and are specifically designed to fulfill specific sub-tasks. For instance, Wang et al (2024d) define application-specific actions such as create_event for a calendar application and send_email for an email client. Task-tailored actions are easier to use and learn than a trajectory of corre-sponding general-purpose actions and, thus, provide a shortcut to the agent. However, the use of task-tailored actions comes with certain limitations: they require additional engineering effort to implement, as the environment must explicitly support these actions, and they cannot be easily generalized across different tasks.\nIn contrast, there also exists domain-specific actions that have a higher degree of generality. For example, Bonatti et al (2024) define the action open_application, which enables an agent to open and switch between applications on a Windows operating system. Similarly, Nakano et al (2022) define a search action, which allows the agent to navigate to specific text positions within a website. These domain-specific actions strike a balance between general-purpose functionality and task-specific relevance."}, {"title": "Executable Code", "content": "Another approach to control computers is to generate code that performs actions within the environment when being executed. While actions generated by an agent such as click(x,y) can be seen as interpretable code, we define executable code here as generating program code that a common interpreter can execute, such as the Python or the Bash interpreter. Executable code varies in its structure and the level of abstraction provided by its application programming interface (API):\nStructure of generated code:\nStraight-line code consists of a sequence of statements without control flow (e.g., Tao et al, 2023). It is akin to predicting a single or multiple actions.\nControl-flow code includes control flow mechanisms such as conditional state-ments (e.g., if), loops (e.g., for), and function definitions. Complex code can represent the agent's entire execution plan, as seen in Sun et al (2023), where the agent dynamically adjusts its plan based on precondition checks failing.\nAPI abstraction level utilized by generated code:\nGeneral-purpose API: Some agents use an API with functions akin to general-purpose actions like clicking elements or screen coordinates. For example, Gur et al (2024) use the Selenium WebDriver API providing such low-level actions."}, {"title": "Action Grounding", "content": "Action grounding, denoted as \\(a \\rightarrow a_t\\), refers to the process of converting an abstract action, such as click submit button, into an executable action, \\(a_t \\in A\\), such as click(e), where e represents the specific UI element. Grounding is essential when a text foundation model generates an abstract, text-based plan that must be transformed into a sequence of executable actions (Gao et al, 2024a; Kim et al, 2023). Several approaches have been proposed for grounding actions:\nPrediction-based grounding: A grounding model predicts the corresponding UI element for an ungrounded action. For example, Li et al (2020b) predict click(e) where e refers to the Settings App icon, based on the abstract action navigate to settings."}, {"title": "5 Agent Perspective", "content": "Section 3 detailed the computer environments and Section 4 the agent-environment interactions, describing the exterior of an agent. This section extends the taxon-omy to the interior of CCAs by first introducing the two most common agent types foundation agents (based on foundation models) and specialized agents (based on domain-specific design). We analyze each design aspect separately while showcasing connections, nuances, and exceptions across and beyond these agent types.\nFoundation Agents: A foundation agent (e.g., Zheng et al, 2024a) uses a gen-eral pre-trained foundation model (such as an LLM or VLM) as its policy \\(\\pi\\) by employing the model's broad knowledge and in-context learning capabilities for episodic improvement. For example, a text-based foun-dation model receives the HTML observation \\(o_t\\) alongside a prompt specifying its role as a web agent, a description of available UI actions A, and the instruction i. The model then generates an action \\(a_t\\), such as click(id=search). During each episode, the agent typically accumulates a history of past observations and actions"}, {"title": "Specialized Agents", "content": "Specialized Agents: A specialized agent (e.g., Humphreys et al, 2022) employs a custom deep learning architecture as its policy \\(\\pi\\), which predicts actions \\(a_t \\in A\\) based on a given observation \\(o_t\\) and instruction i, using predefined output possibili-ties. For example, the architecture might process an image \\(o_t\\) and a text instruction i as inputs, generating logits for each action type such as clicking alongside addi-tional outputs for screen coordinates (x,y). To track the past, observations are continuously aggregated in an internal Markov state. Learning involves environment learning techniques such as reinforcement learning (see Section 5.2.2)."}, {"title": "5.1 Policy - How to Act", "content": "A policy is the decision-making component of an agent (Sutton and Barto, 2018, Chapter 1.3). In the context of computer control, we distinguish three types of policies: Memoryless policies, history-based policies, and state-based policies.\nMemoryless Policies\nThe simplest form of a policy is a memoryless policy (e.g., Chen et al, 2024b), meaning it does not utilize any memory to track past observations or actions and predicts the next action \\(a_t\\) solely based on the current observation \\(o_t\\). Such policies are defined as (see also Eq. (1)):\n\\begin{equation}  a_t \\sim \\pi(\\cdot \\vert o_t, i) \\end{equation}\nMemoryless policies are sufficient for simple tasks but are generally inadequate for computer control, where selecting an appropriate next action \\(a_t\\) at time-step t often depends on aspects of past observations \\(o_0, o_1,..., o_{t-1}\\) rather than solely on the current observation \\(o_t\\). For example, in the context of purchasing multiple items from an online store, an agent must remember which items were already added to the shopping cart. Consequently, while memoryless policies can be utilized to simplify model architectures (e.g., Shvo et al, 2021), more sophisticated policies with memory capabilities are typically required for computer control tasks.\nHistory-based Policies\nA simple strategy to track the past is to accumulate observations and actions in a con-tinuously growing sequence, called history \\(h_t = (o_0,..., o_{t-1}, a_0, . . ., a_{t-1})\\) (e.g., Zheng"}, {"title": "State-based Policies", "content": "Alternatively, state-based policies track the past by continuously aggregating observa-tions into an internal state, called the Markov state \\(m_t\\) (e.g., Humphreys et al, 2022). For example, a vision-only agent updates \\(m_t\\) iteratively for each observed screenshot. The agent learns to track the relevant aspects of the past observations in \\(m_t\\) to better predict future actions. Formally, state-based agents are defined as:\n\\begin{equation}  a_t \\sim \\pi(\\cdot \\vert o_t, i, m_t) \\end{equation}\nThe state \\(m_t\\) is updated at each time-step using a state-update function \\(m_{t+1} = f(o_t, m_t)\\). Specialized agents commonly use state-based policies, incorporating \\(m_t\\) and f into deep learning models such as recurrent neural networks (e.g., Humphreys et al, 2022).\nA notable exception is Zhang et al (2023), who propose a foundation agent with a state-based policy using an external text-based state \\(m_t\\). The foundation model not only generates the next action \\(a_t\\) but also the next state \\(m_{t+1}\\) given the current state \\(m_t\\) and observation \\(o_t\\), effectively operating as both policy and state-update function.\nMixed Policies\nHistory-based and state-based approaches are also mixed. For example, Bonatti et al (2024) prompt their internal foundation model with the past actions and the last observation \\(h = (o_{t-1}, a_0,..., a_{t-1})\\) and in addition keep an external text-based state \\(m_t\\). Similarly, Iki and Aizawa (2022) feed the current observation \\(o_t\\), the last action \\(h = (a_{t-1})\\), and an external text-based state \\(m_t\\) into a fine-tuned model to predict the next action \\(a_t\\) as well as state \\(m_{t+1}\\)."}, {"title": "5.2 Learning Strategy - How to Learn to Act", "content": "Each agent's learning strategy can be conceptualized as implementing the following three steps (not every CCA uses all three steps):\nGeneral pre-training: The agent acquires broad, environment-agnostic knowl-edge. Examples include foundation models learning general-purpose capabilities or vision backbones learning image representations.\nEnvironment learning: The agent learns to adapt to a specific computer environ-ment. This involves explicit parameter (weight) updates or implicit methods, such as storing environment experiences for later retrieval.\nEpisodic improvement: The agent refines its performance within the current episode through methods such as instruction tuning or few-shot learning (Brown"}, {"title": "5.2.1 Leveraging General Pre-Training", "content": "Foundation agents leverage foundation models with broad knowledge and in-context learning capabilities (Brown et al, 2020). These capabilities can eliminate the need for environment-specific fine-tuning, allowing agents to operate in computer environments using only the foundation model's broad knowledge and instructions provided through prompts to adapt to specific environments (e.g., Kim et al, 2023).\nFor example, GPT-4 (OpenAI et al, 2024), when prompted as a web agent, can complete tasks such as filling out forms or navigating website links (Zheng et al, 2024a). In contrast, specialized agents are either trained from scratch (e.g., Humphreys et al, 2022) or initialized with a pre-trained backbone (e.g., an image encoder) to accelerate learning the observation space (Li et al, 2024b). These agents typically require additional fine-tuning to adapt to computer environments (see Section 5.2.2). The foundation model or backbone choice depends on the observation space, action space, and specific task requirements. For example, Zheng et al (2024a) use GPT-4 (OpenAI et al, 2024) as a multi-modal foundation model for their bi-modal agent. Gur et al (2024) employ a coding-proficient foundation model (Chung et al, 2024) to generate executable code. Shaw et al (2023) fine-tune a vision backbone for their vision-based agent. Iki and Aizawa (2022) fine-tune a text backbone for their text-based agent. Song et al (2024a) use pre-trained object detection and OCR models to convert screenshots into text-based observations for direct UI access actions. Gur et al (2024) pre-train an LLM from scratch on only HTML data while utilizing an HTML-specific local and global attention mechanism."}, {"title": "5.2.2 Environment Learning", "content": "Environment learning is about learn-ing to act in a computer environment through experiences in the same or a similar computer environment. This process typically follows one of three main strategies: Reinforcement learn-ing, behavioral cloning, or leveraging long-term memory. \nReinforcement Learning\nIn reinforcement learning, an agent acts in an environment and learns to maximize a cumulative reward by trial and error (Sutton and Barto, 2018). For computer control tasks, such environments are hand-crafted simulations, called controlled environments, designed to mimic real-world computer settings while providing a reward signal for guidance. RL has been implemented with various algorithms, including approximate"}, {"title": "Behavioral Cloning", "content": "In behavioral cloning (BC) (Pomerleau, 1988), an agent learns to mimic a shown behavior through supervised learning. The shown behavior is usually a sequence of recorded observations and actions of a human controlling a computer to achieve an instruction i. As mentioned, BC can be used to first train a somewhat competent agent as initialization to RL in a controlled environment. Typically, RL further enhances the agent by exploring aspects missing from the behavioral data. For instance, Humphreys et al (2022) demonstrate that after training their agent on 2.4 million human-labeled actions, RL increased the task success rate from approximately 30% to over 95%. Nonetheless, some agents rely entirely on BC, which can suffice for simpler tasks (e.g., Gur et al, 2023).\nUnlike RL, BC does not require the agent to execute actions in the environment, making it applicable in uncontrolled environments. For example, Zhang and Zhang (2024) fine-tune a model on Android demonstrations from Rawles et al (2023), while Hong et al (2024) combine Android demonstrations from Rawles et al (2023) with"}, {"title": "Long-Term Memory", "content": "Foundation models exhibit strong few-shot learning capabilities (Brown et al", "experiences": "nEnvironment transitions: The agent memorizes environment transitions as triples (\\(o_t\\), \\(a_t\\), \\(o_{t+1}\\)), where \\(a_t\\) represents the action taken, and \\(o_t\\), \\(o_{t+1}\\) capture the pre- and post-action observations, respectively."}]}