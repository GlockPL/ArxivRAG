{"title": "A Closer Look at System Prompt Robustness", "authors": ["Norman Mu", "Jonathan Lu", "Michael Lavery", "David Wagner"], "abstract": "System prompts have emerged as a critical control surface for specifying the behavior of LLMs in chat and agent settings. Developers depend on system prompts to specify important context, output format, personalities, guardrails, content policies, and safety countermeasures, all of which require models to robustly adhere to the system prompt, especially when facing conflicting or adversarial user inputs. In practice, models often forget to consider relevant guardrails or fail to resolve conflicting demands between the system and the user. In this work, we study various methods for improving system prompt robustness by creating realistic new evaluation and fine-tuning datasets based on prompts collected from from OpenAI's GPT Store and HuggingFace's HuggingChat. Our experiments assessing models with a panel of new and existing benchmarks show that performance can be considerably improved with realistic fine-tuning data, as well as inference-time interventions such as classifier-free guidance. Finally, we analyze the results of recently released reasoning models from OpenAI and DeepSeek, which show exciting but uneven improvements on the benchmarks we study. Overall, current techniques fall short of ensuring system prompt robustness and further study is warranted. We open-source our code, data, and models:", "sections": [{"title": "1. Introduction", "content": "Initially introduced as a sparsely documented feature in OpenAI's GPT API, the concept of the system prompt has grown substantially in popularity and utility (OpenAI, 2023). A degree of consensus on the general purpose of system prompts in conversational LLMs has now emerged: system prompts contain instructions that apply throughout the context window and supersede any conflicting instructions in other messages."}, {"title": "1.1. The Prompt Complexity Wall", "content": "As a motivating example, we establish a simple stress test of models' adherence to system prompts by measuring how well models can enforce system prompts that contain multiple guardrails. Specifically, we adapt a system prompt found in a real-world application, a choose-your-own-adventure game, to include a variable number of additional if-then guardrails. These guardrails involve outputting specific text under particular conditions, which we can trigger with pre-specified user messages and then evaluate with simple functions. We refer to this evaluation as the Monkey Island stress test."}, {"title": "2. Background and Related Work", "content": "System prompts. System prompts may contain many different types of information and instructions, but in this work we focus on guardrails that concretely define desired model behavior, as these can be evaluated more straightforwardly. We operate under an informal definition of guardrails: any specifications of model behavior which admit objective pass/fail evaluation. For our purposes, an instruction to only respond in English constitutes a guardrail, while general instructions to respond humorously do not. Guardrails can also be directly trained into model weights, for instance with RLHF (Bai et al., 2022a).\nBy default, models should follow all instructions and guardrails contained with their system prompts. In cases where subsequent messages contain conflicting instructions, the system prompt must take \u201cprecedence\u201d, i.e. override all other instructions. Even in the absence of conflicting instructions, current models still frequently fail to adhere to the initial instructions within the system prompt.\nWhile many open and proprietary models support system prompts today, few model creators have shared details on their training data. Wallace et al. (2024) use supervised fine-tuning and RLHF to enhance system prompt adherence and precedence as part of a multi-level \u201cinstruction hierarchy\" also encompassing assistant and tool messages, but give little information about their data and models. The Llama 2 (Touvron et al., 2023) and Llama 3 (Dubey et al., 2024) reports give a high level overview of their data collection and training process; however, they do not provide much detail or analysis into the behavior of their models when using system prompts.\nExisting public datasets for system message precedence rely on either a small number of handwritten system messages (Mukherjee et al., 2023) or procedurally generated system messages, e.g. Multifaceted Collection (Lee et al., 2024) focusing on system messages specifying personas and preferences, PriorityRules (Lu et al., 2024), and Persona Drift (Li et al., 2024a). Our system prompts are collected from real AI assistants, covering a diverse set of applications and types of guardrails.\nInstruction following. The ability to prioritize instructions in system messages follows from the ability to take instruction at all. Directly training language models to follow instructions, i.e. instruction tuning (Wei et al., 2021; Khashabi et al., 2020; Weller et al., 2020; Mishra et al., 2021; Sanh et al., 2021; Ouyang et al., 2022), was a major step forward in the development of practically useful LLMs and replaced fragile few-shot prompting methods introduced in the GPT-3 report (Brown, 2020). RULES (Mu et al., 2024) and IFEval (Zhou et al., 2023) are two benchmarks that both evaluate instruction following in LLMs, with RULES focusing on rules and conflicting user inputs while IFEval measures the precise execution of multiple instructions.\nPrompt injection attacks. Unfortunately, a strong tendency in LLMs to follow instructions can be exploited to hijack control of an LLM-based application and execute arbitrary tasks (Branch et al., 2022; Perez & Ribeiro, 2022; Greshake et al., 2023). Studies of custom GPTS and other LLM applications find persistent weaknesses to prompt injection, even when system messages include explicit guardrails and warnings against prompt injection (Yu et al., 2024; Liu et al., 2024). Toyer et al. (2023) hosted a two-sided \"capture-the-flag\"-style game to study the offense/defense balance in prompt injection with motivated human players, and the resulting dataset is now used as a benchmark of prompt injection robustness. Other benchmarks of prompt injection (Schulhoff et al.; Li et al., 2024b) and indirect prompt injection (Yi et al., 2023) have also been created to evaluate various defenses. A variety of other fine-tuning techniques have been explored (Chen et al., 2024; Piet et al., 2024; Yi et al., 2023; Wallace et al., 2024; Wu et al., 2024), though current models remain broadly vulnerable (Rehberger, 2024).\nSafety alignment and jailbreaking. Along with the rapid growth of AI capabilities, the need to avoid user harms and abuse has also increased. Different training techniques such as supervised fine-tuning and RLHF (Bai et al., 2022a;b; Ouyang et al., 2022; Glaese et al., 2022; Achiam et al., 2023) have been used to align model behavior to safety standards, for instance by learning to refuse harmful requests. However jailbreak prompts, first popularized by online users, are able to circumvent safety training by leveraging various tactics often shared with prompt injection attacks (Kang et al., 2023; Zou et al., 2023; Wei et al., 2023; Mazeika et al., 2024)."}, {"title": "3. Benchmarks and Measurements", "content": "To measure different forms of system prompt robustness in LLMs, we assembled a set of new and existing benchmarks covering varied evaluation settings. Our new benchmark, RealGuardrails, draws upon real-world system prompts collected from OpenAI's GPT Store and HugginFace's HuggingChat platforms to evaluate model responses to aligned, conflicting, and unrelated user messages, while also covering longer multi-turn contexts.\nThe other benchmarks additionally measure model behavior when responding to adversarial inputs, generating open-ended completions, and acting as a tool-calling agent. We encourage the reader to view examples from each of these benchmarks in Appendix C."}, {"title": "3.1. RealGuardrails", "content": "Specifications and guardrails in actual applications bear little resemblance to the simple, verifiable instructions found in existing benchmarks. To fill this gap, we introduce RealGuardrails, a benchmark which focuses on more realistic test inputs. RealGuardrails is comprised of two sets of test cases: a handwritten set and a distractor set Both sets use the same 14 system prompts, which are based on real system prompts found on the GPT Store / HuggingChat, edited for clarity. We also added guardrails requiring the model to stay on task to all prompts.\nHandwritten test cases. We manually wrote 239 test cases that either align or conflict with the system prompt in each test case. The aligned test cases still require the model to respond in a manner specified by the system prompt. In conflicting test cases, the user prompt conflicts with the guardrails in the system prompt; the goal is to test whether the model still enforces the system prompt. These test cases do not contain any adversarial inputs, i.e. LLM-specific tactics (e.g., base64 encoding), and instead focus solely on the model's ability to handle cases of clear conflict. About half of these test cases are created by adding a list of banned words to one of the 14 system prompts; we check whether the model has used any of the banned words in its response.\nDistractor test cases. We created 504 distractor test cases which attempt to distract the model away from its system prompt with in-context demonstrations of unrelated tasks. These kind of inputs might arise from an attacker attempting to hijack control of the model. The distractor tasks consist of roleplaying (Jandaghi et al., 2023) and translation (Goyal et al., 2021). For each distractor task, the test cases place multiple task demonstrations within the conversational context. Half the test cases use multiple rounds of user/assistant messages, while the other half concatenate all demonstrations into a single user message. We use either 5, 10, and 20 demonstrations in each test case.\nEvaluation. Given the open-ended nature of our guardrails, we rely on GPT-40 to judge model responses on a pass-fail basis after reasoning through a short chain-of-thought. We create judge prompts (Appendix D.1) specific to each system prompt which highlight the manually annotated guardrails within the system prompt. In contrast to other LLM-as-judge settings like MT-Bench (Zheng et al., 2023), our evaluation criteria are highly specific. This enables our evaluation pipeline to achieve a 94% agreement rate with manual labeling by the authors on a set of 134 model responses."}, {"title": "3.2. Other Benchmarks", "content": "System-RULES. RULES (Mu et al., 2024) is a benchmark consisting of interactive scenarios and test cases designed to assess rule-following ability in LLMs in a programmatic way. The scenarios simulate common security tasks or children's games with rules for the behavior of the model. These rules mandate and prohibit various behaviors, modeled after the helpful-harmless axes of AI alignment (Bai et al., 2022a). The 1,695 test cases are organized by three levels of increasing threat level: benign, basic, and redteam.\nAll test cases in the basic and redteam suites contain user messages which conflict with the scenario rules, and the handwritten redteam test cases further employ a variety of adversarial tactics such as indirectly asking for sensitive information. In our evaluations, we present the rules as a system prompt and remove any explicit guidance to prioritize system messages, so we refer to this modified benchmark as System-RULES (S-RULES). We report average performance on the basic and redteam test suites.\nTensorTrust. Toyer et al. (2023) developed TensorTrust, a two-sided prompt injection game in which human players alternate between constructing prompt-based defenses and attacking other players' defenses. The defender provides the system with an access code and instructs the LLM to output \"Access Granted\u201d if and only if it is provided the access code. Attackers attempt to make the model output \u201cAccess Granted\" without inputting the access code (hijacking) or to reveal the access code (extraction). Models are also required to output \"Access Granted\" when the access code is provided (helpful).\nWe adapt the published dataset of defenses and adversarial attacks to measure system message robustness, ignoring attack instances with post-prompts. We report the overall pass rate by averaging the model's pass rates for hijacking, extraction, and helpful test cases. More details are included in Appendix G.1.\nSystem-IFEval. We extend IFEval to evaluate the ability of LLMs to follow precise instructions embedded in their system message. Originally developed by Zhou et al. (2023), this benchmark is designed to test models on their ability to follow precise, verifiable instructions. Each test case consists of a base task instruction and up to three constraint instructions that place additional non-conflicting requirements on the model response.\nS-IFEval rewrites each test case in IFEval and separates the constraints from the bask task, placing the constraint instructions in the system message and the base task in the user message. Interestingly, our evaluations show that a model's ability to follow constraint instructions in the user message does not always transfer following constraint instructions in the system message, and can benefit from specific training (Table 8). More analysis and implementation details can be found in Appendix G.2.\nAgentDojo. Debenedetti et al. (2024) created AgentDojo to assess prompt injections robustness on tool-calling tasks. The benchmark consists of five task suites in various environments (e.g. a Slack workspace, a banking app, etc.), each with a set of benign user tasks and malicious injection tasks. Models are evaluated with \"Utility Under Attack\", the rate at which user tasks are completed (regardless of injection task success), and \"Attack Success Rate\u201d, the rate at which injection tasks are completed (regardless of user task success). This benchmark remains difficult for open models, so we only report results on the easiest (difficulty 1) tasks."}, {"title": "4. Data Collection", "content": "In addition to our new evaluation benchmark, we also collect fine-tuning data for SFT as well as DPO. Examples are shown in Appendix F.\nSystem prompts. We source realistic system prompts from OpenAI's GPT Store, which hosts user-created custom GPT assistants defined by a system prompt. These custom GPTs are built for a wide variety of use cases and their prompts contain many different types of guardrails. Combining two public collections of extracted prompts with publicly crawled metadata, we identify 651 assistant prompts that are easier to simulate outside of the ChatGPT platform, i.e., do not expect file/image uploads from the user message and do not rely on custom HTTP APIs. We use Claude 3.5 Sonnet to remove prompts that expect file or image uploads; we verified its accuracy by examining a random subset of its judgments. Removing prompts with custom HTTP APIs was done easily with the GPT Store metadata.\nWe also gather publicly shared system prompts from HuggingFace's HuggingChat platform. We combine the prompts from GPT Store and HuggingChat, then filter out extremely long prompts, partially duplicated prompts, non-English prompts and prompts that accept non-English inputs, and obscene prompts. Then, we use Claude 3.5 Sonnet to extract all discrete guardrail clauses from each system message for use in user message generation. We discard prompts without identifiable guardrails such as simple role-playing prompts. Finally, we hold out 14 prompts for use in evaluation as described in Section 3.1 In total we are left with 1,850 assistant prompts for use in training. All relevant filtering prompts are included in Appendix D.2. Additionally, we provide a brief analysis of all the different topics and applications covered by our system prompts in Appendix E.\nAligned/conflicting user messages. We generate many challenging user messages that could lead a model to violate system message guardrails. We also generate benign messages (which are aligned with the system message and do not conflict with it), to retain model utility during training and avoid inappropriate over-refusals. We find that with a bit of prompting Claude 3.5 Sonnet is able to synthesize a set of highly creative user messages and subtly target different sets of guardrails within the same system prompt. For each of our 1,850 system prompts, we generate approximately five user messages that conflict with the system prompt and five that align with it, resulting in a total of 18,497 user prompts."}, {"title": "5. Experiments", "content": "Equipped with realistic training and evaluation data, we turn to investigating various methods to improve the robustness of system messages. We first examine some training methods, and then with our fine-tuned models, we explore several inference methods proposed in prior work."}, {"title": "5.1. Fine-tuning methods", "content": "Starting from base pre-trained models (Llama 3 8B, Qwen 2.5 7B, OLMO 7B, and Llama 3.2 3B), we apply a variety of fine-tuning methods such as supervised fine-tuning (SFT) with either a simple or higher-quality mixture of chat data, instructional segment embeddings, and preference optimization with DPO or SimPO.\nThe results of these training methods on Llama 3 8B are show in Figure 5, and results for Qwen 2.5 7B, OLMO 7B, and Llama 3.2 3B are in the Figure 9. The benchmarks vary quite widely in terms of difficulty and responsiveness to behavior, but broadly we see consistent modest improvements from improving the SFT data quality (\"SFT+\") and large improvements in applying DPO in addition to SFT with better data (\"SFT+ and DPO\").\nSupervised fine-tuning. Table 1 details the components of our higher-quality SFT data mixture. We sample various sources (RealGuardrails SFT, Multifaceted Collection, Glaive v2, and SPML) to cover different conversational features: single-turn and multi-turn, simple and complex system prompts, synthetic and user-generated data, benign and adversarial users, and tool calling. By contrast, the baseline SFT data mixture uses an equivalent number of samples from SlimOrca which contain basic system prompts such as \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\"."}, {"title": "Preference optimization.", "content": "After supervised fine-tuning, we can further optimize against our synthetically generated pairwise preferences via DPO (Rafailov et al., 2024) or SimPO (Meng et al., 2024). DPO is a standard pairwise preference optimization method. We also try SimPO, a reference model-free and length-normalized pairwise preference optimization algorithm which requires less GPU memory and has been shown to yield stronger results in some settings, though here we did not find this to be the case. At similar learning rates to DPO (1e-5), SimPO resulted in unstable training and markedly worse performance, possibly due to the lack of KL regularization in the training objective. With a lower learning rate (1e-6), SimPO is able to consistently improve upon the SFT+ starting model, but not nearly as much as DPO. We also evaluated the use of label-smoothing with DPO (Mitchell, 2023), but did not find this to meaningfully improve results."}, {"title": "Instructional Segment Embeddings.", "content": "As proposed by Wu et al. (2024), we implement instructional segment embeddings (ISE) by assigning a segment ID to each token based on the turn it corresponds to: system, user, assistant, tool, or other (special tokens and turn delimiters such as BOS). In principle ISE, may make it easier for models to distinguish between the various roles and make it more difficult for user messages to impersonate system messages. Our implementation may differ slightly from Wu et al. (2024).\nAdding ISE tends to help a small amount, but can also impair performance in some cases. S-RULES and TensorTrust contain a high proportion of adversarial user messages attempting to override the system message, but we did not see any clear improvement here across the 4 base models. Overall, ISE requires significant changes to training and model code which also precludes the use of efficient inference frameworks such as vLLM. Thus, we did not experiment with ISE beyond supervised fine-tuning from base models.\nContinued fine-tuning. The SFT and DPO methods explored above are also applicable to instruction-tuned models such as Llama 3.1 8B Instruct. We find significant improvements from applying our methods (Figure 10), even though this model has already undergone high-quality instruction tuning. In fact, after applying both SFT+ and DPO, our Llama 3.1 8B Instruct fine-tune exceeds the performance of GPT-40-mini at system prompt-following across all benchmarks (Table 4). This is noteworthy, as GPT-40-mini was trained using OpenAI's instruction hierarchy methods (Wallace et al., 2024), which are designed to enforce robust adherence to the system prompt."}, {"title": "5.2. Inference methods", "content": "We evaluated a variety of inference-time techniques proposed in prior work for controlling model generations, including split-softmax (Li et al., 2024a), a variant of classifier-free guidance that incorporates insights from contrastive decoding (Li et al., 2022), and having the model double-check then edit its initial responses.\nClassifier-free guidance. Following Sanchez et al. (2023), we apply classifier-free guidance in large language models but add a plausibility threshold (inspired by (Li et al., 2022)) to prevent sampling tokens the model itself deems implausible. We explored two straightforward variants: omitting the system prompt in the \u201cnegative prompt\" and omitting detailed rules (for S-RULES only). These methods yield consistent improvements on some benchmarks, though DPO-tuned models and S-RULES showed less or no improvement (Figure 10). Further gains may be possible with better prompting. More details can be found in Appendix H.\nClassifier-free guidance provides consistent improvements across all benchmarks when applied to Llama 3.1 8B Instruct (Table 9; $\\gamma$ = 1.0 is the baseline without classifier-free guidance). However, it offers little or no improvement on our DPO-tuned Llama model (Table 10) or on S-RULES.\nWe report results with a fixed plausibility threshold of a = 0.1. We experimented with different hyperparameters $\\gamma$, $\\alpha$ (Figure 13, Figure 14), and found that the plausibility threshold provides modest gains.\nDouble-checking. The success of recent reasoning models is due in large part to their ability to self-reflect on intermediate outputs before generating a final answer. This enables reasoning models to more robustly defend against adversarial inputs (Zaremba et al., 2025). As a simple approximation of this behavior, we also evaluated our non-reasoning fine-tunes of Llama 3 with a double-checking strategy where the original model response is fed back into the model with instructions to edit the response to better follow the system prompt (Appendix D.2). As shown in Figure 10, this yielded mixed results on the benchmarks, suggesting that RL-based reasoning training is needed for effective self-reflection.\nSplit-softmax. Li et al. (2024a) introduce split-softmax, an inference technique which up-weights attention scores on system message tokens during generation. We extend this method further and also experimented with only applying the attention score reweighting on various subsets of middle layers. We did not see much improvement on model performance from any evaluated configurations."}, {"title": "5.3. AgentDojo", "content": "Table 3. Our fine-tunes of Llama 3.1 8B Instruct improve both utility and security metrics on the easiest AgentDojo tasks. Utility under attack measures the percentage of instances in which the user task was completed, and attack success rate measures the percentage of instances in which the injection task was completed. Our DPO fine-tune trades a small regression in utility for a small increase in security.\nAgentDojo is a challenging benchmark which requires chaining tool-calls to accomplish multi-step objectives. The addition of indirect prompt injection attacks further increases its difficulty. We evaluated Meta's Llama 3.1 8B Instruct against our best fine-tunes of it, which used SFT+, or SFT+ and DPO. Our fine-tunes are able to drastically reduce attack success rate, while still improving the success rate on user tasks (utility under attack)."}, {"title": "6. Discussion", "content": "What causes models to fail to adhere to system prompts, even in non-adversarial settings? And what are the most promising paths forward to building more robust models? We can look for some clues by comparing model behaviors across individual benchmarks and test suites. Full tables of results for all evaluated models, including fine-tuning and inference methods, are available in Table 4 through Table 7 in the appendix."}, {"title": "6.1. Reasoning models", "content": "We evaluated two recent reasoning models on our system prompt benchmarks: OpenAI's 03-mini and DeepSeek's R1. Comparing 03 mini to the non-reasoning GPT-40 model in Table 4, we see that o3 mini is substantially more robust in following system prompts. 03 mini fares particularly well on the RealGuardrails distractors and the Monkey Island stress test (Figure 2). Both of these evaluations require models to retrieve pertinent information earlier in the context window while ignoring irrelevant information elsewhere. This mode of behavior may be a particular strength of reasoning models, whereas on other benchmarks that require resolving conflict such as S-RULES and TensorTrust, 03 mini does not show the same level of improvements.\nDeepSeek R1. In absolute terms, DeepSeek R1 performed quite poorly on many of our benchmarks. Our best fine-tune of Llama 3.1 8B Instruct outperforms R1 on every single benchmark, sometimes by a significant margin. The published chat template for R1 shows that system messages are simply pre-pended to the conversation without any explicit identifying tokens as is used in the Llama 3 Instruct template. We interpret this as system prompt following simply not ranking as a major priority of DeepSeek when developing the model.\nDeepSeek R1 still seems to generally outperform DeepSeek V3, a non-reasoning model fine-tuned from the same base model as R1. Particularly on the RealGuardrails distractors, but also on S-RULES and TensorTrust, reasoning training offers robustness gains."}, {"title": "6.2. Benchmark analysis", "content": "RealGuardrails distractors. Distractors can be quite effective in inducing off-task behavior (Figure 8). Placing the distractors in multiple conversation turns, i.e. a prompt/answer/prompt/answer pattern across multiple user/assistant messages, is generally more distracting than placing all the distractors in a single user message. Increasing the number of demonstrations also increases difficulty in the multi-turn setting, echoing findings from Anil et al. (2024).\nSystem prompt complexity. A subset of our handwritten test cases in RealGuardrails are formed by adding a guardrail prohibiting the model from outputting a list of banned words, intended to add an incremental degree of complexity. We should expect these more test cases to be strictly more difficult, and indeed we find in Figure 7 that both Llama 3.1 8B Instruct along with our SFT+ and DPO struggle with banlists. Adding too many guardrails to a system prompt seems to overwhelm the model's \u201cworking memory\", similar to results found in the Monkey Island stress test (Figure 2)."}, {"title": "6.3. Recommendations", "content": "There is plenty of room for improvement in system prompt following, an class of AI behavior that demands a high degree of robustness. Even among leading edge commercial models, benchmarks are not yet close to saturation, particularly when using many guardrails (e.g., Monkey Island stress test), long context (e.g., distractors), or adversarial attacks. We commend this research problem to the community, and hope that our new RealGuardrails datasets enable experimenting with new methods on open models.\nReasoning training. It is difficult to draw strong conclusions from results with reasoning models given the paucity of publicly available information on how exactly reasoning models differ in their training from non-reasoning models, but overall reasoning appears highly promising for improving system prompt robustness, particularly when facing long contexts and highly complex system prompts. Data providing realistic demonstrations of system prompt adherence such as our RealGuardrails-SFT may be important for the fine-tuning stage of reasoning training.\nNegative learning signals. We found the use of negative samples in DPO to be very effective, and using negative samples in classifier-free guidance also offered significant improvements. This may be related to the binary nature of the task at hand: the types of guardrails studied in this work generally have clear right and wrong answers. Training with a greater quantity and quality (e.g., on-policy) data, or applying full reinforcement learning, will most likely yield additional system prompt robustness.\nInference mechanisms. Classifier-free guidance would seem to work against a core principle of deep learning-models perform best in settings most similar to training. That it works at all, and in fact quite well in the case of Llama 3.1 8B Instruct, suggests that it may be amplifying mechanisms for enforcing system prompt precedence that already exist within the model. These results can also guide further explorations of internal representations when handling system prompts, which could be important for developing multi-layer defenses against prompt injection."}, {"title": "Impact Statement", "content": "Our work seeks to improve methods of controlling LLM behavior. Technical research in AI has many societal consequences which have been well discussed in other work, although one point worth highlighting here is the importance of reliable control for the deployment of advanced AI."}]}