{"title": "Mitigating Metric Bias in Minimum Bayes Risk Decoding", "authors": ["Geza Kovacs", "Daniel Deutsch", "Markus Freitag"], "abstract": "While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or MetricX has outperformed traditional decoding methods such as greedy or beam search, it introduces a challenge we refer to as metric bias. As MBR decoding aims to produce translations that score highly according to a specific utility metric, this very process makes it impossible to use the same metric for both decoding and evaluation, as improvements might simply be due to reward hacking rather than reflecting real quality improvements. In this work we find that compared to human ratings, neural metrics not only overestimate the quality of MBR decoding when the same metric is used as the utility metric, but they also overestimate the quality of MBR/QE decoding with other neural utility metrics as well. We also show that the metric bias issue can be mitigated by using an ensemble of utility metrics during MBR decoding: human evaluations show that MBR decoding using an ensemble of utility metrics outperforms a single utility metric.", "sections": [{"title": "Introduction", "content": "Minimum bayes risk (MBR) decoding is a decoding approach where n candidate translations are sampled from the MT system, and they are used as pseudoreferences for a reference-based utility metric. MBR decoding computes the utility metric for all O(n\u00b2) pairs of candidates and pseudoreferences, selecting the candidate that achieves the best average score across all pseudoreferences. Quality Estimation (QE) decoding\u00b9 selects the candidate that scores best according to a QE utility metric. Previous work on MBR decoding has shown that it results in improvements on the utility metric (Amrhein and Sennrich, 2022; Cheng and Vlachos, 2023; Eikema and Aziz, 2022), however other metrics do not improve as much as the utility metric (Guttmann et al., 2024; Vamvas and Sennrich,\n2024). This issue of MBR/QE decoding exhibiting bias towards the utility metric complicates our ability to use automatic metrics to compare the quality of MBR/QE-based MT systems, as we cannot tell whether improvements in automatic metrics from MBR/QE decoding correspond to actual improvements in quality, or if it simply reward hacking. Prior work has assumed that this issue can be avoided by using a different metric for evaluating MBR decoding outputs (Tomani et al., 2023), though this assumption has never been tested.\nIn this work we compare the results of human vs metric-based evaluation of MBR/QE decoding with a wide variety of metrics to show that the quality of MBR/QE decoding is overestimated by not only the utility metric, but also other similar metrics. While MBR/QE decoding with a single utility metric results in significant gains in automatic metrics, it does not perform better than greedy decoding in our human evaluations. This may be due to MBR decoding preferring fluent yet inaccurate candidates. Using an ensemble of metrics as the utility helps us mitigate the metric bias issue, with human evaluations showing that MBR decoding with an ensemble utility metric results in significantly better translations than greedy decoding or MBR/QE decoding with a single utility metric.\nIn this paper we contribute:\n1.  A large-scale analysis of metric bias in MBR and QE decoding with metrics commonly used in MT, showing that this metric bias issue holds across many different metrics and language pairs, and is not resolved by simply using a different metric for evaluation.\n2.  Mitigation strategies for MBR bias using QE filtering followed by MBR decoding, as well as MBR decoding using an ensemble of metrics as the utility function.\n3.  A human evaluation showing that MBR decoding with ensembles outperforms MBR decoding with a single metric."}, {"title": "Related Work", "content": "Cheng and Vlachos (2023); Eikema and Aziz (2022); Guttmann et al. (2024) find that MBR decoding improves automated metrics on various high, medium, and low resource language pairs. Freitag et al. (2023a, 2022); Tomani et al. (2023) find that human raters prefer the outputs of MBR/QE decoding over greedy decoding.\nMBR variants achieve speedups via heuristics (Trabelsi et al., 2024; Jinnai and Ariu, 2024), filtering pseudoreferences via a QE metric (Deguchi et al., 2024, 2023) or filtering via another reference-based metric (Vamvas and Sennrich, 2024; Eikema and Aziz, 2022). Quality-aware translation, which incorporates quality estimation into the training process, has been found to improve translation quality over standard MBR (Tomani et al., 2023).\nOther techniques for aligning translation models with human preferences include direct preference optimization (Rafailov et al., 2024; Yang et al., 2024), reinforcement learning from human feedback (Christiano et al., 2017), and reinforcement learning from AI feedback (Bai et al., 2022).\nGuttmann et al. (2024); Vamvas and Sennrich (2024) show evidence of metric bias in MBR decoding, as they find that neural evaluation metrics favor models using MBR on the metric used as the utility function. However, these papers only cover only 2 metrics, and neither have human evaluations.\nSellam et al. (2020b); Freitag et al. (2023b); Glushkova et al. (2023) find that ensembling metrics can improve their ability to detect critical errors and improve agreement with human preferences, though they do not investigate the effects of ensembling utility metrics on MBR decoding.\nReward hacking (Skalse et al., 2022) is an issue in reinforcement learning where the reward function improves but the system's behavior is not aligned with human preferences. The metric bias problem in MBR decoding can be viewed as an instance of reward hacking, as the utility function improves while not necessarily improving quality."}, {"title": "Study 1: Metric Bias in MBR Decoding", "content": "To investigate metric bias in MBR/QE decoding, we perform MBR/QE decoding via various utility metrics and compare how they perform on various evaluation metrics. We investigate MBR decoding using these reference-based utility metrics:"}, {"title": "Methodology", "content": "To investigate metric bias in MBR/QE decoding, we perform MBR/QE decoding via various utility metrics and compare how they perform on various evaluation metrics. We investigate MBR decoding using these reference-based utility metrics:"}, {"title": "Results", "content": "Results are shown in Table 1 for average scores across all language pairs on the test datasets. We observe that for all reference-based metrics, the best-performing system is MBR decoding using the same utility metric. This result also holds for all QE metrics, but that is by definition, because QE decoding picks the sample with the best QE score. These results also hold on individual languages and the dev set (Appendix G and E).\nWe can also see that MBR decoding outputs for utility metrics which are similar to the evaluation metric tend to score better than when the MBR utility metric is dissimilar to the evaluation metric. For example, MBR/QE decoding with neural metrics (MetricX and COMET families) performs better than greedy when evaluated with other neural metrics, but worse than greedy if evaluated via lexical metrics. Likewise, MBR decoding with lexical metrics (sentBLEU, chrF, chrF++, and TER) and semantic metrics (YiSi) perform highly when evaluated by lexical and semantic metrics, but poorly when evaluated via neural metrics. The pattern also holds for similar metrics within the same family \u2013 XCOMET-XXL prefers MBR/QE decoding using CometKiwi23-XXL and XCOMET-XL, and MetricX prefers outputs from MetricX-QE.\nThese results suggest the existence of metric bias in MBR decoding \u2013 that is, they suggest that MBR decoding will result in a disproportionately large improvement in the utility metric and metrics similar to the utility metric, relative to the actual improvement in quality. In order to address this issue, in the next section we will investigate ensembling metrics during MBR decoding as a means of avoiding overfitting to a particular utility metric."}, {"title": "Study 2: MBR Decoding using Ensembles of Metrics", "content": "As a mitigation strategy for utility metric bias in MBR decoding, we investigate how using an ensemble of metrics performs for MBR decoding. We explore the following ensembling techniques (see Appendix C for pseudocode for these techniques):"}, {"title": "Methodology", "content": "As a mitigation strategy for utility metric bias in MBR decoding, we investigate how using an ensemble of metrics performs for MBR decoding. We explore the following ensembling techniques (see Appendix C for pseudocode for these techniques):\n1.  rankAvg: For each metric, assigns a rank to each of the 128 samples (where 0 is best and 127 is worst). Select the sample where the average rank across metrics is minimized.\n2.  rankMed: Select the sample where the median"}, {"title": "Results", "content": "Results for a subset of ensembles averaged across all language pairs on the test sets are at Table 1 with additional ensembles shown in Appendix F. Results on the dev sets are shown in Appendix E. Breakdowns per language pair can be found in Appendix G. As expected, ensembles tend to perform better if judged by metrics that are better represented in the ensemble; for example, if judging by MetricX, the best ensembles are mxQE(32)mxMBR and rankAvg:mxmxqe, both of which are ensembles consisting of MetricX and MetricX-QE.\nThat said, observe that compared to MBR/QE decoding with a single utility metric, ensembles often improve on automated evaluations even according to metrics not included in the ensemble. For example, if we use the XCOMET or CometKiwi families of metrics to evaluate rankAvg:noNCnoLex and noncQE(32)noncnolexMBR (which do not include any metrics from the XCOMET or CometKiwi families), they outperform MBR/QE decoding with any single metric outside the XCOMET or CometKiwi families. Similarly, if lexical metrics are used to evaluate the rankAvg:noLex and al-IQE(32)nolexMBR ensembles, which do not include any lexical metrics, they still outperform MBR/QE decoding with any single neural metric. This suggests that ensembles help reduce metric bias towards a single metric, which results in improved automated evaluation scores according to other metrics not included in the ensemble."}, {"title": "Study 3: Human Evaluation", "content": "For the human evaluation, we chose the following baselines and ensembles to evaluate:"}, {"title": "Methodology", "content": "For the human evaluation, we chose the following baselines and ensembles to evaluate:\n1.  Greedy decoding\n2.  Reference translation\n3.  MetricX (MBR decoding)\n4.  MetricX-QE (QE decoding)\n5.  AfriCOMET for African languages (MBR decoding)\n6.  AfriCOMET-QE for African languages (QE decoding)\n7.  IndicCOMET for Indic langauges (MBR decoding)\n8.  rankAvg:noNC (single-step ensemble)\n9.  rankAvg:noNCnoLex (single-step ensemble)\n10. mxQE(32)mxMBR (multi-step ensemble)\n11. noncQE(32)noncnolexMBR (multi-step ensemble)\nWe evaluated the following conditions only on en-de and zh-en due to budget constraints:\n1.  XCOMET-XXL (MBR decoding)\n2.  CometKiwi23-XXL (QE decoding)\n3.  COMET22 (MBR decoding)\n4.  rankAvg:all (single-step ensemble)\nWe chose MetricX, MetricX-QE, AfriCOMET, AfriCOMET-QE, and IndicCOMET because they had shown good performance in previously-published evaluations (Tomani et al., 2023; Wang et al., 2024; Sai B et al., 2023; Freitag et al., 2023b), had good performance in automated evaluations on the dev set (Appendix E), and lacked restrictions on commercial use. In our en-de and zh-en evaluations we also included metrics and ensembles with restrictions on commercial use (XCOMET, CometKiwi, rankAvg:all) for comparison. The 6 language pairs and datasets we evaluate are en-ha en-sw en-ml en-hi (from FLORES200 test) and en-de zh-en (from WMT2023). We chose these languages to have a wide distribution in resource level. For each language pair, we sampled 400 source segments to evaluate. WMT2023 was evaluated with document context, whereas FLORES200 segments were evaluated in isolation. We asked each rater to provide MQM annotations for all translation candidates for each source segment (we evaluted 15 systems on en-de and zh-en and 11 systems on others), and compute scores as described in Freitag et al. (2021). Scores range from 0 to 25, lower is better. To control for variance between raters, the same rater was used to score all candidate translations resulting from each source segment."}, {"title": "Results", "content": "Results are shown in Table 3. We observe that overall the best-performing system is rankAvg:noNC, which significantly outperforms greedy (p<0.001 on pairwise t-test). rankAvg:noNC also performs"}, {"title": "Ethics Statement", "content": "MBR decoding is resource-intensive, and using ensembles of multiple metrics increases computational complexity compared to a single utility metric. To mitigate this issue, we presented two-step ensembles that use QE filtering followed by MBR decoding, which reduce the computational cost below the cost of standard MBR decoding with a single metric."}, {"title": "Methodology Details", "content": "For each language pair, we obtained 5-shot examples for our prompts from the dev split of FLORES-200 by randomly sampling among those reference pairs that had perfect MetricX QE scores (scores of 0). We used MetricX QE filtering to ensure we used high-quality examples as our 5-shot examples. The sampled examples and prompt text for each language pair is included in our dataset release."}, {"title": "Instructions for Computing Metrics", "content": ",\n, chrF++, and TER scores were computed with sacreBLEU 2.4.2 (Post, 2018) on python 3.11.8 with the following parameters:\nchrF: -m chrf\nchrF++: -m chrf -chrf-word-order 2\n -m bleu -sentence-level\nTER: -m ter\nFor other metrics, we used the publicly released models on HuggingFace, running with the unbabel-comet package version 2.2.1 available on pip, on Python 3.10.14. We ran on an NVIDIA A100 GPU for all metrics except XCOMET-XXL and CometKiwi23-XXL, which required an NVIDIA A100 80GB GPU."}, {"title": "Metrics Included in Each Ensemble", "content": "This section presents the same information that is present in Table 2, but in textual format. The following are the groups of metrics included in the single-step ensembles that we include in our study. For each of these metric groups the rankAvg, rankMed, rankMax, and rank75q ensembling techniques are used to generate an ensemble.\n1.  all: All metrics, both reference-based and QE (MetricX, MetricX-QE, XCOMET-XXL, XCOMET-XL, CometKiwi23-XXL, CometKiwi23-XL, CometKiwi22, BLEURT, YiSi, chrF, chrF++, sentBLEU, TER, AfriCOMET and AfriCOMET-QE for African languages, IndicCOMET for Indic languages)"}, {"title": "Pseudocode for Ensembles", "content": "rankAvg ensembling strategy:\ndef rankAvg(\n):\nsample_list: List[str], metric_list: List[str]\nsample_ranks =\nget_ranks_for_samples_by_ensemble(sample_list,\nmetric_list)\nscore_list = [np.mean(x) for x in\nsample_ranks]\nreturn select_samples_by_score(sample_list,\nscore_list)\nrankMed ensembling strategy:\ndef rankMed (\n):\nsample_list: List[str], metric_list: List[str]\nsample_ranks =\nget_ranks_for_samples_by_ensemble(sample_list,\nmetric_list)\nscore_list = [np.median(x) for x in\nsample_ranks]\nreturn select_samples_by_score(sample_list,\nscore_list)\nrankMax ensembling strategy:\ndef rankMax (\n):\nsample_list: List[str], metric_list: List[str]\nsample_ranks =\nget_ranks_for_samples_by_ensemble(sample_list,\nmetric_list)\nscore_list = [np.max(x) for x in sample_ranks]\nreturn select_samples_by_score(sample_list,\nscore_list)\nrank75q ensembling strategy:\ndef rank75q(\n):\nsample_list: List[str], metric_list: List[str]\nsample_ranks =\nget_ranks_for_samples_by_ensemble(sample_list,\nmetric_list)\nscore_list = [np.quantile(x, q=[0.75])[0] for\nx in sample_ranks]\nreturn select_samples_by_score(sample_list,\nscore_list)\nHere are helper functions that were used:\ndef get_ranks_for_samples_by_ensemble(\nsample_list: List[str], metric_list: List[str]\noutput = [[None for y in metric_list] for x\nin sample_list]\nfor metric_idx, metric in\nenumerate (metric_list):\nsample_to_rank =\nrank_samples_by_metric(sample_list,\nmetric)\nfor sample_idx, sample in\nenumerate (sample_list):\noutput [sample_idx] [metric_idx] =\nsample_to_rank[sample]\nreturn output\ndef select_samples_by_score(\nsample_list: List[str],\nscore_list: List[float]\nsample_with_score = zip(sample_list,\nscore_list)\ntop_candidate, top_score =\nmin(sample_with_score, key=lambda x: x[1])\nreturn top_candidate"}, {"title": "Correlation Between Human Evaluation MQM Scores and Metrics", "content": null}]}