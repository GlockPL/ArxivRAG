{"title": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird's-Eye-View", "authors": ["Zichen Yu", "Quanli Liu", "Wei Wang", "Liyong Zhang", "Xiaoguang Zhao"], "abstract": "Recently, LSS-based multi-view 3D object detection provides an economical and deployment-friendly solution for autonomous driving. However, all the existing LSS-based methods transform multi-view image features into a Cartesian Bird's-Eye-View(BEV) representation, which does not take into account the non-uniform image information distribution and hardly exploits the view symmetry. In this paper, in order to adapt the image information distribution and preserve the view symmetry by regular convolution, we propose to employ the polar BEV representation to substitute the Cartesian BEV representation. To achieve this, we elaborately tailor three modules: a polar view transformer to generate the polar BEV representation, a polar temporal fusion module for fusing historical polar BEV features and a polar detection head to predict the polar-parameterized representation of the object. In addition, we design a 2D auxiliary detection head and a spatial attention enhancement module to improve the quality of feature extraction in perspective view and BEV, respectively. Finally, we integrate the above improvements into a novel multi-view 3D object detector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves the superior performance.", "sections": [{"title": "I. INTRODUCTION", "content": "MULTI-VIEW 3D object detection is a promising technology in the field of autonomous driving due to its low-cost deployment and rich semantic information [1], [2]. Early attempts [3]\u2013[5] approach this task primarily from the perspective of monocular 3D object detection, where monocular 3D object detection is first performed for each view, and then the predictions from all views are fused through post-processing. While feasible, this detection scheme ignores information across views, resulting in sub-optimal performance.\nRecently, many efforts [6]\u2013[10] integrate cross-view information with the help of the Bird's-Eye-View (BEV), which removes inefficient fusion post-processing, and achieve significant advances in both detection performance and efficiency.\nIn particular, the LSS-based paradigm utilizes the Lift-Splat-Shoot (LSS) [11] mechanism to construct an explicitly dense BEV representation and has become one of the mainstream solutions. It is mainly composed of four modules: an image-view encoder for image feature extraction, a view transformer to transform image features from image-view to BEV by per-pixel categorical depth estimation, a BEV encoder composed of a series of 2D convolution modules for further BEV feature extraction, a detection head for 3D object detection in BEV space. Among them, the view transformer module is pivotal to the whole framework, which bridges the gap between the image coordinate system and the Cartesian BEV coordinate system, integrating cross-view information into a unified Cartesian BEV representation. The Cartesian BEV representation is an intuitive and traditional representation, but it suffers from the following two limitations: 1). The uniform grid distribution of the Cartesian BEV representation mismatches the non-uniform image information distribution. As shown in the Fig. 1(a), the nearby region has richer and denser image information than the distant region, but the Cartesian BEV representation adopts the same grid granularity for all different distances. As a result, the nearby grid distribution is too sparse for the rich information, leading to information loss. On the contrary, the distant grid distribution is too dense for the sparse information, leading to computational redundancy. 2). The Cartesian BEV representation struggles to exploit the view symmetry of the surround-view cameras. As shown in the Fig. 2(a), for the same imaging in different cameras, they are transformed into view-symmetric BEV features after the view transformation. But the subsequent translation-invariant regular 2D convolution operation destroys this symmetry, which results in different features being learned at different azimuths, increasing the difficulty of representation learning.\nIn this paper, we innovatively choose the polar BEV representation to substitute the Cartesian representation. It rasterises the BEV space angularly and radially, which is conducive to overcoming the above limitations: 1). The grid distribution of the polar BEV representation is consistent with the distribution of image information, which is dense in the near and sparse in the far as shown in Fig. 1(b). Compared with the Cartesian BEV representation, the polar BEV representation is naturally able to capture finer-grained information in the nearby region, and it also reduces computational redundancy in the distant region. 2). The polar BEV representation can conveniently preserve the view symmetry of the surround-view cameras. As shown in the Fig. 2(b), assuming that different cameras capture the same imaging of the object, their features are approximately parallel in the polar BEV representation, so that azimuth-equivalent object features can be learned using just regular 2D convolution operations.\nHowever, it is non-trivial to switch to the polar BEV representation. To this end, we elaborately tailor three modules: a polar view transformer to generate the Polar BEV representation, a polar temporal fusion module for fusing cached historical polar BEV features and a polar detection head to predict the polar-parameterized representation of the object. Specifically, in the polar view transformer, we first lift the multi-view image features to the cylindrical coordinate system instead of the traditional Cartesian coordinate system, then the BEV space is angularly and radially rasterized, and finally the polar BEV representation is generated by bev pooling [12]. In the polar temporal fusion module, since ego-motion can be more conveniently formulated in the Cartesian coordinate system due to its linear property, we utilize the Cartesian coordinate as an intermediate proxy to align the multi-frame polar bev features. In the polar detection head, we adopt polar parameterization for 3D object detection to exploit the preserved view symmetry, allowing the azimuth-equivalent prediction targets to be learned. Through the cooperation of these three modules, the Cartesian BEV representation can be successfully replaced with the polar BEV representation.\nIn addition, the quality of image features in perspective view is crucial for detection performance [13], [14]. Many LSS-Based works [8], [15], [16] introduce explicit depth supervision, which significantly enhances the depth-awareness of image features, but neglects the object-awareness. In this paper, we directly guide the network to learn the object-aware feature representation in perspective view by introducing 2D auxiliary supervision. Specifically, we impose object classification supervision in perspective view to improve the semantic discriminability of image features. To enhance the sensitivity of the network to object location, we additionally apply supervision on 2D object bounding box and center regression. It is worth noting that these 2D auxiliary tasks are active only during training, so it does not slow down inference.\nNaturally, the feature extraction in the BEV space is also important. However, it is inevitable that a large amount of background noise exists in the BEV representation, which may interfere with the feature extraction for foreground objects. To mitigate the negative effects of background noise, we propose a spatial attention enhancement (SAE) module consisting of two convolutional layers. It predicts a spatial attention weight map for weighting with the BEV feature map, guiding the network to focus on the foreground region while suppressing the background noise.\nWe integrate the above improvements to form a novel framework called PolarBEVDet. In summary, the main contributions of this paper are as follows:\n\u2022\tThe polar BEV representation is proposed to replace the traditional Cartesian BEV representation for the LSS-based paradigm. It can mitigate the information loss in the nearby region and the computational redundancy in the\n\u2022\tfar region, while also conveniently preserving the view-symmetry of multiple views.\n\u2022\tThe 2D auxiliary supervision in perspective view is proposed, which enhances the object-awareness of image feature representations.\n\u2022\tThe spatial attention enhancement module is proposed to suppress the background noise and highlight the foreground information in BEV features, improving the quality of BEV representation.\n\u2022\tWe evaluate the proposed PolarBEVDet on the challenging benchmark nuScenes [17]. Without any bells and whistles, our PolarBEVDet achieves remarkable detection performance (63.5% NDS and 55.8% mAP) on the nuScnes test split."}, {"title": "II. RELATED WORKS", "content": "A. Multi-view Camera-only 3D Object Detection\nWhen dealing with multi-camera systems, previous works [3]\u2013[5] utilize monocular 3D object detection to process each image separately, and then merge the detection results through post-processing in a unified coordinate system. This paradigm cannot simultaneously exploit the information from multi-view images and tends to miss truncated objects. Recent methods transform multi-view features from perspective view to a unified BEV representation for detection, which is not only convenient for the integration of multi-view information, but also more conducive to downstream tasks such as object tracking and trajectory prediction. In general, these methods can be roughly categorized into LSS-based and query-based methods.\n1) LSS-based methods: As a pioneer of this paradigm, BEVDet [6] and BEVDet4D [7] follow the Lift-Splat-Shoot [11], which transforms the multi-view features into a dense BEV representation by predicting the categorical depth distribution for each pixel in the image feature map. BEVDepth [8] and BEVStereo [15] further introduce explicit depth supervision and stereo information to improve the quality of depth estimation. BEVPoolv2 [12] upgrades the view transformation process from the perspective of engineering optimization, significantly reducing computation and storage consumption. SA-BEV [18] proposes semantic-aware BEVPooling, which generates semantic-aware BEV features by filtering background information based on image semantic segmentation results. AeDet [19] proposes the azimuth-equivariant convolution (AeConv) and azimuth-equivariant anchor to preserve the radial symmetry properties of BEV features. To take full advantage of the temporal information, SOLOFusion [16] utilizes both short-term, high-resolution and long-term, low-resolution temporal stereo. All of the above methods adopt the Cartesian BEV representation and ignore the object-awareness of image features. As far as we know, we are the first to explore the polar BEV representation and exploit 2D auxiliary supervision to improve the object-awareness of image features in the LSS-based paradigm.\n2) Query-based methods: According to the meaning of query, query-based methods can be categorized into methods based on dense bev query and sparse object query.\nThe dense bev query-based methods generate a dense bev representation with the help of a set of grid-shaped learnable BEV queries. Specifically, BEVFormer [9], BEVFormerV2 [13] utilize temporal self-attention and spatial cross-attention to efficiently aggregate spatial-temporal information from multi-view images and historical BEV features into BEV queries.\nInspired by Transformer-based 2D object detection [20], [21], the sparse object query-based methods leverage object queries to represent 3D objects, and utilize the attention mechanism to aggregate features directly from multi-view images, thus avoiding explicit view transformation of image features. In particular, DETR3D [22] projects a set of 3D reference points generated by object queries into the multi-view images, and then samples the 2D features of the projection points to update the queries. However, this feature querying approach suffers from the problem of inadequate receptive fields. Therefore, PETR [10] and PETRv2 [23] propose the 3D position embedding to transform image features into 3D position-aware features, and then employ the global attention mechanism for the interaction between object queries and image features. Subsequently, StreamPETR [24] focuses on long temporal modeling to efficiently propagate long-term historical information through object queries, dramatically improving performance at negligible computational cost. To avoid computationally expensive global attention, SparseBEV [25] designs adaptive spatio-temporal sampling to generate sample locations guided by queries, and adaptive mixing to decode sampled features with dynamic weights from queries. While the query-based methods can achieve superior detection performance, they have higher localization errors and deployment efforts than the LSS-based paradigm.\nB. Polar-Based 3D Perception\n1) Polar-based 3D Lidar perception: Due to the uneven distribution of lidar points in space and even long-tailed distribution, PolarNet [26] proposes a point cloud segmentation network based on the polar bev representation, which balances the number of points in each grid cell and indirectly aligns the attention of the segmentation network with the long-tailed distribution of points along the radial axis. Building upon PolarNet, Panoptic-PolarNet [27] further implements instance segmentation by center regression on the polar BEV map to form an efficient panoptic segmentation framework for lidar point clouds. When dealing with stream-based lidar perception, PolarStream [28] adopts the polar coordinate system instead of the Cartesian coordinate system to reduce computation and memory waste. For Lidar-based 3D object detection, in order to alleviate the feature distortion problem of the polar representation, PARTNER [29] designs a global representation re-alignment module consisting of dual attention and introduces instance-level geometric information into the detection head, achieving excellent performance beyond the Cartesian-based methods.\n2) Polar-based 3D vision perception: PolarBEV [30] employs a height-based feature transformation to generate a polar bev representation for BEV semantic segmentation and"}, {"title": "III. METHOD", "content": "In this paper, we propose a novel multi-view 3D object detection framework, PolarBEVDet, in which we utilize the polar BEV representation instead of the Cartesian BEV representation commonly used in LSS-based methods. We elaborately tailor three modules for the polar BEV representation: a polar view transformer for polar BEV representation generation, a polar temporal fusion module for fusing cached historical polar BEV features and a polar detection head for predicting the polar-parameterized representation of the object. Furthermore, we impose the 2D auxiliary supervision and the spatial attention enhancement module to improve the feature representation quality in perspective view and BEV, respectively. The Image-view encoder and BEV encoder have no special design and simply follow the previous LSS-based methods [6], [8].\nA. Polar View Transformer\nIn polar view transformer, we follows the LSS paradigm [11] to transform image features \\(F_{img} = \\{F_{img}^k \\in \\mathbb{R}^{C \times H_F \times W_F}, k = 1,2,..., N_{view}\\}\\) into a dense BEV feature for subsequent perception. Lift and Splat are two key steps in the LSS paradigm, which are used to lift the image features from 2D to 3D space and feature aggregation for generating the BEV feature map, respectively.\nIn the Lift step, we preset a set of discrete depths \\(\\{d_0, d_1,..., d_{N_D-1}\\}\\) and utilize a depth estimation network to predict a depth distribution \\(D_k \\in \\mathbb{R}^{N_D \times H_F \times W_F}\\) over these discrete depths for each image pixel. The frustum points \\(P_k \\in \\mathbb{R}^{3 \times N_D \times H_F \times W_F}\\) and corresponding frustum features \\(F_{3D} \\in \\mathbb{R}^{C \times N_D \times H_F \times W_F}\\) are derived for each image, where the frustum features are obtained by scaling the image features with the predicted depth distribution. To obtain the polar BEV representation, we transform all the frustum points of each view into the cylindrical coordinate system instead of the Cartesian coordinate system consistently used in LSS-based methods. Specifically, given a frustum point \\(p = (u,v,d)\\), it is first projected to a 3D point \\(P_{cart} = (x,y,z)\\) in the Cartesian coordinate system according to the camera intrinsic and extrinsic as follows:\n\\([x, y, z,1]^T = T^{-1} K^{-1}[u * d, v * d, d, 1]^T,\\)\nwhere \\(T \\in \\mathbb{R}^{4x4}\\) and \\(K \\in \\mathbb{R}^{4x4}\\) denote the extrinsic and intrinsic matrices. Then, the corresponding cylindrical coordinate \\(P_{cyli} = (\\theta,r, z)\\) is computed as follows:\n\\(\\theta = arctan2(y, x),\\)\n\\(r = \\sqrt{x^2 + y^2}.\\)\nIn the Splat step, the BEV space is rasterized into a set of polar grids angularly and radially. The azimuth range [-\u03c0, \u03c0] is uniformly divided into \\(N_{\\theta}\\) intervals with an interval of \\(\\delta_{\\theta}\\), while the radial range \\([r_{min}, r_{max}]\\) is uniformly divided into \\(N_r\\) intervals with an interval of \\(\\delta_r\\). The polar grid index (i, j) associated with each frustum point is then computed as follows:\n\\(i = [(\\theta + \\pi)/\\delta_{\\theta}],\\)\n\\(j = [(r - r_{min})/\\delta_r].\\)\nThe frustum features belonging to the same polar grid are aggregated by sum pooling [12] so that the 3D frustum features are splatted into a 2D poalr BEV representation. Finally, to facilitate subsequent feature extraction via convolutional operations, we array this representation to obtain a regular feature map \\(F \\in \\mathbb{R}^{C \\times N_{\\theta} \\times N_r}\\).\nB. Polar Temporal Fusion\nTemporal information facilitates the detection of the occluded objects and can significantly improve the detection performance, especially the velocity prediction. In order to efficiently utilize the temporal information, we cache the polar bev features from previous T timestamps in memory. When needed, we align these historical features to the current timestamp for temporal fusion. This temporal alignment process involves finding the position of each element of the current polar bev feature map in the historical feature map based on ego-motion, and then warping the historical features to the current coordinate system by bilinear interpolation.\nHowever, describing the ego-motion in the polar coordinate system is challenging, while the linear properties of the Cartesian coordinate system make the motion formulation simpler and more intuitive. Therefore, we use the Cartesian coordinates as an intermediate proxy for the alignment of multi-frame polar bev features. Concretely, given a current polar bev feature map \\(F_t \\in \\mathbb{R}^{C \\times N_{\\theta} \\times N_r}\\) and a previous polar bev feature map \\(F_{t-1} \\in \\mathbb{R}^{C \\times N_{\\theta} \\times N_r}\\) (for simplicity, we use the case where T = 1 as an example), we compute the polar coordinate \\(p = (\\theta_t, r_t)\\) of the element at \\((i_t, j_t)\\) in the current polar bev feature map as follows:\n\\(\\theta_t = -\\pi + i_t \\times \\delta_{\\theta},\\)\n\\(r_t = r_{min} + j_t \\times \\delta_r.\\)\nThe corresponding Cartesian coordinate \\(p = (x_t, y_t)\\) is then computed for simplifying the coordinate transformation as follows:\n\\(x_t = r_t \\times cos(\\theta_t),\\)\n\\(y_t = r_t \\times sin(\\theta_t).\\)\nThe ego-motion leads to a position shift of the objects in the feature map, even for stationary objects. Therefore, we compensate for this shift by inverse transformation, obtaining the position \\(p_{t-1} = (x_{t-1}, y_{t-1})\\) of the current element in the previous Cartesian coordinate system as follows:\n\\([x_{t-1}, y_{t-1}]^T = R^{-1}[x_t, y_t]^T + T_{t-1},\\)\nwhere \\(R^{-1} \\in \\mathbb{R}^{2x2}\\) and \\(T_{t-1} \\in \\mathbb{R}^{2}\\) represent the horizontal rotation and translation matrices of the ego-vehicle from timestamp t to timestamp t \u2212 1. Then we compute its coordinate \\(p_{t-1} = (\\theta_{t-1}, r_{t-1})\\) in the previous polar coordinate system and its position \\((i_{t-1}, j_{t-1})\\) in the feature map in turn. Finally, the aligned feature map \\(F_{t-1}\\) at \\((i_t, j_t)\\) can be obtained by:\n\\(F_{t-1}(i_t, j_t) = bilinear(F_{t-1}, (i_{t-1}, j_{t-1})),\\)\nwhere \\(bilinear(F, P)\\) denotes the bilinear interpolation of the point P in the feature map F.\nThe aligned BEV feature map \\(F_{t-1}^{\\'}\\) is concatenated with the current BEV feature map \\(F_t\\) and then fused by the 1 \u00d7 1 convolution layers to generate the temporal poalr BEV feature \\(F_{temp} \\in \\mathbb{R}^{C'N_{\\theta} \\times N_r}\\).\nC. Polar Detection Head\nWe apply an anchor-free 3D object detection head following previous works [33] where the predicted target is the polar-parameterized representation of the object rather than the Cartesian-parameterized representation.\nThe original detection head contains a center heatmap branch and a group of attribute regression branches. The center heatmap branch outputs a 2D heatmap of K channels, each corresponding to one of the K object classes. It provides the object class and the rough location \\((C_x, C_y)\\) of the object\u2019s center in the Cartesian x-y plane. Meanwhile, the attribute regression branches utilize the center feature to predict the key attributes respectively: a sub-voxel center offset \\((O_x, O_y)\\) to refine the center position, the object\u2019s height z, the 3D dimensions (w, l, h), the object\u2019s orientation \\(a_c\\) and the velocity \\((V_x, V_y)\\) along the Cartesian x-y axes.\nHowever, the original detection head is built on the Cartesian BEV representation and parameterization, which is not compatible for our polar BEV representation. Moreover, as shown in Fig. 4(a), for the same imaging at different azimuths, it needs to regress different targets for the orientation and velocity of the object, inevitably increasing the prediction difficulty.\nTherefore, for the Polar BEV representation, we apply the polar parameterization to the object and tailor the polar detection head. In particular, the azimuth \u03b8 and radius r are used to describe the location of the object\u2019s center. To determine them, we predict a heatmap to locate the rough position \\((C_{\\theta}, C_r)\\) of the object\u2019s center in the polar coordinate system and a center offset \\((O_{\\theta}, O_r)\\) to compensate for the quantization error. For the orientation of the object, the conventional definition is the angle \\(a_c\\) between the object\u2019s forward direction and the Cartesian x-axis. In order to effectively utilize the view symmetry property preserved by the polar bev representation, we reformulate it as the angle \\(a_p\\) between the forward direction and the polar axis on which the object\u2019s center is located as follows:\n\\(a_p = a_c - \\theta.\\)\nSimilarly, the original velocity is decomposed into velocity components \\((v_x, v_y)\\) along the Cartesian x-axis and y-axis, respectively. We re-decompose it into radial velocity \\(v_r\\) and tangential velocity \\(v_{\\theta}\\) as prediction targets as follows:\n\\(v_r = V_{abs} \\times cos(a_{vel} - \\theta),\\)\n\\(v_{\\theta} = V_{abs} \\times sin(a_{vel} - \\theta),\\)\nwhere \\(V_{abs}\\) is the absolute value of velocity and \\(a_{vel}\\) is the angle between the motion direction and the Cartesian x-axis. For the height and dimensions of the object, the original definition and prediction targets are retained. As shown in the Fig. 4(b), the same imaging at different azimuths corresponds to azimuth-equivalent prediction targets by polar parameterization, which greatly eases the optimization difficulty.\nAs a result, the predicted object is represented as \\((\\theta = c_{\\theta} + O_{\\theta}, r = C_r+O_r, z, w, l, h, a_p, v_{\\theta}, v_r)\\), which can be effortlessly converted back to the result of the Cartesian parameterization.\nD. 2D Auxiliary Detection Head\nIn BEV perception system, the quality of image features in perspective view is crucial for the performance of 3D object detection [13], [14]. Ideally, image features should be depth-aware and object-aware. However, due to the existence of the view transformer, the final object detection in BEV space can only provide implicit and indirect supervision of these awareness capabilities. This leads to confusing feature representation learning in the perspective view. Many LSS-Based works [8], [15], [16] have introduced explicit depth supervision from LiDAR, remarkably improving the depth-awareness of image features, but neglecting the object-awareness.\nIn this paper, we directly guide the network to learn the object-aware feature representation in perspective view by introducing auxiliary 2D detection and corresponding supervision as follows:\n1) Classification: The classification task aims to differentiate between object classes, including the background, car and pedestrian, etc.. By applying auxiliary classification supervision in the perspective view, image feature representations with higher semantic discriminabilitycan be learned. Therefore, we attach a lightweight classification branch comprising two convolutional layers to the image features for predicting the confidence scores of the K object classes. During training, Hungarian matching [20] is adopted for positive/negative sample assignment and the generalized focal loss [34] is used for classification supervision.\n2) Box Regression: To enhance the network\u2019s sensitivity to object position, we introduce an auxiliary supervision for 2D object detection in perspective view, which forces the network to learn more discriminative features for localization. These features are subsequently transformed into the BEV space to generate BEV representations with high position sensitivity, enabling more precise object localization. In parallel with the object classification branch, we attach another branch to predict the distance from the pixel position to the 4 sides of the bounding box following FCOS [35]. In the training phase, the 2D GT bounding boxes are obtained by projecting the annotated 3D bounding boxes to the image, and L1 loss and GIoU [36] loss are used for regression supervision.\n3) Center Regression: Object detection in BEV space is center-based, so the sensitivity to the object center is very crucial. Inspired by this, in order to further improve the precision of object localization, we introduce a center regression branch to predict the offset from the pixel position to the object center and supervise it with L1 loss. Besides, we further predict a center heatmap for locating the object center and supervise it with penalty-reduced focal loss [37]. The ground-truth center is the projection point of the 3D object\u2019s center in the image.\nE. Spatial Attention Enhancement\nFor 3D object detection task, it is necessary to focus more on the foreground information. However, it is inevitable that substantial background noise exists in the BEV feature representations, which may interfere with the feature extraction of foreground objects. To mitigate the negative impact of background noise, we propose a spatial attention enhancement (SAE) module. It improves the quality of BEV feature representation and detection performance by guiding the network to focus on the foreground regions while suppressing the background noise.\nAs shown in Fig., given the extracted BEV feature map \\(F \\in \\mathbb{R}^{C \\times N_{\\theta} \\times N_r}\\), a attention weight map \\(M \\in \\mathbb{R}^{1 \\times N_{\\theta} \\times N_r}\\) is predicted by feeding it to a subnetwork \\(\\Phi_s\\), followed by a sigmoid function. Its expected values in the foreground and background regions are close to 1 and 0, respectively. By weighting the BEV features with it, the features in the foreground region are highlighted and the background noise is suppressed, enabling adaptive feature enhancement. Furthermore, to prevent incorrect information suppression and stabilize the training, a skip connection is employed. Finally, the enhanced BEV features \\(F\u2019 \\in \\mathbb{R}^{C \\times N_{\\theta} \\times N_r}\\) can be represented as:\n\\(F\u2019 = (1 + M)F, M = sigmoid(\\Phi_s(F)),\\)\nwhere the sub-network \\(\\Phi_s\\) consists of a 3 \u00d7 3 convolutional layer followed by batch normalization and ReLU activation, and concludes with a 1 \u00d7 1 convolutional layer for the final output."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Evaluation Metric\nWe evaluate our model on the nuScenes [17", "metrics": "mean Average Translation Error (mATE)", "16": "in which single-task detection head replaces multi-task detection head to speed up the inference", "6": ".", "39": "optimizer with weight decay of 1e-7/1e-2. The learning rate is initialized to 1e-4", "40": "ResNet101 [40", "41": "backbones. The ResNet50 and ResNet101 backbones are initialized by ImageNet [42", "17": "pre-training", "5": "checkpoint. The model is trained for 60 epochs for ResNet50 and ResNet101 without CBGS strategy", "split": "In Tab. I"}, {"16": "by a clear margin of 1.9% mAP and 2.1% NDS", "25": "by 1.4% mAP and 1.0% NDS. By equipping the nuImages [17", "40": "as the image backbone and scaling up the input size to 512\u00d71408", "split": "In Tab. II"}, {"16": "with ConvNext-Base [43", "41": "backbone. In addition", "25": "PolarBEVDet significantly outperforms it by 1.5% mAP and 0.8% NDS and achieves 7.3% remarkably less mATE", "analysis": "To analyze the impact of each component in PolarBEVDet", "distances": "The Cartesian BEV and the Polar BEV representation possess distinctly different grid distribution. In Fig. 5(a)", "azimuths": "When a vehicle turns at a large angle, the camera orientation changes significantly. In order to be able to accurately detect surrounding objects despite this situation, the autonomous driving system needs to be robust to changes in azimuth. To reveal the effect of polar and Cartesian representations on the detection robustness to different azimuths, we follow AeDet [19", "LEFT\u2019": "becomes the revolved view [\u2018BACK LEFT\u2019, \u2018FRONT LEFT\u2019, \u2018FRONT\u2019, \u2018FRONT RIGHT\u2019, \u2018BACK RIGHT\u2019, \u2018BACK\u2019"}]}