{"title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders", "authors": ["Maya Varma", "Ashwin Kumar", "Rogier van der Sluijs", "Sophie Ostmeier", "Louis Blankemeier", "Pierre Chambon", "Christian Bluethgen", "Jip Prince", "Curtis Langlotz", "Akshay Chaudhari"], "abstract": "Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain.", "sections": [{"title": "1 Introduction", "content": "Medical images (e.g. X-rays, computed tomography (CT) scans) are essential diagnostic tools in clinical practice. Since medical conditions are often characterized by the presence of subtle features, images are generally acquired with high spatial resolution and large fields of view in order to capture the required level of diagnostic detail for interpretation by radiologists [22]. However, high-resolution medical images, especially volumetric (3D) images, can result in large data storage costs and increased or even intractable computational complexity for downstream computer-aided diagnosis (CAD) models [14, 46]. This is likely to become a significant concern in the near future due to the rapid growth of medical imaging volumes stored by hospitals [35], the expanding use of CAD tools in the clinic [11, 39], and newer paradigm shifts towards large-scale foundation models [3, 4, 7]. Many existing CAD models address this challenge by interpolating images to lower resolutions, despite the lower performance of models trained on interpolated data [20, 43].\nA promising solution lies in powerful autoencoder methods, which are capable of encoding images as downsized latent representations and decoding latent representations back to images. Recent works, particularly in the context of latent diffusion models, have demonstrated that downsized latent representations can capture relevant spatial structure from high-resolution input images while simultaneously improving efficiency on tasks such as image generation [41]. These findings suggest that autoencoders may hold potential for addressing the aforementioned storage and efficiency challenges in the medical domain by encoding high-resolution images as downsized latent representations, which can be used to develop downstream CAD models at a fraction of the computational cost.\nSeveral large-scale autoencoders have been introduced in recent years [32, 41]; however, directly applying these models to the medical domain is challenging since medical images include a diverse range of clinically-relevant features (e.g. tumors, lesions, fractures), anatomical regions of focus (e.g. head, chest, knee), and modalities (e.g. 2D and 3D images). An effective generalizable autoencoding approach in the medical image domain must operate across a wide range of medical images and preserve clinically relevant features in both downsized latents as well as decoded reconstructions. However, existing autoencoder models are either (a) developed for natural images [41], which represent a significant domain shift from medical images, or (b) developed for a focused set of medical images (e.g. chest X-rays) [32] and are not explicitly trained to preserve clinically-relevant features across diverse medical images.\nIn this work, we address these limitations by introducing MedVAE, a family of 6 large-scale, generalizable 2D and 3D autoencoder models developed for the medical image domain. We first curate a large-scale training dataset with over one million 2D and 3D images, and we perform model training using a novel two-stage training scheme designed to optimize quality of latent representations and decoded reconstructions.\nWe evaluate the quality of latent representations (using 8 CAD tasks) and reconstructed images (using both automated and manual perceptual quality evaluations) with respect to the preservation of clinically-relevant features. Evaluations are derived from 20 multi-institutional, open-source medical datasets with 4 imaging modalities (X-ray, full-field digital mammograms, CT, and magnetic resonance imaging) and 8 anatomical regions. We measure the extent to which MedVAE latent representations and reconstructed images can contribute to downstream storage and efficiency benefits while simultaneously preserving clinically-relevant features. Ultimately, our results demonstrate that (1) downsized MedVAE latent representations can be used as drop-in replacements for high-resolution images in CAD pipelines while maintaining or exceeding performance; (2) downsized latent representations reduce storage requirements (up to 512x) and improve downstream efficiency of CAD model training (up to 70x in model throughput) when compared to high-resolution input images; and (3) decoded reconstructions effectively preserve clinically-relevant features as verified by an expert reader study. Our results also demonstrate that MedVAE models outperform existing natural image autoencoders.\nUltimately, our work demonstrates the potential that large-scale, generalizable autoencoders hold in addressing the critical storage and efficiency challenges currently faced by the medical domain. Utilizing MedVAE latent representations instead of original, high-resolution images in training pipelines can improve model efficiency while preserving clinically-relevant features."}, {"title": "2 Results", "content": "Autoencoding methods are capable of encoding high-resolution images as downsized latent representations. For a given 2D input image with dimensions $H \\times W$ with $B$ channels, an autoencoding method will output a downsized latent representation of size $H/(\\sqrt{f}) \\times (W/\\sqrt{f})\\times C$. Here, $f$ represents the downsizing factor applied to the 2D area of the image and $C$ represents a pre-specified number of latent channels. 3D autoencoding methods follow a similar formulation, where input images are 3D in nature with dimensions $H \\times W \\times S$ with $B$ channels. Here, the downsizing factor $f$ is applied to the 3D volume of the image; as a result, the latent representation will have dimensions $(H/(\\sqrt{f}) \\times (W/\\sqrt{f}) \\times (S/(\\sqrt{f}) \\times C$. Autoencoding methods are also capable of decoding latent representations back to reconstructed high-resolution images.\nWe aim to develop large-scale, generalizable medical image autoencoders capable of preserving diverse clinically-relevant features in both latent representations and reconstructions. To this end, we first collect a large-scale training dataset with 1,021,356 2D images and 31,374 3D images curated from 19 multi-institutional, open-source datasets [1, 2, 5, 8, 10, 13, 17, 23, 25-28, 31, 37, 40, 42, 44, 45, 52]. Images are obtained from two chest X-ray datasets, six full-field digital mammogram (FFDM) datasets, four T1- and T2-weighted head magnetic resonance imaging (MRI) datasets, one knee MRI dataset, two head/neck CT datasts, two whole-body CT datasets, and two chest CT datasets."}, {"title": "2.1 Training MedVAE autoencoders", "content": "We then utilize this dataset to train a family of generalizable autoencoders for medical images. Motivated by prior work on natural images [41], we utilize variational autoencoders (VAEs) as the model backbone. We perform model training using a novel two-stage training scheme designed to optimize quality of latent representations and decoded reconstructions. Specifically, the first stage involves training base autoencoders using 2D images (Fig. 1a); we maximize the perceptual similarity between input images and reconstructed images using a perceptual loss [54], a patch-based adversarial objective [24], and a domain-specific embedding consistency loss. Whereas existing works on autoencoders train using only this stage, the medical image domain introduces the added complexity of subtle, fine-grained features required for clinical interpretation;"}, {"title": "2.2 Latent representation quality", "content": "We first evaluate whether clinically-relevant features are preserved in MedVAE latent representations. To this end, we measure the extent to which latent representations can serve as drop-in replacements for high-resolution input images in CAD pipelines without any customization or modifications to CAD model architectures.\nWe evaluate latent representation quality using the following 8 CAD tasks: malignancy detection on 2D FFDMs [5], calcification detection on 2D FFDMs [5], BI-RADS prediction on 2D FFDMs [40], bone age"}, {"title": "2.3 Storage and efficiency benefits of latent representations", "content": "Next, we evaluate the extent to which downsized MedVAE latent representations can reduce storage requirements and improve downstream efficiency of CAD pipelines when compared to high-resolution input images. Using a 2D high-resolution network (HRNet_w64) and 3D squeeze-excitation network (SEResNet-152) as our base CAD architectures, we report latency, throughput, and maximum batch size. Latency is the time (in milliseconds) to perform a forward pass of the network on one batch. Throughput is the number of samples that can be evaluated by the network in one second. Finally, we report the maximum batch size (in powers of 2) for a forward pass that will fit on a single A100 GPU (2D) and an A6000 GPU (3D). We assume a high-resolution input image size of 1024 \u00d7 1024 with 1 channel for 2D settings and an input volume size of 256 x 256 \u00d7 256 with 1 channel for 3D settings.\nResults are provided in Figure 2. We demonstrate that training CAD models directly on downsized latent representations can lead to large improvements in model efficiency. In the 2D setting, we observe that as the downsizing factor increases to $f = 64$, the latency decreases by 69x, the throughput increases by 70x, and the maximum batch size increases by 32x. In the 3D setting, as the downsizing factor increases to $f = 512$, the latency decreases by 62x, the throughput increases by 55x, and the maximum batch size increases by 512x. Storage costs decrease proportionally with the downsizing factor (i.e. 64x for 2D and 512x for 3D)."}, {"title": "2.4 Reconstructed image quality", "content": "We evaluate whether clinically-relevant features are preserved in reconstructed images using both automated and manual perceptual quality evaluations. These evaluations quantify the extent to which the encoding and subsequent decoding processes retain relevant features.\nFor automated evaluations, we use perceptual metrics to compare reconstructed images with the original inputs. We report peak signal-to-noise ratio (PSNR) and the multi-scale structural similarity index measure (MS-SSIM). For 2D evaluations, we measure perceptual quality on X-rays [13, 28]; FFDMs [5, 26, 37, 40, 42, 44]; and musculoskeletal X-rays [38]. In addition to full-image evaluations, we additionally include a fine-grained perceptual quality assessment, where we extract regions containing wrist fractures by using bounding boxes [38]; then, the original region and reconstructed region are compared using perceptual metrics. For 3D evaluations, we compute metrics on brain MRIs [10, 23, 25, 31]; head CTs [8]; abdomen CTs [27]; CTs from a wide range of anatomies [52]; lung CTs [1]; and knee MRIs [2].\nIn Table 3, we compare 2D MedVAE with interpolation methods and large-scale natural image autoencoders across four types of 2D images. We find that 2D MedVAE achieves the highest perceptual quality across all"}, {"title": "3 Discussion", "content": "High-resolution medical images can result in large data storage costs and increased or intractable computational complexity for trained models. As the volume of data stored by hospitals continues to increase and large-scale foundation models become more commonplace, methods for inexpensively storing and efficiently processing high-resolution medical images become a critical necessity. In this work, we aim to address this need by introducing MedVAE, a family of 6 large-scale autoencoders for medical images developed using a novel two-stage training procedure. MedVAE encodes high-resolution medical images as downsized latent representations. We demonstrate with extensive evaluations that (1) downsized latent representations can effectively replace high-resolution images in CAD pipelines while maintaining or exceeding performance, (2) downsized latent representations reduce storage requirements (up to 512x) and improve downstream efficiency (up to 70x in model throughput) when compared to high-resolution input images, and (3) reconstructed images effectively preserve relevant features necessary for clinical interpretation by radiologists.\nSeveral prior works have introduced powerful autoencoders capable of generating downsized latents for images. In particular, recent work on latent diffusion models has involved the development of several large-scale autoencoders, such as VQ-GANs and VAEs, trained on eight million natural images [12, 29, 30, 41]; downsized latents generated by these models were shown to capture relevant spatial structure as well as improve efficiency of downstream diffusion model training [41]. However, recent works have demonstrated that models trained on natural images often generalize poorly to medical images due to significant distribution shift [6, 15, 48], suggesting that existing natural image autoencoders may not be well-suited for the complexity of the medical image domain. Our evaluations on both latent representations and reconstructed images support this point, demonstrating that existing large-scale natural image autoencoders consistently underperform our domain-specific medical image autoencoders. These findings demonstrate the need for domain-specific models capable of understanding complex and fine-grained patterns across diverse imaging modalities and anatomical regions.\nOur work aims to reduce computational costs associated with automated medical image interpretation by proposing the use of training datasets comprised of downsized MedVAE latent representations rather than high-resolution medical images. For instance, given a chest X-ray training dataset with images of size 1024 \u00d7 1024 with 1 channel, our 2D MedVAE model with $f = 64$ and $C = 1$ can generate downsized latent representations of size 128 \u00d7 128 with 1 channel, contributing to substantial downstream efficiency and storage benefits. We demonstrate with eight CAD tasks that latent representations do not result in the loss of clinically-important information; at a 2D downsizing factor of $f = 16$ and a 3D downsizing factor of $f = 64$, we observe equivalent or better performance than high-resolution images with substantial improvements over multiple existing downsizing methods. MedVAE models can also generalize beyond the images included in the training set, as shown by performance on 2D musculoskeletal X-rays and 3D spine CTs. Importantly, the efficiency benefits of using latent representations are significant; in particular, using latent representations can contribute to large increases in batch sizes, which can be particularly useful in the modern era of self-supervised foundation models that rely heavily on the use of large batch sizes during training.\nThe MedVAE autoencoder family includes two 3D autoencoders that are explicitly designed to downsize 3D medical imaging modalities (e.g. CT, MRI), a previously underresearched setting. Our results demonstrate that at a 3D downsizing factor of $f = 64$, the volumetric latent representations generated by 3D MedVAE are substantially higher quality than those generated by stitching together 2D slices downsized using 2D baselines. This suggests that 3D autoencoders can better capture clinically-important volumetric patterns, such as fractures that span multiple slices. Efficiency benefits in the 3D setting are also notable, particularly since training downstream CAD models on high-resolution 3D volumes is often computationally expensive or intractable. At significantly higher downsizing factors ($f = 512$), we observe the benefits of 3D autoencoder training to be less pronounced, suggesting that users will need to carefully consider the tradeoffs between latent representation quality and desired downstream efficiency when selecting a MedVAE model.\nIn addition to generating high-quality latent representations, MedVAE models also include a trained decoder, which can reconstruct the original high-resolution image from the downsized latent. This is a particularly useful capability in the medical imaging domain, since high-resolution images are necessary for effective clinical interpretation by radiologists. We demonstrate with a reader study consisting of three radiologists that reconstructed images can effectively preserve clinically-relevant signal needed for diagnoses; in this setting,"}, {"title": "4 Methods", "content": "In this section, we provide background information on autoencoders.\n2D autoencoding methods can be formulated as follows. We begin with a training dataset $D = \\{x_i\\}_{i=1}^{N}$ consisting of $N$ high-resolution input images $x_i \\in X$. Each high-resolution image $x_i$ has dimensions $H\\times W$ with $B$ channels, which can be expressed as $x_i \\in \\mathbb{R}^{H\\times W\\times B}$. An autoencoding method learns an encoding function $g : X \\rightarrow Z$, where $Z$ represents a low-dimensional latent space and $z_i \\in Z$ represents the downsized latent representation corresponding to the input $x_i$. Let $f$ represent the downsizing factor applied to the 2D area of the image; then, the latent representation $z_i$ can be expressed as $z_i \\in \\mathbb{R}^{(H/(\\sqrt{f})\\times (W/(\\sqrt{f})\\times C}$, where $C$ is a pre-specified number of latent channels. Autoencoding methods also learn a decoding function $h : Z \\rightarrow X$, which reconstructs the image $\\hat{x_i}$ from the latent representation $z_i$. The encoding and decoding functions $g$ and $h$ are optimized in an end-to-end manner with the goal of maximizing perceptual similarity between $x_i$ and $\\hat{x_i}$.\n3D autoencoding methods follow a similar formulation, where each image $x_i$ represents a 3D volume with dimensions $H\\times W\\times S$ with $B$ channels. Here, the downsizing factor $f$ is applied to the 3D volume of the image; as a result, the latent representation $z_i$ can be expressed as $z_i \\in \\mathbb{R}^{(H/F)\\times (W/F)\\times (S/(VF)\\times C}$, where $C$ is a pre-specified number of latent channels."}, {"title": "4.1 Background", "content": null}, {"title": "4.2 Curating a large-scale training dataset", "content": "We first collect a large-scale, open-source training dataset $D$ for training medical image autoencoders. We incorporate diverse modalities and anatomical features in order to ensure that trained autoencoders gain proficiency in processing the wide variety of diagnostic features that occur in medical images. Our dataset consists of 1,021,356 2D images and 31,374 3D images obtained from 19 multi-institutional, open-source datasets.\n2D images include chest X-rays and FFDMs, selected because (a) chest X-rays are well-studied with large amounts of publicly-available data and (b) FFDMs are a challenging class of images due to large dimensions and the presence of fine-grained features critical for diagnoses (e.g. microcalcifications). We collect images from two chest X-ray datasets and six FFDM datasets [5, 13, 26, 28, 37, 40, 42, 44].\n3D images include head MRIs, knee MRIs, and high-resolution whole-body (head, neck, abdomen, chest, lower limb) CTs. We selected these datasets since (a) head MRIs/CTs are a commonly obtained examination, and (b) high-resolution CTs tend to contain subtle features and consume large amounts of storage. These images were curated from four T1- and T2-weighted head MRI datasets (14,296), one knee MRI dataset (3,564), two head/neck CT datasets (10,156), two whole-body CT datasets (1,434), and two chest CT datasets (1,924) [1, 2, 8, 10, 17, 23, 25, 27, 31, 45, 52]."}, {"title": "4.3 Training autoencoders for medical images", "content": "In this section, we discuss our two-stage approach for training generalizable autoencoders for medical images. Motivated by prior work on natural images [41], we elect to use variational autoencoders (VAEs) as our backbone. In the first stage of training, we optimize for reconstruction quality by maximizing perceptual similarity between the input image $x$ and the reconstructed image $\\hat{x}$. Whereas existing works train autoencoders solely using this approach, the medical image domain introduces the added complexity of subtle, fine-grained features required for clinical interpretation of images; thus, we introduce a second stage of training, where the latent representation space $Z$ is refined with continued fine-tuning. Our approach is intended to explicitly preserve diverse clinically-relevant features in both latent representations and reconstructed images. In total, the MedVAE family includes four 2D VAEs and two 3D VAEs trained with various downsizing factors.\nStage 1: Training Base Autoencoders (Fig. 1a). We begin by performing base training of the autoencoders using the collected 2D images in order to optimize the quality of reconstructions $\\hat{x}$. In line with prior work [41], each MedVAE autoencoder learns an encoder and decoder (corresponding to functions $g$ and $h$) end-to-end using a fully convolutional VAE. Each MedVAE autoencoder accepts single-channel, high-resolution medical images $x_i$ as input, applies function $g$ to transform the input to a downsized latent representation $z_i$, and"}, {"title": "4.4 Evaluating latent representations", "content": "We evaluate the quality of latent representations $z$ with a set of eight clinically-relevant CAD tasks, which directly evaluate the preservation of clinically-relevant features in 2D and 3D images (Appendix Table 1). For each CAD task, we measure the difference in classification performance between models trained using latent representations and those trained using original, high-resolution images; this serves as an indicator of latent quality by directly measuring the retention of important diagnostic features. These evaluations also provide insights into potential performance gains afforded by training downstream models directly on MedVAE latent representations rather than high-resolution images.\nBelow, we provide implementation details for each 2D CAD task.\n1. Malignancy Detection: We evaluate the quality of FFDM latent representations on a binary\nmalignancy detection task, which involves predicting the presence or absence of a malignancy. We use\nimages from the Chinese Mammography Dataset (CMMD), which includes a total of 5202 deidentified\nFFDMs from 1775 patients [5, 9]. CMMD includes labels indicating the presence of masses and\ncalcifications as well as biopsy-confirmed labels indicating benign and malignant findings. We assigned\n80% of patients to the training set (1420 patients with 2982 images) and the remaining 20% to the test set\n(355 patients with 762 images). The average size of an FFDM after preprocessing was 1999.2 \u00d7 793.9 \u00d7 1."}, {"title": "4.5 Evaluating reconstructed images", "content": "We evaluate the quality of reconstructions $\\hat{x}$ using both automated and manual perceptual quality evaluations. Perceptual quality assessments measure information loss resulting from the autoencoding process by comparing the original image to the reconstructed (decoded) image. These evaluations quantify the extent to which the encoding and subsequent decoding process retains relevant features.\nFor 2D images, we evaluate full-image perceptual quality on chest X-rays, FFDMs, and musculoskeletal X-rays; we also evaluate fine-grained perceptual quality on musculoskeletal X-rays. Chest X-rays are obtained from CANDID-PTX [13] and MIMIC-CXR [28]; FFDMs are obtained from RSNA Mammography [42], VinDR-Mammo [40], CSAW-CC [44], EMBED [26], CMMD [5], and INBreast [37]; musculoskeletal X-rays are obtained from GRAZPEDWRI-DX [38]. We compute two standard perceptual quality metrics: PSNR and MS-SSIM. For 2D fine-grained perceptual quality evaluations, we extract 7677 images containing fractures from GRAZPEDWRI-DX, and we use bounding boxes provided by the authors to isolate the region of the fracture [38]. We then compute PSNR scores on these regions.\nFor 3D full-volume perceptual quality evaluations, we evaluate full-image perceptual quality on head MRIs, head CTs, abdomen CTs, whole-body CTs, lung CTs, and knee MRIs. Head MRIs are obtained from Alzheimer's Disease Neuroimaging Initiative (ADNI) [25], Harvard Aging Brain Study (HABS) [10], A4 dataset [23], and Open Access Series of Imaging Studies (OASIS) brain dataset [31]; head CTs are obtained from CQ500 [8]; whole-body CTs are obtained from TotalSegmentator dataset [52]; abdomen CTs are obtained from the Abdominal Multi-Organ Segmentation (AMOS) dataset [27]; lung CTs are obtained from LIDC-IDRI [1]; and knee MRIs are obtained from MRNet [2]. For each volume, a center crop of volume dimensions 160 \u00d7 160 \u00d7 160 was extracted. For the AMOS and CQ500 datasets, the crop region was expanded to dimensions 320 \u00d7 320 \u00d7 160 to include both soft-tissue and bony features. We compute two standard perceptual quality metrics: PSNR and MS-SSIM.\nFor manual evaluations of reconstructed image quality, we perform a reader study with 3 radiologists. Each expert reader is presented with a pair of chest X-rays, consisting of an original high-resolution image $x$ on the left and a reconstructed image $\\hat{x}$ on the right (Appendix Fig. 1). A total of 50 unique chest X-rays with fractures, randomly sampled from CANDID-PTX, are selected and presented in a randomized order [13]. The reader study poses three distinct questions on image fidelity, preservation of clinically-relevant features, and the presence of artifacts. Each question is scored based on a 5-point Likert scale ranging between -2 and 2. Below, we provide additional details on each of these questions:\n1. Image Fidelity: This question aims to assess how closely the reconstructed CXR image resembles the\noriginal image in terms of image fidelity considering the overall similarity, level of detail preservation,\nand visual quality. A higher rating indicates a closer resemblance to the original image, while a lower\nrating implies a greater deviation or degradation.\n2. Preservation of clinically-relevant features: This question evaluates the extent to which the\nreconstructed chest X-rays image preserves the diagnostic information present in the original image\ngiven the clarity and visibility of anatomical structures, abnormalities, and other important diagnostic\nfeatures. A higher rating indicates a greater preservation of diagnostic information, while a lower rating\nsuggests a significant loss that may affect the accuracy of diagnosis."}, {"title": "4.6 Statistical analysis", "content": "For latent representation evaluations, we report classification performance using AUROC, calculated using the torchmetrics library. We report mean and standard deviations across three runs with different random seeds. For automated perceptual quality evaluations on 2D images, we calculate PSNR and MS-SSIM on a random sample of 1000 images for each image type; we report mean and standard deviations across four runs with different random seeds. For automated perceptual quality evaluations on 3D images, we calculate PSNR and MS-SSIM on a single random sample of 100 images for each image type. For manual perceptual quality evaluations with expert readers, we report mean scores and 95% confidence intervals across three readers."}]}