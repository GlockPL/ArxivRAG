{"title": "Audio Explanation Synthesis with Generative Foundation Models", "authors": ["Alican Akman", "Qiyang Sun", "Bj\u00f6rn W. Schuller"], "abstract": "The increasing success of audio foundation models across various tasks has led to a growing need for improved interpretability to understand their intricate decision-making processes better. Existing methods primarily focus on explaining these models by attributing importance to elements within the input space based on their influence on the final decision. In this paper, we introduce a novel audio explanation method that capitalises on the generative capacity of audio foundation models. Our method leverages the intrinsic representational power of the embedding space within these models by integrating established feature attribution techniques to identify significant features in this space. The method then generates listenable audio explanations by prioritising the most important features. Through rigorous benchmarking against standard datasets, including keyword spotting and speech emotion recognition, our model demonstrates its efficacy in producing audio explanations.", "sections": [{"title": "I. INTRODUCTION", "content": "Generating explanations for large artificial intelligence (AI) models has been gaining importance as they are used in various domains such as audio processing and computer vision. Most existing explainable artificial intelligence (XAI) methods try to extract important features in the input space towards the model's final decision, that can be categorised into perturbation-based [1]\u2013[4] and backpropagation-based [5]\u2013[9] techniques [10]. These methods aim to identify relevant input features, such as pixels for computer vision tasks and tokens for natural language processing tasks. On the other hand, providing audio explanations is a useful method due to its intuitiveness on audio-based tasks and higher expressiveness over other modalities in specific scenarios, such as where understanding visual explanations needs expertise [11]. Aiming to generate listenable and interpretable audio explanations, [12], [13] exploit non-negative matrix factorisation (NMF) [14] to decompose audio into meaningful components.\nFoundation models are extensively used in audio processing to achieve state-of-the-art performance on various tasks such as automatic speech recognition, keyword spotting, and speaker recognition [15]\u2013[21]. In addition to that, these models offer a generalised and meaningful embedding space due to their broad range of training data; some foundation models such as EnCodec [22] enable generation from this space. Although certain studies target to explain transformer-based foundation models by leveraging their attention mechanism and presenting attention weights as explanations [23]\u2013[25], they do not consider computing feature importance in the meaningful embedding space to understand model behaviour. Testing with Concept Activation Vectors (TCAV) [26] and Network Dissection [27] focus on explaining a model's behaviour with provided concepts by exploiting the model internal representation. However, these methods require user-defined concepts without considering unleashing the learnt concepts which are already embedded in the latent space of a foundation model.\nTo address these issues, we propose a method which com-bines prominent feature attribution methods with foundation models to explain model behaviour in audio processing tasks. We first exploit an audio foundation model as an encoder, and train an additional model on this backbone depending on the type of the downstream task. To understand the behaviour of the final model on the task, we analyse important features for a decision in the latent space using a feature attribution method. In the final step, we use the generative part of the foundation model to construct the relevant audio in the input space. We verify that our method can generate high-fidelity explanations through experiments that simulate removing relevant features and assess the original model's performance on these essential features. The main contributions of this study are as follows:\n\u2022\tWe propose a novel audio explanation method that inte-grates common feature attribution methods into the latent space of foundation models. Our approach takes advantage of this meaningful space for creating understandable explanations without mapping feature relevance to the input space where individual features are difficult to interpret like audio frequencies.\n\u2022\tOur method leverages the generative capacity of foun-dation models from latent space to produce meaningful audio explanations in the input space. In this way, our method generates audio explanations in a listenable format that are interpretable to the end-user.\n\u2022\tWe evaluate our model on keyword spotting and speech emotion recognition tasks. We show that while our method provides high-fidelity explanations, it captures meaningful high-level audio components for the investigated tasks."}, {"title": "II. RELATED WORK", "content": "Adapting existing feature attribution methods to understand audio model predictions is a common practice. [28] explores the interpretability of deep audio models through the utilisation of layer-wise relevance propagation (LRP) [5], [29], a technique that computes relevance scores for each neuron in a deep neural network by recursively propagating relevance scores from the output. They examine the correlation between feature relevance scores and fundamental concepts like phonemes and distinct frequency ranges in classification tasks related to spoken digits and speaker gender. In [30], the authors use DFT-LRP [31], a recently introduced variant of LRP integrating Fourier transformation, to explain audio event detection models with different architectures. They evaluate the importance of individual time-frequency components regarding the predicted classes of the models. However, there is still room for enhancement in interpreting the feature importance maps provided by these methods.\nDecomposing an audio input into meaningful components offers a practical approach for identifying the audio elements pertinent to XAI. CoughLIME [12] extends the LIME method to explain audio processing models tailored specifically for cough data. A critical aspect of CoughLIME, distinguishing it from applying standard LIME to audio spectrograms, involves decomposing the input audio into interpretable components using NMF. The authors in [13] introduce an interpreter network built from scratch, incorporating NMF as an audio decomposition technique. Their network is trained to develop surrogate models that replicate the output of the original classifier and generate temporal activations of pre-learnt NMF components. However, these methods primarily focus on computing relevance in the input space without delving into the intermediate representations within a deep model."}, {"title": "III. METHODOLOGY", "content": "This section elaborates on the design of our method to gener-ate meaningful audio explanations. We begin by introducing the general structure of audio foundation models and their usage for downstream tasks. Following this, we provide a detailed description of our explainer system, which involves assigning importance in the embedding space of a foundation model. Lastly, we outline the steps for producing meaningful audio explanations using our approach. We present an overview of our system in Figure 1.\n\nA. Audio Foundation Models\nAudio foundation models are typically pre-trained on large datasets of audio samples to learn patterns from the audio signals, which can then be fine-tuned on specific tasks. They exploit self-supervised learning strategies to discover general representations from large-scale data without requiring expensive labels. To learn these representations which reflect meaningful patterns in audio signals, they build a high-level embedding space using deep learning frameworks such as autoencoders. This framework uses an encoder-decoder pair to project the audio input into the embedding space and then reconstruct it. In this paper, we focus on audio foundation models using autoencoder architecture to be able to generate listenable explanations using its decoder component. The standard autoencoder architecture can be formulated as follows:\n$Z = \\text{Encoder}(X) \\in \\mathbb{R}^{T\\times L};  \\text{X} = \\text{Decoder}(Z)$,\nwhere $X \\in [-1,1]^{D\\times F_s}$ represents an audio signal input of duration $D$ with sample rate $F_s$, $Z$ represents the latent vector with $T$ denoting the number of audio frames after down-sampling in the encoder, $L$ the feature dimension of the encoder, and $\\text{X}$ represents the reconstructed audio signal.\n\nB. System Design\nWe aim to understand the important audio features for a model decision by leveraging the high-level embedding space of foundation models. For this purpose, we target explaining foundation model-based audio models trained on specific audio tasks such as audio classification. Thus, our system starts with finetuning a foundation model with an autoencoder style framework on a desired task. To maintain the learnt representation space during finetuning, we freeze the weights of the encoder part and only update the additional task-specific model part such as a classification head. Then, our framework uses feature attribution methods to determine the most relevant features in the latent space for a model decision. Without backpropagating feature attribution computation to the audio input space, our method learns the important high-level components in the latent space, which is not restricted with the dimensions of the input space. We formulate our feature attribution method in the latent space as follows:\n$\\text{att} = IG(\\text{Classifier}(Z)) \\in \\mathbb{R}^{T\\times L}$,"}, {"title": "C. Explanation Generation", "content": "To generate listenable audio explanations, our method first extracts the relevant latent vector based on the computed feature attributions in Equation 2. While keeping the latent dimensions with high importance, it replaces less important dimensions with a base latent vector which is obtained by encoding a noise audio with appropriate length. It then uses the decoder part of the foundation model of interest to transform the relevant latent vector into audio explanations. The explanation generation can be written as:\n$X_e = \\text{Decoder}(Z_e)$,\nwhere $Z_e$ represents the relevant latent vector for a specific prediction and $X_e$ represents the audio explanation in the input space. Although our method generates an audio explanation in the input space, it goes beyond only selecting features in this space by the integration of meaningful latent space."}, {"title": "IV. EXPERIMENTS", "content": "We evaluated our method on two datasets, namely, Speech Commands [32], and the Toronto Emotional Speech Set (TESS) [33], to assess its performance across keyword spotting and speech emotion recognition tasks. In this section, we provide implementation details of our method, followed by a comprehensive quantitative and qualitative evaluation. The implementation code and sample audio explanations is available on our project page\u00b9.\n\nA. Implementation Details\nWe choose the EnCodec neural audio codec [22] as our foundation model to learn the audio representations from the raw waveform, which is trained across diverse domains including general audio, speech, and music. EnCodec comprises two key components: an encoder that extracts features based on a convolutional neural network (CNN), and a decoder module that reconstructs the same audio. While our method leverages the encoder module to extract meaningful audio representations and assign importance based on the classification model, the decoder part allows it to map these features to the input space. In our experiments, we use the EnCodec version for 24 kHz audio at 1.5 kbps bandwidth. We also eliminate the quantisation part of the model to increase the accuracy on the classification model with higher dimensional representation.\nAs our classification model, we train a transformer-based classifier on top of the embeddings extracted by EnCodec. Note that we only train a base model without extra tuning. In the classifier architecture for keyword spotting on Speech Commands, we use 3 layers of transformer with an 8 head multi-head attention module. We employ a dropout probability of 0.1 and the dimension of the feed-forward network model is 512 for each transformer layer. For speech emotion recognition on TESS, we use a gated recurrent unit (GRU) based recurrent neural network with 2 layers and 128 hidden dimensions and employ a dropout of 0.2. Our classifiers achieve an accuracy of 85.4% on the test set for Speech Commands and 96.4% on the arranged test set for TESS. To arrange the TESS test set, we select random emotions from each spoken word using 0.2 split ratio and share the test indices on our project page for reproducibility. To compute the feature attribution in the latent space, we use Integrated Gradients (IG) [6] which calculate the integral of the gradients of the model's output along the straight line path from the baseline to the input.\n\nB. Quantitative Evaluation\nWe conduct fidelity experiments to measure how well the prediction of the underlying model and the generated explanation agree. Since our method investigates feature importance beyond the space of the original input features by integrating Encodec latent space, it is not possible to select important features in this space. Thus, our strategy involves selecting the latent dimensions with the highest relevance with respect to the IG algorithm by a ratio of a. We set the remaining dimensions to a base value using a base latent vector which is obtained by encoding a noise audio with appropriate length. We compute the fidelity score as the fraction of samples where"}, {"title": "C. Qualitative Evaluation", "content": "To evaluate the quality of generated explanations, we observe our model's behaviour on audios from the same spoken words and separate emotion classes on the TESS dataset. By following similar procedures with quantitative evaluation experiments, we select an audio sample and generate the audio explanation using our method by setting \u03b1 = 0.2 in the first experiment.\nIn the second experiment setting, we removed the explanation from the original audio sample in the embedding space with the same ratio. We then generate the irrelevant audio part by using our framework. Ideally, we expect that while the generated audio in the first experiment can represent the emotion in the original audio sample, the irrelevant audio in the second experiment represents a neutral state of the same word spoken. In Figure 2, we present an example of audio from the class \u201cHappy\u201d to conduct these experiments. We also present the audio of the same word from the class \"Neutral\" to enable comparison for explanation-removed audio. We observe that while the generated explanation can represent the emotion in the original audio, explanation-removed audio looks more similar to the neutral version of the spoken word. We use spectrogram representation to increase the interpretability of the visuals.\nWe also investigate the classifier model behaviour for the TESS dataset after explanation removal by a ratio of \u03b2 = 0.1 using our method. In Figure 3, we present the confusion matrix of the classifier for each class in TESS dataset. The results show that majority of the audios are classified as \"Neutral\" when explanation is removed by a small ratio which shows our explanation generation method focuses on emotions."}, {"title": "V. CONCLUSION", "content": "In this paper, we presented a novel audio explanation method which targets audio-processing foundation models. Unlike existing feature attribution methods which assign importance in the input space, our method integrates the latent space of a generative foundation model to generate meaningful and listenable explanations. The experiments demonstrated that our method delivers high-fidelity explanations, effectively capturing meaningful audio components pertinent to the specific task. Our work enlightens the way to promising research in inter-preting state-of-the-art audio models as well as encompassing their application for model debugging and justification. An extension of our work could involve using rapidly growing audio generative AI models to produce higher quality audio explanations."}]}