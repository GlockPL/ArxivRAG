{"title": "ByteCheckpoint: A Unified Checkpointing System for LLM Development", "authors": ["Borui Wan", "Mingji Han", "Yiyao Sheng", "Zhichao Lai", "Mofan Zhang", "Junda Zhang", "Yanghua Peng", "Haibin Lin", "Xin Liu", "Chuan Wu"], "abstract": "The development of real-world Large Language Models (LLMs) necessitates checkpointing of training states in persistent storage to mitigate potential software and hardware failures, as well as to facilitate checkpoint transferring within the training pipeline and across various tasks. Due to the immense size of LLMs, saving and loading checkpoints often incur intolerable minute-level stalls, significantly diminishing training efficiency. Besides, when transferring checkpoints across tasks, checkpoint resharding, defined as loading checkpoints into parallel configurations differing from those used for saving, is often required according to the characteristics and resource quota of specific tasks. Previous checkpointing systems [16, 3, 33, 6] assume consistent parallel configurations, failing to address the complexities of checkpoint transformation during resharding. Furthermore, in the industry platform, developers create checkpoints from different training frameworks [23, 36, 21, 11], each with its own unique storage and I/O logic. This diversity complicates the implementation of unified checkpoint management and optimization. To address these challenges, we introduce ByteCheckpoint, a PyTorch-native multi-framework LLM checkpointing system that supports automatic online checkpoint resharding. ByteCheckpoint employs a data/metadata disaggregated storage architecture, decoupling checkpoint storage from the adopted parallelism strategies and training frameworks. We design an efficient asynchronous tensor merging technique to settle the irregular tensor sharding problem and propose several I/O performance optimizations to significantly enhance the efficiency of checkpoint saving and loading. Experimental results demonstrate ByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to 529.22\u00d7) and loading (by up to 3.51x) costs, compared to baseline methods.", "sections": [{"title": "1 Introduction", "content": "Sustained progress in Large Language Models (LLMs) such as GPT [2, 1], Gemini [25, 22] and Llama [32, 4] families empowers various downstream AI applications like chatbot [29], personalized AI character [24], coding assistant [26], etc. To facilitate efficient large-scale training, several distributed training frameworks (e.g., Megatron-LM [23], PyTorch FSDP [36], DeepSpeed [21], etc.) have emerged in recent years. Most of them are equipped with advanced parallelism strategies to improve Model FLOPs Utilization (MFU), including Pipeline Parallelism [17, 8] (PP), Tensor Parallelism [23] (TP), Data Parallelism (DP) with zero redundancy optimizer [20], Sequence Parallelism [9] (SP) and various combinations of them [13]. Even with efficient training framework support, the immense model sizes and massive training corpora still make the development of LLMs resource-intensive and time-consuming, which can scale to 12288 GPUs [11] and over several months [7]. Besides, LLM training typically comprises multiple interrelated stages, including Pre-Training (PT), Supervised Fine-Tuning (SFT), and Reinforcement Learning with Human Feedback (RLHF) [19]. These stages are designed to enhance model performance and align with diverse real-world tasks. Furthermore, it is crucial to run accompanying jobs like auto-evaluation [7] during the Pre-Training stage, allowing for continuous monitoring of the model quality via multifaceted evaluation metrics, and enabling timely hyper-parameter adjustments or debugging [3] based on the feedback. The enormous scale, prolonged job running times, and complex pipeline of LLM development pose significant challenges to the underlying industry platforms.\nDuring the full life cycle of LLM training, training states need to be carefully managed for failure recovery [27, 34], and downstream [3] or concurrent [7] task transfers. As the mainstream method for training state management in Deep Learning (DL) jobs, checkpointing captures snapshots of training states (e.g., model states, optimizer states, dataloader states, etc.), stores them in persistent storage, and exports them for different purposes. Since training states cannot be modified before the entire copies are obtained, efficient I/O performance of checkpointing systems is significant to reduce checkpoint stalls, and thereby improve the Effective Training Time Ratio 3 (ETTR) for better resource utilization.\nPrevious works [16, 3, 33, 6] explore efficient checkpointing systems for various DL workloads. However, two common requirements hinder their deployment to real-world LLM production. Firstly, there is a consistent need for checkpoint resharding, defined as saving distributed checkpoints with certain parallelism strategies and configurations but loading them into different ones. For example, when large-scale hybrid parallel [13] Pre-Training is completed and the LLM training enters the alignment (SFT or RLHF) phase, relatively fewer GPUs are involved and the TP, DP, and PP degrees are decreased correspondingly due to the reduction in the scale of training datasets. Checkpoints stored in the PT phase must be resharded to align with the new parallelism configuration and the total number of accessible GPUs. Besides, checkpoints can also be loaded for other running tasks in the cluster like model evaluation, model debugging, etc. The parallelism degrees and strategies are often tuned for those tasks based on the assigned GPU quota and the task characteristics. Additionally, for tasks running on elastic tidal resources [14, 35], frequent changes in the number of GPUs also bring about the need for checkpoint resharding when resuming the task to new hardware settings. Most of the existing checkpointing systems [16, 3, 25, 6] assume consistent parallelism configuration during saving and loading, failing to fulfill the resharding requirement in real-world production. Secondly, in the industry platforms, engineers often choose different training frameworks based on the characteristics of their tasks and ease-of-use of frameworks (e.g., Megatron-LM [23] for efficient large-scale distributed training, PyTorch DDP [15] for simple single-node model debugging) and save checkpoints in the storage systems. Since different frameworks have their own checkpoint module and save/load logic, developing efficient checkpoint management and optimization functionalities for those frameworks often incurs extensive domain-specific knowledge and considerable engineering efforts. None of the previous checkpointing systems provide flexible multi-framework checkpointing support to address this issue. We depict these requirements in Fig. 1.\nRecently, PyTorch introduced Distributed Checkpoint [30] (DCP) to facilitate automatic online checkpoint resharding for Data-Parallel frameworks (i.e., DDP and FSDP). However, DCP exhibits limitations in its compatibility with advanced parallelism strategies, such as 3D parallel training, and lacks the capability to save/load checkpoints for multiple training frameworks. Additionally, the system designs of DCP are not optimized for large-scale training. This oversight leads to potential stability issues and detrimentally affects the training efficiency, increasing ETTR and prolonging the duration required for loading.\nInspired by the concepts of DCP [30], in this paper, we introduce ByteCheckpoint, a PyTorch-native checkpointing system crafted for the full life-cycle of LLM development. ByteCheckpoint stores the entire training states, including model, optimizer, and dataloader states, in selected storage backend and supports efficient automatic online checkpoint resharding across arbitrary parallel configurations. This versatility makes it adaptable to different hardware setups and task scenarios. Additionally, ByteCheckpoint is compatible with multiple training frameworks, broadening its applicability. For model and optimizer states checkpointing, one critical observation is that even though different frameworks have different abstracts for distributed tensor shards (e.g., DTensor in FSDP [36], veScale [11], ShardedTensor in Megatron-LM [23]), it is feasible to standardize these abstractions using a unified representation, specifically the triple tuple consisting of (fully qualified name, offsets, lengths). This approach facilitates the comprehensibility of tensor management across diverse training frameworks. Based on this observation, ByteCheckpoint adopts a data/metadata disaggregated storage architecture. We employ a customized serialization method to extract the metadata information of tensor shards and construct a consolidated global checkpoint metadata file. This file establishes a mapping between runtime tensor shard instances and storage files to maintain a global view of distributed checkpoints. During the resharding process, each newly initialized process retrieves and parses the metadata file to selectively load the required segments from the stored checkpoints. To settle the challenge of irregular tensor resharing (see Sec. 4.1) and guarantee checkpoint correctness during resharding, we design an asynchronous tensor merging technique, which hides the communication cost within the normal checkpointing workflow. To reduce checkpoint stalls during training, we propose I/O performance optimization techniques from two orthogonal perspectives. Within each training process, we design a fine-grained fully asynchronous save pipeline to hide the cost of each checkpoint phase from others. A Ping-Pong pinned memory pool is also adopted to accelerate the Device-to-Host (D2H) copy and reduce the cost of CPU memory reallocation. From the perspective of multiple training processes, we distribute balanced saving workloads to each process to enhance parallel processing capabilities for checkpointing. For the loading efficiency, we craft a resharding-aware, zero redundancy loading mechanism: we develop a partial file reading technique that avoids unnecessary data reads from storage systems to CPU memory within each training process, and combine it with asynchronous tensor transfers to further utilize idle inter-GPU bandwidth during loading and eliminate redundant tensor reads across multiple processes.\nWe implement ByteCheckpoint as a framework-agnostic Python library, offering two straightforward Application Programming Interfaces (APIs), i.e., bytecheckpoint.save() and bytecheckpoint.load(). These APIs shield users from the intricacies of checkpoint management and I/O logic, thereby simplifying integration and enhancing usability across various tasks and environments."}, {"title": "2 Related Work", "content": "We summarize the features of different checkpointing systems in Table 1 and give a detailed discussion about existing works as follows:\nCheckpointing without online resharding. Several previous works investigate reducing checkpointing costs from different perspectives. Check-N-Run [3], tailored for recommendation models, employs differential checkpointing to store only the modified portions of the model, alongside quantization techniques to reduce checkpoint size. However, differential checkpointing is not directly applicable to the Pre-Training of LLMs, as LLMs typically exhibit dense updating patterns. The quantization technique is beyond the scope of this work, given that lossless checkpoints are imperative in our production settings. CheckFreq [16] proposes to pipeline model states snapshot and persist operations with compute to reduce checkpoint stalls, and designs an online checkpoint frequency tuning algorithm to further reduce cost. Gemini [33] advocates in-memory checkpointing with inter-machine backup for fast failure recovery and interleaves the checkpoint communication traffic for state backup with training traffic, enabling frequent checkpointing on each iteration. Both CheckFreq [16] and Gemini [33] lack support for prevalent 3D [18] parallel training for LLMs. Furthermore, Gemini [33]'s in-memory checkpointing solution demonstrates vulnerabilities in real-world large-scale GPU clusters, where multiple machines within the same backup group can experience simultaneous failures, leading to unacceptable catastrophic state loss. Other checkpointing techniques like Just-In-Time checkpointing [6] (JIT Checkpointing) also suffer from the same reliability problem as Gemini [33]. Besides, for industrial checkpointing systems, it is impractical to solely store the latest checkpoints during training. This is due to the necessity of accessing checkpoints from various steps for purposes like auto-evaluation [7], hyper-parameter tuning, or even for restarting training. Regarding the latter, after making configuration adjustments, the checkpoint with the highest evaluation scores is likely to be selected as the new starting point. Different from these works, ByteCheckpoint champions both optimized I/O performance and flexible automatic online checkpoint resharding, supporting checkpointing for general purposes.\nCheckpointing with online resharding. Some industrial initiatives are dedicated to developing checkpointing systems that incorporate automatic online resharding. DCP [30] relies on centralized communication calls to generate a global checkpoint metadata file, scatter save/load plans to all ranks, and synchronize their I/O transactions to make checkpointing atomic. For resharding, DCP implements a quadratic algorithm that identifies overlapping regions across each dimension between saved and newly instantiated tensor shards, loading matched parts to populate new ones. Megatron Dist Checkpointing [28] (MCP) reuses the same workflow of DCP [30] and extends the available file storage formats, such as Zarr [31]. However, both systems are constrained by their limited support for parallelism strategies and training frameworks (e.g., DCP supports DP parallelism strategy in DDP and FSDP frameworks, MCP only works for Megatron-LM [23]). Therefore, they cannot meet the multi-framework support requirement. Besides, when deployed for large-scale training where the use of remote persistent storage is essential, their I/O performance remains sub-optimal due to the lack of customized optimizations. Additionally, the centralized communication pattern used for process orchestration (See Sec. 4.2) in DCP [30] and MCP [28] also presents potential stability risks. ByteCheckpoint address these limitations, enabling flexible multi-framework checkpointing support, as well as guaranteeing stability and efficiency at scale."}, {"title": "3 Background and Motivation", "content": ""}, {"title": "3.1 Checkpointing in Deep Learning", "content": "In deep learning (DL) training jobs, GPU states, including model and optimizer states, are maintained in GPU memory to enhance computation and communication efficiency. Additional crucial states, such as Random Number Generator (RNG) states in the dataloader and iteration numbers, are stored in CPU memory to track the runtime training progress. Due to the volatile nature of these storage media, any interruption to the running jobs can result in the loss of these states, thus wasting the elapsed GPU and CPU cycles. This loss is even more critical in large-scale LLM training, which typically involves tens of thousands of GPUs. Consequently, it is essential to periodically checkpoint these training states into persistent storage to tolerate any potential faults. When processes are restarted, they can resume training from these checkpoints, which helps minimize the waste of training progress. Furthermore, these persistent checkpoints are also required by accompanying or downstream tasks. Therefore, efficient checkpointing and flexible checkpoint transferring are crucial for enhancing productivity. The checkpointing of GPU states typically involves three phases: Device-to-Host (D2H) copy, which takes a snapshot of GPU states and stores them in CPU memory. Serialization, which converts CPU tensors into compact byte objects. This step reduces the storage footprint and enhances the portability of the GPU states. Dump to storage, which writes serialized GPU states to persistent files and creates the final checkpoints.\nAlthough recent studies have explored in-memory checkpointing [33], demonstrating significant performance advantages in failure recovery, the necessity for storing checkpoints in persistent storage continues to be crucial in the industry. Software and hardware failures are inevitable in large-scale training clusters and no in-memory checkpointing technique can ensure that checkpoints retained in memory will be preserved in the event of various failures. The cost of losing training progress in industry-level LLM Pre-Training, which involves substantial data and massive GPU resources, is prohibitively high. Therefore, using persistent storage for checkpointing remains the most reliable and robust method to safeguard training progress and ensure continuity in production environments."}, {"title": "3.2 Challenges of Checkpointing", "content": "We summarize the observed challenges of checkpointing in our production platform as follows:\nHigh I/O cost of checkpoint saving and loading. The mainstream LLMs such as Llama [32, 4] and GPT [2, 1] feature extensive model sizes, which can even reach 405B [5]. This expansion trend is further fueled by the adoption of sparse model structures such as Mixture of Experts (MoE) [?] applied to Mixtral[10]. Consequently, the memory requirements of model and optimizer states also scale up significantly, imposing substantial overhead for saving and loading model checkpoints. Consider an LLM that has NB parameters, conducting mixed-precision training with Adam [12] optimizer. The model parameters are stored using the bfloat16 data type, necessitating 2N GB of memory. Additionally, the optimizer states, which include a replica of parameters along with their momentum and variance, are stored in the float32 data type, culminating in a memory requirement of 12N GB. In our production environment, checkpoints are stored in remote persistent storage like Hadoop Distributed File System (HDFS). During the saving and loading processes, movements of massive checkpoint data across various storage media (e.g., GPU, CPU, local SSDs, HDFS) result in significant GPU idling due to the necessary synchronization to ensure checkpoint integrity. By monitoring our LLM training jobs, we observe that the end-to-end cost of saving the distributed checkpoints of a 32B model once can take 260 seconds, substantially exceeding the time required for a single training iteration. A similar issue also arises in the checkpoint loading or resharding process where we resume training after removing unhealthy machines or initiating new downstream tasks. These observations highlight the critical need for more efficient checkpointing systems that can better tackle the extensive data transfers involved in running jobs, thereby improving overall efficiency and resource utilization. The commonly used two-step asynchronous checkpointing [16], adopted by many checkpointing modules [11, 30, 28], still falls short in terms of efficiency, particularly when dealing with remote persistent storage in large-scale training scenarios. ByteCheckpoint introduces more refined checkpoint save and load pipelines, incorporating multiple I/O optimization techniques to enhance large-scale checkpointing I/O efficiency.\nPoor performance, flexibility, and scalability of offline scripts. One intuitive approach for checkpoint resharding is to consolidate distributed checkpoints into a single global checkpoint. This approach bypasses the need to address complex parallelism configuration remapping during loading. However, running merging scripts introduces significant I/O and communication overheads, and can easily lead to out-of-memory (OOM) issues, due to the substantial memory consumption for loading large quantities of checkpoint data into CPU memory to execute the merging algorithm. Another solution is to develop customized resharding scripts each time when a new requirement is put forward. Nevertheless, this approach lacks scalability. Given K models with different structures and M parallelism strategies, the total number of offline scripts required would be $K \\times M \\times (M - 1)$, which increases linearly with the number of models, and quadratically with the number of parallelism strategies, respectively. The development, storage, and maintenance of these offline scripts exhibit substantial engineering efforts and costs. Moreover, integrating the checkpoint resharding process into the workflow of various LLM tasks, such as SFT, RLHF, and hyper-parameter tuning adds to the code's complexity, which is not user-friendly. An industrial checkpointing system must account for these costs and offer more flexible management. Unlike checkpointing systems [16, 3, 33, 6] that only focus on fault-tolerance and assume constant parallelism configurations, ByteCheckpoint enables automatic online checkpoint resharding, shielding users from the intricate transformation logic.\nHigh development cost for multi-framework support. In industry platforms, we maintain diverse training frameworks such as Megatron-LM [23], DDP [15], FSDP [36], DeepSpeed [21], and veScale [11], allowing users to select the most suitable one based on specific characteristics and scale of their tasks. Most of these tasks have the requirement to save and load checkpoints efficiently, with minimal runtime overheads that could potentially impact the performance of original tasks. For instance, a prevalent need is to save checkpoints asynchronously without severely blocking ongoing tasks. However, since each framework has its unique checkpoint module and save/load logic, efficiently managing diverse checkpoint files from different frameworks becomes challenging. Besides, it is awkward to engage in repeated engineering efforts to tailor optimized implementations for each one. ByteCheckpoint introduces a unified representation for checkpoints from different frameworks, integrating it into a disaggregated storage architecture for metadata and data to facilitate seamless multi-framework checkpointing support."}, {"title": "4 System Design", "content": "In this section, we first present the storage architecture of ByteCheckpoint in Sec. 4.1, followed by an illustration of how it facilitates automatic online resharding and a detailed explanation of the technique ByteCheckpoint employs to settle the irregular tensor sharding problem. After that, We introduce ByteCheckpoint's workflow, and API usage cases in Sec. 4.2."}, {"title": "4.1 Disaggregated Storage Architecture", "content": "Overview. ByteCheckpoint separates the storage of checkpoint data and the metadata files, thereby disaggregating checkpoints from arbitrary parallelism settings and training frameworks. As depicted in Fig. 2, each distributed checkpoint residing in different processes can be represented as (storage files, metadata). For each process, The actual numerical values of tensor shards in model or optimizer states are concatenated and stored in the corresponding storage files. Meanwhile, their metadata information is extracted separately with our customized serialization method. Metadata mainly includes three components: TensorMeta, this component records the basic information of individual tensor shards, which is essential to recover their runtime states. ShardMeta, this component records the relative position information of tensor shards from a global view. For this data field, We use the abstract of DCP [30] and extend it to support different frameworks by unified tensor shard representation into the tuple: (FQN, offsets, lengths). ByteMeta, this component records the byte start offset and length of each tensor shard within the storage files, facilitating the retrieval of numerical values necessary to reconstruct a runtime tensor. All metadata are consolidated into a single global file, accessible by all processes during the loading (resharding) phase. A mapping, i.e., ShardtoByteMap, between saved tensor shards and storage files is established with ShardMeta and ByteMeta, ensuring accurate data retrieval.\nOnline checkpoint resharding. In Fig. 3, we illustrate how our architecture facilitates flexible, automatic online resharding. For newly instantiated tensor shards, The process begins by consulting the ShardtoByteMap in the global metadata file. With the information from ShardMeta and ByteMeta, each process runs the resharding algorithm in DCP [30] (see Sec. 2) to identify the matched parts between saved and instantiated tensor shards. It then retrieves corresponding values from different storage files. After that, TensorMeta is utilized to recover the final runtime states of tensor shards, which is not depicted in Fig. 3 for clarity. The whole procedure happens silently when bytecheckpoint.load() is called, without any interaction with users.\nSettle irregular tensor sharding. The automatic online resharding mechanism operates effectively when each tensor shard retains its original shape before checkpoint saving However, many mainstream training frameworks [23, 36, 11] modify the original shape of tensor shards to enhance memory or communication efficiency. For example, in Megatron-LM [23] and veScale [11], all tensor shards of the optimizer states in the same layer (module) are flattened and merged into a one-dimension vector. Therefore, the (FQN, offsets, lengths) representation cannot be directly applied due to irregular tensor sharding. As shown in Fig. 4, when 2D (TP with ZeRO-style DP) parallel training is applied, some tensors in optimizer states are not regularly divided along the axis independently, making it fail to represent them with the triple tuple format. Other Data-Parallel training frameworks like FSDP [36] also have similar designs. To avoid this problem during checkpoint saving, FSDP [36] synchronously conducts an all-gather operation for each module in the model, allowing all processes to possess the full GPU states. Besides, when the full GPU states exceed the GPU memory capacity, the D2H copy operation is interleaved with the all-gather operation across each module to avoid OOM issues. It is also the default approach adopted by DCP [30] in supporting FSDP [36]. However, this method not only leads to substantial communication overhead but also results in considerable execution bubbles caused by frequent synchronization between the GPU and CPU, which severely hampers training progress. To efficiently address the problem of irregular tensor resharding, ByteCheckpoint proposes asynchronous tensor merging. Rather than simply executing synchronous all-gather and D2H operations, ByteCheckpoint first identifies all irregular tensor shards (e.g., tensor shard B0 in Fig.4). It then employs asynchronous Peer-to-Peer (P2P) communications across processes to collect and integrate these irregular shards into regular ones, without blocking the default checkpointing workflow (See Sec.4.2). To enhance the efficiency, the irregular tensor shards are distributed among processes using a Worst-Fit workload balancing algorithm, based on their sizes. The launches of all P2P waiting and D2H copy operations for those tensor shards are strategically postponed until they enter the serialization phase, thereby removing frequent synchronization and enabling better overlapping between communication and I/O operations."}, {"title": "4.2 Workflow", "content": "The workflow of ByteCheckpoint is illustrated in Fig. 5. ByteCheckpoint offers a unified interface that accommodates various training frameworks and facilitates interactions with a range of storage backends, including memory file systems, local disks, HDFS, and Network File System (NFS). By decoupling the Planner Layer and the Storage Layer from the Execution Layer, ByteCheckpoint's extensibility is significantly enhanced. We first give descriptions of each layer in ByteCheckpoint's workflow, then introduce our stability optimization for planning.\nAPI Layer. For users working with various training frameworks, manually managing distributed checkpoints can be challenging and error-prone. To alleviate this issue, we offer straightforward and user-friendly APIs, akin to torch.save() and torch.load(), that require no additional learning cost for DL engineers and researchers. These APIs simplify the process by shielding users from the complexities of the underlying system, such as information extraction of process group and device mesh configurations, save/load plan generation, efficient I/O operations, etc. Users only need to provide a dictionary containing keys like \"model,\" \"optimizer,\" or \"extra states\" (CPU states) when saving and loading checkpoints. Moreover, our API is independent of parallelism strategies and the specifics of the distributed environment. We give the API usage examples in Fig. 6.\nPlanner Layer. The Planner Layer mainly comprises two steps: one gathering step and one scattering step. It first identifies the items to be saved or loaded from the provided dictionary, constructs the TensorMeta and ShardMeta for saving, or retrieves the global metadata file for loading purposes. After that, the coordinator planner, located in the process with rank 0, gathers TensorMeta and ShardMeta or the ShardtoByteMap from others and applies workload distribution algorithms (refer to Sec. 5 for a detailed explanation of the saving/loading workload partitioning mechanism) to create save/load plans. These plans specify which tensor shards each process will save, transfer (for merging irregular shards), or load. The finalized plans are scattered to each process and then forwarded to the Execution Layer to carry out the actual I/O tasks. The saving workflow includes an additional gathering step to gather the ByteMeta constructed from the storage files upon the completion of I/O tasks and assemble collected metadata into the final global metadata file.\nExecution Layer. Given the save/load plans from the Planner Layer, the Execution Layer manages interactions with storage backends and executes the actual I/O tasks detailed in the plans. During the saving process, the different phases associated with saving each tensor shard (outlined in Sec. 3) are executed separately in a fully asynchronous pipeline, while the P2P communications for tensor merging are conducted concurrently. By default, the checkpoint-saving workloads are handled by separated I/O processes to move them from the critical path of training. For loading, each process retrieves data from storage files and employs advanced techniques such as the zero redundancy loading mechanism to reduce loading costs. Most I/O performance optimization techniques are incorporated in the Execution Layer, further details on these optimizations are included in Sec. 5.\nStorage Layer. The Storage Layer handles various storage backends and implements background optimizations tailored to the characteristics of the selected storage backend. For example, in scenarios where the persistent storage backend is set to HDFS (the most common setup in our platform), an additional uploading phase is integrated into the checkpoint-saving pipeline. The HDFS Engine leverages the shared memory of machines (e.g., the /dev/shm directory) to temporarily store the bytes returned from I/O processes, and utilizes background threads to efficiently upload these data to the remote HDFS without blocking main training process and I/O processes. Similar to the Planner Layer, the Storage Layer operates independently from the Execution Layer. This independence significantly simplifies the integration of new backends, thereby reducing development costs. Moreover, any optimizations in the Storage Layer can benefit users from all training frameworks.\nStability Optimization. As introduced above, for the Planner Layer, we adopt a similar planning protocol as DCP [30]. However, in this protocol, centralized gathering and scattering steps pose a significant burden on the coordinator process, particularly when executed at scale. To improve the stability of this planning during large-scale training, we implement a tree-based communication topology. Processes on the same machine are organized into a first-level subtree, where the process with local rank 0 is designated as the root. For inter-machine communication, we iteratively group multiple machines, assigning the process with the lowest global rank in each group as the root. This process continues until all processes are integrated into a path leading to the global root (rank 0). Existing connections between processes are leveraged during this topology construction. For instance, in large-scale 3D parallel training, a TP-DP-PP three-level communication tree can be directly built without the need to establish any new connections. Following this setup, each process begins by extracting TensorMeta and ShardMeta from local tensor shards. Subsequently, the coordinator process (rank 0) hierarchically gathers all these data through the communication tree. The scattering step is executed via the reverse path."}, {"title": "5 Performance Optimization Techniques", "content": "In this section, we introduce ByteCheckpoint's I/O optimizations applied in the saving and loading processes."}, {"title": "5.1 Saving Optimizations", "content": "Fully asynchronous pipeline. As shown in Fig. 7a, ByteCheckpoint separates different phases of checkpoint saving for each tensor shard and pipelines the execution of them independently to hide the costs associated with each phase from others effectively. If the tensor shard is irregularly sharded, the save pipeline will start from the P2P data transfer phase (see Sec. 4.1). When using HDFS as the storage backend, an additional upload phase is incorporated into the pipeline. Compared to the save pipelines in CheckFreq [16] and DCP [30], ByteCheckpoint can further reduce the end-to-end completion time of checkpointing. This enhancement leads to less runtime overheads, optimizing the overall training efficiency.\nPing-Pong pinned memory pool. The efficiency of the D2H copy is crucial since any modifications on GPU states cannot be applied until the completion of this process. To minimize the interruption, ByteCheckpoint employs a pinned CPU memory pool which leverages the high PCIe bandwidth to accelerate D2H tensor transfers, while also eliminating the costs of CPU memory reallocation. To reuse the allocated pinned memory and prevent copied GPU states from data corruption, a straightforward method requires halting until the completion of I/O tasks across different save function calls. However, in scenarios where checkpoint creation is more frequent, this approach can incur considerable delays and severely impede the training process. To address this issue, we design a Ping-Pong buffering mechanism within the pinned memory pool. As illustrated in Fig. 7b, this mechanism maintains two pinned memory pools that alternate their roles: one pool gathers newly copied states from GPU memory (serving as the read buffer), while the other transfers states to the I/O workers (acting as the write buffer). This approach effectively avoids the need for synchronization within the same memory pool, thereby minimizing delays associated with consecutive save function calls.\nWorkload balancing. For training that employs data parallelism strategies, the model states are replicated across all processes within the DP groups, leading to duplicated data storage and prolonged checkpointing duration. In DCP [30], a simple deduplication algorithm is employed: it designates the process with group rank 0 in each DP group as responsible for saving all model states. This approach, however, results in significantly imbalanced workloads. To address this imbalance, we implement a workload-balanced deduplication mechanism. By adopting the Worst-Fit algorithm, we distribute the saving tasks according to the size of each tensor shard. This approach promotes a more even distribution of workloads across all processes within the same DP group, thereby enhancing the efficiency for parallel processing of saving tasks among various training processes and reducing the end-to-end completion time.\nPlan and metadata cache. Apart from the tree-based communication topology (see Sec. 4.2) designed for enhancing planning stability, ByteCheckpoint further optimizes the planning efficiency for checkpoint saving by introducing plan and metadata caching mechanisms, transforming the planning process into a one-time cost. Once the save plans and the global metadata file are created for the first time, they are cached for subsequent reuse. This allows the system to bypass the repetitive planning process, therefore reducing unnecessary communication overheads."}, {"title": "5.2 Loading Optimizations", "content": "Fully asynchronous pipeline. Similar to saving, for checkpoint loading, we also enable the fully asynchronous pipeline to reduce loading costs. Specifically, we pipeline the executions of file reading, deserialization, and Host-to-Device (H2D) copy for each tensor shard, achieving higher efficiency.\nZero redundant loading #1: partial file reading. As illustrated in Fig. 3, during the resharding process, newly initiated processes may only need to access specific sections of the saved tensor shards from the checkpoint storage files. DCP [30] overlooks this aspect and reads the entire tensor shard, subsequently performing in-memory slicing to isolate the required segments. This approach leads to unnecessary I/O costs that negatively impact overall efficiency. Besides, it can also bring potential OOM issues when CPU memory is scarce. Furthermore, when checkpoints are stored on remote persistent storage such as HDFS, each process must download the whole storage files to access the fragments in tensor shards, thereby exacerbating communication overhead. ByteCheckpoint addresses this inefficiency by implementing a partial file reading mechanism, allowing each process to directly access only the needed segments of the data without any redundancy (shown in Fig. 8a). Specifically, for each tensor shard that is newly instantiated, ByteCheckpoint utilizes the ShardMeta and ByteMeta of the saved tensor shards to convert multidimensional retrieval indices into a series of byte-level, one-dimensional indices, enabling direct data access based on these byte-level indices. In this way, required tensor segments can be directly read into CPU memory, without the need for extra in-memory slicing.\nZero redundant loading #2: overlap tensor reading with transferring. The partial file reading technique effectively eliminates redundant tensor reads within a single process. However, when resharding to configurations that include data parallelism, the same tensor shards are still requested and read by multiple processes within a DP group. Besides, during large-scale training, a massive number of concurrent read requests can strain the bandwidth of HDFS, creating a bottleneck for checkpoint loading [11]. To completely eliminate repetitive tensor loading within DP groups and alleviate the load on HDFS, ByteCheckpoint utilizes idle inter-GPU bandwidth in the training cluster to combine tensor reading with tensor transferring. As depicted in Fig. 8b, within each DP group, tensor reading tasks are distributed evenly across all processes to avoid redundant reads. We initiate I/O threads that employ the partial file reading technique to read designated tensor shards. Concurrently, in the main thread, shards loaded in CPU memory are copied into GPU memory and then transferred to other processes that require those shards. Given that the tensor transfer communication pattern among the DP group constitutes the all-to-all (all2all) collective primitive, we implement a ring-based all2all algorithm to avoid communication contention between processes, thereby boosting communication efficiency effectively. It is recognized that the distribution of all repeated tensor shards may not always be perfect even across processes in each DP group. To address any discrepancies, tail send and receive requests are managed separately after the completion of all primary all2all communication tasks."}, {"title": "6 Evaluation", "content": "We evaluate the performance of ByteCheckpoint with various real-world LLM production workloads in our platform. This section is organized as follows: we first introduce the settings for our experiments (Sec. 6.1), then verify the correctness of ByteCheckpoint's automatic online resharding functionality in Sec. 6.2. Finally, we study the efficiency of ByteCheckpoint in terms of checkpoint saving and loading in Sec. 6.3 and Sec. 6.4, respectively."}, {"title": "6.1 Experimental Setup", "content": "Detailed experimental configurations are given in Table 2. Without ambiguity, we use strategies in ZERO [20", "36": ".", "2": "."}, {"36": "we employ the ZeRO-2 strategy to partition optimizer states and gradients across GPUs"}]}