{"title": "ByteCheckpoint: A Unified Checkpointing System for LLM Development", "authors": ["Borui Wan", "Mingji Han", "Yiyao Sheng", "Zhichao Lai", "Mofan Zhang", "Junda Zhang", "Yanghua Peng", "Haibin Lin", "Xin Liu", "Chuan Wu"], "abstract": "The development of real-world Large Language Models (LLMs) necessitates checkpointing of training states in persistent storage to mitigate potential software and hardware failures, as well as to facilitate checkpoint transferring within the training pipeline and across various tasks. Due to the immense size of LLMs, saving and loading checkpoints often incur intolerable minute-level stalls, significantly diminishing training efficiency. Besides, when transferring checkpoints across tasks, checkpoint resharding, defined as loading checkpoints into parallel configurations differing from those used for saving, is often required according to the characteristics and resource quota of specific tasks. Previous checkpointing systems [16, 3, 33, 6] assume consistent parallel configurations, failing to address the complexities of checkpoint transformation during resharding. Furthermore, in the industry platform, developers create checkpoints from different training frameworks [23, 36, 21, 11], each with its own unique storage and I/O logic. This diversity complicates the implementation of unified checkpoint management and optimization. To address these challenges, we introduce ByteCheckpoint, a PyTorch-native multi-framework LLM checkpointing system that supports automatic online checkpoint resharding. ByteCheckpoint employs a data/metadata disaggregated storage architecture, decoupling checkpoint storage from the adopted parallelism strategies and training frameworks. We design an efficient asynchronous tensor merging technique to settle the irregular tensor sharding problem and propose several I/O performance optimizations to significantly enhance the efficiency of checkpoint saving and loading. Experimental results demonstrate ByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to 529.22\u00d7) and loading (by up to 3.51x) costs, compared to baseline methods.", "sections": [{"title": "1 Introduction", "content": "Sustained progress in Large Language Models (LLMs) such as GPT [2, 1], Gemini [25, 22] and Llama [32, 4] families empowers various downstream AI applications like chatbot [29], personalized AI character [24], coding assistant [26], etc. To facilitate efficient large-scale training, several distributed training frameworks (e.g., Megatron-LM [23], PyTorch FSDP [36], DeepSpeed [21],\nDuring the full life cycle of LLM training, training states need to be carefully managed for failure recovery [27, 34], and downstream [3] or concurrent [7] task transfers. As the mainstream method for training state management in Deep Learning (DL) jobs, checkpointing captures snapshots of training states (e.g., model states, optimizer states, dataloader states, etc.), stores them in persistent storage, and exports them for different purposes. Since training states cannot be modified before the entire copies are obtained, efficient I/O performance of checkpointing systems is significant to reduce checkpoint stalls, and thereby improve the Effective Training Time Ratio 3 (ETTR) for better resource utilization.\nPrevious works [16, 3, 33, 6] explore efficient checkpointing systems for various DL workloads. However, two common requirements hinder their deployment to real-world LLM production. Firstly, there is a consistent need for checkpoint resharding, defined as saving distributed checkpoints with certain parallelism strategies and configurations but loading them into different ones. For example, when large-scale hybrid parallel [13] Pre-Training is completed and the LLM training enters the alignment (SFT or RLHF) phase, relatively fewer GPUs are involved and the TP, DP, and PP degrees are decreased correspondingly due to the reduction in the scale of training datasets. Checkpoints stored in the PT phase must be resharded to align with the new parallelism configuration and the total number of accessible GPUs. Besides, checkpoints can also be loaded for other running tasks in the cluster like model evaluation, model debugging, etc. The parallelism degrees and strategies are often tuned for those tasks based on the assigned GPU quota and the task characteristics. Additionally, for tasks running on elastic tidal resources [14, 35], frequent changes in the number of GPUs also"}, {"title": "2 Related Work", "content": "We summarize the features of different checkpointing systems in Table 1 and give a detailed discussion about existing works as follows:\nCheckpointing without online resharding. Several previous works investigate reducing checkpointing costs from different perspectives. Check-N-Run [3], tailored for recommendation models, employs differential checkpointing to store only the modified portions of the model, alongside quantization techniques to reduce checkpoint size. However, differential checkpointing is not directly applicable to the Pre-Training of LLMs, as LLMs typically exhibit dense updating patterns. The quantization technique is beyond the scope of this work, given that lossless checkpoints are imperative in our production settings. CheckFreq [16] proposes to pipeline model states snapshot and persist operations with compute to reduce checkpoint stalls, and designs an online checkpoint frequency tuning algorithm to further reduce cost. Gemini [33] advocates in-memory checkpointing with inter-machine backup for fast failure recovery and interleaves the checkpoint communication traffic for state backup with training traffic, enabling frequent checkpointing on each iteration. Both CheckFreq [16] and Gemini [33] lack support for prevalent 3D [18] parallel training for LLMs. Furthermore, Gemini [33]'s in-memory checkpointing solution demonstrates vulnerabilities in real-world large-scale GPU clusters, where multiple machines within the same backup group can experience simultaneous failures, leading to unacceptable catastrophic state loss. Other checkpointing techniques like Just-In-Time checkpointing [6] (JIT Checkpointing) also suffer from the same reliability problem as Gemini [33]. Besides, for industrial checkpointing systems, it is impractical to solely store the latest checkpoints during training. This is due to the necessity of accessing checkpoints from various steps for purposes like auto-evaluation [7], hyper-parameter tuning, or even for restarting training. Regarding the latter, after making configuration adjustments, the checkpoint with the highest evaluation scores is likely to be selected as the new starting point. Different from these works, ByteCheckpoint champions both optimized I/O performance and flexible automatic online checkpoint resharding, supporting checkpointing for general purposes.\nCheckpointing with online resharding. Some industrial initiatives are dedicated to developing checkpointing systems that incorporate automatic online resharding. DCP [30] relies on centralized communication calls to generate a global checkpoint metadata file, scatter save/load plans to all ranks, and synchronize their I/O transactions to make checkpointing atomic. For resharding, DCP implements a quadratic algorithm that identifies overlapping regions across each dimension between saved and newly instantiated tensor shards, loading matched parts to populate new ones. Megatron Dist Checkpointing [28] (MCP) reuses the same workflow of DCP [30] and extends the available file storage formats, such as Zarr [31]. However, both systems are constrained by their limited support for parallelism strategies and training frameworks (e.g., DCP supports DP parallelism strategy in DDP and FSDP frameworks, MCP only works for Megatron-LM [23]). Therefore, they cannot meet the multi-framework support requirement. Besides, when deployed for large-scale training where the use of remote persistent storage is essential, their I/O performance remains sub-optimal due to the lack of customized optimizations. Additionally, the centralized communication pattern used for process orchestration (See Sec. 4.2) in DCP [30] and MCP [28] also presents potential stability risks. ByteCheckpoint address these limitations, enabling flexible multi-framework checkpointing support, as well as guaranteeing stability and efficiency at scale."}, {"title": "3 Background and Motivation", "content": "In deep learning (DL) training jobs, GPU states, including model and optimizer states, are maintained in GPU memory to enhance computation and communication efficiency. Additional crucial states, such as Random Number Generator (RNG) states in the dataloader and iteration numbers, are stored in"}, {"title": "3.1 Checkpointing in Deep Learning", "content": "CPU memory to track the runtime training progress. Due to the volatile nature of these storage media, any interruption to the running jobs can result in the loss of these states, thus wasting the elapsed GPU and CPU cycles. This loss is even more critical in large-scale LLM training, which typically involves tens of thousands of GPUs. Consequently, it is essential to periodically checkpoint these training states into persistent storage to tolerate any potential faults. When processes are restarted, they can resume training from these checkpoints, which helps minimize the waste of training progress. Furthermore, these persistent checkpoints are also required by accompanying or downstream tasks. Therefore, efficient checkpointing and flexible checkpoint transferring are crucial for enhancing productivity. The checkpointing of GPU states typically involves three phases: Device-to-Host (D2H) copy, which takes a snapshot of GPU states and stores them in CPU memory. Serialization, which converts CPU tensors into compact byte objects. This step reduces the storage footprint and enhances the portability of the GPU states. Dump to storage, which writes serialized GPU states to persistent files and creates the final checkpoints.\nAlthough recent studies have explored in-memory checkpointing [33], demonstrating significant performance advantages in failure recovery, the necessity for storing checkpoints in persistent storage continues to be crucial in the industry. Software and hardware failures are inevitable in large-scale training clusters and no in-memory checkpointing technique can ensure that checkpoints retained in memory will be preserved in the event of various failures. The cost of losing training progress in industry-level LLM Pre-Training, which involves substantial data and massive GPU resources, is prohibitively high. Therefore, using persistent storage for checkpointing remains the most reliable and robust method to safeguard training progress and ensure continuity in production environments."}, {"title": "3.2 Challenges of Checkpointing", "content": "We summarize the observed challenges of checkpointing in our production platform as follows:"}, {"title": "High I/O cost of checkpoint saving and loading.", "content": "The mainstream LLMs such as Llama [32, 4] and GPT [2, 1] feature extensive model sizes, which can even reach 405B [5]. This expansion trend is further fueled by the adoption of sparse model structures such as Mixture of Experts (MoE) [?] applied to Mixtral[10]. Consequently, the memory requirements of model and optimizer states also scale up significantly, imposing substantial overhead for saving and loading model checkpoints. Consider an LLM that has NB parameters, conducting mixed-precision training with Adam [12] optimizer. The model parameters are stored using the bfloat16 data type, necessitating 2N GB of memory. Additionally, the optimizer states, which include a replica of parameters along with their momentum and variance, are stored in the float32 data type, culminating in a memory requirement of 12N GB. In our production environment, checkpoints are stored in remote persistent storage like Hadoop Distributed File System (HDFS). During the saving and loading processes, movements of massive checkpoint data across various storage media (e.g., GPU, CPU, local SSDs, HDFS) result in significant GPU idling due to the necessary synchronization to ensure checkpoint integrity. By monitoring our LLM training jobs, we observe that the end-to-end cost of saving the distributed checkpoints of a 32B model once can take 260 seconds, substantially exceeding the time required for a single training iteration. A similar issue also arises in the checkpoint loading or resharding process where we resume training after removing unhealthy machines or initiating new downstream tasks. These observations highlight the critical need for more efficient checkpointing systems that can better tackle the extensive data transfers involved in running jobs, thereby improving overall efficiency and resource utilization. The commonly used two-step asynchronous checkpointing [16], adopted by many checkpointing modules [11, 30, 28], still falls short in terms of efficiency, particularly when dealing with remote persistent storage in large-scale training scenarios. ByteCheckpoint introduces more refined checkpoint save and load pipelines, incorporating multiple I/O optimization techniques to enhance large-scale checkpointing I/O efficiency."}, {"title": "Poor performance, flexibility, and scalability of offline scripts.", "content": "One intuitive approach for checkpoint resharding is to consolidate distributed checkpoints into a single global checkpoint. This approach bypasses the need to address complex parallelism configuration remapping during loading. However, running merging scripts introduces significant I/O and communication overheads, and can easily lead to out-of-memory (OOM) issues, due to the substantial memory consumption for loading large quantities of checkpoint data into CPU memory to execute the merging algorithm. Another solution is to develop customized resharding scripts each time when a new requirement is put forward. Nevertheless, this approach lacks scalability. Given K models with different structures and"}, {"title": "4 System Design", "content": "In this section, we first present the storage architecture of ByteCheckpoint in Sec. 4.1, followed by an illustration of how it facilitates automatic online resharding and a detailed explanation of the technique ByteCheckpoint employs to settle the irregular tensor sharding problem. After that, We introduce ByteCheckpoint's workflow, and API usage cases in Sec. 4.2."}, {"title": "High development cost for multi-framework support.", "content": "In industry platforms, we maintain diverse training frameworks such as Megatron-LM [23], DDP [15], FSDP [36], DeepSpeed [21], and veScale [11], allowing users to select the most suitable one based on specific characteristics and scale of their tasks. Most of these tasks have the requirement to save and load checkpoints efficiently, with minimal runtime overheads that could potentially impact the performance of original tasks. For instance, a prevalent need is to save checkpoints asynchronously without severely blocking ongoing tasks. However, since each framework has its unique checkpoint module and save/load logic, efficiently managing diverse checkpoint files from different frameworks becomes challenging. Besides, it is awkward to engage in repeated engineering efforts to tailor optimized implementations for each one. ByteCheckpoint introduces a unified representation for checkpoints from different frameworks, integrating it into a disaggregated storage architecture for metadata and data to facilitate seamless multi-framework checkpointing support."}, {"title": "4.1 Disaggregated Storage Architecture", "content": "Overview. ByteCheckpoint separates the storage of checkpoint data and the metadata files, thereby disaggregating checkpoints from arbitrary parallelism settings and training frameworks. As depicted in Fig. 2, each distributed checkpoint residing in different processes can be represented as (storage files, metadata). For each process, The actual numerical values of tensor shards in model or optimizer states are concatenated and stored in the corresponding storage files. Meanwhile, their metadata information is extracted separately with our customized serialization method. Metadata mainly includes three components: TensorMeta, this component records the basic information of individual tensor shards, which is essential to recover their runtime states. ShardMeta, this component records the relative position information of tensor shards from a global view. For this data field, We use the abstract of DCP [30] and extend it to support different frameworks by unified tensor shard representation into the tuple: (FQN, offsets, lengths). ByteMeta, this component records the byte start offset and length of each tensor shard within the storage files, facilitating the retrieval of numerical values necessary to reconstruct a runtime tensor. All metadata are consolidated into a single global file, accessible by all processes during the loading (resharding) phase. A mapping, i.e., ShardtoByteMap, between saved tensor shards and storage files is established with ShardMeta and ByteMeta, ensuring accurate data retrieval.\nOnline checkpoint resharding. In Fig. 3, we illustrate how our architecture facilitates flexible, automatic online resharding. For newly instantiated tensor shards, The process begins by consulting the ShardtoByteMap in the global metadata file. With the information from ShardMeta and ByteMeta, each process runs the resharding algorithm in DCP [30] (see Sec. 2) to identify the matched parts between saved and instantiated tensor shards. It then retrieves corresponding values from different storage files. After that, TensorMeta is utilized to recover the final runtime states of tensor shards, which is not depicted in Fig. 3 for clarity. The whole procedure happens silently when bytecheckpoint.load() is called, without any interaction with users.\nSettle irregular tensor sharding. The automatic online resharding mechanism operates effectively when each tensor shard retains its original shape before checkpoint saving However, many mainstream training frameworks [23, 36, 11] modify the original shape of tensor shards to enhance memory or communication efficiency. For example, in Megatron-LM [23] and veScale [11], all tensor shards of the optimizer states in the same layer (module) are flattened and merged into a one-dimension vector. Therefore, the (FQN, offsets, lengths) representation cannot be directly applied due to irregular tensor sharding. As shown in Fig. 4, when 2D (TP with ZeRO-style DP) parallel training is applied, some tensors in optimizer states are not regularly divided along the axis independently, making it fail to represent them with the triple tuple format. Other Data-Parallel training frameworks like FSDP [36] also have similar designs. To avoid this problem during checkpoint saving, FSDP [36] synchronously conducts an all-gather operation for each module in the model, allowing all processes to possess the full GPU states. Besides, when the full GPU states exceed the GPU memory capacity, the D2H copy operation is interleaved with the all-gather operation across each module to avoid OOM issues. It is also the default approach adopted by DCP [30] in supporting FSDP [36]. However, this method not only leads to substantial communication overhead but also results in considerable execution bubbles caused by frequent synchronization between the GPU and CPU, which severely hampers training progress. To efficiently address the problem of irregular tensor resharding, ByteCheckpoint proposes asynchronous tensor merging. Rather than simply executing synchronous all-gather and D2H operations, ByteCheckpoint first identifies all irregular tensor shards (e.g., tensor shard B0 in Fig.4). It then employs asynchronous Peer-to-Peer (P2P) communications across processes to collect and integrate these irregular shards into regular ones, without blocking the default checkpointing workflow (See Sec.4.2). To enhance the efficiency, the irregular tensor shards are distributed among processes using a Worst-Fit workload balancing algorithm, based on their sizes. The launches of all P2P waiting and D2H copy operations for those tensor shards are strategically postponed until they enter the serialization phase, thereby removing frequent synchronization and enabling better overlapping between communication and I/O operations."}, {"title": "4.2 Workflow", "content": "The workflow of ByteCheckpoint is illustrated in Fig. 5. ByteCheckpoint offers a unified interface that accommodates various training frameworks and facilitates interactions with a range of storage backends, including memory file systems, local disks, HDFS, and Network File System (NFS). By decoupling the Planner Layer and the Storage Layer from the Execution Layer, ByteCheckpoint's"}, {"title": "API Layer.", "content": "For users working with various training frameworks, manually managing distributed checkpoints can be challenging and error-prone. To alleviate this issue, we offer straightforward and user-friendly APIs, akin to torch.save() and torch.load(), that require no additional learning cost for DL engineers and researchers. These APIs simplify the process by shielding users from the complexities of the underlying system, such as information extraction of process group and device mesh configurations, save/load plan generation, efficient I/O operations, etc. Users only need to provide a dictionary containing keys like \"model,\" \"optimizer,\" or \"extra states\" (CPU states) when saving and loading checkpoints. Moreover, our API is independent of parallelism strategies and the specifics of the distributed environment. We give the API usage examples in Fig. 6."}, {"title": "Planner Layer.", "content": "The Planner Layer mainly comprises two steps: one gathering step and one scattering step. It first identifies the items to be saved or loaded from the provided dictionary, constructs the TensorMeta and ShardMeta for saving, or retrieves the global metadata file for loading purposes. After that, the coordinator planner, located in the process with rank 0, gathers TensorMeta and ShardMeta or the ShardtoByteMap from others and applies workload distribution algorithms (refer to Sec. 5 for a detailed explanation of the saving/loading workload partitioning mechanism) to create save/load plans. These plans specify which tensor shards each process will save, transfer (for merging irregular shards), or load. The finalized plans are scattered to each process and then forwarded to the Execution Layer to carry out the actual I/O tasks. The saving workflow includes an additional gathering step to gather the ByteMeta constructed from the storage files upon the completion of I/O tasks and assemble collected metadata into the final global metadata file."}, {"title": "Execution Layer.", "content": "Given the save/load plans from the Planner Layer, the Execution Layer manages interactions with storage backends and executes the actual I/O tasks detailed in the plans. During the saving process, the different phases associated with saving each tensor shard (outlined in Sec. 3) are executed separately in a fully asynchronous pipeline, while the P2P communications for tensor merging are conducted concurrently. By default, the checkpoint-saving workloads are handled by separated I/O processes to move them from the critical path of training. For loading, each process retrieves data from storage files and employs advanced techniques such as the zero redundancy loading mechanism to reduce loading costs. Most I/O performance optimization techniques are incorporated in the Execution Layer, further details on these optimizations are included in Sec. 5."}, {"title": "Storage Layer.", "content": "The Storage Layer handles various storage backends and implements background optimizations tailored to the characteristics of the selected storage backend. For example, in scenarios where the persistent storage backend is set to HDFS (the most common setup in our platform), an additional uploading phase is integrated into the checkpoint-saving pipeline. The HDFS Engine leverages the shared memory of machines (e.g., the /dev/shm directory) to temporarily store the bytes returned from I/O processes, and utilizes background threads to efficiently upload these data to the remote HDFS without blocking main training process and I/O processes. Similar to the Planner Layer, the Storage Layer operates independently from the Execution Layer. This independence significantly simplifies the integration of new backends, thereby reducing development costs. Moreover, any optimizations in the Storage Layer can benefit users from all training frameworks."}, {"title": "Stability Optimization.", "content": "As introduced above, for the Planner Layer, we adopt a similar planning protocol as DCP [30]. However, in this protocol, centralized gathering and scattering steps pose a significant burden on the coordinator process, particularly when executed at scale. To improve the stability of this planning during large-scale training, we implement a tree-based communication topology. Processes on the same machine are organized into a first-level subtree, where the process with local rank 0 is designated as the root. For inter-machine communication, we iteratively group multiple machines, assigning the process with the lowest global rank in each group as the root. This process continues until all processes are integrated into a path leading to the global root (rank 0). Existing connections between processes are leveraged during this topology construction. For instance, in large-scale 3D parallel training, a TP-DP-PP three-level communication tree can be directly built without the need to establish any new connections. Following this setup, each process begins by extracting TensorMeta and ShardMeta from local tensor shards. Subsequently, the coordinator process (rank 0) hierarchically gathers all these data through the communication tree. The scattering step is executed via the reverse path."}, {"title": "5 Performance Optimization Techniques", "content": "In this section, we introduce ByteCheckpoint's I/O optimizations applied in the saving and loading processes."}, {"title": "5.1 Saving Optimizations", "content": "Fully asynchronous pipeline. As shown in Fig. 7a, ByteCheckpoint separates different phases of checkpoint saving for each tensor shard and pipelines the execution of them independently to hide the costs associated with each phase from others effectively. If the tensor shard is irregularly sharded, the save pipeline will start from the P2P data transfer phase (see Sec. 4.1). When using HDFS as the storage backend, an additional upload phase is incorporated into the pipeline. Compared to the save pipelines in CheckFreq [16] and DCP [30], ByteCheckpoint can further reduce the end-to-end completion time of checkpointing. This enhancement leads to less runtime overheads, optimizing the overall training efficiency."}, {"title": "Ping-Pong pinned memory pool.", "content": "The efficiency of the D2H copy is crucial since any modifications on GPU states cannot be applied until the completion of this process. To minimize the interruption, ByteCheckpoint employs a pinned CPU memory pool which leverages the high PCIe bandwidth to accelerate D2H tensor transfers, while also eliminating the costs of CPU memory reallocation. To reuse the allocated pinned memory and prevent copied GPU states from data corruption, a straightforward method requires halting until the completion of I/O tasks across different save function calls. However, in scenarios where checkpoint creation is more frequent, this approach can incur considerable delays and severely impede the training process. To address this issue, we design a Ping-Pong buffering mechanism within the pinned memory pool. As illustrated in Fig. 7b, this mechanism maintains two pinned memory pools that alternate their roles: one pool gathers newly copied states from GPU memory (serving as the read buffer), while the other transfers states to the I/O workers (acting as the write buffer). This approach effectively avoids the need for synchronization within the same memory pool, thereby minimizing delays associated with consecutive save function calls."}, {"title": "Workload balancing.", "content": "For training that employs data parallelism strategies, the model states are replicated across all processes within the DP groups, leading to duplicated data storage and prolonged checkpointing duration. In DCP [30], a simple deduplication algorithm is employed: it designates the process with group rank 0 in each DP group as responsible for saving all model states. This approach, however, results in significantly imbalanced workloads. To address this imbalance, we implement a workload-balanced deduplication mechanism. By adopting the Worst-Fit algorithm, we distribute the saving tasks according to the size of each tensor shard. This approach promotes a more even distribution of workloads across all processes within the same DP group, thereby enhancing the efficiency for parallel processing of saving tasks among various training processes and reducing the end-to-end completion time."}, {"title": "Plan and metadata cache.", "content": "Apart from the tree-based communication topology (see Sec. 4.2) designed for enhancing planning stability, ByteCheckpoint further optimizes the planning efficiency for checkpoint saving by introducing plan and metadata caching mechanisms, transforming the planning process into a one-time cost. Once the save plans and the global metadata file are created for the first time, they are cached for subsequent reuse. This allows the system to bypass the repetitive planning process, therefore reducing unnecessary communication overheads."}, {"title": "5.2 Loading Optimizations", "content": "Fully asynchronous pipeline. Similar to saving, for checkpoint loading, we also enable the fully asynchronous pipeline to reduce loading costs. Specifically, we pipeline the executions of file reading, deserialization, and Host-to-Device (H2D) copy for each tensor shard, achieving higher efficiency."}, {"title": "Zero redundant loading #1: partial file reading.", "content": "As illustrated in Fig. 3, during the resharding process, newly initiated processes may only need to access specific sections of the saved tensor shards from the checkpoint storage files. DCP [30] overlooks this aspect and reads the entire tensor shard, subsequently performing in-memory slicing to isolate the required segments. This approach leads to unnecessary I/O costs that negatively impact overall efficiency. Besides, it can also bring potential OOM issues when CPU memory is scarce. Furthermore, when checkpoints are stored on remote persistent storage such as HDFS, each process must download the whole storage files to access the fragments in tensor shards, thereby exacerbating communication overhead. ByteCheckpoint addresses this inefficiency by implementing a partial file reading mechanism, allowing each process to directly access only the needed segments of the data without any redundancy (shown in Fig. 8a). Specifically, for each tensor shard that is newly instantiated, ByteCheckpoint utilizes the ShardMeta and ByteMeta of the saved tensor shards to convert multidimensional retrieval indices into a series of byte-level, one-dimensional indices, enabling direct data access based on these byte-level indices. In this way, required tensor segments can be directly read into CPU memory, without the need for extra in-memory slicing."}, {"title": "Zero redundant loading #2: overlap tensor reading with transferring.", "content": "The partial file reading technique effectively eliminates redundant tensor reads within a single process. However, when resharding to configurations that include data parallelism, the same tensor shards are still requested and read by multiple processes within a DP group. Besides, during large-scale training, a massive number of concurrent read requests can strain the bandwidth of HDFS, creating a bottleneck for checkpoint loading [11]. To completely eliminate repetitive tensor loading within DP groups and alleviate the load on HDFS, ByteCheckpoint utilizes idle inter-GPU bandwidth in the training cluster to combine tensor reading with tensor transferring. As depicted in Fig. 8b, within each DP group, tensor reading tasks are distributed evenly across all processes to avoid redundant reads. We initiate I/O threads that employ the partial file reading technique to read designated tensor shards. Concurrently, in the main thread, shards loaded in CPU memory are copied into GPU memory and then transferred to other processes that require those shards. Given that the tensor transfer communication pattern among the DP group constitutes the all-to-all (all2all) collective primitive, we implement a ring-based all2all algorithm to avoid communication contention between processes, thereby boosting communication efficiency effectively. It is recognized that the distribution of all repeated tensor shards may not always be perfect even across processes in each DP group. To address any discrepancies, tail send and receive requests are managed separately after the completion of all primary all2all communication tasks."}, {"title": "6 Evaluation", "content": "We evaluate the performance of ByteCheckpoint with various real-world LLM production workloads in our platform. This section is organized as follows: we first introduce the settings for our experiments (Sec. 6.1), then verify the correctness of ByteCheckpoint's automatic online resharding functionality in Sec. 6.2. Finally, we study the efficiency of ByteCheckpoint in terms of checkpoint saving and loading in Sec. 6.3 and Sec. 6.4, respectively."}, {"title": "6.1 Experimental Setup", "content": "Detailed experimental configurations are given in Table 2. Without ambiguity, we use strategies in ZERO [20] to refer to sharding plans employed in FSDP [36]."}, {"title": "Models and testbed.", "content": "In our experiments, we adopt two model types (dense and sparse) with different parameter sizes, both of which are implemented based on the model structure of GPT-3 [2]. For the DenseGPT models, we utilize a training cluster that includes machines with NVIDIA A100 80GB GPUs while for the SparseGPT models, we use another cluster with NVIDIA H800 80GB GPUs. All"}, {"title": "6.2 Checkpoint Correctness", "content": "ByteCheckpoint supports flexible automatic resharding, which enables loading distributed checkpoints into arbitrary parallelism configurations in an online manner. In Fig. 9, we demonstrate normalized training loss curves before and after resharding with ByteCheckpoint under various situations. Take Fig. 9a as an example, specifically, this test comprises two phases: we first apply the ZeRO-2 strategy and train the DenseGPT 6.7B model with 16 GPUs, then save the checkpoints with ByteCheckpoint. After that, we resume the training with the ZeRO-3 strategy and reduce the number of GPUs to 8, then load checkpoints from the old configurations. The normalized loss curve after resharding can smoothly match that in the previous phase and continue to display a consistent decline trend. Experimental results for the opposite transformation can be found in Fig. 9b. Similar findings are observed with the SparseGPT 28B model, which is trained using Megatron-LM [23]. Additionally, we include a scenario without checkpoint resharding (Figure 9d) to illustrate the broad applicability of ByteCheckpoint. These experimental results confirm the automatic online resharding ability of ByteCheckpoint, which can flexibly accommodate different parallelism configurations and training frameworks."}, {"title": "6.3 Saving Efficiency", "content": "We investigate the acceleration of checkpointing provided by ByteCheckpoint relative to established baseline methods. Table 3 shows the checkpoint stalls incurred by various checkpointing methods at different checkpoint frequencies. Notably, ByteCheckpoint significantly reduces the runtime checkpointing overhead, achieving reductions ranging from 23.82\u00d7 to 529.22 \u00d7. This improvement decreases average checkpoint stalls from minute-long duration into seconds or even sub-seconds. We observed that the acceleration on DenseGPT 10B is particularly pronounced compared to the baseline. This is because FSDP necessitates time-consuming all-gather and D2H copy operations for each tensor shard to address the issue of irregular tensor sharding. This process introduces substantial communication and synchronization overheads (see Sec. 4.1), which escalates as the training scale increases (up to 256 GPUs for DenseGPT 10B), leading to severe checkpoint stalls. In contrast, ByteCheckpoint employs an asynchronous P2P communication method that effectively reduces these overheads. This approach merges only the irregularly sharded tensors rather than all tensor shards asynchronously and postpones the D2H copy phase to reduce frequent synchronization."}, {"title": "6.4 Loading Efficiency", "content": "We compare the end-to-end resharding times of different methods on DenseGPT 6.7B and SparseGPT 28B to study the loading performance of ByteCheckpoint. As depicted in Fig. 10, ByteCheckpoint finish checkpoint resharding consistently faster than baselines. Specifically, ByteCheckpoint achieves 1.55 \u00d7 and 1.62 \u00d7 acceleration over FSDP [36] checkpoint loading module for SFT and Auto-eval tasks, respectively. These gains are even more pronounced (3.37\u00d7 faster on the SFT task and 3.51 \u00d7 faster on the Auto-eval task) when comparing with the loading pipeline of Megatron-LM [23], which involves the costly execution of offline resharding scripts. Thanks to the zero redundancy loading mechanism, ByteCheckpoint can detect unnecessary tensor segments for each process as well as repeated tensor shards across processes when loading saved checkpoints into new parallelism configurations. It then leverages the partial file reading to avoid redundant segments and overlaps the tensor data reading with transferring across GPUs, therefore accelerating the whole loading process."}, {"title": "7 Conclusion", "content": "We propose ByteCheckpoint, a PyTorch-native LLM checkpointing system designed for efficient automatic online checkpoint resharding and flexible multi-framework support. ByteCheckpoint utilizes a disaggregated storage architecture for checkpoint data and metadata, effectively decoupling the checkpoint storage from various parallelism configurations and training frameworks. ByteCheckpoint addresses the challenges of irregular tensor resharding through the use of asynchronous tensor merging and enhances the stability of planning by implementing a tree-based communication topology. Additionally, we propose several I/O performance optimization techniques aimed at boosting system efficiency. These techniques include fine-grained save/load pipelines, the Ping-Pong memory pool, workload-balanced saving, zero redundancy loading, etc. Extensive Experimental results demonstrate the superiority of ByteCheckpoint over baselines on various real-world checkpoint saving and loading tasks."}]}