{"title": "SCA: Highly Efficient Semantic-Consistent\nUnrestricted Adversarial Attack", "authors": ["Zihao Pan", "Weibin Wu", "Yuhang Cao", "Zibin Zheng"], "abstract": "Abstract\u2014Unrestricted adversarial attacks typically manipu-\nlate the semantic content of an image (e.g., color or texture) to\ncreate adversarial examples that are both effective and photore-\nalistic. Recent works have utilized the diffusion inversion process\nto map images into a latent space, where high-level semantics are\nmanipulated by introducing perturbations. However, they often\nresults in substantial semantic distortions in the denoised output\nand suffers from low efficiency. In this study, we propose a novel\nframework called Semantic-Consistent Unrestricted Adversarial\nAttacks (SCA), which employs an inversion method to extract\nedit-friendly noise maps and utilizes Multimodal Large Lan-\nguage Model (MLLM) to provide semantic guidance throughout\nthe process. Under the condition of rich semantic information\nprovided by MLLM, we perform the DDPM denoising process\nof each step using a series of edit-friendly noise maps, and\nleverage DPM Solver++ to accelerate this process, enabling\nefficient sampling with semantic consistency. Compared to ex-\nisting methods, our framework enables the efficient generation\nof adversarial examples that exhibit minimal discernible semantic\nchanges. Consequently, we for the first time introduce Semantic-\nConsistent Adversarial Examples (SCAE). Extensive experiments\nand visualizations have demonstrated the high efficiency of SCA,\nparticularly in being on average 12 times faster than the state-of-\nthe-art attacks. Our code can be found at https://github.com/Pan-\nZihao/SCA.\nIndex Terms\u2014Unrestricted adversarial attack, latent space,\ndiffusion inversion process, semantic consistency.", "sections": [{"title": "I. INTRODUCTION", "content": "EEP neural networks have excelled in learning features\nand achieving impressive performance across various\ntasks. However, they are vulnerable to small perturbations,\nknown as adversarial samples, which raise significant security\nconcerns for critical decision-making systems. To ensure hu-\nman visual imperceptibility and maintain photorealism, adver-\nsarial examples must retain high semantic consistency with the\noriginal images, appearing natural and realistic despite minor\nperturbations [1]\u2013[4].\nA recent trend in adversarial attacks is the emergence of\nunrestricted adversarial attacks [2], [5]\u2013[7], which alter seman-\ntic features such as color and texture to produce adversarial\nexamples that are both visually plausible and highly effective"}, {"title": "II. RELATED WORK", "content": "Adversarial attacks seek to maximize a model's classifica-\ntion error while preserving image semantics. Below, we review\nkey approaches.\nNorm-Bounded Attacks. Norm-bounded attacks are pop-\nular for their simplicity and effectiveness, using gradient-\nbased optimization to maximize model loss. Techniques like\nFGSM [17], L-BFGS [18], PGD [19], and the CW attack [20]\nconstrain perturbations within an \\(l_p\\) norm, ensuring minimal\nmodifications. However, these methods often introduce distor-\ntions detectable by humans and robust models, making them\nless effective in real-world applications. Norm constraints also\nlimit flexibility, as \\(l_p\\) norms do not always align with human\nperception, leading to visually suboptimal results.\nUnrestricted Adversarial Attacks. Unrestricted attacks,\nwhich focus on natural transformations like shape, texture,\nand color, offer more flexibility than norm-bounded methods.\nShape-based approaches [21], [22] alter image geometry but\nstruggle to maintain realism. Texture-based attacks [5], [23]\nmodify perceptual features but often produce unnatural results.\nColor-based attacks [1], [5], [24]\u2013[27] achieve photorealism\nbut lack flexibility and transferability across models. These\napproaches balance realism and adversarial effectiveness, but\neach comes with trade-offs.\nLatent Space-Based Attacks. Recent methods exploit gen-\nerative models like GANs [12], [13], [28], [29] and diffusion\nmodels [9], [30]\u2013[32] to manipulate the latent space and gener-\nate adversarial images. While offering control over image gen-\neration, these approaches often introduce noticeable semantic\nshifts, reducing stealthiness. Our framework addresses this\nby maintaining semantic consistency in adversarial examples,\nenhancing both stealth and practical applicability."}, {"title": "III. PRELIMINARIES", "content": "Diffusion models (DMs) generate data by iteratively de-\nnoising a Gaussian-distributed random variable to approximate\na learned data distribution through forward and reverse pro-\ncesses. In the forward process, an image \\(x_0\\) is transformed\ninto Gaussian noise via:\n\n\\(x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} n_t, t = 1,...,T\\)\n\nwhere \\(n_t \\sim N(0, I)\\) is noise, and \\(\\beta_t\\) controls the noise\ninjection. This leads to \\(x_T \\sim N(0,I)\\). Marginalizing over\nintermediate states gives:\n\n\\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t\\)\n\nwith \\(\\epsilon_t \\sim N(0, I)\\). In the reverse process, the model\ndenoises \\(x_T\\) to reconstruct \\(x_0\\) as:\n\n\\(x_{t-1} = \\mu_t(x_t) + \\sigma_t z_t, t = T, . . ., 1\\)\n\nwhere \\(\\sigma_t = \\eta \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}} \\beta_t\\) controls stochasticity. The mean\nprediction is:\n\n\\(\\mu_t(x_t) = \\sqrt{\\bar{\\alpha}_{t-1}} \\frac{\\beta_t}{1 - \\bar{\\alpha}_t} x_t + \\sqrt{\\alpha_t} (\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} - \\alpha_t \\bar{\\alpha}_t) \\epsilon_{\\theta}(x_t)\\)"}, {"title": "IV. METHOD", "content": "The pipeline of our proposed method is shown in Fig. 2.\nThe core idea of Semantic-Consistent Unrestricted Adversarial\nAttack is to enhance semantic control throughout the entire\ngeneration process of unrestricted adversarial examples, which\nis achieved by introducing a novel inversion method that\ncan \"imprint\" the image more strongly onto the noise maps.\nFurthermore, the powerful semantic guidance provided by\nMLLMs further restricts the direction of perturbations in the\nlatent space, enabling the clean image to produce our desired\nadversarial changes, namely imperceptibility and naturalness.\nSpecifically, our method can be divided into two parts:\nSemantic Fixation Inversion and Semantically Guided\nPerturbation. We first map the clean image into a latent\nspace through Semantic Fixation Inversion, and then iteratively\noptimize the adversarial objective in the latent space under\nsemantic guidance, causing the content of the image to shift\nin the direction of deceiving the model until the attack is\nsuccessful. The algorithm of SCA is presented in Algorithm\n1."}, {"title": "A. Problem Definition", "content": "For a deep learning classifier \\(F_\\theta(\\cdot)\\) with parameters \\(\\theta\\), let \\(x\\)\nrepresent the clean image and \\(y\\) the corresponding ground truth\nlabel. Unrestricted adversarial attacks seek to introduce subtle\nadversarial perturbations (e.g., image distortions, texture, or\ncolor alterations, etc.) to the input \\(x\\), resulting in an adversarial\nexample \\(x_{adv}\\) that misguides the classifier \\(F_\\theta(\\cdot)\\):\n\n\\(x_{adv} = \\arg \\max L(F_\\theta(x_{adv}), y), s.t. d(x, x_{adv}) < \\epsilon\\)\n\nwhere \\(L(\\cdot)\\) is the loss function and \\(d(x, x_{adv})\\) represents the\nsemantic distance between \\(x\\) and \\(x_{adv}\\). Based on the above,\nwe believe that ideal unrestricted adversarial examples should\nhave high semantic consistency."}, {"title": "B. Semantic Fixation Inversion", "content": "In the context of Semantic Fixation Inversion, the key ob-\njective is to extract rich and semantically relevant information\nfrom the original image during the inversion process. This\nsemantic information acts as an informative prior, guiding\nboth the perturbation of the latent space and the image\nreconstruction, facilitating a Semantic Consistency Attack. In\nour method, the semantic priors are derived from edit-friendly\nnoise maps and the powerful capabilities of Multimodal Lan-\nguage Models (MLLMs). Recent research predominantly fo-\ncuses on inverting the diffusion sampling process to determine\n\\(x_T\\), which can be subsequently denoised into the input image\n\\(x_0\\). Among the inversion techniques, inverting the DDPM\n(Denoising Diffusion Probabilistic Models) scheduler is often\nfavored over DDIM (Denoising Diffusion Implicit Models)\ninversion, as the former can achieve results in fewer steps\nwhile maintaining perfect reconstruction [33].\nAlthough the DDPM scheduler provides efficient inversion,\nmore advanced approaches exist for sampling in Diffusion\nModels (DMs), significantly reducing the number of steps and,\nconsequently, the number of DM evaluations. In our work,\nwe adopt a more efficient inversion approach [34], which\nretains the essential inversion properties while accelerating the\nprocess. Previous work by Song et al. [35] has shown that the\nreverse diffusion process in DDPM can be viewed as a first-\norder stochastic differential equation (SDE). Solving this SDE\nwith higher-order methods, such as the dpm-solver++ [36],\nenables more efficient computation and reduces the number\nof required steps.\nLeveraging the semantic understanding capability of\nMLLMs, which have been shown to provide accurate semantic\ninterpretations across diverse image domains, we employ\nMLLMs to generate rich captions \\(P\\) for the clean input image\n\\(x_0\\). These captions are then converted into conditional embed-\ndings \\(C_p\\) via a function \\(\\phi(P)\\), serving as the conditional input\nfor the pre-trained diffusion model parameterized by \\(\\epsilon_\\theta\\). This\nprocess integrates seamlessly with the existing formulation for\nconditional noise estimation \\(\\hat{\\epsilon}_\\theta (x_t, C_p)\\) as given in equation (4).\nThe reverse diffusion process, which uses the second-order\ndpm-solver++ SDE solver, can be written as:"}, {"title": "C. Semantically Guided Perturbation", "content": "In this section, we introduce an optimization technique for\nlatents aimed at enhancing the attack performance on unre-\nstricted adversarial examples. Drawing from previous work\n[9], perturbing the text embedding \\(C_p\\) to generate adversarial\ncontent is suboptimal because it is crucial for semantic con-\nsistency, preserving a wealth of semantic information. Addi-\ntionally, due to the discrete nature of textual representations,\ncurrent techniques struggle to generate smooth and diverse\nperturbations based on gradient methods. Consistent with\nstate-of-the-art approaches [9], [10], [30], we opt to optimize\nthe adversarial objective within the latent space. Building upon\nSemantic Fixation Inversion, we utilize rich semantic priors\nto guide the optimization process. We define the denoising\nprocess of diffusion models as \\(D(\\cdot)\\) through (10), involving \\(T\\)\niterations:\n\n\\(x_0 = \\mu_1(x_1, x_2, C_p) + \\sigma_1 z_1\\)\n\n\\(= \\mu_1 (\\mu_2(x_2, x_3, C_p) + \\sigma_2 z_2, \\mu_3(x_3, x_4, C_p) + \\sigma_3 z_3, C_p)\\)\n\n\\(+ \\sigma_1 z_1\\)\n\n\\(= D(x_T, z_1, z_2, ..., z_T, C_p)\\)\n\nwith\n\n\\(x_{t-1} = \\mu_t(x_t, x_{t+1}, C_p) + \\sigma_t z_t, t = T, ..., 1\\)\n\nTherefore, the reconstructed image is denoted as \\(x_0 = \\)\nD\\((x_T, \\{z_t\\}_{t=1}^T, C_p,T\\). The computational procedure of the\nVAE is omitted in this context, as it remains differentiable.\nOur adversarial objective is expressed by:\n\n\\(\\arg \\max L(F_\\theta(x_0, y), \\)\n\n\\(\\delta \\)\n\ns.t. \\(||\\delta|| \\leq \\kappa, x_0 = D(x_T, \\{z_t\\}_{t=1}^T, C_p, T\\)\n\nand \\(d(x, x_{adv}) < \\epsilon\\)\n\nwhere \\(\\delta\\) is the adversarial perturbation on the latent space\nand \\(d\\) represents the semantic distance. Previous works [9],\n[10] typically design the loss function \\(L\\) as comprising two\ncomponents: i) the cross-entropy loss \\(L_{ce}\\), which primarily\nguides adversarial examples towards misclassification, and ii)\nthe mean square error loss \\(L_{mse}\\), which mainly ensures that\nthe generated adversarial examples are as close as possible to\nthe clean image in terms of \\(l_2\\) distance. However, extensive\nexperiments have confirmed that the additional introduction\nof \\(L_{mse}\\) in the loss function has a very limited role in se-\nmantic preservation [9], perhaps because pixel-level numerical\nchanges are not a high-level semantic constraint. Furthermore,\nthe introduction of two optimization objectives increases the\ndifficulty of optimization and limits the capability of the\nattack. Specific experimental results regarding this component\nare presented in our ablation study. Since good semantic\nconsistency has already been achieved during the Semantic\nFixation Inversion phase, we let \\(L = L_{ce}\\).\nExisting methods [9], [10], [30] generally assume that a\nperturbation \\(\\delta\\) will not disrupt semantic consistency when it\nis small, hence imposing a norm constraint on \\(\\delta\\). However,\nwe find that even small perturbations in the latent space can"}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\nDatasets. We carry out our experiments using the\nImageNet-compatible dataset [45], which includes 1,000 im-\nages from the ImageNet validation set [46]. This dataset has\nbeen frequently employed in related work [9], [26], [44], [47],\n[48].\nSemantic Consistency Evaluation. We use CLIP Score\n[49], SSIM [50], PSNR [51], and LPIPS [52] to measure the\nsemantic consistency between the generated unrestricted ad-\nversarial examples and the clean image from multiple aspects.\nAttack Evaluation. We compare SCA with other unre-\nstricted adversarial attacks, including SAE [1], ADef [21],\ncAdv [5], tAdv [5], ACE [24], ColorFool [25], NCF [26],\nAdvST [27], and ACA [9]. The default parameter settings\nfor these attacks are used. The evaluation metric employed\nis attack success rate (ASR), which measures the proportion\nof images misclassified by the target models.\nTarget Models. To assess the adversarial robustness of\nvarious architectures, we target both convolutional neural\nnetworks (CNNs) and vision transformers (ViTs). The CNN\nmodels include MobileNet-V2 (MN-v2), Inception-v3 (Inc-\nv3), ResNet-50 (RN-50), ResNet-152 (RN-152), DenseNet-\n161 (Dense-161), and EfficientNet-b7 (EFb7). For ViTs, we\nevaluate MobileViT (MobViT-s), Vision Transformer (ViT-B),\nSwin Transformer (Swin-B), and Pyramid Vision Transformer\n(PVT-v2).\nImplementation Details. All experiments are performed\nusing PyTorch on an NVIDIA Tesla A100. The DDPM steps\nare set to T = 10 \u2192 20, with attack iterations \\(N_a\\) = 10,\n\\(\\eta\\) = 0.04, \\(\\kappa\\) = 0.1, and \\(\\mu\\) = 1. The version of Stable\nDiffusion [11] used is v1.5, and image captions are generated\nautomatically via LLaVA-NeXT."}, {"title": "B. Semantic Consistency Comparison", "content": "In order to comprehensively evaluate the semantic consis-\ntency preservation ability of current unrestricted adversarial\nattack methods from multiple perspectives, we introduce four\nindicators, namely CLIP Score, SSIM, PSNR and MSE. The\nspecific results are shown in Table I.\nIn summary, the experimental results show that the adversar-\nial examples we generate outperform existing methods in most\nindicators, especially in LPIPS and CLIP Score. This further\nproves the superiority of our SCA in maintaining semantic\nconsistency."}, {"title": "C. Comparison of Attack Success Rates", "content": "We systematically evaluate the adversarial robustness of\nconvolutional neural networks (CNNs) and vision transformers\n(ViTs) using several prominent adversarial methods: SAE\n[1], ADef [21], cAdv and tAdv [5], ACE [24], ColorFool\n[25], NCF [26], AdvST [27], ACA [9], and our proposed\nSCA. These methods were applied to MobileNetV2 (MN-v2),\nResNet-50 (RN-50), MobViT-s, and ViT-B, with performance\nmeasured using the Average Attack Success Rate (%), which\ncaptures the transferability of adversarial examples across non-\nsubstitute models.\nOur results confirm that SCA achieves state-of-the-art per-\nformance by enhancing the imperceptibility and stealthiness of\nadversarial examples through semantic consistency. Crucially,\nthis semantic alignment does not compromise attack efficacy;\ninstead, it enhances transferability without sacrificing perfor-\nmance."}, {"title": "D. Image Quality Comparison.", "content": "In this section, we conduct a detailed quantitative assess-\nment of the image quality for adversarial examples, following"}, {"title": "E. Qualitative Comparison", "content": "In this section, we present a qualitative analysis of various\ncurrent unrestricted attack methods, illustrated in Fig. 3. Our\nfindings reveal that the SCA generates adversarial examples\nthat exhibit a greater degree of naturalness compared to\nmethods that rely heavily on color and texture transformations.\nNotably, techniques such as SAE and ColorFool often produce"}, {"title": "F. Time Analysis", "content": "In this section, we present a comparison of the attack speeds\nfor various unrestricted attacks. Using MN-v2 as the surrogate\nmodel, we measure the inference time on an NVIDIA Tesla\nA100. Table IV shows the average time (in seconds) required\nto generate an adversarial example per image. Compared with\nthe current state-of-the-art methods, we have increased the\nspeed by about 12 times while maintaining the diversity of\ngenerated content and the success rate of attacks. This is\nmainly due to our novel inversion method, and the use of DPM\nSolver++ greatly accelerates the Diffusion sampling process.\nExperiments have shown that we can reduce the number of\nsampling steps to 10 to 20 steps without affecting the quality of\nthe generated images. From Table IV, it can be seen that some\nmethods take very short time, but experiments have shown that\ntheir effects are generally poor. Overall, our method is the most\nefficient."}, {"title": "G. Ablation Study", "content": "In the process of compute the gradients for optimization\nbased on In the gradient optimization process using (10) and\n(11), we apply the random gradient-free (RGF) method. To\nevaluate its effectiveness, we conducted ablation tests under\nthree conditions: without gradient estimation (\"NE\"), using\nthe Skip Gradient method (\"SG\"), and using RGF. Table V\nshows the results. Our SCA performs poorly without gradient\nestimation and with the Skip Gradient method. The ASR for\n\"No estimation\" is similar to the clean image, showing that\nwe can't ignore the diffusion model's sampling and directly\noptimize with the classifier's loss in the latent space. The Skip\nGradient method also underperforms, likely due to the need\nfor more sample steps, as we use complex optimizations to\nspeed up sampling. Additionally, our new inversion method\ndiffers from prior approximations, validating the importance\nof the RGF method in our framework.\nWe also tested the impact of using MLLM models for\nsemantic consistency. In this experiment, we replaced LLaVA-\nNexT with BLIP-2 for generating captions during perturbation\nand also tried unconditional generation without prompts. As\nshown in Figure 1, BLIP-2 produces very brief descriptions,\nleading to a significant semantic drift (Figure 4(b)). In con-\ntrast, LLaVA-NexT generates detailed captions, maintaining\nbetter semantic alignment with the clean image (Figure 4(c)).\nAdditional experiments confirm that richer captions improve\nsemantic preservation. In the case of unconditional genera-\ntion (Figure 4(a)), the output remains nearly identical to the\noriginal, failing to produce adversarial perturbations, likely\ndue to overly strong semantic constraints. This highlights"}, {"title": "VI. CONCLUSION", "content": "In this paper, We propose a novel attack framework called\nSemantic-Consistent Unrestricted Adversarial Attack (SCA)\nvia Semantic Fixation Inversion and Semantically Guided\nPerturbation. The core idea of SCA is to enhance semantic\ncontrol throughout the entire generation process of unrestricted\nadversarial examples. We initially utilize an effective inversion\nmethod and a powerful MLLM to extract rich semantic priors\nfrom a clean image. Subsequently, we optimize the adversarial\nobjective within the latent space under the guidance of these\npriors. Experiments demonstrate that the adversarial examples\nexhibit a high degree of semantic consistency compared to\nexisting methods. Furthermore, our method is highly efficient.\nConsequently, we introduce Semantic-Consistent Adversarial\nExamples (SCAE). Our work can shed light on further under-\nstanding the vulnerabilities of DNNs as well as novel defense\napproaches."}]}