{"title": "Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection", "authors": ["Mingyu Derek Ma", "Yanna Ding", "Zijie Huang", "Jianxi Gao", "Yizhou Sun", "Wei Wang"], "abstract": "Generative Language Models rely on autoregressive decoding to produce the output sequence token by token. Many tasks such as preference optimization, require the model to produce task-level output consisting of multiple tokens directly by selecting candidates from a pool as predictions. Determining a task-level prediction from candidates using the ordinary token-level decoding mechanism is constrained by time-consuming decoding and interrupted gradients by discrete token selection. Existing works have been using decoding-free candidate selection methods to obtain candidate probability from initial output logits over vocabulary. Though these estimation methods are widely used, they are not systematically evaluated, especially on end tasks. We introduce an evaluation of a comprehensive collection of decoding-free candidate selection approaches on a comprehensive set of tasks, including five multiple-choice QA tasks with a small candidate pool and four clinical decision tasks with a massive amount of candidates, some with 10k+ options. We evaluate the estimation methods paired with a wide spectrum of foundation LMs covering different architectures, sizes and training paradigms. The results and insights from our analysis inform the future model design.", "sections": [{"title": "1 Introduction", "content": "Generative LMs respond to queries by generating tokens to form an output sequence and optimize themselves by learning to generate the correct tokens (Lewis et al., 2020). The simplicity of token-level inference and optimization compromises its performance on end tasks, as there is a gap between the token-level paradigm and sequence-level task results and learning signals (Longpre et al., 2023; Ethayarajh et al., 2024). Some tasks use generative LM to select the answer(s) from a given pool of options where each candidate answer is a natural language sequence. For example, multiple-choice QA considers answer options as the candidate pool (Khashabi et al., 2020); the large collection of labels are candidate answers for extreme label classification tasks (Amigo and Delgado, 2022); and disease ontology forms the candidate space for diagnosis tasks (Singhal et al., 2023). The typical practice is to decode a complete output sequence and then match it with candidates (Mishra et al., 2022). However, selecting candidates using full decoding not only cuts off the gradient flow and disables direct optimization on decoded results but also limits the output bandwidth due to time-consuming discrete decoding.\nExisting works perform candidate selection without decoding for outcome-level optimization or efficient parallel predictions (Rafailov et al., 2023; Ma et al., 2025). For example, Ma et al. (2023b) calculate averaged logits of MCQA options to select an answer without decoding; Xu et al. (2023a) estimate the NLI result using logits of a single token. Though these decoding-free candidate selection practices are widely used, there is no formal definition or clear investigation of the properties of each method. There is also no consensus about the guiding principles for deploying those methods under various tasks and data scenarios with diverse numbers, lengths, and complexity of candidate sequences. In this work, we formally define the decoding-free generative candidate selection task, and conduct the first systematic evaluation"}, {"title": "2 Problem Formulation", "content": ""}, {"title": "2.1 Decoding Paradigm of Generative LMs", "content": "The ordinary sequence-to-sequence formulation of generative LM takes the input sequence seqin = tin 1,..., tiregin and is expected to generate an output sequence se\u011dout = tout, ..., tout. The output sequence generation involves encoding the input sequence to contextual vector representation (i.e., output of the final transformer block), and decoding the outputs following\nseq\u0302out = ffull-decode(fencode(seqin)).\nDuring inference, the decoding function ffull-decode involves seqout| discrete decoding steps, in which each step produces one output token. For (k + 1) - th step of decoding, which is conditioned on both the input sequence and k generated tokens, the model produces logits zk \u2208 R|V| over the vocabulary V after passing output of encoding through unembedding matrix, and then obtain a probability distribution over the possible next token (w e V) in the output sequence P(w|t1:k seqin) = softmax(z). Then a discrete token at this autoregressive decoding step is produced by Equation 2.\nfoutfout"}, {"title": "2.2 Candidate Selection with Answer Pools", "content": "When performing tasks using generative LMs, we include task instruction and query in the input sequence seqin and expect the derived answer of the query ans from the generated output sequence se\u011dout can match the ground-truth answer ans. Some tasks expect open-ended free generation where the final answer is the generated output (ans = se\u011dout), such as translation, creative story generation and dialogue conversation (Wang et al., 2023; Ma et al., 2023a). However, many tasks have an existing answer candidate pool, and the output sequence needs to find a matched candidate as the final prediction. We notate the candidate pool as C and a candidate as c\u2208 {C1, C2, ...,C|C|} where C is the total number of candidate options. Each candidate is a natural language sequence. The answer of the query has to be one of the candidate, i.e. ans e C. For example, answer options are candidates for multiple-choice question answering (Talmor et al., 2018), segments of the input sentences are candidates of information extraction (Sun et al., 2024; Zhao et al., 2024; Ma et al., 2024a), passages in large archives serve as candidates for information retrieval (Lewis et al., 2021), and drugs within medication databases are candidates for prescription tasks (Yu et al., 2024; Ma et al., 2021)."}, {"title": "2.3 Ordinary Approaches for Cand. Selection", "content": "Classification methods train a |C|-way classification head with specialized parameters, where each candidate is treated as a class label. Retrieval methods first create an index with an encoded representation of each candidate. During prediction, the most matched candidates are retrieved, where the match is measured by the similarity between the candidate embedding and the query.\nTo select candidates using a generative approach with full decoding, the LM first generates a free-form output with discrete tokens se\u011dout, then an additional mapping fmap from se\u011dout to candidates is needed to produce the probability over all candidates following Equation 3. This function can be a heuristic rule, semantic similarity matching, or manual processing. Then, the predicted answer ans where a\u00f1s e C is produced by\nans = argmaxcec P(c| tsegin).\nP(c|tsegin) = {Pc1, Pc2, ...,PC}\n= fmap(ffull-decode(fencode(seqin)))"}, {"title": "2.4 Decoding-free Generative Cand. Selection", "content": "At this point, we formally define the task of decoding-free generative candidate selection. In \u00a72.3, we introduce generative candidate selection using decoding, where the answer is reflected by the output sequences. However, there are multiple severe limitations of decoding-based candidate selection. On the one hand, the discrete argmax operator for token selection interrupts the gradient flow, making applying objectives on task outcomes inefficient, such as using reinforcement learning with one-per-outcome sparse rewards instead of token-level feedback. On the other hand, the decoding process is time and resource-consuming, limiting the output bandwidth of generative LMs.\nDecoding-free generative candidate selection fest is a function to produce the candidate prediction probability given seqin without discrete decoding. Given the encoded representation, the function calculates the logits of the first decoding step zo (before the first discrete decoding) and then performs various approaches on top of the logits to estimate the probability of candidate outcomes. The assumption is that the model's intended preference over outcome candidates can be reflected in logits of tokens of the candidate sequences.\nThe estimation method directly produces the probability distribution over all candidate outputs following Equation 7. The predicted answer can be yielded by ans = argmaxcec P (c|tsegin).\nP(c|tsegin) = fest(fencode(seqin))\nCompared with Equation 3, the full decoding process"}, {"title": "3 Generative Candidate Selection Methods", "content": "We formally define candidate selection methods to be analyzed. We introduce ordinary approaches in \u00a73.2. For decoding-free generative candidate selection methods, there are three key design choices: 1) output step used to obtain the logits, i.e. source of the raw logits data; 2) candidate sequence keyword selection, i.e. what tokens represent a candidate option; 3) estimation methods fest that produces probability over candidates given the candidate sequences and the logits at a certain output step. We discuss the first and second factors in \u00a75.2 and \u00a75.3, and define the estimation methods in \u00a73.1. Though some of the estimation designs have been used by existing works, there is no justification or empirical analysis to support their design choices. To the best of our knowledge, this work is the first to provide a formal summary of these approaches and systematically investigate the properties of each design of generative selection methods."}, {"title": "3.1 Estimating Candidates Probabilities from Logits", "content": "Logits of k-th token. Each candidate is represented by a sequence with c\u00b2| tokens, i.e., c\u00b2 = 1,..., c\u00b2. From the logits across all tokens in the vocabulary zo, we calculate the logit for a single token (e.g., the first or the last token) of each candidate sequence and apply softmax to these selected logits to determine the probability pi of predicting a candidate c\u00b2 among all candidates C following Equation 4. We consider two variants in our evaluation: first token and last token estimation.\nAveraged token logits. We average the logits across all tokens for each candidate and apply softmax to these averaged logits across all candidates to compute choice probabilities following Equation 5 where (\u00b7) represents the averaging operator. Sum of token logits. For each choice, we sum the logits across all tokens of the candidate sequence.\nPci = exp(logit(c))/(\u03a3cexp(logit(c)))\nPci = exp((logit(c)))/\u03a3exp({logit(c))))\nPci = exp(\u2211logit(c))/\u03a3j=0 exp(k=0 logit(c))\nWe then apply softmax to these summed logits to determine the probability of selecting each choice following Equation 6."}, {"title": "3.2 Ordinary Candidate Selection Methods", "content": "Full decoding. This method performs full decoding to obtain an output sequence following \u00a72.1, then uses a mapping function to find the corresponding predicted answer from the given candidate pool following \u00a72.3. We use the task-specific mapping function introduced along with each dataset. Typical practices include using regular expressions to match patterns, such as \u201cAnswers: \", and predicting the candidates with the highest semantic similarity with the output sequence.\nDense retrieval. We can also formulate the candidate selection task as a retrieval task and use dense passage retrieval as one of the reference models. Specifically, the question and each candidate choice are embedded into a high-dimensional vector space using these encoders, and cosine similarity between them is computed. This similarity score quantifies the relevance of each choice to the posed question and determines the probability of each choice being the correct answer. For our experimental setup, we use the Facebook DPR question encoder and context encoder (Karpukhin et al., 2020) to generate embeddings of the questions and candidate choices.\""}, {"title": "4 Evaluation Settings", "content": "We apply the introduced candidate selection methods to ultimate downstream tasks to reflect their influence on end tasks. We introduce the selected tasks in \u00a74.1 and base generative LMs in \u00a74.2."}, {"title": "4.1 Testbed Tasks", "content": "We evaluate generative candidate selection methods on two typical types of candidate selection tasks. The first type contains a limited number of answer candidates so that all plausible choices can fit in the input prompt of the model if needed. The second type of task has a massive candidate pool with a large amount of candidates, which cannot fit in the input prompt. We show a comparison between these two settings in Table 4. We cover more details of the tasks in Appendix C.1, task statistics"}, {"title": "4.1.1 Tasks with Limited # of Candidates", "content": "We use five tasks with the provided candidate pools: (1) CommonsenseQA (Talmor et al., 2018), (2) MMLU (Hendrycks et al., 2021b,a), (3) GPQA (Rein et al., 2023), (4) BIG-Bench (Srivastava et al., 2022), and (5) ARC (Clark et al., 2018), covering commonsense questions, science and liberal arts subjects in different education levels, logical reasoning questions, etc. Instances in all datasets contain one correct option and multiple distractors. They vary in difficulty and number of candidates per instance (3 to 5 candidates).\nTo require the model to answer in a specific format without intermediate thinking processes, we add specific instructions in the input prompt, as shown in Appendix C.4. When incorporating the candidate information, we use candidate sequences without indication heads (e.g. A, B) to estimate the selection for decoding-free methods. For the full decoding baseline, candidate sequences with indicators are included in the input for a fair comparison. For the mapping function fmap used by the full decoding, which converts output sequence seqout to candidate selection ans, we capture the first occurrence of a candidate sequence or indication head with regular expressions as the prediction as further elaborated in Appendix C.5."}, {"title": "4.1.2 Tasks with Massive # of Candidates", "content": "We adapt four professional decision-making tasks introduced by Ma et al. (2024b) where the answer has to fall in a large-scale expert-defined coding system as the second category testbeds. The goal is to select multiple candidates from the pool as the predicted clinical decisions. They include: (6) Diagnosis decisions on ICD-10-CM coding system. Given the patient records of a hospital admission and the history diagnoses of the patient, the task aims to produce a set of diagnoses, each has to choose from chapters in the International Classification of Diseases (10th revision) coding system with 94k+ options. (7) Procedure decisions on ICD-10-PCS coding system. The task determines a set of actions to be implemented to intervene in the patient's health status given the patient record at admission time. Candidates for procedures are codes in ICD-10-Procedure Coding System ontology with 85k+ options. (8) Lab orders on LOINC coding system. Given the admission patient record, the task selects a set of lab items from the candidate pool of 3rd-level codes of the Logical Observation Identifiers Names and Codes system. (9) Prescriptions on ATC coding system. The goal is to identify a set of medications, each coded as a pharmacological subgroup in the Anatomical Therapeutic Chemical classification system, to be prescribed to the patient given admission medical record."}, {"title": "4.2 Base Generative LMs", "content": "We assess decoding-free candidate selection approaches while using various pretrained generative language models, including both decoder-only models in the Mistral and LLaMA families, as well as encoder-decoder models in the Flan-T5 family. For LLAMA (AI@Meta, 2024) and Mistral (Jiang et al., 2023) models, we use both models without instruction tuning (LLaMA3 8B and Mistral v0.3 7B) and after instruction tuning (LLaMA3 Instruct 8B and Mistral Instruct v0.3 7B). Among Flan-T5 models, we use the 11B variant (Chung et al., 2022). When preparing the input sequence seqin, we apply the chat template for the models trained with the prompt template, and we append the generation prompt to indicate the start of the answer segment."}, {"title": "5 Experiments Results", "content": "Table 1 and Table 2 show the candidate selection performance on tasks with limited and large-scale candidate pools, respectively. The runtime of each method is shown in Table 3. Given longer candidate sequences, we introduce a new decoding-free candidate selection approach named Sample Avg., which calculates average logits for every other token in candidate sequences. Besides the analysis for output steps, candidate token selection (Figure 3), candidate length and model sizes (Figure 4), we additionally demonstrate that adding chat template for instruction-tuned model hurts the estimation performance in Appendix D.1, the effect of chain-of-thought reasoning in Appendix D.2, additional ablation study on candidate length in Appendix D.3 and performance breakdown in Appendix D.4."}, {"title": "5.1 Characteristics of Generative Candidate Selection Methods", "content": "Insight 1: Estimation methods provide reasonable initial guesses for challenging tasks and decision intuition especially when full decoding is weak. In Table 1 with limited candidates, for more challenging datasets such as GPQA, decoding-free candidate selection approaches (also referred to as \u201cestimation methods\") provide a reasonable initial guess and do not necessarily perform significantly worse than full decoding. Compared to full decoding, estimation methods even provide better performance for CommonsenseQA using LLaMA3 and all MCQA tasks except GPQA using Mistral v0.3. We observe these two models still struggle to handle the format for answering the question for some tasks during decoding, so it is hard to project its knowledge to interpretable results since the only surface to represent knowledge, outputting sequences, is not working for a weak base model. While knowledge by estimation methods is easier to exhibit through token logits.\nFor the results on clinical decisions with massive candidates presented in Table 2, all methods experience a decrease in performance on these more challenging tasks compared to the ones with a limited candidate space. Among decoder-only models, estimation methods can outperform full decoding for lab orders and prescriptions, particularly in non-instruction-tuned variants. Specifically, all estimation approaches surpass Mistral v0.3 in lab test orders, with Sample Avg. achieving the highest increase of 29.25 points compared to full decoding. Additionally, four of the estimation methods outperform LLaMA3 and Mistral v0.3's decoding methods in prescription decision making. The estimation methods provide hints of candidate selections in token logits. It is particularly useful when the full decoding approach of non-instruction-tuned models struggles to follow instructions (as shown in qualitative analysis in Appendix D.5). When the model is able to understand the instruction and produce reasonable output (using instruction-tuned models), full decoding is still better than estimation. This aligns with our observation in Table 1. To summarize, full decoding may impede the accurate selection of candidates, especially for non-instruction-tuned models, whereas decoding-free methods can provide an initial guess in some cases since they are not influenced by trajectory biases.\nInsight 2: Estimation methods lag behind when full decoding performs well. In Table 1, we observe an overall drop in performance when using estimation approaches, especially when the full decoding method achieves reasonable accuracy. This aligns with the intuition that estimation methods"}, {"title": "5.2 Effort of Output Steps", "content": "We investigate the middle ground between complete decoding-free methods and full decoding. We allow the LLM to generate output for a certain number of decoding steps and then use the logits of the next step to perform candidate selection estimation.\nInsight 6: Logits of the first output step is the most informative. The ablation study is shown in Figure 3(a). The estimation performance drops significantly when the output steps increase after the first step. There is only 1 unique token for output steps 1 and 3 across all decoding outputs, as all outputs start from a phrase leading to the answer, i.e. \"Based on the provided information, I would suggest the following diagnoses:...\". Though the uncertainty of the first decoded token is very small, the logit distribution contains the most helpful signals across all output steps. The estimation performance rises after generating the lead phrase starting from the tenth output step.\nUsing the logits of the first output step, without additional subsequent decoding, has been the default setting to estimate the candidate selection in many works. It is also most efficient without additional decoding steps. We empirically show that using the logits of the first output step to estimate the candidate selection is the optimal solution in terms of both performance and efficiency."}, {"title": "5.3 Effect of Selected Candidate Keywords", "content": "We investigate the estimation capabilities when only the logits of the most important keywords of each candidate sequence are considered. We prompt GPT-40 to select a certain number of the most important and informative tokens among all of each candidate sequence. We then only calculate the candidate probability using logits of the selected tokens.\nInsight 7: Using full candidate sequence for estimation is better than selecting essential tokens. In Figure 3(b), we observe that as the considered tokens become more concise and selective (the number of selected candidate tokens becomes fewer), the estimated results of various methods converge to a similar range with a worse recall for most estimation methods. This indicates that it is not necessary to only use essential tokens of the candidate sequence during estimation if it is not First-only logits being used to derive the selection."}, {"title": "5.4 Sensitivity to Model Sizes, Architectures, and Candidate Length", "content": "Insight 8: Estimation performance increases with larger decoder-only models, while staying constant with encoder-decoder ones. Fig"}, {"title": "7 Conclusion and Future Work", "content": "We provide the first formal definition and comprehensive evaluation of decoding-free generative candidate selection methods. We demonstrate that estimation methods can excel in scenarios where base models struggle with answer formats, offering a simpler yet effective alternative to full decoding. Additionally, our findings emphasize the importance of the initial output step logits, revealing that selective token usage can undermine performance and scalability across model sizes. These insights pave the way for more informed designs of candidate selection methods. Future work can build on these findings to refine estimation techniques."}, {"title": "Appendix", "content": ""}, {"title": "A Potential Questions", "content": "Do decoding-free candidate selection methods not involve decoding? Decoding-free methods only use the logits of the first potential output token without producing the token. Calculating logits could be considered an early step in the token decoding process. However, no complete decoding step (as shown in Equation 2) is involved in decoding-free methods (as shown in Equation 7).\nWhy do you need to do decoding-free candidate selection? Compared with producing a response to a query through full decoding (as demonstrated in Equation 3), accurate decoding-free candidate selection methods (as shown in Equation 7) are needed, especially for two scenarios. 1) Accurate outcome-based optimization. To optimize the model with the feedback directly from the predicted outcome ans, we need to know the model's prediction over potential candidates C without interrupting the gradient flow (such as argmax operator). These optimization tasks include preference optimization, which learns to choose the winner option over the loser one (Rafailov et al., 2023); bias mitigation, which obtains detected bias and mitigates the bias level (Ma et al., 2023b); and information extraction, which derives the possibility of extracting different subsequence spans and performing contrastive learning (Ma et al., 2023c).\n2) Efficient answer production. The token dependency of full decoding prevents the decoding mechanism from outputting the answer in parallel. Even though the output sequence se\u011dout is generated, an additional step (fmap) is needed to convert the output sequence se\u011dout to the predicted answer ans (e.g. through sequence matching or semantic similarity). Decoding-free candidate selection produces the probability over all potential answers P (c| tsegin) directly without autoregressive generation and supports parallel inference, significantly improving the time and resources needed for producing the answers to queries.\nWhat are the potential usage and broader impact of the evaluation done in this work? The conclusions and observations derived from our evaluation provide evidence for more informed and confident design choices for both optimizations with outcome-level feedback and efficient answer production without decoding. When researchers and industry practitioners need to define a function to estimate the possibility of potential answers using generative language models without decoding the output sequence, they can: 1) choose the best estimation method corresponding to their model architecture and end tasks according to our evaluation results; 2) understand the empirical tradeoff between efficiency, in terms of runtime, and estimation quality, in terms of performance difference; 3) decide whether they are confident to use estimation method instead of decoding (especially when the estimation methods provide better performance for non-instruction-tuned models). With the wise decision of the candidate selection method, they can obtain better performance after training the model with answer-level rewards, such as through preference alignment, and produce the predicted answers faster with lower resource usage by replacing decoding with estimation.\nWhat are the differences between the two types"}, {"title": "B Advantages and Properties of Decoding-free Generative Candidate Selection", "content": ""}, {"title": "B.1 Advantages", "content": "Decoding-free candidate selection methods are especially beneficial for two scenarios. 1) Accurate outcome-based optimization. To optimize the model with the feedback directly from the predicted outcome ans, we need to know the model's prediction over potential candidates C without interrupting the gradient flow. 2) Efficient answer production. The token dependency of full decoding prevents the decoding mechanism from outputting the answer in parallel. Even though the output sequence se\u011dout is generated, an additional step (fmap) is needed to convert the output sequence se\u011dout to the predicted answer ans. Decoding-free candidate selection produces the probability over all potential answers directly without autoregressive generation and supports parallel inference, significantly improving the time and resources needed for producing the answers to queries."}, {"title": "B.2 Difference Compared with Ordinary Approaches", "content": "The common property between the generative candidate selection and classification is that both settings require a given set of selections to produce the final output. However, candidate selection is different from classification in many key aspects summarized in Table 5, specifically: 1) Support of dynamic candidates. A classification model has to use the same set of output labels across all instances (e.g. positive or negative for sentiment classification). However, candidate selection methods allow the task to have a different set of output candidates for each instance (e.g. different answer options for each question in MCQA). 2) No need of additional parameters. The classification head is an additional set of parameters specialized for the defined output classes. Different classification tasks have to use a separate set of parameters. Candidate selection methods support various candidates with the generative LM's native parameters only, without any additional parameters. 3) No need for specialized"}, {"title": "of tasks used in the evaluation?", "content": "We quantify the effects of various decoding-free estimation methods in downstream scenarios by using two types of evaluation tasks: tasks with limited numbers of candidates (specifically 5 multiple-choice QA tasks) and tasks with massive numbers of candidates (specifically 4 clinical decision tasks). We summarize their core differences in Table 4.\nThe first type (limited candidates) has a limited number of candidates, among which only one option is correct; all candidates' information can be contained in the input prompt, and the candidate pool is unique for each instance. The second type (passive candidates) has a much larger pool of candidates with multiple correct answers (detailed statistics in Table 6). Thus, it is not feasible to feed candidates in the input prompt. The specific tasks we used (the four clinical decision tasks) use the same output candidate pool across instances of the same task. The examined methods should also support dynamic candidate pools across instances for tasks with massive numbers of candidates.\nWhy not use an agent-based system to handle massive candidates? We can provide multiple functions and tools for LLM agents to search, match, or traverse relevant candidates from a large pool of candidates. Compared with full decoding, it will provide more information about the candidate pool and potentially lead to better performance. However, formulating the candidate selection task as an agent is based on and expanded from the idea of decoding discrete tokens to produce answers from output sequences (as described in Equation 3); it does not enjoy the benefits of decoding-free methods, and it is not a comparable setting of the methods we focus on in this paper.\nWhich decoding methods are you using to compare? We use the default decoding setting for each model specified in their generation configuration file. Our work does not aim to propose a new decoding method or compare the performance of various decoding methods. Instead, we emphasize the benefits and limitations of decoding-free candidate selection methods.\nHow is the evaluation performed in this paper different from the evaluations provided in the previous works that use those decoding-free methods? Existing works do not consider how to represent the response candidate from the logits of a single output step as a standalone problem. Thus, they do not provide justification, theoretical proof, or evaluation of the design choice of the decoding-free candidate selection method they used in their works. Our work aims to raise awareness of the importance of this design choice and conduct the first thorough definition of the task and systematic evaluation."}, {"title": "C Details of Experimental Setup and Implementations", "content": ""}, {"title": "C.1 Testbeds", "content": ""}, {"title": "C.1.1 Tasks with Limited Candidates", "content": "The tasks with limited number of candidates include: (1) CommonsenseQA (Talmor et al., 2018) includes questions testing commonsense knowledge across over 2,000 concepts such as highways, housing, and eating, assessing a broad understanding of everyday scenarios. (2) MMLU (Hendrycks et al., 2021b,a) covers a wide range of 57 subjects including mathematics, medicine, computer science, and law, designed to test specialized knowledge in diverse fields. (3) GPQA (Rein et al., 2023) contains challenging questions in biology, physics, and chemistry, written and validated by experts to test deep domain-specific knowledge. (4) BIG-Bench (Srivastava et al., 2022) includes tasks like boolean expression evaluation and causal judgement based on stories, focusing on logical reasoning capabilities. We select the \u201clogical deduction\" category with three objects for our experiments. (5) ARC (Clark et al., 2018) comprises 7,787 multiple-choice questions at grade-school level, divided into a Challenge Set and an Easy Set, to test scientific knowledge. We opt for the Easy Set in our experiments.\nWe report accuracy and per-instance runtime for these tasks. These datasets are split into subsets such as train or test. We select one of the dataset splits with available answer keys for our study. For the mapping function fmap converting output sequence seqout to candidate selection ans, we capture the first the answer candidate sequence or candidate indication head (e.g. A, B, C D) appeared in the output sequence with regular expressions and use the matched candidate as the prediction. All candidate options are added in the input prompt, thus full decoding and decoding-free selection methods use the same amount of available information. We make sure all input sequences for full decoding or decoding-free selection methods are exactly the same."}, {"title": "C.1.2 Tasks with Massive Candidates", "content": "As for the clinic decision datasets, the candidate sequence lengths are generally longer than the first testbed type, as shown in Table 6. Please refer to (Ma et al., 2024b) for more data and experimental setup details. We report recall and per-instance runtime for these four tasks. For the mapping function fmap used by the full decoding approach to select a candidate from the output sequence, we follow the original benchmark setting by selecting the candidate with the highest cosine similarity between sentence embeddings of the candidate definition and the generated output seqout produced by BERT model (Reimers and Gurevych, 2019). The candidates are too many to fit in the input prompt, thus while other methods have access to the candidates information, full decoding method is not aware of candidates.\nDifferent from tasks in \u00a74.1.2, multiple candidates need to be selected for the four tasks of the second type, significantly increasing the difficulty of candidate selection. For full decoding, the model can determine the number of predictions made because the generation of the end-of-sentence token indicates stopping making additional predictions. Decoding-free candidate selection methods rely on candidate probability, and it is hard to determine a fixed threshold for all instances. Thus, we take 20 candidates with the highest probabilities, which contain more predictions than ground-truth answers for most testing instances. We then only report recall, indicating the portion of ground-truth answers"}, {"title": "C.5 Extracting Predicted Answer from the Decoding Output", "content": "Unlike estimation approaches, where the selection is deterministic, exact decoding requires parsing the output to extract the choice from the response, notated by the fmap function in Equation 3. For MCQA tasks, we identify the choice by matching specific substring formats (e.g. 'Answer: A', '(A)', 'A[,.)]'). We treat the first occurring option as the choice made by the LMs, and the rest of the options are considered explanations. For the clinical dataset, we use a sentence transformer to find the most relevant diagnosis codes that appear in the response following the implementation of CliBench. For more details on parsing clinical decision outputs, we refer readers to (Ma et al., 2024b)."}, {"title": "D Additional Experiments", "content": ""}, {"title": "D.1", "content": "Effect of Chat Template to Estimation Performance\nWe compare three estimation approaches\u2014First, Average, and Sum-with and without the use of prompt templates across five multiple-choice ques"}, {"title": "D.2 Effect of Chain-of-Thought Reasoning", "content": "We evaluate the estimation performance using CoT reasoning on the MMLU college_mathematics subset with the Meta-Llama-3-8B-Instruct model under three distinct settings:\n\u2022 No CoT, logits of the first output step: This default setting, as described in the submission paper, does not involve CoT reasoning.\n\u2022 CoT, logits of the first output step: The input prompt includes CoT instructions and in-context examples. The logits from the first output step are used for decoding-free estimation methods.\n\u2022 CoT, logits after thoughts: Similar to the previous setting, but the model generates a step-by-step reasoning process before producing the final answer. The logits from the output step immediately preceding the final answer are used for estimation."}, {"title": "D.3 Performance vs Candidate Length for Other QA Tasks", "content": "In addition to the performance vs candidate length ablation study shown in Figure 4.1(a) for MMLU, we report a similar analysis for other QA tasks. For the MCQ datasets, we sorted the questions according to candidate length and split them into 12 sub"}]}