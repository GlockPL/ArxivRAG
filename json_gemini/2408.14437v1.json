{"title": "Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview", "authors": ["Ilkin Aliyev", "Kama Svoboda", "Tosiron Adegbija", "Jean-Marc Fellous"], "abstract": "Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven nature of biological neural processing, and offer the potential for ultra-low-power artificial intelligence. However, realizing their efficiency benefits requires specialized hardware and a co-design approach that effectively leverages sparsity. We explore the hardware-software co-design of sparse SNNs, examining how sparsity representation, hardware archi-tectures, and training techniques influence hardware efficiency. We analyze the impact of static and dynamic sparsity, discuss the implications of different neuron models and encoding schemes, and investigate the need for adaptability in hardware designs. Our work aims to illuminate the path towards embedded neuro-morphic systems that fully exploit the computational advantages of sparse SNNS.", "sections": [{"title": "I. INTRODUCTION", "content": "Energy-efficient and high-performance computing architec-tures have become more essential than ever in the era of per-vasive machine learning (ML) and artificial intelligence (AI). Spiking Neural Networks (SNNs), which mimic the event-driven communication of biological neurons, hold the promise of surpassing the energy efficiency of conventional Artificial Neural Networks (ANNs) [1]. An important reason for the potential efficiency of SNNs is that they exploit the inherent sparsity observed in biological neural systems, characterized by sparse coding [2], [3] and sparse connectivity [4], [5], and computation using partial synchrony instead of firing rate [6]. This sparsity translates directly into potential computational savings in hardware implementations, especially if the sparsity is explicitly exploited [7], [8].\nIn sparse coding, only a fraction of neurons are activated at a time. As a result, hardware designs that explicitly exploit sparse coding in SNN models can conserve energy by pow-ering down inactive neurons, thus only consuming power for processing active signals on demand. This selective activation aligns well with event-driven processing, where computations are performed only when events (spikes) occur, reducing the overall energy consumption. Moreover, sparse connectivity implies that each neuron is connected to only a subset of other neurons, rather than a fully connected network. This reduces the complexity of the inter-neuronal communication infrastructure required. For an SNN hardware accelerator, this translates to fewer necessary connections and routing paths, which can simplify the accelerator's physical layout and reduce the energy costs associated with data transfer and storage. Furthermore, the high reliability of digital hardware (compared to biological neurons) makes it possible to increase the sparsity of the computations beyond what is observed in the brain to potentially achieve even more energy efficiency.\nHowever, translating this theoretical efficiency into tangible gains on real-world hardware remains a critical challenge. Specialized hardware platforms that explicitly exploit the characteristics of SNN workloads are necessary to reap the full benefits of sparse SNN computations [7], [8]. As such, the co-design of hardware and software holds the key to unlocking the energy-saving potential of SNNs. Algorithms and models must be tailored to work in synergy with hardware architectures optimized to handle the unique computational characteristics of sparse SNNs.\nKey considerations in this co-design process involve how sparsity is represented and how it interacts with the underly-ing hardware. Sparse operations in SNN computations often require different approaches for hardware acceleration than dense operations. Additionally, model configurations, such as the synaptic connectivity patterns, neuron models, encoding schemes, and the balance between different kinds of sparsity, including static and dynamic sparsity, can have profound impacts on hardware efficiency. For example, static sparsity, which refers to a fixed pattern of zero-valued weights in the SNN model, allows for predetermined optimizations like memory compression and skipping of computations with zero weights. On the other hand, dynamic sparsity, referring to the temporal event-based neuron activations, offers potential for further efficiency, but requires flexible hardware to handle variable, irregular, and unpredictable computational loads.\nThis paper provides an overview of the multifaceted field of hardware-software co-design for sparse SNNs, emphasiz-ing the critical role of sparsity in achieving energy-efficient neuromorphic computing. We investigate the dynamic na-ture of sparsity, exploring its dependency on various factors such as training hyperparameters, neuron models, and input encoding methods. Through empirical analysis and detailed exploration, we quantify the impact of these factors on sparsity, offering valuable insights for optimizing SNNs for hardware implementation. We also address the challenges"}, {"title": "II. BACKGROUND ON SPIKING NEURAL NETWORKS", "content": "While both SNNs and ANNs ultimately map input patterns to outputs, their computational models differ significantly. ANNs rely on continuous-valued activation functions, whereas SNNs utilize discrete binary spikes within the temporal do-main to represent information [9]. As depicted in Figure 1, using the integrate-and-fire neuron model [10] as an example, SNN neurons accumulate incoming spikes, integrating their weighted influence over time. A neuron fires an output spike only when its membrane potential surpasses a defined thresh-old. In this section, we present a brief overview of SNNs underpinned by the importance of sparsity as a core feature.\n1) Neuron models: At the heart of an SNN lies the indi-vidual neuron model and its synapses which determine the network's learning dynamics. Simple neuron models like the Leaky Integrate-and-Fire (LIF) [11] mimic the thresholding behavior of neurons, i.e., spikes are generated when their mem-brane potential exceeds a threshold. More complex models, such as the Hodgkin-Huxley [12], do not have an explicit threshold but detail the dynamics of membrane ion channels for greater biological realism and introduce computational overhead. The choice of neuron model profoundly impacts the efficiency obtained from network sparsity, learning dynamics, and the suitability for different hardware implementations.\n2) Spatiotemporal dynamics: SNNs fundamentally differ from traditional ANNs in their approach to information pro-cessing. While both utilize the activation patterns of neurons to encode information, SNNs introduce the precise timing of neuronal spikes as an additional dimension [13]. This timing allows neurons to convey information through single spikes, bursts, or complex temporal patterns. Learning mechanisms like Spike Timing Dependent Plasticity (STDP) [14], which modify synaptic strengths based on the relative timing between pre- and post-synaptic spikes, enable SNNs to learn both spatial and temporal patterns. This unique capability positions SNNs for applications in sequence recognition, temporal pre-diction, and adaptive behavior within dynamic environments. Furthermore, the inherent temporal sparsity of SNNs, where neurons only fire when necessary, contributes significantly to their energy efficiency [15].\n3) Learning in SNNs: Although backpropagation [16] has become a workhorse for effectively training ANNs in prac-tice, training SNNs presents unique challenges due to the non-differentiability of spike-based signals. Several training methods address this, including ANN-to-SNN conversion [17], where a conventional ANN is trained and then converted to an SNN, potentially sacrificing accuracy and efficiency gains. Unsupervised methods utilize STDP [14], but often suffer from slow convergence, high sensitivity to noise and high sensitiv-ity to parameter setting. More recently, supervised learning with surrogate gradients [18] has shown promise by using differentiable surrogate functions during backpropagation-like training, allowing optimization of SNNs for both accuracy and hardware efficiency.\n4) Input encoding: Input encoding in SNNs, the translation of real-world data into spikes, significantly affects informa-tion representation, network sparsity, robustness to noise, and hardware efficiency. Different encoding methods offer distinct trade-offs. For example, rate coding [19] encodes information in the average firing rate over time, offering high performance in deep networks (e.g., VGG9, VGG11) [20], but often at the cost of reduced sparsity due to the high spike rate. Temporal coding [21], conversely, focuses on the precise timing of spikes or patterns of spikes within short time frames [22]. While gen-erally sparser than rate coding, it can sometimes lead to lower accuracy, though methods like time-to-first-spike (TTFS) have achieved high accuracy in certain applications [23], [24]. Delta encoding [25] strikes a balance by using the temporal change of input features to generate spikes, offering a compromise between sparsity and accuracy. Radix encoding [26] aims for ultra-short spike trains, achieving high accuracy with few time steps, but may require specialized hardware. Direct coding [27] bypasses explicit input encoding, allowing the training algorithm to learn the optimal mapping of input data to spiking patterns. The choice of encoding scheme depends on various factors, including input data characteristics, neuron models, and the target application.\n5) Applications: SNNs are well-suited for tasks where tem-poral dynamics and efficient processing are vital (e.g., machine learning implementations on resource-constrained devices). They are particularly well-suited for processing data from event-based sensors (such as neuromorphic vision sensors or dynamic audio sensors) [28], where the sensor output aligns naturally with the sparse, spike-based communication in SNNs. This enables low-power, real-time processing in resource-constrained edge computing systems. SNNs also show promise in embedded pattern recognition tasks [29] where stringent power constraints must be adhered to. Their ability to learn temporal patterns makes them applicable to tasks such as gesture recognition [30], anomaly detection in time-series data [31], or adaptive control systems [32]."}, {"title": "III. NEUROBIOLOGICAL FOUNDATIONS OF SPARSITY", "content": "Neuroscience research reveals that sparsity may be funda-mental to the brain's organization and function, influencing storage [2], energy consumption [33], robustness to noise [34], and processing efficiency [35]. This sparsity manifests in various ways:\n1) Sparse neural coding: The brain employs a sparse dis-tributed coding scheme, where only a small subset of neurons are active in response to specific stimuli or tasks, enhancing energy efficiency and robustness to noise [36], [37].\n2) Structural sparsity: The brain exhibits a high degree of sparse connectivity\u2014i.e., neurons form connections with only a fraction of other neurons [5]. This minimizes metabolic wiring costs and promotes modular and specialized subnet-works for efficient processing.\n3) Sparsity, plasticity, and learning: Sparsity interacts dy-namically with learning mechanisms such as STDP [38], allowing for flexible synaptic modifications and synaptic prun-ing, which refines network representation during development and learning [39].\n4) Computational models of sparsity: Theoretical models suggest that sparsity enhances brain computing power by reducing redundancy and facilitating pattern separation [40], aiding in classification tasks [41].\nUnderstanding the biological basis of sparsity is crucial for developing neuromorphic computing systems that aim to mimic the brain's efficiency and low power consumption. Insights from biological sparsity can inspire the design of algorithms, hardware optimizations, and plasticity mechanisms for more efficient AI in resource-constrained systems."}, {"title": "IV. UNDERSTANDING THE DYNAMICS OF SPARSITY IN PRACTICAL SNNS", "content": "The inherent sparsity of SNNs is key to their energy efficiency. Sparsity is a dynamic property influenced by var-ious factors like the network's training algorithms, neuron models, input encoding methods, and dataset characteristics. This section explores the impact of neuron models, their hyperparameters, and encoding methods on sparsity."}, {"title": "A. Sparsity in the LIF and Lapicque neuron models", "content": "To examine the sparsity characteristics of neuron models, we consider two simple models: Lapicque [42] and leaky integrate-and-fire (LIF) [11]. The Lapicque model, introduced in 1907, represents a neuron as a single point with a membrane potential that evolves in response to incoming inputs (I(t)). If the membrane potential (uj(t)) exceeds a threshold (\u03b8), the neuron fires a spike and resets to its resting value (Urest):\n$\\frac{duj (t)}{dt} =\\begin{cases}\nI(t) & \\text{if } u_j(t) < \\theta \\\\\nU_{rest} & \\text{if } u_j(t) \\geq \\theta\n\\end{cases}$                                  (1)\nDue to its simplicity, sparsity emerges naturally in the Lapicque model. If the input is insufficient to push the membrane potential above the threshold, the neuron remains silent. Sparsity in this model is primarily determined by the distribution of input weights and the chosen threshold value. However, its lack of temporal dynamics limits the complexity of sparsity patterns it can exhibit.\nThe LIF model extends the Lapicque model by introducing a \"leak\u201d term, simulating the gradual decay of the membrane potential towards its resting state. The interplay between input strength, the membrane potential's leak term, and the firing threshold governs the neuron's spiking behavior. The leak's time constant influences how quickly the neuron \"forgets\" previous inputs, impacting sparsity. A shorter time constant leads to a more rapid decay of the membrane potential in the absence of new inputs and effectively increases the amount of input current required to reach the firing threshold, thereby leading to sparser activity. The LIF neuron's characteristics can be expressed as:\n$u_j[t + 1] = \\beta \\cdot u_j[t] + \\sum_{i} W_{ij} \\cdot S_i[t] - S_j[t] \\cdot \\theta$                            (2)\n$S_j[t] =\\begin{cases}\n1, & \\text{if } u_j[t] > 0 \\\\\n0, & \\text{otherwise}\n\\end{cases}$                                     (3)\nwhere \u03b2 (decay factor) controls the membrane potential decay rate, and impacts how the previous potential $u_j[t]$ affects the current potential $u_j[t+1]$. \u03b8 represents the firing threshold to produce a spike $s_j[t]$. A higher \u03b2 and \u03b8 can lead to sparser firing. More complex neuron models can similarly be analyzed based on their configurable parameters."}, {"title": "B. Practical impacts of model hyperparameters on sparsity", "content": "Model hyperparameters can significantly influence SNN sparsity and hardware efficiency. For example, a previous study [43] showed that using the fast sigmoid surrogate gradient function instead of arctan increased sparsity and improved frames per second/watt (FPS/W) by 11%. Fine-tuning neuronal parameters like decay rate and threshold further reduced latency by 48% with minimal accuracy loss. To further examine the LIF and Lapicque neuron models, we performed experiments to explore the impacts of their different hyperparameters on sparsity. These experiments underscore the importance of sparsity-aware hardware-software co-design in the development of SNNs, illustrating the need for carefully balancing the trade-offs between accuracy and sparsity.\nWe used snnTorch [44] to build spiking neuron models and PyTorch to train a convolutional SNN (CSNN) on the Street View House Numbers (SVHN) dataset. We used a VGG-9-based [45] CSNN architecture with the structure: 64C3-P1-112C3-P1-192C3-P1-216C3-P1-480C3-P1-504C3-P1-560C3-P1-1064FC-P1-5000FC-P1-Dropout, where xCy denotes convolutional layers with x filters of size y \u00d7 \u0443. Depending on the neuron model employed, P1 represents either the LIF or Lapicque layer. xFC is a fully connected layer containing x neurons. Training was performed for 200 epochs. Given prior research on its ability to enhance sparsity [43], we used the fast sigmoid surrogate gradient function for training the network. Network parameters were updated via the Adam optimizer with an initial learning rate of 5.0 \u00d7 10-3.\n1) Beta-threshold exploration: We started by performing a detailed exploration of \u03b2 (the leakage factor) and \u03b8 (the firing threshold) for both the LIF and Lapicque neuron models using direct encoding, evaluating accuracy and sparsity. The Lapicque neuron is implemented using RC circuit parameters, with \u03b2 = $e^{\\frac{-T}{RC}}$. R (resistance) defaults to 1 and C (capacitance) is inferred based on the value of \u03b2. The results of these explorations can be seen in Figures 2 and 3.\nIn Figure 2a, we observe that the accuracy using the LIF neuron remains relatively high across various values of \u03b2 and \u03b8, with a maximum accuracy of 95.29% achieved at \u03b2 = 0.15 and \u03b8 = 2. Figure 2b shows the LIF sparsity, measured by the number of spikes. Here, we see that in general, the sparsity increases (fewer spikes) as \u03b2 and \u03b8 increase. The sparsest configuration, with 92,069 spikes, occurs at \u03b2 = 0.9 and \u03b8 = 2. These observations suggest that higher thresholds and decay factors encourage more selective neuronal activity, leading to higher sparsity. Combining insights from both figures illustrates the trade-offs between sparsity and accuracy. While high sparsity, and in effect, hardware efficiency, can be achieved by increasing the threshold and decay factor, it may come at a cost to accuracy. As such, a balance must be found where accuracy remains high without significantly compromising on sparsity. In this case, for example, the sparsest configuration (\u03b2 = 0.9, \u03b8 = 2) might represent a satisfactory balance when hardware efficiency is the priority. This configuration increased sparsity by 13.5% compared to the best accuracy configuration, with only a 0.83% decrease in accuracy, indicating that this configuration is efficient in terms of sparsity while maintaining high performance.\nTo illustrate the diversity in the sparsity of different neuron models and the need for detailed exploration of software and hardware configurations, Figure 3 depicts a similar exploration for the Lapicque neuron. In Figure 3a, we observe a notable decline\u2014more so than for the LIF neuron\u2014in accuracy as \u03b8 increases, particularly at lower \u03b2 values. Interestingly, the accuracy remains relatively robust at higher decay factors (\u03b2 = 0.95), even with an increase in threshold, maintaining a minimum accuracy of 86.99%.\nAs with the LIF neuron, the sparsity for the Lapicque neuron increases as \u03b2 and \u03b8 increase. In this case, the optimal balance for the Lapicque neuron model was found with \u03b8 at 2.0 and \u03b2 at 0.7, achieving 93.23% accuracy and 61,761 spikes. This configuration increased sparsity by 33.0% compared to the best accuracy configuration, with a 1.53% accuracy loss. These results suggest that while the Lapicque neuron might be more sensitive to changes in \u03b2 and \u03b8 than the LIF neuron, the Lapicque neuron might be more efficient regarding sparsity than the LIF model while maintaining comparable accuracy.\n2) Impacts of encoding methods: To investigate how dif-ferent encoding approaches affect accuracy and sparsity, we used the optimal \u03b2 and \u03b8 (highlighted in Figures 2 and 3) for the LIF and Lapicque neurons, respectively, and trained the model using each encoding methods. Table I presents a comparative analysis of the accuracy and sparsity of LIF and Lapicque neuron models across three encoding methods: rate encoding, delta encoding, and direct encoding.\nThe direct encoding method yields the highest accuracy for both models, with LIF achieving 94.46% and Lapicque achiev-ing 93.23%, while also demonstrating a moderate amount of sparsity with 92,069 spikes for LIF and 61,761 spikes for Lapicque. This indicates a good performance in terms of both accuracy and sparsity. In this case, the Lapicque model performs similarly to the LIF model, with slightly lower accuracy and fewer spikes. This makes it an attractive option for scenarios with stricter hardware requirements where minimizing the number of spikes is crucial.\nIn contrast, rate encoding shows moderate accuracy levels (77.99% for LIF and 75.88% for Lapicque) but results in much higher spike counts, particularly for the LIF model with 1,091,195 spikes, indicating lower sparsity. Delta encoding exhibits the lowest accuracy for both models (38.40% for LIF and 40.53% for Lapicque) and maintains the lowest spike counts (79,969 for LIF and 34,246 for Lapicque). Although delta encoding achieved the highest amount of sparsity, this was at the cost of a significant loss of accuracy.\nIn summary, the choice of neuron model and encoding method significantly impacts the accuracy and sparsity of SNNs. Direct encoding consistently outperforms rate and delta encoding in terms of accuracy, with the Lapicque neuron model exhibiting slightly lower accuracy but higher sparsity compared to the LIF model. While rate encoding offers mod-erate accuracy, it suffers from low sparsity. Delta encoding, despite achieving the highest sparsity, is impractical due to its significantly lower accuracy. The optimal choice depends on the specific application requirements, with direct encoding being preferable for high accuracy and the Lapicque model being advantageous for hardware-constrained scenarios prior-itizing sparsity."}, {"title": "V. CHALLENGES OF HARDWARE-SOFTWARE CO-DESIGN OF SNNS", "content": "While the potential benefits of SNNs are substantial, real-izing these advantages in the real world necessitates careful co-design of the algorithms and the specialized hardware that supports them. In this section, we identify several key challenges that must be addressed to enable successful co-design for SNNs.\n1) Mapping algorithms to hardware: The complexity of the chosen neuron model has profound implications for hard-ware design. Complex models (e.g., Hodgkin-Huxley) might demand a large number of computations per timestep [46], straining embedded neuromorphic devices. Implementing on-chip learning rules, especially those beyond simple STDP, adds complexity in terms of memory technologies, update mechanisms, and potential trade-offs between flexibility and power consumption. Input encoding also plays a crucial role on the challenge of mapping SNN algorithms to hardware. For example, rate-based encoding can lead to dense activity, reduc-ing the benefits of hardware-level sparsity support [47], while temporal encoding might necessitate specialized hardware for spike-time processing [48].\n2) Scalability and network architectures: Building large-scale SNNs necessitates the efficient routing of the poten-tially massive number of spike events, requiring specialized routing fabrics or memory-centric architectures that reduce communication overhead [49]. Implementing diverse SNN topologies introduces unique challenges at both the software and hardware levels. For example, deep convolutional SNNS need efficient distribution of convolutional kernels and man-agement of spike-based data [8]. Replicating the connectivity of large-scale brain regions onto resource-constrained neuro-morphic platforms might demand simplifying assumptions or distributed implementation strategies. Furthermore, real-time systems require optimized hardware-software mappings for real-time performance and sparsity [50].\n3) Accuracy vs. efficiency trade-offs: SNN optimizations for efficiency, like reducing the bit precision of weights and activations (i.e., quantization) [51], can significantly increase sparsity. However, such optimizations for efficiency might also carry the risk of severe accuracy degradation. Finding hardware-aware, optimal quantization strategies is important. Similarly, although pruning away weights creates sparsity, different SNN architectures might exhibit varying degrees of sensitivity to pruning. In addition, changes to the SNN architecture can create new trade-off considerations for dif-ferent workloads. For instance, expanding an integrate-and-fire neuron model to a more bio-realistic leaky mechanism might increase the area overhead [47], leading to important workload-specific trade-off considerations regarding the effi-ciency impacts of the more realistic neuron model.\n4) Neuromorphic hardware heterogeneity: Analog neuro-morphic chips [52], [53] might offer superior energy efficiency but can suffer from device mismatch and noise, impacting accuracy. Digital platforms offer flexibility but could demand more complex circuitry to achieve equivalent sparsity benefits.\nThe diversity of hardware also means navigating specialized programming tools and abstractions, potentially creating ven-dor lock-in and hindering the portability of SNN solutions.\n5) Lack of standardized tools and benchmarks: Comparing the performance of SNNs fairly across different algorithms and hardware platforms is hindered by a lack of standardization. Several current benchmarks focus on simple datasets (e.g., MNIST, FashionMNIST) [54], which do not fully capture the strengths of SNNs in handling temporal or spiking sensor data (e.g., neuromorphic vision sensors, audio). While there is a growing body of work focused on developing neuromorphic datasets (e.g., [55]) the development of SNNs would benefit from more comprehensive benchmarks that include tasks like dynamic object recognition, spatiotemporal pattern analysis, and processing event-based sensor data, reflecting real-world scenarios where SNNs could excel. Additionally, software frameworks for SNNs [44] currently lack the maturity of debugging, profiling, and hardware mapping tools available for ANNs, potentially slowing down the research and deployment cycle of SNNs."}, {"title": "VI. SURVEY OF SPARSITY-AWARE HARDWARE ARCHITECTURES FOR SNNS", "content": "Addressing the challenges in hardware-software co-design for sparsity-aware SNNs demands specialized hardware that can seamlessly handle the unique computational demands of these networks. The potential for extreme energy efficiency hinges on architectures explicitly designed to exploit irregular sparsity patterns and event-driven communication. This section surveys some of the recent ad-vances in this field, providing a representative overview of the strategies being explored to unlock the power of sparsity-aware SNN hardware.\nOne notable sparsity-aware implementation is Cerebron, a reconfigurable architecture that effectively handles both spatial and temporal sparsity in SNNs [56]. It utilizes an online channel-wise workload scheduling strategy to maximize data reuse and reduce computation time. This leads to significant reductions in prediction energy and faster processing, high-lighting the importance of exploiting sparsity for neuromor-phic computing.\nLiu et al. [57] introduced the MISS (Memory-based Ir-regular Sparsity Support) framework to tackle irregular spar-sity with a combination of software and hardware optimiza-tions. The framework applies unstructured pruning to synap-tic weights for increased efficiency. The hardware utilizes a sparsity-stationary data flow to optimize memory usage and minimize processing overheads associated with sparsity, improving energy efficiency and speed. The MISS framework achieved an average of 36% improvement in energy efficiency and 23% speedup over baseline SNN accelerators by ex-ploiting irregular sparsity in both input spikes and synaptic weights. Kuang et al. [58] presented an accelerator called ESSA (Efficient Sparse SNN Accelerator) that targets both temporal sparsity (in spike events) and spatial sparsity (in weights) for enhanced SNN inference throughput. Key de-sign features include adaptive spike compression for efficient handling of sparse spike patterns and a flexible fan-in/fan-out trade-off to work within neuromorphic system constraints. Results showed that ESSA achieved a performance equivalent of 253.1 GSOP/s and an energy efficiency of 32.1 GSOP/W for 75% weight sparsity on a Xilinx Kintex Ultrascale FPGA, showing significant improvements in throughput and energy savings compared to other neuromorphic processors.\nUnlike the prior works, which focused on inference, Yin et al. [7] proposed a sparsity-aware accelerator for training called SATA. SATA (Sparsity-Aware Training Accelerator for SNNs) focuses on making SNN training more efficient using backpropagation through time (BPTT). Its systolic-based accelerator architecture exploits various forms of sparsity (in spikes, firing function gradients, and membrane potentials) to improve training energy efficiency. SATA's analysis demon-strates that SNN training can be less energy-intensive than traditional ANN training. The analysis showed that although SNN training consumed approximately 1.27 times more total energy than ANNs when considering sparsity, it improved computational energy efficiency by 5.58 times over non-sparsity exploiting methods.\nAnother sparsity-aware implementation, the MF-DSNN ac-celerator [59], focuses on a temporal coding scheme that removes the need for multiplication, enhancing biological realism. In concert with minimizing weight data access, this design achieves superior performance and energy efficiency, exemplifying the advantage of leveraging temporal sparsity.\nFurther exploring optimizations, an event-driven SNN ac-celerator by Kuang et al. [60] features on-chip sparse weight storage and a self-adaptive spike compression/decompression mechanism, optimizing its handling of input spike sparsity. This enhances both speed and computational efficiency, evi-denced by its high GSOPs/s performance even under elevated weight sparsity. The SpinalFlow architecture [61] tackles SNN efficiency through a novel data flow strategy that processes compressed, time-stamped sequences of input spikes. This substantially reduces storage overheads and computational cost, demonstrating the power of optimized data handling in lowering energy consumption and boosting performance.\nIn the domain of sparsity-aware design space exploration, Aliyev et al. [62] focused on exploring the vast design space of sparsity-aware SNN accelerators. Their work sought configurations that provide peak performance by carefully aligning hardware and SNN model parameters. The proposed hardware leverages SNN sparsity for significant reductions in resource usage and increased speed. This work underscores the value of thorough design space exploration in creating highly efficient SNN accelerators. The proposed sparsity-aware SNN accelerator designs achieved up to 76% reduction in hardware resources and a speed increase of up to 31.25 times, validating the effectiveness of tailoring hardware to specific sparsity-aware configurations for optimal performance.\nCollectively, these advancements showcase the rapid progress in SNN accelerator design. By strategically exploiting different dimensions of sparsity, more efficient, effective, and biologically plausible computing models can be created."}, {"title": "VII. CONCLUSION", "content": "We provided an overview of the hardware-software co-design of sparse SNNs, emphasizing the critical role of spar-sity in achieving energy-efficient neuromorphic computing. Key takeaways include the understanding that sparsity is a dy-namic property, influenced by various factors such as network architecture, training algorithms, neuron models, and input encoding methods. The exploration of different sparsity-aware hardware architectures reveals the potential for significant performance and energy efficiency gains through specialized designs that exploit irregular sparsity patterns and event-driven communications. The insights presented in this paper pave the way for future research in developing neuromorphic systems that fully exploit the computational advantages of sparse SNNs, enabling highly energy-efficient artificial intelligence in resource-constrained systems."}]}