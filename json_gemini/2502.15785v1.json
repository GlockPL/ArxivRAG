{"title": "Masking the Gaps: An Imputation-Free Approach to Time Series Modeling with Missing Data", "authors": ["Abhilash Neog", "Arka Daw", "Sepideh Fatemi Khorasgani", "Anuj Karpatne"], "abstract": "A significant challenge in time-series (TS) modeling is the presence of missing values in real-world TS datasets. Traditional two-stage frameworks, involving imputation followed by modeling, suffer from two key drawbacks: (1) the propagation of imputation errors into subsequent TS modeling, (2) the trade-offs between imputation efficacy and imputation complexity. While one-stage approaches attempt to address these limitations, they often struggle with scalability or fully leveraging partially observed features. To this end, we propose a novel imputation-free approach for handling missing values in time series termed Missing Feature-aware Time Series Modeling (MissTSM) with two main innovations. First, we develop a novel embedding scheme that treats every combination of time-step and feature (or channel) as a distinct token. Second, we introduce a novel Missing Feature-Aware Attention (MFAA) Layer to learn latent representations at every time-step based on partially observed features. We evaluate the effectiveness of MissTSM in handling missing values over multiple benchmark datasets.", "sections": [{"title": "1 Introduction", "content": "Multivariate time-series (TS) modeling is important in a number of real-world applications. However, a persistent challenge is the presence of missing values on arbitrary sets of features at varying time-steps, introducing \u201cgaps\u201d in the data that can impair the application of State-of-the-art (SOTA) models unless specific adaptations are made. A common approach for handling missing data is to use imputation methods [1, 2, 3]. Recent deep learning (DL)-based imputation techniques [4, 5, 6] can learn complex, nonlinear temporal dynamics which are difficult for simple imputation techniques (like interpolation). However all such frameworks rely on a two-stage process, imputation of missing values, followed by feeding the imputed time-series to a TS model. This introduces two critical challenges: first, the propagation of imputation errors into subsequent TS modeling performance, and second, the inherent trade-offs between imputation efficacy and imputation complexity.\nIn this regard, several approaches have been proposed to model time-series with missing values, such as [7] embed time intervals between observations as additional auxiliary features to handle irregular sequences, but relies on recurrent networks which struggle with long-term dependencies. ODE-based methods [8, 9] offer a continuous-time framework for irregular sampling but are computationally demanding and difficult to scale. Recent methods, like [10] implicitly handle missing values via attention mask, or use an additional mask channel ([11]), but in a univariate scenario.\nTo address the above limitations, we ask the question: \u201ccan we circumvent the need for imputation by designing a DL framework that can directly model multivariate TS with missing values?\u201d To answer this question, we draw inspiration from the recent success of masked modeling approaches in domains including vision [12] and language [13] where \u201cmasked-attention\" operations embedded in Transformer blocks are effectively utilized to reconstruct data from partial observations. Based on this insight, we propose a novel Missing Feature-aware Time Series Modeling (MissTSM) Framework, which capitalizes on the information contained in partially observed features to perform"}, {"title": "2 Missing Feature Time-Series Modeling (MissTSM)", "content": "downstream TS modeling tasks without explicitly imputing the missing values. It uses two main innovations. First, we develop a novel embedding scheme, termed Time-Feature Independent (TFI) Embedding, which treats every combination of time-step and feature (or channel) as a distinct token, encoding them into a high-dimensional space. Second, we introduce a novel Missing Feature-Aware Attention (MFAA) Layer to learn latent representations at every time-step based on partially observed features. Additionally, we use the framework of Masked Auto-encoder (MAE) [12] to perform self-supervised learning of latent representations, which can be re-used for downstream tasks such as forecasting and classification. To evaluate the ability of MissTSM to model TS with missing values, we consider two synthetic masking techniques: missing completely at random (MCAR), and periodic masking, to simulate varying scenarios of missing values. We show that MissTSM achieves consistently competitive performance as SOTA models on multiple benchmark datasets without using any imputation techniques."}, {"title": "2.1 Notations and Problem Formulations", "content": "Let us represent a multivariate TS as $X \\in \\mathbb{R}^{T \\times N}$, where $T$ is the number of time-steps, and $N$ is the dimensionality (number of variates) of the TS. We assume a subset of variates (or features) to be missing at some time-steps of $X$, represented in the form of a missing-value mask $M \\in [0, 1]^{T \\times N}$, where $M(t,d)$ represents the value of the mask at $t$-th time-step and $d$-th dimension. Let us denote $X_{(t,:)} \\in \\mathbb{R}^{N}$ as the multiple variates of the TS at a particular time-step $t$, and $X_{(:,d)} \\in \\mathbb{R}^{T}$ as the uni-variate time-series for the variate $d$. In this paper, we consider two downstream tasks for TS modeling: forecasting and classification. For forecasting, the goal is to predict the future $S$ time-steps of $X$ represented as $Y \\in \\mathbb{R}^{S \\times N}$, and, for TS classification, the goal is to predict output labels $Y \\in \\{1, 2, ..., C\\}$ given $X$, where $C$ is the number of classes."}, {"title": "2.2 Learning Embeddings for Time-Series with Missing Features using TFI Embedding", "content": "Prior embedding techniques such as in Transformer or iTransfomer models cannot handle missing values (See Appendix A.1 for more details) directly. To address this challenge, we propose a novel Time-Feature Independent (TFI) Embedding scheme for TS with missing features, where the value at each combination of time-step $t$ and variate $d$ is considered as a single token $X_{(t,d)}$, and is independently mapped to an embedding using $\\text{TFIEmbedding} : \\mathbb{R} \\rightarrow \\mathbb{R}^{D}$ as: $h_{(t,d)} = \\text{TFIEmbedding}(X_{(t,d)})$. In other words, the TFIEmbedding Layer maps $X \\in \\mathbb{R}^{T \\times N}$ into the TFI embedding $H^{\\text{TFI}} \\in \\mathbb{R}^{T \\times N \\times D}$ (see Figure 5(c) in the Appendix A.1). The TFIEmbedding is applied only on tokens $X_{(t,d)}$ that are observed (for missing tokens, i.e., $M_{(t,d)} = 0$, we generate a dummy embedding that gets masked out in the MFAA layer). The advantage of such an approach is that even if a particular value in the TS is missing, other observed values in TS can be embedded \"independently\" without being affected by missing values. Later, we demonstrate how our Missing Feature-Aware Attention Layer takes advantage of TFI embedding scheme to compute masked cross-attention among observed features at a time-step to account for missing features."}, {"title": "2.3 Missing Feature-Aware Attention (MFAA) Layer", "content": "We propose a novel Missing Feature-Aware Attention (MFAA) Layer (see Figure 1) to leverage the power of \"masked-attention\u201d for learning latent representations at every time-step using partially observed features. MFAA works by computing attention scores based on the partially observed features at a time-step $t$, which are then used to perform a weighted sum of observed features to obtain the latent representation $L_t$. These latent representations are later fed into an encoder-decoder based self-supervised learning framework to reconstruct the TS.\nMathematical Formulation: To obtain attention scores from partially-observed features at a time-step, we apply a masked scaled-dot product operation followed by a softmax operation. We first define a learnable query vector $Q \\in \\mathbb{R}^{1 \\times D}$ which is independent of the variates and time-steps. The positionally-encoded embeddings at time-step $t$, $Z_{(t,:)}$, are used as key and value inputs in the MFAA Layer. Specifically, the query, key, and value vectors are obtained using linear projections as, $Q_t = QW_Q$, $K_t = Z_{(t,:)} W_K$, $V_t = Z_{(t,:)} W_V$. Here, $Q \\in \\mathbb{R}^{1 \\times d_k}$ and $K_t, V_t \\in \\mathbb{R}^{N \\times d_k}$, where $d_k$ is the dimension of the vectors after linear projection. The linear projection matrices for"}, {"title": "2.4 Putting Everything Together: Overall Framework of MissTSM", "content": "Figure 1 shows the MissTSM framework. We opted for a masked TS modeling approach (such as Ti-MAE [15]) due to their recent success. MissTSM has two main stages: (1) Self-Supervised Learning Stage: where multivariate TS (with missing values) is reconstructed using an encoder-decoder architecture, with the goal of learning meaningful representations, (2) Fine-tuning Stage: where latent representations learned by encoder are fed into a MLP to perform downstream tasks."}, {"title": "3 Experiments", "content": "Datasets and Baselines: We consider three popular TS forecasting datasets: ETTh2 [16], ETTm2 [16] and Weather [17]. For classification, we use three real-world datasets, namely, Epilepsy, EMG, and Gesture. We follow the evaluation setup proposed in AutoFormer [18] for the forecasting datasets and evaluation setup proposed in TF-C [19] for the classification ones.. For our experiments, we consider five SOTA TS-modeling baselines, SimMTM [20], PatchTST [21], Autoformer [18], iTransformer [22] and DLinear [23]. In order to apply these methods on data with missing values,"}, {"title": "4 Conclusions and Future Work", "content": "We empirically demonstrate the effectiveness of the MissTSM framework across multiple benchmark datasets and synthetic masking strategies. However, a limitation of MFAA layer is that it does not explicitly learn the non-linear temporal dynamics, and relies on subsequent transformer encoder blocks to learn the dynamics. Future work can explore modifications of MFAA layer to address this limitation."}, {"title": "A Additional Details: Methodology", "content": null}, {"title": "A.1 Limitations of Existing Methods", "content": "The first step in time-series modeling using transformer-based architectures is to learn an embedding of the time-series $X$, which is then fed into the transformer encoder. Traditionally, this is done using an Embedding-layer (typically implemented using a multi-layered perceptron) as $\\text{Embedding}: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}^{D}$ that maps $X \\in \\mathbb{R}^{T \\times N}$ to the embedding $H \\in \\mathbb{R}^{T \\times D}$, where $D$ is the embedding dimension. The Embedding layer operates on every time-step independently such that the set of variates observed at time-step $t$, $X_{(t,:)}$, is considered as a single token and mapped to the embedding vector $h_t \\in \\mathbb{R}^{D}$ as $h_t = \\text{Embedding}(X_{(t,:)})$ (see Figure 5(a)). An alternate embedding scheme was recently introduced in the framework of inverted Transformer [22], where the uni-variate time-series for the $d$-th variate, $X_{(:,d)}$, is considered as a single token and mapped to the embedding vector: $h_d = \\text{Embedding}(X_{(:,d)})$ (see Figure 5(b)). While both these embedding schemes have their unique advantages, they are unsuitable to handle time-series with arbitrary sets of missing values at every time-step. In particular, the input tokens to the Embedding layer of Transformer or iTransformer requires all components of $X_{(t,:)}$ or $X_{(:,d)}$ to be observed, respectively. If any of the components in these tokens are missing, we will not be able to compute their embeddings and thus will have to discard either the time-step or the variate, leading to loss of information."}, {"title": "A.2 2D Positional Encodings", "content": "We add Positional Encoding vectors $PE$ to the TFI embedding $H^{\\text{TFI}}$ to obtain positionally-encoded embeddings, $Z = PE + H^{\\text{TFI}}$. Since TFI embeddings treat every time-feature combination as a token, we use a 2D-positional encoding scheme defined as follows:\n\n$PE(t, d, 2i) = sin(\\frac{t}{10000^{(4i/D)}}); PE(t, d, 2i + 1) = cos(\\frac{t}{10000^{(4i/D)}}),$\n\n$PE(t, d, 2j + D/2) = sin(\\frac{d}{10000^{(4j/D)}}); PE(t, d, 2j + 1 + D/2) = cos(\\frac{d}{10000^{(4j/D)}}),$\nwhere $t$ is the time-step, $d$ is the feature, and $i, j \\in [0, D/4)$ are integers."}, {"title": "B Experimental Setup", "content": null}, {"title": "B.1 Dataset Description", "content": "Forecasting Dataset Details\nETT. The ETT [16] dataset captures load and oil temperature data from electricity transformers. ETTh2 includes 17,420 hourly observations, while ETTm2 comprises 69,680 15-minute observations. Both datasets span two years and contain 7 variates each.\nWeather. Weather [17] is a 10-minute frequency time-series dataset recorded throughout the year 2020 and consists of 21 meteorological indicators, like humidity, air temperature, etc.\nFollowing previous works in this area, we use a train-validation-test split of 6:2:2 for the ETT datasets and 7:1:2 for the Weather dataset. We standardized the input features by subtracting off the mean and dividing by the standard deviation for every feature over the training set. Again, following the approach used in previous works, we compute the MSE in the normalized space of all features considering all features together.\nClassification Dataset Details\nEpilepsy. Epilepsy [27] contains univariate brainwaves (single-channel EEG) sampled from 500 subjects (with 11,500 samples in total), with each sample classified as having epilepsy or not (binary classification).\nGesture. Gesture [28] dataset consists of 560 samples, each having 3 variates (corresponding to the accelerometer data) and each sample corresponding to one of the 8 hand gestures (or classes)\nEMG. EMG [29] dataset contains 163 EMG (Electromyography) samples corresponding to 3-classes of muscular diseases.\nWe make use of the following readily available data splits (train, validation, test) for each of the datasets: Epilepsy = 60 (30 samples per each class)/20 (10 samples per each class)/11420 (Train/Val/Test) Gesture = 320/20/120 (Train/Val/Test) EMG = 122/41/41 (Train/Val/Test)"}, {"title": "B.2 Synthetic Masked Data Generation", "content": "Random Masking: We generated masks by randomly selecting data points across all variates and time-steps, assigning them as missing with a likelihood determined by $p$ (masking fraction). The selected data points were then removed, effectively simulating missing values at random. For multiple runs, we created multiple such versions of the synthetic datasets and compared all baseline methods and MissTSM on the same datasets.\nPeriodic Masking: We use a sine curve to generate the masking periodicity with given phase and frequency values for different features. Specifically, the time-dependent periodic probability of seeing missing values is defined as $p(t) = p+a(1-p)sin(2\\pi\\nu t + \\phi)$, where, $\\phi$ and $\\nu$ are randomly chosen across the feature space, a is a scale factor, and p is an offset term. We vary p from low to high values to get different fractions of periodic missing values in the data. To implement this masking strategy, each feature in the dataset was assigned a unique frequency, randomly selected from the range [0.2, 0.8]. This was done to reduce bias and increase randomness in periodicity across the feature space. Additionally, the phase shift was chosen randomly from the range [0, 2$\\pi$]. This was applied to each feature to offset the sinusoidal function over time. Like frequency, the phase value was different for different features. This generated a periodic pattern for the likelihood of missing data."}, {"title": "B.3 Implementation Details", "content": "The experiments have been implemented in PyTorch using NVIDIA TITAN 24 GB GPU. The baselines have been implemented following their official code and configurations. We consider Mean Squared Error (MSE) as the metric for time-series forecasting and F1-score for the classification tasks.\nForecasting experiments. MissTSM was trained with the MSE loss, using the Adam [30] optimizer with a learning rate of le-3 during pre-training for 50 epochs and a learning rate of 1e-4 during finetuning with an early stopping counter of 3 epochs. Batch size was set to 16. All the reported"}, {"title": "B.4 Hyper-parameter Details", "content": "For MissTSM, we start with the same set of hyper-parameters as reported in the SimMTM paper as initialization (see Table 1), and then search for the best learning rate in factors of 10, and encoder/decoder layers in the range [2, 4]. Note that we only perform hyper-parameter tuning on 100% data, and use the same hyper-parameters for all experiments involving the dataset, such as different missing value probabilities. Our goal is to show the generic effectiveness of our MissTSM framework even without any rigorous hyper-parameter optimization. Additionally, we would also like to note that our model sizes are relatively very small (number of parameters for ETTh2=28,080, Weather= 149,824, and ETTm2= 28,952), compared to other baselines such as SimMTM (ETTh2=4,694,186), iTransformer (ETTh2=254,944), and PatchTST (ETTh2=81,728)."}, {"title": "C Additional Results", "content": null}, {"title": "C.1 Embedding of 1D data", "content": "To understand the usefulness of mapping 1D data to multi-dimensional data in TFI embedding, we present (in Table 3) an ablation comparing performances on ETTh2 with and without using high-dimensional projections in TFI Embedding under the no missing value scenario. Projecting 1D scalars independently to higher-dimensional vectors may look wasteful at the time of initialization of TFI Embedding, when the context of time and variates are not incorporated. However, it is during the cross-attention stage (using MFAA layer or later using the Transformer encoder block) that we can"}, {"title": "C.2 Forecasting", "content": "Table 4 compares the forecasting performance of MissTSM with five SOTA baseline methods in terms of the Mean Squared Error (MSE) metric on three datasets (ETTh2, ETTm2 and Weather) with varying forecasting horizons, imputation techniques (Spline and SAITS), and masking schemes. We provide the mean and standard deviations over 5 different samples of the masking schemes. We choose a missing value probability of 60% for MCAR masking and 70% for periodic masking to simulate scenarios with varying (and often extreme) amounts of missing information. We can see that in the no masking experiment, the performance of all methods (with the exception of AutoFormer) are mostly comparable to each other across all three datasets, with MissTSM and PatchTST having a slight edge on the ETTh2/ETTm2 and Weather datasets, respectively. For the MCAR masking experiments, we observe a trend across all the datasets that the MissTSM framework performs slightly better than the baselines for longer-term forecasting (such as forecasting horizon of 720), and comparable to the best-performing baselines on other forecasting horizons. For the Periodic masking experiment, we can see that MissTSM is consistently better than the baselines for ETTh2 dataset, while for the ETTm2 and Weather datasets, the forecasting performance is comparable to the other baselines. These results demonstrate the effectiveness of our proposed MissTSM framework to circumvent the need for explicit imputation of missing values while achieving comparable performance as SOTA.\nBy being imputation-free, MissTSM does not suffer from the propagation of imputation errors (from the imputation scheme) to forecasting errors (from the time-series models). In Appendix Figure 13, we provide empirical evidence of this error propagation, where we see a positive correlation between imputation errors and forecasting errors of baseline methods, indicating that reducing imputation errors is crucial for improving forecasting accuracy. This finding underscores the limitations of traditional two-stage approaches and suggests that using more sophisticated imputation models is necessary to achieve lower forecasting errors. We also report the computation time of SimMTM (with Spline and SAITS) and MissTSM in Appendix Table 5, where we demonstrate that MissTSM is significantly faster as it does not involve any expensive interpolations as an additional advantage."}, {"title": "C.3 Classification", "content": "Full classification results (on all the datasets) are shown in Figure 9\nReal-world results on Physio-Net: We compare the performance of the MissTSM framework with six imputation baselines\u2014 M-RNN [31], GP-VAE [32], BRITS [25], Transformer [26], and SAITS [24]-on the real-world PhysioNet classification dataset [33] that is highly sparse with 80% missing values (see Appendix for additional details), as shown in Figure 10. We follow the same evaluation setup as proposed in [24]. MissTSM achieves an F1-score of 57.84%, representing an approximately 15% improvement over SAITS, the best-performing imputation model, which scored 42.6%. This substantial performance gain on a real-world dataset with missing values highlights the advantages"}, {"title": "C.4 Ablations on Forecasting and Classification task", "content": "In the ablation experiments, our goal is to quantify the effectiveness of the TFI-Embedding scheme and the MFAA Layer on MissTSM. To achieve this, we compare MissTSM with Ti-MAE, which can be viewed as an ablation of MissTSM without the TFI-Embedding and MFAA Layers. We refer to this ablation of MissTSM as MAE. For both the forecasting (see Fig. 11) and classification (see Fig. 12) tasks, we compare the MissTSM framework with MAE trained on spline and SAITS"}, {"title": "C.5 Experiment on Computational cost comparison", "content": "We consider a case study of a classification task on the Epilepsy dataset. Dataset is 80% masked under MCAR. Spline and SAITS are the imputation techniques and SimMTM is the time-series model used. We report the total modeling time as the sum of imputation time and the time-series model training time.\nIn Table 5, we observe that, while SimMTM integrated with SAITS achieves the highest F1 score, the total imputation time for SAITS is significantly higher than that of Spline. This additional computational overhead substantially increases the overall modeling time. Moreover, SAITS has approximately 1.3 million trainable parameters, further increasing the overall model complexity of the time-series modeling task. This highlights the potential trade-off between imputation efficiency and complexity (by imputation complexity we are referring to both model and time complexity).\nIn the case of our proposed method, we do not have the extra overhead of imputation complexity. Simultaneously, MissTSM also achieves competitive performance."}, {"title": "C.6 Imputation error propagation", "content": "Figure 13 captures the propagation of imputation errors and forecasting errors for the weather dataset (at 720 forecasting horizon). It demonstrates that there is an overall positive correlation between the imputation error and forecasting errors, thereby demonstrating propagation of the imputation errors into the downstream time-series models."}, {"title": "C.7 Analysis of impact of frequency and phase parameters", "content": "In the following, we provide additional details regarding an ablation we conducted to understand the impact of frequency and phase parameters. Given the varying frequency and phase for each feature, we modify the intervals of both to assess their impact on the results. Dataset=ETTh2, Fraction=90%\nCase 1. With the phase interval held constant, we lower the frequency range and examine two intervals: one in the high frequency region ([0.6, 0.9]) and one in the low frequency region ([0.1, 0.3]). The performance comparison between these new strategies and the original configuration is shown in Table 6."}]}