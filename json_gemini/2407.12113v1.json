{"title": "A Graph-based Adversarial Imitation Learning Framework for Reliable &\nRealtime Fleet Scheduling in Urban Air Mobility", "authors": ["Prithvi Poddar", "Steve Paul", "Souma Chowdhury"], "abstract": "The advent of Urban Air Mobility (UAM) presents\nthe scope for a transformative shift in the domain of urban\ntransportation. However, its widespread adoption and economic\nviability depends in part on the ability to optimally schedule\nthe fleet of aircraft across vertiports in a UAM network,\nunder uncertainties attributed to airspace congestion, chang-\ning weather conditions, and varying demands. This paper\npresents a comprehensive optimization formulation of the fleet\nscheduling problem, while also identifying the need for alternate\nsolution approaches, since directly solving the resulting integer\nnonlinear programming (INLP) problem is computationally\nprohibitive for daily fleet scheduling. Previous work has shown\nthe effectiveness of using (graph) reinforcement learning (RL)\napproaches to train real-time executable policy models for\nfleet scheduling. However, such policies can often be brittle on\nout-of-distribution scenarios or edge cases. Moreover, training\nperformance also deteriorates as the complexity (e.g., number\nof constraints) of the problem increases. To address these\nissues, this paper presents an imitation learning approach where\nthe RL-based policy exploits expert demonstrations yielded\nby solving the exact optimization using a Genetic Algorithm.\nThe policy model comprises Graph Neural Network (GNN)\nbased encoders that embed the space of vertiports and aircraft,\nTransformer networks to encode demand, passenger fare, and\ntransport cost profiles, and a Multi-head attention (MHA)\nbased decoder. Expert demonstrations are used through the\nGenerative Adversarial Imitation Learning (GAIL) algorithm.\nInterfaced with a UAM simulation environment involving 8\nvertiports and 40 aircrafts, in terms of the daily profits\nearned reward, the new imitative approach achieves better\nmean performance and remarkable improvement in the case\nof unseen worst-case scenarios, compared to pure RL results.", "sections": [{"title": "I. INTRODUCTION", "content": "In the modern landscape of urban transportation, the\nemergence of Urban Air Mobility (UAM) introduces a revo-\nlutionary dimension to the concept of commuting. As cities\ngrapple with increasing populations and traffic congestion,\nthe concept of UAM proposes the use of electric vertical\ntake-off and landing (eVTOL) aircraft [1] as an alternate\nform of automated air transportation. With a projected market\nsize of $1.5 trillion by 2040 [2], its economic viability will be\ndriven by the ability to operate a sufficiently large number of\neVTOLs in any given market (high-penetration), placing the\ndomain of UAM fleet scheduling at the forefront of innova-\ntions. Following the air-space and aircraft safety constraints\nwhile being robust against dynamic environmental changes,\nmitigating energy footprint, and maximizing profitability\nmakes developing an optimal scheduling policy challenging.\nThe complexities inherent in UAM fleet scheduling are\ncharacterized by:\nDynamic environments where real-time factors such as\nairspace congestion, weather conditions, demand uncertainty\ncharacterized by dynamic traffic patterns, changing weather\nconditions, and regulatory constraints demand intelligent\nscheduling solutions that can swiftly adapt to real-time\nchallenges [1].\nState information sharing among all vertiports and eV-\nTOLs is necessary for trajectory and speed adjustments to\nensure the safety of the operations [3].\nScarce resources such as vertiport parking lots, charging\nstations, and air corridors must be optimally allocated to\nprevent unnecessary delays and conflicts [4].\nContemporary solutions to such scheduling problems take\nthe form of complex nonlinear Combinatorial Optimiza-\ntion (CO) problems [5], which can be addressed through\nclassical optimization, heuristic search, and learning-based\napproaches. While these approaches provide local optimal\nsolutions for small UAM fleets [6]\u2013[8], they often present\ncomputational complexities that render them impractical\nfor online decision-making and larger fleets. Furthermore,\ndespite the computational expenses, these methods would\nstill be viable options if we disregard uncertainties in the\neVTOL's operations (e.g. occasional failures of the air-\ncraft) and environmental constraints (like the closure of air\ncorridors due to bad weather conditions). However, these\nfactors cannot be disregarded because of the safety concerns\nthey impose, making it especially challenging to use such\napproaches.\nIn the pursuit of addressing these complexities and pre-\nsenting an efficient online scheduler, we extend our previous\nwork in [9] by proposing a specialized Graph Neural Net-\nworks (GNN) [10], [11] based adversarial imitation learning\nframework that learns from the optimal schedules (expert\ndata) generated by classical optimization algorithms and\nbuilds upon existing multi-agent task allocation methods for\nscaling up to larger UAM fleets. We begin by posing the\nUAM fleet scheduling problem as an Integer Non-Linear Pro-\ngramming (INLP) problem with a small number of eVTOLS\nand vertiports and generate optimal solutions using an elitist\nGenetic Algorithm. These form the expert demonstrations"}, {"title": "II. RELATED WORKS", "content": "The existing body of work in Urban Air Mobility (UAM)\nfleet planning has witnessed notable growth, propelling the\noptimization and learning formalism to advance this emerg-\ning concept. However, a considerable portion of this literature\nneglects crucial guidelines outlined by the FAA [1] concern-\ning UAM airspace integration, particularly in aspects such\nas air corridors, range/battery constraints, and unforeseen\nevents like route closures due to adverse weather or eVTOL\nmalfunctions [7], [13], [14]. Traditional methods, includ-\ning Integer Linear Programming (ILP) and metaheuristics,\nprove inadequate for efficiently solving related NP-hard fleet\nscheduling problems [15]\u2013[20]. For context, we focus on\nhour-ahead planning to allow flexibility in adapting to fluc-\ntuating demand and route conditions affected by weather and\nunforeseen aircraft downtime. While learning-based methods\nhave shown promise in generating policies for combinatorial\noptimization problems with relatable characteristics [11],\n[21]\u2013[27], their current forms often oversimplify complex-\nities or tackle less intricate problem scenarios. In response,\nour paper introduces a centralized learning-based approach\ntailored to address the complexities and safety guidelines\ninherent in UAM fleet scheduling. Our proposed approach\ninvolves the creation of a simulation environment that encap-\nsulates these complexities, facilitating the training of a policy\nnetwork for generating hour-ahead sequential actions for\neVTOLs across a generic 12-hour operational day. The policy"}, {"title": "III. PROBLEM DESCRIPTION AND FORMULATION", "content": "A. UAM Fleet Scheduling as an Integer Non-Linear Pro-\ngramming Problem\nWe begin by posing the UAM fleet scheduling problem\nas an optimization problem [9], enabling us to use a Ge-\nnetic Algorithm to generate the expert solution. Consider a\nUAM network comprising N vertiports and $N_K$ number of\neVTOLs, each with a maximum passenger carrying capacity\nof C(= 4). Let V and K denote the set of all vertiports\nand eVTOLs, respectively, with each vertiport $i \\in V$ capable\nof accommodating a maximum of $C_{mark}$ (= 10) number of\neVTOLs while also having charging stations for $C_{charge}$ (= 6)\neVTOLS. Some vertiports might not have charging stations\nand are called vertistops ($V_s \\subset V$). We consider there to be\n4 air corridors between two vertiports with two corridors\nfor each direction. Each vertiport $i \\in V$ has an expected\ntake-off delay $T_{TOD}$ that affects every eVTOL taking off,\nand we consider this estimated take-off delay to be less\nthan 6 minutes at each vertiport. The probability of route\nclosure between any two vertiports i and j is considered\nto be $P_{closure}$ (<0.05) while the probability of an eVTOL k\nbecoming dysfunctional is considered to be $P_{fail}$ (\u22640.005).\nEvery episode considers a randomly assigned take-off delay\n(< 30 mins) that is drawn from a Gaussian distribution with\nmean $T_{TOD}$ and a standard deviation of 6 minutes. Additional\nassumptions that are made regarding the problem setup are:\n1) The decision-making is done by a central agent that has\naccess to the full observation of the states of the eVTOLS\nand the vertiports, and 2) An eVTOL can travel between any\ntwo vertiports given it has sufficient battery charge and the\nresistive loss of batteries is negligible.\nWe define $R_{i,j}$ as the operation cost of transporting a\npassenger from vertiport i to j while $F_{passenger}^t$ is the price\na passenger is charged for traveling from i to jat time\nt, that is based on passenger demand. We follow the de-\nmand model as mentioned in our previous work [9]. Let\n$Q(i,j,t)$ represent the demand between vertiports i and\nj at time t. A subset of vertiports $V_B \\subset V$ are consid-\nered to be high-demand vertiports and we account for two\npeak hours of operation: 8:00 \u2013 9 : 00 am ($T_{peak1}$) and\n4:00-5:00 pm ($T_{peak2}$) such that $V_B$ experience high\ndemands during both peak hours. The demand is high from\n$V - V_B$ to $V_B$ during $T_{peak1}$ and vice-versa during $T_{peak2}$.\nThe passenger fare is computed by adding a variable fare\n$F_{passenger}^t$ to a fixed base fare of $F_{base}$ = $5. The variable\nfare between two vertiports i and j is dependent on the\npassenger demand $Q(i,j,t)$ and the operational cost $R_{i,j}$\nand is computed as $F_{passenger}^t = R_{i,j} \\times Q_{factor}(i, j,t)$, where\n$Q_{factor}(i, j,t) = max(log(Q(i, j,t)/10), 1)$. A constant elec-\ntricity pricing, $Price_{elec}$ = $0.2/kWh is used [31].\nThe eVTOL vehicle model is considered to be the City\nAirbus eVTOL [32] with a battery model similar to the one\ndescribed in [8]. The aircraft has a maximum cruising speed\nof 74.5 mph and a passenger seating capacity of 4. The\noperating cost of this vehicle is $0.64 per mile [8]. It has\na maximum battery capacity of $B_{max}$ = 110 kWh. Let $B_k^t$\nrepresent the charge left in eVTOL k at time t and if the\neVTOL travels from vertiport i to j, then the charge at the\nnext time step is $B_k^{t+1} = B_k^t - B_{i,j}^{charge}$, where $B_{i,j}^{charge}$ is the\ncharge required to travel from i to j.\nWe consider an operating time horizon of T hours (with\nstart time $T_{start}$ and end time $T_{end}$) and the aim of the UAM\nscheduling problem is to maximize the profits earned in the\nT hours. This is achieved by smartly assigning (based on\nthe demand, battery charge, operation cost, etc.) a journey\nto an eVTOL during each decision-making instance t \u2208 T. A\njourney is defined as the commute of an eVTOl between\ntwo vertiports i and j. If i = j, the eVTOL has to wait\nfor $T_{wait}$ (= 15) minutes at the vertiport before assigning a\njourney in the next decision-making instance. We assume that\neach eVTOL can take off at any time within the T horizon,\nand we consider the schedule for a single-day operation\nwhere the operational time begins at 6:00 AM ($T_{start}$) and\nends at 6:00 PM ($T_{end}$).", "Optimization formulation": "Based on the notations de-\nscribed above, the optimization problem is formulated as\nfollows (similar to our previous work [9]). For every eVTOL\nk\u2208 Ve, let $S_{jour}^k$ be the set of journeys taken during time\nperiod of T and, $N_{passengers}^{k,l}$ be the number of passengers\ntransported during the trip, $l\\in S_{jour}^k$ by eVTOL k. Let $B_k^l$ be\nthe battery charge of eVTOL k just before its l'th journey,\n$V_{start}^k$ and $V_{end}^k$ be the respective start and end vertiports\nof eVTOL k during it l'th journey, $T_{takeoff}^k$ and $T_{landing}^k$ be\nthe corresponding takeoff time and landing time. Then the\nobjective of the optimization problem is to maximize the net\nprofit z that can be computed by subtracting the operational\ncosts $C^O$ and the cost of electricity $C^E$ from the revenue\ngenerated. These values are computed as:\n$C^O = \\sum_{k \\in K} \\sum_{l \\in S_{jour}^k} N_{passengers}^{k,l} R_{i,j}, i = V_{start}^k, j = V_{end}^k$  (1)\n$C^E = \\sum_{k \\in K} \\sum_{l \\in S_{jour}^k} Price_{elec} \\times B_{i,j}^{charge}, i = V_{start}^k, j = V_{end}^k$ (2)\n$Revenue = \\sum_{k \\in K} \\sum_{l \\in S_{jour}^k} N_{passengers}^{k,l} \\times F_{passenger}^{t}, i = V_{start}^k, j = V_{end}^k$ (3)\nTherefore, the objective function can be formulated as:\nmax z = Revenue - $C^O - C^E$ (4)"}, {"title": "With the following constraints:", "content": "$N_{passengers}^{k,l} = min(C,Q_{act}(i, j,t))$ (5)\n$V_{start}^k, j = V_{end}^k,t= T_{takeoff}^k$ (6)\n$B_k^{takeoff} > B_{i,j}^{charge},i=V_{start}^k, j = V_{end}^k, k \\in K, \\forall l \\in S_{jour}^k$ (7)\n$\\sum_{k \\in K} \\delta^{k,t} \\leq C_{park}, \\forall i \\in V, t \\in [T_{start}, T_{end}]$\n$V_{end}^k \\neq i, i_{Ad} = 0, \\forall i \\in V, \\forall k \\in K, \\forall l \\in S_{jour}^k$ (8)\nWhere $Q_{act}(i, j,t)$ is the actual demand, and A is a matrix\nthat represents the availability of routes between vertiports.\n$A_{ij} = A_{ji} = 1$ if the route between i and j is open and 0\nif the route is closed."}, {"title": "B. Generating the Expert Demonstrations", "content": "Given the INLP problem formulation in Sec.III-A, we use\nan elitist Genetic Algorithm (GA) to generate the expert\nsolutions for a small number of eVTOLS ($N_k$ = 40) and\nvertiports (N = 8). The GA uses a population size of 100,\nmax iterations of 100, mutation probability of 0.1, elite\nratio of 0.01, and cross-over probability of 0.5. The GA is\nimplemented in batches of 60 decision variables which are\nthen simulated sequentially and the objective functions are\ncomputed. Upon simulating a batch of the GA solution, we\nachieve an updated state of the environment which is then\nused to compute the next 60 decision variables. This process\nis repeated until the episode is over. Each decision variable\ntakes an integer value between 1 and N. We generate the GA\nsolutions for 100 different scenarios (for each scenario, the\nlocations of the vertiports stay fixed) which are then used as\nexpert demonstrations for the imitation learning algorithm."}, {"title": "C. MDP Formulation", "content": "Having expressed the UAM fleet scheduling problem as\nan INLP problem, we now define it as a Markov Decision\nProcess (MDP) that sequentially computes the action for each\neVTOL at the decision-making time instant t \u2208 T. The state,\naction, reward, and the transitions are described below:\nState Space: The state-space at time t consists of infor-\nmation about 1) the vertiports, represented as a graph $G_v$,\n2) the eVTOLs, represented as a graph $G_e$, 3) the passenger\ndemand model Q, 4) passenger fare $F_{passenger}$, 5) operational\ncosts R (computed based on the per mile operational cost\nof the eVTOL, the passenger demands and the price for\nelectricity), and 6) time when it is safe to launch an eVTOL\nto the corridors $T_{cor} \\in R^{N \\times N \\times 2}$.\nAt any decision-making time step, the state information is\nprocessed by a transformer-aided multihead-attention graph\ncapsule convolutional neural network (that was presented\nin [9]) that acts as the policy network for the imitation\nlearning algorithm. Details about the policy network have\nbeen discussed in IV-A. Further details on the state space\nare as follows:\nGraph Representation of the Vertiport Network: $G_v$ =\n(V, Ev, Av) represents the vertiport network as a graph where\n$V(V)$ is the set of vertiports, Ev represents the set\nof edges/routes between the vertiports, and $A_v$ represents\nthe adjacency matrix of the graph. To consider the route\nclosure probability, a weighted adjacency matrix is com-\nputed as $A_v = (1_{N \\times N} - p_{closure}) \\times A$. The node properties\n8f of the vertiport i \u2208 V, at time t are defined as 8 = [Xi, Yi, park charge TTOD, \u0130stop], where (xi,yi) are the x-y\ncoordinates of the vertiport, $C_{park}^i$ is the number of eVTOLs\ncurrently parked, $T_{charge}^i$ is the earliest time when a charging\nstation will be free, $T_{TOD}^i$ is the expected take-off delay, and\n$\u0130_{stop}^i$ is a variable that takes the value 1 if the node is a\nvertistop and 0 if the node is a vertiport.\nGraph Representation of the eVTOLs: $G_e$ = (Ve, Ee, Ae)\nrepresents the graph that encodes the information about\nthe eVTOL network. Ve represents the set of eVTOLS, Ee\nrepresents the set of edges, and Ae represents the adjacency\nmatrix. Ge is considered to be fully connected and each\neVTOL k \u2208 Ve is represented by it properties i.e. y =\n[xk,yk, Bk, Tflight, 7dec, fail], v \u2208 R6. Here, x,y) represent\nthe coordinates of the destination vertiport, B is the current\nbattery level, Tflight is the next flight time, 7dec is the next\ndecision making time, and Pfail is the probability of failure.\nAction Space: At each decision-making time instance,\neach agent takes an action from the available action space.\nThe action space consists of all the available vertiports.\nTherefore the action space will be of size NK. During a\ndecision-making step, if an agent chooses the vertiport at\nwhich it currently is, then it waits for 15 minutes in the\nvertiport, until it makes a new decision.\nReward: We consider a delayed reward function where the\nagent gets a reward only at the end of an episode. The reward\nis ratio of the profit earned to the maximum possible profit\nin an episode, i.e. $\\sum_{i \\in v, j \\in v, t \\in [T_{start},T_{end}](Q(i, j,t) \\times F_{passenger})$.\nTransition Dynamics: Since demand and electricity pric-\ning can vary from that of the forecasted values, the transition\nof the states is considered to be stochastic. The transition is\nan event-based trigger. An event is defined as the condition\nthat an eVTOL is ready for takeoff. As environmental uncer-\ntainties and communication issues (thus partial observation)\nare not considered in this paper, only deterministic state\ntransitions are allowed."}, {"title": "IV. PROPOSED GRAPH-BASED ADVERSARIAL IMITATION\nLEARNING APPROACH", "content": "We propose an adversarial imitation learning formulation\nto train a transformer-aided GNN-based policy network\ncalled CapTAIN [9] that will assign actions to the eVTOLs.\nThe policy network takes the state information from all the\neVTOLs and vertiports as input and assigns an action to the\neVTOLs that are ready for take-off. The policy is trained\nusing the generative adversarial imitation learning (GAIL)\n[12] algorithm that uses the solutions generated by the GA\nas the expert demonstrations. The following section presents\nfurther information about the GAIL algorithm and the state\nencoding and decoding."}, {"title": "A. Policy Network", "content": "We use the Capsule Transformer Attention-mechanism\nIntegrated Network (CapTAIN) [9] as the policy network\nfor GAIL. CapTAIN combines graph neural networks and\ntransformers to encode the state-space information and used\na multi-head attention-based decoder to generate the action.\nThe information from the veriports and eVTOLS ($G_v$ and\n$G_e$) are first passed through a Graph Capsule Convolutional\nNetwork (GCAPCN), introduced in [28], which takes the\ngraphs as the inputs and generates the feature embeddings\nfor the vertiports and eVTOLs. Simultaneously, a transformer\narchitecture is used to compute learnable feature vectors\nfor the passenger demand model and the passenger fares\n(information that can be represented as time-series data).\nAdditionally, a simple feed forward network computes the\nembeddings for the passenger transportation cost R which\ncan be represented as a N\u00d7N matrix, and another feed\nforward network processes the corridor availability. We keep\na track of the time at which it is safe for a new eVTOL\nto enter a corridor, subject to various safety restrictions\nand minimum separation, using a $N \\times N \\times 2$ tensor, Tcor,\nwhich is flattened and fed into the feed forward network.\nThe outputs from the transformer, GCAPCN, and the feed\nforward networks form the encoded embedding which is\nthen passed through a multi-head attention [30] decoder\nto generate the probabilities of choosing each action, for\nthe decision-making agent. Further technical details about\nCapTAIN have been discussed in [9]."}, {"title": "B. Training with Generative Adversarial Imitation Learning", "content": "The policy network \u03c0 is trained on the expert demonstra-\ntions generated by the Genetic Algorithm, using Generative\nAdversarial Imitation Learning (GAIL), an imitation learning\napproach introduced in [12]. The policy is learnt using a two-\nplayer zero-sum game which can be defined as:\n$\\underset{D}{\\text{argmin }} \\underset{DE(0,1)}{\\text{argmax }} E_{\\pi} [log D(s,a)] +$\n$E_{\\pi e} [log (1 \u2013 D(s,a))] \u2013 \u03bbH(\\pi)$ (9)\nWhere Te is the policy followed by the expert demonstration.\nD is a discriminator that solves a binary classification prob-\nlem D : S \u00d7 A \u2192 (0,1), where I represents the state-space\nand A represents the action-space. D tries to distinguish\nbetween the distribution of data generated by from the\nexpert data generated by ME, When D cannot distinguish data\ngenerated by the policy \u03c0 from the true data, then \u03c0 has suc-\ncessfully matched the true data. H(\u03bb) = $E_{\\pi} [-log \\pi (as)]$ is\nthe entropy. The discriminator (approximated by a neural network)\ntries to maximize the objective, while the policy tries to\nminimize it. GAIL alternates between an Adam gradient [6]\nstep on the parameters w of Dw with the gradient:\n$\u00cat; [\\bigtriangledown log D_w(s,a)] + \u00ca\\pi [\\bigtriangledown log 1 \u2013 D_w(s,a)]$ (10)\nand a Trust Region Policy Optimization (TRPO) [7] gradient\nstep on the parameters \u03b8 of theta which minimizes a cost\nfunction c(s, a) = log$D_{wi+1}(s,a)$ with the gradient:"}, {"title": "\u0395\u03c4\u2081 [Velog \u03c0\u03bf (as)Q(s,a)] \u2013 \u03bb\u2207\u0473H(\u03c0\u03bf)", "content": "algorithm or GA (that is impractical to be used directly/online\ndue to its high computing cost). While prior work had\nshown that graph RL is uniquely capable of learning policies\nwith performance clearly better than standard RL or other\nonline approaches, there is still a significant optimality gap\nwhen compared with partly-converged results from a global\noptimizer such as GA. The imitation learning extension\npresented here (CapTAIN-GAIL) was found to successfully\nreduce this optimality gap, with the initial policy seeded\nby the earlier CapTAIN results. Our case studies involved\nscheduling the flights of 40 aircraft across 8 vertiports. When\ntested over 100 seen scenarios, CapTAIN-GAIL generated\nsolutions with performance that is statistically similar to the\nsolutions generated by the GA. Both across seen (during\ntraining) and unseen scenarios the new CapTAIN-GAIL\nresults (in terms of the profit metric) are found to be sig-\nnificantly better than those by CapTAIN, thereby supporting\nour hypothesis of taking an imitation learning approach\nV. EXPERIMENTS AND RESULTS\nA. Simulation Details\nWe use the simulation environment presented in [9] that\nis implemented in Python and uses the Open AI Gym\nenvironment interface. We consider a hypothetical city cov-\nering an area of 50 \u00d7 50 sq. miles with 8 vertiports (2 of\nwhich are vertistops) and 40 eVTOLs. The locations of the\nvertiports remain the same throughout training and testing the\nalgorithm. The rest of the operational details stay the same,\nas discussed above. To train the CapTAIN policy network,\nwe use the GAIL implementation from imitation [33], a\npython package for imitation learning algorithms, and PPO\n[34] from stable-baselines3 [35]."}, {"title": "B. Training Details", "content": "We begin by training the CapTAIN policy using just\nPPO, for 1.5 million steps. Next, we generate the expert\ndemonstrations for 100 scenarios (un-seen by CapTAIN\nduring training) using a standard elitist Genetic Algorithm\n(GA) (using the parameters as described in Sec.III-B) and\ncompare the performance of CapTAIN against GA. These\ntest scenarios are generated by fixing the seed values for\nthe random number generators in the simulation. Finally,\nwe train the imitation learning policy (CapTAIN-GAIL)\non the expert demonstration, with a soft start by beginning\nthe training with the pre-trained weights from CapTAIN,\nand finally compare the performance of CapTAIN-GAIL\nagainst CapTAIN and GA. All the training, experiments, and\nevaluations are computed on a workstation running Ubuntu\n22.04.4 LTS and equipped with an Intel Core i9-12900K\nprocessor and Nvidia GeForce RTX 3080 Ti graphics pro-\ncessor. Further evaluation details and results are discussed in\nthe sections ahead."}, {"title": "C. Comparing CapTAIN vs. Genetic Algorithm", "content": "We begin by testing the performance of a policy trained\npurely using reinforcement learning (i.e. CapTAIN trained\nusing PPO) against GA. The daily profit earned by both the\nalgorithms in each test scenario, is used to compare their\nperformance and Fig.3 plots the difference in the profits\nearned by GA and CapTAIN in each of the 100 test scenarios.\nIt can be seen that GA generates more profits in the\nmajority of scenarios (GA performs better in 66 out of 100\ncases), with GA generating an average profit of $17603\nwhile CapTAIN generating an average profit of $15727.\nTo test the significance of this difference, we perform a\nstatistical T-test with the null-hypothesis being that both\nmethods generate the same amount of profit. The p-value\nof the test turns out to be 3.08 \u00d7 10-5(<0.05) which\nmeans that GA has a significant statistical advantage over\nCapTAIN. This motivates the use of imitation learning for\ntraining a policy that can mimic GA while being significantly\nmore efficient than GA in terms of computational time."}, {"title": "D. Evaluating CapTAIN-GAIL", "content": "We train the imitation learning policy with a soft start,\ni.e. we use the pre-trained CapTAIN policy from Sec.V-C as\nthe initial policy for GAIL. The generator policy in GAIL is\ntrained for 20 iterations with 20000 steps in each iteration.\nThe trained imitation learning policy is then tested on the 100\ntest cases and its performance is compared against CapTAIN\nand GA. We further test the generalizability of CapTAIN-\nGAIL on a new set of 100 unseen test cases that were not a\npart of the expert demonstrations.\n1) Average Profits: Fig.4 plots the average profits earned\nby all three methods, in the test scenarios that were a\npart of the expert demonstrations. It can be noticed that\nCapTAIN generates the least profit while CapTAIN-GAIL\nand GA almost earn equal profits on average. A more detailed\nanalysis of the performance of the three methods is discussed\nbelow.\n2) CapTAIN-GAIL vs. CapTAIN: Fig.5 compares the dif-\nference in the profits earned by CapTAIN-GAIL and Cap-\nTAIN. CapTAIN-GAIL generates more profits than CapTAIN\nin 73 out of 100 cases, with the average profit of CapTAIN-\nGAIL being $17587 while the average profit of CapTAIN\nis $15727. To statistically evaluate the significance of this\ndifference, we conduct a T-test with the null-hypothesis being\nthat both the algorithms generate equal average profits. With\na p-value of 6.55 \u00d7 10-5(<0.05), we have statistical\nevidence that CapTAIN-GAIL performs significantly better\nthan CapTAIN."}, {"title": "3) CapTAIN-GAIL vs. GA:", "content": "Next, we evaluate the perfor-\nmance of CapTAIN-GAIL against GA. With Fig.6 plotting\nthe difference in the profits earned by these two methods, we\ncan see that CapTAIN-GAIL performs better than GA in 55\nout of the 100 cases, which is roughly half the number of test\ncases. The average profit earned by GA is $17603 while\nthe average profit earned by CapTAIN-GAIL is $17587.\nUpon performing a T-test with the null hypothesis being that\nboth methods earn the same average profit, we get a p-value\nof 0.97(> 0.05) which indicates that statistically, both\nmethods have a similar performance. This result matches\nour expectations since the imitation learning policy should at\nleast perform similar to the expert when tested on the expert\ndemonstrations.\nEven though both CapTAIN-GAIL and GA perform sim-\nilarly in terms of the average profits they generate, it shall\nbe noticed in Fig.6 that the difference in the profit values\nare quite large. This indicates that in cases where GA\nperforms better, it outperforms CapTAIN-GAIL by a large\nmargin while the opposite is also true, i.e. CapTAIN-GAIL\noutperforms GA by a large margin, in the cases where it\nperforms better. One would ideally assume the difference\nin the profits to be close to 0, given that both the methods\ngenerate similar amount for profits, and this discrepancy in\nthe performance of these two algorithms shall remain the\nsubject of future research."}, {"title": "4) Generalizability Analysis:", "content": "To test the generalizability\nof CapTAIN-GAIL, we test it on a new set of 100 unseen\nscenarios that were not a part of the expert demonstrations on\nwhich it was trained, and compare it's performance against\nCapTAIN. Fig.7 plots the difference in the profits earned by\nCapTAIN-GAIL and CapTAIN. When compared to Fig.5, we\nnotice that CapTAIN-GAIL shows improvement in terms of\nperforming better in cases where CapTAIN generates more\nprofits (which can be seen by the reduced negative values in\nFig.7).\nFurther analysing the mean profits earned by both the\nmethods in the unseen scenarios (as shown in Fig.8) and\ncomparing it to Fig.4, we notice that CapTAIN-GAIL\nachieves better mean performance and remarkably better\nbounding of the unseen worst-case scenarios (that can be\nseen in the reduce standard deviation between Fig.4 and\nFig.8), when compared to CapTAIN. In the unseen sce-\nnarios, CapTAIN-GAIL generates an average profit of\n$17813 as compared to $17587 in the expert demonstration\ncases while CapTAIN generates $15902. Additionally,\nthe standard deviation of the profits earned, goes down\nfrom $3363 in the expert demonstration cases to $3161\nin the unseen cases. This shows that CapTAIN-GAIL is\ngeneralizable across scenarios that are unseen in the expert\ndemonstrations. Conducting a T-test with the null-hypothesis\nbeing that both the methods generate the same average\nprofits, gives a p-value of"}]}