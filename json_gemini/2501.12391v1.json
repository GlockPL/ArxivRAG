{"title": "Physics of Skill Learning", "authors": ["Ziming Liu", "Yizhou Liu", "Eric J. Michaud", "Jeff Gore", "Max Tegmark"], "abstract": "We aim to understand physics of skill learning, i.e., how skills are learned in neural networks during training. We start by observing the Domino effect, i.e., skills are learned sequentially, and notably, some skills kick off learning right after others complete learning, similar to the sequential fall of domino cards. To understand the Domino effect and relevant behaviors of skill learning, we take physicists' approach of abstraction and simplification. We propose three models with varying complexities \u2013 the Geometry model, the Resource model, and the Domino model, trading between reality and simplicity. The Domino effect can be reproduced in the Geometry model, whose resource interpretation inspires the Resource model, which can be further simplified to the Domino model. These models present different levels of abstraction and simplification; each is useful to study some aspects of skill learning. The Geometry model provides interesting insights into neural scaling laws and optimizers; the Resource model sheds light on the learning dynamics of compositional tasks; the Domino model reveals the benefits of modularity. These models are not only conceptually interesting \u2013 e.g., we show how Chinchilla scaling laws can emerge from the Geometry model, but also are useful in practice by inspiring algorithmic development \u2013 e.g., we show how simple algorithmic changes, motivated by these toy models, can speed up the training of deep learning models.", "sections": [{"title": "1 Introduction", "content": "Language models are demonstrating impressive skills in, e.g., coding and mathematics. Many tasks, including language modeling, are complex composite tasks that can be decomposed into many atomic skills [1, 2, 3, 4, 5, 6]. The learning dynamics of skills appear to be complex and intriguing: Throughout training, a skill can be completely learned, partially learned, or not learned at all. Even for learned skills, they could display quite diverse learning curves, including sudden jumps (grokking), gradual improvements, or non-monotonic oscillations. Despite the diverse phenomenology observed in real-world experiments, our intuitive understanding of them is quite limited. Intuitive understanding, or physics-like understanding, has the potential to bridge between theory (mathematics-like understanding) and experiments (engineering-like understanding).\nTo gain some intuition about skill learning, we take physicists' approach of abstraction and simplification (see illustration in Figure 1): when trying to understand a cow in the wild, physicists would make assumptions to simplify the subject matter. It is science but also art to determine the appropriate level of abstraction and simplification. As Einstein famously put it, \"Everything should be made as simple as possible, but not simpler.\" In the same philosophy, we will propose three models trading off between reality and simplicity \u2013 the Geometry model, the Resource model and the Domino model. Each of these models is able to capture some realistic aspects of rich skill dynamics. Note that people use similar words for skills, e.g., tasks, quanta, abilities, etc. In this paper, we stick to the terms \u201cskill\u201d and \"task\" and use them interchangeably (pedagogically, \u201ca skill = the ability to perform a task\").\nAs a motivation, we start by making an observation called the Domino effect, which shows that skills tend to learn sequentially, and notably, some skills start to learn right after other skills finish learning. For example, when we train two independent sparse parity tasks (with frequencies $p_1 = 1$ and $p_2 = 0.1$, shown in Figure 2 left) on a two-layer MLP using the Adam optimizer, the second task starts to progress rapidly only after the first task finishes. Quantitatively, learning task 2 only takes two more times (instead of $p_1/p_2 = 10$ times that one would reasonably expect since the gradient signals differ by 10 times). In a more complicated setup (shown in Figure 2 right), compositional task dependency can also lead to the Domino effect. It is thus very intriguing to understand the"}, {"title": "2 Three Models of Skill Learning: From cows in the wild to spherical cows in a vacuum", "content": "We aim to propose models that manifest the Domino effect shown in Figure 2. We start by introducing the philosophy in Section 2.1. The philosophy leads to the Geometry model in Section 2.2. In Section 2.3, we analyze the phenomenological behavior of the Geometry model in the overparametrized regime, observing the Domino effect. In Section 2.4, we give a resource interpretation to the Geometry model, resulting in the (simpler) Resource model. In section 2.5, we study both the Geometry model and the Resource model in the underparametrized regime. In Section 2.6, we further simplify the Resource model to the Domino model, by assuming a strong hierarchy of tasks."}, {"title": "2.1 Philosophy: Coarse-graining your models", "content": "Einstein has famously stated that \u201cmodels should be made simple, but not simpler\u201d. This means that we want to make models simple enough in order for intuition to work, but also expressive enough to be relevant to the phenomenon we are interested in. For example, to model the behavior of ideal gas, it suffices to have the ideal gas law $PV = NRT$ which states that only four macroscopic variables are important: pressure $P$, volume $V$, temperature $T$, and the number of atoms $NR$. It is unnecessary to keep track of the positions and velocities of all gas atoms. This is a common philosophy in physics, where we want to \u201ccoarse-grain\u201d many irrelevant microscopic details to relevant macroscopic variables, reducing the model complexity to a great extent."}, {"title": "2.2 Introducing The Geometry Model", "content": "Geometry Model: Suppose that a network with parameters $\\theta \\in \\mathbb{R}^{ndim}$ is tasked with $ntask$ independent tasks. Tasks have frequencies (importance weight) $(p_1, p_2,..., p_{ntask})$ where $p_1 \\ge p_2 \\ge > p_{ntask} > 0$. We assume that tasks are linearly represented in the model parameter space \u2013 the $i^{th}$ task is associated with a task vector $t_i \\in \\mathbb{R}^{ndim}$ in the parameter space. Starting from the initialization $\\theta_0$, the model with parameter $\\theta$ has the skill level for the $i^{th}$ task $s_i = (\\theta - \\theta_0) \\cdot t_i$. The loss is $l = \\sum_{ntask} p_iL(s_i)$ or $l = \\sum_{ntask} p_iL(s_i)^2$ where $p$ is the empirical frequency when finite batch effect is of interest, and $L$ is chosen to be $L(s) = (1 - s)^2$ (regression) or $L(s) = -log(1+e^{-s})$ (classification). The loss $l$ is then minimized using an optimizer chosen by the user, resulting in skill dynamics $s_i(t)$ and loss evolution $l(t)$.\nThere are two assumptions made by the Geometry model: (1) independence. Tasks are independent, so the total loss is a weighted average of all tasks, weighted by their frequencies. (2) linear representation. Tasks are linearly represented by the model parameters so that skill levels are determined solely by their projections onto corresponding task vectors. The independence assumption may not be fully realistic, but is usually adopted due to its simplicity (e.g., in [3]), so let us roll with it for now; we will also aim to remove this assumption in Section 5 when we study the compositional structure of tasks. The linear representation assumption is not too unrealistic since it is aligned with recent observations on task arithmetic [5, 6, 7]. The Geometry model only abstracts away some details (task structure, and how the model represents tasks) while leaving other low-level details (e.g., optimization) intact. Below we can add more specifications to the Geometry model for simulation.\nChoice of task vectors To model independent skills, we assume task vectors are randomly drawn from a Gaussian distribution $t_i \\sim N(0, \\frac{1}{ndim}I)$. We call a model overparametrized if $ntask \\le ndim$ but otherwise underparametrized. In the overparametrized case, we further simplify the setup by orthogonalizing these random vectors so that task vectors are orthogonal. This is not an essential assumption but it further simplifies the mental picture (progressing along one task vector does not change the overlaps with other task vectors). In the under-parameterized case, it is impossible to orthogonalize all task vectors hence skill correlation (or superposition) becomes inevitable, which is one of the most intriguing phenomena in today's large language models.\nPower-law distribution Following [3], we usually choose a Zipfian-like (power-law) distribution, i.e., $p_i = i^{-\\alpha} /\\sum_{ntask} n^{-\\alpha}$, where a larger $\\alpha$ dictates a more heavy-tailed distribution."}, {"title": "2.3 The Overparametrized regime: ntask \u2264 ndim", "content": "We now train the Geometry model and see if we can observe anything intriguing in skill dynamics (e.g., the Domino effect). Note that we have to specify optimizers to complete the Geometry model \u2013 we choose SGD, SignGD, and Adam. For simplicity, we start with a two-task setup."}, {"title": "2.4 Introducing the Resource model", "content": "Resource model We have provided a resource interpretation to the gradient-aligned dimensions and observed that the rise of resources for one task usually corresponds to a decrease of resources for others. This suggests that the number of model parameters $ndim$ behaves as a resource pool that is allocated to tasks. Based on this intuition, we propose the following Resource model:\nResource Model Suppose a network is tasked with $ntask$ tasks whose frequencies are $p_1 \\ge p_2 \\ge \\ge p_{ntask} > 0$. The skill levels of these tasks are initially $s_i = 0 \\forall i$, and we define unskill level $u_i = 1 - s_i$. The gradient magnitude of skill $i$ is defined as $N_i = p_iu_i$. The resource obtained by task $i$ depends on the relative gradient magnitudes, $N_i = \\frac{p_iu_i}{\\sum_{j=1}^{ntask} p_ju_j + N_0}$, where $N_0$ characterizes the wasted resources that are not allocated to any task. The decreasing rate of unskill (or increasing rate of skill) is proportional to $N_i$:\n$\\frac{du_i}{dt} = -N_{eff}\\frac{N_i}{\\sum_{j}^{Ntask} N_j + N_0} = -N_{eff} \\frac{p_iu_i}{\\sum_{j=1}^{Ntask} p_ju_j + N_0}$       (1)\nThe effective learning rate $N_{eff} = 2\\sqrt{ndim}N_{geo}$, where $N_{geo}$ is the learning rate used in the Geometry model."}, {"title": "2.5 The Underparametrized regime: Ntask > Ndim", "content": "The philosophy behind the \u201cscaling is all you need\" narrative lies in the assumption that the current large language model, albeit having billions of parameters, still falls short of capability in the face of"}, {"title": "2.5.1 The Geometry model", "content": "To study the under-parametrized regime of the Geometry model, no extra treatment is needed except for setting $ntask > ndim$, and no longer orthogonalization of task vectors. Each task vector $t_i$ is a vector of unit length with a random angular direction. The inner product between two task vectors indicates whether they are collaborative (when the inner product is positive) or competitive (when the inner product is negative). We analyze both the regression setup (using squared loss $L(s) = (1 \u2013 s)^2$) and the classification setup (using cross-entropy loss $L(s) = -log(\\sigma(s))$ with $\\sigma(s) = \\frac{1}{1+e^{-s}}$). We set $ndim = 10$, and vary $ntask = {5, 10, 20, 40, 100}$. We use the SignGD optimizer (learning rate $3 \u00d7 10^{-4}$) with a batch size of 128. Skill dynamics are shown in Figure 8 top left (Mean squared error) and bottom left (Cross entropy loss). It is expected that as the number of tasks increases, some tasks cannot be learned at all, or even become worse than random guessing due to the negative correlation"}, {"title": "2.5.2 The Resource model", "content": "To characterize the under-parametrized regime, extra treatment is needed for the Resource model how we could model correlations between different skills. It turns out we could use the inner products from the Geometry model. We define the correlation matrix $C$ with $C_{ij} = t_i \u00b7 t_j$. Note that $C_{ij}$ are not new phenomenological parameters since they can be determined by task vectors of the Geometry model. It is clear that $C_{ii} = 1$. The resource model in the overparametrized regime (section 2.4) was a special case where all non-diagonal terms are zero, $C_{ij} = 0, \\forall i \\ne j$. Now we present more general Resource models that capture the correlations between tasks.\nRegression We first present the Resource model with correlation in the regression setup. Correlations from other skills enter into the nominator of the right-hand side:\n$\\frac{du_i}{dt} = -N_{eff}\\frac{\\sum_{k=1}^{Ntask} C_{ik}p_ku_k}{\\sum_{j=1}^{Ntask} p_ju_j + N_0}$       (4)\nIt is easy to check that when C is an identity matrix, Eq. (4) degrades to Eq. (1). The skill dynamics is shown in Figure 8 top right, manifesting good agreement with the Geometry model.\nClassification Since language modeling is formulated as next-token prediction, which is a classification problem, it is more realistic to use the cross entropy loss rather than the mean squared error in the Resource model. Note that the term $u_k = 1 - s_k$ on the right-hand side of Eq. (4) is simply the gradient of the squared error $L(s_k) = (1 \u2212 s_k)^2$. By this analogy, we can replace $u_k$ with the gradient of the cross-entropy loss $L(s_k) = \u2212log(\u03c3(sk))$, which is $\\frac{e^{-s_k}}{1 + e^{-s_k}}$, giving\n$\\frac{ds_i}{dt} = N_{eff}\\frac{\\sum_{k=1}^{Ntask} C_{ik}p_k\\frac{e^{-s_k}}{1 + e^{-s_k}}}{\\sum_{j=1}^{Ntask} p_j\\frac{e^{-s_j}}{1 + e^{-s_j}} + N_0}$     (5)\nThe skill dynamics are shown in Figure 8 bottom right, also manifesting good agreement with the Geometry model. In particular, the non-monotonic skill learning dynamics are also captured by the Resource model. The analytical behavior of the Resource model in the underparametrized regime is left for future work."}, {"title": "2.6 Introducing the Domino Model", "content": "We first introduce the Domino model without justification:\nDomino Model Suppose a network is tasked with $ntask$ tasks whose frequencies are $p_1 \\gg p_2 \\gg p_3 \\gg \\ldots \\gg p_{ntask} > 0$. The skill levels of these tasks are initially $s_i = 0 \\forall i$. The skills are learned in a strict sequential order. Suppose it takes $t_0$ time to learn one task, then skill $n$ has level (illustrated Figure 9 left):\n$s_n =  \\begin{cases}\n0, & \\text{if } t \\le (n-1)t_0, \\\\\n t/t_0, & \\text{if } (n-1)t_0 < t < nt_0, \\\\\n1, & \\text{if } t > nt_0,\n\\end{cases}$   (6)\nWe now show that the Domino model can be derived from the Resource model as a special case. Although the Resource model is already quite simple, solving the differential equations Eq. (1) is still cumbersome and may prohibit intuitive thinking. We want to further impose a few simplifying assumptions so that Eq. (1) can simplify to something that can be solved without any effort. We make two assumptions: (1) No waste assumption, i.e., $N_0 = 0$; (2) Strong hierarchy assumption, i.e., $p_1 \\gg p_2 \\gg p_3 \\gg \\ldots$. With these two assumptions, initially, we have\n$\\frac{du_1}{dt} = -N_{eff}, \\frac{du_i}{dt} = 0 (i > 1)$,    (7)"}, {"title": "3 Implications for Neural Scaling Laws", "content": "Neural scaling laws, the phenomenon that model performance progressively improves as resources (data, parameters, compute) scale up, are the main driving force for today's deep learning [8, 9]. It is thus intriguing to understand the mechanisms behind the neural scaling laws. In particular, as formulated by Michaud et al. [3], we are interested in how $\\alpha_N$ ($\\ell \\sim N^{-\\alpha_N}$, exponent against the number of parameters) and $\\alpha_S$ ($\\ell \\sim S^{-\\alpha_S}$, exponent against the number of steps or data) are dependent on $\\alpha$ ($\\alpha$ specifies the data distribution $p_j \\propto j^{-\\alpha}$). We first revisit the Quanta model in [3], and then discuss how our models can enrich the Quanta model family."}, {"title": "3.1 Revisit the Quanta model and compare with the Domino model", "content": "The premise of the Quanta model [3] is the hypothesis that \u201cnetwork knowledge and skills are quantized into discrete chunks (quanta).\u201d, which is also shared by our models. The quanta model can provide accurate predictions for $\\alpha_N$ but usually underestimates $\\alpha_S$.\nPredicting $\\alpha_N$. The Quanta model assumes that each task requires a fixed amount of resources ($C$ parameters) hence a network with $ndim$ parameters is able to learn $n_0 = ndim/C$ tasks. The unlearned tasks contribute to the loss function (taking $ntask \\rightarrow \\infty$): $\\ell = \\sum_{i=n_0}^{ntask} p_i \\sim \\sum_{i=n_0}^{infty} i^{-\\alpha} \\sim \\int_{n_0}^{infty} i^{-\\alpha} \\sim n_0^{-\\alpha+1} \\sim ndim^{-\\alpha+1}$ ($\u03b1 > 1$), giving the parameter scaling exponent $\\alpha_N = \u03b1 \u2212 14$. The Domino"}, {"title": "3.2 The Geometry Model", "content": "Predicting $\\alpha_N$. The loss is a power law function of $ndim$: $\\ell = Andim^{-\\alpha_N}$. The $\\ell(ndim)$ relation is shown in Figure 10 (a), and the extracted scaling exponent $\\alpha_N(\\alpha)$ is shown in (b). For reference, we also plot out $\\alpha_N = \\alpha \u2212 1$, which is what the Quanta model (and the Domino model) would predict. The Geometry model systematically gives a larger $\\alpha_N$ than the Quanta model. In particular, for $\\alpha = 1$, the Geometry model gives $\\alpha_N \\approx 0.34$, which happens to be the scaling exponent observed in the Chinchilla models [9]. By contrast, the Quanta and the Domino models would predict $\\alpha_N = 0$, which is far off.\nBesides the power law scaling behavior in the early stage $ndim < 250$, There exists a critical point around $ndim \\approx 600$ when the loss suddenly decreases. The existence of such a critical point is expected since when $ndim \\ge Ntask = 1000$, the model can learn all skills, resulting in zero loss. The left shift of the critical point might be due to superposition. We do not expect that the critical phenomenon (if truly meaningful) is observable in the current scales of large language models, since we conjecture that today's language models are still in the underparametrized regime (i.e., cannot overfit to all language data).\nPredicting $\\alpha_S$. The loss is a power law of training steps S: $\\ell = BS^{-\\alpha_S}$. The $\\ell(S)$ relation is shown in Figure 10 (c), and the extracted scaling exponent $\\alpha_S(\\alpha)$ is shown in (d). For reference, we also plot out $\\alpha_S = (\\alpha - 1)/\\alpha$ (Quanta model) and $\\alpha_S = \\alpha - 1$ (Domino model). There are a few interesting observations: (1) The Geometry model + SignGD results in the $\\alpha_S(\\alpha)$ dependence agreeing with the Domino model (especially for large \u03b1, when tasks have a stronger hierarchy, an assumption of the Domino model). (2) Adam results in smaller $\\alpha_S$ than SignGD. This suggests that optimization details can affect $\\alpha_S$ to a great extent, which justifies the recent exploration of new optimizers for speeding up the training of large models. (3) The Quanta model systematically underestimates $\\alpha_S$ (also observed by the Quanta paper [3])."}, {"title": "3.3 Multitask sparse parity", "content": "Problem setup Multitask sparse parity is a binary classification problem on binary strings. Input bit strings are $ntasks + n$ bits long, where there are $ntasks$ subtasks (skills) and n is another parameter that regulates the complexity of the subtasks. For each $i \\in {1, . . ., Ntasks}$, we choose a random subset of k of the trailing n bit indices $S_i$. Only one bit in the first $ntasks$ bits is 1 on any input, and the rest are 0. When bit i is 1, the label for that bit string is the parity (sum modulo 2) of the bits $S_i$. The trailing"}, {"title": "4 Implications for Optimization", "content": "We have found that different optimizers (or the same optimizer with different hyperparameters) can lead to quite different training dynamics in the Geometry model. It is thus interesting to ask: can these observations or insights be generalized to real-world training? If this is the case, we can achieve faster idea iteration \u2013 instead of training big models for a few days, one can experiment with simpler models in a few seconds or minutes and get transferable insights. We investigate how these insights can apply grokking (Section 4.2), data weighting (Section 4.3) and optimizers (understanding recently proposed optimizers in Section 4.4). But first, we present a simple quadratic loss function that can demonstrate the Domino effect in Section 4.1."}, {"title": "4.1 Quadratic losses", "content": "In Section 2.3, we have shown that the Geometry model is able to induce the Domino effect. The Geometry model is still a bit complicated though; to further simplify, we present a simple quadratic loss function that can manifest the Domino effect. The loss function is quadratic with strong hierarchies:\n$\\ell = x_1^2 + 0.1 x_2^2 + 0.01 x_3^2 + 0.001 x_4^2$       (9)\nNote that $x = (x_1, x_2, x_3, x_4)^T$ are not necessarily the parameters to be optimized by optimizers. We assume $\\theta$ are the parameters that are directly optimized by optimizers, and $x = R\\theta$, where $R$ is a rotation matrix. When $R = I$ (identity matrix), we say the loss is basis-aligned, because the eigen-directions correspond to the optimized parameters. We choose the initial point to be $x = (1, 1, 1, 1)^T$. Otherwise if $R \\ne I$, we call the loss non-basis-aligned. In particular, we choose $R = H_4$ (the $4 \u00d7 4$ Hadamard matrix $H_4 = ((1, 1, 1, 1), (1, 1, -1, -1), (1, -1, 1, -1), (1, -1, -1, 1))$. We choose the initial point to be $x = (1, 1, 1, -1)^T (since Hx = x_0)$. SGD is invariant to rotations, as shown in Figure 12 top two panels. However, SignGD is sensitive to rotations (bottom two panels), due to element-wise operations generally in adaptive optimizers. When the loss is basis-aligned, four sub-losses decrease at the same pace due to elementwise normalization. By contrast, when the loss is non-basis-aligned, four sub-losses display a Domino effect one loss starts to decrease only when the previous loss decreases to around zero."}, {"title": "4.2 Grokking", "content": "Grokking refers to the phenomenon when generalization happens long after memorization. The learning dynamics is featured by a fast overfitting phase and a slow phase transiting to generalization. Most investigations on grokking use the Adam optimizer, because of its popularity in machine learning. However, we observed in Figure 4 that SignGD can outperform Adam for the specific case, which inspires us to try SignGD in the grokking setup, and see how it compares with Adam."}, {"title": "4.3 Data Reweighting", "content": "As we see in Figure 5, less frequent skills tend to be learned later in training, which becomes worse when $\\alpha$ is large. Can we effectively reduce \u03b1 through re-weighting? The natural idea is that: when a data point from a less frequent skill is seen by the network, a larger weight is placed on it to counteract its rareness. Although this sounds plausible in theory, it is not immediately implementable in practice. We also do not know how to decompose a complex task, say language modeling, into multiple skills, let alone obtain statistics about their frequencies. We adopt a workaround: the sequential learning of skills tells us that a less frequent skill always has higher losses than a more frequent skill, given a fixed training step. This means that we could use losses as a surrogate for frequencies (higher losses mean lower frequencies). So instead of minimizing the standard mean loss $\\ell = \\frac{1}{i=1}(l_i)$, we minimize the weighted mean loss $\\ell' = \\frac{1}{i=1}(w_il_i)$ where we choose $w_i = \\frac{1}{l_i}$, which puts more weight on data points with higher losses. The reweighting itself may not be very novel (e.g., the focal loss [12] that puts more focus on hard examples is similar to our proposal here), but its interpretation in the context of skill learning is new, to the best of our knowledge.\nWe pre-train a GPT-2 small model (based on NanoGPT) on OpenWebText. We use 8 V100 GPUs, choose block size 1024, batch size 480 blocks. We use the Adam Optimizer, with a linear warmup learning rate schedule for 2k steps to maximum learning rate 6 \u00d7 10-4, and a cosine decay schedule from 2k to 10k, ending at lr 3 \u00d7 10-5. We use the standard mean loss as the baseline. For the weighted loss experiments, the first A steps use the weighted loss while the rest 10k - A steps use the standard loss. We compute the decrease of validation loss for the weighting runs compared to the baseline, and show them in Figure 14 right. We find that focusing on hard examples for the first 2k steps yields the most speedup (learning curves shown in Figure 14 left), while further increasing weighting steps can reduce improvement or even slow down training. We hypothesize that this is because of noisy tokens [13]. These tokens always have high losses throughout training, so putting large weights on them (especially in the later stage of training) effectively increases noise hence preventing convergence. This suggests that better weighting schemes should take both skill quality and frequency into consideration we want to speed up low-frequency, high-quality skills. However, noisy tokens correspond to low-frequency (because noises are very diverse), and low-quality skills. We cannot distinguish between these two solely based on frequencies (losses).\nA promising future direction would be inferring frequencies p based on losses l. In fact, if we assume skill independence and MSE loss, the effective model suggests that $\\frac{\\sqrt{l_i}}{2p_i} = C$: according to the conservation laws of Eq. (1), we know $u_i^{1/p_i} = C$ for all skills (tokens), and $\\ell = u^2$, we have $\\frac{\\sqrt{\\ell_i}}{2p_i} = C$. However, questions remain about how to estimate C. Also, conservation law analysis needs to be done for more realistic setups (with skill correlation and dependence, and the cross-entropy loss)."}, {"title": "4.4 Understanding of recently proposed optimizers", "content": "Pre-training a large language model is very time and energy-consuming. To mitigate this, there is growing interest in the optimizer community in proposing more effective optimizers that can speed up large language model (LLM) training, e.g., Lion [14] (discovered by symbolic search), AdEMAMix [15] (maintaining an older momentum), Sophia [16] (combining Adam and Hessian method), Soap [17] (combining Shampoo and Adam), MARS [18] (variance reduction), Muon [19] (adaptive updates in eigenspace). Although these new optimizers show effectiveness in specific setups, why and when they are effective remains unclear. The insights we gained from common \"toy landscapes\" (e.g., convex functions, the Rosenbrock banana function) fail to capture many aspects of real training dynamics (e.g., high dimensionality, batch size effects, etc). Therefore, it is desirable to have a model of intermediate complexity that can model key aspects of LLM training but also remains sufficiently simple and intuitive. We believe that the Geometry model proposed in Section 2 can nicely serve this purpose.\nWe hypothesize that the biggest reason for the slow training dynamics of LLM is due to these two patterns in skill interaction: (1) Correlation (Interference). LLMs are under-parametrized, meaning"}, {"title": "5 Implications for task compositionality", "content": "Before we move on, let us reflect on our philosophy: In Section 2, we proposed a Geometry model and then proposed the simpler Resource model that mimics the behavior of the Geometry model. The proposal of the Resource model may come off as being redundant and pointless \u2013 because one can simply fully simulate the Geometry model without ever resorting to the Resource model. However,"}, {"title": "5.1 Dependent skills can also lead to the Domino effect", "content": "Suppose a neural network takes in a bit string $(x_1, x_2, \\ldots , x_n)$ $(n = 32)$ and is tasked with computing three sparse parities $y_1 = x_1 \\oplus x_2, y_1 = x_3 \\oplus x_4, y_3 = x_1 \\oplus x_2 \\oplus x_3 \\oplus x_4$. It is known that learning k-parity (k is the number of bits to be summed) becomes exponentially slow when k increases [21]. Consequently, a good strategy for the network would be computing $y_3$ not from scratch, but instead, learning $y_1$ and $y_2$ first, and then leveraging $y_3 = y_1 \\oplus y_2$ to compute $y_3$. If this is the case, we expect to see the learning of task 3 only begins after both tasks 1 and 2 complete learning. This is indeed the case empirically, as shown in Figure 2 right or Figure 16 top left. We train a two-layer MLP (50 hidden neurons) on 10000 training samples (full batch) using the Adam optimizer (learning rate $10^{-3}$).\nIntriguingly, it appears that task 3 kicks off learning right after the first two tasks get perfect accuracy. There are two hypotheses for this to happen. Hypothesis 1: learning task 3 is independent of tasks 1 and 2. Although all three tasks have the same frequencies, task 3 has smaller gradients initially due to larger k, so this comes back to the resource explanation in the last section. Hypothesis 2: learning task 3 is dependent on task 1 and 2. To rule out the first hypothesis, we conduct an ablation experiment by replacing the third task with $y_3 = x_5 \\oplus x_6 \\oplus x_7 \\oplus x_8$, which is independent of the first two tasks, whose skill dynamics is shown in Figure 16 bottom left. It is clear that learning of y3 compared to y3 is much delayed, suggesting that the network must have leveraged (at least in some implicit way) y1 and y2 to compute y3.\nResource model Can we induce similar skill dynamics (Figure 16 top left) using Resource models? It might be cumbersome to deal with the Geometry model since specific details must be filled in (how does \"task dependence\u201d in the language of gradients?) to complete the Geometry model. Instead, it is more convenient to work with the Resource model. We can add a small modification to Eq. (1) (we have set $p1 = p2 = p3 = 1$):"}, {"title": "5.2 Simulating skill dynamics with arbitrary dependence graphs", "content": "With the Resource model", "U_{ntask})$": "n$\\frac{du_i"}, {"interesting": "demonstrating several speed-ups and slow-downs"}]}