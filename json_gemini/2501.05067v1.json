{"title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "authors": ["Jiaxing Zhao", "Boyuan Sun", "Xiang Chen", "Xihan Wei", "Qibin Hou"], "abstract": "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential.", "sections": [{"title": "1. Introduction", "content": "In recent years, the rapid advancement of multimodal large language models (MLLMs) [2, 7, 15, 20, 38, 49-51, 61, 76, 92] has led to significant progress in leveraging large language models [1, 9, 17, 23, 49, 53, 64] for image understanding. However, human-computer interaction based solely on images is insufficient for many application scenarios, as most real-world interactions occur in video form. The primary challenge in video understanding lies in managing temporal dynamics, as models must capture and interpret actions and events that evolve over time. Semantic understanding presents another major obstacle, as videos contain not only objects and actions but also complex semantic elements, such as character intentions and emotional expressions. Furthermore, the inherent complexity of video data, combined with the scarcity of high-quality annotated data, results in substantial computational costs and limits the model's learning capabilities. These factors make video understanding a more complex task than image understanding, attracting widespread research interest.\nAs shown in Fig. 1, a typical video MLLM [30, 34, 39, 45, 67] consists of a visual encoder for feature extraction, a text encoder for textual representation, a visual projector to map visual features into a compatible space, and a large language model (LLM) decoder to generate contextually relevant text based on the combined representations. Among them, the visual projector is crucial as it bridges the visual encoder and LLM, enabling visual understanding by mapping visual features into a space compatible with LLMs. Therefore, designing an appropriate visual projector for LLMs is a central focus in many MLLM works."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Multimodal Large Language Model", "content": "Currently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3, 49-51, 61] often achieve better performance but are not open-sourced. Meanwhile, community models [16, 22, 25, 27, 28, 33, 35, 36, 77, 78, 82, 88], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training"}, {"title": "2.2. Projector for Video MLLMS", "content": "As described in Sec. 1, the specific designed visual projectors are crucial for LMMs. We categorize them into three categories and select a representative method from each category to illustrate their strengths in Fig. 2.\nImage-based projector refers to a projector that extracts features from every frame of the input video. Consider-ing the success of simple projectors such as linear projection [12, 13, 36, 38] and cross-attention [6, 65, 78] in Image LLMs, many Video LLMs [4, 27, 30, 34, 46, 62] directly adopt similar schemes as image-based projectors. Besides, some more complex image-based projectors, such as Q-Former [2, 18, 29, 93], have also found applications in video MLMMs [31, 82]. The image-based projector can capture detailed information within individual frames, thereby leading to superior performance in tasks related to scene details. However, limited to the high computational cost and the absence of temporal modeling, the image-based projector faces challenges dealing with temporal related task.\nSpatial-temporal projector aims to consider the relationships between video frames and attempt to reduce the number of visual tokens. VideoLLaMa2 [16] introduces 3D convolution as the Spatial-Temporal Convolution Connector for spatial-temporal aggregation. PLLaVA [72] integrates pooling strategies in both temporal and spatial dimensions. VideoLLaMB [68] designs recurrent memory bridge layers to preserve crucial visual information and semantic coherence. Those spatial-temporal projectors provide significant advantages in handling spatial-temporal related question. However, the fusion of spatial and temporal information may lead to a loss of detailed image perception.\nToken-compress projector is designed for enhancing the model's capacity to handle more input frames. As a typical approach, LLaMa-VID [33] attempt to tackle the computation and memory challenges by compressing visual features. BLIP-3-Video [56] integrate adaptive pooling strategies to compress visual tokens. LongVA [84], on the other hand, addresses the issue by expanding the capacity of LLMs, increasing the number of tokens they can process. Some approaches [66, 69, 81] also consider agent-based"}, {"title": "3. Method", "content": "In this section, we first introduce the motivation of LLaVA-Octopus then describe its architecture, the detailed training process, and the implementation specifics."}, {"title": "3.1. Motivation", "content": "As discussed in Sec. 1, each type of visual projector excels in specific domains tailored to different user instructions. However, in practical scenarios, complex and multifaceted user instructions frequently transcend the boundaries of a single task, leading to unsatisfactory user experiences. Motivated by this, we propose a video MLLM that can handle various scenarios based on user instructions.\nTo achieve this, we first selected representative methods from each category of projectors as candidates. Specifically, we chose the basic MLP2x_GELU as the image-based projector, the STC module from VideoLLaMA2 [16] as the spatial-temporal projector, and and LLaMA-VID's [33] token-compress projector. This selection ensures a comprehensive coverage of the diverse requirements posed by user instructions. Then, we design an instruction-driven adaptive projector based on the selection, and build the LLaVA-Octopus upon it."}, {"title": "3.2. LLaVA-Octopus", "content": "In Fig. 3, we present a detailed architecture diagram of LLaVA-Octopus. LLaVA-Octopus primarily consists of four key components: the visual encoder, the text encoder, the instruction-driven adaptive projector, and the large language model decoder. Among these components, the Instruction-Driven Adaptive Projector is the core innovation of LLaVA-Octopus. Specifically, for user instructions, we use BERT [19] to encode the instructions, generating textual features of the commands. In particular, we focus on the [CLS] token output by BERT, which contains the semantic information of the instruction. We choose BERT as the text encoder as it is a powerful pre-trained language model that can capture deep semantic information in text. BERT encodes input text using a bidirectional Transformer structure, and the generated [CLS] token can effectively represent the semantics of the entire sentence, providing a solid foundation for subsequent weight generation.\nThen, we use two multi-layer perceptrons (MLPs) to generate feature weights. The first MLP takes the [CLS] token as input and, after processing through multiple layers of the neural network, generates intermediate feature representations. This MLP is capable of capturing high-level semantic information from the instruction. The second MLP then takes the output of the first MLP as input and further processes it to generate the final weight values, with each weight value corresponding to one of the visual projectors. We demonstrate the detailed architecture of projector fusion gate in Fig. 4. The generated weights are then used to weight the visual features extracted by the three different projectors, dynamically selecting and combining the most suitable features. Specifically, suppose the three projectors extract features F1, F2 and F3, and the generated weights are w\u2081, w2 and w3, respectively. Then, the final visual representation F is given by:\n$$F = W\u2081. F\u2081 + W2. F2 + W3 F3.$$\nTo ensure that the features obtained from the three different types of projectors are consistent in terms of token numbers, we make the following adjustments to the MLP, STC, and LLaMA-VID projectors. First, for the MLP, the original setting extracts 8 video frames, resulting in a token count of 14 \u00d7 14 \u00d7 8+8=1576. To align the token counts, we remove the separators between each image, reducing the token count to 1568. Second, for STC, the original setting results in a token count of 13 \u00d7 13 \u00d7 4 = 676 for 8 video frames. To ensure token consistency, we modify the sampler parameters in the STC module. Specifically, we use a stride of (2, 2, 2) and (1, 2, 2), with padding of (1, 1, 1). These modifications ensure that the STC projector produces a token count of 1568. Finally, for the LLaMA-VID projector, we use 128 frames to represent the video. For each frame, we use 6 context tokens and 6 content tokens. To ensure token consistency, we add a separator token every 4 frames. Specifically, the number of tokens for every 4 frames is 49, and the total token count for 128 frames is 49 x 32 = 1568."}, {"title": "3.3. Model Training", "content": "The training process of our LLaVA-Octopus consists of two main phases: multi-task pre-training and instruction tuning. In the Fig. 5, we show the proportions of video-text pairs and image-text pairs in both stages, as well as the composition of our instruction data. We provide detailed descriptions of these two phases in the following.\nMulti-task pre-training. During the multi-task pre-training phase, we primarily focus on training the three visual projectors. In this phase, we only adjust the parameters of these three projectors while keeping all other parameters frozen. We utilize two types of data: image-text pairs and video-text pairs. For image-text data, we utilize CC-3M [57] and RealWorldQA [70], totaling 559K samples. As for video-text data, we use WebVid-10M [8], CLVERER [79], NEXT-QA [71], Youcook2 [91], Charades [58], Charades-Ego [59], TGIF [32], and ShareGPT4Video [14], totaling 2.04M samples. The total number of samples used for multi-task pre-training is 2.6M, and none of the datasets contain test data. The detailed distribution of multi-task pre-training phase is shown in Tab. 1.\nInstruction tuning. We use the three pre-trained projectors and leverage the pre-trained BERT [19] features in instruction tuning. The weights of the projector fusion gate are initialized randomly. During the instruction tuning phase, we train the parameters of all three projectors, the projector fusion gate, and the large language model decoder. Meanwhile, we keep the parameters of the visual encoder and BERT frozen to maintain their stability and consistency.\nThe instruction data we utilized are derived from Oryx [42], as detailed in Table 2. The open-source portion of the Oryx dataset encompasses question answering (QA) data, video caption data, and multiple-choice QA data. Specifically, we integrate comprehensive datasets that include question-answering and captioning tasks from VideoChatGPT-Plus [47], ShareGPT4Video [14], and LLaVA-Hound [87]. To enhance performance on multiple-choice benchmarks, we have also incorporated Cinepile [55], NextQA [71], and PerceptionTest [54] into our training dataset.\nOn one hand, current large models use different datasets, and some methods even use private data, making it difficult to objectively evaluate the capabilities of model architectures. On the other hand, full-scale multi-task pre-training and instruction tuning require substantial computational resources and time costs. Therefore, we not only use the full dataset for multi-task pre-training and instruction tuning but also provide a simplified setup where we only use VideoLLAVA [34] data for these stages. The multi-task pre-training data for VideoLLAVA consist of a subset of 558K LAION-CC-SBU image-text pairs and 702K video-text pairs provided by Valley [43]. For the instruction tuning stage, the data includes 665K image-text instruction pairs from LLaVA1.5 [36] and 100K video-text instruction pairs from Video-ChatGPT [46]. Under this setup, we conduct"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Implementation Details. We employ the Qwen2.5-7B-Instruct model [63] as the LLM and SigLIP (so400m-patch14-384) [80] as the visual backbone. All experiments are performed on 8 NVIDIA A100 GPUs."}, {"title": "4.2. Main Results", "content": "Results on Video Question Answering Benchmark. In Tab. 3, we demonstrate the performance of our LLaVA-Octopus against state-of-the-art methods on three zero-shot video QA benchmarks. MSVD-QA [11] is a dataset comprising questions about short real-world video clips, typically lasting 10-15 seconds. ActivityNet-QA [10] consists of human-annotated action-related QA pairs derived from the ActivityNet dataset, with an average duration of 2 minutes. Additionally, we evaluate our model on the Video-based Generative Performance benchmark introduced by"}, {"title": "4.3. Ablation Study", "content": "Effectiveness of Each Projectors. To demonstrate the impact of different projectors, in Tab. 6, we first conduct ablation studies using various numbers and types of projectors. We know that when weighting the visual tokens obtained from three different projectors, the tokens derived from the image-based and spatial-temporal projectors are temporally and spatially alignable. However, the tokens generated by the token-compress projector, due to the compression of tokens, cannot be directly aligned in terms of temporal and spatial dimensions with those from the other two projectors. The features resulting from the token-compress projector, when added to those from the other two projectors, can to some extent disrupt the spatial and temporal relationships. Nevertheless, incorporating the tokens from the token-compress projector significantly preserves the temporal integrity, as experimental results have demonstrated that this temporal integrity brings substantial benefits. It can be observed that compared to using a single type of projector, each addition of a new type of projector results in performance improvements on the MVBench benchmark.\nImapct of Different Projector Fusion Strategies. To demonstrate the effectiveness of our proposed projector fusion gate, we conduct ablation studies using different projector fusion strategies in Tab. 7. Specifically, we perform experiments under average, concatenation, random weight, and random choose settings, in addition to our proposed method. We demonstrate the random weight and concatenation paradigm in Fig. 6.The results show that our projector fusion gate outperforms other strategies on both the MVBench and Video MME benchmarks. This demonstrates that our projector fusion gate can effectively determine the weight of each projector's contribution to the final visual embedding based on user instructions, thereby better adapting to different task scenarios."}, {"title": "4.4. Qualitative analysis", "content": "In Fig. 7, we demonstrate some qualitative examples of LLaVA-Octopus include scene detail-related questions, spatial-temporal-related questions, and dynamic counting questions. LLaVA-Octopus achieves correct responses in each of these scenarios, illustrating its ability to integrate the strengths of different visual projectors and and overcome the inherent limitations imposed by a single projector. This versatility allows our method to perform well not only on specific types of problems but also in a wide range of comprehensive instruction scenarios."}, {"title": "5. Conclusions", "content": "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus dynamically adjusts the feature weights from different visual projectors based on user instructions, effectively leveraging the unique strengths of each projector. By dynamically selecting and combining the most suitable features, LLaVA-Octopus significantly enhances its performance in various multimodal tasks. Our experimental results demonstrate that LLaVA-Octopus achieves outstanding performance across multiple benchmarks, particularly in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its promising application potential."}, {"title": "6. More Comparisons of Different Projectors", "content": "As discussed in Sec. 1, the significance of visual projectors and the applicability of different types of visual projectors to various visual task scenarios constitute a crucial motivation for LLaVA-Octopus. We have provided some examples in Fig. 2 to illustrate this phenomenon. To further demonstrate its generalizability and reinforce our motivation, we supplement more additional examples in Fig. 8, Fig. 9 and Fig. 10.\nSpecifically, in Fig. 8, we present examples of Scene Details Related Questions using representative methods of the three types of projectors. In complex backgrounds, when questions require a more detailed understanding of the scene, the Image-based Projector demonstrates superior performance. In Fig. 9, we show examples of Spatial-temporal Related Questions using representative methods of the three types of projectors. It can be seen that the method based on the Spatial-temporal Projector, VideoLLaMA2 [16], shows a clear advantage. In Fig. 10, we demonstrate the effectiveness of different projector methods in problems that require temporal consistency. Similar to the discussion in the paper, we chose Dynamic Counting Problems to represent this category. It is evident that the temporal consistency of both the Image-based Projector and the Spatial-temporal Projector is severely compromised, leading to poor performance in this type of problem. In contrast, the method with Token-compress Projector shows good performance in this category."}, {"title": "7. More Qualitative Results", "content": "In our main paper, we claim that proposed LLaVA-Octopus can tackle different video understanding scenarios and comprehensive user instructions. We have verified this through both extensive quantitative and qualitative experiments in Sec. 4. Here, we present more qualitative results in Fig. 11 and Fig. 12 to further support our conclusion.\nSpecifically, in Fig. 11 and Fig. 12, we present the performance of our LLaVA-Octopus on three types of questions: Scene Details Related Questions, Spatial-temporal Related Questions, and Dynamic Counting Problems. It can be seen that due to the reasonable integration of image-based projector, spatial-temporal projector, and token-compress projector in our model architecture, our LLaVA-Octopus is capable of providing accurate answers to all three types of questions."}]}