{"title": "MedSAGa: Few-shot Memory Efficient Medical Image Segmentation using Gradient Low-Rank Projection in SAM", "authors": ["Navyansh Mahla", "Annie D'souza", "Shubh Gupta", "Bhavik Kanekar", "Kshitij Sharad Jadhav"], "abstract": "The application of large-scale models in medical image segmentation demands substantial quantities of meticulously annotated data curated by experts along with high computational resources, both of which are challenges in resource-poor settings. In this study, we present the Medical Segment Anything Model with Galore (MedSAGa) where we adopt the Segment Anything Model (SAM) to achieve memory-efficient, few-shot medical image segmentation by applying Gradient Low-Rank Projection (GaLore) to the parameters of the image encoder of SAM. Meanwhile, the weights of the prompt encoder and mask decoder undergo full parameter fine-tuning using standard optimizers. We further assess MedSAGa's few-shot learning capabilities, reporting on its memory efficiency and segmentation performance across multiple standard medical image segmentation datasets. We compare it with several baseline models, including LoRA fine-tuned SAM (SAMed) and DAE-Former. Experiments across multiple datasets and these baseline models with different number of images for fine tuning demonstrated that the GPU memory consumption of MedSAGa is significantly less than that of the baseline models, achieving an average memory efficiency of 66% more than current state-of-the-art (SOTA) models for medical image segmentation. The combination of substantially lower memory requirements and comparable to SOTA results in few-shot learning for medical image segmentation positions MedSAGa as an optimal solution for deployment in resource-constrained settings.", "sections": [{"title": "1. Introduction", "content": "Image segmentation plays an important role in various aspects of healthcare, enabling precise analysis and diagnosis from medical imaging data such as MRI, CT scans, and ultrasound [21]. By accurately delineating anatomical structures or pathological regions, medical image segmentation could assist clinicians in tracking the health of the patients by identifying abnormalities and planning treatments [16, 27]. This is performed by clinical experts who manually outline the borders for segmentation which can subsequently be used in deep learning algorithms. However, labeling medical images requires consensus of multiple clinical experts making it expensive and difficult, especially in resource-constrained settings. Few-shot learning and zero-shot learning prove to be very useful in such scenarios [15].\nOver the past decade, a multitude of deep learning models, including U-Net [20], and transformer-based models such as TransUNet [7] and DAE-Former [3], have been developed for image segmentation tasks. The latest large-scale models (LM) such as GPT-4 [1], SAM [12], DALL-E [19] and SegGPT [23] provided a platform to solve different image segmentation tasks. These models are trained on huge datasets and the performance of these models is highly competitive even in zero-shot learning. However, resources such as memory and the compute required for training and fine-tuning these models for downstream tasks are significantly large making it difficult to deploy them in a resource-constrained setting. Although the SAM and SegGPT models show SOTA performances, these models are not trained on medical images and thus, cannot be utilized off-the-shelf for tasks like medical image segmentation. Hence, an efficient fine-tuning strategy is required to utilize the above-mentioned large-scale models for downstream tasks like medical image segmentation is required. There are several Parameter Efficient Fine-Tuning (PEFT) strategies which are categorized into additive, selective, reparameterized, and hybrid fine-tuning based on their operations [26]. In this work, we utilize SAM for the medical image segmentation task and adopt the Gradient Low-rank projection (GaLore) strategy to fine-tune it on medical image segmentation data [28]. We summarize our contributions in the following points:\n1. We demonstrate Medical Segment Anything Model with GaLore (MedSAGa), a framework integrating the Gradient Low-rank Projection (GaLore) optimization with SAM.\n2. We perform rigorous experimentation on four diverse medical image segmentation datasets and compare the performance of MedSAGa with standard benchmarks in few-shot settings.\n3. Our results showcase the significance of our framework by demonstrating notable reductions in memory over existing models while delivering comparable performance."}, {"title": "2. Related Works", "content": "A major milestone for medical image segmentation was achieved by developing U-Net, a model based on convolutional neural networks. Its novel architecture provides precise medical image segmentation even when trained on a limited number of images [20]. Further, several different variations of U-Net have been developed like DenseUNet [5] and ResUNet [18], which improved the segmentation performance by making significant changes in the structure of the U-Net architecture. The TransUNet model proposed in study [7] utilizes the transformer architecture for encoding in conjunction with the U-Net architecture [6]. It benefits from the global contextual information reception capabilities of transformers, that U-Net's convolutional layers lack due to their local receptive fields. However, transformers require substantial computational resources, particularly in terms of GPU memory, because they process the entire image as a sequence of patches [8]. This sequential processing leads to high memory demands, especially when handling larger images typical in medical imaging applications. The hybrid nature of TransUNet, which combines convolutional operations with transformer mechanisms, increases the complexity of the model. This complexity can make the training process more computationally intensive [7]. Further, the transformer layers in TransUNet require multiple self-attention calculations which are computationally expensive, especially for higher resolution inputs [7]. Due to the complexity and larger number of parameters, training TransUNet can be time and resource-consuming. This is exacerbated when fine-tuning on specific tasks as transformers generally take longer to train than their purely convolutional counterparts. The use of advanced data augmentation and training strategies necessary to achieve optimal performance further extends the training duration and computational expense of TransUNet [7]. An architecture based on transformers, called DAE-Former [3], was further proposed, which utilizes an efficient dual attention mechanism and has been demonstrated to be better than TransUNet in terms of both computational efficiency and accuracy. However, the self-attention mechanism of DAE-Former, although redesigned for efficiency, still retains the intrinsic quadratic computational complexity with respect to the number of tokens. This complexity can lead to significant computational overhead when processing large datasets typical in medical imaging [3]. Thus, although these models prove to be quite efficient in image segmentation when trained for a particular task, for a diverse and resource-constrained domain such as healthcare, a generalized large-scale model capable of few-shot learning should be the most practical framework.\nIn the domain of large-scale free lunch models for image segmentation, SAM (Segment Anything Model) represents a significant milestone, trained on an extensive corpus of over 1B masks from 11M images [12]. It is noted for its superior performance, especially in zero-shot settings where it performs similarly to or even better than many supervised models. However, SAM doesn't perform well on medical image segmentation as it is trained on natural images and hence, cannot capture the semantics required in medical image segmentation [13]. After fine-tuning the SAM model for medical image segmentation task, SAM shows promising few shot learning results [17].\nParameter Efficient Fine-tuning (PEFT) techniques provide a solution for adapting such large-scale models for downstream tasks by reducing the number of trainable parameters while maintaining comparable performance [25]. One such approach, known as LoRA (Low-Rank Adaptation) does this by freezing the pre-trained model weights and integrating low-rank matrices into the transformer layers [10]. This significantly reduces the number of trainable parameters required for fine-tuning the large-scale models for various downstream tasks. A medical image segmentation model, SAMed (Customized Segment Anything Model for Medical Image Segmentation) utilizes these LoRA layers for fine-tuning the image encoder of SAM [26]. It further updates the prompt encoder and mask decoder parameters on medical image segmentation datasets which enhances the segmentation capability of SAM on medical images. This approach provides significant improvement in performance while utilizing less memory for fine-tuning on medical images [26].\nA novel approach to reducing memory consumption was introduced in GaLore (Gradient Low-Rank Projection) [28] which presents a memory-efficient technique for fine-tuning transformer-based models by projecting gradients into a low-rank space during training. This method effectively reduces the memory footprint of optimizer states while preserving the capability to learn full-rank weights. As a result, GaLore enables more efficient training on consumer-grade GPUs without the necessity for model parallelism or check-pointing. In our framework, MedSAGa, we leverage GaLore for fine-tuning the SAM architecture, showcasing its effectiveness in medical image segmentation, particularly in resource-constrained environments."}, {"title": "3. Methodology", "content": "3.1. Overview\nThrough MedSAGa, our primary aim is to harness the few-shot capabilities of a large-scale image segmentation model such as SAM and apply it to a resource-constrained setting like healthcare [4]. As seen in fig. 1, MedSAGa is more efficient than most SOTA models in both the training as well as inferencing stage (which uses weights+activations+others memory). By resource-constrained settings, we refer to the requirement of a substantially less number of annotated data to train or fine-tune the segmentation models and an inability to procure huge memory and computational resources required to train large-scale models. SAMed attempted to address this by integrating LoRA layers into its image encoder [26], however, it still requires huge memory and computational resources while facing the challenge of not utilizing the entire parameter matrix for training. Hence, in our work MedSAGa, we demonstrate a more refined approach to adapt SAM in resource-constrained settings by applying GaLore to all the parameters of the image encoder and then fine-tuning it, which significantly reduces the memory and computational cost while still maintaining full parameter structure. In addition to this, we fine-tune the prompt encoder and mask decoder without GaLore to perform improved semantic segmentation on medical images, as is demonstrated in SAMed [26]. We note here that since the prompt encoder and mask decoder in SAM are lightweight [26], applying Galore for fine-tuning them would not lead to a substantial improvement in memory consumption. Fig. 1 shows the memory utilization by MedSAGa and the other SOTA models while fine-tuning on medical images. Furthermore, as seen in fig. 2, it can be observed that after a specific number of images, the performance graph plateaus out supporting the idea to use few-shot learning in resource constrained settings giving very close performance to using the entire dataset while using a smaller proportion of images.\nSAM generates multiple segmentation masks to avoid ambiguity. However, we align MedSAGa with the working of SAMed and generate multiple segmentation masks, each representing a different tissue or segment of the anatomy in addition to the background mask. These masks are then further post-processed to give the final segmentation result. For the training phase, we adopt warmup to stabilize the training process and use the AdamW optimizer for improved performance as was suggested in the SAMed architecture [26].\n3.2. The Architecture\nTo reduce the resources required for utilizing SAM, we apply GaLore optimization to all the parameters of the image encoder. This approach harnesses the gradually changing low-rank structure of the gradient explained in the GaLore paper, which improves memory efficiency while still giving comparable results [28]. Instead of reducing the weight parameter size as is done in LoRA, GaLore projects the Gradient matrix at time t, $G_t \\in \\mathbb{R}^{m\\times n}$, into a low-rank matrix $\\bar{G_t}$, which can be represented by eq. 1.\n$\\bar{G_t} = P_t p_t (P_t^+ G_t Q_t) Q_t^+ \\cdot$ (1)\nwhere Pt and Qt are projection matrices with dimensions $\\mathbb{R}^{m\\times r}$ and $\\mathbb{R}^{n\\times r}$ respectively, and pt is an element-wise stateful gradient regularizer. If Wo is the initial weight matrix, Wr represents the weight matrix at time T, and $\\eta$ is the learning rate, then the gradient update rule in GaLore is as follows in eq. 2.\n$W_T = W_0 + \\eta \\sum_{t=0}^{T-1} \\bar{G_t}.$ (2)\nIn our approach, we apply this Gradient low-rank projection to all the parameters of the image encoder which includes all the projection layers (q, k, v and o) as opposed to SAMed which applies LoRA only to the q and v projection layers in its best-performing model. We apply GaLore to all the parameters of the Image encoder as it adds only a negligible memory overhead. For the MedSAGa approach to function as an auto-segmentation model, we do not provide any prompts to the prompt encoder. Instead, we utilize the default embedding of the prompt encoder of SAM, which it uses when no prompt is given, and only fine-tune it during the training phase.\nThe mask decoder of SAM consists of a lightweight transformer layer and a segmentation head [26]. In our approach, we fine-tune the entire mask decoder directly without applying any optimization as it is already lightweight. Furthermore, as was developed in the SAMed architecture, we change the segmentation head of the mask decoder to adapt it to give precise semantic segmentation for each class of tissue or anatomy present in the image. Let us consider there are k classes in total including 1 background class in the medical image, the mask decoder of MedSAGa predicts k segmentation masks $M \\in \\mathbb{R}^{h\\times w \\times k}$, each corresponding to a single class in the image. We then further utilize a combination of argmax and softmax functions to generate a segmentation map $\\hat{Y}$ as shown in eq. 3 where d = \u22121 represents the channel dimensions..\n$\\hat{Y} = argmax(Softmax(M, d=-1), d=-1)$ (3)\nThese adjustments make MedSAGa is an easy to implement solution in the SAM architecture with minimal engineering required, thereby making our solution very practical and adaptable for varied settings.\n3.3. Training Strategies\nFor performing training, we utilize a combination of Cross Entropy loss and Dice loss, similar to that utilized in the SAMed approach as represented by the eq. 4.\n$L = A L_{CE}(M, \\mathcal{D}(S)) + (1-A) L_{Dice}(M, \\mathcal{D}(S))$, (4)\nwhere L represents the net loss value, $L_{CE}$ represents the Cross Entropy loss, $L_{Dice}$ represents the Dice loss and A represents the loss weight. $\\mathcal{D}$ represents downsampling to align the resolution of the ground truth mask (S) with the Med-SAGa output, compensating for lower spatial resolution of MedSAGa.\nWarmup is applied in MedSAGa to stabilize the training process. By allowing the learning rate to increase gradually, we enable the model to slowly adapt the weights to the specific characteristics of the medical data, thereby avoiding poor convergence and instability early in the training phase and reducing the chances of overfitting [24].\nInstead of Adam optimizer, in MedSAGa we utilize the AdamW optimizer. AdamW decouples weight decay from the gradient updates and applies it directly to the weights. This method proves to be more effective in regularization, maintaining a better separation between the weight decay and the adaptive learning rate aspects of Adam [14]. In MedSAGa, the AdamW approach ensures that the regularization is not overly influenced by the learning rate adaptations specific to different weights."}, {"title": "4. Experiments and Results", "content": "We demonstrate the performance of MedSAGa through rigorous experimentation on 4 different medical datasets by comparing it to several baseline models.\n4.1. Datasets and Evaluation Metrics\nDatasets. We utilize four different datasets covering different parts of the human anatomy for our experimentation. All the baselines and results that we present in the further sections have been tested on each of these 4 datasets to evaluate the robustness of the MedSAGa architecture. For each dataset, the number of few-shot images used for experimentation are chosen depending upon the size of the dataset and the number of classes.\nThe AMOS dataset is a large-scale clinical dataset of 500 MRI and 100 CT scans which consists of annotations of 15 abdominal organs [11]. Each slice was padded to obtain final slices of dimension 512 \u00d7 512. For the training set, we only considered the slices that had masks of at least 5 organs to overcome class imbalance. In total, 10,300 slices were satisfying the above criteria out of which we used 500, 1000, 2000, 5000, and 7000 slices for the few shot experimentation. The ChestX-ray8 dataset consists of 108,948 frontal-view X-ray images of 32,717 unique patients [22]. The images consist of 8 labels, mined from the text corpus of the corresponding radiological reports using NLP. We predict the segmentations for three classes whose masks were available: left lung, right lung and heart. Out of the 108,948 images in the dataset, we used only 50, 100, 200, 350, 500 and 1800 images as performance saturation was reached at quite an early stage (Refer Fig. 2). The Ischemic Stroke Lesion Segmentation (ISLES) dataset is a multi-center MRI dataset of acute to subacute stroke lesions [9]. It consists of 400 multi-vendor MRI images, each consisting of a number of slices having a dimension of 112 \u00d7 112. The DWI modality was used for experimentation and as a pre-processing step, thresholding was applied to only use slices containing lesions while training. For performing few-shot experiments, 200, 500, 700, 1000, 2000 and 3,200 (maximum number of slices obtained after thresholding) slices were used in the ISLES dataset. The Spleen dataset was retrieved from the Medical Segmentation Decathlon challenge [2]. It consists of 61 3D volume portal-venous phase CT scans from patients undergoing treatment for liver metastases. Out of these, we utilize only those images for training that have a mask available. Data leakage between the train and test sets was avoided by splitting the data according to the patients instead of according to individual slices for all datasets.\nEvaluation Metrics We use dice score and HD95 metrics to compare the model performances. Also, we show the memory utilized by the model for training and fine tunning.\n4.2. Implementation details and evaluation metrics\nAll the experiments were run on a single NVIDIA RTX A6000 GPU with 48GB GPU RAM. A batch size of 12 was used for all the experiments. The warmup and loss weight strategies were the same as used in SAMed, i.e., the loss weights for cross entropy and dice loss were set to 0.2 and 0.8, respectively. For the warmup, the initial learning rate was set to 0.005, while the warmup period was set to 250. The $\\beta_1$, $\\beta_2$, and the weight decay of the AdamW optimizer were set to 0.9, 0.999, and 0.1, respectively. For the GaLore optimizer, AdamW was used as the base optimizer with a learning rate of 1 \u00d7 10-3, and $\\beta_1$ and $\\beta_2$ were set to 0.9 and 0.999, respectively. The plane change rate (T) used was 200, as is recommended in the GaLore Paper. The base pre-trained model architecture for training our SAM-based model was the vit_b model, which occupies 5.25% (18.81M) of the original model size (358M).\n4.3. Comparison with SOTA\nThe results of the memory utilization by MedSAGa and other baselines are depicted in Fig. 1. From this it is evident that the MedSAGa model utilizes the lowest memory as compared to the other SOTA image segmentation models, giving an average memory efficiency of 66% more as compared to current state-of-the-art (SOTA) models for medical image segmentation. Here, we would like to mention that since the memory utilization reported in the Fig. 1 are the model memories, they are independent of the training data and depend only on the model parameters. Hence, MedSAGa is highly memory efficient as compared to the other standard baselines in both training and inferencing phases.\nThe results of the segmentation performance are compiled in the Table 1. The table shows the performance metrics, the mean Dice Scores and the mean HD95 scores, for various few-shot settings across the four medical image datasets. These results depict the ability of MedSAGa as the best approach in terms of memory efficiency while still giving comparable segmentation performances in most of the settings. In the domain of SAM-based models for medical image segmentation, we beat the state-of-the-art model, SAMed not only in terms of memory efficiency but also in segmentation performance in almost every few shot settings across all datasets.\nComparing with DAE-Former, MedSAGa performs better segmentation in very low resource settings for most datasets like Chest X-ray, ISLES and Spleen datasets with a slight drop in the dice score when higher number of training images are used. Furthermore, MedSAGa utilizes 61% less memory compared to DAE-Former."}, {"title": "5. Ablation Studies", "content": "We present our ablation studies in two categories. In the first part, we demonstrate two variations of integrating Gradient low-rank projection optimization in the MedSAGa architecture. In the first variation (referred to as MedSAGa_v1), we apply the GaLore optimization only to the attention parameters of the image encoder for fine-tuning and the prompt encoder and mask decoder are fine-tuned without using GaLore. Even though GaLore is only applicable to the immediate attention layer succeeding MLP layer of the transformer neural network, we apply it on all the attention parameters in ViT of SAM to justify the same. In the second variation (referred to as MedSAGa_v2), we apply GaLore to all the parameters of the image encoder and fine-tune it and do not perform any fine-tuning on the prompt encoder and mask decoder. Both these variations were tested on the Chest X-ray dataset and the results of MedSAGa_v1, MedSAGa_v2 along with the best-performing model, MedSAGa are presented in Table ??. Our second category of ablation studies is based on experimenting with the effects of applying warmup while fine-tuning MedSAGa. Fig. 4 shows the results of segmentation performance on all the datasets when fine-tuned with and without warmup. The results of the same study on other datasets is mentioned in the supplementary material."}, {"title": "6. Limitations and Future Scope", "content": "The MedSAGa methodology leverages a large-scale model akin to SAM for few-shot image segmentation within resource-constrained environments, exhibiting a marked reduction in memory utilization compared to other SOTA models, while still yielding comparable segmentation performance. However, attaining optimal segmentation performance with MedSAGa may not always be feasible, as varying architectural designs may excel in capturing the distinctive features of diverse datasets. Additionally, as highlighted in the GaLore literature, further improvements in memory overhead can be achieved by reducing the projection layer dimensions via techniques such as quantization and streamlined parameterization, which can be done as part of the future work. Since this work presents SAM in culmination with GaLore (a gradient low-rank optimization technique) it can be applied to various end-to-end training techniques of models involving ViTs. Not just image segmentation, the use of GaLore alongside ViT can be interestingly used for a wide-variety of downstream tasks in a resource-constraint environments."}, {"title": "7. Conclusion", "content": "In our work, we demonstrate the use of large-scale models like SAM for few-shot medical image segmentation in resource-constrained settings like healthcare by using Gradient low-rank projection (GaLore) for fine-tuning the image encoder. This allows us to achieve significant memory efficiency while still utilizing full parameter training. Our rigorous experiments on diverse medical image seg-"}]}