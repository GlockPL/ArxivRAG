{"title": "MedSAGa: Few-shot Memory Efficient Medical Image Segmentation using Gradient Low-Rank Projection in SAM", "authors": ["Navyansh Mahla", "Annie D'souza", "Shubh Gupta", "Bhavik Kanekar", "Kshitij Sharad Jadhav"], "abstract": "The application of large-scale models in medical image segmentation demands substantial quantities of meticulously annotated data curated by experts along with high computational resources, both of which are challenges in resource-poor settings. In this study, we present the Medical Segment Anything Model with Galore (MedSAGa) where we adopt the Segment Anything Model (SAM) to achieve memory-efficient, few-shot medical image segmentation by applying Gradient Low-Rank Projection (GaLore) to the parameters of the image encoder of SAM. Meanwhile, the weights of the prompt encoder and mask decoder undergo full parameter fine-tuning using standard optimizers. We further assess MedSAGa's few-shot learning capabilities, reporting on its memory efficiency and segmentation performance across multiple standard medical image segmentation datasets. We compare it with several baseline models, including LoRA fine-tuned SAM (SAMed) and DAE-Former. Experiments across multiple datasets and these baseline models with different number of images for fine tuning demonstrated that the GPU memory consumption of MedSAGa is significantly less than that of the baseline models, achieving an average memory efficiency of 66% more than current state-of-the-art (SOTA) models for medical image segmentation. The combination of substantially lower memory requirements and comparable to SOTA results in few-shot learning for medical image segmentation positions MedSAGa as an optimal solution for deployment in resource-constrained settings.", "sections": [{"title": "1. Introduction", "content": "Image segmentation plays an important role in various aspects of healthcare, enabling precise analysis and diagnosis from medical imaging data such as MRI, CT scans, and ultrasound [21]. By accurately delineating anatomical structures or pathological regions, medical image segmentation could assist clinicians in tracking the health of the patients by identifying abnormalities and planning treatments [16, 27]. This is performed by clinical experts who manually outline the borders for segmentation which can subsequently be used in deep learning algorithms. However, labeling medical images requires consensus of multiple clinical experts making it expensive and difficult, especially in resource-constrained settings. Few-shot learning and zero-shot learning prove to be very useful in such scenarios [15].\nOver the past decade, a multitude of deep learning models, including U-Net [20], and transformer-based models such as TransUNet [7] and DAE-Former [3], have been developed for image segmentation tasks. The latest large-scale models (LM) such as GPT-4 [1], SAM [12], DALL-E [19] and SegGPT [23] provided a platform to solve different image segmentation tasks. These models are trained on huge datasets and the performance of these models is highly"}, {"title": "2. Related Works", "content": "A major milestone for medical image segmentation was achieved by developing U-Net, a model based on convolutional neural networks. Its novel architecture provides precise medical image segmentation even when trained on a limited number of images [20]. Further, several different variations of U-Net have been developed like DenseUNet [5] and ResUNet [18], which improved the segmentation performance by making significant changes in the structure of the U-Net architecture. The TransUNet model proposed in study [7] utilizes the transformer architecture for encoding in conjunction with the U-Net architecture [6]. It benefits from the global contextual information reception capabilities of transformers, that U-Net's convolutional layers lack due to their local receptive fields. However, transformers require substantial computational resources, particularly in terms of GPU memory, because they process the entire image as a sequence of patches [8]. This sequential processing leads to high memory demands, especially when handling larger images typical in medical imaging applications. The hybrid nature of TransUNet, which combines convolutional operations with transformer mechanisms, in-"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overview", "content": "Through MedSAGa, our primary aim is to harness the few-shot capabilities of a large-scale image segmentation model such as SAM and apply it to a resource-constrained setting like healthcare [4]. As seen in fig. 1, MedSAGa is more efficient than most SOTA models in both the training as well as inferencing stage (which uses weights+activations+others memory). By resource-constrained settings, we refer to the requirement of a substantially less number of annotated data to train or fine-tune the segmentation models and an inability to procure huge memory and computational resources required to train large-scale models. SAMed attempted to address this by integrating LoRA layers into its image encoder [26], however, it still requires huge memory and computational resources while facing the challenge of not utilizing the entire parameter matrix for training. Hence, in our work MedSAGa, we demonstrate a more refined approach to adapt SAM in resource-constrained settings by applying GaLore to all the parameters of the image encoder and then fine-tuning it, which significantly reduces the memory and computational cost while still maintaining full parameter structure. In addition to this, we fine-tune the prompt encoder and mask decoder without GaLore to perform improved semantic segmentation on medical images, as is demonstrated in SAMed [26]. We note here that since the prompt encoder and mask decoder in SAM are lightweight [26], applying Galore for fine-tuning them would not lead to a substantial improvement in memory consumption. Fig. 1 shows the memory utilization by MedSAGa and the other SOTA models while fine-tuning on medical images. Furthermore, as seen in fig. 2, it can be observed that after a specific number of images, the performance graph plateaus out supporting the idea to use few-shot learning in resource constrained settings giving very close performance to using the entire dataset while using a smaller proportion of images.\nSAM generates multiple segmentation masks to avoid ambiguity. However, we align MedSAGa with the working of SAMed and generate multiple segmentation masks, each representing a different tissue or segment of the anatomy in"}, {"title": "3.2. The Architecture", "content": "To reduce the resources required for utilizing SAM, we apply GaLore optimization to all the parameters of the image encoder. This approach harnesses the gradually changing low-rank structure of the gradient explained in the GaLore paper, which improves memory efficiency while still giving comparable results [28]. Instead of reducing the weight parameter size as is done in LoRA, GaLore projects the Gradient matrix at time t, Gt \u2208 Rmxn, into a low-rank matrix Gt, which can be represented by eq. 1.\n\n\u011et = Ptpt(P+GtQt)Q+ \u2022\n\nwhere Pt and Qt are projection matrices with dimensions Rmxr and Rn\u00d7r respectively, and pt is an element-wise stateful gradient regularizer. If Wo is the initial weight matrix, Wr represents the weight matrix at time T, and \u03b7 is the learning rate, then the gradient update rule in GaLore is as follows in eq. 2.\n\nWr = Wo + \u03b7 \u03a3Gt.\n\nIn our approach, we apply this Gradient low-rank projection to all the parameters of the image encoder which includes all the projection layers (q, k, v and o) as opposed to SAMed which applies LoRA only to the q and v projection layers in its best-performing model. We apply GaLore to all the parameters of the Image encoder as it adds only a negligible memory overhead. For the MedSAGa approach to function as an auto-segmentation model, we do not provide any prompts to the prompt encoder. Instead, we utilize the default embedding of the prompt encoder of SAM, which it uses when no prompt is given, and only fine-tune it during the training phase.\nThe mask decoder of SAM consists of a lightweight transformer layer and a segmentation head [26]. In our approach, we fine-tune the entire mask decoder directly without applying any optimization as it is already lightweight. Furthermore, as was developed in the SAMed architecture, we change the segmentation head of the mask decoder to adapt it to give precise semantic segmentation for each class of tissue or anatomy present in the image. Let us consider there are k classes in total including 1 background class in the medical image, the mask decoder of MedSAGa predicts k segmentation masks M \u2208 Rh\u00d7w\u00d7k, each corresponding to a single class in the image. We then further utilize a"}, {"title": "3.3. Training Strategies", "content": "For performing training, we utilize a combination of Cross Entropy loss and Dice loss, similar to that utilized in the SAMed approach as represented by the eq. 4.\n\nL = ALCE(M, D(S)) + (1 \u2212 1)LDice(M, D(S)),\n\nwhere L represents the net loss value, LCE represents the Cross Entropy loss, LDice represents the Dice loss and A represents the loss weight. D represents downsampling to align the resolution of the ground truth mask (S) with the Med-SAGa output, compensating for lower spatial resolution of MedSAGa.\nWarmup is applied in MedSAGa to stabilize the training process. By allowing the learning rate to increase gradually, we enable the model to slowly adapt the weights to the specific characteristics of the medical data, thereby avoiding poor convergence and instability early in the training phase and reducing the chances of overfitting [24].\nInstead of Adam optimizer, in MedSAGa we utilize the AdamW optimizer. AdamW decouples weight decay from the gradient updates and applies it directly to the weights. This method proves to be more effective in regularization, maintaining a better separation between the weight decay and the adaptive learning rate aspects of Adam [14]. In MedSAGa, the AdamW approach ensures that the regularization is not overly influenced by the learning rate adaptations specific to different weights."}, {"title": "4. Experiments and Results", "content": "We demonstrate the performance of MedSAGa through rigorous experimentation on 4 different medical datasets by comparing it to several baseline models."}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "Datasets. We utilize four different datasets covering different parts of the human anatomy for our experimentation. All the baselines and results that we present in the further sections have been tested on each of these 4 datasets to evaluate the robustness of the MedSAGa architecture. For each dataset, the number of few-shot images used for experimentation are chosen depending upon the size of the dataset and the number of classes.\nThe AMOS dataset is a large-scale clinical dataset of 500 MRI and 100 CT scans which consists of annotations of 15 abdominal organs [11]. Each slice was padded to obtain final slices of dimension 512 \u00d7 512. For the training set, we only considered the slices that had masks of at least 5 organs to overcome class imbalance. In total, 10,300 slices were satisfying the above criteria out of which we used 500, 1000, 2000, 5000, and 7000 slices for the few shot experimentation. The ChestX-ray8 dataset consists of 108,948 frontal-view X-ray images of 32,717 unique patients [22]. The images consist of 8 labels, mined from the text corpus of the corresponding radiological reports using NLP. We predict the segmentations for three classes whose masks were available: left lung, right lung and heart. Out of the 108,948 images in the dataset, we used only 50, 100, 200, 350, 500 and 1800 images as performance saturation was reached at quite an early stage (Refer Fig. 2). The Ischemic Stroke Lesion Segmentation (ISLES) dataset is a multi-center MRI dataset of acute to subacute stroke lesions [9]. It consists of 400 multi-vendor MRI images, each consisting of a number of slices having a dimension of 112 \u00d7 112. The DWI modality was used for experimentation and as a pre-processing step, thresholding was applied to only use slices containing lesions while training. For performing few-shot experiments, 200, 500, 700, 1000, 2000 and 3,200 (maximum number of slices obtained after thresholding) slices were used in the ISLES dataset. The Spleen dataset was retrieved from the Medical Segmentation Decathlon challenge [2]. It consists of 61 3D volume portal-venous phase CT scans from patients undergoing treatment for liver metastases. Out of these, we utilize only those images for training that have a mask available. Data leakage between the train and test sets was avoided by splitting the data according to the patients instead of according to individual slices for all datasets.\nEvaluation Metrics We use dice score and HD95 metrics to compare the model performances. Also, we show the memory utilized by the model for training and fine tunning."}, {"title": "4.2. Implementation details and evaluation metrics", "content": "All the experiments were run on a single NVIDIA RTX A6000 GPU with 48GB GPU RAM. A batch size of 12 was used for all the experiments. The warmup and loss weight strategies were the same as used in SAMed, i.e., the loss weights for cross entropy and dice loss were set to 0.2 and 0.8, respectively. For the warmup, the initial learning rate was set to 0.005, while the warmup period was set to 250. The B1, B2, and the weight decay of the AdamW optimizer were set to 0.9, 0.999, and 0.1, respectively. For the GaLore optimizer, AdamW was used as the base optimizer with a learning rate of 1 \u00d7 10-3, and \u1e9e\u2081 and \u1e9e2 were set to 0.9 and 0.999, respectively. The plane change rate (T) used was 200, as is recommended in the GaLore Paper. The"}, {"title": "4.3. Comparison with SOTA", "content": "The results of the memory utilization by MedSAGa and other baselines are depicted in Fig. 1. From this it is evident that the MedSAGa model utilizes the lowest memory as compared to the other SOTA image segmentation models, giving an average memory efficiency of 66% more as compared to current state-of-the-art (SOTA) models for medical image segmentation. Here, we would like to mention that"}, {"title": "5. Ablation Studies", "content": "We present our ablation studies in two categories. In the first part, we demonstrate two variations of integrating Gradient"}, {"title": "6. Limitations and Future Scope", "content": "The MedSAGa methodology leverages a large-scale model akin to SAM for few-shot image segmentation within resource-constrained environments, exhibiting a marked reduction in memory utilization compared to other SOTA models, while still yielding comparable segmentation performance. However, attaining optimal segmentation performance with MedSAGa may not always be feasible, as varying architectural designs may excel in capturing the distinctive features of diverse datasets. Additionally, as highlighted in the GaLore literature, further improvements in memory overhead can be achieved by reducing the projection layer dimensions via techniques such as quantization and streamlined parameterization, which can be done as part of the future work. Since this work presents SAM in culmination with GaLore (a gradient low-rank optimization technique) it can be applied to various end-to-end training techniques of models involving ViTs. Not just image segmentation, the use of GaLore alongside ViT can be interestingly used for a wide-variety of downstream tasks in a resource-constraint environments."}, {"title": "7. Conclusion", "content": "In our work, we demonstrate the use of large-scale models like SAM for few-shot medical image segmentation in resource-constrained settings like healthcare by using Gradient low-rank projection (GaLore) for fine-tuning the image encoder. This allows us to achieve significant memory efficiency while still utilizing full parameter training. Our rigorous experiments on diverse medical image seg-"}]}