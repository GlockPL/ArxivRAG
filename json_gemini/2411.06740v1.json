{"title": "Dockformer: A transformer-based molecular docking paradigm for large-scale virtual screening", "authors": ["Zhangfan Yang", "Junkai Ji", "Shan He", "Jianqiang Li", "Ruibin Bai", "Zexuan Zhu", "Yew Soon Ong"], "abstract": "Molecular docking enables virtual screening of compound libraries to identify potential ligands that target proteins of interest, a crucial step in drug development; however, as the size of the compound library increases, the computational complexity of traditional docking models increases. Deep learning algorithms can provide data-driven research and development models to increase the speed of the docking process. Unfortunately, few models can achieve superior screening performance compared to that of traditional models. Therefore, a novel deep learning-based docking approach named Dockformer is introduced in this study. Dockformer leverages multimodal information to capture the geometric topology and structural knowledge of molecules and can directly generate binding conformations with the corresponding confidence measures in an end-to-end manner. The experimental results show that Dockformer achieves success rates of 90.53% and 82.71% on the PDBbind core set and PoseBusters benchmarks, respectively, and more than a 100-fold increase in the inference process speed, outperforming almost all state-of-the-art docking methods. In addition, the ability of Dockformer to identify the main protease inhibitors of coronaviruses is demonstrated in a real-world virtual screening scenario. Considering its high docking accuracy and screening efficiency, Dockformer can be regarded as a powerful and robust tool in the field of drug design.", "sections": [{"title": "I. INTRODUCTION", "content": "N drug discovery, identifying the candidate compounds that target biological macromolecules remains challenging because of the long development time and expensive wet-laboratory experiments. Virtual screening using molecular docking approaches can significantly improve the initial hit rate of drug candidates with great diversity and high binding affinity [1], [2]. Recently, the number of synthesizable molecules in make-on-demand libraries has expanded from 3.5 million to 29 billion. The docking performance can steadily improve as the library size increases [3]. However, in large-scale virtual screening (LSVS) tasks, the computational cost and time of docking methods become major challenges that most researchers in academia and industry cannot overcome [4].\nTraditional docking approaches use scoring functions to measure the binding affinity of a given protein-ligand complex and then find the best binding conformation by applying optimization algorithms to minimize these functions [5]. For example, GOLD uses a genetic algorithm to search complex conformations [6], and AutoDock combines a genetic algorithm with a simulated annealing algorithm [7]. Although these optimization-based docking methods are commonly used in modern drug designs because of their good usability and interpretability, they still face the following challenges: the scoring functions are generally not precise enough, and optimization algorithms cannot guarantee that the global optimum is found each time. Although several advanced methods can offer reliable binding affinity predictions [8], [9], docking approaches require multiple independent optimization processes to sample possible binding conformations for each protein-ligand pair, leading to very high computational costs in LSVS tasks [2], [10].\nInspired by the groundbreaking advancement of AlphaFold2 in protein structure prediction [11], a series of deep learning (DL)-based methods have emerged to solve molecular docking tasks [12]. According to the types of neural network architectures, they can be divided into three categories: graph neural networks (GNNs)-based [13], transformer-based [14] and diffusion model-based docking methods [15]. The primary motivation of these studies is twofold: first, improving the ligand docking accuracy with the aid of the power learning capabilities of DL technologies, and second, speeding up the screening process by directly predicting ligand binding conformations to skip the time-consuming optimization of traditional docking approaches [16]. However, although tremendous efforts have been made to develop DL-based docking tools, few can perform well in docking accuracy and screening speed simultaneously due to inadequate generalizability and non-end-to-end architectures [17], [18]. Therefore, how to use DL models to generate protein-ligand binding conformations precisely and efficiently is still an open question in LSVS tasks.\nIn this study, a novel transformer-based architecture named Dockformer, which is shown in Fig. 1, is proposed. Dockformer uses two separate encoders to leverage multimodal information to generate latent embeddings of proteins and ligands and can thus effectively capture molecular geometric details, including 2D graph topology and 3D structural knowledge. A binding module is then employed to detect intermolecular relationships effectively on the basis of learned latent embeddings. Finally, in the structure module, the established relationships are utilized to generate the complex conformations directly, and the coordinates of the ligand atoms are calculated in an end-to-end manner. In addition, the corresponding confidence measures of each generated conformation are utilized to distinguish binding strengths instead of traditional scoring functions. Dockformer achieves superior docking accuracy compared with that of state-of-the-art DL-based and optimization-based docking methods and simultaneously speeds up the conformation generation process by orders of magnitude. Thus, this method is capable of meeting the rapid throughput requirements of LSVS tasks. Dockformer, as a robust and reliable protein-ligand docking approach, may significantly reduce the development cycle and cost of drug design.\nThe remainder of this paper is organized as follows: Section II introduces related works in molecular docking. In Section III, the architecture details of Dockformer are presented. Section IV analyzes docking performance and utilizes confidence metrics for large-scale virtual screening, while discussing optimization algorithms for physical plausibility. Finally, Section V reviews the use of AI technologies for screening large-scale compound libraries and discusses the potential of de novo drug design using generative models and deep reinforcement learning to streamline the screening process."}, {"title": "II. RELATED WORK", "content": "DL-based molecular docking methods can be divided into three main categories. The models in the first class encode proteins and ligands as graphs and use equivariant GNNs to predict intermolecular binding interactions [19]. For instance, DeepDock utilizes GNNs to construct a mixture density network, which is based on the distance likelihood of ligand-target node pairs and can act as a scoring function [20]. Then, DeepDock can accurately search complex conformations by optimizing the scoring function. EquiBind employs a SE(3)-equivariant GNN to detect the interactions between protein residues and ligand atoms, and uses gradient descent algorithms to determine the translation, rotation and torsion of binding conformations [13]. Similarly, TankBind uses a trigonometry-aware GNN to predict protein-ligand intermolecular distances. Then, it adopts a multi-dimensional scaling method to reconstruct the ligand atom coordinates based on the pair distances [21]. KarmaDock combines the methodologies of DeepDock and EquiBind, which utilizes a graph transformer neural network to learn pair distance distributions and employs E(n)-equivariant GNNs to generate binding conformations directly [22]. For molecular docking tasks, graph models can directly handle the structural geometry and effectively process the symmetry properties of molecular representations, enabling the movement direction and amplitude of ligand atoms to be updated in each message passing iteration. However, the oversmoothing issue results in the inadequate generalizability of these GNN-based methods. The performance of the methods has not yet reached that of conventional docking methods [17].\nThe models in the second class are based on transformer architectures, which can efficiently capture long-range dependencies among intra- and intermolecular tokens. For example, Uni-Mol has pioneered the use of atom and pair representations to encode ligands and protein pockets, and employed the self-attention layers with pair biases to share information between each representation. It generates the coordinates of ligand atoms via two distance matrices predicted by pairwise representations [14]. Inspired by Alphafold2, GAABind incorporates the additional triangular self-attention layers into the main architecture of Uni-Mol, which can capture the geometric and topological properties of binding pockets and ligands [23]. In addition, considering transformer models typically exist urgent requirements of enormous training data, CarsiDock and HelixDock customized large-scale complex structure datasets for pretraining and used the crystallized structure dataset for fine-tuning to improve their generalization abilities [24], [25]. Although these transformer-based models can achieve satisfactory docking accuracy, most still require independent optimization procedures to generate binding poses from the predicted interaction distance maps or distributions. Therefore, their inference processes remain time-consuming and are expensive for LSVS tasks [18]. In addition, these models output physically implausible conformations with steric clashes and incorrect bond lengths and angles since they ignore the essential topological information of molecules during the conformation generation process [12].\nUnlike the aforementioned models, which treat molecular docking as regression problems, models in the third class frame docking problems as generative modeling tasks. Specifically, DiffDock uses a denoising diffusion probabilistic model over the non-Euclidean manifold of ligand conformations, and then maps the manifold to predict the translation, rotation and torsion of ligands [15]. DynamicBind adopts an equivariant geometric diffusion network to construct a smooth energy landscape, which can be used to recover ligand conformations based on the unbound structures of proteins [26]. Furthermore, NeuralPLexer employed a diffusion-based generative model to predict complex structures by solely inputting protein sequences and ligand molecular graphs [27]. AlphaFold3, which was recently proposed, applies diffusion transformer models as decoders to simultaneously calculate each atom coordinate of proteins and ligands and yields very remarkable prediction accuracy in molecular docking tasks [28]. However, such generative models require the sampling of many noisy conformations for prediction, leading to very high computational complexity and slow docking speed. Despite the superior prediction performance of AlphaFold3, its inference time is longer than that of most docking approaches, making it incapable of virtually screening the billions of compounds in large-scale libraries."}, {"title": "III. DOCKFORMER", "content": "The essence of molecular docking lies in detecting non-bonded interactions between the atoms of a ligand and a protein pocket. The protein is conventionally regarded as a rigid body to simplify the calculations, and docking algorithms aim to generate the binding conformation of ligands within the protein-ligand complex on the basis of the predicted atom interactions. Consequently, the proposed Dockformer algorithm is designed to directly predict the 3D coordinates of all heavy atoms of the ligands for a given protein pocket. An architecture overview of Dockformer is depicted in Fig. 1. Dockformer consists of three stacked primary blocks. First, two independent encoders are used to encode the multimodal information of the ligand and binding pocket, and intramolecular interactions are exploited to produce their intrinsic representations. Second, a binding module captures intermolecular interactions between the binding pocket and ligand to generate the corresponding latent embeddings. Finally, the latent embeddings are fed into the structure module to predict the binding conformation of the ligand by considering the precise 3D coordinates of each atom."}, {"title": "B. Featurization Methodology", "content": "The network architecture simultaneously incorporates the 1D sequence, 2D graph and 3D geometry information of the ligand and protein pocket as inputs, enabling valuable insights from distinct modalities. Let $A^n = [A_1, A_2, ..., A_N]$ denote the atom features used to encode the sequence information, where N is the number of heavy atoms and $A_n$ represents the atom type of the n-th atom by using a one-hot encoding scheme. Additionally, 2D graph information is encoded as the chemical bonds and structural interconnections between atom pairs in the ligand. Two-dimensional graph pair features $\\Phi_{ij}$ contain two representations. First, $\\Phi_{ij}^{SPD}$ represents the connection feature, which uses the shortest path distance between atoms i and j to reflect their connection relation in the graph. Second, $Edge_{ij}$ records the edge feature to reflect the bond information. Denoting the edges along the shortest path of atoms i and j as $E_{ij} = (e_1, e_2, ..., e_n)$, the edge feature can be calculated by $edge_{ij} = MLP(\\Sigma_{e \\in E_{ij}}W_{edge})$, where $W_{edge}$ are the learnable parameters. Notably, both $\\Phi_{ij}^{SPD}$ and $\\Phi_{ij}^{edge} \\in R^{N \\times N}$ need to be calculated for the protein pocket, which is considered rigid during the docking process.\nNext, the 3D geometric information of the ligand or protein pocket is encoded as two representations. Regarding the atoms as point clouds, the first representation is the learnable global position embedding GPE; to reflect spatial information according to the 3D coordinate ${P_i^1, P_i^2, P_i^3}$ of each atom, where $i \\in 1,2,..., I$ and $I$ denote the dimensional number. GPE of the n-th atom can be calculated by\n$GPE_n = MLP(Concat(sine(P_i^1), sine(P_i^2), sine(P_i^3)))$, (1)\nwhere $sine$ uses sine and cosine functions to map each coordinate to a required position vector. These vectors are subsequently concatenated to generate a global position vector whose dimension is subsequently reduced to match the embedding dimension via a shadow multilayer perceptron (MLP). The calculation of sine can be described by\nsine(P_{i, i}) = \\begin{cases} sin(P_i / 10000^{2i / d}), \\text{ if i is even; }\\\\ cos(P_i / 10000^{2i / d}), \\text{ if i is odd. } \\end{cases} (2)\nThe second representation is the 3D pair feature $\\Phi_{ij}^{3D}$ to encode the geometric relation. The interatomic distance $d_{ij}$ between each atom pair i and j is calculated. Each element is encoded by K Gaussian basic kernel functions $N(d_{ij}; \\mu_{\\kappa}, \\sigma_{\\kappa}) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_{\\kappa}} exp(-\\frac{1}{2\\sigma_{\\kappa}^2} (d_{ij} - \\mu_{\\kappa})^2)$, where $\\kappa \\in {1, ..., K}$, $\\mu_{\\kappa}$ and $\\sigma_{\\kappa}$ represent the predefined mean and the standard deviation, respectively. The transformed distance is $\\hat{d}_{ij} = u_{ij} d_{ij} + v_{ij}$, where $u_{ij}$ and $v_{ij}$ are learnable parameters that share values for pairs of the same atom types. Finally, $\\Phi_{ij}^{3D}$ can be obtained via the nonlinear transformation of $N(d_{ij})$, described by\n$\\Phi_{ij}^{3D} = LeakyReLU(N(\\hat{d}_{ij}) W_1^{3D}) W_2^{3D}$, (3)\nwhere $W_1^{3D} \\in R^{K \\times K}$ and $W_2^{3D} \\in R^{K \\times 1}$ are the weights of the linear transformations and LeakyReLU is the activation function. The featurization methodology can effectively capture the intricacies and diversities of molecular structures, thereby enhancing the performance and generalization capabilities of the proposed model."}, {"title": "C. Encoder Modules", "content": "Two encoders are used to update the representations of both the ligand and protein pockets, which share the same architecture but have different weights. The architecture of the encoder module is depicted in Fig. 2. Specifically, the atom embeddings $A_n^0$ are initialized with the atom features $A_n$ and the global position embedding $GPE_n$ described by\n$A_n^1 = LayerNorm(Linear(A_n^0) + GPE_n), (4)\nwhere $LayerNorm$ and $Linear$ indicate the layer initialization and linear transformation operations, respectively. The pair embeddings $\\Phi_{ij}^0$ are initialized with the 3D pair features $\\Phi_{ij}^{3D}$ and the 2D pair features concatenating the connection feature $\\Phi_{ij}^{SPD}$ and the edge feature $\\Phi_{ij}^{edge}$, which is presented as follows:\n$\\Phi_{ij}^0 = Concat(\\Phi_{ij}^{SPD}, \\Phi_{ij}^{edge}) + \\Phi_{ij}^{3D}. (5)\nThen, the atom and pair embeddings are updated through modified multihead self-attention layers, which build attention weights for each atom and incorporate the current pairwise representation as an additional bias to provide the geometric and spatial information. The equations can be described as follows:\n$Q_n^{l,h}, K_n^{l,h}, V_n^{l,h} = Linear(A_n^{l}), (6)\nM_{ij}^{l,h} = Q_n^{l,h} (K_n^{l,h})^T / \\sqrt{d} + \\Phi_{ij}^{l}.$\nwhere $h \\in {1, ..., H}$ and where $H$ denotes the number of attention heads. $M_{ij}^{l,h}$ represents the attention weight matrix of the h-th head on the l-th layer, which needs to be refined further to learn the flexible molecule structure information in the ligand encoder. Therefore, a talking-head attention scheme [29] is leveraged to build a structural understanding of molecules across different modalities, presented as follows:\n$M^{l,h} = softmax(M^{l,h} W_e) W^T, (7)\nwhere $W_o \\in R^{H \\times H}$ and $W_s \\in R^{H \\times H}$ are learnable parameters. Finally, the updated atomic representations can be obtained as follows:\n$A_n^{l+1} = LayerNorm (A_n^l + MLP (Concat(\\M^{l,1}, ..., M^{l,h}, ..., M^{l,H}) W_o)), (8)\nwhere $W_{oh} \\in R^{d \\times N}$. Simultaneously, the pairwise representations consider the interactive relationships among atoms, which can be updated by concatenating the attention weight matrices directly:\n$\\Phi_{ij}^{l+1} = Concat_h{M_{ij}^{l,h}}. (9)\nFinally, $L_e$ encoder blocks are stacked to obtain the updated atomic and pairwise representations termed $A^{L_e}$ and $\\Phi^{L_e}$. $L_e$ is set to 15 for both encoders in the experiments."}, {"title": "D. Binding Module", "content": "Intramolecular interactions are used to update the atomic and pairwise representations through two separate encoders, whereas intermolecular interactions of atoms between the ligand and protein pocket are taken into account by a binding block. Similarly to the method in [14], the binding block employs a similar backbone design with encoders for the sake of simplicity. Fig. 3 presents the architecture of the binding module. The initialized atomic and pairwise representations of ligand-protein complex, denoted by $C^0$ and $\\Psi^0$, are generated by concatenating those of the ligand and protein pocket, described by:\n$C^0 = Concat(A^{L_e}_{ligand}, A^{L_e}_{protein}), (10)\n\\Psi^0 = Concat(\\Phi^{L_e}_{ligand}, \\Phi^{L_e}_{protein}),$\nwhere $C^0$ and $\\Psi^0$ are used as inputs of binding blocks, and the padding of $\\Psi^0$ is initialized as 0. Similarly, the complex atomic and pairwise representations are updated via Eqs. (8) and (9). $C^{L_b}$ and $\\Psi^{L_b}$ are achieved through $L_b$ stacked binding blocks, where $L_b$ is set to 4 in the experiments. Finally, the atomic and pairwise representations are disassembled and then retransformed to project into the 1-dimensional intra- and intermolecular distance matrices $D^{intra}_{ij}$ and $D^{inter}_{ik}$, which are calculated as follows:\n$d^{intra}_{ij} = W_{intra}LayerNorm(Concat(C^{L_b}_i, C^{L_b}_j, \\Psi^{L_b}_{ij})), (11)\nd^{inter}_{ik} = RELU(W_{inter} Concat(C^{L_b}_i, C^{L_b}_k, \\Psi^{L_b}_{ik})),\nD^{intra}_{ij} = W_{intra}LeakyReLU(d^{intra}_{ij}),\nD^{inter}_{ik} = W_{inter}LayerNorm(d^{inter}_{ik}),\nD^{Inter} = (D^{Inter} + (D^{Inter})^T)/2,$\nwhere i and j are the indices of the ligand atoms and k is the index of the atoms in the protein pocket. $W_{intra}$, $W_{intra}$, $W_{inter}$ and $W_{inter}$ are learnable parameters."}, {"title": "E. Structure Module", "content": "In previous works, most docking methods, including evolutionary and gradient descent algorithms, use geometry optimization approaches to generate binding conformations. These methods optimize the coordinates of each ligand atom by minimizing the error between the predicted distance matrices and the ground-truth distance matrices. However, these optimization methods are time-consuming and lack robustness because the standalone optimization process needs to be employed for each protein-ligand pair. In addition, the prediction of the binding conformation depends heavily on the precision of the predicted distance matrices, which might introduce more noise. Therefore, Dockformer uses an end-to-end prediction method to generate 3D coordinates of ligand atoms through intra- and intermolecular structure modules, which separately capture potential structural information from ligand-to-ligand and protein-to-ligand interactions, respectively. Specifically, the final complex atomic and pairwise representations are fed into the modules to predict the translation of each ligand atom and update their corresponding coordinates. As illustrated in Fig. 4, the difference between these modules is that the intramolecular module uses self-attention layers, whereas the intermolecular module adopts cross-attention layers. The atomic and pairwise representations of both ligands and proteins can be updated via Eqs. (8) and (9). The coordinates $P_i^l$ of ligand atom i in the l-th layer in both modules can be subsequently calculated from the updated representations as follows:\n$a^{intra, l}_{ij} = Linear (Linear(a^{intra, l-1}_{ij}) + M^{intra, l}_{ij}),\na^{inter, l}_{ik} = Linear (Linear(a^{inter, l-1}_{ik}) + M^{inter, l}_{ik}),\nP_i^{intra, l+1} = P_i^l + \\sum_{j=1}^n a^{intra, l}_{ij} \\frac{P_i^l - P_j^l}{||P_i^l - P_j^l||^2} + (12)\n\\sum_{k=1}^m a^{inter, l}_{ik} \\frac{P_i^l - P_k^l}{||P_i^l - P_k^l||^2}.$\nwhere $a^{intra}$ and $a^{inter}$ are the score matrices and where $M^{intra}$ and $M^{inter}$ denote the attention matrices of the attention layers in both modules. $l \\in 1,..., L_s$, where $L_s$ indicates the number of stacked layers and is set to 8 in the experiments."}, {"title": "F. Loss Functions", "content": "The training process of Dockformer is divided into two phases. First, the outputs of the binding module are mapped to predict the distances between atom pairs. The loss function $L_{dist}$ of the predicted intra- and intermolecular distances against the corresponding ground truth distances can be quantified as follows:\n$L_{dist} = L_{intra_{dist}} + L_{inter_{dist}},$\n$L_{intra_{dist}} = \\frac{1}{2N^2} \\sum_{i,j}(D^{intra}_{ij} - D^{intra}_{ij}^{GT})^2,$\n$L_{inter_{dist}} = \\frac{1}{NM} \\sum_{i,k}smooth_{L1}(D^{inter}_{ik} - D^{inter}_{ik}), (13)\nwhere $D^{intra}$ and $D^{inter}$ denote the ground truth intramolecular and intermolecular distances, respectively. N and M are the atom numbers of the ligand and protein products, respectively, and $i,j \\in {1, ..., N}$ and $k \\in {1,...,M}$ are the atom indices. An L2 loss function is employed for minimizing the intramolecular distance error, whereas a robust L1 loss function $smooth_{L1}(x)$ is used for minimizing the intermolecular error [30], presented as:\nsmooth_{L1}(x) = \\begin{cases} 0.5x^2, & |x| < 1;\\\\ x - 0.5, & otherwise. \\end{cases} (14)\nAfter the first training phase, the encoders and binding module can produce effective latent embeddings for both the ligand and protein pockets by considering the interactions between each other. Since the structure module can generate complex conformations in an end-to-end manner, the loss function $L_{coord}$ with respect to the coordinates of the ligand atoms against the corresponding ground truth coordinates is incorporated during the second training phase, described by\n$L_{coord} = \\frac{1}{N} \\sum_i^N (P_i^l - P_i^{GT})^2, (15)\nwhere $P_i^l$ is the output coordinate of the structure module and where $P_i^{GT}$ denotes the ground truth coordinate of the i-th atom in the cocrystal structure. Because the structure module will be recycled R times, only the coordinate loss of the last iteration $L^R_{coord}$ is used to refine the complex conformation without extra computational cost.\nIn addition, considering that Dockformer was developed to screen small-molecule compounds for a specific target protein virtually, allocating confidence assessment indicators for each generated ligand\u2013protein complex conformation will be constructive. Inspired by the confidence measure in AlphaFold2 [11], the distance difference test $DDT_{true}$ between the predicted distance $D_{ij}$ and ground truth distance $\\bar{D}_{ij}$ is used to calculate the target confidence of the predicted conformation.\nDDT_{true} = \\frac{100}{4} \\sum_{t \\in {0.5,1,2,4}} \\frac{\\sum_{d_{ij} < 8} \\mathbb{1}(|D_{ij} - \\bar{D}_{ij}| < t|)}{\\sum_{d_{ij} < 8} \\mathbb{1}}, (16)\nwhere $D_{ij}$ is obtained through the same projection head with atomic representations $C^{L_b}$ and pairwise representation $\\Psi$ in Eq. (11). The confidence indicator $L_{conf}$ can be defined as follows:\n$L_{conf} = \\sum_{ij} PDT_{ij}log \\bar{P}_{DDT_{ij}} + \\sum_{ik} PDT_{ik}log \\bar{P}_{DDT_{ik}}, (17)\n\\bar{P}_{DDT} = onehot(DDT_{true}),\nPDDT = softmax(MLP(\\Psi_{ij})).$"}, {"title": "G. Benchmark Datasets and Implementation Details", "content": "Similarly to most DL-based docking approaches, Dockformer is trained with the latest version of the well-established PDBbind V2020, which includes the cocrystal structures and the corresponding experimentally determined binding affinities of 19443 protein-ligand complexes released before 2020 [31]. The dataset is divided into training and validation sets with a partition ratio of 9:1, using the same filtering protocol provided in Uni-Mol [14]. In addition, the core set of PDBbind (also termed CASF-2016), which contains 285 hand-curated high-resolution complexes, is used to evaluate the docking ability of Dockformer. Dockformer is further evaluated on an independent test dataset named PoseBusters, which was recently developed and includes 428 crystal complexes released since 2021 [12]. By using this dataset, overlap between the training and test datasets is avoided, enhancing the rationality and reliability of the evaluations. In addition, for the proposed Dockformer, the pockets are used as Dockformer's inputs instead of entire proteins to reduce the computational complexity because the binding sites of target proteins have been well studied in most virtual screening tasks. Even without exact pockets, some prediction algorithms can also effectively identify potential binding sites of proteins. Dockformer is trained using eight NVIDIA RTX A6000 GPUs and a 128-core Intel(R) Xeon(R) Gold 6338 CPU @ 2.00 GHz. In both training phases, the adaptive moment estimation (Adam) optimizer is used to minimize the loss functions smoothly with a learning rate of $10^{-4}$, and an early stopping mechanism based on validation error is employed to prevent overfitting with predefined patience of 20 epochs."}, {"title": "IV. EXPERIMENTS", "content": "In this section, the docking accuracy of Dockformer was evaluated on the PDBbind core set and PoseBusters dataset and compared with that of five commonly used optimization-based docking algorithms, namely, GOLD [6], Glide [32], LeDock [33], AutoDock Vina [34] and Mini Vina, and nine state-of-the-art DL-based docking approaches, i.e., DeepDock [20], EquiBind [13], TankBind [21], DiffDock [15], Uni-Mol [14], KarmaDock [22], GAABind [23], CarsiDock [24] and Umol [35].\nDL-based docking methods are susceptible to the input training sample distrubtion, and changing the search space dramatically decreases the model capacity. Therefore, the trained models provided by the official repositories with the default search spaces were used in the comparison study. Specifically, since EquiBind, DiffDock, TankBind and Umol are trained for blind docking tasks, their search spaces cover the entire crystal protein. The binding pockets of Uni-Mol, GAABind and Dockformer are defined as the protein residues within the range of 6\u00c5 from any heavy atom of a crystal ligand, whereas those of KarmaDock and CarsiDock are considered the protein residues within the range of 12\u00c5 and 5-7\u00c5, respectively. DeepDock considers the protein surface mesh nodes within 10\u00c5 of any crystal ligand atom as inputs. In addition, the number of binding pocket boxes was set to 12\u00c5 for all conventional docking methods. The root mean square deviation (RMSD), which measures the geometric similarity between the predicted binding conformations and the crystal structures of the ligands, was used to evaluate the docking approaches. Generally, binding pose predictions are considered successful when their RMSDs are below the threshold of 2.0\u00c5 [36]."}, {"title": "B. Computational Complexity versus Accuracy", "content": "In addition to accuracy, computational complexity is an important performance indicator that requires attention, especially when the screening compound library becomes extremely large. Most docking methods cannot traverse the entire library within an acceptable running time. As depicted in Fig. 6, Dockformer yields the highest docking accuracy and requires the least amount of time using the PoseBusters dataset among all the docking methods. Although Dockformer has the largest DL network, end-to-end binding conformation generation results in low computational complexity in the inference process. Both LeDock and CarsiDock can also achieve competitive docking accuracies but with a long inference time. Thus, these approaches are unsuitable for LSVS tasks. Moreover, most DL-based docking approaches are more efficient than optimization-based approaches are but sacrifice gains in accuracy. In addition, the docking accuracy of DL-based methods improves as the size of the network architecture increases.\nThe computational cost of optimization-based docking methods is substantially greater than that of DL-based methods. The adopted stochastic optimization algorithms require more computing resources to determine the position, orientation, and torsion angles of each ligand conformation according to the scoring functions. The DL-based methods, including EquiBind, TankBind, Uni-Mol, GAABind and CarsiDock, use DL algorithms to construct intra- and intermolecular distance maps and then adopt optimization algorithms to calculate the coordinates of ligand atoms for binding pose generation. These methods cannot abandon independent optimization procedures for each ligand, which are faster than those of optimization-based docking methods but still time-consuming because of the iterative gradient descent processes. Only Dockformer and KarmaDock use end-to-end network architecture modules to generate binding conformations in a batch fashion. These modules have prominent advantages in docking efficiency. A modified version of Dockformer termed Dockformer Raw was also evaluated in this experiment. Dockformer Raw uses the same geometry optimization strategy as that used in TankBind to predict binding conformations.\nThe experimental results demonstrate that Dockformer Raw performs worse in terms of both accuracy and inference time. This finding highlights that the superiority of end-to-end structures can effectively mitigate the computational burden of iterative optimization. Notably, AlphaFold3 achieves a success rate of 90.2% on the PoseBusters dataset, a rate much higher than that achieved of each the docking methods mentioned in our experiments. However, even in the case of the least number of tokens, the inference time of AlphaFold3 is 22 s on 16 A100 graphics processing units (GPUs). This time is much longer than that of most competitors because the model framework of AlphaFold3 is much larger [28], and it uses a stepwise denoising method to decode the atom coordinates of the whole complex structure with a diffusion transformer. However, most researchers in academia and industry cannot afford such computational costs. Note that AlphaFold3 was not included in the comparison studies in this work because the source code is not available."}, {"title": "C. Confidence Assessment", "content": "Most DL-based approaches cannot be applied to virtual screening tasks directly because they can generate only the binding conformations but cannot predict the binding strengths of these conformations. These approaches are usually aided by well-established scoring functions, increasing the computational complexity of screening [24]. To avoid this disadvantage, DeepDock learns a statistical potential based on the distance likelihood, and KarmaDock trains mixture density networks to learn intermolecular distance distributions as scoring functions. Empirical evidence has demonstrated that such learned scoring functions lead to more powerful screening performance than conventional physics-based methods do [20], [22].\nSimilarly, Dockformer allocates confidence assessment indicators for each generated complex conformation following the protocol of AlphaFold2. The target confidence of the predicted conformation is estimated via the distance difference test between the predicted and ground truth distances and can be used to describe the binding strengths between proteins and ligands for virtual screening, which is described in Section III-F. To verify the effectiveness of the confidence measures, a scatter diagram of the generated complex conformations with the corresponding confidence indicators and RMSD values is illustrated in Fig. 7(a). We find an apparent linear relationship between confidence and RMSD, represented by the orange line, through a simple linear regression method. The results suggest that higher confidence indicates lower RMSDs of the generated conformations.\nIn addition, the effectiveness of the confidence indicators was verified by distinguishing the strong and weak binders. The predicted binding poses are allocated positive or negative labels, depending on whether their RMSDs are above or below the threshold of 2.0\u00c5. The confidence indicators were subsequently used to classify these conformations, and the receiver operating characteristic curves are presented in Fig. 7(b). Areas under the curve (AUCs) of 0.7506 and 0.7438 are achieved on two benchmark datasets. These values are much larger than 0.5000, indicating the powerful classification performance of the confidence indicators. Therefore, on the basis of these confidence indicators, Dockformer can be applied to large-scale virtual screening tasks without the need for additional scoring functions."}, {"title": "D. Large-scale Virtual Screening Task", "content": "In this section, Dockformer is applied to a real-world virtual screening scenario to verify its screening power. Considering that COVID-19 has challenged economic and health care systems worldwide, Dockformer was utilized to screen potential drug candidates for this disease with high transmission and mortality rates. The main protease Mpro, whose binding site is highly conserved among all coronaviruses, was selected as the target protein and can serve as a drug target for the design of broad-spectrum inhibitors [37", "38": [39], "40": "."}]}