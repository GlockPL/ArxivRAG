{"title": "Statically Contextualizing Large Language Models with Typed Holes", "authors": ["ANDREW BLINN", "XIANG LI", "JUNE HYUNG KIM", "CYRUS OMAR"], "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary\nLLM-based code completion systems often hallucinate broken code because they lack appropriate code context,\nparticularly when working with definitions that are neither in the training data nor near the cursor. This\npaper demonstrates that tighter integration with the type and binding structure of the programming language\nin use, as exposed by its language server, can help address this contextualization problem in a token-efficient\nmanner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into\nthe Hazel live program sketching environment. The Hazel Language Server is able to identify the type and\ntyping context of the hole that the programmer is filling, with Hazel's total syntax and type error correction\nensuring that a meaningful program sketch is available whenever the developer requests a completion. This\nallows the system to prompt the LLM with codebase-wide contextual information that is not lexically local\nto the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's\ngoal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language\nserver, which provides error localization and error messages. To evaluate these techniques, we introduce\nMVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have\nbeen written from scratch to avoid data contamination, and that can easily be ported to new languages because\nthey do not have large external library dependencies. These applications serve as challenge problems due to\ntheir extensive reliance on application-specific data structures. Through an ablation study, we examine the\nimpact of contextualization with type definitions, function headers, and errors messages, individually and in\ncombination. We find that contextualization with type definitions is particularly impactful. After introducing\nour ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench\nto TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages.\nFinally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language\nservers can implement to expose capabilities that AI code completion systems of various designs can use to\nincorporate static context when generating prompts for an LLM.", "sections": [{"title": "1 Introduction", "content": "Recent advances in generative AI have triggered an avalanche of new AI programming assistants\u2014\nthe most prominent is Copilot [23], but it has many competitors\u2014that generate code completions\nby prompting a large language model (LLM) pre-trained on a corpus of diverse natural language\ndocuments as well as code written in various programming languages [7, 17, 17, 49, 73, 78]. Once\ntrained, an LLM iteratively transforms an input token sequence, called the prompt, into next-token\nprobability distributions from which completions are sampled. LLMs are able to learn statistical\nregularities in the training data [29], with limited reasoning abilities emerging as LLMs scale up in\nsize [89]. As a result, AI assistants have become capable enough to substantially impact developer\nproductivity [9, 61, 74, 88]. For example, one study reports a 50% increase in productivity when\nusing Copilot [88]. The impact is particularly pronounced for developers working with high resource\nlibraries and languages, i.e. those well-represented in the training data [78].\nContemporary Al assistants construct the prompt primarily using the program text appearing in\na textual window around the developer's cursor (the cursor window) [66]. This approach leads to\npoor performance in situations where critical task-relevant context comes from definitions that\nappear neither in the cursor window nor in the training data (the semantic contextualization\nproblem) [2, 19, 38, 60, 67, 77, 82, 83]. For example, consider the following cursor window, which\nwould arise when a developer is implementing a GUI component using the model-view-update\n(MVU) architecture (central to popular GUI application frameworks like React [8] and Elm [18]):\n(* update the room booking data after a user action *)\nfunction update (model: Model, action: Action): Model {\nCorrectly completing this function definition requires knowing the definitions of this specific\ncomponent's Model and Action types, which commonly appear in different files in the repository\nand therefore outside the cursor window. Various other files might also contain relevant definitions,\ne.g. other types that can be reached from the definitions of Model and Action, and useful helper\nfunctions for working with values of these types. Without access to these definitions, an LLM will\neither be unable to generate sufficiently probable completions (which may result in no completion"}, {"title": "2 Static Retrieval and Error Correction in the Hazel Assistant", "content": "We first introduce Hazel and the Hazel Assistant by example from the developer's perspective.\nWe continue by describing how the Hazel Assistant prompts GPT-4 and interfaces with the Hazel\nLanguage Server to generate code completions augmented with static retrieval and error correction.\nThen, we introduce the MVUBench benchmark suite and report the results of an ablation study of\neach of these features. We investigate their relative contributions to the overall performance of the\nassistant on these high-context MVU tasks, relative to various baselines that establish bounds on\nperformance."}, {"title": "2.1 Hazel", "content": "Hazel is a web-based live functional programming environment that features total syntax and type\nerror recovery via automatic hole insertion [47, 87]. This ensures that every editor state in Hazel is\na semantically meaningful program sketch and that Hazel's various editor services, include code\ncompletion, never experience gaps in service [52]."}, {"title": "2.2 Hazel Assistant", "content": "The Hazel Assistant is a programming assistant that generates code completions by two mechanisms.\nTo provide fast, local completions, the Hazel Assistant generates type-directed completions [12],\nusing localized syntactic and static information to inform small completions with type-directed\nlookahead as shown in Figure 4(a-b). This feature can be invoked even when there are syntax errors\nbecause Hazel tracks syntactic obligations in a backpack, e.g. as shown in Figure 4(b) where both\n= and end are necessary to complete the case expression [47].\nTo request an LLM completion from the Hazel Assistant, the developer can fill any expression\nhole with ?? which starts to animate as suggested in Figure 3(c). GPT-4, our underlying model\nin this section, is not particularly fast as of this writing, so the developer can continue to edit\nelsewhere while waiting for GPT-4 to return a completion. For this example, Figure 5 shows an\nexample of a GPT-4-generated completion. The developer can inspect this completion (which would\ndisplay any type errors found) and accept it with the Tab key."}, {"title": "2.3 The Hazel Assistant Trialogue", "content": "Our generative hole filling process consists of the following steps, construed here as a \u2018trialogue'\nbetween programmer, the Hazel Language Server, and a language model. This is depicted in Figure 1\nas a series of chat messages. In our setup, the primary interaction is between the language model\nand the Hazel Language Server, which acts on behalf of the user in response to a request for LLM\nhole filling, kicking off the following exchange. Here we use the messaging terminology from the\nOpenAI Chat API [54], which distinguishes System, User, and Model messages:\n(1) System Message: Hazel Crash Course and few-shot examples\n(2) User Message: Program sketch augmented by static retrieval\n(i.e. relevant semantic context from the language server)\n(3) Model Message: Suggested hole filling\n(4) User Message: Syntax and type errors in the completion, if any\n(5) Model Message: A corrected completion, if necessary\nWe repeat steps 4-5, i.e. we perform syntactic and static error correction when needed, stopping\nafter at most two iterations to limit latency."}, {"title": "2.4 System Message: The Hazel Crash Course", "content": "The system message is generic, common to each prompt. For an instruction-tuned model (GPT-4),\nthe system message consists of three sections:\nFirst, we provide a list of instructions delineating the task. In particular, we instruct the model to\nprovide a code fragment to replace a sentinel value representing the target hole in the program\nsketch. For example, the model is given the instructions:\n\u2022 \"Reply only with code\"\n\u2022 \"DO NOT include the program sketch in your reply\"\nSecond, an informal specification of Hazel syntax with emphasis on \u2018negative characterization',\nlisting differences from syntactically-similar higher-resource languages. As this kind of \u2018prompt\nengineering' is as-yet a task-sensitive and inexact process, this section, along with the one above,\nwas constructed though an ad-hoc process of discovering repeated syntactic errors in model output.\nFor example:\n\u2022 \"No 'rec' keyword is necessary for 'let' to define a recursive function\"\n\u2022 \"There is no dot accessor notation for tuples; use pattern matching\""}, {"title": "2.5 Type Retrieval", "content": "We augment this sketch with additional static information obtained via the Hazel Language\nServer, serialized into text, displayed as a kind of projected view of the codebase a static program\nslice contextualized to the relevant program hole. Specifically, we retrieve the following static\ninformation (diagrammed in Figure 6):\n\u2022 Type Retrieval: The expected type at the cursor, along with the definitions of any types aliases\noccurring in that type, and the definitions of aliases occurring in that definition, and so on\nrecursively until we arrive at base types.\n\u2022 Header Retrieval: A selection of values, annotated with their types, filtered from the typing\ncontext based on a type-directed metric of relatedness to the expected type described below.\nWhile we use Hazel to illustrate our approach, our goal is to outline an approximate API which\ncould be implemented by any language server for a typed language which could drive a similar\nsystem in another language. We'll define the approximate methods for such an API as we go, and\nlater collect them in section 5."}, {"title": "2.5.1 Relevant Type Definitions", "content": "Given the above program sketch, the expected type of the hole\n?? is (Model, Action) -> Model. While in this example, adding the expected type to the prompt\nis strictly redundant, as it already appears as the function's type annotation, in general Hazel's\nbidirectional type system [51, 87] allows a similar expected type to be extracted in any position for\nwhich there exists type constraints, such as in function argument position, or in a module signature\nincluding update.\nAbsent this sort of context, this type is elucidatory on its own. Based on the provided comment,\na language model might and likely will \u2018guess' that these refer to the state and state changes of an\nModel-View-Update application. But as we shall see, it is unlikely to guess the precise structures of\nthe types the programmer has actually used. An example demonstrating the common case is show\nin Figure 7:\nHence we do automatically what a programmer in an unfamiliar codebase might do manually:\nrecursively pursue type definitions to unwind the local semantic context hinted at by the type\nexpectation. Providing this list to a language model is analogous to a human using the IDE to hover\nover types and jump iteratively to their definitions. First, we extract relevant type aliases:\n\u2022 Model\n\u2022 Action\nThen, we retrieve their definitions:\n\u2022 type Model = (Grid, Emoji, [Emoji])\n\u2022 type Action = SelectEmoji(Emoji) + StampEmoji (Row, Col) +"}, {"title": "2.6 Relevant Headers from the Typing Context", "content": "In addition to relevant type definitions, we augment the prompt with the names and types (which\ntogether we term the headers) of relevant values \u2013 typically functions \u2013 from the typing context.\nFrom a user interface perspective, this is analogous to a type-directed autocomplete menu.\nOur extraction method divides into three stages:\n(1) Use the expected type to identify a list of target types\n(2) Filter the typing context for values with types related in a certain way to these target types\n(3) Assign scores to each element of the resulting list, and return the prefix of that list truncated\nat some scoring and length thresholds (here, score > 0.0 and 10 items respectively)\nThe resulting context entries are formatted as code sketches, again to facilitate language model\ningestion. For example the pair (string_of_int, Int -> String) is formatted as:\nlet string_of_int : Int -> String =\nin\nHere, the body of the definition is simply omitted. Interestingly, we originally used ellipsis (...) \nin place of the body, but this resulted in an increased chance the model (especially the smaller\nStarCoder2 model) would itself emit the token ... in lieu of a full completion."}, {"title": "2.6.1 Identification of Target Types", "content": "First, we deconstructing the expected type to identify relevant\nsub-components which could be used, in conjunction with their relevant elimination forms, to\nconstruct the target. Our initial target type is simply the type of the hole itself:\nTarget types = (Model, Action) -> Model\nThen, if that type is a compound type, we consider its components. In particular, if the type is a\nproduct type, we consider its components to be targets, and if the type is an arrow type (as it is\nhere), we consider its return type to be a target.\nTarget types = (Model, Action) -> Model, Model,\nIn principle, we could continue this deconstruction recursively indefinitely, but for our immediate\npurpose of identifying likely-relevant types, we've found it suffices to extend one more iteration;\nthat is, product/arrow types containing product/arrow types. It simplifies our calculations to\ninternally normalize all type definitions (Hazel is structurally typed). Here, we will only do so\nopportunistically for clarity of presentation:\nType of hole = ((String, (Grid, Emoji, [Emoji])), Action) -> (Grid, Emoji, [Emoji])\nTarget types = (Model, Action) -> Model, Model, Grid, Emoji, [Emoji]\nOne can likely see other ways of extending target type extraction. Possibilities included destruc-\nturing more compound types such as records, or for function types, also considering the input\ntypes as a kind of negative target, in that we may want to prioritize types that consume a relevant\ntype from the local context. For now we proceed with the simple approach outlined.\nWe do not, however, return unaliased base types such as Bool or String as target types. Early\nexperimentation indicated that, given that there are typically many standard library functions on\nbase types, often with no a priori way to distinguish their relevance based on types, such functions\nwould often act as confounders, since which happened to be included was incidental. In practice, a\nstandard library would already be well-understood by an LLM from the pre-training or fine-tuning\nstep. For Hazel, we replicate much of the OCaml standard library to sidestep this need."}, {"title": "2.6.2 Filtering the Context", "content": "For each target type, we filter the typing context to retrieve types\nwhich can be used, again in conjunction with appropriate elimination forms, to produce the target.\nThis is essentially similar to target type extraction. In particular, we return types which are\nconsistent with the target, arrow types whose return type is consistent with the target, and product\ntypes whose have a component consistent with the target. For example:\nTarget type Grid yields\n\u2022 updateGrid: (Grid, Row, Col, Emoji) -> Grid\n\u2022 clearGrid: Grid -> Grid\n\u2022 fillRowInGrid: (Grid, Row, Emoji) -> Grid"}, {"title": "2.6.3 Sorting and scoring the filtered context", "content": "Prior work [12] has surveyed various ways in which\nsemantic information can be used to sort typing-context-originating suggestions for relevance. For\nour purposes here we use a simple scheme intended as a proof-of-concept to establish a baseline\nfor more sophisticated methods.\nBy default, Hazel context entries are sorted by locality of definition, which provides a reasonable\ndefault for relevance. Thus we sort stably with respect to the locality ordering for context entries\nhaving the same score."}, {"title": "2.7 Syntactic and Semantic Error Correction", "content": "The use of instruction-tuned language models makes available a lightweight form of program\nrepair based on an iterative loop of generating completions and retrieving error messages from\ncompilers or static analyzers. The general technique of looping LLM code generation on compiler\nerrors appears to have emerged in tandem with early LLM code generation experiments [59] [84]\nand has been examined in greater detail by Joshi et al. [33].\nAfter receiving a response from the model, we substitute the received completion into the\noriginal program sketch. We then query the Hazel Language Server to parse the resulting program.\nHazel parsing is strongly incremental, enabling the (partial) type-checking of programs even in the\npresence of unrecognized or missing delimiters. We then query the language server for a list of\nstatic errors, which include syntax and type errors. If there are any such errors, we serialize them\nto a string, and send them to the language model.\nIn order to maintain model context, we append the errors to a growing log of messages beginning\nwith the original prompt. The number of correction rounds which can be performed in this way\nis thus limited by the length of the context window; in our case, using the 8k token window of\nGPT4-0613, we are effectively capped at 5 rounds. However, we have noticed that 2 rounds are\noften sufficient to eliminate static errors, and that rounds in excess of 2 tend to show diminishing\nreturns, so we have capped the maximum number of rounds at 2.\nTo support this in another language, its language server must be able to localize static errors,\nreporting locations and error messages. Ideally, it would produce a list of errors, rather than just\nthe first error encountered, as is supported by Hazel's total type error localization and recovery\nsystem [87]. This could be achieved by implementing the following method:"}, {"title": "2.8 Experimental Evaluation", "content": "We now evaluate the effectiveness of this method of proactive static contextualization and retro-\nspective correction for LLM code completion."}, {"title": "2.8.1 MVUBench", "content": "Hazel is a low-resource language, so we are unable to conduct an at-scale\nevaluation in this context. Instead, as previously motivated, we construct a benchmark suite of five\nMVU applications including the EmojiPainter example from the previous sections:\n\u2022 Todo (TO): Maintains a list of tasks\n\u2022 Room Booking (BO): Manages a room booking schedule\n\u2022 Emoji Painter (EM): Paints emoji stamps on a small canvas\n\u2022 Playlist Manager (PL): Manages a music playlist\n\u2022 Password Strength Checker (PA): Rates a password via a dynamic set of properties\nThe baseline program sketch provided to the language model for each of these programs is\nsimply the type-annotated function header for its corresponding update function, along with a\nsingle-line comment describing that function's purpose, including the name of the application, in\nline with the running example.\nEach application also comes with a simulated repository containing relevant (and less relevant)\ntype and utility function definitions.\nWe also provide a small test suite for each example, consisting of 10-15 tests ensuring that each\nMVU action behaves as a user might reasonably expect without additional specification.\nIn such a situation, a naive language model completion would be informed only by the update\nfunction type aliases (which are often generic terms such as Model) and the single-line comment\n(which only hints at the intended functionality). While it is still possible that in very typical\nsituations, the model might correctly guess appropriate types and names, more likely (as we shall\nsee) it will hallucinate plausible-but-incorrect completions. By varying the methods through which\nadditional context is provided, and the corrective methods applied to resulting completions, we\nprovide a baseline analysis for the relative effects and interactions of these methods on LLM code\ncompletion."}, {"title": "2.8.2 Feature Ablation Experiment", "content": "Our main experiment consisted of 320 completions trials, each\nof which makes between one and three calls to the language model. These 320 trials divide as\nfollows:\n\u2022 8 feature ablation configurations\nType Retrieval: Whether to include expected type and type definitions\nHeader Retrieval: Whether to include relevant headers from the typing context\nError Rounds: Whether to perform up to 2 static error correction\n\u2022 5 program sketches (TO, BO, EM, PL, PA)\n\u2022 20 completion trials per combination (to account for model non-determinism [56]). We\nran these experiments at temperature 0.6 (a hyperparameter effecting the stochasticity of\ntoken sampling), selected based on trial experiments as a balance between noisy variance\nand producing a range of interestingly distinct completions"}, {"title": "2.8.3 Comparison Baseline 1: No Context", "content": "The ablation configurations lacking all static retrieval\nfeature serves as a lower bound baseline \u2013 without any context except the brief comment on\nthe update function, we would expect even high-performing models to perform poorly due to\nlack of context. This is the current reality for AI programming assistants that do not attempt\nrepository-level retrieval."}, {"title": "2.8.4 Comparison Baseline 2: Exhaustive Retrieval", "content": "An additional baseline configuration, beyond\nthose outlined above, is to perform exhaustive retrieval of all application code, excluding tests, up\nto the context window limit. This serves as a token-inefficient upper bound on performance."}, {"title": "2.8.5 Comparison Baseline 3: Vector Retrieval with Confounds", "content": "Finally, we compare our approach\nto vector retrieval. Given that our test corpus consists of 5 relatively small programs, we have\nemulated a larger more realistic codebase by combining those five programs, minus tests and\nupdate functions, along with Hazel's standard library, to create a 1000-line simulated codebase from\nwhich context can be drawn. Combining these programs has the effect of creating some possible\nlexical confounders, e.g. two types having the same name; we contend that this construction, albeit\nsynthetic, nonetheless emulates a legitimate source of confusion for a scope-unaware method like\nvector retrieval.\nWe have used the simplest standard RAG strategy, uniformly dividing the codebase into 150-\ncharacter chunks, which were submitted to OpenAI's Ada (text-embedding-ada-002), a commercial\nembeddings model [49]. The retrieved 1536-element vectors, along with their associated text chunks,\nare then stored locally in a JSON file acting as a basic vector database.\nIn order to retrieve chunks relevant to a provided sketch (our function headers and comments),\nwe submit that sketch to the same API endpoint, and then search our vector database for the top 6\nchunks with the highest cosine similarity [49].\nThe above parameters (150 character chunks, 6 entries) are chosen so that the total (900 characters)\nlines up with the average length of the total static retrieval context (types + relevant context) for\nour 5 examples, with the chunk size being set as small as possible while still being able to fully\ncontain most type definitions in our corpus.\nIt should be noted that there exist a variety of more advanced chunking strategies which may\nyield better results, including overlapping windows, chunks aligned to inferred authorial intent [76],\nand semantic chunking which takes into account source syntax. However, all these strategies have\ncomplex trade-offs which take us beyond our immediate comparative goals; for example, chunking\nby top-level definitions (a language-aware approach) might prevent issues with a poorly-truncated\ndefinition being included in a prompt, but seeing as definitions can range widely in size, being\nforced to include an entire definition may prevent multiple chunks which are together more relevant\nfrom being included.\nAs such, we have elected to leave the RAG baseline structurally agnostic, so as to more cleanly\ncontrast it with semantic methods, while noting it is likely that ultimately these two methods are\nnot exclusive and can be used synergistically in a production setting (for example, balancing the\nratio of typed semantic versus associative RAG depending on the amount of static information\navailable at a given lexical location). We return to this theme in section 6."}, {"title": "2.9 Hazel GPT-4 Results", "content": ""}, {"title": "2.9.1 Token and Time Performance", "content": "Figure 14 shows the time taken in seconds for all trials. The\ntime taken is dominated by the number of round trips through the API, with each round scaling in\nproportion to the sum of the length of the context and the length of the generation. Generally these\ntimes are too long for use in a practical completion setting; our intention is to determine a ceiling\non current performance with respect to correctness rather than present a practical system. Note\nhowever that these times will likely decrease quickly with hardware and software advances. As of\nMay 2024 GPT-40[53] performs on average twice as fast as the GPT-4-0613 model checkpoint used\nfor our experiments. However, the long worst-case times for error rounds suggests that capping at\na single correction round may be more practical, or motivate the use of summarization to reduce\ntoken count during error rounds."}, {"title": "2.10 Hazel StarCoder2-15B Results", "content": "To assess the effectiveness of static retrieval with smaller completion models, we conducted tests\nusing StarCoder2-15B, a model small enough to be run locally on consumer hardware. The average\npercentage of tests passed, shown in the rightmost column of Figure 16, exhibits a consistent trend\nwith the GPT-4 results. In the absence of any type or header information, StarCoder2 performed\npoorly. The addition of type information drastically improves performance, increasing the percent-\nage of correct solutions by an order of magnitude. Furthermore, incorporating headers leads to an\nadditional 50% increase in relative performance.\nHowever, two examples, BO and TO, experienced degraded performance after the inclusion of\nheaders. After close examination of the headers and the output programs, we discovered that the\ncompletions tending to use type-appropriate but in-fact irrelevant retrieved headers. We hypothesize\nthat smaller completion models, such as StarCoder2, are more sensitive to code that appears near the\nend of the context window, making them more susceptible to the influence of irrelevant information.\nWe touch on this failure mode again when we consider related work in section 6.\nVector retrieval baseline performance was significantly worse (in absolute and relative terms)\nthan with the larger model. We conjecture that this is due to a heightened sensitivity to erroneous\nsyntax in the prompt created by chunk truncation."}, {"title": "3 Static Retrieval in TypeScript", "content": "To confirm that the above results are not an artefact of using a low-resource language, we also\nexperimented with static retrieval in TypeScript."}, {"title": "3.1 TypeScript Methodology", "content": "Our methodology roughly follows the Hazel experiments. As TypeScript is a high resource language,\nwell-represented in training sets, we did not need to provide a syntax crash course as we did for\nHazel. TypeScript lacks the explicit support for typed holes and thus a convenient way to extract\nsemantic information. We emulated typed holes using a previously established approach [14] using\ngeneric functions (Figure 17).\nSpecifically, we prefix the sketch file with the declaration: declare function _<T>(): T. Then,\nwe represent a program hole as an application of that generic function: _() . Calling the TypeScript\nlanguage server's hover method on the hole gives us a corresponding type signature. It should be\nnoted that this method of emulating typed holes is not fully general. While it works consistently\nfor holes replacing the bodies of function definitions, it fails in some syntactic positions, including\nas an operand of infix operators.\nStatic retrieval of type definitions is performed via the TypeScript language server. In particular\nwe use coordinated calls to the Go to Type Definition and Hover methods to recursively retrieve\nrelevant types from the source lexically.\nThere does not appear to be any direct way of retrieving a typing context given a lexical\nlocation, or even a complete list of variables in scope using the TypeScript language server. We\nexperimented with different methods to retrieve relevant headers, including scanning the repository\nusing CodeQL, but were did not find a fully satisfactory general approach. Rather than incurring\nthe engineering cost of a compiler-level intervention, we simulated the retrieval of relevant headers\nmanually, emulating the same methodology as the Hazel Language Server. As such, our TypeScript\nimplementation should be considered a rough proof-of-concept; our experience here motivated our\nprospective LSP extension outlined in section 5.\nWe used the TypeScript compiler to collate static errors for correction rounds.\nAdapting MVUBench to TypeScript was done with the aid of Claude [4], an LLM chat agent (See\nsection 9 for more about our supporting LLM usage). Transliterated code was manually adjusted to\nestablish basic conformance to TypeScript idioms, for example adding elements to array at the end,\nversus at the start is standard for linked lists in functional languages like Hazel. Our experience\nhere suggests that MVUBench can be ported with relative ease to other similar languages."}, {"title": "3.2 TypeScript GPT-4 Results", "content": "In broad strokes the TypeScript results (Figure 18) are similar to the Hazel results. We see, somewhat\nunsurprisingly, that a higher-resource language, well represented in the training set, achieves better\noverall completions from the language model. Unlike with Hazel, some trials passed some tests\neven with no type information provided. With type definitions included, the TypeScript results are\nflatter than the Hazel results."}, {"title": "3.3 TypeScript StarCoder2-15B Results", "content": "The TypeScript StarCoder2 results (Figure 19) appear roughly in line with the Hazel results\nmodulo the considerations of the previous section."}, {"title": "4 Threats to Validity", "content": "The improvement seen from the inclusion of relevant function headers is highly contingent on\nthe fact that many relevant functions have already been implemented. While we believe that this\napproximates a common case in programming practice for which naive contextualization strategies\nfail, validating this claim would require larger-scale study, using at-scale programs which are more\nneutrally selected.\nMore broadly, MVUBench is not (and is not meant to) be representative of all coding tasks, but\nrather to present a challenge to contemporary techniques and help evaluate approaches to semantic\ncontextualization (e.g. vector retrieval, which we evaluate in subsubsection 2.8.5)."}, {"title": "5 ChatLSP", "content": "Here we sketch a conservative extension to the Language Server Protocol to support static con-\ntextualization, motivated in part the awkwardness of implementing static contextualization in\nTypeScript using its existing language server. The interface differs somewhat from the API we\nsketched incrementally in section 2, as the LSP is presentation-centric, operating in terms of strings\nand affordances rather than language-specific semantic data types.\nImmediately following, we will sketch how one might implement this ChatLSP API in terms\nof our Static Contextualization API, the latter serving more as an internal interface for language\nserver implementers."}, {"title": "5.1 ChatLSP API Methods", "content": "(1) aiTutorial: A constant (lexical-context-independent) method for low resource languages (like\nHazel) to specify a textual tutorial intended for LLMs having robust support for in-context\nlearning. For high resource languages, the default implementation will simply return a string\nstating which language is in use.\n(2) expectedType: Returns a string specifying the expected type at the cursor, if available\n(3) retrieveRelevantTypes: Returns a string containing type definitions that may be relevant\nat the cursor location\n(4) retrieveRelevantHeaders: Returns a string containing headers that may be relevant at the\ncursor location\n(5) errorReport: Returns an error report that can be used to determine if an error round is\nneeded, and if so, how the feedback should be presented to the LLM.\nThis API gives leeway to the language server to decide how to implement these commands. For a\nlanguage with a rich static analyzer, e.g. GHC (Haskell) with its support for hole-oriented program-\nming and existing functionality to retrieve relevant headers (e.g. see the work of Gissurarson[26]),\nit should be very straightforward to implement these five ChatLSP-specific commands.\nTo sketch the language server side of this interface, we collect the section 2 static contextualization\nAPI below. First, we define the following types aliases:\n\u2022 type Header = (Name, Type)\n\u2022 type Context = [Header]"}, {"title": "5.2 Static Contextualization Language Server API", "content": "\u2022 getExpectedType: (Program, LexicalLocation) -> Type\n\u2022 getTypingContext: (Program, LexicalLocation) -> Context\n\u2022 extractAliases: Type -> [TypeAlias]\n\u2022 getTypeDefinition: TypeAlias -> Type\n\u2022 getTargetTypes: Type -> [Type]\n\u2022 filterContext: Context, Type -> Context\n\u2022 scoreEntry: Header -> Float\n\u2022 getStaticErrors: Program -> [StaticError]\nChatLSP API methods (2) and (5) correspond directly to getExpectedType and getStaticErrors.\nThe following pseudocode outlines how methods (3) and (4) could be implemented using the Static\nContextualization Language Server API:\nretrieveRelevantTypes : Type \u2192 [Type]\nretrieveRelevantTypes t = concatMap (\\alias \u2192\ngetTypeDefinition alias\nin def : getRelevantTypes def) (extractAliases t)\nretrieveRelevantHeaders : Type \u2192 Context \u2192 [Header]\nretrieveRelevantHeaders t context =\nlet relevantTypes = retrieveRelevantTypes t\nfilteredHeaders = concatMap (filterContext context) relevantTypes\nsortedHeaders = sortBy scoreEntry filteredHeaders\nin take NUMHEADERS sortedHeaders\nUsage (given a Program and a LexicalLocation)\nretrieveRelevantTypes (getExpectedType (Program, LexicalLocation))\nrelevantHeaders = retrieveRelevantHeaders\n(getExpectedType Program LexicalLocation)\n(getTypingContext Program LexicalLocation)"}, {"title": "6 Related work", "content": "The introduction covered the broader literature on LLMs for code, so we focus here specifically on\nother methods for semantic contextualization of LLM-based code generation systems.\nError correction using"}]}