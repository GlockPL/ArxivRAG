{"title": "BlockFound: CUSTOMIZED BLOCKCHAIN FOUNDATION MODEL FOR ANOMALY DETECTION", "authors": ["Jiahao Yu", "Xian Wu", "Hao Liu", "Wenbo Guo", "Xinyu Xing"], "abstract": "We propose BlockFound, a customized foundation model for anomaly blockchain transaction detection. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models, BlockFound introduces a series of customized designs to model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized mask language learning mechanism for pretraining with RoPE embedding and FlashAttention for handling longer sequences. After training the foundation model, we further design a novel detection method for anomaly detection. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockFound's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockFound is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work not only provides new foundation models for blockchain but also sets a new benchmark for applying LLMs in blockchain data.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of blockchain technology, cryptocurrencies have gained significant attention and are increasingly being used in real-world applications. A lot of Decentralized Finance (DeFi) protocols have emerged, offering a wide range of financial services, such as lending, borrowing, and trading, to users. However, the decentralized nature of these protocols also makes them vulnerable to various security threats, including the presence of malicious attacks such as double-spending attack (Karame et al., 2012), partition attacks (Saad et al., 2019), and front-running attacks (Eskandari et al., 2020). These attacks seriously threaten the asset security of billions of blockchain users. For example, at least 3.24 billion USD were lost to DeFi attacks from April 2018 to April 2022 (Werner et al., 2022).\nHaving a runtime anomalous transaction detection is critical for protecting user assets. Such systems aim to detect suspicious transactions that deviate from typical patterns and provide early warnings of potential security threats, enabling quick interventions to minimize the impact of malicious activities. Moreover, as the complexity and volume of transactions in DeFi continue to grow, robust anomaly detection mechanisms will become increasingly essential to ensure the long-term stability and security of blockchain-based financial services."}, {"title": "2 BACKGROUND", "content": "Blockchain Blockchain is a decentralized, distributed ledger technology designed to enable secure, transparent, and tamper-resistant record-keeping. Originally developed as the backbone of cryptocurrencies like Bitcoin Nakamoto (2008), blockchain consists of a chain of blocks, each containing a list of transactions. The core innovation of blockchain is its ability to achieve consensus across a network of nodes without relying on a central authority. This is accomplished through consensus mechanisms such as Proof of Work (PoW) Back et al. (2002) or Proof of Stake (POS) Saleh (2021), which ensure that all participating nodes agree on the state of the ledger. Blockchain's im-\nSmart Contracts and Transactions Smart contracts are self-executing agreements where the terms are directly written into code, enabling automated and secure transactions. These contracts are a fundamental component of decentralized applications (DApps), which run on peer-to-peer networks using blockchain technology to create systems that are secure, transparent, and resistant to censorship. Deployed on platforms like Ethereum, DApps facilitate more complex, programmable transactions beyond simple value transfers. Each blockchain transaction can trigger the execution of a smart contract, which autonomously processes conditions, manages assets, and updates the ledger. A typical transaction includes details such as the sender and recipient addresses, the amount of cryptocurrency or tokens transferred, and any data required to execute smart contracts. This automation enhances transparency, security, and trust within decentralized applications, making blockchain an ideal infrastructure for DeFi, gaming platforms, and supply chain management systems. However, smart contracts are immutable once deployed, meaning that any errors or vulnerabilities in the contract code can lead to significant risks, including financial loss. As a result, detecting anomalous transactions within smart contracts is crucial for maintaining the security and reliability of blockchain-based systems."}, {"title": "3 EXISTING TECHNIQUES AND LIMITATIONS", "content": "LLM-based detection. Recently, a study (Gai et al., 2023) has utilized a large language model to detect anomalous transactions. Specifically, it adopts a GPT-like causal language modeling approach, training the LLM by predicting the next token in the transaction trace. Anomalous transactions are detected by ranking scores based on the log-likelihood of the predicted trace. However, this approach faces several fundamental limitations. Firstly, unlike natural language, transactions do not naturally form sequential data, making the prediction of the next token less meaningful for transaction traces. Moreover, the tokenization method used in the study is suboptimal, e.g., numerical values such as transaction fees are rounded to avoid vocabulary explosion, potentially obscuring critical transaction details. Effective tokenization is crucial for the successful application of LLMs in this context, as it directly impacts both the representation of smart contracts and the sequence length of processed transactions. In addition to developing specialized language models for blockchain data, some approaches (Chen et al., 2023a) directly adopt existing language models (e.g., ChatGPT) without further fine-tuning. These methods involve feeding ChatGPT with raw input transactions (e.g., corresponding JSON files) and are limited by the maximum input length of the model and knowledge of the model.\nRule-based and traditional ML-based approaches. In contrast to LLM-based approaches, non-LLM methods for anomalous transaction detection can be grouped into two main categories: traditional machine learning-based and heuristic-based approaches. The first category applies conventional machine learning models, such as Gaussian mixture models, to estimate the density of input transactions (Yang et al., 2019). Transactions with lower density scores are flagged as potentially anomalous. However, these methods are highly dependent on the quality and expressiveness of the transaction features used to generate hidden representations, limiting their generalizability. The second category consists of heuristic-based techniques. For instance, one method suggests detecting anomalous transactions by analyzing sequence length (Gai et al., 2023), under the assumption that shorter transactions are more likely to be benign. However, this assumption is overly simplistic and flawed, as will be demonstrated in \u00a75.2. Heuristic-based methods often suffer from being too rigid and can be easily bypassed by adversaries who do not conform to such patterns."}, {"title": "4 KEY TECHNIQUES", "content": "In this section, we first provide the overview of key techniques and then introduce them in detail. The pseudo algorithm can be found in Appendix A."}, {"title": "4.1 TECHNIQUE OVERVIEW", "content": "Tokenizer. As demonstrated in Figure 1, a blockchain transaction mainly consists of three types of inputs: function and address signature in hash values, function logs in natural languages, and function arguments in numbers. This hybrid data type makes a transaction naturally to be multi-modal. As such, directly applying existing tokenizers designed for language models to blockchain transactions will be problematic. First, existing tokenizers will treat hash values as numbers and divide them into sub-tokens. However, these numbers themselves are meaningless, instead, they are just used to represent different entities. Second, the numbers in blockchain transactions have a very large value range and large values frequently show up. Directly applying the existing tokenizers will divide a large number into many sub-tokens and thus result in ultra-long sequences for individual transactions. To solve the first issue, we use one-hot tokenization for hash values. We only consider the top 7,000 frequent hash values in our training dataset and treat the rest as \u201cOOV\" (Out of Vocabulary). This method can constrain the vocabulary size, which helps reduce model parameters and improve training efficiency. We further train our own number tokenization model to handle numbers. Different from existing tokenizers, our model can better tailor to the large numbers in blockchain transactions and give shorter token sequences for large numbers. Finally, we still apply the text tokenizer to function logs to capture their semantic meanings. As demonstrated in \u00a75, our customized tokenizer is critical for learning foundation models and final anomaly detection.\nModel design. We make a different design choice from BlockGPT (Gai et al., 2023) and use a BERT structure together with MLM for our foundation model. The key rationale is to reduce training complexity and improve overall training efficiency. Specifically, we do not need to generate new transactions, and training GPT models are in general more difficult than BERT as predicting the future without any context is harder than filling missing parts with certain context. Besides, our main focus is to learn patterns of normal transactions. As such, we select BERT with MLM, which provides enough pattern-learning capabilities and is more efficient than GPT models. We choose to apply RoPE embedding and FlashAttention in our model to handle long input sequences. The reason we choose this technique combination rather than other popular ones like LongLoRA (Chen et al., 2023b) is to consider computational cost and algorithmic simplicity. These techniques still keep a one-stage pretraining is simpler and more efficient than two-stage training, which is required by LoRA-based approaches.\nPost-training detection. With a trained foundation model, we feed a masked transaction into the model and use the reconstruction error as the metric for identifying abnormal transactions. We also try to build another detection model using the transaction embeddings of the foundation model. As specified in \u00a7B.1, we leverage one-class contrastive learning (Sohn et al., 2020) to learn a detection model using either only the <CLS> token embeddings or the embeddings of all tokens. We try to fine-tune the entire model or only train the detection model However, none of these trials can outperform simply masking testing transactions and calculating the reconstruction errors. As such, we stick to the simplest approach, which enables the best detection performance and the least computational cost."}, {"title": "4.2 TOKENIZATION", "content": "To address these challenges in tokenization we discussed above, BlockFound introduces a custom tokenizer specifically designed for the unique multi-model characteristics of blockchain transaction data. We first flatten the raw JSON data into a sequence of function calls and apply a depth-first search to track function callings. We use \u201c[START]\u201d and \u201c[END]\" tokens to help the model identify the beginning and end of each function call within the sequence. Additionally, \u201c[Ins]\u201d and \u201c[OUTs]\u201d tokens are used to mark the input and output arguments of functions, which can vary in number. To further distinguish between data types, we use tokens like \"data\" and \"address\" to indicate whether the argument is a data value or an address. These special tokens enable the model to clearly recognize the type and boundaries of variable-length information, improving accuracy in transaction tracing.\nAfter pre-processing the transaction trace, we then treat unique hash addresses as individual tokens, which can significantly reduce the overall token count. Given the large number of unique addresses, we rank them by frequency and retain the top 7,000 most frequent addresses. Addresses that fall outside the top 7,000 are treated as a single \u201cOOV\u201d token, as shown in Figure 1. In real-world"}, {"title": "4.3 MODEL DESIGN", "content": "BlockFound adapts the RoBERTa model (Liu et al., 2019) to train an auto-encoder specifically for transaction tracing. BlockFound employs a MLM strategy, where m% of tokens in each transaction are randomly masked. The model is then trained to reconstruct the original transaction from the masked tokens, learning robust representations in the process. However, tokenized transaction data can be significantly longer than typical natural language sequences, posing additional challenges during training. To address these challenges, we incorporate two key techniques: (1) We replace the absolute position embeddings in RoBERTa with Rotary Position Embeddings (RoPE) (Su et al., 2024), which provide more efficient handling of long-range dependencies. (2) We leverage FlashAttention (Dao et al., 2022) to accelerate the attention mechanism, improving memory efficiency and reducing computational overhead, making it feasible to train on long transaction sequences.\nRotary Position Embeddings. Attention-based models require explicit positional information due to the permutation-invariant nature of the attention mechanism. Traditional approaches such as Si-"}, {"title": "4.4 POST-TRAINING DETECTION", "content": "After training, we can deploy BlockFound for detecting anomalous transaction sequences. The motivation behind applying BlockFound for transaction anomaly detection is that since the model is trained on benign transaction sequences, it can accurately predict masked tokens if the sequence is also benign. Hence, the anomalous score of a transaction can be derived based on the prediction results on the masked tokens. Specifically, for a given transaction, we randomly mask a ratio of the tokens, similar to the training process, and input the masked sequence into the trained model. The probability distribution over the possible tokens for each MASK position represents the likelihood of each token in that position. We construct a candidate set of the top-s most likely tokens for each masked position. If the true token appears within the top-s candidate set, we consider the token as benign. Conversely, if the true token is not in the top-s candidate set, it is treated as anomalous. The reason why we do not directly predict based on the most likely token is that the addresses and values are more challenging than nature language texts to predict, and having a candidate set tolerant to the prediction error is more reasonable. After ranking the transactions by the anomalous score, we can select the top k transactions with the highest anomalous score as anomalous. k can be dynamically adjusted based on how the smart contract developers trade off between false positives and security of the transactions."}, {"title": "5 EXPERIMENTS", "content": "In this section, we present the experimental evaluation of BlockFound in anomalous transaction detection. We begin by introducing the experimental setup, including the dataset and evaluation metrics. Then, we compare BlockFound to other detection methods to showcase the effectiveness of BlockFound. Additionally, we conduct ablation studies to analyze the impact of hyper-parameters of BlockFound."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Dataset. We primarily focus on Ethereum and Solana transactions in our experiments. We sample transactions from interactions with 5 DeFi applications for Ethereum and 10 applications for Solana to ensure diverse transaction patterns. For each DeFi application, transactions are ordered by their block timestamps and split into 80% for training and 20% for evaluation as benign transactions. This per-application sequential split is crucial to prevent time travel data leakage, ensuring that the model is trained exclusively on past data without access to future information. Such a methodology can maintain the integrity of performance metrics by avoiding artificially inflated results that could arise if the model inadvertently learned from future transactions.\nSpecifically, our Ethereum dataset consists of 3,383 benign transactions for training, 709 benign transactions for testing, and 10 malicious transactions. The data was collected from October 2020 to"}, {"title": "5.2 EXPERIMENTAL RESULTS", "content": "Comparison with Baselines. We show the FPR, Recall, and Precision of BlockFound and other baselines in Table 1 and Table 2. As the results show, BlockFound outperforms all baseline"}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "In this work, we presented BlockFound, a transformer-based model designed for detecting anomalous transactions in DeFi ecosystems such as Ethereum and Solana. By leveraging masked language modeling and carefully designed tokenization techniques, BlockFound efficiently handles the complexity and diversity of transaction data. Additionally, we open-sourced the code, model, and datasets used in this work, making BlockFound the first open-source solution for LLM-based anomalous transaction detection in DeFi. We hope that this contribution will serve as a valuable re-"}, {"title": "B.1 IMPLEMENTATION DETAILS", "content": "Our method We detail the hyper-parameters and training process of our customized language models, each trained from scratch for either the Solana or Ethereum tasks. Recall that for the Solana dataset, the model is based on a BERT-large architecture, with a hidden dimension of 1024, 24 hidden layers, and 16 attention heads. For the Ethereum dataset, the model uses a BERT-base architecture, with a hidden dimension of 768, 12 hidden layers, and 12 attention heads. The complete set of training hyper-parameters is detailed in Table 4 and Table 5. The Solana model was trained over two days using eight A100 GPUs, while the Ethereum model required 1.5 hours of training on the same hardware."}, {"title": "B.2 ADDITIONAL EXPERIMENTS", "content": "As mentioned in \u00a74.1, we also explore post-detection methods using a one-class contrastive learning approach. In this experiment, we apply the method to the Ethereum dataset. Specifically, after pre-training our customized LLM on the Ethereum task, we extract feature representations for each transaction by either using the <CLS> token embeddings or the average embeddings of all tokens. We then perform one-class contrastive learning on the training set, treating positive samples as those originating from the same DeFi application and negative samples as those from different DeFi applications. Through this contrastive learning process, we aim to obtain more robust feature representations of the transactions. Finally, we apply kernel density estimation (KDE) to the features learned through one-class contrastive learning, where a lower density score for a transaction indicates a higher probability of it being anomalous. Details of the hyper-parameter settings can be found at https://shorturl.at/9dFL1.\nAs shown in Table 6, neither <CLS>-CL (i.e., one-class contrastive learning using input feature from <CLS> token embeddings) nor Average-CL (i.e., using input feature from the average embeddings of all tokens) outperforms our method. Compared with post-detection using one-class contrastive learning method, BlockFound achieves relatively good performance without requiring additional computation resources. Therefore, we continue to use the simplest approach\u2014our current masked prediction method\u2014as the post-detection method."}]}