{"title": "Image Super-Resolution with Taylor Expansion Approximation and Large Field Reception", "authors": ["Jiancong Feng", "Yuan-Gen Wang", "Mingjie Li", "Fengchuang Xing"], "abstract": "Self-similarity techniques are booming in blind super-resolution (SR) due to accurate estimation of the degradation types involved in low-resolution images. However, high-dimensional matrix multiplication within self-similarity computation prohibitively consumes massive computational costs. We find that the high-dimensional attention map is derived from the matrix multiplication between Query and Key, followed by a softmax function. This softmax makes the matrix multiplication between Query and Key inseparable, posing a great challenge in simplifying computational complexity. To address this issue, we first propose a second-order Taylor expansion approximation (STEA) to separate the matrix multiplication of Query and Key, resulting in the complexity reduction from O(N2) to O(N). Then, we design a multi-scale large field reception (MLFR) to compensate for the performance degradation caused by STEA. Finally, we apply these two core designs to laboratory and real-world scenarios by constructing LabNet and RealNet, respectively. Extensive experimental results tested on five synthetic datasets demonstrate that our LabNet sets a new benchmark in qualitative and quantitative evaluations. Tested on the RealWorld38 dataset, our RealNet achieves superior visual quality over existing methods. Ablation studies further verify the contributions of STEA and MLFR towards both LabNet and RealNet frameworks.", "sections": [{"title": "I. INTRODUCTION", "content": "INGLE-Image Super-Resolution (SISR) [1-4] has obtained widespread application in various fields, such as remote sensing imagery, security surveillance, and medical imaging. SISR methods are generally divided into two categories: non-blind SR and blind SR. The non-blind SR [5-11] assumes that the degradation type is known, which degradation function is typically represented as:\n$Xlr = Xhrs$,\nwhere xhr and x\u0131r denote the HR and LR images respectively, ands denotes the bicubic downsampling operation with a scale factor of s. However, the non-blind SR represented by Eq. (1) fails to accurately approximate the real-world degradation due to the inaccessibility of the reference image. Subsequently, researchers have developed blind SR methods, which assume that the LR image is obtained from the HR image through a degradation function with a complex and unknown degradation type, and this degradation function is typically assumed to follow the equation:\n$Xlr = (xhr Kernel) \u2193s +N$,\nwhere denotes the convolutional operation, Kernel represents the unknown blur kernel,\u2193s signifies the downsampling operation with a scale factor of s, and N accounts for sophisticated noise. According to Eq. (2), we can clearly see that a single HR image corresponds to infinitely many possible LR images, which is typically an ill-posed problem. Nonetheless, a multitude of blind SR methods has been proposed to overcome this problem. They can be broadly divided into two main types: methods without blur kernel estimation [12-15] and methods with blur kernel estimation [16-18].\nAs for methods without blur kernel estimation, SRMD [12] is the first to combine the preset blur kernel, noise, and LR image as the input to the SR network. After that, [14, 19] incorporated Maximum A Posteriori Probability into iterative optimization. For these methods, the selection of parameters for the preset blur kernel is crucial for achieving effective results in practical applications. However, it is challenging to manually choose appropriate preset blur kernel parameters in the real world. To overcome this challenge, researchers have developed some methods to estimate the blur kernel. IKC [16] proposed to iteratively estimate the blur kernel, which divides the SR reconstruction process into two steps: 1) estimating the blur kernel from the LR image and 2) combining the estimated blur kernel and the LR image to reconstruct the SR image. However, the independence between the training of the blur kernel estimation model and the SR model leads to incompatibility issues. To address this issue, DANv1 [17] proposed an alternating optimization strategy between the blur kernel estimation and SR models, which makes the two models form a closed loop that can be trained in an end-to-end manner. Although DANv1 has achieved impressive performance in blind SR tasks, NLCUNet [20] has made significant advancements by incorporating the Non-Local Sparse Attention (NLSA) [21] module. It screens global features in the feature map by calculating self-similarities, enabling implicit estimation of the blur kernel. Therefore, the self-similarity technique has received the widest application in blind SR tasks. However, blind SR models that incorporate the calculation of self-similarity suffer from high computational complexity, making them unsuitable for resource-constrained devices. It is worth noting that the commonly used module for calculating self-similarity is Non-Local Attention (NLA) [22].\nIn this paper, we go deep into the execution process of NLA. We find that during the self-similarity calculation, a matrix multiplication operation between the high-dimensional attention map and the Value is required, leading to high computational complexity. The attention map is obtained by multiplying the Query matrix with the Key matrix. Then the softmax function is further performed, which hinders the separation of Query and Key. Based on this finding, we propose a second-order Taylor expansion approximation (STEA) to significantly reduce the computational complexity. Compared to self-similarity calculations, STEA loses the ability to filter global features. Hence, we design a multi-scale large field reception (MLFR) to compensate for the performance loss caused by STEA. Finally, we apply our two core designs to laboratory and real-world scenarios by constructing LabNet and RealNet, respectively. Extensive experiments validate the effectiveness of the proposed method. The major contributions of this paper are summarized as follows.\nWe propose the second-order Taylor expansion approximation (STEA) strategy, which significantly reduces the computational complexity in self-similarity-based blind SR tasks.\nWe design a multi-scale large field reception (MLFR) to compensate for the performance loss caused by STEA, showcasing competitive performance over the state-of-the-art.\nWe construct the LabNet and the RealNet by integrating our two core designs (STEA and MLFR) to respectively verify the effectiveness of our designs in the laboratory and real-world scenarios. Extensive experimental results demonstrate that our LabNet achieves state-of-the-art performance while maintaining linear computational complexity, and our RealNet effectively handles various real-world degradations and achieves superior performance in diverse blind SR scenarios.\nThe structure of this paper is organized as follows: Section II provides a comprehensive overview of the proposed methodology. Specifically, Section II-A presents a detailed explanation of the intricate aspects of STEA. Section II-B elucidates the specific details of MLFR. Section II-C outlines the implementation specifics of LabNet, while Section II-D delves into the implementation details of RealNet. Subsequently, Section III encompasses the dataset and evaluation metrics (Section III-A), implementation details of LabNet (Section III-B), implementation details of RealNet (Section III-C), comparison with state-of-the-art techniques (Section III-D), ablation studies and analysis of LabNet (Section III-E), ablation study and analysis of RealNet (Section III-F) and adjustable parameter details in RealNet (Section III-G). Section IV concludes the paper."}, {"title": "II. METHODOLOGY", "content": "In this section, we provide a detailed explanation of the STEA and MLFR algorithms, as well as their deployment to the LabNet (testing for laboratory settings) and RealNet (testing for real-world scenarios)."}, {"title": "A. Second-Order Taylor Expansion Approximation (STEA)", "content": "The self-similarity has been widely applied to the blind SR task, which is commonly implemented by a Non-Local Attention (NLA) [22]. However, we found that the NLA brings extremely high computational complexity. This mainly arises from the matrix multiplication between the high-dimensional Attention Map and the V matrix, as highlighted by the red dashed box of Fig. 3. Without loss of generality, Q, K, V \u2208 RNxd denote query, key, and value matrix, respectively. Then, we know the computational complexity of NLA is O(N2 \u00d7 d), where N and d represent the dimension of the feature map and the number of channels, respectively. In this part, we propose an STEA algorithm to significantly reduce the computational complexity of NLA. To this end, we first analyze the execution process of the NLA module. As shown in Fig. 3, the Attention Map is derived from the matrix multiplication of Q and KT. Due to the limitation imposed by the softmax operation, the matrix multiplication between Q and KT cannot be separated, resulting in a O(N2) complexity. To address this challenge, we propose a Second-Order Taylor Expansion to approximate the NLA module. Here, we mathematically rewrite the execution process of the NLA as follows:\n$Q = XWq, K = XWk, V = XW\u03c5,$\nwhere Wq, Wk and W stand for the parameters of three 1\u00d71 convolutions, respectively. Q, K and V represent the Query, Key and Value, respectively. X denotes the input feature map. Next, we perform the matrix multiplication between Q and KT, followed by the softmax operation, to obtain the Attention Map:\n$AttMap = SoftMax(QKT),$\nwhere AttMap represents the Attention Map, and SoftMax(\u00b7) denotes the softmax function. By substituting Eq. (3) to Eq. (4), we have\n$AttMap = SoftMax(XWqWXT).$\nWe denote Wqk = WqW, resulting in the below expression:\n$AttMap = SoftMax(XWqkXT).$\nIn the original NLA module, the multiplication between Q and KT followed by the softmax operation gives rise to softmax attention. Although softmax attention can effectively enhance the performance of the SR network by making the matrix full-rank, it introduces high computational complexity since such a high-dimensional Attention Map (AttMap) must be multiplied with V in the subsequent computation. For that, we propose to simplify the softmax function as an exponential form:\n$AttMap = SoftMax(QK = 1 xe KT = 1 XeXWqkXT,$\nwhere k is a learnable parameter used to approximate the denominator of softmax. By now, we can see that the operation between QKT and V is still nonlinear due to the presence of an exponential function, which disables us from separating Q and K from QKT. In subsequent multiplication by V, the basic operation of the matrix cannot be flexibly used, failing to reduce the high computational complexity. For this purpose, we exploit a second-order Taylor expansion as a substitute for the exponential function, achieving a good trade-off between efficiency and accuracy."}, {"title": "B. Multi-Scale Large Field Reception (MLFR)", "content": "As mentioned in Section II-A, compared to the NLA module, our STEA achieves higher efficiency but sacrifices the ability to extract global features from the feature maps. To this end, we design a Multi-Scale Large Field Reception (MLFR) strategy to obtain a large receptive field, as shown in Fig. 5. For the unfilled parts of the dilated convolution, we use normal convolution to fill in the dilation. Specifically, we process the input feature map X through three separate branches. In the first branch, the input is processed by a dilated convolution with an expansion rate of 6 and a depth-wise convolution with a kernel size of 7 \u00d7 7, respectively. The output results are concatenated along the channel dimension. Simultaneously, in the second branch, the input is processed by a dilated convolution with an expansion rate of 4 and a depth-wise convolution with a kernel size of 5 \u00d7 5, respectively. Similarly, the output results are concatenated along the channel dimension. In the last branch, the input is processed by a dilated convolution with an expansion rate of 2 and a depth-wise convolution with a kernel size of 3 \u00d7 3, respectively. Again, the output results are concatenated along the channel dimension. We represent the above execution process using the following equation:\n$Output(i,j) = Concat(DDWC(X)dr=i, DWC(X)ks=j)i=2,4,6, j=3,5,7$\nwhere Concat(\u00b7,\u00b7) stands for concatenate operation, DDWC(\u00b7) stands for dilation convolution operation on the input, dr is the dilation rate, and DWC(\u00b7) stands for depth-wise convolution operation on the input, ks is the convolution kernel size. Furthermore, each value of i corresponds uniquely to one value of j. Subsequently, we employ 1 \u00d7 1 convolutions to perform channel-wise feature fusion on the outputs of the three branches, effectively reducing the dimension of the channels:\n$Fusion(i,j) = Conv1\u00d71(Output(i,j))i=2,4,6, j=3,5,7$\nwhere Fusion(i,j) represents the output obtained after channel-wise feature fusion of the input, and Conv1\u00d71(\u00b7) denotes the 1 \u00d7 1 operation performed on the input. Similarly, each value of i corresponds uniquely to one value of j. In addition, the input X undergoes a 1 \u00d7 1 bypass convolution in addition to the aforementioned three branches:\n$Bypass = Conv1\u00d71(X).$\nThe purpose of the 1 \u00d7 1 convolution in Eq. (18) is to perform subtle adjustments to the channels of the input X. Finally, we concatenate the outputs of the three branches with the output of the bypass 1 \u00d7 1 convolution along the channel dimension and perform channel-wise feature fusion using a 1 \u00d7 1 convolution to reduce the channel dimension:\n$FinalOutput = Conv1\u00d71(Concat(Fusion(i,j), Bypass)i=2,4,6).$\nIn summary, we utilize multiple dilated convolutions with different scales to extract features with various receptive fields. For the unpadded regions in dilated convolutions, we employ depth-wise convolutions to fill in the dilation. Additionally, we utilize 1 \u00d7 1 convolutions to perform subtle adjustments on the input as bypass convolutions. In the final step of feature fusion, we concatenate all the branches along the channel dimension and then employ a 1 \u00d7 1 convolution for channel-wise feature fusion, resulting in the final output as shown in Eq. (19)."}, {"title": "C. Application to Laboratory Scenario (LabNet)", "content": "To show the effectiveness of our STEA and MLFR designs in a laboratory scenario, we construct a novel network termed LabNet for testing. As shown in Fig. 2, our LabNet is an end-to-end blind SR framework composed of three parts: shallow feature extraction, deep feature extraction, and HR image reconstruction. For the shallow feature extraction part, we employ a 3 \u00d7 3 convolution to extract shallow features from the input LR image. Subsequently, the extracted shallow features are fed into the UNet for deep feature extraction. Each LSTEA module in the UNet consists of multiple LSTEA blocks. When the input features pass through an LSTEA block, they undergo a channel-wise LayerNorm operation, followed by two branches. One branch utilizes multiple depth-wise convolutions for local feature extraction, while the other branch employs the STEA and MLFR modules for large-scale feature extraction. The channel features are then fused using a 1 \u00d7 1 convolution and filtered using the channel attention module. Subsequently, the features pass through the GDFN [24] module, which performs a nonlinear mapping operation on the input to enhance network performance. Finally, for the HR image reconstruction part, the channel dimension is adjusted using a 3 \u00d7 3 convolution. The H and W dimensions are then expanded to the target size using a pixel shuffle operation, and a final refinement is performed using a 3\u00d73 convolution to obtain the ultimate output results."}, {"title": "D. Application to Real-world Scenario (RealNet)", "content": "To show the effectiveness of our STEA and MLFR designs in real-world scenarios, we construct a novel network termed RealNet for testing. As illustrated in Fig. 6, our RealNet comprises three key components: shallow feature extraction, deep feature extraction, and HR image reconstruction. Firstly, we perform shallow feature extraction on the input LR image using two separate 3 \u00d7 3 convolutions. Subsequently, the extracted features are separately subjected to deep feature extraction in the denoising branch and the deblurring branch. Both the Denoising and Deblurring modules are composed of multiple LSTEA blocks. The features extracted from the Denoising module and the Deblurring module are then fused in proportion using Adapter modules. Finally, we employ a Fusion module for HR image reconstruction.\nAdapter Module. For images contaminated by noise, there are numerous detailed features within the image that can be learned by the deblurring branch. Similarly, for blurry images, there are many low-frequency features within the image that can be learned by the denoising branch. Therefore, as depicted in Fig. 6, we propose the Adapter module. The parameters a and \u03b2 control the proportions of denoising and deblurring, respectively. Analyzing the denoising branch, we take the features extracted by the deblurring branch in the previous step, apply a 3 \u00d7 3 convolution, and perform a channel-wise concatenation with the features extracted by the denoising branch. Subsequently, a 1 \u00d7 1 convolution is employed for channel-wise feature fusion, followed by a channel attention module for channel-wise feature selection. Similarly, a similar processing approach is utilized for the deblurring branch.\nFusion Module. To adjust the proportions of the outputs from the final denoising module and deblurring module, we also utilize two adjustable parameters (a and \u03b2). After adjusting the two sets of output feature maps, we perform a channel-wise concatenation and then apply a 1\u00d71 convolution for channel information fusion and adjustment of channel dimensions. Finally, the output feature maps are resized to match the target size using a pixel shuffle operation and fine-tuned with a 3 \u00d73 convolution.\nRealNet-GAN. To achieve better visual results, we employ a GAN training strategy for the proposed RealNet. Specifically, we use RealNet as the generator and utilize a U-Net as the discriminator [25]. The training process consists of two parts. Firstly, we train a PSNR-oriented model using the l\u2081 loss, which is then used as the generator in the GAN training strategy. We then train a GAN-oriented model using a combination of l\u2081 loss [26], perceptual loss [27], and adversarial loss [28]. The overall loss function of our network is\n$Lentire = L1 + Lperc + 0.1 \u00d7 Ladv,$\nwhere L1, Lperc, and Ladv represent the l\u2081 loss, perceptual loss, and adversarial loss, respectively. According to our empirical study, their weights are set to be 1, 1, and 0.1, respectively. We utilize the {conv1,..., conv5} feature maps before activation in the pre-trained VGG19 [29] model (with weights {0.1, 0.1, 1, 1, 1}) as the perceptual loss."}, {"title": "III. EXPERIMENTS", "content": "We train all our proposed SR models using the DF2K dataset (DIV2K [30] + Flickr2K [31]). There are 3450 2K-resolution images in the dataset. For the quantitative evaluation, we conducted tests on five standard super-resolution image benchmark datasets: Set5 [32], Set14 [33], BSD100 [34], Urban100 [35], and Manga109 [36]. To conduct a qualitative evaluation, we compared the visual results of our approach with previous methods on the RealWorld38 [25] dataset. The quantitative performance metrics we use for comparison are the Peak Signal-to-Noise Ratio (PSNR) [37] and Structural Similarity (SSIM) [38]."}, {"title": "B. Implementation Details of LabNet", "content": "Our LabNet is built on the foundation of the LSTEA module, forming a U-Net. Each LSTEA module consists of multiple LSTEA blocks. During the training process, we set the size of the input image to 64 \u00d7 64, and we use the Mean Absolute Error (MAE) as the loss function. The total iterations are 1.2\u00d7106. The initial learning rate is set to 4\u00d710-4 and is halved every 3\u00d7105 iterations. We use Adam [44] as the optimizer with B\u2081 = 0.9 and \u03b22 = 0.99. All models are trained on Tesla T4 GPUs.\nData Preprocessing of LabNet. For the training dataset, a blur operation with a blur kernel size of 21 \u00d7 21 is applied. For different scaling factors (x2, x3, and \u00d74), the variance of the blur kernel is uniformly sampled during the training process from the ranges [0.2, 2.0], [0.2, 3.0], and [0.2, 4.0], respectively. For the test dataset, we uniformly select 8 blur kernels from the ranges [0.80, 1.60], [1.35, 2.40], and [1.8, 3.2] for different scaling factors (x2, x3, and x4), respectively."}, {"title": "C. Implementation Details of RealNet", "content": "Our RealNet is constructed by two branches: denoising and deblurring. As illustrated in Fig. 6, both the denoising branch and the deblurring branch consist of 4 modules, each composed of 5 LSTEA modules. The entire network is trained for 30 epochs. The initial learning rate is set to 1\u00d710-3 and is halved every 15 epochs. We use Adam as the optimizer with \u03b2\u2081 = 0.9 and B2 = 0.99. All models are trained on Tesla T4 GPUs.\nData Preprocessing of RealNet. As shown in Fig. 1, we first independently and in parallel apply second-order noise and second-order blur processing to the HR image, resulting in two corresponding LR images. Then, these two LR images are passed through the denoising and deblurring branches of RealNet, respectively. Finally, a fusion operation is performed to obtain the restored SR image."}, {"title": "D. Comparisons with State-of-the-art", "content": "Quantitative Evaluations in Lab Scenarios. From Table II, it can be observed that our LabNet significantly outperforms other state-of-the-art deep SR models on nearly all benchmarks and scaling factors. For instance, compared to NLCUNet [20] with a scale factor of x2, our LabNet achieves performance improvements of 0.03 dB, 0.05 dB, 0.11 dB, and 0.02 dB on the Set14, BSD100, Urban100, and Manga109 datasets, respectively. Similarly, compared to NLCUNet with a scale factor of \u00d74, our LabNet achieves performance improvements of 0.04 dB, 0.02 dB, and 0.11 dB on the Set14, BSD100, and Manga109 datasets, respectively. Our LabNet consistently outperforms DCLS [42] across all scale factors. Additionally, as shown in Table I, when the input image size is (1,3,64,64) and the scaling factor is x4, our LabNet achieves an inference time of 1.18 seconds on a GPU, which is 1.05 seconds faster than NLCUNet but slightly slower than DCLS. Furthermore, our LabNet has significantly fewer GFLOPs compared to NLCUNet and is comparable to DCLS. In summary, our Lab-Net not only achieves comparable performance to NLCUNet but also exhibits higher efficiency. This can be attributed to the utilization of LSTEA in our LabNet, which combines STEA and MLFR to capture multi-scale features with a large receptive field while maintaining linear complexity.\nQualitative Evaluations in Lab Scenarios. As shown in Fig. 9, by comparing the reconstruction result of the 'zebra' in Fig. 9, we can observe that the black and white stripes on the legs of the zebra generated by our LabNet are close to the GT image. Furthermore, by comparing the reconstruction result of 'YamatoNoHane', we can observe that our LabNet performs better in recovering the textual content in the image compared to other methods.\nQuantitative Evaluation in Real-World Scenarios. As commonly observed, GAN-based SR methods often generate lower values in terms of PSNR and SSIM metrics while producing visually captivating results. As shown in Table II, Real-ESRGAN and BSRGAN perform significantly lower than non-GAN-based SR methods on the five benchmark test datasets. However, Fig. 10 reveals the impressive visual outcomes achieved by Real-ESRGAN and BSRGAN. Additionally, due to the absence of corresponding GT images for the degraded images in the RealWorld38 dataset, we only conducted visual comparisons between our RealNet and other blind SR methods.\nQualitative Evaluation in Real-World Scenarios. The experimental results are presented in Fig. 10. We evaluated the generalization capability of our model using the RealWorld38 dataset, which consists of real-world images. Our RealNet outperforms previous methods for reconstructing LR images in real-world scenarios. Among all the methods, the non-deep learning bicubic interpolation method performs the worst, followed by DANv2, DCLS, and NLCUNet. These methods only employ Gaussian blur and downsampling during training, resulting in visually unsatisfactory outcomes. BSRGAN [43] and Real-ESRGAN achieve favorable visual results due to their training on datasets with complex degradations. However, upon comparing the 'OST_009\u2032 image in Fig. 10, we observe that the result obtained by BSRGAN exhibits a significant color deviation from the LR input, while the result produced by Real-ESRGAN evokes a strong sense of discordance. In contrast, our RealNet reconstruction result closely resembles the color of the input LR image and demonstrates improved overall coherence."}, {"title": "E. Ablation Study and Analysis of LabNet", "content": "1) Impact of STEA: Table III showcases the effect of employing first-order Taylor expansion approximation (FTEA) and second-order Taylor expansion approximation (STEA) on the performance of blind SR tasks. By comparing the PSNR and SSIM values of the first and second rows in Table III, we observed an improvement in performance across all test datasets when DWC(\u00b7) was applied to the Value. Furthermore, comparing the PSNR and SSIM values of the second and third rows in Table III, we found that utilizing STEA for the exponential function outperformed FTEA across all test datasets. Particularly on the Urban100 dataset, the PSNR was higher by 0.11 dB when using a STEA compared to a FTEA. Furthermore, we found that when the order of Taylor expansion is greater than 2, the performance does not show a significant improvement.\nEfficiency Analysis. As shown in Table IV, when applying DWC(.) to Value, there is a slight increase in the number of parameters and a slight rise in MFLOPs. By comparing rows 2 to 4 in Table IV, as the order of Taylor expansion increases, the third-order Taylor expansion approximation (TTEA) parameter count and MFLOPs are greater than STEA, which are greater than FTEA. Ultimately, we used STEA to build our LabNet and RealNet.\n2) Impact of MLFR: Table VI highlights the influence of incorporating multi-scale dilated convolutions in MLFR on blind SR performance. By comparing the PSNR and SSIM values of the first to third rows in Table VI, we found that in MLFR, when utilizing dilated convolutions with dilations of 2, 4, and 6 simultaneously, the performance was the best across all test datasets. Particularly on the Urban100 dataset, the PSNR was 0.12 dB higher when using dilated convolutions with dilations of 2, 4, and 6 compared to dilated convolutions with dilations of 4 and 6.\nEfficiency Analysis. As shown in Table V, in MLFR, the maximum number of parameters and MFLOPs are achieved when utilizing dilated convolutions with Dilated = 6, 4, and 2 simultaneously.\n3) Impact of STEA and MLFR: As shown in Table VII, LabNet achieves the best performance on all five benchmarks when both STEA and MLFR are used simultaneously."}, {"title": "F. Ablation Study and Analysis of RealNet", "content": "The experimental results from Table VIII highlight the following insights: Our RealNet without NLA has the fastest inference speed, processing a 64 x 64 input image in just 0.40 seconds. In contrast, Our RealNet with NLA and NLSA have much higher computational costs, with inference times of 4.21 seconds and 5.15 seconds, respectively. Our RealNet with the STEA, achieves high efficiency by processing an input image in only 0.48 seconds. Our RealNet with the STEA and the MLFR strikes a favorable balance between efficiency and visual result, processing an input image in 0.53 seconds."}, {"title": "G. Details of adjustable parameters in RealNet", "content": "Adjusting a single parameter: Users can adjust the values of the denoising and deblurring parameters in RealNet to achieve the desired effects. In the visualization shown in Fig. 7, Increasing a1 slightly enhances image details, but excessive increase may lead to distorted edges. Gradually increasing 03 improves the distinction between objects and the background, but setting it too high can negatively impact the image. 2 progressively enhances image visualization as it increases, but setting it to extremely large values can damage the image. Careful adjustment of a4 is necessary to prevent color distortion, as it is related to the deep layer. It is advisable to avoid setting B\u2081 to \u1e9e4 to excessively large values to prevent color distortion in the resulting image.\nJoint modification of multiple parameters: Users can still fine-tune the denoising and deblurring ratios (i.e., ai and Bi) according to their preferences, even after the training process."}, {"title": "IV. CONCLUSION", "content": "In this paper, we presented a second-order Taylor expansion approximation (STEA) to handle the high computational complexity of self-similarity techniques. Then, we designed a multi-scale large-field reception (MLFR) to compensate for the super-resolution (SR) performance loss caused by STEA. \u03a4\u03bf confirm the superiority of our designed STEA and MLFR, we respectively constructed the LabNet for testing in laboratory scenarios and the RealNet for testing in real-world scenarios. Extensive experimental results demonstrated the effectiveness of our method in blind SR tasks. The ablation study further validates their contribution to the overall framework."}]}