{"title": "Image Super-Resolution with Taylor Expansion Approximation and Large Field Reception", "authors": ["Jiancong Feng", "Yuan-Gen Wang", "Mingjie Li", "Fengchuang Xing"], "abstract": "Self-similarity techniques are booming in blind super-resolution (SR) due to accurate estimation of the degradation types involved in low-resolution images. However, high-dimensional matrix multiplication within self-similarity computation prohibitively consumes massive computational costs. We find that the high-dimensional attention map is derived from the matrix multiplication between Query and Key, followed by a softmax function. This softmax makes the matrix multiplication between Query and Key inseparable, posing a great challenge in simplifying computational complexity. To address this issue, we first propose a second-order Taylor expansion approximation (STEA) to separate the matrix multiplication of Query and Key, resulting in the complexity reduction from O(N2) to O(N). Then, we design a multi-scale large field reception (MLFR) to compensate for the performance degradation caused by STEA. Finally, we apply these two core designs to laboratory and real-world scenarios by constructing LabNet and RealNet, respectively. Extensive experimental results tested on five synthetic datasets demonstrate that our LabNet sets a new benchmark in qualitative and quantitative evaluations. Tested on the RealWorld38 dataset, our RealNet achieves superior visual quality over existing methods. Ablation studies further verify the contributions of STEA and MLFR towards both LabNet and RealNet frameworks.", "sections": [{"title": "I. INTRODUCTION", "content": "INGLE-Image Super-Resolution (SISR) [1-4] has obtained widespread application in various fields, such as remote sensing imagery, security surveillance, and medical imaging. SISR methods are generally divided into two categories: non-blind SR and blind SR. The non-blind SR [5-11] assumes that the degradation type is known, which degradation function is typically represented as:\n$$Xlr = Xhrs,$$\nwhere xhr and x\u0131r denote the HR and LR images respectively, ands denotes the bicubic downsampling operation with a scale factor of s. However, the non-blind SR represented by Eq. (1) fails to accurately approximate the real-world degradation due to the inaccessibility of the reference image. Subsequently, researchers have developed blind SR methods, which assume that the LR image is obtained from the HR image through a degradation function with a complex and\n$$Xlr = (xhr Kernel) \u2193s +N,$$\nwhere denotes the convolutional operation, Kernel represents the unknown blur kernel,\u2193s signifies the downsampling operation with a scale factor of s, and N accounts for sophisticated noise. According to Eq. (2), we can clearly see that a single HR image corresponds to infinitely many possible LR images, which is typically an ill-posed problem. Nonetheless, a multitude of blind SR methods has been proposed to overcome this problem. They can be broadly divided into two main types: methods without blur kernel estimation [12-15] and methods with blur kernel estimation [16-18].\nAs for methods without blur kernel estimation, SRMD [12] is the first to combine the preset blur kernel, noise, and LR image as the input to the SR network. After that, [14, 19] incorporated Maximum A Posteriori Probability into iterative optimization. For these methods, the selection of parameters for the preset blur kernel is crucial for achieving effective results in practical applications. However, it is challenging to manually choose appropriate preset blur kernel parameters in the real world. To overcome this challenge, researchers have developed some methods to estimate the blur kernel. IKC [16] proposed to iteratively estimate the blur kernel, which divides the SR reconstruction process into two steps: 1) estimating the blur kernel from the LR image and 2) combining the estimated blur kernel and the LR image to reconstruct the SR image. However, the independence between the training of the blur kernel estimation model and the SR model leads to incompatibility issues. To address this issue, DANv1 [17] proposed an alternating optimization strategy between the blur kernel estimation and SR models, which"}, {"title": "II. METHODOLOGY", "content": "In this section, we provide a detailed explanation of the STEA and MLFR algorithms, as well as their deployment to the LabNet (testing for laboratory settings) and RealNet (testing for real-world scenarios).\nThe self-similarity has been widely applied to the blind SR task, which is commonly implemented by a Non-Local Attention (NLA) [22]. However, we found that the NLA brings extremely high computational complexity. This mainly arises from the matrix multiplication between the high-dimensional Attention Map and the V matrix, as highlighted by the red dashed box of Fig. 3. Without loss of generality, Q, K, V \u2208 RNxd denote query, key, and value matrix, respectively. Then, we know the computational complexity of NLA is O(N2 \u00d7 d), where N and d represent the dimension of the feature map and the number of channels, respectively. In this part, we propose an STEA algorithm to significantly reduce the computational complexity of NLA. To this end, we first analyze the execution process of the NLA module. As shown in Fig. 3, the Attention Map is derived from the matrix multiplication of Q and KT. Due to the limitation imposed by the softmax operation, the matrix multiplication between Q and KT cannot be separated, resulting in a O(N2) complexity. To address this challenge, we propose a Second-Order Taylor Expansion to approximate the NLA module. Here, we mathematically rewrite the execution process of the NLA as follows:\n$$Q = XWq, K = XWk, V = XW\u03c5,$$\nwhere Wq, Wk and W stand for the parameters of three 1\u00d71 convolutions, respectively. Q, K and V represent the Query, Key and Value, respectively. X denotes the input feature map. Next, we perform the matrix multiplication between Q and KT, followed by the softmax operation, to obtain the Attention Map:\n$$AttMap = SoftMax(QKT),$$,\nwhere AttMap represents the Attention Map, and SoftMax(\u00b7) denotes the softmax function. By substituting Eq. (3) to Eq. (4), we have\n$$AttMap = SoftMax(XWqWXT).$$\nWe denote Wqk = WqW, resulting in the below expression:\n$$AttMap = SoftMax(XWqkXT).$$\nIn the original NLA module, the multiplication between Q and KT followed by the softmax operation gives rise to softmax attention. Although softmax attention can effectively enhance\nwhere k is a learnable parameter used to approximate the denominator of softmax. By now, we can see that the operation between QKT and V is still nonlinear due to the presence of an exponential function, which disables us from separating Q and K from QKT. In subsequent multiplication by V, the basic operation of the matrix cannot be flexibly used, failing to reduce the high computational complexity. For this purpose, we exploit a second-order Taylor expansion as a substitute for the exponential function, achieving a good trade-off between efficiency and accuracy.\n$$AttMap = SoftMax (QK = = xe KT = = XeXWqkXT$$,\n$$\nAtt Map  1\nk\nXeXWqkXT\n+\n(XWqkXT)2\n2!\n+O(XWqkXT)\n),\n1\nk\n1\nk\nwhere In stands for identity matrix and O(\u00b7) stands for the high-order remainder of the Taylor expansion. By ignoring"}, {"title": "B. Multi-Scale Large Field Reception (MLFR)", "content": "As mentioned in Section II-A, compared to the NLA module, our STEA achieves higher efficiency but sacrifices the ability to extract global features from the feature maps. To this end, we design a Multi-Scale Large Field Reception (MLFR) strategy to obtain a large receptive field, as shown in Fig. 5. For the unfilled parts of the dilated convolution, we use normal convolution to fill in the dilation. Specifically,\n$$FinalOutput = \u00d7 [DWC(XW)+ \nXW1B1W3 + XW1B1W2B2W3].$$\nAs shown in Fig. 4, we design the same network block to learn W1, W2, and W3, which first reduces the number of channels to a quarter and then restores it to the original channel number. Meanwhile, an efficient Conv1\u00d71 operation is adopted to approximate Wv.\nAs shown in Fig. 3, N and d are a multiplication of the width and height of a feature map and the number of channels. In general, N is far larger than d. According to Eq. (15), our STEA strategy reduces the complexity of the original NLA from O(N\u00b2 \u00d7 d) to O(N \u00d7 d\u00b2). The subsequent experimental results validate that our STEA is more efficient compared to the NLA module, however, exhibits a slight performance degradation due to the ignoring of remaining terms of Taylor extension and the use of the learning-based matrix. To compensate for such performance degradation, we design a Multi-Scale Large Field Reception (MLFR), which will be detailedly described in the following.\nwe process the input feature map X through three separate branches. In the first branch, the input is processed by a dilated convolution with an expansion rate of 6 and a depth-wise convolution with a kernel size of 7 \u00d7 7, respectively. The output results are concatenated along the channel dimension. Simultaneously, in the second branch, the input is processed by a dilated convolution with an expansion rate of 4 and a depth-wise convolution with a kernel size of 5 \u00d7 5, respectively. Similarly, the output results are concatenated along the channel dimension. In the last branch, the input is processed by a dilated convolution with an expansion rate of 2 and a depth-wise convolution with a kernel size of 3 \u00d7 3, respectively. Again, the output results are concatenated along the channel dimension. We represent the above execution process using the following equation:\n$$FinalOutput = Concat(Fusion(i,j),\nBypass)\ni=2,4,6\nj=3,5,7.\n$$Output(i,j) = Concat(DDWC(X)dr=i,\nDWC(X)ks=j)i=2,4,6,\nj=3,5,7$$\nwhere Concat(\u00b7,\u00b7) stands for concatenate operation, DDWC(\u00b7) stands for dilation convolution operation on the input, dr is the dilation rate, and DWC(\u00b7) stands for depth-wise convolution operation on the input, ks is the convolution kernel size. Furthermore, each value of i corresponds uniquely to one value of j. Subsequently, we employ 1 \u00d7 1 convolutions to perform channel-wise feature fusion on the outputs of the three branches, effectively reducing the dimension of the channels:\n$$Fusion(i,j) = Conv1\u00d71(Output(i,j))\ni=2,4,6\nj=3,5,7,$$\nwhere Fusion(i,j) represents the output obtained after channel-wise feature fusion of the input, and Conv1\u00d71(\u00b7) denotes the 1 \u00d7 1 operation performed on the input. Similarly, each value of i corresponds uniquely to one value of j. In addition, the input X undergoes a 1 \u00d7 1 bypass convolution in addition to the aforementioned three branches:\n$$Bypass = Conv1\u00d71(X).$$\nThe purpose of the 1 \u00d7 1 convolution in Eq. (18) is to perform subtle adjustments to the channels of the input X. Finally, we concatenate the outputs of the three branches with the output of the bypass 1 \u00d7 1 convolution along the channel dimension and perform channel-wise feature fusion using a 1 \u00d7 1 convolution to reduce the channel dimension:\nIn summary, we utilize multiple dilated convolutions with different scales to extract features with various receptive fields. For the unpadded regions in dilated convolutions, we employ depth-wise convolutions to fill in the dilation. Additionally, we utilize 1 \u00d7 1 convolutions to perform subtle adjustments on the input as bypass convolutions. In the final step of feature fusion, we concatenate all the branches along the channel dimension and then employ a 1 \u00d7 1 convolution for channel-wise feature fusion, resulting in the final output as shown in Eq. (19)."}, {"title": "C. Application to Laboratory Scenario (LabNet)", "content": "To show the effectiveness of our STEA and MLFR designs in a laboratory scenario, we construct a novel network termed LabNet for testing. As shown in Fig. 2, our LabNet is an end-to-end blind SR framework composed of three parts:"}, {"title": "D. Application to Real-world Scenario (RealNet)", "content": "To show the effectiveness of our STEA and MLFR designs in real-world scenarios, we construct a novel network termed RealNet for testing. As illustrated in Fig. 6, our RealNet comprises three key components: shallow feature extraction, deep feature extraction, and HR image reconstruction. Firstly, we perform shallow feature extraction on the input LR image using two separate 3 \u00d7 3 convolutions. Subsequently, the extracted features are separately subjected to deep feature extraction in the denoising branch and the deblurring branch. Both the Denoising and Deblurring modules are composed of multiple LSTEA blocks. The features extracted from the Denoising module and the Deblurring module are then fused in proportion using Adapter modules. Finally, we employ a Fusion module for HR image reconstruction.\nAdapter Module. For images contaminated by noise, there are numerous detailed features within the image that can be learned by the deblurring branch. Similarly, for blurry images, there are many low-frequency features within the image that can be learned by the denoising branch. Therefore, as depicted in Fig. 6, we propose the Adapter module. The parameters a and \u1e9e control the proportions of denoising and deblurring, respectively. Analyzing the denoising branch, we take the features extracted by the deblurring branch in the previous step, apply a 3 \u00d7 3 convolution, and perform a channel-wise concatenation with the features extracted by the denoising branch. Subsequently, a 1 \u00d7 1 convolution is employed for channel-wise feature fusion, followed by a channel attention module for channel-wise feature selection. Similarly, a similar processing approach is utilized for the deblurring branch.\nFusion Module. To adjust the proportions of the outputs from the final denoising module and deblurring module, we also utilize two adjustable parameters (a and \u03b2). After adjusting the two sets of output feature maps, we perform a channel-wise concatenation and then apply a 1\u00d71 convolution for channel information fusion and adjustment of channel dimensions. Finally, the output feature maps are resized to match the target size using a pixel shuffle operation and fine-tuned with a 3 \u00d73 convolution.\nRealNet-GAN. To achieve better visual results, we employ a GAN training strategy for the proposed RealNet. Specifically, we use RealNet as the generator and utilize a U-Net as the discriminator [25]. The training process consists of two parts. Firstly, we train a PSNR-oriented model using the l\u2081 loss, which is then used as the generator in the GAN training strategy. We then train a GAN-oriented model using a combination of l\u2081 loss [26], perceptual loss [27], and adversarial loss [28]. The overall loss function of our network is\n$$Lentire = L1 + Lperc + 0.1 \u00d7 Ladv,$$\nwhere L1, Lperc, and Ladv represent the l\u2081 loss, perceptual loss, and adversarial loss, respectively. According to our empirical study, their weights are set to be 1, 1, and 0.1, respectively. We utilize the {conv1,..., conv5} feature maps before activation in the pre-trained VGG19 [29] model (with weights {0.1, 0.1, 1, 1, 1}) as the perceptual loss."}, {"title": "III. EXPERIMENTS", "content": "We train all our proposed SR models using the DF2K dataset (DIV2K [30] + Flickr2K [31]). There are 3450 2K-"}, {"title": "IV. CONCLUSION", "content": "In this paper, we presented a second-order Taylor expansion approximation (STEA) to handle the high computational complexity of self-similarity techniques. Then, we designed a multi-scale large-field reception (MLFR) to compensate for the super-resolution (SR) performance loss caused by STEA. \u03a4\u03bf confirm the superiority of our designed STEA and MLFR, we respectively constructed the LabNet for testing in laboratory scenarios and the RealNet for testing in real-world scenarios. Extensive experimental results demonstrated the effectiveness of our method in blind SR tasks. The ablation study further validates their contribution to the overall framework."}]}