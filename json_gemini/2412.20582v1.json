{"title": "Bridging the Gap: A Decade Review of Time-Series Clustering Methods", "authors": ["JOHN PAPARRIZOS", "FAN YANG", "HAOJUN LI"], "abstract": "Time series, as one of the most fundamental representations of sequential data, has been extensively studied across diverse disciplines, including computer science, biology, geology, astronomy, and environmental sciences. The advent of advanced sensing, storage, and networking technologies has resulted in high-dimensional time-series data, however, posing significant challenges for analyzing latent structures over extended temporal scales. Time-series clustering, an established unsupervised learning strategy that groups similar time series together, helps unveil hidden patterns in these complex datasets. In this survey, we trace the evolution of time-series clustering methods from classical approaches to recent advances in neural networks. While previous surveys have focused on specific methodological categories, we bridge the gap between traditional clustering methods and emerging deep learning-based algorithms, presenting a comprehensive, unified taxonomy for this research area. This survey highlights key developments and provides insights to guide future research in time-series clustering.", "sections": [{"title": "1 Introduction", "content": "Time series, an ordered sequence of real-valued data, has been widely acknowledged as one of the most basic data formats. With the development of technologies in sensing, storage, networking, and data mining, massive raw data could be obtained, stored, and processed on the fly [174, 181]. Due to the advantage of chronological representation, we could see the application of time series in almost every scientific field or industry [74, 102, 126, 140, 151, 166, 185, 186, 249], including but not limited to: Environment [99, 123, 216], Biology [220, 239, 268], Finance [4, 64, 79, 217], Psychology [129], Artificial Intelligence [227]. However, in the information era, increasing data sizes that contain thousands or even millions of dimensions have become increasingly common. This has introduced a new layer of complexity, requiring efficient adaptive solutions [53, 104, 105, 147, 148] and presenting challenges in analyzing the underlying relationships between time series in various tasks, including indexing [54, 176, 182, 183, 187, 212], anomaly detection [31-36, 149, 150, 175, 180, 222], clustering [18, 178, 179, 184], classification [177, 211], and forecasting [188].\nClustering has been one of the earliest concepts developed in the field of unsupervised machine learning, which is shown to be one of the most efficient tools to help unveil the latent structure from the raw data. The main goal of clustering is to find a partition of various given objects, in which the similarity within each group is maximized, and minimized between groups. In other words, a group of \"similar\" data samples is viewed as a cluster. Consequently, the characteristics within each cluster of objects consist of the pattern in the dataset, i.e., common features shared within the cluster. For example, in computer vision tasks, the pattern can be a certain type of edge or color for similar objects [153], while in the time-series domain, the position of rapid growth or decline across time steps may have practical meaning.\nOn the one hand, data samples sharing the same pattern would have a small distance and thus be partitioned into the same cluster. On the other hand, a good clustering strategy would in turn facilitate the search for these representative features. Figure 1 depicts the examples of time-series clustering across different domains and applications.\nAs one of the most well-known clustering methods, the k-Means algorithm [161] provides an expectation-maximization (EM) based strategy to search the medoids and clustering assignments based on Euclidean Distance for each iteration. Its good performance has enabled the application in different fields such as electrical engineering, computer science, biology, and finance. However, traditional methods such as k-Means have suffered from significant performance degradation in the time-series domain. Traditional measurements in Euclidean space have proven inefficient for addressing the variety of distortions in time-series data, including shifting, scaling, and occlusion [178], which are common in real-world scenarios. To solve this problem, many methods have been proposed, which can offer invariances to the inherent distortions and robustness to noise or outliers. For example, Dynamic Time Warping (DTW) [46] proposes an elastic measure to deal with the many-to-many alignment issue and finds the optimal warping path that minimizes the total distance between 2 sequences. In order to further reduce the computation cost, k-Shape [178] introduces (i) shape-based distance (SBD) with time complexity of O(nlog(n)) and (ii) a novel centroid computation derived from an optimization problem, which achieves a significant improvement in clustering tasks [178, 184]. In recent years, numerous clustering methods have emerged from the deep learning era, capturing significant attention and interest. Various unsupervised learning strategies, like contrastive learning, enable neural networks to generate representative features in a reduced dimension space, which significantly alleviates the pressure of the downstream tasks such as dissimilarity measure and centroid computation. Leveraging parallel computing strategies and advanced GPU resources, a clustering model can be trained and deployed in significantly less time.\nPast reviews [3, 143] have explored time-series clustering algorithms proposed in decades and provided insights from various perspectives of views, i.e., data representation, dissimilarity measure, clustering methods, and evaluation metrics. However, many survey papers, as mentioned above, either only discuss the conventional time-series clustering before the deep learning era [3, 143] or mainly focus on end-to-end deep representation learning [130]. [7] reviews the conventional time-series clustering works and prior deep clustering methods. However, it mainly focuses on the case study in the context of biological time-series clustering without a comprehensive study on both sides as mentioned. To the best of our knowledge, this work is the first attempt to build a bridge between the conventional time-series clustering methods and the deep learning-based models, providing a novel and comprehensive taxonomy for various time-series clustering in each category. We anticipate that this work will provide valuable insights for next-generation clustering algorithm designs."}, {"title": "2 Time-series Clustering Overview", "content": "In this section, we first introduce the definition of time-series data, and the difference between univariate and multivariate time series. Then we present the problem formulation of time-series clustering and the general pipeline of the clustering process, which motivates the newly proposed taxonomy in the following section."}, {"title": "2.1 On the Definition of Time-series Data", "content": "Time-series data can be categorized based on their major characteristics. From the perspective of dimensionality, they can be classified into three main types: univariate, multivariate, and tensor fields. Additionally, depending on the sampling strategies employed during data acquisition, time-series data can be either regular or irregular. In the following sections, we will explore each of these categories in detail. The formal definition of time-series data is provided below.\nDefinition 2.1 (Time series). A time series $x_i$ is defined as a sequence of observations with $T > 1$ time steps $X_i = \\{x_{i1}, x_{i2}, ..., x_{iT} \\}$, where $x_{it} \\in R^d$. Based on the dimension $d$, each observation $x_{it}$ at time $t$ can be a real-valued number ($d = 1$) or a vector ($d > 1$).\nDefinition 2.2 (Subsequence). Given a time series $x_i = \\{x_{i1}, x_{i2}, . . ., x_{iT} \\}$, where $x_{it} \\in R^d$, a subsequence $C_i$ is defined as a sequence of consecutive time steps obtained from $x_i$: $C_i = \\{x_{im}, x_{i(m+1)}, ***, x_{i(m+L-1)} \\}$, where $L$ is the length of the subsequence, $1 \\le m \\le T + 1 - L$.\nDefinition 2.3 (Sliding Windows). Given a Time series $x_i = \\{x_{i1}, x_{i2}, ..., x_{iT} \\}$, where $x_{it} \\in R^d$, sliding windows are defined as a set of subsequences extracted by sliding a \"window\" of length $L$ across the current time series $x_i$, with a stride size $s$ ($1 \\le L < T - 1$ and $1 < s < T \u2013 L$). The size of the sliding windows matrix is $(\\lfloor\\frac{T-L}{s}\\rfloor + 1, L)$."}, {"title": "2.1.1 Univariate versus Multivariate", "content": "From the discussion above, we could clearly categorize the univariate and multivariate time series based on the dimension of each observation across time steps. Univariate time series (UTS) represents an ordered sequence of real-valued data in one dimension (d = 1), e.g., ECG200 in UCR dataset [40] consists of 200 ECG recordings of a single patient, each indicating the changes of electric activity during one heartbeat. Compared with univariate time series, the multivariate time series (MTS) contains observations of more than one dimension (d > 1). For example, PenDigits in UEA dataset [14] records the movement of both x and y coordinates when writers draw digits between 0 and 9. Compared with UTS, Methods designed for MTS need to address the dependencies across multiple channels, which adds complexity and poses additional challenges to the clustering task in MTS scenarios.\nFigure 2 presents examples of clustering for both univariate and multivariate time series, highlighting the importance of considering time steps from all channels in the development of algorithms for multivariate time-series clustering. The left side of this figure illustrates that we can directly categorize two univariate time series into two distinct clusters by observing the difference in the number of peaks between them. On top of that, the right side of the figure shows that clustering two multivariate time series by using the rule that only considers a single channel, a univariate time series, is inadequate. Rather, a MTS clustering algorithm should incorporate the impact of all channels to ensure accuracy."}, {"title": "2.2 On the Definition of Time-series Clustering", "content": "Clustering, as one of the earliest concepts in the machine learning field, has been widely applied in the time-series domain. The overall goal of clustering is to find a solution to group different data samples in a way that, the distances, or the dissimilarity measurements, within each group are minimized, and the distances between each group are maximized. This clustering procedure not only finds a special partition way for a whole dataset, but also provides valuable insights for understanding the latent structure of the data and strongly facilitates downstream tasks such as time-series classification, segmentation, anomaly detection, etc. The detailed definition is provided below.\nDefinition 2.4. [3] Given a dataset of N time-series data $X = \\{x_1, x_2,\\cdots,x_N\\}$, where $x_i \\in R^{d\\times T}$, the process of time-series clustering is to partition $X$ in to K clusters $C = \\{C_1, C_2,\\cdots,C_K\\}$, $K < N$. In general, homogenous time-series data that share similar characteristics are grouped together based on a pre-defined dissimilarity measure."}, {"title": "2.3 Time-series Clustering Pipeline", "content": "Before delving into any specific clustering algorithms, it is important to know the general process of time-series clustering. Building upon the insights presented in prior studies [3, 7, 143], we summarize the common pipeline of time-series clustering into three parts (shown in Figure 5): representation, dissimilarity measure and clustering procedure. This decomposition will become beneficial not only for the comparative evaluation of diverse time-series clustering algorithms, but also help uncover the essence of time-series clustering and develop novel algorithms."}, {"title": "2.3.1 Representation", "content": "The representation procedure, also named as data representation or feature extraction, denotes the data format of time series for facilitating downstream tasks. Raw time-series data is the basic format with rich feature information from natural signals, which is widely applied in different clustering algorithms. However, the noise interference from the signal recording poses a challenge in the search of meaningful patterns, and also the large dimension from the original space also significantly increases the time and cost for data analysis. Therefore, an effective approach to extracting meaningful features while retaining the essential information becomes beneficial. The definition of the time-series representation can be found in Definition 2.5.\nDefinition 2.5. Given a time-series data $x_i = \\{x_{i1}, x_{i2}, ..., x_{iT}\\}$ with $T$ time steps, we want to find a transformation $\\Phi : x_i \\rightarrow x_i'$, where $x_i' = \\{x_{i1}', x_{i2}',...,x_{iK}' \\}$ denotes a new representation, specifically $K < T$ in the dimension reduction scenario. The transformation space should retain the essential information from the original space in such a way that, if $x_i$ is similar to $y_i$, then $x_i'$ is similar to $y_i'$, and vice versa.\nAs can be seen from Figure 5, there are two main types of time-series representation: numeric and symbolic. The numeric time-series representation utilizes real-valued array (univariate) or matrix (multivariate) to denote the feature information in original signals, usually with reduced dimensions. Representative techniques include Discrete Fourier Transform (DFT) [6, 60, 113], Discrete Wavelet Transform (DWT) [6, 37, 113], Piecewise Linear Approximation (PLA) [117], Piecewise Aggregate Approximation (PAA) [118, 255], or neural networks after the emergence of deep learning [29, 71, 83, 162, 256, 258]. Symbolic time-series representation [144, 164, 208], on the other hand, offers the benefit from both dimensionality reduction and the wealth of text-based methodology, e.g. hashing, and sequence matching. Representative methods are Symbolic Aggregate approximation (SAX) [116, 145], indexable Symbolic Aggregate approximation (iSAX) [145], Symbolic Fourier Approximation (SFA) [208], and 1d-SAX [164]. As depicted in Figure 5, the input of symbolic representation could be either raw time-series data or a transformed representation."}, {"title": "2.3.2 Dissimilarity Measure", "content": "As shown in Definition 2.4, the primary objective of time-series clustering lies in the process of partitioning the data in a way that time series exhibiting the same pattern should be grouped together. To solve this problem in a mathematical way, the dissimilarity measure, also called distance measure, is proposed to quantify the proximity or separation relationship between two sequences (could be raw data or transformed representation as shown in Figure 5). Based on the definition, two identical time series should have a zero dissimilarity measure and a sufficiently large value when they belong to different clusters. With different designs, the dissimilarity measure can capture the similarity information in time, shape, and structure [3]. Generally, there are three major types of dissimilarity measures: lock-step, elastic, and sliding measure. An overview of each is depicted in Figure 5.\nThese three types of dissimilarity measures all have their own pros and cons. On the one hand, the one-to-one mapping assumption of the lock-step measure simplifies the comparison between different time series with much less time complexity, i.e., close to O(n), where n is the sequence length. However, on the other hand, dissimilarity measures like Euclidean Distance impose limitations on varying lengths of time-series data, and the one-to-one mapping may suffer from the noise interference inherent in natural signals. On the contrary, the elastic measures provide a flexible alignment in different regions which is robust to various kinds of disturbances and achieves great success in diverse time-series analysis tasks. However, according to the discussion in prior works [182], a majority of elastic measures do not perform significantly better than sliding measures on the benchmark while there exists a nonnegligible gap between the time consumption. In other words, sliding measures may provide a good trade-off between runtime and accuracy in comparison to lock-step or elastic measures in different cases.\n\u2022 Lock-step Measure focuses on one-to-one mapping for two whole time series. The final result is usually calculated by the summation or mean value of errors across each time point. The representative methods are Euclidean Distance (ED) [60, 73], Minkowski (i.e., $L_p$-norm) [19, 73, 182], Lorentzian (i.e., the natural logarithm of L1) [73, 182], Manhattan [73, 182], Jaccard [73, 182].\n\u2022 Elastic Measure has been frequently utilized in scenarios when the one-to-one mapping assumption does not hold firmly. Due to the noise interference and the nature of signals, two time-series data samples may be similar but exhibit different distortion in amplitude (scaling) and offset (translation), where the lock-step measures are likely to fail or suffer from performance degradation. To solve this problem, elastic measure methods are proposed to create one-to-many/many-to-one mapping in an \u201celastic\" way, which can provide a flexible alignment across time points in various regions [182]. Representative elastic measures are dynamic time warping [25, 46, 198, 207], the Longest Common Subsequence (LCSS) [10, 233], Move-split-merge (MSM) [89, 215].\n\u2022 Sliding Measure is another type of dissimilarity measure, which follows a sliding mechanism to create a global align-ment for different sequences. Representative sliding measures include variants of Normalized Cross-Correlation (NCC), such as Shape-based distance (SBD) [178]. Thanks to the advantage of Fast Fourier Transform (FFT), the cost can be reduced to O(n log n), which is a significant speed improvement compared to the original version of DTW (time complexity of O(n\u00b2)). [182],"}, {"title": "3 Time-series Clustering Taxonomy", "content": "In this section, we describe our proposed taxonomy of the time-series clustering algorithms including both traditional and deep learning-based strategies. All methods are divided into 4 categories: (i) Distance-based, (ii) Distribution-based, (iii) Subsequence-based, and (iv) Representation-learning-based. Figure 6 illustrates our proposed taxonomy with a sketch for each category. Next, we review the definition of each category in the following subsections."}, {"title": "3.1 Distance-based", "content": "As can be seen from the name itself, the crucial concept behind Distance-based methods is the way to measure the distance $Dist$ between two raw time series $X_A$ and $X_B$, where $Dist (X_A, X_B) = 0$ if the two time series are the same. Euclidean Distance (ED), as one of the most widely used distance measures in a variety of data formats, has achieved great success in many research fields. However, it is noticed that, for its lock-step design, this efficient distance measure usually suffers from the variance issue inherent in time-series data, e.g., scaling variance, shift variance, occlusion variance, etc. To deal with this problem, many distance measures tailored for time-series data have been proposed, such as Dynamic time warping (DTW) and Shape-based Distance (SBD).\nGiven the distance between each pair of time series, one can tell that $X_A$ and $X_B$ should be put together if $Dist (X_A, X_B)$ is small and vice versa. To make a proper decision for each time series, there come methods under two second-level categories: partitional and hierarchical models. More detail will be discussed in Section 4.\n\u2022 The partitional clustering algorithm focuses on partitioning $N$ unlabeled time series into $K$ clusters with centroids. Each time series has the smallest distance from the centroid of the current cluster. The cluster centroids and assignments are usually optimized iteratively through the training process.\n\u2022 The hierarchical clustering is an approach of grouping objects into clusters through a hierarchy of clusters. Depending on the hierarchy structure. methods can be divided into two types: agglomerative and divisive. In the process, all time series will keep merging (agglomerative) or get separated apart (divisive)."}, {"title": "3.2 Distribution-based", "content": "Distribution-based clustering focuses modeling the distribution of the time-series data, which offers guidance in building the boundary between each cluster. It is worth noting that, the distribution here should be considered a general version, including both (i) explicit distribution, e.g., the density of data points; and (ii) implicit distribution, e.g., encoding the time series using a pre-trained dictionary. Here we name four second-level categories: Model-based, Density-based, Feature-based, and Encoding-based models. More detail will be discussed in Section 5.\n\u2022 The Model-based clustering methods focus on modeling the explicit distribution of time-series data with learnable parameters, such as the Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM). Each time series can be modeled and represented by a set of parameters, which could serve as guidance for further time-series clustering.\n\u2022 The Density-based clustering methods define clusters as regions of concentrated objects, with cluster boundaries typically occurring in sparse or empty areas. For example, these methods expand clusters in dense neighborhoods and establish boundaries where data points become sparse.\n\u2022 The Feature-based time-series clustering methods are proposed to find descriptive features to represent the characteristics of time series in a global way. Considering the inherent variance issue in time-series data modality, noise interference could pose a challenge in the distance measure between samples. To solve this problem, descriptive features could serve as a noise-robust representation for the clustering purpose.\n\u2022 The Encoding-based time-series clustering methods focus on building a mapping function $F : X \\rightarrow Z$ between the original space and the transformed space (also called the latent space), where the latent representation $Z$ contains the essential feature information of the original data. When the dimension of the transformed space is smaller than the original space, this process is often viewed as a dimension reduction technique."}, {"title": "3.3 Subsequence-based", "content": "Subsequence-based clustering is a special case in the whole time-series clustering categories as defined in Section 2.2. In the process, representative subsequences will be extracted from the entire time series as an informative pattern for clustering purposes. As the noise perturbation sometimes leads to performance degradation when considering all time steps, by selecting descriptive subsequences models may circumvent this issue in different cases. There are two major second-level categories: Sliding-based and Shapelet-based models, which will be discussed in Section 6."}, {"title": "3.4 Representation-learning-based", "content": "Similar to the aforementioned categories like feature-based or encoding-based models, the Representation-learning-based methods also focus on the design of a new representation for the original time-series data. However, they both need explicit mathematical formulas to calculate the numeric value, while the representation-learning-based methods obtain the representation through a learning process. The new representation can serve as an input to some simple clustering models such as k-Means [161]. With the advent of deep neural networks and unsupervised learning strategies, representation-learning-based algorithms have achieved great success in numerous tasks. Depending on the characteristics of learning strategies, we divide three second-level categories: Comparative-based, and Generative-based models. More detail will be discussed in Section 7.\n\u2022 The Comparative-based time-series clustering methods learn an encoding mapping function $\\Phi : X \\rightarrow Z$ in a comparative way, e.g., comparing similar/dissimilar time-series samples. The encoder mapping function can be learned by a neural network, which is going to be discussed in detail in the following sections.\n\u2022 The Generative-based time-series clustering methods, in contrast to comparative-based clustering methods, learn the robust representation by casting constraints on the generation output. One good example is the reconstruction task: by jointly learning the encoding mapping function $\\Phi : X \\rightarrow Z$ and the decoding mapping function $D: Z \\rightarrow X$ in a reconstruction way, deep neural networks are able to find a good latent space for data representation with possible fewer dimensions."}, {"title": "4 Distance-based Methods", "content": "In this section, we explore distance-based clustering methods that leverage and solely depend on measures of dissimilarity, either between data, between clusters, or a combination of both, to group similar elements together. The methods within this category typically do not require specific data representation or feature selection. They operate effectively by utilizing raw time series alone to generate meaningful outputs. Furthermore, this categorization can be further divided into two sub-categories: partitional methods and hierarchical methods. The partitional methods divide a set of n unlabeled time-series data into k clusters and ensure that each cluster comprises at least one time-series data while the hierarchical algorithms group data through establishing a hierarchical structure. In the subsequent sections, we will delve into a more detailed exploration of the concepts and representative clustering methods of these two sub-categories in Section 4.1 and 4.2."}, {"title": "4.1 Partitional Clustering", "content": "The partitional clustering algorithm partitions n unlabeled time-series data into k clusters and guarantees each cluster contains at least one time-series data. Moreover, partitional clustering methods can be classified into two main categories as illustrated in Figure 7: crisp partitional methods and fuzzy partitional methods. In crisp partitional methods, each data point is exclusively assigned to a single cluster. In contrast, fuzzy partitional methods associate each data with membership likelihoods and permit each data to belong to multiple clusters simultaneously. In the following sections, we will discuss representative methods belonging to these two categories separately in more detail and related methods can be found in Table 1.\nCrisp partitional methods\nOne of the most widely-used partitional clustering algorithms is k-Means [161]. Initially, given the number of centroids k, k-Means randomly selects k time-series data as its initial centroids. Then, k-Means repeatedly uses Euclidean Distance (ED) to compute distances between each object and all centroids, assigns each object to one cluster whose centroid is the closest to that object, and updates each centroid to the mean of objects in that cluster until one of the pre-defined criteria is meet. However, since k-Means is sensitive to the initialization of centroids, [13] proposed k-Means++ to improve the performance of k-Means by defining centroids initialization rules which iteratively adding the new centroid based on the probability proportional to objects' distances from their corresponding the nearest previously selected centroids and aims at separating initial centroids from each other as far as possible.\nA variation of k-Means clustering is named k-Medians [100]. Compared with the iteration described for k-Means, unlike k-Means, instead of using ED as a distance measure and computing the mean for each cluster as its centroid, k-Medians applies Manhattan distance as its distance measure and calculates the median.\nAnother important clustering algorithm that is worth mentioning is k-Medoids [112] whose objective is also to minimize the distance between the data point assigned to a cluster and the centroid of that cluster. However, different from k-Means, k-Medoids can use arbitrary distance measures and it uses actual and the most representative data points named medoids as centroids. In other words, the medoid is an actual data point within a cluster, which minimizes the average distance to all other points in that same cluster. Partitioning Around Medoids (PAM) [112] is one of the most classic and representative methods that belong to the family of the k-Medoids clustering algorithm. Different from k-Means, PAM applies an algorithm named PAM Swap to perform centroid updates. For each medoid m, PAM Swap swaps it with a non-medoid point o, performs data points assignment, and calculates the total cost of the clustering. If the total cost decreases, the swap is performed and repeated. Otherwise, PAW Swap will undo the last swap and end. Compared with k-Means, incorporating medoids as centroids enhances the algorithm's robustness to the outliers and elevates the interpretability of centroids. Nevertheless, akin to k-Means, the k-Medoids clustering algorithm also requires the number of clusters as its input.\nFurthermore, another extended variant of k-Means is called k-DBA [190] which incorporates Dynamic Time Warping (DTW) [207] as distance measure and adapts DTW barycenter averaging (DBA) as its centroid computation. For each refinement, DBA updates each coordinate of the average sequence with the barycenter of its associated coordinates obtained by calculating DTW between the average sequence and each other sequence individually. K-Spectral Centroid (K-SC) [250] clustering algorithm, modified from k-Means, incorporates a scaling and translation invariant (STI) distance measure as well as matrix decomposition technique to update its centroids.\nAnother algorithm in this category is k-Shape [178] and it is a current state-of-the-art time-series clustering algorithm. Different from k-Means, k-Shape utilizes SBD as its distance measure. Adopting normalized cross-correlation and speeding up the computation in the frequency domain, SBD becomes a cheaper and more efficient algorithm compared to some other good-performing algorithms such as DTW. To compute centroids, k-Shape aligns objects within the same cluster towards the cluster's centroid based on SBD, and, since cross-correlation captures similarity, the objective function becomes finding a new centroid to maximize the sum of squared similarities within a cluster. Then, the optimization problem can be modified into maximization of the Rayleigh Quotient [76] and the new centroid becomes the eigenvector associated with the largest eigenvalue.\nFuzzy Partitional methods\nBesides partitioning n objects into k clusters in a crisp manner which forces each object to become a part of exactly one cluster, there are fuzzy partitional algorithms that enable one object belonging to more than one cluster to a certain degree. One of the most representative algorithms in this category is named Fuzzy c-Means (FCM) [51] [26]. In FCM, each data is assigned a fuzzy membership value for each cluster. These membership values are real numbers, ranging from 0 to 1 and representing the degree of likelihood that the data belongs to a specific cluster. Moreover, for each data $x_i$ and cluster $C_j$ pair, FCM calculates the distance between $x_i$ and the cluster centroid $c_j$ and weights the distance by the membership that $x_i$ belongs to $C_j$. With the goal of minimizing the sum of weighted distances across all data-cluster pairs, FCM iteratively updates membership values and cluster centroids until convergence. After convergence, the clustering configuration is finalized based on ultimate membership values.\n[75] applied FCM to functional MRI and discussed its possible optimizations respecting to three different parameters: 1. data set pre-processing methods; 2. distance measures; 3. cluster numbers. Since they expected that clustering the pixel time courses should be performed based on similarity, they introduced two cross-correlation-based (CC-based) distance and performed comparisons among those two newly proposed measures plus ED to find the best measure. Considering that many real-world situations involve the problem of short time series (STS) and unevenly sampled time series and motivated by observations in the field of molecular biology, [170] proposed a modified fuzzy clustering scheme by applying the proposed STS distance which was able to capture both the similarity of shapes formed by relative change of amplitude and the corresponding temporal information into a standard fuzzy clustering algorithm."}, {"title": "4.2 Hierarchical Clustering", "content": "Hierarchical clustering is an approach to grouping objects into clusters by constructing a hierarchy of clusters, which has great visualization power in time-series clustering. Hierarchical clustering methods can be divided into two types: agglomerative and divisive. While agglomerative hierarchical (AH) clustering algorithms start by creating clusters individually for each time series and then iteratively merging small clusters into large clusters until meeting certain criteria, divisive hierarchical (DH) clustering algorithms tend to assign all time series into one cluster and perform division till satisfying certain criteria. Moreover, related methods can be found in Table 2.\nIn AH clustering, since all merging operations are performed among the cluster level and each cluster contains at least one object, an extra similarity or dissimilarity measure should be introduced between two clusters and how to measure and represent the distance between two clusters in AH clustering methods becomes an important topic to explore. Thus, the idea of linkage is used and, in this survey, we list some widely used linkages to measure the distance between clusters [112]:\n\u2022 Single linkage: In single linkage, given two clusters $C_i$ and $C_j$ of time series, we initially need to compute all pairwise distances $D = \\{dist(x, y) | \\forall x \\in C_i \\text{ and } \\forall y \\in C_j\\}$, where $dist(x, y)$ is a distance function used to measure the distance between two time series $x$ and $y$. Then, the distance between $C_i$ and $C_j$ is defined as the shortest distance in $D$.\n\u2022 Complete linkage: In complete linkage, given two clusters $C_i$ and $C_j$ of time series, we need to calculate all pairwise distances $D$ following the same expression in the single linkage. Then, the distance between $C_i$ and $C_j$ is defined as the longest distance in $D$.\n\u2022 Average linkage: In average linkage, given two clusters $C_i$ and $C_j$ of time series, we should obtain all pairwise distance $D$ following the same expression mentioned above. After that, the distance between $C_i$ and $C_j$ is defined as the average value of $D$.\n\u2022 Centroid linkage: In Centroid linkage, given two clusters $C_i$ and $C_j$ of time series, we initially need to compute the centroid (the mean time series) of each cluster and denote them as $C_i'$ for cluster $C_i$ and $C_j'$ for cluster $C_j$. Then, the distance between those two clusters is represented as $dist(C_i', C_j')$.\n\u2022 Ward's linkage [237]: In Ward's linkage, considering two clusters $C_i$ and $C_j$, the distance between two clusters, denoted as $\\Delta(C_i, C_j)$, is defined as the increase in total within-cluster variance that occurs after merging.\nIt is worth mentioning that although the majority of papers are using AH with different linkages as their clustering methods, [201] introduced a method named Online Divisive-Agglomerative Clustering (ODAC) system which applies DH and an agglomerative phase to cluster time-series data streams. ODAC uses DH with Rooted Normalized One-Minus-Correlation (RNOMC) as its dissimilarity measure and a splitting criterion that divides the node based on the most dissimilar pair of streams. Additionally, it incorporates an agglomerative phase to reassemble a previously divided node, responding to variations in the correlation structure between time series.\nCompared with partitional clustering algorithms, hierarchical clustering algorithms do not require the pre-definition of the number of clusters, and mentioned by [143], given the suitable distance measure, hierarchical clustering may cluster time series of varying length. However, hierarchical clustering algorithms are difficult to adjust the clusters after they start and, thus, they are sometimes weak in quality and performance. Moreover, [3] stated that hierarchical clustering has quadratic computational complexity, and thus, due to its computational complexity and poor scalability, it is not optimal to run hierarchical clustering on large datasets."}, {"title": "5 Distribution-based Methods", "content": "In this section, we will discuss distribution-based clustering methods, where time-series data are grouped based on their explicit or implicit distribution. Emphasizing on extracting, selecting, learning, and utilizing the distribution of time-series data makes the distribution-based clustering methods be distinguished from the distance-based clustering methods."}]}