{"title": "QuLTSF: Long-Term Time Series Forecasting with Quantum Machine Learning", "authors": ["Hari Hara Suthan Chittoor", "Paul Robert Griffin", "Ariel Neufeld", "Jayne Thompson", "Mile Gu"], "abstract": "Long-term time series forecasting (LTSF) involves predicting a large number of future values of a time series based on the past values and is an essential task in a wide range of domains including weather forecasting, stock market analysis, disease outbreak prediction. Over the decades LTSF algorithms have transitioned from statistical models to deep learning models like transformer models. Despite the complex architecture of transformer based LTSF models 'Are Transformers Effective for Time Series Forecasting? (Zeng et al., 2023)' showed that simple linear models can outperform the state-of-the-art transformer based LTSF models. Recently, quantum machine learning (QML) is evolving as a domain to enhance the capabilities of classical machine learning models. In this paper we initiate the application of QML to LTSF problems by proposing QuLTSF, a simple hybrid QML model for multivariate LTSF. Through extensive experiments on a widely used weather dataset we show the advantages of QuLTSF over the state-of-the-art classical linear models, in terms of reduced mean squared error and mean absolute error.", "sections": [{"title": "INTRODUCTION", "content": "Time series forecasting (TSF) is the process of predicting future values of a variable using its historical data. TSF is an import problem in many fields like weather forecasting, finance, power management etc. There are broadly two approaches to handle TSF problems: statistical models and deep learning models. Statistical models, like ARIMA, are the traditional work horse for TSF since the 1970's (Hyndman, 2018; Hamilton, 2020). Deep learning models, like recurrent neural networks (RNN's), often outperform statistical models in large-scale datasets (Lim and Zohren, 2021).\nIncreasing the prediction horizon strain's the models predictive capacity. The prediction length of more than 48 future points is generally considered as long-term time series forecasting (LTSF) (Zhou et al., 2021). Transformers and attention mechanism proposed in (Vaswani, 2017) gained a lot of attraction to model sequence data like language, speech etc. There is a surge in the application of transformers to LTSF leading to several time series transformer models (Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022; Liu et al., 2022; Wen et al., 2023). Despite the complicated design of transformer based models for LTSF problems, (Zeng et al., 2023) showed that a simple autoregressive model with a linear fully connected layer can outperform the state-of-the-art transformer models.\nQuantum machine learning (QML) is an emerging field that combines quantum computing and machine learning to enhance tasks like classification, regression, generative modeling using the currently available noisy intermediate-scale quantum (NISQ) computers (Preskill, 2018; Schuld and Petruccione, 2021; Simeone, 2022). Hybrid models containing classical"}, {"title": "BACKGROUND", "content": "The fundamental unit of quantum information and computing is the qubit. In contrast to a classical bit, which exist in either 0 or 1 state, the state of a qubit can be 0 or 1 or a superposition of both. In the Dirac's ket notation the state of a qubit is given as a two-dimensional amplitude vector\n$\\psi = \\begin{pmatrix} \\alpha_0 \\\\ \\alpha_1 \\end{pmatrix} = \\alpha_0|0\\rangle + \\alpha_1|1\\rangle,$\nwhere $\\alpha_0$ and $\\alpha_1$ are complex numbers satisfying the unitary norm condition, i.e., $|a_0|^2 + |a_1|^2 = 1$. The state of a qubit is only accessible through measurements. A measurement in the computational basis collapses the state $|\\psi\\rangle$ to a classical bit $x \\in \\{0,1\\}$ with probability $|\\alpha_x|^2$. A qubit can be transformed from one state to another via reversible unitary operations also known as quantum gates (Nielsen and Chuang, 2010).\nShor's algorithm (Shor, 1994) and Grover's algorithm (Grover, 1996) revolutionized quantum al-\ngorithms research by providing theoretical quantum speedups compared to classical algorithms. The implementation of these algorithms require larger number of qubits with good error correction (Lidar and Brun, 2013). However, the current available quantum devices are far from this and often referred to as the noisy intermediate-scale quantum (NISQ) devices (Preskill, 2018). Quantum machine learning (QML) is an emerging field to make best use of NISQ devices (Schuld and Petruccione, 2021; Simeone, 2022)."}, {"title": "2.2 Quantum Machine Learning", "content": "The most common QML paradigm refers to a two step methodology consisting of a variational quantum circuit (VQC) or ansatz and a classical optimizer, where VQC is composed of parametrized quantum gates and fixed entangling gates. The classical data is first encoded into a quantum state, using a suitable data embedding procedure. Then, the VQC applies a parametrized unitary operation which can be controlled by altering its parameters. The output of the quantum circuit is given by measuring the qubits. The parameters of the VQC are optimized using classical optimization tools to minimize a predefined loss function. Often VQC's are paired with classical neural networks, creating hybrid QML models. Several packages, for instance (Bergholm et al., 2018), provide software tools to efficiently compute gradients for these hybrid QML models."}, {"title": "PRELIMINARIES", "content": "Consider a multivariate time series dataset with M variates. Let L be the size of the look back window or sequence length and T be the size of the forecast window or prediction length. Given data at L time stamps $X_{1:L} = \\{X_1,\\dots,X_L\\} \\in \\mathbb{R}^{L\\times M}$, we would like to predict the data at future T time stamps $\\hat{X}_{L+1:L+T} = \\{\\hat{X}_{L+1},\\dots,\\hat{X}_{L+T}\\} \\in \\mathbb{R}^{T\\times M}$ using a QML model. Predicting more than 48 future time steps is typically considered as Long-term time series forecasting (LTSF) (Zhou et al., 2021). We consider the channel-independence condition where each of the univariate time series data at L time stamps $X_L = \\{x^{m}_{1},\\dots,x^{m}_{L}\\} \\in \\mathbb{R}^{L\\times 1}$ is fed separately into the model to predict the data at future T time stamps $\\hat{X}^{m}_{L+1:L+T} = \\{\\hat{x}^{m}_{L+1},\\dots,\\hat{x}^{m}_{L+T}\\} \\in \\mathbb{R}^{T\\times 1}$, where $m \\in \\{1,\\dots,M\\}$. To measure discrepancy between the ground truth and"}, {"title": "4 QuLTSF", "content": "QML model for LTSF and is illustrated in Fig. 1. It is a hybrid model consisting of input classical layer, hidden quantum layer and an output classical layer. The input classical layer maps the L input features in to a $2^N$ length vector. Specifically, the input sequence $x_L\\in \\mathbb{R}^{L\\times 1}$ is given to the input classical layer with trainable weights $W_{in} \\in \\mathbb{R}^{2N\\times L}$ and bias $b_{in} \\in \\mathbb{R}^{2N\\times 1}$, and it outputs a $2^N$ length vector\n$y_1 = W_{in}x_{1:L}+b_{in}.$\nThe output of the input classical layer, $y_1 \\in \\mathbb{R}^{2^N\\times 1}$, is given as input to the hidden quantum layer which consists of N qubits. We use amplitude embedding (Schuld and Petruccione, 2021) to encode $2^N$ real numbers in $y_1$ to a quantum state $|\\phi_{in}\\rangle$. We use hardware efficient ansatz (Simeone, 2022) as a VQC, and is composed of K layers each containing a trainable parametrized single qubit gate on each qubit and a fixed circular entangling circuit with CNOT gates as shown in Fig. 1. Every single qubit gate has 3 parameters and the total number of parameters in K layers is 3NK. The output of VQC is given as\n$|\\phi_{out}\\rangle = (VQC)|\\phi_{in}\\rangle.$\nWe consider the expectation value of Pauli-Z observable for each qubit, which serves as the output of hidden quantum layer and is denoted as $y_2 \\in \\mathbb{R}^{N\\times 1}$. Finally, $y_2$ is passed through output classical layer with trainable weights $W_{out} \\in \\mathbb{R}^{T\\times N}$ and bias $b_{out} \\in \\mathbb{R}^{T\\times 1}$, which maps N length quantum hidden layer output to predicted T length vector\n$\\hat{x}_{L+1:L+T} = W_{out}y_2+b_{out}.$\nThe parameters of the hidden quantum layer and two classical layers can be jointly trained, similar to classical machine learning, using software packages like PennyLane (Bergholm et al., 2018)."}, {"title": "5 EXPERIMENTS", "content": "In this section, we validate the superiority of the proposed QuLTSF model through extensive experiments.\nThe code for experiments is publicly available on GitHub\u00b9. All experiments are conducted on SMU's Crimson GPU cluster2."}, {"title": "Dataset Description", "content": "We evaluate the performance of our proposed QuLTSF model on the widely used Weather dataset. It"}, {"title": "Baselines", "content": "We choose all three state-of-the-art linear models namely Linear, NLinear and DLinear proposed in (Zeng et al., 2023) as the main baselines. We also consider a few transformer based LTSF models FEDformer (Zhou et al., 2022), Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021) as other baselines. Moreover (Wu et al., 2021; Zhou et al., 2021) showed that transformer based models outperform traditional statistical models like ARIMA (Box et al., 2015) and other deep learning based models like LSTM (Bai et al., 2018) and DeepAR(Salinas et al., 2020), thus we do not include them in our baselines."}, {"title": "Evaluation Metrics", "content": "Following common practice, in the state-of-the-art we use MSE (1) and Mean Absolute Error (MAE) (5) as evaluation metrics\nMAE = Ex\\frac{1}{M} \\sum_{m=1}^{M} |\\hat{X}_{L+1:L+T}^{m} - X_{L+1:L+T}^{m}||_1"}, {"title": "Hyperparameters", "content": "The number of qubits N = 10 and number of VQC layers K = 3 in the hidden quantum layer. Adam optimizer (Kingma and Ba, 2017) is used to train the model in order to minimize the MSE over the training set. Other hyperparameters are adapted from"}, {"title": "5.5 Results", "content": "For a fair comparison, we choose the fixed sequence length L = 336 and 4 different prediction lengths T\u2208 {96,192,336,720} as in (Zeng et al., 2023; Zhou et al., 2022; Wu et al., 2021; Zhou et al., 2021). Table 1 provides comparison of MSE and MAE of QuLTSF with 6 baselines. The best and second best results are highlighted in bold and underlined respectively. Our proposed QuLTSF outperform all the baseline models in all 4 cases.\nTo further validate QuLTSF against the baseline linear models we conduct experiments with varying sequence lengths L \u2208 {48,96,192,336,504,720} and plot the MSE results for a fixed smaller prediction length T = 96 in Fig. 2, and for a fixed larger prediction length T = 720 in Fig. 3. In all cases QuLTSF outperforms all the baseline linear models."}, {"title": "Discussion and Future Work", "content": "QuLTSF uses generic hardware-efficient ansatz. Similar to classical machine learning in QML we need to choose the ansatz, if possible, based on dataset and domain expertise. Searching for optimal ansatz is a research direction by itself (Du et al., 2022). Finding better ansatzes for QML based LTSF models for different datasets is an open problem. One potential way is to use parameterized two qubit rotation gates (You et al., 2021).\nOther possible future direction is to use efficient data preprocessing, for example reverse instance normalization (Kim et al., 2021) to mitigate the distribution shift between training and testing data. This is already being used in the state-of-the-art transformer based LTSF models like PatchTST (Nie et al., 2023) and MTST (Zhang et al., 2024). These models also show better performance than the linear models in (Zeng et al., 2023). Interestingly, our simple QuLTSF model outperforms or comparable to these models in limited settings. For instance, for the setting (L = 336, T = 720) MSE of PatchTST, MTST and QuLTSF are 0.320, 0.319 and 0.315 respectively. For the setting (L = 336, T = 336) MSE of PatchTST, MTST and QuLTSF are 0.249, 0.246 and 0.248 respectively (see Table 2 in (Zhang et al., 2024) for PatchTST and MTST; and Table 1 for QuLTSF). Thus, QML models with efficient data preprocessing may lead to improved results."}, {"title": "CONCLUSIONS", "content": "We proposed QuLTSF, a simple hybrid QML model for LTSF problems. QuLTSF combines the power of VQC's with classical linear neural networks to form an efficient LTSF model. While the simple linear models outperform the complex transformer based LTSF models, the introduction of hidden quantum layer further improved the performance. This opens up a new direction of applying hybrid QML models for future LTSF research."}]}