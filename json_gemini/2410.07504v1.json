{"title": "Using LLMs to Discover Legal Factors", "authors": ["Morgan GRAY", "Jaromir SAVELKA", "Wesley OLIVER", "Kevin ASHLEY"], "abstract": "Factors are a foundational component of legal analysis and compu-\ntational models of legal reasoning. These factor-based representations enable\nlawyers, judges, and AI and Law researchers to reason about legal cases. In this\npaper, we introduce a methodology that leverages large language models (LLMs)\nto discover lists of factors that effectively represent a legal domain. Our method\ntakes as input raw court opinions and produces a set of factors and associated def-\ninitions. We demonstrate that a semi-automated approach, incorporating minimal\nhuman involvement, produces factor representations that can predict case outcomes\nwith moderate success, if not yet as well as expert-defined factors can.", "sections": [{"title": "1. Introduction", "content": "Recently, large language models (LLMs) have been applied automatically to annotate\nlegal case texts from particular legal domains in terms of factors from pre-existing fac-\ntor lists. In this paper, we describe and assess a methodology for employing LLMs to\ndiscover factors in case texts without using a pre-existing factor list.\nOur method takes as input raw court opinions and produces a set of factors and\nassociated definitions. We evaluate the extent to which an LLM can identify from scratch\nany factors in the cases from a legal domain where the LLM has no apparent access to\na pre-existing list of factors or their definitions for that domain. We demonstrate that a\nsemi-automated approach, with a human in the loop produces factor representations that\ncan predict case outcomes with moderate success, if not yet as well as expert-defined\nfactors can. In the absence of predefined factors from courts or legislative bodies, legal\nscholars manually analyze hundreds of cases to identify factors, a process that is highly\ntime-consuming and costly. Our methodology could enable a more efficient process of\nidentifying factor representations of legal domain cases."}, {"title": "2. Related Work", "content": "Legal factors have played an important role both in empirical legal studies and in AI and\nLaw. Factors are \"consideration[s] a decision maker must or may take into account to\ndetermine an outcome.\" \"[They] can be prescribed in a statute or regulation, or created\nby courts,\" [1, p. 2, 3]. Courts employ factors in a variety of legal domains, such as\nassessing spousal support or determining violations of the right to a speedy trial [1, p.\n2f], determining copyright fair use, works made for hire, or consumer confusion as to the\nsource of goods in trademark infringement, (Beebe [2, p. 1584f], [3,4]). Empirical legal\nscholars have studied courts' use of multi-factor tests in legal domains. They often begin\nwith a canonical list of factors as set out in a statute or in appellate court decisions and\napply machine learning (e.g., decision trees) to evaluate which factors are most important\nand how courts employ them [2,3,4,5].\nAI and Law researchers have computationally modeled legal case-based reasoning\nand argumentation in terms of factors. For purposes of these models, factors have been\ndefined operationally as stereotypical fact patterns that tend to strengthen or weaken a\nside's argument in favor of a legal claim [6,7,8,9,10,11]. Factors have been used to model\nshifts in legal concepts as applied over time in legal decisions [12]. More recent models\nemploy factors in developing formal models of precedential constraint [13].\nTraditionally, representing cases in terms of factors has been a manual activity,\nwhether for statistical analysis or computational modeling. Researchers read cases and\nidentified the sentences from which one could infer that facts associated with a factor\nhad occurred in the cases and indicating that a court decided the case as it did because,\nor in spite, of the presence of that factor. See, e.g., [14] More recently, researchers have\ndeveloped machine learning pipelines automating to some degree annotating factors in\ncase-related texts, for example trade secrets factors in law-student-prepared case sum-\nmaries by Ashley and Br\u00fcninghaus [15] or case opinions by Falakmasir and Ashley [16],"}, {"title": "2.1. Research Questions", "content": "In light of the related work, we evaluate the following open research questions:\nRQ1: How effective are LLMs at automatically synthesizing a factor-based representa-\ntion of a legal domain from raw court opinions?\nRQ2: How effective are humans at synthesizing a factor-based representation of a legal\ndomain from raw court opinions with the assistance of LLMs?\nRQ3: How effectively can LLMs discover new factors or propose meaningful variations\nto existing factors within a pre-defined list?"}, {"title": "3. Data", "content": "Our raw data come from the DIAS Corpus, obtained through the Harvard Caselaw Ac-\ncess Project (CAP), [14,20], which contains U.S. court opinions on whether police of-\nficers had reasonable suspicion of drug trafficking. The cases were identified through\nthe efforts of lawyers and law students in [14,20]. Out of the corpus of 300, we work\nwith a random subset of 174 cases. Random sampling ensures unbiased selection, and\nthe smaller subset preserves data for future testing. The Fourth Amendment governs the\nadmissibility of evidence in federal and state courts and state courts follow the guidance\nof the US Supreme Court in their interpretation of the amendment. Since every decision\nin a federal or state court interpreting reasonable suspicion in a drug interdiction\nstop relies on the same legal standard, our data can come from any jurisdiction in the\nU.S."}, {"title": "4. Methodology", "content": "The first step is to process raw opinions by identifying the court's analysis and conclu-\nsion sections. This reduces both data noise and the amount of text needed for prompts\nin later stages. While courts may present relevant facts throughout the opinion, in the\nanalysis section, they consistently highlight factors that are relevant to their conclusions.\nTo this end, we prompted gpt-4-1106-preview to identify the analysis portion of an\nopinion as well as the court's conclusion on the issue of whether the officer had reason-\nable suspicion to make the detention. Essentially, we sought to replicate a portion of the\nwork in [22,23] with zero-shot LLMs. We numbered each paragraph and instructed the\nmodel with a prompt providing: 1) Explanation of what an analysis section of an opinion\nis; 2) explanation of what a court's conclusion is; 3) an example of a court's analysis\non the relevant issue; 4) an example of the court's conclusion on the relevant issue; and\n5) instructions to return the span of paragraphs encompassing the court's analysis and\nconclusion.\nTo assess the model's performance, we annotated a sub-sample of 103 out of 173\ncases used in this work. An attorney, an expert in drug interdiction law, identified the\nparagraphs containing the court's analysis and conclusion. We then collected the sets of\nfactors identified through gold standard annotation that describe factors of suspicion in\nthe text. We focus on identifying factor sets as they will be converted into a dichotomous\nvector for use in evaluation, as explained in Section 5. Table 1 shows the results of\ncomparing the sets of gold standard factors identified by the model and annotator. For\neach of the 103 examples, individual scores for recall, precision, and accuracy were\ncalculated, and the average of these scores across all cases is reported in the table. Recall\nmeasures the extent to which the model identified the same set of factors in the analysis\nand conclusion spans as the expert annotator. Here, we focus on recall to ensure that\nfactors are not being missed. The high recall score indicates the process is reliable."}, {"title": "4.1. Inducing Factors with LLMs", "content": "In the next stage, we prompt an LLM to replicate the work of an attorney in reviewing\nnumerous case opinions, identifying relevant facts and potential factors, and ultimately\ndefining a canonical list of factors. We structured a prompt to focus only on the legal\nissue at hand: \"the legal issue here is whether a police officer had reasonable suspicion\nto detain a motorist.\" The prompt contains no other knowledge about the domain or\nissue. We then introduce the concept and definition of a legal factor. Next, we provide an\nexample of a factor from another legal domain, namely the example of the \"Disclosure-\nin-negotiations\" factor from the trade-secret domain [24]:\nThis factor describes a plaintiff's disclosure of its product information in negotiations\nwith a defendant. You should notice how this description of the factor is sort of\nbroad, but specific. In specific, it is referencing about the disclosure of facts during\nnegotiations. Broadly, a disclosure could happen in negotiation in many different"}, {"title": "4.2. Refined Factor Representation of Rudimentary Induced Factors", "content": "Next, the rough factors are refined into a Refined Factor Representation (RFR). The goal\nis to define each factor specifically enough to capture its core concept, while keeping it\nbroad enough to cover all its potential manifestations. For example, a factor such as the\none induced in 4.1 is refined into something like the following:\nNervous Behavior and Evasive Answers: Exhibiting nervous behavior such as exces-\nsive sweating, avoiding eye contact, rapid breathing, or shaking, and providing eva-\nsive or inconsistent answers during a traffic stop can contribute to an officer's rea-\nsonable suspicion of criminal activity. This includes observable nervousness beyond\nwhat is typical, changes in the driver's story, and actions such as avoiding eye contact\nor attempting to distance oneself from the vehicle.\nTo accomplish this, we asked a human annotator to perform the refinements follow-\ning a set of guidelines. The annotator was blind in the sense that he was not previously\naware of the reasonable suspicion for drug trafficking domain or the factors from our\nprior work [14,20]. He read three drug trafficking cases and the guidelines, which ex-\nplained factor-based legal domains using an example from trade-secret misappropriation\nand included the LLM prompt that produced the rough factors described in Section 4.1.\nThe annotator was then instructed to refine each of the rough factors as per the guidelines:"}, {"title": "5. Evaluation", "content": "Here, we evaluate the quality of the RFR synthesized from raw LLM-induced factor out-\nputs for the domain of reasonable suspicion of drug trafficking. Our evaluation assess-\nment is based on the experiment in [20] where sets of factors identified in court opinions\nwere converted into a binary vector representing the individual facts present/absent in\na particular case. In order to identify the factor sets we rely on annotations of factors\ndescribed in sentences as in [19]. Like [20] we use the binary representation to make\npredictions about whether the court concluded that suspicion was found. By using this\nevaluation method, we can measure whether the refined factor representation produces\nfactor sets that represent the domain in such a manner that meaningful predictions and\nanalysis can be made. We ultimately assess the reliability of the refined factor represen-\ntations by comparing 4 factor representations that differ based on the annotator that was\nused to identify factor sets and on the refiner who/that refined the factor representation:\nGold/DIAS CFR gold standard annotations made by an expert annotator using the\nCanonical Factor Representation (CFR) of DIAS factors identified in [14,19,20].\nLLM/DIAS CFR: annotations made by an LLM (gpt-40 or llama3-70b-8192), us-\ning a guideline prompt prepared for human annotator using the Canonical Factor\nRepresentation of DIAS factors identified in [14,19,20].\nLLM/Human RFR: annotations made by an LLM (gpt-40 or llama3-70b-8192),\nusing human's RFR of raw factors induced by an LLM in 4.1."}, {"title": "6. Discussion", "content": "The following diagrams help to explain the difference in predictive performance of fac-\ntors identified in the Dias CFR and those in the RFR synthesized by the human annotator\n(Human RFR) or LLMs (Llama RFR or GPT RFR).\nThe diagrams depict semantic relations between these two sets of factors. We em-\nbedded the definitions of the factor representations and then measured the cosine similar-\nity between the RFR and CFR factors. For each RFR factor, we calculated the three most\nsimilar CFR factors. To filter out low similarity matches, we disregarded any similarity\nscore within the top three for an RFR-CFR factor pair that was lower than the average\nof all top three scores. The plot in Figure 1 demonstrates the similarity between Human\nRFR factors on the left and CFR factors on the right. The plot in Figure 2 demonstrates\nthe similarity between Llama RFR factors on the left and CFR factors on the right. If the\nCFR factor is connected to an \"Unmatched Source\" it means that no similar RFR factor\nwas identified. If an RFR Factor on the left is connected to \"Unmatched Target\" it means\nthat the RFR factor was not identified as similar to any CFR factor. The weight of the\nline indicates the strength of the similarity; heavier lines signify higher cosine similarity."}, {"title": "6.1. Limitations and Conclusions", "content": "We have demonstrated that it is feasible for an LLM, with a human in the loop, to take\na set of raw opinions and produce a representation of a legal domain as factors (RQ2).\nOur attempts to fully automate this process using an LLM resulted in weak to moderate\nperformance (RQ1); so far, it performs better with human involvement. Our methodology\nshows promise in identifying new factors for pre-defined factor lists (R3).\nA limitation of our work is its reliance on both open-source and proprietary models,\nwith prompts tuned specifically for proprietary models and limited tuning applied to\nopen-source alternatives. The knowledge embedded in these models plays a crucial role\nin the outcomes. All models, including humans, returned factors that were not legally\nrelevant. This underscores the need to incorporate fundamental legal knowledge into\nmodel prompting for more accurate and relevant factor discovery.\nEventually, our methodology could suggest factors to consider for legal argument.\nAdvocates may need to make sense of a collection of legal decisions to identify predic-\ntive factors that can align supportive cases in an argument and distinguish non-supportive\nones. Also, with large sets of cases in a domain, researchers could employ the methodol-\nogy to generate preliminary lists of possible factors to use in conceptually organizing the\ncases or in predicting case outcomes and for comparing factor lists' predictive accuracy."}]}