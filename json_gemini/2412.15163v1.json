{"title": "Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents", "authors": ["Jessica Woodgate", "Paul Marshall", "Nirav Ajmeri"], "abstract": "Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics.", "sections": [{"title": "1 Introduction", "content": "Social norms are standards of expected behaviour that govern a multi-agent system (MAS) and enable coordination between agents (Levy and Griffiths 2021; Wright 1963). Norms can be established through top-down prescriptions or emerge bottom-up via interactions between agents (Morris-Martin, De Vos, and Padget 2019). However, when agents are solely self-interested, norms may emerge that exploit some agents for the benefit of others. Where ethics involves one agent's concern for another (Murukannaiah and Singh 2020), norms which result in the subjugation of agents are unethical. If agents learn norms by appealing to existing behaviours in a society without evaluating how ethical those behaviours are, they risk perpetuating unethical norms.\nPrevious works on promoting norms which are considerate of others, such as Tzeng et al. (2022) and Dell'Anna et al. (2020), appeal to individual or societal preferences over values. Other works observe the behaviour of others to encourage cooperation: Oldenburg and Zhi (2024) infer norms by observing apparent violations of self-interest; Guo et al. (2020) learn contextual priority of norms from observing experts; Chen et al. (2017) imply norms through reciprocity. However, approaches which appeal to preferences or existing behaviours to promote cooperation define ethical behaviour by reference to descriptive statements, which are statements that express what states of affairs are like (Kim, Hooker, and Donaldson 2021). Attributing ethics descriptively may lead to the issue of deriving an ought from an is-just because something is the case doesn't mean it ought to be. Where existing norms or behaviours are unethical, approaches that encourage cooperation through descriptive facts thereby risk propagating unethical norms which reflect what is the case, rather than what ought to be.\nTo mitigate the is-ought gap, we turn to normative ethics. Normative ethics is the study of practical means to determine the ethical acceptability of different courses of action (Woodgate and Ajmeri 2024). Normative ethics principles are justified by reason in philosophical theory. These principles are normative in that they are prescriptive, indicating how things ought to be, rather than descriptive, indicating how things are (Kim, Hooker, and Donaldson 2021).\nThe principle of maximin-to maximise the minimum experience is a widely respected fairness principle in normative ethics advanced by Rawls (1958). Rawls states that in a society with unequal distribution which is not to the benefit of all, those benefiting the least should be prioritised. We hypothesise that creating agents that promote the emergence of ethical norms, while avoiding the is-ought gap, is aided by an appeal to Rawlsian ethics (Woodgate and Ajmeri 2022).\nContribution We propose RAWL\u00b7E, a novel method to design socially intelligent norm-learning agents that consider others in individual decision making by operationalising the principle of maximin from Rawlsian ethics. A RAWL-E agent includes an ethics module that applies maximin to assess the effects of its behaviour on others.\nNovelty Operationalising Rawlsian ethics in learning agents to enable explicit norm emergence is a novel contribution. RAWL\u00b7E goes beyond existing works on norm-learning agents: Ajmeri et al.'s (2020) agents incorporate ethical decision-making, but do not incorporate learning. Agrawal et al. (2022) address the emergence of explicit norms, but optimise norms based on the sum of payoffs received by other agents, which might be unfair for some agents. Zimmer et al. (2021) and Balakrishnan et al. (2022) operationalise Rawlsian ethics in learning agents, but do not consider the role of norms. As a RAWL-E agent gains experience, it learns to achieve its goals whilst behaving in ways that support norms which prioritise those who are least advantaged in situations of unequal resource distribution."}, {"title": "2 Related Works", "content": "Research on combining normative ethics with norm emergence and learning is relevant to our contributions.\nNormative Ethics and Norm Emergence Ethical norm emergence has been examined through the lens of agent roles. Anavankot et al. (2023) propose norm entrepreneurs that influence the dynamics of norm-following behaviours and thus the emergence of norms. Vinitsky et al. (2023) study norm emergence through sanction classification. Levy and Griffiths (2021) manipulate rewards using a central controller to enable norm emergence. Neufeld et al. (2022) use deontic logic to implement a normative supervisor module in RL agents. Yaman et al.'s (2023) agents sanction one another to encourage effective divisions of labour. Maranh\u00e3o et al. (2022) formally reason about normative change. However, a gap remains in agents learning norms based on what ought to be the case, rather than what is. We address this gap by implementing principles from normative ethics to encourage the emergence of norms that can be justified independently to a specific situation.\nTraditional approaches encourage norm emergence by maximising social welfare-how much society as a whole gains. Shoham and Tennenholtz (1997) promote highest cumulative reward. Yu et al. (2014) utilise majority vote. Agrawal et al. (2022) sum the payoffs for different stakeholders. Focusing on social welfare alone may lead to situations where a minority is treated unfairly for the greater good (Anderson, Anderson, and Armen 2004), and mutual reward does not specify how to coordinate fairly (Grupen, Selman, and Lee 2022). To mitigate weaknesses associated with only maximising social welfare, we implement Rawlsian ethics, emphasising improving the minimum experience.\nNormative Ethics and Learning Jing and Doorn (2020) emphasise the importance of focusing on positive standards alongside preventative ethics, which involves negative rules denoting wrongdoing. As ethics is dynamic, it may not always be possible to determine which behaviours to restrict. Svegliato et al. (2021) implement divine command theory, prima facie duties, and virtue ethics; Nashed et al. (2021) implement the veil of ignorance, golden rule and act utilitarianism. Dong et al. (2024) optimise federated policies under utilitarian and egalitarian criteria. A gap exists, however, in applying normative ethics in RL to norm emergence. RAWL E addresses that gap."}, {"title": "3 Method", "content": "We now present our method to design RAWL E agents who operationalise Rawlsian ethics to support the emergence of ethical norms.\n3.1 Schematic\nDefinition 1. Environment E is a tuple (AG, D,N) where,\nAG = {ag1, ..., agn} is a set of agents; D is the amount of total resources; N is the set of norms.\nDefinition 2. A RAWL\u00b7E agent is a tuple : (d, v, G, A, Z, NM, EM, DM\u3009 where, d\n\u2208Dis the amount of resources to which the agent has access; v is a measure of its well-being; G is the set of goals 91,..., gi;\nA are the actions available to the agent to help achieve its goals; Z are the behaviours which the agent has learned; NM is its norms module; EM is its ethics module; and DM is its interaction module.\nDefinition 3. A goal g\u2208 Gis a set of favourable states an agent aims to achieve.\nDefinition 4. A behaviour \u03da \u2208 Z is a tuple (pre, act), where pre \u2208 Expr is its precondition; act \u2208 Expr is its action; and Expr is any logical expression that can be evaluated as either true or false based on the values of its variables.\nA behaviour has a precondition denoting the conditions within which the behaviour arises, and a postcondition, which is the action implied by the precondition. Each agent keeps a record of their learnt behaviours.\nA behaviour is encoded in the form of an if-then rule:\n<behaviour> ::= IF  THEN \nDefinition 5. A norm n \u2208 N, where N C Z, is a behaviour adopted by a society.\nNorms are the prescription and proscription of agent behaviour on a societal level (Savarimuthu et al. 2013).\nDefinition 6. N, where N \u2286 Z, denotes the set of emerged norms, i.e., the behaviours adopted by the society as norms, which form a normative system describing a society.\nNorms emerge when the same behaviours are adopted by other agents (Tuomela 1995). Norm emergence is accepted to have happened when a predetermined percentage of the population adopt the same behaviours. As following previous literature, we assume a norm to have emerged when it reaches 90% convergence (Kittock 1995).\nDefinition 7. A sanction F represents a positive or negative reaction to behaviour which provides feedback to the learner in the form of a reward.\nSanctions are positive or negative reactions to behaviour which help enforce norms. A self-directed sanction is a sanction directed towards and affecting only its sender (Nardin et al. 2016). The self-directed sanction provides feedback to the learner as a reward.\n3.2 Interaction and Norm Learning\nTo make decisions and pursue their goals, RAWL-E agents use ethics module, norms module, and interaction module."}, {"title": "Ethics Module", "content": "Ethics module, EM, assesses how actions affect the well-being of other agents. To evaluate the well-being of others, RAWL\u00b7E agents implement Rawlsian ethics. Adapted from Leben (2020), an ethical utility function u(d) \u2192 (v) models a distribution of resources, where d is a vector of resource distribution which sums to D, the amount of total resources, and (v) is a measurement of well-being for agents considering that resource distribution. Where w is a vector of inputs (e.g., observed well-being of agents), Rawlsian ethics is expressed as:\n$MA(d) = \\min_w u(d, v_i)$\n(1)\nVia MA(d), the ethics module evaluates whether the agent's action improves the minimum experience. It generates a positive self-directed sanction \\S if an action improves the minimum experience, and a neutral or negative sanction -\\S if it does not change or worsens. In analogy to the real world, a positive sanction represents happiness from helping others, while a negative sanction represents guilt. To implement MA, ethics module takes as input Ut and Ut+1, where U is a vector of well-being V1,..., Un for all agents ag1,..., agn at times t and t + 1. Ethics module identifies the minimum experience minwu(d, v) at t and t + 1, storing the results in vmint and Umint+1, respectively. Therefore:\n$F_{t+1}(s_t, s_{t+1}) =\n\\begin{cases}\n\\xi, & \\text{if } V_{mint} < V_{mint+1} \\\\\n0, & \\text{if } V_{mint} = V_{mint+1} \\\\\n-\\xi & \\text{if } V_{mint} > V_{mint+1}\n\\end{cases}$\n(2)\nAlgorithm 1 describes internals of the ethics module. The inputs are Ut and Ut+1. To implement MA, store Umin, and Umint+1 (lines 1\u20132). Compare Umint and Umint+1 to assess how action a taken in st affected Umint+1 (Line 3). Generate sanction Ft+1 (Lines 4-7). Output Ft+1 for interaction model to combine with environmental reward rt+1 through reward shaping so that rt+1 = rt+1 + Ft+1. (Line 8)."}, {"title": "Norms Module", "content": "Norms module, NM, tracks patterns of behaviour the agent learns. Norms module stores behaviours in a behaviour base and norms in a norm base. For each behaviour, it computes and stores the numerosity num, obtained from the number of times the behaviour is used, and the reward r' (described in interaction module) received from using the behaviour. The fitness of each behaviour \u03c4 is obtained from numr' decayed over time. Where \u03b7 is the age of the behaviour and \u03bb is the decay rate,\n$\\tau(\\zeta) = num\\cdot r'\\cdot \\lambda^{\\eta}$\n(3)\nAlgorithm 2 describes the internals of the norm module. Inputs to the norm module include vt, at, rt+1, where vt is the precondition obtained from the agent's view of state st (for scalability, vt is a subset of st); at is the action taken in St. Norms module searches the behaviour base to retrieve a behaviour matching (pre, act) to vt, at (line 1). If there is a matching behaviour, update \u03c4(\u03b6) (lines 2\u20133). If there is no match, behaviour learner creates a new behaviour with vt, at and adds it to behaviour base (lines 5\u20136). Every tclip_behaviours steps, if behaviour base exceeds the maximum capacity, behaviour base is clipped to the maximum capacity by removing the least fit behaviours (lines 8-9). Norms module compares behaviour base with norm base shared by the society and stores emerged norms in norm base (line 10)."}, {"title": "Interaction Module", "content": "Interaction module, DM, implements RL with deep Q network (DQN) architecture (Sutton and Barto 2018). Via DQN, RAWL\u00b7E agent learns a behaviour policy to achieve goals while promoting ethical norms. At each time step t, agent selects a batch of B random experiences to train its Q network against its target network, computing the Huber loss (Huber 1964). To prevent overfitting, every C steps weights of target network are updated to weights of the Q network \u03b8. At each step, agent receives an observation of the environment, a vector of features x(s) visible in state s, which it stores in the experience replay buffer. Each feature of x(s) coresponds to a feature in the agent's DQN. With probability e, agent selects an action randomly or using DQN. Using DQN, actions a \u2208 A are selected which policy \u03c0(s) estimates will maximise expected return and help achieve goals G. Agent acts asynchronously and receives a reward from its environment r. DM obtains shaped reward Ft+1 from EM. To encourage an agent to learn behaviours which promote ethical norms whilst pursuing goals, DM combines self-directed sanction Ft+1 with environmental reward rt+1 through reward shaping so that rt+1 = rt+1 + Ft+1. Transition (at, St, St+1, rt+1) is stored in experience replay buffer. DM obtains view vt from state st and passes vt to NM for norm learning."}, {"title": "4 Simulation Environment", "content": "We evaluate RAWL-E agents in a simulated harvesting scenario where they forage for berries. Cooperative behaviours may emerge, such as agents learning to throw berries to one another. To demonstrate the efficacy of modular ethical analysis, the scenario includes environmental rewards for cooperation.\n4.1 Scenario\nThe environment represents a cooperative multi-agent scenario with a finite population of agents on a o \u00d7 p grid. Time is represented in steps. At the beginning of each episode, the grid is initialised with k = 4 agents, and binitial = 12 berries at random locations. An agent begins with hinitial = 5.0 health. Agents may collect berries, throw berries to other agents, or eat berries. An agent receives a gain in health hgain = 0.1 when it eats a berry. Agent health decays hdecay = -0.01 at every time step. An agent dies if its health level reaches 0 and episode ends when all agents have died.\nAgents act asynchronously, in a different random order on each step of the simulation. At each step, each agent agi decides to move (north, east, south, west), eat a berry, or throw a berry to another agent agj if agi has at least hthrow = 0.6 health. When an agent has eaten a berry, a berry regrows at a random location on the grid. At each step, an agent forages for a berry in its location. An agent observes its health, its berries, distance to the nearest berry, and each agent's well-being. Well-being is represented by a function of an agent's health and number of berries it has in its bag:\n$ag_{well-being} = \\frac{ag_{health} + (ag_{berries} \\times h_{gain})}{h_{decay}}$\n(4)\nFor each agent, at each time step:\n(1) Receive observation st\n(2) Choose a using DQN: move (north, south, east, west), eat, throw\n(3) Forage for berry; update health (hdecay at each step, hgain if berry eaten)\n(4) Receive transition: rt+1, St+1, check if done\n(5) Pass transition to Q network to learn\n(6) Every C steps, update \u03b8 of target network\n(7) Pass transition to norms module, update norm base\n(8) Check health, if agent has died remove from the grid\nFor testing, we run each simulation e = 2000 times, with each simulation running until all agents have died, or a maximum of tmax = 50 steps. We select these numbers empirically. Agents clip behaviour every tclip_behaviours steps, clip norm base every tclip_norms steps, and check for emerged norms every step.\n4.2 Society Types for Evaluation\nWe implement two types of agent societies for evaluation.\nBaseline Cooperative: DQN A society consists of standard DQN agents who do not implement an ethics module but receive environmental rewards for cooperative behaviour. DQN agent makes decisions according to its observations and expected reward.\nRAWL-E: Rawlsian DQN A society of RAWL-E agents act in ways that promote Rawlsian ethics. RAWL-E agent makes decisions according to its observations and expected reward, considering the well-being of all agents.\n4.3 Environmental Rewards\nAn agent receives a positive reward if it forages for a berry in a location where a berry is growing, if it eats a berry when it has berries in its bag, or if it survives to the end of the episode. An agent receives a negative reward if it attempts to eat or throw a berry to another agent when it doesn't have any, or if it dies. Agent deaths are included in raw rewards to provide incentives for societies to survive.\nSelf-directed sanction of a RAWL-E agent is 0.4 if the minimum experience was improved, -0.4 if the agent could have improved the minimum experience but did not (i.e., if an action was available to improve the minimum experience but the agent chose an alternative action), and 0 otherwise. To avoid obvious results by giving RAWL-E agents additional rewards, we normalise rewards between baseline and RAWL E agents such that RAWL-E agents receive lower raw rewards. This allows for fairer comparison between societies.\n4.4 Metrics and Hypotheses\nEmerged norms N describe the standards of expected behaviour in a society. To evaluate N, we examine cooperative norms which emerge by their fitness and numerosity. We assess the effects of those norms on societal outcomes with the following metrics and hypotheses.\nVariables To quantitatively assess societal outcomes, for each simulation run, we record the following variables:\nV1 (agwell-being) Number of days an agent has left to live, a function of number of berries an agent carries and their current health (Equation 4).\nV2 (agresource) Number of berries eaten by an agent.\nMetrics To assess fairness on an individual and at societal level, we compute the metrics M\u2081 (inequality) and M2 (minimum experience) on each variable.\nM\u2081 (inequality) Gini index across the society. Lower is better. O denotes perfect equality; 1 denotes perfect inequality.\nM2 (minimum experience) Lowest individual experience across the society. Higher is better.\nTo assess the sustainability of the society, we compute the metrics M3 (social welfare) and M4 (robustness).\nM3 (social welfare) How much society as a whole gains (Mashayekhi et al. 2022). Higher is better.\nM4 (robustness) Length of episode. Higher is better.\nHypotheses We evaluate the following hypotheses. Null hypotheses for each indicate no difference.\nH\u2081 (minimum experience) Norms emerging in RAWL.E society lead to higher minimum individual experience.\nH2 (inequality) Norms emerging in RAWL\u00b7E society lead to lower inequality.\nH3 (social welfare) Norms emerging in RAWL-E society lead to higher social welfare.\nH4 (robustness) Norms emerging in RAWL E society lead to higher robustness.\nFor each hypotheses, we test the significance and compute effect size. For significance, we conduct Mann-Whitney U test which is a non-parametric test for comparing two independent groups (Mann and Whitney 1947). We use Mann-Whitney U because the sample size k is small. p < 0.01 indicates significance. For effect size, we compute Cohen's d which assesses the magnitude of difference between means, standardised by the pooled standard deviation (Cohen 1988), calculated as $\\frac{\\mu_1 - \\mu_2}{S_{pooled}}$, where <0.2 (negligible), [0.2,0.5) (small), [0.5,0.8) (medium), and \u22650.8 (large)."}, {"title": "5 Experimental Results", "content": "To evaluate the behaviour of RAWL-E agents, we run agents in two experiment scenarios with different demonstrations of unequal resource allocation. For testing, we run e = 2000 episodes, with each episode running until tmax = 50, or until the agents die. For qualitative analysis, we examine the emerged norms and actions promoted. For quantitative analysis, we examine fairness and sustainability metrics.\n5.1 Emerged Norms\nRAWL E agent's norms model learns emerging norms from patterns of behaviour. To evaluate these norms, we run e episodes for each society and store N from each episode. At each step, agents compare behaviour bases and store norms repeated by 90% of agents in shared norm base N."}, {"title": "Summary of Findings", "content": "Our results support our hypotheses. Our main findings are: (1) in a society of RAWL E agents, social welfare is improved, indicated by higher cumulative resource consumption, (2) inequality is reduced, indicated by a lower Gini index, (3) minimum individual experience is higher than the baseline; the combination of reduced inequality and improved minimum individual experience suggests that RAWL.E societies are fairer, and (4) RAWL\u00b7E societies survive longer, indicating higher robustness. Together, these results suggest RAWL\u00b7E agents promote the emergence of norms which improve fairness and social welfare, thereby promoting considerate behaviour, further leading to a more sustainable society.\nWe observe that results are better (higher fairness, social welfare, and robustness) for RAWL-E than baseline in both scenarios. However, the difference is more apparent in the allotment harvest than capabilities harvest. We attribute this difference to the fact that in the capabilities harvest agents are in a more confined space than the allotment harvest, and must navigate around one another to reach berries.\nThreats to Validity Threats arise from the simplicity of our scenarios. While this abstraction limits real-world applicability, our focus is on demonstrating the operationalisation of normative ethics rather than capturing realism. To address this threat, we present our agent architecture decoupled from the environment. Also, using shaped rewards to operationalise ethics offers an adaptable method compatible with various RL algorithms and diverse scenarios."}, {"title": "6 Discussion and Conclusion", "content": "Developing agents that behave in ways that promote ethical norms is crucial for ethical MAS. Operationalising principles from normative ethics in individual decision making helps address the problem of deriving an ought from an is. Our results show that, compared to societies of baseline agents who don't implement normative ethics, RAWLE agents societies have higher social welfare, and are more fair by higher minimum experience and reduced disparity.\nDirections and Key Takeaways Applying normative ethics presents challenges, and there is often disagreement on the subject (Moor 2006). Conflicts may arise when different principles promote different actions (Robinson 2023). Additionally, the application of a principle may lead to unintuitive outcomes or fail to promote one action over another (Guinebert 2020). Utilising a variety of principles in reasoning is beneficial to examine scenarios from different perspectives, improving the amplitude of ethical reasoning. Directions include operationalising a variety of principles, and investigating circumstances in which principles conflict.\nWe utilise rewards to promote learning ethical behaviour when not all states can be known in advance. However, modifying rewards combines different objectives in a single numerical scale, allowing implicit comparisons between outcomes (Nashed, Svegliato, and Blodgett 2023). Directions include combining promotion of ethical behaviour with explicit prevention of unethical outcomes.\nThe scenarios we implement are abstracted to demonstrate how the method can be implemented. Operationalising normative ethics provides a mechanism to systematically assess the rightness and wrongness of actions in a range of situations (Binns 2018). Applying our method to more complex and real world scenarios, with a range of different RL algorithms, is another direction for future work.\nReproducibility Our codebase is publicly available (Woodgate, Marshall, and Ajmeri 2024)."}, {"title": "A Details of Experimental Setups", "content": "This section provides details about the computing infrastructure and hyperparameter selection.\nA.1 Computing Infrastructure\nWe conducted the simulation experiments on a workstation with Intel Xeon Processor W-2245 (8C 3.9 GHz), 256GB RAM, and Nvidia RTA A6000 48GB GPU.\nA.2 Hyperparameter Selection\nTable 4 lists the simulation parameters and range of values tried per parameter. Results are consistent across the range of values tried, with societies of RAWL\u00b7E agents having higher social welfare, fairness, and robustness than societies of baseline agents."}, {"title": "B Environmental Rewards", "content": "To avoid obvious results by giving RAWL-E agents additional rewards, we normalise rewards between baseline and RAWL E agents such that RAWL-E agents receive lower raw rewards. This allows for fairer comparison between societies."}, {"title": "C Metrics", "content": "Here, we provide further details about the metrics used to evaluate societies of RAWL-E and baseline agents.\nTo assess the fairness of a society, we compute M\u2081 (inequality) and M2 (minimum experience).\nM\u2081 (inequality) Examining the inequality of a society to assess fairness is supported by the principle of egalitarianism, which states that disparity amongst members should be minimised (Murukannaiah et al. 2020). We use the Gini index for inequality as it is well studied and has been used previously in MAS (Endriss 2013).\nM2 (minimum experience) Examining the minimum individual experience to assess fairness is justified by Rawlsian ethics, which argues that those who benefit the least should be prioritised (Rawls and Kelly 2001).\nThe fairest society will have the lowest inequality and highest minimum individual experience. However, the notion of fairness is abstract and achieving perfect fairness is challenging, if not impossible (Dignum 2021). We thus aim for satisfactory outcomes that promote equitable systems, which have a higher goal of fairness, but might not be perfect.\nTo assess the society's sustainability, we compute the metrics M3 (social welfare) and M4 (robustness).\nM3 (social welfare) Measuring social welfare (how much society as a whole gains (Mashayekhi et al. 2022)) is supported by the principle of utilitarianism, which states that ethical actions are those which maximise utility (Ong et al. 2024).\nM4 (robustness of society) Robustness relates to the degree a society is sensitive to exogenous influence, exhibited as the ability to resist and withstand adversity (Mu\u00f1oz, Billsberry, and Ambrosini 2022)."}, {"title": "D Results for Emerged Norms", "content": "D.1 Results for Emerged Cooperative Norms\nA norm is emerged when it is adopted by over 90% of the population. In societies of RAWL\u00b7E agents, the cooperative norms which emerge have higher fitness and are used more frequently, indicated by higher numerosity."}, {"title": "D.2 System of Emerged Norms", "content": "Figures 6 and 7 display the system of emerged norms N in baseline and RAWL-E societies. To obtain N, we run e = 2000 episodes for tmax = 50 steps and track the norms which emerge in each episode. We combine N for e to obtain the list of all norms which emerge. Norms with \"throw\" consequent are cooperative, as throwing is an act of agents helping one another. To distill specific norms into generalised rules, we aggregate antecedent conditions which produce the same outcome. For example, all instances of the condition \u201cno berries\u201d result in the consequent \u201cmove\u201d. Therefore, specific norms are aggregated to the generalised rule of: \nIF  THEN"}, {"title": "E Simulation Results", "content": "Table 8 provides additional details of the simulation results. We observe that societies of RAWL-E agents have significantly lower inequality than societies of baseline agents for both agwell-being and agresource. The effect is medium to large in both scenarios: 1.58 for agwell-being and 0.63 for agresource in the capabilities harvest; 1.58 for agwell-being and 1.32 for agresource in the allotment harvest.\nFor minimum experience, societies of RAWL-E agents show significantly higher results than baseline societies for agwell-being in both scenarios with a large effect, however, the minimum experience is not significantly different for agresource.\nSocial welfare is significantly higher for agwell-being in societies of RAWL E agents than baseline societies in the allotment harvest with a medium effect of 0.64. The difference in social welfare is not significant for agresource.\nFurther, in both scenarios, societies of RAWL-E agents are more robust than baseline societies; however, the effect is negligible (0.18 in capabilities harvest and 0.11 in allotment harvest).\nIn both scenarios, we observe that in societies of RAWL.E agents cooperative norms which emerge are more generalised than cooperative norms emerging in societies of baseline agents. For example, in Figure 6b a general norm emerges:\nIF  THEN \nIn contrast, the cooperative norms which emerge in baseline societies in Figure 6a are more specialised than in RAWL E societies. This indicates that cooperative norms in RAWL E societies cover a wider range of circumstances."}]}