{"title": "What's Wrong? Refining Meeting Summaries with LLM Feedback", "authors": ["Frederic Kirstein", "Terry Ruas", "Bela Gipp"], "abstract": "Meeting summarization has become a criti- cal task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context un- derstanding compared to traditional methods. However, they still struggle to maintain rele- vance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 au- tomatically generated meeting summaries an- notated by humans on nine error types, includ- ing structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given sum- mary measured by relevance, informativeness, conciseness, and coherence. This post-hoc re- finement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar com- plex text generation tasks requiring robustness, action planning, and discussion towards a goal.", "sections": [{"title": "Introduction", "content": "Meeting summaries are essential for professional conversations, they serve as a reference for subse- quent processes, update absentees, and reinforce the most important topics discussed. The growing importance of summarization systems is evident from the recent release of tools in virtual meeting software (e.g., Zoom\u00b9, Microsoft Teams\u00b2, Google Meet\u00b3). Still, meeting summarization faces chal- lenges, such as handling spoken language idiosyn- crasies and identifying salient content (Kirstein"}, {"title": "Related Work", "content": "Meeting Summarization and its parent domain dialogue summarization are transitioning from tra- ditional encoder-decoder models to LLMs. Tradi- tional models, such as BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020a), improved through techniques tailored to specific challenges like language, structure, comprehension, speaker, salience, and factuality (Kirstein et al., 2024a,b). These models integrated methods such as AMR- graphs for speaker relations (Hua et al., 2023), role vectors for speaker correlation (Asi et al., 2022; Naraki et al., 2022), and additional train- ing stages to bridge the gap between pre-training on written texts and spoken dialogue tasks (Raf- fel et al., 2020; Khalifa et al., 2021; Lee et al., 2021b). Recently, LLMs have been explored for meeting summarization by prompting the model to create a TL;DR (Laskar et al., 2023; Kirstein et al., 2024b), showing comparable performance to specialized encoder-decoder models but with better context comprehension. They thereby use LLMs without any adaptations and serve as the first works to report on LLM performance on meeting summarization. Our work examines the effective- ness of LLMs as post-processors for summaries, assessing if this approach can achieve high-quality summaries without requiring techniques tailored to a specific challenge of meeting summarization. We compare this against original summaries, single- LLM baselines, and human summaries, providing an updated benchmark for LLMs in meeting sum- marization. For the creation of QMSum Mistake, we extend the work by Kirstein et al. (2024b), re- fining their definition of errors.\nSelf-correction methods have been extensively studied in recent literature (Pan et al., 2023), in- cluding training-time correction strategies like"}, {"title": "QMSum Mistake Dataset", "content": "QMSum Mistake consists of 200 samples, with 169 (85%) automatically created meeting summaries annotated on nine error types (Section 3.1) and 31 error-free summaries serving as controls to analyze if the mistake identification is too sensitive. Ta- ble 1 provides dataset statistics. The samples stem from QMSum's (Zhong et al., 2021) training and test sets, including AMI (staged business meetings) (Carletta et al., 2005), ICSI (academic meetings) (Janin et al., 2003), and parliament meetings. As gold summaries lack typical errors of automatic summaries, we generate summaries using encoder- decoder models (i.e., LED (Beltagy et al., 2020), DialogLED (Zhong et al., 2022), PEGASUS-X (Phang et al., 2022)) for more severe mistakes in automatic summaries such as coreference and struc- ture errors and LLMs (i.e., GPT-3.5, Phi-3 mini 128k (Abdin et al., 2024)) for subtle errors such as relevance. Models have a context size of at least 16k to fit the entire meeting in the input, use default settings, and generate up to 200 tokens to match gold summary lengths. Table 9 shows examples"}, {"title": "Observable errors", "content": "We refine existing error types (Kirstein et al., 2024b; Chang et al., 2024) into nine error types with minimal overlap. Table 2 holds the short defi- nitions. Preliminary testing and annotator feedback inform the refinement of the error types and point out overlap in error definitions, making a clear dis- tinction difficult. This leads to major adaptations to precisely delimit the repetition, incoherence, struc- ture, and linguistic inaccuracy errors, while the omission errors undergo minor tweaks in wording. Hallucination errors are packed into a single cate- gory to reduce overlap for edge cases between these two. The initial observations further indicate that errors so far were designed to capture missing or in- correct information, not the inclusion of unrelated content, which our summary-generating models tend to generate. Thus, we add the 'Irrelevance' category."}, {"title": "Mistake Identification", "content": "Table 3 shows GPT4's accuracy in identifying summarization-related errors (Section 3.1) on the QMSum Mistake dataset. We chose GPT4 for its context size, understanding capabilities, robustness to handle spoken language, and superior results compared to Gemini (Team et al., 2024) and Phi (Abdin et al., 2024) in early experiments. We provide complementary analysis for the discarded models in Appendix B."}, {"title": "Mistake identification protocol (MIP)", "content": "We consider two prompting strategies to identify possible mistakes in a summary: direct and CoT prompting. In Direct prompting (Tyen et al.,"}, {"title": "Mistake identification discussion", "content": "While both setups achieve high accuracy scores, the single-instance setup struggles to consistently beat an always true baseline on the whole QM- Sum Mistake dataset. Overall, this aligns with the hypothesis behind current LLM-based automatic metrics that leverage similar models to assess text characteristics such as fluency, readability, or clar- ity (Li et al., 2024).\nImpact of mistake identification protocol on ac- curacy of error detection. Comparing results across the four MIP variants (Table 3a), we find that accuracy in detecting mistakes increases sig- nificantly on all error types when using a multi- instance setup compared to the single-instance ap- proach. While the difference between single and multi-instance is comparably small (~7%) for both omission error types (T-OM, P-OM), the accuracy can deviate by up to ~29.5% in the case of HAL. Figure 2 shows that the average accuracy across all error types reveals a gain of at least 13.5% when using multiple LLM instances for detection, which aligns with recent works (Huang et al., 2024; Tyen et al., 2024). We observe the average false negative rates decrease by ~27% from single (CoT) (30.0%, worst overall) to multi (CoT) (3.4%, best overall). We hypothesize that the weaker single-model per- formance may stem from the extended content and its additional tasks, which must be handled by a single model compared to the multi-instance set- ting. As a result, the single-instance approach is unable to process the long dependencies, which limits contextualization and comprehension (Lee"}, {"title": "Summary Refinement", "content": "Building on the finding that an LLM can iden- tify typical meeting summarization errors (Sec- tion 4.2), we analyze how the quality of original predicted summaries changes when an LLM re- fines them based on identified mistakes. Our multi- model refinement approach mimics a four-stage human review process to form a refinement proto- col (Figure 1): (1) locating errors using the best- performing MIP, (2) generating feedback on iden- tified errors (feedback protocol), (3) structuring feedback (transfer protocol), and (4) refinement. Following, we explore the setup of the feedback and transfer protocols to derive a refinement proto- col for meeting summarization."}, {"title": "Feedback protocol (FP)", "content": "Feedback on an error can range from pointing out its existence, similar to someone highlighting a text passage and leaving a short comment, to in-depth explanations of what is wrong with the marked passage and rewrite suggestions. Following this analogy, our feedback protocol consists of an es- sential and an additional detail part. The essential part includes minimal feedback on the existence of an error type and a short explanation about why and where it was detected, but may not mention all error instances. The additional detail part considers three optional information sources: CoT explanation (Wei et al., 2023), correction suggestion (Zhang et al., 2023a), and the original transcript. CoT explanation, the output of MIP's CoT prompting (Section 4.1), contains all observed error instances and details on why they are considered errors. It helps the refinement model derive a rewriting plan through detailed, structured information but may lead to confusion if the reasoning is wrong (Tyen et al., 2024). Correction suggestions provide ex- amples of how to correct the error, either as tips or precise rewrites that can be directly applied. The transcript provides all available information in its original form, allowing it to decide whether to ac- cept or reject the feedback and how to integrate it. The three optional information sources can be combined, determining how much information is required and if feedback without a transcript is as informative as adding the transcript for lookup."}, {"title": "Transfer protocol (TP)", "content": "We consider two approaches for structuring feed- back for the refinement model: direct feedback (Mousavi et al., 2023) and consolidation (Zhang et al., 2023a). Direct feedback transfers derived feedback without additional processing, stating whether an error type is observed or not. In the case of CoT explanation, it informs the model step- by-step which sentences are erroneous or error-free, why they are correct or incorrect, and what should be changed (or kept) to have a correct sentence. Consolidation considers only identified errors and generates an editing plan using an intermediate LLM, extracting what information to add, remove, or alter from the feedback protocol. The consolida- tion protocol does not affect an appended transcript."}, {"title": "Experimental setup", "content": "We refine the erroneous summaries from QMSum Mistake using each refinement protocol variant with the multi-instance CoT-prompted MIP. GPT4 is used as the backbone model for the refiner and optional intermediate LLM to consolidate feed- back, with other model families explored in Ap- pendix B. We focus the following experiment on evaluating how summary quality changes based on feedback and show a setup for a meeting sum- marization refinement protocol. We consider a one-shot improvement here and provide insights on multi-round improvement in Appendix B.3. To help understand and categorize the quality changes, we report metric results for the original erroneous summaries (ORIG), error-free QMSum gold summaries (GOLD), summaries generated by one GPT4 (GPT-S), and summaries refined by one GPT4 (GPT-R) as references in Table 4."}, {"title": "Evaluation approach", "content": "ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020b), established metrics for meeting summary evaluation (Kirstein et al., 2024a), yield scores too similar for interpretation across protocol variants (see Table 8). As human evaluation on all gener- ated refined summaries (total ~3.4k) is infeasible, we use the LLM-based metric AUTOCALIBRATE (Liu et al., 2023) to report Likert scores on rele- vance (REL), informativeness (INF), conciseness (CON), and coherence (COH). Since this metric is not developed for meeting summarization, we assess alignment with human judgment by hav- ing six annotators rate a subset of 200 summaries according to AUTOCALIBRATE prompts (inter- annotator agreement (Krippendorff's alpha): REL: 0.775, INF: 0.798, CON: 0.833, COH: 0.803). As the LLM-based evaluation aligns sufficiently with annotator labels (accuracy: 89.1%), we use AUTO- CALIBRATE as our main quality proxy. Neverthe- less, we manually check every fourth score tuple and model reasoning to confirm alignment with the evaluation task and human judgment. In case of misalignment, three annotators would instead rate the summary. As AUTOCALIBRATE only assesses specific characteristics and does not con- sider omission, hallucination, or repetition, we also set up a GPT4-powered ranking system, motivated by typical human annotation rankings, based on ob- servable errors from Section 3.1 (see Appendix D for prompt details). We follow the approach used before to ensure reliability and alignment with hu- man annotations (inter-annotator agreement: 0.784 Krippendorff's alpha, GPT4 acc.: 92.1%)."}, {"title": "Summary refinement discussion", "content": "Influence of feedback and transfer protocols on quality. Table 4 shows the overall ranking and Likert scores of each refinement protocol vari- ant. ORIG summaries are consistently ranked low- est, indicating that refinement positively influences quality, as observed in the assigned Likert scores. Having only the essential part in the FP leads to minor improvements in ranking and Likert scores for both TPs compared to the ORIG summary, but falling behind the scores of most protocol variants using additional information. This indicates that pointing out errors on a high level already leads to quality improvement. The result is expected, as the minimalistic explanation may not contain every error instance, precise reasoning, or all information to resolve specific errors such as omission. Com- paring the essential parts scores of both TPs reveals that the Likert scores and rankings differ notably"}, {"title": "Final Considerations", "content": "In this paper, we investigated GPT4's ability to find mistakes in a given meeting summary and refine them accordingly. We found that GPT4 achieves a high accuracy of ~89% on average, measured against human labels, in identifying typical mis- takes (e.g., repetition of content) when using a ded- icated model instance paired with CoT prompting to identify individual errors. However, it struggles to identify similar and subjective errors, such as hallucination (72% acc.) with omission and irrele- vance (81% acc.). We showed strong evidence that a dedicated LLM can refine a summary based on identified errors. By providing a CoT explanation for each error type containing reasoning why and where an error was observed, we improve the qual- ity of relevance, informativeness, conciseness, and coherence significantly. These refined summaries are comparable in quality, with error-free gold sum- maries. Our post hoc refinement approach can be applied to refine meeting summaries generated by traditional models and LLMs and marks an early entry into methods that allow the full potential of LLMs for meeting summarization. We leave the development of more sophisticated refinement pro- tocols, e.g., using multi-agent discussion, and the application of our multi-LLM approach to similar complex text generation tasks (e.g., story writing to reflect on given setting) and real-world applications (e.g., assisting LLM agents to check the outcome to a task) to future work. We release QMSum Mistake to encourage research on refinement."}, {"title": "Potential Impact", "content": "The multi-LLM approach proposed here, influ- enced by psychological observations on produc- tivity and collaboration, exemplifies how other aca- demic fields can inform NLP research (Wahle et al., 2023b). This work demonstrates the potential for enhancing complex text generation tasks requiring robust output such as machine translation (Feng et al., 2024), reasoning (Kalyanpur et al., 2024), question answering (Kim et al., 2024), or paraphras- ing (Becker et al., 2023; Wahle et al., 2023a), that may benefit from an output-challenging system that assesses content alignment. By incorporating multi- LLM strategies and personalization, we open new avenues for improving NLP outputs across various applications, underscoring the value of interdisci- plinary approaches in advancing NLP technologies and their real-world applicability."}, {"title": "Limitations", "content": "Although our proposed QMSum Mistake might seem small (i.e., 200 samples), its size is com- parable to the original QMSum dataset (i.e., 232 samples). We contribute to extending the original dataset with careful human error annotations for almost all examples available. Another possible limitation in our work is the use of only GPT4 in our main experiments. We chose GPT4 because of its large context size (e.g., 128k tokens) and bet- ter initial results in identifying errors. Evaluating and error annotation and refinement for multiple models by humans would be time-consuming and financially unfeasible. However, we report the de- tailed results in Appendix B to provide insights on other language families and different models (e.g., Phi (Abdin et al., 2024), Gemini (Team et al., 2024)) considered in our study. We evaluate their performance on mistake identification and quality changes when refining a summary."}, {"title": "Ethics Statement and Broader Impact", "content": "Our research abides by ethical guidelines for AI research and is committed to privacy, confidential- ity, and intellectual property rights. We've ensured that the datasets in our study, publicly available, do not house sensitive or personal details. While our study leverages existing resources and generative models, it's important to note that these models can possess biases and may occasionally generate summaries with distortions, biases, or inappropri- ate content. To counteract this, we've configured our models to omit potentially harmful or unsafe content. While our research aims to enhance meet- ing summarization to benefit communication and productivity across sectors, we're acutely aware of the ethical challenges posed by AI in this do- main. Meeting summarization models must be wielded with respect to privacy and consent, es- pecially when processing sensitive or confidential material. It's paramount that these models neither violate privacy nor perpetuate harmful biases. As the field evolves, we stress the importance of main- taining these ethical considerations and encourage fellow researchers to uphold them, ensuring that AI advancements in meeting summarization are both beneficial and ethically grounded. An integral aspect of our ethical commitment is reflected in our approach to annotator recruitment and manage- ment. The team of annotators, consisting of interns, student assistants, and doctoral students, was metic- ulously selected through internal channels. This strategy was chosen to uphold a high standard of annotation quality\u2014a quality we found challeng- ing to guarantee through external platforms such as Amazon Mechanical Turk. Ensuring fair com- pensation, these annotators were remunerated in accordance with institutional guidelines for their respective positions. Further, flexibility in the anno- tation process was also a priority. Annotators had the freedom to choose their working times and en- vironments to prevent fatigue from affecting their judgment."}]}