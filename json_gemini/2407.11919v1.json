{"title": "What's Wrong? Refining Meeting Summaries with LLM Feedback", "authors": ["Frederic Kirstein", "Terry Ruas", "Bela Gipp"], "abstract": "Meeting summarization has become a criti-cal task since digital encounters have becomea common practice. Large language models(LLMs) show great potential in summarization,offering enhanced coherence and context un-derstanding compared to traditional methods.However, they still struggle to maintain rele-vance and avoid hallucination. We introducea multi-LLM correction approach for meetingsummarization using a two-phase process thatmimics the human review process: mistakeidentification and summary refinement. We release QMSum Mistake, a dataset of 200 au-tomatically generated meeting summaries an-notated by humans on nine error types, includ-ing structural, omission, and irrelevance errors.Our experiments show that these errors canbe identified with high accuracy by an LLM. Wetransform identified mistakes into actionablefeedback to improve the quality of a given sum-mary measured by relevance, informativeness,conciseness, and coherence. This post-hoc re-finement effectively improves summary qualityby leveraging multiple LLMs to validate outputquality. Our multi-LLM approach for meetingsummarization shows potential for similar com-plex text generation tasks requiring robustness,action planning, and discussion towards a goal.", "sections": [{"title": "1 Introduction", "content": "Meeting summaries are essential for professionalconversations, they serve as a reference for subse-quent processes, update absentees, and reinforce the most important topics discussed. The growingimportance of summarization systems is evidentfrom the recent release of tools in virtual meetingsoftware (e.g., Zoom\u00b9, Microsoft Teams\u00b2, GoogleMeet\u00b3). Still, meeting summarization faces chal-lenges, such as handling spoken language idiosyn-crasies and identifying salient content (Kirstein"}, {"title": "2 Related Work", "content": "Meeting Summarization and its parent domaindialogue summarization are transitioning from tra-"}, {"title": "3 QMSum Mistake Dataset", "content": "QMSum Mistake consists of 200 samples, with 169(85%) automatically created meeting summariesannotated on nine error types (Section 3.1) and 31error-free summaries serving as controls to analyzeif the mistake identification is too sensitive. Ta-ble 1 provides dataset statistics. The samples stemfrom QMSum's (Zhong et al., 2021) training andtest sets, including AMI (staged business meetings)(Carletta et al., 2005), ICSI (academic meetings)(Janin et al., 2003), and parliament meetings. Asgold summaries lack typical errors of automaticsummaries, we generate summaries using encoder-decoder models (i.e., LED (Beltagy et al., 2020),DialogLED (Zhong et al., 2022), PEGASUS-X(Phang et al., 2022)) for more severe mistakes inautomatic summaries such as coreference and struc-ture errors and LLMs (i.e., GPT-3.5, Phi-3 mini128k (Abdin et al., 2024)) for subtle errors such asrelevance. Models have a context size of at least16k to fit the entire meeting in the input, use defaultsettings, and generate up to 200 tokens to matchgold summary lengths."}, {"title": "3.1 Observable errors", "content": "We refine existing error types (Kirstein et al.,2024b; Chang et al., 2024) into nine error typeswith minimal overlap. Table 2 holds the short defi-nitions. Preliminary testing and annotator feedbackinform the refinement of the error types and pointout overlap in error definitions, making a clear dis-tinction difficult. This leads to major adaptations toprecisely delimit the repetition, incoherence, struc-ture, and linguistic inaccuracy errors, while theomission errors undergo minor tweaks in wording.Hallucination errors are packed into a single cate-gory to reduce overlap for edge cases between thesetwo. The initial observations further indicate thaterrors so far were designed to capture missing or in-correct information, not the inclusion of unrelatedcontent, which our summary-generating modelstend to generate. Thus, we add the 'Irrelevance'category."}, {"title": "4 Mistake Identification", "content": "Table 3 shows GPT4's accuracy in identifyingsummarization-related errors (Section 3.1) on theQMSum Mistake dataset. We chose GPT4 for itscontext size, understanding capabilities, robustnessto handle spoken language, and superior resultscompared to Gemini (Team et al., 2024) and Phi(Abdin et al., 2024) in early experiments. Weprovide complementary analysis for the discardedmodels in Appendix B."}, {"title": "4.1 Mistake identification protocol (MIP)", "content": "We consider two prompting strategies to identifypossible mistakes in a summary: direct and CoTprompting. In Direct prompting (Tyen et al.,"}, {"title": "4.2 Mistake identification discussion", "content": "While both setups achieve high accuracy scores,the single-instance setup struggles to consistentlybeat an always true baseline on the whole QM-Sum Mistake dataset. Overall, this aligns with thehypothesis behind current LLM-based automaticmetrics that leverage similar models to assess textcharacteristics such as fluency, readability, or clar-ity (Li et al., 2024).\nImpact of mistake identification protocol on ac-curacy of error detection. Comparing resultsacross the four MIP variants (Table 3a), we findthat accuracy in detecting mistakes increases sig-nificantly on all error types when using a multi-instance setup compared to the single-instance ap-proach. While the difference between single andmulti-instance is comparably small (~7%) for bothomission error types (T-OM, P-OM), the accuracycan deviate by up to ~29.5% in the case of HAL.Figure 2 shows that the average accuracy across allerror types reveals a gain of at least 13.5% whenusing multiple LLM instances for detection, whichaligns with recent works (Huang et al., 2024; Tyen et al., 2024). We observe the average false negativerates decrease by ~27% from single (CoT) (30.0%,worst overall) to multi (CoT) (3.4%, best overall).We hypothesize that the weaker single-model per-formance may stem from the extended content andits additional tasks, which must be handled by asingle model compared to the multi-instance set-ting. As a result, the single-instance approach isunable to process the long dependencies, whichlimits contextualization and comprehension (Lee"}, {"title": "5 Summary Refinement", "content": "Building on the finding that an LLM can iden-tify typical meeting summarization errors (Sec-tion 4.2), we analyze how the quality of originalpredicted summaries changes when an LLM re-fines them based on identified mistakes. Our multi-model refinement approach mimics a four-stagehuman review process to form a refinement proto-col (Figure 1): (1) locating errors using the best-performing MIP, (2) generating feedback on iden-tified errors (feedback protocol), (3) structuringfeedback (transfer protocol), and (4) refinement.Following, we explore the setup of the feedbackand transfer protocols to derive a refinement proto-col for meeting summarization."}, {"title": "5.1 Feedback protocol (FP)", "content": "Feedback on an error can range from pointing outits existence, similar to someone highlighting a textpassage and leaving a short comment, to in-depthexplanations of what is wrong with the markedpassage and rewrite suggestions. Following thisanalogy, our feedback protocol consists of an es-sential and an additional detail part. The essentialpart includes minimal feedback on the existence ofan error type and a short explanation about why andwhere it was detected, but may not mention all errorinstances. The additional detail part considers threeoptional information sources: CoT explanation(Wei et al., 2023), correction suggestion (Zhanget al., 2023a), and the original transcript. CoTexplanation, the output of MIP's CoT prompting(Section 4.1), contains all observed error instancesand details on why they are considered errors. Ithelps the refinement model derive a rewriting plan"}, {"title": "5.2 Transfer protocol (TP)", "content": "We consider two approaches for structuring feed-back for the refinement model: direct feedback(Mousavi et al., 2023) and consolidation (Zhanget al., 2023a). Direct feedback transfers derivedfeedback without additional processing, statingwhether an error type is observed or not. In thecase of CoT explanation, it informs the model step-by-step which sentences are erroneous or error-free,why they are correct or incorrect, and what shouldbe changed (or kept) to have a correct sentence.Consolidation considers only identified errors andgenerates an editing plan using an intermediateLLM, extracting what information to add, remove,or alter from the feedback protocol. The consolida-tion protocol does not affect an appended transcript."}, {"title": "5.3 Experimental setup", "content": "We refine the erroneous summaries from QMSumMistake using each refinement protocol variantwith the multi-instance CoT-prompted MIP. GPT4is used as the backbone model for the refiner andoptional intermediate LLM to consolidate feed-back, with other model families explored in Ap-pendix B. We focus the following experiment onevaluating how summary quality changes basedon feedback and show a setup for a meeting sum-marization refinement protocol. We consider aone-shot improvement here and provide insights on multi-round improvement in Appendix B.3.To help understand and categorize the qualitychanges, we report metric results for the originalerroneous summaries (ORIG), error-free QMSumgold summaries (GOLD), summaries generated byone GPT4 (GPT-S), and summaries refined by oneGPT4 (GPT-R) as references in Table 4."}, {"title": "5.4 Evaluation approach", "content": "ROUGE (Lin, 2004) and BERTScore (Zhang et al.,2020b), established metrics for meeting summaryevaluation (Kirstein et al., 2024a), yield scores toosimilar for interpretation across protocol variants(see Table 8). As human evaluation on all gener-ated refined summaries (total ~3.4k) is infeasible,we use the LLM-based metric AUTOCALIBRATE(Liu et al., 2023) to report Likert scores on rele-vance (REL), informativeness (INF), conciseness(CON), and coherence (COH). Since this metricis not developed for meeting summarization, weassess alignment with human judgment by hav-ing six annotators rate a subset of 200 summariesaccording to AUTOCALIBRATE prompts (inter-annotator agreement (Krippendorff's alpha): REL:0.775, INF: 0.798, CON: 0.833, COH: 0.803). As the LLM-based evaluation aligns sufficiently withannotator labels (accuracy: 89.1%), we use AUTO-CALIBRATE as our main quality proxy. Neverthe-less, we manually check every fourth score tupleand model reasoning to confirm alignment withthe evaluation task and human judgment. In caseof misalignment, three annotators would insteadrate the summary. As AUTOCALIBRATE onlyassesses specific characteristics and does not con-sider omission, hallucination, or repetition, we also"}, {"title": "5.5 Summary refinement discussion", "content": "Influence of feedback and transfer protocolson quality. Table 4 shows the overall rankingand Likert scores of each refinement protocol vari-ant. ORIG summaries are consistently ranked low-est, indicating that refinement positively influencesquality, as observed in the assigned Likert scores.Having only the essential part in the FP leads tominor improvements in ranking and Likert scoresfor both TPs compared to the ORIG summary, butfalling behind the scores of most protocol variantsusing additional information. This indicates thatpointing out errors on a high level already leadsto quality improvement. The result is expected, asthe minimalistic explanation may not contain everyerror instance, precise reasoning, or all informationto resolve specific errors such as omission. Com-paring the essential parts scores of both TPs revealsthat the Likert scores and rankings differ notably"}, {"title": "6 Final Considerations", "content": "In this paper, we investigated GPT4's ability to findmistakes in a given meeting summary and refinethem accordingly. We found that GPT4 achievesa high accuracy of ~89% on average, measuredagainst human labels, in identifying typical mis-takes (e.g., repetition of content) when using a ded-icated model instance paired with CoT promptingto identify individual errors. However, it strugglesto identify similar and subjective errors, such ashallucination (72% acc.) with omission and irrele-vance (81% acc.). We showed strong evidence thata dedicated LLM can refine a summary based onidentified errors. By providing a CoT explanationfor each error type containing reasoning why andwhere an error was observed, we improve the qual-ity of relevance, informativeness, conciseness, andcoherence significantly. These refined summariesare comparable in quality, with error-free gold sum-maries. Our post hoc refinement approach can beapplied to refine meeting summaries generated bytraditional models and LLMs and marks an earlyentry into methods that allow the full potential ofLLMs for meeting summarization. We leave thedevelopment of more sophisticated refinement pro-tocols, e.g., using multi-agent discussion, and theapplication of our multi-LLM approach to similarcomplex text generation tasks (e.g., story writing toreflect on given setting) and real-world applications(e.g., assisting LLM agents to check the outcome toa task) to future work. We release QMSum Mistaketo encourage research on refinement."}, {"title": "Potential Impact", "content": "The multi-LLM approach proposed here, influenced by psychological observations on productivity and collaboration, exemplifies how other academic fields can inform NLP research (Wahle et al.,2023b). This work demonstrates the potential for enhancing complex text generation tasks requiringrobust output such as machine translation (Fenget al., 2024), reasoning (Kalyanpur et al., 2024),question answering (Kim et al., 2024), or paraphrasing (Becker et al., 2023; Wahle et al., 2023a), thatmay benefit from an output-challenging system thatassesses content alignment. By incorporating multi-LLM strategies and personalization, we open newavenues for improving NLP outputs across variousapplications, underscoring the value of interdisci-plinary approaches in advancing NLP technologiesand their real-world applicability."}, {"title": "Limitations", "content": "Although our proposed QMSum Mistake mightseem small (i.e., 200 samples), its size is com-parable to the original QMSum dataset (i.e., 232samples). We contribute to extending the originaldataset with careful human error annotations foralmost all examples available. Another possiblelimitation in our work is the use of only GPT4 inour main experiments. We chose GPT4 because ofits large context size (e.g., 128k tokens) and bet-ter initial results in identifying errors. Evaluatingand error annotation and refinement for multiplemodels by humans would be time-consuming andfinancially unfeasible. However, we report the de-tailed results in Appendix B to provide insightson other language families and different models(e.g., Phi (Abdin et al., 2024), Gemini (Team et al.,2024)) considered in our study. We evaluate theirperformance on mistake identification and qualitychanges when refining a summary."}, {"title": "Ethics Statement and Broader Impact", "content": "Our research abides by ethical guidelines for AIresearch and is committed to privacy, confidential-ity, and intellectual property rights. We've ensuredthat the datasets in our study, publicly available, donot house sensitive or personal details. While ourstudy leverages existing resources and generativemodels, it's important to note that these modelscan possess biases and may occasionally generatesummaries with distortions, biases, or inappropri-ate content. To counteract this, we've configuredour models to omit potentially harmful or unsafecontent. While our research aims to enhance meet-ing summarization to benefit communication andproductivity across sectors, we're acutely awareof the ethical challenges posed by AI in this do-main. Meeting summarization models must bewielded with respect to privacy and consent, es-pecially when processing sensitive or confidentialmaterial. It's paramount that these models neitherviolate privacy nor perpetuate harmful biases. Asthe field evolves, we stress the importance of main-taining these ethical considerations and encouragefellow researchers to uphold them, ensuring thatAI advancements in meeting summarization areboth beneficial and ethically grounded. An integralaspect of our ethical commitment is reflected inour approach to annotator recruitment and manage-ment. The team of annotators, consisting of interns,student assistants, and doctoral students, was metic-ulously selected through internal channels. Thisstrategy was chosen to uphold a high standard ofannotation quality\u2014a quality we found challeng-ing to guarantee through external platforms suchas Amazon Mechanical Turk. Ensuring fair com-pensation, these annotators were remunerated inaccordance with institutional guidelines for theirrespective positions. Further, flexibility in the anno-tation process was also a priority. Annotators hadthe freedom to choose their working times and en-vironments to prevent fatigue from affecting theirjudgment."}]}