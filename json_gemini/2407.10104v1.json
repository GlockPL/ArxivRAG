{"title": "A Self-Supervised Learning Pipeline for Demographically Fair Facial Attribute Classification", "authors": ["Sreeraj Ramachandran", "Ajita Rattani"], "abstract": "Published research highlights the presence of demographic bias in automated facial attribute classification. The proposed bias mitigation techniques are mostly based on supervised learning, which requires a large amount of labeled training data for generalizability and scalability. However, labeled data is limited, requires laborious annotation, poses privacy risks, and can perpetuate human bias. In contrast, self-supervised learning (SSL) capitalizes on freely available unlabeled data, rendering trained models more scalable and generalizable. However, these label-free SSL models may also introduce biases by sampling false negative pairs, especially at low-data regimes (< 200K images) under low compute settings. Further, SSL-based models may suffer from performance degradation due to a lack of quality assurance of the unlabeled data sourced from the web. This paper proposes a fully self-supervised pipeline for demographically fair facial attribute classifiers. Leveraging completely unlabeled data pseudolabeled via pre-trained encoders, diverse data curation techniques, and meta-learning-based weighted contrastive learning, our method significantly outperforms existing SSL approaches proposed for downstream image classification tasks. Extensive evaluations on the FairFace and CelebA datasets demonstrate the efficacy of our pipeline in obtaining fair performance over existing baselines. Thus, setting a new benchmark for SSL in the fairness of facial attribute classification.", "sections": [{"title": "1. Introduction", "content": "Automated facial analysis-based algorithms encompass face detection, face recognition, and facial attribute classification (such as gender-, race-, age classification, and BMI prediction) [17, 47, 29, 42]. Numerous existing studies [16, 24, 4, 45, 1, 25] confirmed the performance disparities of facial attribute classifiers between demographic groups. Most of these bias mitigation techniques are predominantly supervised learning techniques [49, 11, 40, 12, 28, 26], that aim to introduce fairness constraints during model training using demographically annotated training data. However, sensitive demographic data is often laboriously annotated through large-scale human efforts, raising privacy issues and General Data Protection Regulation (GDPR) [13] related legal constraints. Further, labeled data are often scarce in real-world applications, leading to limited generalizability and scalability (due to higher annotation costs) of the trained models. Further, there is a potential perpetuation of human biases from the data labeling process [43]. Further, these supervised bias mitigation techniques are often Pareto inefficient [51] which means fairness is often obtained at the cost of reduced overall performance.\nThis prompts a shift towards self-supervised learning (SSL), which can take advantage of the abundance of unlabeled data using contrastive learning [10, 9, 37, 50] for model training. Contrastive learning aims to create dense representations that exhibit low intra-class variance and high inter-class variance even without explicit labels. Thus, there is no requirement for annotated labels and sensitive attributes allowing for highly scalable training methodologies, reducing manual effort, and bypassing privacy concerns related to sensitive information. Readers are referred to Appendix A for background mathematical details on contrastive loss.\nHowever, this label-free approach of SSL can also introduce bias by inadvertently sampling false negative pairs i.e., in a single batch, it might inadvertently pair two images of the same class (unknown) as negative pairs [10, 9, 22, 37]. This issue is particularly relevant in low-compute settings, where training on massive datasets is impractical due to time and resource constraints. Therefore, this study focuses on a low-data regime (<200K images) to investigate the effectiveness of SSL under such constraints, as the limited number of total data points increases the chances of sampling false negative pairs, thus amplifying biases on the learned representation. Further, these SSL methods may also suffer from the issue of lack of quality assurance, a"}, {"title": "2. Related Work", "content": "Fairness in Facial Attribute Classification: Research has consistently shown that facial-attribute classification algorithms exhibit significant biases, affecting accuracy across different gender-racial groups and exacerbating societal inequalities [21, 6, 33, 25, 3]. In response, a variety of supervised learning-based mitigation strategies have been developed [31, 21, 39, 26] to mitigate bias in facial attribute classifiers. For example, [38] proposed learning fair representations by reversing gradients of the classification loss for sensitive attributes through a gradient reversal layer. [23] extended this approach by minimizing the mutual information between the representation and sensitive attribute labels to further reduce their correlations. In FD-VAE [35], latent code for sensitive attributes capture information related to both target and sensitive attributes, which is then discarded to ensure fairness. These mitigation techniques are often Pareto inefficient [51, 44, 32] and lack scalable and generalizability [51, 39, 49, 11].\nSSL Approaches: Self-supervised methods such as SimCLR [9] maximize and minimize the similarity between augmented views of the same image and different images using a contrastive loss, a large batch size, and a projection head. Whereas BYOL [15] learns representations by predicting the target network's representation of a transformed view of the same image using an online network without using negative pairs. Barlow Twins [48] learns representations by minimizing the redundancy between the output representation dimensions, enforcing off-diagonal elements of the cross-correlation matrix between distorted views to be zero while preserving diagonal elements. VicReg [2] learns representations by regularizing the variance and covariance of the embeddings along each dimension individually. Finally, the DINO [7] method learns representations by matching the output of a student network to a teacher network, where the teacher network is updated using an exponential moving average of the student network's parameters. DINOV2 [34] improves on it by introducing data curation methods to improve generalization by finding images similar to curated data sets using embedding similarities. Finally, a method proposed by [14] uses a facial-region-aware SSL technique for a large-scale facial analysis model trained on 3.3M images. As our work focuses on low data regimes and compact models, a direct comparison with [14] is beyond the scope of this study."}, {"title": "3. Proposed Method", "content": "In this section, we will discuss our data curation and training pipeline in detail."}, {"title": "3.1. Data Curation Pipeline", "content": "To train a self-supervised model that can reach the level of performance of a model trained using supervised methods typically requires a substantial amount of unlabeled raw data often scraped from the Web. For example, the state-of-the-art CLIP model [37] was trained on a massive dataset of 400 million images to achieve the same accuracy in linear-probe testing as a comparatively smaller model (Efficient-Net) trained on only approximately 1.2 million images from the ImageNet dataset. Given the intractability of such large-scale training in a low-compute setting, our approach diverges from this standard by concentrating on smaller models and compact datasets, i.e., the size of carefully curated datasets is \u2264 200,000 images. Given this scale, the selection and curation of our training data demand exceptional quality that has a diverse and balanced distribution. In order to meet these requirements, we have applied a new data curationpipeline taking cues from the innovative approaches proposed in DinoV2 paper [34] and applying an implicit fairness constraint by reproducing the data distribution of existing fair and balanced datasets such as FairFace [20].\nData Sources: In the first stage of the data curation pipeline, we gather a large collection of unlabeled datasets, aiming to cover a wide variety of face images. We refer to this collection as our unlabeled/noncurated dataset. It primarily includes a randomly sampled subset from two prominent sources: the LAION Face Dataset [50] and the BUPT Face Dataset [46]. While these datasets offer a wealth of images, varying in quality and resolution (~ 50M), they lack the specific facial attribute labels needed for our downstream task of facial attribute classification.\nWhereas a dataset like FairFace [20] (~ 100K) was curated and manually annotated with the objective of a balanced distribution to be used in fair attribute classification training and evaluation. In our approach, we want to emulate its fair and diverse data distribution, implicitly, through automated retrieval methods. To do this, we first process both our uncurated collection and the FairFace dataset(retrieval set) through a pre-trained ViT-L/16 vision encoder of the CLIP model. This step produces image embeddings, which are essentially numerical representations that capture the essence of each image.\nDeduplication and Retrieval: In the second stage, we use the generated embeddings for data filtering. To ensure we keep only unique images, we first run a deduplication step on our noncurated dataset using cosine similarity on the image embeddings to identify and remove duplicates. With a deduplicated set, we can now perform a similarity search to find and retrieve images that are close matches to the good-quality, diverse examples found in the FairFace dataset (retrieval set). For each image in the retrieval set, we retrieve M similar embeddings from the noncurated set. A relatively small value for M(<5) was found to be sufficient in retrieving good-quality images that closely mirrored the distribution of the retrieval set and the rest was discarded. And this also speeds up the querying process. In order to carry out this search quickly and efficiently, we employed the Faiss [19] package from Meta, which provides KNN search capabilities. This library helps us sift through the vast array of image embeddings using GPU acceleration to find the best matches. At this point, we have a dataset of approximately 500K images.\nImage Quality Filtering: In the final stage, we have the option to filter our images further with a quality assessment model. This optional step involves utilizing a non-reference-based facial image quality assessment tool, CR-FIQA proposed in [5] to generate quality scores for the filtered set. We may then subsequently filter out lower-quality images by setting up an appropriate threshold based on empirical evidence. After this step, our final dataset size is ~200K images, similar to that of the CelebA [29] dataset.\nThe result of this carefully designed data curation pipeline is an augmented curated dataset a combination of the noncurated images that have passed our quality and similarity checks, along with the initial curated set (Fair-Face). This filtered dataset delivers the necessary diversity and balance for our facial attribute classification tasks, automating what traditionally required extensive manual labor."}, {"title": "3.2. Self-supervised Training Pipeline", "content": "In this section, we outline how the augmented-curated dataset is integrated into a self-supervised training pipeline to operate in a low-data regime."}, {"title": "3.2.1 Pseudolabeling using zero-shot techniques", "content": "In this stage, we use a pretrained CLIP model to create pseudo labels for our augmented-curated dataset. We specifically follow the zero-shot classification method outlined in the CLIP paper [37]. Readers are referred to Appendix B for more details. We also note that the choice of encoder is crucial here, as biases from the encoder may persist. We hypothesize that strategically selecting a fair encoder trained on multiple tasks across various domains would yield better results than our arbitrary choice of CLIP.\nThis decision to generate pseudo labels stems from our intention to employ Supervised Contrastive Loss (SupCon Loss) [22] instead of standard contrastive loss (refer Appendix A for mathematical details). The SupCon loss is specifically adapted to leverage the structured information that pseudo-labels provide and to eliminate the issue of false negative pairs, a known challenge of label-independent contrastive learning for SSL models. Given pseudo labels, naturally arises the question of training using the standard cross-entropy loss used for classification. However, pseudo-labels often imply noisy and low-quality labels. In such scenarios, the SupCon loss proves to be more robust and can handle the inherent uncertainty of pseudo labels because it concentrates on relative rather than absolute label assignment. Therefore, the model can generalize better and produce more adaptable representations for downstream classification tasks [22]."}, {"title": "3.2.2 Supervised Contrastive Learning", "content": "Moving to supervised scenarios where labels are available whether manually labeled or pseudo-labeled, standard contrastive loss (readers are referred to Appendix A for more information on contrastive loss.) needs adjustment to account for labeled data where multiple samples can be of the same class. To handle this, SupCon Loss extends the standard contrastive loss by modifying it to include supervisory signals.\n$$C_{supcon} = \\frac{-1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp(z_i \\cdot z_p / \\tau)}{\\sum_{a \\in A(i)} exp(z_i \\cdot z_a / \\tau)}$$\nHere, P(i) refers to the set of positive indices linked with ith image, sharing the same label in the multiviewed batch (set of augmented samples), but excluding i itself. $z_i$ is the output feature embedding of the augmented views, $\\tau$ is a positive scalar known as the temperature parameter used to scale the logits, and A(i) is the set of all images except the ith image.\nThus, overall, in the supervised contrastive learning stage, we first create two augmented versions for every batch of pseudo-labeled input data. These augmented versions are fed into an encoder network, producing embeddings of 512 dimensions. These embeddings are then normalized to the unit hypersphere. During the training process, these embeddings are further processed through a projection network, normally a 3-layer MLP network. It's important to note that this projection network is not used after the pretraining stage. The supervised contrastive loss is calculated based on the output feature embeddings from this projection network using eq. 1. Finally, we add a linear classifier on top trained using cross-entropy loss to achieve the final classification performance, a process called linear probing.\nWhat attributes to use? The use of SupCon loss typically ties a model with the specific task due to its dependency on attribute labels (here pseudo-labels). To avoid limiting our model to a specific attribute classification task, we use all 40 attributes (Young, Beard, Attractive, etc.) similar to those annotated for the CelebA [29] dataset, in the pseudo-label generation phase discussed in section 3.2.1 (zero-shot, refer Appendix B for details) to generate 40 different pseudo-labels for each sample in the augmented curated dataset. This results in a model that could be generalized across a range of facial attributes. When calculating the loss, we determined the SupCon loss for each attribute individually and then computed the mean of these losses to obtain the overall loss measure."}, {"title": "3.3. Meta-Weighted SupCon with Fairness Constraints", "content": null}, {"title": "3.3.1 Fairness Constraint", "content": "Until this stage, our training stage does not explicitly impose a fairness constraint (other than the implicit fairness constraint by replicating Fairface distribution during data curation). Since we aim to develop a model that generates fair embeddings without access to explicit labels, applying any fairness constraint that involves explicit sensitive attribute labeling naturally becomes an issue.\nTo this front, we adopt the label-free Min-Max fairness constraint, drawing inspiration from [18, 8]. Specifically, our objective is to minimize the average of the top-k losses which can be represented equivalently as:\n$$L^{supcon}(k, \\theta) = \\frac{1}{2N} \\sum_{i=1} max\\{C^{supcon} (x_i; \\theta) - \\lambda(k, \\theta), 0\\} + \\lambda(k, \\theta),$$\nIn this equation, $\\lambda(k, \\theta)$ signifies the k-th highest contrastive loss within the set of all contrastive losses in a batch and $C^{supcon} (x_i; \\theta)$ is the SupCon loss from eq. 1. Readers are referred to Appendix C for mathematical background on the Min-Max fairness constraint.\nChallenge: Despite its advantages, SupCon loss still faces challenges when dealing with noisy labels and potential class imbalance. Specifically, the inherent noise in our pseudo-labeling process, coupled with the possibility of an imbalanced dataset, can introduce bias during training. Specifically, if certain classes are underrepresented, the model may not learn equally effective representations for all classes, impacting the overall fairness and performance of the downstream classification task.\nSolution through proxy: The work by [8] addresses this issue by applying the fairness constraints on a small but manually labeled validation set first. Then the corresponding validation loss is used as a proxy for the training objective. Our method further improves on this by eliminating the need for the manually annotated validation set, making it fully compatible with the self-supervised learning pipeline. Thus, instead of a small, labeled validation set, we sample a small subset of images, which have been pseudolabeled with high confidence, from our augmented curated dataset.\nThe validation loss therefore becomes,\n$$\\lambda^{val} (k, \\theta, \\omega) = \\frac{1}{M} \\sum_{j=1} [L_{cls}(g_{\\omega}(f_{\\theta}(x_j)), \\hat{y}_j) - \\lambda^{val} (k, \\theta, \\omega)]_+ + \\lambda^{val} (k, \\theta, \\omega)$$\nwhere instead of $\\lambda^{val} (k, \\theta)$ from eq. 2 we have $\\lambda^{val} (k, \\theta,\\omega)$ which denotes the k-th largest classification loss within the subset of the curated set with high-confidence pseudo labels $\\{L_{cls}(g_{\\omega}(f_{\\theta}(x_j)), \\hat{y}_j)\\}_{j=1}^{M}$, $g_{\\omega}$ is the additional linear layer used for computing the validation loss and $f_{\\theta}$ is the base model."}, {"title": "3.3.2 Meta Weight Learning", "content": "To use the obtained validation loss as a proxy during the training, we follow a similar approach to that of the works by [8] and reweighting techniques [41]. These methods work by dynamically adjusting the influence of individual training samples to align more closely with the desired objective; in our case, validation loss with fairness constraint was applied. Given the defined validation loss $\\lambda^{val} (k, \\theta, \\omega)$ in eq.3, our goal is to iteratively refine the weights of our training samples in a manner that optimally balances the competing demands of maintaining a low contrastive loss while adhering to the fairness constraints inferred from the pseudo-labeled validation subset. This balance is crucial for ensuring that our model learns effective and generalized representations and does so in a manner that respects the fairness constraint.\nWeight Update Mechanism: To achieve this, we introduce a weight update mechanism that weights individual training samples dynamically. Specifically, each training sample, $x_i$ is assigned a dynamic weight $w_i$ that reflects its current contribution towards meeting the combined objectives of contrastive loss as well as the validation loss. The weight of the training sample is updated based on the gradient similarity between training loss and validation loss [41]. Specifically, for each training sample, we calculate the gradient of the supervised contrastive loss with respect to the model parameters as $\\nabla_{\\theta}L_{supcon}(f_{\\theta}(x_i), \\hat{y}_i)$, and adjust its weight $w_i$ according to the similarity of this gradient to that of the validation loss and normalizes it, reinforcing samples that guide the model towards fairness. Finally, the weighted SupCon loss is recalculated using the updated weights, and backpropagation is applied. These gradient values can be estimated using forward-mode automatic differentiation libraries such as PyTorch using multiple forward and backward passes (refer Algorithm in Appendix D).\nTraining Challenges and Mitigation Strategy: The meta-weighting strategy introduced above also comes with a caveat of increased computational complexity. As evident from the algorithm described in Appendix D, it requires two forward and three backward passes in each training iteration, a significant increase from the standard single forward and backward passes. This modification nearly triples the training time and demands additional memory to store those intermediate states, presenting notable challenges in computational efficiency and memory resource allocation.\nTo address this challenge, we apply our strategy selectively during the later stages of training. For an initial portion of the training process (determined by hyperparameter, $k$), we employ the standard SupCon loss in eq. 1. In the later half of the training, we freeze certain layers of the network and apply the meta-weighting strategy only to the remaining actively trained layers. This staged training substantially enhances the efficiency by reducing the computational load during the initial training phase. Furthermore, it ensures that the network benefits from the fine-tuning capabilities of the meta-weighting scheme in the critical later stages. For example, with k equal to 0.7, i.e., 210 out of 300 epochs trained using SupCon and rest applying the meta-weighting strategy on the unfrozen layers, we obtain ~ 3\u00d7 faster training speed up."}, {"title": "4. Experiments and Results", "content": null}, {"title": "4.1. Dataset", "content": "For our experiments, we used the FairFace [20] as our curated dataset. For evaluation purposes, we used FairFace for intra-dataset and CelebA [29] for cross-dataset evaluation. For the non-curated dataset, we used both the LAION-FACE [50] dataset as well as the BUPT-GlobalFace Dataset [46], which are not annotated with any task-specific attribute labels. More information about the datasets used is available in Appendix E."}, {"title": "4.2. Experimental Configuration and Metrics", "content": "Model Architecture: For all our experiments, we used a ResNet-18 architecture as the backbone encoder. Although other architectures were also considered, we used this architecture because ours is a low-data regime approach, and a small model was deemed more appropriate. For the contrastive training, the Resnet-18 encoder takes the augmented views as input. It is then followed by the projection network, which is a 3-layer multi-layer perception (MLP) with a hidden layer size of 2048 and output dimension of 512. The projection network produces the final representation used for the corresponding final loss calculation. After the aforementioned contrastive pre-training, the Projection Network is discarded, making the inference-time model architecture identical to a standard cross-entropy model utilizing the same encoder. During the linear probe evaluation phase, we kept the encoder frozen and added a linear layer at the end to be trained via Logistic Regression for each attribute, following a typical self-supervised learning protocol.\nTraining hyperparameters and SSL Baselines: For training all our models, we used two Nvidia A6000 GPUs as our computational infrastructure. The training was done in batches of 256 over 300 epochs. We used a learning rate scheduler with cosine decay with warmup, a base learning rate of $1e-4$, and a weight decay of $5e-4$ using AdamW [30] optimizer. We applied the same data augmentation strategy from the SimCLR [9] paper, i.e., we sequentially apply 3 simple augmentations: random cropping and resize, random color distortions, and random Gaussian blur. We then benchmarked our method against several SSL approaches namely, SimCLR [9], BYOL [15], Barlow Twins [48], VicReg [2], and DINO [7]. We also compared our method against supervised bias mitigation techniques namely, GRL [38], LNL [23], and FD-VAE [35], whenever feasible.\nWe report the results from our proposed approach under two configurations, (1) Ours: uses the data curation pipeline, pseudo labeling along with the SupCon loss modification (See Section 3.2) and (2) Ours Weighted: uses, in addition to that, fairness constraints and meta-weighted learning from Section 3.3.\nAdditionally, we compared our proposed approach to the same backbone ResNet-18 model trained using supervised cross-entropy loss. However, note that comparing our SSL approach trained from scratch to the supervised fine-tuned (ImageNet weights) model may not be entirely fair. Therefore this comparison serves primarily as a reference to gauge the performance potential of our SSL pipeline, rather than as a direct, fair comparison between the methods.\nMetrics Evaluated: We evaluated the trained models' overall performance accuracy and fairness using widely recognized metrics [39]. The metrics include the Equalized Odds Difference (EOD), which assesses fairness by ensuring consistent true positive rates (TPR) and false positive rates (FPR) across subgroups, Degree of Bias, which is calculated as the standard deviation of utilities across subgroups, and the Selection Rate (SeR), which measures the utility ratio between the least and most performing groups. We also evaluated the Demographic Parity Difference (DEP), which aims for prediction independence from membership in sensitive groups, as well as Max-Min Fair-"}, {"title": "4.3. Evaluation on Fair Face", "content": "In this section, we evaluate the performance of our method in facial attribute classification tasks using the validation set of the FairFace [20] benchmark dataset. We specifically examine our SSL model's effectiveness in a linear probe evaluation setup, focusing on gender classification based on facial features while treating race as a sensitive attribute.\nAs can be seen from the Table, when compared to existing SSL methods our proposed method significantly outperforms existing SSL approaches in accuracy and, notably, across all fairness metrics. Specifically, our method has an accuracy improvement of 6-7% and STD, on average, improved by 3.5. Similar improvements can be observed on other fairness metrics as well. For example, we see a significant improvement in the minimum group accuracy (12-14%). The inferior performance of standard SSL approaches is due to the limited data availability, absence of fairness-specific modifications during training, and the inherent nature of contrastive loss at a smaller scale (i.e., false negative pairing) that likely amplifies biases present in the data. Our meta-learning-based weighted method (denoted as Ours-Weighted in the Table), although improves accuracy (+0.32%), did not necessarily improve fairness. This could be because we used high-confidence pseudo labels (noisy) rather than manually annotated labels for the validation set like in existing weighting methods as well as the simplifications we applied to improve the training efficiency. This underscores the effectiveness of our proposed SSL pipeline in achieving high performance while also addressing fairness concerns, setting a new benchmark for future SSL research.\nWe also note that our model also outperforms the equivalent supervised model that was trained from scratch in terms of accuracy as well as demographic parity. For reference, we also show the results of a fine-tuned supervised model, which was pre-trained on a much larger ImageNet dataset (1.2M params) using a supervised training method and then subsequently fine-tuned on FairFace. However, recall that comparison with the supervised models primarily serves as a reference to gauge the performance potential of our self-supervised training pipeline rather than as a direct, fair comparison between the methods."}, {"title": "4.4. Cross Dataset Evaluation on CelebA", "content": "In this section, we conducted a comprehensive study on CelebA where gender served as the sensitive attribute for assessing fairness across 6 target attributes individually, providing a detailed evaluation of our method's capabilities. We chose these 6 target attributes (Attractiveness, Brown Hair, High Cheekbones, Mouth Slightly Open, Smiling, and Young) to ensure a sufficient and balanced sample size across the target and sensitive (gender) attributes. Note that we omit less relevant evaluation metrics due to limited space and duplicated information. This analysis also lets us evaluate the impact of combining 40 attribute SupCon losses(Section 3.2.2) during training, as this was not done for any of the baseline SSL methods [9, 15, 48, 7, 2] we are comparing against.\nOur proposed method (both weighted and unweighted versions) consistently outperforms the other SSL models in terms of accuracy across the selected attributes. The mean improvement in accuracy of our unweighted model over the best performing SSL baseline (BYOL) is 3.17%, while our weighted model achieves a mean accuracy improvement of 3.23% over BYOL. The accuracy improvements range from ~ 1% to ~ 13%, with the most significant gain observed in the Mouth Slightly Open attribute."}, {"title": "5. Conclusion & Future Research", "content": "In this paper, we introduced a novel SSL pipeline for a demographically fair facial attribute classification. Our method combines advanced data curation pipeline, supervised contrastive learning with pseudo-labels, and meta-weight learning to obtain SOTA performance in both accuracy and fairness over existing SSL and supervised approaches. To the best of our knowledge, this marks the first benchmark evaluation of the fairness of SSL models on the FairFace dataset. Extensive evaluation on widely used Fair-Face and CelebA datasets demonstrates the effectiveness of our approach in learning fair representation without the need for sensitive attribute labels. Our work also highlights the potential of SSL techniques in addressing the challenges of supervised methods of addressing fairness, thus offering a more scalable approach.\nWe also plan to explore methods to adapt our approach to deal with unknown labels, potentially incorporating semi-supervised or unsupervised learning techniques."}]}