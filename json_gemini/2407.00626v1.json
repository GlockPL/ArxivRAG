{"title": "Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models", "authors": ["Sangwoong Yoon", "Himchan Hwang", "Dohyun Kwon", "Yung-Kyun Noh", "Frank C. Park"], "abstract": "We present a maximum entropy inverse reinforcement learning (IRL) approach for improving the sample quality of diffusion generative models, especially when the number of generation time steps is small. Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data. Since we employ an energy-based model (EBM) to represent the log density, our approach boils down to the joint training of a diffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum Entropy IRL (DxMI), is a minimax problem that reaches equilibrium when both models converge to the data distribution. The entropy maximization plays a key role in DxMI, facilitating the exploration of the diffusion model and ensuring the convergence of the EBM. We also propose Diffusion by Dynamic Programming (DxDP), a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by transforming the original problem into an optimal control formulation where value functions replace back-propagation in time. Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps. Additionally, DxMI enables the training of an EBM without MCMC, stabilizing EBM training dynamics and enhancing anomaly detection performance.", "sections": [{"title": "1 Introduction", "content": "Generative modeling is a form of imitation learning. Just as an imitation learner produces an action that mimics a demonstration from an expert, a generative model synthesizes a sample resembling the training data. In generative modeling, the expert to be imitated corresponds to the underlying data generation process. The intimate connection between generative modeling and imitation learning is already well appreciated in the literature [1, 2].\n\nThe connection to imitation learning plays a central role in diffusion models [3, 4], which generate samples by transforming a Gaussian noise through iterative additive refinements. The training of a diffusion model is essentially an instance of behavioral cloning [5], a widely adopted imitation learning algorithm that mimics an expert's action at each state. During training, a diffusion model is optimized to follow a predefined diffusion trajectory that interpolates between noise and data. The trajectory provides a step-by-step demonstration of how to transform Gaussian noise into a sample, allowing diffusion models to achieve a new state-of-the-art in many generation tasks.\n\nBehavioral cloning is also responsible for the diffusion model's key limitation, the slow generation speed. A behavior-cloned policy is not reliable when the state distribution deviates from the expert demonstration [6, 7]. Likewise, the sample quality from a diffusion model degrades as the gap"}, {"title": "2 Preliminaries", "content": "Diffusion Models. The diffusion model refers to a range of generative models trained by reversing the trajectory from the data distribution to the noise distribution. Among diffusion models, we focus on discrete-time stochastic samplers, such as DDPM [3], which synthesize a sample through the following iteration producing $x_0, x_1, ..., x_T \\in \\mathbb{R}^D$:\n\n$x_0 \\sim N(0,I)$ and $x_{t+1} = a_tx_t + \\mu(x_t, t) + \\sigma_t\\epsilon_t$ for $t = 0, 1, . . ., T \u2013 1$,\n\nwhere $\\epsilon_t \\sim N(0, I)$ and $\\mu(x,t)$ is the output of a neural network. The coefficients $a_t \\in \\mathbb{R}$ and $\\sigma_t \\in \\mathbb{R}_{>0}$ are constants. Note that we reverse the time direction in a diffusion model from the convention to be consistent with RL. The final state $x_T$ is the sample generated by the diffusion model, and its marginal distribution is $\\pi(x_T)$. We will often drop the subscript T and write the distribution as $\\pi(x)$. The conditional distribution of a transition in Eq. (1) is denoted as $\\pi(x_{t+1}|x_t)$. For continuous-time diffusion models [24], Eq. (1) corresponds to using the Euler-Maruyama solver.\n\nThe generation process in Eq. (1) defines a $T$-horizon Markov Decision Process (MDP) except for the reward. State $s_t$ and action $a_t$ are defined as $s_t = (x_t, t)$ and $a_t = x_{t+1}$. The transition dynamics is defined as $p(s_{t+1}|s_t, a_t) = \\delta(a_t,t+1)$, where $\\delta(x_{t,t})$ is a Dirac delta function at $(x_t, t)$. With a reward function defined, a diffusion model can be trained with RL [11, 23, 19, 25]. In this paper, we consider a case where the reward is the log data density $\\log p(x)$, which is unknown.\n\nEnergy-Based Models. An energy-based model (EBM) $q(x)$ uses a scalar function called an energy $E(x)$ to represent a probability distribution:\n\n$q(x) = \\frac{1}{Z} \\exp(-E(x)/\\tau), E : \\mathcal{X} \\rightarrow \\mathbb{R}$,\n\nwhere $\\tau > 0$ is temperature, $\\mathcal{X}$ is the compact domain of data, and $Z = \\int_{\\mathcal{X}}\\exp(-E(x)/\\tau)dx < \\infty$ is the normalization constant.\n\nThe standard method for training an EBM is by minimizing KL divergence between data $p(x)$ and EBM $q(x)$, i.e., $\\min_q KL(p||q)$, where $KL(p||q) := \\int_x p(x) \\log(p(x)/q(x))dx$. Computing the gradient of $KL(p||q)$ with respect to EBM parameters requires MCMC sampling, which is computationally demanding and sensitive to hyperparameters. The algorithm presented in this paper serves as an alternative method for training an EBM without MCMC.\n\nEBMs have a profound connection to maximum entropy IRL, where Eq. (2) serves as a model for an expert's policy [15, 16, 2]. In maximum entropy IRL, $x$ corresponds to an action (or a sequence of actions), and $E(x)$ represents the expert's cost of the action. The expert is then assumed to generate actions following $q(x)$. This assumption embodies the maximum entropy principle because Eq. (2) is a distribution that minimizes the cost while maximizing the entropy of the action. Here, $\\tau$ balances cost minimization and entropy maximization."}, {"title": "3 Diffusion by Maximum Entropy Inverse Reinforcement Learning", "content": "3.1 Objective: Generalized Contrastive Divergence\n\nWe aim to minimize the (reverse) KL divergence between a diffusion model $\\pi(x)$ and the data density $p(x)$. This minimization can improve the sample quality of $\\pi(x)$, particularly when $T$ is small.\n\n$\\min_{\\pi \\in \\Pi} KL(\\pi(x)||p(x)) = \\max_{\\pi \\in \\Pi} \\mathbb{E}_{\\pi}[\\log p(x)] + H(\\pi(x))$,"}, {"title": "3.2 Alternating Update of EBM and Diffusion Model", "content": "In DxMI, we update a diffusion model and an EBM in an alternative manner to find the Nash equilibrium. We write $\\theta$ and $\\phi$ as the parameters of the energy $E_\\theta(x)$ and a diffusion model $\\pi_\\phi(x)$, respectively. While EBM update is straightforward, we require a subroutine described in Section 4 for updating the diffusion model. The entire procedure of DxMI is summarized in Algorithm 1.\n\nEBM Update. The optimization with respect to EBM is written as $\\min_\\theta \\mathbb{E}_{p(x)} [E_\\theta(x)] - \\mathbb{E}_{\\pi_\\phi(x)} [E_\\theta(x)]$. During the update, we also regularize the energy by the square of energies $\\gamma(\\mathbb{E}_{p(x)} [E_\\theta(x)^2] + \\mathbb{E}_{\\pi_\\phi(x)} [E_\\theta(x)^2])$ for $\\gamma > 0$ to ensure the energy is bounded. We set $\\gamma = 1$ unless stated otherwise. This regularizer is widely adopted in EBM [34, 35].\n\nDifficulty of Diffusion Model Update. Ideally, diffusion model parameter $\\phi$ should be updated by minimizing $KL(\\pi_\\phi||q_\\theta) = \\mathbb{E}_{\\pi_\\phi(x)} [E(x)/\\tau] \u2013 H(\\pi_\\phi(x))$. However, this update is difficult in practice for two reasons.\n\nFirst, marginal entropy $H(\\pi_\\phi(x))$ is difficult to estimate. Discrete-time diffusion models (Eq. (1)) do not provide an efficient way to evaluate $\\log \\pi_\\phi(x)$ required in the computation of $H(\\pi_\\phi(x))$, unlike some continuous-time models, e.g., continuous normalizing flows [36, 24]. Other entropy estimators based on $k$-nearest neighbors [37] or variational methods [38, 39] do not scale well to high-dimensional spaces."}, {"title": "4 Diffusion by Dynamic Programming", "content": "In this section, we present a novel RL algorithm for a diffusion model, Diffusion by Dynamic Programming (DxDP), which addresses the difficulties in updating a diffusion model for the reward. DxDP leverages optimal control formulation and value functions to perform the diffusion model update in DxMI efficiently. Note that DxDP can be used separately from DxMI to train a diffusion model for an arbitrary reward.\n\n4.1 Optimal Control Formulation\n\nInstead of solving $\\min_\\phi KL(\\pi_\\phi(x_T)||q_\\theta(x_T))$ directly, we minimize its upper bound obtained from the data processing inequality:\n\n$KL(\\pi_\\phi(x_T)||q_\\theta(x_T)) \\leq KL(\\pi_\\phi(x_{0:T})||q_\\theta(x_T)\\bar{q}(x_{0:T-1}|x_T))$.\n\nHere, we introduce an auxiliary distribution $\\bar{q}(x_{0:T-1}|x_T)$, and the inequality holds for an arbitrary choice of $\\bar{q}(x_{0:T-1}|x_T)$. In this paper, we consider a particularly simple case where $\\bar{q}(x_{0:T-1}|x_T)$ is factorized into conditional Gaussians as follows:\n\n$\\bar{q}(x_{0:T-1}|x_T) = \\prod_{t=0}^{T-1} q(x_t|x_{t+1}), where q(x_t|x_{t+1}) = N(x_{t+1}, s_t^2I), s_t > 0$.\n\nNow we minimize the right-hand side of Eq. (6): $\\min_\\phi KL(\\pi_\\phi(x_{0:T})||q_\\theta(x_T)\\bar{q}(x_{0:T-1}|x_T))$. When we plug in the definitions of each distribution, multiply by $\\tau$, and discard all constants, we obtain the following problem:\n\n$\\min_{\\phi} \\mathbb{E}_{\\pi_\\phi(x_{0:T})} \\big[ E_\\theta(x_T) + \\tau \\sum_{t=0}^{T-1} \\log \\pi_\\phi(x_{t+1}|x_t) + \\tau \\sum_{t=0}^{T-1} \\frac{1}{2s_t^4} ||x_{t+1} - x_t||^2 \\big]$,\n\nwhich is an optimal control problem. The controller $\\pi_\\phi(\\cdot)$ is optimized to minimize the terminal cost $E_\\theta(x_T)$ plus the running costs for each transition $(x_t, x_{t+1})$. The first running cost $\\log \\pi_\\phi(x_{t+1}|x_t)$ is responsible for conditional entropy maximization, because $\\mathbb{E}_{\\pi}[\\log \\pi_\\phi(x_{t+1}|x_t)] = \u2013H(\\pi_\\phi(x_{t+1}|x_t))$. The second running cost regularizes the \u201cvelocity\u201d $||x_{t+1} - x_t||^2$. The temperature $\\tau$ balances between the terminal and running costs.\n\nWe have circumvented the marginal entropy computation in GCD, as all terms in Eq. (8) are easily computable. For the diffusion model considered in this paper (Eq. (1)), the conditional entropy has a particularly simple expression $H(\\pi_\\phi(x_{t+1}|x_t)) = D \\log \\sigma_t + 0.5D \\log 2\\pi$. Therefore, optimizing the entropy running cost amounts to learning $\\sigma_t$'s in diffusion, and we treat $\\sigma_t$'s as a part of the diffusion model parameter $\\phi$ in DxMI.", "4.2": "Dynamic Programming:\n\nWe propose a dynamic programming approach for solving Eq. (8). Dynamic programming introduces value functions to break down the problem into smaller problems at each timestep, removing the need for back-propagation in time. Then, a policy, a diffusion model in our case, is optimized through iterative alternating applications of policy evaluation and policy improvement steps.\n\nValue Function. A value function, or cost-to-go function $V_\\psi(x_t)$, is defined as the expected sum of the future costs starting from $x_t$, following $\\pi$. We write the parameters of a value function as $\\psi$.\n\n$V(x_t) = \\mathbb{E}_{\\pi} \\big[ E_\\theta(x_T) + \\tau \\sum_{t'=t}^{T-1} \\log \\pi(x_{t'+1}|x_{t'}) + \\tau \\sum_{t'=t}^{T-1} \\frac{1}{2s_t^2} ||x_{t+1} - x_t||^2 | x_t \\big]$,\n\nfor $t = 0, . . ., T \u2013 1$. Note that $V_T(x) = E(x_T)$. A value function can be implemented with a neural network, but there are multiple design choices, such as whether to share the parameters with $\\pi(x)$ or $E(x), and also whether the parameters should be shared across time. We explore the options in our experiments.\n\nPolicy Evaluation. During policy evaluation, we estimate the value function for the current diffusion model by minimizing the Bellman residual, resulting in the temporal difference update.\n\n$\\min_\\psi \\mathbb{E}_{x_t,x_{t+1} ~ \\pi} \\big[(sg[V_{\\psi}^{t+1}(x_{t+1})] + \\tau \\log(\\pi(x_{t+1}|x_t) + \\frac{\\tau}{2s_t^2} ||x_t - x_{t+1}||^2 - V_{\\psi}(x_t))^2\\big]$,\n\nwhere sg[.] denotes a stop-gradient operator indicating that gradient is not computed for the term.\n\nPolicy Improvement. The estimated value is used to improve the diffusion model. For each $x_t$ in a trajectory $x_{0:T}$ sampled from $\\pi_\\phi(x_{0:T})$, the diffusion model is optimized to minimize the next-state value and the running costs.\n\n$\\min_{\\phi} \\mathbb{E}_{\\pi_\\phi(x_{t+1}|x_t)} \\big[ V^{t+1}(x_{t+1}) + \\tau \\log \\pi_\\phi(x_{t+1}|x_t) + \\frac{\\tau}{2s_t^2}||x_t - x_{t+1}||^2 | x_t \\big]$\n\nIn practice, a single gradient step is taken for each iteration of policy evaluation and policy improve-ment.\n\nAdaptive Velocity Regularization (AVR). We additionally propose a method for sys-tematically determining the hyperparameter $s_t$'s of the auxiliary distribution $(\\bar{X}_{0:T-1}|X_T)$. We can optimize $s_t$ such that the inequality Eq. (6) is as tight as possible by solving $\\min_{s_0,...,s_{T-1}} KL(\\pi_\\phi(X_{0:T})||q_\\theta(X_T)\\bar{q}(X_{0:T-1}|x_T))$. After calculation (details in Appendix A), the optimal $s_t$ can be obtained analytically: $(s_t^+)^2 = \\mathbb{E}_{x_t,x_{t+1}~\\pi} [||x_t - x_{t+1}||^2]/D$. In practice, we can use exponential moving average to compute the expectation $\\mathbb{E}_{x_t,x_{t+1}~\\pi} [||x_t - x_{t+1}||^2]$ during training: $s_t \\leftarrow as_t + (1 \u2013 a)||x_t - x_{t+1}||^2/D$ where we set $a = 0.99$ for all experiment."}, {"title": "4.3 Techniques for Image Generation Experiments", "content": "When using DxDP for image generation, one of the most common applications of diffusion models, we introduce several design choices to DxDP to enhance performance and training stability. The resulting algorithm is summarized in Algorithm 2.\n\nTime-Independent Value Function. In image generation experiments (Section 5.2), we let the value function be independent of time, i.e., $V(x_t) = V_\\psi(x_t)$. Removing the time dependence reduces the number of parameters to be trained. More importantly, a time-independent value function can learn better representation because the value function is exposed to diverse inputs, including both noisy and clean images. On the contrary, a time-dependent value function $V(x_t)$ never observes samples having different noise levels than the noise level of $x_t$.\n\nTime Cost. Also, in the value update (Eq. (10)) step of image generation experiments, we introduce time cost function $R(t) > 0$, which replaces the running cost terms $\\tau \\log \\pi_\\phi(x_{t+1}|x_t) + \\frac{\\tau}{2s_t^4}||x_t - x_{t+1}||^2$. The time cost $R(t)$ only depends on time $t$. The modified value update equation is given as follows:\n\n$\\min_\\psi \\mathbb{E}_{x_t,x_{t+1} ~ \\pi} [(sg[V_{\\psi}(x_{t+1})] + R(t) \u2013 V_{\\psi}(x_t))^2]$.\n\nMeanwhile, we retain the running cost terms in the diffusion model (policy) update step (Eq. (11)). The time cost $R(t)$ is predetermined and fixed throughout training. The introduction of time cost is motivated by the observation that the running costs can fluctuate during the initial stage of training, posing difficulty in value function learning. The time cost stabilizes the training by reducing this variability. Moreover, the time cost ensures that the value function decreases monotonically over time. Such monotonicity is known to be beneficial in IRL for episodic tasks [16].\n\nWe employ two types of R(t): \u201clinear\" and \u201csigmoid\". A linear time cost is given as $R(t) = c$ where we use $c = 0.05$. The linear time cost encourages the value to decrease linearly as time progresses. The sigmoid time cost is $R(t) = \\sigma(-t+T/2) - \\sigma(-t-1+T/2)$, where $\\sigma(x) = (1+\\exp(-x))^{-1}$. With the sigmoid time cost, the value function is trained to follow a sigmoid function centered at $T/2$ when plotted against the time. Other forms of $R(t)$ are also possible.\n\nSeparate tuning of $\\tau$. In image generation, we assign different values of temperature for entropy regularization $\\log \\pi_\\phi(x_{t+1}|x_t)$ and velocity regularization $||x_t - x_{t+1}||^2/(2s_t^2)$, such that the resulting running cost becomes $\\tau_1 \\log \\pi_\\phi(x_{t+1}|x_t) + \\tau_2 \\frac{||x_t - x_{t+1}||^2}{(2s_t^2)}$. Typically, we found $\\tau_1 > \\tau_2$ beneficial, indicating the benefit of exploration. Setting $\\tau_1 \\neq \\tau_2$ does not violate our maximum entropy formulation, as scaling $\\tau_2$ is equivalent to scaling $s_t$'s, which can be set arbitrarily."}, {"title": "5 Experiments", "content": "In this section, we provide empirical studies that demonstrate the effectiveness of DxMI in training a diffusion model and an EBM. We first present a 2D example, followed by image generation and anomaly detection experiments. More details on experiments can be found in Appendix C.\n\n5.1 2D Synthetic Data\n\nWe illustrate how DxMI works on 2D 8 Gaussians data. DxMI is applied to train a five-step diffusion model (T = 5) with a corresponding time-dependent value network, both parametrized by time-conditioned multi-layer perceptron (MLP). The last time step (T = 5) of the value network is treated as the energy. The sample quality is measured with sliced Wasserstein distance (SW) to test data. Also, we quantify the quality of an energy function through the classification performance to uniform noise samples (Table 1).\n\nFirst, we investigate the effect of maximum entropy regularization $\\tau$. Setting an appropriate value for $\\tau$ greatly benefits the quality of both the energy and the samples. When $\\tau = 0.1$, the samples from DxMI have smaller SW than the samples from a full-length DDPM do. The energy also accurately captures the data distribution, scoring high AUC against the uniform noise. Without entropy regularization ($\\tau = 0$), DxMI becomes similar to GAN [40]. The generated samples align moderately well with the training data, but the energy does not reflect the data distribution. When $\\tau$ is too large ($\\tau = 1$), the generated samples are close to noise. In this regime, DxMI behaves similarly"}, {"title": "5.2 Image Generation: Accelerating Diffusion Models", "content": "On image generation tasks, we show that DxMI can be used to fine-tune a diffusion model with reduced generation steps, such as T = 4 or 10. We test DxMI on unconditional CIFAR-10 [51] (32 \u00d7 32) and conditional ImageNet [52] downsampled to 64 \u00d7 64, using three diffusion model backbones, DDPM [3], DDGAN [46], and variance exploding version of EDM [50]. The results can be found in Table 2 and 3. Starting from a publicly available checkpoint of each pretrained backbone, we first adjust the noise schedule for the target sampling steps T. When adjusting the noise, we follow the schedule of FastDPM [8] for DDPM and Eq. (5) of [50] for EDM. No adjustment is made for DDGAN, which is originally built for T = 4. The adjusted models are used as the starting point of DxMI training. A single fine-tuning run takes less than 24 hours on four A100 GPUs. We set $\\tau_1 = 0.1$ and $\\tau_2 = 0.01$. The sigmoid time cost is used for all image generation experiments. The sample quality is measured by FID [53], Precision (Prec., [54]), and Recall (Rec., [54]. ResNet is used as our value function and is trained from scratch. More experimental details are in Appendix C.\n\nShort-run diffusion models fine-tuned by DxMI display competitive sample quality. Unlike distillation methods, which are often limited by their teacher model's performance, DxMI can surpass the pre-trained starting point (CIFAR-10 T = 10). Although it does not support single-step generation, DxMI offers a principled approach to training a high-quality generative model with a moderate computation burden. Note that DDGAN does not fit the formulation of DxMI, as $\\pi(x_{t+1}|x)$ in DDGAN is not Gaussian. Nevertheless, DxMI can still enhance sample quality, showing its robustness.\n\nFurthermore, DxMI outperforms SFT-PG [11], another IRL approach implemented with a policy gradient. For a fair comparison, we have ensured that the backbone and the initial checkpoint of SFT-PG and DxMI are identical. Thus, the performance gap can be attributed to the two differences between SFT-PG and DxMI. First, DxMI uses dynamic programming instead of policy gradient."}, {"title": "5.3 Energy-Based Anomaly Detection and Localization", "content": "We demonstrate the ability of DxMI to train an accurate energy function on an anomaly detection task using the MVTec-AD dataset [59], which contains 224\u00d7224 RGB images of 15 object categories. We follow the multi-class problem setup proposed by [58]. The training dataset contains normal object images from 15 categories without any labels. The test set consists of both normal and defective object images, each provided with an anomaly label and a mask indicating the defect location. The goal is to detect and localize anomalies, with performance measured by AUC computed per object category. This setting is challenging because the energy function should reflect the multi-modal data distribution. Following the preprocessing protocol in [58, 57], each image is transformed into a 272\u00d714\u00d714 vector using a pre-trained EfficientNet-b4 [60]. DxMI is conducted in a 272-dimensional space, treating each spatial coordinate independently. With the trained energy function, we can evaluate the energy value of 14x14 spatial features and use max pooling and bilinear interpolation for anomaly detection and localization, respectively.\n\nWe use separate networks for the energy function and the value function in this experiment, as the primary goal is to obtain an accurate energy function. We employ an autoencoder architecture for the energy function, treating the reconstruction error of a sample as its energy [61]. The diffusion model and the value function are five-step time-conditioned MLPs. Unlike conventional diffusion models, DxMI allows for a flexible choice of $\\pi(x_0)$. We set the initial distribution for the sampler to the data distribution applied with noise, aiming to identify the energy value more precisely near the data distribution. More experimental details can be found in C.\n\nDxMI demonstrates strong anomaly classification and localization performance, as shown in Table 4. This result indicates that the trained energy function effectively captures the boundary of normal data. When entropy maximization is disabled by $\\tau = 0$, the diffusion model fails to explore and only exploits regions of minimum energy, resulting in poor performance. We observe that a moderate level of $\\tau = 0.1$ benefits both the sampler and the energy function, as it encourages exploration and provides a suitable level of diversity in negative samples."}, {"title": "6 Related Work", "content": "Faster Diffusion Models. Significant effort has been dedicated to reducing the number of genera-tion steps in diffusion models during sampling while preserving sample quality. One popular approach is to keep the trained diffusion model unchanged and improve the sampling phase independently by tuning the noise schedule [8, 62, 63, 9], improving differential equation solvers [50, 64, 65, 66], and"}, {"title": "7 Conclusion", "content": "In this paper, we leverage techniques from sequential decision making to tackle challenges in generative modeling, revealing a significant connection between these two fields. We anticipate that this connection will spur a variety of algorithmic innovations and find numerous practical applications.\n\nBroader Impacts. DxMI may facilitate deep fakes or fake news. However, trained on relatively low-resolution academic datasets, the models created during our experiments are not capable enough to cause realistic harm. Generative models trained solely using DxMI may possess fairness issues.\n\nLimitations. Training multiple components simultaneously, DxMI introduces several hyperparam-eters. To reduce the overhead of practitioners, we provide a hyperparameter exploration guideline in Appendix B. DxMI is not directly applicable to training a single-step generator. However, a diffusion model fine-tuned by DxMI can be distilled to a single-step generator."}, {"title": "A Adaptive Velocity Regularization", "content": "We are interested in the following optimization problem:\n\n$\\min_{s_0,..., s_{T-1}} KL(\\pi_\\phi(X_{0:T})||q_\\theta(X_T)\\bar{q}(X_{0:T-1}|x_T))$.\n\nPlugging in our choice of $\\log q(x_t| x_{t+1}) = -\\frac{||x_t-x_{t+1}||^2}{2s_t^2} \u2013 D \\log s_t$, we can rewrite the optimization problem as follows.\n\n$\\min_{s_0,..., s_{T-1}} \\mathbb{E}_{x_t, x_{t+1}~\\pi} [ \\sum_{t=0}^{T-1} \\frac{||x_t - x_{t+1}||^2}{2s_t^2} + D \\log s_t ]$,\n\nwhere constant term with respect to $s_t$ is omitted. Since the object function is separable, we can solve the optimization for each $s_t$ independently.\n\n$\\min_{s_t} \\frac{\\mathbb{E}_{x_t, x_{t+1}~\\pi} [||x_t - x_{t+1}||^2]}{2s_t^2} + D \\log s_t, t = 0, ..., T \u2013 1$.\n\nThis optimization has an analytic solution: $(s_t^+)^2 = \\mathbb{E}_{x_t, x_{t+1}~\\pi} [||x_t - x_{t+1}||^2] /D."}, {"title": "B Guideline for Hyperparameters", "content": "Value function. The most crucial design decision in DxMI is how to construct the value function. This design revolves around two primary axes. The first axis is whether the value function should be time-dependent or time-independent. The second axis is whether the value function should share model parameters with other networks, such as the sampler or the energy network.\n\nOur experiments demonstrate various combinations of these design choices. In a 2D experiment, the time-dependent value function shares parameters with the EBM, which is a recommended approach for smaller problems. In an image experiment, we employ a time-independent value function that shares parameters with the energy network, effectively making the value function and the energy function identical. This design choice promotes monotonicity and efficient sample usage, as a single value function learns from all intermediate samples.\n\nIn an anomaly detection experiment, we use a time-dependent value function that does not share parameters with any other network. This design is suitable when a specific structure needs to be enforced on the energy function, such as with an autoencoder, and when making the structure time-dependent is not straightforward.\n\nWhile these are the options we have explored, there are likely other viable possibilities for designing the value function.\n\nCoefficient $\\tau$. Although the coefficient $\\tau$ plays an important role in DxMI, we recommend running DxMI with $\\tau = 0$ when implementing the algorithm for the first time. If everything is in order, DxMI should function to some extent. During normal training progression, the energy values of positive and negative samples should converge as iterations proceed.\n\nAfter confirming that the training is progressing smoothly, you can start experimenting with increasing the value of $\\tau$. Since $\\gamma$ determines the absolute scale of the energy, the magnitude of $\\tau$ should be adjusted accordingly. We set $\\gamma = 1$ in all our experiments and recommend this value. In such a case, the optimal $\\tau$ is likely to be less than 0.5.\n\nLearning rate. As done in two time-scale optimization [53], we use a larger learning rate for the value function than for the sampler. When training the noise parameters $\\sigma_t$ in the diffusion model, we also assign a learning rate 100 times larger than that of the sampler."}, {"title": "C Details on Implementations and Additional Results", "content": "C.1 2D Experiment\n\nSample quality is quantified using sliced Wasserstein-2 distance (SW) with 1,000 projections of 10k samples. The standard deviation is computed from 5 independent samplings. Density estimation"}, {"title": "D Interpretation of the policy improvement method", "content": "In this section, we show that our policy improvement method introduced in Eq. (11) can effectively minimize the KL divergence between the joint distributions, $KL(\\pi_\\phi(X_{0:T})||q_\\theta(X_T)\\bar{q}(X_{0:T-1}|x_T))$. The policy improvement at each timestep $t$ can be expressed as\n\n$\\min_{\\pi(x_{t+1}|x_t)} \\mathbb{E}_{x_t, x_{t+1}~\\pi} [V_{t+1}(x_{t+1}) + \\tau\\log \\pi(x_{t+1}|x_t) \u2212 \\tau \\log \\bar{q}(x_t|x_{t+1})] + const.$\n\nHere omitted the parameters $\\phi$ and $\\psi$ to avoid confusion. Using the definition of value function, above minimization can be expressed as\n\n$\\min_{\\pi(x_{t+1}|x_t)} \\mathbb{E}_{\\pi(x_{t:T})} \\big[ E(x_T) + \\tau \\sum_{t'=t}^{T-1} \\log \\pi(x_{t'+1}|x_{t'}) - \\tau \\sum_{t'=t}^{T-1} \\log \\bar{q}(x_{t'} |x_{t'+1}) \\big];$"}]}