{"title": "Turn That Frown Upside Down:\nFaceID Customization via Cross-Training Data", "authors": ["Shuhe Wang", "Xiaoya Li", "Xiaofei Sun", "Guoyin Wang", "Tianwei Zhang", "Jiwei Li", "Eduard Hovy"], "abstract": "Existing face identity (FaceID) customization methods\nperform well but are limited to generating identical faces\nas the input, while in real-world applications, users of-\nten desire images of the same person but with variations,\nsuch as different expressions (e.g., smiling, angry) or an-\ngles (e.g., side profile). This limitation arises from the lack\nof datasets with controlled input-output facial variations,\nrestricting models' ability to learn effective modifications.\nTo address this issue, we propose CrossFaceID, the first\nlarge-scale, high-quality, and publicly available dataset\nspecifically designed to improve the facial modification ca-\npabilities of FaceID customization models. Specifically,\nCrossFaceID consists of 40,000 text-image pairs from ap-\nproximately 2,000 persons, with each person represented\nby around 20 images showcasing diverse facial attributes\nsuch as poses, expressions, angles, and adornments. Dur-\ning the training stage, a specific face of a person is used\nas input, and the FaceID customization model is forced to\ngenerate another image of the same person but with altered\nfacial features. This allows the FaceID customization model\nto acquire the ability to personalize and modify known fa-\ncial features during the inference stage. Experiments show\nthat models fine-tuned on the CrossFaceID dataset retain\nits performance in preserving FaceID fidelity while signifi-\ncantly improving its face customization capabilities.\nTo facilitate further advancements in the FaceID cus-\ntomization field, our code, constructed datasets, and trained\nmodels are fully available to the public.", "sections": [{"title": "1. Introduction", "content": "Face identity (FaceID) customization is an important im-\nage generation task [18, 23, 27, 25, 8, 16, 26, 36], allowing\nusers to achieve personalized facial customization using a\npre-trained text-to-image diffusion model. Although exist-\ning methods demonstrate effectiveness, they exhibit a sig-\nnificant limitation: they can only generate images with the\nexactly same face as the input, while in real-world applica-\ntions, users often desire images of the same individual but\nwith variations, such as different facial expressions (e.g.,\nsmiling) or angles (e.g., side profile), shown in Figure 1.\nThis issue is primarily due to the lack of such a FaceID\ncustomization dataset where input and output faces exhibit\ncontrolled variations. In current datasets for training face\ncustomization models, the input and output faces in the\ndataset are often identical [15, 3, 40, 19]. This setup re-\nstricts the model's ability to learn effective facial feature\nmodifications during training, instead reinforcing its focus\non maintaining face consistency. Consequently, the model\nstruggles during inference when it is required to modify fa-\ncial features while preserving FaceID consistency due to the\nlack of relevant training experience.\nTo address the above issue, in this paper, we propose\nCrossFaceID, the first large-scale, high-quality, and pub-\nlicly available dataset specifically designed to improve the\nfacial modification capabilities of FaceID customization\nmodels. Specifically, to obtain multiple public images of\nthe same person, CrossFaceID gathered 40,000 images from\napproximately 2,000 celebrities, with each celebrity repre-\nsented by around 20 images showcasing diverse facial at-\ntributes such as poses, expressions, angles, and adornments.\nTo annotate these images, we use GPT-4 to generate de-\ntailed descriptions for these 40,000 images, particularly fo-\ncusing on the facial features of the individuals, resulting in\na set of 40,000 one-to-one text-image pairs.\nIn this way, during the training stage, we start with a pre-\ntrained FaceID customization model to ensure a solid foun-\ndation for preserving FaceID fidelity. Then, we employ a\ncross-training method, where a specific face of a person is\nused as input, and the pre-trained model is forced to gen-\nerate another image of the same person but with altered fa-\ncial features. This allows the pre-trained FaceID customiza-\ntion model to acquire the ability to personalize and modify"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Text-to-image Models", "content": "Text-to-image refers to the process of generating im-\nages from textual descriptions using pre-trained image gen-\neration models [24, 6, 7, 23, 25, 27, 13]. These models\nare trained to understand the relationship between textual\ninput and visual content, enabling them to create images\nthat match the given description. Thanks to the success of\nthe transformer model [33], most early text-to-image ap-\nproaches can be broken down into two stages: (1) using an\nimage encoder, such as DARN [10], PixelCNN [31], Pix-\nelVAE [11], or VQ-VAE [32], to convert an image into\nseveral tokens; and (2) training the model to predict these\nimage tokens based on the provided text input within the\ntransformer framework [33]. Recently, diffusion models\n[28, 29, 18, 5, 23, 27, 25, 2, 13] have emerged as the new\nstate-of-the-art approach for image generation, offering in-\nnovative solutions for the text-to-image task. In this ap-\nproach, the text prompt is first encoded into embeddings\nusing a pre-trained language model such as T5 [22] or CLIP\n[21], and then these encoded embeddings are used to guide\nthe diffusion process, resulting in the generation of high-\nquality images. For example, GLIDE [18] employs a cas-\ncaded diffusion architecture with CLIP [21] as the text en-\ncoder to condition on natural language descriptions, facil-\nitating both image generation and editing. Imagen [27]\nadopts T5 [22], a generic large language model pre-trained\non text-only corpora, as the text encoder of diffusion mod-\nels, to further enhance the text understanding."}, {"title": "2.2. ID Customization", "content": "FaceID customization for text-to-image models refers\nto the process of personalizing the text-to-image genera-\ntion model by tailoring it to better recognize, and gener-\nate facial features and attributes specific to individual users\n[30, 37, 38, 4, 34, 35, 17, 19]. Most of these works are\noptimization-free methods, which directly encode FaceID\ninformation into the generation process. For instance, Face0\n[30] substitutes the last three text tokens with the projected\nface embedding in the CLIP [21] space, using the result-\ning combined embedding to guide the diffusion process.\nIn a similar vein, PhotoMaker [17] adopts a related ap-\nproach but enhances its ability to extract FaceID embed-\ndings by fine-tuning specific Transformer [33] layers in the\nimage encoder and merging the class and image embed-\ndings. Additionally, IP-Adapter [37] and InstantID [34]\nleverage FaceID embeddings from a face recognition model\nrather than CLIP image embeddings, ensuring consistent ID\nrepresentation. However, these methods primarily concen-\ntrate on improving FaceID fidelity while overlooking cus-\ntomization. As illustrated in Figure 1, due to the complex\narchitecture required to preserve FaceID, it is challenging\nfor them to customize the generated face simply by modi-\nfying the input prompt. For example, generating an image\nof a person smiling when the input face does not show a\nsmile becomes difficult. In this paper, we address this is-\nsue by altering only the composition of the training data,\nwithout changing the model structure. Extensive experi-\nments demonstrate that our approach can effectively modify\nor preserve the input FaceID based on the input text prompt."}, {"title": "3. Dataset Construction: CrossFaceID", "content": "In this section, we provide details on the dataset con-\nstruction of CrossFaceID, which aims to address the issue\nin existing FaceID customization datasets, where input and\noutput faces do not exhibit controlled variations. The con-\nstruction process starts with images of the same individ-\nual. Next, we gather multiple images that showcase the\nperson under varying facial attributes, such as different ex-\npressions, angles, or adornments. Using GPT-4 [1], we then\ngenerate detailed textual descriptions for each image, par-\nticularly focusing on the facial features of the individual,\nresulting in a dataset of one-to-one text-image pairs."}, {"title": "3.1. Image Collection", "content": "To gather multiple public images of the same individ-\nual, we focus on celebrities, as they have numerous pub-\nlicly available images on the Internet showcasing variations\nin facial features. As a result, we crawled approximately\n60K images from 1626 celebritys."}, {"title": "3.2. Image Filtering", "content": "The collected images from the Internet may include\nnoise, such as blurred faces or low resolution, which could\nsignificantly impact the quality of the subsequent FaceID\ncustomization models. To address this issue, we manu-\nally established rules to carefully select high-quality images\nsuitable for FaceID customization training:\n(1) The image must include faces, with a maximum\nof three faces allowed. This condition is based on two key\nreasons: firstly, when an image contains multiple faces, the\nmodel may have difficulty identifying which face to focus\non, potentially mixing up facial features between individu-\n    als. Secondly, during the inference stage, the trained FaceID\ncustomization model is typically designed to work with a\nsingle primary face. Limiting the number of faces ensures\nconsistency with the inference process.\n(2) The image resolution must be at least 512x512 or\nhigher. High-resolution images generally contain finer fa-\ncial details, such as subtle expressions, skin texture, and\nsmall features like wrinkles or dimples. As a result, they\noffer richer visual information for the FaceID customiza-\ntion model to analyze, leading to improved feature extrac-\ntion and more effective learning.\n(3) The face should be at least 4% of the image. This is\nbecause larger facial regions provide more pixels dedicated\nto facial features, enabling the FaceID customization model\nto better capture details such as expressions, and facial tex-\ntures, which are critical for FaceID customization. As for\nthe number \"4%\", it was determined through iterative re-\nfinements and validated via human and model evaluations."}, {"title": "3.3. Image Annotations with GPT-40", "content": "Since the crawled images lack captions, which are essen-\ntial for training FaceID customization models, we leverage\nGPT-40 [14] to annotate these images. This annotation pro-\ncess generate detailed textual descriptions with a specific\nemphasis on the individual's facial features.\nGiven a person image, we prompt GPT-40 [14] to gener-"}, {"title": "4. CrossFaceID Based FaceID Customization", "content": "In this section, we first provide an overview of the fun-\ndamental concepts for training face ID customization mod-\nels in Section 4.1. Following that, we delve into the train-\ning and inference processes utilizing our constructed Cross-\n    FaceID dataset in Section 4.2."}, {"title": "4.1. Preliminaries", "content": null}, {"title": "4.1.1 Diffusion Models", "content": "Diffusion models [12, 25, 20] are a class of generative mod-\nels that have gained prominence for their ability to generate\nhigh-quality, realistic data, such as images, by simulating\na gradual process of transforming random noise into struc-\ntured data. Specifically, during each training process, noise\n\u03b5 is sampled and added to the input image xo based on a"}, {"title": "4.1.2 IP-Adapter", "content": "IP-Adapter [37] is a method for enabling image prompt\ncapabilities alongside text prompts, without altering the\noriginal text-to-image models. It utilizes a distinct decou-\npled cross-attention mechanism, embedding image features\nthrough several extra cross-attention layers, while keeping\nthe other model parameters intact. Specifically, in original\ndiffusion models, the text features from the CLIP [21] or\nT5 [22] text encoder are incorporated into the model by in-\nputting them into the cross-attention layers. Given the latent\nimage features Z and the text features Ctext, the output of\ncross-attention Z' can be expressed as:\n$Z' = Attention(Q, K, V) = Softmax(\\frac{QKT}{\\sqrt{d}})V$,\n$Q = ZWq, K = C_{text}W_k, V = C_{text}W_v$\nwhere Q, K, and V represent the query, key, and value\nmatrices in the attention operation, respectively, while Wq,\nWk, and Wv are the weight matrices of the learnable layers."}, {"title": "4.1.3 InstantID", "content": "InstantID [34] is an improved version of IP-Adapter [37],\ndesigned to generate customized images with different\nposes or styles based on a face ID image, while maintaining\nhigh fidelity. It can be decomposed into three key compo-\nnents: (1) FaceID Embedding, which extracts facial infor-\nmation using advanced visual models; (2) Spatial Facial In-\nformation Extraction, which encodes fine-grained features\nfrom the face image as supplementary spatial facial data to\nimprove the face ID embedding; and (3) Image Adapter,\nwhich utilizes cross-attention similar to IP-Adapter to com-\nbine the face ID embedding with the text embedding.\nFaceID Embedding. For a given image, a pre-trained\nface model is used to detect the face and encode it as the face"}, {"title": "4.2. FaceID Customization on CrossFaceID", "content": null}, {"title": "4.2.1 Training", "content": "During the training phase, we keep the training structure of\nFaceID customization unchanged as described in Section 4,\nwhile modifying the arrangement of the input and output."}, {"title": "4.2.2 Inference", "content": "The inference process follows the same approach as the dif-\nfusion models outlined in Section 4.1.1. It begins with a\nsample of Gaussian noise, represented by \u0445\u0442, where T is\na predefined number of timesteps. This initial state, com-\nposed entirely of unstructured noise, serves as the starting\npoint, representing a meaningless input image. At each\ntimestep t, the model takes the noisy image xt as the input\nand utilizes the text prompt condition Ctext and the input\nface condition Cid to predict the clean image or the noise\nthat should be removed, progressively refining the image\ntowards the final clean output 20. The predicted noise ee is\nthen used to update the noisy image, denoising step by step:\n$x_{t-1} = a_t X_t - \\sigma_t \\epsilon_\\theta(x_t, C_{text}, C_{id},t)$\nwhere at and \u03c3\u03c4 are two coefficients controlling the denois-\ning process. Over several timesteps T, the noise is gradually"}, {"title": "5. Experiments", "content": "To assess the effectiveness of CrossFaceID, we train\nthe state-of-the-art FaceID customization model, InstantID\n[34], as well as its original version, IP-Adapter [37], us-\ning our constructed CrossFaceID dataset. We then perform\ncomparative experiments to evaluate both FaceID fidelity\nand FaceID customization capabilities."}, {"title": "5.1. Main Results", "content": "In this section, we conduct experiments to separately\nevaluate the FaceID fidelity and FaceID customization ca-\npabilities of models trained on our constructed Cross-\n    FaceID dataset. For clarity, we refer to the official In-\nstantID model as \"InstantID,\" and the model further fine-\ntuned on our CrossFaceID dataset as \"InstantID + Cross-\n    FaceID.\" Similarly, the InstantID model pre-trained on our\ncurated LAION dataset is referred to as \"LAION,\" while the\nmodel further trained on our CrossFaceID dataset is called\n\"LAION + CrossFaceID.\u201d Below are results showing their\nabilities in FaceID fidelity and FaceID customization."}, {"title": "5.1.1 FaceID Fidelity", "content": "Figure 4 illustrates the performance of FaceID customiza-\ntion models in maintaining FaceID fidelity. From these re-\nsults, we can conclude that (1) for both the official InstantID\nmodel and the LAION-trained model, the ability to main-\ntain FaceID fidelity remains consistent before and after fine-\ntuning on our CrossFaceID dataset. Such as the case 1, all\nmodels, including the two fine-tuned on the CrossFaceID\ndataset generate the exact same girl as the input image and\nwearing shirt in a garden as the input text \"a beautiful girl\nwearing casual shirt in a garden\". This demonstrates that\nour constructed CrossFaceID dataset does not compromise\nthe FaceID fidelity performance of these FaceID customiza-\ntion models. (2) The model trained on our curated LAION\ndataset demonstrates performance comparable to the offi-\ncial InstantID model in maintaining FaceID fidelity. For\ninstance, in case 2, both the official InstantID model and\nthe LAION-trained model successfully generate the desired\nimages based on the input. This ensures the fairness of our\nexperiments when further fine-tuning CrossFaceID on mod-\nels with comparable baseline performance."}, {"title": "5.1.2 FaceID Customization", "content": "Figure 5 demonstrates the performance for FaceID cus-\ntomization models in customizing or editing FaceID. From\nthese results, we can clearly observe an improvement in\nthe models' ability to customize FaceID after being fine-\ntuned on our constructed CrossFaceID dataset. For exam-"}, {"title": "5.2. Quantitative Results", "content": "To more effectively evaluate the effectiveness of our\nCrossFaceID dataset, we conduct quantitative experiments\non two test sets: CrossFaceID-test and Unsplash-50 [9].\nDue to the lack of test sets for evaluating the abilities of\nmodels in customizing FaceID, we collected CrossFaceID-\n    test. CrossFaceID-test consists of 200 text-image pairs\nsourced from the Internet. For each image, we include a\nversion of the same person with a different facial expression"}, {"title": "5.3. Human Evaluations", "content": "While automated evaluations, as conducted above, effec-\ntively measure objective aspects like FaceID fidelity and\nprompt adherence, they fall short in assessing subjective\nqualities, such as whether the customized face accurately\nrepresents the requested attributes (e.g., expressions or an-\ngles) while maintaining resemblance to the input person.\nTo address this limitation, we incorporate human evalua-\ntions into our experiments. In this way, we collected 200\ncelebrity faces from the Internet and manually designed\nprompts to force the evaluated FaceID models to gener-\nate images showing different expressions and angles (e.g.,\nsmile, sadness, turning head and wearing attire). The gen-\nerated images are then evaluated by 10 human participants,\nwho score them based on three criteria: (1) Customization:\nwhether the generated image accurately follows the input"}, {"title": "6. Conclusion", "content": "In this paper, we propose CrossFaceID, the first large-\nscale, high-quality, and publicly available dataset specifi-"}]}