{"title": "IMC 2024 Methods & Solutions Review", "authors": ["Shyam Gupta", "Dhanisha Sharma", "Songling Huang"], "abstract": "Abstract-For the past three years, Kaggle has been hosting the Image Matching Challenge, which focuses on solving a 3D image reconstruction problem using a collection of 2D images. Each year, this competition fosters the development of innovative and effective methodologies by its participants. In this paper, we introduce an advanced ensemble technique that we developed, achieving a score of 0.153449 on the private leaderboard and securing the 160th position out of over 1,000 participants. Additionally, we conduct a comprehensive review of existing methods and techniques employed by top-performing teams in the competition. Our solution, alongside the insights gathered from other leading approaches, contributes to the ongoing advancement in the field of 3D image reconstruction. This research provides valuable knowledge for future participants and researchers aiming to excel in similar image matching and reconstruction challenges.\nIndex Terms-3d scene reconstruction, ALIKED, descriptors, SIFT, lightglue, keypoints, COLMAP, image pairs, SFM, attention, descriptors.", "sections": [{"title": "I. INTRODUCTION", "content": "The process of reconstructing 3D models from diverse image collections, known as Structure from Motion (SfM) [17], is critical in Computer Vision but remains challenging, especially with images captured under varied conditions like different viewpoints, lighting, and occlusions. This competition [8] addresses these complexities across six distinct categories:\n1) Phototourism and historical preservation: Includes diverse viewpoints, sensor variations, and challenges posed by ancient historical sites.\n2) Night vs. day and temporal changes: Combines images from different times, lighting conditions, and weather, testing algorithms against temporal variations.\n3) Aerial and mixed aerial-ground: Involves images from drones with arbitrary orientations, alongside ground-level shots.\n4) Repeated structures: Focuses on disambiguating perspectives of symmetrical objects.\n5) Natural environments: Challenges include irregular structures like trees and foliage.\n6) Transparencies and reflections: Deals with objects like glassware that lack texture and create reflections, presenting unique computational hurdles.\nThe competition aims to advance understanding in Computer Vision by bridging traditional image-matching techniques with modern machine learning approaches. By tackling these varied categories, participants contributed to evolving solutions for robust 3D reconstruction from real-world image datasets."}, {"title": "II. EXISTING METHODS & TECHNIQUES", "content": "Every solution has a unique way of solving the problem. However, there is a standard flow of data most of the solutions followed.\nMATCHFORMER [1]\nMatchFormer(2022) was a novel approach to matching multiple views of a scene, crucial for tasks like Structure-from-Motion (SfM), Simultaneous Localization and Mapping (SLAM), relative pose estimation, and visual localization. Traditional methods using detectors and hand-crafted local features are computationally heavy. Recent deep learning methods use Convolutional Neural Networks (CNNs) for feature extraction but are often inefficient due to overburdened decoders.\nMatchFormer proposed a new pipeline called extract-and-match, which uses a pure transformer model to perform feature extraction and matching simultaneously. This approach is more intuitive and efficient compared to previous methods. MatchFormer introduces a hierarchical transformer with a matching-aware encoder that uses interleaved self- and cross-attention mechanisms. This design improves computational efficiency and robustness, especially in low-texture scenes."}, {"title": "ALIKED [2]", "content": "Efficiently and robustly extracting image keypoints and descriptors is essential for various visual measurement applications, including simultaneous localization and mapping (SLAM) [6], computational photography, and visual place recognition. Traditional methods relied on hand-crafted algorithms, which were not very efficient or robust. Modern approaches use deep neural networks (DNNs) for better performance.\nKeypoints and Descriptors\n1) Keypoints: Distinctive points in an image that are used for tasks like image matching and 3D reconstruction.\n2) Descriptors: Descriptions of the keypoints that allow different keypoints to be compared and matched across images.\nEarly DNN methods extracted descriptors at predefined keypoints, but newer methods use a single network to extract both keypoints and descriptors simultaneously. These newer methods generate a score map and a descriptor map from which keypoints and descriptors are extracted.\nChallenges with Existing Methods is Existing methods use fixed-size convolutions that lack geometric invariance, which is crucial for accurate image matching. This problem is partially solved by estimating the scale and orientation of descriptors. However, these methods can only handle affine transformations, not more complex geometric transformations.\nDeformable Convolution Networks (DCNs) can model any geometric transformation by adjusting each pixel's position in the convolution, enhancing descriptor representation. However, DCNs are computationally expensive.\nSparse Deformable Descriptor Head (SDDH) are used to improve efficiency, the paper introduces the Sparse De-formable Descriptor Head (SDDH):\n1) SDDH: Extracts deformable descriptors only at detected keypoints instead of the entire image, significantly reducing computational costs. It uses adjustable positions"}, {"title": "COMPLEX DENSE KE\u03a5\u03a1\u039f\u0399\u039d\u03a4 \u039cETHODS", "content": "A. LOFTR (Local Feature Transformer)\nLOFTR provides dense correspondences between images without needing descriptors. It uses a transformer-based architecture to establish correspondences directly from image patches.\n\u2022 Transformer Layers: Utilizes multi-head self-attention to relate features across the entire image.\n\u2022 High Computational Cost: Despite its accuracy, the dense matching process is computationally intensive.\nB. SuperGlue [15]\nSuperGlue matches keypoints by considering the entire context of both images simultaneously using a graph neural network with attention mechanisms.\n\u2022 Graph Neural Network: Models relationships between keypoints across images.\n\u2022 Transformer-Based: Uses self and cross-attention to enhance matching robustness.\n\u2022 Training and Computation: Requires significant computational resources for training and inference.\nWhy LightGlue is Preferred\nC. Efficiency\n\u2022 Adaptive Matching: LightGlue adjusts its computational effort based on the difficulty of the image pair, making it faster for easy matches.\n\u2022 Early Discarding: Discards non-matchable points early, reducing unnecessary computations.\nWhile LOFTR (Learning to Optimize Frameworks) [14] and Superglue have made significant strides in multimodal research, they do possess certain drawbacks when compared to LightGlue.\nIn summary, while LOFTR and Superglue have advanced multimodal research, their drawbacks in terms of complexity, computational requirements, and generalization"}, {"title": "COLMAP [17] [19]", "content": "COLMAP (Construction and Localization MAPping) is a versatile and widely-used photogrammetry software designed for 3D reconstruction and structure-from-motion (SfM) tasks. It provides a suite of tools for processing images to create 3D models by detecting, describing, and matching keypoints across images, and then using these matches to estimate the 3D structure and camera positions.\nKey features of COLMAP include:\n\u2022 Feature Extraction and Matching: Uses algorithms like SIFT for detecting and matching keypoints across multiple images.\n\u2022 Structure-from-Motion (SfM): Estimates camera poses and reconstructs sparse 3D points.\n\u2022 Multi-View Stereo (MVS): Generates dense 3D models by computing depth maps and fusing them into a consistent 3D reconstruction.\n\u2022 Scalability: Efficiently handles large datasets with thousands of images.\n\u2022 User Interface: Provides a graphical interface for easy interaction and visualization, along with command-line tools for automation.\nCOLMAP is favored for its robustness, accuracy, and ease of use, making it suitable for applications in archaeology, architecture, cultural heritage preservation, and more.\nOMNIGLUE [16]\nOmniGlue addresses the generalization limitations of current learnable image matchers, which typically excel in specific domains with abundant training data but falter in diverse, unseen domains. Traditional methods like SIFT, despite being hand-crafted, often outperform these advanced models in unfamiliar contexts due to their domain-agnostic nature.\nOmniGlue introduces two key innovations to enhance generalizability:\n1) Foundation Model Guidance: Utilizes the broad visual knowledge of large pre-trained models like DINOv2 [7] to guide the matching process, enhancing performance in domains not covered during training.\n2) Keypoint-Position Guided Attention: Disentangles positional encoding from matching descriptors, avoiding over-reliance on geometric priors from training data, thereby improving cross-domain performance."}, {"title": "III. ABBREVIATIONS AND ACRONYMS", "content": "1) Correspondences: Points in one image that match points in another image, allowing the images to be aligned.\n2) Sparse Interest Points: Keypoints in an image that are distinctive and used for matching across images.\n3) High-Dimensional Representations: Numerical descriptions of keypoints that capture their local visual appearance.\n4) Robustness: The ability to handle variations in viewpoint, lighting, and other changes.\n5) Uniqueness: The ability to discriminate between different points to avoid false matches.\n6) Transformer Model: A type of neural network architecture that uses self-attention mechanisms to process input data.\n7) Pareto-Optimal: A state where no criterion (like efficiency or accuracy) can be improved without worsening another.\n8) Simultaneous Localization and Mapping (SLAM): A technique used in robotics and computer vision to create a map of an environment while simultaneously keeping track of the device's location within that environment.\n9) Self-attention: A mechanism in neural networks where each element of a sequence pays attention to other elements to understand its context better.\n10) Cross-attention: A mechanism where elements of one sequence pay attention to elements of another sequence, useful in tasks like machine translation and feature matching.\n11) Positional Patch Embedding (PosPE): A method to incorporate positional information into patches of an image to improve feature detection.\n12) Geometric Invariance: The ability of a method to handle various transformations (like rotation, scaling) in the input data.\n13) Deformable Convolution Network (DCN): A type of neural network that can adjust the position of each pixel in the convolution, allowing it to model more complex transformations.\n14) Neural Reprojection Error (NRE) Loss: A loss function used to measure the difference between predicted and actual keypoint locations in image matching tasks.\n15) Affine Transformations: Transformations that include scaling, rotation, and translation.\n16) Specularities: Bright spots of light that appear on shiny surfaces when they reflect light sources. These can create difficulties in image matching and 3D reconstruction."}, {"title": "IV. OUR SOLUTION", "content": "Our solution aims to implement a complex pipeline of image feature extraction, matching, and 3D reconstruction, integrating a variety of advanced image processing and 3D reconstruction tools. Our solution progressed in 3 steps as follows:\n1) The get_keypoints method uses a deep learning model (such as LoFTR) to extract key points from the image. Then, the matches_merger method and the key-points_merger method are used to merge the key points from different images into a unified dataset to ensure the uniqueness and consistency of the key points.\n2) The wrapper_keypoints method and the reconstruct_from_db method use COLMAP to perform 3D reconstruction from the key points and matching data in the database to obtain the camera pose.\n3) Finally, the create_submission method generates a submission file and formats the output results to participate in a specific challenge or competition. The entire pipeline achieves efficient and accurate image processing through accurate feature point extraction, reliable matching filtering, and efficient 3D reconstruction, which is suitable for application scenarios such as autonomous driving, robot navigation, and virtual reality that require sophisticated image processing and 3D reconstruction.\nWhat made the difference?\nHere are a few points we missed on. In Winner's solutions you can observe, they detected these edge cases and solved them efficiently with novel methods, which resulted them better scores.\n1) Not correcting image orientation place a significant role. since the algorithms we used are not designed for affine transformations neither they are scale & rotation invariant.\n2) We did not solve for transparent & Low light images.\n3) We should have used an ensemble of aliked+lightglue [2] [4] for key points detection and feature extraction. If we would have done these changes. We could have scored higher."}, {"title": "TOP SOLUTIONS", "content": "We have summarized most of the terms and latest research you shold have known for a successful submission in the competition. Top Medal Baggers in Kaggle mostly used permutation & combination of these techniques to get best scores.\nV. 1ST PLACE SOLUTION\nThe final solution combined 3D image reconstruction (I3DR) with COLMAP [19] for non-transparent scenes and direct image pose estimation (DIP) for transparent scenes. They used an ensemble of ALIKED extractors and LightGlue matchers, cross-validation, multi-GPU acceleration, and a new cropping method. Integration of OmniGlue enhanced match accuracy, and multiple reconstructions were merged for robust results.\nSolving Transparent Surface Keypoint Matching\nThe solution started with performing orientation correction [5]. For detecting keypoints they used ALIKED + LightGlue, However faced with a problem of keypoint detection on transparent surfaces.This was a problem which was faced by most of the kagglers. These top solutions discuss and address such problems in detail.\nThe initial SfM pipeline with COLMAP did not work with transparent scenes. To address this, they experimented with different strategies, hypothesizing that direct pose estimation might help compute the rotation matrix. They assumed cameras were positioned close to the object, capturing it from all sides.\nApproach #1\nThey placed cameras in a circle around the object and sorted images using several methods:\n\u2022 Optical Flow: Calculated magnitude for each image pair and assigned a weight equal to the standard deviation of the magnitude."}, {"title": "VI. 2ND PLACE SOLUTION", "content": "The second-place solution for IMC 2024 devised separate strategies for conventional and transparent scenes due to their distinct characteristics, which were identified through extensive trials.\nPREPROCESSING\n1. Rotation Detection: Utilized a rotation detection model to predict and correct image rotations. Retained original rotations if less than 10% of images were predicted as rotated, acknowledging the model's potential inaccuracies. [5]\n2. Shared Camera Intrinsics: If image dimensions were identical, set all cameras to share the same internal parameters, occasionally improving results by 0.01.\n3. Transparency Detection: Calculated the average difference between images to classify scenes as transparent or not, enabling separate handling for each type.\nMODELING TECHNIQUES\n1. Global Features: Developed a robust global feature descriptor combining point and patch features. Extracted point features (ALIKED) and patch features (DINO), establishing one-to-one correspondence based on spatial relationships. Used clustering and the VLAD algorithm to generate global descriptors. This method outperformed existing techniques (NetVLAD, AnyLoc, DINO, SALAD) on VPR-related datasets.\n2. Local Features: Utilized three types of local features: Dedode v2 + Dual Softmax, DISK + LightGlue, and SIFT + Nearest Neighbor. The Dedode v2 detector produced rich and evenly distributed feature points. The G-upright descriptor and dual softmax matcher were selected for this purpose.\n3. MST-Aided Coarse-to-Fine SfM Solution:\n1) Constructed a similarity graph with images as vertices and similarities as edges. Computed the Minimum Spanning Tree (MST) to obtain an optimal data association, used for the initial SfM. This stage focused on removing incorrect associations and improving coarse-grained accuracy.\n2) Utilized full data associations and the coarse model from Stage 1 to provide initial camera pose priors for geometric verification. This filtered out incorrect feature matches, maintaining coarse-grained advantages while improving fine-grained accuracy.\n4. Post-Processing:\nEmployed pixsfm to optimize the SfM model. Deployed an HLoc-based relocalization module to process unregistered images, typically resulting in a 0-0.01 score improvement.\nHandling Transparent Scenes:Explored various local features (ALIKED, DISK, LOFTR, DKMV3), but none were satisfactory.\nMany Kagglers dealt with the problem by separately handling transparent and conventional scenes with tailored preprocessing and modeling techniques, this solution achieved significant accuracy improvements in both types of scenes. Thereby, resulting them in 2nd place gold."}, {"title": "VII. 3RD PLACE SOLUTION", "content": "The third-place solution for IMC 2024, VGGSfM, is a structure-from-motion method based on Visual Geometry Grounded Deep Structure From Motion, which was enhanced for this competition. The approach involved several key strategies:\nVGGSfM Across All Frames: Applied VGGSfM to all input frames, improving mAA by 4% compared to the baseline. However, due to GPU memory limitations on Kaggle servers, this method had to be integrated into the existing pycolmap pipeline.\n1) Additional Tracks: Utilized VGGSfM's track predictor to estimate 2D matches and fed them into pycolmap. Nearest frames were identified using NetVLAD or"}, {"title": "VIII. 4TH PLACE SOLUTION", "content": "HANDLING TRANSPARENT IMAGES\nForeground Segmentation\n\u2022 Observation: Many keypoints in transparent scenes appeared in the background, disrupting camera pose estimation.\n\u2022 Solution: Employed the DINOv2 Segmenter, identifying foreground objects as class5 (\"bottle\" in VOC2012). This allowed high-precision segmentation by focusing on transparent objects.\n\u2022 Keypoint Detection: Detected keypoints at the original image scale (1024x1024 grid units) without resizing, which was efficient given the uniform image size. Key-points were detected only in the foreground area using the segmented results from DINOv2.\nFeature Matching\n\u2022 Strategy: Limited the search for matches to corresponding grids during keypoint detection, significantly reducing the search range and focusing on relevant areas, improving matching efficiency and accuracy.\nLEVERAGING ALIKED AND LIGHTGLUE\nNon-Transparent Scenes\n\u2022 Keypoint Detection: Generated keypoints for images rotated in 90-degree increments using ALIKED-n16, retaining keypoints for each rotation.\n\u2022 Matching Stage: Utilized LightGlue to evaluate matches. For each fixed set of keypoints, evaluated matches with rotated counterparts, adopting the combination with the highest number of matches. This ensured robust matching regardless of image rotation.\nAdditional Techniques\nExhaustive Matching\n\u2022 Instead of searching for pairs using embedding-based similarity measures like DINOv2 or EfficientNet, exhaustive matching for all image pairs was performed, mitigating the risk of missing matches due to low similarity scores.\nUsing All Images\n\u2022 By incorporating images beyond those listed in the submission file (up to 100 images for validation), the solution increased the number of triangulated points, enhancing 3D reconstruction accuracy."}, {"title": "IX. 5TH PLACE SOLUTION", "content": "BUILD LOCAL EVALUATION DATASETS\nTo manage the large dataset realistically, three subsets were created using random sampling strategies. Results across these subsets showed high correlation, validating the chosen subset for local cross-validation (CV) reporting.\nBUILD A GENERAL SFM PIPELINE\nThe pipeline was divided into three modules:\nA. Proposing Pair Candidates by Global Descriptors\nUtilized pretrained models like EVA-CLIP Base, ConvNeXt Base, and Dinov2 ViT Base to extract global features. Customized similarity thresholds based on scene diversity (e.g., Lizard versus Cylinder) using cosine similarity."}, {"title": "B. Matching Pairs in the Candidate List", "content": "Focused on lightweight detector-based methods for image matching due to their efficiency. Experimented with SIFT + NN and LightGlue combinations, noting significant performance boosts in CV but slight drawbacks in LB performance.\nC. Reconstruction with Colmap\nExplored various approaches including single-camera usage and manual initial pair settings, aiming for improved reconstruction accuracy. Incremental mapping enhanced consistency in CV results but showed minimal impact on LB.\nCUSTOMIZE THE PIPELINE FOR EACH SPECIFIC CATEGORY\nD. Transparent Objects\nImplemented segmentation models like MobileSAM for mask detection and keypoint extraction (e.g., ALIKED). Opted for the smallest mask encompassing most keypoints to focus solely on the object, significantly boosting CV performance with LightGlue.\nE. Finding the Best Pairs\nImplemented methods to find consecutive pairs efficiently. Used exhaustive matching for all pairs and built matrices based on match counts, achieving nearly 100% accuracy in pair selection. But for dark/dim images, I assume that these scenes may have plenty of false positive matches because of their natural properties, so they tried tuning parameters with much more strict values (e.g., increasing the matching threshold to 0.5, etc.). Nonetheless, it didn't show any improvement. so they tried dopplegangers [3]. First, they run SfM one time to get all the matching pairs. then used the Doppelganger model to filter out pairs that have high probabilities as false positives (doppelgangers). It didn't show any improvement on the church scene on the local CV."}, {"title": "X. EVALUATION METRIC- MEAN AVERAGE ACCURACY (MAA)", "content": "Submissions are evaluated based on the mean Average Accuracy (mAA) of the registered camera centers C = -RTT.\nGiven the set of cameras of a scene parameterized by their rotation matrices R and translation vectors T, and the hidden ground truth, the evaluation computes the best similarity transformation T (scale, rotation, and translation altogether) that registers the highest number of cameras onto the ground truth starting from triplets of corresponding camera centers.\nA camera is registered if ||Cg \u2013 T(C)|| < t, where Cg is the ground-truth camera center corresponding to C and t is a given threshold. Using a RANSAC-like approach, all possible (N choose 3) feasible similarity transformations T' derived by Horn's method on triplets of corresponding camera centers (C, Cg) are exhaustively verified. Here, N is the number of cameras in the scene.\nEach transformation T' is refined into T\" by registering the camera centers again using Horn's method, incorporating previously registered cameras with the initial triplets. The best model T, among all T\" with the highest number of registered cameras, is returned."}, {"title": "XI. CONCLUSION", "content": "Joining this competition for the first time came with a lot of learning and experiences. We learnt a lot about sfm problems, how edge cases can lead to poor reconstruction results, how using SOTA will not guarantee top score alone, which we saw in case of 2nd place solution. Special thanks to my co-authors for writing this competition review with me."}]}