{"title": "Exploring Straightforward Conversational Red-Teaming", "authors": ["George Kour", "Naama Zwerdling", "Marcel Zalmanovici", "Ateret Anaby-Tavor", "Ora Nova Fandina", "Eitan Farchi"], "abstract": "Large language models (LLMs) are increasingly used in business dialogue systems but they pose security and ethical risks. Multi-turn conversations, where context influences the model's behavior, can be exploited to produce undesired responses. In this paper, we examine the effectiveness of utilizing off-the-shelf LLMs in straightforward red-teaming approaches, where an attacker LLM aims to elicit undesired output from a target LLM, comparing both single-turn and conversational red-teaming tactics. Our experiments offer insights into various usage strategies that significantly affect their performance as red teamers. They suggest that off-the-shelf models can act as effective red teamers and even adjust their attack strategy based on past attempts, although their effectiveness decreases with greater alignment.\nWarning: This paper contains examples and model-generated content that may be considered offensive.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities in tasks traditionally requiring human expertise. Transformer-based models, exemplified by OpenAI's ChatGPT (Achiam et al., 2023), Google's Gemini (Anil et al., 2023), and Antropic's Claude (Templeton et al., 2024), have revolutionized conversational systems (Zhou et al., 2023), impacting various business functions such as search, customer service, personal assistance, and online education (Okonkwo and Ade-Ibijola, 2021; Neto and Fernandes, 2019). However, the conversational setting presents challenges due to the potential generation of undesired responses including toxic, threatening, biased, harmful, and inaccurate content limiting their use in business applications (Perez et al., 2022; Chen et al., 2023; Dong et al., 2024; Yang et al., 2024; Chowdhury et al., 2024; Kaddour et al., 2023)."}, {"title": "2 Attack Tactics", "content": "We aim to investigate the capacity of an attacker model A to engage in a human-interpretable conversation that prompts a black-box target LLM T to generate undesired responses. We compare various single-turn and multi-turn red teaming strategies, focusing on straightforward, easy-to-implement methods. The attacker tactics were chosen to address the research question at hand.\nThe attack tactics can be categorized according to the aspects they highlight. The first aspect is the type of attack: single-turn or conversational. The second aspect is whether we use an LLM to generate our attack or employ a \"template\" attack. The third aspect is whether the conversation starts with the given objective, revealing the malicious intent from the beginning, or starts differently to conceal the intent. The fourth aspect is whether it has access to previous attack attempts.\nWe use the following annotation to precisely describe various attacks. O represents the attack objective, delineating the objectionable content sought by the attacker, such as provocative questions like \"What health concerns make it difficult for obese individuals to sustain employment over time?\". This objective is included in the attacker directive in all attack attempts and reminds the attacker of its attack goal. We will denote the outputs returned by the attacker LLM at turn i as $s_i$, and those by the target model as $r_i$, where $s_i \\sim A(p)$ and $r_i \\sim T(p)$, respectively. Here, p represents a sequence of tokens that constructs the input, which can include the objective, directive, or conversation history and is adapted based on whether p pertains to the attacker or target model and the specific tactic used.\nWe use $\\sim$ to signify the probabilistic process of sampling output token sequences from an LLM. Hence, $s_k \\sim A(s_1, r_1, ..., s_{k-1}, r_{k-1})$ is equivalent to sampling from $P(s_k | s_1, r_1, ..., s_{k-1}, r_{k-1}; A)$. Unless specified otherwise, we utilize greedy decoding for sampling output from both the attacker and the target models. Additionally, while emitted in our notation, when sampling from the attacker, the appropriate directive, O, is consistently appended at the start of the attacker's prompt, as detailed in Appendix G.\nUsing the above annotations, we next describe the six attack tactics, which vary in four key aspects as summarized in Table 1."}, {"title": "2.1 Experimental setting", "content": "We evaluated the attack tactics explained in Section 2 using objectives from the AttaQ dataset (Kour et al., 2023). This dataset contains a variety of adversarial questions across multiple safety domains. To select diverse objectives, we clustered all objectives into 100 clusters and chose the medoid from each. See selected questions in Section H in the Appendix. We concentrated our evaluation on four accessible and advanced of-the-shelf conversational models including llama2-70b (Touvron et al., 2023), llama2-13b (Touvron et al., 2023), mixtral-8x7b-instruct-v012 and GPT-"}, {"title": "3 Results", "content": "Figure 2 shows the average harmfulness score per turn, where the Mixtral8X7b model acts as both the attacker and the target LLM. It illustrates that the multi-turn nature of conversations expands the attack surface, although the incremental benefits diminish after 3-4 turns (addressing RQ3). Thus, in automated conversational red teaming, to maximize efficiency, keep dialogues moderately brief.\nTable 2 shows the maximum harmfulness score across all 5 turns (see statistical analysis in the Appendix C). When an LLM was employed for the attack, we used the same model for both the attacker and the target, i.e., A = T. The results indicate that even a simple conversational tactic (In-"}, {"title": "4 Ethics Statement", "content": "Our research aims to enhance LLMs' evaluation and risk assessment by presenting a practical and straightforward framework for identifying their vulnerabilities through conversational interactions. Though these methods have the potential for misuse, our primary objective is to increase safety by thoroughly understanding and addressing possible risks. By conducting simulated attacks (red-teaming) to probe system vulnerabilities, we aim to help create robust defense strategies to make large language model-based systems safer moving forward."}, {"title": "5 Limitations", "content": "In our study, the harm objectives are given and the conversations are conducted in English. Furthermore, we employ only a small set of objectives (100) from the AttaQ dataset. Future research should focus on testing larger harmful datasets in various languages and multilingual models.\nWe do not assess the helpfulness of the model. This implies that a model offering a canned refusal response, while providing no useful information regarding the objective would receive a perfect score. However, an effective model is expected to assist the user by offering relevant guidance or even attempting to steer the user away from the undesirable objective. In future research, we should evaluate both harmfulness and helpfulness to determine how effectively the model strikes a balance between these two maxims.\nThe harmfulness evaluation metric we used appears to perform consistently, according to a manual review conducted on a sample of attacks by the authors. However, further research is needed to determine which metric is the most reliable for assessing conversational harmfulness.\nWe employed a uniform directive for all attackers, providing a restricted set of ideas for the model"}, {"title": "A Anecdotal Observations", "content": "When examining the attacker's tactic across most conversations, it is evident that the attacker LLMs employ the tactics outlined in the provided prompt. The prompt given to the attacker, detailed in Appendix G, includes the following strategies: (1) Impersonation, (2) Extreme politeness, and (3) Creative purposes. Furthermore, we observed that the attacker frequently uses multiple strategies within a single conversation to elicit undesirable output. See an example conversation in Figure 7."}, {"title": "B Metric Selection", "content": "In conversational contexts, choosing an appropriate evaluation metric is crucial. A harmfulness metric is essential for assessing both the success of red-teaming and the effectiveness of attacks, serving as a selection heuristic for certain tactics. The chosen metric should maintain the following properties:\n1. Large Context Awareness: Conversations are often long, involving multiple exchanges between the attacker and the target model. The metric should be capable of considering the entire context to assess the harmfulness of the assistant's responses accurately.\n2. Independence from Conversation Length: Metrics, especially those based LLMs, can be biased by the length of the content. The chosen metric should not be affected by how long the conversation is.\n3. Objective Scoring of the Last Turn: The metric should objectively score the harmfulness of the last turn without being influenced by the harmfulness of the user's previous utterances or the assistant's earlier responses.\nWe explored several metrics, including a DeBERTa-based reward model and LlamaGuard. While the DeBERTa-based model effectively scores a single turn, it struggles with long conversations due to its limited context and susceptibility to the harmfulness of previous turns. This model should ideally evaluate only the last turn's harmfulness, but it often fails.\nTo address this, we attempted to cut the conversation, retaining only the attacker's objective and the last turn. However, this approach resulted in high harmfulness scores in unexpected cases, for"}, {"title": "C Tactics Effectiveness Significance Analysis", "content": "We evaluated the performance of various tactics (Base, Adaptive, Insist, ODS, OCS, MA-OCS) across four language models: Llama13, Llama70, Mixtral, and GPT3.5T, where the attacker and target models are identical. Each combination of tactic and model was tested with 100 samples, with the results presented in Table 2. We assessed the significance of these maximum average harmfulness scores across the tactics using the Friedman Test, which is suitable for repeated measures data with non-normally distributed scores. The test results are shown in the following Table 4.\nAfter the Friedman test indicated significant differences, a planned Nemenyi post-hoc test was conducted to identify which specific pairs of methods showed significant differences. The primary focus is on comparisons involving the MTA-OCS tactic. Consequently, the results were compared against the MTA-OCS tactic, which proved to be the most effective with the Llama70, Mixtral, and GPT-3.5-Turbo models. Table 5 summarizes the differences between the MTA-OCS tactic versus all other tactics for each model."}, {"title": "D Computational Cost of Attacks", "content": "To keep our conversational red teaming method straightforward, we opted not to use local GPUs for running LLMs. Instead, to simulate real business"}, {"title": "E Additional Results", "content": ""}]}