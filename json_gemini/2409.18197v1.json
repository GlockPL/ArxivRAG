{"title": "Autonomous Network Defence using Reinforcement Learning", "authors": ["Myles Foley", "Kate Highnam", "Chris Hicks", "Vasilios Mavroudis"], "abstract": "In the network security arms race, the defender is significantly disadvantaged as they need to successfully detect and counter every malicious attack. In contrast, the attacker needs to succeed only once. To level the playing field, we investigate the effectiveness of autonomous agents in a realistic network defence scenario. We first outline the problem, provide the background on reinforcement learning and detail our proposed agent design. Using a network environment simulation, with 13 hosts spanning 3 subnets, we train a novel reinforcement learning agent and show that it can reliably defend continual attacks by two advanced persistent threat (APT) red agents: one with complete knowledge of the network layout and another which must discover resources through exploration but is more general.", "sections": [{"title": "INTRODUCTION", "content": "Securing a computer network and its subsystems against attackers is a complex task that requires both the right combination of tools and expert knowledge [12]. At present, this task is usually still handled by human operators.\nHowever, human involvement increases the operational costs and the response times, leaving these systems at risk. For instance, in 2020, attackers were estimated to spend a median of 24 days undetected inside a defensive environment [7]. During this time the attacker can further infiltrate, compromise, exfiltrate and perform other malicious activities within the network. Compared with an adversary, a defender typically faces increased complexity as they need to remove the threat whilst minimising operational disruption.\nTo advance autonomous defence, we investigate the application of reinforcement learning (RL). In recent years, RL has excelled in game-playing scenarios and has even exceeded human level ability (e.g., ATARI arcade games [11], Dota 2 [3]). Despite its reach potential, only a few works have sought to apply RL in the context of network security [4, 6, 9].\nIn this paper, we present a hierarchical RL agent for autonomous network defence. We demonstrate its success in winning the first CAGE environment challenge [1, 13]. The challenge uses the Cy-bORG environment [14] to simulate a live computer network with several hosts and critical servers, as seen in Figure 1. This envi-ronment utilises a high-fidelity emulator, realised using virtual machines running on Amazon Web Services (AWS), to ensure that all available actions and states are realistic [14]."}, {"title": "THE CYBORG ENVIRONMENT", "content": "To showcase our model on the problem of network defence we leverage the recently proposed CybORG environment [14] as spec-ified in the CAGE Challenge [13]. The same network structure is used as the agent's environment and is seen in Figure 1. This net-work delivers a contained, yet realistic setup, that can be emulated using Amazon Web Services (AWS) or simulated using Python. The network consists of nine users and four servers that are present across three subnets.\nThe game is turn-based between the attacker and defender. Each agent, which has limited visibility of the network state, agent chooses an action for each time-step. Based on the selected ac-tions, the environment samples from a probability distribution (e.g. a valid node restoration may occasionally fail) to update its state and returns a reward to the agents. To prevent trivial solutions, the adversaries are given an initial foothold on a predetermined device"}, {"title": "HIERARCHICAL RL LEARNING MODEL", "content": "RL is a form of machine learning that optimises a given reward, r. RL problems are formulated around an agent that exists in an envi-ronment, this agent takes an action, a, which changes the agents state, s, in the environment. The agent learns a policy that in-creases the long-term reward, accounted for by a discount factor $\\gamma$, the optimal policy is one that maximises the reward, thus solving the task [15]."}, {"title": "Proximal Policy Optimisation", "content": "Proximal Policy Optimisation (PPO) is an efficient policy gradient method for RL [11]. PPO can achieve human and super-human performance in a range of complex environments including 49 separate ATARI arcade games [11] and Dota 2 [3].\nTaking a policy $\\pi$ as $\\pi_{\\theta}$ (where $\\theta \\in \\mathbb{R}$) define an objective func-tion based on the expected total reward obtained from the envi-ronment: $J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [\\sum_{t=0} \\gamma^t r_t]$. This can then be solved using an actor-critic architecture using a critic to measure how good an action is and an actor to select actions. This policy gradient is then: $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [\\nabla_{\\theta} \\log \\pi_{\\theta}(s, a) A_{\\pi_{\\theta}}(s)]$.\nWhere $A_{\\pi_{\\theta}}(s) = Q_{\\pi_{\\theta}}(s,a) - V_{\\pi_{\\theta}}(s)$ is the advantage function to describe the advantage of action a over the average action given by policy $\\pi_{\\theta}$ [15]. A deep neural-network can then approximate the state-value function $V_{\\pi_{\\theta}}(s)$ and another can model the policy $\\pi_{\\theta}$. PPO also introduces a clipping function in gradient descent."}, {"title": "Curiosity", "content": "Curiosity promotes exploration in an environment via an intrinsic reward. Both sub agents (as defined in Section 3.3) use the Intrinsic Curiosity Module (ICM) proposed by [10], this incentivises agents to take actions where there is uncertainty in outcome, thus pro-moting the exploration to unknown states. ICM also reduces noise in this process by using only the relevant information in the state space. Again, we use the implementation in RLLib [8] modifying parameters after a grid-search. The effect of curiosity on these sub agents is illustrated clearly in the b-line-defence agent, where cu-riosity improves the reward by nearly double. ICM particularly overcomes the problem of developing strategies in the presence of randomness in the adversary and the possibility of actions being unsuccessful."}, {"title": "Hierarchy", "content": "To solve the problem as described in Section 2 we develop a hier-archical model that can defend against two different adversaries, the B_line and the Meander agents. In doing so we demonstrate a state-of-the-art solution to the network defence as in the CAGE Challenge.\nFor each of the sub agents in this hierarchy we train a PPO Agent with curiosity against a single adversary. We name these the b-line-defence (trained to defend against the B_line adversary), and the meander-defence (trained against the Meander adversary). Training occurs over episodes of 100 timesteps; the network and agents are then reset to their initial state. This continues until the reward of the defensive agent has converged.\nThe choice of PPO with curiosity was motivated by the im-proved performance in training as compared to APEX DQN [5], IMPALA [2], PPO [11], and PPO with curiosity [10]. This improved performance is seen as the ability to maximise the reward against the adversary, and thus minimising the effect of the adversary on the network.\nWe then train an RL agent 'controller', this deals with the high-level problem of identifying the adversary currently attacking the network. In training the controller defends each episode against a random adversary. The controller then selects one of the pre-trained sub-agents at each time-step to execute a low-level action. In this way the controller should identify the adversary and choose the agent trained to deal with the threat and mitigate it."}, {"title": "RESULTS", "content": "Here we present the results of our CAGE Challenge winning ap-proach to autonomous network defence."}, {"title": "Challenge Evaluation", "content": "CAGE Challenge submissions were evaluated by testing the pro-posed defensive 'blue' agent against three unique attacking 'red' agents. The B_lineAgent, RedMeanderAgent, and SleepAgent are"}, {"title": "Extended Evaluation", "content": "To help motivate, understand, and measure the performance of our hierarchical RL agent we perform an extended evaluation. Ta-ble 2 shows that whilst our B-line-defence and Meander-defence agents perform well against the B_lineAgent and RedMeanderA-gent, respectively; the policies fail to generalise and the average performance of these agents in both tasks is unsatisfactory. Our Hi-erarchical agent performs very well at choosing the best specialised sub-agent for each observation. This is evidenced in comparison with a privileged Hierarchical-perfect agent and a Hierarchical-chance agent that chooses a blue agent uniformly at random. The Hierarchical -perfect agent has perfect knowledge of the current red agent adversary and always chooses the corresponding de-fensive blue agent. Our Hierarchical agent does far better than chance and scores higher than the sum of the B-line-defence and Meander-defence agents."}, {"title": "DISCUSSION", "content": "The results show our hierarchical approach to autonomous network defence, which is state-of-the-art in the CAGE environment, out-performs the defensive capabilities of any sub-agent trained against a single red agent. We suggest that neither the B-line-defence or Meander-defence agent can fully generalise from training against a single adversary and that our hierarchical architecture provides a general mechanism to combine specialised sub-agents into a more widely applicable defensive capability. The theory that avoid-ing overfitting is critical to high-performance autonomous net-work defence is also supported by the relative performance of the B_lineAgent and RedMeanderAgent shown in Table 2. The B_lineAgent, which uses specialised knowledge of the network structure to execute a more efficient but less general attack, trains a worse-performing B-line-defence agent; in the average case, than the less specialised RedMeanderAgent. Additionally, our experi-ments with curiosity, a technique which encourages greater gen-eralisation [10], substantially improves the performance of the B-line-defence agent even against the B_lineAgent itself."}, {"title": "CONCLUSION", "content": "This paper investigates the application of intelligent agents to au-tonomously defend a computer network. Utilising the recently pro-posed CAGE Challenge scenario, and CybORG (an autonomous net-work defence environment), we develop a hierarchical RL agent that can defend against multiple APT adversaries over varying lengths of time and overcomes the performance limitations of training against a single adversary. Surprisingly, we show that our hierarchy of specialised agents outperforms any of its individual components and provides for a more generalised defensive capability. We pro-vide an open-source implementation of our state-of-the-art solution which we hope will help to fill the gap in autonomous network defence research. Promising directions for future research include adaptive fine-tuning of specialised agents by the controller, encour-aging even greater generalisation, and formalising the principles which can ensure and quantify it, and multi-agent team defensive capabilities."}]}