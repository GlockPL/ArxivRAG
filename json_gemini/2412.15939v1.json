{"title": "Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation", "authors": ["Gautier Evennou", "Antoine Chaffin", "Vivien Chappelier", "Ewa Kijak"], "abstract": "The rise of the generative models quality during the past years enabled the generation of edited variations of images at an important scale. To counter the harmful effects of such technology, the Image Difference Captioning (IDC) task aims to describe the differences between two images. While this task is successfully handled for simple 3D rendered images, it struggles on real-world images. The reason is twofold: the training data-scarcity, and the difficulty to capture fine-grained differences between complex images. To address those issues, we propose in this paper a simple yet effective framework to both adapt existing image captioning models to the IDC task and augment IDC datasets. We introduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational cost, and show it outperforms two-streams approaches by a significant margin on real-world IDC datasets. We also propose to use synthetic augmentation to improve the performance of IDC models in an agnostic fashion. We show that our synthetic augmentation strategy provides high quality data, leading to a challenging new dataset well-suited for IDC named Syned\u00b9.", "sections": [{"title": "1. Introduction", "content": "Misinformation, frequently propagated through manipulated or out-of-context images, poses a significant challenge. Image Difference Captioning (IDC) offers a solution by generating textual descriptions that enable humans to readily ascertain whether an image has undergone semantic alterations. This paper delves into IDC [29], a recent approach that goes beyond traditional image analysis by generating detailed textual descriptions of the differences between two images. IDC finds its application across various fields, from detecting subtle changes in satellite imagery [3, 12, 21, 31] to identifying anomalies in medical imagery [22] or misinformation explanation.\nAlthough single-image captioning presents considerable challenges on its own, IDC introduces further complexities, as it involves describing the subtle differences present in a pair of similar images. Ideally, captions should disregard common objects between the images and instead emphasize the nuanced changes between them. The advancement of IDC in recent years relies significantly on the emergence of vision-language models and cross-domain learning techniques.\nThe second challenge of IDC is the availability of sufficiently large and diverse dataset for the task. Creating a high-quality IDC dataset is particularly difficult and resource-intensive, as it requires image pairs with detailed descriptions of their differences, encompassing various types of changes. This process can rely on expensive and time-consuming crowd-sourced labor [53], or alternatively, on the use of 3D rendered scenes [29] or on capturing temporal variations [41]. Existing IDC training/evaluation pipeline suffer from shortcomings such as suboptimal metrics, inconsistencies in ground truths, small scale of real-world datasets, or the missing categorization of modifications.\nIn this paper, we propose an innovative use of synthetic data and advanced multimodal architectures to adress IDC limitations in terms of both data and model. We propose the application of generative models [2, 34] to produce synthetic data, providing IDC models with diverse and complex image pairs. Concerning the advancements in multimodal models, we discuss how the pre-training of multimodal models like CLIP [6] and BLIP2 [16] is instrumental for IDC tasks. We explore the potential of BLIP2 in IDC, offering a more 'out-of-the-box' solution contrasting sharply with the previous models reliance on complex, multi-step training and separate processes for image encoding. Despite its larger size and the associated costs for finetuning, we demonstrate the feasibility and effectiveness of LoRa [10] to adapt BLIP2 to IDC, capitalizing on its extensive model capacity while maintaining a low adaptation cost. The main contributions are :\n1. We provide a framework for synthetic augmentation to design well-suited IDC datasets based on diffusion models. We release Syned, our synthetic augmented version of Emu Edit [37], an image editing dataset, to provide a challenging benchmark for the IDC task. We demonstrate the significant improvement brought by this targeted synthetic augmentation.\n2. We propose BLIP2IDC, a new state-of-the-art IDC model based on BLIP2. Extensive experiments conducted on synthetic and real benchmark datasets demonstrate the strong performances of BLIP2IDC in real-world scenarii, and good generalization to unseen content (Fig. 1).\n3. We present a comprehensive evaluation of several leading models on a new real IDC dataset based on Emu Edit outputs, contributing to a clearer understanding of the current model capabilities in IDC."}, {"title": "2. Related Work", "content": "Image Captioning and Multimodal Models. Image difference captioning is closely related to Image Captioning (IC) and Visual Question Answering (VQA). Image Captioning [16, 17, 48] aims to describe the content of an image with fine-grained captions. IC models are trained on web-scale datasets [8, 20, 52] to harness as much visual knowledge as possible. IC models attempt to connect text and images with various solutions: syntactic trees from image features [26], RNN [7] decoding of CNN features with [50] or without attention [1, 47], pre-training task using object-based anchors [18], Seq2Seq learning with unified vocabulary for all the linguistic and visual tokens [48] or leveraging additional embeddings to make a bridge between vision tokens and text tokens [16]. Multimodal models specialized in VQA use cross-modal attention to tune the visual features extraction to the text prompt, enabling interaction and focusing the model on information relevant to the question. While highly efficient [23, 49], those models are prone to inconsistency due to their prompt dependency [54].\nImage Difference Captioning (IDC). IDC focuses on telling apart the discrepancies between two near-identic images. The differences changing the meaning of the image (semantic changes) are the ones the model should focus upon. On the contrary, editiorial changes (non-semantic) such as compression or rescale should be ignored. Prior work struggles with two main hardships: how to represent the difference features and how to gather this specific kind of data. Deep learning based architectures, such as CNN, CLIP-based encoding and RNN are widely used in this topic to learn the features [3, 9, 12, 21, 31, 38, 51]. Previous work extract features for each image independently, thus discarding the images correlation in the pixel space. After this extraction step, they fuse embeddings using concatenation [29, 31, 44, 51] before feeding them to whether a difference encoder for change representation, or directly to a decoder to generate the text descriptions. The difference encoder and decoder are transformer-based [46] or RNN-based [31]. Another axis of work is to define pretraining tasks to find a suitable representation space for the differences [44]. Recently, the use of a multimodal model such as CLIP [32] enables for a better representation, thus achieving state-of-the-art performances. Yet, current adaptation to IDC [6] struggles with exploiting all its pretraining. Moreover, because high-quality data is hard to find, most methods are trained and evaluated on 3D rendered datasets, that do not accurately represent real-world performance.\nImage Editing. The recent progress in generative models [5, 34] enables realistic generation of images. This further propell work [2, 25, 30] on image editing, where semantic modifications are performed to an image using a textual prompt, while keeping the main subject of an image. Recent works [37, 53] focused on using already existing real-world text-image datasets such as MSCOCO [20] to perform editions on those images, by building on modifications of the associated caption. The main hurdle in these methods is the fully supervised generation and verification process, where each generation is reviewed by operators and several level of filters are used to keep the best generations. While this process produces a high quality dataset, it is time-consuming and costly. We show in this paper that such setup nevertheless allows the automatic creation of IDC datasets: a couple is created using the original image and its modified version, and the prompt used to perform the edit is used as the target modification description."}, {"title": "3. BLIP2IDC", "content": "BLIP2 [16] is a multimodal model that introduces the Querying Transformer (QFormer) as the main block to connect text and image representations. It relies on a two-stage pretraining: a vision-language representation learning stage with a frozen image encoder, and a vision-to-language generative learning stage with a frozen LLM. The QFormer is composed of two transformers modules. The first one attends to the image through cross attention between learned embeddings, called queries, and vision encoder outputs. The second module interacts with the ground truth text and with the queries embeddings to ensure vision-language alignment. BLIP2 is pre-trained to jointly optimize three pre-training objectives that are Image-Text Contrastive Learning, Image-Text Matching, and Image-grounded Text Generation. As a contribution we show that, taking advantage of its pretraining for Image Captioning, BLIP2 can be adapted to IDC at a low computational cost and without changing its architecture."}, {"title": "3.1. Adaptation", "content": "We argue that the classic IDC two-streams encoding scheme featured in Fig. 2 results in a suboptimal comparison of images at the features level. For standard image captioning, BLIP2 takes one image as input and uses a ViT model [4, 46] as a frozen image encoder. In the context of IDC, feeding BLIP2 jointly with the two images to be compared enables the attention layers of the visual encoder and QFormer to focus early on the differences between the 2 images, and to encode the two images in relation to each other rather than separately. Our BLIP2 adaptation pipeline is shown in Fig. 3. We give a single image as input, resulting from the vertical concatenation of two images, allowing the model to attend to the differences early, while avoiding any modification of the architecture of the model. Unlike BLIP2, which only trains the QFormer, the ViT and the LLM should be fine-tuned to compensate for this modification of the input domain.\nAlthough images are stretched by concatenation, it does not undermine the performances. We conjecture this behaviour is due to the way BLIP2 was pretrained, where images are randomly cropped then stretched to a fixed square size, without padding. Thus, the BLIP2 model learned to be robust to various stretching operations."}, {"title": "3.2. Efficient fine-tuning", "content": "Modules fine-tuning. Although BLIP2IDC does not require to change the architecture of the model, the model should be fine-tuned for the IDC task to adapt to the new task and type of data. Unlike the original training of BLIP2 which only fine-tunes the QFormer, all the components, including the ViT, QFormer and image-grounded text decoder, are finetuned to get the best IDC performance (see Fig. 6 in the result section).\nLow Rank Adaptation (LoRA). We use Low Rank Adaptation (LoRA), to reduce training resources while maintaining performance. By fine-tuning just 0.1% of all parameters\u2014specifically, the attention modules' Q, K, V layers\u2014we achieve top performance with minimal resource use. This approach allows BLIP2IDC to adapt to IDC tasks, balancing high performance with resource efficiency."}, {"title": "3.3. Advantages over existing IDC models", "content": "The latest leading models for IDC are CLIP4IDC [6] and SCORER [44]. CLIP4IDC utilizes a two-step training approach, first to adapt image-pairs visual representations for captioning and then to generate captions based on visual differences encoding, through contrastive and cross-entropy losses respectively. SCORER follows the traditional IDC framework as depicted in Fig. 2, enhancing performance with innovative modules for view-shift invariance and caption informativeness, and is presented as showing particular strength on the CLEVR-DC dataset.\nThese models are characterized by complex training procedures with several steps and separate image encoding processes. In contrast, our model BLIP2IDC simply concatenates images for joint encoding employing early attention mechanisms while learning in only one step. This approach proves crucial for IDC, as evidenced by our experimental results in Sec. 5. BLIP2IDC also benefits from knowledge gained through the large-scale unsupervised pre-training of BLIP2 for image captioning, making it compelling given the challenges associated with training an IDC model from scratch on fewer data."}, {"title": "4. Datasets", "content": "The structure of IDC datasets is a triplet $(I_{ref}, I_{modified}, GT)$ which respectively represents the original image $I_{ref}$, the modified image $I_{modified}$, and the ground-truth set of reference descriptions of the differences GT. We compare our approach to recent IDC models on standard IDC datasets: CLEVR-Change [29], CLEVR-DC [13], Spot-The-Diff [31] and Image Editing Request [41]. These datasets are distinguished by various properties, summarized in Tab. 1: the number of image pairs ($I_{ref}, I_{modified}$), the origin of the $I_{ref}$ images, which may be real images or generated by 3D models, the number of reference captions (ground-truths GT) per triplet, the way in which $I_{modified}$ and GT are obtained, the number and type of transformations performed, and whether some manual intervention was required to create the dataset. This may happen at the level of triplet filtering or the creation of ground-truth legends.\nThese properties can limit the creation of these datasets, which can be costly, as well as their usefulness, for example when the diversity of transformation types is reduced. We discuss the limitations of these datasets hereafter, as well as the issues posed by the evaluation of the IDC task. To handle those limitations, we propose a method based on text-guided image editing methods for creating IDC datasets without manual intervention, which can then be adapted to different use cases. Furthermore, instruction-based image editing capabilities are evaluated on some benchmarks (IE datasets), like MagicBrush [53] or Emu Edit test set generations (EE) [37]. Those datasets are also composed of triplets ($I_{ref}, T_{instruction}, I_{edited}$), with $T_{instruction}$ the modification that maps $I_{ref}$ to $I_{edited}$ and $I_{edited}$ the edited image. We thus propose to adapt those datasets for the IDC task."}, {"title": "4.1. Existing datasets", "content": "CLEVR-Change. Introduced by Robust Change Captioning [29], the goal of this dataset is to evaluate primary abilities for visual understanding such as being able to tell the shape, the color, the material and the position of an object and if one or none of those properties changed. This dataset enables to assess if an IDC model understands spatial relationships and if it is robust to non-semantic changes. Five types of scene changes are defined. Editing instruction and ground-thruths captions are automatically constructed following a template. However, this dataset was created with a 3D engine, and all the elements are 3D scenes far from the real world setting, leading to a model that doesn't transfer very well to real-life examples.\nCLEVR-DC. CLEVR-DC [13] is a derivative of CLEVR-Change with extreme shift in viewpoints. It uses the same types of modifications as CLEVR-Change. The goal of this dataset is to evaluate the view-invariancy of the IDC model but it shares the limitations of the original dataset.\nSpot-The-Diff (STD). Unlike CLEVR-Change, STD [12] uses a real world setting for the pairs of images. To cope with the issue of producing modified images, it uses temporal evolution. Thus, a pair of images is made of one image taken at a given time and another of the same place taken at a later time, with a fixed point of view. Most of the images are set in a parking lot, heavily restricting the diversity of ground-truth as most changes in a parking are add/remove people/car. Although heavily biased, this dataset enables one to test IDC models on real-world images.\nImage Editing Request (IER). Born from crawling Reddit and Zhopped, the IER dataset [41] relies on instructions from online users in Photoshop specialized forums. The input image and editing instruction were posted, and people sent their modified images on Reddit. Crawling those results of the manual edition led to a really high quality and diverse dataset but with a really small scale. Each image pair of the test set has three GT captions, written by three different annotators. IER provides a wide range of semantic modifications, as for example \"add a sailor hat to ducks\", \"replace the background by a spaceship\", or \"replaces brooms with lightsabers\".\nMagicBrush. This dataset is the first large-scale, manually-annotated instruction guided image editing dataset [53] covering various global and local editing scenarii. MagicBrush comprises 10K ($I_{ref}, T_{instruction}, I_{edited}$) triplets, which is sufficient to fine-tune large-scale image editing models with the use of LoRA. For this dataset, modifications are not grouped into categories like in CLEVR-Change, making it hard to explain performance and target specific weaknesses.\nEmu Edit (EE). This dataset [37] from Meta is composed of pairs of original images from MSCOCO [20] and modified images generated by the Emu Edit model, a state-of-the-art generation model, which takes an image and an editing instruction as inputs. This dataset also contains fine-grained information about the type of modifications generated, grouped in eight different categories: Add, Text, Color, Background, Local Style, Global and Remove. Paired with the real-world setting of original images which enables for greater usability, these properties make it a great candidate for a challenging IDC benchmark."}, {"title": "4.2. IDC task issues", "content": "IDC models are trained on triplets ($I_{ref}, I_{modified}, GT$) and evaluated on a test set of pairs ($I_{ref}, I_{modified}$) by comparing the generated caption with the reference captions GT, using various automatic evaluation metrics, such as BLEU, ROUGE, METEOR and CIDEr [15, 19, 27, 28]. Some aspects of the real-world datasets weaken the quality of the evaluation.\nSuboptimal metrics. The references-based metrics more accurately evaluate the quality of generated sentences if multiple references are provided. Five references is usually the minimum used. However, none of the avalaible real-world datasets provides a consistent and sufficient number of references, making evaluations less accurate.\nLack of consistency in ground truths. In STD and IER, the humanly-annotated ground-truth captions do not consistently refer to the same difference or set of differences, especially when several modifications are made to the image. Some captions describe a single modification, others several. An IDC model trained to describe only the main difference will struggle with this type of ground truth, and it is the case for all existing IDC models. Either all the differences should be mentionned in each ground-truth, or just one, consistently, to avoid conflicting objectives.\nTiny real-world datasets. As real-world datasets are very time-consuming to make, they are small in size, with at most 10,000 text-image pairs. As a result, they can lead to models specialized in specific subsets of possible modifications, as for STD. On the other side, when modifications are very diverse, as for IER, a type of modification can occur only few times. Metrics then heavily depend on the data splits, according to whether a modification appears only in the training or in the test set. This is all the more possible if the modifications are not grouped into classes allowing stratified sampling.\nModifications categorization. Grouping changes into categories is useful for stratified sampling, but also for analyzing results, as it makes it easier to identify difficult changes and the weaknesses of IDC models."}, {"title": "4.3. Synthetic augmentation", "content": "To cope with the current shortcomings of IDC datasets mentioned above, we propose a pipeline to generate synthetic training samples based on real-world original images. The global pipeline is illustrated in Fig. 4. The idea is to leverage the emerging prompt-based image editing models to produce modified images based on a set of original images and editing instructions belonging to a defined range of modifications. Additional GT captions are generated by a Large Language Model (LLM) as variations of the editing instructions, ensuring consistent GTs.\nSuch a pipeline can be used either to augment an existing IDC dataset, or to create a new dataset whose images and modifications are tailored to a particular downstream task with new specific types of data or modifications.\nTo evaluate this pipeline, we applied it to augment the EE dataset [37], as this dataset provides real-world edited images alongside editing instructions well-suited for an image editing model and with a clear distinction between each class of modifications. As image editing model, we choose the InstructPix2Pix [2] model fine-tuned on the MagicBrush [53] dataset, the state-of-the-art solution for executing real-world edits. The train set is augmented by generating eight new modified images per original ones from the EE dataset and their associated editing instructions, without any manual curation.\nTo ensure meaningful evaluation, we leverage the Llama-2-7b-chat-hf model to generate 4 additional variations of each editing instructions from the EE test set (cf. supplementary). Each of the 2,022 samples in the test set is therefore described by 5 GT captions: the original instruction and the 4 reference captions provided by the LLM. Note that the train set images (see Fig. 5) were not modified by the same editing model as in the test set.\nThe resulting Syned dataset comprises :\n\u2022 a train set of 28,720 pairs of ($I_{ref}, I_{modified}$): 8 variations of each of the 3,590 original images from MSCOCO, generated by InstructPix2Pix. The different modifications are categorized in 8 classes.\n\u2022 a test set of 2,022 samples with 5 references each,"}, {"title": "5. Experiments", "content": "5.1. Evaluation protocol\nModels and datasets. We compare BLIP2IDC to other state-of-the-art IDC models with the results reported in their respective paper on the four standard IDC datasets: CLEVR-Change, CLEVR-DC, STD, and IER. We also introduce the EE dataset to the IDC task, on which we also train and evaluate CLIP4IDC [6] and SCORER [44] for the sake of comparison.\nFinetuning.\nBLIP2IDC is fine-tuned as described in Sec. 3.2, with a LoRA of rank 8. For our own adaptation of BLIP2, we use the decoder-only BLIP2's LLM version, with opt2.7B and vit-base-patch16-224 as the ViT model. We use LORA implementation from the peft [24] python library to train BLIP2 and to store adaptation weights in a lightweight 20 MB file. For CLIP4IDC and SCORER, we either used the hyper-parameters from their respective online repository, or hyperparameters communicated by the authors (see supplementary).\nStandard data augmentation. To increase the robustness of the model, we use Random Gaussian Blur and JPEG Compression as non-disruptive data augmentation scheme. Augmentations should indeed leave the ground-truth semantic content untouched. Transformations based on crop, color jitter, horizontal/vertical flip, brightness or contrast can add meaningful differences and thus change the expected difference caption.\nMetrics. Based on previous work [3, 9, 12, 21, 31, 38, 51], CIDEr (C) is the main metric used to evaluate the generated difference captions. This n-gram-based approach ensures that the evaluated text captures the most relevant aspects of the image as agreed upon by multiple human annotators. In a semantic-dependent context, the CIDEr metric rewards descriptions that accurately reflect the consensus understanding of the image content. BLEU-4 (B), ROUGE-L (R) and METEOR (M) are also used as secondary metrics to assess the sentences quality."}, {"title": "5.2. BLIP2IDC evaluation", "content": "We evaluate the performance of BLIP2IDC against other methods on the four existing IDC datasets. All results except CLIP4IDC on CLEVR-DC and BLIP2IDC are reported from previous work.\nResults on 3D scenes. Results per type of semantic changes on CLEVR-Change are given in Tab. 2. BLIP2IDC ranks first on almost all types of semantic changes, with a 10% increase in the captioning performance on the most difficult 'Move' change, and a significant improvement over previous state-of-the-art method on the CIDEr metric. On CLEVR-DC, that introduces extreme viewpoint changes, we observe that CLIP4IDC achieves the best results, followed by BLIP2IDC (Tab. 3). Both models are better than SCORER although the latter was specially designed to ensure extreme-view-shift invariancy.\nResults on real-world images. According to Table 3, the BLIP2IDC method ranks first on the CIDEr metric by a significant 9.2% margin with respect to state-of-the-art on STD. The IER dataset is made of photoshopped images in a real world setting. Thus, the impressive improvement over previous state-of-the-art is expected since BLIP2 was pretrained on 129M images in a real-world setting [14, 20, 36]. Our results shows that our adaptation outperforms state-of-the-art models, even those based on CLIP which was trained at large scale too. Overall, those results show that efficiently adapting a powerful pretrained model yields state-of-the-art performance, whereas most existing models fall behind due to the lack of data during training.\nAblation study. The performance variations across different BLIP2IDC configurations, depending on the finetuned modules, are depicted in Fig. 6. This analysis highlights the significant impact of fine-tuning both the ViT and LLM modules. Given the mismatch between the ViT and LLM modules for both the input and output domains, the QFormer not surprisingly stands out as the less critical component for fine-tuning, as it already had undergone extensive training during the BLIP2 pretraining phase."}, {"title": "5.3. Automatically generated dataset", "content": "We introduced in Sec. 4.3 Syned, a new dataset designed for the IDC task, built upon the EE dataset. New data are generated by a different instruction-based image editing model, thus broadening the scope and diversity of the new modified images wrt the original EE dataset, as depicted in Fig. 5. We evaluate the usefulness of Syned as data augmentation for different IDC models in Tab. 4. We show an improvement with the proposed synthetic augmentation (EE + Syned) for both CLIP4IDC and BLIP2IDC. Note that the available implementation of SCORER enables to reach at most a CIDEr score of 23.2 which is not significant in this setting and can be achieved only by reward hacking."}, {"title": "5.4. Qualitative analysis", "content": "We present in Fig. 1 a comparative analysis of state-of-the-art IDC models outputs across different scenarii: in-distribution image-pairs from the train and test set of EE, and out-of-distribution images from the web, to assess the zero-shot capabilities of the models.\nWe observe SCORER's limitations in handling real-world images in all cases. It only succeed in identifying a text modification in the train sample, and confuses actions like adding or removing. CLIP4IDC performs well on samples from EE but struggles with zero-shot generalization. Specifically, it misidentifies the \"table tennis set\" due to its absence in the training data, confusing it with a similar but incorrect object. Additionnaly, it fails to understand the context in which the object is added. BLIP2IDC, on the other hand, excels on data in-distribution and demonstrates superior capability in generalization through effective use of cross-attention. It accurately recognizes and describes the entire scenes in Fig. 1(c-d), including complex additions like a table tennis set, by leveraging its pretraining. This allows BLIP2IDC to not only identify the presence of fire in Fig. 1(d) but also specify the object affected, showcasing its ability to connect semantic changes with their context."}, {"title": "5.5. Limitations", "content": "BLIP2 is pre-trained on large image-text datasets sourced from the web, such as LAION [35]. As BLIP2IDC is a finetuned version of BLIP2, it inherits the biases observed on the BLIP2 model due to its pretraining data. Our synthetic augmentation pipeline also relies on several synthetic generation models and thus inherits their limitations, such as ethnics misrepresentation or adversarial vulnerabilities. The LLM used for generating the ground truth variations will hallucinate, even with careful prompting.\nFurthermore, Image Editing models, while improving, are still not able to perform every type of modifications. Careless use of these models can deteriorate the dataset quality if the editing instructions are not well aligned with the model capacity. We finally emphasize the in-domain limitation: IDC datasets can hardly be exhaustive in their ability to handle all types of change, due to the multiplicity of modification interpretations and descriptions. For a given domain, a more suitable approach is to curate a limited number of well defined changes and to build upon them, allowing for better control."}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel framework that adapt existing image captioning model to the IDC task in order to benefit from their extensive pre-training. Image captioning pretraining data are indeed easier to obtain than the one for IDC. This allows to effectively leverage the global world knowledge from large-scale unlabeled dataset and transfer it to IDC using a smaller amount of data. As a demonstration, we adapt BLIP2 to the IDC task. By innovatively encoding image differences at the pixel level rather than relying on the traditional dual-stream scheme, we ensure a more informative and direct approach to understanding image variations. BLIP2IDC allows to outperform existing methods on standard benchmarks. Additionnaly, we introduce a synthetic augmentation strategy that not only adresses critical challenges of data scarcity and the need for robust, real-world applicable architectures but also sets a new benchmark in IDC performance. This paves the way to the application of IDC to more diverse data and types of edits."}]}