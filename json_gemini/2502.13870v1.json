{"title": "SPEX: Scaling Feature Interaction Explanations for LLMs", "authors": ["Justin Singh Kang", "Landon Butler", "Abhineet Agarwal", "Yigit Efe Erginbas", "Ramtin Pedarsani", "Kannan Ramchandran", "Bin Yu"], "abstract": "Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths (\u2248 20). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths (\u2248 1000). SPEX exploits underlying natural sparsity among interactions\u2014common in real-world data\u2014and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-40 mini) and compositional reasoning in vision-language models.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) perform remarkably well across many domains by modeling complex interactions among features\u00b9. Interactions are critical for complex tasks like protein design, drug discovery, or medical diagnosis, which require examining combinations of hundreds of features. As LLMs are increasingly used in such high-stakes applications, they require trustworthy explanations to aid in responsible decision-making. Moreover, LLM explanations enable debugging and can drive development through improved understanding (Zhang et al., 2023a).\nCurrent post-hoc explainability approaches for LLMs fall into two categories: (i) methods like Shapley values (Lundberg & Lee, 2017) and LIME (Ribeiro et al., 2016) compute marginal feature attribution but do not consider interactions. As a running example, consider a sentiment analysis task (see Fig. 1(a)) where the LLM classifies a review containing the sentence \"Her acting never fails to impress\". Marginal attribution methods miss this interaction, and instead attribute positive sentiment to \"never\" and \"fails\" (see Fig. 1(a)). (ii) Interaction indices such as Faith-Shap (Tsai et al., 2023) attribute interactions up to a given order d. That is, for n input features, they compute attributions by considering all O(nd) interactions. This becomes infeasible for small n and d. This motivates the central question of this paper:\nCan we perform interaction attribution at scale for a large input space n with reasonable computational complexity?\nWe answer this question affirmatively with Spectral Explainer (SPEX) by leveraging information-theoretic tools to efficiently identify important interactions at LLM scale. The scale of SPEX is enabled by the observation that LLM outputs are often driven by a small number of sparse interactions between inputs (Tsui & Aghazadeh, 2024; Ren et al., 2024a). See Fig. 1 for examples of sparsity in various tasks. SPEX discovers important interactions by using a sparse Fourier transform to construct a surrogate explanation function. This sparse Fourier transform searches for interactions via a channel decoding algorithm, thereby avoiding the exhaustive search used in existing approaches.\nOur experiments show we can identify a small set of interactions that effectively and concisely reconstruct LLM outputs with n \u2248 1000. This scale is far beyond what current interaction attribution benchmarks consider, e.g., SHAP-IQ (Muschalik et al., 2024) only considers datasets with no more than 20 features. This is summarized in Fig 2; marginal attribution methods scale to large n but ignore crucial interactions. On the other hand, existing interaction indices do not scale with n. SPEX both captures interac."}, {"title": "2. Related Work", "content": "LLMs are capable of generating rationalizations for their outputs, but such rationalizations are another form of model output, susceptible to the same limitations (Sarkar, 2024). In contrast, this work focuses on explanations in the form of feature attributions that are grounded in the model's inputs and outputs, and can be applied to any ML model., i.e., model-agnostic methods. Moreover, model-agnostic methods can be applied to LLMs incapable of explaining their own output such as protein language models as well as encoder-only models (see experiments in Sec. 6)\nModel-Agnostic Feature Attributions LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017), and Banzhaf values (Wang & Jia, 2023) are popular model-agnostic feature attribution approaches. SHAP and Banzhaf use game-theoretic tools for feature attribution, while LIME fits a sparse linear model. Chen et al. (2018) utilize tools from information theory for feature attributions. Other methods (Sundararajan et al., 2017; Binder et al., 2016) instead utilize internal model structure to derive feature attributions.\nInteraction Indices Tsai et al. (2023) and Sundararajan et al. (2020) extend Shapley values to consider interactions."}, {"title": "3. Overview: Fourier Transform Formulation", "content": "We review background on the Fourier transform for SPEX.\nModel Input Let x be the input to the LLM where x consists of n input features, e.g., words. For x = \"Her acting never fails to impress\", n = 6. In Fig. 1(b) and (c), n refers to the number of documents or image patches. For SC [n], we define xs as a masked input where S denotes the coordinates in x we replace with the [MASK] token. For example, if S = {3}, then the masked input xs is \"Her acting [MASK] fails to impress\". Masks can be more generally applied to any input. In Fig. 1(b) and (c), masks are applied over documents and image patches respectively.\nValue Function For input x, let f (xs) \u2208 R be the output of the LLM under masking pattern S. In sentiment analysis, (see Fig. 1(a)) f(xs) is the logit of the positive class. If xs is \"Her acting [MASK] fails to impress\", this maskingpattern changes the score from positive to negative. For text generation tasks, we use the well-established practice of scalarizing generated text using the negative log-perplexity\u00b3 of generating the original output for the unmasked input (Paes et al., 2024; Cohen-Wang et al., 2024). Since we only consider sample-specific explanations for a given x, we suppress dependence on x and write f (xs) as f(S).\nFourier Transform of Value Function Let F2 = {0,1}n, and addition between two elements in F2 as XOR. Since there are 2n possible masks S, we equivalently write f : F2 \u2192 R, where f(S) = f(m) with S = {i : mi = 1}. That is, m\u2208 F2 is a binary vector representing a masking pattern. If mi = 0 we evaluate the model after masking the ith input. The Fourier transform F : F2 \u2192 R of f is:\n$$f(m) = \\sum_{k \\in F_2} (-1)^{(m,k)} F(k).$$\nThe Fourier transform is an orthonormal transform onto a parity (XOR) function basis (O'Donnell, 2014).\nSparsity f is sparse if F(k) \u2248 0 for most of the k \u2208 F2. Moreover, we call f low degree, if large F(k) have small |k|. Ren et al. (2024b); Kang et al. (2024); Valle-Perez et al. (2018); Yang & Salman (2019) and experiments in Appendix B establish that deep-learning based value functions f are sparse and low degree. See Fig. 1 for examples."}, {"title": "4. Problem Statement", "content": "Our goal is to compute an approximate surrogate f. SPEX finds a small set of k with |k| < n denoted K, and F(k) for each k \u2208 K such that\n$$f(m) = \\sum_{k \\in K}(-1)^{(m,k)} f(k).$$ This goal is motivated by the Fourier sparsity that commonly occurs in real-world data and models. Some current interac."}, {"title": "5. SPEX: Algorithm Overview", "content": "We now provide a brief overview of SPEX (see Fig. 3). A complete overview is provided in Appendix A. The high-level description consists of three parts:\nStep 1: Determine a minimal set of masking patterns m to use for model inference, and query f (m) for each m.\nStep 2: Efficiently learn the surrogate function f from the set of collected samples f (m).\nStep 3: Use f and its transform \u00ca to identify important interactions for attribution."}, {"title": "5.1. Masking Pattern Design: Exploiting Structure", "content": "We first highlight two important properties of Fourier transform related to masking design structure.\nAliasing (Coefficient Collapse) Property: For b \u2264 n and M\u2208 F2\u00d7n, let u : F2 \u2192 R denote a subsampled version of f. Then u has Fourier transform U:\n$$u(l) = f(Ml) \u2192 U(j) = \\sum_{Mk=j} F(k).$$Shift Property: For any function f: F2 \u2192 R, if we shift the input by some vector p \u2208 F2, the Fourier transform changes as follows:\n$$f_p(m) = f(m+p) \u2194 F_p(k) = (-1)^{(p,k)} F(k).$$"}, {"title": "5.2. Computing the Surrogate Function", "content": "Once we have the samples, we use an iterative message passing algorithm to estimate F(k) for a small (a-priori unknown) set of k \u2208 K.\nBipartite Graph We construct a bipartite graph depicted in Fig. 5. The observations Uc(j) = (Uc,o(j), . . ., Uc,p(j)) are factor nodes, while the values F(k) correspond to variable nodes. F(k) is connected to Uc(j) if Mck = j.\nMessage Passing The messages from factor to variable are computed by attempting to decode a singleton via the Berlekamp-Massey algorithm. If a k is successfully decoded, k is added to K and F(k) is estimated and sent to factor node F(k). The variable nodes send back the average of their received messages to all connected factor nodes."}, {"title": "6. Experiments", "content": "Datasets We use three popular datasets that require the LLM to understand interactions between features.\n1. Sentiment is primarily composed of the Large Movie Review Dataset (Maas et al., 2011), which contains both positive and negative IMDb movie reviews. The dataset is augmented with examples from the SST dataset (Socher et al., 2013) to ensure coverage for small n. We treat the words of the reviews as the input features.\n2. HotpotQA (Yang et al., 2018) is a question-answering dataset requiring multi-hop reasoning over multiple Wikipedia articles to answer complex questions. We use the sentences of the articles as the input features.\n3. Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019) is a comprehension benchmark requiring discrete reasoning operations like addition, counting, and sorting over paragraph-level content to answer questions. We use the words of the paragraphs as the input features.\nModels For DROP and HotpotQA, (generative question-answering tasks) we use Llama-3.2-3B-Instruct (Grattafiori et al., 2024) with 8-bit quantization. For Sentiment (classification), we use the encoder-only fine-tuned DistilBERT model (Sanh et al., 2019; Odabasi, 2025).\nBaselines We compare against popular marginal metrics LIME, SHAP, and the Banzhaf value. For interaction indices, we consider Faith-Shapley, Faith-Banzhaf, and the Shapley-Taylor Index. We compute all benchmarks where computationally feasible. That is, we always compute marginal attributions and interaction indices when n is sufficiently small. In figures, we only show the best performing baselines. Results and implementation details for all baselines can be found in Appendix B."}, {"title": "6.1. Metrics", "content": "We compare SPEX to other methods across a variety of well-established metrics to assess performance.\nFaithfulness: To characterize how well the surrogate function f approximates the true function, we define faithfulness (Zhang et al., 2023b):\n$$R^2 = 1 - \\frac{\\|\\hat{f} - f \\|^2}{\\| f - \\bar{f} \\|^2},$$ where $\\|f\\|^2 = \\sum_{m \\in F_2} f(m)^2$ and $\\bar{f} = \\frac{2}{2^n} = \\sum_{m \\in F_2} f(m)$. Faithfulness measures the ability of different explanation methods to predict model output when masking random inputs. We measure faithfulness over 10,000 random test masks per-sample, and report average R2 across samples.\nTop-r Removal: We measure the ability of methods to identify the top r influential features to model output:\n$$Rem(r) = \\frac{|f(1) - f(m^*)|}{|f(1)|}$$\n$$m^* = \\argmax_{\\|m\\|=n-r} |f(1) \u2212 f(m)|.$$\nRecovery Rate@r: Each question in HotpotQA contains human-labeled annotations for the sentences required to correctly answer the question. We measure the ability of interaction indices to recover these human-labeled annotations. Let Sp* \u2286 [n] denote human-annotated sentence indices. Let S\u2081 denote feature indices of the ith most important interaction for a given interaction index. Define the recovery ability at r for each method as follows\n$$Recovery@r = \\frac{1}{r} \\sum_{i=1}^r \\frac{|S_{p^*} \\cap S_i|}{|S_i|}$$\nIntuitively, (9) measures how well interaction indices capture features that align with human-labels."}, {"title": "6.2. Faithfulness and Runtime", "content": "Fig. 4 shows the faithfulness of SPEX compared to other methods. We also plot the runtime of all approaches for the Sentiment dataset for different values of n. All attribution methods are learned over a fixed number of training masks.\nComparison to Interaction Indices SPEX maintains competitive performance with the best-performing interaction indices across datasets. Recall these indices enumerate allpossible interactions, whereas SPEX does not. This difference is reflected in the runtimes of Fig. 4(a). The runtime of other interaction indices explodes as n increases while SPEX does not suffer any increase in runtime.\nComparison to Marginal Attributions For input lengths n too large to run interaction indices, SPEX is significantly more faithful than marginal attribution approaches across all three datasets.\nVarying number of training masks Results in Appendix B show that SPEX continues to out-perform other approaches as we vary the number of training masks.\nSparsity of SPEX Surrogate Function Results in Appendix B, Table 3 show surrogate functions learned by SPEX have Fourier representations where only ~ 10-100 percent of coefficients are non-zero."}, {"title": "6.3. Removal", "content": "Fig. 6 plots the change in model output as we mask the top r features for different regimes of n.\nSmall n SPEX is competitive with other interaction indices for Sentiment, and out-performs them for HotpotQA and DROP. Performance of SPEX in this task is particularly notable since Shapley-based methods are designed to identify a small set of influential features. On the other hand, SPEX does not optimize for this metric, but instead learns the function f(.) over all possible 2n masks.\nLargen SPEX out-performs all marginal approaches, indicating the utility of considering interactions."}, {"title": "6.4. Recovery Rate of Human-Labeled Interactions", "content": "We compare the recovery rate (9) for r = 10 of SPEX against third order Faith-Banzhaf and Faith-Shap interaction indices. We choose third order interaction indices because all examples are answerable with information from at mostthree sentences, i.e., maximum degree d = 3. Recovery rate is measured as we vary the number of training masks.\nResults are shown in Fig. 7a, where SPEX has the highest recovery rate of all interaction indices across all sample sizes. Further, SPEX achieves close to its maximum performance with few samples, other approaches require many more samples to approach the recovery rate of SPEX.\nExample of Learned Interaction by SPEX Fig. 7b displays a long-context example (128 sentences) from HotpotQA whose answer is contained in the three highlighted sentences. SPEX identifies the three human-labeled sentences as the most important third order interaction while ignoring unimportant contextual information. Other third order methods are not computable at this length."}, {"title": "7. Case Studies", "content": "In this section, we apply SPEX to two case studies: debugging incorrect responses and visual question answering. Refer to Appendix B for further details on implementation."}, {"title": "7.1. Debugging Incorrect LLM Responses", "content": "LLMs often struggle to correctly answer modified versions of popular puzzle questions, even when these alterations trivialize the problem (Williams & Huckle, 2024). In this spirit, we consider a variant of the classic trolley problem:\nA runaway trolley is heading away from five people who are tied to the track and cannot move. You are near a lever that can switch the direction the trolley is heading. Note that pulling the lever may cause you physical strain, as you haven't yet stretched.\nTrue or False: You should not pull the lever."}, {"title": "7.2. Visual Question Answering", "content": "VQA involves answering questions based on an image. Petsiuk et al. (2018); Frank et al. (2021); Parcalabescu & Frank (2023) consider model-agnostic methods for attributing the marginal contributions of image regions to the generated response. In many compositional reasoning tasks, interactions are key and marginal attributions are insufficient. We illustrate this using an image of a dog playing with a basketball and prompting the LLAVA-NeXT-Mistral-7B model(Liu et al., 2023) with \"What is shown in this image?\". This yields the response \"A dog playing with a basketball.\".\nIn Fig. 8, SHAP indicates that image patches containing the ball and the dog are important, but does not capture their interactions. Positive interactions obtained via SPEX reveal that the presence of both the dog and the basketball together contributes significantly more to the response than the sum of their individual contributions. This suggests that the model not only recognizes the dog and the basketball as separate objects but also understands their interaction-dog playing with the ball-as crucial for forming the correct response. Negative interactions between different parts of the dog indicate redundancy, implying that the total effect of these regions is less than the sum of their marginal contributions."}, {"title": "8. Conclusion", "content": "Identifying feature interactions is a critical problem in machine learning. We have proposed SPEX, the first interaction based model-agnostic post-hoc explanation algorithm that is able to scale to over 1000 features. SPEX achieves this by making a powerful connection to the field of channel coding. This enables SPEX to avoid the O(nd) complexity that existing feature interaction attribution algorithms suffer from. Our experiments show SPEX is able to significantly outperform other methods across the Sentiment, Drop and"}, {"title": "Impact Statement", "content": "Getting insights into the decisions of deep learning models offers significant advantages, including increased trust in model outputs. By reasoning about the rationale behind a model's decisions with the help of SPEX, we can develop greater confidence in its output, and use it to aid in our own reasoning. When using analysis tools like SPEX, it's crucial to avoid over-interpretation of results."}, {"title": "Limitations", "content": "Sparsity is central to our algorithm, and without an underlying sparse structure, SPEX can fail. Furthermore, even though we make strides in terms of sample efficiency, the number of samples still might remain too high for many applications, particularly when inference cost or time is high. Another consideration is the degree of human understanding we can extract from computed interactions. Manually parsing interactions can be slow, and useful visualizations of interactions vary by modality. Further improvements in visualization and post-processing of interactions are needed."}, {"title": "Future Work", "content": "SPEX works in a non-adaptive fashion, pre-determining the masking patterns m. For greater sample efficiency, adaptive algorithms might be considered, where initial model inferences help determine future masking patterns. In addition, we have focused on model-agnostic explanations, but future work could consider combining this with internal model structure. Finally, interactions are a central aspect of the attention structures in transformers. Studying the connection between SPEX and sparse attention (Chen et al., 2021) is another direction for future research."}, {"title": "A. Algorithm Details", "content": "A.1. Introduction\nThis section provides the algorithmic details behind SPEX. The algorithm is derived from the sparse Fourier (Hadamard) transformation described in Li et al. (2014). Many modifications have been made to improve the algorithm and make it suitable for use in this application. Application of the original algorithm proposed in Li et al. (2014) fails for all problems we consider in this paper. In this work, we focus on applications of SPEX and defer theoretical analysis to future work.\nRelevant Literature on Sparse Transforms This work develops the literature on sparse Fourier transforms. The first of such works are (Hassanieh et al., 2012; Stobbe & Krause, 2012; Pawar & Ramchandran, 2013). The most relevant literature is that of the sparse Boolean Fourier (Hadamard) transform (Li et al., 2014; Amrollahi et al., 2019). Despite the promise of many of these algorithms, their application has remained relatively limited, being used in only a handful of prior applications. Our code base is forked from that of (Erginbas et al., 2023). In this work we introduce a series of major optimizations which specifically target properties of explanation functions. By doing so, our algorithm is made significantly more practical and robust than any prior work.\nImportance of the Fourier Transform The Fourier transform does more than just impart critical algebraic structure. The orthonormality of the Fourier transform means that small noisy variations in f remain small in the Fourier domain. In contrast, AND interactions, which operate under the non-orthogonal M\u00f6bius transform (Kang et al., 2024), can amplify small noisy variations, which limits practicality. Fortunately, this is not problematic, as it is straightforward to generate AND interactions from the surrogate function f. Many popular interaction indices have simple definitions in terms of F. Table 1 highlights some key relationships, and Appendix C provides a comprehensive list."}, {"title": "A.2. Directly Solving the LASSO", "content": "Before we proceed, we remark that in cases where n is not too large, and we expect the degree of nonzero |k| \u2264 d to be reasonably small, enumeration is actually not infeasible. In such cases, we can set up the LASSO problem directly:\n$$\\hat{F} = \\underset{F}{argmin} \\sum_{m} |f(m) - \\sum_{k < d} (-1)^{(m,k)} F(k)|^2 + \\lambda ||F||_1.$$\nNote that this is distinct from the Faith-Banzhaf and Faith-Shapley solution methods because those perform regression over the AND, M\u00f6bius basis. We observe that the formulation above typically outperforms these other approaches in terms of faithfulness, likely due to the properties of the Fourier transform.\nMost popular solvers use coordinate descent to solve (10), but there is a long line of research towards efficiently solving this problem. In our code, we also include an implementation of Approximate Message Passing (AMP) (Maleki, 2010), which can be much faster in many cases. Much like the final phase of SPEX, AMP is a low complexity message passing algorithm where messages are iteratively passed between factor nodes (observations) and variable nodes.\nA more refined version of SPEX, would likely examine the parameters n and the maximum degree d and determine whether or not to directly solve the LASSO, or to apply the full SPEX, as we describe in the following sections."}, {"title": "A.3. Masking Pattern Design and Model Inference", "content": "The first part of the algorithm is to determine which samples we collect. All steps of this part of the algorithm are outlined in Algorithm 1. This is governed by two structures: the random linear codes Mc and the BCH parity matrix P. Random linear codes have been well studied as central objects in error correction and cryptography. They have previously been considered"}, {"title": "5. SPEX: Algorithm Overview", "content": "We now provide a brief overview of SPEX (see Fig. 3). A complete overview is provided in Appendix A. The high-level description consists of three parts:\nStep 1: Determine a minimal set of masking patterns m to use for model inference, and query f (m) for each m.\nStep 2: Efficiently learn the surrogate function f from the set of collected samples f (m).\nStep 3: Use f and its transform \u00ca to identify important interactions for attribution."}, {"title": "5.1. Masking Pattern Design: Exploiting Structure", "content": "We first highlight two important properties of Fourier transform related to masking design structure.\nAliasing (Coefficient Collapse) Property: For b \u2264 n and M\u2208 F2\u00d7n, let u : F2 \u2192 R denote a subsampled version of f. Then u has Fourier transform U:\n$$u(l) = f(Ml) \u2192 U(j) = \\sum_{Mk=j} F(k).$$Shift Property: For any function f: F2 \u2192 R, if we shift the input by some vector p \u2208 F2, the Fourier transform changes as follows:\n$$f_p(m) = f(m+p) \u2194 F_p(k) = (-1)^{(p,k)} F(k).$$"}, {"title": "5.2. Computing the Surrogate Function", "content": "Once we have the samples, we use an iterative message passing algorithm to estimate F(k) for a small (a-priori unknown) set of k \u2208 K.\nBipartite Graph We construct a bipartite graph depicted in Fig. 5. The observations Uc(j) = (Uc,o(j), . . ., Uc,p(j)) are factor nodes, while the values F(k) correspond to variable nodes. F(k) is connected to Uc(j) if Mck = j.\nMessage Passing The messages from factor to variable are computed by attempting to decode a singleton via the Berlekamp-Massey algorithm. If a k is successfully decoded, k is added to K and F(k) is estimated and sent to factor node F(k). The variable nodes send back the average of their received messages to all connected factor nodes."}, {"title": "5. SPEX: Algorithm Overview", "content": "We now provide a brief overview of SPEX (see Fig. 3). A complete overview is provided in Appendix A. The high-level description consists of three parts:\nStep 1: Determine a minimal set of masking patterns m to use for model inference, and query f (m) for each m.\nStep 2: Efficiently learn the surrogate function f from the set of collected samples f (m).\nStep 3: Use f and its transform \u00ca to identify important interactions for attribution."}, {"title": "5.1. Masking Pattern Design: Exploiting Structure", "content": "We first highlight two important properties of Fourier transform related to masking design structure.\nAliasing (Coefficient Collapse) Property: For b \u2264 n and M\u2208 F2\u00d7n, let u : F2 \u2192 R denote a subsampled version of f. Then u has Fourier transform U:\n$$u(l) = f(Ml) \u2192 U(j) = \\sum_{Mk=j} F(k).$$Shift Property: For any function f: F2 \u2192 R, if we shift the input by some vector p \u2208 F2, the Fourier transform changes as follows:\n$$f_p(m) = f(m+p) \u2194 F_p(k) = (-1)^{(p,k)} F(k).$$"}, {"title": "5.2. Computing the Surrogate Function", "content": "Once we have the samples, we use an iterative message passing algorithm to estimate F(k) for a small (a-priori unknown) set of k \u2208 K.\nBipartite Graph We construct a bipartite graph depicted in Fig. 5. The observations Uc(j) = (Uc,o(j), . . ., Uc,p(j)) are factor nodes, while the values F(k) correspond to variable nodes. F(k) is connected to Uc(j) if Mck = j.\nMessage Passing The messages from factor to variable are computed by attempting to decode a singleton via the Berlekamp-Massey algorithm. If a k is successfully decoded, k is added to K and F(k) is estimated and sent to factor node F(k). The variable nodes send back the average of their received messages to all connected factor nodes."}, {"title": "A.4. Message Passing for Fourier Transform Recovery", "content": "Using the samples (17), we aim to recover the largest Fourier coefficients F(k). To recover these samples we apply a message passing algorithm, described in detail in Algorithm 4. The factor nodes are comprised of the C2b vectors Uc(j) \u2200j \u2208 F2b. Each of these factor nodes are connected to all values k that are comprise their sum, i.e., {k | Mck = j}. Since the number of variable nodes is too great, we initialize the value of each variable node, which we call F(k) to zero implicitly. The values F(k) for each variable node indexed by k represent our estimate of the Fourier coefficients."}, {"title": "A.4.1. THE MESSAGE FROM FACTOR TO VARIABLE", "content": "Consider an arbitrary factor node Uc(j) initialized according to (17). We want to understand if there are any large terms F(k) involved in the sum in (17). To do this, we can utilize the signature sequences (\u22121)Pk. If Uc(j) is strongly correlated with the signature sequence of a given k, i.e., if |((\u22121)Pk, Uc(j))| is large, and Mck = j, from the perspective of Uc(j), it is likely that F(k) is large. Searching through all Mck = j, which, for a full rank Mc contains 2n-b different k is intractable, and likely to identify many spurious correlations. Instead, we rely on the structure of the BCH code from which P is derived to solve this problem.\nBCH Hard Decoding The BCH decoding procedure is based on an idea known generally in signal processing as \u201ctreating interference as noise\". For the purpose of explanation, assume that there is some k* with large F(k*), and all other k such that Mck = j correspond to small F(k). For brevity let Ac(j) = {k | Mck = j}. We can write:\n$$U_c(j) = F(k^*)(-1)^{Pk^*} + \\sum_{A_c(j) \\backslash k^*} (-1)^{Pk}F(k)$$\nAfter we normalize with respect to Uc,0(j) this yields:\n$$\\frac{U_c(j)}{U_{c,0}(j)} = \\frac{(-1)^{Pk^*} +  \\frac{\\sum_{A_c(j) \\backslash k^*} (-1)^{Pk}F(k)}{F(k^*) +  \\sum_{A_c(j) \\backslash k^*} F(k)}}{1 + \\frac{\\sum_{A_c(j) \\backslash k^*} F(k)}{F(k^*)}},$$\n$$= A(j)(-1)^{Pk^*} + w(j).$$\nAs we can see, the ratio (20) is a noise-corrupted version of the signature sequence of k*. To estimate Pk we apply a nearest-neighbor estimation rule outlined in Algorithm 2. In words, if the ith coordinate of the vector (20) is closer to -1 we estimate that the corresponding element of Pk to be 1, conversely, if the ith coordinate is closer to 1 we estimate the corresponding entry to be 0. This process effectively converts the multiplicative noise A and additive noise w to a noise vector in F2. We can write this as Pk* + n. According to the Lemma A.1 if the hamming weight n is not too large, we can recover k*."}, {"title": "A.4.2. THE MESSAGE FROM VARIABLE TO FACTOR", "content": "The message from factor to variable is comparatively simple. The variable node takes the average of all the messages it receives, adding the result to its state, and then sends that average back to all connected factor nodes. These factor nodes then subtract this value from their state and then the process repeats."}, {"title": "A.5. Computational Complexity", "content": "Generating masking patterns m Constructing each masking pattern requires n2b for each Mc. The algorithm for computing it efficiently involves a gray iteratively adding to an n bit vector and keeping track of the output in a Gray code. Doing this for all C, and then adding all p additional shifting vectors makes the cost O(Cpn2b).\nTaking FFT For each uc,i we take the Fast Fourier transform in b2b time, with a total of O(Cpb2b). This is dominated by the previous complexity since, b \u2264 n\nMessage passing One round of BCH hard decoding is O(nct + t\u00b2). For soft decoding, this cost is multiplied by 2dchase, which we is a constant. Computing the correlation vector is O(np), dominated by the computation of Pk. In the worst case, we must do this for all C2b vectors Uc(j). We also check that Mck = j before sending the message, which costs O(nb). Thus, processing all the factor nodes costs O(C2b(nct + t\u00b2 + n(p + b))). The number of active (with messages to send) variable nodes is at most C2b, and computing their factors is at most C. Thus, computing factor messages is at most C22b messages. Finally, factor nodes are updated with at most C2b variable messages sending messages to at most C factor nodes each, each with a cost of O(np). Thus, the total cost of processing all variable nodes is O(C22b + C22bnp). The total cost of message is dominated by processing the factors.\nThe total complexity is then O(2b(nct + t\u00b2 + n(p + b)). Note that p = nc n = t log(nc"}]}