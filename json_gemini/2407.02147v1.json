{"title": "LlamAr & Gemmar: Enhancing LLMs Through Arabic Instruction-Tuning", "authors": ["Hasna Chouikhi", "Manel Aloui", "Cyrine Ben Hammou", "Ghaith Chaabane", "Haithem Kchaou", "Chehir Dhaouadi"], "abstract": "Large language models (LLMs) have greatly impacted the Natural Language Processing (NLP) field, particularly for the English language. These models have demonstrated capabilities in understanding and generating human-like text. The success of language models largely depends on the availability of high-quality instruction datasets, which consist of detailed task descriptions and corresponding responses that are essential for training the models to accurately address a variety of prompts. However, the availability and quality of these resources vary by language. While models perform well in English, they often struggle with languages like Arabic, due to the lack of datasets for fine-tuning Arabic-specific tasks. To address this issue, we introduce InstAr-500k, a new Arabic instruction dataset created by generating and collecting content that covers several domains and instruction types. We then assess this dataset by fine-tuning two open-source models, Llama-3-8B-Instruct and Gemma-7B-IT, on several downstream tasks to scale improvements in their functionality. Based on multiple evaluations, our fine-tuned models achieve state-of-the-art performance on several Arabic NLP benchmarks. These outcomes emphasize the effectiveness of our dataset in elevating the capabilities of language models for Arabic. Our instruction dataset bridges the performance gap between English and Arabic language models by providing resources that amplify Arabic NLP development. Building on this foundation, we developed two state-of-the-art models, LlamAr-8B and GemmAr-7B, which are specifically tuned to excel at a wide range of Arabic NLP tasks.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) has significantly developed the field of language technologies Naveed et al. [2023]; Kaddour et al. [2023]. These models exhibit capabilities in natural language understanding and generation. A fundamental aspect of improving these models involves instruction-tuning, where LLMs are trained on input/output pairs to refine their ability to follow specific user instructions.\nInstruction-tuning has been extensively developed for English, contributing significantly to advance- ments in language technologies through improved model performance and understanding. Previous studies have highlighted the effectiveness of this approach in improving models' knowledge and reasoning capabilities, Li et al. [2024]. Despite these achievements, there is a significant disparity"}, {"title": "2 Preliminaries", "content": "Instruction-tuning is a method designed to enhance the capabilities of pre-trained large language models (LLMs) by fine-tuning them with datasets composed of explicit natural language instructions and their corresponding responses Ouyang et al. [2022]; Mishra et al. [2022]. This technique aims to guide LLMs to better understand and respond to a variety of human requests, particularly those that include clear indications of the task to be performed Wei et al. [2022]; Sanh et al. [2022].\nThe practice of instruction-tuning can vary, including supervised learning with demonstrations or reinforcement learning from feedback data. However, supervised learning remains more common due to the scarcity of open resources for reinforcement learning-based approaches Wang et al. [2022]; Zhang et al. [2023a].\nIn recent developments, publicly released foundation models have somewhat alleviated the high costs associated with training strong pre-trained language models. Nevertheless, these models often perform poorly in non-English languages, highlighting the need for more diverse linguistic datasets Touvron et al. [2023a;b]; Jiang et al. [2023]. By using instruction datasets, models can generalize to new scenarios without dedicated retraining, allowing non-experts to interact with them naturally. The goal is to improve the model's capabilities, ensuring it maintains its high performance and delivers accurate responses in new linguistic contexts Gudibande et al. [2023]."}, {"title": "2.1 Instruction-tuning", "content": "Instruction-tuning is a method designed to enhance the capabilities of pre-trained large language models (LLMs) by fine-tuning them with datasets composed of explicit natural language instructions and their corresponding responses Ouyang et al. [2022]; Mishra et al. [2022]. This technique aims to guide LLMs to better understand and respond to a variety of human requests, particularly those that include clear indications of the task to be performed Wei et al. [2022]; Sanh et al. [2022].\nThe practice of instruction-tuning can vary, including supervised learning with demonstrations or reinforcement learning from feedback data. However, supervised learning remains more common due to the scarcity of open resources for reinforcement learning-based approaches Wang et al. [2022]; Zhang et al. [2023a].\nIn recent developments, publicly released foundation models have somewhat alleviated the high costs associated with training strong pre-trained language models. Nevertheless, these models often perform poorly in non-English languages, highlighting the need for more diverse linguistic datasets Touvron et al. [2023a;b]; Jiang et al. [2023]. By using instruction datasets, models can generalize to new scenarios without dedicated retraining, allowing non-experts to interact with them naturally. The goal is to improve the model's capabilities, ensuring it maintains its high performance and delivers accurate responses in new linguistic contexts Gudibande et al. [2023]."}, {"title": "2.2 Instruction-datasets", "content": "The instruction fine-tuning datasets are composed of paired textual data, wherein each pair consists of an \"instruction input\" and a corresponding \"answer output\". The \"instruction input\" denotes the diverse range of requests or prompts issued by humans to the model, spanning multiple task types, including but not limited to classification, summarization, paraphrasing, and others. Conversely, the \"answer output\" represents the model's generated responses that adhere to human expectations and align with the intended outcome of the original instruction.\nThere are generally two methods for creating instruction datasets: human-crafted datasets and LLM-generated datasets. Initially, humans created this type of dataset, but with the development of LLMs, it became possible to generate it using LLMs.\nHuman-Crafted Datasets:\nHuman-crafted datasets are developed by individuals who follow specific rules and requirements to manually organize instructions. The creation process uses the deep intuitive understanding of language and context that human annotators possess, enabling precise interpretation of nuances and subjectivity. This iterative methodology produces high-quality, unique, and contextually rich datasets that enhance the performance of language models across various tasks.\nThese datasets appear in several forms: they may consist of annotated natural language data tailored for instruction output, like Flan Longpre et al. [2023] and P3 Bach et al. [2022], or may be completely new datasets created from scratch, as seen with Aya collection Singh et al. [2024].\nSynthetic Datasets:\nSynthetic data, generated by algorithms rather than collected from real-world events, plays a pivotal role in training machine learning models where actual data may be scarce or sensitive. LLMs like GPT-3.5-Turbo Ye et al. [2023] and GPT-4 OpenAI et al. [2024] are particularly effective in creating high-quality synthetic datasets. These LLMs can simulate realistic and diverse data points by leveraging their deep learning capabilities. For example, datasets like InstructWild Ni et al. [2023] and Self-Instruct Wang et al. [2022] illustrate the application of LLMs in generating textual content that mimics human writing for NLP tasks.\nAdditionally, the ability of LLMs to continuously learn and adapt ensures that the synthetic data remains relevant and reflective of evolving real-world conditions. This process is not only cost- effective but also speeds up the development cycle of machine learning models, making it a valuable tool across various domains and languages."}, {"title": "3 Methodology", "content": "To refine large language models for better performance in Arabic, we relied on a methodology that combines monolingual knowledge distillation Kuulmets et al. [2024] and fine-tuning strategies"}, {"title": "3.1 Overview", "content": "To refine large language models for better performance in Arabic, we relied on a methodology that combines monolingual knowledge distillation Kuulmets et al. [2024] and fine-tuning strategies"}, {"title": "3.2 Training Data", "content": "We offer in this section a detailed explanation of the dataset construction process, emphasizing the used methodologies and providing a quality analysis. The following Table 1 lists datasets used in our study, highlighting their origins and the range of tasks they cover. Each dataset was carefully selected to contribute to the effective training and fine-tuning of our Arabic language models."}, {"title": "3.2.1 Instruction Dataset Construction", "content": "Improvements in the chosen open-source LLMs for Arabic language understanding are centered on the development of an extensive Arabic instruction dataset."}, {"title": "Synthetic data:", "content": "The construction of InstAr-500k dataset was a multi-stage process that used multiple tools and frameworks to ensure the production of high-quality data for fine-tuning our models, as illustrated in the previously mentioned pipeline. We began by using high-quality raw text data from the 101 Billion Arabic Words Dataset Aloui et al. [2024], specifically focusing on the Modern Standard Arabic (MSA) portion of the data, which served as the foundation for generating instruction-response pairs.\nUsing Lang Chain, we split this raw text into manageable segments to ensure coherence, fol- lowed by cleaning to remove unwanted characters, standardize Unicode, and address special characters. We also filtered the data based on token count to meet our fine-tuning specifica- tions. To facilitate the generation of diverse and relevant data, we created seed task system prompts for tasks such as summarization, explanation, extraction, and open question answer- ing (QA).\nFinally, we used the Command R+ model, hosted on self-managed HuggingFace TGI instances, to generate instruction-response pairs from the cleaned text. This model processed the seed prompts and produced a diverse set of instructions and corresponding responses, completing our dataset construction process. Refer to Appendices A, B, and C for more details about the prompts used, contexts, and their outputs for three tasks: Open QA, Extraction, and Explanation."}, {"title": "\u2022 Data Combination:", "content": "After cleaning both the human-crafted and generated datasets, we integrated them into a unified framework, where we further enhanced the dataset's consistency through zero-shot classification. This technique enabled us to classify the instructions by topics, such as politics or sports, and by task types, such as Open QA or Explanation. Subsequent human evaluation ensured that the data remained both consistent and diverse. Relying on feedback from this evaluation, we repeated the classification as necessary to further enrich the dataset's diversity.\nFollowing this, we reformulated the data into a standardized format that includes key elements such as the instruction (the task or query, expressed in Arabic), the expected output (the response to the instruction), the source (origin of the data), the task (specific nature of the task), the topic (broader subject area of the instruction), and system prompts (specific prompts guiding the system in generating responses). This formatting step ensured that all data adhered to a standardized structure (see Figure 2)."}, {"title": "3.2.2 Quality analysis", "content": "The InstAr-500k dataset includes a diverse range of tasks and sources, offering an examination of its scope and content. The high quality of the dataset results from multiple rounds of prompt engineering and detailed human evaluations to ensure clarity, relevance, and accuracy. Additionally, using high-quality raw text as the context in the synthetic data generation process has significantly improved the overall quality of the dataset."}, {"title": "\u2022 Token Length Distributions:", "content": "Figure 3 illustrates the token length distributions for both the Llama and Gemma tokenizers, providing insights into the dataset's characteristics. The distributions highlight the dataset's extensive scope, with 333,886,144 tokens processed in the inputs and 24,139,403 tokens gen- erated, showcasing the dataset's productive output. The logarithmic scaling reveals concen- trated clusters of outputs and instructions within specific length ranges from 0 to 2,000 tokens, emphasizing the tokenizer's impact on segmentation in computational linguistics."}, {"title": "Categorical Variety and Zero-Shot Classification:", "content": "The InstAr-500k dataset contains a wide range of categories with a well-balanced distribu- tion across the following areas: Religion, Sports, Politics, Science & Technology, Economy & Finance, Entertainment, History, Health, Geography, and Travel. We used zero-shot classi- fication to assign labels to instructions. We sourced these candidate labels for the zero-shot algorithm from the insights provided in the study referenced in Adelani et al. [2024]. This approach enabled the model to generalize to new, unseen categories based on its acquired knowledge, thus eliminating the time-consuming and subjective process of manual labelling."}, {"title": "\u2022 Task Variety:", "content": "The InstAr-500k dataset includes a diverse array of tasks: Classification, Open QA, Closed QA, Text Completion, Explanation, Brainstorming, Rewrite, Extraction & Explanation, Gen- eration, Extraction, and Summarization. We applied zero-shot classification to datasets that initially had mixed tasks, such as CIDAR Alyafeai et al. [2024] and the Aya Collection Singh"}, {"title": "3.3 Fine-tuning", "content": "In this subsection, we provide the technique and the parameters employed for Supervised Fine- Tuning (SFT) within the LLaMA Factory framework Zheng et al. [2024]. For this, we used two models: Gemma 7B-IT and Llama3-8B-Instruct. Table 2 details their architectural parameters, including the number of layers, hidden dimensions, and other metrics.\nIn addition to model architecture, the choice of hyperparameters has a prominent role in the fine- tuning process, directly influencing the model's output. During the fine-tuning of Gemma 7B-IT and Llama3 8B-Instruct models, specific hyperparameters were selected to optimize training. Table 3 summarizes the key hyperparameters, highlighting the calibration necessary for the model training.\nTo further enhance the model's performance, we integrated a range of advanced configurations and techniques within our fine-tuning process:"}, {"title": "\u2022 ROPE:", "content": "We used dynamic Rotary Positional Embeddings (RoPE) Su et al. [2023] to improve long context extrapolation and enhance performance on downstream tasks with short context lengths. This dynamic RoPE scaling effectively manages longer sequences, offering superior performance across different context lengths."}, {"title": "\u2022 Flash Attention:", "content": "To further optimize our fine-tuning process, we used Flash Attention Dao et al. [2022] (flash_attn2) as a booster. This improves memory efficiency and computational speed, enabling us to manage larger batches and longer sequences more effectively"}, {"title": "\u2022 Learning Rate Scheduler:", "content": "We implemented a cosine learning rate scheduler. This approach helped us gradually reduce the learning rate over time, ensuring smoother convergence and preventing abrupt changes that could destabilize the training process."}, {"title": "\u2022 Optimizer:", "content": "We employed the AdamW_torch optimizer Loshchilov & Hutter [2019], which combines the Adam optimization algorithm with weight decay correction. This choice helped us to maintain efficient and stable training by preventing overfitting and ensuring better generalization."}, {"title": "\u2022 Precision:", "content": "We used bfloat16 (bf16) precision during training. This allowed us to faster compute and reduce memory usage without significantly sacrificing model accuracy, enhancing overall training efficiency"}, {"title": "LoRA:", "content": "In our fine-tuning process, we used LoRA (Low-Rank Adaptation) Hu et al. [2021] to efficiently adapt models for Arabic language tasks. LoRA helped us reduce the number of trainable parameters, making the fine-tuning process more efficient without compromising performance."}, {"title": "Impact of hyperparameters fine-tuning", "content": "In our efforts to boost the performance of Llama3-8B-Instruct and Gemma-7B-IT, we refined quan- tization techniques by reducing the precision of model parameters. This adjustment enabled the model to prioritize important features during training, resulting in more efficient memory usage and faster training times.\nImplementing a warm-up step was crucial for optimization. We improved model convergence by starting with a lower learning rate and gradually increasing it. This gradual warm-up established the model's foundation, leading to smoother training and enhanced overall performance. Addition- ally, changing the learning rate to smaller values ensured gradual and precise updates to the model's parameters, preventing irregular behavior.\nWe also exposed the model to a larger and more diverse dataset by increasing the number of training examples. This strategy boosted the model's generalization ability and reduced overfitting, making it adaptable to various scenarios. For specific configurations of LoRA, we defined precise rela- tionships between hyperparameters. For example, setting the alpha (a) based on the rank (r)(a = 2 * r; r = 8 and a = 16) helped in the exploration-exploitation trade-off, allowing the model to escape local minima and discover optimal solutions.\nWe adjusted the cutoff value during parameter trimming to allow the model to capture complex patterns effectively. By retaining a larger set of parameters, the model handled fine-tuning tasks better and understood complex information, balancing general and specific pattern recognition.\nBy increasing the frequency of updates, or save steps, from 100 to 1000, we achieved a more con- trolled learning process. This change allowed the model's parameters to be adjusted and evaluated more frequently, enhancing convergence and stability. The synchronized approach of maintaining equal save and eval steps ensured effective use of computational resources and prompt issue identi- fication.\nFinally, Max Gradient Normalization reinforced training stability by normalizing gradients to a maximum norm. This adjustment prevented extreme gradient values, especially when dealing with outliers or noisy data. Our journey of optimizing LLMs has led to significant performance improve- ments."}, {"title": "3.4 Environment", "content": "The dataset production environment included several key components. We used Text Generation Inference (TGI) from Hugging Face for Command R+ inference, deployed to an on-premises Ku- bernetes cluster on a custom-built Nvidia HGX 8xL40S cluster, allowing for efficient management through a Helm chart. The deployment leveraged GPU resources to accelerate the inference process"}, {"title": "3.4.1 Dataset generation environment", "content": "The dataset production environment included several key components. We used Text Generation Inference (TGI) from Hugging Face for Command R+ inference, deployed to an on-premises Ku- bernetes cluster on a custom-built Nvidia HGX 8xL40S cluster, allowing for efficient management through a Helm chart. The deployment leveraged GPU resources to accelerate the inference process"}, {"title": "3.4.2 Fine Tuning environment", "content": "We used the Azure AI platform to create a solid environment for our Natural Language Generation (NLG) model development and testing. We chose the Standard_NC96ads_A100_v4 instance type, equipped with 4 x NVIDIA A100 GPUs. This selection provided us with the necessary computa- tional resources to handle large-scale data processing and model training."}, {"title": "Configuration and Software:", "content": "After setting up the infrastructure, we configured the necessary NVIDIA drivers and CUDA to optimize the GPU performance. We then installed JupyterLab to enable testing for efficient testing and iterative development of our synthetic data generation process. We used the HuggingFace text generation inference Docker image for rapid inference and text generation. This pre-built image provided a ready-to-use environment with all necessary dependencies and libraries, allowing us to focus on model development without spending time on manual setup."}, {"title": "\u2022 Model Evaluation:", "content": "We used the Open Arabic LLM Leaderboard (OALL)Elfilali et al. [2024] to track progress and rank our models' performance. LLMs on the OALL are evaluated with LightEval, a unified framework from the Hugging Face, to test and assess causal language models across multiple evaluation tasks. This includes Arabic translations of benchmarks like MMLU Hendrycks et al. [2020], Exam Hardalov et al. [2020], ARC-Challenge Clark et al. [2018], ARC-Easy Clark et al. [2018], BOOLQ Clark et al. [2019], COPA Roemmele et al. [2011], HellaSwag Zellers et al. [2019], \u039f\u03a1\u0395\u039d\u0392\u039f\u039f\u039a-QA Mihaylov et al. [2018], PIQA Bisk et al. [2020], RACE Lai et al. [2017], SCIQ Welbl et al. [2017], and TOXIGEN Hartvigsen et al. [2022]. The leaderboard also features benchmarks specifically created for Arabic and its cultural context, such as AlGhafa Almazrouei et al. [2023] and ACVA Huang et al. [2023]."}, {"title": "4 Analysis", "content": "Our study applied a set of benchmarks to evaluate the performance of our models across multi- ple domains. These benchmarks were selected to cover a broad range of tasks, ensuring a diverse assessment. Key components of our evaluation included the Arabic MMLU Benchmark, which is the translated version of MMLU (Massive Multi-task Language Understanding) provided by the OALL team. This benchmark is a standardized method for assessing AI performance on a range of tasks, from simple mathematics to complex legal reasoning in Arabic. It consists of 57 tasks across numerous domains, including elementary mathematics, history, computer science, and law, requiring models to demonstrate a broad knowledge base and problem-solving skills.\nAdditionally, we used the ACVA Benchmark (Arabic Cultural and Value Alignment), introduced by AceGPT Huang et al. [2023], as a benchmark for evaluating our model's alignment with Arabic cultural nuances and values. This examination is crucial for understanding the model's adaptation"}, {"title": "4.1 Benchmarks", "content": "Our study applied a set of benchmarks to evaluate the performance of our models across multi- ple domains. These benchmarks were selected to cover a broad range of tasks, ensuring a diverse assessment. Key components of our evaluation included the Arabic MMLU Benchmark, which is the translated version of MMLU (Massive Multi-task Language Understanding) provided by the OALL team. This benchmark is a standardized method for assessing AI performance on a range of tasks, from simple mathematics to complex legal reasoning in Arabic. It consists of 57 tasks across numerous domains, including elementary mathematics, history, computer science, and law, requiring models to demonstrate a broad knowledge base and problem-solving skills.\nAdditionally, we used the ACVA Benchmark (Arabic Cultural and Value Alignment), introduced by AceGPT Huang et al. [2023], as a benchmark for evaluating our model's alignment with Arabic cultural nuances and values. This examination is crucial for understanding the model's adaptation"}, {"title": "4.2 Results", "content": "The results from various models on the Open Arabic LLM Leaderboard (OALL)6 reveal diverse per- formances across multiple Arabic NLU tasks. Our fine-tuned models, LlamAr-8B and GemmAr- 7B, exhibit strong performances with average scores of 62.41% and 60.92%, respectively.\nFigure 5 illustrates the performance of several key benchmarks, including Arabic_MMLU, Al- Ghafa, ACVA, RACE_Ar, and COPA_Ar. These benchmarks are compared against some of the top-performing models on the OALL leaderboard, such as Llama3-8B-Instruct_AI@Meta [2024], openchat-3.5 Wang et al. [2024], Qwen2-7B-Instruct and Gemma-7B Team et al. [2024b]. The re- sults highlight that our models, LlamAr-8B and GemmAr-7B, stand out as the best-performing models, with higher scores for Arabic tasks."}, {"title": "5 Related Work", "content": "Significant efforts have focused on diversifying instruction datasets, primarily focusing on the English language. These datasets can be categorized into two main types: those generated by Large Lan- guage Models (LLMs) and those created by humans using templates. Examples of LLM-generated datasets include Stanford Alpaca Taori et al. [2023], Databricks' Dolly Conover et al. [2023], and SELF-INSTRUCT Wang et al. [2022]. Auto-Instruct Zhang et al. [2023b] aims to improve in- struction quality for LLMs by leveraging their generative abilities to produce multiple instructions, which are then ranked by a scoring model trained on 575 NLP tasks. In contrast, human-crafted instruction datasets use templates to ensure consistency and coverage of various instruction types. Prominent examples include P3 Bach et al. [2022] and NATURAL INSTRUCTIONS Mishra et al. [2022], which focus on natural language processing tasks. Although the majority of research has fo- cused on the English language, there have been significant contributions in Arabic. CIDAR Alyafeai et al. [2024] was the first open Arabic instruction-tuning dataset, and the Aya Collection Singh et al. [2024] offers a dataset in 101 languages, including Arabic.\nFine-tuning large language models presents several challenges, such as potential knowledge erosion, where modifying all parameters can lead to forgetting previously learned tasks Kemker et al. [2018]. To address this issue, Han et al. [2024] introduced Parameter Efficient Fine-Tuning (PEFT), which modifies only a subset of parameters to help retain previously acquired knowledge more effectively"}, {"title": "6 Conclusion", "content": "In our continuous pursuit of improving Arabic NLP, we are delighted to introduce LlamAr-8B and GemmAr-7B, two state-of-the-art fine-tuned versions of Large Language Models specifically developed for Arabic using our crafted Arabic-instructed dataset InstAr-500k. Through evalu- ations, LlamAr-8B and GemmAr-7B showcased their abilities, securing top positions among leading models on the OALL leaderboard. By combining the strength of LLMs with our customized training approach, we aim to remove any barrier that hinders the performance of Arabic language tasks and make LlamAr-8B and GemmAr-7B trusted companions for researchers and Arabic LLM enthusiasts. We are excited about the potential impact of these models and look forward to witnessing the different use cases that will be built on LlamAr-8B and GemmAr-7B."}, {"title": "Limitations", "content": "Despite the promising outcomes we have achieved, several limitations need to be addressed.\nFirstly, hardware constraints limited our ability to experiment with alternative parameter settings, particularly those affecting GPU memory.\nSecondly, although the dataset's diversity represents an improvement over previous versions, it could be further explored in other tasks like Brainstorming and Role Playing.\nThirdly, the dataset currently includes only Modern Standard Arabic (MSA) instructions and lacks dialectal variations. This restricts its applicability to various regions.\nMoreover, the evaluation metrics display a Western-centric bias, with subtopics like US History and European History potentially affecting the relevance of our findings across different contexts.\nAs researchers, we acknowledge the need for continuous expansion and refinement of resources, addressing both technical feasibility and cross-cultural representation. This recognition forms the"}, {"title": "Ethical Considerations", "content": "In this paper, we applied a new finetuning approach for two Large Language Models (LLMs) using InstAr-500k dataset. Our goal was to enhance the performance and adaptability of these models for Arabic speakers while being mindful of ethical implications and striving for responsible practices.\nWe addressed bias and fairness by meticulously curating and auditing the data to ensure cultural sensitivity, diversity, and inclusivity. This helped promote equitable representations and reduce po- tential biases in the models' responses. Additionally, We prioritized user privacy and data protection, as our finetuning methodology did not involve collecting or storing any personally identifiable infor- mation. We constructed the Arabic instruction dataset using synthetic data, anonymized content, or data obtained with informed consent, adhering to secure data handling practices and relevant data protection regulations.\nWe upheld transparency and accountability by disclosing the capabilities and limitations of our fine-tuned models, as well as any potential risks associated with their use. This included clear explanations of the models' evaluation, the Arabic-instructed dataset used, and any known limi- tations or biases specific to the Arabic language or cultural context. By addressing these ethical considerations, we aim to contribute to the responsible development and deployment of LLMs for Arabic chat applications, ensuring the protection of user privacy, the promotion of accurate and reliable information, and the alignment of models with cultural values and norms in Arabic-speaking societies."}, {"title": "A Open QA", "content": ""}, {"title": "B Extraction", "content": ""}, {"title": "C Explanation", "content": ""}]}