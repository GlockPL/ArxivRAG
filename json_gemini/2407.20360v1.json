{"title": "EVALUATING LARGE LANGUAGE MODELS FOR AUTOMATIC ANALYSIS OF TEACHER SIMULATIONS", "authors": ["David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Antonio Moreno-Cediel", "Mariano Albaladejo-Gonz\u00e1lez", "Eva Garcia-Lopez", "Justin Reich", "Erin Barno"], "abstract": "Digital Simulations (DS) provide safe environments where users interact with an agent through conversational prompts, providing engaging learning experiences that can be used to train teacher candidates in realistic classroom scenarios. These simulations usually include open-ended questions, allowing teacher candidates to express their thoughts but complicating an automatic response analysis. To address this issue, we have evaluated Large Language Models (LLMs) to identify characteristics (user behaviors) in the responses of DS for teacher education. We evaluated the performance of DeBERTaV3 and Llama 3, combined with zero-shot, few-shot, and fine-tuning. Our experiments discovered a significant variation in the LLMs' performance depending on the characteristic to identify. Additionally, we noted that DeBERTaV3 significantly reduced its performance when it had to identify new characteristics. In contrast, Llama 3 performed better than DeBERTaV3 in detecting new characteristics and showing more stable performance. Therefore, in DS where teacher educators need to introduce new characteristics because they change depending on the simulation or the educational objectives, it is more recommended to use Llama 3. These results can guide other researchers in introducing LLMs to provide the highly demanded automatic evaluations in DS.", "sections": [{"title": "1 Introduction", "content": "In recent years, we have witnessed significant technological growth that has impacted numerous sectors, including education [1]. One noteworthy example is Digital Simulations (DS), which can be used to improve learning experiences [2]. The quality of education is intrinsically linked to the competence and preparation of those who deliver it, so it is essential to develop simulators to support teacher education [3]. For this reason, Teacher Moments (TM), a web application to provide DS, has been developed for teaching practice. The DS are meticulously scripted simulations where participants interact with an agent through conversational prompts, creating a dynamic and interactive learning experience [4]. Through TM, teacher candidates can practice real classroom scenarios in a controlled and safe environment. TM allows teacher candidates to practice classroom scenarios without real students, which prevents their mistakes from negatively impacting students' education [2]. Since TM is accessible on any computer connected to the Internet, at any time, and from any location, it provides a flexible platform for continuous teaching development. Teacher candidates can repeatedly simulate teaching experiences, which is one of the most effective methods for honing practical teaching skills to manage diverse classroom situations, respond to unexpected student behaviors, and integrate effective teaching strategies [5].\nIt is important to complement educational simulations with an evaluation of the users' responses and provide them with personalized feedback[6, 7]. Without this feedback, teacher candidates could continuously reproduce wrong behaviors, leading to the internalization of bad habits [8]. Therefore, in DS, it is essential to analyze the users' responses and provide them with appropriate feedback [9]. However, manually analyzing the users' responses brings scalability issues and limits the flexibility of the simulators. An alternative is to automatically analyze the users' responses using Artificial Intelligence (AI) [10].\nIn DS, it is common to provide open-ended questions, allowing teacher candidates to express their thoughts and beliefs [4]. However, this method complicates the automatic analysis of responses, since it demands an in-depth understanding to identify specific behaviors or characteristics [11]. From this section onward, we define a characteristic as a behavior that an educator needs to identify in a user's response to provide appropriate feedback. For instance, recognizing unacceptable behaviors such as rudeness or unfairness towards a student. Identifying these educational characteristics is even more complex because the characteristics to be identified can also be open-ended and not predefined. Teacher educators can create different simulations based on the competencies they want to train or evaluate. Teacher candidates complete the simulations, and the feedback they should receive for their responses depends on the circumstances and the educational goals set by teacher educators in the simulation [12]. Consequently, we face a classification problem where the element to classify is variable. Due to the traditional supervised AI model's failure to generalize to new contexts and problems, they are not suitable for identifying variable characteristics in users' responses [13].\nLarge language models (LLMs) have shown significant performance improvements and capabilities across various domains in the last few years [14]. Due to their pre-training, they outperform traditional supervised AI models in natural language processing tasks [15]. As a result, LLMs are the most suitable for automatically analyzing responses in DS. However, it is necessary to evaluate any AI model before integrating it into real-world applications [16]. Consequently, before integrating LLMs in DS and any AI model in education, it is crucial to evaluate their behavior to identify educational characteristics, especially how they perform with unseen characteristics that an educator could introduce. To the best of our knowledge, no authors have previously conducted this analysis that can guide researchers to integrate LLMs to provide personalized feedback in DS.\nTo address this gap, we have evaluated whether the performance of LLMs depends on the educational characteristics to identify. It is important to know whether the LLM's capability to identify specific behaviors in DS varies depending on the behavior to identify. We have also analyzed how zero-shot, few-shot, and fine-tuning impact the model's performance in previously seen characteristics and in new (unseen) characteristics. This second analysis reports the performance of LLMs for analyzing DS responses in the case where we have predefined behaviors and where a teacher educator needs to analyze a new behavior. To develop these analyses, we have employed a dataset with users' responses in DS for teacher education in TM. Expert educators have manually identified and labeled 14 characteristics, with a total of 4,822 response/characteristic pairs. These experiments aim to answer the following Research Questions (RQs) that should be considered when introducing LLMs to provide feedback in DS:\n\u2022 RQ1. Does the effectiveness of LLMs in identifying educational characteristics within DS for teacher education change depending on the characteristic itself?\n\u2022 RQ2. What is the best LLM configuration to integrate into DS for teacher education when the characteristics to identify are predefined?\n\u2022 RQ3. What is the best LLM configuration to integrate into DS for teacher education when the characteristics to identify are open-ended?\nThe rest of the paper is organized as follows. Section 2 provides a background of LLMs in education. Section 3 includes the methodology followed to answer the aforementioned RQs. Section 4 shows the results obtained, and Section 5 contains the discussion about the outcomes. Finally, we present the research conclusions and future work in Section 6."}, {"title": "2 State of the art", "content": "Advancements in LLMs have driven the development of numerous educational technology innovations [17, 18]. These innovations aim to automate time-consuming and laborious tasks of generating and analyzing data, typically text [19]. LLMs are based on the Transformer architecture, which in its original design consisted of an encoder and/or a decoder [20]. Bidirectional Encoder Representations from Transformers (BERT) [21] employs an encoder-only architecture. In contrast, other models use a decoder-only architecture, such as Llama 3 [22]. Nevertheless, both types of LLMs are pre-trained on gigantic text datasets, reducing the data necessary to achieve the targeted task of the model [23]. After this pre-training, there are three types of paradigms to use these models [20]:\n\u2022 Zero-shot. The model does not receive any additional training specific to the task to be achieved. In this case, the model only has the knowledge acquired during the pre-training.\n\u2022 Few-shot. The model receives a few examples of the task to achieve. These additional examples are usually provided to the model through the prompt inserted into the LLMs.\n\u2022 Fine-tuning. The model undergoes further training with a dataset related to the specific task to achieve.\nDue to their huge capabilities, LLMs are being introduced in different applications in educational environments, such as automatic assessment, personalized feedback, student performance prediction, and teaching support [24]. Automatic assessment requires complex AI models when users have open-ended questions and require deep analysis [15]. This is particularly beneficial in large-scale educational settings where manual grading would be impractical [25]. Additionally, LLMs can offer more consistent and unbiased evaluations of students. For instance, Mizumoto and Eguchi [26] employed LLMs for automatic essay assessment. Beyond automatic assessment, LLMs can provide comments and recommendations based on students' responses [27]. In generating feedback, the model has to evaluate the student's work, and then the model provides recommendations to the student. However, Dai et al. [28] identified an irregular performance of GPT-3.5 in zero-shot tasks for assessing student essays. The precision and recall in identifying essays that did not follow the topic established by the educator were zero, which indicates the need to thoroughly evaluate these models before using them in real-world environments.\nThis paper focuses on automatic assessment, primarily identifying characteristics that represent the behaviors that the teacher educators wish to analyze in the teacher candidates' responses. Similar to this approach, Gombert et al. [29] identified energy characteristics in free-text responses of German K-12 students during formative science assessments. The authors evaluated LLMs and traditional supervised AI models to identify energy characteristics. However, their study did not assess the AI models on new characteristics not present during training. As a result, the performance of these AI models on new characteristics that educators could introduce remains unknown. Additionally, our focus differs from that study in that we evaluate the responses of teacher candidates using DS rather than K-12 students.\nThe proposed automatic evaluation can be complemented with expert knowledge to provide feedback to the students. Educators can determine what comments and recommendations to show based on the presence or absence of specific characteristics, similar to a rule-based system [30]. This approach is particularly valuable as it enables teacher educators to provide feedback aligned with the educational objectives and scope of the developed simulation, effectively combining automated assessment with expert insights [9]. For instance, a teacher educator could analyze whether teacher candidates are reprimanding a child in front of their classmates and, in such cases, offer recommendations on why this practice should be avoided and how to handle such situations properly.\nLLMs have also been applied to improve professionals' training and education [31]. For instance, in medical education, they have been used for personalized learning, automatic assessments, writing assistance, and literature reviews [24]. However, as far as we know, only one previous study has focused on using LLMs for DS [12]. This study used a smaller version of our dataset and tested supervised AI models and GPT-3 for characteristic identification. However, the study did not assess whether the model's performance varied across different characteristics. Furthermore, it did not evaluate the models' performance on new, unseen characteristics that an educator might introduce in real-world applications. This previous work also did not fine-tune the LLMs. Thus, there is a need to provide guidelines on these aspects for future researchers before they integrate LLMs into their DS."}, {"title": "3 Material and Methods", "content": "To address the research questions, a dataset including teacher candidates' responses collected from the TM platform was used. These responses were meticulously labeled by experts to identify specific educational characteristics. To evaluate the effectiveness of various LLMs in identifying these characteristics within DS, different configurations were implemented and tested on different subsets of the data. The goal was to assess how well the models could identify certain characteristics in teacher candidates' responses.\nSpecifically, the LLMs Llama 3 [22] and DeBERTaV3 [32] were employed. Llama 3 was used in zero-shot, few-shot, and fine-tuning configurations. Complementing this, DeBERTaV3, an encoder-only architecture known for its efficacy in classification tasks [19, 32], was also evaluated. The assessment focused on the models' ability to identify both characteristics present in the training data and new ones. This comprehensive approach served a dual purpose: to determine the most effective LLM configurations for predefined characteristic identification tasks, and to explore their potential in identifying open-ended characteristics within the teacher candidates' responses.\nIn the following sections, the methodology and experimental setup will be detailed. This includes the process of data collection and labeling, the specific configurations, and the strategic division of the dataset into subsets for training and evaluation purposes. The evaluation metrics used to assess the performance of the models will also be described. Additionally, the results of using Llama 3 in zero-shot, few-shot, and fine-tuning configurations, as well as DeBERTaV3's, will be presented and analyzed."}, {"title": "3.1 Dataset compilation", "content": "The dataset was collected from a simulation known as Jeremy's Journal. In this simulation, participants assume the role of a middle school English teacher who assists a student named Jeremy Green, who is facing personal challenges. This simulation was integrated into an online professional learning course for teacher candidates, conducted in early 2021. The course was attended by 5,458 participants, but the focus was placed on the subset of participants who completed the simulation and consented to participate in the research, totaling 494 participants.\nIn this study, the responses to three specific questions about the case presented in the simulation were analyzed. To evaluate these responses, fourteen distinct characteristics were identified, each represented by a binary label (1 if the characteristic is present, O if it is not). Specifically, six characteristics were assigned to the first question, five to the second one, and three to the third one. The labeling process was conducted by three expert evaluators. To ensure reliability and consistency, 20% of all responses were randomly sampled and evaluated by all three experts, enabling the assessment of inter-rater reliability. The reliability of the labeling process was quantified using Cohen's kappa, with values between 0.57 and 0.61, indicating considerable agreement among raters. These binary labels serve as the ground truth necessary for fine-tuning and evaluating the performance of the language models.\nBefore the data was used for model evaluation, preprocessing was carried out. This included the removal of duplicate responses and the resolution of any encoding errors present in the data. After preprocessing, a total of 1,201 unique responses were obtained. Additionally, to prepare the data for model training and testing, the responses were processed so that each sample in the dataset contained one response paired with one characteristic. This resulted in a total of 4,822 unique response-characteristic pairs. The characteristics along with their descriptions are shown in Table 9.\nIn Figure 1, the distribution of responses for each characteristic is presented, showing both positive and negative labels. Some characteristics have imbalanced cases, such as \u201cnot_well", "rejects_policy\", with 191 negative labels and only 5 positive labels. Conversely, \u201cchange_for\" and \u201cstudent_catch_up\" have more balanced distributions, with a relatively higher number of positive labels. These insights highlight areas where responses from participants varied significantly.\nThis imbalance indicates that some characteristics are far more frequently observed than others. This could be due to the inherent nature of the characteristics or the specific context of the simulation. Additionally, the skewed distributions might reflect varying levels of clarity or specificity in the characteristics themselves. If characteristics are more ambiguous or open to interpretation, this could lead to inconsistent labeling by the expert evaluators. Such inconsistency in labeling could affect the performance of language models, as the models might struggle to learn from and predict these traits accurately. Therefore, clear and precise definitions for each characteristic are crucial to ensure more reliable and accurate model predictions. These observations about the ambiguity or openness of some characteristics will be discussed in later sections.\"\n    },\n    {\n      \"title\": \"3.2 Dataset splitting\",\n      \"content\": \"The dataset was divided into three different splits to facilitate the evaluation of the language models. One split (test set) contains characteristics that are not present in the other two splits. The other two splits (training set and evaluation set) share common characteristics. The process of constructing these subsets involved several steps.\nFrom the full dataset, the samples corresponding to a specific subset of characteristics were extracted to form the first split, referred to as the test set. This test set contains characteristics that do not appear in the remaining splits of the dataset.\nThe remaining data, which excludes the characteristics in the test set, was then split into two parts: the training set and the evaluation set. This split was done using a 70/30 stratified method based on both the characteristics and their corresponding labels (0 or 1). Stratified splitting means that the data was divided in such a way that both the training and evaluation sets have the same distribution of characteristics. For example, if a specific characteristic is present in 10% of the data, it will also be present in about 10% of each of these two sets. In addition, the ratio of zeros (absent) and ones (present) is maintained for each characteristic. This ensures that both sets are representative and balanced, making the scoring both fair and reliable.\nThe training set, comprising 70% of this remaining data, was used to train the models. The evaluation set, consisting of the remaining 30%, was used to assess the performance of the model on characteristics that were present during training.\nThis approach ensures that the test set contains unique characteristics, while the training and evaluation sets are balanced and representative. This setup allows for a robust assessment of how well the models perform on both familiar and unfamiliar characteristics.\nTo determine which characteristics to include in the test set, three different strategies were employed. The first strategy, called the \\\"low-sample experiment\\\", involved selecting three characteristics with a reduced set of samples. This approach was taken to minimize the number of samples in the test set and thus maximize the number available for the training set. By doing so, the model could be trained on a larger dataset, potentially improving its performance.\nThe second strategy, known as the \\\"random selection experiment\\\", involved selecting three random characteristics. This was done to gain insights into the importance of different characteristics in the results. By randomly selecting characteristics, the variability in model performance due to the specific characteristics included in the test set could be better understood.\nThe third strategy, referred to as the \\\"performance-matched experiment\\\", focused on selecting three characteristics for which the performance of the Llama 3 model in a zero-shot setting was similar in both the test and evaluation sets. This approach was aimed at minimizing the effect of characteristic selection on the results, ensuring that the performance evaluation was not overly influenced by the specific characteristics chosen for the test set.\"\n    },\n    {\n      \"title\": \"3.3 Models and configurations\",\n      \"content\": \"Llama 3 [22] and DeBERTaV3 [32] were used in this study. The 8-billion parameter version of Llama 3 was chosen for several reasons. It is one of the latest state-of-the-art open-weight model, making it accessible for research and practical applications. Additionally, Llama 3 is recognized for its high performance and cost-effectiveness [33]. In particular, the instruction-tuned version of Llama 3 was used to enhance its ability to follow and execute specific instructions accurately. Due to GPU memory constraints, the 8-billion parameter version was specifically selected, ensuring effective use within the available computational resources.\nDeBERTaV3 was also selected for its performance as one of the most advanced encoder-only models currently available. This model stands out for its ability to outperform other well-known models such as XLNet [34], ROBERTa [35] and ELECTRA [36] in various benchmarks. DeBERTaV3 combines the strengths of DeBERTa and ELECTRA using a novel embedding sharing paradigm called GDES. Its ability to handle intricate language understanding tasks with high accuracy made it an ideal choice for our study.\nTo thoroughly evaluate the performance of these models, five different configurations were tested. Using Llama 3, four configurations were evaluated:\n\u2022 Zero-shot configuration: In this setup, the model is provided only with the instruction, the response, and the characteristic it needs to classify. No additional examples are given.\n\u2022 Few-shot configuration: In addition to the instruction, the response, and the characteristic, the model is provided with five examples of already labeled responses/characteristics from the training set. This helps the model to better understand the task by seeing examples of how similar tasks were labeled [37].\n\u2022 Fine-tuned zero-shot configuration: This setup uses the same prompt as the zero-shot configuration, but the model has been previously fine-tuned with the training dataset. The fine-tuning process involves updating the weights of the model weights based on the training data to improve its performance on specific tasks. The following sections will detail the fine-tuning process of the models.\n\u2022 Fine-tuned few-shot configuration: Similar to the few-shot configuration, but using the fine-tuned model. The model receives the instruction, the response, the characteristic, and five examples of labeled respons-es/characteristics from the training set. The fine-tuning step helps the model perform better by leveraging the specific characteristics of the training data.\nFor the fifth configuration, DeBERTaV3 was used:\n\u2022 DeBERTaV3 configuration: This model is fine-tuned using the training subset. Unlike Llama 3, DeBERTaV3 is an encoder-only model, which fundamentally changes how it can be used. Encoder-only models like DeBERTaV3 are designed for tasks such as classification or sentence encoding, but they cannot generate text or follow instructions in the same way as decoder or encoder-decoder models like Llama 3 [19] do. This architectural difference means that DeBERTaV3 cannot be used in zero-shot or few-shot configurations that require the model to understand and follow written instructions or examples within the input. Instead, DeBERTaV3 is fine-tuned on the task and then directly provided with the response and the description of the characteristic it needs to classify during inference. This fine-tuning process adapts the pre-trained knowledge of the model to the specific task of characteristic classification, allowing it to make accurate predictions without the need for explicit instructions or examples in the input.\nEach of these five configurations were tested on both the evaluation and test datasets for each of the three experiments (low-sample, random selection, and performance-matched). This comprehensive testing approach ensures that the performance of the model can be evaluated across different scenarios and datasets, providing a robust assessment of their capabilities. Using these configurations, the research questions can be effectively addressed, allowing the determination of the most effective LLM configuration for both predefined and open-ended identification of characteristics within the TM platform. The prompts used for few-shot and zero-shot configurations can be found in the appendix (see B).\"\n    },\n    {\n      \"title\": \"3.4 Inference and fine-tuning methods\",\n      \"content\": \"Specific inference and fine-tuning methods were used to evaluate the models. These methods were designed to ensure accurate and efficient model performance for a binary classification task.\nFor the inference of Llama 3, the vLLM tool was used [38] with the greedy-search decoding method. To determine if the characteristic was present in the response, the model was prompted to generate a \\\"1\\\" if the characteristic was present and a \\\"0\\\" if it was not. A guided choice method was employed to ensure the model generated either a \\\"0\\\" or a \\\"1\\\". This involved focusing only on the probabilities of the tokens corresponding to \u201c0": "nd \u201c1\", normalizing these probabilities, and then selecting the token with the highest normalized probability. This approach ensured that the model's output was restricted to the desired binary classification.\nFor the inference of DeBERTaV3, the Transformers library was used [39]. Since the model was already fine-tuned and included a binary classification head, the inference process was straightforward. The model was directly provided with the response and the description of the characteristic, and it outputted the classification without any additional steps. This approach is consistent with the architecture of DeBERTaV3 as an encoder-only model, which, as previously mentioned, is not suitable for zero-shot or few-shot configurations and it is instead used in its fine-tuned state for direct classification tasks.\nThe fine-tuning of Llama 3 involved a supervised fine-tuning (SFT) approach. The same prompt used for the zero-shot configuration was applied. During fine-tuning, all tokens corresponding to the prompt were masked to ensure that weight updates were based solely on the prediction of the label (0 or 1). The Trainer API from the Transformers library was used too. Additionally, the QLoRA technique [40] was employed to reduce the computational resources and time required for training. QLoRA is a combination of two techniques: Low-Rank Adaptation (LoRA) [41] and 4-bit quantization [42]. This technique helps optimize the model efficiently by reducing the model size and accelerating the training process.\nFor the fine-tuning of DeBERTaV3, the input included both the response and the description of the characteristic. A binary classification head, initialized randomly, was added to the pre-trained model. Full-parameter fine-tuning was then performed, meaning the entire model, including the new classification head, was trained. The Trainer API from the Transformers library was used for this training process [39]."}, {"title": "3.5 Evaluation procedure", "content": "The evaluation of the models was conducted using four key metrics: precision, recall, F1 score, and balanced accuracy. These metrics provide a comprehensive assessment of the models' performance in classifying the characteristics accurately.\nPrecision measures the proportion of true positive predictions among all positive predictions (true positives plus false positives) made by the model. It indicates how many of the positive identifications were actually correct. High precision is crucial when the cost of false positives is high [43].\nRecall (or sensitivity) measures the proportion of true positive predictions among all actual positives. It reflects the model's ability to identify all relevant instances. High recall is important when the cost of false negatives is high [43].\nF1 Score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall, making it useful when a balance between the two is desired. The F1 score is particularly useful when the class distribution is imbalanced [44].\nBalanced Accuracy is the average of recall obtained on each class. It adjusts for imbalanced class distributions by considering both the sensitivity and the specificity (true negative rate) of the model. This metric is particularly useful in scenarios where class imbalance is present [45]. Unlike the F1 score, which balances precision and recall, balanced accuracy takes into account both the positive and negative classes, making it more suitable for assessing performance across all classes in an imbalanced dataset."}, {"title": "4 Results", "content": "In this section, the results of the experiments are presented. Tables 2 to 7 show the performance metrics, including balanced accuracy (BAC), F1 score, precision, and recall, for each of the three experiments: low-sample, random, and performance-matched. Each experiment was evaluated using five configurations (Llama 3 zero-shot, Llama 3 few-shot, Llama 3 fine-tuned zero-shot, Llama 3 fine-tuned few-shot, and DeBERTaV3) on both the evaluation and test datasets.\nLow-Sample experiment In the low-sample experiment (Table 2 and Table 3), it is observed that the Llama 3 fine-tuned few-shot configuration consistently performed best on the test dataset with a BAC of 0.926, an F1 score of 0.899, and the highest recall of 0.879. This suggests that fine-tuning with few-shot examples significantly improves the capability of the model to generalize to unseen characteristics.\nOn the other hand, the Llama 3 fine-tuned zero-shot configuration showed the highest performance in the evaluation set with a BAC of 0.867 and an F1 score of 0.744, indicating that the model retains strong performance even without additional few-shot examples when tested on seen characteristics. DeBERTaV3, while performing reasonably well, fell behind the Llama 3 configurations, particularly in the test dataset where it achieved a BAC of 0.762 and an F1 score of 0.656.\nRandom experiment The random experiment (Table 4 and Table 5) highlighted some variability in model performance. On the test dataset, Llama 3 fine-tuned few-shot achieved the best results with a BAC of 0.764 and an F1 score of 0.703, showing its robustness across randomly selected characteristics. Notably, the Llama 3 fine-tuned zero-shot configuration performed poorly in this scenario with a BAC of 0.621 and an F1 score of 0.428, indicating challenges in generalizing without few-shot examples. Interestingly, Llama 3 fine-tuned zero-shot achieved the highest precision (0.76), while Llama 3 few-shot had the highest recall (0.752). This suggests that the zero-shot configuration was more conservative in its predictions, making fewer but more accurate positive classifications. In contrast, the few-shot configuration was more sensitive, correctly identifying a higher proportion of positive cases but at the cost of more false positives.\nIn the evaluation set, DeBERTaV3 exhibited the highest BAC (0.834) and F1 score (0.723). Llama 3 fine-tuned few-shot also demonstrated strong performance with a BAC of 0.828 and an F1 score of 0.655. Noteworthy, Llama 3 fine-tuned few-shot achieved the highest recall (0.762), indicating its effectiveness in identifying a larger proportion of positive cases. On the other hand, Llama 3 fine-tuned zero-shot showed the best precision (0.784), suggesting more accurate positive predictions but potentially at the cost of missing some positive cases.\nPerformance-matched experiment For the performance-matched experiment (Table 6 and Table 7), the Llama 3 few-shot configuration scored highest on the test dataset with a BAC of 0.882 and a recall of 0.869, highlighting its ability to maintain high recall across conditions. The fine-tuned few-shot configuration also performed well with a BAC of 0.838 and the highest F1 score of 0.770. In addition, this configuration also achieved the highest precision of 0.85, indicating its balanced performance across different metrics and its ability to make accurate positive predictions.\nIn the evaluation set, DeBERTaV3 achieved the highest F1 score of 0.744 and Precision of 0.767, suggesting its strong performance with characteristics seen during training. However, Llama 3 fine-tuned few-shot achieved the highest BAC of 0.853 and Recall of 0.892, indicating its effectiveness in handling seen characteristics using fine-tuning and few-shot examples."}, {"title": "5 Discussion", "content": "The results indicate that the Llama 3 fine-tuned few-shot configuration generally provides the best performance across different scenarios, especially for unseen characteristics in the test datasets. This configuration showed consistently high BAC and F1 scores, highlighting the benefits of fine-tuning combined with few-shot learning to improve the generalization ability of the model.\nDeBERTaV3, while effective, showed a more significant drop in performance on the test datasets compared to the evaluation sets, indicating it might be less robust to predict completely new characteristics. However, it still maintained competitive performance on the evaluation sets, especially in the performance-matched and random experiments.\nLlama 3 zero-shot and few-shot configurations performed variably, with few-shot generally outperforming zero-shot, suggesting that providing additional examples helps the model understand and classify the characteristics more accurately. Overall, these findings suggest that fine-tuning and few-shot learning are critical for improving the generalization capabilities of language models. The use of these techniques allows models to adapt better to new, unseen characteristics, making them more reliable and effective in practical applications.\nA notable comparison can be made with the work of Littenberg-Tobias et al. [12], where researchers used the weighted F1 score to evaluate their binary classification models for detecting specific educational characteristics in teacher responses. Their study used the same dataset source, meaning that many of the characteristics in our study overlap with theirs. In their work, they compared the performance of GPT-3 with traditional machine learning algorithms and found that the latter performed better. The task in both their study and our current research is the same: determining whether a specific characteristic is present in teacher candidates' responses. While the weighted F1 score provides a holistic view of the performance of the model across both classes, it may not be the most effective for imbalanced datasets where the primary interest is the positive class. The weighted average can obscure the capacity of the model to correctly identify the less frequent but critical educational characteristics.\nIn contrast, the binary F1 score (the one used in this study) offers a clearer evaluation for such scenarios. This metric focuses specifically on the positive class, balancing precision and recall to give a more accurate measure of how well the model identifies the presence of the characteristics. Emphasizing the positive class ensures that both false positives and false negatives are adequately considered, providing a more meaningful assessment of the effectiveness of the model in real-world educational settings.\nTo compare our approach with that of Littenberg-Tobias et al. [12], the results were computed using the weighted F1 score and grouped by the same characteristics."}, {"title": "6 Conclusions and future work", "content": "DS usually includes open-ended questions enabling the teacher candidates to express their own thoughts. In these simulations, teacher educators need to identify specific behaviors or characteristics in the teacher candidates' responses to provide adequate evaluation and avoid the repetition of mistakes. To address this issue, we have evaluated different configurations of LLMs to identify characteristics in the responses of DS, using a dataset from DS for teacher education. With this dataset, we evaluated the performance of DeBERTaV3 and Llama 3, combined with zero-shot, few-shot, and fine-tuning. We conducted three experiments evaluating the models' performance in three different dataset splits. In these experiments, we discovered a significant variation in the LLMs' performance depending on the characteristic to identify. Besides, we noted that DeBERTaV3 obtained a balanced accuracy of 0.8 in identifying labels seen during its training but on unseen responses. For new labels, this model decreased its performance, even achieving an accuracy of 0.597 in one of the experiments. In contrast, Llama 3 performed better than DeBERTaV3 in predicting new characteristics and showing more stable performance. In most of the experiments, few-shot and fine-tuning improved the performance of Llama 3 for the identification of both seen labels and new labels. Consequently, we recommend using Llama 3 for the DS where teacher educators need to introduce new characteristics, since they could change depending on the simulation or the educational objectives.\nThis work could be extended by testing LLMs in other simulation environments. An interesting gap for research is to evaluate the generalization ability of few-shot and fine-tuned LLMs to other simulations designed for other professionals and characteristics. Furthermore, future work could include exploring why some characteristics are more difficult to identify. This work could provide recommendations for educators on defining these characteristics. Additionally, future researchers could evaluate the adoption and perception of educators regarding the integration of LLMs in DS to provide the required evaluations that these environments need for a comprehensive education."}, {"title": "A Characteristics description", "content": "The following table shows the description of each characteristic."}, {"title": "B Prompt Design", "content": ""}, {"title": "B.1 Zero-shot Prompt", "content": "For the zero-shot configuration", "0": "f the characteristic is not present in the teacher's response and a '1' if it is.<|eot_id|><|\nstart_header_id|>user<|end_header_id|>\nYou", "response": "nThanks for this Jeremy! I'm sorry to hear you weren't feeling well. Could you stay after class for a quick 5 minutes so that I can run-through the work with you?\nThis is the characteristic you are looking for in the response:\nSuggests that Jeremy needs to catch up on the academic work.\nYou have to evaluate if the characteristic is present in the teacher's response.<|eot_id|><|\nstart_header_id|>assistant<|end_header_id >\nB.2 Few-shot Prompt\nIn the few-shot configuration, the following prompt was used to guide the Llama 3 model in classifying whether a specific characteristic was present in the teacher candidate's response. The prompt includes five examples sampled from the training subset to help the model understand the task. Note that the same examples are used for all three experiments, however, the characteristics in these examples are not included in the test sets of any of the experiments.\n<|begin_of_text|><|start_header"}]}