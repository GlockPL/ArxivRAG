{"title": "tuGEMM: Area-Power-Efficient Temporal Unary GEMM Architecture for Low-Precision Edge AI", "authors": ["Harideep Nair", "Prabhu Vellaisamy", "Albert Chen", "Joseph Finn", "Anna Li", "Manav Trivedi", "John Paul Shen"], "abstract": "General matrix multiplication (GEMM) is a ubiquitous computing kernel/algorithm for data processing in diverse applications, including artificial intelligence (AI) and deep learning (DL). Recent shift towards edge computing has inspired GEMM architectures based on unary computing, which are predominantly stochastic and rate-coded systems. This paper proposes a novel GEMM architecture based on temporal-coding, called tuGEMM, that performs exact computation. We introduce two variants of tuGEMM, serial and parallel, with distinct area/power-latency trade-offs. Post-synthesis Power-Performance-Area (PPA) in 45 nm CMOS are reported for 2-bit, 4-bit, and 8-bit computations. The designs illustrate significant advantages in area-power efficiency over state-of-the-art stochastic unary systems especially at low precisions, e.g. incurring just 0.03 mm\u00b2 and 9 mW for 4 bits, and 0.01 mm\u00b2 and 4 mW for 2 bits. This makes tuGEMM ideal for power constrained mobile and edge devices performing always-on real-time sensory processing.", "sections": [{"title": "I. INTRODUCTION AND BACKGROUND", "content": "General matrix multiplication (GEMM) performs multiply-and-accumulate operations on matrices and forms the fundamental building block for deep neural networks (DNNs). While the application performance of DNNs has increased steadily over the years, its computational demands have increased exponentially [19]. Traditionally, GEMM was implemented as software libraries for CPUs and GPUs [14], [15]. However, more recently, dedicated GEMM hardware units have been implemented within GPUs and DNN accelerators to improve compute efficiency. The increasing demand for hardware acceleration resulted in companies like Nvidia introducing the tensor cores [22] capable of performing 4x4 matrix multiplication, and Google introducing Tensor Processing Units (TPUs) [6] with Matrix Multiply Units (MXUs). Further, with the latest push towards edge computing and on-device AI [8], [17], focus has shifted towards developing low-footprint GEMM hardware. Towards that goal, Nvidia and Google introduced Jetson Xavier NX [4], and edge TPU [3] respectively, both of which deploy reduced compute on device, albeit at the expense of inference accuracy. In the current landscape, various such lightweight systems have been proposed [10], [16], including deep learning accelerators (DLAs) in modern smartphones [5]. These systems predominantly operate on binary values and trade off inference accuracy to meet the Power-Performance-Area (PPA) constraints for edge devices.\nOn the other hand, unary compute-based implementations offer a promising alternative solution to the increasing parallel computation complexity inherent to binary implementations, delivering low area and low power designs. This emerging paradigm replaces the multiple parallel bits with a single serial bit-stream. Unary computing manifests in two major forms, rate and temporal coding, with rate-based methods being more prevalent. Recently proposed uGEMM [21] is a unified rate-and-temporal encoded GEMM architecture that incorporates unary arithmetic units to perform stochastic GEMM operations. It provides significant PPA improvements compared to previous unary designs, while maintaining high accuracy, making it a promising candidate for edge devices. However, being a stochastic approach, it doesn't perform exact compute and falls under the widely researched domain of approximate computing. In contrast, our work focuses on exact, not approximate, GEMM compute based purely on temporal encoding.\nRecent works have emphasized the current trend of AI/DL to move towards lower precision. Authors in [12] performed training with 5-bit weights and 4-bit activations, and in [11] with 4-bit weights and 8-bit activations, both with minimal accuracy degradation. More recently, IBM researchers achieved 8-bit precision for training and 4-bit precision for inference across many deep learning datasets [20], followed by the work in [18] that shows both training and inference can be performed with 4-bits with negligible impact on accuracy. Additionally, Akida NSoCs [2] employ 1, 2, 4-bit computations for their weights and activations targeting edge inference. This growing affinity towards low precision influences this work to explore low bit-width implementations for edge AI.\nWe propose a novel GEMM architecture, tuGEMM, based on exact temporal compute, targeting area-power efficiency for low precision edge AI. Our key contributions are as follows:\n\u2022 We propose a novel temporal-coding-based GEMM architecture that performs exact computations in contrast to existing stochastic and rate-based approaches.\n\u2022 Two architecture variations, serial and parallel, that offer different area-latency tradeoffs are introduced.\n\u2022 Gate-level implementations for both designs are presented, and post-synthesis PPA numbers for 2-bit, 4-bit and 8-bit implementations are reported.\n\u2022 Latency evaluation for tuGEMM compute is performed using a representative DNN workload, ResNet18.\n\u2022 We illustrate superior area-power efficiency over state-of-the-art unary approach, especially for low precision.\nSection II presents the encoding mechanism and the architecture of the two design variants. Section III evaluates tuGEMM against uGEMM based on latency and post-synthesis"}, {"title": "II. TUGEMM ARCHITECTURE", "content": "General matrix multiplication (GEMM) is central to the compute-intensive operations in DNNs. Its general format is:\n$Y = \\alpha AB + BC$\nwhere A, B and C are generic MXN, NXP and MxP input matrices, Y is the MxP output matrix, and \u03b1 and \u03b2 are scaling factors. This work focuses on non-scaled GEMM operation, i.e., \u03b1 = \u03b2 = 1. This section describes the input encoding and the micro-architecture of the proposed tuGEMM hardware."}, {"title": "A. Input Encoding", "content": "The key idea of unary hardware implementation is encoding values as serial bitstreams on a single bitline. Such an input encoding allows the hardware to be repurposed with significantly less area and power. Unary encoding can be accomplished in two ways: rate and temporal coding. Rate-based systems encode values in the frequency of ones randomly distributed across the bitstream, whereas temporal coding encodes values in the time duration for which a signal is asserted. As a result, a temporally encoded bitstream consists of consecutive ones followed by consecutive zeros, resulting in only two transitions. This naturally leads to improved dynamic power consumption, compared to rate coding with multiple signal transitions due to the distributed occurrence of ones.\nRate coding typically implements stochastically-generated bitstreams using expensive random number generators (RNGs) and suffers from the correlation problem, requiring additional hardware to mitigate it [1], [7], [9]. In contrast, temporal encoding uses a single contiguous n-cycle wide logic pulse to represent a value n, analogous to the spike encoding employed in neuromorphic computing [13], and can enable exact deterministic compute in an efficient manner as it does not require RNGs. Our proposed approach distinguishes from previous works by utilizing temporal-unary-encoded exact compute."}, {"title": "B. Serial Architecture", "content": "The serial architecture (Fig. 1) consists of an MxP array of output counter cells surrounded by peripheral logic that performs unary encoding and co-ordinates the dataflow into the array. The counter array receives unary-encoded input matrices A, B from the left and top respectively, and implements multiplication in unary fashion. The multiplication compute occurs in N steps, where N is the common matrix dimension, which is equal to the number of columns in A and number of rows in B. Each step computes the outer product of ith column from A and ith row from B. During each column-row outer product, the MxP output counters update their counts with the MxP output values, taking as many cycles as the magnitude of the maximum output value (due to unary multiplication which will be described shortly). Thus, these outer products are accumulated over the N steps, at the end of which the final counter values reflect the output matrix Y. To eliminate a separate adder, the MXP counters are initialized with the binary-encoded input matrix C (following sections focus only on A x B multiplication). The serial architecture performs the N steps serially, and is named so. It has four components:\n1) index counter: Each column of A is indexed simultaneously with the corresponding row of B. This indexing is generated by the index counter that counts up from 0 to N \u22121, incrementing each time by one after every step, indicated by step_done signal. Once its count reaches N, the index counter asserts an output_ready signal, implying GEMM has finished.\n2) vector generator: Two vector generators receive index i from the index counter and use it to index into the input matrices A and B to generate the ith column of A (M-dimensional vector), and ith row of B (P-dimensional vector).\n3) column/row counters: M column counters and P row counters convert the binary values from the vector generator to unary signals and co-ordinates the unary multiplication. In every step, the column and row values from the vector generator are loaded into the counters, which then begin counting towards zero (decrement if the initialized count is positive, increment if negative). The counters operate in a nested fashion such that the row counters are updated by 1 every cycle whereas the column counters only update their values (by 1) once all row counters reach zero. This cycle repeats until all the column counters reach zero thus completing one step, eventually triggering the index counter for the next step via step_done signal. During every step, the column and row counters assert M unary_col and P unary_row signals respectively, whenever their corresponding counts are non-zero. These signals represent the converted unary signals derived from the vector generator values, and enable column-row outer product in the output array as will be described next. Each counter also asserts a neg_col/row signal, if the corresponding initialized count is negative, to determine the direction of update in the output counter cells.\n4) output counter array: It consists of MXP counter cells (initialized with matrix C), where each counter accumulates the unary column-row outer product within a step, and is enabled when unary_col/row are both asserted. If enabled, it"}, {"title": "C. Parallel Architecture", "content": "A key observation is that the N computation steps are independent of each other. Hence, unlike serial, the parallel architecture (Fig. 2) computes all the N steps in parallel giving it its name, and is designed for reduced latency at the cost of increased area and power. To achieve this, it integrates the two vector generators, and the column and row counters into a single vector counter that is replicated N times. It also houses an MxP array of output adder cells instead of the output counter array as in serial architecture, where each cell is now capable of adding the counts from all N steps in parallel. Note that there is no need for an index counter used earlier to serialize the N steps. The two main components here are:\n1) vector counters: The ith vector counter generates unary signals for ith column from A and ith row from B. Once all the M column counters within a vector counter reach zero, it asserts its col_done signal. The GEMM computation finishes when all the vector counters assert this signal, generating output_ready signal via an AND gate. The vector counters output N sets of M-dimensional unary and neg signals from the left and N sets of P-dimensional unary and neg signals from the top, to be used by the output adder array.\n2) output adder array: This component (Fig. 3) holds majority of the hardware modifications with respect to the serial design. Firstly, the counter in the output counter cell of serial architecture is replaced by an adder and a register for accumulation, as necessitated by the requirement for computing all steps in parallel. Secondly, each output adder cell is capable of processing N different pairs of unary and neg signals in parallel. Each of the N pairs generates '1', '-1' (neg block is used to generate '-1' in two's complement) or '0' based on the unary_col/row and neg_col/row signals controlled using a simple multiplexer, that are fed into a binary adder which accumulates into a register which holds final value for that cell in the GEMM output, after output_ready is asserted."}, {"title": "III. EVALUATION AND RESULTS", "content": "Serial and parallel tuGEMM designs are implemented in System Verilog, and synthesized with Nangate45 Open Cell Library using Synopsys Design Compiler. Post-synthesis area and power are compared against uGEMM [21] for 8-bit 16x16 matrices (M=N=P=16) at 400 MHz (uGEMM uses this configuration). We further extend the design space to larger 32x32 matrices and lower precision (4 bits and 2 bits).\nAll post-synthesis tuGEMM numbers are reported in Table I, with 16x16 values illustrated in Fig. 4, alongside uGEMM."}, {"title": "B. Latency Evaluation", "content": "In this section, latencies for a complete GEMM compute are assessed. Assume w is the input bit-width for this discussion.\n1) Worst-Case Latency: With two's complement numbers, the largest representable value is 2w-1. As the column/row counters perform unary encoding, it can take up to 2w-1 cycles for the row counter to reach zero and $(2^{w-1}) * (2^{w-1}) = (2^{w-1})^2$ cycles for the column counter to reach zero and generate the maximum value in any step. Since serial tuGEMM operates through N such steps serially, it can take up to a maximum of $N * (2^{w\u22121})^2$ cycles in the worst case. As can be seen from Table I, the increase in area and power for parallel design compared to serial design is less than N-fold, potentially resulting in an overall boost in energy efficiency. Worst-case latency scales exponentially with bitwidth, hence tuGEMM is best suited for low precision. However, average-case latencies for real workloads can be much lower depending on the frequency of large values.\n2) Average-Case Latency for Edge AI/DL: Given the input matrices may not hold the largest absolute value, the actual latency can be significantly lower in most cases. In order to profile maximum values, we use a representative edge DNN workload, INT8 quantized ResNet18. During inference, in"}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "This work introduces a novel temporal unary GEMM design, tuGEMM, capable of exact compute with very high area-power efficiency. In 45nm CMOS, 8-bit 16x16 serial tuGEMM consumes just 0.05 mm\u00b2 area and 18 mW power. The parallel design reduces serial latency by 16x while incurring only an increase of 5x/4x in area/power. Compared to state-of-the-art unary stochastic uGEMM, serial and parallel tuGEMM are about 15x/11x and 3.7x/3.8x more efficient in area/power respectively. This does incur a latency penalty which can be partially mitigated in tuGEMM by exploiting data sparsity and frequently occurring small values. For 4-bit and 2-bit precisions, tuGEMM consumes minimal area and power with very reasonable latency, and thus can be excellent candidates for low-precision always-on edge-AI devices. Future research plans include exploring different input encodings targeting latency optimization, and incorporating tuGEMM in DLAs."}]}