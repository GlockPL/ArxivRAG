{"title": "Shrinking POMCP: A Framework for Real-Time UAV Search and Rescue", "authors": ["Yunuo Zhang", "Baiting Luo", "Ayan Mukhopadhyay", "Daniel Stojcsics", "Daniel Elenius", "Anirban Roy", "Susmit Jha", "Miklos Maroti", "Xenofon Koutsoukos", "Gabor Karsai", "Abhishek Dubey"], "abstract": "Efficient path optimization for drones in search and rescue operations faces challenges, including limited visibility, time constraints, and complex information gathering in urban environments. We present a comprehensive approach to optimize UAV-based search and rescue operations in neighborhood areas, utilizing both a 3D AirSim-ROS2 simulator and a 2D simulator. The path planning problem is formulated as a partially observable Markov decision process (POMDP), and we propose a novel \"Shrinking POMCP\" approach to address time constraints. In the AirSim environment, we integrate our approach with a probabilistic world model for belief maintenance and a neurosymbolic navigator for obstacle avoidance. The 2D simulator employs surrogate ROS2 nodes with equivalent functionality. We compare trajectories generated by different approaches in the 2D simulator and evaluate performance across various belief types in the 3D AirSim-ROS simulator. Experimental results from both simulators demonstrate that our proposed shrinking POMCP solution achieves significant improvements in search times compared to alternative methods, showcasing its potential for enhancing the efficiency of UAV-assisted search and rescue operations.", "sections": [{"title": "I. INTRODUCTION", "content": "Search and rescue (SAR) operations are critical, time-sensitive missions conducted in challenging environments like neighborhoods, wilderness [1], or maritime settings [2]. These resource-intensive operations require efficient path planning and optimal routing [3]. In recent years, Unmanned Aerial Vehicles (UAVs) have become valuable SAR assets, offering advantages such as rapid deployment, extended flight times, and access to hard-to-reach areas. Equipped with sensors and cameras, UAVs can detect heat signatures, identify objects, and provide real-time aerial imagery to search teams [4].\nHowever, the use of UAVs in SAR operations presents unique challenges, particularly in path planning and decision-making under uncertainty. Factors such as limited battery life, changing weather conditions, and incomplete information about the search area complicate the task of efficiently coordinating UAV movements to maximize the probability of locating targets [3]. To address these challenges, researchers have proposed formalizing UAV path planning for SAR missions as partially observable Markov decision processes (POMDPs) [5]\u2013[7]. POMDPs provide a mathematical framework for modeling sequential decision-making problems in uncertain environments where the system's state is not fully observable [8].\nPOMDP-like planning is crucial for search operations due to inherent uncertainties [9]. In UAV-based SAR, POMDPS capture uncertainties in target locations, sensor observations, and environmental conditions while optimizing UAV paths [4]. They model unknown environmental states, imperfect sensor information [10], and the complex interdependence between decisions and future observations [11]. POMDPs naturally address partial observability and long-term action consequences [10]. However, solving large-scale POMDP problems remains computationally challenging, with complexity growing exponentially with state space, observation space, and planning horizon sizes, often making exact solutions intractable for real-world applications [12].\nTo address this challenge, recent research has focused on online POMDP solutions, aiming to find good policies quickly by interleaving planning and execution and using sampling-based techniques to explore the belief space efficiently [13], [14]. Online POMDP frameworks have been applied to UAV path planning for SAR operations, addressing uncertainties in target motion and sensor observations [15], partial observability of victim locations and environmental hazards [16], and challenges in multi-UAV search missions [17]. Despite these advancements, computational efficiency under strict time constraints remains a critical challenge for real-time applications.\nThis paper presents a novel online path planner for UAVs designed to enhance the efficiency of search and rescue operations in urban environments. Our approach combines advanced simulation techniques with an innovative POMDP formulation and solution approach. This method, called Shrinking POMCP (partially observable Monte Carlo planning), guides the agent toward the next best non-sparse region for planning (we define a sparse region as a region that has probability of target appearing in that region less than a given threshold). This innovation is particularly crucial for real-world applications with strict time constraints, as it allows for more effective decision-making within limited computational resources. We demonstrate the effectiveness of the approach using an Airsim-based simulator.\nThe outline of this paper is as follows. We first describe the necessary background concepts (\u00a7II), followed by the problem formulation (\u00a7III), solution framework (\u00a7III-A) and description of the POMDP planning algorithm (\u00a7IV), the primary contribution of this paper. We conclude the paper with a description of metrics and experimental results (\u00a7V)."}, {"title": "II. BACKGROUND AND RELATED RESEARCH", "content": "POMDPs, or Partially Observable Markov Decision Processes, are a mathematical framework for modeling decision-making in situations where an agent must make decisions in an environment that is not fully observable. A POMDP is formally defined as a tuple (S, A,T,R,O, Z), where:\n\u2022 S is a finite set of states.\n\u2022 A is a finite set of actions.\n\u2022 T:S\u00d7A\u00d7S\u2192 [0, 1] is the transition function, where T(s'|s, a) gives the probability of transitioning to state s' given action a is taken in state s.\n\u2022R:S\u00d7A\u2192R is the reward function.\n\u2022 O is a finite set of observations.\n\u2022 Z: S\u00d7A\u00d7O \u2192 [0, 1] is the observation function, where Z(o|s', a) is the probability of observing o given action a is taken and the system transitions to state s'.\nIn a POMDP, the agent maintains a belief state b(s), which is a probability distribution over all possible states. The belief state is updated after each action and observation using Bayes' rule. Solving a POMDP involves finding an optimal policy \u03c0* that maximizes the expected cumulative reward. However, exact solutions to POMDPs are computationally intractable for all but the smallest problems. As a result, much research has focused on approximate methods, including Point-based Value Iteration (PBVI) [18], Heuristic Search Value Iteration (HSVI) [19], and Monte Carlo Tree Search (MCTS) methods, such as POMCP [13]. Note that Partially Observable Monte Carlo Planning (POMCP) is an online POMDP solver that extends Monte Carlo Tree Search (MCTS) to POMDPS [13]. (MCTS) is a heuristic search algorithm for decision processes, particularly effective in large state spaces.\nKey components of MCTS include: 1) Selection: Starting from the root node, a child selection policy is recursively applied to descend through the tree until reaching a leaf node. 2) Expansion: If the leaf node is not terminal and is within the computational budget, one or more child nodes are added to expand the tree. 3) Simulation: A simulation is run from the new node(s) according to the default policy to produce an outcome. 4) Backpropagation: The simulation result is then 'backed up' through the selected nodes to update their statistics.\nDuring selection phase, UCT (Upper Confidence Bounds for Trees) [20] is used as child selection policy: $UCT = X_j + C\\sqrt{\\frac{2\\ln n}{n_j}}$, Where $X_j$ is the average reward from node j, n is the number of times the parent node has been visited, $n_j$ is the number of times child j has been visited, and C is an exploration constant.\nSaisubramanian et al. [21] introduced the Goal Uncertain Stochastic Shortest Path (GUSSP) problem, a specialized form of POMDP. GUSSPs extend the Stochastic Shortest Path framework to handle goal uncertainty, maintaining a belief state over possible goal configurations. While including an observation function for goals like POMDPs, GUSSPs simplify the problem by assuming full current state observability and myopic goal observations, resulting in a more tractable solution space compared to general POMDPs.\nDespite advancements, existing approaches to solving POMDPS and GUSSPs face challenges in real-time applications. The computational complexity often exceeds time constraints of real-world scenarios. Even with GUSSP simplifications, the problem remains computationally demanding for large state spaces. This highlights the need for efficient algorithms that can provide quality solutions within tight time bounds, especially for robotics and autonomous systems requiring rapid decisions."}, {"title": "A. AirSim and ROS2", "content": "Microsoft AirSim is an open-source simulator for au-tonomous vehicles, developed by Microsoft Research [22]. It was designed to bridge the gap between simulation and reality in the field of artificial intelligence, particularly for drones and self-driving cars. AirSim provides a platform for researchers and developers to test and train AI algorithms in a realistic, physics-based environment without the risks and costs associated with real-world testing [22]. The simulator leverages Unreal Engine [23] to create detailed environments and supports various autonomous system sensors like cameras, GPS, and IMUs [24]. To provide communication and integrate external processing, the system supports the Robot Operating System v2 (ROS2) [25]. AirSim has been upgraded for DARPA by Microsoft and extended with STR Algorithm Development Kit to provide mission generation and randomization."}, {"title": "III. PROBLEM FORMULATION", "content": "Overall, the problem we are interested in is to perform autonomous target localization with multiple targets in an urban environment using an unmanned aerial vehicle (UAV) (fig. 1). A key aspect of this problem is the uncertainty in target locations. The quadrotor does not possess prior knowledge of exact target positions. Instead, it maintains a probabilistic map representing its belief state, which is continuously updated based on a perception system, which eventually leads to update of belief system. The perception system is also responsible for the tracking and identification of the target if they are in the"}, {"title": "A. Solution Approach", "content": "Figure 2 shows our software system for mission execution. The perception module detects objects with specific attributes and identifies relationships between them, handling environmental novelties due to varying camera views, occlusion, and weather perturbations. While state-of-the-art object detectors like YOLO world [26] perform well in closed-world setups, they struggle with novel attributes and relations in SAR operations. Our two-stage approach first detects generic objects, then uses a vision-language model (VLM) [27] to detect attributes and relations. VLMs, trained on diverse datasets, can handle novelties effectively.\nThe probabilistic world model maintains and updates the belief map using raw sensor information received from the perception components, and other sensors such as inertial measurement unit outputting the current drone position, obstacle map, and belief map. It efficiently represents probability distributions and their relations, generating updates from a formal specification derived from mission parameters. The world model also maintains the flight rules including location of no-fly areas. The planner component is responsible for generating flight plans \u2013 sequence of waypoints. The navigator component is responsible for finding the shortest and safest path (while avoiding collisions) in the 3-D environment. The height adjustment component is utilized to increase or decrease the height of the drone if no-safe path can be found. Overall, the system works as an interactive protocol between the planner and the navigator. When the planner publishes a waypoint (using ROS2), the determined waypoint is then passed to the navigator component, which guides the quadrotor while ensuring collision avoidance. This process continues until the drone reaches the waypoint or the navigator determines it's unreachable. In either case, a new decision epoch is initiated, allowing the drone to adapt dynamically to its environment based on the most current information.\nNote while there are innovations in each of the above component, due to space limitations this paper is restricted to the description of the planning algorithm."}, {"title": "IV. PLANNER", "content": "In this section, we describe our POMDP framework and our approach for computing near-optimal actions for the POMDP. A major bottleneck in directly using online search algorithms (e.g., POMCP [13]) in our setting is scalability\u2014the management of the UAV at each time step can only afford limited latency. To address this problem, we propose a \"shrinking POMCP\" algorithm. Intuitively, once a search tree is constructed, we hypothesize that the agent can traverse the most promising actions down the nodes of the search tree, provided it is in a sparse likelihood region of the map. For example, consider that the agent is in the lower left corner of a grid, with the target likely in the upper left corner. The agent can construct a search tree once and (likely) take multiple steps toward the target region without recomputing again. This technique is designed to dynamically reduce the decision space as planning progresses. The key innovation lies in its ability to guide the agent towards the next best non-sparse region for planning, effectively concentrating computational resources on the most promising areas of the state space. Note that to reduce complexity, we discretize the 3-D state space into a two dimensional slice at a given mission height, set to ensure that the perception component can operate efficiently. If required, the operating height is changed and the 2-D planner can be invoked again. The probabilistic world model can generate the belief distribution at any given height."}, {"title": "A. POMDP Formalization", "content": "Decision Epoch: In our framework, the decision epoch is defined as the moment when the solver is triggered to determine the next waypoint for the quadrotor. This dynamic decision-making process occurs at discrete time intervals, transitioning from time t to t+1, and is initiated by specific events rather than at fixed time intervals. In practice, these events can be monitored and controlled by a meta-controller. Specifically, the solver is activated to make a new decision when one of two conditions is met: either the quadrotor successfully reaches its previously issued waypoint, or it encounters a situation where the current waypoint is unreachable due to obstacles obstructing all valid paths. At each decision epoch, the solver receives a comprehensive update of the system's state, including the most recent belief states, the obstacles detected by the quadrotor's cameras in its immediate vicinity, the quadrotor's current position, and a request for the next waypoint. This event-driven approach to decision epochs ensures that the system remains responsive to the dynamic nature of the environment and the quadrotor's progress, allowing for adaptive and efficient navigation strategies.\nStates: In our POMDP framework, we define the state space S to encompass the position of the quadrotor and the locations of all targets. Let $s_t \\in S$ denote the pre-decision state at time t. Each state $s_t$ is represented as a (3+2M)-dimensional vector:"}, {"title": null, "content": "$s_t = [x_q, y_q, z_q, x_1, y_1, ..., x_M, y_M]$                                                                                                     (1)\nwhere\n\u2022 $(x_q, y_q) \\in R^2$ represents the quadrotor's horizontal position.\n\u2022 $z_q \\in R^+$ denotes the quadrotor's altitude\n\u2022 $(x_i, y_i) \\in R^2$ represents the location of the i-th target, for i = {1, ..., M}\nThus, the complete state space S is of dimensions equalling $R^2 \\times R^+ \\times (R^2)^M$. This formulation captures the full spatial configuration of the system at any given time t, incorporating both the UAV's three-dimensional position and the two-dimensional locations of all M targets within the neighborhood area. To simplify the problem, we discretize the operational area into a grid. Let the original map be a square of side length L. We partition this map into an N \u00d7 N grid, where each cell represents an area of (L/N) \u00d7 (L/N) (in our implementation, N = 20, resulting in 20m \u00d720m cells). Formally, we can define the grid G as:\n$G = \\{(i, j) | i, j \\in \\{0, 1, . . ., N - 1\\}\\}$                                                                                                                              (2)\nAt each decision epoch, the agent's position is mapped to one of these grid cells.\nActions: The action space A consists of four cardinal directions:\n$A = \\{West, South, East, North\\}$                                                                                                                                                                            (3)\nEach action $a \\in A$ corresponds to moving to an adjacent grid cell in the specified direction. For an agent in cell (i, j) at time t, an action $a \\in A$ results in a transition to a new cell (i', j') at time t+1, where the new coordinates depend on the chosen direction.\nThe actual waypoint $w_{t+1}$ within the chosen grid cell is determined by finding a valid position closest to the quadrotor's current position $x_t$:\n$w_{t+1} = arg \\underset{w \\in V(i',j')}{min} ||w - x_t||$                                                                                                                                                                         (4)\nwhere $V (i', j')$ is the set of valid positions within the grid cell (i', j'). We describe the translation of the grid-based decision into a specific waypoint for the quadrotor later.\nObservation: In our POMDP framework, the observation space O provides partial information about the state. At each time step t, an observation $o_t \\in O$ is defined as a tuple $(\\Psi_t, B_t)$, where $\\Psi_t = (x_q, y_q, z_q) \\in R^3$ represents the exact current position of the quadrotor, and $B_t$ is the updated belief state. The belief state $B_t$ is a probabilistic map over the grid G, where for each cell (i, j) \u2208 G, $B_t(i, j) \\in [0, 1]$ represents the probability of a target being present in that cell.\nReward: The reward function for our POMDP framework is designed to guide the quadrotor agent in efficiently locating targets within a specified neighborhood area. The reward structure comprises two primary components: target capture and token capture. This dual-component design balances the agent's focus between achieving the primary objective and maintaining comprehensive environmental awareness. The reward function R is formally defined as:\n$R = R_{target} + \\alpha \\cdot R_{token}$                                                                                                                                                                                                 (5)\nwhere $R_{target}$ denotes the reward for target capture, $R_{token}$ represents the reward for token capture, and \u03b1 is a hyperparameter controlling the relative importance of token capture.\nTarget Capture Reward ($R_{target}$): The target capture component directly addresses the primary objective of the simulation. It is defined as a binary function:\n$R_{target} = \\begin{cases} 1, & \\text{if the agent captures a target} \\\\ 0, & \\text{otherwise} \\end{cases}$                                                                                                                                                                               (6)\nThis component provides a significant positive reinforcement upon successful target capture, incentivizing the agent to prioritize navigation towards known or suspected target locations.\nToken Capture Reward ($R_{token}$): The token capture component serves as an exploration incentive, encouraging comprehensive coverage of the environment while prioritizing areas of higher probability. It is defined as:\n$R_{token} = \\sum_i I(i) \\cdot P_i$                                                                                                                                                                                                        (7)\nWhere I(i) is an indicator function for cell i, and $P_i$ is the normalized probability token value for cell i. Note that\n$I(i) = \\begin{cases} 1, & \\text{if cell i is visited for the first time} \\\\ 0, & \\text{if cell i has been visited before} \\end{cases}$                                                                                                                                                                         (8)\nand\n$P_i = \\frac{p_i}{\\sum_j p_j}$                                                                                                                                                                                                                            (9)\nwhere $p_i$ represents the raw probability value assigned to cell i in the probabilistic map, and $P_i$ is its normalized form.\nThis cumulative reward structure incentivizes the exploration of new cells while weighting the reward based on the likelihood of finding targets in each cell. Cells with higher normalized probabilities yield greater rewards upon first visit, potentially facilitating the discovery of targets in areas deemed more promising by the probabilistic map.\nThe hyperparameter \u03b1 in eq. (5) allows for fine-tuning of the agent's behavior, balancing the emphasis between target acquisition and environmental exploration. A higher \u03b1 value encourages more thorough exploration, while a lower value prioritizes immediate target capture."}, {"title": "Algorithm 1 Shrinking POMCP", "content": "Require: Initial belief $b_0$, max_iterations, max_time, max_level, $P_{\\epsilon}$\nEnsure: Action sequence A\n1: Initialize belief tree T with root node $b_0$\n2: while iterations < max_iterations and time < max_time do\n3: $s_0 \\sim b_0$\n4: SimulateV($s_0$, $b_0$, 0)\n5: end while\n6: return GetActionSequence($b_0$, max_level, $P_{\\epsilon}$)"}, {"title": "B. Shrinking MCTS", "content": "At each decision epoch, the algorithm constructs a belief tree T with alternating action (A) and belief (B) nodes. The root node $b_0 \\in B$ represents the initial belief state (Line 1 in algorithm 1). Figure 3 illustrates this tree structure, where action nodes are represented by squares and belief nodes by circles. The red arrows represent the action sequence decided by the agent.\nThe algorithm begins by sampling a random state $s_0 \\sim b_0$ to initialize the root node of the tree (Line 3 in algorithm 1). This initialization step ensures that the search starts from a plausible state within the current belief distribution. The tree expansion process involves a series of iterations, each comprising four main phases: selection, expansion, simulation, and backpropagation. In the selection phase, the algorithm traverses the tree from the root node using the Upper Confidence Bound for Trees (UCT) strategy. For a belief node b, the best action $a^*$ is selected according to the equation:\n$a^* = arg max_a[Q(b, a) + c \\sqrt{\\frac{log(N(b))}{N(b, a)}}]$                                                                                                                                       (10)\nwhere Q(b,a) is the estimated value of action a in belief state b, N(b) is the number of visits to node b, N(b,a) is the number of times action a was selected from belief state b, and c is an exploration constant (Line 8 in algorithm 2). This selection strategy balances exploitation of known high-value actions with exploration of less-visited branches.\nThe simulation phase employs a simulator G, which accepts a state and an action as inputs. This simulator produces three outputs: the probable subsequent state derived from the transition function, the associated observation, and the corresponding reward. This process can be formally represented as:\n(s', o, r) ~ G(s, a)\nwhere s' is the next state, o is the observation, and r is the reward, all generated based on the current state s and action a (Line 9 in algorithm 2).\nThe expansion phase occurs when the selected action leads to an unexplored observation. In this case, a new belief node is added to the tree (Line 11 in algorithm 2). If the observation has been encountered before, the algorithm follows the existing path (Line 13 in algorithm 2). When a leaf node is reached during the simulation phase, the algorithm expands it by creating child nodes for all possible actions. Then, a rollout is performed till a terminal node. The result of this simulation is then propagated back up the tree in the backpropagation phase, updating the statistics (Q-values and visit counts) of all traversed nodes (Line 16 in algorithm 2).\nThe key innovation of the Shrinking MCTS algorithm lies in its decision-making process, which aims to move the agent to the next best non-sparse region while avoiding goal sampling"}, {"title": "Algorithm 2 SimulateV(s, b, depth)", "content": "1: if IsTerminal(s) or depth > max_depth then\n2: return 0\n3: end if\n4: if b is leaf node then\n5: Expand(b)\n6: return Rollout(s, b)\n7: end if\n8: a = arg max [Q(b,a) + c$\\sqrt{\\frac{log(N(b))}{N(b,a)}}$ ]\n9: (s', o,r) = G(s, a)\n10: if b has no child corresponding to o then\n11: b' = CreateNewBeliefNode(b, a, o)\n12: else\n13: b' = b.child(a, o)\n14: end if\n15: q = r + y \u00b7 SimulateV(s', b', depth + 1)\n16: UpdateStats(b, a, q)\n17: return q"}, {"title": "Algorithm 3 GetActionSequence(b, max_level, $P_{\\epsilon}$)", "content": "1: A = [], depth = 0\n2: while depth < max_level and P(b) \u2264 $P_{\\epsilon}$ do\n3: a = arg max Q(b, a)\n4: A.append(a)\n5: b~G(s,a)\n6: depth+ = 1\n7: end while\n8: return A"}, {"title": null, "content": "oscillation. Each belief node s in the tree stores two critical pieces of information: the quadrotor's position y(s) = (xq, yq) and the normalized probability P(s) at that position. A belief node is classified as Non-Sparse if its normalized probability exceeds a predefined threshold, i.e., P(b) > $P_{\\epsilon}$. In fig. 3, a Belief Node with Non-Sparse Region is represented as a circle filled with black.\nUnlike standard MCTS, which typically selects a single action at each step, our Shrinking approach determines an entire action sequence. As shown in fig. 3, this sequence (represented by red arrows) guides the agent towards high-probability areas. The decision sequence is determined by traversing the tree from the root, selecting the best action at each level until either a maximum depth max_level is reached or a Belief Node with Non-Sparse Region is encountered. In the figure, we can see this process leading to the action sequence {a1,a2, a3, a8}, terminating at a black node representing a non-sparse region.\nMathematically, this process can be described as taking k actions {a1, a2,..., ak} such that $a_i$ = arg max Q(bi, a) and $b_{i+1}$ = $\\tau(b_i, a_i, o_i)$; the constraints we impose for termination are P($b_{i+1}$) < $P_{\\epsilon}$ or k \u2264 max_level, where \u03c4(\u0432, \u0430, \u043e) represents the belief update function given action a and observation o. The sequence terminates when either P($b_{k+1}$) > $P_{\\epsilon}$ or k = max_level, as illustrated in fig. 3 where the sequence ends at a black node (non-sparse region). This approach efficiently guides the agent towards promising areas while avoiding the oscillation often seen in standard MCTS implementations.\nThis approach allows the algorithm to dynamically shrink the decision space by focusing on actions that lead to non-sparse regions, effectively guiding the agent towards areas of the state space with higher certainty. By doing so, the Shrinking POMCP algorithm can potentially overcome the limitations of traditional POMDP solvers in environments with vast or sparse state spaces, leading to more efficient and effective planning in partially observable domains.\nRollout In Monte Carlo planning, rollouts present a significant computational challenge. These rollouts are essential for approximating the value of leaf nodes within the search tree using a computationally inexpensive strategy. Our approach employs the A* algorithm [28] as the rollout policy, balancing efficiency and effectiveness in pathfinding. While each individual rollout is computationally inexpensive, the cumulative cost of performing thousands of rollouts for each decision becomes a significant bottleneck. To address this challenge and improve overall performance, we implement the A* algorithm using C programming language [29]. We found that this approach led to better results than standard random rollouts.\nEach belief node s in our tree structure encapsulates information about the quadrotor's position p(s). This positional information is represented at two distinct levels of granularity:\n1) Fine-grain level: The exact position on a high-resolution map of dimensions L \u00d7 L.\n2) Discretized level: A coarser N \u00d7 N grid, where each cell corresponds to a region of the fine-grain map.\nWhile the Monte Carlo Tree Search (MCTS) simulation operates on the discretized N \u00d7 N grid for computational efficiency, the rollout process necessitates a more precise position determination. Given a current state and action, we first identify the target cell in the discretized grid. The challenge then becomes determining the exact position within this target cell.\nTo address this, we employ a sampling-based approach that balances accuracy and computational efficiency. Let $c_t$ represent the set of all possible positions in the current cell, and $Y_t$ represent the set of all possible positions in the target cell. We define a sampling function S($\\gamma_t$) that returns a subset of positions from $Y_t$.\nFrom this sampled subset, we identify the set of valid positions V(S($\\gamma_t$)):\nV(S($\\gamma_t$)) = {$ \\gamma \\in S(\\gamma_t) | \\gamma$ is a valid position}                                                                                                                                                                              (11)\nThe next exact position $\\gamma_{next}$ is then determined by finding the position in V (S($\\gamma_t$)) that minimizes the Euclidean distance from the current position $\\gamma_{current}$:\n$\\gamma_{next} = arg min_{\\gamma \\in V(S(\\gamma_t))} || \\gamma - \\gamma_{current} ||$                                                                                                                                                                          (12)\nwhere || || denotes the Euclidean distance.\nOnce $\\gamma_{next}$ is determined, we calculate the rollout value by running the A* algorithm from $\\gamma_{current}$ to $\\gamma_{next}$, using the given obstacle map. Let L($\\gamma_{current}, \\gamma_{next}$) denote the path length returned by A*. The rollout value R is then computed as a function of this path length:\nR= f(L($\\gamma_{current}, \\gamma_{next}$))                                                                                                                                                                                               (13)\nwhere f is a monotonically decreasing function. This formulation ensures that shorter paths, indicating easier navigation, result in higher rollout values, while longer paths, suggesting more complex navigation, yield lower values."}, {"title": "C. Height Adjustment", "content": "Our planning approach initially assumes constant altitude but incorporates an adaptive height adjustment strategy to balance obstacle avoidance with smooth flight patterns, mitigating undesirable \"up and down\" motions due to system noise.\nThe quadrotor starts at altitude $h_{init}$. For each target cell $C_t$ in the planned path, we evaluate the number of valid positions Nv ($C_t$, h) at current height h:\nNv($C_t$, h) = |{$ \\gamma \\in C_t | \\gamma$ is valid at height h}|                                                                                                                                                                              (14)\nWe define an \"obstacle_tolerance_threshold\" \u03c4. If Nv($C_t$, h) \u2265 \u03c4, the waypoint's altitude remains unchanged. Otherwise, we incrementally adjust the height:\n$h_{new}$ = min($h + \\Delta h, h_{max}$)                                                                                                                                                                                                  (15)\nwhere $ \\Delta h$ is the height increment and $h_{max}$ is the maximum allowed height. This process repeats until Nv ($C_t$, $h_{new}$) \u2265 \u03c4 or $h_{new}$ = $h_{max}$.\nIf Nv ($C_t$, $h_{max}$) < \u03c4, we designate the cell as a no-fly zone and re-execute the Monte Carlo Tree Search (MCTS) algorithm for an alternative path. This strategy ensures safe obstacle avoidance while minimizing unnecessary altitude changes, resulting in smoother and more efficient flight trajectories."}, {"title": "V. ANALYSIS AND EVALUATION", "content": "We evaluate our proposed framework using both the AirSim-ROS2 simulator and the two-dimensional simulator described in section III-A and report the performance of Shrinking POMCP against baseline."}, {"title": "A. Environment Simulators", "content": "We test our approach in two simulation environments:\n1) 3D AirSim-ROS2 Simulator: This advanced environment incorporates the full functionality of our framework, including the Probabilistic World Model and Navigator components.\n2) Two-dimensional Simulator: In this simplified environment, we test our approach without considering altitude effects. The Probabilistic World Model and the Navigator are replaced with surrogate ROS2 nodes that provide equivalent functionality, allowing for efficient testing and validation of our algorithms."}, {"title": "B. Hyperparameters", "content": "The testing environment for this mission scenario is set within a 400m \u00d7 400m map, with a strict time constraint of 5 minutes to locate all targets. The quadrotor's initial altitude is set at 10 meters, providing a balance between coverage area and detail resolution.\nSeveral hyperparameters are adjusted to optimize the search strategy. The discount factor in the POMDP framework is tested with values of 0.8, 0.9, and 0.995, influencing the balance between immediate and future rewards. The \u03b1 value in the reward function discussed in section IV, which affects the weighting of different reward components, is varied among 0, 1, and 10. Shrinking MCTS runs 3000 iterations for every decision epoch, with the exploration parameter in UCT set to \u221a2. For vertical navigation, the height adjustment parameter dh (discussed in section IV-C) is set to 3m, with a maximum allowable altitude of 30m. These parameters collectively define the operational constraints and decision-making framework for the quadrotor's search mission."}, {"title": "C. Baselines", "content": "In our experiments, we evaluate the performance of our Shrinking POMCP approach against three baseline methods: Lawnmower algorithm, Greedy algorithm, and standard MCTS (POMCP without shrinking). We test these methods on two types of belief maps: Uniform belief map and Sparse belief map with one peak. All methods are compared on the same map, with identical no-fly zones and starting agent positions for each scenario to ensure a fair comparison.\nLawnmower Algorithm: This baseline performs a systematic search in non-zero belief areas. At the start of each episode, the agent moves to the nearest non-zero probability position on the probability map. It then executes a lawnmower pattern search, systematically covering the non-zero belief region until all targets are found. This method ensures complete coverage of the search area but may not be optimal in terms of efficiency.\nGreedy Algorithm: This approach makes local decisions based on immediate information. At each step, the agent selects its next position by choosing the adjacent cell with the highest value in the resized array. While this method can be effective in quickly identifying high-probability areas, it may suffer from getting trapped in local maxima.\nMonte Carlo Tree Search (MCTS): We implement the standard POMCP algorithm without our proposed shrinking approach as a baseline. This method makes decisions only for cardinal directions (WEST, SOUTH, EAST, NORTH), moving to the next adjacent cell in the discretized grid at each step. This baseline allows us to directly compare the performance gains achieved by our Shrinking POMCP approach."}, {"title": "D. Results (Two-Dimensional)", "content": "Belief Maps: Three types of initial belief distributions are tested in this scenario. The first is a Uniform Belief (fig. 4), where probabilities are evenly distributed across the entire map. The second is a Sparse Belief with One Peak (fig. 4), characterized by a high probability concentration at a single location that gradually diffuses outward. The third is a Sparse Belief with Three Peaks (fig. 4), featuring multiple"}, {"title": null}]}