{"title": "Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering", "authors": ["Ruofan Liang", "Zan Gojcic", "Merlin Nimier-David", "David Acuna", "Nandita Vijaykumar", "Sanja Fidler", "Zian Wang"], "abstract": "The correct insertion of virtual objects in images of real-world scenes requires a deep understanding of the scene's lighting, geometry and materials, as well as the image formation process. While recent large-scale diffusion models have shown strong generative and inpainting capabilities, we find that current models do not sufficiently \"understand\" the scene shown in a single picture to generate consistent lighting effects (shadows, bright reflections, etc.) while preserving the identity and details of the composited object. We propose using a personalized large diffusion model as guidance to a physically based inverse rendering process. Our method recovers scene lighting and tone-mapping parameters, allowing the photorealistic composition of arbitrary virtual objects in single frames or videos of indoor or outdoor scenes. Our physically based pipeline further enables automatic materials and tone-mapping refinement.", "sections": [{"title": "1 Introduction", "content": "Virtual object insertion enables a range of applications from virtual production,\nto interactive gaming and synthetic data generation. To produce photorealistic\ninsertions, the interactions between the virtual objects and the environment need\nto be modeled faithfully, such as accurate specular highlights and shadows.\nA standard virtual object insertion pipeline typically includes three key steps:\ni) lighting estimation from the input image, ii) 3D proxy geometry creation,\nand iii) composited image rendering in a rendering engine. However, the first\nand arguably most important step is still an open research question. Lighting\nestimation is particularly challenging when dealing with limited inputs such as\na single image, captured using a consumer device with a low dynamic range.\nIndeed, inverse rendering is a fundamentally ill-posed problem.\nTo constrain its solution space, prior works either aimed to define hand-crafted\npriors [9,18,31,78] or to learn them from the data [15\u201317,22,23,32,36,58,64,65,80].\nHowever, the former often fall short when applied to real-world scenes, while the\nlatter suffer from scarcity of the ground truth data. As a result, these algorithms\nare often heavily tailored to a specific domain, e.g. indoor [15\u201317,52,65] or outdoor\nscenes [22, 23, 58, 64,80]."}, {"title": "2 Related Work", "content": "Inverse rendering is the task of recovering intrinsic properties of a scene,\nincluding materials, shape, and lighting, from a single or multiple images [5]."}, {"title": "3 Preliminaries", "content": "Diffusion Models. Diffusion models are a family of generative models built\naround two key processes. A forward process, which gradually adds noise to data\nsamples $x \\sim p(x)$ removing their structure over time t. This is achieved using\na noise schedule determined by $a_t$ and $\\sigma_t$ as $x_t = a_tx + \\sigma_t\\epsilon$, $\\epsilon \\sim N(0, I)$. In\ncontrast, the reverse process gradually removes this noise, restoring the structure.\nThe reverse process is parameterized by a conditional neural network, $\\epsilon_\\theta$, trained\nto predict the noise $\\epsilon$ at a given timestep t according to the following simplified\nobjective [20]:\n$\\mathbb{E}_{x,\\epsilon \\sim N(0,1),t \\sim T} [w(t)|\\epsilon_\\theta(x_t, t, c) - \\epsilon||^2],$ (1)\nwhere c represents a condition (e.g. text, image, etc.) that allows controlling the\ngeneration process, w(t) represents a time-conditional weighting, and T is a set\ncontaining a selection of timesteps.\nIn this work, we use a Latent Diffusion Model (LDM) [48] in which the\ndiffusion process is conducted in a lower-dimensional latent space. Specifically,\nthe encoder $\\mathcal{E}$ maps samples from the data distribution $x \\sim p(x)$ into a latent\nspace $\\mathcal{Z}$. The decoder $\\mathcal{D}$ performs the inverse operation, such that $\\mathcal{D} (\\mathcal{E}(x)) \\approx x$.\nAs follows, for LDMs x in Eq. 1 is replaced by its latent $z = \\mathcal{E}(x)$.\nPersonalization and Fine-tuning. Fine-tuning all parameters of a pretrained\nDM requires significant computational resources and time. To alleviate this,\nLORA [24] injects trainable low-rank decomposition matrices and aims to learn\nonly the variations from the pretrained weights. Specifically, consider a linear\nlayer represented as $h = W_0x$. Here $W_0\\in \\mathbb{R}^{n \\times n}$ and $x \\in \\mathbb{R}^n$ are the pretrained"}, {"title": "4 Method", "content": "Given a single image as input, DiPIR recovers scene lighting and tone-mapping\nparameters, with the goal of photorealistic insertion of virtual objects. An overview\nof our method is shown in Fig. 2. Sec. 4.1 describes our representation and the\ndifferentiable rendering process, while Sec. 4.2 and Sec. 4.3 provide details on\ndiffusion model guidance and optimization formulation, respectively.\n4.1 Physically-based Virtual Object Insertion\nVirtual scene. Inserting a virtual object X into an image $I_{bg} \\in \\mathbb{R}^{h \\times w \\times 3}$\nrequires creating a 3D proxy virtual scene, viewed from the correct camera pose.\nHere, we assume that the user provides a specific placement (pose) for X, but in\nsome cases, an appropriate pose can also be determined automatically, e.g. by\ndetecting the floor plane and scene scale.\nTo model the effects of the inserted object on the original image, such as\nshadows cast by X, we also assume a known proxy geometry P. We found a\nsimple ground plane acting as a shadow catcher underneath the virtual object\nto be sufficient in all of our experiments. This proxy plane can be easily placed\nmanually or automatically generated based on e.g. depth or LiDAR data."}, {"title": "6 Applications", "content": "Since our method recovers physically based lighting information, arbitrary new\nvirtual objects can be inserted after the optimization, as shown in Fig. 2. DiPIR\ncan also optimize other scene attributes such as materials and local lighting. We\nperform preliminary experiments in this direction.\nMaterial optimization. When combined with differentiable rendering, DMs\ncan provide a guidance signal for material attributes, as shown in Fig. 7. Given a\npurely diffuse car and enabling Metallic and Roughness properties as optimizable\nparameters, the diffusion guidance can optimize and make the car look more\nshiny. By additionally changing the text prompt to \"a carmine red car\" and\nmaking the base color of the car an optimizable parameter, we show that the\nDM can propagate the text-condition to the PBR attribute and change the color\nof the car to red. When enabling local emission as an optimizable parameter, the\ndiffusion model can also turn on the headlights of cars in night scenes."}, {"title": "7 Discussion", "content": "Our method leverages large diffusion models' inherent scene understanding\ncapabilities as guidance to a physically based inverse rendering pipeline. We\ndesign a diffusion guidance signal with scene-specific personalization and a\ndifferentiable inverse rendering pipeline to recover lighting and tone-mapping\nparameters. Our method enables inserting virtual objects into scenes, but also\noptimizing other scene parameters such as the materials of the inserted object\nor account for tone-mapping mismatches between cameras. We believe that this\ncombination of the differentiable rendering process and data-driven priors can be\nused successfully in many other content creation applications such as relighting\nand animation.\nLimitations and future work. Our Spherical Gaussians-based lighting repre-\nsentation is adequate for general objects [35], but might not behave realistically\nfor highly specular materials. For more complex lighting representations adding\ngenerative priors on the environment map [41] is a direction worth exploring. The\nrendering formulation could be extended to account for effects such as reflections\nfrom the scene itself onto the inserted object (e.g. color bleeding), but that\nmight introduce more ambiguities and require knowing the materials of the proxy\ngeometry (refer to Supplement C.4 for failure case examples). Finally, while DM\npersonalization significantly improves the quality of the results, it adds overhead\nand complexity to the pipeline. Recent personalization methods that do not\nrequire test-time finetuning [54] could be used to mitigate this overhead."}]}