{"title": "Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering", "authors": ["Ruofan Liang", "Zan Gojcic", "Merlin Nimier-David", "David Acuna", "Nandita Vijaykumar", "Sanja Fidler", "Zian Wang"], "abstract": "The correct insertion of virtual objects in images of real-world scenes requires a deep understanding of the scene's lighting, geometry and materials, as well as the image formation process. While recent large-scale diffusion models have shown strong generative and inpainting capabilities, we find that current models do not sufficiently \"understand\" the scene shown in a single picture to generate consistent lighting effects (shadows, bright reflections, etc.) while preserving the identity and details of the composited object. We propose using a personalized large diffusion model as guidance to a physically based inverse rendering process. Our method recovers scene lighting and tone-mapping parameters, allowing the photorealistic composition of arbitrary virtual objects in single frames or videos of indoor or outdoor scenes. Our physically based pipeline further enables automatic materials and tone-mapping refinement.", "sections": [{"title": "1 Introduction", "content": "Virtual object insertion enables a range of applications from virtual production, to interactive gaming and synthetic data generation. To produce photorealistic insertions, the interactions between the virtual objects and the environment need to be modeled faithfully, such as accurate specular highlights and shadows.\nA standard virtual object insertion pipeline typically includes three key steps: i) lighting estimation from the input image, ii) 3D proxy geometry creation, and iii) composited image rendering in a rendering engine. However, the first and arguably most important step is still an open research question. Lighting estimation is particularly challenging when dealing with limited inputs such as a single image, captured using a consumer device with a low dynamic range. Indeed, inverse rendering is a fundamentally ill-posed problem.\nTo constrain its solution space, prior works either aimed to define hand-crafted priors [9,18,31,78] or to learn them from the data [15-17,22,23,32,36,58,64,65,80]. However, the former often fall short when applied to real-world scenes, while the latter suffer from scarcity of the ground truth data. As a result, these algorithms are often heavily tailored to a specific domain, e.g. indoor [15-17,52,65] or outdoor scenes [22, 23, 58, 64,80]."}, {"title": "2 Related Work", "content": "Inverse rendering is the task of recovering intrinsic properties of a scene, including materials, shape, and lighting, from a single or multiple images [5]."}, {"title": "3 Preliminaries", "content": "Diffusion Models. Diffusion models are a family of generative models built around two key processes. A forward process, which gradually adds noise to data samples $x \\sim p(x)$ removing their structure over time t. This is achieved using a noise schedule determined by $a_t$ and $\\sigma_t$ as $x_t = a_tx + \\sigma_t\\epsilon$, $\\epsilon \\sim N(0, I)$. In contrast, the reverse process gradually removes this noise, restoring the structure. The reverse process is parameterized by a conditional neural network, $\\epsilon_\\theta$, trained to predict the noise $\\epsilon$ at a given timestep t according to the following simplified objective [20]:\n\n$\\mathbb{E}_{x \\sim p(x), \\epsilon \\sim N(0,1), t \\sim T} [w(t)||\\epsilon_\\theta(x_t, t, c) - \\epsilon||_2]$,\n\nwhere c represents a condition (e.g. text, image, etc.) that allows controlling the generation process, w(t) represents a time-conditional weighting, and T is a set containing a selection of timesteps.\nIn this work, we use a Latent Diffusion Model (LDM) [48] in which the diffusion process is conducted in a lower-dimensional latent space. Specifically, the encoder E maps samples from the data distribution $x \\sim p(x)$ into a latent space Z. The decoder D performs the inverse operation, such that $D (E(x)) \\approx x$. As follows, for LDMs x in Eq. 1 is replaced by its latent $z = E(x)$.\nPersonalization and Fine-tuning. Fine-tuning all parameters of a pretrained DM requires significant computational resources and time. To alleviate this, LORA [24] injects trainable low-rank decomposition matrices and aims to learn only the variations from the pretrained weights. Specifically, consider a linear layer represented as $h = W_0x$. Here $W_0\\in \\mathbb{R}^{n \\times n}$ and $x \\in \\mathbb{R}^n$ are the pretrained"}, {"title": "4 Method", "content": "Given a single image as input, DiPIR recovers scene lighting and tone-mapping parameters, with the goal of photorealistic insertion of virtual objects. An overview of our method is shown in Fig. 2. Sec. 4.1 describes our representation and the differentiable rendering process, while Sec. 4.2 and Sec. 4.3 provide details on diffusion model guidance and optimization formulation, respectively.\n4.1 Physically-based Virtual Object Insertion\nVirtual scene. Inserting a virtual object X into an image $I_{bg} \\in \\mathbb{R}^{h \\times w \\times 3}$ requires creating a 3D proxy virtual scene, viewed from the correct camera pose. Here, we assume that the user provides a specific placement (pose) for X, but in some cases, an appropriate pose can also be determined automatically, e.g. by detecting the floor plane and scene scale.\nTo model the effects of the inserted object on the original image, such as shadows cast by X, we also assume a known proxy geometry P. We found a simple ground plane acting as a shadow catcher underneath the virtual object to be sufficient in all of our experiments. This proxy plane can be easily placed manually or automatically generated based on e.g. depth or LiDAR data."}, {"title": "4.2 Diffusion Guidance", "content": "The composited image produced by our differentiable rendering pipeline serves as the input to a DM that is used to compute a guidance signal, employing an optimization objective similar to Score Distillation Sampling (SDS) [46]. However, while DMs inherently have robust priors for lighting, we found that they do not provide out-of-the-box the necessary guidance for our specific needs. Consequently, we propose an adaptive score distillation loss specifically designed for object insertion tasks that exploits a personalization strategy which we detail in the following section.\nPersonalization with concept preservation. Off-the-shelf DMs often do not provide robust guidance for virtual object insertion, especially in out-of- distribution scenes such as outdoor driving environments. A potential solution, inspired by [59], is to adapt the DM using an image from the target scene. In our experience, however, this approach often resulted in too much overfitting to the target scene's content, reducing the model's ability to adapt to the scene with a newly inserted object. This led to artifacts and an unstable optimization process."}, {"title": "4.3 Optimization Formulation", "content": "After completing the diffusion model's personalization, we optimize the lighting and tone-mapping parameters using the following loss function:\n\n$L = L_{LDS} + \\lambda_{consistency} L_{consistency} + \\lambda_{reg}L_{reg}$.\n\nThe loss' main component is the diffusion guidance $L_{LDS}$, which provides a perceptual realism objective on the edited result with object insertion. We use the text prompt \u201ca photo of a {concept class} in a scene in the style of sks rendering", "ways": "i) encouraging lighting consistency of the foreground object and the scene, such as the reflections and scale of the inserted object; and (ii) encouraging accurate shadows cast by the inserted object onto the background scene, such as the scale, direction, and color of the shadow. However, we empirically observe that these two signals can conflict in the early phase of the optimization.\nTo address this, we initialize two separate environment maps, $L_{fg}, L_{shadow} \\in \\mathbb{R}^{H \\times W \\times 3}$, to light the foreground inserted object (Eq. 4) and cast shadows (Eq. 5) respectively. As the optimization progresses, the two environment maps are progressively fused into a single environment map $L_{fused}$ through scaling the the $L_{fs}$ by the relative luminance of between $L_{fg}$ and $L_{shadow}$. We also use the following two regularization terms for this fusion process. First, we encourage the consistency between the normalized luminance $\\hat{L}^{fg}, \\hat{L}^{shadow} \\in \\mathbb{R}^{H \\times W}$ of the environment maps by minimizing:\n\n$L_{consistency} = \\sum_{i,j} \\hat{L}_{shadow} \\log(\\frac{\\hat{L}_{fg}}{\\hat{L}_{shadow}}) \\Delta \\Omega_{ij}$,\n\nwhere $\\Delta \\Omega_{ij}$ is the corresponding solid angle at pixel (i,j) and the gradient for $\\hat{L}^{shadow}$ is detached. Second, as the shadow environment map $L_{shadow}$ is supervised mainly via the shadow ratio $\\beta_{shadow} \\in \\mathbb{R}^{H \\times W \\times 3}$ of Eq. 5, to encourage concentrated high peaks for the sharp shadows and suppress the ambient light in $L_{shadow}$, we also add a L2 regularizer in log-space with the Cauchy loss [7]:\n\n$L_{reg} = \\sum_{i,j,c} \\log (1+2 \\frac{(L_{shadow})^2}{\\gamma^2})$,"}, {"title": "5 Experiments", "content": "In this section, we evaluate DiPIR on a collection of outdoor and indoor scenes, covering diverse lighting conditions and different application domains. We first outline the experiment settings in Sec. 5.1 and then provide evaluation results on benchmark datasets in Sec. 5.2. Finally, we conduct an ablation study in Sec. 5.3.\n5.1 Experiment Settings\nWaymo dataset. Following the data split from [69], we use 48 scenes with diverse illumination conditions from the Waymo Open Dataset [57] including 32 daytime (17 sunny and 15 cloudy), 9 twilight, and 7 night scenes. Each scene consists of an input image, a 3D car asset randomly selected from an asset bank, the ground plane, and a location to insert the car. Inspired by applications in synthetic data generation, this process is fully automatic: we use the LiDAR point cloud and semantic segmentation [60] to fit a ground plane and detect the empty space to insert a car.\nPolyHaven dataset. We use 11 HDR environment maps from PolyHaven [1] and manually place a known ground plane and virtual object in each scene. Each"}, {"title": "6 Applications", "content": "Since our method recovers physically based lighting information, arbitrary new virtual objects can be inserted after the optimization, as shown in Fig. 2. DiPIR can also optimize other scene attributes such as materials and local lighting. We perform preliminary experiments in this direction.\nMaterial optimization. When combined with differentiable rendering, DMs can provide a guidance signal for material attributes, as shown in Fig. 7. Given a purely diffuse car and enabling Metallic and Roughness properties as optimizable parameters, the diffusion guidance can optimize and make the car look more shiny. By additionally changing the text prompt to \"a carmine red car\" and making the base color of the car an optimizable parameter, we show that the DM can propagate the text-condition to the PBR attribute and change the color of the car to red. When enabling local emission as an optimizable parameter, the diffusion model can also turn on the headlights of cars in night scenes."}, {"title": "7 Discussion", "content": "Our method leverages large diffusion models' inherent scene understanding capabilities as guidance to a physically based inverse rendering pipeline. We design a diffusion guidance signal with scene-specific personalization and a differentiable inverse rendering pipeline to recover lighting and tone-mapping parameters. Our method enables inserting virtual objects into scenes, but also optimizing other scene parameters such as the materials of the inserted object or account for tone-mapping mismatches between cameras. We believe that this combination of the differentiable rendering process and data-driven priors can be used successfully in many other content creation applications such as relighting and animation.\nLimitations and future work. Our Spherical Gaussians-based lighting repre- sentation is adequate for general objects [35], but might not behave realistically for highly specular materials. For more complex lighting representations adding generative priors on the environment map [41] is a direction worth exploring. The rendering formulation could be extended to account for effects such as reflections from the scene itself onto the inserted object (e.g. color bleeding), but that might introduce more ambiguities and require knowing the materials of the proxy geometry (refer to Supplement C.4 for failure case examples). Finally, while DM personalization significantly improves the quality of the results, it adds overhead and complexity to the pipeline. Recent personalization methods that do not require test-time finetuning [54] could be used to mitigate this overhead."}, {"title": "A Diffusion Personalization and Score Distillation", "content": "An intuitive approach to understanding the diffusion guidance is to directly visualize the text-to-image generation result of the diffusion model. In this section, we provide additional analysis and ablative visualization on our design choices of LORA personalization (Fig. 8) and concept preservation (Fig. 9).\nPersonalizing diffusion model. Due to the high stochasticity in the diffusion denoising process, the images generated by a pre-trained diffusion model often cannot be tailored to a specific input image. However, in the setting of using the diffusion model for solving the inverse rendering problem of the given scene, it is important to preserve the key context (e.g., shapes, lighting, and shadowing effects) from the unseen input background image. In Fig. 8, we visualize the effect of LORA personalization. After personalization, the diffusion model can generate images in a similar domain to the target scene."}, {"title": "C Additional Results", "content": "In this section, we provide further experimental details and additional results.\nC.1 User Study\nUser study is a standard approach for assessing perceptual realism of virtual object insertion [15-17,64,66]. Following prior works, we conduct a user study on Amazon Mechanical Turk to compare with prior methods and ablate our design choices.\nUser interface. Participants receive a pair of two object insertion results: one generated using our proposed method, the other one using a baseline approach. Participants are instructed to evaluate the differences between the two images, focus on the lighting effects of the inserted objects, and select the image they deemed to be more realistic:"}]}