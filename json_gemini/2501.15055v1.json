{"title": "GROUP LIGANDS DOCKING TO PROTEIN POCKETS", "authors": ["Jiaqi Guan", "Jiahan Li", "Xiangxin Zhou", "Xingang Peng", "Sheng Wang", "Yunan Luo", "Jian Peng", "Jianzhu Ma"], "abstract": "Molecular docking is a key task in computational biology that has attracted increasing interest from the machine learning community. While existing methods have achieved success, they generally treat each protein-ligand pair in isolation. Inspired by the biochemical observation that ligands binding to the same target protein tend to adopt similar poses, we propose GROUPBIND, a novel molecular docking framework that simultaneously considers multiple ligands docking to a protein. This is achieved by introducing an interaction layer for the group of ligands and a triangle attention module for embedding protein-ligand and group-ligand pairs. By integrating our approach with diffusion-based docking model, we set a new S performance on the PDBBind blind docking benchmark, demonstrating the effectiveness of our proposed molecular docking paradigm.", "sections": [{"title": "1 INTRODUCTION", "content": "The 3D binding structures of molecular ligands and protein pockets are not only fundamental to unraveling the intricate protein-ligand interactions but also play a pivotal role in rational drug design. In this context, the molecular docking task, which involves predicting the optimal binding configuration between a ligand and a protein, stands as a cornerstone in modern computational biology. Conventional computational approaches leveraging the pre-defined score functions to search or optimize a low-energy pose configuration (Halgren et al., 2004; Trott & Olson, 2010) are usually slow and inaccurate, which limits their applicability in real-world drug discovery scenarios. In recent years, deep learning-based algorithms (St\u00e4rk et al., 2022; Lu et al., 2022; Zhang et al., 2022; Corso et al., 2022) have showcased remarkable success in accurately predicting the protein-ligand binding structures, which brings new breakthroughs in the field of molecular docking.\nWhile these deep learning methods vary in modeling architectures and objectives, they all treat each protein-ligand pair individually. Given the limited number of well-studied protein targets compared to the vast space of molecules, many ligands share common protein targets in available databases. Exploiting the correlated information among these complexes could enhance docking performance.\nIn this work, we leverage a key biochemical insight: if multiple ligands can bind to the same protein pocket, their docked 3D structures are likely to be similar (Paggi et al., 2021)."}, {"title": "2 RELATED WORK", "content": "Molecular Docking. Molecular docking aims at predicting the binding structure of a small molecule ligand to a protein, which is a critical technique in drug discovery and enables understanding of the intermolecular interactions and potential binding modes (Ferreira et al., 2015). Traditional molecular docking approaches usually design scoring functions and then adopt search, sampling, or optimization algorithms (Shoichet et al., 1992; Meng et al., 1992; Trott & Olson, 2010). The integration of deep learning techniques has seen advancements in enhancing scoring functions (Ragoza et al., 2017; McNutt et al., 2021; M\u00e9ndez-Lucio et al., 2021). Recently, with the development of geometric deep learning, direct prediction of the binding pose has become an emerging paradigm and achieved promising results. As representative examples, TANKBind (Lu et al., 2022) and Uni-Mol (Zhou et al., 2023) predict protein-ligand distance map and using post-optimization algorithms to recover the pose from the predicted distance map, while EquiBind (St\u00e4rk et al., 2022) and E3Bind (Zhang et al., 2023) directly predict the coordinates of the ligand binding pose. FABind (Pei et al., 2024) and FAbind+(Gao et al., 2024) enhance docking accuracy by directly introducing a pocket prediction module. Among these, Corso et al. (2022) framed molecular docking as a generative modeling problem and proposed DiffDock, a diffusion model (Ho et al., 2020; Song et al., 2021) that learns the distribution over the product space of the ligand's translation, rotation, and torsion, along with a confidence model to pick up the final prediction from sampled ligand poses. Although recent physics-based docking methods (Paggi et al., 2021; McNutt & Koes, 2024) have utilized similar binding molecules or poses to enhance docking accuracy, our proposed GroupBind is the first deep learning-based method capable of integrating with existing docking frameworks and improving performance by leveraging group binding data.\nConsistency in Data. Consistency is a common nature of biological data. This essential feature provides us insights to better understand function, structure, and evolution in biology. Conserved sequences, maintained by natural selection, are identical or similar sequences in nucleic acids or proteins across species, and many multiple sequence alignment (MSA) algorithms, such as ProbCons (Do et al., 2005) and CONTRAlign (Do et al., 2006), are developed to reveal such patterns. Liao et al. (2009); Hashemifar et al. (2016) developed effective protein-protein interaction (PPI) network alignment algorithms, which help identify conserved subnetwork and thus enable accurate identification of functional orthologs across species. Consistency also exists in other domains beyond biology and such methodology of mining consistency from data has also been extensively studied, e.g., network alignment (i.e., finding the node correspondence across different networks) which is of great value in social networks analysis and web mining (Zhang & Tong, 2016; Zhang et al., 2021).\nConsistency-Inspired Models. By virtue of its ubiquity and usefulness, consistency serves as a prior knowledge that plays crucial roles in various areas. Xiao & Quan (2009) and Hu et al. (2020)"}, {"title": "3 METHOD", "content": "In this section, we present GROUPBIND, a new and effective paradigm that leverages other ligands binding to the same protein pocket to improve single ligand docking accuracy, where we dock group ligands to this pocket at the same time. In Section 3.1, we first define notations and briefly recap the diffusion model for molecular docking, which we will integrate our paradigm into as an example in this work. Then, we introduce the modeling objective of GROUPBIND in Section 3.2. In Section 3.3, we illustrate how we parameterize GROUPBIND with message passing across group ligands and triangle attention between group ligands and proteins. Finally, in Section 3.4, we describe the auxiliary distance prediction during training and the confidence model for group ligands."}, {"title": "3.1 PRELIMINARIES", "content": "Problem Definition and Notation. In the molecular docking task, we are provided with a protein (pocket) P with Np residues and a ligand L with n atoms. Our goal is to find the docking pose x \u2208 R\u00b3n of the ligand in this binding site. Following previous work (Lu et al., 2022; Corso et al., 2022), we represent a protein pocket as a residue-level proximity 3D graph Gp = {VP,Ep}, where Vp denotes the residue vertex set consisting of residue vertex features VP \u2208 RNpXNa (such as amino acid types) and Ca atom positions of residues xp \u2208 RNP\u00d73, and Ep denotes the edge set which is built upon residue 3D proximity. Similarly, the small molecule ligand is also represented as a 3D graph GL = {VL, EL}. Since bond lengths, angles, and small rings in the ligand can be regarded as rigid structures, the torsion angles almost determine the flexibility of ligand conformations x. Thus, we can also define the space of ligand poses on a submanifold Mc \u2208 Rm+6 instead of R3n given the seed conformation c (Corso et al., 2022), where m is the number of rotatable bonds in the ligand and the six additional degrees of freedom correspond to the roto-translation relative to the protein pocket.\nDiffusion Model for Molecular Docking. Recently, DiffDock (Corso et al., 2022) proposed to formulate molecular docking as a generative task, with the objective of modeling the conditional distribution p(g|GP,GL) and g \u2208 Mc. For a given seed conformation c, we can define a bijection P\u2192 Mc, where P = T(3) \u00d7 SO(3) \u00d7 SO(2)m is the production space of ligand translation r, rotation R, and torsion angles 0. Leveraging recent progress of diffusion models on Riemannian manifolds (De Bortoli et al., 2022), we can define the forward diffusion independently in each manifold, resulting in a direct-sum tangent space: T\u201eP = TrT3 + TRSO(3) \u2295 T\u0189SO(2)m \u2243 R3 \u2295R3 \u2295RM. Based on this formulation, we can train the diffusion model by score matching on translational scores, rotational scores and torsional scores, and sampling ligand poses by running the reverse diffusion process (Song et al., 2021)."}, {"title": "3.2 MODELING GROUP LIGANDS", "content": "Existing deep learning-based models for molecular docking consider each protein-ligand pair individually. However, one important biological observation suggests that when multiple ligands are capable of binding to a common protein pocket, the resulting docking poses are expected to exhibit a significant degree of similarity. To include this inductive bias, we propose to model the docking poses of a group of ligands given the common protein pocket, instead of modeling them individually. Specifically, we are docking a ligand GLo along with k other ligands of interest GL\u2081,...,GLK to a shared protein pocket Gp. The K other ligands GL1:K could be the ones whose docking poses we are also interested in, or retrieved from some database as the augmented ligands to help us dock GLo. Importantly, we do not utilize the binding structural information of augmented ligands GL1:K during the inference phase, but only leverage these ligands known to bind to the same protein to improve single protein-ligand docking performance. Under this formulation, our modeling objective becomes the product space Pg of translation, rotation and torsion angles of this group of ligands, i.e.\n$$p ((ro:K, Ro:\u03ba, 00:K)|GP, GL0:K)$$,\nwhere Pg = T(3)(K+1) \u00d7 SO(3)(K+1) \u00d7 SO(2)\u03a3=0 mk.\nConsidering the effectiveness of generative models in molecular docking, such as diffusion models (Corso et al., 2022), we choose to adopt a diffusion framework to model the binding distribution. However, our modules can be integrated into other frameworks that use pairwise representations of molecules. The key technical challenge lies in enabling each ligand to become aware of other ligands in the group to improve docking performance. In the following, we will explain how we address this by constructing graphs among group ligands and introducing novel network architectures that allow the model to account for interactions within the group."}, {"title": "3.3 PARAMETERIZATION OF GROUPBIND", "content": "Construct Ligand Group. Ligands are grouped together when they share the same protein binding pocket, as outlined in Algorithm 1 in Appendix. We group complexes based on the protein's UniProt ID and name provided in the original PDBBind dataset. To align proteins within the same group, we use the Kabsch algorithm (Kabsch, 1976) to compute the optimal roto-translation based on the longest common subsequence of amino acids. The reference protein is chosen as the one with the lowest average root-mean-square deviation (RMSD) from others in the group. The resulting optimal rigid transformations are applied to both the protein and ligand in other complexes. To ensure high-quality training data, we also filter out complexes with protein-ligand minimum distances that are either too close or too far.\nBuild Group Ligands Graph. To facilitate the exchange and aggregation of useful docking information, we construct a graph across ligands in the group by placing all group ligands in the same coordinate system and adding edges between different ligands. This design facilitates the exchange and aggregation of useful docking information among ligands.. A straightforward approach to constructing the group ligands graph is to dynamically build a radius graph or k-NN graph based on the diffused group ligands, which are the ligands with added noise during the diffusion process. This is similar to how the protein-ligand heterogeneous graph is constructed. However, in scenarios where the noise level is high, the edges formed based on 3D proximity relative to the protein may inadvertently link two atoms in different ligands with entirely distinct interactions with the protein pocket. In such cases, the message passing process among group ligands might not yield helpful insights and, in fact, could have adverse effects. Conversely, considering the computational resource constraints, creating a fully-connected graph for group ligands and discerning which connections are pertinent from the data is also impractical. Given that ligands binding to the same protein pocket typically exhibit some degree of structural similarity, we propose an approach to construct the group ligands graph that is independent of the protein pocket. This independence implies that the construction of the group ligands graph is solely based on the structural and 3D geometry similarities among ligands. To achieve this, we initially conduct 2D molecular graph matching, establishing a one-to-one mapping through the Maximum Common Substructure (MCS) (Raymond & Willett, 2002). For those unmatched atoms, we build edges by connecting their neighbors within 4\u00c5 if these ligands are presented as training data. Otherwise, i.e. in the inference phase, we perform 3D matching (Wildman & Crippen, 1999) on generated molecular conformers and similarly connect neighbors within a 4\u00c5 radius. The edges among group ligands are independent of the diffusion time, and since the C-C bond length is about 1.5\u00c5. The 4\u00c5 is chosen by assuming each atom can interact with corresponding 2-hop neighbors.\nMessage Passing across Group Ligands. To make group ligands aware of each other, we introduce a message passing mechanism across group ligands based on the aforementioned group ligands graph. Similar to DiffDock, we utilize irreducible representations (irreps) of SO(3) (Thomas et al., 2018; Fuchs et al., 2020) to construct the message passing layer, implemented with the e3nn library (Geiger & Smidt, 2022). Denoting the group ligands graph as A, the node features as hj and hk, where j, k represent atoms from two different ligands, with k being in the neighborhood of j within the group ligands graph, i.e. {j \u2208 VL,, k \u2208 VL,,| g \u2260 g', (j, k) \u2208 Ex}, the message passing layer for each ligand atom j within group ligands at l-th layer is then defined as follows:\n$$Dah_{j}^{(l)}= BN(\\frac{1}{|NA(j)|} \\sum_{k\u2208NA(j)} \u03c8_{jk} ) $$,\n$$\u03c8_{jk} = \u03a6e([he_{j}^{(l)}, h_{k}^{(l)}, e_{jk}]),$$\nwhere BN indicates the batch normalization layer, Y (rjk) are the spherical harmonics of edge vector rjk up to l = 2, and wjk denotes the spherical tensor product with path weights Vjk, which is obtained through a MLP de taking as input the edge embedding ejk and scalar node features. The node feature update he will be combined with other updates derived from the base docking neural network to obtain the updated the ligand node feature h+1).\nTriangle Attention Between Group Ligands and Protein. While the above message passing mechanism establishes connections between different ligands within the group, it lacks consideration for the interaction consistency with the protein. Inspired by AlphaFold2 (Jumper et al., 2021), we introduce a triangle attention module upon pair embeddings to incorporate geometric consistency. It is worth noting that Lu et al. (2022); Zhang et al. (2022) also employ triangle attention modules to enforce geometric constraints. However, a key distinction lies in our attention module operates across different ligands, reflecting a fundamentally different underlying motivation: if atom j in ligand Lg can interact with amino acid i, then the corresponding atom k in a similar ligand Lg' should also interact with amino acid i.\nTo incorporate this idea, we first construct pair representations zij for protein-ligand edges, and gjk for group ligand-ligand edges, which are both obtained from MLPs with scalar node features and edge embeddings:\n$$z_{ij}^{(l)} = \u03a6_{1} ([h_{i}^{(l)}, h_{j}^{(l)}, e_{ij}]),$$\n$$Sjk^{(l)} = \u03a6_{9} ([h_{j}^{(l)}, h_{k}^{(l)}, e_{jk}]).$$\nWhen ligand atoms j and k exhibit structural similarity, it is expected that they would engage in similar interactions with any protein atom i. To capture this, we implement a triangle multi-head"}, {"title": "3.4 TRAINING AND INFERENCE", "content": "Auxiliary Distance Prediction. By constructing eij as a hidden variable, we can encode more structural information into it, not merely using it as an edge feature. We propose to predict the binned protein-ligand distances from the pair representations during the training phase, i.e. dj = $a(e). It provides the supervised signal to make pairwise features encode specific geometric information. Since in this work, we integrate GroupBind with the DiffDock framework, the auxiliary training loss becomes a weighted cross entropy loss based on the diffusion time t:\n$$L_{dist} =  \\frac{1}{N_{pairs}} \\sum_{i,j} \\sum_{b=1}^{B} -e^{-d}log d_{j}$$,\nwhere B is the total number of distance bins.\nConfidence Model. Following DiffDock, we also train a confidence model to perform ranking on sampled ligand poses. We run the trained diffusion model to collect a set of candidate poses for each target protein and generate the binary label by checking whether the ligand RMSD is less than 2 \u00c5. Notably, unlike DiffDock, our confidence model involves ranking a group of ligand poses that can bind to the same protein. In the preliminary experiments, we observed that training the confidence model to directly predict the group score did not yield optimal results. Instead, training the confidence model to predict individual scores and subsequently computing the group score by averaging them proved more effective, i.e. the group confidence is\n$$C(X0,..., XK) = \\frac{1}{K} \\sum_{k=0}^{K} \u03a6_{e}(Xk)$$,\nwhere e is the confidence model, and we rank ligand poses (x0,...,xk) as a group by the group confidence c(x0, ..., XK) instead of ranking them individually."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nDataset. We evaluate our method on PDBBind v2020 dataset (Liu et al., 2015), which contains structures of 19443 protein-ligand complexes collected from PDB (Berman et al., 2003). We use the same processed proteins as St\u00e4rk et al. (2022), which uses reduce\u00b2 to correct and add missing receptor hydrogens. We use the same time-based split following previous work (St\u00e4rk et al., 2022; Lu et al., 2022; Corso et al., 2022), resulting in 17k complexes from 2018 or earlier for training/validation and 363 complexes from 2019 with no ligand overlap for testing.\nBaselines. We compare our model with search-based methods SMINA Koes et al. (2013), GN-INA McNutt et al. (2021), and various state-of-the-art deep learning methods EquiBind St\u00e4rk et al. (2022), TANKBind Lu et al. (2022) and DiffDock Corso et al. (2022). For searching-based methods SMINA and GNINA, we also test their variants by combining them with P2Rank or EquiBind to identify the initial binding site first.\nGroupBind. We test the performance of GroupBind under different settings, denoted as different postfixes: 1. Ref: Utilizes the native protein pocket, where the pocket center is defined as the centroid of the centers of mass (CoMs) of the group ligands. 2. P2R: Uses the top-3 P2Rank predicted pockets. For each pocket, we sample the same number of ligand poses and apply our confidence model to select the top-K ligands across all poses. 3. S(G): Groups ligands within the test set for self-augmentation, denoted as self-augment variant (\u201c-S\u201d). The 363 test ligands form 86 groups (255 ligands) with more than one ligand in each group, and 108 groups with single ligand. 4. A(G): Expands the augmented ligand database using ligands from the training set, allowing for the augmentation of 32 out of the 108 single ligands. 5. N(G): No augmented ligand database is used, only the original ligands are utilized for docking."}, {"title": "4.2 IMPROVING DOCKING PERFORMANCE WITH GROUPBIND", "content": "Following prior work St\u00e4rk et al. (2022); Lu et al. (2022); Corso et al. (2022), in assessing the quality of the generated complexes, we calculate the permutation symmetry corrected Meli & Biggin (2020)"}, {"title": "4.3 EFFECT OF AUGMENTED LIGANDS", "content": "We train GroupBind with the maximum number of ligands in each group as 5. However, we can assign a different maximum number of ligands during the sampling phase. In Figure 4b, we investigate the influence of using different numbers of augmented ligands in sampling. Similarly, we report the perfect selection results to isolate the influence of the confidence model. We can see that with augmented ligands, the success rate clearly improves, no matter the augmented ligands come from the test set itself (SG) or the whole dataset (AG). The optimal number of augmentation ligands does not strictly match the ones we used during training. Upon closer inspection of the structural similarity within each group, we find a subtle correlation between group similarity and the best ligand RMSD. This suggests that combining dissimilar ligands proves advantageous for our model in discerning correct interactions with proteins, ultimately yielding more accurate docking poses. Consequently, this highlights the pivotal role of the quality of augmentation ligands over their sheer quantity in enhancing the overall performance of our model. In Figure 4c, the presence of dissimilar ligands binding to the same pocket provides valuable information to our model, leading to improved docking accuracy and generally lower RMSD values (Spearman correlation = -0.39). This suggests that grouping dissimilar ligands is advantageous for our model in discerning correct interactions with proteins, ultimately yielding more accurate docking poses. In Figure 4d, we split the test set"}, {"title": "4.4 ABLATION STUDIES", "content": "To investigate the effectiveness of each component in GroupBind, we perform the ablation study as shown in Table 2. We employ a smaller neural network in both DiffDock and GroupBind variants, and the results of perfect selection under 10 samples are reported to isolate the influence of the confidence model. See more experimental setup details and full results in the Appendix Section C. It can be seen that each component can improve the docking performance substantially. One surprising thing is that even though the introduction of the auxiliary distance loss is straightforward, it can further improve the docking performance dramatically, proving the effectiveness of providing supervised signals to guide the learning of pairwise embedding."}, {"title": "4.5 DOCKING LIGANDS TO NEW PROTEINS", "content": "As our method relies on ligands from the same binding group, we considered cases where no binding information is available for a new protein. In such cases, ligands binding to homologous proteins can be used as an alternative. To simulate this, we used FoldSeek (Van Kempen et al., 2024) (E-value <1e-5) to identify homologous proteins and collected ligands (Tanimoto similarity > 0.5) binding to them for proteins without known ligands in PDBBind. These ligands were used as augmented inputs in our method. If neither ligands from the same protein nor homologous proteins are available, the method defaults to a diffusion-based docking model. Results in Table 3 demonstrate improved docking performance, showing our method's flexibility across various scenarios."}, {"title": "5 CONCLUSION", "content": "We introduce GROUPBIND for simultaneously docking multiple ligands to protein pockets, a novel paradigm for the molecular docking task grounded in sound physical intuition. We develop a novel model equipped with specialized modules to capture the consistency in protein-ligand interactions, holding the potential for seamless integration with diverse deep learning-based docking frameworks. Empirical results demonstrate the efficacy of our paradigm and the augmented group ligands. The limitation of our model lies in the high computation cost of the triangle attention module, rendering it impractical for constructing a relatively dense graph between group ligands and the protein. Furthermore, the proposed paradigm is contingent on the availability of group ligand binding information, presenting a limitation in scenarios where such data is absent. One potential future work is to leverage non-binding ligand information and construct a model that learns to selectively identify ligands beneficial for the docking process."}, {"title": "A CASE MOTIVATION", "content": "PDB entries 6g2c and 6g29 contain distinct ligands (Tanimoto similarity = 0.26) that bind to the same pocket. Despite their differences, several key interactions are conserved: for instance, a hydrogen bond is formed between N3 in 6g2c / N12 in 6g29 and an oxygen atom in the side chain of a pocket residue, while \u03c0-stacking interactions occur between the ring C7-C8-N9-C10-N11 in 6g2c / ring C8-N7-N6-C4-C3-C2 in 6g29 and two benzene rings in the side chains of pocket residues. These conserved interactions ultimately result in similar binding poses for these distinct ligands."}, {"title": "B DATA PROCESSING DETAILS", "content": "We illustrate how we prepare the data for GROUPBIND in Section 1. After processing the original PDBBind v2020 dataset, we obtain 7323 groups (17649 complexes) for the training set, with 46.3% groups having more than one ligand. By setting the maximum ligands in each group as 5, we have 8237 datapoints with 51.3 % datapoints with more than one ligand.\nIn Figure 6a, we present a histogram showing the number of ligands per pocket. There are 7697 unique pockets for the 19k complexes in the PDBBind dataset. Among these, 3,681 pockets (47.8%) contain more than one ligand. Additionally, Figure 6b illustrates the Tanimoto similarity of ligands within each group (with two or more ligands). This analysis shows that while some pockets are indeed bonded with similar ligands, a larger portion of pockets contain ligands that are dissimilar."}, {"title": "C EXPERIMENTAL DETAILS", "content": "C.1 MODEL DETAILS\nWe use the same way to construct the protein-ligand heterogeneous graph as DIFFDOCK and follow the same node and edge featurization to make sure the performance improvement come from our introduced GROUPBIND paradigm. The triangle attention module between protein and group ligands is employed at the beginning of each layer, with 4 attention heads and 128-dimension key / query / value hidden features. The model consists of 5 layers with 48 scalar features and 10 vector features, except that in the ablation study Section 4.4, we use a smaller model with 24 scalar features and 6 vector features.\nC.2 TRAINING DETAILS\nSince our GROUPBIND operates on the pocket level, we set a smaller sigma 10.0 for the translational noise. The other diffusion related parameters remain same as DIFFDOCK. For the auxiliary distance loss (Section 7), we map the distance to 40 bins from 2 \u00c5 to 22 \u00c5, and the distance loss is weighted by 0.1 and added to the score matching loss. We train our model with AdamW Loshchilov & Hutter (2017) optimizer with the learning rate of 0.001 and the weight decay of 10.0.\nD ADDITIONAL RESULTS\nFollowing prior work, we also evaluate the top-1 and top-5 centroid distance in Table 4. It can be seen that our models are consistently better than all previous baselines. Augmentation with ligands from the training set also improves the model's performance in terms of centroid distances. We also provide the full results of ablation studies (Section 4.4) and different numbers of augmented ligands (Section 4.3) in Table 5 and Table 6 for further reference.\nE TRAINING AND INFERENCE TIME\nWe trained our model on eight NVIDIA A100 GPUs for 350 epochs. Each epoch takes about 9min so the total training time is about 52.5 hrs. In comparison, DiffDock is trained on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days). Considering the performance of A100 is about twice that of A6000, the actual training cost for our model and DiffDock is approximately 1:2. The"}, {"title": "F ADDITIONAL EXPERIMENTS", "content": "reduced training time for our model can be attributed to training at the pocket-level and grouping ligands, which leads to a smaller number of training data points compared to DiffDock.\nThe average time for our method to generate 40 samples per pocket is 30s (with a maximum number of ligands in a group set to 5). If we are interested in docking poses of multiple different ligands for the same protein, the actual average inference time for each ligand will be lower."}]}