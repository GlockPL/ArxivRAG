{"title": "Operationalizing the Blueprint for an AI Bill of Rights:\nRecommendations for Practitioners, Researchers, and Policy Makers", "authors": ["Alex Oesterling", "Usha Bhalla", "Suresh Venkatasubramanian", "Himabindu Lakkaraju"], "abstract": "As Artificial Intelligence (AI) tools become ubiquitous and are increasingly employed in diverse\nreal-world applications, there has been significant interest in regulating these tools. To this end, several\nregulatory frameworks have been introduced by different countries worldwide. For example, the Euro-\npean Union recently passed the AI Act, and the White House issued an Executive Order on safe, secure,\nand trustworthy AI, associated guidance from the Office of Management and Budget, all of which built\non the Blueprint for an AI Bill of Rights issued by the White House Office of Science and Technology\nPolicy. Many of these frameworks emphasize the need for auditing and improving the trustworthiness of\nAI models and tools, underscoring the importance of safety, privacy, explainability, fairness, and human\nfallback options. Although these regulatory frameworks highlight the necessity of enforcing the afore-\nmentioned principles, practitioners often lack detailed guidance on implementing them. While there is\nextensive research on operationalizing each of these aspects, it is frequently buried in technical papers\nthat are difficult for practitioners to parse. In this write-up, we address this shortcoming by providing an\naccessible overview of the various approaches available in existing literature to operationalize regulatory\nprinciples. For the sake of clarity and precision, we will specifically focus on the Blueprint for an AI Bill\nof Rights throughout this document. We provide easy-to-understand summaries of state-of-the-art liter-\nature and highlight various gaps that exist between the regulatory guidelines in the Blueprint for an AI\nBill of Rights and existing AI research, including the trade-offs that emerge when operationalizing each\nof the highlighted principles. We hope that this work not only serves as a starting point for practitioners\ninterested in learning more about operationalizing the regulatory guidelines outlined in the Blueprint for\nan AI Bill of Rights but also provides researchers with a list of critical open problems whose resolution\ncan help bridge the gaps between regulations and state-of-the-art AI research. Finally, we note that this\nis a working paper and we invite feedback in line with the purpose of this document as described in the\nintroduction.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, AI tools have become ubiquitous, finding applications in various real-world settings,\nincluding healthcare, finance, education, and e-commerce. For instance, in healthcare, AI assists in diagnos-\ntics and personalized treatment plans, while in finance, it enhances fraud detection and risk management.\nThe emergence of generative AI in recent years has further accelerated the adoption of AI technologies\nacross multiple domains, leading to innovations such as automated content creation, advanced language\nmodels, and personalized learning systems. However, the widespread use of AI tools has also brought about\nsignificant challenges and risks. Concerns about privacy, biases and lack of transparency in AI decision-\nmaking, and the security of AI systems have become increasingly prominent. For example, AI systems can\ninadvertently perpetuate biases present in training data, leading to unfair treatment in hiring processes or\nloan approvals. Moreover, the potential misuse of AI for malicious purposes, such as deepfakes or auto-\nmated cyber-attacks, poses substantial security threats.\nTo address these concerns, several regulatory frameworks have been introduced. Most notable among them\nare the European Union's (EU) AI Act [1], the White House's Executive Order on safe, secure, and trust-\nworthy AI (and associated guidance from the Office of Management and Budget) [2], which builds on the\nBlueprint for an AI Bill of Rights [3] put forth by the United States White House Office of Science and\nTechnology Policy (OSTP) as well as the AI Risk Management Framework [4] developed by the National\nInstitute for Standards and Technology. These are pivotal frameworks aimed at ensuring the ethical and\nresponsible use of technology. The EU AI Act seeks to promote the deployment of trustworthy and safe A\u0399\nsystems while ensuring the protection of human rights against potential harms of AI. It provides guidelines\nfor responsible development as well as placement of AI systems on the market and in services [5]. The\nBlueprint for an AI Bill of Rights outlines principles for the ethical development and deployment of AI\ntechnologies. It stresses the need for safe and effective AI systems, protection against algorithmic discrim-\nination, data privacy, transparency, and the availability of human alternatives and recourse. Finally, the AI\nRisk Management Framework provides a guide to understanding the risks present in AI systems as well as\na framework for designers, developers, and deployers of AI to evaluate and mitigate the risks relevant to\nthe specific contexts of their system. These frameworks share a commitment to protecting individual rights,\nensuring transparency, and promoting fairness in the use of technology, reflecting a global effort to ensure\nthe responsible and trustworthy use of AI in real-world applications.\nWhile these frameworks introduce and emphasize the necessity of comprehensive guidelines, there is a lack\nof clarity on how these guidelines can be effectively followed. Although extensive research exists on opera-\ntionalizing various notions of trustworthiness, including transparency, fairness, and privacy, this information\nis often buried in dense technical papers that practitioners find difficult to navigate. For example, concepts\nlike group fairness\u2014which ensures that different demographic groups are treated equally and individual\nfairness-which ensures that similar individuals receive similar treatment\u2014are crucial but complex. Simi-\nlarly, there are different approaches to enabling model transparency, such as building simpler models that are\ninherently interpretable versus constructing post hoc explanations for more complex models. The detailed\nmethodologies for implementing these types of fairness and transparency are often hidden in academic litera-\nture, making practical enforcement challenging. Furthermore, the rapid pace of technological advancements\ncomplicates the consistent application and enforcement of regulatory frameworks, potentially undermining\nthe very protections these frameworks aim to provide.\nIn this write-up, we aim to address this shortcoming by providing an overview of various existing methods\nin literature for operationalizing the aforementioned regulatory principles. For clarity and precision, we\nfocus specifically on the Blueprint for an AI Bill of Rights in this document as it begins to lay out how these\npractices should look based on existing research. We present easy-to-understand summaries of state-of-the-\nart literature and highlight the gaps that exist between the regulatory guidelines and current AI research,\nincluding the trade-offs that emerge when implementing these principles. We hope that this work aids\npractitioners in understanding how to operationalize the regulatory guidelines outlined in the Blueprint for\nan AI Bill of Rights and also provides researchers and policy makers with a list of critical open problems and\ngaps between regulations and state-of-the-art Al research. Our goal is to strike a balance between providing\ndeep technical understanding while concisely developing useful intuition about methods for operationalizing"}, {"title": "2 Safe and Effective Systems", "content": "Safe and Effective Systems ensures that systems are designed to limit potential harms, are tested rigorously\nbefore deployment, and are monitored after deployment. This principle asks that developers of automated\nsystems consult with the communities that will be affected by the system, as well as all other stakeholders,\nto confirm that all possible concerns and risks are identified before development. Then, during development,\nextensive testing is required to guarantee that the automated system is safe, with the expectation that system\ndeployment may be delayed or prevented if tests are not passed. In accordance with this principle, developers\nshould confirm that excessive data is not collected and that all data is used appropriately and safely. Finally,\nensuring the long-term safety and efficacy of an automated system requires continuous monitoring and\nreporting after deployment, and mechanisms for adaptation and updates if necessary.\nRegardless of the intentions of the designers of any technology, users can always intentionally or acciden-\ntally abuse technological systems. Such potential risks must be identified and considered during the design\nand development of automated systems to limit unintended harms and prevent ill use after deployment.\nThe recent increased pace of automated system adoption has exacerbated the opportunity for misuse, as\ndecreased time has been spent studying and testing systems in controlled environments before deployment.\nFor example, while automated plagiarism detection systems are built to uphold academic integrity and re-\nduce the workload of teaching staff, the effect that their immediate widespread adoption has had on students'\nconceptualizations of collaboration and originality is still unknown. Furthermore, such software can also be\nactively abused to further personal agendas, such as targeting and discrediting specific academics by quickly\nsearching large bodies of work [6]. As another example, consider the exponential growth of utilization of\nlarge language models by the public. The adoption of these systems outpaced the recognition and dissem-\nination of their limitations, and many users placed too much trust in the factuality and abilities of these\nmodels, resulting in hallucinated outputs being treated as facts and used in legal proceedings and academic\nwork [7-9].\nDeveloping safe and effective systems begins with a responsible design process that consults all stakehold-\ners and identifies potential social, environmental, and individual risks a system might pose. To engage in\nresponsible design, developers need to determine who to involve in the design process and what is at stake\nfor those affected by a system. Then, depending on the context and impact of the automated system, practi-\ntioners can select from a wide body of literature on participatory approaches to design [10, 11]. Early work\nin user-centered design considers the needs of users during the design process [12], while service design\nconsiders a larger set of stakeholders impacted by a system [13]. Participatory design practices dive deeper,\nunderstanding inherent power balances between providers and users in the design process and seeks to nav-\nigate and limit the impact of these political structures [14, 15]. Action research involves stakeholders as\nco-researchers and developers of systems [16], while value-sensitive design emphasizes the ethical values\nof stakeholders during the design process. Finally, mechanism design proposes methods rooted in social\nchoice theory [17] to quantitatively aggregate stakeholder preferences [18]. Methodologies and tools from\nthese various design approaches can guide reflective and critical thinking, help to identify unintended con-\nsequences of systems, provide means to address such problems, and amplify underrepresented communities\nand voices in the design process.\nAfter determining an appropriate design process, practitioners must consider what data, if any, is necessary\nto construct their automated system. If data is required to train or design a system, then this data must be of\nhigh quality and an accurate representation of the population that will eventually be affected by the system\nafter deployment. For instance, a movie recommendation system trained on adult user preferences may not\ncorrectly learn the preferences of children and may recommend inappropriate content, so the developers of\nthis system should consider collecting data on child movie preferences. As mentioned previously, develop-\ning a deep understanding of stakeholders and impacted communities can help indicate what types of data\nto collect and from whom to ensure relevance. In addition to curating high-quality data, deployers of auto-\nmated systems should be aware of the risks of data reuse\u2014especially in high stakes settings\u2014where data\nis collected for one purpose and then used in a completely different context, especially by another group.\nFor instance, genetic information collected by the FBI for security purposes could be used by researchers to\ntrain a genomics model, despite this data being collected in the context of criminal investigations and thus\nlikely containing demographic biases that could drastically impact performance in the reused setting. If a\nbiased dataset becomes a standard benchmark in academic research or industry, it will result in a large scale\nshift to systems reflecting and potentially perpetuating these biases when deployed. Thus, in high-stakes\nsettings, this data should come with explicit reports on the context within which data was collected and its\npurpose.\nFor a system to be safe, it must be robust to a wide variety of inputs and perform in a predictable manner.\nLiterature in adversarial robustness and distribution shifts addresses this issue in two different settings.\nAdversarial robustness considers a malicious adversary who is trying to break a system's behavior with\nsmall, unnoticeable perturbations of inputs. Adversarial training, or training a machine learning model on a\nmixture of ground-truth and adversarial examples, provides a potential solution, minimizing the impact of\nmalicious attacks [19]. Furthermore, several methods have been proposed that provide certified robustness:\nby training with these methods, a system can be guaranteed to be robust up to a certain level of attack\n[20, 21]. Distribution shift research considers the case where a machine learning model receives unexpected\ninputs, specifically the case where the input data to a model after deployment (i.e. the testing or inference\ndata) is different from the data it was trained on. The change in performance due to distribution shift can\nbe counteracted using techniques from Distributionally Robust Optimization (DRO) [22]. Depending on\nthe application, a system may be at risk from adversarial attacks, distribution shifts, or both. Developers\nand designers should evaluate these risks and accordingly consider methods for improving their systems'\nrobustness.\nBoth before and after deployment, automated systems must be tested and continually monitored to ensure\nthey remain safe and effective. Testing should occur internally, by the developers of an automated system,\nand externally, through public audits and independent reporting. Internal tests should consider edge cases\nand measure potential consequences and harmful outputs of the system. They might also include user stud-\nies of the system to evaluate limitations, effects on various stakeholders, and compliance with the principles\nof explainability, protection against discrimination, and privacy. An example framework for internal testing"}, {"title": "3 Algorithmic Discrimination Protection", "content": "Algorithmic Discrimination Protection states that users should be protected from unjustified differential or\nharmful treatment by an automated system on the basis of race, color, ethnicity, sex, religion, age, national\norigin, disability, veteran status, genetic information, or any other identity protected in existing legislation.\nAlgorithmic discrimination protections should be considered and implemented during the design, develop-\nment, and deployment of automated systems to mitigate the disparate treatment of individuals and groups.\nThese protections include ensuring accurate representation (in design input and data collection) of the peo-\nples impacted by an algorithm, active interventions to develop a system that minimizes discrimination, and\ncontinual testing and monitoring to limit discrimination after system deployment. Furthermore, the principle\nemphasizes the need for auditing and reporting of algorithms for disparate impacts and treatment.\nAs automated decision-making systems are increasingly integrated into all aspects of life, including many\nhigh-stakes domains such as healthcare, hiring, and credit, preventing algorithmic discrimination is pivotal\nin protecting the civil liberties and rights of the American people. Current protections against discrimination\nmust be expanded to the digital sphere to ensure that decisions made not just by people but also by automated\nsystems result in fair treatment and protection of all. This is especially true in settings where algorithmic\ndecisions can have high or long-term impact on the quality of life of individuals. For instance, a bank using\nan automated loan approval system determines which members of the community are able to start a small\nbusiness or afford college. These opportunities have lasting impacts on wealth-building and the financial\nprosperity of families and communities. While some assume that automating the loan approval process\nremoves human subjectivity and bias, many systems-especially machine learning systems-learn the biases\nof the human data they are trained on, and thus can perpetuate or even exacerbate those biases.\nWhile the principle of algorithmic discrimination protections highlights the importance of preventing unjust\ntreatment by automated systems, implementing this principle may be very difficult in practice. Discrimi-\nnation protections begin at the design stage, where the system designers must ask whether the fundamental\npurpose, inputs, and potential outcomes of the system are perceived to be fair. In many cases, the decision of\nwhether or not to give sensitive attributes as inputs to the system is nontrivial. Ignoring sensitive attributes\ncan avoid discrimination, but in some cases, considering and protecting these attributes may be more fair.\nFor example, an automated grading system likely should not consider the demographic information associ-\nated with the authors of its inputs, and as such would be more fair if it was unaware. On the other hand,\nin the case of a system that awards need-based scholarships, the system might be more fair overall if it\nconsiders applicants and their demographics holistically.\nThe second step to building a fair system is to pick the definition of fairness that best suits the use case,\noutcomes, and stakeholders of the system. These definitions can be broadly categorized into group fairness\nand individual fairness. Group fairness definitions evaluate the differences in system performance across\ndifferent demographic groups. Popular examples of this include demographic parity [31], which checks for\nthe difference between selection rates of various groups, equalized odds [32], which evaluates the difference\nbetween both true positive and true negative outcomes for various groups, equality of opportunity [32],\nwhich considers the difference between only true positive outcomes for groups, and multi-calibration/multi-\naccuracy, which considers performance on arbitrary partitions of subgroups [33, 34]. Causal fairness metrics\nuse methods from causal inference to study the effect that sensitive group attributes and demographic labels\nhave on the outcome or prediction of an automated system. Popular metrics include path-specific fairness"}, {"title": "4 Data Privacy", "content": "Data privacy states that only minimal and strictly necessary data should be collected by designers, develop-\ners, and deployers of automated systems, and that permission should be asked for and respected regarding\nthe collection, use, access, transfer, and deletion of personal data. These protections should be provided\nby default and implemented in good faith, without unnecessary and obfuscatory interfaces or language that\nburden or confuse users. Furthermore, in sensitive domains such as health, work, education, finance, and\ncriminal justice, additional protections and restrictions should be placed such that only the most necessary\ndata is used and is protected by ethical review and use prohibitions. Surveillance technology should be\noverseen to scope impact and limit potential harm and should be completely avoided in high-stakes settings\nwhere it is likely to limit rights and opportunities. Finally, the collection and use of private data should\nalways come with reporting to confirm that data privacy is being protected and its impact is well understood.\nPrivacy has always been a core component of American civil liberties, but its interpretation in the digital\nage is a new and evolving subject. With the extreme popularity of big data, increased surveillance and data\ncollection can lead to a variety of infringements on user privacy. For example, social media companies may\ncollect user data to sell to third-party agencies, such as a hiring agency, who may then try to infer their\ncandidates' political leanings. While in this case the hiring agency is intentionally trying to recover private\ndata, many automated systems can still inadvertently leak such data if explicit privacy protections are not\nput in place. For instance, many websites leverage browsing behavior for targeted advertisements, often\nwithout the knowledge of their users. The subsequent advertisements can inadvertently reveal protected and\nprivate identities, such as gender and sexual orientation or pregnancy status [43]. Even when users have\nconsented to data collection, they should still have agency over the use of the data, they should be informed\nabout potential risks, and data collection should still be minimized in scope to prevent harm, inadvertent\nor adversarial. Thus, it is important that developers understand the data privacy impacts of their automated\nsystems and enact limitations and safeguards against overcollection and misuse.\nWhile privacy is a highly valued and agreed upon right in the United States, both defining privacy and\nensuring it are very difficult in modern society, with the digital world being no different. First, user data\nmust be collected and stored in a nonintrusive and safe manner. During system design, the data privacy\nprinciple states that consent should be clearly asked for and given before a system collects or uses your data.\nThis requires that the method of asking for permission is easy to understand and use, prioritizes privacy by\ndefault, and clearly explains how data will be used after collection. For example, a website that leverages\nuser cookies should ask for consent from the site user before collecting the cookies, and should not by\ndefault suggest that the user share unnecessary cookies for the website to sell to advertising agents. If an\norganization collects personal data, it must also be stored in a secure manner to protect user privacy. In\ndistributed settings, methods such as federated learning [44] can be used to limit the transfer and storage\nof personal data. This distributed training approach only communicates model updates to central servers,\nrather than the training data itself, allowing for personal data to remain private and secure.\nIn addition to collecting the minimal data necessary to maximize user privacy, it is important to ensure data\nprivacy is maintained on deployed systems with public access. For all data-based systems, careful thought\nshould be put into how systems may inadvertently reveal their data to other users or adversaries. In the\ncase of machine learning systems, if the weights of a model are made public, then training data can often\nbe recovered, violating the privacy of those who provided the data [45]. This is true even if adversaries do\nnot have access to the weights and instead are able to query the model many times with a variety of in-\nputs, also known as a Membership Inference Attack (MIA) [46]. As such, in high-risk domains, developers\nshould consider additional security measures when training machine learning models, such as methods in\ndifferential privacy. Differential Privacy [47] uses statistical guarantees to ensure that an adversary querying\na model is unable to distinguish between data points that were or were not present in the model's training\ndata. This is generally achieved by adding noise to the objective function [48] during learning or to sys-\ntem outputs during inference [47]. In deep networks, a popular method is Differentially-Private Stochastic\nGradient Descent (DP-SGD) which adds noise and clips gradient values during backpropagation to achieve\nprivacy [49].\nAfter implementing privacy protections during the design of an automated system, developers should pro-\nvide mechanisms to give users agency over the data they provide. The data privacy principle states that users\nshould be able to access all data about themselves as well as other metadata, such as who else has access\nto this data. Along with access to this information, developers should also provide pathways for users to\nrequest this data be withdrawn from use or corrected. Given that machine learning models can leak training\ndata, developers need to ensure data can be deleted from both storage bases and from the models themselves.\nTo do so, developers can retrain their models, but retraining for every user modification or deletion can be\ncomputationally inefficient or infeasible. This has motivated a body of research on machine unlearning,\nwhich seeks to find efficient approximations to retraining using methods from security and privacy [50, 51]."}, {"title": "5 Notice and Explanation", "content": "Notice and explanation states that users who are at the receiving end of outcomes from automated systems\nshould be clearly informed that an automated system is being used, and they should also be provided with\nan explanation of how and why the system contributed to the outcome. Furthermore, this principle asserts\nthat designers, developers, and deployers of automated systems must provide documentation about system\nbehavior, the role of automation, and information regarding the individuals and organizations responsible for\nthe system. Most importantly, stakeholders are entitled to clear, timely, accessible, and valid explanations\nof the outcomes and decisions made by automated systems.\nThe principle of notice and explanation provides critical guidelines that are essential in today's world where\nautomated systems are powering diverse real-world applications involving high-stakes decisions. For in-\nstance, automated systems are being employed in hiring, credit, and courtrooms in ways that profoundly\nimpact the lives of the American public. However, this impact may not always be obvious given the min-\nimal transparency around such systems. For example, a job applicant might not know whether a recruiter\nrejected their resume or if instead a hiring algorithm placed them at the bottom of its list. Similarly, a de-\nfendant should know if a judge that denied their bail was given a recommendation by an automated system.\nHowever, even knowing that a system was used in a decision-making process is not enough, and affected\nindividuals must also be provided with reasons or explanations for their outcomes. The lack of transparency\naround automated systems prevents individuals who are at the receiving end of outcomes from contesting\nor appealing negative decisions impacting their lives. The principle of notice and explanation outlines pro-\ncedural guidelines to address the aforementioned challenges and promote transparency in the adoption and\nuse of automated systems employed in real-world applications.\nWhile this principle emphasizes the need for explanations, it does not provide prescriptive guidance in\nterms of what approaches or methodologies can be used to operationalize these guidelines. For example,\nproviding explanations for outcomes produced by automated systems typically involves understanding the\nrationale behind the predictions of the underlying machine learning models. This is a technically challeng-\ning problem that has been studied extensively in machine learning and statistics literature [53] under the\nbroad umbrella of interpretable or explainable machine learning. Prior research on this topic has proposed\ntwo broad solutions to the problem of understanding the behavior or predictions of ML models. The first\napproach involves developing models that are inherently interpretable and are therefore easy to understand\nby design. For example, simple predictive models such as linear or logistic regression, which output fea-\nture coefficients that can be interpreted as importance scores assigned to individual features, and (shallow)\ndecision trees/lists/sets, which output rules that can be used to make predictions on data instances are of-\nten considered inherently interpretable models [54, 55]. Such models are easily understandable to diverse\nclasses of end users and stakeholders with little to no expertise in machine learning and statistics. While the\naforementioned simple models are readily interpretable, prior works have demonstrated that these models\nmay not be as accurate as their complex counterparts that are hard to interpret (e.g., deep neural networks or\nensemble models such as random forests and boosted trees). To this end, a second approach to understand-\ning the behavior of ML models, frequently referred to as \u201cpost hoc\u201d explainability, has risen in popularity.\nThese methods allow for the use of complex or black-box models that are hard to interpret but accurate in\nreal-world applications by constructing \u201cpost hoc"}, {"title": "6 Human Alternatives, Consideration, and Fallback", "content": "Human Alternatives, Consideration, and Fallback proposes that, when applicable, users should be able\nto refuse use of an automated system and request a human alternative as a replacement. Importantly, the\nhuman alternatives must be easily accessible; users should know whether or not they are interacting with\nan automated system, in what capacity, and what the purpose and impact of the system are, so that they\ncan decide whether they need to request a human alternative. Furthermore, the process for requesting a\nhuman alternative should be straightforward. Human fallbacks should be timely and should also be available\nafter the use of an automated system, such that they can address any problems that arise or remedy faulty\ndecisions made by the automated system. The people behind these alternatives should be effective and\nproperly trained for the task the automated system performs. Providing human fallback is most important\nin sensitive domains such as medicine, finance, and criminal justice, where automated decisions can have\ndrastic impacts on their stakeholders, and where human consideration and empathy are of utmost concern.\nThere are many settings in which users may prefer or need a human alternative to an automated system.\nConsider the scenario of a medical clinic using a machine learning model to screen for risk of heart disease.\nIf this model has a known bias towards a specific subgroup, patients of that population may wish to have\na human medical professional consider their case instead of the automated system. In this situation, they\nshould be provided with an alternative if they request one, or they should be able to have a human medical\nprofessional check the output of the model and verify that the explanation of the decision made by the\nautomated system is correct. Note that there also exist settings in which human alternatives may not be\nnecessary or appropriate, such as in settings where automated systems have minimal direct impact on their\nusers and do not pose any risks (e.g. online machine translators, movie recommendation systems).\nImplementing human alternatives is relatively straightforward: wherever a practitioner deploys an auto-\nmated system, they can add an option for users to select a human operator instead. However, an important\nconsideration in the implementation of a human fallback is how much users trust a system. The perceived\n\"impartiality\" of automatic systems and their perceived inability to make mistakes may result in users be-\ning inhibited from requesting human fallback even when it could be in their best interests. The context\nand behavior of an automated system can also alter its perceived trustworthiness. Research has shown that\nproviding explanations, even if incorrect or misleading, can increase user trust in a system by a factor of\nalmost 10 [78]. Furthermore, factors such as the speed of an automated system and its interface also impact\na users likelihood to trust the system [79]. Users should be provided with sufficient information regarding\nthe benefits and harms of both the automated system and human alternative, such as wait times, accuracy,\nconsistency, and more. Furthermore, there should not be negative consequences or punishments to users for\nrequesting an alternative, and both the system and human alternatives should be made easy to use. Finally,\nin cases where a system or model is highly uncertain or is accurately aware that its output is incorrect or\npotentially harmful, the system itself should propose deferring to a human alternative, essentially auditing\nitself. This method is sometimes referred to as selective classification (or classification with a reject op-\ntion), and while it has been shown to decrease error rates, recent work has also found that it may increase\ndisparities among different subgroups [80, 81]."}, {"title": "7 Intersections", "content": "While all of the goals enumerated in the Blueprint for an AI Bill of Rights are important independently, it\nis also necessary to consider the ways in which they can coexist or come into conflict with each other. For\ninstance, optimizing an automated system to achieve auxiliary goals such as fairness or privacy generally\ncomes at a cost to the performance of the system in terms of accuracy. In this section, we list potential\nconflicts that may arise in operationalizing the AI Bill of Rights to bring awareness to nuances operational-ization.\nWhile at first glance the principles of protecting against algorithmic discrimination and protecting data\nprivacy may seem aligned, as they both prevent misuse of personal data, in practice these principles can\ncome into conflict when developing an automated system. For example, optimizing for group fairness\nrequires the collection of subgroup labels and sensitive attribute information, which may be at odds with the\nprinciple of privacy. Furthermore, the principle of data privacy states that users should be able to remove\ntheir data from automated systems, but if these deletions all come from one subgroup, this may result in\nmodels becoming unfair [82].\nIn many cases, increased transparency and explainability of automated systems can aid users in preventing\nalgorithmic discrimination, particularly if explanations highlight biases of a model. However, there exist\nsome challenges to ensuring systems are simultaneously fair and explainable. In general, explainability asks\nfor predictors to be as simple or interpretable as possible, but oftentimes making a model more fair requires\nincreasing its complexity [83]. In some cases, forcing a model to be more interpretable by requiring simpler\ndecision rules may bias it toward increased reliance on sensitive attributes.\nFurthermore, protecting the right to explanation and protecting data privacy can often work against each\nother in practice. As explanations provide more information about the decision of a model, malicious actors\nare better able to reconstruct the model and can recover more information about training data if proper\nprotections are not taken. Additionally, the right to be forgotten, allowing users to remove their data from\nautomated systems, may result in explanation methods that fail to be faithful or accurate after a user's data\nhas been deleted from the system, invalidating past explanations and algorithmic recourse. Implementing\nhuman fallbacks also requires consideration of its interactions with the other principles in the Blueprint for\nan AI Bill of Rights. For instance, providing explanations often inflates a user's trust in a system, making\nthem more likely to accept an automated decision instead of requesting human fallback [84]. However,\nproper reporting can allow consumers to better understand the risks involved with using an automated system\nand make a more educated choice about requesting a human alternative. Understanding if a system is fair,\nprivate, or robust to a wide variety of inputs can improve a user's confidence in engaging with that system.\nWhile there may exist tensions between the principles outlined in the Blueprint for an AI Bill of Rights,\nthese principles also provide an avenue for navigating such tradeoffs. As outlined in the principle for Safe\nand Effective systems, by properly testing systems, practitioners can be aware of potential limitations and\ncollisions between principles in practice. Furthermore, by reporting performance along various axes and\nbeing explicit about shortcomings, failure cases, and user protection tradeoffs of automated systems, users\ncan make informed decisions about whether to use a system, disengage from it, or request human fallback."}, {"title": "8 Reporting", "content": "In order to evaluate various systems on their compliance with the above principles, it is necessary to stan-\ndardize a reporting structure for these systems. In addition, this standardization will allow consumers to\nfairly compare between automated systems to determine what to use, and whether to opt for human alterna-\ntives instead. A report should include information on the type of model used in an automated system, the\ndata used to train it or the nature of this data (when possible), and the intention of the providers in creating\nthis automated system. As part of these intentions, providers should highlight intended contexts for use, the\nexpected types of users for the system, and the expected outcomes for these users. Extensive testing should\nalso be conducted, using multiple sets of evaluation data. The results of this testing, in addition to a descrip-\ntion of the evaluation data, should be provided to users. Results should include baseline performance metrics\nincluding loss and specific metrics such as fairness and privacy. In the above sections, we mention some\nconsiderations for reporting for each of the principles that should be included in a report. Some example\nframeworks for reporting are the model cards framework [85], datasheets framework [30] and the SMACTR\nauditing framework [23]. Finally, this reporting should be constantly-updated, with continual monitoring\nto track system performance over its whole lifecycle. Establishing a standardized reporting structure will\nrequire collaboration between academics and policymakers as well as regulation and enforcement to ensure\nadoption by system providers."}, {}]}