{"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "authors": ["JI LIU", "JUNCHENG JIA", "HONG ZHANG", "YUHUI YUN", "LEYE WANG", "YANG ZHOU", "HUAIYU DAI", "DEJING DOU"], "abstract": "Despite achieving remarkable performance, Federated Learning (FL) encounters two important problems, i.e., low training efficiency and limited computational resources. In this paper, we propose a new FL framework, i.e., FedDUMAP, with three original contributions, to leverage the shared insensitive data on the server in addition to the distributed data in edge devices so as to efficiently train a global model. First, we propose a simple dynamic server update algorithm, which takes advantage of the shared insensitive data on the server while dynamically adjusting the update steps on the server in order to speed up the convergence and improve the accuracy. Second, we propose an adaptive optimization method with the dynamic server update algorithm to exploit the global momentum on the server and each local device for superior accuracy. Third, we develop a layer-adaptive model pruning method to carry out specific pruning operations, which is adapted to the diverse features of each layer so as to attain an excellent trade-off between effectiveness and efficiency. Our proposed FL model, FedDUMAP, combines the three original techniques and has a significantly better performance compared with baseline approaches in terms of efficiency (up to 16.9 times faster), accuracy (up to 20.4% higher), and computational cost (up to 62.6% smaller).", "sections": [{"title": "1 INTRODUCTION", "content": "With a large quantity of data distributed in numerous edge devices, cloud servers may have some shared insensitive data for efficient processing [1]. Due to data privacy and security concerns, multiple legal restrictions [2, 3] have been put into practice, making it complicated to aggregate the distributed sensitive data within a single high-performance server. As big amounts of training data generally lead to high performance of machine learning models [4], Federated Learning (FL) [5-7] becomes a promising approach to collaboratively train a model with considerable distributed data while avoiding raw data transfer.\nTraditional FL exploits non-Independent and Identically Distributed (non-IID) data on mobile devices to collaboratively train a global model [5]. Within the training process, the weights or the gradients of the global model are transferred between the devices and the server while the raw data stays in each device. FL typically utilizes a parameter server architecture [6, 8], where a parameter server (server) coordinates the distributed training process. The training process generally consists of multiple rounds, each composed of three stages. First, the server chooses a set of devices and broadcasts the global model to them. Second, each of the chosen devices trains the received model using local data and then sends back the updated model. Third, the server gathers the updated models and aggregates them to form a new global model. This process continues when the predefined condition, e.g., a maximum round number or the convergence of the global model, is not achieved.\nAlthough keeping data within the devices can protect data privacy and security, FL encounters two major challenges, which hinder its application in real-world environments. The first challenge is the limited data within a single edge device, which leads to ineffective local training while potential large amounts of shared insensitive data remain useless on the server. The second challenge is the modest edge devices with limited communication and computation capacity [9], which corresponds to inefficient local training on devices. To address the first challenge, we leverage the shared insensitive data on the server and design a new FL framework to improve the accuracy of the global model. To address the second challenge, we propose an adaptive optimization method and an adaptive pruning method within the proposed FL framework to improve model accuracy and reduce training cost.\nIn addition to the distributed data in edge devices, insensitive data can be transferred to the servers in the cloud to leave space for critical sensitive personal data [10, 11], e.g., Amazon Cloud [12], Microsoft Azure [13], and Baidu AI Cloud [14]. The insensitive data may exist on the server by default before the training process. For instance, the server may have prior learning tasks, either training or inference, which have already collected some data to be re-used for the new task of interest. In addition, some end users may offload some insensitive data to the server with certain incentive mechanism such as for crow-sourcing tasks [15], or simply to free up some local storage space [16]. Some other recent works are based on the insensitive shared data on the server [17-19]. The shared insensitive data can help improve the efficiency of the FL training process without the restriction of data distribution (see details in Section 3.2). The shared insensitive data can be transferred to the devices for local training in order to improve the accuracy of the global model [1, 20]. Although the shared data on the server is not sensitive, they still contain some important personal information. Thus, transferring the shared data to devices may still incur privacy or security problem. In addition, this approach leads to significantly high communication overhead. Furthermore, the existing approaches are inefficient when the shared server data is simply processed as that in edge devices [11] or when knowledge transfer is utilized to deal with heterogeneous models [21, 22].\nAdaptive optimization methods, such as Stochastic Gradient Descent (SGD) with Momentum (SGDM) [23], Adaptive Moment Estimation (Adam), and Adaptive Gradient (AdaGrad), have gained superb advance in accelerating the training speed and the final accuracy. Most of the existing FL literature adopts the adaptive optimization methods either on the server side"}, {"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "content": "or on the device side [27, 27-31]. However, applying the adaptive optimization on either of them often leads to inefficient learning results with inferior accuracy. The adaptive optimization methods can be exploited on both sides, as shown in [32], but the momentum transfer may incur severe communication costs with limited bandwidth between the devices and the server.\nModel pruning reduces the size of a model into a slim one while the accuracy remains acceptable. These kinds of methods can be exploited in the training process to improve the efficiency of the training and to significantly reduce the overhead brought by a big model [33]. However, existing pruning approaches incur severe accuracy degradation. The simple application of existing pruning methods in FL does not consider the diverse dimensions and features of each layer [34]. In addition, as a big model may consist of numerous neurons, simple model pruning strategies cannot choose proper portions of the model to prune and may lead to low training efficiency and inferior accuracy [1].\nIn this work, we introduce a novel efficient FL framework, i.e., FedDUMAP, which enables collaborative training of a global model based on the sensitive device data and insensitive server data with a powerful server and multiple modest devices. We denote the sensitive data as the data that contain personal information and cannot be shared or transferred according to the users. In addition, the insensitive data can contain personal information while it can be transferred to a cloud server. In order to handle the two aforementioned problems, we utilize the shared insensitive data on the server to improve the accuracy of the global model with adaptive optimization with the consideration of the non-IID degrees, which represent the difference between a dataset (either on the server or a device) and the global dataset, i.e., the data on all the devices.\nFedDUMAP consists of three modules, i.e., FedDU, FedDUM, and FedAP. FedDU is an FL algorithm, which dynamically updates the global model based on the distributed sensitive device data and the shared insensitive server data. In addition, FedDU dynamically adjusts the global model based on the accuracy and the non-IID degrees of the server data and the device data. Furthermore, we propose a novel adaptive optimization method on top of FedDU, i.e., FedDUM, to improve the accuracy of the model without transferring the momentum from devices to the server or from the server to the devices. We decouple the optimization on the server and the device sides while exploiting the model generated with adaptive optimization from each device to enable the adaptive optimization on the server side. Besides, FedAP removes useless neurons in the global model to reduce the model size so as to reduce the computational overheads and the computation costs on devices without degrading noticeable accuracy. FedAP considers the features of each layer to identify a proper pruning rate, based on the non-IID degrees. Furthermore, FedAP prunes the neurons according to the rank values, which can preserve the performance of the model. To the best of our knowledge, we are among the first to propose exploiting non-IID degrees for dynamic global model updates while utilizing adaptive optimization and adaptive pruning for FL. This manuscript is an extension of a conference version [35]. In this paper, we make four following contributions:\n(1) A novel dynamic FL algorithm, i.e., FedDU, which utilizes both shared insensitive server data and distributed sensitive device data to collaboratively train a global model. FedDU dynamically updates the global model with the consideration of the model accuracy, normalized gradients from devices, and the non-IID degrees of both the server data and the device data.\n(2) A new adaptive optimization method, i.e., FedDUM, which decouples the optimization between the server side and the device side while exploiting the models generated from devices for the optimization on the server side without additional communication cost."}, {"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "content": "(3) An original adaptive pruning method, i.e., FedAP, which considers the features of each layer to identify a proper pruning rate based on the non-IID degrees. FedAP prunes the model based on the rank values to preserve the performance of the global model.\n(4) Extensive experimentation demonstrates significant advantages of FedDUMAP, including FedDU, FedDUM, and FedAP, in terms of efficiency, accuracy, and computational cost based on three typical models and two real-life datasets.\nThe rest of this paper is organized as follows. Section 2 explains the related work. Section 3 proposes our framework, i.e., FedDUMAP, including FedDU, FedDUM, and FedAP. Section 4 presents the experimental results using three typical models and two real-life datasets. Finally, Section 5 concludes the paper."}, {"title": "2 RELATED WORK", "content": "FL was proposed to train a model using the distributed data within multiple devices while only transferring the model or gradients [5]. Some works ([36-45] and references therein) either focus on the device scheduling or the model aggregation within the server or even with a hierarchical architecture to improve the accuracy of the global model, which only deals with the distributed device data. In order to leave space for sensitive data on devices and with incentive mechanisms [11], some insensitive data are transferred to the server or the cloud, which can be directly utilized for training. When all the data are transferred to the server, the server data can be considered as IID data [11]. However, transferring all the data including the sensitive data incurs severe privacy and security issues with significant communication costs, which is not realistic [46]. Some existing works utilize the insensitive server with heterogeneous models [47], or within one-shot model training [4], based on knowledge transfer methods [48, 49], or label-free data [22, 50]. However, the aforementioned approaches are inefficient [4, 47] or ineffective [22] without the consideration of the non-IID degrees or big models. Furthermore, while it may lead to significant communication overhead, the transfer of the insensitive server data to devices [1, 20] may incur severe privacy and security issues. The proper devices can be selected for training based on the server data [51], which can be integrated with our proposed approach.\nWithout adaptive optimization within the model aggregation process, traditional methods, e.g., FedAvg [5], may incur the client drift problem, which makes the global model over-fitted to local device data [52]. While control parameters [52] can help alleviate the problem, they require the devices to participate all through the training process [52, 53], which may not be feasible in cross-device FL. Adaptive optimization methods, e.g., AdaGrad [24], Yogi [25], Adam [26, 27] and momentum [32, 54-56] can be exploited to address the client drift problem. However, the existing approaches generally consider only one side, i.e., either server side [57] or device side [28], which may lead to inferior accuracy. Furthermore, the devices may have heterogeneous non-IID data, which makes the direct application of the momentum method insufficient [29]. Although adaptive optimization is applied on both the device side and the server side in some recent works [32], the communication of momentum between the server and devices may incur high communication costs.\nModel pruning can reduce the size of the model, which corresponds to smaller communication overhead and com-putation costs compared with the original model within the training process [33, 58-60] and the inference process [61] of FL. However, the shared insensitive server data is seldom utilized. Two types of techniques exist for the pruning process, including filter (structured) pruning and weight (unstructured) pruning. The unstructured pruning set the chosen parameters to 0 while the structure of the original model remains unchanged. Although the accuracy remains almost the same as that of the original one [62] while reducing communication overhead [33], the unstructured pruning cannot reduce"}, {"title": "3 METHOD", "content": "In this section, we propose our FL framework, i.e., FedDUMAP. We first present the system model. Then, we detail FedDU, which dynamically updates the global model. Afterward, we explain the adaptive momentum-based optimization, i.e., FedDUM, to improve the accuracy of the global model. Finally, we reveal the design of FedAP, which reduces the size of the global model to reduce communication and computational costs."}, {"title": "3.1 System Model", "content": "Figure 1 shows the training process of FedDUMAP, where the FL system consists of a server and N edge devices. The description of major notations is summarized in Table 1. We consider a powerful server and modest devices. The shared insensitive data is stored on the server, and the distributed sensitive data is dispersed in edge devices. Both the server data and the device data can be utilized to update the model during the FL training process of FL. Let us assume that a dataset $D_k = \\{x_{k,j}, Y_{k,j}\\}_{j=1}^{n_k}$, composed of $n_k$ samples, resides on Device k. We take $D_0$ as the server data, and $D_k, k \\neq 0$ as the device data. $x_{k,j}$ refers to the j-th sample on Device k, while $y_{k,j}$ represents the corresponding label. We utilize X to represent the set of samples and Y to represent the set of labels. Then, we formulate the objective of the training process as follows:\n$\\min\\limits_{W} F(w), with F(w) = \\frac{1}{N} \\sum\\limits_{k=1}^{N} n_kF_k(w),$\nwhere $F_k(w) = \\frac{1}{n_k} \\sum\\limits_{\\{x_{k,j};Y_{k,j}\\} \\in D_k} f(w, x_{k, j}, Y_{k,j})$ represents the loss function on Device k with $f (w, x_{k,j}, Y_{k,j})$ capturing the error of the inference results based on the model with the sample pair $\\{x_{k,j}, Y_{k,j}\\}$ and w refers to the global model.\nWith the distributed non-IID data in FL [36], we utilize the Jensen-Shannon (JS) divergence [67] to quantize the non-IID degree of a dataset as shown in Formula 2:\n$D(P_K) = \\frac{1}{2} D_{KL}(P_k||P_m) + \\frac{1}{2} D_{KL}(P||P_m),$"}, {"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "content": "where $P_m = \\frac{1}{N} (P_k + \\overline{P}), \\overline{P} = \\frac{\\sum_{k=1}^{N} n_kP_k}{\\sum_{k=1}^{N} n_k}, P = \\{P(y) | y \\in Y\\}$, $P_k (y)$ refers to the possibility that a sample is related to Label y, and $D_{KL} (||)$ corresponds to the Kullback-Leibler (KL) divergence [68] as defined in Formula 3:\n$D_{KL}(P_i||P_j) = \\sum\\limits_{y \\in Y} P_i(y) log (\\frac{P_i(y)}{P_j(y)}).$\nWhen the distribution of a dataset differs from that of the global dataset more significantly, the corresponding non-IID degree becomes higher. Although the raw data is not transferred from devices to the server or from the server to the devices in FedDUMAP, we assume that $P_k$ and $n_k$ incur little privacy concern [69], and can be transferred from devices to the server before the FL training process. When this statistical meta information, i.e., $P_k (y)$, cannot be shared because of restrictions, we can utilize gradients to calculate the data distribution of the dataset on each device [70].\nThe FL training process contains multiple rounds in FedDUMAP, each of which consists of 6 steps. In Step , a set of devices (9t with t referring to the round number) is randomly selected to participate in the training process of Round t and the server broadcasts the global model to the device of 9t. Afterward, each device updates the received model with the adaptive momentum-based optimization based on the local data in Step . Then, in Step , the selected devices upload updated models to the server, and the server aggregates all the received models based on FedAvg [5] in Step . In addition, the server updates (see details of the server update in Section 3.2) aggregated model utilizing the shared insensitive server data and the adaptive momentum-based optimization method (see details in Section 3.3) in Step . Furthermore, in a predefined round, the server performs model pruning based on the accuracy of the global model and the non-IID degrees (see details in Section 3.4). While Steps \u2460 \u2463 resemble those within traditional FL, we propose exploiting the insensitive server data to improve the performance of the global model (in FedDU) with adaptive momentum-based optimization (in FedDUM) on both the server and device sides (Step ) and an adaptive model pruning method (in FedAP) to improve the efficiency of the training process (Step )."}, {"title": "3.2 Server Update", "content": "In this section, we explain the design of FedDU, which exploit both the shared insensitive server data and the distributed sensitive device data to adjust the global model. We exploit the non-IID degrees and the model accuracy to determine the weights of aggregated model and those of the normalized gradients based on the server data in FedDU, so as to avoid over-fitting to the server data.\nWe assume that the size of the insensitive server data is significantly bigger than the dataset within a single device. In this case, the straightforward combination of the aggregated model from the devices and the gradients calculated based on the shared insensitive server data leads to inferior accuracy due to objective inconsistency [71]. Inspired by [71], we normalize the gradients generated using the server data to deal with this issue. The model aggregation in FedDU is formulated as Formula 4.\n$w^t = w^{t-1} - \\eta \\frac{\\tau}{T_{eff}} g^{(t-1)} (w^{t-1}),$\nwhere $w^t$ refers to the weights of the global model in Round t, $w^{t'}$ is the weights of the aggregated model from the selected devices in Round t as formulated in Formula 5 [5], $\\frac{\\tau}{T_{eff}}$ reff corresponds to the size of effective steps for normalized gradients calculated based on the shared insensitive server data as shown in Formula 7, $\\eta$ represents the learning rate, $g^{(t-1)} ()$ refers to the normalized gradients generated based on the server data in Round t as formulated in Formula 6.\n$w^{t'} = \\sum\\limits_{k \\in \\Delta_t} \\frac{n_k}{n'} (w^{t-1} - \\eta g_k^{t-1} (w^{t-1})),$"}, {"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "content": "where $n' = \\sum_{k \\in \\Delta_t} n_k$ refers to the size of the dataset on all the selected devices, $g_k^{t-1} (\\cdot)$ corresponds to the gradients generated using the dataset on Device k.\n$g^{(t-1)} (w^{t-1}) = \\frac{\\sum\\limits_{i=1}^{t} g_0^{(t-1)}(w^{t-i})}{\\tau/B},$\nwhere $w^{t'}$ represents the parameters of the global model after aggregating the gradients based on the server data, at i-th iteration of Round t, $g_0^{t-1} (\\cdot)$ refers to the gradients calculated using the server data, and $\\tau = [\\frac{n_0}{B} |E]$ corresponds to the number of iterations of Round t, E is the number of local epochs, B is the batch size. Each round corresponds to multiple iterations with a mini-batch of sampled shared insensitive server data.\nWhile reff has a significant impact on the training process, we dynamically choose an effective step size with the consideration of the model accuracy, the non-IID degrees of both the server data and the device data, with the number of rounds, as formulated in Formula 7.\n$T_{eff} = f'(acc^t) * \\frac{n_0 * D(\\overline{P})}{n_0 * D(\\overline{P}) + n' * D(P_0)} * C * decay^t * \\tau,$\nwhere $acc^t$ represents the model accuracy calculated based on the server data in Round t, i.e., $w^{t'}$ defined in Formula 5, $n_0$ corresponds to the size of the shared insensitive server data, D(\u00b7) is shown in Formula 2, $\\overline{P_r^t} = \\frac{\\sum_{k \\in \\Delta_t} n_kP_k}{\\sum_{k \\in \\Delta_t} n_k}$ refers to the distribution of the distributed sensitive data in all the chosen devices in Round t, $P_0$ is the distribution of the shared insensitive server data, decay \u2208 (0, 1) is utilized to ensure the convergence of the global model towards the solution of Formula 1, and C corresponds to a hyper-parameter. $f'(acc)$ is calculated using acc. acc is small at the beginning of the training. In this stage, the value of $f'(acc)$ should be prominent so as to take advantage of the server data to improve the accuracy of the global model. Then, at a late stage of the training process, $f'(acc)$ should be small to attain the objective defined in Formula 1 while avoiding over-fitting to the shared insensitive server data.\nWhen the distribution of server data resembles that of the overall device data, i.e., $D(P_0)$ is small, or the distribution of the data of the chosen devices differs much from the overall device data, i.e., $D(\\overline{P_r^t})$ is significant, we take a significant value of reff to improve the accuracy of the global model. Furthermore, the data size improves the importance of the dataset, as well. In addition, the weight of the device data is $\\frac{n' * D(\\overline{P_r^t})}{n_0 * D(P_0)+ n' * D(\\overline{P_r^t})}$, and that of the server data is $\\frac{n_0 * D(P_0)}{n_0 * D(P_0)+ n' * D(\\overline{P_r^t})}$, as the importance of the server data, which is the second part of Formula 7.\nAlgorithm 1 explains FedDU. The model is updated using the local dataset on each chosen device (Lines 1 - 3). Afterward, the models are aggregated using Formula 5 (Line 4). Finally, the aggregated model is updated utilizing the shared insensitive server data based on Formula 4 (Line 5).\nLet us assume that the expected squared norm of stochastic gradients on the server is bounded, i.e., $E ||g_0||^2 \\le G^2$. Then, in Round t, $T_{eff} \\le C \\cdot decay^{t-1} \\cdot \\tau$. Afterward, the server update term can be less than $C \\cdot decay^{t-1} \\cdot \\tau \\cdot \\eta \\cdot G$. In this case, after sufficient steps, the server update becomes negligible, i.e., $E[lim_{t\\rightarrow\\infty} \\eta \\frac{\\tau}{T_{eff}} g^{t-1} (w^{t-1})] < lim_{t\\rightarrow\\infty} C \\cdot decay^{t-1} \\cdot \\tau \\cdot \\eta \\cdot G = 0$, with 0 < decay < 1. Finally, FedDU degrades to FedAvg [5], the convergence of which is guaranteed with a decaying learning rate \u03b7 [72, 73]."}, {"title": "3.3 Momentum-based Optimization", "content": "In order to further improve the accuracy of the global model, we exploit momentum within the server update on the server and within the local iteration on each device. In this section, we propose a simple adaptive momentum approach, i.e., FedDUM, which enables the optimization on both the server and the devices without additional communication cost.\nThe centralized SGDM can be formulated as:\n$m^t = \\beta * m^{t-1} + (1 - \\beta) * g(w^{t-1}), w^t = w^{t-1} - \\eta * m^t,$\nwhere m is momentum. A simple application of the momentum into the FL environment is to decompose the SGDM to each device while aggregating the momentum as formulated in Formula 9.\n$m_k^t = \\beta * m_k^{t-1} + (1 - \\beta) * g_k(w^{t-1}),$\n$w_k^t = w_k^{t-1} - \\eta * m_k^t,$\n$m^t = \\sum\\limits_{k \\in \\Delta_t} \\frac{n_k}{n'} m_k^{t-1},$\n$w^t = \\sum\\limits_{k \\in \\Delta_t} \\frac{n_k}{n'} w_k^{t-1}.$\nIn order to extend the optimization into multiple local epochs, we can formulate the process as follows:\n$m_{k}^{t,t'} = \\beta' * m_{k}^{t,t' -1} + (1 - \\beta') * g'_k(w_{k}^{t,t' -1}),$\n$w_{k}^{t,t'} = w_{k}^{t,t' -1} - \\eta' m_{k}^{t,t'},$\n$m^t = \\sum\\limits_{k \\in \\Delta_t} \\frac{n_k}{n'} m_{k}^{t,E-1},$\n$w^t = \\sum\\limits_{k \\in \\Delta_t} \\frac{n_k}{n'} w_{k}^{t,E-1}.$"}, {"title": "3.4 Adaptive Pruning", "content": "In this section, we present our layer-adaptive pruning method, i.e., FedAP. FedAP considers the diverse features of each layer, the non-IID degrees of both the sensitive shared server data and the distributed sensitive device data, to remove useless filters in the convolutional layers while preserving the accuracy. FedAP can reduce the communication overhead and the computational costs of the training process. Please note that FedAP is performed only once on the server in a predefined round.\nAlgorithm 3 details FedAP. On the server and each device (Lines 2 - 4), we calculate a proper pruning rate based on the shared sensitive server data or the distributed sensitive device data (Line 3). We denote the initial model $W_k$ on the server or Device k. At Round T, the updated model is denoted by W. Then, the difference between the current model and the original one is $\u2206_k = W_k \u2013 W$. Afterward, we can calculate the Hessian matrix, i.e., H(W). We sort the eigenvalues of H(W) in ascending order, i.e., {$x_{km} | m \u2208 (1, d_k)$\\} with m referring the index of an eigenvalue and $d_k$ referring to the rank of the Hessian matrix. We take $B_k(\u2206_k) = H(W) \u2013 \u2207L(\u2206_k + W)$ as a base function with \u2207L(\u00b7) corresponding to the gradients. We denote the Lipschitz constant of $B_k (\u2206_k)$ as $L_k$. Inspired by [62], we take the first $m_k$, which meets the condition of $\u03bb_{m_k+1} \u2013 \u03bb_{m_k} > 4L_k$, to reduce accuracy degradation. Then, we can calculate the proper pruning rate by $p_k = \\frac{m_k}{d_k}$, which is the ratio between the size of pruned eigenvalues and that of all the eigenvalues. As the proper pruning rate significantly differs in diverse devices due to the non-IID distribution, we utilize Formula 15 to generate an aggregated proper pruning rate for the global model (Line 5).\n$p^* = \\sum\\limits_{k=0}^n  \\frac{n_k}{\\sum\\limits_{k'=0}^n n_{k'}}* \\frac{D(P_k) + \\epsilon}{\\sum\\limits_{k'=0}^n D(P_{k'}) + \\epsilon} * p_k,$"}, {"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "content": "where e represents a small value to avoid the division of zero. We take a global threshold value (1) calculated based on the aggregated proper pruning rate to serve as a baseline value for generating the pruning rate of each layer. V is the absolute value of the [R * p*]-th smallest parameter in all the parameters (Lines 6 and 7). Afterward, for each convolutional layer (Line 8), we calculate the proper pruning rate by calculating the ratio between the number of parameters with"}, {"title": "THEOREM 3.1. Local momentum deviates from the centralized one at linear rate $O(e^{\u039b+E})$.", "content": "PROOF. From the perspective of dynamics of Ordinary Differential Equations (ODE) [32], we can formulate the difference between the local optimization and the centralized optimization as:\n$\\left|\\left|m_{k}^{t, E-1} - \\bar{m}_{k}^{t, E-1}\\right|\\right| + \\left|\\left|w_{k}^{t, E-1} - \\bar{w}_{k}^{t, E-1}\\right|\\right| \\le \\Lambda E \\left|\\left|m_{w_{k}^{t, 0} - \\bar{w}_{k}^{t, 0}}\\right|\\right| + (1 - \\beta') \\int_{0}^{E-1} e^{\\Lambda(E-t')} (\\\\Rt,t' \\\\\\\\\\\\\\\\\\\\\\/\\eta') dt'.$"}, {"title": "4 EXPERIMENTS", "content": "In this section, we demonstrate the experimental results to show the advantages of FedDUMAP by comparing it with state-of-the-art baselines, i.e., FedAvg [5], FedKT [4], FedDF [22], Data-sharing [1], Hybrid-FL [11], server-side momentum [25], device-side momentum [75], FedDA [32], HRank [34], IMC [62], and PruneFL [33]."}, {"title": "4.1 Experimental Setup", "content": "We evaluate FedDUMAP using an FL system consisting of a parameter server and 100 devices. In each round, we randomly choose 10 devices. We utilize two real-life datasets, i.e., CIFAR-10 and CIFAR-100 [76], in the experimentation."}, {"title": "4.2 Evaluation with non-IID Data", "content": "We first conduct comparison of FedDU with FedAvg, FedKT, FedDF, Data-sharing, and Hybrid-FL in terms of model accuracy. Then, we compare FedDUM with server-side momentum, device-side momentum, and FedDA. Afterward, we show the advantages of the adaptive pruning method, i.e., FedAP, with the comparison of FedAvg, IMC, HRank, and PruneFL, in terms of model efficiency. Last but not least, we demonstrate that FedDUMAP, which consists of both FedDUM and FedAP, significantly outperforms the eight state-of-the-art baselines in terms of accuracy, efficiency and computational cost. Finally, we present our ablation study."}, {"title": "4.2.1 Evaluation on FedDU", "content": "In this part, we compare FedDU with five baseline methods, i.e., FedAvg, FedKT, FedDF, Data-sharing, and Hybrid-FL. Afterward, we analyze the effect of teff, the effect of $f'(acc)$, the effect of C, and the effect of the non-IID degree of server data. Data-sharing transfers the shared server data to devices in order to combine the local data and the server data, which may have a smaller non-IID degree compared with the original local data, so as"}, {"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "content": "to improve the accuracy of the updated model on the device. Hybrid-FL takes the shared server data of significant size as an ordinary dataset on a device and utilizes the FedAvg algorithm to perform the training process.\nIn order to take advantage of the server data, we have significant effective steps at the beginning, which leads to a quick increase of accuracy. Then, at the end of the training, we reduce the effective steps and focus on the device data to achieve high accuracy. Figure 2 shows that FedDU corresponds to excellent efficiency and high accuracy compared with baseline methods with CIFAR-10. In addition, with more server data, FedDU can achieve better performance in terms of accuracy (up to 5.3% higher) for both CNN and VGG. As shown in Figures 3 and 4, FedDU leads to a higher accuracy compared with FedAvg (up to 5.7%), FedKT (up to 22.6%), FedDF (up to 5.0%), Data-sharing (up to 11.7%), and Hybrid-FL (up to 19.5%) for both CNN and ResNet when p = 5% and 10%. In contrast, Data-sharing has slightly higher accuracy (up to 1.2%) compared with FedDU for VGG, as the more complex model can be better trained with augmented data. Compared with FedDU, Data-sharing needs to transfer the server data to devices, which may incur privacy issues and corresponds to high communication overhead. In addition, Data-sharing leads to a much longer training time to achieve the accuracy of 0.6 (for CNN, up to 15.7 times slower) and 0.4 (for LeNet, up to 28.9 times slower"}]}