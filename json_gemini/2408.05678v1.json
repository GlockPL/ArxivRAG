{"title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data", "authors": ["JI LIU", "JUNCHENG JIA", "HONG ZHANG", "YUHUI YUN", "LEYE WANG", "YANG ZHOU", "HUAIYU DAI", "DEJING DOU"], "abstract": "Despite achieving remarkable performance, Federated Learning (FL) encounters two important problems, i.e., low training efficiency and limited computational resources. In this paper, we propose a new FL framework, i.e., FedDUMAP, with three original contributions, to leverage the shared insensitive data on the server in addition to the distributed data in edge devices so as to efficiently train a global model. First, we propose a simple dynamic server update algorithm, which takes advantage of the shared insensitive data on the server while dynamically adjusting the update steps on the server in order to speed up the convergence and improve the accuracy. Second, we propose an adaptive optimization method with the dynamic server update algorithm to exploit the global momentum on the server and each local device for superior accuracy. Third, we develop a layer-adaptive model pruning method to carry out specific pruning operations, which is adapted to the diverse features of each layer so as to attain an excellent trade-off between effectiveness and efficiency. Our proposed FL model, FedDUMAP, combines the three original techniques and has a significantly better performance compared with baseline approaches in terms of efficiency (up to 16.9 times faster), accuracy (up to 20.4% higher), and computational cost (up to 62.6% smaller).", "sections": [{"title": "1 INTRODUCTION", "content": "With a large quantity of data distributed in numerous edge devices, cloud servers may have some shared insensitive data for efficient processing [1]. Due to data privacy and security concerns, multiple legal restrictions [2, 3] have been put into practice, making it complicated to aggregate the distributed sensitive data within a single high-performance server. As big amounts of training data generally lead to high performance of machine learning models [4], Federated Learning (FL) [5\u20137] becomes a promising approach to collaboratively train a model with considerable distributed data while avoiding raw data transfer.\nTraditional FL exploits non-Independent and Identically Distributed (non-IID) data on mobile devices to collaboratively train a global model [5]. Within the training process, the weights or the gradients of the global model are transferred between the devices and the server while the raw data stays in each device. FL typically utilizes a parameter server architecture [6, 8], where a parameter server (server) coordinates the distributed training process. The training process generally consists of multiple rounds, each composed of three stages. First, the server chooses a set of devices and broadcasts the global model to them. Second, each of the chosen devices trains the received model using local data and then sends back the updated model. Third, the server gathers the updated models and aggregates them to form a new global model. This process continues when the predefined condition, e.g., a maximum round number or the convergence of the global model, is not achieved.\nAlthough keeping data within the devices can protect data privacy and security, FL encounters two major challenges, which hinder its application in real-world environments. The first challenge is the limited data within a single edge device, which leads to ineffective local training while potential large amounts of shared insensitive data remain useless on the server. The second challenge is the modest edge devices with limited communication and computation capacity [9], which corresponds to inefficient local training on devices. To address the first challenge, we leverage the shared insensitive data on the server and design a new FL framework to improve the accuracy of the global model. To address the second challenge, we propose an adaptive optimization method and an adaptive pruning method within the proposed FL framework to improve model accuracy and reduce training cost.\nIn addition to the distributed data in edge devices, insensitive data can be transferred to the servers in the cloud to leave space for critical sensitive personal data [10, 11], e.g., Amazon Cloud [12], Microsoft Azure [13], and Baidu AI Cloud [14]. The insensitive data may exist on the server by default before the training process. For instance, the server may have prior learning tasks, either training or inference, which have already collected some data to be re-used for the new task of interest. In addition, some end users may offload some insensitive data to the server with certain incentive mechanism such as for crow-sourcing tasks [15], or simply to free up some local storage space [16]. Some other recent works are based on the insensitive shared data on the server [17\u201319]. The shared insensitive data can help improve the efficiency of the FL training process without the restriction of data distribution (see details in Section 3.2). The shared insensitive data can be transferred to the devices for local training in order to improve the accuracy of the global model [1, 20]. Although the shared data on the server is not sensitive, they still contain some important personal information. Thus, transferring the shared data to devices may still incur privacy or security problem. In addition, this approach leads to significantly high communication overhead. Furthermore, the existing approaches are inefficient when the shared server data is simply processed as that in edge devices [11] or when knowledge transfer is utilized to deal with heterogeneous models [21, 22].\nAdaptive optimization methods, such as Stochastic Gradient Descent (SGD) with Momentum (SGDM) [23], Adaptive Moment Estimation (Adam), and Adaptive Gradient (AdaGrad), have gained superb advance in accelerating the training speed and the final accuracy. Most of the existing FL literature adopts the adaptive optimization methods either on the server side [24\u201327] or on the device side [27, 27\u201331]. However, applying the adaptive optimization on either of them often leads to inefficient learning results with inferior accuracy. The adaptive optimization methods can be exploited on both sides, as shown in [32], but the momentum transfer may incur severe communication costs with limited bandwidth between the devices and the server.\nModel pruning reduces the size of a model into a slim one while the accuracy remains acceptable. These kinds of methods can be exploited in the training process to improve the efficiency of the training and to significantly reduce the overhead brought by a big model [33]. However, existing pruning approaches incur severe accuracy degradation. The simple application of existing pruning methods in FL does not consider the diverse dimensions and features of each layer [34]. In addition, as a big model may consist of numerous neurons, simple model pruning strategies cannot choose proper portions of the model to prune and may lead to low training efficiency and inferior accuracy [1].\nIn this work, we introduce a novel efficient FL framework, i.e., FedDUMAP, which enables collaborative training of a global model based on the sensitive device data and insensitive server data with a powerful server and multiple modest devices. We denote the sensitive data as the data that contain personal information and cannot be shared or transferred according to the users. In addition, the insensitive data can contain personal information while it can be transferred to a cloud server. In order to handle the two aforementioned problems, we utilize the shared insensitive data on the server to improve the accuracy of the global model with adaptive optimization with the consideration of the non-IID degrees, which represent the difference between a dataset (either on the server or a device) and the global dataset, i.e., the data on all the devices.\nFedDUMAP consists of three modules, i.e., FedDU, FedDUM, and FedAP. FedDU is an FL algorithm, which dynamically updates the global model based on the distributed sensitive device data and the shared insensitive server data. In addition, FedDU dynamically adjusts the global model based on the accuracy and the non-IID degrees of the server data and the device data. Furthermore, we propose a novel adaptive optimization method on top of FedDU, i.e., FedDUM, to improve the accuracy of the model without transferring the momentum from devices to the server or from the server to the devices. We decouple the optimization on the server and the device sides while exploiting the model generated with adaptive optimization from each device to enable the adaptive optimization on the server side. Besides, FedAP removes useless neurons in the global model to reduce the model size so as to reduce the computational overheads and the computation costs on devices without degrading noticeable accuracy. FedAP considers the features of each layer to identify a proper pruning rate, based on the non-IID degrees. Furthermore, FedAP prunes the neurons according to the rank values, which can preserve the performance of the model. To the best of our knowledge, we are among the first to propose exploiting non-IID degrees for dynamic global model updates while utilizing adaptive optimization and adaptive pruning for FL. This manuscript is an extension of a conference version [35]. In this paper, we make four following contributions:\n(1) A novel dynamic FL algorithm, i.e., FedDU, which utilizes both shared insensitive server data and distributed sensitive device data to collaboratively train a global model. FedDU dynamically updates the global model with the consideration of the model accuracy, normalized gradients from devices, and the non-IID degrees of both the server data and the device data.\n(2) A new adaptive optimization method, i.e., FedDUM, which decouples the optimization between the server side and the device side while exploiting the models generated from devices for the optimization on the server side without additional communication cost."}, {"title": "2 RELATED WORK", "content": "FL was proposed to train a model using the distributed data within multiple devices while only transferring the model or gradients [5]. Some works ([36\u201345] and references therein) either focus on the device scheduling or the model aggregation within the server or even with a hierarchical architecture to improve the accuracy of the global model, which only deals with the distributed device data. In order to leave space for sensitive data on devices and with incentive mechanisms [11], some insensitive data are transferred to the server or the cloud, which can be directly utilized for training. When all the data are transferred to the server, the server data can be considered as IID data [11]. However, transferring all the data including the sensitive data incurs severe privacy and security issues with significant communication costs, which is not realistic [46]. Some existing works utilize the insensitive server with heterogeneous models [47], or within one-shot model training [4], based on knowledge transfer methods [48, 49], or label-free data [22, 50]. However, the aforementioned approaches are inefficient [4, 47] or ineffective [22] without the consideration of the non-IID degrees or big models. Furthermore, while it may lead to significant communication overhead, the transfer of the insensitive server data to devices [1, 20] may incur severe privacy and security issues. The proper devices can be selected for training based on the server data [51], which can be integrated with our proposed approach.\nWithout adaptive optimization within the model aggregation process, traditional methods, e.g., FedAvg [5], may incur the client drift problem, which makes the global model over-fitted to local device data [52]. While control parameters [52] can help alleviate the problem, they require the devices to participate all through the training process [52, 53], which may not be feasible in cross-device FL. Adaptive optimization methods, e.g., AdaGrad [24], Yogi [25], Adam [26, 27] and momentum [32, 54\u201356] can be exploited to address the client drift problem. However, the existing approaches generally consider only one side, i.e., either server side [57] or device side [28], which may lead to inferior accuracy. Furthermore, the devices may have heterogeneous non-IID data, which makes the direct application of the momentum method insufficient [29]. Although adaptive optimization is applied on both the device side and the server side in some recent works [32], the communication of momentum between the server and devices may incur high communication costs.\nModel pruning can reduce the size of the model, which corresponds to smaller communication overhead and com-putation costs compared with the original model within the training process [33, 58\u201360] and the inference process [61] of FL. However, the shared insensitive server data is seldom utilized. Two types of techniques exist for the pruning process, including filter (structured) pruning and weight (unstructured) pruning. The unstructured pruning set the chosen parameters to 0 while the structure of the original model remains unchanged. Although the accuracy remains almost the same as that of the original one [62] while reducing communication overhead [33], the unstructured pruning cannot reduce"}, {"title": "3 METHOD", "content": "In this section, we propose our FL framework, i.e., FedDUMAP. We first present the system model. Then, we detail FedDU, which dynamically updates the global model. Afterward, we explain the adaptive momentum-based optimization, i.e., FedDUM, to improve the accuracy of the global model. Finally, we reveal the design of FedAP, which reduces the size of the global model to reduce communication and computational costs."}, {"title": "3.1 System Model", "content": "Figure 1 shows the training process of FedDUMAP, where the FL system consists of a server and N edge devices. The description of major notations is summarized in Table 1. We consider a powerful server and modest devices. The shared insensitive data is stored on the server, and the distributed sensitive data is dispersed in edge devices. Both the server data and the device data can be utilized to update the model during the FL training process of FL. Let us assume that a dataset $D_k = \\{x_{k,j}, Y_{k,j}\\}_{j=1}^{n_k}$, composed of $n_k$ samples, resides on Device k. We take $D_0$ as the server data, and $D_k, k \\neq 0$ as the device data. $x_{k,j}$ refers to the j-th sample on Device k, while $y_{k,j}$ represents the corresponding label. We utilize $\\mathcal{X}$ to represent the set of samples and $\\mathcal{Y}$ to represent the set of labels. Then, we formulate the objective of the training process as follows:\n$\\min_\\mathcal{W} F(w)$, with $F(w) = \\frac{1}{N} \\sum_{k=1}^{M} n_kF_k(w)$,\t\t\t\t\t\t\t\t\t(1)\nwhere $F_k(w) = \\frac{1}{n_k} \\sum_{\\{x_{k,j};Y_{k,j}\\} \\in D_k} f(w, x_{k, j}, Y_{k,j})$ represents the loss function on Device k with $f (w, x_{k,j}, Y_{k,j})$ capturing the error of the inference results based on the model with the sample pair $\\{x_{k,j}, Y_{k,j}\\}$ and w refers to the global model.\nWith the distributed non-IID data in FL [36], we utilize the Jensen-Shannon (JS) divergence [67] to quantize the non-IID degree of a dataset as shown in Formula 2:\n$D(P_K) = \\frac{1}{2}D_{KL}(P_k||P_m) + D_{KL}(P||P_m)$,\t\t\t\t\t\t\t\t\t(2)"}, {"title": "3.2 Server Update", "content": "In this section, we explain the design of FedDU, which exploit both the shared insensitive server data and the distributed sensitive device data to adjust the global model. We exploit the non-IID degrees and the model accuracy to determine the weights of aggregated model and those of the normalized gradients based on the server data in FedDU, so as to avoid over-fitting to the server data.\nWe assume that the size of the insensitive server data is significantly bigger than the dataset within a single device. In this case, the straightforward combination of the aggregated model from the devices and the gradients calculated based on the shared insensitive server data leads to inferior accuracy due to objective inconsistency [71]. Inspired by [71], we normalize the gradients generated using the server data to deal with this issue. The model aggregation in FedDU is formulated as Formula 4.\n$w^t = w^{t-1}-\\frac{\\eta}{\\tau_{eff}}*g^{(t-1)}(w^{t-1})$,\t\t\t\t\t\t\t\t\t(4)\nwhere $w^t$ refers to the weights of the global model in Round t, $w^{\\overline{t-1}}$ is the weights of the aggregated model from the selected devices in Round t as formulated in Formula 5 [5], $\\tau_{eff}$ corresponds to the size of effective steps for normalized gradients calculated based on the shared insensitive server data as shown in Formula 7, $\\eta$ represents the learning rate, $g^{(t-1)}$ refers to the normalized gradients generated based on the server data in Round t as formulated in Formula 6.\n$w^{\\overline{t-1}}= \\sum_{k\\in D_t} \\frac{n_k}{n'}(w^{t-1}-\\eta g^{t-1}(w^{t-1}))$,\t\t\t\t\t\t\t\t\t(5)"}, {"title": "3.3 Momentum-based Optimization", "content": "In order to further improve the accuracy of the global model, we exploit momentum within the server update on the server and within the local iteration on each device. In this section, we propose a simple adaptive momentum approach, i.e., FedDUM, which enables the optimization on both the server and the devices without additional communication cost.\nThe centralized SGDM can be formulated as:\n$m^t = \\beta * m^{t-1} + (1 - \\beta) * g(w^{t-1}), w^t = w^{t-1} - \\eta * m^t$,\t\t\t\t\t\t\t\t\t(8)\nwhere m is momentum. A simple application of the momentum into the FL environment is to decompose the SGDM to each device while aggregating the momentum as formulated in Formula 9.\n$m_k^t = \\beta * m_k^{t-1} + (1 - \\beta) * g_k(w^{t-1})$,\n$w_k^t = w^{t-1} - \\eta * m_k^t$,\n$m^t = \\sum_{k\\in D_t} \\frac{n_k}{n'} m_k^{t-1}$,\n$w^t = \\sum_{k\\in D_t} \\frac{n_k}{n'} w_k^{t-1}$.\nIn order to extend the optimization into multiple local epochs, we can formulate the process as follows:\n$m_{k,t'}^{t'} = \\beta' * m_{k}^{t'-1} + (1 - \\beta') * g'_k(w_{t,t'-1})$,\n$w_{k}^{t'} = w_{k}^{t'-1} - \\eta' * m_{k,t'}^{t'}$,\n$m^t = \\sum_{k\\in D_t} \\frac{n_k}{n'} m_{k,E-1}$,\n$w^t = \\sum_{k\\in D_t} \\frac{n_k}{n'} w_{k,E-1}$.\t\t\t\t\t\t\t\t\t(10)"}, {"title": "3.4 Adaptive Pruning", "content": "In this section, we present our layer-adaptive pruning method, i.e., FedAP. FedAP considers the diverse features of each layer, the non-IID degrees of both the sensitive shared server data and the distributed sensitive device data, to remove useless filters in the convolutional layers while preserving the accuracy. FedAP can reduce the communication overhead and the computational costs of the training process. Please note that FedAP is performed only once on the server in a predefined round.\nAlgorithm 3 details FedAP. On the server and each device (Lines 2 - 4), we calculate a proper pruning rate based on the shared sensitive server data or the distributed sensitive device data (Line 3). We denote the initial model $W_k$ on the server or Device k. At Round T, the updated model is denoted by W. Then, the difference between the current model and the original one is $\\Delta_k = W_k \u2013 W$. Afterward, we can calculate the Hessian matrix, i.e., H(W). We sort the eigenvalues of H(W) in ascending order, i.e., $\\{x_m|m \\in (1, d_k)\\}$ with m referring the index of an eigenvalue and $d_k$ referring to the rank of the Hessian matrix. We take $B_k(\\Delta_k) = H(W) \u2013 \\nabla L(\\Delta_k + W)$ as a base function with $\\nabla L(\\cdot)$ corresponding to the gradients. We denote the Lipschitz constant of $B_k(\\Delta_k)$ as $L_k$. Inspired by [62], we take the first $m_k$, which meets the condition of $\\lambda_{m_k+1} \u2013 \\lambda_{m_k} > 4L_k$, to reduce accuracy degradation. Then, we can calculate the proper pruning rate by $p_k = \\frac{m_k}{d_k}$, which is the ratio between the size of pruned eigenvalues and that of all the eigenvalues. As the proper pruning rate significantly differs in diverse devices due to the non-IID distribution, we utilize Formula 15 to generate an aggregated proper pruning rate for the global model (Line 5).\n$p^* = \\sum_{k=0}^{n} \\frac{\\frac{n_k}{D(P_k)+\\epsilon}}{\\sum_{k=0}^{n} \\frac{n_{k'}}{D(P_{k'})+\\epsilon}} * p_k$\t\t\t\t\t\t\t\t\t(15)\nwhere $\\epsilon$ represents a small value to avoid the division of zero. We take a global threshold value $\\upsilon$ calculated based on the aggregated proper pruning rate to serve as a baseline value for generating the pruning rate of each layer. $\\mathcal{V}$ is the absolute value of the $[R * p^*]$-th smallest parameter in all the parameters (Lines 6 and 7). Afterward, for each convolutional layer (Line 8), we calculate the proper pruning rate by calculating the ratio between the number of parameters with"}, {"title": "4 EXPERIMENTS", "content": "In this section, we demonstrate the experimental results to show the advantages of FedDUMAP by comparing it with state-of-the-art baselines, i.e., FedAvg [5], FedKT [4], FedDF [22], Data-sharing [1], Hybrid-FL [11], server-side momentum [25], device-side momentum [75], FedDA [32], HRank [34], IMC [62], and PruneFL [33]."}, {"title": "4.1 Experimental Setup", "content": "We evaluate FedDUMAP using an FL system consisting of a parameter server and 100 devices. In each round, we randomly choose 10 devices. We utilize two real-life datasets, i.e., CIFAR-10 and CIFAR-100 [76], in the experimentation."}, {"title": "4.2 Evaluation with non-IID Data", "content": "We first conduct comparison of FedDU with FedAvg, FedKT, FedDF, Data-sharing, and Hybrid-FL in terms of model accuracy. Then, we compare FedDUM with server-side momentum, device-side momentum, and FedDA. Afterward, we show the advantages of the adaptive pruning method, i.e., FedAP, with the comparison of FedAvg, IMC, HRank, and PruneFL, in terms of model efficiency. Last but not least, we demonstrate that FedDUMAP, which consists of both FedDUM and FedAP, significantly outperforms the eight state-of-the-art baselines in terms of accuracy, efficiency and computational cost. Finally, we present our ablation study."}, {"title": "4.2.1 Evaluation on FedDU", "content": "In this part, we compare FedDU with five baseline methods, i.e., FedAvg, FedKT, FedDF, Data-sharing, and Hybrid-FL. Afterward, we analyze the effect of $\\tau_{eff}$, the effect of $f' (acc)$, the effect of C, and the effect of the non-IID degree of server data. Data-sharing transfers the shared server data to devices in order to combine the local data and the server data, which may have a smaller non-IID degree compared with the original local data, so as"}, {"title": "4.2.2 Evaluation on FedDUM", "content": "In this section, we compare FedDUM with two adapted adaptive optimization methods, i.e., server-side momentum (FedDU-SM) [25] and device-side momentum (FedDU-DM) [75], as well as FedDA (FedDU-DA) [32]. For a fair comparison, each optimization method is combined with FedDU to utilize the server data in order to improve the accuracy."}, {"title": "4.2.3 Evaluation on FedAP", "content": "In this section, we compare FedAP with three baseline methods, i.e., HRank [34], IMC [62], and PruneFL [33]. We utilize HRank with the shared insensitive server data with multiple pruning rates, e.g., 0.2, 0.4, 0.6, and 0.8, within the FL training process. Similarly, we exploit IMC based on the server data in the FL training process. HRank is a structured pruning method, while PruneFL and IMC are unstructured techniques."}, {"title": "4.2.4 Evaluation on FedDUMAP", "content": "In this section, we compare FedDUMAP, consisting of both FedDUM and FedAP, with the baseline approaches. We present the comparison results of CNN, VGG, LeNet with CIRAR-10 and CIFAR-100."}, {"title": "4.2.5 Ablation Study", "content": "We conduct the ablation study by measuring the final accuracy, the training time to achieve the accuracy of 0.6 for CNN and VGG, 0.3 for LeNet with CIFAR-10, and the computational cost of FedAvg, FedDU, FedDUM, FedAP, FedDUAP, and FedDUMAP. As shown in Figure 13 and Table 12, the efficiency of FedDUMAP significantly outperforms FedDU, FedDUM, and FedAvg (up to 2.4 times faster), while the accuracy of FedDUMAP is much higher than that of FedDU, FedAP, and FedAvg (up to 6.2%). In addition, the computational cost of FedDUMAP is the smallest for CNN, while it is slightly higher (2.1%) than FedAP for VGG due to the dynamic server update. Although FedDUMAP achieves slightly lower accuracy compared with FedDUM (up to 1.0%), FedDUMAP is much more efficient than FedDU and FedDUM (up to 76.8% faster).\nIn order to reveal the advantages of FedDUMAP, we also conduct the ablation study with CIFAR-100. We measure the final accuracy, the training time to achieve the accuracy, and the computational cost of diverse methods.\nAs shown in Figure 14 and Table 13, the efficiency of FedDUMAP significantly outperforms FedDU (up to 2.3 times faster), FedDUM (up to 43.1% faster), and FedAvg (up to 2.3 times faster), while the accuracy of FedDUMAP is much higher than that of FedDU (up to 0.2%), FedAP (up to 6.3%), and FedAvg (up to 5.6%). FedDUMAP corresponds to the shortest training time with CNN, VGG, and LeNet. In addition, the computational cost of FedDUMAP is the smallest for LeNet while it is slightly higher than FedAP for CNN (16.0%) and VGG (3.5%) due to the dynamic server update. Furthermore, FedDUMAP has slightly higher accuracy (up to 1.4% for VGG) compared with that of FedDUM, and the accuracy of FedDUMAP is much higher (up to 5.6% for VGG) than that of FedAvg. In addition, FedDUMAP is much more efficient than FedDUM (up to 43.1% faster for CNN)."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a novel FL framework FedDUMAP, which leverages both the shared insensitive server data and the distributed sensitive device data to train the global model. Furthermore, the non-IID degrees of the data is also considered in the FL training process. FedDUMAP consists of a dynamic update FL algorithm, i.e., FedDU, a simple yet efficient adaptive optimization method on top of FedDU, i.e., FedDUM, and an adaptive pruning method, i.e., FedAP. We conduct extensive experimentation to evaluate the performance of FedDUMAP with different models and real-life datasets. According to the experimental results, FedDUMAP significantly outperforms the baseline approaches in terms of accuracy (up to 20.4% higher), efficiency (up to 16.9 times faster), and computational cost (up to 62.6% smaller)."}]}