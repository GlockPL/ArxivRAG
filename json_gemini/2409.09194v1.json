{"title": "HIERARCHICAL HYPERCOMPLEX NETWORK FOR\nMULTIMODAL EMOTION RECOGNITION", "authors": ["Eleonora Lopez", "Aurelio Uncini", "Danilo Comminiello"], "abstract": "Emotion recognition is relevant in various domains, ranging\nfrom healthcare to human-computer interaction. Physiolog-\nical signals, being beyond voluntary control, offer reliable\ninformation for this purpose, unlike speech and facial ex-\npressions which can be controlled at will. They reflect gen-\nuine emotional responses, devoid of conscious manipulation,\nthereby enhancing the credibility of emotion recognition sys-\ntems. Nonetheless, multimodal emotion recognition with\ndeep learning models remains a relatively unexplored field.\nIn this paper, we introduce a fully hypercomplex network\nwith a hierarchical learning structure to fully capture corre-\nlations. Specifically, at the encoder level, the model learns\nintra-modal relations among the different channels of each\ninput signal. Then, a hypercomplex fusion module learns\ninter-modal relations among the embeddings of the different\nmodalities. The main novelty is in exploiting intra-modal re-\nlations by endowing the encoders with parameterized hyper-\ncomplex convolutions (PHCs) that thanks to hypercomplex\nalgebra can capture inter-channel interactions within single\nmodalities. Instead, the fusion module comprises parameter-\nized hypercomplex multiplications (PHMs) that can model\ninter-modal correlations. The proposed architecture surpasses\nstate-of-the-art models on the MAHNOB-HCI dataset for\nemotion recognition, specifically in classifying valence and\narousal from electroencephalograms (EEGs) and peripheral\nphysiological signals. The code of this study is available at\nhttps://github.com/ispamm/MHyEEG.", "sections": [{"title": "1. INTRODUCTION", "content": "Advancing the Brain-Comuter Interface (BCI) by understand-\ning how the human brain encodes the world is at the core of\nneurocognitive research. A crucial part of this includes learn-\ning how emotions are related to brain signals such as elec-\ntroencephalograms (EEGs). By deciphering the neural re-\nsponses associated with different emotions, researchers can\nnot only improve our understanding of human cognition but\nalso pave the way for more sophisticated BCI systems capable\nof interpreting and responding to users' emotional states in\nreal-time. However, emotions are intrinsically multi-modal,\nthey are expressed through behavioral responses such as body\nlanguage, facial expressions, and speech, as well as involun-\ntary responses, which are reflected in physiological signals.\nIndeed, studies have shown that different emotions result in\ndifferent responses of the brain which yield specific EEG sig-\nnals [1]. Moreover, heart rhythm changes according to dif-\nferent emotions which can be detected through electrocardio-\ngrams (ECGs) [2]. Galvanic skin response (GSR) provides a\nmeasure of the resistance of the skin, which decreases when\none is experiencing emotions such as stress or surprise [3]. Fi-\nnally, also the eyes can reveal insights into what emotions are\nbeing experienced, e.g. it has been shown that pupil diameter\nchanges in different emotional states, increasing when feeling\nanger, fear, and anxiety or arousal and love [4]. Given these\nrelationships between physiological signals and emotional re-\nsponses, researchers are starting to turn to the physiological\napproach for emotion recognition [5]. Indeed, they are di-\nrectly related to real emotions, unlike behavioral reactions,\nwhich can lead to systems that are prone to fake emotions and\ncan be manipulated easily [6].\nNonetheless, existing works focus mostly on a single\nmodality [7, 8] or rely on hand-crafted features extracted\nfrom the raw signals [9]. Indeed, a multimodal approach\nyields a more powerful classifier since it takes into account\nthe complementary information given by the different modal-\nities, given that emotions are expressed in a multimodal\nway. Moreover, relying on handcrafted features requires\nextensive domain knowledge and represents a methodology\nrooted in the past. In contrast, contemporary deep learning"}, {"title": "2. RELATED WORKS", "content": "Many studies have developed machine learning [11, 12] and\ndeep learning-based [8, 13] systems for emotion recognition.\nNevertheless, these methods require either extensive domain\nknowledge for extracting handcrafted features or employ pop-\nular extracted features such as differential entropy (DE) or\npower spectral density (PSD). These approaches do not allow\nthe neural model to learn features on its own and require ad-\nditional preprocessing steps [9]. For these reasons, methods\nthat learn directly from raw signals have started to emerge. A\nrecent study has proposed a reinforcement learning approach\ninspired by brain emotion perception and shows the advan-\ntages of using raw signals [7]. Furthermore, another approach\nto increase the performance of an emotion recognition system\ninvolves designing a multimodal framework [9]. Indeed, re-\ncently proposed methods include a manifold learning-based\ntechnique for multimodal emotion recognition [14], a method\nto improve generalization across unseen target domains from\nEEG and eye movement signals [15], and a hypercomplex-\nbased architecture, HyperFuseNet, with a novel fusion mod-\nule [10]. In this paper, we tackle the limitations of hand-\ncrafted features and single-modality approaches by extend-\ning the method proposed in a preliminary study [10], Hyper-\nFuseNet. Although this model learns directly from raw mul-\ntimodal signals, it suffers from poor generalization ability. To\novercome this limitation, we introduce a hierarchical design,\nresulting in substantial improvements, as demonstrated by the\nexperimental results in Section 4."}, {"title": "3. METHOD", "content": null}, {"title": "3.1. Parameterized Hypercomplex Networks", "content": "Parameterized hypercomplex neural networks (PHNNs) have\nbeen proposed in order to overcome the limitations of the\npopular quaternion models while keeping their advantages\nand useful properties [16, 17]. Indeed, quaternion models\nare limited to 4-dimensional inputs as they work with quater-\nnions, i.e., $q = q_0 + q_1\\hat{i} + q_2\\hat{j} + q_3\\hat{k}$ where $q_i \\in R$ and\n$\\hat{i}, \\hat{j}, \\hat{k} \\in Q$. Instead, PHNNs are flexible to work with any\nn-dimensional data, where $n \\in N$ is a hyperparameter that\nusers can adjust based on their specific requirements. This\nflexibility is achieved through a specific construction of the\nweight matrix within the layers comprising the PHNNs. Clas-\nsic fully-connected and convolutional layers are equivalent\nto parameterized hypercomplex multiplication (PHM) layers"}, {"title": "3.2. Hierarchical Hypercomplex Model", "content": "In this section, we describe the proposed Hierarchical Hyper-\ncomplex (H2) model for multimodal emotion recognition, de-\npicted in Fig. 1. The architecture has a hierarchical structure\nwhere encoders operating in different hypercomplex domains\nlearn modality-specific embeddings, while the hypercomplex\nfusion module learns a fused embedding. Mainly, the hier-\narchical structure refers to the level of relations being con-\nsidered, i.e., intra-modality and inter-modality. The encoders\nmodel relations between channels within a single modality,\nthus they exploit intra-modality relations. In contrast, the hy-\npercomplex fusion module exploits relations among the dif-\nferent modalities themselves, i.e., inter-modal interactions.\nIn detail, the encoders of EEG, ECG, and eye signals com-\nprise two PHC layers with two batch normalization (BN) lay-\ners and ReLU activation functions. Instead, being GSR a 1-\ndimensional signal, its encoder is composed of a single PHM\nlayer together with a BN layer and ReLU. Then, a global\naverage pooling operation is applied to get the final latent\nrepresentation. For PHC and PHM layers, we set $N_{EEG} =\n10$, $N_{ECG} = 3$, $N_{eye} = 4$, and $N_{GSR} = 1$, in accordance with the"}, {"title": "4. EXPERIMENTAL RESULTS", "content": null}, {"title": "4.1. Dataset and preprocessing", "content": "For training and evaluating our models, we utilize a publicly\navailable database for affect recognition, MAHNOB-HCI [3].\nIt provides synchronized recordings of 27 participants during\nan experiment in which each subject was shown fragments of\nmovies that induced different emotional responses. In detail,\nthe subjects were monitored with video cameras, a head-worn\nmicrophone, an eye gaze tracker, as well as physiological sen-\nsors measuring EEG, ECG, respiration amplitude, and skin\ntemperature. In this work, we employ as modalities the EEG,\nECG, GSR, all recorded at 256Hz, and eye data, recorded at\n60Hz. The latter comprises gaze coordinates, eye distances,\nand pupil dimensions. Each synchronized recording is labeled\non a scale of arousal (calm, medium aroused, and excited) and\nvalence (unpleasant, neutral, and pleasant).\nWe apply the same preprocessing as in [10]. The dataset\nprovides EEG with 32 electrodes, among which we select 10\nmost related to emotion, i.e., F3, F4, F5, F6, F7, F8, T7, T8,\nP7, and P8 [19, 20]. EEG, ECG, and GSR are first downsam-\npled to 128Hz. Secondly, EEG signals are referenced to the\naverage reference. Then, EEG and ECG are filtered with a\nband-pass filter of 1-45Hz and 0.5-45Hz, respectively, a low-\npass filter for GSR at 60Hz, and a final notch filter at 50Hz for\neach of them. To account for the initial offset of GSR signals,\nbaseline correction is performed by adjusting it relative to the\nmean value within the preceding 200ms of each trial. For the\nother signals, this correction is automatically achieved after\nthe high-pass filters. Lastly, for eye data, we consider the av-\nerage between measurements of the left and right eye, and we\nmaintain -1 values as they indicate blinks or rapid movements\nwhich can be related to an emotional response.\nSamples are constructed by extracting segments of 10s\nfrom the original 30s of each trial. The dataset is split into\ntraining (80%) and testing (20%) in a stratified manner. The"}, {"title": "4.2. Experimental setup", "content": "For evaluating our models, we utilize as metrics the accuracy\nand the F1-score, i.e., the harmonic mean of the precision and\nrecall, which accounts for imbalanced data. The networks\nare trained with the Adam optimizer, cross-entropy loss, and\nthe once-cycle policy with 0.425% of increasing steps, linear\nannealing strategy, dividing factors of 10, maximum learning\nrate of 7.96 \u00d7 10-6, and minimum and maximum momentum\nof 0.7403 and 0.8314, respectively. The number of epochs is\nset to 50, but we early stop the training when the F1-score\ndoes not improve for 10 epochs."}, {"title": "4.3. Results and discussion", "content": "The main results of our experiments are reported in Tab. 1.\nOur model is compared against two state-of-the-art architec-\ntures that operate with raw signals, i.e., HyperFuseNet [10]\nand the multimodal model proposed by Dolmans et. al [21]\nwhich was originally designed for mental workload classifi-\ncation. The proposed hierarchical architecture significantly\noutperforms both HyperFuseNet and the other state-of-the-\nart model, as demonstrated by the results in Tab. 1. In both\narousal and valence classification, our model brings a sub-\nstantial improvement, i.e., of 40.20% and 57.11% for the F1-\nscore, respectively. This great gap is due to several reasons.\nFirst, the other two models are quite prone to overfitting, thus\nwhen tested on unseen trials they are not able to generalize\nwell. Instead, in our method we address these problems with\nadditional dropout layers with a higher rate with respect to\nHyperFuseNet (which only had one dropout layer), reducing\nthe overall number of layers by removing one in each encoder\nand fusion module, and, most importantly, integrating hyper-\ncomplex algebra also at encoder-level. Indeed, this last aspect\nallows to exploit both intra-modal and inter-modal relations\nin a hierarchical manner. This is the main advantage of the\nproposed model as we demonstrate in the ablation studies in\nthe following section. In fact, it is not due to just a reduction\nof parameters given by hypercomplex algebra as this could re-\nsult in underfitting, as we show in Section 4.4. Instead, thanks\nto its properties the encoders are able to leverage correlations\namong channels of single modalities, thus significantly out-\nperforming HyperFuseNet. Moreover, Figure 2 depicts the\nt-SNE visualization [22] of the features learned from H2 and\nHyperFuseNet. From the image, it is clear that HyperFuseNet\nstruggles to learn discriminant representations. In contrast, in\nthe features learned by the H2 model clusters start to become\ndiscernible, although there is room for further improvement."}, {"title": "4.4. Ablation studies", "content": "In this section, we study the impact of each added component\nof the proposed network H2 and we report the results in Tab. 2\nfor arousal and in Tab. 3 for valence. In the first line of both ta-\nbles, we show the results obtained with HyperFuseNet for ref-\nerence. We investigate first the impact of reducing the number\nof layers of the encoders and fusion module, as described in\nSection 3. This simple tweak already improves performance\nresulting in slightly less overfitting. Then, we substitute fully\nconnected layers with their hypercomplex counterpart, PHM\nlayers with $n_m$, $m \\in \\{EEG, GSR, ECG, eye\\}$ set as described\nin Section 3. In this scenario, the significant reduction in the\nnumber of parameters results in underperformance compared\nto using standard fully-connected layers. This also suggests\nthat fully connected layers may not be optimal for this stage of\nthe architecture in which the encoder should learn a modality-\nspecific representation from multidimensional signals. There-\nfore, we substitute these with standard convolutional layers\nin the real domain. This allows the model to overfit less\nwith respect to linear layers and improves the performance\nfor arousal classification. For valence classification, the mod-\nels are slightly less prone to overfit thus the performance in\nthis scenario is comparable to using fully-connected layers.\nFinally, we introduce hypercomplex algebra, thus integrating\nconvolutions in the hypercomplex domain, i.e., PHC layers,\nresulting in our final proposed H2 model with the hierarchi-\ncal learning structure. From the results, it is clear that this\nis the crucial component that allows to achieve such a great\nimprovement. Indeed, the parameters are reduced even more\nwith respect to PHM layers in the encoder, however, the com-"}, {"title": "5. CONCLUSION", "content": "In this work, we proposed a multimodal hierarchical fully-\nhypercomplex model for emotion recognition from EEG and\nperipheral physiological signals. Specifically, the network\nis equipped with a hierarchical learning structure, where en-\ncoders learn intra-modal correlations, i.e., they exploit both\nintra-channel and inter-channel relations of each signal, and\nthe hypercomplex fusion module learns inter-modal relations.\nOur main contribution is the introduction of hypercomplex al-\ngebra also at the encoder level with PHC operations, which\nresults in a hierarchical structure. We found that this sim-\nple step allows to achieve much better results than previous\nstate-of-the-art methods. Certainly, while the hypercomplex\nfusion module leverages inter-modal relations, the hypercom-\nplex encoders exploit intra-signal correlations which results\nin enriched latent representations and a final improvement of\n40.20% and 57.11% for the F1-score on arousal and valence\nclassification."}]}