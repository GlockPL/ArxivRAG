{"title": "To Ask or Not to Ask?\nDetecting Absence of Information in Vision and Language Navigation", "authors": ["Savitha Sam Abraham", "Sourav Garg", "Feras Dayoub"], "abstract": "Recent research in Vision Language Navigation (VLN)\nhas overlooked the development of agents' inquisitive abil-\nities, which allow them to ask clarifying questions when\ninstructions are incomplete. This paper addresses how\nagents can recognize \u201cwhen\u201d they lack sufficient informa-\ntion, without focusing on \"what\" is missing, particularly\nin VLN tasks with vague instructions. Equipping agents\nwith this ability enhances efficiency by reducing potential\ndigressions and seeking timely assistance. The challenge in\nidentifying such uncertain points is balancing between be-\ning overly cautious (high recall) and overly confident (high\nprecision). We propose an attention-based instruction-\nvagueness estimation module that learns associations be-\ntween instructions and the agent's trajectory. By leveraging\ninstruction-to-path alignment information during training,\nthe module's vagueness estimation performance improves\nby around 52% in terms of precision-recall balance. In our\nablative experiments, we also demonstrate the effectiveness\nof incorporating this additional instruction-to-path atten-\ntion network alongside the cross-modal attention networks\nwithin the navigator module. Our results show that the\nattention scores from the instruction-to-path attention net-\nwork serve as better indicators for estimating vagueness.", "sections": [{"title": "1. Introduction", "content": "Vision Language Navigation (VLN) is the task of en-\nabling a robot to navigate an environment based on a given\ninstruction. There has been substantial research in VLN in\nthe recent past [25]. The majority of previous research has\nfocused on training large neural models for VLN task using\nsynthetic datasets like R2R [2] that contained step-by-step\ninstructions and Reverie [16] that had high-level instruc-\ntions, among others. All these models operate under the\nassumption that agents are designed to act independently\nwithout human intervention. Even when dealing with ab-\nstract or high-level instructions, agents are trained to ex-\nplore and make decisions they deem best for progressing\ntowards their goals. There is no provision for indicating\nuncertainty or indecision about the next course of action.\nTypically, designers impose an upper time limit in these ap-\nproaches, prompting agents to cease operations after a pre-\ndefined number of moves, regardless of goal achievement.\nWhen a robot transitions to a real-world environment, it\noften encounters instructions from various individuals, each\nwith unique communication styles. Adapting to this diver-\nsity poses a challenge in generalizing across different in-\nstructing styles. While some individuals may prefer con-\ncise instructions, others might delve into details but present\nthem in a disorganized manner. A second person unfamiliar\nwith the instructor may find it challenging to follow the in-\nstructions easily. In such situations, humans typically seek\nclarification by asking questions like, \u201cI'm now in front of\nthe kitchen. Where should I go next?\". Implementing a\nsimilar ability is a desired feature in intelligent robots to\ntackle uncertainty due to vagueness in the input instruction.\""}, {"title": "2. Related Work", "content": "In this section we provide a brief survey of recent works\nrelated to vision language navigation and uncertainty aris-\ning from vagueness in input instruction."}, {"title": "2.1. Vision Language Navigation", "content": "Existing methods in VLN vary depending on the infor-\nmation used to predict the next location to move to. Chen\net al. [3] introduced HAMT (History Aware Multimodal\nTransformer), a framework that retains a record of all pre-\nviously visited and observed locations as historical data.\nThis information, along with the input instruction is then\nutilized to determine the next destination, typically chosen\nfrom among the neighboring locations of the current posi-\ntion. VLN-DUET (Dual Scale Graph Transformer), as de-\nscribed in [5], employs a detailed representation of the cur-\nrent observation derived from objects in the image instead\nof a coarse representation of the current location. Addition-\nally, it also maintains a topological map of the nodes visited\nthus far as in [3] to determine the subsequent node for nav-\nigation. This next node may include any unvisited nodes\nobserved thus far, expanding beyond the immediate lo-\ncal neighbors (performing global action planning). Recent\nstudies aim to create large-scale VLN datasets [4, 18] to en-\nhance the generalizability of VLN agents. Speaker follower\nmodels [9] and instruction following and generation mod-\nels [24] focus on refining instruction generation and inter-\npretation abilities of the agent. Numerous researchers have\nalso made significant efforts to enhance vision-language\nalignment in VLN by introducing a variety of pre-training\ntasks [7], [6], [26]. With the emergence of Large Language\nModels (LLMs), recent advancements in VLN have increas-\ningly incorporated LLMs in navigation [17,27].\nIn this paper, we evaluate the effectiveness and general-\nizability of our proposed IV estimation module by incorpo-\nrating it into two navigators, VLN-DUET and HAMT. We\nfirst examine how well these navigators, trained with de-\ntailed instructions from the R2R dataset [2], adapt to vague\ninstructions. We integrate our IV estimation module into\nboth to recognize potential absence of critical information,\nprompting them to seek assistance."}, {"title": "2.2. Knowing when you don't know", "content": "ULN [8] as mentioned in Section 1, introduced the\ndataset based on R2R for the problem of under-specified\nVLN. ULN modified the R2R dataset by adding a high-\nlevel instruction, that intentionally omitted certain details,\nalongside the original fine-grained instruction for each tra-\njectory. These high-level, vague instructions corresponding\nto the fine-grained instructions were manually created and\nvalidated. The paper handled vagueness in instruction by\nintroducing two separate modules - an instruction classifier\nand an uncertainty estimation module. The instruction clas-\nsifier classified an instruction as either high or low level.\nThe uncertainty estimation module predicts a score indicat-\ning the uncertainty of the next move, using candidate moves\nweighted by likelihood and attention scores. In this paper,\nwe introduce an attention-based IV estimation module that\nassesses the uncertainty arising from instruction vagueness\nin the predictions of a VLN model. We demonstrate that the\nalignment between the given instruction and the path taken\nso far is an effective metric for gauging IV. In contrast to\nthe approach in [8], where an instruction classifier labels\nan instruction as low or high-level (indicative of potential\nvagueness) based solely on the instruction content, we con-\ntend that vagueness arises not merely from the instruction\nitself, but from the interaction between the instruction and\nthe surrounding environment.\nVision Dialog Navigation (VDN) involves enabling\nrobots to seek help during navigation [22]. A related work\n[28] introduced an MLP-based uncertainty estimator that\nidentified uncertainty points based on the navigator's cur-\nrent state and was trained using pseudo-labels of uncertainty\nderived from the entropy of the navigator's predicted distri-\nbution. We compare this with our IV model, which uses\ninstruction-path attention, and show that our approach more\naccurately indicates uncertainty and generalizes across dif-\nferent navigators. We demonstrate that relying on the pre-\ndicted distribution's entropy can be misleading, as the nav-\nigator might confidently make incorrect predictions about\nthe next move.\nAnother recent work that explored uncertainty estima-\ntion is KnowNo [19]. It utilised conformal prediction to\ngauge uncertainty in the next-step prediction of a LLM-\nbased planner. Uncertainty due to instruction ambiguity is\nstudied in CLARA [15], where a LLM-based framework was\nused to discern whether a provided user input instruction\nis ambiguous or infeasible. It adopts the method of self-\nconsistency check involving sampling multiple responses\nfrom the LLM for the same query, and assessing the con-\nsistency among these responses. This approach is similar\nto the conventional ensemble method for uncertainty esti-\nmation [12]. It is not efficient to implement an ensemble-\nbased approach for VLN considering the complexity of the\ntask, which typically comprises individual networks for text\nencoding, image encoding, topological graph encoding and\ncross-attention. It requires training all these layers or a se-\nlected sub-network (text encoder) as done in [8] separately,\nfreezing all other layers. Our aim here is to avoid re-training\nany part of the VLN model, and limit the training to just the\nIV module."}, {"title": "3. Method", "content": "3.1. Problem Background and Formulation\nIn a VLN setup for discrete environments, the environ-\nment at any time step t is represented as an undirected graph\n$G_t = (V_t, E_t)$. $G_t$ represents the map of the environment\nexplored so far with $V_t = {N_i}_{i=1}^{K}$, being the set of nodes\nor locations observed so far and E denoting the connectivity\nbetween the locations. Nodes are flagged as either visited\nor navigable (potential next nodes). The graph starts with\nthe agent's initial position and updates at each time step to\ninclude the current position and new navigable nodes.\nThe input instruction $I = {w_i}_{i=1}^{L}$ is a sequence of word\nembeddings, with L words. The agent's goal is to navigate\nfrom its current position to the goal location described in the\ninstruction. The VLN model includes a text encoder ftext\n(typically a multi-layer transformer) that generates contex-"}, {"title": "3.2. Our approach: Instruction Vagueness Estima-\ntion", "content": "The architecture of fiv is shown in Figure 2. The vague-\nness in instruction at any point during traversal is modeled\nas a function of both the instruction and the path being un-\ndertaken. If node Nt is predicted as the most likely node to\nmove to at time stept by factionPredictor,\n$P_t = P_{t-1} + N_t$ \n$score_t^{uncertainty} = f_{iv} (P_t, \\hat{I})$\nHere, Pt-1 denotes the path taken by the agent until time\n(t-1). Appending Nt, the next move suggested by\nf_{actionPredictor} to it gives Pt, the most likely path up to time\nt. The goal of fiv is to determine the uncertainty arising\nfrom instruction vagueness in the prediction of factionPredictor.\nfiv employs a multi-head attention (MHA) network [23] to\nlearn the alignment between this anticipated path at time t\nand the instruction.\n$Instr\\_Path\\_Attnt = MultiHead(\\hat{I}, P_t, P_t)$ \n$MultiHead(\\hat{I}, P_t, P_t) = Concat(head_1, ..., head_h)W^O$ \n$head_i = Attention(\\hat{I}W_i^Q, P_tW_i^K, P_tW_i^V)$ \n$Attention(\\hat{I}, P_t, P_t) = softmax (\\frac{\\hat{I}(P_t)^T}{\\sqrt{d_k}})P_t$\nHere, Instr_Path_Attnt is an enhanced representation\nthat captures the alignment between \u00ce and the likely path,"}, {"title": "3.3. Pre-training task: Relevant Instruction Span\nIdentification", "content": "We propose a pre-training task that serves to initialize the\nweights of the multihead attention network in fiv. The task\nis a instruction to path alignment task. The objective of this\ntask is to predict the segment/chunk of the instruction that\nis most relevant when predicting the agent's move at time-\nstep t, given the path taken so far, Pt. It is formulated as\na sequence-to-sequence classification problem, where each\ntoken in the input instruction is classified as 0 or 1 based on\nits relevance. The task is defined as:\n$R_t = f_{pretrainAlign}(P_t, \\hat{I})$\nIn this context, \u00ce, a L \u00d7 dk dimensional representation of\nthe instruction, is transformed into a L \u00d7 1 dimensional rel-\nevance sequence. L denotes the number of tokens in the\ninstruction. The output sequence indicates the relevance of\neach token (assigned 1 if relevant, otherwise 0) in predicting\nthe move at time step t.\nThe architecture of the network for the pretraining task\nis illustrated in Figure 3. It consists of a multihead attention\nlayer followed by a BiLSTM network [11] that maps the\nrefined representation that incorporates instruction to path\nalignment information, to the relevance sequence [21]. We\nopt for a BiLSTM rather than a more complex transformer\nmodel due to data limitations.\nSupervision: We leverage the instruction to path alignment\ninformation in the dataset, Fine-Grained R2R (FGR2R)\n[10], for training fpretrainAlign. Each fine-grained instruc-\ntion Iorig in FGR2R is segmented into sub-instructions or\nchunks and each node from the ground truth path is mapped\nto the most relevant sub-instruction within the instruction.\nSuch a mapping, denoted as Rel_SI(Nt), provides infor-\nmation about the sub-instruction from Iorig that is aligned\nwith any node Nt in the ground truth path. The network,\nfpretrainAlign, is trained independently, keeping the weights of\nthe different components in the navigator - ftext, fcross-attention\nand others frozen. It may be noted that the navigator acts in\na teacher-forcing manner - that is, the move it makes at any\ntime t is always the ideal move or the move at time step t\nin the ground truth path. This is because, the instruction-\npath alignment information in FGR2R is with respect to the\nground truth paths in the dataset.\nLoss function: The network is trained using a combination\nof BCE loss and Dice loss [14]. While BCE loss ensures ac-"}, {"title": "4. Experiments", "content": "4.1. Research Questions\nThe following are the research questions we aim to ad-\ndress through our experiments:\n1.  RQ1: How does the performance (precision-recall bal-\nance) of our proposed instruction-to-path attention-\nbased instruction vagueness estimator compare to ex-\nisting approaches?\n2.  RQ2: How does utilizing different pseudo-labels for\nuncertainty - labelGP, labelIP and entropy-based label\naffect the performance of IV module?\n3.  RQ3: Does incorporating instruction to path align-\nment knowledge through pre-training of multi-head at-\ntention layer improve the performance of IV module?"}, {"title": "4.2. Dataset", "content": "The original R2R dataset [2] is divided into four princi-\npal splits: train, val_seen, val_unseen, and test.\nThe val_seen split contains environments similar to those\nin the train split, whereas val_unseen and test fea-\nture trajectories in entirely new, unobserved environments.\nThe ULN dataset [8], features a manually created coarse-\ngrained or high-level instruction, Ishort, along with the orig-\ninal fine-grained instruction, Iorig for each trajectory in\nval_seen and val_unseen. We also utilize the instruc-\ntion to path alignment information for each trajectory from\nthe FGR2R variant of R2R [10]. Next, we describe the data\nused for training and evaluation in our experiments.\nfpretrainAlign train/val data: fpretrainAlign training exclusively\nuses the train split of R2R, which comprises approxi-\nmately 11,400 trajectories. We divide this training set into\ntraining and validation subsets in an 80 to 20 ratio and\ndo not use trajectories from val_seen and val_unseen\nduring pre-training.\nfiv train/val data: We train fiv using approximately 2,000\ntrajectories from the val_seen split, providing Iorig as in-\nput instruction for 50% of the trajectories and Ishort for the\nremainder. This approach ensures a balanced dataset en-\nabling fry to discern between the two types of instructions\nduring training.\nfiv test data: We evaluate our instruction vagueness esti-\nmator on trajectories in val_unseen, where Iorig is used\nas the input instruction for 50% of the trajectories and Ishort\nfor the remaining trajectories. We do not use the test split\nof R2R as it lacks annotation for Ishort in it."}, {"title": "4.3. Implementation Details", "content": "The model fpretrainAlign is trained for 7000 iterations with\na batch size of 8, learning rate of 1e-4. Equal weights are\ngiven to both dice and BCE loss. The model fiv is trained\nfor 1000 iterations with a batch size of 8, learning rate of\n1e-4. The model is trained on imitation learning. Both mod-\nels are trained with AdamW optimizer [13] on a single GPU\n- NVIDIA GeForce RTX 4090."}, {"title": "4.4. Baseline", "content": "The uncertainty estimation module in [8] is defined as\nfollows:\n$score_t^{uncertainty} = f_{Base} (a_t, B_t)$\nat and \u00dft are as defined in Equations 2 and 3 in Section 3.1.\nThis approach leverages at, the visual attention on instruc-\ntion from the VLN model, and \u00dft, which are the likelihood\nscores that the VLN agent assigns to potential next moves\nat each time step. These two components are concatenated\nand fed into a classifier, which then assesses the uncertainty\nin the predictions made by the VLN model. The approach\nuses supervised learning with pseudo-labels for uncertainty\nderived from ground truth path - labelGp. We denote this\nmethod as fBase. Comparing to fBase enables us to assess\nwhether the attention scores from the navigator serve as a\nbetter indicator of vagueness than the instruction-to-path at-\ntention scores learned by fiv.\nThe next baseline is the uncertainty estimator used in\nVDN task [28], defined as:\n$score_t^{uncertainty} = f_{VDN}(a_t)$\nHere, pseudo-labels for uncertainty are derived from the en-\ntropy of the navigator's predicted distribution, Bt (if entropy\nis close to that of a uniform distribution, i.e., within a prede-\nfined threshold, \u0454 \u2208 [0, 1], it is considered uncertain). Com-\nparing with fVDN also allows us to evaluate the effectiveness\nof entropy as an uncertainty indicator versus using pseudo-\nlabels based on the ground truth move labelge and align-\nment information label\u2081p.\nThe next baseline involves conformal prediction, as out-\nlined in [19]. This method utilizes a calibration set to deter-\nmine a threshold, 0, for the probability assigned to the most\nprobable next move by the VLN agent. The threshold is set\nas a function of user-defined error tolerance (is set as 0.9\nin our experiments). Moves are accepted if their probabil-\nities fall within this threshold; otherwise, they are deemed\nuncertain. We denote this method as fcp."}, {"title": "4.5. Evaluation Metrics", "content": "We evaluate the Instruction Vagueness (IV) module us-\ning a precision-recall balance metric:\n$Balance = \\frac{Precision - Recall}{Precision + Recall}$\nBalance \u2208 [-1,1]; negative values indicate higher recall,\npositive values indicate higher precision, and 0 indicates a\nbalance with Precision = Recall. The ground truth labels\nfor uncertainty is based on labelGP."}, {"title": "4.6. Results", "content": "Our aim is not to outperform existing VLN models in\nSPL and NE but to address the issue of identifying informa-\ntion gaps during navigation. The improvement in SPL and\nNE are achieved with oracle assistance and is used as an\nindicator of timely assistance. We base our experiments on\ntwo VLN models, though we believe the IV module can be\nintegrated into any VLN model with components as shown\nin Figure 2.\nGeneralization to coarse-grained or high-level instruc-\ntion: Table 1 shows the performance of VLN models,\nDUET [5] and HAMT [3], trained on R2R dataset on fiv\ntest data. The first row shows the SPL and NE when Iorig\nis used as the input instruction for all the cases. The sec-\nond row shows the results when Ishort is used as the input\ninstruction. We see a drop in performance, confirming that\nthe models trained on a specific instruction style (Iorig style)\nstruggle to generalize to a different instruction style (Ishort\nstyle). It may also be noted that DUET performs better than\nHAMT in either cases.\nAs mentioned in Section 4.2, fiv test data comprises a\nbalanced dataset with respect to two instruction styles: 50%\nof instances have high-level instructions as input, denoted\nas Instshort, and the remaining have fine-grained instructions\nas input, denoted as Instorig. It is important to note that while\nIorig is fine-grained and contains more details than Ishort, it\nmay also benefit from additional details, as indicated by po-\ntential improvements that can be made in both SPL and NE\nwith Iorig instructions (see Table 1). These results suggest\nthat the generated paths differ from the ground truth paths.\nThe goal of IV module is to identify uncertain moves and\nreduce the drop that is seen in SPL and NE with the change\nin instruction style. For instance, with IV support, DUET\nis expected to achieve an SPL above 47.35 and NE below\n5.30 when Ishort is used as the input instruction. If effec-\ntive, DUET with IV may reach an SPL higher than 54.46,\nsurpassing the SPL with original fine-grained instructions,\nshowing that timely assistance can guide the agent closer to\nthe ground truth path.\nTable 2 compares various variants of our proposed ap-\nproach fiv with baseline approaches, when integrated with\ntwo navigators - DUET and HAMT. The column Orig/Short\nshows the percentage of instances in each category (InstOrig\nversus InstShort) where an oracle intervention was recom-\nmended by the instruction vagueness estimation module.\nComparing to baselines (RQ1): Table 2 illustrates that\nfcp, an approach based on conformal prediction for un-\ncertainty estimation, exhibits very high recall. It tends to\nbe overly cautious, prompting intervention almost at every\ntime-step, regardless of the input instruction style. This re-\nsulted in a high SPL and low NE.\nNext, we compare fBase with fIV(GP), a variant of fiv.\nBoth these models are trained with uncertainty labels based\non ground truth paths (labelGP). It can be seen that both\nmodels perform better with HAMT than with DUET. This\ncan be attributed to the fact that HAMT makes more in-\ncorrect predictions and hence fBase and fIV(GP) when in-\ntegrated with HAMT, observes more cases of uncertainty\nduring its training. The baseline fBase, relying on attended\nrepresentations at from the navigation model, shows more\nimbalance between precision and recall - with higher pre-\ncision compared to recall, indicating excessive confidence\nin the navigation agent's decisions at each time-step. The\nresults indicate that incorporating an explicit attention net-\nwork in fry to learn instruction-to-path attention represen-\ntations improves precision-recall balance in uncertainty es-\ntimation compared to using the attended representation at\nfrom the navigation agent.\nComparing different pseudo-labels for uncertainty\n(RQ2): We examine the impact of using uncertainty la-\nbels derived from instruction to path alignment information\n(labelip) instead of those derived from ground truth paths\n(labelGp). To do this, we compare the models fIV(GP) and\nfIV(IP). A notable finding is the distinct treatment of in-\nstances from the two styles, with more instances of oracle\nintervention suggested in Instshort than in Instorig. This re-\nsults from label\u2081p, which is not agnostic to the instruction-\nstyle. It influences the IV estimation module to associate\n\"uncertain\" label more with instructions of style Ishort than\nwith Iorig. However, Iorig may also benefit from additional\ndetails as shown in our experiments, refer Table 1. The sig-\nnificant increase in SPL and reduction in NE indicate its ef-\nfectiveness in identifying uncertain points in Instshort trajec-\ntories more efficiently, striking a balance between precision\nand recall. This demonstrates that such instruction-to-path\nalignment information can be a better indicator to identify\nmissing information in instructions at any given time point.\nDUET showed best results with the variant fIV(IP)\u00b7\nWe compare fvDN using entropy-based pseudo-labels\nwith fIV(GP) and \u0192IV(IP). The results show that entropy-based\nlabeling doesn't generalize well across navigators, espe-\ncially with HAMT, where navigation errors are made with\nhigh confidence, causing pseudo-labels to indicate 'cer-\ntainty' even in uncertain cases.\nImpact of pre-training MHA in IV (RQ3): It is not fea-\nsible to assume that labelip is available for new data used\nto train fiv. Instead, we leverage instruction-to-path align-\nment information available from existing data (the FGR2R\ndataset) to pre-train the Multihead Attention (MHA) net-\nwork in frv. Table 2 demonstrates that fIV(GP+pretrain), where\nfIV is trained with labelGp and the MHA was initialized\nwith pre-trained weights, exhibits improvement in SPL, NE\nand precision-recall balance compared to fIV(GP), which\nuses an MHA initialized with random weights. These find-\nings indicate that having an MHA network trained on the\ninstruction-to-path alignment task can help enhance its per-\nformance in the estimation of instruction vagueness.\nNumber of oracle interventions: Figure 5 maps the per-\ncentage of trajectories to the number of times help was re-\nquested within a trajectory when the different models are\nintegrated with DUET. Both fBase and our method reduces\nthe number of questions per trajectory, with our approach\nrequesting more assists than fBase, as expected. Although\nfVDN has more interventions per trajectory, the lower SPL\nand NE (see Table 2) achieved despite more number of\nquestions indicate that our approach is more effective in\nseeking timely assistance.\nQualitative Results (an example with DUET): Figure 4\nillustrates an example with the instruction Ishort: \u201cGo into\nthe room with the French doors\". At time step 4, there is a\npoint of uncertainty because the agent observes two rooms\nwith French doors. At this point, both fBase and fVDN\nconfidently moves to the nearest room with French doors\nbut ends up at the wrong location (red, yellow paths). Our\napproach (green path) reaches the target after two interven-\ntions. In contrast, fCP is overly cautious, seeking assistance\nat every step (blue path).\""}, {"title": "5. Conclusion and Future Scope", "content": "In this paper, we presented a method to estimate un-\ncertainty from vague instructions in under-specified vision-\nlanguage navigation tasks. The approach focuses on align-\ning instructions with the generated path to indicate instruc-\ntion vagueness. Looking ahead, we aim to further develop\nthis alignment-aware approach to precisely identify \"what\ninformation is exactly missing\"."}, {"title": "6. Acknowledgement", "content": "This work was supported by the Centre for Augmented\nReasoning, an initiative by the Department of Education,\nAustralian Government."}]}