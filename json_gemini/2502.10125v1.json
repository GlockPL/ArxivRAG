{"title": "Learning Relational Tabular Data without Shared Features", "authors": ["Zhaomin Wu", "Shida Wang", "Ziyang Wang", "Bingsheng He"], "abstract": "Learning relational tabular data has gained significant attention recently, but most studies focus on single tables, overlooking the potential of cross-table learning. Cross-table learning, especially in scenarios where tables lack shared features and pre-aligned data, offers vast opportunities but also introduces substantial challenges. The alignment space is immense, and determining accurate alignments between tables is highly complex. We propose Latent Entity Alignment Learning (Leal), a novel framework enabling effective cross-table training without requiring shared features or pre-aligned data. Leal operates on the principle that properly aligned data yield lower loss than misaligned data, a concept embodied in its soft alignment mechanism. This mechanism is coupled with a differentiable cluster sampler module, ensuring efficient scaling to large relational tables. Furthermore, we provide a theoretical proof of the cluster sampler's approximation capacity. Extensive experiments on five real-world and five synthetic datasets show that Leal achieves up to a 26.8% improvement in predictive performance compared to state-of-the-art methods, demonstrating its effectiveness and scalability.", "sections": [{"title": "1. Introduction", "content": "Tabular data is a prevalent structured data format, especially in real-world databases. Training models on such data has numerous applications across domains, including medical and financial fields. However, real-world tabular data is often highly heterogeneous, with each table containing a distinct set of features and unique data distributions. This challenge is commonly referred to as the data lake problem or the data silo problem. Existing machine learning approaches primarily focus on learning from individual tables in isolation. In practice, however, tables are often correlated, and leveraging information across tables can significantly enhance predictive performance. For instance, in the financial domain, data from a transaction table can improve predictions in a loan table by joining the tables on shared features - referred to as keys in relational databases - and subsequently training models on the joined table.\nThe join-learn paradigm is effective in ideal relational databases but faces significant challenges with heterogeneous tabular data widely existed in practice. The primary limitation is the absence of shared features. For example, in an anonymous Bitcoin transaction table and a bank transaction table (Figure 1), no shared features exist to facilitate a join. Even when tables share features, identifying them by schema or column names is difficult, with exact match accuracy reaching only 18.4% according to a previous study. While some methods address partially or fuzzily aligned keys, they can not handle scenarios with no shared features, a scenario we refer to as latent alignment learning.\nThe second limitation pertains to efficiency, particularly in scenarios involving many-to-many key relationships. Such cases often leads to significant time cost and high memory consumption. In our study, even joining two tables with 200k and 400k rows without optimization takes approximately four hours, with a memory cost 22\u00d7 higher than that of the individual tables. These challenges hinder the scalability and practical adoption of join-based learning methods for heterogeneous tabular data in real applications.\nA significant challenge arises to enable the training of relational tabular data without shared features. The absence of keys prevents the evaluation of match probabilities based on key similarities, as traditionally done in relational databases. Therefore, identifying new criteria to estimate match probabilities between records across tables becomes crucial. Moreover, designing a model that can effectively learn based on such criteria introduces additional complexity.\nTo address the above challenges, we propose using training loss decay to estimate match probabilities, based on the insight that properly aligned data result in smaller loss than misaligned data. This is formally established in Theorem 4.1 and validated empirically in Figure 2. Matched record pairs are probabilistically identified as those yielding the greatest loss decay. Candidate records are embedded and compared via an attention mechanism to assess alignment. Experimental results show that the soft alignment mechanism achieves performance comparable to perfectly aligned data when candidate records contain the ground truth.\nSoft alignment, despite its advantages, remains insufficient for achieving practical latent alignment learning due to scalability challenges. As the number of candidate pairs increases, it becomes prone to overfitting and incurs significant computational costs. To overcome this challenge, we propose a novel module, the cluster sampler, which selects a small subset of data records from the table. This module organizes data records into clusters using a soft, differentiable clustering approach and samples records from each cluster. The cluster assignments are dynamically updated during training through gradients propagated from subsequent modules. By limiting the number of candidate pairs, the cluster sampler significantly enhances the model's scalability, enabling its effective application to larger tables."}, {"title": "In summary, we propose Latent Entity Alignment Learning (Leal), a coupled model that integrates alignment into supervised learning. Leal addresses the challenge of missing shared features by proposing soft alignment, which learns alignment through training loss. To further improve efficiency, Leal introduces the cluster sampler, designed to mitigate overfitting and reduce computational costs when applied to larger tables. To the best of our knowledge, Leal is the first model to enable machine learning across relational tabular data without shared features and pre-aligned samples. The contributions of this paper can be summarized as follows:\n\u2022 We propose a novel approach, Leal, to enable machine learning across relational tabular data without shared features or pre-aligned samples.\n\u2022 We provide theoretical demonstrations to the advantages of aligned data over misaligned data in terms of loss and the approximation capacity of the cluster sampler design.\n\u2022 We evaluate Leal on five real-world and five synthetic datasets, demonstrating its effectiveness in learning relational tabular data without shared features. Our experiments show that Leal reduces the error by up to 26.8% over state-of-the-art methods trained on individual tables with reasonable computational overhead.", "content": null}, {"title": "2. Related Work", "content": "Machine Learning on Tabular Data. Machine learning on single tables is a well-studied area with diverse paradigms. Deep tabular learning methods utilize deep neural networks to learn tabular data representations, while tree-based approaches like XGBoost and CatBoost rely on gradient boosting decision trees. McElfresh et al. observed that the performance of these methods varies across datasets. The extension of such models to multiple tables without shared features remains an unexplored area of research.\nMulti-Modal Learning. Multimodal learning focuses on integrating information from multiple data modalities, such as images and text. Most multimodal learning algorithms, including VisualBERT and VL-BERT, rely on pre-aligned pairs to supervise alignment, which is incompatible with the Leal setting. Other multimodal learning methods, such as U-VisualBERT and VLMixer, achieve self-supervised learning using externally pre-trained models for specific modalities to extract features or representations for alignment. However, these approaches are not applicable to heterogeneous tabular data, where universally effective pretrained models are unavailable. None of these multi-modal models can be directly applied to latent alignment learning between relational tabular data.\nVertical Federated Learning. Vertical Federated Learning (VFL) is a privacy-preserving learning paradigm for training models across distributed datasets with distinct feature sets. Although privacy is not a concern in the context of Leal, techniques developed to address data heterogeneity in VFL are relevant. Semi-supervised VFL focuses on the challenge of partial data alignment. They learns to complete missing data based on the aligned data. Fuzzy VFL handles scenarios when keys are not precise. Wu et al. use key similarity as the weight for aligned records. Wu et al. encode the keys into data embedding by postional encoding and use attention mechanism to align the data. However, these methods rely on the assumption of shared features across datasets, which does not hold in latent alignment learning."}, {"title": "3. Problem Formulation", "content": "Consider two tables, $X^P \\in \\mathbb{R}^{n_P \\times m_P}$ and $X^S \\in \\mathbb{R}^{n_S \\times m_S}$, where $n_P$ and $n_S$ represent the number of records in the primary table $X^P$ and the secondary table $X^S$, respectively, and $m_P$ and $m_S$ denote the number of features in these tables. We focus on a supervised learning task aimed at predicting a target variable $y \\in \\mathbb{R}^n$. Without loss of generality, we assume that $y$ is associated with $X^P$, referred to as the primary table, while $X^S$ serves as the secondary table.\nWe address a scenario where there are no shared features between $X^P$ and $X^S$, formally stated as $\\Omega_{P} \\cap \\Omega_{S} = \\emptyset$, where $\\Omega_{P}$ and $\\Omega_{S}$ represent the feature sets of $X^P$ and $X^S$, respectively. Despite this, it is assumed that an approximate functional dependency exists between the primary and secondary tables. This dependency is quantified using the information fraction (IF) metric, defined as:\n$IF(Y; X^S | X^P) = \\frac{I(y; X^S | X^P)}{H(y | X^P)}$\nwhere $I(y; X^S | X^P) = H(y | X^P) \u2013 H(y | X^P, X^S)$ is the conditional mutual information between $y$ and $X^S$ given $X^P$, and $H(y | X^P)$ and $H(y | X^P, X^S)$ denote the conditional entropies of $y$ given $X^P$ and both $X^P$ and $X^S$, respectively. The IF measures the additional information provided by $X^S$ about $y$ beyond what is captured by $X^P$. Specifically, $IF(Y; X^S | X^P) = 1$ implies a functional dependency, while $IF(Y; X^S | X^P) = 0$ indicates statistical independence. We consider the case where $|IF(Y; X^S | X^P) \u2013 1| < \\delta$, where $d$ is a small positive constant, indicating a strong functional dependency between the primary and secondary tables.\nThe objective is to learn a predictive model by minimizing the following loss function:\n$\\min_{\\theta} \\frac{1}{n^P}\\sum_{i=1}^{nP} \\mathcal{L}(f(\\theta; x_i^P, X^S), y_i)$\nwhere $f(\\theta; x_i^P, X^S)$ is a model parameterized by $\\theta$, $\\mathcal{L}$ is a loss function, and $y_i$ is the target for the i-th record in $X^P$. The challenge lies in effectively utilizing the secondary table $X^S$ to enhance the predictive performance for $y$, despite the lack of common features between the two tables."}, {"title": "4. Approach", "content": "In this section, we present the overall framework design of Latent Entity Alignment Learning (Leal), with the model structure illustrated in Figure 3. Section 4.1 introduces the soft alignment mechanism, which maps primary and secondary data records to a shared latent space. In Section 4.2, we describe the cluster sampler, which dynamically selects the most relevant secondary records during training. Finally, Section 4.3 outlines the training and inference processes."}, {"title": "4.1. Soft Alignment", "content": "The soft alignment mechanism is motivated by the observation that properly aligned data result in smaller loss compared to misaligned data. We provide a formal expression for linear regression in Theorem 4.1, with the proof detailed in Appendix A.1.\nTheorem 4.1. Let $X^P \\in \\mathbb{R}^{n \\times m_P}$ and $X^S \\in \\mathbb{R}^{n \\times m_S}$ be normalized primary and secondary feature matrices, respectively, y \\in Rn be the target variable, and R be a permutation matrix. For the linear regression model $y = X^P\\alpha + RX^S \\beta$, it holds that $MSE_{aligned} < MSE_{misaligned}$, where $MSE_{aligned}$ and $MSE_{misaligned}$ are the mean squared errors under correct alignment and random alignment.\nTheorem 4.1 suggests that, for linear regression tasks with sufficient samples, properly aligned data consistently results in lower loss. While extending this theoretical result to deep learning remains an open challenge, our experimental results in Figure 2 demonstrate that when n is sufficiently large that prevents the model from memorizing the data, even complex deep learning models struggle to achieve low loss when the data is misaligned.\nTo perform soft alignment between a primary data record $x_i^p$ and K secondary data records $X^S = \\{x_j^S\\}_{j=1}^K$ with distinct features, we first map them into a shared latent space:\n$z^p = f^P(x_i^p), z^S = f^S(x_j^S)$    (1)\nIn this latent space, the distance between $z^p \\in \\mathbb{R}^{1 \\times d}$ and each row vector of $z^S \\in \\mathbb{R}^{K \\times d}$ represents the relationship between $x_i^p$ and each record in $x_j^S$, where d is the dimensionality of the latent space, and K is the number of candidate records from the secondary table. The scaled inner product between $z^p$ and $z^S$ is computed to measure their similarities. The similarity scores for all secondary records are normalized using a softmax function to obtain the soft alignment weights $\\lambda_i$:\n$\\lambda_i = SoftMax(\\frac{z_i^p(z_i^S)^T}{\\sqrt{d}})$    (2)\nThe alignment weights $\\lambda_i$ indicate the importance of each record pair and are applied to the latent vectors. Weighted latent vectors are then aggregated for further processing:\n$\\tilde{z}^p = \\sum_{j=1}^K \\lambda_j z_j^S$,   (3)\nThis weighted alignment module is functionally equivalent to the widely used attention mechanism. Consequently, we directly adopt multi-head attention in our implementation. Specifically, both primary and secondary representations are processed through a self-attention module followed by a feed-forward network. Their intermediate representations are then aggregated using an attention layer, followed by another feed-forward network. This structure can be stacked to form a deep neural network."}, {"title": "4.2. Cluster Sampler", "content": "The cluster sampler is designed to efficiently select K candidate secondary records to feed into the model. For small secondary tables, such as those with hundreds of records, directly feeding the entire table into the model is feasible and demonstrates promising performance in our experiments. However, for large secondary tables, this approach becomes inefficient and may lead to overfitting.\nTo address this, we propose a trainable cluster sampler to dynamically sample the most relevant records during each training step. The cluster sampler selects K records based on two criteria: cluster weights and in-cluster probabilities. The cluster weight is generated by a trainable cluster weight generator, implemented as a feed-forward network that takes the primary record $x^p$ as input and outputs a C-dimensional vector, where C is the number of clusters. The cluster weight is normalized using a softmax function to produce the cluster weight vector $w_i \\in \\mathbb{R}^{1 \\times C}$:\n$w_i = SoftMax(FeedForward(x^p))$   (4)\nThe in-cluster probability is generated using a trainable soft deep K-Means module, inspired by Xie et al.; Ye et al.. A pretrained encoder maps the secondary table $x^S$ into a latent space, resulting in $h_i^S = g^S(x_i^S) \\in \\mathbb{R}^{n2 \\times d}$. Cluster centroids $C \\in \\mathbb{R}^{C \\times d}$ are initialized via K-Means clustering on $h^S$. Following Xie et al., each record is assigned to a cluster with a probability matrix $q \\in \\mathbb{R}^{N2 \\times C}$, computed using the Student's t-distribution kernel:\n$q_{ij} = \\frac{(1 + ||h_i \u2013 c_j||^2/\\gamma)^{-(\\gamma+1)/2}}{\\sum_{l=1}^C(1 + ||h_i \u2013 c_l||^2/\\gamma)^{-(\\gamma+1)/2}}$ (5)\nwhere $\\gamma$ is the degree of freedom, and $c_i$ and $h_i$ represent the i-th cluster centroid and the j-th latent vector, respectively. The final sampling probability for $x_i^S$ is computed as the product of cluster weight and in-cluster probability, followed by a multi-layer perceptron (MLP):\n$p_i = MLP(q \\oplus w) \\in \\mathbb{R}^{n2 \\times 1}$   (6)\nThe K secondary records are sampled based on probabilities $p_i$ during training and selected from the top $p_i$ during inference. The embeddings of these secondary records are then fed into the soft alignment module.\nFurthermore, we theoretically demonstrate that the trainable weights in the cluster sampler design are capable of approximating any target sampling function in Theorem 4.2, which is formally proven in Appendix A.\nTheorem 4.2. For any uniform equi-continuous fixed optimal cluster sampler $h^*$ and any $\\epsilon > 0$, there exists d, C and a corresponding weight $\\theta$ for cluster sampler $h_{cs}^{\\theta}$ such that\n$\\sup_{x \\in X^P}|h^*(x) \u2013 h_{cs}(x;\\theta)| \\leq \\epsilon$.  (7)"}, {"title": "4.3. Training and Inference", "content": "The training process comprises two stages: an unsupervised stage and a supervised stage. The unsupervised stage focuses on learning an initial representation to facilitate cluster initialization, while the supervised stage is designed to train the model to predict the primary target.\nTraining. In the unsupervised stage, the pretrained encoder is trained using an autoencoder (Zhai et al., 2018), where both the encoder $g^S$ and decoder $\\phi^S$ are simple multi-layer perceptrons (MLPs) with LayerNorm (Ba, 2016):\n$\\min_{g^S,\\phi^S} ||X^S \u2013 \\phi^S (g^S(X^S))||^2$   (8)\nIn the supervised stage, during each epoch, each batch of data records from the primary table, referred to as the primary records, are fed into the cluster sampler alongside the entire secondary table. The sampling probabilities are then calculated (lines 6-8), and the indices of the selected records from the secondary table, referred to as secondary records, are output (line 9). The primary record and secondary records are subsequently passed into an attention-based soft alignment module (lines 10-11) for further training to derive the final label (line 12) and loss (line 13), as illustrated in Figure 3. The pretrained encoder, cluster centroids, feed-forward layers, and attention layers are optimized jointly (line 14). The detailed training process is presented in Algorithm 1.\nInference. During inference, the pretrained encoder, cluster centroids, and feed-forward layer remain fixed. The procedure follows the forward pass of the supervised stage, with the only variation being in the cluster sampler. To ensure determinism, the top records with the highest sampling probabilities $p_i$ are selected.\nScalability. Suppose that training an individual table requires $M_o$ memory and $T_o$ time. Let K represent the number of candidate secondary records. The current memory overhead of Leal is $KM_o$, as for each primary record, K neighboring records are fed into the training process. The time complexity of Leal is O($K^2T_0$), which arises from the quadratic complexity of the attention mechanism. However, with the cluster sampler, K is typically less than 100, making the training time and memory overhead of Leal manageable."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nWe conduct experiments on five real-world and five synthetic datasets, covering classification and regression tasks. Each real-world dataset comprises two tables from different sources, with shared features removed. The house dataset combines data from Lianjia and Airbnb for house price prediction. The bike dataset uses Citibike and New York City Taxi routes for travel time prediction. The hdb dataset integrates HDB resale prices and school rankings in Singapore for resale price prediction. The accidents dataset, derived from Slovenian police traffic records, consists of individual and accident-level tables for accident type classification. The hepatitis dataset, sourced from the PKDD'02 Discovery Challenge database, consists of medical check tables for distinguishing Hepatitis B and C cases. Any shared features, if present, are removed from secondary tables. Dataset statistics are summarized in Table 1, where \u201ccls\u201d means classification and \"reg\u201d means regression.\nThe synthetic datasets are generated from the UCI repository and include breast , covertype , gisette , letter , and superconduct . Each synthetic dataset is randomly divided by features into two tables, serving as primary and secondary tables, without any overlapped feature. Detailed statistics of the synthetic datasets are provided in Table 2.\nWe apply one-hot encoding to categorical features, and normalize all features of real-world datasets to mitigate the impact of extreme values. Therefore, all features are treated as numerical in our experiments.\nTraining. All models are trained using the AdamW optimizer with early stopping and a learning rate of 0.001. A batch size of 128 is employed, and training is conducted for up to 150 epochs, with early stopping determined by validation loss. The hyperparameters K and C are selected from the set {1, 5, 10, 20, 100}, while the layer depth is chosen from {1,3,6}. In the unsupervised stage, the depth of the encoder and decoder is set to 2, except for the hdb dataset, where a depth of 1 is used to prevent overfitting. All embedding dimensions are fixed at 100.\nEvaluation. For all datasets, the primary table is divided into training, validation, and test sets in a 7:1:2 ratio. The secondary table is utilized during both training and inference. Evaluation metrics include accuracy for classification tasks and root mean squared error (RMSE) for regression tasks. The mean and standard deviation of test scores are reported over five seeds.\nBaselines. We compare the proposed method with state-of-the-art deep tabular learning methods. \"Solo\" denotes training on the primary table only. The baselines are:\n\u2022 Solo-MLP: A three-layer MLP with hidden sizes of (800, 400, 400) and ReLU activation.\n\u2022 Solo-ResNet : An MLP model incorporating skip connection, ReLU activation and batch normalization.\n\u2022 Solo-FTTrans : A transformer-based model that embeds each feature as a token.\n\u2022 Solo-TabM : A state-of-the-art multi-layer deep ensemble-based model.\nEnvironement. Each task is executed on a single NVIDIA V100 GPU with 32GB memory and an Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz. The system is equipped with 1.48TB of memory, which is not fully utilized. The code is implemented using PyTorch 2.5 and Python 3.10."}, {"title": "5.2. Performance", "content": "The performance of Leal and the baseline methods across various datasets is summarized in Table 3 for real-world datasets and Table 4 for synthetic datasets. Two key findings emerge from these results. First, Leal outperforms state-of-the-art approaches trained solely on the primary table for the majority of datasets. For instance, on the house dataset, Leal reduces RMSE by at least 26.8% compared to all baselines. Second, Leal demonstrates significantly better performance on real-world datasets compared to synthetic datasets. This is attributed to the presence of fuzzy and many-to-many alignments in real-world datasets, which amplify the effectiveness of soft alignment by better capturing complex relationships and dependencies between data records, as also observed in . In contrast, synthetic datasets typically involve only one-to-one alignments, limiting the benefits of soft alignment. Notably, FT-Transformer (FTTrans) fails to handle the high-dimensional gisette dataset due to its encoding approach, which treats each feature as a separate token. This results in an out-of-memory error on all tested GPUs."}, {"title": "5.3. Efficiency", "content": "Table 5 presents the average training time per epoch for Leal and baseline methods, with K = 20 and C = 20, a typical hyperparameter setting for Leal in our experiments. Despite addressing the complex alignment challenges inherent in its design, Leal maintains competitive training efficiency compared to baselines trained on a single table. On average, Leal incurs a computational overhead ranging from 1.27\u00d7 to 22.82x, even with the quadratic growth in the number of record pairs introduced by incorporating the secondary table. This efficiency is largely attributed to Leal's cluster sampler module, which significantly reduces the number of candidate pairs in the soft alignment process, thereby mitigating the computational burden.\nThe overhead of Leal on large datasets is reasonable. For instance, on the covertype dataset with 581K instances in both tables, the alignment-training process for Leal requires approximately 692 minutes (415 \u00d7 100/60 min). For comparison, adopting state-of-the-art approximate nearest neighbor search methods without learning, such as FAISS-IVF , with a recall < 10\u20132, would take around 312 minutes for a the same table scale according to a recent benchmark . This highlights that while Leal introduces additional computational costs, its efficiency remains reasonable for large-scale datasets."}, {"title": "5.4. Ablation Studies", "content": "Effect of Soft Alignment. We evaluate the effectiveness of soft alignment through a controlled experiment. To eliminate the influence of sampling techniques, we select K candidates, including one ground-truth correctly aligned secondary record and K \u2013 1 randomly sampled secondary records. This setup allows us to assess the attention mechanism's ability to identify the correct secondary record. Each experiment is conducted under five seeds, and we report the mean and standard deviation of the accuracy in Figure 4.\nTwo key findings emerge from Figure 4. First, soft alignment effectively identifies the correct alignment at a modest K value, despite the noise introduced by random sampling. For example, to outperform Solo-ResNet, the covertype dataset supports K < 320, while the letter dataset supports K \u2264 10. Second, a comparison of Figure 4 and Table 4 reveals that the performance of Leal in Figure 4 with small K significantly exceeds its performance in Table 4. This indicates that the primary bottleneck for Leal lies in the clustering process. Enhancing the clustering process to improve the probability of including the correct pair in the candidate set would further boost Leal's performance."}, {"title": "Effect of Hyperparameters. We conduct ablation studies to examine the impact of the number of clusters (C) and the number of neighbors (K) on Leal's performance. The results are illustrated in Figure 5 and Figure 6, respectively. From these results, we derive three key findings: First, setting K too large typically has a negative effect, consistent with the observations in Figure 4. Second, the effect of C is dataset-size-dependent. For instance, the large bike dataset benefits from a larger C value, whereas for the small hepatitis dataset, a small C value is sufficient. Third, the effect of C also interacts with the number of neighbors (K). For example, in the hepatitis dataset (Figure 5), when K is small, a larger C value is advantageous; however, when K is large, a smaller C value proves to be more effective. These findings suggest that soft alignment and cluster sampling can complement each other during the training.", "content": null}, {"title": "6. Future Work", "content": "In the future, we aim to extend the model to handle multiple relational tables. The primary challenges are overfitting and efficiency due to the involvement of numerous tables. Successfully addressing these challenges would enable knowledge fusion across real-world tabular data without shared features, supporting more applications in unsupervised multi-modal learning and vertical federated learning.\nAdditionally, we aim to develop efficient methods for identifying relationships between tables. While training a Leal model to evaluate table relationships by performance is feasible, it is computationally expensive and lacks scalability. Exploring efficient approaches could significantly enhance knowledge integration across tables."}, {"title": "7. Conclusion", "content": "In this paper, we theoretically and empirically demonstrate that properly aligned data are more effectively learned by machine learning models compared to misaligned data. Building on this observation, we propose an integrated model, Leal, which seamlessly combines soft alignment and learning for tabular data. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of Leal. Looking ahead, we believe that latent alignment learning represents a promising direction for advancing machine learning to leverage the widely available heterogeneous tabular data without shared features."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Proof", "content": null}, {"title": "A.1. Error Gap Between Aligned and Misaligned Data", "content": "Theorem 4.1. Let $X^P \\in \\mathbb{R}^{n \\times m_P}$ and $X^S \\in \\mathbb{R}^{n \\times m_S}$ be normalized primary and secondary feature matrices, respectively, y \\in Rn be the target variable, and R be a permutation matrix. For the linear regression model $y = X^P\\alpha + RX^S \\beta$, it holds that $MSE_{aligned} < MSE_{misaligned}$, where $MSE_{aligned}$ and $MSE_{misaligned}$ are the mean squared errors under correct alignment and random alignment.\nFor the aligned case, we can derive the mean squared error (MSE) as follows:\n$MSE_{aligned} = \\inf_{\\alpha \\in \\mathbb{R}^{m_P}, \\beta \\in \\mathbb{R}^{m_S}} ||y \u2013 X^P\\alpha \u2013 X^S\\beta||$   (9)\nThe ordinary least squares (OLS) estimator of a is given by:\n$\\hat{\\alpha} := (X^{PT}X^P)^{-1}X^P (y \u2013 E[R]X^S\\beta)$  (10)\nFor a permutation matrix R under uniform distribution, we have $E[R] = \\frac{1}{n}11^T$. Therefore:\n$\\hat{\\alpha} = (X^{PT}X^P)^{-1}X^P (y \u2013 \\frac{1}{n}11^TX^S\\beta)$ (11)\nThe MSE for the misaligned case can be expressed as:\n$MSE_{misaligned} = \\inf_{\\alpha} \\inf_{\\beta} E_R||y \u2013 X^P\\alpha \u2013 RX^S\\beta||^2$   (12)\n$= \\inf_{\\beta} E_R||y \u2013 X^P\\hat{\\alpha} \u2013 RX^S\\beta||^2$   (13)\n(14)\nSubstituting $\\hat{\\alpha}$ from equation 11, we obtain:\n$MSE_{misaligned} = \\inf_{\\beta} E_R ||y \u2013 X^P(X^{PT} X^P)^{-1}(X^{PT}y \u2013 X^{PT}\\frac{1}{n}11^TX^S\\beta) \u2013 RX^S\\beta||^2$    (15)\n$= \\inf_{\\beta} E_R ||(I \u2013 X^P(X^{PT} X^P)^{-1}X^{PT})y + (X^P(X^{PT} X^P)^{-1}X^{PT}\\frac{1}{n}11^TX^S\\beta) \u2013 RX^S\\beta||^2$   (16)\nSince $X^P (X^{PT}X^P)^{-1}X^{PT}$ is a projection matrix that projects any vector onto the column space of $X^P$, and $X^S\\beta$ is orthogonal to the column space of $X^P$, the term $X^P (X^{PT}X^P)^{-1}X^{PT}\\frac{1}{n}11^TX^S\\beta = 0$. Thus:\n$MSE_{misaligned} = \\inf_{\\beta} E_R ||(I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y \u2013 RX^S\\beta||^2$   (17)\n$= \\inf_{\\beta} E_R [||RX^S\\beta||^2 \u2013 2 [(I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y] RX^S\\beta+ ||(I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y||^2]$   (18)\nBy properties of permutation matrices:\n$E_R||RX^S\\beta||^2 = ||X^S\\beta||^2; E_R[R] = \\frac{1}{n}11^T$ (19)\nTherefore:\n$MSE_{misaligned} = \\inf_{\\beta} [||X^S\\beta||^2 - 2 [(I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y] \\frac{1}{n}11^TX^S\\beta+ ||(I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y||^2]$   (20)"}, {"title": "Learning Relational Tabular Data without Shared Features", "content": "Since $I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT}$ projects any vector onto the orthogonal complement of the column space of $X^P$, the term $[(I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y] \\frac{1}{n}11^TX^S\\beta = 0$. Hence:\n$MSE_{misaligned} = \\inf_{\\beta} [||X^S\\beta||^2 + || (I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y||^2]$   (21)\n$= \\inf_{\\beta} ||X^S\\beta||^2 + || (I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y||^2$ (22)\n(23)\nThe minimum is attained at $\\beta = 0$, yielding:\n$MSE_{misaligned} = || (I \u2013 X^P (X^{PT}X^P)^{-1}X^{PT})y||^2$   (24)\n$= \\inf_{\\alpha} \\inf_{\\beta=0} ||y \u2013 X^P\\alpha \u2013 X^S\\beta||^2$   (25)\n(26)\nComparing with Equation 9, we conclude:\n$MSE_{misaligned} \\geq \\inf_{\\alpha \\in \\mathbb{R}^{m_P}, \\beta \\in \\mathbb{R}^{m_S}} ||y \u2013 X^P\\alpha \u2013 X^S\\beta||^2 = MSE_{aligned}$    (27)"}, {"title": "A.2. Approximation Capacity of Cluster Sampler", "content": "Definition A.1 (Definition of optimal cluster sampler). Assume the inputs are uniformly bounded by some constant B. The optimal cluster sampler is defined by the uniform equi-continuous cluster sampler function which achieves the minimal optimization loss for the prediction task in Figure 3.\nOptimal cluster sampler := $\\underset{Uniform \\; equi-continuous \\; cluster \\; sampler}{arginf} Loss(cluster sampler)$   (28)\nThe cluster sampler is defined over bounded inputs ($|X^P|_\\infty < B$, $|X^S|_\\infty \\leq B$) from $\\mathbb{R}^{m_P} \\times \\mathbb{R}^{n_S \\times m_S}$ and output in $\\mathbb{R}^{n_S}$.\nRemark A.2. The existence of such optimal cluster sampler"}]}