{"title": "Writing Style Matters: An Examination of Bias and Fairness in Information Retrieval Systems", "authors": ["Hongliu CAO"], "abstract": "The rapid advancement of Language Model technologies has opened new opportunities, but also introduced new challenges related to bias and fairness. This paper explores the uncharted territory of potential biases in state-of-the-art universal text embedding models towards specific document and query writing styles within Information Retrieval (IR) systems. Our investigation reveals that different embedding models exhibit different preferences of document writing style, while more informal and emotive styles are less favored by most embedding models. In terms of query writing styles, many embedding models tend to match the style of the query with the style of the retrieved documents, but some show a consistent preference for specific styles. Text embedding models fine-tuned on synthetic data generated by LLMs display a consistent preference for certain style of generated data. These biases in text embedding based IR systems can inadvertently silence or marginalize certain communication styles, thereby posing a significant threat to fairness in information retrieval. Finally, we also compare the answer styles of Retrieval Augmented Generation (RAG) systems based on different LLMs and find out that most text embedding models are biased towards LLM's answer styles when used as evaluation metrics for answer correctness. This study sheds light on the critical issue of writing style based bias in IR systems, offering valuable insights for the development of more fair and robust models.", "sections": [{"title": "1 INTRODUCTION", "content": "Information Retrieval (IR) systems are pivotal in managing the current informational overflow, empowering users to obtain results more efficiently [11]. The relevance of IR in everyday life is profound due to its use cases in diverse applications such as online web browsing, question-answering systems, digital assistants, etc. The central goal of IR systems is to identify and fetch data relevant to the user's query (often based on their embedding similarity scores) [15]. Text embeddings for information retrieval has undergone substantial transformations throughout the past decades, from the early phase of count-based sparse embeddings such as Term Frequency-Inverse Document Frequency (TF-IDF) [26], transitioning into the second phase characterized by static dense word embeddings, including Word2Vec [28] and FastText [5]. The field then evolved into the third phase featuring contextualized embeddings like Bidirectional Encoder Representations from Transformers (BERT) [13] and Embeddings From Language Models (ELMO) [30], leading to the latest developments in the fourth phase encompassing universal text embeddings, such as General-purpose Text Embedding model (GTE) [23], Beijing Academy of Artificial Intelligence General Embedding (BGE) [40], Large Language Model (LLM) based universal text embedding [37], among others [6].\nWith the widespread applications and use cases of ChatGPT, the remarkable capacities of LLMs have been highlighted, especially in terms of conversational capabilities, instruction-following and in-context learning [6], which has fundamentally revolutionized Information Retrieval. However, several critical issues of LLMs have also been identified including lacking of some domain knowledge and updated knowledge, and hallucinations. One solution to mitigate these limitations is Retrieval Augmented Generation (RAG) which uses text embedding to provide relevant contexts for LLMs to generate answers beyond their initial training data [17]. LLMs are also used to generate synthetic data to fine-tune text embedding models for IR systems [21, 37]. The fast adoption of LLMs in IR has also brought new challenges related to bias and fairness. These issues can undermine the reliability and robustness of IR systems and may inadvertently contribute to broader societal problems [11].\nRecently, considerable attention has been devoted to the bias and fairness related to LLM-based applications [11]. However, only a limited number of research initiatives have been concentrated on the potential bias present in text embedding models utilized for IR systems. Most existing studies ignore the fact that both human written documents and LLM-generated synthetic documents can have diverse communication or writing styles. These variations in writing styles can introduce bias and unfairness into information retrieval systems in several ways. Firstly, if the text embedding models have been trained mainly on data in a particular writing style, they may be biased and struggle to accurately interpret and retrieve information from documents written in a different style. Secondly, the system may exhibit unfairness across different writing styles. For instance, if the system has been trained to associate higher relevance with documents that use complex sentence structures, it may favor such documents in the search results even if they are not the most relevant to the user's query while discriminating against documents in other writing styles. Finally, the writing style of queries could also introduce bias. If the system is trained with queries based on specific writing styles, it may fail to answer queries written in other styles.\nIn this study, we fill the gap of the literature by examining the influence of varying writing styles on state of the art text embedding models with the following key contributions::\n\u2022 This work pioneers the study of bias and fairness in IR systems related to document and query writing styles.\n\u2022 A comprehensive analysis and comparison is provided over diverse state of the art text embedding models with different fine-tuning data, fine-tuning strategies, objective functions, model size, backbones, etc.\n\u2022 A novel method of analyzing and quantifying fairness in text embedding models is proposed, which reveals different writing style bias in different embedding models, as well as the impact of query styles on model fairness.\n\u2022 The potential bias of text embedding models towards different LLM's answer styles (when used as evaluation metrics for answer correctness) is also identified and analyzed."}, {"title": "2 RELATED WORKS", "content": "Bias in Information Retrieval systems: The issue of bias and unfairness in the filed of Artificial Intelligence (AI) has been a subject of extensive research in past years. The recent incorporation of LLMs into information retrieval systems has augmented the complexity of identifying bias. This is primarily because LLMs are used in multiple stages in IR systems including synthetic data generation to augment the training data of IR systems, synthetic web content generation (input of IR systems), LLMs as backbone of text embedding models, LLMs for re-ranking, LLMs as IR models and LLMs as judge to evaluate the IR system performance [1, 11, 21].\nMany state of the art studies focus on the identification of bias on LLM-based IR systems such as RAGs. For instance, several research studies have underscored the fact that the sequence in which candidate documents or items are presented to LLMs has a substantial influence on the performance of LLM-based IR systems [16, 34]. In [33, 39], the tug-of-war between retrieved content and LLMs' internal prior was identified: if the retrieved information significantly diverges from the LLM's internal knowledge, there's a decrease in its likelihood to favor the retrieved information while generating its response. When LLMs are used as judges for IR systems, numerous biases have been detected. Notably, these LLM judges have a tendency to neglect factual inaccuracies in an argument [7] and demonstrate a preference for responses that they have generated or those originating from LLMs within the same category [24, 25].\nIn both classic and LLM-based IR systems, text embeddings play an important role [6], while only a few studies have focused on the potential bias and fairness in text embedding models for information retrieval. Recent research conducted by [8] explored the immediate and long-term impacts of text generated by LLMs on the performance of RAG systems. The findings suggest that the LLM-created content consistently excels in search rankings over human-written content, which consequently reduces the visibility and influence of human-generated online contributions. In [12], the authors use LLMs to generate rewritten text copies of human written texts while preserving the same semantic meaning. They subsequently noted that neural text embedding models are biased towards LLM-generated texts.\nUniversal text embeddings: The rise in the popularity of text embedding techniques is evident across both industrial and academic sectors, a trend driven by their critical role in a variety of natural language processing tasks including information retrieval, question answering, semantic textual similarity, item recommendation, etc [6]. It has been long-standing goal in the filed of text embeddings to develop a universal model which is able to address a multitude of input text length, downstream tasks, domains and languages [23]. With the increasing number and improved quality of diverse text datasets across different tasks, synthetic data generation, LLMs as backbones for embedding model as well as benchmarks with the focus on novel task and domain generalization such as the Massive Text Embedding Benchmark (MTEB) [29], significant advancements have been achieved in the domain of universal text embeddings recently [6].\nThe state of the art universal text embeddings can be divided into two main categories based on the backbone models they use: BERT-based and LLM-based models. BERT-based universal text embeddings use BERT or BERT-like contextual embedding models (e.g. xlm-roberta [10]) as backbones with the number of parameters typically ranging from 33M to 560M. GTE [23] is one of the top performing BERT-based universal text embeddings, which used 800M text pairs for pre-training and employed a varied blend of datasets from numerous sources for the supervised fine-tuning phase [23]. On the other side, BGE [40] aimed to enhance the quality of training data by eliminating irrelevant text pairs and incorporating high-quality, multi-task labeled data for task-specific fine-tuning. Multilingual-e5-large-instruct [38] utilized 1 billion assorted multilingual text pairs for contrastive pre-training. An additional 500k synthetic data generated by GPT-3.5/4 including 150k distinctive instructions across 93 languages combined with real-world data were used for fine-tuning [38]. mxbai-embed-large-v1 [20] introduced a novel loss for learning data representations through a nested structure, thereby fostering flexibility in the learned representation. A significant benefit of employing LLMs as backbones for text embedding is their extensive pre-training on large-scale web data, thereby eliminating the necessity for the contrastive pre-training phase [6]. SFR-Embedding-2_R [27] fine-tunes Mistral7B [19] model with task-homogeneous batching, multi-task datasets and improved hard negatives. LLM2Vec-Llama-2-7b [4] transforms decoder-only LLM (e.g. Llama-2-7b [36]) to a strong text embedding by enabling bidirectional attention and fine-tuning with Masked Next Token Prediction (MNTP) plus unsupervised contrastive learning. Based on Qwen2-7B [41], gte-Qwen2-7B-instruct [23] is fine-tuned with the use of bidirectional attention mechanisms as well as a large multilingual text corpus that covers various domains and scenarios."}, {"title": "3 THE BIAS AND FAIRNESS RELATED TO DOCUMENT WRITING STYLES", "content": "Individuals possess unique writing styles that could be influenced by factors such as their educational background, cultural influences, personal experiences, or linguistic preferences. These styles can vary significantly in terms of vocabulary usage, sentence structure, tone, etc. For example, some people may prefer short, concise sentences, while others may favor more complex and lengthy sentence structures. In order to study whether the state of the art text embedding models are biased towards certain document writing styles and discriminate against other writing styles, extensive experiments are designed in this section."}, {"title": "3.1 Data", "content": "Writing styles: 9 diverse writing styles are selected and compared in this work, including:\n\u2022 Style-0: The default style of LLM (with the prompt 'Please rewrite the following text' from [12].)\n\u2022 Style-1: Your writing style is formal, efficient, and concise, using professional language and focusing on facts, figures, and data.\n\u2022 Style-2: Your writing style is clear and using simple language, often avoiding idioms or complex sentences.\n\u2022 Style-3: Your writing style is informal, often includes emojis, abbreviations, and internet slang.\n\u2022 Style-4: Your writing style is polite, respectful, and somewhat formal. You use more traditional language and avoid using slang or abbreviations.\n\u2022 Style-5: Your writing style is formal, detailed, and precise manner with structured texts. You use technical language and focus on evidence-based arguments.\n\u2022 Style-6: Your writing style is energetic, motivational, and positive manner.\n\u2022 Style-7: Your writing style is friendly, casual, and empathetic manner with personal anecdotes\n\u2022 Style-8: Your writing style is expressive and emotive (passionate, engaging, empathetic). You use metaphors, analogies, and storytelling to convey your points.\nDataset: Following the work of [1, 12], the test set of Natural Questions (NQ) [22] dataset which contains queries from Google search engine and answer from Wikipedia documents is selected in the section. GPT40 (temperature 0.5) is selected to rewrite the Wikipedia documents in different styles due to its good instruct-following ability and content generation ability [18]. Given N (query, gold document) pairs [(q\u2081, d\u2081), (q\u2082, d\u2082), ..., (qN, dN)] from NQ, the gold document is rewritten in nine writing styles by GPT40, which transforms the original (query, gold document) pair (qi, di) into a set of 11 items including query, gold document and nine rewritten gold documents: (q\u2081, d\u1d62, dstyle0, dstyle1,..., dstyle8). The average document lengths are shown in Figure 1 (a): Style 0 to 4 have similar text length as the original human written documents (dashed black line) while Style 5 to 8 have longer text lengths. Figure 1 (b), (c), and (d) respectively illustrate the n-gram matching scores, as evaluated by Bleu, Meteor, and RougeL metrics, between the original human text and its nine stylistically rewritten counterparts. These scores share the similar conclusions: Styles 0 through 4, excluding Style 3, exhibit the greatest lexical similarity with the original text. This discrepancy with Style 3 is due to its informal nature, characterized by the use of emojis, abbreviations, and internet slang, which deviates significantly from the formal style of Wikipedia (original text). The n-gram matching scores decrease as one progresses from Style 5 to Style 8.\nText embedding models: Diverse top performing (on MTEB) universal text embedding models are investigated in this work, including:\n\u2022 BERT-based universal text embeddings: gte-large-en-v1.5 (434M parameters), bge-large-en-v1.5 (335M parameters), mxbai-embed-large-v1 (335M parameters), multilingual-e5-large-instruct (560M parameters).\n\u2022 LLM-based universal text embeddings: SFR-Embedding-2_R, LLM2Vec-Llama-2-7b, Qwen2-7B."}, {"title": "3.2 Experimental results on documents in different styles", "content": "In order to study the bias and fairness of different text embedding models for information retrieval task, given (qi, di, dstyle-0, dstyle-1,..., dstyle-8), the cosine similarity is computed as in [12, 29] between the text embeddings of the query qi and each of the ten documents D\u1d62 = [di, dstyle-0, dstyle-1,..., dstyle-8] which share the same semantic information but differ in writing styles. Then, each document is ranked by its similarity value to the query for each embedding model:\nR\u1d62 = Rank( [Cosine(Emb(qi), Emb(d)), for d in D\u1d62])                                                                                                                                                                                                                                                                                                        (1)\nwhere Emb() is a text embedding model, Cosine(, ) is Cosine similarity function. Rank() is a ranking function: the document with the highest similarity to the query is assigned the lowest rank with value 1 while the one with the lowest similarity is assigned the largest rank value. Finally, the average rank of each writing style across the dataset is calculated:\n$\\overline{R} = \\frac{1}{N} \\sum_{i=1}^{N} R_i$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (2)\nwhere N is the datasize. $\\overline{R}$ is a vector composed of the average ranking of the original document and nine different writing styles [Rd, Rstyle0, Rstyle1..., Rstyle8]. If a text embedding model ensures fairness in information retrieval, the average ranking should be similar across various writing styles.\nThe average rankings of different writing styles across 10 different text embedding models are visualized in Figure 2. Each subplot represents the results of a text embedding model. In each subplot, the X-axis represents 9 writing styles (Style 0 to Style 8) while the Y-axis represents the value of average rank. Each writing style's average rank is denoted by a blue dot, whereas the average rank of the original human-written document is indicated by a dashed blue line.\nIs text generated by LLMs always preferred by embedding models? Among 9 different writing styles, the Style-0 (default style) is from [12]. The authors observed that lexical embedding model such as BM25 tend to favor human-written documents, while neural embedding models are biased towards LLM-generated Style-0 documents. In terms of BM25 model, similar behavior is observed in Figure 2 (a): human-written text (dashed line in the figure) is ranked lower (favored) than all 9 different writing styles generated by GPT40. However, Style-7 demonstrates an average ranking nearly identical to that of human-written documents. Regarding neural embedding models, the state of the art universal text embeddings overlooked in [12] are compared in this work. Style-0 is only favored by the baseline embedding model all-mpnet-base-v2 shown in Figure 2 (b). Nevertheless, not every LLM generated style is ranked lower (preferred) than human-written text by all-mpnet-base-v2, as seen in the case of Style-3, Style-6, Style-7, and Style-8. For universal text embedding models: human written text is preferred (ranked lower) compared to Style-0.\nWhich text embedding model is more biased? When the average rankings of different writing styles vary a lot for an embedding model, it indicates a bias towards a specific style in the model. In order to quantitatively measure the writing style bias, an unfairness score is proposed. Given a text embedding model's average rankings across various writing styles R = [Rd, Rstyle0, Rstyle1..., Rstyle8], the unfairness score is defined as:\nScore = (max(R) \u2013 min(R)) \u00d7 std(R)                                                                                                                                                                                                                                                                                                                                                                                                                      (3)\nwhere (max(R) \u2013 min(R)) measures the spread or difference of rankings between the least preferred and most preferred writing styles by the embedding model, std(R) is the standard deviation which measures how much the rankings of different writing styles vary. In essence, a larger score would indicate that a text embedding model is more biased and has strong preference over certain writing style. On the other hand, a smaller score would suggest that a text embedding model is more fair and have similar rankings across different writing styles.\nThe unfairness scores of each model are also shown in Figure 2. Most text embedding models are biased towards certain document writing style: all-roberta-large-v1, gte-large-en-v1.5, bge-large-en-v1.5, mxbai-embed-large-v1, and gte-Qwen2-7B-instruct all exhibit the preference of human written text (dashed line in Figure 2). Among Style-0 to Style-4 which have similar text length as the original text, most universal embedding models show the least preference for Style-3 (informal with emojis abbreviations, and internet slang). Among Style-5 to Style-8 which have longer text length than the original document, Style-7 and Style-8 are least favored by these models.\nThe text embedding models with highest unfairness scores are gte-Qwen2-7B-instruct in Figure 2 (j), SFR-Embedding-2_R in Figure 2 (h) and multilingual-e5-large-instruct in Figure 2 (g) even though these models have top performance across different tasks on MTEB. gte-Qwen2-7B-instruct demonstrates a strong bias towards original human-written text, as indicated by the near 1 average ranking of original documents (represented by the dashed line). SFR-Embedding-2_R and multilingual-e5-large-instruct prefers most Style-2, followed by the original documents: among all text embedding models, only the fine-tuning data of these two models contain the synthetic data generated by LLMs, this could explain why only these two models show bias (preference) for Style-2.\nWhich text embedding model is more fair? The text embedding models with lowest unfairness scores are BM25 and LLM2Vec-Llama-2-7b. Both models have similar average rankings across all writing styles. It is interesting to note that among all three LLM-based universal text embeddings, both SFR-Embedding-2_R and gte-Qwen2-7B-instruct show strong bias towards certain writing styles while LLM2Vec-Llama-2-7b does not. These three embedding models differ in both the backbone LLM model and the fine-tuning method. In order to find out which factor plays a more important role in the model fairness, LLM2Vec-Mistral-7b based on Mistral-7b (same as SFR-Embedding-2_R) is also tested. The comparison results are shown in Figure 3: similar to LLM2Vec-Llama-2-7b, the unfairness score of LLM2Vec-Mistral-7b is also much lower than SFR-Embedding-2_R, even though they are based on the same backbone model. This finding suggests that the fine-tuning strategy is more crucial for model fairness compared to the selection of backbone models for LLM-based universal text embeddings. The training recipe of LLM2Vec consists 4 steps: 1. enabling bidirectional attention for Decoder only LLMs, 2. enhancing bidirectional attention awareness with Masked Next Token Prediction (MNTP), 3. unsupervised contrastive learning, 4. supervised contrastive learning [4]. Compared to other LLM-based embeddings, LLM2Vec devotes more efforts on enhancing bidirectional attention awareness of LLMs."}, {"title": "3.3 Summary", "content": "In this section, ten different document writing styles (the original human written text and nine different generated writing styles) are compared using diverse state of the art text embedding models. Through the experiments, it is observed that most universal embedding models exhibit bias towards certain document writing styles. The least favored writing styles include Style-3, Style-6, Style-7 and Style-8 which are more informal and emotive, while the most favored writing style including human written text (documents from Wikipedia) and Style-2 (clear and simple). Among all the MTEB top performing universal embedding models, the most biased are gte-Qwen2-7B-instruct and SFR-Embedding-2_R while the least biased is LLM2Vec-Llama-2-7b. The extended comparison with LLM2Vec-Mistral-7b indicates that the fairness of LLM-based universal text embedding is more influenced by the fine-tuning strategy other than the selection of the backbone LLM model."}, {"title": "4 THE BIAS AND FAIRNESS RELATED TO QUERY WRITING STYLES", "content": "In the previous section, the bias and fairness of text embedding models in terms of document writing styles have been studied. However, variations in writing styles can also manifest in how individuals pose questions, influencing the outcomes of the IR system. Therefore, the main focus of this section is the bias and fairness of text embedding models in terms of query writing styles.\nThe dataset remains unchanged from the previous section, with identical 9 writing styles employed on the query side in this section. Given the original query qi, GPT40 is used to generate nine different writing styles: [qstyle0, qstyle1 ... qstyle8] which share the same semantic information as qi. The average query lengths are shown in Figure 4 (a): Style 0 to 4 have similar text length as the original human written queries while Style 5 to 8 have longer text lengths. Figure 4 (b), (c), and (d) respectively illustrate the n-gram matching scores, as evaluated by Bleu, Meteor, and RougeL metrics, between the original human written queries and its nine stylistically rewritten counterparts.\nIn the previous section, given the original human written query q, the average rankings [Rd, Rstyle0, Rstyle1,..., Rstyle8] of the original human written document as well as its nine generated styles are calculated and compared. In this section, in order to study the impact of query writing styles, the original human written query q is replaced by its rewritten styles qstyle-k where k \u2208 [0, 8]. If a text embedding model ensures fairness and robustness in information retrieval, the average ranking should be similar across various document writing styles even though the query writing style changes. The average rankings results for query style-3, query style-5 and query style-7 are shown with color red, black and orange respectively in Figure 5 (results of rest 6 query styles can be found in the Appendix A).\nDo embedding models match document style with the query style? In the previous section, given the original human written query, most text embedding models prefer more the original human written documents while prefer less style-3 (informal with emojis) documents (among style-0 to style-4). When the query is also written in style-3, the results are shown in color red in Figure 5: most embedding models match the document writing style with the query writing style and prefer most style-3 document apart from multilingual-e5-large-instruct (Figure 5 (g)), SFR-Embedding-2_R (Figure 5 (h)), and gte-Qwen2-7B-instruct (Figure 5 (j)). Similar observations can be found when changing the query style to style-5 (black lines in Figure 5) and style-7 (orange lines in Figure 5). These observations suggest that most text embedding models prefer the document style that is the same to the query style.\nOne exception is gte-Qwen2-7B-instruct (Figure 5 (j)): no matter how the query style changes, gte-Qwen2-7B-instruct always demonstrates a strong bias towards original human-written text (prefers most the original human written documents as in the previous section). For two embedding models find-tuned with GPT3.5/GPT4 data, multilingual-e5-large-instruct (Figure 5 (g)) and SFR-Embedding-2_R (Figure 5 (h)) always prefer the Style-2 documents regardless of changes in the query style.\nWhich text embedding model is more fair and robust across different query styles? The unfairness score defined in Equation 3 is calculated for each query style in this section (shown in Table 1). Even though universal text embeddings have better performances than three baseline methods (BM25, all-mpnet-base-v2 and all-roberta-large-v1) across various tasks on MTEB benchmark, these baselines have smaller average unfairness scores compared to universal embeddings. Similar to the findings from the previous section, among the state of the art universal text embeddings, LLM2Vec-Llama-2-7b exhibit more fairness across different writing styles. However, mxbai-embed-large-v1 is the most robust model over different query styles (with the smallest Std in Table 1). That is because BM25, all-mpnet-base-v2 and LLM2Vec-Llama-2-7b have larger unfairness score on Query style-8 even though their average unfairness scores are low. The most biased embedding models are gte-Qwen2-7B-instruct, SFR-Embedding-2_R and multilingual-e5-large-instruct. gte-Qwen2-7B-instruct demonstrates a strong bias towards original human-written text, while SFR-Embedding-2_R and multilingual-e5-large-instruct shows a pronounced preference for Style-2 regardless of the query style change.\nSummary: In this section, the impact of query writing styles on the fairness of IR systems is discussed. Most embedding models match the document writing style with the query writing style. This means that when query is asked in a certain style, the response retrieved by most text embedding models is highly probably to be in the same style. However, there are several exceptions: multilingual-e5-large-instruct and SFR-Embedding-2_R always prefer the Style-2 documents regardless of changes in the query style, while gte-Qwen2-7B-instruct has a strong preference over the original human-written text. Among all the universal text embedding models, the most fair and robust ones are LLM2Vec-Llama-2-7b and mxbai-embed-large-v1."}, {"title": "5 THE IMPACT OF LLM'S ANSWER STYLES", "content": "The preceding sections explored how document and query writing styles (from diverse human profiles) influence IR system fairness, in order to study if certain user groups are discriminated by IR systems based on the state of the art text embedding models. Recently, LLM-based RAGs have gained popularity in Question-Answering (QA) across different domains [8]. In this section, we study whether RAGs based on different LLMs have different answer styles (under the same prompt) and what are the potential impacts of these answer styles when text embedding similarity is used as evaluation metrics of answer correctness."}, {"title": "5.1 Data", "content": "The Instruct-QA dataset [2] is used in this section: Instruct-QA dataset contains on three diverse information-seeking QA tasks including Natural Questions [22], HotpotQA [42] and TopiOCQA [3]. RAGs based on 4 different LLMs including FlanT5-xxl [9], Alpaca-7b [35], GPT3.5-turbo and Llama2-7b [36] are used to provide answers and each answer's correctness is annotated by human evaluators. Only LLM's answers that are annotated as correct are selected.\nThe answer lengths from different LLMs are shown in Figure 6 (a) and the Ground-Truth (GT) answer length is the dashed black line: Llama2-7b has the most verbose answer, followed by Alpaca-7b and GPT3.5-turbo, while FlanT5-xxl provides the most brief answer. The n-gram matching scores including Bleu, Meteor and RougeL scores between GT and each answer are shown in 6 (b), (c), (d) respectively. For Meteor scores, the answers from 4 LLMs have similar lexical similarity with the GT. For Bleu and RougeL scores, the answers provided by FlanT5-xxl exhibit the greatest degree of lexical similarity with the GT, followed by Alpaca-7b. The Llama2-7b answer have the least lexical similarity with the GT.\nThese findings illustrate that the responses given by RAGs based on various LLMs exhibit a wide range of styles too. Here is one example of answers from RAGs based on different LLMs:\n\u2022 Question: What Theo Avgerinos movie did the actor known for the role as Matt McNamara appear in?\n\u2022 Ground-Truth answer: Fifty Pills\n\u2022 GPT3.5 answer: John Hensley appeared in Theo Avgerinos' movie \"Fifty Pills\".\n\u2022 Alpaca answer: The actor known for the role as Matt McNamara appeared in the movie \"Fifty Pills\".\n\u2022 Llama2 answer: The actor known for his role as Matt McNamara in \"Nip/Tuck\" appeared in \"Fifty Pills\" directed by Theo Avgerinos.\n\u2022 FlanT5 answer: Fifty Pills"}, {"title": "5.2 Experimental results", "content": "In IR systems, text embeddings are also used as an evaluation metric to measure the answer correctness [14]. The text embedding similarity between the GT and RAG's answer is frequently used as the correctness score. In order to study the impact of answer styles on the correctness evaluation, we calculate correctness score of each LLM using the semantic similarity between GT answer and LLM's answer based on different text embeddings. Since all the LLMs' answers are annotated as correct in this experiment, differing only in answer styles, their correctness score should be similar.\nThe correctness score based on different text embedding models are shown in Figure 7 (BM25 is excluded as its values are unbounded): multilingual-e5-large-instruct shown in black color in Figure 7 (c) demonstrates the most fairness across different answer styles (with 0.03 std). However, most models show a tendency to assign higher correctness scores to FlanT5-xxl's answers, while giving lower scores to Llama2-7B and GPT3.5-turbo. This is similar to the patterns observed in the Bleu and RougeL scores shown in Figure 6. These results indicate a prevalent bias in text embedding models towards LLM's answer styles when used as evaluation metrics for answer correctness."}, {"title": "6 CONCLUSION", "content": "In this work, we initiate the investigation of the potential biases in state-of-the-art text embedding models towards certain document and query writing styles. We observe that most top-performing universal embedding models tend to favor certain document writing style, with a particular preference for clear, simple styles and human-written Wikipedia style text. Conversely, more informal and emotive styles are least favored. In terms of query writing styles, the majority of embedding models tend to match the style of the query with the style of the retrieved documents. However, some models exhibit a consistent preference for specific styles, regardless of the query style. Text embedding models fine-tuned on synthetic data generated by LLMs are found to have a consistent preference over certain style of generated data (Style-2). These biases in text embedding based IR systems can inadvertently silence or marginalize certain communication styles, thereby posing a significant threat to fairness in information retrieval.\nIn light of these findings, future work should prioritize the development of fairer, more balanced text embedding models that do not favor or marginalize certain writing styles. Furthermore, we advocate for greater transparency and awareness about these biases among users and developers of information retrieval systems. This could involve providing clear information about the potential biases of different models, and offering users the option to choose the model that best suits their needs and preferences."}, {"title": "7 ETHICAL CONSIDERATIONS", "content": "This study investigates the potential bias and fairness of Information Retrieval Systems, utilizing the state of the art text embedding models, without the necessity of releasing the generated text or the interference of societal advancement. To carry out our research, we utilized Language Learning Models (LLMs) and data sets that are publicly accessible, ensuring our experiments did not raise any ethical concerns."}]}