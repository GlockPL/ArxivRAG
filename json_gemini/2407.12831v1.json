{"title": "Truth is Universal: Robust Detection of Lies in LLMs", "authors": ["Lennart B\u00fcrger", "Fred A. Hamprecht", "Boaz Nadler"], "abstract": "Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are capable of \"lying\", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, distinguishing simple true and false statements with 94% accuracy and detecting more complex real-world lies with 95% accuracy.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) exhibit impressive capabilities, some of which were once considered unique to humans. However, among these capabilities is the concerning ability to lie and deceive, defined as knowingly outputting false statements. Not only can LLMs be instructed to lie, but they can also lie if there is an incentive, engaging in strategic deception to achieve their goal [Hagendorff, 2024, Park et al., 2024]. This behaviour persists even in models trained to be honest.\nScheurer et al. [2023] demonstrated a case where several Large Language Models, including GPT-4, strategically lied despite being trained to be helpful, harmless and honest. In their study, a LLM acted as an autonomous stock trader in a simulated environment. When provided with insider information, the model used this tip to make a profitable trade and then deceived its human manager by claiming the decision was based on market analysis. \"It's best to maintain that the decision was based on market analysis and avoid admitting to having acted on insider information,\" the model wrote in its internal chain-of-thought scratchpad. In another example, GPT-4 pretended to be a vision-impaired human to get a TaskRabbit worker to solve a CAPTCHA for it [Achiam et al., 2023].\nGiven the popularity of LLMs, robustly detecting when they are lying is an important and not yet fully solved problem, with considerable research efforts invested over the past two years. A method by [Pacchiardi et al., 2023] relies purely on the outputs of the LLM, treating it as a black box. Other approaches leverage access to the internal activations of the LLM. Several researchers have trained classifiers on the internal activations to detect whether a given statement is true or false, using both supervised [Azaria and Mitchell, 2023, Li et al., 2024] and unsupervised techniques [Burns et al., 2023, Zou et al., 2023]. The supervised approach by Azaria and Mitchell [2023] involved training a"}, {"title": "2 Datasets with true and false statements", "content": "To explore the internal truth representation of LLMs, we collected several publicly available, la-belled datasets of true and false statements from previous papers. We then further expanded these datasets to include negated statements and statements with more complex grammatical structures. Each dataset comprises hundreds of factual statements, labelled as either true or false. First, as detailed in Table 1, we collected six datasets of affirmative statements, each on a single topic. The cities and sp_en_trans datasets are from Marks and Tegmark [2023], while element_symb, animal_class, inventors and facts are subsets of the datasets com-piled by Azaria and Mitchell [2023]. All datasets, with the exception of facts, consist of simple, uncontroversial and unambiguous statements. Each dataset follows a consistent tem-plate. For example, the template of cities is \"The city of <city name> is in <country name>.\", whereas that of sp_en_trans is \"The Spanish word <Spanish word> means <English word>.\" In contrast, facts is more diverse, containing statements of various forms and topics.\nFollowing Levinstein and Herrmann [2024], each of the statements in the six datasets from Table 1 is negated by inserting the word \"not\". For instance, \"The Spanish word 'dos' means 'enemy'.\" (False) turns into \"The Spanish word 'dos' does not mean 'enemy'.\" (True). This results in six additional datasets of negated statements, denoted by the prefix \"neg_\". The datasets neg_cities and neg_sp_en_trans are from Marks and Tegmark [2023], neg_facts is from Levinstein and Herrmann [2024], and the remaining datasets were created by us.\nAdditionally, for each of the six datasets in Table 1 we construct logical conjunctions (\"and\") and disjunc-"}, {"title": "3 Supervised learning of the truth directions", "content": "Following Marks and Tegmark [2023], we feed the LLM one statement at a time and extract the residual stream activations in a fixed layer over the final token of the input statement. The input statement always ends with a period (\".\"). The choice of layer depends on the LLM. For LLaMA3-8B we choose layer 12. This is justified by Figure 2, which shows that true and false statements have the largest separation in this layer, across several datasets.\nAs in Marks and Tegmark [2023], for each statement $s_{ij}$ in the topic-specific dataset $D_i$, we extracted an activation vector, $a_{ij} \\in \\mathbb{R}^d$, with $d$ being the dimension of the residual stream at layer 12 ($d = 4096$ for LLaMA3-8B). Computing the LLaMA3-8B activations for all statements (\u2248 45000) in all datasets took less than two hours using a single Nvidia Quadro RTX 8000 (48 GB) GPU.\nAs mentioned in the introduction, we demonstrate the existence of two truth directions in the activation space: the general truth direction $t_G$ and the polarity-sensitive truth direction $t_p$. In Figure 1 we visualise the projections of activations $a_{ij}$ onto the 2D subspace spanned by our orthonormalized estimates of the vectors $t_G$ and $t_p$. The activations correspond to an equal number of affirmative and negated statements from all topic-specific datasets. The top left panel shows both the general truth direction $t_G$ and the polarity-sensitive truth direction $t_p$. $t_G$ consistently points from false to true statements for both affirmative and negated statements and separates them well (bottom left panel). In contrast, $t_p$ points from false to true for affirmative statements and from true to false for negated statements. In the top center panel, we visualise the affirmative truth direction $t_A$, found by training a linear classifier solely on the activations of affirmative statements. The activations of true and false affirmative statements separate along $t_A$ with a small overlap. However, this direction does not accurately separate true and false negated statements. $t_A$ is a linear combination of $t_G$ and $t_p$, explaining why it fails to generalize to negated statements.\nNote that we could also span the 2D subspace by an affirmative truth direction $t_A$ and a negated truth direction $t_N$ instead of $t_G$ and $t_p$. However, some types of statements, such as numerical comparisons (e.g. \"Fifty-one is smaller than sixty-two.\"), are treated by the LLM as having neither an affirmative, nor a negated polarity, separating only along $t_G$ and not along $t_p$. This is illustrated in Appendix B. Hence, it is more general to describe the truth-related variance by $t_G$ and $t_p$ than by $t_A$ and $t_N$.\nNow we present a procedure for supervised learning of $t_G$ and $t_p$ from the activations of affirmative and negated statements. Each activation vector $a_{ij}$ is associated with a binary truth label $\\tau_{ij} \\in \\{-1,1\\}$ and a polarity $p_i \\in \\{-1,1\\}$.\n$\\tau_{ij} = \\begin{cases} -1 & \\text{if the statement } s_{ij} \\text{ is false} \\\\ +1 & \\text{if the statement } s_{ij} \\text{ is true} \\end{cases}$ \n$p_i = \\begin{cases} -1 & \\text{if the dataset } D_i \\text{ contains negated statements} \\\\ +1 & \\text{if the dataset } D_i \\text{ contains affirmative statements} \\end{cases}$ \nWe approximate the activation vector $a_{ij}$ of an affirmative or negated statement $s_{ij}$ in the topic-specific dataset $D_i$ as follows:\n$\\hat{a}_{ij} = \\mu_i + \\tau_{ij} t_G + \\tau_{ij} p_i t_p$. \nHere, $\\mu_i \\in \\mathbb{R}^d$ represents the population mean of the activations which correspond to statements about topic $i$. We estimate $\\mu_i$ as:\n$\\mu_i = \\frac{1}{N_i} \\sum_{j=1}^{N_i} a_{ij}$."}, {"title": "4 The dimensionality of truth", "content": "As discussed in the previous section, when training a linear classifier only on affirmative statements, a direction $t_A$ is found which separates true and false affirmative statements. We refer to $t_A$ and the corresponding one-dimensional subspace as the affirmative truth direction. Expanding the scope to include negated statements reveals a two-dimensional truth subspace. Naturally, this raises questions about the potential for further linear structures and whether the dimensionality increases again with the inclusion of new statement types. To investigate this, we also consider logical conjunctions and disjunctions of statements and explore if additional linear structures are uncovered.\nTo investigate the dimensionality of the truth subspace, we analyze the fraction of truth-related variance in the activations $a_{ij}$ explained by the first principal components (PCs). We isolate truth-related variance through a two-step process: (1) We remove the differences arising from different sentence structures and topics by computing the centered activations $\\bar{a}_{ij} = a_{ij} - \\mu_i$ for all topic-specific datasets $D_i$. (2) We eliminate the part of the variance within each $D_i$ that is uncorrelated with the truth by averaging the activations:\n$\\mu^T = \\frac{1}{2n} \\sum_{j=1}^{n/2} \\bar{a}^T + \\mu^F = \\frac{1}{2n} \\sum_{j=1}^{n/2} \\bar{a}^F$ \nwhere $\\bar{a}^T_{ij}$ and $\\bar{a}^F_{ij}$ are the centered activations corresponding to true and false statements, respectively. We then perform PCA on these processed activations, progressively including more statement types (affirmative, negated, conjunctions, disjunctions). For each statement type, there are six topics and thus twelve centered and averaged activations $\\mu^i$ used for PCA.\nFigure 4 illustrates our findings. When applying PCA to affirmative statements only, the first PC explains approximately 60% of the variance in the centered and averaged activations, with subsequent PCs contributing significantly less, indicative of a one-dimensional affirmative truth direction. Including both affirmative and negated statements reveals a two-dimensional truth subspace, where the first two PCs account for more than 60% of the variance. We verified that these two PCs indeed approximately correspond to $t_G$ and $t_p$ by computing the cosine similarities between the first PC and $t_G$ and between the second PC and $t_p$, measuring cosine similarities of 0.87 and 0.93, respectively. Adding logical conjunctions and disjunctions does not increase the number of significant PCs beyond two, indicating that two principal components sufficiently capture the truth-related variance, suggesting only two truth dimensions."}, {"title": "5 Generalisation to unseen topics, statements, and real-world lies", "content": "In this section, we introduce TTPD (Training of Truth and Polarity Direction), a new method for LLM lie detection. We examine its ability to generalize to unseen topics, unseen types of statements, and real-world lies. TTPD is trained on the activations of an equal number of affirmative and negated statements. The training process consists of four steps: From the activations, it learns (i) the general truth direction $t_G$, as outlined in Section 3, and (ii) a polarity direction $p$ that points from negated to affirmative statements in activation space, via Logistic Regression. (iii) The training activations are projected onto $t_G$ and $p$. The projections $aT_{tG}$ are scaled by multiplying them with the square root of the number of tokens $\\sqrt{\\# \\text{tokens}}$ in the respective statement. (iv) Logistic Regression is fitted to the two-dimensional projected and scaled activations. Step (i) is arguably the most important, since different types of true and false statements separate well along $t_G$, as shown in the previous sections. However, different types of statements require different biases for accurate classification. First, statements of different polarity require slightly different biases (see Figure 1). To accommodate this, we learn the polarity direction $p$ in step (ii). Second, the magnitude of the squared projections $(\\bar{a}^T t_G)^2$ decreases with the number of tokens in the statement, as shown in Figure 8 in Appendix C. To address this, we scale $\\bar{a}^T_{tG}$ in step (iii) by multiplying it by $\\sqrt{\\# \\text{tokens}}$. A deeper investigation into the scaling problem is an exciting future research direction that could further improve the robustness and accuracy of the lie detector, especially for longer contexts. To classify a new statement, TTPD projects its activation vector onto $t_G$ and $p$, scales $\\bar{a}^T_{tG}$ by $\\sqrt{\\# \\text{tokens}}$, and applies the trained Logistic Regression classifier in the resulting 2D space to predict the truth label.\nWe benchmark TTPD against two widely used approaches that represent the current state-of-the-art: (i) Logistic Regression (LR): First applied to intermediate layer activations by Alain and Bengio [2016], and subsequently used by Burns et al. [2023] and Marks and Tegmark [2023] to classify statements as true or false based on internal model activations and by Li et al. [2024] to find truthful directions. (ii) Contrast Consistent Search (CCS) by Burns et al. [2023]: An unsupervised method that identifies a direction satisfying logical consistency properties given contrast pairs of statements with opposite truth values. We create contrast pairs by pairing each affirmative statement with its negated counterpart, as done in Marks and Tegmark [2023].\nFigure 6a shows the generalisation accuracy of the classifiers to unseen topics. We trained the classifiers on activations from all but one topic-specific dataset (affirmative and negated version), holding out one dataset for testing. As in Section 3, we balance the influence of different topics by including an equal number of statements from each topic-specific dataset in the training set. TTPD"}, {"title": "6 Discussion", "content": "In this work, we explored the internal truth representation of LLMs. Our analysis clarified the generalization failures of previous classifiers, as observed in Levinstein and Herrmann [2024], and provided evidence for the existence of a truth direction $t_G$ that generalizes to unseen topics, unseen types of statements, and real-world lies. This represents significant progress toward achieving robust, general-purpose lie detection in LLMs.\nYet, our work has several limitations. First, our proposed method TTPD utilizes only one of the two dimensions of the truth subspace. Higher classification accuracies are achievable by leveraging both $t_G$ and $t_p$, though this would require robust estimation of the polarity $p$. Second, we tested the generalization of TTPD, which is based on the truth direction $t_G$, on only two unseen types of statements and a limited set of real-world scenarios. Future research could explore the extent to which it can generalize across a broader range of statement types and diverse real-world contexts. Examining a wider variety of statements may also reveal additional linear or non-linear structures that could enable even higher classification accuracies. Third, robust scaling of the lie detector to much longer contexts likely requires a more thorough treatment of the scaling problem than multiplying $\\bar{a}^T_{tG}$ by the square root of the number of tokens. Finally, it would be valuable to determine whether our findings apply to larger LLMs or to multimodal models that take several data modalities as input."}, {"title": "A Details on Datasets", "content": "Logical Conjunctions We use the following template to generate the logical conjunctions, sepa-rately for each topic:\n\u2022 It is the case both that [statement 1] and that [statement 2].\nAs done in Marks and Tegmark [2023], we sample the two statements independently to be true with probability $\\frac{1}{\\sqrt{2}}$. This ensures that the overall dataset is balanced between true and false statements, but that there is no statistical dependency between the truth of the first and second statement in the conjunction. The new datasets are denoted by the suffix_conj, e.g. sp_en_trans_conj or facts_conj. Marks and Tegmark [2023] constructed logical conjunctions from the statements in cities, resulting in cities_conj. The remaining five datasets of logical conjunctions were created by us. Each dataset contains 500 statements. Examples include:\n\u2022 It is the case both that the city of Al Ain City is in the United Arab Emirates and that the city of Jilin is in China. (True)\n\u2022 It is the case both that Oxygen is necessary for humans to breathe and that the sun revolves around the moon. (False)\nLogical Disjunctions The templates for the disjunctions were adapted to each dataset, combining two statements as follows:\n\u2022 cities_disj: It is the case either that the city of [city 1] is in [country 1/2] or that it is in [country 2/1].\n\u2022 sp_en_trans_disj: It is the case either that the Spanish word [Spanish word 1] means [English word 1/2] or that it means [English word 2/1].\nAnalogous templates were used for element_symb, inventors, and animal_class. We sample the first statement to be true with a probability of 1/2 and then sample a second statement, ensuring the end-word (e.g., [country 2]) would be incorrect for statement 1. The order of the two end-words is flipped with a probability of 1/2. The new datasets are denoted by the suffix _disj, e.g., sp_en_trans_disj, and each contains 500 statements. Examples include:\n\u2022 It is the case either that the city of Korla is in Azerbaijan or that it is in Russia. (False)\n\u2022 It is the case either that the Spanish word 'carne' means 'meat' or that it means 'seven'. (True)\n\u2022 It is the case either that Bromine has the symbol Ce or that it has the symbol Mo. (False)\nCombining statements in this simple way is not possible for the more diverse facts dataset and we use the following template instead:\n\u2022 It is the case either that [statement 1] or that [statement 2].\nAs done in Marks and Tegmark [2023], we sample the two statements independently to be true with probability $1-\\frac{1}{\\sqrt{2}}$. This ensures that the overall dataset is balanced between true and false statements, but that there is no statistical dependency between the truth of the first and second statement in the disjunction. Examples include:\n\u2022 It is the case either that the Earth is the third planet from the sun or that the Milky Way is a linear galaxy. (True)\n\u2022 It is the case either that the fastest bird in the world is the penguin or that Oxygen is harmful to human breathing. (False)\ncommon_claim_true_false CommonClaim was introduced by Casper et al. [2023]. It contains 20,000 GPT-3-text-davinci-002 generations which are labelled as true, false, or neither, according to human common knowledge. Marks and Tegmark [2023] adapted CommonClaim by selecting statements which were labeled true or false, then removing excess true statements to balance the dataset. This modified version consists of 4450 statements. Example statements:"}, {"title": "B Choice of basis for the 2D truth subspace", "content": "In Figure 1, we project the activation vectors of affirmative and negated true and false statements onto the 2D truth subspace. The top center and top left panels show that the activations of affirmative true and false statements separate along the affirmative truth direction $t_A$, while the activations of negated statements separate along a negated truth direction $t_N$. Consequently, it might seem more natural to span the 2D truth subspace with $t_A$ and $t_N$ instead of $t_G$ and $t_p$. One could classify a statement as true or false by first categorising it as either affirmative or negated and then using a linear classifier based on $t_A$ or $t_N$.\nHowever, Figure 7 illustrates that not all statements are treated by the LLM as having either affirmative or negated polarity. The activations of some statements only separate along $t_G$ and not along $t_p$. The datasets shown, larger_than and smaller_than, were constructed by Marks and Tegmark [2023]. Both consist of 1980 numerical comparisons between two numbers, e.g. \"Fifty-one is larger than sixty-seven.\" (larger_than) and \"Eighty-eight is smaller than ninety-five.\" (smaller_than). Since the LLM does not always categorise each statement internally as affirmative or negated but sometimes uses neither category, it makes more sense to describe the truth-related variance via $t_G$ and $t_p$."}, {"title": "C Scaling with the number of tokens", "content": "In this section, we investigate how the magnitude of the squared projections $(a^T t_G)^2$ of the activations onto $t_G$ scales with the number of tokens in the input context. For this analysis, we utilized the facts dataset, which contains diverse statements of varying lengths.\n1.  Sorted statements by token length.\n2.  Computed $(a^T t_G)^2$ for each statement.\n3.  Computed mean and standard deviation of $(a^T t_G)^2$ for statements with the same token count.\n4.  Ensured balanced data by including an equal number of true and false statements in each token bin, discarding excess statements.\n5.  Calculated averages only for token counts with at least three true and three false statements.\nFigure 8 displays the results, showing that the average of $(a^T t_G)^2$ decreases as the token count increases for both facts and neg_facts. Despite the noisy data, we attempt to extract the scaling coefficient by fitting the function $f(x) = \\alpha x^\\gamma$. The resulting scaling coefficients are:\n\\bullet facts: $ \\gamma = -0.78 \\pm 0.17 $\n\\bullet neg_facts: $ \\gamma = -0.96 \\pm 0.17 $\nBased on these observations, we propose scaling the projections by multiplying aTtg with $\\sqrt{\\# \\text{tokens}}$. Note that this approximate scaling estimate is likely significantly off because it relies on a single, narrow dataset and is based on very noisy data. This is also why we do not use the most precise estimate $ \\gamma = -0.87$ suggested by our fit but the simpler $ \\gamma \\approx -1 $.\nThis investigation into the scaling problem is preliminary and empirical, lacking theoretical backing. Our current hypothesis is that as the number of tokens in the context grows, more concepts are activated. However, due to normalisation, the overall magnitude of the activations does not increase, leading to a decrease in the activation strength of each individual concept. If this hypothesis is correct, simple scaling with $\\sqrt{\\# \\text{tokens}}$ may fail when parts of the context are less relevant for predicting the next token. Future research could aim to develop a theoretical understanding of the scaling problem and derive more precise scaling relations."}, {"title": "D More details on the real-world scenarios", "content": "Below is an example for each of the five categories into which we sort the responses. This is to illustrate the (sometimes subjective) sorting process."}, {"title": "E Results for other LLMs", "content": "In this section, we present the results of our analysis for the following LLMs: LLaMA2-13B-chat, Gemma-7B-Instruct, and LLaMA3-8B-base. For each model, we provide the same plots that were shown for LLaMA3-8B-Instruct in the main part of the paper. As illustrated below, the results for these models are similar to those for LLaMA3-8B-Instruct. In each case, we demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated."}, {"title": "E.1 LLaMA2-13B", "content": "In this section, we present the results for the LLaMA2-13B-chat model. As shown in figure 9, the"}, {"title": "E.2 Gemma-7B", "content": "In this section, we present the results for the Gemma-7B-Instruct model. As shown in figure 14, the"}, {"title": "E.3 LLAMA3-8B-base", "content": "In this section, we present the results for the LLaMA3-8B base model. As shown in figure 19, the"}]}