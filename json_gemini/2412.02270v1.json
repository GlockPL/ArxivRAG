{"title": "Sustainable Self-evolution Adversarial Training", "authors": ["Wenxuan Wang", "Chenglei Wang", "Huihui Qi", "Menghao Ye", "Xuelin Qian", "Peng Wang", "Yanning Zhang"], "abstract": "With the wide application of deep neural network models in various computer vision tasks, there has been a proliferation of adversarial example generation strategies aimed at deeply exploring model security. However, existing adversarial training defense models, which rely on single or limited types of attacks under a one-time learning process, struggle to adapt to the dynamic and evolving nature of attack methods. Therefore, to achieve defense performance improvements for models in long-term applications, we propose a novel Sustainable Self-Evolution Adversarial Training (SSEAT) framework. Specifically, we introduce a continual adversarial defense pipeline to realize learning from various kinds of adversarial examples across multiple stages. Additionally, to address the issue of model catastrophic forgetting caused by continual learning from ongoing novel attacks, we propose an adversarial data replay module to better select more diverse and key relearning data. Furthermore, we design a consistency regularization strategy to encourage current defense models to learn more from previously trained ones, guiding them to retain more past knowledge and maintain accuracy on clean samples. Extensive experiments have been conducted to verify the efficacy of the proposed SSEAT defense method, which demonstrates superior defense performance and classification accuracy compared to competitors.", "sections": [{"title": "1 Introduction", "content": "Deep learning has been widely applied in computer vision tasks such as image classification [9, 50, 73] and object detection [6, 40, 51], resulting in significant advancements. However, the vulnerability of deep learning models to adversarial attacks [17, 28, 54] has become a critical concern. Adversarial examples involve intentionally crafted small perturbations that deceive deep learning models, leading them to produce incorrect outputs. This poses a serious threat to the reliability and security of these models in real-world applications. For example, attackers can use adversarial glasses to mislead facial recognition systems. Consequently, research on defense mechanisms [18, 24, 36] has become increasingly essential and urgent.For example, attackers can use adversarial glasses to mislead facial recognition systems.\nNowadays, researchers study various model defense methods to address highly destructive adversarial samples, including input sample denoising [23, 26] and attack-aware detection [15, 16]. Among these defense methods, adversarial training [13, 75] stands out as one of the most effective defense strategies. Adversarial training is a game-based training approach aimed at maximizing perturbations while minimizing adversarial expected risk. Its core idea is to integrate generated adversarial examples into the training set, enabling the model to learn from these examples during training and enhance its robustness.\nTherefore, it is essential for deep models to achieve sustainable improvement in defense performance for long-term application scenarios, as shown in Fig. 1. This has brought about the following challenges: (1) How to achieve the sustainability of the adversarial training strategy when new adversarial examples are constantly being born; (2) How to solve the model catastrophic forgetting problem caused by continuous exposure to new adversarial examples; (3) How to balance the model's robustness on adversarial examples and accuracy on clean data.\nIn this study, to address the above three challenges for the defense model against the ongoing generation of new adversarial examples, we propose a novel and task-driven Sustainable Self-Evolution Adversarial Training (SSEAT) framework to ensure the model maintains its accuracy and possesses robust and continuous defense capabilities. Our SSEAT defense method comprises three components: Continual Adversarial Defense (CAD), Adversarial Data Reply (ADR), and Consistency Regularization Strategy (CRS). To achieve sustainability in the adversarial training strategy (Challenge (1)), drawing inspiration from the continue learning paradigm, we propose a CAD pipeline, which learns from one type of adversarial example at each training stage to address the continual generation of new attack algorithms. To address the issue of catastrophic forgetting when continuously learning from various attacks (Challenge (2)), we introduce an ADR module to establish an effective re-learning sample selection scheme, advised by classification uncertainty and data augmentation. Meanwhile, to realize the trade-off between the model's robustness against adversarial examples and accuracy on clean data (Challenge (3)), We design a CRS module to help the model not overfit to current attacks and prevent the model from losing knowledge on clean samples. Overall, our SSEAT method effectively addresses a range of defense challenges arising from continuously evolving attack strategies, maintaining high classification accuracy on clean samples, and ensuring lifelong defense performance against ongoing new attacks.\nWe summarize the main contributions of this paper as follows:\n\u2022 We recognize the challenges of continuous defense setting, where adversarial training models must adapt to ongoing new kinds of attacks. This deep model defense task is of significant practical importance in real-world applications.\n\u2022 We propose a novel sustainable self-evolution adversarial training algorithm to tackle the problems under continuous defense settings.\n\u2022 We introduce a continual adversarial defense pipeline to learn from diverse types of adversarial examples across multiple stages, an adversarial data reply module to alleviate the"}, {"title": "2 Related Work", "content": "2.1 Adversarial Attacks\nThe impressive success of deep learning models in computer vision tasks [6, 40, 68, 70] has sparked significant research interest in studying their security. Many researchers are now focusing on adversarial example generation [17, 28, 54, 60]. Adversarial examples add subtle perturbations that are imperceptible to the human eye on clean data, causing the model to produce incorrect results. Depending on the access rights to the target model and data, attacks can be divided into black-box attacks [1, 41, 48, 52] and white-box attacks [4, 20, 43, 49]. Most white-box algorithms [20, 33, 43] obtain adversarial examples based on the gradient of the loss function to the inputs by continuously iteratively updating perturbations. In black-box attacks, some methods [57, 61, 76] involve iteratively querying the outputs of the target model to estimate its gradients by training a substitute model, while others [5, 44, 47] concentrate on enhancing the transferability of adversarial examples between different models. Over time, new algorithms for generating attack examples are continually being developed. Therefore, our focus is on addressing the ongoing creation of new attacks while maintaining the model's robustness against them.\n2.2 Adversarial Training\nAdversarial training [20] is a main method to effectively defend against adversarial attacks. This approach involves augmenting the model's training process by incorporating adversarial examples, thus the data distribution learned by the model includes not only clean samples but also adversarial examples. Many adversarial training research mainly focuses on improving training efficiency and model robustness [12, 34, 64, 75]. For example, Zhao [74] uses FGSM instead of PGD during training to reduce training time and enhance efficiency, Dong [13] investigates the correlation between network structure and robustness to develop more robust network modules, Chen [7] uses data enhancement or generative models to alleviate robust overfitting, and Lyu [42] adopts regularization training strategies, such as stopping early to smooth the input loss landscape. Meanwhile, the trade-off between robustness and accuracy has attracted much attention [36, 46, 56]. TRADES [71] utilizes Kullback-Leibler divergence (KL) loss to drive clean and adversarial samples closer in model output, balancing robustness and accuracy. In addition, some studies [53, 72] try to use curriculum learning strategies to improve robustness while reducing the decrease in accuracy on clean samples. Most current adversarial training approaches rely on a single or limited adversarial example generation algorithm to enhance model robustness. However, in real-world scenarios, existing defense methods struggle to address the ongoing emergence of diverse adversarial attacks. Inspired by the continue"}, {"title": "2.3 Contiune Learning", "content": "Continue learning [2, 30, 65] aims at the model being able to continuously learn new data without forgetting past knowledge. Continue learning can be divided into three main categories. One is based on the regularization of model parameters [29, 69] by preserving important parameters from the past while updating less critical ones. However, this method's performance is not ideal when applied to scenarios involving a large number of tasks. One is based on knowledge playback [3, 10, 37, 45, 55], where important past samples are stored in memory and used for training when encountering new tasks. Another approach involves dynamically expanding model parameters [19, 25, 35] to assign different parameters to different tasks. This method helps alleviate catastrophic forgetting and enhances model performance, but it requires significant memory and computational resources. Nevertheless, continue learning models are also susceptible to adversarial examples. Wang et. al. [59] attempt to combine adversarial training with continue learning paradigms, but they do not consider the defensive performance of the current model against old attacks in long-term application scenarios. Differently from previous work, our focus is on ensuring that when the model continues to encounter new adversarial attacks, it can maintain robustness against past adversarial examples and improve resilience against new adversarial attacks."}, {"title": "3 Methodology", "content": "3.1 Task Definition and Framework Overview\nContinuous Defense Setting. As deep learning models become widely used in fields like healthcare, manufacturing, and military, ensuring their security has become a primary focus for researchers. Investigating adversarial examples with high transferability provides valuable insights into deep models. Consequently, there has been a constant influx of diverse new attack methods [17, 28, 54] in recent years. To develop defense algorithms suitable for long-term applications, we have designed a new Continuous Defense Setting (CDS). In the CDS, a model trained on clean data needs to continually cope with and learn from newly generated adversarial examples, meanwhile, due to limited storage resources, it is impractical to retain a vast number of learned samples. Therefore, our research focuses on addressing the challenge of catastrophic forgetting, improving the model's defense against various attacks, and maintaining high accuracy on clean samples.\nFramework Overview. To address the ongoing generation of diverse adversarial samples and tackle model defense challenges in long-term application scenarios, we propose a Sustainable Self-evolution Adversarial Training (SSEAT) algorithm under CDS, containing Continual Adversarial Defense (CAD), Adversarial Data Reply (ADR), and Consistency Regularization Strategy (CRS) three components. As shown in Fig. 2, based on the continue learning paradigm, the CAD employs a min-max adversarial training optimization process to continually learn from new attack samples, as described in Sec. 3.2. To alleviate the catastrophic forgetting issue and boost the model robustness of the diverse attacks, in Sec. 3.3, we"}, {"title": "3.2 Continual Adversarial Defense", "content": "In a classification task, a dataset D consists of n pairs $(x_i, y_i)$, where $x_i$ represents input samples and $y_i$ denotes corresponding class labels ranging from integers 1 to K. The classification model $f_\\theta$ is intended to map the input space X to the output space $\\Delta^{K\u22121}$, generating probability outputs through a softmax layer. To deal with the boom-growing attack strategies, the concept of adversarial robustness extends beyond evaluating the model's performance solely on P. It involves assessing the model's ability to handle perturbed samples within a certain distance metric range around P. Specifically, our goal is to achieve $l_p$ \u2013 robustness, where we aim to train a classifier $f_\\theta$ to accurately classify samples $(x + \\delta, y)$ under any $\\delta$ perturbation such that $|\\delta|_p \u2264 \\epsilon$. Here, $(x, y)$ follows the distribution P, and p \u2265 1 with a small $\\epsilon > 0$.\nThe core concept of adversarial training is to incorporate generated adversarial examples into the training set, allowing the model to learn from these adversarial examples during training, thereby acquiring more robust features and enhancing the model's defense capability. Adversarial training can be formalized as a min-max optimization problem: the goal is to find model parameters $\\theta$ that enable the correct classification of adversarial examples,\n$\\min_\\theta E_{(x,y)\\sim D} \\max_{\\|\\delta\\|_p\\leq\\epsilon} L_{adv} (\\theta,x + \\delta, y)$\nwhere $L_{adv}$ represents the loss function, and we use the standard cross-entropy loss to design the loss function $L_{adv}$.\nIn our practical CDS, the adversarial training model will continue to encounter adversarial examples generated in various ways. Thus, we design a novel Continuous Adversarial Defense (CAD) pipeline, at each stage of CAD, the model is exposed to a batch"}, {"title": "3.3 Adversarial Data Reply", "content": "During the CAD, as more and more attack examples are incorporated into training, the model increasingly struggles to avoid catastrophic forgetting, hindering its ability to maintain sustainable defense capabilities in long-term application scenarios. Thus, we introduce a novel Adversarial Data Reply (ADR) strategy to realize an effective rehearsal sample selection scheme, enhance adversarial example diversity, and obtain high-quality replay data. High-quality sample data should accurately reflect their class attributes and demonstrate clear distinctions from other classes in the feature space. We consider samples located at the distribution center to be the most representative, while those at the classification boundary are the most distinctive. Therefore, based on these two characteristics, we select diverse and representative replay data within the feature space.\nHowever, accurately computing the relative position of samples in the feature space requires significant computational resources and time. Therefore, we utilize our classification model to infer the uncertainty of samples, thereby indirectly revealing their relative positions in the feature space. In practical implementation, we perform various data augmentations to obtain augmented samples. Subsequently, we calculate the variance of the model's output results for these samples subjected to different data augmentations to assess their uncertainty. We think that when the model's predictions for a sample are more certain, the sample may be closer to the core of the class distribution; conversely, when uncertainty in predictions increases, the sample may be closer to the class boundary.\nFirst, we define the learning samples for each round as,\n$D_t =\\begin{cases}D_{init}, t = 0\\\\D_{adv} t > 0\\end{cases}$\nwhere $D_t$ represents the sample set of the tth learning stage in CAD task.\nWe assume that the prior distribution of samples $p (x\\vert x)$ is a uniform mixture of various data augmentations, where x represents the augmented samples generated via color jitter, shear, or cutout. We utilize the Monte Carlo method to approximate the uncertainty"}, {"title": "3.4 Consistency Regularization Strategy", "content": "In practical application models, in addition to achieving sustainable defense against attack examples, it is crucial to maintain high recognition accuracy on original clean samples. To prevent the model's learned data distribution from straying too far from the space of clean sample data, we leverage the knowledge distillation method and propose a novel Consistency Regularization Strategy (CRS), to ensure that the same sample fed into both previous model $f_{\\theta_{t\u22121}}$ and current training model $f_{\\theta_t}$, after undergoing independent data augmentations, still yields similar predictions.\nFor a given training sample $(x, y) \\sim D$ and augmentation $A \\sim A$, the training loss is given by,\n$\\max_{\\|\\delta\\|_\\infty\\leq\\epsilon} L_{CE} (f_{\\theta_t}, (A (x) + \\delta), y)$\nwhere A represents the baseline augmentation set. $f_{\\theta_t}$ represents the model parameters during the tth rounds of CAD.\nConsidering data points $((x, y)$ drawn from distribution D, and augmentations $A_1$ and $A_2$ sampled from set A, we denote the adversarial noise of $A_i (x)$ as $\\delta_i$. It is obtained by $\\delta_i := argmax_{\\|\\delta\\|_p\\leq\\epsilon} L (A_i (x), y, \\delta; \\theta_t)$. Our objective is to regularize the temperature-scaled distribution $f_{\\theta_t} (x; t)$ of adversarial examples across augmentations for consistency. Here, \u03c4 is the temperature hyperparameter. Specifically, we use temperature scaling to adjust the classifier:\n$f_{\\theta_t} (x; \\tau) = Softmax(\\frac{z_{\\theta_t}(x)}{\\tau})$, where $z_{\\theta_t}(x)$ is the logit value of"}, {"title": "4 Experimental", "content": "4.1 Experimental Settings\nDatasets. We evaluate our SSEAT model over the CIFAR-10 and CIFAR-100 dataset [31], which is commonly used for adversarial attack and defense research.Our method only uses 1000 images for training and all 1000 images for testing. In each stage, the training and test data are converted into attack according to the corresponding attack algorithm. The converted training part of the data is used for SSEAT training, and the test part is only used for the final black box test.\nAttack Algorithms. Under the CDS task, we use various attack algorithms, i.e., FGSM [20], BIM [32], PGD [43], RFGSM [58], MIM [14], NIM [38], SIM [38], DIM [67],VMIM [62], ZOO [8] and RLB-MI [21] to generate adversarial samples under l\u221e for training our SSEAT model and further evaluate the robustness against the attacks. The perturbation amplitude of all attacks is set to \u20ac = 8/255 and the attack step size to a = 2/255.\nImplementation Details. We summarize the training procedure of our SSEAT framework in Alg. 1. The model uses resnet18 [22] as the classification network structure. We implement Torchattacks [27] to generate 100 adversarial images per category for each adversarial attack strategy, for a total of 1000 images. On the CIFAR-10 and CIFAR-100 dataset, before training, each image is resized to 32\u00d732 pixels, and underwent data augmentation, which included horizontal flipping and random cropping. For the training hyperparameters of experiments, we use a batch size of 8, the epoch for training clean samples is set to 40, and the epoch for training adversarial samples is set to 20. We train the model using the SGD optimizer with momentum 0.9 and weight decay 5 \u00d7 10-4. In addition, we set the memory buffer size to 1000.\nCompetitors. We compare our SSEAT method with several adversarial training works, such as the PGD-AT [43], TRADES[71], MART[63], AWP [66], RNA [13], LBGAT [11] and RIFT[75].\nEvaluation Metrics. To comprehensively assess the model's performance in both robustness and classification within the CDS task, we employ two indicators: (1) The model's classification accuracy across all kinds of adversarial examples after completing all adversarial training stages, which is more practical and differs from the 'classification accuracy against each attack after every adaptation step' [59]; (2) The model's classification accuracy on the original clean data after completing all adversarial training stages."}, {"title": "4.2 Experimental Results", "content": "We have conducted extensive experiments over the CIFAR-10 and CIFAR-100 datasets with various attack orders in the CDS task, compared to several competitors and baseline. All attack results are reported under the black-box condition.\nFor the CDS task, our SSEAT model can achieve the best robustness against ongoing generated new adversarial examples. To better evaluate our SSEAT model under the CDS task, We report model robustness over various attack orders compared to several competitors over two datasets. As shown in Tab. 1, Tab. 2, and Tab. 3, our SSEAT method can beat all adversarial training competitors for all adversarial examples, which demonstrate the efficacy of SSEAR framework to tackle the continuous new attacks under black-box condition.\nFor the CDS task, our SSEAT model can maintain competitive classification accuracy over the clean data. For real-life applications, in addition to continuously improving the defense performance against new attack methods, the model also needs to have good recognition effects on clean samples. Thus, we report the classification accuracy over original data in Tab. 1, Tab. 2, and Tab. 3. The results show our SSEAT method not only achieves strong robustness under complex changes, but also has good performance on the original task. our SSEAT successfully defends against different attacks while maintaining good performance on clean samples. It clearly reveals the advantage of SSEAT in terms of generalization. Achieving a balanced trade-off between robustness and accuracy greatly enhances the practicality of the adversarial training strategy in real scenarios.\nFor the CDS task, our SSEAT can realize the best defense results on unknown attacks. Without loss of generality, we use the SSEAT model, trained on CIFAR-10. Interestingly, our SSEAT outperforms other competitors on all unknown attack defenses as"}, {"title": "4.3 Ablation Study", "content": "To verify the role of each module in our SSEAT, we conducted extensive ablation studies on the following variants: (1) 'Baseline (CAD)': conduct the experiments over the continue learning pipeline; (2) '+CRS': based on 'Baseline", "+ADR": "based on the \"baseline\", add ADR for selecting key rehearsal data into the memory buffer; (4)'+ADR+CRS (Ours)': based on the 'baseline", "Baseline (CAD)": "ased on the basic continue learning pipeline, can only realize learning from continuous attacks, however, the performance on clean samples is too poor to meet the actual application scenarios; (2) Comparing the results between '+Baseline (CAD)' and '+CRS', obviously, we can notice the classification accuracy over the clean samples can be obviously improved. CRS can better handle the trade-off between model robustness and accuracy; (3) Comparing '+Baseline (CAD)' and 'ADR', such results illustrate that our sampling strategy is effective, and can select representative and diverse samples at each training stage; (4) Comparing the '+CRS', '+ADR', and '+ADR+CRS (Ours)', the results clearly demonstrate the ADR and CRS modules can complement each other on both classification accuracy and defense robustness.\nEvaluation of different attack strengths. we adopt multiple attack budgets and attack iterations to simulate different attack"}, {"title": "5 Conclusion", "content": "In the real world, data is often subject to noise and disturbances, necessitating models with strong robustness and adaptability. To realize the adaptive defense ability of deep models with adversarial training when facing the continuously generated diverse new adversarial samples, we proposed a novel and task-driven Sustainable Self-evolution Adversarial Training (SSEAT) method. The SSEAT framework we propose aims to achieve autonomous model evolution and defense against various adversarial attacks, enabling models to effectively adapt to complex and changing environments, thereby enhancing their reliability and stability in multi-media practical applications. Inspired by the continue learning, the SSEAT framework can continuously learn new kinds of adversarial examples in each training stage, and realize the consolidation of old knowledge through the data rehearsal strategy of high-quality data selection. At the same time, a knowledge distillation strategy is used to further maintain model classification accuracy on clean samples when facing multiple adversarial attacks. We have verified the SSEAT model efficacy over multiple continue defense setting orders, and the ablation experiments show the effectiveness of the components in the SSEAT method."}]}