{"title": "Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling", "authors": ["Xinyue Fang", "Zhen Huang", "Zhiliang Tian", "Minghui Fang", "Ziyi Pan", "Quntian Fang", "Zhihua Wen", "Hengyue Pan", "Dongsheng Li"], "abstract": "LLMs obtain remarkable performance but suffer from hallucinations. Most research on detecting hallucination focuses on the questions with short and concrete correct answers that are easy to check the faithfulness. Hallucination detections for text generation with open-ended answers are more challenging. Some researchers use external knowledge to detect hallucinations in generated texts, but external resources for specific scenarios are hard to access. Recent studies on detecting hallucinations in long text without external resources conduct consistency comparison among multiple sampled outputs. To handle long texts, researchers split long texts into multiple facts and individually compare the consistency of each pairs of facts. However, these methods (1) hardly achieve alignment among multiple facts; (2) overlook dependencies between multiple contextual facts. In this paper, we propose a graph-based context-aware (GCA) hallucination detection for text generations, which aligns knowledge facts and considers the dependencies between contextual knowledge triples in consistency comparison. Particularly, to align multiple facts, we conduct a triple-oriented response segmentation to extract multiple knowledge triples. To model dependencies among contextual knowledge triple (facts), we construct contextual triple into a graph and enhance triples' interactions via message passing and aggregating via RGCN. To avoid the omission of knowledge triples in long text, we conduct a LLM-based reverse verification via reconstructing the knowledge triples. Experiments show that our model enhances hallucination detection and excels all baselines.", "sections": [{"title": "Introduction", "content": "Recent research showed that large language models (LLMs) achieved state-of-the-art performance in various NLP tasks (Qin et al. 2023; Fang et al. 2024). However, these models often suffer from hallucinations: generate incorrect or fabricated content in a factual way, which undermines models' credibility (Ji et al. 2023) and limits LLMs' application in fields requiring factual accuracy (Huang et al. 2023). Detecting hallucination in model responses is crucial for LLMs' boom.\nExisting hallucination detection studies primarily focus on tasks (e.g. question answering (QA) (Zhang et al. 2023) and arithmetic calculation (Xue et al. 2023)), where the questions have short and concrete correct answers. In these tasks, the consistency among their concrete answers can be easily checked for hallucination (Jiang et al. 2021). The research on detecting hallucination on generated long text is more challenging because (1) generating text is open-ended and seldom has concrete answers; and (2) the long text contains multiple facts and the consistency among multiple facts is hard to verify. Some studies use external knowledge to verify the factuality of outputted responses (Mishra et al. 2024; Li et al. 2024a; Sadat et al. 2023), but high-quality external resources for specific tasks are expensive and hard to access (Luo, Xiao, and Ma 2023). Therefore, it is valuable to investigate hallucination detection for long text generation without external resources (i.e. zero-resource setting). Hence, in this paper, we aim to explore hallucination detection for long text generation with black-box models (powerful LLMs, like GPT-4) in a zero-resource setting.\nCurrently, the studies on black-box zero-resource hallucination detection for text generation can be divided into two categories: (1) Self-checking (Friel and Sanyal 2023; Liu et al. 2023) designs prompt texts using chain-of-thought (CoT) to check whether models' responses contains hallucinations by the LLMs' own capabilities. Though that can be easily implemented and simply applied in various scenarios, it relies on the model's own ability and thus probably leads to overconfidence: The model tends to believe that its own responses are correct, leading to missed detection. (2) Consistency comparison (Zhang et al. 2023; Ma, Dai, and Sui 2024) samples multiple responses to check whether the sampled responses are highly inconsistent, which indicates confusion (hallucination) about the facts in responses (Farquhar et al. 2024). The method is effective when the responses are very short with few concrete claims, which are easy to compare consistency in the sampled responses. In addition, to detect hallucinations in long response texts, multiple responses have various wording or lexical representations about a same knowledge fact or term (Donald Trump vs Donald John Trump), which results in the difficulty of comparing facts claimed in multiple responses.\nTo address those issues in hallucination detection for long texts, researchers proposed divide-and-conquer (Zhang et al. 2024) based on consistency comparison. It has three steps: (1) sample multiple additional responses appending to the original response to the user's query; (2) divide each"}, {"title": "Method", "content": "Our method consists of three modules: (1) Triple-Oriented Response Segmentation (Sec. 3.2) extracts knowledge facts from the model output responses. (2) Graph-based Contextual Consistency Comparison with RGCN (Sec. 3.3) constructs a graph carrying the extracted knowledge triples and utilizes an RGCN (relational graph convolutional network) (Schlichtkrull et al. 2018) to propagate and integrate messages across the graph. This method considers the dependencies between each knowledge triple (fact) and its surrounding triples during detection. (3) Reverse Verification via Triples Reconstruction (Sec. 3.4) achieves reverse verification for hallucination detection by reconstructing each triple via three LLM-based tasks.\nWe feed each knowledge triple extracted (Sec. 3.2) to detect hallucinations (Sec. 3.3 and Sec. 3.4), and then we judge the original long text response relying on the results of each triple from (Sec. 3.3 and Sec. 3.4)."}, {"title": "Triple-Oriented Response Segmentation", "content": "To better align knowledge facts in the consistency comparison, we propose to segment the responses by extracting knowledge triples as knowledge facts and checking the answers' consistency among the triples. Our motivation is that due to the impact of wording, comparing textual consistency can lead to mismatches. Because hallucination detection considers the semantics of knowledge fact instead of specific word choices, we use a triple-based comparison method to provide better alignment than traditional textual comparison. Specifically, the steps are as follows:\n\u2022 Extraction. Inspired by the latest method (Hu et al. 2024), we design prompts to extract knowledge triples from responses using an LLM.\n\u2022 Verification. To ensure the accuracy of the extracted knowledge triples, we pair each response with its triples and prompt the LLM to confirm their semantic equivalence. If any ambiguities exist between the extracted triples and the response text, we instruct the LLM to adjust the semantics of the triples according to the text's semantics. The details of the prompts are in App.A.\nKnowledge triples have a structured format and are easy to compare, simplifying alignment and comparing consistency between responses, enhancing detection accuracy."}, {"title": "Graph-based Contextual Consistency Comparison with RGCN", "content": "To effectively consider dependencies between triples, we propose Graph-based Contextual Consistency Comparison (GCCC), which constructs a knowledge graph for each response and then conducts message passing and aggregation via RGCN. The intuition is that traditional consistency comparison focuses on comparing individual knowledge facts: it verifies a single piece of knowledge fact by comparing it only with the corresponding fact in the sampled responses at a time. It results in ignoring the triples that are mutually dependent on the given triple within the context information. Therefore, we propose to construct graphs considering both the original response and the sampled responses. We employ RGCN for message passing and aggregation on these graphs. We divide the process into two stages: (1) knowledge triple modeling via graph learning. We build a graph for each response, and then we obtain node (entity) embeddings via RGCN processing to model the multiple knowledge triples for a response. (2) triples consistency comparison. We compare the consistency of triples across the graphs at the embedding level."}, {"title": "Knowledge Triple Modeling via Graph Learning", "content": "This stage is divided into three steps: firstly, we convert each response into a graph. Then, we obtain the initial node (entity) embeddings for each graph using sentence-BERT (Wang et al. 2020). Finally, we employ the RGCN to perform message passing and aggregation using the initial node embeddings on each graph, updating the node embeddings."}, {"title": "Graph Construction", "content": "For a user's query, the model generates an original response Ro, and we sample multiple additional responses Rsampled = {R1,R2,...,Rn}. (hi, ri, ti) is a single triple in Ro and (hi,j, ri,j,ti,j) is a single triple in j-th sampled response Rj. We construct the graph Go = (Vo, Eo) for the original response, in which vertices (v \u2208 V) represent the head and tail entities from each triple. An edge (e \u2208 Eo) represents the relation between the head entity and the tail entity. Similarly, we construct the graph Gj = (Vj, Ej) for each sampled response Rj. By doing so, we construct several response-specific graphs for each user's query."}, {"title": "Representation Initialization", "content": "Using sentence-BERT, we encode the head entities, relation, and tail entities in knowledge triples as vector representations. For the original response, we represent each triple embedding as: (hi, ri,ti) = BERT(hi, ri, ti). For each sampled response, we represent each triple embedding as: (hi,j,ri,j,ti,j) = BERT(hi,j,ri,j,ti,j). We treat the head and tail entity embedding from the original response as Go's initial node (entity) embeddings. Similarly, we obtain the initial node (entity) embeddings for the graph Gj corresponding to j-th sampled response."}, {"title": "Message Passing and Aggregation", "content": "We use the RGCN to perform message passing and aggregation on the graph. As Eq.1 shows that for each layer l, the new representation of each node v is denoted as $e_v^{(l+1)}$. For each relation r \u2208 R, we denote the set of all neighbors of the node v that are connected through an edge of relation r as $N_r(v)$. For each neighbor in N_r(v), we multiply its representation $e_u^{(l)}$ by a weight matrix $W_r^{(l)}$ and normalize it using the hyperparameter $C_{v,r}$. In addition to aggregating information from neighbors, $e_v^{(l+1)}$ also includes its own representation $e_v^{(l)}$ from the previous layer 1 and transform it by a weight matrix $W_0^{(l)}$.\n$e_v^{(l+1)} = \\sigma\\left(\\sum_{r \\in R} \\frac{1}{C_{v,r}} \\sum_{u \\in N_r(v)} W_r^{(l)} e_u^{(l)} + W_0^{(l)} e_v^{(l)}\\right)$                                                                  (1)\n The updating for $e_v^{(l+1)}$ integrates information from v's neighbors through relation-specific change, while also incorporating v's own representation. These operations ensure that the updated node embedding is informed by both its context and its intrinsic properties. Triples containing the node can also incorporate contextual information, enhancing the accuracy when comparing the consistency of triples, thereby improving the detection of hallucinations."}, {"title": "Triples Consistency Comparison", "content": "Based on the graph representations from RGCN, we detect hallucinations in the original response by comparing the consistency of triples across multiple graphs. Firstly, we align triples between the original response's graph G\u3002and each sampled graph Gj. Then we compare the consistency of the aligned triples to calculate the consistency score."}, {"title": "Triples Alignment", "content": "For each triple (hi, ri, ti) in the original response and each triple (hi,j, ri,j, ti,j) in the sampled response, we first check whether the head entities of these two triples are the same. If so, we calculate the similarity S(ri, ri,j) between the relation representation ri of relation ri and the representation ri,j of ri,j. If S(ri, ri,j) exceeds the pre-defined threshold \u03b8r, we regard the two triples as aligned. Otherwise, they are considered unaligned. For every triple in the original response, we apply the above operations to align each triple from every sampled response with it."}, {"title": "Consistency score calculation", "content": "After aligning the triples, we need to further compare whether they are consistent with each other to calculate the consistency score. Specifically, as Eq. 2 shows, for a triple (hi, ri, ti) in the original response and its aligned triple $(h_{i,j}, r_{i,j}, t_{i,j})^a$ in j-th sampled response, $e^t_i$ and $e^t_{i,j}$ are the node embeddings of the tail entity ti and $t_{i,j}$ after RGCN processing. We compute the similarity between $e^t_i$ and $e^t_{i,j}$. If their similarity S($e^t_i$, $e^t_{i,j}$) exceeds the threshold \u03b8t, we increase the consistency score $C_{i,j}$ of (hi,ri,ti) by 1. This indicates that there is a triple consistent with the triple (hi, ri, ti) in j-th sampled response.\nConversely, we use $e^m_{t_{i,j}}$ to denote the node embedding of the tail entity in the unaligned triple $(h_{i,j}, r_{i,j}, t_{i,j})^m$ in the j-th sampled response. If the similarity between $e^t_i$ and $e^m_{t_{i,j}}$ exceeds the threshold \u03b8t, we updated the consistency score $C_{i,j}$ of (hi,ri,ti) by subtracting 1. This is because if two triples share the same head entity and different relationships, their tail entity should typically be dissimilar.\n$C_{i,j} = \\begin{cases} C_{i,j} + 1 & \\text{if } S(e^t_i, e^t_{i,j}) > \\theta_t\\\\ C_{i,j} - 1 & \\text{if } S(e^t_i, e^m_{t_{i,j}}) > \\theta_t\\\\ C_{i,j} & \\text{otherwise} \\end{cases}$                      (2)\nTo obtain the final consistency score for each triple in the response, we sum its comparison results with each sampled response as Ci = \u2211j=1 (Ci,j)\nDuring the message passing and aggregation process with RGCN on a graph, each node integrates features from its neighboring nodes. This allows triples containing the node to aggregate contextual information from surrounding triples, Considering the dependencies between the triple to be verified and the surrounding triples."}, {"title": "Reverse Verification via Triple Reconstruction", "content": "To address the omission issue mentioned in Sec. 3.3, we propose a LLM-based reverse verification method (RVF), which contains three reconstruction tasks that check whether LLM can reconstruct the knowledge triples' head entity, relation, and tail entity, respectively. Traditional reverse strategies prompt the LLMs to reconstruct questions to verify each knowledge fact from generated responses. The reconstructed question may have multiple correct answers, which leads to a low probability of answering the knowledge facts that we aim to verify. It increases the chance of misjudging these knowledge facts. To address this, we add constraints to the reconstructed questions to reduce the space of correct answers and increase the probability of answering the triples we want to verify. The three tasks are as follows:."}, {"title": "Head Entity with Question Answering Task (HEQA)", "content": "We prompt LLMs to reconstruct a question for each triple, with the head entity as the expected answer, and then obtain the model's responses. We check if these responses are consistent with the head entity. Specifically, to reduce the space of correct answers for reconstructed questions, we first follow the method from (Manakul, Liusie, and Gales 2023) to initially verify the triples in the original responses. Then, we filter out a set of triples ft with high factual accuracy. For each triple (hi, ri,ti) in the original response, we add ft (excluding (hi, ri, ti) if it exists in ft ) as constraints in the questions during the LLM reconstruction process. The model's responses to the question must satisfy these constraints. We repeatedly prompt the LLM to generate answers A to the question. The total number of A denoted as NA. We count the times that the model responses match the head entity hi (denoted as Nh) and calculate the fact score Sh as the ratio of Nh to NA, where $S_h = \\frac{N_h}{N_A}$."}, {"title": "Relation Regeneration Task (RR)", "content": "We mask the relation in the triple with a special token and prompt the model to predict multiple times. Then we check whether the model's predictions are identical to the relation for measuring the consistency. It can reduce the space of correct answers because the relationship between two entities is limited. Specifically, for each triple (hi, ri, ti), we mask ri with a special token and prompt the LLM for multiple times to predict the original ri given hi and ti. We define the fact score Sr as the proportion of the predicted relations that match the original relation ri, where $S_r = \\frac{N_c}{N_p}$. Here, Ne is the number of matched predictions, and Np is the total number of predictions."}, {"title": "Fact Triple Selection based on Tail Entity Task (FT-STE)", "content": "Models often generate long texts centered around a few key entities, which typically serve as the head entities in extracted triples. The limited number of head entities allows us to use surrounding context related to the head entity as constraints to effectively narrow down the space of correct answers for reconstructed questions. However, tail entities in long-text responses are more diverse, so we cannot directly use surrounding context as constraints for the tail entities in reconstructed answers. Instead, we use a more direct approach by providing a list of options to limit the space of correct answers. We instruct the model to select the factual triple from it; and then, we compare whether the model's selections are consistent with the original triple. It reduces the space of correct answers by providing a limited set of choices. Specifically, for each triple (hi, ri, ti), we replace ti with other entities of the same type to generate multiple similar triplets; and then, we prompt the LLM to choose the factual one. We define the fact score St as the proportion of times (hi, ri, ti) is selected, where $S_t = \\frac{N_t}{N_s}$. Nt is the number of times (hi, ri, ti) is selected, and Ns is the total number of selections. See the prompt templates used in the above three tasks in App.B.\nFinally, we sum up the fact scores from these three tasks and the consistency score mentioned in Sec. 3.3 with different weights to make a judgment about each triple in the original response, as shown in Eq. 3\n$F(h_i, r_i, t_i) = W_1 \\cdot S_h + W_2 \\cdot S_r + W_3 \\cdot S_t + W_4.C_i$                        (3)\nIn our proposed reverse detection method, the three tasks use different strategies to reduce the space of correct answers in the reconstructed questions. It avoids the issue in traditional reverse detection techniques where the reconstructed"}, {"title": "Experimental Setting", "content": "Datasets. We utilized three datasets: (1) PHD: The dataset consists of 300 samples. Each sample is a Wikipedia article about an entity generated by ChatGPT (gpt-3.5-turbo) and annotated by human annotators. (2) WikiBio: The dataset consists of 238 passages generated by GPT3 and annotated at the sentence level. Although it lacks passage-level labels, we follow (Yang et al., 2023) to aggregate sentence labels to derive pseudo-labels at the passage level. (3) sub-WikiBio: There are only 12 fact samples in WikiBio dataset. The sample distribution is too imbalanced. Therefore, we extracted all 12 fact samples and 48 randomly selected hallucination samples to create a subset. In our experiment, we refer to this subset as the WikiBio subset.\nImplemention Details. We use the recorded responses for each sample as original responses and generate 10 additional sampled responses using ChatGPT. we set the generation temperature to 1.0 to ensure the randomness of sampled responses. We use GPT-4 (gpt-4-preview-1106) to extract triple knowledge from responses and reconstruct questions in reverse verification. At this point, we set the temperature to 0.0 to maximize the reproducibility of the result."}, {"title": "Baselines", "content": "We compare our method against four baselines: (1) Reverse Validation via QG (RVQG). is a method that uses LLMs to reconstruct a question about the text to be verified. It compares if the model's response to the reconstructed question is consistent with the text (2) Semantic Entropy (SE). breaks down the entire response into factual claims and prompts LLMs to reconstruct questions about it. For each claim, they repeatedly ask the LLM reconstructed questions. And then cluster the claim and the model's responses. They measure the entropy of the cluster containing the claim to assess its validity. (3) SelfCheckGPT via BERTScore (SelfCk-BS). is a variant of SelfCheckGPT, using BERTScore to measure consistency between original response and sampled responses. (4) SelfCheckGPT via NLI (SelfCk-NLI). is another variant of SelfCheckGPT that uses an NLI model to measure consistency between the original response and the sampled responses. However, SE, SelfCk-BS, and SelfCk-NLI all return the likelihood scores of a sample being a hallucination, rather than labels indicating fact or hallucination. To align these methods with our task, we set thresholds for these baselines on different datasets using the same approach as for our method. If a sample score exceeds the threshold, we classify it as a hallucination. Details are in App.C."}, {"title": "Evaluation Metrics", "content": "We evaluate how well the method detects hallucinatory responses using metrics: (1) F1 is the harmonic mean of precision and recall, providing a comprehensive evaluation of the classification performance of the method; (2) Accuracy is the proportion of correctly classified samples out of the total number of samples."}, {"title": "Overall Performance", "content": "We analyze the effectiveness of our method by comparing it with four baselines, results shown in Tab.5. Our method outperforms the baselines on all metric values. SelfCk-NLI directly uses an NLI model to assess if any sentence in the original response contradicts the sampled responses and performs the worst on all metrics. SelfCk-NLI does not perform as well as SelfCk-BS, suggesting that NLI models have limited ability to compare consistency between texts. It is even less effective than assessing via simple embedding similarity measures. Reverse validation methods (RVQG and SE) perform worse than our method on all metrics. We attribute this to our use of a graph-based consistency comparison method (Sec. 3.3), which considers dependencies between triples during comparison."}, {"title": "Ablation study", "content": "We conduct an ablation study to verify the importance of each component as shown in Tab 6. RVF means abandoning the reverse validation from our full model. The performance drop across most datasets indicates that RVF effectively addresses the omission issues in GCCC to improve the overall effectiveness. However, the performance did not drop on the WikiBio. The reason is that WikiBio contains many hallucination samples (95%), causing our method, baselines, and their variants to show a bias towards predicting hallucinations in this dataset. In these abnormal conditions, the RVF module does not perform effectively, as its advantage lies in correctly identifying hallucination samples. With a more balanced sample distribution in the dataset, our full model performs better than"}, {"title": "Analysis on Contextual Integration in GCCC", "content": "To verify that our graph-based method effectively aggregates node information, we design an experiment to compare two scenarios: (S1) using RGCN for message passing and aggregation on the graph; (S2) without RGCN, examining the similarity between nodes and their surrounding neighbors. Specifically, we conduct two experiments as follows."}, {"title": "t-SNE Visualization of Node Representation Distribution", "content": "The first experiment uses t-SNE dimensionality reduction to project the node representations from both scenarios into a two-dimensional space to observe the distribution. Fig.2 shows that in both the PHD and the WikiBio, the node distribution in the (S1) (red nodes) is more compact compared to the (S2) (blue nodes). This indicates that after using RGCN, the node representations become more similar to those of their neighbors. RGCN effectively integrates the features of neighboring nodes into each node's representation, blending information for every node."}, {"title": "Quantitative Analysis of Node Representation Similarity", "content": "We perform a quantitative analysis by obtaining the cosine similarity of nodes representations under both (S1) and (S2). Tab.3 shows that the representations' similarity between two nodes is significantly higher after processing with RGCN compared to without RGCN. This also indicates that our method integrates contextual information for each node by using RGCN."}, {"title": "Triple Dependencies Error Detection", "content": "We designed an experiment to verify that our method can also detect errors in triple dependencies. Specifically, we create a new dataset, TripleCom, by selecting samples with errors in triple dependencies extracted from the PHD and WikiBio datasets. The proportion of such error is approximately 10.5% in these datasets. Then we test GCA and four baselines on this dataset, with implementation details matching those in Sec.4.1.2. Our method GCA achieves the best performance on all metrics in the TripleCom dataset, demonstrating its effectiveness in detecting errors in the dependencies between multiple triplets."}, {"title": "Conclusion", "content": "In this paper, we propose a graph-based context-aware hallucination detection method on long-text generation, where our method follows a zero-resource setting and use only black-box LLMs. Our method extracts knowledge triples from output responses for a better alignment. We then construct a graph to carry contextual information so that considers dependencies between knowledge triples in the response. It indeed addresses the issue of ignoring the contextual information in existing methods that only focus on individual knowledge facts. We construct three reconstruction tasks for reverse verification to verify the knowledge triples. Experiments show that our method excels in detecting hallucinations, outperforming all baseline methods."}, {"title": "A Details of Triple Extraction and Verification Prompt Templates", "content": "Table 5 shows the prompts for extracting and verifying triplets mentioned in Sec. 3.2. For extracting triplets, the prompt uses a few-shot setup by adding two examples in the context. In contrast, the prompt for verifying the extracted triplets does not include any examples."}, {"title": "B Details of Reverse Verification via Triple Reconstruction Prompt Templates", "content": "Table 6, Table 7, and Table 8 show the prompts used in the reverse verification strategy mentioned in Sec. 3.4. Table 6 specifically shows the prompts used in the head entity with question answering task, divided into prompts for generating reconstructed questions and prompts for answering those reconstructed questions. Table 7 shows the prompts used in the relation regeneration task. It includes prompts for the model to predict the masked relation in a triple (It uses a one-shot setup.) and prompts to compare if the model's new prediction matches the masked relation. Table 8 shows the prompts used in the fact triple selection based on tail entity task. It includes prompts for the model to replace the tail entity in a triple with another entity from the same category, and prompts for the model to select the factually correct triple from multiple alternatives. Both prompts use a one-shot setup."}, {"title": "C Method of Setting Thresholds", "content": "As mentioned in Sec. 4.1.3, SE, SelfCk-BS, and SelfCk-NLI all return likelihood scores indicating the chance that a sample is a hallucination, rather than labels that specify whether it is a fact or a hallucination. To align with our task, we set thresholds for these three methods in a similar way to our approach. The only difference is that our method returns likelihood scores for a sample being a fact. Specifically, we set the threshold by calculating the mean \u00b5 and variance 62 of the hallucination likelihood scores for all samples obtained by the method in the specific data sets, and then selecting the value from \u00b5 to \u03bc + 30 that maximizes the final metrics as the threshold for the method in the specific data sets."}, {"title": "D Similarity of Representations Between Nodes", "content": "Table 9 presents the full version of results showing the similarity in node distribution in graphs created from two datasets, both with and without the use of RGCN. The similarity between the representations of two nodes is significantly higher after processing with RGCN compared to without RGCN. This suggests that our method incorporates contextual information for each node by utilizing RGCN."}, {"title": "E Significance Test", "content": "To assess whether our method significantly improves performance over the baseline on three datasets, we compare our results to the best-performing baseline, RVQG, using a paired t-test. We find that the difference in F1 scores between our method and RVQG is significant (p-value = 0.049), indicating a notable improvement in F1 score with our method. Additionally, we observe a significant difference in accuracy (p-value = 0.0073), further confirming that our method performs better than RVQG in terms of accuracy. These results demonstrate the effectiveness of our method for detecting hallucinations in long text responses generated by black-box models in zero-resource scenarios."}, {"title": "F Case Study", "content": "In Table 10, we compare the results of detecting hallucinations in long text responses with other methods. For this response, RVQG and SelfCK-NLI do not identify any factual inaccuracies. SE and SelfCK-BS could detect some hallucinations but missed some of the errors. For example, they overlook the hallucination that needs to consider the dependencies between multiple knowledge facts: \"He was also the first to be born in the French language, as his father had adopted the language and culture of his adopted country\" (the fact is that Honor\u00e9 IV's native language was French because he was born in Paris, not because his father adopted the language and culture of the country). Conversely, GCA provides a comprehensive and accurate detection result, identifying all factual inaccuracies, since it considers the dependencies between each triple in the response and thoroughly checks each one."}]}