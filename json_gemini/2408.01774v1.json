{"title": "STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios", "authors": ["Dongyang Xu", "Yiran Luo", "Tianle Lu", "Qingfan Wang", "Qing Zhou", "Bingbing Nie"], "abstract": "Accurate behavior prediction for vehicles is essential but challenging for autonomous driving. Most existing studies show satisfying performance under regular scenarios, but most neglected safety-critical scenarios. In this study, a spatio-temporal dual-encoder network named STDA for safety-critical scenarios was developed. Considering the exceptional capabilities of human drivers in terms of situational awareness and comprehending risks, driver attention was incorporated into STDA to facilitate swift identification of the critical regions, which is expected to improve both performance and interpretability. STDA contains four parts: the driver attention prediction module, which predicts driver attention; the fusion module designed to fuse the features between driver attention and raw images; the temporary encoder module used to enhance the capability to interpret dynamic scenes; and the behavior prediction module to predict the behavior. The experiment data are used to train and validate the model. The results show that STDA improves the G-mean from 0.659 to 0.719 when incorporating driver attention and adopting a temporal encoder module. In addition, extensive experimentation has been conducted to validate that the proposed module exhibits robust generalization capabilities and can be seamlessly integrated into other mainstream models.", "sections": [{"title": "I. INTRODUCTION", "content": "In autonomous and semi-autonomous vehicles, the ability to understand and predict driver behavior plays a critical role in ensuring the safety and efficiency of road transportation [1, 2]. Recent advancements in machine learning and computer vision have paved the way for significant improvements in this area [3, 4]. However, the dynamic and complex nature of the driving environment introduces unique challenges, particularly in safety-critical scenarios where the timely and accurate prediction of driver actions can make the difference between safety and hazard [5, 6].\nIncorporating driver attention into predictive models represents a significant departure from conventional approaches. Skilled drivers have the ability to detect and foresee potential traffic hazards rapidly. Some studies indicate that driver attention (DA) is a crucial risk indicator, proven to accurately predict driving patterns or vehicular movement, which are vital components in the decision-making process, especially under safety-critical conditions [7-10]. Moreover, both naturalistic driving assessments and controlled laboratory simulations consistently confirm the crucial role of DA in identifying objects that could lead to conflicts [11, 12].\nTraditional models often depend solely on external environmental factors and historical driver data, overlooking the crucial element of where and how drivers focus their attention in varying situations [13, 14]. To bridge this gap, we introduce the STDA (Fig. 1): Spatio-Temporal dual-encoder network incorporating Driver Attention, by integrating driver attention into the driver behavior prediction model, thus offering a more holistic view of the driver's state and intentions. Furthermore, we designed a temporal encoding module to enhance the ability of the model to comprehend temporal dynamics. By merging spatial information from the driving environment with temporal patterns of driver behavior and attention, STDA seeks to offer a comprehensive framework for predicting driver actions more accurately and swiftly than before.\nThe contributions of this study are summarized as follows:\n1) We developed a driver behavior model named STDA, tailored explicitly for safety-critical scenarios. It weaves together spatial and temporal encodings, yielding more precise predictions of driver behaviors.\n2) We integrated driver attention into the STDA to better align the model with human cognitive processes and enhance its interpretability. Moreover, we explored methods for effectively fusing driver attention data with input images.\n3) We conducted a comprehensive suite of experiments and ablation studies to demonstrate its superior efficacy and ability to maintain computational expeditiousness.\nThe structure of this paper is as follows: in Section II, we detail the previous relevant studies focused on driver attention prediction and driver behavior prediction under safety-critical scenarios. Subsequently, Section III elaborates on the formulation of the STDA with four critical parts: the driver attention prediction module, the fusion module, the temporary encoder module, and the behavior prediction module. The results and analysis of the experiments are presented in Section IV, followed by a discussion and conclusion in Section V."}, {"title": "II. RELATED WORKS", "content": "The field of driver attention prediction has seen significant advancements in recent years. [15] employs U-Net as the backbone and incorporates the Swin-transformer to predict DA. [16] merges a transformer with a convolution network and then utilizes a Conv-LSTM to process the features for DA prediction. [17] utilizes a Convolutional Long Short-Term Memory (Conv-LSTM) network to capture temporal characteristics and employs a pyramid-dilated convolution approach to extract spatial attributes. It then leverages an attention mechanism to combine these temporal and spatial features, using the fused features to predict DA. Recently, inverse reinforcement learning (IRL) has represented a significant advancement in imitating driver attention, particularly in scenarios involving imminent rear-end collisions. This approach utilizes rich visual inputs, such as semantic cues, depth perception, and road lane information, extracted through pre-trained convolutional neural network (CNN)-based models. Although including detailed visual information enhances the model's performance, it also introduces a higher computational complexity than traditional end-to-end CNN architectures [18]. However, these models still encounter limitations in terms of downstream expansion.\nDriver behavior prediction is an essential aspect of intelligent transportation systems. Various methodologies for predicting driver behavior have been adopted, ranging from traditional statistical models to advanced machine-learning techniques. These models are broadly categorized into two distinct types: model-driven methods and data-driven methods. Each category signifies a unique approach to understanding and forecasting driving behaviors, leveraging various methodologies and theoretical foundations to enhance prediction accuracy and reliability across diverse driving scenarios. [19] develops a state space framework that integrates a Markov chain to accurately predict the likelihood of vehicles either shifting to a different lane or maintaining their current path. [20] develops mathematical models that utilize kinematic characteristics, such as velocity and steering wheel movement, to predict the likelihood of a lane change. However, model-based methods suffer from poor generalization and present difficulties in parameter calibration. For data-driven methods, [21] introduces a sophisticated hierarchical reinforcement learning framework to optimize making and executing lane change decisions. [22] leverages the RNN-LSTM architecture to create a time-series model for analyzing driving behaviors, utilizing vision-based signals to discern intentions for lane changes. However, the methods above do not consider driver attention and are not oriented towards dangerous scenarios."}, {"title": "III. METHODOLOGY", "content": "In this study, we proposed a spatio-temporal dual-encoder network incorporating driver attention (STDA) for safety-critical scenarios (Fig. 1), which is aligned with the hazard perception mechanism of human drivers. STDA takes first-person image streams as input and outputs driver behaviors. Given that drivers make comprehensive decisions based on historical data. To improve the performance of the model, images from historical periods were used as inputs. Specifically, the traffic frame sequence $S_t = {F_{t-T}, ..., F_t}$ was sampled from the image streams, where $F_t$ represents the traffic frame to which the driver responded in safety-critical scenarios. T is the length of historical frames. Each of the frames in a sequence was fed into the DA prediction models, which predict visually prominent regions that stand out from their surrounding backgrounds. The feature fusion module, which combines the predicted DA areas with the original first-person image streams, named the DA-integrating feature. A temporal encoding network processes DA-integrating features to obtain temporal features. A CNN-based deep learning model was adopted to extract high-order features, and an MLP was utilized to predict driver behaviors. Subsequent sections will introduce the driver attention prediction module, the fusion module, the temporary encoder module, and the driver behavior prediction module.\nPredicting where a driver is looking can help an autonomous vehicle better understand traffic situations, much like a human driver would. In the STDA model for predicting driver attention, we employed an encoder-decoder architecture. The encoder utilizes MobileNet-V2 as its backbone, owing to its low memory usage and rapid prediction capabilities. Next, we adopted Post-CNN to post-process the extracted feature $M_t \\in \\mathbb{R}^{T \\times 1280 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to obtain $P_t \\in \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to reduce the amount of calculation. A spatial attention mechanism was implemented to process $P_t$ facilitating the selective processing of visual information by prioritizing relevant regions within the visual field, thereby enhancing perceptual efficiency and accuracy in analyzing complex scenes. Formally, Conv2d projections were adopted to compute a set of queries, keys, and values (Q, K and V),\n$Q = W_q * X, K = W_k * X, V = W_v * X$ (1)\nwhere $X \\in \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$, and * denotes the convolution operator. The weight matrices of $W_q, W_k, and W_v$ refer to the self-learned parameters of convolution operations with a stride is 1 and a filter size of 1x1. We converted the $Q, K, V \\in \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to $Q, K, V \\in \\mathbb{R}^{T \\times SP \\times 256}$ to match the dimensions required for the attention computation process, where SP denotes the spatial dimension $\\frac{H}{32} \\times \\frac{W}{32}$. The scaled dot product was utilized to calculate the attention weights between Q and K, to determine the spatial attention by aggregating V for each query,\n$A = softmax(\\frac{QK^T}{\\sqrt{D_k}})V + X'$ (2)\nwhere $D_k$ is the number of channels, values of X' and X are identical, $X' \\in \\mathbb{R}^{T \\times SP \\times 256}$ after reshaping, and $\\varepsilon$ is a learnable parameter. We converted $A \\in \\mathbb{R}^{T \\times SP \\times 256}$ into $A \\in \\mathbb{R}^{T \\times 256 \\times 3}$ to meet the input requirements of subsequent modules. The processed features A were passed into an inverted residual block and a Conv-GRU with 128 hidden channels and a 3x3 kernel size for sequence prediction. Enhancement of spatial-temporal feature extraction is facilitated through the integration of two critical gating mechanisms in the Conv-GRU. The complete Conv-GRU within STDA is represented as follows:\n$R_t = \\Psi(BN(W_{ar} * A'_{t}) + BN(W_{hr} * H_{t-1}) + b_r)$ (3)\n$Z_t = \\Psi(BN(W_{az} * A'_{t}) + BN(W_{hz} * H_{t-1}) + b_z)$ (4)\n$\\tilde{H}_t = \\Phi (\\frac{BN(W_{ah} * A'_{t}) + BN(W_{hh} * (R_t \\odot H_{t-1}) + b_h}{\\sqrt{2}})$ (5)\n$H_t = (1 - Z_t) \\odot H_{t-1} + Z_t \\odot \\tilde{H}_t$ (6)\nwhere W represents the weights. $\\Psi$ and $\\Phi$ denote the sigmoid function and the hyperbolic tangent respectively; $\\odot$ denotes the Hadamard product. For the decoder, a Post-CNN was adopted to enrich the extracted feature channels, and up-sampling was utilized to align the upstream features for residual connection. Subsequently, the spatial attention mechanism was applied to enhance spatial features, followed by an inverted residual block and up-sampling to decrease the channel dimensions for improved feature representation. Two inverted residual blocks were utilized to further enrich features, thereby facilitating prediction. Additionally, another self-attention layer was added to enhance channel information. Finally, an inverted residual block was adopted to reduce the channel dimensions, and the features were upsampled to match the input image size using nearest-neighbor interpolation.\nThe traffic frame sequence $S_t \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$ was fed to the DA prediction model, which outputs features $A_t \\in \\mathbb{R}^{T \\times 1 \\times H \\times W}$ representing the driver attention at each timestep t, and we adopted channel extension to get $A_t \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$. We passed the driver attention image stream and the original image stream into the fusion module.\nTwo fusion approaches were designed further to explore the impact of fusion methods on model performance. The first approach, STDA-B, employs an image blending tactic that precisely amalgamates the original image with the driver attention heatmap on a per-pixel basis. This integration delineates areas of primary importance on the original image, enhancing the neural network's ability to focus on and assimilate pivotal visual elements. It can be calculated as:\n$F_f = Blend(F_t, A_{t,i},i)$ (7)\nwhere $F_f \\in \\mathbb{R}^{3 \\times H \\times W}$ represents the fused image at each timestep t, and $A_{t,i} \\in \\mathbb{R}^{3 \\times H \\times W}$ is the driver attention heatmap corresponding to the ith frame, and the $A_{t,i}$ corresponds to the $F_t$. The fused frame sequence $S'_{f} = {F'_{f-T}, ..., F'_{f}}$.\nAnother method, termed STDA-C, utilizes the cross-attention mechanism. STDA-C is predicated on the cross-attention mechanism and capitalizes on the synergistic integration of the original image with the driver attention heatmap. This method strategically directs focus towards regions of interest with enhanced precision by the neural network. It maintains the original image's contextual integrity and emphasizes the salient areas as delineated by the attention heatmap. The cross-attention mechanism facilitates the selective prioritization of the most pertinent features, enabling a more nuanced and compelling analysis. This approach is anticipated to enhance the neural network's performance. It can be calculated as:\n$S_t^{mip} = LN(A_t^{mlp} + softmax(\\frac{S_t^{mlp}Q_t^{mlpT}}{\\sqrt{D_k}})Q_t^{mlp})$ (8)\nwhere MLP was utilized to project the images features into high dimensional space. $A_t^{mlp} = MLP(A_t)$, with $A_t^{mlp} \\in \\mathbb{R}^{T \\times HW \\times 64}$ and $S_t^{mlp} \\in \\mathbb{R}^{T \\times HW \\times 64}$, where HW means H \u00d7 W. LN denotes the Layer Normalization. To maintain consistency in the size of the fused feature output with the original, another MLP was used to back-project, ensuring the dimensions remain unchanged.\nThe feature fusion module, which combines the predicted DA areas with original image streams, named the DA-integrating feature $S'_{f} \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$.\nIn the STDA model, temporal encoding was employed to enhance the neural network's ability to recognize and interpret temporal patterns over time. The input to this module is the DA-integrating feature $S'_{f}$. A feed-forward network (FFN) was used to project $S'_{f}$ into a high-dimensional space, enabling the model to assimilate and integrate more comprehensive information within an intricate representational framework. We utilized Batch Normalization (BN) to standardize the processed high-dimensional features, enhancing the stability of the training process. Ultimately, the time dimension was compressed to extract the final features $S_{out}$. It can be calculated as:\n$S_{out} = Squeeze(BN(FFN(S'_{f})))$ (9)\nwhere FFN was used to project the image features into a high-dimensional space. Conversion to a high-dimensional space can improve STDA's ability to interpret time series data. Subsequently, the time dimension was compressed to convert $S'_{f} \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$ into $S_{out} \\in \\mathbb{R}^{3 \\times H \\times W}$. By reverting it to the traditional image feature dimensions, integration with existing mainstream image models is seamlessly facilitated.\nThe temporal encoder network enables STDA to efficiently encode and decode temporal information while preserving image resolution, significantly enhancing the capability to interpret dynamic scenes.\nThe processed DA-integrating feature $S_{out}$ was passed into a CNN-based network to further extract high-order features. The STDA process starts with the input being processed through a convolutional layer equipped with Batch Normalization and ReLU activation, followed by Max Pooling to reduce spatial dimensions and highlight key features. Sequentially arranged Convolutional and Identity Blocks increase the depth of network, crucial for capturing complex patterns in the data. As the architecture deepens, the number of channels increases, encoding more detailed information, while the spatial resolution decreases, focusing the network on high-level abstractions. This process is followed by Adaptive Average Pooling and a Flatten layer, culminating in a multi-layer perceptron that outputs the predicted behavior."}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "We adopted Cost-Sensitive Learning (CSL) [28] to help the model learning. It seeks to re-balance classes by adjusting loss values for different classes during training, by assigning a higher cost to the misclassification of minority classes in imbalanced datasets, effectively shifting the classifier focus"}, {"title": "V. CONCLISION", "content": "This study introduces a novel approach: the Spatio-Temporal dual-encoder incorporating Driver Attention, designed for predicting driver behavior in safety-critical scenarios. By integrating spatial and temporal data along with a focus on driver attention, the STDA enhances the G-mean and IBA metrics in predicting driver behaviors. Two fusion module, STDA-B and STDA-C are proved as effective approaches to utilize driver attention especially STDA-B which employs an image blending tactic, enhancing model's performance. The inclusion of driver attention not only aligns the model with human cognitive processes but also significantly boosts its interpretability. The STDA model maintains computational efficiency through detailed experimentation while delivering state-of-the-art performance and demonstrating robust generalization capabilities. These attributes underscore STDA's potential for seamless integration into mainstream models, broadening its utility. Future work will expand STDA's applicability to complex driving scenarios and improve its real-time processing capabilities. Additionally, field testing will be essential to move the algorithm from theoretical validation to practical implementation."}]}