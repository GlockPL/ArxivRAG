{"title": "STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios", "authors": ["Dongyang Xu", "Yiran Luo", "Tianle Lu", "Qingfan Wang", "Qing Zhou", "Bingbing Nie"], "abstract": "Accurate behavior prediction for vehicles is essential but challenging for autonomous driving. Most existing studies show satisfying performance under regular scenarios, but most neglected safety-critical scenarios. In this study, a spatio-temporal dual-encoder network named STDA for safety-critical scenarios was developed. Considering the exceptional capabilities of human drivers in terms of situational awareness and comprehending risks, driver attention was incorporated into STDA to facilitate swift identification of the critical regions, which is expected to improve both performance and interpretability. STDA contains four parts: the driver attention prediction module, which predicts driver attention; the fusion module designed to fuse the features between driver attention and raw images; the temporary encoder module used to enhance the capability to interpret dynamic scenes; and the behavior prediction module to predict the behavior. The experiment data are used to train and validate the model. The results show that STDA improves the G-mean from 0.659 to 0.719 when incorporating driver attention and adopting a temporal encoder module. In addition, extensive experimentation has been conducted to validate that the proposed module exhibits robust generalization capabilities and can be seamlessly integrated into other mainstream models.", "sections": [{"title": "I. INTRODUCTION", "content": "In autonomous and semi-autonomous vehicles, the ability to understand and predict driver behavior plays a critical role in ensuring the safety and efficiency of road transportation [1, 2]. Recent advancements in machine learning and computer vision have paved the way for significant improvements in this area [3, 4]. However, the dynamic and complex nature of the driving environment introduces unique challenges, particularly in safety-critical scenarios where the timely and accurate prediction of driver actions can make the difference between safety and hazard [5, 6].\nIncorporating driver attention into predictive models represents a significant departure from conventional approaches. Skilled drivers have the ability to detect and foresee potential traffic hazards rapidly. Some studies indicate that driver attention (DA) is a crucial risk indicator, proven to accurately predict driving patterns or vehicular movement, which are vital components in the decision-making process, especially under safety-critical conditions [7-10]. Moreover, both naturalistic driving assessments and controlled laboratory simulations consistently confirm the crucial role of DA in identifying objects that could lead to conflicts [11, 12].\nTraditional models often depend solely on external environmental factors and historical driver data, overlooking the crucial element of where and how drivers focus their attention in varying situations [13, 14]. To bridge this gap, we introduce the STDA (Fig. 1): Spatio-Temporal dual-encoder network incorporating Driver Attention, by integrating driver attention into the driver behavior prediction model, thus offering a more holistic view of the driver's state and intentions. Furthermore, we designed a temporal encoding module to enhance the ability of the model to comprehend temporal dynamics. By merging spatial information from the driving environment with temporal patterns of driver behavior and attention, STDA seeks to offer a comprehensive framework for predicting driver actions more accurately and swiftly than before.\nThe contributions of this study are summarized as follows:\n1) We developed a driver behavior model named STDA, tailored explicitly for safety-critical scenarios. It weaves together spatial and temporal encodings, yielding more precise predictions of driver behaviors.\n2) We integrated driver attention into the STDA to better align the model with human cognitive processes and enhance its interpretability. Moreover, we explored methods for effectively fusing driver attention data with input images.\n3) We conducted a comprehensive suite of experiments and ablation studies to demonstrate its superior efficacy and ability to maintain computational expeditiousness.\nThe structure of this paper is as follows: in Section II, we detail the previous relevant studies focused on driver attention prediction and driver behavior prediction under safety-critical scenarios. Subsequently, Section III elaborates on the formulation of the STDA with four critical parts: the driver attention prediction module, the fusion module, the temporary encoder module, and the behavior prediction module. The results and analysis of the experiments are presented in Section IV, followed by a discussion and conclusion in Section V."}, {"title": "II. RELATED WORKS", "content": "The field of driver attention prediction has seen significant advancements in recent years. [15] employs U-Net as the backbone and incorporates the Swin-transformer to predict DA. [16] merges a transformer with a convolution network and then utilizes a Conv-LSTM to process the features for DA prediction. [17] utilizes a Convolutional Long Short-Term Memory (Conv-LSTM) network to capture temporal characteristics and employs a pyramid-dilated convolution approach to extract spatial attributes. It then leverages an attention mechanism to combine these temporal and spatial features, using the fused features to predict DA. Recently, inverse reinforcement learning (IRL) has represented a significant advancement in imitating driver attention, particularly in scenarios involving imminent rear-end collisions. This approach utilizes rich visual inputs, such as semantic cues, depth perception, and road lane information, extracted through pre-trained convolutional neural network (CNN)-based models. Although including detailed visual information enhances the model's performance, it also introduces a higher computational complexity than traditional end-to-end CNN architectures [18]. However, these models still encounter limitations in terms of downstream expansion.\nDriver behavior prediction is an essential aspect of intelligent transportation systems. Various methodologies for predicting driver behavior have been adopted, ranging from traditional statistical models to advanced machine-learning techniques. These models are broadly categorized into two distinct types: model-driven methods and data-driven methods. Each category signifies a unique approach to understanding and forecasting driving behaviors, leveraging various methodologies and theoretical foundations to enhance prediction accuracy and reliability across diverse driving scenarios. [19] develops a state space framework that integrates a Markov chain to accurately predict the likelihood of vehicles either shifting to a different lane or maintaining their current path. [20] develops mathematical models that utilize kinematic characteristics, such as velocity and steering wheel movement, to predict the likelihood of a lane change. However, model-based methods suffer from poor generalization and present difficulties in parameter calibration. For data-driven methods, [21] introduces a sophisticated hierarchical reinforcement learning framework to optimize making and executing lane change decisions. [22] leverages the RNN-LSTM architecture to create a time-series model for analyzing driving behaviors, utilizing vision-based signals to discern intentions for lane changes. However, the methods above do not consider driver attention and are not oriented towards dangerous scenarios."}, {"title": "III. METHODOLOGY", "content": "In this study, we proposed a spatio-temporal dual-encoder network incorporating driver attention (STDA) for safety-critical scenarios (Fig. 1), which is aligned with the hazard perception mechanism of human drivers. STDA takes first-person image streams as input and outputs driver behaviors. Given that drivers make comprehensive decisions based on historical data. To improve the performance of the model, images from historical periods were used as inputs. Specifically, the traffic frame sequence $S_t = {F_{t-T}, ..., F_t}$ was sampled from the image streams, where $F_t$ represents the traffic frame to which the driver responded in safety-critical scenarios. T is the length of historical frames. Each of the frames in a sequence was fed into the DA prediction models,\nPredicting where a driver is looking can help an autonomous vehicle better understand traffic situations, much like a human driver would. In the STDA model for predicting driver attention, we employed an encoder-decoder architecture. The encoder utilizes MobileNet-V2 as its backbone, owing to its low memory usage and rapid prediction capabilities. Next, we adopted Post-CNN to post-process the extracted feature $M_t \\mathbb{E} \\mathbb{R}^{T \\times 1280 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to obtain $P_t \\mathbb{E} \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to reduce the amount of calculation. A spatial attention mechanism was implemented to process $P_t$, facilitating the selective processing of visual information by prioritizing relevant regions within the visual field, thereby enhancing perceptual efficiency and accuracy in analyzing complex scenes. Formally, Conv2d projections were adopted to compute a set of queries, keys, and values (Q, K and V),\n$Q = W_q * X, K = W_k * X, V = W_v * X$ (1)\nwhere $X \\in \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$, and * denotes the convolution operator. The weight matrices of $W_q, W_k, and W_v$ refer to the self-learned parameters of convolution operations with a stride is 1 and a filter size of 1x1. We converted the $Q, K, V \\in \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to $Q, K, V \\in \\mathbb{R}^{T \\times SP \\times 256}$ to match the dimensions required for the attention computation process, where SP denotes the spatial dimension $\\frac{H}{32} \\times \\frac{W}{32}$. The scaled dot product was utilized to calculate the attention weights between Q and K, to determine the spatial attention by aggregating V for each query,\n$A = softmax(\\frac{QK^T}{\\sqrt{D_k}})V + X'$ (2)\nwhere $D_k$ is the number of channels, values of X' and X are identical, $X' \\in \\mathbb{R}^{T \\times SP \\times 256}$ after reshaping, and $\\varepsilon$ is a learnable parameter. We converted $A \\in \\mathbb{R}^{T \\times SP \\times 256}$ into $A \\in \\mathbb{R}^{T \\times 256 \\times \\frac{H}{32} \\times \\frac{W}{32}}$ to meet the input requirements of subsequent modules. The processed features A were passed into an inverted residual block and a Conv-GRU with 128 hidden channels and a 3x3 kernel size for sequence prediction. Enhancement of spatial-temporal feature extraction is facilitated through the integration of two critical gating mechanisms in the Conv-GRU. The complete Conv-GRU within STDA is represented as follows:\n$R_t = \\Psi(BN(W_{ar} * A'_t) + BN(W_{hr} * H_{t-1}) + b_r)$ (3)\n$Z_t = \\Psi(BN(W_{az} * A'_t) + BN(W_{hz} * H_{t-1}) + b_z)$ (4)\n$\\tilde{H_t} = \\Phi(\\frac{BN(W_{aa} * A'_t) + BN(W_{hh} * (R_t \\odot H_{t-1}) + b_h}{BN(W_{ao} * A'_t) + BN(W_{ho} * (R_t \\odot H_{t-1}) + b_o})$ (5)\n$H_t = (1 - Z_t) \\odot H_{t-1} + Z_t \\odot \\tilde{H_t}$ (6)\nwhere W represents the weights. $\\Psi$ and $\\Phi$ denote the sigmoid function and the hyperbolic tangent respectively; $\\odot$ denotes the Hadamard product. For the decoder, a Post-CNN was adopted to enrich the extracted feature channels, and up-sampling was utilized to align the upstream features for residual connection. Subsequently, the spatial attention mechanism was applied to enhance spatial features, followed by an inverted residual block and up-sampling to decrease the channel dimensions for improved feature representation. Two inverted residual blocks were utilized to further enrich features, thereby facilitating prediction. Additionally, another self-attention layer was added to enhance channel information. Finally, an inverted residual block was adopted to reduce the channel dimensions, and the features were upsampled to match the input image size using nearest-neighbor interpolation.\nThe traffic frame sequence $S_t \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$ was fed to the DA prediction model, which outputs features $A_t \\in \\mathbb{R}^{T \\times 1 \\times H \\times W}$ representing the driver attention at each timestep t, and we adopted channel extension to get $A_t \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$. We passed the driver attention image stream and the original image stream into the fusion module.\nTwo fusion approaches were designed further to explore the impact of fusion methods on model performance. The first approach, STDA-B, employs an image blending tactic that precisely amalgamates the original image with the driver attention heatmap on a per-pixel basis. This integration delineates areas of primary importance on the original image, enhancing the neural network's ability to focus on and assimilate pivotal visual elements. It can be calculated as:\n$F_f = Blend(F_t, A_{t,i}, \\lambda)$ (7)\nwhere $F_f \\in \\mathbb{R}^{3 \\times H \\times W}$ represents the fused image at each timestep t, and $A_{t,i} \\in \\mathbb{R}^{3 \\times H \\times W}$ is the driver attention heatmap corresponding to the ith frame, and the $A_{t,i}$ corresponds to the $F_t$. The fused frame sequence $S'_f = {F'_{f-T}, ..., F'_f}$.\nAnother method, termed STDA-C, utilizes the cross-attention mechanism. STDA-C is predicated on the cross-attention mechanism and capitalizes on the synergistic integration of the original image with the driver attention heatmap. This method strategically directs focus towards regions of interest with enhanced precision by the neural network. It maintains the original image's contextual integrity and emphasizes the salient areas as delineated by the attention heatmap. The cross-attention mechanism facilitates the selective prioritization of the most pertinent features, enabling a more nuanced and compelling analysis. This approach is anticipated to enhance the neural network's performance. It can be calculated as:\n$S'_t = LN(\\alpha_{mlp}^{mip}S_{mlp}) + softmax(\\frac{Smip SmipT}{\\sqrt{D_k}})$ (8)\nwhere MLP was utilized to project the images features into high dimensional space. $A_{mlp} = MLP(A_t)$, with $A_{mlp} \\in \\mathbb{R}^{T \\times H \\times W \\times 64}$ and $S_{mlp} \\in \\mathbb{R}^{T \\times H \\times W \\times 64}$, where HW means H \u00d7 W. LN denotes the Layer Normalization. To maintain consistency in the size of the fused feature output with the original, another MLP was used to back-project, ensuring the dimensions remain unchanged.\nThe feature fusion module, which combines the predicted DA areas with original image streams, named the DA-integrating feature $S'_f \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$.\nIn the STDA model, temporal encoding was employed to enhance the neural network's ability to recognize and interpret temporal patterns over time. The input to this module is the DA-integrating feature $S'_f$. A feed-forward network (FFN) was used to project $S'_f$ into a high-dimensional space, enabling the model to assimilate and integrate more comprehensive information within an intricate representational framework. We utilized Batch Normalization (BN) to standardize the processed high-dimensional features, enhancing the stability of the training process. Ultimately, the time dimension was compressed to extract the final features $S_{out}$. It can be calculated as:\n$S_{out} = Squeeze(BN(FFN(S'_f)))$ (9)\nwhere FFN was used to project the image features into a high-dimensional space. Conversion to a high-dimensional space can improve STDA's ability to interpret time series data. Subsequently, the time dimension was compressed to convert $S'_f \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$ into $S_{out} \\in \\mathbb{R}^{3 \\times H \\times W}$. By reverting it to the traditional image feature dimensions, integration with existing mainstream image models is seamlessly facilitated.\nThe temporal encoder network enables STDA to efficiently encode and decode temporal information while preserving image resolution, significantly enhancing the capability to interpret dynamic scenes.\nThe processed DA-integrating feature $S_{out}$ was passed into a CNN-based network to further extract high-order features. The STDA process starts with the input being processed through a convolutional layer equipped with Batch Normalization and ReLU activation, followed by Max Pooling to reduce spatial dimensions and highlight key features. Sequentially arranged Convolutional and Identity Blocks increase the depth of network, crucial for capturing complex patterns in the data. As the architecture deepens, the number of channels increases, encoding more detailed information, while the spatial resolution decreases, focusing the network on high-level abstractions. This process is followed by Adaptive Average Pooling and a Flatten layer, culminating in a multi-layer perceptron that outputs the predicted behavior."}, {"title": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this study, the Personalized Situation Awareness of Drivers (PSAD) dataset [23] was utilized to train our model. The PSAD dataset offers multi-modal data, including first-person driving recorder traffic frames, driver attention distribution information, and driver emergency response information. It is suitable for training driver behavior prediction models. The PSAD establishment process unfolds as follows: 2724 safety-critical scene videos were collected from driving recorders; volunteers were recruited to watch the videos in a laboratory environment; their visual gaze information was collected using eye trackers, and their emergency responses were collected using driving simulators. Due to potential distractions faced by the volunteer during the experiment, we carefully selected data from subject 101 for model training because we think subject 101 was the most careful during the experiment. Analysis reveals that, when confronted with various safety-critical scenarios, the frequencies of braking, turning right, and turning left by the volunteer were 1730, 319, and 264, respectively.\nAnalysis of the data composition reveals substantial non-uniformity. Applying brakes is the predominant response among drivers when encountering perilous circumstances, with only a minority opting for steering. Therefore, creating an appropriate framework to enhance model performance in the face of long-tail challenges is paramount.\nWe adopted Cost-Sensitive Learning (CSL) [28] to help the model learning. It seeks to re-balance classes by adjusting loss values for different classes during training, by assigning a higher cost to the misclassification of minority classes in imbalanced datasets, effectively shifting the classifier focus\ntowards these typically underrepresented classes. Specifically, it can be calculated as:\n$E_{cost} = \\sum_{i=1}^{N} \\sum_{j=1}^{N} C_{ij}CE(y_i)$ (10)\nwhere N is the number of classes, $C_{ij}$ is the cost of misclassifying an example from true class i as class j. The $y_i$ is the predicted probabilities of class i, and CE is the cross-entropy loss. The purpose of CSL is to minimize the misclassification costs ($E_{cost}$). Due to the limited value of accuracy in unbalanced datasets, where correctly predicting braking can lead to misleadingly high results, this study does not employ accuracy. To undertake a comprehensive performance assessment of our proposed model, particularly consideration of data imbalance, we employed a suite of six evaluative metrics. Recall measures the model's capability to correctly identify true positive instances within the pool of actual positives. Precision gauges the accuracy with which the model classifies instances as positive.\nTo enhance the evaluation, four additional metrics were employed. The F1-Score, a synthesized metric, blends precision and recall. The Geometric Mean (G-mean) evaluates performance uniformity across different class categories, critical in the context of imbalanced data. The Index of Balanced Accuracy (IBA) [29], particularly relevant for our dataset, averages sensitivity and specificity, offering an equitable view of performance across various classes. Another metric is Specificity, crucial for imbalanced datasets, which assesses the model's ability to correctly classify true negatives. These additional metrics are particularly salient in our analysis, reflecting the model's adeptness at handling imbalanced class distributions.\nWe implemented STDA within the PyTorch. For the images, the resolution of input images were cropped to 224 \u00d7 224. Following [10], we first pre-trained the DA prediction model, for training the DA model. We initialized the learning rate at 0.02 and decayed it exponentially by a factor of 0.8 after each epoch. We used stochastic gradient descent with a momentum of 0.9 for optimization. For training the whole model (STDA), we adopted Cost-sensitive Learning as loss function and used the Adam as the optimizer to train the model, the learning rate was 0.0001 and batch size was 2 per card. STDA was trained on V100 16G GPU. As for the throughout, we configured the batch size to 256 and conducted assessments on an A800 80G GPU.\nAnalyzing TABLE I, regarding computational efficiency, the FLOPs for the STDA variants are 8.65G and 8.70G, respectively. This places them in the higher echelon of computational intensity. Despite this, the throughput efficiency, measured at 757.6 and 732.3 images per second for STDA-B and STDA-C, respectively, signifies a commendable processing velocity compared to existing models. Such metrics underscore the model's high computational efficiency, which is crucial in safety-critical scenarios where the window of time to situations is extremely limited.\nThe STDA-B model, when compared to other mainstream models. The specificity of the STDA-B model is noteworthy, reflecting its effectiveness in accurately identifying negative instances and thus reducing false positives, a vital feature when handling imbalanced datasets. This is further supported by its F1-Score, which, while not at the very top, signifies a well-rounded balance between precision and recall. Considering the G-mean and IBA scores, STDA-B stands out in maintaining performance consistency across classes, which is beneficial when dealing with varied class distributions, especially in imbalanced contexts. STDA-B prioritizes balanced performance across classes rather than achieving high accuracy on the majority class at the expense of minority classes. In summary, the experimental results demonstrate that STDA is effective for imbalanced datasets, ensuring that it does not disproportionately favor the majority class. STDA is suitable for scenarios like safety-critical conditions where hazardous but rare events form a long-tail distribution.\nThe underwhelming performance of knowledge distillation models might stem from the distillation process's complexities and mismatches between teacher and student models. Additionally, imblanced datasets may lead to overfitting on dominant classes while overlooking rarer ones. Moreover, we failured to pretrain it may also be a potential reason for its poor effect. Addressing data imbalances and incorporating pretraining stages could further improve model robustness and performance.\nIt is observed that STDA-B outperforms STDA-C in prediction performance. The superior efficacy of image blending compared to cross-attention-based fusion can be attributed to its direct and uniform integration of features, which can preserve spatial coherency and reduce feature distraction. In contrast to cross-attention mechanisms that selectively emphasize features and may inadvertently skew feature representation towards dominant patterns, blending ensures a more holistic and balanced integration of visual data, potentially yielding a richer and more accurate feature landscape for analysis.\nAblation experiments on STDA were conducted to determine the utility of the designed modules for the model. As for removing the DA module and temporary module, we initialized the DA module to ensure the proper functioning of the fusion module. Results are presented in TABLE II. For STDA-B, incorporating the DA module resulted in a 4.5% increase in G-Mean, highlighting significant improvement in handling imbalanced datasets. The temporary encoder module alone did not significantly affect the F1-score or Specificity but led to a modest increase in G-Mean. Employing both modules yielded appreciable improvements across all metrics, especially in G-Mean and IBA, with increases of 8.9% and 18%, respectively. For STDA-C, the enhancements were even more pronounced. The combination DA module with temporary module significantly boosted G-Mean by 28.3%. These findings highlight the synergistic impact of the DA and temporary encoder modules on enhancing STDA's performance.\nThe DA module improves the model's comprehension of its surroundings by emphasizing the areas requiring the most attention, enhancing its robustness and generalization ability. For instance, within the safety-critical scenarios depicted in Fig. 3, When the car is about to change lanes, DA module will immediately pay attention to the surrounding vehicles interacting, providing a prior knowledge to the behavior prediction model. This augmentation enables timely and informed decision-making in complex traffic conditions. The temporary encoder module enhances STDA's temporal feature encoding capabilities, enriching its feature space for more nuanced data interpretations. Combining these modules performs well across individual classes and maintains uniform performance across the dataset, achieving the best performance, as evidenced by the significant increases in G-Mean and IBA."}, {"title": "V. CONCLISION", "content": "This study introduces a novel approach: the Spatio-Temporal dual-encoder incorporating Driver Attention, designed for predicting driver behavior in safety-critical scenarios. By integrating spatial and temporal data along with a focus on driver attention, the STDA enhances the G-mean and IBA metrics in predicting driver behaviors. Two fusion module, STDA-B and STDA-C are proved as effective approaches to utilize driver attention especially STDA-B which employs an image blending tactic, enhancing model's performance. The inclusion of driver attention not only aligns the model with human cognitive processes but also significantly boosts its interpretability. The STDA model maintains computational efficiency through detailed experimentation while delivering state-of-the-art performance and demonstrating robust generalization capabilities. These attributes underscore STDA's potential for seamless integration into mainstream models, broadening its utility. Future work will expand STDA's applicability to complex driving scenarios and improve its real-time processing capabilities. Additionally, field testing will be essential to move the algorithm from theoretical validation to practical implementation."}]}