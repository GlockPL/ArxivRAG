{"title": "EMBEDDED PROMPT TUNING: TOWARDS ENHANCED CALIBRATION OF PRETRAINED MODELS FOR MEDICAL IMAGES", "authors": ["Wenqiang Zu", "Shenghao Xie", "Qing Zhao", "Guoqi Li", "Lei Ma"], "abstract": "Foundation models pre-trained on large-scale data have been widely witnessed to achieve success in various natural imaging downstream tasks. Parameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models to new domains by updating only a small portion of parameters in order to reduce computational overhead. However, the effectiveness of these PEFT methods, especially in cross-domain few-shot scenarios, e.g., medical image analysis, has not been fully explored. In this work, we facilitate the study of the performance of PEFT when adapting foundation models to medical image classification tasks. Furthermore, to alleviate the limitations of prompt introducing ways and approximation capabilities on Transformer architectures of mainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT) method by embedding prompt tokens into the expanded channels. We also find that there are anomalies in the feature space distribution of foundation models during pre-training process, and prompt tuning can help mitigate this negative impact. To explain this phenomenon, we also introduce a novel perspective to understand prompt tuning: Prompt tuning is a distribution calibrator. And we support it by analyzing patch-wise scaling and feature separation operations contained in EPT. Our experiments show that EPT outperforms several state-of-the-art fine-tuning methods by a significant margin on few-shot medical image classification tasks, and completes the fine-tuning process within highly competitive time, indicating EPT is an effective PEFT method. Our code will be released once accepted.", "sections": [{"title": "Introduction", "content": "Benifiting from massive training data, foundation models [1] have been widely witnessed to achieve impressive performance in various natural imaging downstream tasks, e.g., classification [2, 3], segmentation [4, 5, 6, 7], and detection [8, 9, 10].\nRecent studies have attempted to explore the potential of foundation models in medical image analysis [11, 12, 13, 14, 15]. Although foundation models have robust representation and generalization capabilities on natural images, their adaptabilities are still challenged when facing downstream tasks with a significant domain gap [16]. Therefore, foundation models need to be re-trained on medical datasets to incorporate domain-specific knowledge.\nHowever, due to the huge number of parameters in foundation models, training from scratch would result in significant computational and memory cost. To address this issue, parameter-efficient fine-tuning (PEFT) [17] methods have been"}, {"title": "Related Work", "content": "The goal of PEFT is to achieve higher performance by fine-tuning as few parameters as possible when adapting foundation models to downstream tasks. PEFT generally fine-tunes Transformer at three positions: input, backbone, and linear head. Since the linear head is essential for downstream tasks, it typically needs to be fine-tuned. Therefore, PEFT pays more attention on the input and backbone, and has correspondingly raised two representative methods: prompt tuning and adapter tuning.\nPrompt tuning introduces extra tokens into the input image and fine-tunes them. VPT [24] prepends prompt tokens before embedding tokens parallelly. DVPT [31] employs cross-attention mechanism between prompts and features to capture distribution-specific knowledge. VP [25] adds values of prompt tokens and embedding tokens across all channels. EVP [32] primarily focuses on high-frequency components of features. CoOp [33] utilizes learnable text to addressing the issue of semantic consistency across different sentences. CoCoOp [34] introduces a Meta-net to generate prompts and enhance the generalizability. KgCoOp [35] constructed a regularization term to minize the gap between generated and crafted prompts."}, {"title": "Method", "content": "Leto be the softmax operators. A bold lower case character, eg x, denotes a vector. A bold upper case character, e.g. W, denotes a matrix while $W_{i,j}$, $W_{i,:}$ and $W_{:,j}$ is the (i, j) th element, i-th row, j-th column, respectively. We use ReLU(v) = max(v, 0) to denote the ReLU activation function where max(\u00b7) function is applied entry wise to a vector.\nFor descriptive purposes, we use single head attention to illustrate. Moreover, we use $X \\in \\mathbb{R}^{d\\times n}$ to describe the input shape in the subsequent explanations, instead of $X \\in \\mathbb{R}^{n \\times d}$ depicted in Fig 1. We use Q, K, and V to denote the multiplication of $W_q \\in \\mathbb{R}^{d \\times d}$, $W_k \\in \\mathbb{R}^{d \\times d}$, $W_v \\in \\mathbb{R}^{d \\times d}$, and $X \\in \\mathbb{R}^{d\\times n}$, where d represents the embedding dimensions, and n represents the number of patches. Therefore, we have:\n$Q = W_qX, K = W_kX, V = W_vX$\nThen, a self-attention operation of input token sequence $X \\in \\mathbb{R}^{d\\times n}$ can be represented as\n$\\text{Att}(X) = V \\sigma(KQ^\\top)$ \\label{eq:self_att}\nThe normalizing factor of $\\sqrt{d_{kq}}$ is subsumed in the weight matrices $W_k$ for notational simplicity. And we omitted the expression of multihead.\nWith and $\\ref{eq:self_att}$, a standard Transformer layer + can be represented as\n$\\begin{aligned} & \\text{MLP}(X) = [W_2 \\text{ReLU}(W_1X_{:,1} + b_1) + b_2 + X_{:,1},\\dots, W_2 \\text{ReLU}(W_1X_{:,n} + b_1) + b_2 + X_{:,n}] \\\\ &\\Upsilon (X) = \\text{MLP}(\\text{Att}(X) + X). \\end{aligned}$ \\label{eq:trans_layer}\nVPT prepends prompt tokens before all original input tokens parallely, while VP adds the values of prompt tokens with original input tokens across all channels. These prompt introducing ways either do not fine-tune the original input tokens fine-grainedly or significantly disrupt the information of original input tokens. Therefore, we propose Embedded Prompt Tuning (EPT), embedding prompt tokens into expanded channels. In this way, EPT can not only preserve original information well, but also introduce extra useful context to optimize embedding tokens.\nAs shown in Fig 1, the concept of our proposed EPT method and its differences compared to VPT are illustrated. We use the EPT prompts $P_E$ to differentiate P. $P_E \\in \\mathbb{R}^{d_p\\times n}$ have a completely different shape from the prompts $P \\in \\mathbb{R}^{d \\times n_p}$ in VPT, where $d_p$ represents the prompt length. It should be noted that the prompts $P_E$ are not directly inserted into the patch tokens but only appear during the softmax operation on $K^TQ$ in the attention calculation, and they disappear after the computation is completed. The purpose of this approach is to scale $K^TQ$ without altering its shape, enabling subsequent computations."}, {"title": "Analysis of EPT", "content": "In the context of few-shot learning, researchers face the challenge of overfitting due to an insufficient number of training samples, which is characterized by accurate classification on the training set but poor performance on the test set. To tackle this challenge, we attempt to approach it from the following perspective: calibrating the distribution of the few-shot samples by reducing the intra-class distance of samples from the same class, thereby increasing the degree of separation between features of different classes during training, ultimately achieving higher accuracy [30].\nBased on our observations, EPT can help mitigate the negative impact caused by foundation models when learning pre-trained data distribution. Additionally, we notice that the patch-wise scaling operation, by implementing varying scaling ratios for different samples, facilitates the aggregation of features from the same class towards the intra-class center. Consequently, it increases the potential for greater separation of features between samples from different classes. Moreover, by introducing prompts, the dimensions of features are increased, thereby enhancing the high-dimensional expression and decoupling capability of features. Through analyzing these two operations intuitively and theoretically, we propose a new perspective to understand prompt tuning: Prompt is a distribution calibrator.\nTherefore, first, we define the intra-class distance for a better understanding of the problem. For a dataset X =\n$\\left\\{x_{k,i}, k \\in [K], i \\in [n_k]\\right\\}$ with K classes, where $x_{k,i} \\in \\mathbb{R}^d$ is the i-th sample in the k-th class,and the number of samples $n_k$ in each class is balanced with $n_1 = \\dots = n_K = n$. We borrowed the definition from [43], and we use $\\bar{x_k}$ to represent the center of the samples in class k, where the intra-class distance matrix is defined as\n$\\Sigma_\\mathcal{W} = \\frac{1}{N} \\Sigma^K_{k=1} \\Sigma^{n_k}_{i=1} (x_{k,i} - \\bar{x_k})^T (x_{k,i} - \\bar{x_k})$ \\label{intra_class}\nAccording to the definition, a smaller value of $\\Sigma_\\mathcal{W}$ indicates a higher degree of feature clustering, which may lead to better separability.\nFor dataset that satisfies $X = \\left\\{x_{k,i} \\geq 0, k \\in [K], i \\in [n_k]\\right\\}$, If we apply different scaling operations to samples of the same class, we can derive the conclusion stated in Lemma 1.\nLet $c_{k,i}, c_{k,j} \\in [0, 1]$ be the scaling operations on the samples $x_{k,i}$ and $x_{k,j}$ of class k, respectively, and satisfy $(c_{k,i} \u2013 c_{k,j}) (\\|x_{k,i}\\|_2 - \\|x_{k,j}\\|_2) \\leq 0$, and $(\\|c_{k,i}x_{k,i}\\|_2 - \\|c_{k,j}x_{k,j}\\|_2) (\\|x_{k,i}\\|_2 - \\|x_{k,j}\\|_2) \\geq 0$. Then, the new intra-class distance matrix and the original intra-class distance matrix of class k satisfy $\\Sigma\u2019_\\mathcal{W} < c_k^2\\Sigma_\\mathcal{W}$, where $c_k$ is the scaling factor of the k-th class center."}, {"title": "Experiment", "content": "This article aims to investigate the performance of mainstream PEFT methods in few-shot medical classification tasks, and emphatically validate the effectiveness of our designed EPT method, particularly by comparing the characteristics of VPT and similar PEFT methods to explore their commonalities and capabilities. First, we will introduce datasets, tasks, and experimental settings of our study.\nDatasets We evaluate on the MedFMC2023 grand challenge public classification dataset [23] which covers three different modalities: (1)Thoracic Disease Screening (ChestDR). 2,140 for training, 2,708 for validation, and 3,869 for testing. (2)Pathological Tumor Tissue Classification (Colon). 5,654 for training, 4,355 for validation and 7651 for testing. (3)Endoscopy Images for Lesion Classification (Endo). 1,810 for training, 2,055 for validation and 2936 for testing.\nTasks Each task is formulated as N-Way K-shot classification tasks. Due to the backbone network is pre-trained on ImageNet-21k, which owns a domain gap with medical images, the classification tasks are cross-domain few-shot classification tasks. The few-shot medical classification task, especially the multi-label classification task, particularly highlights the generalization performance of these PEFT methods. We mainly focus on few-shot scenerios rather than fully supervised training, and K is set to 1,5,10. 1-shot refers to every N images that sample from each class, rather than one image. Among the three datasets, ChestDR and Endo are multi-label classification tasks with 19 and 4 categories respectively, while Colon is a single-label classification task with 2 categories.\nTo evaluate the performance of experimental results, we compute the overall accuracy (Acc) for the multi-class classification tasks in the datasets of ColonPath, and the mean average precision (mAP) for the multi-label classification tasks in the datasets of ChestDR and Endo.\nWe compare widely used PEFT methods with our baseline method VPT. To reduce the error caused by random sampling, we follow the setup of MedFMC and performed 5 random samplings. We calculate the average accuracy score for four runs for each sampling.\nBaselines We follow MedFMC and compare EPT with other widely applied PEFT methods. We compare the results of Vision Transformer architecture [44] (ViT), on image classification in Sec 4.2. Due to the fact that the EPT method is also a prompt-based fine-tuning approach, it shares many similarities with VPT and VP. Additionally, given the superior performances of VPT and VP, we primarily compare our results with VPT and VP. Unlike selecting the Full method as the primary reference in [24], our experiments chose the Partial-1[45] fine-tuning method as the primary reference. The reason is that for few-shot fine-tuning, fine-tuning all parameters incurs computational costs that do not match the"}, {"title": "Main Results", "content": "EPT on MedFMC We present average results of various PEFT methods for 1-shot, 5-shot and 10-shot classification on three datasets: Chest, Colon and Endo, as shown in Tab 1. Several main findings can be observed.\nFirst, EPT outperforms other PEFT methods by a substantial margin on three datasets in most cases. Specifically, it surpasses the second-best method Adapter by 2.05% and the third-best method VPT by 2.74%, respectively. EPT still achieves second place in the Colon 5-shot task, with a performance gap of only 0.24 from Bias. This demonstrates that the superior performance of EPT when adapting pre-trained foundation models to cross-domain few-shot scenarios, e.g., medicial image analysis. Second, in the category of prompt tuning, EPT surpasses VPT and VP by 2.74% and 5.09%, respectively. In addition, EPT outperforms LoRA by 5.04%. This not only indicates that EPT has addressed previous shortcomings in the way of introducing prompts, embedding prompts in the channel direction is a more promising approach. Moreover, EPT has better approximation capabilities on Transformer architectures, thereby breaking through limitations of prompt tuning. Third, EPT, VPT and VP surpasses Linear by 6.24%, 3.50%, and 1.15%, respectively, and prompt tuning achieves very competitive ranks among all PEFT methods. This shows that prompt tuning helps to alleviate the negative impact of foundation models in learning pre-training data distributions, and calibrates their performance in new domains. Fourth, Full is 9.34% lower than Linear, potentially due to overfitting. When adapting foundation models to few-shot scenarios, fine-tuning more parameters is not always better, and PEFT methods are very meaningful.\nAnalysis of Feature Distribution As shown in Fig 2, we present visualization results of feature distribution after fine-tuning on the Colon dataset.\nIn Fig 2 (a), it displays the feature distribution of samples from the same class on the first principal component obtained by principal component analysis (PCA). We select 2,628 samples labeled as existing tumours, and four methods were compared: Full, Linear(Pretrained backbones), VPT, and EPT. The x-axis represents feature values, and the y-axis represents the number of samples. Results show that: (1) EPT exhibits a more narrower range of sample distribution, with more pronounced feature values, demonstrating a more concentrated intra-class feature distribution. (2) Foundation models only pre-trained on natural images displays a clear different behavior, and lack continuity, suggesting that samples may cluster into two distinct groups that are significantly far from each other. (3) Full displays that samples are overly dispersed and exhibit lower feature values, which could be due to overfitting and the failure of capturing common features."}, {"title": "Futher Analysis", "content": "In this section, we conduct a more in-depth analysis of VPT and EPT. First, we performed an experiment by setting the prompt grad in VPT to false, which means completing the training with the prompt being frozen and only the linear head being trainable. Additionally, we also conducted a convergence experiment. As for the convergence experiment, we select a specific sampled dataset for training (provided in [23]). For the convergence experiment, we test methods on the default testing set and record the distribution of results, which reflects the convergence performance of the different methods\nSet Prompt Grad False The impact of grads of prompt parameters is shown in Tab 3. The outcome demonstrates that the performance of foundation models still improves in downstream tasks compared to models that are only pre-trained on natural images, even though we freeze the prompt parameters of VPT. This suggests that prompt tuning enhances the feature separability and strengthen the distribution calibration characteristics by introducing additional prompt tokens and increasing the input dimensions."}, {"title": "Conclusion", "content": "In this paper, we propose an effective PEFT method in cross-domain few-shot scenarios, e.g., medical image analysis, namely Embedded Prompt Tuning (EPT). EPT embeds prompt tokens into the expanded channels. In this way, EPT can introduce more useful context, while preserving the original information. Through analyzing patch-wise scaling and feature separation operations from EPT, we also find that prompt tuning is a distribution calibrator, mitigating the negative impact caused by foundation models when learning feature distributions on pre-training data. Experiments show that our EPT can not only achieve superior performance, but also complete the fine-tuning process within competitive time. Considering limitations, there is still significant improvement space for EPT in fine-grained perception tasks, e.g., segmentation. In the future, we will continue to unleash the power of EPT on fine-grained perception tasks, and conduct a further theoretical exploration of the essence of EPT in distribution calibration."}]}