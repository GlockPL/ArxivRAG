{"title": "EMBEDDED PROMPT TUNING: TOWARDS ENHANCED\nCALIBRATION OF PRETRAINED MODELS FOR MEDICAL IMAGES", "authors": ["Wenqiang Zu", "Shenghao Xie", "Qing Zhao", "Guoqi Li", "Lei Ma"], "abstract": "Foundation models pre-trained on large-scale data have been widely witnessed to achieve success\nin various natural imaging downstream tasks. Parameter-efficient fine-tuning (PEFT) methods\naim to adapt foundation models to new domains by updating only a small portion of parameters\nin order to reduce computational overhead. However, the effectiveness of these PEFT methods,\nespecially in cross-domain few-shot scenarios, e.g., medical image analysis, has not been fully\nexplored. In this work, we facilitate the study of the performance of PEFT when adapting foundation\nmodels to medical image classification tasks. Furthermore, to alleviate the limitations of prompt\nintroducing ways and approximation capabilities on Transformer architectures of mainstream prompt\ntuning methods, we propose the Embedded Prompt Tuning (EPT) method by embedding prompt\ntokens into the expanded channels. We also find that there are anomalies in the feature space\ndistribution of foundation models during pre-training process, and prompt tuning can help mitigate\nthis negative impact. To explain this phenomenon, we also introduce a novel perspective to understand\nprompt tuning: Prompt tuning is a distribution calibrator. And we support it by analyzing patch-\nwise scaling and feature separation operations contained in EPT. Our experiments show that EPT\noutperforms several state-of-the-art fine-tuning methods by a significant margin on few-shot medical\nimage classification tasks, and completes the fine-tuning process within highly competitive time,\nindicating EPT is an effective PEFT method. Our code will be released once accepted.", "sections": [{"title": "1 Introduction", "content": "Benifiting from massive training data, foundation models [1] have been widely witnessed to achieve impressive\nperformance in various natural imaging downstream tasks, e.g., classification [2, 3], segmentation [4, 5, 6, 7], and\ndetection [8, 9, 10].\nRecent studies have attempted to explore the potential of foundation models in medical image analysis [11, 12, 13,\n14, 15]. Although foundation models have robust representation and generalization capabilities on natural images,\ntheir adaptabilities are still challenged when facing downstream tasks with a significant domain gap [16]. Therefore,\nfoundation models need to be re-trained on medical datasets to incorporate domain-specific knowledge.\nHowever, due to the huge number of parameters in foundation models, training from scratch would result in significant\ncomputational and memory cost. To address this issue, parameter-efficient fine-tuning (PEFT) [17] methods have been"}, {"title": "2 Related Work", "content": "proposed. The main idea of PEFT is to freeze most of parameters in foundation models, and only fine-tune a small\nnumber of existing or additionally introduced parameters. In medical image analysis, the data avalilability is limited\ndue to several factors, e.g., significant imaging noise [18], high annotation cost [19], privacy policies [20], and rare\ndiseases [21] etc. These factors exacerbate the data-hungry nature of the foundational models [22]. Therefore, there is a\nmore urgent need for research on fine-tuning methods in the field of medical image analysis. To our best knowledge, the\neffectiveness of PEFT in such cross-domain few-shot scenarios has not yet been fully evaluated. Therefore, we launch\nthe first comprehensive benchmark test of PEFT for few-shot medical image classification tasks on MedFMC [23].\nPrompt tuning is a common paradigm of PEFT by introducing extra prompt tokens into embedding tokens of the input\nimage. VPT [24] prepends prompts before images parallelly, while VP [25] adds prompts into images. Intuitively,\nprevious prompt tuning methods have flaws in the way of introducing prompts. VPT cannot adjust each original\ntoken individually and fine-grainedly, whereas VP significantly disrupts the information contained in original tokens.\nMoreover, Wang et al. [26] points that prompt tuning has limited approximation capabilities on Transformer architectures,\nperforming less effectively compared to LoRA [27]. Therefore, we ask: Can we design a new prompt tuning method\nthat effectively enhances the information of original tokens? In this paper, we propose the Embedded Prompt Tuning\n(EPT) method to answer this question. By embedding prompt tokens into expanded channels, EPT can introduce useful\ncontext to optimize input tokens fine-grainedly when maintaining the original information.\nAdditionally, we observe that foundation models pre-trained on natural images, when directly applied to downstream\nmedical image classification tasks, would result in an apparent distance between samples from the same class in the\nfeature space. Furthermore, prompt tuning helps mitigate this negative impact [28, 29], and we wonder: Is prompt\ntuning fundamentally a type of distribution calibrators? To figure it out, we further analyze two micro-operations of\nEPT: patch-wise scaling and feature separation, and provide preliminary theoretical analysis. In our findings, patch-wise\nscaling shorten the intra-class distribution distance by drawing closer the representation of common features among\nsamples from the same class, and feature separation enhances the representation [30] and decoupling capabilites of\nfeatures by expanding the input dimension.\nIn summary, our work proposes a novel PEFT method for cross-domain few-shot learning and medical image analysis.\nWe hope our methods and findings can inspire and facilitate more valuable works in the future. Our contributions can\nbe summarized as follows:\n1. We propose a novel parameter-efficient prompt tuning method, namely EPT, which not only addresses\nshortcomings in prompt introducing ways of previous prompt tuning methods, but also exhibits stronger\napproximation capabilites on Transformer architectures.\n2. We develop a new perspective to understand prompt tuning: Prompt is a distribution calibrator. We also support\nit by analyzing patch-wise scaling and feature separation operations in EPT intuitively and theoretically.\n3. We launch the first comprehensive benchmark evaluation of PEFT for medical image classification tasks on\nMedFMC, offering further inspirations for future research.\n4. Extensive experiments demonstrate that EPT achieves superior performance in cross-domain few-shot sce-\nnarios, e.g., medical image analysis, outperforming several state-of-the-art fine-tuning methods on few-shot\nmedical image classification tasks by a large margin, and completes the fine-tuning process within highly\ncompetitive time."}, {"title": "2.1 Parameter-Efficient Fine-Tuning", "content": "The goal of PEFT is to achieve higher performance by fine-tuning as few parameters as possible when adapting\nfoundation models to downstream tasks. PEFT generally fine-tunes Transformer at three positions: input, backbone,\nand linear head. Since the linear head is essential for downstream tasks, it typically needs to be fine-tuned. Therefore,\nPEFT pays more attention on the input and backbone, and has correspondingly raised two representative methods:\nprompt tuning and adapter tuning.\nPrompt tuning introduces extra tokens into the input image and fine-tunes them. VPT [24] prepends prompt tokens\nbefore embedding tokens parallelly. DVPT [31] employs cross-attention mechanism between prompts and features\nto capture distribution-specific knowledge. VP [25] adds values of prompt tokens and embedding tokens across all\nchannels. EVP [32] primarily focuses on high-frequency components of features. CoOp [33] utilizes learnable text to\naddressing the issue of semantic consistency across different sentences. CoCoOp [34] introduces a Meta-net to generate\nprompts and enhance the generalizability. KgCoOp [35] constructed a regularization term to minize the gap between\ngenerated and crafted prompts."}, {"title": "2.2 PEFT for Medical Image Analysis", "content": "PEFT begins to be adopted when transferring foundational models to medical image analysis scenarios recently[22].\nDVPT [31], as a variant of VPT, explores the potential of prompt tuning in medical image analysis scenarios. Zhang\net al. adapts SAM [4] to medical domains by LoRA. Wu et al. [41] proposes medical sam adapter (Med-SA) to\nincorporate medical knowledge into SAM and designs Hyper-Prompting Adapter (HyP-Adpt) to adapt to different\nprompt conditioning strategies. However, the effectiveness of PEFT has yet to be fully evaluated in cross-domain\nfew-shot scenarios, e.g., medical image analysis. VPPT [42] conducts experiments of prompt tuning in natural imaging\nfew-shot scenarios, but adapter tuning has not been tested and the domain gap is not significant. Dutt et al. [22] tests the\nperformance of adapter tuning in medical image analysis scenarios, but prompt tuning methods do not receive sufficient\nattention. Wang et al. [23] proposed MedFMC benchmark in order to promote the research of PEFT in adapting\nfoundation models to medical domains. In this paper, we conduct the first comprehensive effectiveness evaluation of\nPEFT in cross-domain few-shot scenarios, e.g., medical image analysis."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminaries", "content": "Notations and Organization Leto be the softmax operators. A bold lower case character, eg x, denotes a vector. A\nbold upper case character, e.g. W, denotes a matrix while W_i,j, W_i,: and W_:,j is the (i, j) th element, i-th row, j-th\ncolumn, respectively. We use ReLU(v) = max(v, 0) to denote the ReLU activation function where max(\u00b7) function\nis applied entry wise to a vector."}, {"title": "3.1.1 Transformers", "content": "For descriptive purposes, we use single head attention to illustrate. Moreover, we use X \u2208 R^{d\\times n} to describe the\ninput shape in the subsequent explanations, instead of X \u2208 R^{n\\times d} depicted in Fig 1. We use Q, K, and V to denote\nthe multiplication of W_q \u2208 R^{d\\times d}, W_k \u2208 R^{d\\times d}, W_v \u2208 R^{d\\times d}, and X \u2208 R^{d\\times n}, where d represents the embedding\ndimensions, and n represents the number of patches. Therefore, we have:\nQ = W_qX, K = W_kX, V = W_vX\nThen, a self-attention operation of input token sequence X \u2208 R^{d\\times n} can be represented as\nAtt(X) = V\\sigma(Q K^T) \\qquad (1)\nThe normalizing factor of \\frac{1}{\\sqrt{d_{kq}}} is subsumed in the weight matrices W_k for notational simplicity. And we omitted the\nexpression of multihead.\nWith (1) and (2), a standard Transformer layer + can be represented as\nMLP(X) = [W_2 ReLU (W_1X_{:,1} + b_1) + b_2 + X_{:,1},\n. . , W_2 ReLU (W_1X_{:,n} + b_1) + b_2 + X_{:n}] \\qquad (3)\nT(X) = MLP(Att(X) + X)."}, {"title": "3.1.2 Visual Prompt Tuning", "content": "Visual prompts are learnable parameters prepended to other tokens before a Transformer encoder layer. These visual\nprompts P \u2208 R^{d\\times n_p} have the same dimension as other tokens, where n_p is the prompt length. Then the Transformer\nlayers are represented as\n[Z^1, X^1] = \\tau^1 ([P, X^0])\n[Z^i, X^i] = \\tau^i ([Z^{i-1}, X^{i-1}]) i = 2, 3, . . . , N \\qquad (4)\ny = Head (X^N_{:,0}),\nwhere Z^i \u2208 R^{d\\times n_p} represents the prompts computed by the i-th Transformer layer \\tau^i, and X^N_{:,0} represents the CLS\ntoken."}, {"title": "3.2 Embedded Prompt Tuning", "content": "VPT prepends prompt tokens before all original input tokens parallely, while VP adds the values of prompt tokens with\noriginal input tokens across all channels. These prompt introducing ways either do not fine-tune the original input\ntokens fine-grainedly or significantly disrupt the information of original input tokens. Therefore, we propose Embedded\nPrompt Tuning (EPT), embedding prompt tokens into expanded channels. In this way, EPT can not only preserve\noriginal information well, but also introduce extra useful context to optimize embedding tokens.\nAs shown in Fig 1, the concept of our proposed EPT method and its differences compared to VPT are illustrated. We use\nthe EPT prompts P_E to differentiate P. P_E \u2208 R^{d_p\\times n} have a completely different shape from the prompts P \u2208 R^{d\\times n_p}\nin VPT, where d_p represents the prompt length. It should be noted that the prompts P_E are not directly inserted into the\npatch tokens but only appear during the softmax operation on K^TQ in the attention calculation, and they disappear\nafter the computation is completed. The purpose of this approach is to scale K^TQ without altering its shape, enabling\nsubsequent computations."}, {"title": "3.3 Analysis of EPT", "content": ""}, {"title": "3.3.1 Intuition", "content": "In the context of few-shot learning, researchers face the challenge of overfitting due to an insufficient number of training\nsamples, which is characterized by accurate classification on the training set but poor performance on the test set. To\ntackle this challenge, we attempt to approach it from the following perspective: calibrating the distribution of the\nfew-shot samples by reducing the intra-class distance of samples from the same class, thereby increasing the degree of\nseparation between features of different classes during training, ultimately achieving higher accuracy [30].\nBased on our observations, EPT can help mitigate the negative impact caused by foundation models when learning\npre-trained data distribution. Additionally, we notice that the patch-wise scaling operation, by implementing varying\nscaling ratios for different samples, facilitates the aggregation of features from the same class towards the intra-class\ncenter. Consequently, it increases the potential for greater separation of features between samples from different classes.\nMoreover, by introducing prompts, the dimensions of features are increased, thereby enhancing the high-dimensional\nexpression and decoupling capability of features. Through analyzing these two operations intuitively and theoretically,\nwe propose a new perspective to understand prompt tuning: Prompt is a distribution calibrator."}, {"title": "3.3.2 Interpretation", "content": "Therefore, first, we define the intra-class distance for a better understanding of the problem. For a dataset X =\n{x_{k,i}, k \u2208 [K], i \u2208 [n_k]} with K classes, where x_{k,i} \u2208 R^d is the i-th sample in the k-th class,and the number of samples\nn_k in each class is balanced with n_1 = ... = n_K = n. We borrowed the definition from [43], and we use \\bar{x}_k to\nrepresent the center of the samples in class k, where the intra-class distance matrix is defined as\n\\Sigma_W = \\sum_{k=1}^K \\sum_{i=1}^n \\frac{1}{N} (x_{k,i} - \\bar{x}_k)(x_{k,i} - \\bar{x}_k)^T \\qquad (11)\nAccording to the definition, a smaller value of \\Sigma_W indicates a higher degree of feature clustering, which may lead to\nbetter separability.\nFor dataset that satisfies X = {x_{k,i} \u2265 0, k \u2208 [K], i \u2208 [n_k]}, If we apply different scaling operations to samples of the\nsame class, we can derive the conclusion stated in Lemma 1.\nLemma 1 Let c_{k,i}, c_{k,j} \u2208 [0, 1] be the scaling operations on the samples x_{k,i} and x_{k,j} of class k, respectively, and\nsatisfy (c_{k,i} \u2013 c_{k,j}) (||x_{k,i}||_2 - ||x_{k,j}||_2) \u2264 0, and (||c_{k,i}x_{k,i}||_2 - ||c_{k,j}x_{k,j}||_2) (||x_{k,i}||_2 - ||x_{k,j}||_2) \u2265 0. Then, the\nnew intra-class distance matrix and the original intra-class distance matrix of class k satisfy \\Sigma'_W < c_k^2\\Sigma_W, where c_k\nis the scaling factor of the k-th class center.\nProof of Lemma 1 is presented in Appendix. Lemma 1 demonstrates that a scaling operation c_{k, i} that induces intra-class\nclustering more effectively relative to applying the same scaling factor to all samples within a class. The actual effect is\nthat the farther the sample x_{k,i} is from the origin, the smaller the scaling factor it has, and vice versa. The advantage is\nthat it can bring the samples closer to the intra-class center.\nFurthermore, we can observe that by embedding prompts in the softmax function in EPT, we can achieve the scaling\nfunctionality defined in Lemma 1, thereby increasing the intra-class concentration. Therefore, we conducted an analysis\nof the mechanism behind EPT. Considering the complex structure of the Transformer and the dataset, to simplify the\nproblem, we assume the number of patches (including the CLS token) is 2 and proceed with the analysis based on\nthis assumption.\nLet u_1 and u_2 be the first column vectors of the K^TQ matrix for two different samples X_{k,1} \u2208 R^{d\\times n} and X_{k,2} \u2208 R^{d\\times n}\nwithin the same class k, which denote the CLS token, and n=2. Let u_{p,1} and u_{p,2} correspond to the vectors after the\nprompt. Then, we can draw the following conclusion.\nProposition 1 Consider an original matrix \\sigma (K^TQ) and a prompted matrix \\sigma \\left(\\begin{bmatrix}\nK^TQ \\\nP_E\n\\end{bmatrix}\\right), We can always find\na prompt P_E \u2208 R^{d_p\\times n} such that, for the CLS tokens of X_{k,1} \u2208 R^{d\\times n} and X_{k,2} \u2208 R^{d\\times n}, when ||\\sigma (u_1)||_2 \u2264 ||\\sigma (u_2) ||_2,\nthen \\frac{||\\sigma_p(u_{p,1})_{d_p:}\\||_2}{||\\sigma(u_1)||_2} >  \\frac{||\\sigma_p(u_{p,2})_{d_p:}\\||_2}{||\\sigma(u_2)||_2}"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experiment Setup", "content": "This article aims to investigate the performance of mainstream PEFT methods in few-shot medical classification tasks,\nand emphatically validate the effectiveness of our designed EPT method, particularly by comparing the characteristics\nof VPT and similar PEFT methods to explore their commonalities and capabilities. First, we will introduce datasets,\ntasks, and experimental settings of our study.\nDatasets We evaluate on the MedFMC2023 grand challenge public classification dataset [23] which covers three\ndifferent modalities: (1)Thoracic Disease Screening (ChestDR). 2,140 for training, 2,708 for validation, and 3,869 for\ntesting. (2)Pathological Tumor Tissue Classification (Colon). 5,654 for training, 4,355 for validation and 7651 for\ntesting. (3)Endoscopy Images for Lesion Classification (Endo). 1,810 for training, 2,055 for validation and 2936 for\ntesting.\nTasks Each task is formulated as N-Way K-shot classification tasks. Due to the backbone network is pre-trained\non ImageNet-21k, which owns a domain gap with medical images, the classification tasks are cross-domain few-shot\nclassification tasks. The few-shot medical classification task, especially the multi-label classification task, particularly\nhighlights the generalization performance of these PEFT methods. We mainly focus on few-shot scenerios rather than\nfully supervised training, and K is set to 1,5,10. 1-shot refers to every N images that sample from each class, rather than\none image. Among the three datasets, ChestDR and Endo are multi-label classification tasks with 19 and 4 categories\nrespectively, while Colon is a single-label classification task with 2 categories.\nTo evaluate the performance of experimental results, we compute the overall accuracy (Acc) for the multi-class\nclassification tasks in the datasets of ColonPath, and the mean average precision (mAP) for the multi-label classification\ntasks in the datasets of ChestDR and Endo.\nWe compare widely used PEFT methods with our baseline method VPT. To reduce the error caused by random sampling,\nwe follow the setup of MedFMC and performed 5 random samplings. We calculate the average accuracy score for four\nruns for each sampling.\nBaselines We follow MedFMC and compare EPT with other widely applied PEFT methods. We compare the results\nof Vision Transformer architecture [44] (ViT), on image classification in Sec 4.2. Due to the fact that the EPT method is\nalso a prompt-based fine-tuning approach, it shares many similarities with VPT and VP. Additionally, given the superior\nperformances of VPT and VP, we primarily compare our results with VPT and VP. Unlike selecting the Full method\nas the primary reference in [24], our experiments chose the Partial-1[45] fine-tuning method as the primary reference.\nThe reason is that for few-shot fine-tuning, fine-tuning all parameters incurs computational costs that do not match the"}, {"title": "4.2 Main Results", "content": "EPT on MedFMC We present average results of various PEFT methods for 1-shot, 5-shot and 10-shot classification\non three datasets: Chest, Colon and Endo, as shown in Tab 1. Several main findings can be observed.\nFirst, EPT outperforms other PEFT methods by a substantial margin on three datasets in most cases. Specifically, it\nsurpasses the second-best method Adapter by 2.05% and the third-best method VPT by 2.74%, respectively. EPT still\nachieves second place in the Colon 5-shot task, with a performance gap of only 0.24 from Bias. This demonstrates that\nthe superior performance of EPT when adapting pre-trained foundation models to cross-domain few-shot scenarios, e.g.,\nmedicial image analysis. Second, in the category of prompt tuning, EPT surpasses VPT and VP by 2.74% and 5.09%,\nrespectively. In addition, EPT outperforms LoRA by 5.04%. This not only indicates that EPT has addressed previous\nshortcomings in the way of introducing prompts, embedding prompts in the channel direction is a more promising\napproach. Moreover, EPT has better approximation capabilities on Transformer architectures, thereby breaking through\nlimitations of prompt tuning. Third, EPT, VPT and VP surpasses Linear by 6.24%, 3.50%, and 1.15%, respectively,\nand prompt tuning achieves very competitive ranks among all PEFT methods. This shows that prompt tuning helps\nto alleviate the negative impact of foundation models in learning pre-training data distributions, and calibrates their\nperformance in new domains. Fourth, Full is 9.34% lower than Linear, potentially due to overfitting. When adapting\nfoundation models to few-shot scenarios, fine-tuning more parameters is not always better, and PEFT methods are very\nmeaningful.\nAnalysis of Feature Distribution As shown in Fig 2, we present visualization results of feature distribution after\nfine-tuning on the Colon dataset.\nIn Fig 2 (a), it displays the feature distribution of samples from the same class on the first principal component obtained\nby principal component analysis (PCA). We select 2,628 samples labeled as existing tumours, and four methods were\ncompared: Full, Linear(Pretrained backbones), VPT, and EPT. The x-axis represents feature values, and the y-axis\nrepresents the number of samples. Results show that: (1) EPT exhibits a more narrower range of sample distribution,\nwith more pronounced feature values, demonstrating a more concentrated intra-class feature distribution. (2) Foundation\nmodels only pre-trained on natural images displays a clear different behavior, and lack continuity, suggesting that\nsamples may cluster into two distinct groups that are significantly far from each other. (3) Full displays that samples\nare overly dispersed and exhibit lower feature values, which could be due to overfitting and the failure of capturing\ncommon features."}, {"title": "4.3 Abalation Study", "content": "In this section, EPT is consistently evaluated using the default experimental settings: ViT-Base/16 as the backbone.\nExcept for the ablation study on prompt length, the prompt length for prompt-based fine-tuning methods is set to achieve\nthe best performance individually."}, {"title": "EPT on Different Data Scale", "content": "We vary the training data from 1, 5, 10-shot to 20% of the entire training set. The\naverage accuracy of different fine-tuning methods on various data scales is presented in Fig 3. First, we observe that our\nmethod consistently outperforms others across all data scales, displaying superior data scalability. Moreover, as the data\nscale increases, all fine-tuning methods exhibit a significant increase in accuracy, but their performance growth rate tend\nto slow down. Further observation reveals that the performance on Colon converges faster than on Chest and Endo.\nThis might be because that multi-label tasks require more data to learn sufficient discriminative information."}, {"title": "Prompt Length", "content": "The average accuracy of different prompt tuning methods on various prompt length is presented in\nFig 4. We vary prompt length from 1, 5, 10, 20, 50, 100 to 150. Since EPT has a different way of introducing prompts\nfrom VPT, the number of prompt parameters from VPT is four times that of EPT when introducing a prompt with\nlength of 1 in a single Transformer layer. Therefore, to maintain relatively consistent number of parameters, we use\nrelative prompt length on the x-axis. This means when the relative prompt length is 1, the actual prompt length of VPT\nis 1, and the actual prompt length of EPT is 4.\nAs the prompt length increases, the performance of VPT generally declines across three datasets. In contrast, EPT\nshows a clear upward trend on Colon and Endo, and remains relatively stable on Chest. This suggests that EPT can\nbetter address overfitting issues. Moreover, when the prompt length is only 1, the overall performance of EPT and VPT\nis nearly identical. As the prompt length and the number of prompt parameters increase, yet remain within the scope\nof PEFT, EPT gradully begins to outperform VPT by a large margin. This indicates that EPT has better parameter\nscalability, with a much higher upper limit on approximation capabilities compared to VPT. We can also observe that"}, {"title": "5 Conclusion", "content": "In this paper, we propose an effective PEFT method in cross-domain few-shot scenarios, e.g., medical image analysis,\nnamely Embedded Prompt Tuning (EPT). EPT embeds prompt tokens into the expanded channels. In this way, EPT\ncan introduce more useful context, while preserving the original information. Through analyzing patch-wise scaling\nand feature separation operations from EPT, we also find that prompt tuning is a distribution calibrator, mitigating the\nnegative impact caused by foundation models when learning feature distributions on pre-training data. Experiments show\nthat our EPT can not only achieve superior performance, but also complete the fine-tuning process within competitive"}]}