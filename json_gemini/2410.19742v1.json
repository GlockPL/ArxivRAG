{"title": "SALINA: Towards Sustainable Live Sonar Analytics in Wild Ecosystems", "authors": ["Chi Xu", "Rongsheng Qian", "Hao Fang", "Xiaoqiang Ma", "William I. Atlas", "Jiangchuan Liu", "Mark A. Spoljaric"], "abstract": "Sonar radar captures visual representations of underwater objects and structures using sound wave reflections, making it essential for exploration, mapping, and continuous surveillance in wild ecosystems. Real-time analysis of sonar data is crucial for time-sensitive applications, including environmental anomaly detection and in-season fishery management, where rapid decision-making is needed. However, the lack of both relevant datasets and pre-trained DNN models, coupled with resource limitations in wild environments, hinders the effective deployment and continuous operation of live sonar analytics.\nWe present SALINA, a sustainable live sonar analytics system designed to address these challenges. SALINA enables real-time processing of acoustic sonar data with spatial and temporal adaptations, and features energy-efficient operation through a robust energy management module. Deployed for six months at two inland rivers in British Columbia, Canada, SALINA provided continuous 24/7 underwater monitoring, supporting fishery stewardship and wildlife restoration efforts. Through extensive real-world testing, SALINA demonstrated an up to 9.5% improvement in average precision and a 10.1% increase in tracking metrics. The energy management module successfully handled extreme weather, preventing outages and reducing contingency costs. These results offer valuable insights for long-term deployment of acoustic data systems in the wild.", "sections": [{"title": "1 Introduction", "content": "Sonar radar employs sound wave reflections to visualize objects and structures within its detection range. This technology proves invaluable across a wide range of sensing applications, including underwater exploration and mapping, continuous surveillance, and studying marine life [3, 15]. Typical analytical tasks include object detection [34], counting [17, 22, 32], and trajectory tracking [7, 14]. With advancements in sonar technology and AI-empowered data processing, the demand for real-time sonar data analysis has significantly increased, particularly in time-sensitive applications such as environmental anomaly detection and in-season fishery management. In these contexts, timely and accurate sonar analytics can greatly enhance fast response efforts, enabling more effective decision-making.\nDespite decades of research, achieving live and accurate sonar analytics remains challenging due to several factors. Sonar radars are often deployed in environments where optical cameras are ineffective due to low or zero visibility, such as underwater. As shown in Figure 1, objects such as fish, wild animals, and intruders frequently appear small and blurry in sonar frames, complicating typical analytics tasks [1]. Additionally, sonar data typically exhibit limited diversity and suffer from significant noise, with insufficient texture details necessary for effectively training and fine-tuning deep neural networks (DNNs) [19].\nAnother challenge is balancing data fidelity and freshness in live sonar analytics [36, 37, 39]. Sonar data is rich in information by nature. Unlike modern embedded cameras with built-in processing capabilities [42], off-the-shelf sonar radars generate raw echo frames and basic sensory data, requiring reliable edge infrastructure for further processing. Without efficient handling, this results in substantial overhead and degrades overall analytics performance.\nIn addition, unlike urban surveillance scenarios with robust network infrastructure, sonar radar is often deployed in wild ecosystems with limited network coverage and energy resources. Field studies in North America showed that individuals and agencies rely on satellite internet providers, such as SpaceX's Starlink [6, 44], for accessing computing power to process streamed sonar data. However, Starlink's peak power consumption of up to 200 Watts is double that of the sonar radar system's 100 Watts. In these areas, sustainable energy sources such as solar power are typically the only viable option. As shown in Figure 2, the temperate forest environment exacerbates the challenge, as changing weather conditions, sudden storms, and cloud cover significantly impact energy availability. Therefore, unique energy and network characteristics must be considered when designing sustainable and enduring solutions for live sonar analytics.\nIn this work, we pose the following question: \"Can a live sonar streaming and analytics system be developed to operate effectively under the extreme constraints of wild ecosystems?\" We anticipate that an affirmative answer will offer insights for the long-term operation of similar systems on processing generalized acoustic data.\nTo address this question, we developed SALINA for Sustainable LIve soNar Analytics, in close collaboration with a multidisciplinary team of biologists, forest technologists, and electricians. SALINA's novelty lies in three key aspects: sonar data channel population, DNN model enhancement with spatial-temporal feature adaptations, and sustainable sonar streaming and energy planning. The sonar data channel population ensures robust data perception of both human and machine intelligence, while the DNN model enhancement incorporates spatial-temporal feature adaptations to further improve analytics performance in challenging conditions. Additionally, edge-cloud collaboration is leveraged during inference to balance accuracy and power efficiency. Furthermore, the energy planning module is designed to handle volatile weather patterns, such as sudden storms and fluctuating cloud cover, ensuring continuous operation while maintaining resource efficiency in rapidly changing environments.\nSALINA was deployed for six months at two inland river sites in British Columbia, Canada, located deep within temperate forests and operating entirely off-grid, relying solely on sustainable solar power. It provided continuous 24/7 underwater monitoring to support the strategic planning of the First Nations' fishery stewardship and wildlife restoration efforts. The analytics results also contributed to biological research on tracking North American Atlantic salmon. Extensive real-world experiments showed SALINA's superiority, with up to 9.5% improvement in average precision and 10.1% improvements in tracking metrics when monitoring underwater objects. The energy planning module effectively schedules energy usage, preventing system outages due to extreme weather conditions and saving considerable contingency costs. Our contributions can be summarized as follows.\n\u2022 We developed the first known system for real-time processing and analysis of acoustic sonar data, incorporating edge and cloud collaboration for live analytics in wild ecosystems.\n\u2022 We addressed unique challenges in sonar analytics by constructing a novel channel population pipeline to resolve issues such as acoustic shadow and reverberation, improving detection and tracking accuracy.\n\u2022 Through reconstructing data channels and finetuning pre-trained DNN models, we observed improved detection and tracking performance, compared to state-of-the-art methods [15] on three benchmarking datasets.\n\u2022 SALINA's sustainable sonar streaming and energy planning module is explicitly designed to withstand volatile weather patterns, such as sudden storms and fluctuating cloud cover, ensuring continuous operation and resource efficiency in rapidly changing environments.\n\u2022 The sonar dataset used in this study has been organized and released for community use, with the potential to generate new insights and discoveries that benefit society.\nThe remainder of this paper is outlined as follows:\nSection 2 presents comprehensive backgrounds and research motivations, discussing the sonar analytics characteristics and challenges; Section 3 introduces the SALINA architecture and its design overview; Section 4 details the data preprocessing and wrangling techniques; Section 5 describes the DNN model adaptations for both on-premise and cloud inference; Section 6 explains the sonar streaming and energy planning design; Section 7 provides system evaluations; Section 8 offers further discussion and Section 9 concludes the paper."}, {"title": "2 Background and Motivations", "content": "In this section, we introduce the background, opportunities, and challenges in developing live sonar analytics in wild ecosystems."}, {"title": "2.1 Sonar Frame Capture", "content": "In this work, we focus primarily on data analytics of multi-beam sonar. In contrast to passive sonar and side scan sonar [38], multi-beam sonar works by emitting multiple beams of sound waves from a transducer, which is mounted on a fixed rack or towed behind a boat. Each sonar frame visualizes the reflected intensity of these beams, providing clear underwater imagery even in low-visibility conditions. This enables effective detection of aquatic life, mapping of structures, and monitoring of submerged assets, making it suitable for real-time underwater exploration and analysis. As shown in Figures 1b and 3, the frames are typically presented in grayscale or false-color, where lighter areas indicate stronger reflections or denser objects, and darker areas represent weaker reflections or softer, less dense materials."}, {"title": "2.2 Characteristics of Live Sonar Analytics", "content": "Emerging demand for real-time feedback. Advancements in sonar technology and AI-driven data processing have created a growing need for real-time feedback in applications such as environmental anomaly detection and in-season fishery management, where timely insights are crucial for effective decision-making. For example, during the salmon return season, real-time sonar monitoring enables fishery programs to track populations, assess environmental impacts, and adjust operations promptly, ensuring compliance with regulations and sustainable harvests. Without live sonar data, decisions would rely on delayed or less accurate information, potentially leading to poor management and environmental risks. Despite these benefits, achieving both high data fidelity and freshness in live sonar analytics remains challenging [36, 37, 39]. Beyond fishery management, live sonar analytics can support intrusion detection and disaster management, providing precise and timely insights to adapt to diverse scenarios and dynamics.\nLack of quality dataset and pre-trained models. Compared to conventional computer vision detection and tracking (D&T) tasks, identifying and continuously tracing objects in sonar data is inherently challenging. Objects in real-world scenarios, such as fish, wild animals, and intruders, often appear small and blurry in echo frames [1]. In addition, underwater datasets typically have limited diversity, and real-world samples are accompanied by heterogeneous noise and lack texture details [19], making it difficult to train and fine-tune DNN models for D&T tasks. Furthermore, there are only a few publicly available datasets for pre-training, partly due to the limited number of research teams focusing on this area.\nNoises cause detection loss. Compared with environmental noise, gaussian noise, and motion blur which are commonly seen and handled in conventional video analytics systems, speckle noise is a type of granular noise that frequently appears in images or signals acquired through coherent imaging systems such as sonar radar. It arises due to interference between coherent wavefronts and varies with the relative positions of the object and sensor. As shown in Figure 3, it manifests as random fluctuations in brightness or intensity, creating a grainy or speckled appearance. In sonar analytics, such noises can adversely affect D&T tasks, leading to a loss of accuracy or degraded performance.\nAcoustic shadows and reverberations. Additionally, acoustic shadows and reverberations caused by sound waves can further complicate object observation by obscuring object presence. For example, when placed underwater, the interaction of sound waves bouncing back and forth between the river bottom and the water surface before reaching the transducer generates multiple shifting bottom images [15]. Consequently, this type of echo may produce multiple images of individual objects, appearing offset from each other, often referred to as \"ghost objects\". A typical example is presented in Figure 3, showing a moving acoustic shadow and ghost objects caused by a Coho salmon in sonar frames.\nFurthermore, we also illustrate the form of acoustic shadows in Figure 4. In this figure, the objects closer to the sonar radar have larger acoustic shadows in the sonar frame, while those farther from the radar and closer to the bottom have smaller shadows. Such acoustic shadows can potentially interfere with the detection and tracking of other objects. Note that the presence and intensity of these ghost objects can vary based on factors such as the river's shape, sonar radar orientation, and water level. These factors determine how much these visual anomalies distort the perceived location and size of objects in the sonar frame, potentially leading to inaccurate object detection or misclassification. To improve monitoring and research accuracy, it is crucial to quantify the impact of these acoustic properties and develop methods to mitigate their effects on visual data interpretation."}, {"title": "2.3 Deployment Challenges", "content": "Limited compute power and energy constraints at the first mile. Sonar data contains a vast amount of information that needs to be processed in real time. In contrast to modern embedded IP cameras that equip powerful processing units, off-the-shelf sonar radar only outputs raw echo frames and other basic sensory status data. This necessitates a reliable edge infrastructure to handle data pre-processing and wrangling. Possible edge designs include deploying lightweight models on low-power devices, offloading computationally intensive tasks to cloud servers, and employing data compression techniques to reduce transmission costs. Without these optimizations, processing overheads can degrade overall system performance. Note in wild environments, sustainable energy sources such as solar power are typically the only viable option. The changing weather conditions, sudden storms, and cloud cover significantly impact energy availability, posing challenges in continuous operation and maintenance.\nThin network coverage in the wild. Unlike urban surveillance scenarios [12, 43], which benefit from well-established network infrastructure, sonar radar is often deployed in wild environments with limited network coverage [25]. Geographic factors such as mountains, dense forests, or other natural obstacles can obstruct network signal transmission, further reducing coverage. In these areas, individuals and agencies often rely on satellite internet providers, such as SpaceX's Starlink, for connectivity. Therefore, these unique network constraints must be carefully considered when designing and developing a live sonar analytics system."}, {"title": "3 SALINA Overview", "content": "Considering the unique characteristics and challenges in sonar analytics, we present SALINA, a sustainable live sonar analytics system. The major modules of SALINA are presented in Figure 5, which we will detail further."}, {"title": "3.1 Data Preprocessing and Wrangling", "content": "In this work, we employ advanced data preprocessing and wrangling techniques to enhance live sonar analytics. By converting acoustic intensity measurements to a grayscale frame, we facilitate the use of various image processing methods to populate data channels for deep neural network (DNN) models to work on downstream tasks. For example, for real-time background subtraction, we utilize the Mixture of Gaussians (MOG) method [33, 47], which models the background using multiple Gaussian distributions per pixel. This approach adapts dynamically to scene changes, ensuring efficiency and robustness.\nAdditionally, we propose a novel hedging design to utilize background removal data by creating three-channel outputs compatible with conventional RGB formats. The first channel retains the original sonar frame, while the second and third channels apply guided filtering [11] with different guidance images. This strategy improves the differentiation of noises and acoustic shadows, enhancing the performance of detection and tracking tasks. By integrating these preprocessing steps, our methodology ensures real-time processing capabilities and significantly improves the accuracy of downstream analytics. Further details are presented in Section 4."}, {"title": "3.2 Sonar Dataset Preparation", "content": "Due to the scarcity of public underwater sonar datasets, we developed custom datasets for training, fine-tuning, and benchmarking purposes. In sonar analytics scenarios, visual similarities between objects make it challenging to distinguish them based on appearance alone, necessitating the use of motion and behavioral patterns for effective tracking. This complexity is compounded by factors such as intricate backgrounds, inconsistent frame-to-frame visibility, and varying numbers of objects in the scene - issues that are less common in controlled laboratory tracking studies. To address these challenges, we hired a third-party annotation service to collect multiple object-tracking annotations for all objects in the converted sonar clips in grayscale. The objects include salmon, otter, and smolt. Annotators were provided with the sonar frames and instructed to tightly box all visible objects of interest using v7 software [18]."}, {"title": "3.3 DNN Model Adaptations", "content": "Section 5 explores DNN model adaptations for both on-premise and cloud inference to generate sonar analytics results. The need for both on-premise and cloud inference arises from the distinct advantages each offers. On-premise inference offers edge-only processing with immediate feedback, a low energy profile, and resilience to occasional network outages. In contrast, cloud inference provides greater computational power, enabling complex and resource-intensive analyses with higher accuracy, assuming stable energy and network are available for data transmission to the cloud. Leveraging both on-premise and cloud inference also addresses the heterogeneous settings of different sonar sites, where device capabilities, energy availability, and network conditions vary significantly in the wild.\nWe adopt Convolutional Neural Networks (CNN) for on-premise inference, leveraging their modularity and anchor-free architecture to reduce false positives in sonar data. For cloud inference, we adapt the deformable detection transformer (DETR) [46] as our still sonar frame detector. We simplify its design by excluding multi-scale feature representations and concentrating on the last stage of the backbone. The modified detector for sonar frames incorporates a spatial transformer encoder and decoder, which encode each frame into spatial object queries and memory encodings. Additionally, we introduce a temporal transformer to enhance object detection by leveraging temporal information between adjacent sonar frames, performing co-attention between online queries and temporally aggregated features. We refer to these adaptations for sonar data as the Spatial-Temporal Sonar Vision Transformer (STSVT)."}, {"title": "3.4 Sustainable Sonar Streaming", "content": "In Section 6, we introduce a joint design for sonar streaming and energy planning. This integration is necessary because supporting cloud inference for better analytics accuracy requires sonar streaming over satellite networks, which consumes the majority of power in the entire system. Our design considers both weather-affected Starlink connectivity and solar photovoltaic (PV) energy production. We observe that Starlink's throughput is significantly affected by precipitation, with an average 15% drop during precipitation and notable reductions during heavy rain due to rain attenuation affecting Ka- and Ku-band radio waves. Similarly, solar PV energy production is highly sensitive to weather conditions. We leverage a modified U-Net architecture for short-term solar PV output forecast, incorporating residual blocks and pruning to enhance efficiency. Our multi-stratum streaming optimization integrates satellite communication performance metrics and PV energy forecasts to dynamically adjust streaming configurations and data rates, ensuring Pareto-optimality even in volatile weather conditions."}, {"title": "4 Sonar Data Preprocessing and Wrangling", "content": "As a default approach in sonar analytics, converting acoustic intensity measurements to a grayscale frame facilitates the use of image processing and computer vision techniques. This conversion results in a single-channel image representation, where each pixel value corresponds to the intensity of the sonar return at that location. Such a representation is essential for leveraging existing DNN-based detection and tracking models, which are typically designed for processing visual data.\nA common approach for preprocessing these grayscale sonar images involves the application of Gaussian blur [8]. Gaussian blur smooths the image by reducing high-frequency noise, which is often prevalent in sonar data due to various environmental and sensor-related factors. This process helps in enhancing the detection of significant features and objects by suppressing noise that might otherwise lead to performance degradation in downstream tasks.\nIn the context of background subtraction, state-of-the-art approaches [15] involve computing the average of all frames in the sonar clips and subtracting this average from each frame. This approach, known as frame differencing with mean subtraction, effectively highlights moving objects while suppressing static background elements. However, this method has notable limitations when applied to real-time settings. The requirement to maintain and process the entire sonar clips to compute the average frame is computationally intensive and may not be feasible in scenarios where real-time processing is essential.\nIn our work, we propose several real-time steps to address these challenges. First, we use the Mixture of Gaussians (MOG) method [33] to differentiate between foreground and background in sonar frames. MOG is computationally efficient, dynamically updates with each frame, and can handle speckle noise, reverberations, and acoustic shadows by modeling each pixel as a mixture of Gaussian distributions. This allows it to separate random fluctuations from the stable background and adapt to the varying patterns of reverberations and shadows, minimizing their impact on detection performance. Mathematically, the MOG model is formulated as follows:\n$p(x) = \\sum_{k=1}^{K} \\pi_{k}N(x | \\mu_{k}, \\Sigma_{k})$\nwhere p(x) is the probability of a pixel value x, K is the number of Gaussian components, $\\pi_{k}$ are the mixing coefficients with $\\sum_{k=1}^{K} \\pi_{k} = 1$ and $\\pi_{k} \\geq 0$, and $N(x | \\mu_{k}, \\Sigma_{k})$ represents the Gaussian distribution with mean $\u00b5_{k}$ and covariance $\\Sigma_{k}$. The parameters $\\mu_{k}$, $\\Sigma_{k}$, and $\\pi_{k}$ are continuously updated based on incoming pixel values, allowing the model to adapt to new background conditions dynamically. The update steps for the parameters are as follows:\n1. Mean (\u00b5):\n$\\mu_{k}^{(t+1)} = (1 - \\alpha) \\mu_{k}^{(t)} + \\alpha x_{t}$\n2. Covariance ($\\Sigma_{k}$):\n$\\Sigma_{k}^{(t+1)} = (1 - \\alpha) \\Sigma_{k}^{(t)} + \\alpha (x_{t} - \\mu_{k}^{(t+1)}) (x_{t} - \\mu_{k}^{(t+1)})^{T}$\n3. Weight ($\\pi_{k}$):\n$\\pi_{k}^{(t+1)} = (1 - \\alpha) \\pi_{k}^{(t)} + \\alpha M_{k}$"}, {"title": "4.2 Hedging Design in Channel Population", "content": "Furthermore, we propose a novel hedging design to effectively utilize background removal data from MOG outputs for sonar analytics. Leveraging original sonar frames and MOG outputs, this design populates three data channels to fit the RGB format used by existing DNN detection models. We show an illustration example in Figure 6. The first channel (Figure 6a) is the original sonar frame, preserving the raw intensity data. The second and third channels (Figures 6b and 6c) undergo guided filtering, but with different guiding images, to enhance the differentiation between noises and acoustic shadows.\nThe guided filter [11] is an edge-preserving smoothing technique that leverages a guidance image to perform filtering. Mathematically, the guided filter can be defined as follows: Given an input image I and a guidance image G, the output image Q is computed by minimizing the following cost function for each window $w_{j}$ centered at pixel j:\n$E(a_{j}, b_{j}) = \\sum_{i \\in w_{j}} ((I_{i} - (a_{j}G_{i} + b_{j}))^{2} + \\epsilon a^{2})$\nwhere $a_{j}$ and $b_{j}$ are the linear coefficients that fit the guidance image G to the input image I within the window $w_{j}$, and $\u03f5$ is a regularization parameter to prevent overfitting.\nThe solution to this minimization problem yields the coefficients:\n$a_{j} = \\frac{[\\sum_{i \\in w_{j}} G_{i}I_{i} - \\mu_{j}K_{j}]}{\\sigma_{j}^{2} + \\epsilon}$\n$b_{j} = I_{j} - a_{j}\\mu_{j}$\nwhere $\\mu_{j}$ and $\\sigma_{j}^{2}$ are the mean and variance of G in the window $w_{j}$, and $I_{j}$ is the mean of I in the same window. The output image Q is then computed as:\n$Q_{i} = a_{j}G_{i} + b_{j}$\nIn our design, the second channel uses the original sonar frame as the input image I and the MOG foreground result as the guidance image G. This configuration helps to highlight features that are consistent with the foreground, reducing noise and emphasizing significant sonar returns.\nThe third channel reverses this relationship: the MOG foreground result becomes the input image I, and the original sonar frame serves as the guidance image G. This setup enhances features that differ from the foreground model, effectively isolating moving objects and suppressing static elements.\nThis hedging strategy leverages the strengths of both configurations. The first guided filter (original as input, MOG as guide) emphasizes foreground consistency, which helps to suppress noise and highlight significant features. The second guided filter (MOG foreground as input, original as guide) isolates moving objects, differentiating them from static background elements. Together, these channels preserve the edges of objects (Figure 6d) and provide a robust representation (Figure 7a) that is well-suited for DNN-based detection models. We further propose a motion detection solution by using the converted three channels as input for Canny edge detection. This approach, performed in real time, serves as an effective motion detector that is resilient to noise. The motion detector helps to reduce the number of frames that need to be processed in subsequent stages, significantly reducing the processing and transmission load. An example of the identified prominent edges can be seen in Figure 7b."}, {"title": "5 DNN Model Adaptation", "content": "We then introduce the design of the Spatial-Temporal Sonar Vision Transformer (STSVT) for cloud inference.\nRecent trends in object detection have adopted Transformer architectures to eliminate predefined anchor boxes and many hand-designed components such as non-maximum suppression (NMS), resulting in significant performance improvements [5, 45, 46]. Deformable DETR [46] uses an attention mechanism to aggregate multi-scale feature maps, enhancing model generalization at different scales. However, our preliminary experiments show that noises, acoustic shadows, and reverberations in sonar frames weaken pixel correlation, negatively impacting the Transformer's point-to-point attention mechanism. To address this, incorporating temporal information between frames proves beneficial, as it improves the model's resilience to noise and allows it to better track moving objects. Accordingly, our adapted Spatial-Temporal Sonar Vision Transformer (STSVT) introduces the following two major components:\nSpatial Transformer. We choose the recently proposed Deformable DETR as our still frame detector. To simplify the design, we do not use multi-scale feature representations in either the encoders or decoders of the spatial transformer. Instead, we use only the last stage of the backbone as the input to reduce the complexity. As shown in Figure 9, the modified detector includes a spatial Transformer encoder and a spatial Transformer decoder, which encodes each frame (including the previous reference frame and current frame) into two compact representations: spatial object query and memory encoding.\nTemporal Transformer. The temporal transformer first encodes the spatial details from multiple frames, aggregating this spatial information via a temporal deformable attention mechanism. It then fuses object queries from multiple adjacent frames, selecting relevant object queries and combining them through several self-attention layers. With these intermediate outputs, it then generates detection results. As shown in Figure 9, we introduce additional encoder-decoder architecture to encode information between frames using memory and object queries.\nThe primary difference between our Spatial-Temporal Sonar Vision Transformer (STSVT) and existing models lies in the extensive utilization of temporal information between frames. This is particularly crucial for detecting moving objects in sonar data. We set the model to process multiple frames (ranging from 2 to 16) to better capture the displacement information of underwater objects. By doing so, the model can more effectively detect and track moving objects, even in varying underwater conditions."}, {"title": "6 Sonar Streaming and Energy Planning", "content": "When both on-premise and cloud resources are available, cloud inference is preferred for better sonar analytics performance. However, this presents challenges related to data transmission and energy planning. For instance, at one sonar site relying on solar power and satellite communication (e.g., Starlink) for real-time sonar streaming and cloud inference, the lack of a proper energy plan led to a power overload. During peak transmission, the Starlink dish consumed up to 200 Watts, double that of the sonar radar system's 100 Watts, resulting in unexpected contingency costs to repair the power breaker. Additionally, the site uses rechargeable batteries to support overnight operations, but continuous Starlink usage sometimes depleted the battery, causing system outages. These incidents emphasize the need for careful coordination between sonar streaming and energy planning to ensure sustainable operations.\nSupporting cloud inference in remote ecosystems, where solar power is often the primary energy source and satellite communication serves as the main network connection, requires careful planning. Adverse weather conditions such as rain, sudden storms, and fluctuating cloud cover can disrupt satellite connectivity and reduce solar power efficiency. To address these challenges, we propose a joint design for sonar streaming and energy planning. The design is based on our observations of how weather conditions impact network connectivity and power availability, and ensure continuous and efficient operations in volatile wild environments."}, {"title": "6.1 Weather Impact on Satellite Streaming Connectivity", "content": "We have observed that Starlink's throughput can be significantly affected by weather conditions. As shown in Figure 10, our results indicate an inverse correlation between throughput and precipitation amount. Specifically, throughput drops by an average of 15% during any form of precipitation. Throughput is particularly constrained during heavy rain events (> 4 mm per hour), affecting both downloads and uploads. The primary cause of this decrease in throughput is rain attenuation. Ka- and Ku-band radio waves, which are used by Starlink, are particularly susceptible to degradation in the presence of rain. The attenuation of these radio waves leads to a significant reduction in the signal quality received by the satellite terminals. Additionally, the presence of clouds can further interrupt data communications. Even light clouds can reduce satellite signal strength by approximately 10%, with thicker clouds associated with heavy rainfall further obstructing the network paths in both uplinks and ground-satellite links.\nWe also find that when streaming sonar data, the current dish's power consumption averages around 51.3 Watts but can go as high as 166.5 Watts, with the dish's power consumption positively correlated with precipitation. We present the density analysis in Figure 11. It is clear that, without rain, power consumption is lower in general, with occasional spikes due to other factors such as tuning the dish direction or establishing connections with farther away satellites. When there is rain or heavy clouds, power consumption becomes persistently higher, likely due to the interference from the rain or clouds."}, {"title": "6.2 Sustainable Energy Planning", "content": "In parallel, solar PV energy production is also highly sensitive to weather conditions. Solar panels generate electricity based on the amount of sunlight they receive, which can be drastically reduced by cloud cover, rain, or snow. Figures 12 and 13 show the volatile weather patterns on-site, highlighting the sudden changes and fluctuating cloud cover. The variability in solar energy production necessitates the need for efficient energy management strategies to ensure a stable power supply and limit peak energy use, especially in off-grid setups in wild ecosystems.\nWe utilize a short-term PV output forecast model [26] that learns a mapping from the sky image to future PV power output. This mapping is trained on fisheye lens images. Several examples are shown in Figure 14. These sky images are frames captured by a 6-megapixel 360-degree fish-eye Hikvision DS-2CD6362F-IV27 camera. This network camera is a cost-effective, low-power option (15 Watts) for sky monitoring. It captures at 2048 \u00d7 2048 pixels resolution at 20 fps. The PV output generation data are logged with a 1-minute frequency and are minutely averaged. To further optimize performance, we fine-tune the forecast model using our captured sky images and power data from three 300W PV modules with 18.6% efficiency, a typical configuration for powering sonar sites in wild ecosystems."}, {"title": "6.3 Multi-Stratum Streaming Optimization", "content": "Sonar frames are usually rectangular with a high height-width ratio, which poses a challenge for efficient streaming and processing. In this work, we explore a multi-stratum streaming mechanism by splitting the rectangle into multiple near-square strata, each streamed with different configurations. This approach improves streaming efficiency and detection performance. Another compelling argument for performing multi-stratum streaming is the significant impact of frame aspect ratio on inference results. Using images with an aspect ratio close to a square, rather than a significant width-height difference, leads to better detection performance. When images are resized to fixed sizes (e.g., 416x416 or 640x640) as inputs, large aspect ratio differences can cause distortion, negatively affecting the detection models' ability to learn object features accurately. Square-like images minimize this distortion, maintaining the target objects' original proportions and improving detection accuracy, which is crucial since many objects in sonar frames are thin and long. Additionally, CNN models process images at a uniform scale, so near-square images ensure an even distribution of computational resources, enhancing efficiency and effectiveness.\nAs shown in Figure 15, we propose a multi-stratum streaming framework that integrates satellite connectivity metrics and PV energy production forecasts to adjust streaming configurations, such as downscaling factors, framerates, and preprocessing filters, all of which impact final data rates. This approach allows for dynamic adjustment of both internet usage and energy consumption based on current and forecast conditions.\nWe formulate the multi-stratum streaming optimization problem as follows: consider a sonar stream that is segmented into N strata. Each stratum i\u2208 [1,..., N] introduces a set of control parameters $s_{i,j}$. A configuration consisting of M selections for stratum i is c = $[s_{1,1}, s_{1,2}, ..., s_{i,M}, ..., s_{i,M}]$, representing a specific combination of these control parameters. The set of all configurations is denoted by S. For any given configuration c, we define three critical metrics: the bandwidth usage B(c), the analytics performance A(c), and the power consumption P(c). The goal of profiling is to identify configurations that are Pareto-optimal. A configuration c is considered Pareto-optimal if there is no other configuration c' that simultaneously uses less bandwidth, less power, and achieves higher analytics performance. Formally, the set of Pareto-optimal configurations P is defined as follows:\n$P = \\{c \\in S : \\nexists c'\\in S \\text{ such that } B(c') < B(c), P(c') < P(c), \\text{ and } A(c') > A(c)\\}$\nTo solve the optimization problem, we first identify the key configurations that impact sonar analytics, such as frame size scaling factor, frame rate, and power-related operations. By focusing on these parameters, we can limit the search space for efficient computing. To achieve this, we discretize the parameter space and evaluate all possible combinations. Specifically, we employ an exhaustive search over the discrete parameter space to identify Pareto-optimal configurations. However, if the parameter space were to be treated as continuous, more advanced search techniques [9, 16] would be necessary to efficiently navigate the vast search space."}, {"title": "6.4 Continuous Operation", "content": "In our streaming optimization, the bandwidth constraint is determined through continuous monitoring of the Starlink connection, ensuring that the selected configurations do not exceed the available bandwidth. Similarly, the power consumption constraint considers both current energy production and future forecasts. These forecasts are used to estimate energy availability over upcoming periods, ensuring that the chosen configuration remains sustainable over the forecasted timeframe. This approach effectively prevents unexpected shutdowns or system outages due to energy depletion. It is noted that in the current system design, we have accounted for sufficient safety margins for rechargeable battery power to support overnight operations. Specifically, energy planning is conducted during the day when energy is more abundant, and configurations are aware of reserving additional energy during adverse weather conditions or low energy production. This ensures that the system maintains continuous and stable operations throughout the night. Thus, even under constrained energy conditions, our system remains reliable during the overnight period."}, {"title": "7 System Evaluations", "content": "We collaborated closely with biologists, forest technologists, and electricians to set up the system for continuous monitoring and surveillance at two different rivers in British Columbia, Canada: YK River and KN River. At YK River, we used the ARIS Explorer 1800 Sonar, which has an effective detection range of 35 meters. While at KN River, we employed the ARIS Explorer 3000 Sonar, with a detection range of 15 meters. The edge device used was a Jetson ORIN Nano 8GB. Onsite solar power was provided by three 300W 18.6% efficiency Q. Peak-G4.1 PV modules. The cloud server setup included a Lambda Vector Server equipped with four A5000 GPUs. The system was deployed for six months, monitoring underwater environments. As noted in Section 3.2, we also employed a third-party annotation service to label multiple object-tracking annotations for all objects in the converted grayscale sonar clips, including salmon, otter, and smolt. These annotations were used for training and fine-tuning the models, ensuring reliable performance."}, {"title": "7.2 Channel Population Impacts", "content": "We used our channel population results for training and validating on the YK dataset", "15": "considered as SOTA)", "0.50": 0.95}]}