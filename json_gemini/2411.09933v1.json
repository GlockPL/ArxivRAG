{"title": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging", "authors": ["Kaito Baba", "Ryota Yagi", "Junichiro Takahashi", "Risa Kishikawa", "Satoshi Kodera"], "abstract": "With the rapid advancement of large language models (LLMs), foundational models (FMs) have seen significant advancements. Healthcare is one of the most crucial application areas for these FMs, given the significant time and effort required for physicians to analyze large volumes of patient data. Recent efforts have focused on adapting multimodal FMs to the medical domain through techniques like instruction-tuning, leading to the development of medical foundation models (MFMs). However, these approaches typically require large amounts of training data to effectively adapt models to the medical field. Moreover, most existing models are trained on English datasets, limiting their practicality in non-English-speaking regions where healthcare professionals and patients are not always fluent in English. The need for translation introduces additional costs and inefficiencies. To address these challenges, we propose a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging (JRadiEvo). This is the first attempt to extend a non-medical vision-language foundation model to the medical domain through evolutionary optimization of model merging. We successfully created a model that generates accurate Japanese reports from X-ray images using only 50 translated samples from publicly available data. This model, developed with highly efficient use of limited data, outperformed leading models from recent research trained on much larger datasets. Additionally, with only 8 billion parameters, this relatively compact foundation model can be deployed locally within hospitals, making it a practical solution for environments where APIs and other external services cannot be used due to strict privacy and security requirements.", "sections": [{"title": "Introduction", "content": "In recent years, foundational models (FMs) have seen remarkable advancements, transforming various fields by offering more sophisticated and powerful solutions [1]. A key driver of this progress has been the rise of large language models (LLMs), which have greatly expanded the capabilities of FMs, particularly in processing and generating text with high accuracy and contextual understanding. This has sparked exponential growth in research [2], leading to the development of vision-language models that integrate visual and textual data [3\u20135], as well as fine-tuning approaches that enhance model performance for specific tasks [6, 7].\nHealthcare is one of the most critical application areas for foundational models. The need to develop models tailored to healthcare is essential, particularly because physicians often face the challenge of reviewing large volumes of medical data, such as X-rays, which can be time-consuming and demanding. Advanced FMs can help alleviate this burden by enabling quicker and more efficient diagnoses, improving the overall effectiveness of healthcare delivery and patient outcomes. In response to this need, various FMs have been fine-tuned specifically for the healthcare domain, further enhancing their accuracy and effectiveness in clinical settings [7\u20139].\nHowever, despite these advancements, several challenges remain. One significant issue is that most of the models developed so far, such as LLaVA-Med [7] and MedPaLM 2 [8], are predominantly in English, whereas many healthcare professionals and patients are not always proficient in English. For these models to be truly practical, there is a pressing need to expand their capabilities to non-English languages. Relying on a two-step process, where the model first generates output in English and then translates it, can introduce additional costs and complexity, making it less efficient and accessible. Additionally, publicly available datasets that can be used to train these models, such as MIMIC-CXR [10] and IU X-Ray [11], are overwhelmingly in English, with very few datasets available in other languages. Translating the large amounts of data needed for training into other languages with high quality is a costly and resource-intensive process. This scarcity of non-English datasets makes it difficult to develop models that can handle non-English languages. Furthermore, due to privacy concerns, it is challenging to collect and use patient data for model training, further complicating the creation of such datasets. Also, the use of large models through APIs, such as GPT-4 [12], is often impractical in healthcare settings because of the stringent privacy regulations that protect patient data, which limits the deployment of these models in real-world clinical environments.\nTo address these challenges, this paper presents a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging [13] (JRadiEvo), a first attempt to extend a multimodal vision-language model for non-English medical text generation by utilizing evolutionary optimization of model merging [13]. JRadiEvo was developed by merging a non-medical vision-language model, medical text-to-text models, and a Japanese-language text-to-text model using an evolutionary algorithm. This innovative approach enabled the efficient creation of a Japanese radiology report generation model using only a minimal amount of Japanese-language data, addressing the critical need for non-English medical text generation in a resource-constrained environment.\nBelow we outline our key contributions, which aim to advance the field of multimodal foundational models in healthcare:\n1. Efficient use of limited non-English medical data: In the context of the difficulty in collecting non-English datasets, JRadiEvo demonstrates the ability to create a non-English medical report generation model by translating and utilizing only 50 cases from publicly available English datasets. This approach highlights the efficiency of the development process, demonstrating how a non-English medical report generation model can be created using extremely limited data and annotations. Additionally, it is noteworthy that not only was the dataset used after translation limited to 50 cases, but the entire dataset used to create JRadiEvo consisted of just 50 cases. This underscores the fact that JRadiEvo efficiently utilizes a very limited amount of data, demonstrating an effective approach to handling medical data under strict privacy and security constraints.\n2. Novel application of model merging in the medical vision-language model: Traditionally, adapting models to the medical domain has relied on fine-tuning or training from scrach. To the best of our knowledge, there are no existing study of applying model merging alone to adapt a vision-language model to the medical domain. While recent research [13] has proposed using evolutionary optimization of model merging for vision-language models, this approach has been limited to natural images. To our knowledge, no prior studies have extended this technique to medical images or other domain-specific imagery beyond natural images.\n3. Lightweight model for local deployment: JRadiEvo is an 8B parameter model, making it lightweight enough to be deployed on local hospital computing systems without the need for external APIs. This local deployment capability addresses critical privacy and security concerns, allowing hospitals to maintain control over patient data. Additionally, given the challenges of equipping facilities with expensive GPUs proportional to patient numbers, JRadiEvo's compact size and low GPU memory requirements make it practical for widespread use.\n4. Cost-efficient training process: JRadiEvo eliminates the need for computationally expen-sive backpropagation during training, enabling a far more efficient learning process compared to training a new model or fine-tuning. Additionally, by leveraging model merging instead of fine-tuning, JRadiEvo avoids the common issue of catastrophic forgetting [14\u201316] that often occurs during fine-tuning, allowing for a more stable and efficient development process."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Foundation models", "content": "Medical large language models In the medical domain, several large language models (LLMs) have been developed and fine-tuned to achieve high performance. Notable models include ChatDoctor [17], DoctorGLM [18], BioGPT [19], Med-Alpaca [20], PMC-LLaMA [21], Med-Gemini [22], Med-PaLM [23], and Med-PaLM 2 [8]. These models have demonstrated impressive capabilities in understanding and generating medical text by leveraging the power of LLMs fine-tuned for healthcare-specific tasks.\nVision-language models As for multimodal vision-language models (VLMs), prominent models like Flamingo [5], Coca [24], BLIP [25], PaLI-X [26], CogVLM [27], GPT-4V [3], and LLaVA [4] have been developed to integrate visual and textual data, pushing the boundaries of what can be achieved in multimodal learning.\nMedical vision-language models Recently, there has been growing interest in extending these vision-language models to the medical domain, leading to the development of models like XrayGPT [28], MedFlamingo [9], Med-PaLM M[29], LLaVA-Med [7], and CheXagent [30]. These models incorporate LLMs into vision-language frameworks specifically designed for medical applica-tions. LLaVA-Med [7] is a vision-language model specifically designed for the medical field by using instruction-following data generated with GPT-4 [12] to perform instruction-tuning on the LLaVA [4] model. CheXagent [30] represents a significant advancement in medical vision-language models. It constructs a large-scale instruction-tuning dataset by aggregating publicly available datasets and adding new labels to existing ones, enabling accurate chest X-ray interpretation and showcasing the potential of instruction-tuned vision-language models in medical imaging.\nDespite the advancements of these models, most rely on fine-tuning or training from scratch, which requires large datasets. In the medical field, where privacy and security concerns make it difficult to create large datasets, this dependency poses a significant challenge. While several publicly available English datasets can be used for training, there are very few non-English datasets, making it difficult to develop practical models for non-English-speaking regions. JRadiEvo addresses this issue by proposing an efficient method for creating a vision-language model in a non-English language using just 50 translated examples from a public dataset. Note that not only were 50 examples translated, but the entire medical image-text dataset for creating JRadiEvo consisted of only 50 cases. Furthermore, while existing models output text in English, JRadiEvo generates reports directly in Japanese, eliminating the need for translation and demonstrating the potential for practical use in non-English-speaking regions."}, {"title": "2.2 Model merging", "content": "Model merging is a technique that allows the strengths of multiple pre-trained models to be combined without the need for additional training. One prominent approach involves using linear or spherical linear interpolation (SLERP [31]) to merge the weights of different fine-tuned models. Another technique, known as Task Arithmetic [32], enables manipulation of features obtained through fine-tuning by creating task vectors, which are derived by subtracting the weights of the original pre-trained model from those of the fine-tuned model. TIES-Merging [33] takes this concept further by addressing redundant changes in task vectors and resolving conflicts in parameter signs between multiple task vectors before merging them. This method involves a three-step process: removing small, insignificant parameter changes, resolving sign conflicts between task vectors, merging the adjusted vectors. Additionally, DARE [34] proposes randomly dropping some of the changes and rescaling the remaining ones, which can be combined with techniques like Task Arithmetic [32] and TIES-Merging [33] to enhance the merging process."}, {"title": "3 JRadiEvo", "content": "JRadiEvo efficiently adapted a vision-language model (VLM) to the non-English medical domain through the evolutionary optimization of TIES-Merging [33] combined with DARE [34]."}, {"title": "3.1 Problem setting", "content": ""}, {"title": "3.1.1 Vision-language models", "content": "VLM is designed to generate a text response y given an image $x_1$ and accompanying text $x_T$. A typical VLM utilizing a large language model (LLM) is composed of three main components: a vision encoder $M_v$ that extracts features from the image, a projector $M_p$ that transforms these image features into the latent space of the LLM, and an LLM component $M_L$ that generates the output text. The formulation of this process can be expressed as:\n$y = M_L(M_P(M_v(x_1)), x_T)$.\n(1)\nIn this setup, the LLM component $M_L$ is a pre-trained model that has already acquired strong language capabilities, such as Llama 3 [35]. To adapt it for vision-related tasks, the projector $M_p$, and optionally the LLM component $M_L$ are trained or fine-tuned."}, {"title": "3.1.2 Model merging", "content": "Let $\\theta_{init} \\in \\mathbb{R}^d$ represent the trainable parameters of the pre-trained LLM, where d is the parameter dimension. Given a set of K tasks ${t_1, t_2,\\dots,t_k}$, the LLM is fine-tuned on these tasks, resulting in a set of fine-tuned parameter vectors ${\\theta_1^{ft},\\theta_2^{ft},\\dots, \\theta_k^{ft}}$. Here, each task $t_i$ corresponds to a specific domain or task for fine-tuning the pre-trained LLM, such as vision tasks, medical applications, or adaptation to the Japanese language.\nAs demonstrated in previous research [32], the task vector for a task t is defined as the difference between the fine-tuned weights for task t and the original pre-trained weights, i.e., the task vector $T_t \\in \\mathbb{R}^d$ is given by:\n$T_t = \\theta_t^{ft} - \\theta_{init}$.\nThis task vector corresponds to the capabilities acquired through fine-tuning on task t. By manipulat-ing these task vectors and merging it with the original weights $\\theta_{init}$, we can merge the capabilities of multiple fine-tuned models without additional training. The details of the merging process we adopted are described in Section 3.2."}, {"title": "3.2 Model merging for JRadiEvo", "content": "In JRadiEvo, following the approach of previous work [13], we optimized TIES-Merging [33] combined with DARE [34] using an evolutionary algorithm. For the VLM, we also adhered to the strategy used in earlier research [13] by focusing on the parameters of the LLM component $M_L$ during the merging process, i.e., $\\theta$ represents the parameters of the LLM component $M_L$ of a VLM fine-tuned on vision-to-text data. Meanwhile, $\\theta_2^{ft},\\theta_3^{ft},\\dots$, are the parameters of the LLM"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental setup", "content": "Datasets For our experiments, we used the MIMIC-CXR [10], a publicly available dataset that consists of 377,110 chest X-ray (CXR) images and 227,835 corresponding English-language radiology reports. The images are provided in both DICOM and JPEG formats [37]. The dataset is officially split into training, validation, and test sets, containing 368,960, 2,991, and 5,159 images, respectively."}, {"title": "4.2 Comparison with leading models from previous study and instruction-tuning approaches", "content": "To evaluate the effectiveness of our evolutionary model merging approach, we compared the results with those obtained by LoRA [52] instruction-tuning the same vision-language model (VLM). Additionally, we compared our results with the performance of leading models from recent research."}, {"title": "4.2.1 Experimental conditions", "content": "As the base VLM, we used Bunny-v1_1-Llama-3-8B-V\u00ba [46], the same model used in the merging process in JRadiEvo, and the fine-tuning. During this process, the weights of the image encoder $M_v$ were kept fixed, and we prepared two models: one where only the LLM component $M_L$ was tuned, and another where both the LLM $M_L$ and projector $M_p$ were tuned. Both models were trained using LORA [52]. For the LoRA setup, the hyperparameters were set with a decomposition rank r of 8 and a scaling factor a of 16. The learning rate was was set to 2 \u00d7 10-4 and decayed with a cosine annealing [53]. The model was trained for one epoch with a batch size of 8 and gradient accumulation steps set to 2. The training was conducted using a single NVIDIA A100 GPU with 80GB memory.\nAs for the dataset, we followed the same procedure outlined in Section 4.1, using the training data from MIMIC-CXR [10]. Specifically, samples without corresponding reports were excluded, only the first image was used for reports with multiple images, and we focused on AP and PA view images. From this dataset, 2,000 samples were randomly selected and translated into Japanese using GPT-3.5 [41]\u00b9. This translated dataset was then used for instruction-tuning.\nFor comparison with previous studies, we used CheXagent [30], a recent leading model specifically designed for chest X-ray images. CheXagent [30], proposed in 2024, is one of the most recent advancements in the field. It was trained on a large instruction-tuning dataset, which was created by aggregating publicly available data and adding new labels to existing datasets. Since CheXagent is trained in English, reports were first output in English using CheXagent and then translated into Japanese using GPT-40 [54]\u00b9. This translated Japanese reports were used for comparison.\nAs another comparison, we used GPT-40 [54]\u00b9, a high-performance VLM that can output directly in Japanese."}, {"title": "4.2.2 Results and discussion", "content": "The results are shown in Table 2. Also, an example comparison of generated text from JRadiEvo and the ground truth on the test data is shown in Table 3.\nEffectiveness of JRadiEvo in generating radiology reports We can see from this table that our model, JRadiEvo, achieved the highest scores in both ROUGE-L and METEOR metrics. Given that ROUGE-L is considered the most aligned with human judgment in evaluating generated text, as shown in previous research [36], this underscores the effectiveness of our application of evolutionary model merging to medical text generation.\nEfficient use of limited datasets When compared to the latest model, CheXagent [30], JRadiEvo outperformed it across all evaluation metrics. Despite CheXagent being trained on a vast instruction-tuning dataset, JRadiEvo, with only 50 training samples, significantly surpassed it in generating X-ray reports. This demonstrates JRadiEvo's ability to effectively utilize a extremely limited dataset to create a powerful medical foundation model.\nPracticality in non-English-speaking regions Additionally, while CheXagent [30] produces reports in English, our model eliminates the need for an additional translation step by directly generating reports in Japanese. This is particularly important in non-English-speaking medical environments, where neither doctors nor patients may be fluent in English. Requiring translation every time is time-consuming and costly. Thus, JRadiEvo demonstrates potential for practical use in non-English-speaking regions.\nParameter efficiency and local deployment In comparison with GPT-40 [54], while it scored higher on BLEU, JRadiEvo outperformed it on METEOR and ROUGE-L, which is more closely aligned with human evaluation. This shows that the performance is comparable to, or even surpasses, that of GPT-40. Considering that JRadiEvo is a lightweight model with only 8 billion parameters, whereas GPT-4o is significantly larger, this highlights JRadiEvo's impressive parameter efficiency. Furthermore, unlike GPT-40, which requires API access, JRadiEvo's modest size allows it to be deployed locally in hospitals. This makes JRadiEvo a more practical option for use in privacy- and security-sensitive environments in hospitals.\nSuperiority over instruction-tuning Comparing JRadiEvo with the two models that instruction-tuned the same VLM shows that JRadiEvo outperforms them across all metrics. This highlights the effectiveness of adopting evolutionary model merging over traditional instruction-tuning approaches. Moreover, increasing the amount of data used for instruction-tuning in an attempt to further enhance the model can lead to catastrophic forgetting [14\u201316]. In our experiments, instruction-tuning with 2,000 data points yielded good results, but when we increased the dataset to 10,000 data points, catastrophic forgetting occurred, severely impairing the language functionality and rendering the model unusable. As previous research [55] has indicated, instruction tuning with LoRA [52] in Japanese is not effective for relatively small models, a finding that our results also confirm. This suggests that similar challenges may arise in other non-English languages as well. In contrast, JRadiEvo leveraged evolutionary model merging to successfully adapt the source VLM to the medical domain without experiencing catastrophic forgetting. This highlights the potential of model merging as a viable approach for domain adaptation in non-English languages, especially for smaller models."}, {"title": "4.3 Analysis of merged LLM contributions", "content": "To investigate the contributions of the merged LLMs, we compared the density and weight parameters after optimization, following the approach outlined in previous research [13]. Specifically, we analyzed the retained percentage ${k_{t_1}, k_{t_2},..., k_{t_k}}$ and the merging weight ${c_{t_1}, c_{t_2},..., c_{t_K}}$ described in Section 3.2. The results of this comparison are presented in Figure 1.\nAs shown in Figure 1, the weight for OpenBioLLM [48] is significantly higher, and its density is also relatively high. This suggests that the medical knowledge embedded in OpenBioLLM was crucial for adapting the non-medical VLM to the medical domain. In contrast, while MMedLM [47] has a high density, its weight is much lower, indicating that its contribution was less significant. This suggests that, in this setup, OpenBioLLM's medical knowledge was primarily utilized, potentially rendering MMedLM less influential in the model merging process.\nAdditionally, when examining the density and weight of Llama 3 Swallow [49], the LLM enhanced for Japanese language proficiency, we see that it was utilized to some extent, though not as heavily as OpenBioLLM. This suggests that the original VLM and LLMs had a some capacity for handling Japanese, albeit imperfectly, and the lack of medical knowledge was a more significant limitation. However, unlike MMedLM, which saw a dramatic reduction in weight, Llama 3 Swallow's contribu-tion was not negligible, indicating that its role was still necessary. In fact, when asked directly in Japanese, the other VLM and LLMs could generate responses that, while somewhat awkward and unnatural from a native speaker's perspective, were still intelligible."}, {"title": "5 Conclusion", "content": "In this study, we proposed a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging (JRadiEvo), marking the first attempt to extend a multimodal vision-language model for non-English medical text generation using evolutionary model merging. Despite utilizing only 50 translated samples from publicly available data, JRadiEvo demonstrated superior performance compared to leading models from recent studies trained on much larger datasets. This highlights the effectiveness of our approach in efficiently leveraging limited data to create a powerful and practical medical foundation model.\nWhile JRadiEvo has shown promising results in evaluation metrics, human judgment by medical experts or further refinement may be needed to make it suitable for clinical use. Future work includes efforts to close this gap to ensure the model's reliability in real-world medical settings."}, {"title": "6 Acknowledgments", "content": "This work was supported by Cross-ministerial Strategic Innovation Promotion Program (SIP) on \"Integrated Health Care System\u201d Grant Number JPJ012425."}]}