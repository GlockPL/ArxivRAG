{"title": "Enhancing Mental Health Support through Human-Al Collaboration: Toward Secure and Empathetic Al-Enabled Chatbots", "authors": ["RAWAN ALMAKINAH", "ANDREA NORCINI-PALA", "LINDSEY DISNEY", "M. ABDULLAH CANBAZ"], "abstract": "Access to mental health support remains limited, particularly in marginalized communities where structural and cultural barriers hinder timely care. This paper explores the potential of AI-enabled chatbots as a scalable solution, focusing on advanced large language models (LLMs)-GPT v4, Mistral Large, and LLama V3.1-and assessing their ability to deliver empathetic, meaningful responses in mental health contexts. While these models show promise in generating structured responses, they fall short in replicating the emotional depth and adaptability of human therapists. Additionally, trustworthiness, bias, and privacy challenges persist due to unreliable datasets and limited collaboration with mental health professionals. To address these limitations, we propose a federated learning framework that ensures data privacy, reduces bias, and integrates continuous validation from clinicians to enhance response quality. This approach aims to develop a secure, evidence-based AI chatbot capable of offering trustworthy, empathetic, and bias-reduced mental health support, advancing Al's role in digital mental health care.", "sections": [{"title": "1 Introduction", "content": "In recent years, the global mental health crisis has reached unprecedented levels, with the World Health Organization (WHO) reporting a 25% increase in anxiety and depression during the COVID-19 pandemic alone [30]. Mental health issues have become a significant global concern, with millions of cases going undetected and untreated due to various structural and attitudinal barriers [5, 11, 16]. Structural barriers, such as limited access to mental health services and high costs, prevent individuals from receiving timely and effective care [12]. Attitudinal barriers, including the stigma surrounding mental health and lack of awareness, further exacerbate the problem, leading many to avoid seeking help even when it is available [5, 41]. As a result, untreated mental health conditions often worsen, leading to adverse physical, economic, and emotional outcomes [4, 37].\nTo address these challenges, digital mental health solutions have emerged as a promising alternative to traditional mental health services. The rapid development of technologies, increased internet accessibility, and widespread use of smartphones have paved the way for innovative approaches to mental health care [12]. Among these solutions, AI-enabled chatbots have gained considerable attention due to their ability to provide cost-effective, scalable, and accessible mental health support. Chatbots such as Woebot [15], Wysa [21], and ViTalk [12] have demonstrated their potential in offering empathic interaction and cognitive-behavioral therapy (CBT), a widely used intervention in treating mental health challanges like anxiety and depression [15, 22]. These AI-enabled systems not only mimic human-like interactions but also offer anonymity and 24/7 availability, making them an appealing option for individuals hesitant to seek traditional mental health services due to stigma or other concerns [6, 19].\nThe rise of large language models (LLMs), such as GPT-3 and GPT-4, has further revolutionized the field of AI and its application to mental health care [2]. These models, trained on vast amounts of text data, possess the ability to generate coherent, human-like responses in real-time, making them well-suited for use in chatbots aimed at providing psychological support. LLMs can be fine-tuned to understand and respond to specific mental health needs, including crisis intervention and emotional support, allowing them to play a critical role in mental health first aid [27, 36]. By simulating natural conversations, these Al systems have the potential to fill the gaps in mental health services, particularly in underserved communities where access to mental health professionals is limited [15].\nHowever, despite the promising potential of AI-enabled chatbots in mental health, significant challenges remain. Ethical and practical concerns regarding the reliability, security, and trustworthiness of these systems have hindered their full integration into healthcare frameworks [6, 9]. Ensuring that chatbots are not only effective but also empathetic-able to recognize and respond to users' emotional states with reduced bias and enhanced security is critical for gaining public trust and promoting widespread adoption. Moreover, the collaboration between data scientists and mental health practitioners is essential to address these concerns and ensure that AI-enabled chatbots are aligned with evidence-based mental health practices [18].\nIn this paper, we focus on analyzing large language models (LLMs) for their capacity to comprehend and respond to emergency mental health scenarios, specifically focusing on mental health first aid. Following this analysis, we propose a conceptual design for an AI-enabled chatbot that integrates state-of-the-art advancements in AI, human-computer interaction (HCI), and mental health care practices. This interdisciplinary approach is critical to address the growing mental health needs, where AI and HCI can help overcome accessibility and stigma-related barriers. By adopting a human-Al collaboration approach, we aim to build a trustworthy, secure, and reduced bias AI-enabled chatbot, with mental health professionals in the loop to ensure ethical and practical challenges are met. Our framework also suggests the integration of federated learning and encryption techniques to further enhance the security and reliability of AI chatbots in mental health care systems."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Knowledge/Comprehension Measurement", "content": "Prompt engineering is the process of carefully crafting the input provided to an Al model in order to generate desired outputs [38]. In the context of using LLMs like ChatGPT, prompt engineering plays a crucial role in guiding the model to respond appropriately and effectively, especially in specialized or sensitive domains like mental health. By tailoring prompts to include specific instructions, context, or even role-based personas, users can control the behavior of the model and improve its performance on complex tasks [34]. This is particularly important when utilizing off-the-shelf Al tools, as these models do not inherently understand the full context of the problem they are addressing without clear guidance.\nFor example, when generating therapeutic responses, instructing the model to act as a professional counselor through prompt engineering ensures that the tone, language, and structure of the response are aligned with professional standards. Without such prompts, the model might produce generic or inappropriate responses, potentially diminishing the effectiveness of AI-assisted support - or worse, causing harm. Figure 1 demonstrates an example of such a prompt, where specific instructions were given to simulate responses as if from a licensed mental health professional.\nTo evaluate the capabilities of AI-enabled chatbots in replicating the expertise of mental health professionals, we adopted a comparative approach using two large language models (LLMs): ChatGPT and Mistral. The goal was to measure the models' ability to comprehend and provide supportive responses to complex mental health issues.\nWe developed specific prompts to include the persona (or profiles) for the LLMs to simulate the behavior of mental health professionals. These profiles included contextual information designed to reflect the therapeutic language and professional demeanor of licensed counselors. The purpose of creating such profiles was to ensure that the LLMs responded with the level of professionalism and empathy required in mental health interactions. Figure 1 presents an example of a prompt crafted for one of the data points, incorporating the therapist's background, education, and the patient's condition. It is important to note that each prompt used in our analysis featured a unique persona tailored to the therapist's qualifications and experience, as noted in the dataset. This personalized approach aimed to enhance the LLMs' ability to communicate empathetically and utilize appropriate mental health terminology, drawing from the 'CounselChat' dataset [7]."}, {"title": "2.2 Question Set Selection", "content": "The CounselChat dataset consisted of user-submitted questions on a wide range of mental health topics, with responses provided by certified counselors. For this study, we focused on questions related to eight crisis-related categories: anxiety, depression, grief and loss, self-harm, stress, anger management, trauma, and military issues. This filtering process resulted in approximately 700 questions.\nTo refine the analysis further, we identified the ten most-viewed questions on the platform, which predominantly fell within the categories of anxiety, depression, anger management, and military issues. These top 10 questions were then selected for manual analysis."}, {"title": "2.3 Analysis of Response Generation and Comparison for Top 10 Questions", "content": "For this part of the analysis, we utilized two of the most popular models: OpenAI's ChatGPT v4 and its open-source competitor, Mistral Large. The selected questions were input into both models, using the respective prompts shown in Figure 1. The responses generated by the LLMs were compared to those of human counselors, with a focus on assessing strengths and weaknesses in empathy, comprehension, and therapeutic value."}, {"title": "2.4 Preliminary Qualitative Analysis of Therapist vs LLM Generated Responses", "content": "We began our analysis with a manual comparison of responses generated by ChatGPT and Mistral against those provided by human therapists. This preliminary qualitative assessment allowed us to explore the differences in tone, empathy, and overall communication style. It quickly became evident that while AI-generated responses can be coherent and professional, they often lack the nuanced emotional intelligence found in human responses.\nKey distinctions emerged from this analysis: (1) The AI responses tended to have a polished but somewhat generic tone, delivering psychoeducational content in a way that felt mechanical and impersonal. This lack of a \"human touch\" made them easily identifiable as machine-generated. (2) In contrast, therapist responses were more concise and dialogic, incorporating questions aimed at fostering engagement and building a 'working therapeutic relationship.' This approach aligns with established mental health practices that emphasize checking in with the client to create a more interactive and supportive dialogue, as opposed to the didactic and instructional style often seen in Al responses. (3) Despite these differences, certain similarities were present. All responses-whether from the therapist, ChatGPT, or Mistral-featured empathetic statements and included essential psychoeducational content related to anxiety, as demonstrated in Appendix A.\nThese qualitative findings prompted the need for a more detailed quantitative analysis to further explore the extent of these differences, which we carried out using cosine similarity."}, {"title": "2.5 Quantitative Analysis of Therapist vs LLM Generated Responses", "content": "To quantify the differences between the LLM-generated responses and those of human therapists, we conducted a Cosine Similarity Analysis. This technique compares two sets of text data-therapist vs. LLM responses-by converting them into vector form and measuring the cosine of the angle between them. A cosine similarity score of 1 indicates identical vectors, while a score of 0 represents completely dissimilar vectors [8].\nFor this analysis, we employed Term Frequency-Inverse Document Frequency (TF-IDF) to calculate cosine similarity. TF-IDF is particularly well-suited for this task because it assigns weight to words based on their importance in individual documents (therapist or LLM responses) and their rarity across the entire dataset. In the context of mental health responses, where certain key terms like \"empathy\u201d or \u201canxiety\u201d carry significant meaning, TF-IDF helps capture both the specific therapeutic language used by human therapists and the critical elements of AI-generated responses. This ensures that the comparison isn't simply based on word counts, but rather on the significance of the terms used in context. The formula for cosine similarity is:\nCosine Similarity = $\\frac{A \\cdot B}{||A||||B||}$\nUsing Python, we calculated cosine similarity scores between the therapist responses and those generated by ChatGPT and Mistral for selected questions. The results, shown in Figure 2, reveal a clear gap between the LLMs and human therapists. The majority of the cosine similarity scores fall between 0.21 and 0.48, indicating that while the LLMs can"}, {"title": "2.6 Insights and Implications for Human-Al Interaction in Mental Health", "content": "The comparative analysis of LLM-generated responses, both through cosine similarity and qualitative methods, offers valuable insights into the current limitations and potential of AI in interacting with human emotions in mental health contexts. The consistently low cosine similarity scores across models highlight a key shortcoming: while Al models such as Mistral and Llama v3.1 can generate structured, coherent responses, they often lack the deeper emotional attunement and psychological nuance characteristic of human therapists. This gap is especially critical in therapeutic settings, where understanding and addressing complex emotions requires more than delivering factual information-it requires empathy, personalization, and adaptive interaction.\nThe use of detailed personas and profiles was intended to guide LLMs toward more human-like interactions, helping them adopt the tone and therapeutic language expected of mental health professionals. While these personas aided in shaping the responses toward a more professional demeanor, the cosine similarity analysis reveals that the models still struggled to capture the depth and emotional responsiveness that are critical in therapeutic dialogues. The qualitative analysis further supports this, showing that Al responses, although well-structured, often lacked the emotional sensitivity and adaptive feedback seen in human therapists' responses. This suggests that current LLMs are more successful at mimicking surface-level professional behavior than at engaging meaningfully with the underlying emotional complexities of clients in distress.\nThis limitation, however, does not entirely undermine the role of AI in mental health support. While LLMs may not yet be capable of replicating the full range of human emotional intelligence, their ability to provide accurate, structured responses could serve as a foundation for more specialized therapeutic tools. Importantly, the findings underscore the need for continuous human oversight in AI-enabled mental health applications. Without clinical guidance, there is a significant risk that LLMs may deliver responses that, while 'correct' in form, fail to meet the emotional needs of users or could even cause harm. Future research must explore whether Al systems can be refined to better detect and respond to emotional cues, or if their role should be limited to more supportive, adjunct functions in a therapeutic setting.\nLooking ahead, the implications for Human-AI interaction in mental health are clear. To develop more effective AI systems, future research should focus on improving models' ability to process not only the linguistic content of user input but also the emotional undertones that are essential in therapeutic exchanges. This may involve integrating more sophisticated emotional recognition capabilities and utilizing multi-modal data, such as voice tone or facial expressions, to provide a fuller understanding of the user's emotional state. Such advancements would move Al closer to providing responses that align not just with the content but also with the emotional needs of individuals, making AI a more effective tool in mental health care."}, {"title": "2.7 Feasibility Analysis of Needs for Al-Enabled First Aid Mental Support", "content": "Building on the insights regarding the limitations and potential of current AI models in mental health interactions, we now turn to the feasibility of developing an AI-enabled chatbot that can effectively provide first aid mental health support. To ensure such a system can meet the unique demands of mental health care, we must consider key factors such as trustworthiness, security, bias mitigation, empathy, and privacy.\nThe development of an AI-enabled chatbot for mental health support must adhere to a rigorous set of criteria to ensure its effectiveness, reliability, and acceptance by both users and healthcare professionals. As mental health continues to be a growing global concern, exacerbated by the COVID-19 pandemic, the role of digital tools in providing scalable, accessible, and private support has become increasingly important. To design a feasible AI-enabled chatbot, it is essential to address key concerns surrounding trustworthiness, security, bias mitigation, empathy, and privacy, shown in Figure 5."}, {"title": "3 Federated Learning as a Potential Solution", "content": "Google introduced Federated Learning in 2016, a recent trend in which a central server supervises a group of edge nodes (clients) that collaboratively learn a shared machine-learning (ML) model without sharing their sensitive data [25, 33, 35]. All decentralized participating nodes learn the shared ML model when a trusted curator aggregates their locally computed model's parameters to a centralized server and then sends back the resulting model, as depicted in Figure 6, preserving the privacy of their data [17, 26, 35]. Federated Learning has been a trend in different fields, especially healthcare, due to patient data-sensitive nature [35]. Although studies have proposed frameworks asserting that federated learning is a promising approach to utilize and protect the privacy of patient data silos that reside on health institutional servers, it suffers from multiple challenges [17, 25, 35].\nTwo broad classifications of federated learning challenges are training-related concerns and privacy and security concerns [25]. Communication exhaustion or cost due to training iterations and the heterogeneity of data (non-identically distributed data) and devices used in the training process are instances of training-related concerns. Whereas membership inference attacks, data poisoning attacks, model poisoning attacks, byzantine attacks, and backdoor attacks are instances of privacy and security concerns [23, 25, 26, 40]. Therefore, practical and effective techniques to alleviate the above problems are urgently needed when considering federated learning in building AI-enabled chatbots as mental health support.\nFor example, implementing a cryptographic technique in federated learning, such as homomorphic encryption, is required to preserve data privacy [17, 29]. Homomorphic encryption in federated learning can preserve data privacy by giving participating clients the ability to perform operations on ciphertext, producing outcomes like those gained when performing the same operations on the original plaintext [29, 40]. Also, Minimizing the communication rounds among edge nodes and the aggregator server as well as the size of messages sent in these rounds could solve the communication cost in federated learning [23]. Another technique is applying [13] suggestion of incorporating human intelligence with Al intelligence to validate model construction, spot unobserved data flaws, evaluate data bias, and proactively identify, control, and mitigate risk. The Human AI collaboration must include multi-stakeholder participation to assess all aspects of AI [31].\nIn this paper, we propose a framework for building an AI-enabled chatbot that utilizes federated learning in training the AI model and magnifies the role of the human element in the loop. Stakeholders must assess the state of AI use for mental health. As [18] advised, it is the responsibility of stakeholders, including all of us, to make an active move to transform mental health support by incorporating the mental health clinicians' expertise with the expertise of data scientists and other scientists. Our ultimate goal is to create a reliable, evidence-proved, reduced bias, and empathic AI-enabled chatbot that can serve as first aid for mental health, especially in underrepresented and underserved communities."}, {"title": "4 First Aid Mental Health", "content": "Although some researchers have raised concerns that AI-enabled chatbots may lack empathy and personalization, with these limitations highlighted as obstacles in previous studies [6, 28], recent findings challenge this view. A study by [39] asserted that the empathic responses generated by large language models (LLMs) can, in fact, surpass those of humans. An empathic AI-enabled chatbot is grounded in empathy theory from psychology, where cognitive empathy, a learnable and teachable skill, plays a central role in counseling sessions between clinicians and patients [20, 24, 32]. Building on this foundation, we aim to incorporate a model with proven superior empathy into our proposed framework. Specifically, [39] found that ChatGPT-4 outperformed its peers in a between-subject study evaluating the empathic quality of LLM-generated responses.\nFigure 7 lays out the main stakeholders involved, the general techniques to be used, and the data and operation flow. We reassert [13] point of view on the importance of human intelligence in the process, where constant, expert feedback is crucial for the success of the entire framework.\nThe proposed framework's success depends on the number of edge nodes (hospitals and clinics) participating in the federated learning process. Each participating node will enlarge the final dataset used and empower the machine learning model training process. Since federated learning depends on accumulating models' parameters that were trained on unique datasets from multiple and distinct edge nodes, the resulting aggregated model on the centralized server will be trained on diversified data, ensuring more inclusive and unbiased digital mental health support. Having multiple parties (hospitals in our case) who collaboratively train local models with their local datasets and push the model's parameters to an aggregator in federated learning will minimize the bias problem and make the model more generalizable in mental health [25, 33].\nEach participating hospital or clinic (for example, hospital 1 and hospital 2 in figure 7) will create reports from doctor/patient sessions, anonymize them, and add them to a local database. It is important to emphasize that all datasets used should follow the same format to ensure the data heterogeneity problem is solved in federated learning. The local data engineer is responsible for confirming compliance with the agreed-upon data format. Moreover, it is important to highlight the role of mental health clinicians and data engineers in diversifying the input data to eliminate bias propagation problem [10] in federated learning. The local empathic machine-learning model will then be trained on these reports and wait for an aggregator request to push the model parameters only while preserving data privacy by not sharing them.\nThe model parameters will not be pushed to the aggregator until local data scientists and mental health clinicians from the participating edge node validate the local model's result. This step, which incorporates human intelligence, is crucial to making the resulting model more reliable. To eliminate false positive (FP) and false negative (FN) cases, clinician expertise must be seriously aligned with machine learning models. A local data scientist should test the chatbot's performance using evaluation metrics. For example, deepeval [1] could be used to check the chatbot's answer relevancy, hallucination, and bias. This process must take place with the help of a local mental health clinician in order to formulate the different test cases. A local mental health clinician should also validate the model by interacting with it and providing different prompts and scenarios to examine the model responses, rationality, fluency, relevance, and coherence. A semantic textual similarity analysis, for example, using the Multilingual Text Enhancer (MLTE) approach [14], could also be performed to test the semantic similarity of the LLM response to the expected response.\nThe pushed local model parameters will be used by an aggregating model on a centralized server. The model will enhance its performance and push back its parameters to all participating edge nodes to enhance their performance in turn. Data scientists (developers) will be involved in validating the performance of the central server's resulting model and providing constant updates. The whole process of federated learning will use a reliable and evidence-proven encryption technique. An example of encryption technique is the homomorphic encryption that allows performing operations without decrypting data in advance [3]."}, {"title": "5 Conclusion", "content": "The development of AI-enabled mental health support tools presents both opportunities and challenges. This paper has explored these aspects by analyzing the capabilities and limitations of large language models (LLMs) in replicating the therapeutic responses of human mental health professionals. Our analysis, which utilized cosine similarity metrics and qualitative assessment, revealed that although models like ChatGPT-4 and Mistral produce grammatically correct and coherent responses, they often lack the nuanced empathy and emotional depth characteristic of human therapists. This finding underscores a key limitation: while AI models can imitate surface-level responses, they struggle to fully engage with the emotional and psychological complexities of distressed individuals. Moreover, the cross-comparison between LLMs demonstrated that even state-of-the-art models exhibit variations in their output, further complicating their use in sensitive contexts such as mental health.\nA key contribution of this paper is the novel framework we proposed for developing a more reliable, empathetic, and secure AI-enabled chatbot for first-aid mental health support. By integrating human oversight-via a \"human-in-the-loop\" approach-alongside a federated learning system, we aim to address crucial challenges such as bias mitigation, privacy, and trustworthiness. Federated learning ensures that the chatbot can learn from diverse datasets distributed across multiple healthcare nodes, without compromising user privacy. This decentralized approach not only helps reduce bias but also enhances the model's generalizability across different population groups, making it a more inclusive solution for underrepresented and underserved communities.\nOne of the key findings of this study is that while LLMs have the potential to offer support, their effectiveness depends significantly on expert intervention. Our analysis highlights that incorporating human intelligence at various stages-model validation, data scrutiny, and feedback gathering-can help mitigate issues such as hallucinations, bias propagation, and shallow emotional engagement. The role of clinicians, in particular, is indispensable in ensuring that Al responses align with therapeutic best practices and are free from harmful stereotypes or inaccuracies.\nFurthermore, this paper emphasizes the importance of using cutting-edge cryptographic techniques, like homo-morphic encryption, to safeguard sensitive mental health data during federated learning. The integration of such privacy-preserving technologies, coupled with constant human oversight, creates a multi-layered framework that prioritizes both security and ethical considerations.\nIn conclusion, while LLMs offer a promising avenue for mental health support, it is evident that the current models are not yet ready to fully replace human therapists, especially in emotionally sensitive scenarios. However, with the right balance of advanced AI techniques, expert human involvement, and secure, privacy-preserving frameworks, AI-enabled chatbots could become a valuable tool in bridging the mental healthcare gap. The novel framework proposed in this paper presents a pathway toward developing an Al system that is reliable, reduced in bias, empathetic, and privacy-focused, capable of providing first-aid mental health support to diverse populations. As we move forward, a collaborative effort between mental health professionals, data scientists, and users is crucial to making this solution both effective and widely accepted."}, {"title": "6 Future Work", "content": "While large language models keep advancing and unlocking massive potentials, a possible research area would be using the proposed AI-enabled chatbot by mental health clinicians to assess their patients' conditions. If the proposed chatbot is used as a patient's mental health companion, it could analyze the patient's language patterns to detect early signs of treatment responses as well as make early psychological and pharmacological intervention recommendations. Besides, collecting feedback from the AI-enabled chatbot users through a controlled experiment would increase the feasibility of the model. This would help understand the most important features to incorporate for mental health support during a crisis and the sense of urgency."}]}