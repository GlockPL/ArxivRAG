{"title": "Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format", "authors": ["Chao Fang", "Man Shi", "Robin Geens", "Arne Symons", "Zhongfeng Wang", "Marian Verhelst"], "abstract": "The widely-used, weight-only quantized large language models (LLMs), which leverage low-bit integer (INT) weights and retain floating-point (FP) activations, reduce storage requirements while maintaining accuracy. However, this shifts the energy and latency bottlenecks towards the FP activations that are associated with costly memory accesses and computations. Existing LLM accelerators focus primarily on computation optimizations, overlooking the potential of jointly optimizing FP computations and data movement, particularly for the dominant FP-INT GEMM operations in LLM inference.\nTo address these challenges, we investigate the sensitivity of activation precision across various LLM modules and its impact on overall model accuracy. Based on our findings, we first propose the Anda data type: an adaptive data format with group-shared exponent bits and dynamic mantissa bit allocation. Secondly, we develop an iterative post-training adaptive precision search algorithm that optimizes the bit-width for different LLM modules to balance model accuracy, energy efficiency, and inference speed. Lastly, a suite of hardware optimization techniques is proposed to maximally exploit the benefits of the Anda format. These include a bit-plane-based data organization scheme, Anda-enhanced processing units with bit-serial computation, and a runtime bit-plane Anda compressor to simultaneously optimize storage, computation, and memory footprints. Our evaluations on FP-INT GEMM operations show that Anda achieves a 2.4\u00d7 speedup, 4.0x area efficiency, and 3.1\u00d7 energy efficiency improvement on average for popular LLMs including OPT, LLaMA, and LLaMA-2 series over the GPU-like FP-FP baseline. Anda demonstrates strong adaptability across various application scenarios, accuracy requirements, and system performance, enabling efficient LLM inference across a wide range of deployment scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) [11], [63], [70], [72], [86] have demonstrated remarkable proficiency in a wide array of natural language processing tasks, including text generation, question answering, and automatic summarization. The extraordinary success of LLMs can be attributed to the scaling law [3], which posits that performance improves dramatically with the ever-increased model size, training data volume, and computational resources. The evolution of the GPT series [5], [38], [63] on model size strikingly illustrates the scaling law: while GPT-1 comprised a modest 117 million parameters, its successor GPT-4 is speculated to encompass over a trillion parameters. However, the exponential growth in LLM model sizes has created substantial deployment challenges, imposing enormous demands on storage and computational resources.\nTo address these challenges, quantization techniques [12], [16], [24], [51], [52], [66], [78] have been widely adopted in LLMs to reduce memory footprint and lower deployment costs. To maximally shrink the model size, the most common strategy for LLMs today is weight-only quantization [8], [17], [24], [34], [35], [47], [51], [64], [66], [78], [81], which aggressively lowers the precision of the weights while maintaining high precision for activations due to the presence of outliers [16], [77]. In particular, the widely adopted W4A16 scheme [24], [66], which quantizes weights into 4-bit integers (INT4) while retaining activations in 16-bit floating-point format (FP16), significantly reduces memory requirements and bandwidth, lowering GPU memory usage by nearly 4\u00d7 [83] and facilitating deployment to smaller devices [51], [62].\nWith the increasing importance of weight-only quantization, FP-INT GEMM operations [32] have become indispensable in LLM inference. As illustrated in Fig. 2, FP-INT GeMMS constitute a significant portion of the computational workload across various weight-only quantized LLMs and context lengths in text generation tasks. They dominate in typical applications with sequences under 4K tokens, comprising over 90% of operations on average, and remain substantial even for context lengths exceeding 10K tokens in applications from LongBench [4]. Such prevalence highlights the urgent need to optimize FP-INT GeMMs for efficient LLM inference.\nNVIDIA's new FP-INT GeMM kernel [62], as well as some specialized GPU kernels [24], [35], [57], [78], are a consequence of this evolution. However, these optimized GPU kernels still rely on using FP units by converting the INT weights to FP values for execution in FP GeMM operators [52]. Efforts have been made to develop dedicated FP-INT arithmetic units [32], while the additional costs of exponent alignment and normalization persist, resulting in complicated hardware implementation. To reduce hardware cost, another optimization approach is to convert the FP activations to a block floating point (BFP) data format for computation [13], [19], [44]. Since grouped BFP elements share an exponent, the overhead of exponent alignment and normalization within a group disappears, simplifying the operation to INT arithmetic. However, to mitigate accuracy loss when converting FP activations to BFP format on a pre-trained network, costly retraining [12]\u2013[14], [26], [39], [44], [61], [85] is required, hindering agile LLM deployment. Alternatively, accuracy can be preserved by using a large mantissa field conversion [32], [42], but this significantly increases the energy consumption due to computational and memory access overhead of the additional bits.\nIn summary, processing FP activations remains a major bottleneck in weight-only quantized LLM inference, and existing methods struggle to balance model accuracy, computational efficiency, and energy consumption. To overcome the above limitations, we propose Anda to unlock efficient LLM inference. Anda introduces a novel variable-length grouped activation data format, coupled with the algorithm innovation and specialized hardware optimizations. As shown in Fig. 1, Anda first employs a fast, training-free adaptive precision search algorithm during compile time, using the same calibration data as post-training weight-only quantization [25]. Guided by user-defined accuracy constraints, our one-shot process identifies the desired mantissa bit length, which instructs the activation precision across various LLM modules during inference. Combining the flexible Anda data format with our specialized hardware architecture allows running these dominant FP-INT GeMM operations at lower precision, significantly improving inference speed and energy efficiency for weight-only quantized LLMs. More concretely, our contributions are as follows:\n\u2022 We investigate the potential of BFP activation quantization across popular LLM models within different modules. Based on these insights, we propose Anda: a variable-length grouped data format with shared exponents and adjustable mantissa widths for activations.\n\u2022 We develop an adaptive search algorithm to optimize the mantissa widths for different LLM modules without retraining. This algorithm balances model accuracy, energy efficiency, and inference speed based on a user-defined accuracy loss tolerance."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Weight-only quantization [8], [17], [24], [34], [35], [47], [51], [64], [66], [78], [81] has emerged as a pivotal technique for efficient LLM inference. Unlike weight-activation quantization [12], [16], [52], [79], [89], which reduces precision for both weights and activations, weight-only quantization focuses solely on compressing model parameters using a much more aggressive quantization scheme.\nFig. 3 illustrates the architecture of a weight-only quantized LLM, composed of a series of Transformer blocks that each contains an attention layer and a feed-forward layer. The light blue background highlights the dominant computational modules involving FP-INT GeMM operations, which can be categorized into four module types based on the positions of the FP activations: the first type involves \\(A_{qkv}\\) interacting with \\(W_q\\), \\(W_k\\), and \\(W_v\\) to compute the query (Q), key (K), value (V) matrices, respectively; the second type involves \\(A_o\\) multiplying with \\(W_o\\) to compute the output matrix; the other two types are up-projection and down-projection modules of the feed-forward layer, respectively, involving \\(A_u\\) and \\(A_d\\) with interacting to corresponding weights.\nWeight-only quantized LLMs offer significant advantages in storage efficiency [62], [66]. Compared to W8A8 weight-"}, {"title": "B. Block Floating Point", "content": "Reducing the computation and storage overhead of FP16 activations is crucial for optimizing the efficiency of LLMs. BFP [19] offers a promising solution by sharing exponents within groups of values, preserving dynamic range while mitigating the impact of outliers and simplifying computations.\nThe BFP format can characterized by two key parameters: group size and mantissa length. Fig. 4 shows the process of converting FP16 tensors to BFP numbers using two different instances of the BFP format. Initially, FP16 tensors are divided into groups. Within each group, the largest exponent is selected as the shared exponent and other mantissas are right-shifted based on their exponent differences. Bits exceeding the specified mantissa length are truncated, and zero is represented by all mantissa bits being 0. As illustrated in Fig. 4, this conversion process can lead to precision loss due to mantissa truncation, with some elements becoming zero, thereby posing a significant challenge to maintaining model accuracy.\nCurrent approaches to address this fall into two categories. On the one hand, BFP-aware training fine-tunes the model after the quantization [12]\u2013[14], [23], [26], [39], [41], [44], [61], [85], at the expense of a costly training process, making it rather impractical for agile LLM deployment. On the other hand, direct conversion of pre-trained FP models to BFP formats [22], [23], [44], [50], [61] requires long mantissas to avoid the significant accuracy loss, which increases computation and storage overhead, diminishing the advantages of BFP. To avoid the storage of these long mantissas, methods like FIGNA [32] and [42] propose dynamic conversion to BFP during computation. This approach stores activations in FP16 format and expands to long mantissas with shared exponents"}, {"title": "C. Opportunities towards Activation Optimizations", "content": "We explore opportunities for LLM activation optimization by investigating the sensitivity of model accuracy to reduced mantissa lengths in BFP formats. This study converts FP-INT GeMM activation tensors (\\(A_{qkv}\\), \\(A_o\\), \\(A_u\\), \\(A_d\\)) from FP16 to BFP format, as shown in Fig. 4. Model accuracy is evaluated using perplexity (PPL) on the WikiText2 dataset, with lower PPL indicating higher accuracy. We assume a 1% accuracy loss tolerance in practical scenarios. We aim to uncover efficient activation representations while maintaining LLM performance within acceptable limits.\nSensitivity to group size: Fig. 5 illustrates the sensitivity to shared exponent group size for two different LLM models across various mantissa lengths. The experiments reveal a clear trade-off between group size and the minimum required mantissa length to maintain model accuracy. Larger activation group sizes allow more efficient parallel computations, yet at a greater accuracy tolerance or increased mantissa lengths. Based on these observations, we select a group size of 64 for subsequent experiments, as it offers a good balance between computational efficiency and accuracy tolerance.\nSensitivity to LLM model: With this group size of 64, we continue our exploration across a wider range of recent LLMS, to derive their sensitivity to reduced mantissa lengths. Fig. 6 reveals varying sensitivities among different models. Notably, models such as OPT-2.7B, OPT-6.7B, OPT-13B, and OPT-30B are less sensitive to mantissa reduction, allowing for the direct removal of 5 mantissa bits, while other models could only tolerate the removal of 4 mantissa bits. As more mantissa bits are removed, differences in accuracy sensitivity become more pronounced. This insight inspires us to consider a variable-length BFP datatype, potentially enabling more aggressive"}, {"title": "III. ANDA DATA FORMAT", "content": "In this section, we present unique features of the Anda data format and demonstrate its benefits towards FP-INT operations in weight-only quantized LLM inference. Furthermore, we introduce a mantissa bit-width search method to efficiently identify the optimized Anda precision combinations that satisfy a user-defined accuracy drop."}, {"title": "A. Anda Format Features", "content": "Based on the findings of our previous study, we propose the Anda format: an innovative variable-length mantissa BFP scheme designed for efficient LLM inference. Anda's structure comprises a sign bit, a shared exponent, and a variable-length mantissa, building upon traditional BFP conversion processes as previously shown in Fig. 4. Its key feature is the ability to dynamically select mantissa lengths for different tensors based on their precision sensitivity, maintaining consistency within each tensor while optimizing the accuracy-efficiency trade-off.\nTable I compares Anda with prior BFP formats, categorizing them based on supported mantissa lengths. Uni-length formats, such as VS-Quant [12] and FIGNA [32], use fixed mantissa lengths, while multi-length formats like FAST [85] and Da-Capo [41] offer limited flexibility with 2~3 predefined lengths. Anda surpasses both by providing a continuous range of mantissa lengths, allowing fine-grained precision control across different LLM modules. Enabled by specialized hardware units, as detailed in Sec. IV, smaller mantissa widths result in a lower inference latency, computational cost and memory storage cost. This allows Anda format to carefully balance model precision and computational efficiency, providing a more aggressive compression in less sensitive model parts while preserving critical precision elsewhere."}, {"title": "B. Efficient FP-INT GeMM Using Anda Format", "content": "We then compare the workflows of GeMM workloads of several SotA approaches to illustrate the advantages of replacing FP16 activations with the Anda data format. Taking the W4A16 quantization scheme as an example, we examine the FP-INT GEMM computation process (a) on existing GPU platforms [52]; (b) on GPU platforms with dedicated FP-INT processing units; (c) using FIGNA's dynamic conversion scheme [32]; and (d) with our proposed Anda approach. Fig. 8 depicts the four schemes, with colors indicating the data types used throughout the computational process.\nFig. 8(a) shows the workflow of W4A16 LLMs on common GPU platforms. The absence of dedicated FP-INT computation units in GPU necessitates converting INT4 weights to FP16 before processing, with tensor cores operating in FP16 mode. This scheme not only brings additional format conversion overheads, but requires costly FP computations.\nGPU platforms equipped with dedicated FP-INT processing units, as illustrated in Fig. 8(b), can eliminate the need for converting INT4 weights to FP16, thereby reducing data conversion overheads and computation costs. However, as pointed out by FIGNA [32], the high alignment and normalization overhead associated with FP-INT processing units still results in high computational expenses.\nTo efficiently deploy W4A16 LLMs, FIGNA proposes a computation scheme using a BFP variant with corresponding hardware support to overcome the issues with dedicated FP-INT units. As depicted in Fig. 8(c), activations are stored in FP16 format in memory, converted to the FIGNA format before computation, after which a 14-bit mantissa is multiplied with INT4 weights for GeMM computation. The final results are then converted again to FP16 and written back to memory. This scheme reduces the computation overhead by converting costly FP GeMM to INT operations. However, since FP16 activations need to be repeatedly accessed during computation, frequent data conversion from FP16 to FIGNA introduces additional overhead, affecting overall efficiency.\nAs presented in Fig. 8(d), our proposed Anda format computation scheme offers some unique advantages in contrast with the previous approaches. Firstly, the activations are no longer stored in memory in FP16 format, but directly in the Anda data format, reducing storage overhead and data access overhead while avoiding frequent data conversion. Secondly, the shared exponent enables INT dot-product operations within a group, followed by FP32 accumulation across groups, reducing the computational overhead of FP-INT GeMMs. Thirdly, the variable-length mantissa considerably decreases dot-product operations and memory accesses use the minimal necessary word length. Finally, converting only the final FP32 results back to Anda format before writing to memory minimizes the storage requirement and the additional overhead from switching data format."}, {"title": "C. Adaptive Precision Combination Search", "content": "To leverage the Anda format for fast deployment and hardware performance gains, we propose an adaptive precision search algorithm for offline compile-time optimization of activation precisions in weight-only quantized LLMs. Our algorithm is built around two key strategies. (a) We narrow the search space to the precision of only four key tensor types i.e., \\(A_{qkv}\\), \\(A_o\\), \\(A_u\\), and \\(A_d\\), based on their sensitivity to model accuracy as demonstrated in Fig. 7. This precision combination is represented as a 4-tuple [\\(M_{qkv}\\), \\(M_o\\), \\(M_u\\), \\(M_d\\)]. (b) We employ a training-free, one-shot calibration process reusing the small amount of calibration data from the post-training weight-only quantization process, being several thousands of tokens with hundred batches [24], [51], [66]. Though prior layer-wise methods [18], [28], [76] may achieve finer precision adjustments, their prolonged search times significantly extend the deployment process. In contrast, our module-wise approach rapidly assigns mantissa lengths while maintaining consistency across layers and can easily be integrated into standard post-training deployment workflows.\nAs outlined in Algorithm 1, we take the LLM model \\(L\\), a calibration dataset \\(D\\), an accuracy loss tolerance \\(\\delta\\), and a maximum number of iterations \\(N\\) as inputs. The accuracy tolerance \\(\\delta\\) specifies the acceptable level of performance degradation, while the maximum number of iterations \\(N\\) serves as a termination criterion, ensuring the algorithm concludes within a reasonable time frame. With these inputs, our algorithm finds the optimal 4-tuple precision combination within the given iterations that best balances model accuracy and inference efficiency across the model's key activation components. The search process consists of three key steps.\nStep 1: Initialize search starting points. A priority queue with precision combinations of equal precision across all modules is initialized first. These precision combinations range from aggressive (e.g., [4, 4, 4, 4]) to conservative (e.g., [13, 13, 13, 13]). This strategy enables the rapid discovery of efficient combinations while ensuring the existence of feasible solutions, as validated by our prior experiments in Fig. 6.\nStep 2: Check the promising combination. In each iteration, the combination with the lowest bit operations (BOPs) is extracted from the priority queue and added to the visited set. The BOP metric [1], [43], [49], [71] quickly estimates computational cost by calculating the total number of bit operations for the necessary multiplications under a given combination. This allows us to efficiently prioritize promising combinations without a full model evaluation. The accuracy of the promising combination is then examined on the calibration dataset.\nStep 3: Update and relax the best combination. If the evaluated combination yields lower BOPs than the current best while maintaining accuracy within the specified tolerance, it becomes the new best combination. To generate nearby precision candidates, the algorithm then relaxes this best combination by decreasing the mantissa length of each tensor type by one, while keeping the other tensor types unchanged. For example, if the current best combination is [6,7,5,5], the generated candidates will be [5, 7, 5, 5], [6, 6, 5, 5], [6,7,4,5], and [6,7,5, 4]. The generated candidates that have not been visited before are added to the priority queue. If the accuracy constraint is not met, no update is made. Step 2 and 3 are repeated until the maximum number of iterations is reached or the search space is exhausted.\nOur algorithm aims to efficiently optimize FP activations in weight-only quantized LLMs during the post-training phase."}, {"title": "D. Precision Combination Search Efficiency", "content": "Most weight-only quantization processes [24], [51], [66] rely on a small calibration dataset, which we can reuse in the activation precision search. Ensuring a rapid search process is critical to avoid extending post-training deployment time. Therefore, the algorithm is designed to find a near-optimal solution quickly, within an acceptable accuracy tolerance, to enable efficient hardware deployment.\nThe efficiency of our algorithm is enhanced by two key mechanisms: First, we introduce a constraint that updates the best combination only when a new precision combination offers a lower computational cost, employing a relaxation strategy similar to gradient descent to accelerate convergence. While this may miss the global optimum, it ensures a high-performance combination within limited iterations. Second, we set an iteration limit to complete the search within a reasonable timeframe, avoiding deployment delays. It is here important to note that the relatively limited search space of only 4 precision variables allows for fast convergence with just a few iterations. The execution time of each iteration is roughly the time of a forward pass over the calibration dataset to validate the precision combination.\nTo demonstrate our algorithm's search efficiency, we compare it with the conventional brute-force approaches [12]\u2013[14] on the OPT-125M model. As shown in Fig. 9, the search space for OPT-125M contains over 10,000 possible combinations, and our algorithm identifies the precision combination [7, 7, 6, 5] in just 10 iterations, maintaining accuracy within 1% loss. In practice, we limit the search to 32 iterations, ensuring that time overhead remains minimal while achieving a near-optimal precision combination. By avoiding time-consuming backward propagation or complex solving processes, our algorithm operates approximately twice as fast as Omniquant [66] and ten times faster than GPTQ [24], the current SoTA methods for post-training weight-only LLM quantization."}, {"title": "IV. ANDA ARCHITECTURE", "content": "In this section, we first present the three key components of the Anda architecture: (a) A variable-length activation data layout in on-chip memory storage, (b) an Anda-enhanced bit-serial processing unit, and (c) a runtime bit-plane compressor for output activations. These components collectively enhance storage efficiency, computational performance, and energy"}, {"title": "A. Bit-plane Data Layout Scheme", "content": "Anda-based activation values feature a variable-length mantissa, necessitating careful data layout arrangement in the on-chip buffer to maintain regular memory access. Otherwise, irregular memory accesses caused by an ineffective data layout could completely undo the benefits provided by Anda.\nTo tackle these challenges, we propose the bit-plane data layout scheme as illustrated in Fig. 10. Unlike prior fixed-length data arrangement methods [30], [41], [61], [67], which treat each FP data element as an atomic unit, our approach separates and reorganizes the sign bit, mantissa, and exponent of FP numbers within grouped data blocks from a bit-plane view. A transposed data arrangement [48] is introduced where bits of the same significance across multiple numbers are packed together to keep the regularity of memory access.\nTaking the common memory bank word width into account, 64 Anda-type values are grouped to implement the bit-plane data layout scheme. As shown in Fig. 10, Group #0 shows the layout for 4-bit mantissa Anda numbers, while Group #1 presents the arrangement for 5-bit mantissa Anda numbers. The variable mantissa length only reflects on the different memory address depths, without impacting memory bandwidth utilization, and can be easily managed during address generation. Hence, in both cases, the bit-plane data layout efficiently accommodates these formats with varying lengths, maintaining consistent access patterns. Furthermore, the bit-plane organization inherently facilitates parallel processing, inspiring the design of a novel processing unit for the Anda data format to enhance LLM inference in both computing and energy efficiency."}, {"title": "B. Anda-enhanced Bit-serial Processing Unit", "content": "The Anda-enhanced bit-serial processing unit (APU), as depicted in Fig. 11, serves as the key computational element of the Anda architecture, embracing Anda processing element (PE) and an FP accumulator. Anda PE efficiently executes dot-product operations between variable-length Anda format activations and INT weights, seamlessly integrating with the bit-plane data layout scheme to enhance performance. The FP accumulator follows the PE to complete the APU functionality by accumulating the cross-group dot-product results.\nThe computation process begins with the Anda PE storing the sign and exponent in internal registers. Concurrently, the INT weights are stored in the PE using a double-buffer design, allowing overlapped weight loading and computation to minimize loading latency. The PE then loads the bit-plane mantissas and performs computations with the INT weights. By employing bit-serial processing of mantissas, the Anda PE can adapt to Anda format data of varying lengths without additional hardware overhead.\nTo further optimize hardware efficiency, the Anda PE implements a first-element-then-bit-plane reduction pattern. In this approach, a partial sum is obtained for each bit-plane by accumulating all elements within that bit-plane using an adder tree. This method reduces storage requirements by storing only one partial sum per bit-plane instead of all intermediate results. It also minimizes data movement and processing overhead by performing subsequent shift operations only on the single partial sum rather than individual elements. Furthermore, it significantly reduces hardware resource consumption by using a single shared accumulator for all bit-plane accumulations.\nThe bit-plane partial sums are then sequentially accumulated to complete the dot-product operation. Upon completion, the Anda PE dynamically shifts the dot-product result based on the Anda mantissa length and converts it to FP16 using the shared exponent. The result is then multiplied with the group-wise scale factor of the INT weights, followed by cross-group accumulation using the FP accumulator. Finally, the accumulated FP32 result is converted to FP16 for output."}, {"title": "C. On-the-fly Bit-plane Compressor", "content": "The bit-plane compressor (BPC) is a critical component of the Anda architecture, enabling on-the-fly conversion of FP16 activation values into the compressed Anda format. It efficiently addresses the challenges of variable-length Anda activation storage and transfer in LLM inference by processing a large number of activation values in parallel and outputting them in a bit-serial manner.\nFig. 12 illustrates the architecture of the proposed BPC. It consists of 16 parallel lanes, each capable of processing 64 grouped FP16 values simultaneously. Within each lane, the FP field extractor decomposes the FP16 inputs into their sign,"}, {"title": "D. Overall Architecture", "content": "Fig. 13 illustrates the overall architecture of Anda, which includes the top controller, address generator, activation buffer, weight buffer, matrix computation unit (MXU), vector unit, and bit-plane compressor. The LLM inference is orchestrated as follows: Initially, the instruction memory is programmed through the I/O interface of the top controller, which governs the address generator during operation. The address generator produces read and write addresses for both activation and weight buffers. Both the activation buffer and weight buffer follow the proposed bit-plane-based data layout for efficient data handling. The MXU, featuring a 16\u00d716 APU array, performs FP-INT GeMM operations following typical output stationary dataflow [45]. The weight data dispatcher, equipped with registers, allows overlapping weight loading and computation, broadcasting weights row-wise to each APU for data reuse. The activation data dispatcher supplies a bit-plane vector of activations each cycle, sequentially feeding it into the MXU and sharing it across columns to maximize input reuse and enable multiple calculations with the same input. Upon completing the GeMM computation, the output results are delivered to the BPC via the output data dispatcher. Complementing MXU, the vector unit processes the non-linear functions of the transformer block. FP16 outputs of MXU or vector unit can be optionally compressed to Anda format by the BPC, optimizing storage efficiency. Processed outputs are written back to the activation buffer. Finally, activation results are transferred to external memory for subsequent operations."}, {"title": "V. EVALUATION", "content": "LLM Benchmarks: To demonstrate the wide applicability of our proposed method, we benchmark various open-source LLMs using PyTorch and Hugging Face libraries. We evaluate their performance on WikiText-2 [60], Penn Treebank (PTB) [59], and C4 [65] datasets. The models used in our benchmarks range in size from 1.3B to 30B parameters and are selected from OPT [86], LLaMA [72], and LLaMA2 [73] families, enabling the effectiveness assessment of our method across different model scales and architectures.\nQuantization Baselines: To validate the model accuracy when replacing FP activations with the proposed Anda format, we compare against the following SotA competitors: (a) Full-precision baseline where both activations and weights are represented in FP16. (b) Weight-only PTQ baseline using Omniquant [66] with W4A16g128 scheme, which uses 4-bit weight quantization with a group size of 128. (c) Lossless BFP baseline that adopts FIGNA's [32] approach of using extended mantissa lengths to maintain model accuracy. (d) Aggressive BFP baseline that employs 4-bit mantissa to activations using VS-Quant [12] quantization scheme. For fair comparison in the PTQ scenario, we directly apply VS-Quant's 4-bit data format without the typically required costly retraining. For baseline (c), (d), and our Anda method, we use baseline (b)"}, {"title": "B. Inference Accuracy", "content": "We evaluate inference accuracy on validation datasets using the Anda format explored by adaptive precision search algorithm across all benchmarks. For each benchmark, 128 random sequences of length 2048 are sampled from the training dataset for calibration [66]. We also limit the adaptive precision search algorithm to 32 iterations. Note that Anda is capable of adapting mantissa bits according to the user-defined accuracy tolerance. Therefore, we report results for two accuracy constraints: 0.1% representing minimal loss and 1% representing acceptable loss for most scenarios.\nTable II compares Anda's performance against baseline quantization methods, where PPL values are shown in black, the relative accuracy drop is displayed in red, and the BOPs reduction is presented in green. Note that the occasional slight exceedance of the validation accuracy loss over the"}, {"title": "C. PE-level Evaluation", "content": "We quantitatively compare the proposed Anda PE with common FP-FP units [58], enhanced FP-INT units, and dedicated PE units from iFPU [42] and FIGNA [32], respectively. We also introduce FIGNA-M11 and FIGNA-M8 as baselines, representing bit-parallel PEs with 11-bit and 8-bit mantissas that achieve 0.1% and 1% accuracy degradation targets, respectively, based on the results from Fig. 14. Here, M(x) denotes the number of preserved mantissa bits. To ensure an equitable evaluation, all PEs are configured with equal computational throughput per cycle. We process an identical dot product workload across different PEs to measure area efficiency (TOPS/mm\u00b2) and energy efficiency (TOPS/W).\nFig. 15 (a) and (b) show the area and power consumption of Anda and baseline PEs. Anda presents significant reductions, consuming less than 60% of the power and area compared to FP-FP and FP-INT PEs. This is primarily due to shared exponents, which eliminate complex alignment and normalization processes. Compared to iFPU [42], Anda offers 12% and 29% reductions in area and power, respectively, by avoiding high-overhead ultra-wide multipliers and registers needed for maintaining FP16 precision. While Anda incurs a 27% power and 18% area overhead compared to FIGNA due to its bit-serial structure, its adaptive precision capability can significantly reduce execution time, leading to higher efficiency. Fig. 15 (c) and (d) further exhibit superior area and energy efficiency of Anda PE with variable-length mantissas. Refering back to Fig. 14, the retained mantissa lengths of Anda typically range between 4~8 bits with negligible 1% accuracy impact, resulting in the area and energy efficiency improvements of 1.38~2.48\u00d7 and 1.52~2.74\u00d7 over FIGNA, respectively. Moreover, comparing FIGNA and Anda at fixed mantissa lengths, Anda introduces some control logic overhead due to its bit-serial design. At 11 bits, Anda has 12% and 17% lower area and energy efficiency against FIGNA-M11; at 8 bits, it's 5% and 15% lower against FIGNA-M8. However, Anda's ability to dynamically adjust mantissa lengths based on model accuracy requirements allows it to potentially achieve higher utilization at the system level, which will be analyzed in the next subsection."}, {"title": "D. System-level Evaluation", "content": "Fig. 16 compares system-level speedup, area efficiency, and energy efficiency between Anda and several baselines across various LLM models. We also introduce bit-parallel FIGNA-M11 and FIGNA-M8 as baselines for 0.1% and 1% accuracy loss. Anda enables precision-scalable inference within a single hardware architecture, in contrast to FIGNA's separate implementations for each precision level.\nSpeedup: Anda, utilizing the precision combinations identified in Fig. 14, implements scalable computation and achieves 2.14\u00d7 and 2.49\u00d7 speedups on average over the GPU-like FP-FP baseline at 0.1% and 1% accuracy loss, respectively. Com-"}, {"title": "E. Power and Area Breakdown", "content": "We conduct a detailed hardware analysis of the Anda architecture for LLaMA-13B inference within 1% accuracy loss."}, {"title": "F. Accuracy-Performance Trade-off", "content": "This section explores speedup and energy efficiency improvements of the Anda system over the FP-FP baseline with"}, {"title": "VII. CONCLUSION", "content": "This work presents Anda, a variable-length grouped activation data format that addresses energy and performance bottlenecks of weight-only quantized large language model (LLM) inference by exploiting redundancy in floating point activations across different models and their modules. To fully harness the potential of Anda, we develop an iterative post-training algorithm that optimizes bit-width allocation across LLM modules, balancing accuracy, energy efficiency, and inference speed. We design complementary hardware optimizations to maximize the benefits of Anda, including a bit-plane-based data organization scheme in memory, Anda-enhanced bit-serial processing units, and a runtime bit-plane compressor. Our evaluations show that Anda achieves a 2.4\u00d7 speedup, a 4.0\u00d7 enhancement in area efficiency, and a 3.1\u00d7 improvement in energy efficiency on average for popular LLMs compared to the GPU-like FP-FP baseline. With its adaptability across various application scenarios and performance requirements, Anda enables efficient LLM inference in diverse deployment environments, paving the way for broader adoption of LLMs in resource-constrained settings."}, {"title": "VI. RELATED WORKS AND DISCUSSIONS", "content": "Bit-serial & bit-parallel computing. Bit-serial computing [2", "7": [37], "46": [55], "68": [75], "55": "and Bitlet-X ["}]}