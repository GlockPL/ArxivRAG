{"title": "GrabDAE: An Innovative Framework for Unsupervised Domain Adaptation Utilizing Grab-Mask and Denoise Auto-Encoder", "authors": ["Junzhou Chen", "Xuan Wen", "Ronghui Zhang", "Bingtao Ren", "Di Wu", "Zhigang Xu", "Danwei Wang"], "abstract": "Unsupervised Domain Adaptation (UDA) aims to adapt a model trained on a labeled source domain to an unlabeled target domain by addressing the domain shift. Existing Unsupervised Domain Adaptation (UDA) methods often fall short in fully leveraging contextual information from the target domain, leading to suboptimal decision boundary separation during source and target domain alignment. To address this, we introduce GrabDAE, an innovative UDA framework designed to tackle domain shift in visual classification tasks. GrabDAE incorporates two key innovations: the Grab-Mask module, which blurs background information in target domain images, enabling the model to focus on essential, domain-relevant features through contrastive learning; and the Denoising Auto-Encoder (DAE), which enhances feature alignment by reconstructing features and filtering noise, ensuring a more robust adaptation to the target domain. These components empower GrabDAE to effectively handle unlabeled target domain data, significantly improving both classification accuracy and robustness. Extensive experiments on benchmark datasets, including VisDA-2017, Office-Home, and Office31, demonstrate that GrabDAE consistently surpasses state-of-the-art UDA methods, setting new performance benchmarks. By tackling UDA's critical challenges with its novel feature masking and denoising approach, GrabDAE offers both significant theoretical and practical advancements in domain adaptation.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, machine learning and deep neural networks have revolutionized visual classification tasks, achieving remarkable accuracy and efficiency. Yet, their effectiveness typically hinges on two pivotal assumptions: the need for extensive labeled data for model training and the prerequisite that the data employed for testing or application must be consistent in distribution with the training dataset [1]-[4]. A model's success in accurately classifying data is contingent upon these conditions. Notably, a disparity in distribution between training and testing datasets, a challenge often referred to as domain shift, frequently leads to suboptimal performance during the testing or application phase [4], [5].\nSignificant manpower and resources are required for the collection and annotation of data, which underpins the training of models for visual tasks. The promise of training models on small datasets or those directly relevant to the target visual task is thus highly valued [6]. However, the issue of domain shift poses a significant limitation on model's ability to extend applicability to the target domain. As a highly competitive proposal, unsupervised Domain Adaptation (UDA) can effectively overcome this limitation [7], [8]."}, {"title": "II. RELATED WORK", "content": "Masking Techniques. The technique of masking input data before prediction was initially employed in self-supervised learning tasks within natural language processing. Recently, this concept has been effectively integrated into self-supervised pre-training methodologies within the realm of computer vision. By providing partially masked images to the network and training it to reconstruct the attributes of the masked regions, this approach has shown substantial promise in enhancing model understanding of image content [7], [20], [21]. The MAE proposed by He et al. [22] underscores the redundancy of pixels in images and the potential of masking techniques to improve model efficiency and focus. Moreover, masking images before model training has become a prevalent practice, aiding in better capturing contextual cues within images for more accurate predictions [5], [17].\nIn the task of classification in UDA, masking techniques, such as the MIC method, involve masking random blocks of target images to enforce network learning of context for semantic segmentation of the entire image. This method en-ables models to better utilize contextual cues by focusing on learning the semantics of the masked regions [7]. Our method, GrabDAE, diverges from previous works by employing a directed masking approach. Rather than treating each image region equally, it prioritizes regions critical for the visual task of classification, using traditional data segmentation methods like GrabCut [23] to refine target domain data. This approach ensures that the model focuses on areas of interest within the target domain, thereby eliminating irrelevant factors and facilitating more effective learning of semantic clues.\nUnsupervised Domain Adaptation (UDA). UDA endeavors to train models using labeled data with the aim of generalizing to the unlabeled target data [9], [24]. In general, UDA is conventionally divided into two sequential steps. The first step involves learning features from the source data with label, and the second step involves transforming the features of source data into data features usable in the target domain [3], [25]. This adaptation facilitates the deployment of the classifiers trained on the source domain in the target domain. Current research in domain adaptation primarily focuses on methods to minimize domain variance and align distict domain features into a shared space [26]\u2013[29] or employ adversarial learning to aid the model in better adapting to diverse data distributions [3], [4], [30]\u2013[32].\nAdversarial training has emerged as a promising technique for addressing the challenges presented by domain variation [3], [4], [30]\u2013[32]. In this context, the problem is conceptu-alized as a minimax game involving feature learning and a domain classifier [24], [33]. Feature learning aims to confuse and learn from cross-domain features, while the discrimina-tor's task is to differentiate whether data features are from the target or source domain [13], [34]. Generally, after passing through the feature extraction module, the data's features are separately fed into a classifier and a discriminator for domain adaptation.\nDespite these advances, most domain adaptation methods cannot adequately address the relationship between the data distribution inherent to the target domain and its correspond-ing decision boundaries, potentially leading to diminished discriminability within the target domain [13], [33], [35]\u2013[38]. Our approach seeks to overcome this limitation by incorporating self-supervised learning through reconstruction loss and minimizing consistent loss between the pseudo labels and the predictions of masked target domain data. This strategy significantly improves the recognition of inter-class disparities, consequently improving the accuracy of our model.\nDenoise Auto-Encoder (DAE). The Denoising Auto-Encoder (DAE) consists of an encoder that maps noisy inputs to a latent representation space and a decoder that reconstructs the corrupted inputs from this representation [39]. Much like how humans can recognize partially occluded or damaged images, a well-trained DAE mimics this ability by ensuring that the hidden layer representation remains consistent for both intact and damaged inputs. This consistency allows the DAE to effectively reconstruct clean input signals from corrupted or occluded inputs, showcasing the model's robustness and versatility in handling diverse input conditions [19]. DAEs are widely used in self-supervised pretraining within com-puter vision, focusing on reconstructing inputs from latent representations and highlighting the most informative features for classification [25], [40]. Our novel adaptation of the DAE leverages masked images to facilitate the learning of domain-adaptive contextual relationships, thereby significantly im-proving the model's ability to interpret structural information within the target domain [21], [36]. By introducing Gaussian noise perturbations at the encoder stage, our approach ensures that feature vectors across different domains conform to a Gaussian distribution, promoting effective transfer learning and enhancing domain alignment."}, {"title": "III. METHODS", "content": "In this section, we introduce the technical notation of our method for unsupervised domain adaptive image classification and specify the objectives in each considered setting."}, {"title": "A. Unsupervised Domain Adaption", "content": "Our method aims to train an image classification model $f_s$ capable of predicting correct labels for $T = \\{x_t\\}$, achieving similar performance to that on the labeled source domain $S = \\{(x_s, y_s)\\}$.\nFirst, we assume that the two domains possess a shared label scope. In UDA, the training process incorporates unsupervised learning on unlabeled target data $T$ and supervised learning on labeled source data $S$. Hence, the supervised categorical cross-entropy loss is computed solely based on predictions for the labeled data, denoted as $\\hat{y_s} = f_s(x_s)$.\n$L_{cls} = \\frac{1}{n_s} \\sum_{x_i \\in S} L_{ce}(y, \\hat{y_s})$\nHere, labeled source data contributes to learning domain-invariant features, while unlabeled target data facilitates fea-ture alignment across the two domains by leveraging un-derlying data distribution. The objective is to minimize the distribution discrepancy between source and target domains, improving the model's performance on the target domain by learning from the source. We achieve this through optimizing the following total loss function:\n$\\min L_{cls} + L_s + L_{DA}(x, x_t)$\nwhere $L_{cls}$ denotes the supervised loss for source domain predictions, $L_s$ the self-supervised loss for the target domain, and $L_{DA}$ the domain adaptation loss facilitating feature align-ment across domains.\nIn our framework, Model $f$ distills knowledge from the source domain to minimize the supervised loss $L_{cls}$. The domain adaptation loss is decomposed as\n$L_{DA} = L_{re} + L_D$\nThe model first uses the original, unprocessed target domain images to generate pseudo-labels, providing a supervisory signal for the unlabeled data. Following this, the model applies GrabMask to the target images and learns the consistency between these processed images and the pseudo-labels. This process reduces the influence of background noise, allowing the model to better focus on extracting domain-invariant features and mitigating domain differences.\nOur GrabDAE framework also employs a Teacher-Student architecture, where pseudo-labels are generated by an Expo-nential Moving Average (EMA) teacher model. The teacher model makes soft-label predictions based on the original target domain images, ensuring stability and robustness in the pseudo-labels. In the student model, labeled source do-main data and two types of target domain data (original and GrabMask-processed) are fed into the model. By com-puting the classification loss on the source domain and do-main adaptation loss, the model's transferability is further optimized. This dual-optimization mechanism enhances the model's performance on the source domain classification task and strengthens its adaptability to target domain data through contrastive learning and consistency regularization."}, {"title": "B. GrabMask", "content": "The Grab-Mask module is a vital component of the pro-posed GrabDAE framework for unsupervised domain adap-tation. The purpose of Grab-Mask is to guide the model to focus on regions of interest and produce accurate classification results, which do not necessarily require distant contextual information.\nWe leverage Gaussian Mixture Models (GMMs) to perform soft segmentation [41]. The method involves iteratively re-fining an initial user-defined foreground and background seg-mentation by modeling image pixels as a mixture of Gaussian distributions. Initially, we introduce seed points to provide a rough estimate of the foreground and background regions. Foreground and background distinction is further refined using the Gibbs energy equation:\n$E(y) = D_i(y_i) + V_{i,j} (y_i, y_j)$\nwhere the label $y_i$ is assigned to pixel $i$, $D$ quantifies the cost of assigning label $y_i$ to pixel $i$ based on the intensity value of the pixel, and $V$ measures the cost of assigning different labels to neighboring pixels $i$ and $j$, encouraging neighboring pixels to have the same label. Specifically, the smoothness term $V$ is defined as:\n$V(i, j) = \\gamma \\cdot exp(\\frac{||Z_i - Z_j||^2}{\\sigma^2}) \\cdot I[l_i \\neq l_j]$\nwhere:\n\u2022 $z_i$ and $z_j$ represent the color vectors of pixels $i$ and $j$.\n\u2022 $\\sigma^2$ is the variance of the color differences in the local neighborhood.\n\u2022 $I[l_i \\neq l_j]$ is an indicator function that equals 1 if the labels $l_i$ and $l_j$ of pixels $i$ and $j$ are different, and 0 otherwise.\n\u2022 $\\gamma$ is a constant controlling the strength of the smoothness term.\nTo ensure that the alignment of minimum of the energy function $E$ with a segmentation of superior quality, infor-mation derived from the frequency distributions of observed grayscale values is integrated into its formulation. The iterative minimization process ensures convergence and enhances clas-sification capabilities by directing attention towards objects of interest.\nIn synthesis, the Grab-Mask module, serves as a gateway for preprocessed, saliency-enhanced image representations through its sophisticated utilization of GMM-based soft seg-mentation and energy minimization. These representations are subsequently fed into the Unsupervised Domain Adaptation (UDA) process, guiding the model's learning towards the most visually pertinent cues. This not only enhances adaptability to domain shifts but also strengthens overall classification performance."}, {"title": "C. Self-Supervised Learning for UDA", "content": "To improve the adaptation of the model $f_s$ to the specific data, we implement a self-supervised learning approach. This method utilizes pseudo-labels derived from target data, iter-atively refining model predictions to better align with these labels. This strategy progressively boosts the model's gener-alization capabilities towards the target domain $T$, exploiting the consistency between pseudo-labels and the prediction of masked target data to achieve robust adaptation. This iterative process includes generating initial pseudo-labels for target data $T$, using them for self-supervised training [6], [42]\u2013[44], and applying consistency loss to maintain prediction stability. Over iterations, pseudo-labels are refined, enhancing model adaptation to the target domain. This approach leverages self-supervised learning to minimize dependency on labeled data, making the adaptation process scalable and efficient.\nGrabDAE withholds highly correlated information within the samples using GrabMask. We obtain the masked target image $x^M$ through GrabMask:\n$x^M = GrabMask(x^T)$\nSubsequently, the prediction for the masked target domain data $\\hat{y}^M$ is derived using the student network $f_s$:\n$\\hat{y}^M = f_s(x^M)$\nIn the target domain, samples lack labels. Self-supervised learning can obtain the advanced features of target domain data by leveraging the labels of $x^M$ and the pseudo-labels of $x^T$. The self-supervised loss is introduced as:\n$L_s = L_{ce}(\\hat{y}^M, p^T)$\nSignificantly, the pseudo-label is the prediction of the com-plete target domain image $x_t$ made by a teacher network $f_t$.\n$p^T = arg \\max f_t(x^T)$\nThroughout the self-supervised training process, the student model $f_s$ is trained on masked target data and the teacher model $f_t$ dynamically generates pseudo-labels from original Target data. Specifically, the teacher model $f_t$ updates itself after each step, incorporating an exponentially weighted aver-age of the student model's weights. Note that no gradients are propagated back into teacher model $f_t$."}, {"title": "D. Denoise Auto-Encoder", "content": "In Fig. 3, model is structured in three phases: feature reconstruction, classification, and discrimination. In the feature reconstruction phase, image data is passed through $g$ and the DAE to remove noise and enhance the stability of learned representations. In the classification phase, both original and reconstructed features are passed through a Dropout layer and then input to $C$, ensuring regularization and promoting better generalization. Finally, in the discrimination phase, both feature types are input into $D$ to align feature distributions across domains by distinguishing between domain-specific and domain-invariant features.\nIn this setup, $g$ and $D$ engage in a minimax game: $g$ extracts generalizable features from the data, while $D$ distinguishes the features from the source and target domains. The discriminator loss can be computed as:\n$L_D = - \\frac{1}{n} \\sum_{x_i \\in T} \\sum_{y \\in Y} L_{ce}(D(g(x)), y^d)$\nwhere $n = n_s + n_t$, $y^d$ denotes the domain label, with $y^d = 1$ indicating the source domain and $y^d = 0$ indicating the target domain. The superscript * represents either s (source domain) or t (target domain).\nTo boost the adaptability of the model for the target domain, our framework emphasizes aligning the category-level feature space. This alignment needs to filter out task-independent nuisance factors while maintaining pure semantic information. A typical DAE is a self-training deep neural network architecture that does not leverage label information. Figure. 4 visualizes the process of reconstructing a corrupted input into a purified one. In our experiments, the input x is corrupted by introducing Gaussian noise, with the extent of corruption being determined by a randomly generated ratio v. This process removes information about the selected components from the input pattern, inducing the Auto-Encoder to subsequently learn to \"fill\" these introduced \"gaps\".\nThe encoder operates on the corrupted data $\\hat{x}$ to encode it into the latent representation $y = f_e(x) = \\sigma(Wx + b)$. Here, $\\theta$ represents the parameters of the encoder. Subsequently, the decoder outputs the reconstructed data $z = g_{\\theta'}(y) = \\sigma(W'y + b)$ from the representation, where $\\theta'$ denotes the parameters of the decoder.\nThe process of reconstruction is implemented through the following optimization:\n$\\arg \\min_{\\theta,\\theta'} [L_{re}(X, g_{\\theta'} (f_e(X)))]$\nThrough learning the above reconstruction loss, the model can obtain advanced semantic information about the target domain dataset."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate our proposed model GrabDAE, we conduct experiments on three public benchmarks.\nOffice-Home [19]. Office-Home comprises 15,500 images spanning 65 classes and 4 distinct domains: Clipart (C), Art (A), Product (P), and Real-World (R). Each domain encom-passes 65 categories.\nVisDA-2017 [45]. Designed to address the simulation-to-real-world transition, VisDA-2017 encompasses approximately 280K images distributed across 12 classes.\nOffice31 [46]. Consisting of three subsets\u2014Amazon (A), Webcam (W), and DSLR (D)\u2014Office31 features images sourced from amazon.com for the Amazon subset, and images captured by a web camera and a DSLR camera for the Webcam and DSLR subsets, respectively. Overall, the dataset comprises 4,652 samples spread across 31 categories."}, {"title": "B. Experiment Protocols", "content": "As shown in Fig. 3, the architecture of GrabDAE consists of four components: the domain discriminator $D$, the feature extractor $g$, the DAE module, and the classifier $C$. The feature extractor $g$ is implemented using the Swin Transformer. For classification, a conventional softmax classifier is employed, while the domain discriminator is implemented through Condi-tional Domain Adversarial Networks (CDAN) [53]. The DAE module is designed with three primary sub-components: a noise injection stage, followed by dedicated encoding and decoding processes."}, {"title": "C. Results", "content": "In the task of image classification in UDA, the model is trained with the source data and subsequently evaluated using the target data.\nResults on VisDA-2017. As shown in Table I, our method reaches 91.6% accuracy and outperforms the baseline(CVPR 2024) by 0.7%. Notably, for some challenging categories, such as \"bicycle,\" our method consistently realizes a significant performance improvement, increasing from 92.8% to 96.2%, an enhancement of 3.4%.\nResults on OfficeHome. As shown in Table II, our Grab-DAE framework achieves noticeable performance gains and surpasses TVT, DAMP, and MIC by a large margin. Impor-tantly, our GrabDAE achieves an improvement of more than 3.4% accuracy over the SOTA method PMTrans, yielding 92.4% accuracy. Incredibly, GrabDAE almost surpasses the SOTA methods in all subtasks, which demonstrates the strong ability of GrabDAE to alleviate the large domain gap.\nResults on Office-31. Table III shows the quantitative results of various models in different tasks. Overall, our Grab-DAE achieves superior performance in most tasks, reaching 95.6% accuracy and surpassing SOTA methods. Specifically, GrabDAE demonstrates significant improvements over other methods, with an accuracy increase of +0.3% over PMTrans, +1.7% over TVT, and +3.0% over CDTrans."}, {"title": "D. Ablation Study", "content": "To unveil the individual contributions of the GrabDAE framework's components, we conducted comprehensive abla-tion studies. These experiments were designed to validate the effectiveness of our approach in Unsupervised Domain Adap-tation for image classification. Using the Swin Transformer with SDAT [16] as our baseline, we explored the impact of the innovative Grab-Mask module and other key elements of GrabDAE.\n1) Evaluating the Grab-Mask Module: A focal point of our study was the Grab-Mask module. We compared it against alternatives like MaskRNN [57] and saliency detection [59] to highlight the comparative advantages of the Grab-Mask mod-ule in domain adaptation. Table IV shows that incorporating Grab-Mask into GrabDAE significantly enhances accuracy by 2.3% over the baseline and by 15.1% over the variant using MaskRNN. This improvement showcases Grab-Mask's ability to effectively focus the model on relevant visual information, reducing the impact of background distractions.\nThe superior performance of Grab-Mask can be attributed to its saliency-aware preprocessing mechanism, which effec-tively enhances the model's focus on relevant visual features and reduces the influence of irrelevant background elements. Unlike MaskRNN, which may introduce noise or inaccuracies during the masking process, Grab-Mask leverages Gaussian Mixture Models to perform soft segmentation, providing a more reliable and accurate representation of salient regions within images. Additionally, compared to a standalone saliency module, Grab-Mask integrates seamlessly into the GrabDAE framework, enabling more effective feature extraction and domain adaptation.\n2) Component-Wise Effectiveness: We further dissected GrabDAE's performance by examining different combinations of classification loss ($L_{cls}$), self-supervised loss ($L_s$), and reconstruction loss ($L_{re}$). The collective application of these components, as seen in Table V, yielded the highest average accuracy of 92.4%. This configuration outperformed others, emphasizing the crucial role of reconstruction loss in enhanc-ing feature quality and minimizing domain differences.\n3) t-SNE Visualization of Model Performance: To elucidate the efficacy of our training scheme on domain-specific feature distribution, we utilized t-SNE to visualize the embeddings of the target domain within the OfficeHome dataset, particularly focusing on the Rw \u2192 Ar UDA task. Figure. 5 offers a comparative insight into the feature representations learned through our GrabDAE framework versus those from other alternative models. Figure. 5(b) shows the visualization of features without adaptation, while Figure. 5(a) and (c) de-pict features after global alignment from other models-(c) illustrate the feature embeddings from both the source and target domains as learned by the competing models. Figure. 5(d) showcases the embeddings derived from GrabDAE and reveals a more cohesive alignment of domain distributions, which demonstrates the GrabDAE's capability in mitigating the divergence between domains and aligning category-level features more effectively. This visual comparison underscores the superior adaptability and performance of our method in handling domain shifts, highlighting its potential in fostering robust domain adaptation.\nIn summary, this ablation study clearly outlines the essential roles played by the Grab-Mask module and GrabDAE's loss components in achieving superior domain adaptation per-formance. The empirical evidence supports our framework's efficacy in addressing domain shifts, paving the way for future research to expand our methodology to a broader range of tasks and challenges in domain adaptation."}, {"title": "V. CONCLUSION", "content": "In this study, we presented GrabDAE, a novel Unsupervised Domain Adaptation (UDA) framework for image classifica-tion. This framework addresses domain transfer challenges during training by purifying features through the reconstruc-tion of masked features using the Denoise Auto-Encoder (DAE) module. Our empirical evaluations demonstrate that GrabDAE achieves superior category-level alignment and competitive, often superior, adaptation performance compared to more complex methods. Moreover, the applications of our approach extend beyond object classification to areas such as 3D visual learning, object detection, and semantic segmentation, which are also affected by domain shifts.\nFuture work could explore integrating additional self-supervised learning techniques to enhance feature extraction and domain alignment. Another promising direction is ap-plying the extension of the GrabDAE framework to differ-ent Unsupervised Domain Adaptation (UDA) settings, such as open-set domain adaptation [38], [60], where the target domain contains unknown classes not present in the source domain. Extending the framework to handle more complex and heterogeneous datasets, including multi-modal data, may also broaden its applicability and robustness. Extending GrabDAE to handle more complex and heterogeneous datasets, including multi-modal data, would also broaden its applicability and robustness. Overall, GrabDAE offers a strong foundation for UDA in image classification, with substantial potential for adaptation to diverse domain-shifted tasks."}]}