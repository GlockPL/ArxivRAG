{"title": "CIRCUITSYNTH: Leveraging Large Language Models for Circuit Topology Synthesis", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Xin Zhang"], "abstract": "Circuit topology generation plays a crucial role in the design of electronic circuits, influencing the fundamental functionality of the circuit. In this paper, we introduce CIRCUITSYNTH, a novel approach that harnesses LLMs to facilitate the automated synthesis of valid circuit topologies. With a dataset comprising both valid and invalid circuit configurations, CIRCUITSYNTH employs a sophisticated two-phase methodology, comprising Circuit Topology Generation and Circuit Topology Refinement. Experimental results demonstrate the effectiveness of CIRCUITSYNTH compared to various fine-tuned LLM variants. Our approach lays the foundation for future research aimed at enhancing circuit efficiency and specifying output voltage, thus enabling the automated generation of circuit topologies with improved performance and adherence to design requirements.", "sections": [{"title": "I. INTRODUCTION", "content": "Circuit topology synthesis stands as a complex and critical aspect of electronic circuit design. The configuration and in-terconnection of components directly influence critical circuit functionality and performance. With the increasing demands for integration and complexity in modern electronic systems, the role of circuit topology synthesis becomes crucial in meet-ing design specifications and performance criteria. However, relying solely on human intervention for topology synthesis is a formidable challenge. As the complexity of contemporary circuit designs grows, the search space expands exponentially, rendering exhaustive or random exploration impractical.\nIn this context, we introduce CIRCUITSYNTH, an innovative approach that harnesses the power of LLMs for automated circuit topology synthesis. Figure 1 provides an overview of the proposed method, with the goal of generating a valid circuit topology in the form of a netlist based on a text prompt. CIRCUITSYNTH employs a sophisticated methodology, leveraging an extensive dataset of valid and invalid circuit configurations.\nOur proposed approach adopts a two-phase model archi-tecture, comprising Circuit Topology Generation and Circuit Topology Refinement. In the initial phase, we fine-tune a LLM to produce a circuit topology in an autoregressive manner. Since the generated circuits may not consistently meet validity constraints, in the Circuit Topology Refinement phase we undertake two pivotal steps. Firstly, we employ a classifier to gauge the likelihood that a generated circuit topology adheres to the requisites of a valid circuit design. This ensures alignment with the necessary design parameters. Secondly, in Generation Enhancement we refine the circuit topology generation process, by minimizing the combined loss of negative log-likelihood with the circuit invalidity score. We evaluate our model by generating new circuit topologies using the trained model and passing them to the SPICE simulator to verify their validity. Experimental results demonstrate the effectiveness of CIRCUITSYNTH compared to various LLM variants, emphasizing its potential for automating circuit topol-ogy synthesis.\nContributions: The key contributions are summarized as follows: (a) Introduction of CIRCUITSYNTH, a novel method-ology leveraging LLMs for automated circuit topology synthe-sis; (b) Generation of a comprehensive dataset encompassing both valid and invalid circuit configurations, facilitating the development and evaluation of CIRCUITSYNTH; (c) Development of a circuit validity classifier to assess a circuit's validity, enhancing the reliability of CIRCUITSYNTH outputs."}, {"title": "II. OUR APPROACH", "content": "We curated a dataset of power converter circuit designs with 5 device components using Random Search (RS) [2] and NGSpice [3] simulator. Together we collected a total of 862,606 circuits (567,307 valid and 295,299 invalid ones). In this study we only consider devices including capacitors C,\ninductors L, phase-I switches Sa and phase-II switches St. More details about our dataset are described in Appendix A."}, {"title": "C. Circuit Topology Generation", "content": "We formulate the task of circuit topology generation as a text generation problem. In this paper, we work with datasets D = {(Xi, Yi)}=1 where X\u2081 refers to the input prompt and Y refers to the valid circuit topology netlist. Each entry in the netlist corresponds to a node in an undirected graph 9, with edges denoting interconnections between these nodes. The choice of encoding strategy for representing the netlist textually plays a pivotal role, as it significantly influences the effectiveness of LLMs in our task. Therefore, we adopt the \"Incident\" encoding strategy, known for its superiority over other encoding methods in various graph-related tasks [6]. Figure B.1 in Appendix B demonstrates an example of how a netlist is represented using the incident encoding method. Formally, the circuit topology generation process can be described as:\nY~EX~D[PLLM(Y|X)]\nWhere PLLM is the pretrained LLM parameterized by a set of parameters 0. This can be further decomposed into:\nPLLM(Y|X) = \\prod_{t=1}^{N} PLLM(Y_{t}|X, \\{Y_{j}\\}_{j=1}^{t})\nTo tune the pretrained LLM, we use the conventional negative log-likelihood objective:\nLLLM = \\sum_{t=1}^{N} - log PLLM(y_{t}|X, \\{y_{j}\\}_{j=1}^{t})\nThis objective generally ensures fluency of text in the incident encoding method. However, this objective can be incomplete. The circuit topology generated may not include all the com-ponents in the component pool or satisfy circuit validity constraints. In order to ensure the circuits meet additional constraints, we introduce the circuit topology refinement phase that learns to incorporate specific constraints into the circuit topology generation process."}, {"title": "D. Circuit Topology Refinement", "content": "This phase comprises two main steps: (a) Circuit Validity Estimation, which evaluates whether the generated circuit topology adheres to necessary constraints, and (b) Generation Enhancement, which utilizes feedback from the constraint estimation to improve the overall circuit topology generation process. In this study, the primary focus is on optimizing for circuit validity. We elaborate on these steps below.\n1) Circuit Validity Estimation: We train a dedicated classi-fier, fvalid, on the dataset D = {Dvalid\u016aDinvalid}, which includes both valid and invalid circuit topologies. The output of the classifier, pvalid, represents the probability of validity for a given circuit topology. We employ a RoBERTa-based classifier [7] optimized with binary cross-entropy loss, achieving an F1 score of 92% for binary classification of circuit validity."}, {"title": "2) Generation Enhancement", "content": "To refine the circuit topology generation process, we utilize the pretrained circuit validity classifier to compute a circuit validity loss:\nLvalid = (1 - Pvalid)\nSubsequently, we combine this circuit validity loss with the standard negative log-likelihood loss LLLM to ensure adherence to circuit validity constraints. Formally, the combined loss function is defined as:\nL = 11LLLM + 12Lvalid\nHere, 11 and 12 are learnable coefficients that determine the relative importance of each loss component.\nDuring training, a significant challenge arises when sam-pling Y from the distribution PLLM to feed into the circuit validity classifier. This sampling process involves drawing samples from a non-differentiable categorical distribution, hindering gradient propagation [8]. To address this issue, we employ the Gumbel-softmax relaxation [4], [5] to approximate the discrete sampling process \u0177(t) ~ PLLM. Firstly, we apply the Gumbel-Max trick to reparameterize sampling from PLLM. This is given as follows:\n\u0177(t)\nu(i) ~ uniform(0, 1)\nz(i,t) = \u2013 log(-log(u(i)))\n\u1ef9(t) = one-hot argmax\u2208V(Pit) + z(i,t))] z(it)\nWhere V is the size of the vocabulary, PLLM refers to the logits, i.e., pre-softmax activation of PLLM at the t-th genera-tion step for the i-th word, and z(i,t) are i.i.d. samples from the standard Gumbel distribution. Next, we approximate the discrete argmax operation in Equation 8 with the continuous softmax operator to ensure differentiability as:\n\u1ef9(t) = softmax\nwhere 7 is a temperature hyperparameter, which controls how close \u1ef9(t) is to \u0177(t). Finally, to enable gradient flow during training, we utilize the straight-through estimator [9]. In this approach, we use \u0177(t) in the forward pass and \u1ef9(t) in the backward pass, allowing for efficient backpropagation."}, {"title": "III. EXPERIMENTS", "content": "In this section, we provide details about the models, base-lines and metrics used to train and evaluate. We trained and compared two LLMs for CIRCUITSYNTH: GPT-Neo-2.7 and StableLM-3B-4E1T (refereed to as GPT-Neo and StableLM in subsequent sections). More model descriptions and implementation details are provided in Appendix C and Appendix D, respectively."}, {"title": "A. Baselines", "content": "We conducted experiments with the following baselines:\n1) Zero-Shot Generation: We provide prompts containing the components pool as input to LLMs including Llama-2 (13b) [10] and Flan-Ul2 (20b) [11] without any fine-tuning.\n2) In-Context Learning (ICL): We provide demonstrations of input prompts with component pools and example circuits preceding a new input prompt to Llama-2 (13b) and Flan-Ul2 (20b) models. We limit the number of in-context examples to k\u2208 {5,10,20} and experiment with incident encoding and netlist array-like structures for representing circuit topologies.\n3) Parameter-Efficient Fine-Tuning (PEFT): We adopt sim-ple PEFT-based approaches [12] to tune Llama-2 (13b) and Flan-Ul2 (20b) models to investigate if a relatively smaller language model (GPT-Neo/StableLM) trained using our ap-proach can achieve comparable performance to much larger LLMs PEFT-tuned for the same task. We performed Prompt-tuning which involves learning task-specific soft prompts and adding them to the input while keeping the pre-trained model parameters frozen. In our study, we explore different numbers of trainable soft prompt tokens, ranging from 100 to 500.\n4) Vanilla Fine-Tuning: GPT-Neo and StableLM models are fine-tuned with the objective of minimizing the negative log-likelihood for generating circuit topologies.\n5) CIRCUITSYNTH (CS): We introduce CIRCUITSY\u039d\u03a4\u0397, our complete framework aimed at enhancing fine-tuned cir-cuit topology generation using a circuit validity classifier to improve the validity of generated circuit topologies."}, {"title": "B. Metrics", "content": "In our evaluation setup, we report the fraction of unique cir-cuit topologies that are estimated as valid by the independent validity classifier and the SPICE simulator as E(fvalid(\u0177)) and E(fsvalid (y)) respectively, based on 1000 samples of unique circuit topologies. In our experiments, a circuit topology is considered valid if it has a validity score, surpassing 0.6, from the circuit validity classifier. Additionally, we report the efficiency of the generated circuit as computed using the SPICE simulator as E(fser (y)). The SPICE simulator evaluates the validity and efficiency of a given netlist by verifying its electrical characteristics and performance metrics through detailed circuit simulations. It evaluates parameters such as voltage levels, timing, and power consumption of the circuit topology with certain duty cycles under given condi-tions. Furthermore, we calculate a Duplicate Generation Rate (DGR), denoted by p, defined as \"# topologies generated\" divided by \"# unique topologies\"."}, {"title": "IV. RESULTS & DISCUSSION", "content": "Table I presents the evaluation results of our circuit synthe-sis models, showing the ratios of valid, unique, and efficient circuits, along with the DGR (p). Each model is assessed by generating 1000 unique sample circuit topologies. Our findings suggest that CIRCUITSYNTH models consistently outperforms most baselines across various evaluation metrics. Note that the smaller language models tuned using our method perform comparably with larger prompt-tuned models."}, {"title": "B. Performance of Zero-Shot and ICL Methods", "content": "We observe a significant performance gap between zero-shot generation methods and fine-tuned approaches. Zero-shot generation struggles to produce valid netlist-like structures for subsequent classification or simulation. Even with ICL, where the model is provided with examples, there is only a marginal improvement and the completion rate for all components remains unsatisfactory. Furthermore, increasing the number of in-context examples k resulted in diminishing returns."}, {"title": "C. Effectiveness of CIRCUITSYNTH", "content": "1) Comparison with Similar-sized Model Fine-tuning: Our CIRCUITSYNTH models demonstrate a marked superiority in terms of generating valid circuits compared to vanilla fine-tuned GPT-Neo and StableLM architectures. Specifically, the CIRCUITSYNTH model utilizing GPT-Neo demonstrates a no-table enhancement in circuit validity and efficiency compared to its vanilla counterpart. As both models have a similar size, this observation underscores the efficacy of our approach in improving the synthesis of valid circuit topologies. Also, the duplicate generation rate for our CIRCUITSYNTH models is significantly lower compared to other fine-tuned models, indicating faster production of unique circuit topologies.\n2) Comparison with PEFT-tuned Models: Our experiments demonstrate that PEFT-based prompt tuning of a Flan-ul2 model (~ 20-billion parameters), yields performances com-parable to that of our models with ~ 3-billion parame-ters. Notably, we find that a smaller language model (GPT-Neo/StableLM) trained using our approach can achieve com-parable performance to LLMs fine-tuned for the same task. The duplicate generation rate of our approach is much lower than the prompt-tuned LLama-2/Flan-ul2 models, which indi-cates that our method empowers a smaller language model to produce unique circuit topologies faster than PEFT-tuned 20 billion parameter models. An intriguing finding is the capacity of a smaller fine-tuned model to effectively capture extensive information from a larger prompt-tuned model. This highlights the effectiveness of our proposed methodology."}, {"title": "D. Validity Correlation", "content": "To evaluate the correlation between classifier prediction probabilities and simulator-assessed ground truth validity scores, we used a two-sample t-test. We analyzed 1000 sample circuit topology generations each from CSGPT-Neo and CS Stable LM. The classifier prediction probabilities were treated as the continuous predictor variable, while the binary ground truth validity served as the outcome variable. With a statistically significant p < 0.05, we found a strong correlation between the classifier predictions and the ground truth validity scores provided by the simulator."}, {"title": "E. Ablation Study", "content": "We present an ablation study to investigate the impact of natural language incident encoding on the model's perfor-mance by replacing the encoding with an array-like structure to represent netlists (see the left part of Figure B.1). Table II shows the results of our evaluation of CSGPT-Neo and CS StableLM with and without natural language incident en-coding. For CSGPT-Neo, we observe significant improvement in both metrics with natural language incident encoding. Conversely, for CSStableLM, the impact is less pronounced. We hypothesize that this disparity is due to differences in their training data. StableLM, trained on both natural language and coding datasets, may rely less on explicit natural language representations for effective circuit synthesis. Thus, our abla-tion study shows that the benefits of natural language incident encoding depend on the model architecture."}, {"title": "F. Potential Emergent Capability: Efficiency Scores", "content": "An emergent capability of our models is the production of circuit topologies with good efficiency scores. Despite the primary objective being the generation of valid circuits, our models demonstrate the additional ability to optimize for efficiency, further enhancing their practical utility."}, {"title": "V. CONCLUSION", "content": "In this study, we introduced CIRCUITSYNTH, a novel method that harnesses LLMs to automate circuit topology syn-thesis. Leveraging a circuit validity classfier and the Gumbel-softmax trick, CIRCUITSYNTH was trained to enhance the overall validity of generated circuits. Experimental results demonstrate that CIRCUITSYNTH yields significant improve-ments across multiple metrics when compared to various LLM variants. CIRCUITSYNTH offers a promising avenue for revo-lutionizing electronic circuit design, promising advancements in efficiency, performance, and scalability."}]}