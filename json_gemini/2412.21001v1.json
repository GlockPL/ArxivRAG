{"title": "LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency", "authors": ["Xiao-Yin Liu", "Guotao Li", "Xiao-Hu Zhou", "Zeng-Guang Hou"], "abstract": "Abstract-Offline preference-based reinforcement learning (PbRL) provides an effective way to overcome the challenges of designing reward and the high costs of online interaction. However, since labeling preference needs real-time human feedback, acquiring sufficient preference labels is challenging. To solve this, this paper proposes a offLine prEference-based RL with high Sample Efficiency (LEASE) algorithm, where a learned transition model is leveraged to generate unlabeled preference data. Considering the pretrained reward model may generate incorrect labels for unlabeled data, we design an uncertainty-aware mechanism to ensure the performance of reward model, where only high confidence and low variance data are selected. Moreover, we provide the generalization bound of reward model to analyze the factors influencing reward accuracy, and demonstrate that the policy learned by LEASE has theoretical improvement guarantee. The developed theory is based on state-action pair, which can be easily combined with other offline algorithms. The experimental results show that LEASE can achieve comparable performance to baseline under fewer preference data without online interaction.", "sections": [{"title": "I. INTRODUCTION", "content": "REINFORCEMENT learning (RL) has been widely applied in robot control [1], [10] and healthcare [11], [12] fields. However, in the real-world scenarios, RL faces two serious challenges: 1) Since RL learns the optimal policy through trial and error, it is dangerous for agent to interact directly with environment, especially human-in-loop control [40]; 2) Due to the complexity of human cognitive and control system, it is difficult to design a quantitative reward function to accurately reflect the human intention or preference [9]. Many researches have been explored to solve the above two challenges.\nOffline RL, learning policy from previous collected dataset without interaction with environment, has become an effective way for challenge 1 [4], [8]. However, due to limited data coverage, offline RL suffers from the distribution shift between the learned policy and the behavior policy [28], which leads to poor agent performance. To address this problem, offline RL either directly constrain the policy closed to the behavior policy [2] or penalize value function for out-of-distribution (OOD) data [4], [8]. The above methods improve the performance of agent through incorporating conservatism into policy learning.\nPreference-based RL (PbRL), also known as RL from human feedback (RLHF), which learns the reward function from human preference data without the demand for tedious hand-engineered reward design [14], [43], has been developed recently for challenge 2. However, compared with data (s, a, s'), the collecting cost of preference data is higher since it demands human real-time feedback to label preference, which often brings high sample complexity (low sample efficiency) [3], [13]. Some data augmentation techniques, such as generating unlabeled data through interaction with simulator [3] or temporal cropping [35], have been proposed to solve problem of the limitation of preference data. However, the above methods need online interaction with environment (simulator) and still demand large human feedback. Thus, in PbRL, the sample efficiency is still of utmost importance.\nIn offline PbRL setting, existing methods [17], [33], [34] typically involve two steps: reward learning and policy learning. They learns the policy based on offline static dataset without reward and preference dataset, and introduce transformer [17] or diffusion model [33] to improve the performance of reward model and agent performance. However, the theoretical guarantees of reward generalization and policy improvement are not provided. Therefore, this paper aims to design a high sample efficiency offline PbRL algorithm that can achieve comparable performance with baseline from fewer preference data without interaction with environment, and develop the related theory for reward generalization bound and policy improvement guarantee. The key questions can be summarized as follow:\n1) In algorithm, the collected preference data fails to cover all state-action region and the collecting cost is too high. How to use the limited preference data to learn accurate reward model and guarantee agent performance?\n2) In theory, what factors affect the reward generalization and what is the detailed relationship between offline RL algorithm itself, the reward gap and the improvement of policy performance?\nFor question 1, motivated by model-based offline RL, which learns the environment model to broaden data coverage [36], [37], we also train the transition model to achieve data augmentation (improve sample efficiency). Specifically, the agent generates two different trajectories through interaction with the learned transition model, and then the pretrained reward model generates pseudo label for these two trajectories. However, the preference model may generate incorrect pseudo label, which may bring unstable training and low performance.\nFor question 2, there are very fewer algorithms for offline PbRL theory. The most relevant work is [18], where the theory of policy improvement guarantee is established. However, the generalization bound for reward model is not considered, and this theory is based on the whole trajectory rather than state-action pair in offline algorithms, which is not conducive to the theoretical analysis of specific offline PbRL algorithms since many offline RL are based on state-action pair.\nTo solve the above problems, this paper proposes a novel offLine prEference-based RL with high Sample Efficiency algorithm (LEASE), where a selecting mechanism is designed to guarantee the quantity of generated dataset. This selecting mechanism is based on the principles of high confidence and low uncertainty, and can effectively reduce the error of pseudo-labels. Moreover, we develop a new generalization bound for reward model, and provide the theory of policy improvement guarantee for offline PbRL based on state-action pair. The theoretical and experimental results show that LEASE has policy improvement guarantee and can achieve comparable performance to baseline under fewer preference data on D4RL benchmark. The contributions are given below:\n1) A novel learning framework, named LEASE, is proposed for offline PbRL, where an innovative preference augmentation technique is utilized to improve sample efficiency.\n2) An uncertainty-aware mechanism is designed for screening the generated preference data so as to guarantee the stability of reward training and improve the accuracy of reward prediction.\n3) The generalization bound of reward model and the theory of policy improvement based on state-action pair are developed to analyze factors that influence reward and policy performance.\nThe framework of this paper is as follows: Section II introduces the related works about offline PbRL. Section III provides the basic notations for algorithm. Section IV illustrates the two main problems that this paper aims to solve. Section V introduces the details of designed selecting mechanism and derives the generalization bound for reward model. Section VI illustrates the implementation details of the proposed method and provides theory for offline PbRL. Section VII conducts experiments to validate the effectiveness of LEASE and analyze the effects of each part for agent performance. Section VIII further discusses the offline PbRL theory and algorithm. Section IX summarizes the entire work."}, {"title": "II. RELATED WORK", "content": "This section presents a brief overview for the algoriths and related theories of offline PbRL, and the differences and advantages between our work and the previous related works.\nThe algorithm for offline PbRL. Offline PbRL eliminates the demand for interaction with environment and handcraft designed reward. OPAL [34] is the first algorithm that combining offline RL and PbRL. PT [17] utilized transformer-based architecture to design preference model capable of generating non-Markovian rewards. OPPO [30] directly optimized the policy in a high-level embedding space without learning a separate reward function. However, the above methods demands a large amount of preference dataset. FTB [33] trains a diffusion model to achieve data augmentation, but this consumers larger training cost. Instead, our method LEASE trains simple transition model to generate unlabeled data and designed selecting mechanism to ensure the quality of the generated data, which can achieve superior performance under fewer dataset and less time.\nThe theory for offline PbRL. There are few algorithms that provide theoretical guarantees for offline PbRL, including the generalization bound of reward model and the guarantee of policy improvement. Zhu et al. [27] studied offline PbRL, but the analysis are restricted to linear model. Zhan et al. [18] extended to general function approximation, but the generalization analysis of reward model is not provided and the theory is based on trajectory. LEASE gives the theoretical analysis for reward model, and provides the theoretical guarantee for policy based on state-action pair. The generalization bound of reward model has generality and can be applicable to methods that train reward model using pseudo-labels, and the theory of policy improvement guarantee of offline PbRL can be easily combined with other offline RL.\nData selection techniques. Data screening mechanism essentially belongs to the field of semi-supervised learning. The goal of semi-supervised learning is using unlabeled data to improve model performance when labeled data is limited. Data selection techniques is used to filter the data with clean labels from a noisy dataset. Han et al. [7] selected unlabeled data with small losses from one network. Li et al. [5] used a two-component GMM to separate the dataset into a clean set and a noisy set. Rizve et al. [31] modeled the prediction uncertainty of unlabeled data to screen data. Xiao et al. [6] selected data with high confidence as clean data. Motivated by [31] and [6], we trained ensemble reward model to select data with high confidence and low variance to guarantee the quality of unlabeled data.\nIn PbRL methods, Surf [3] is the most similar to our method LEASE that uses data augmentation technique. The differences between them mainly includes the below three aspects: 1) Surf belongs to online RL, but LEASE belongs to offline RL. Surf generates data through interaction with environment (simulator) while LEASE generates data through the learned transition model. Note that, in most human-in-loop-control scenarios, the cost of online interaction is large and the realistic simulator is difficult to create; 2) Surf only uses confidence for label filtering, whereas LEASE employs both confidence and uncertainty principles, which effectively reduce pseudo-label error. The results in Table III have validated this; 3) LEASE provides the general theoretical framework for offline PbRL, but Surf don't provide theoretical analysis."}, {"title": "III. PRELIMINARIES", "content": "In this section, we provides the basic notations for reinforcement learning algorithm, including offline RL, preference-based RL and model-based RL.\nOffline reinforcement learning. The framework of RL is based on the Markov Decision Process (MDP) that is described by the tuple M = (S, A, R, T, \u03c1, \u03b3), where S is the state space, A is the action space, R: S \u00d7 A \u2192 R is the reward function, T : S \u00d7 A \u2192 \u2206(S) is the transition dynamics, p is the initial state distribution, and \u03b3\u2208 (0,1) is the discount factor [40]. The term \u0394(\u03a9) denotes the set of probability distribution over space \u03a9. The goal of RL is to optimize the policy \u3160 that maximizes the expected discounted return J(\u03c0, R) := E(s,a)~d(s,a) [R(s,a)]/(1 \u2212 \u03b3), where d(s, a):= d(s)\u3160(as) is the state-action marginal distribution under the learned policy \u03c0. The discounted state marginal distribution d(s) is denoted as (1 \u2212 \u03b3) \u03a3\u03c4=oyP(st = \u03c2|\u03c0), where P(St = \u03c2|\u03c0) is the probability of reaching states at time t by rolling out \u03c0. The policy \u03c0 can be derived from Q-learning [41], which learns the Q-function that satisfies Bellman operator: TQ(s, a) := R(s,a) + Es'~TM(s'|s,a) [maxa'\u2208AQ(s', a')].\nThe goal of offline RL is to learn a policy from offline dataset Doffline = {(si, ai, ri, si)}{=1 collected by the behavior policy \u03bc. The policy learning includes two parts: policy evaluation (minimizing the Bellman error) and policy improvement (maximizing the Q-function), that is\n$Q \\leftarrow \\arg \\min _{Q} \\mathbb{E}_{s, a, s^{\\prime} \\sim D}\\left[Q(s, a)-\\mathcal{T} Q(s, a)\\right]^{2}$,\n$\\pi \\leftarrow \\arg \\max _{\\pi} \\mathbb{E}_{s \\sim D, a \\sim \\pi}[Q(s, a)]$, (1)\nwhere D can be a fixed offline dataset or replay buffer generated by the current policy interacting with the environment. The operator is the Bellman operator based on sample, denoted as TQ(s, a) := R(s, a) + y maxa'\u2208AQ (s', a') [4].\nPreference-based reinforcement learning. Different from standard RL setting, the reward function is not available in PbRL. Instead, PbRL learns the reward function R from preferences between pairs of trajectory segments to align the human intention [15], where a trajectory segment o of length L is defined as a sequence of states and actions {Sk, ak,\u2026\u2026, Sk+L\u22121,ak+L\u22121} \u2208 (S \u00d7 A)L. Given a pair of segments (\u03c3\u03bf, \u03c3\u2081), human choose which segment is preferred, i.e., y \u2208 {0,1} [14]. The preference label y = 1 indicates \u03c3\u03b9 \u27a2 \u03c3\u03bf and y = 0 indicates \u03c3\u03bf \u27a2 \u03c3\u2081, where \u03c3i \u27a2 \u03c3j denotes that the segment i is preferable to the segment j.\nThe preference data is stored in the dataset Di as a tuple (\u03c3\u03bf, \u03c31,y). To obtain the reward function R parameterized by \u03c8, the Bradley-Terry model [16] is utilized to define a preference predictor following previous works [17], [32]:\n$P_{\\psi}\\left(\\sigma_{1} \\succ \\sigma_{0}\\right)=\\frac{\\exp \\sum_{t} R_{\\psi}\\left(s_{t}, a_{t}\\right)}{\\exp \\sum_{t} R_{\\psi}\\left(s_{t}, a_{t}\\right)+\\exp \\sum_{t} R_{\\psi}\\left(s_{t}, a_{t}\\right)}$.\n(2)\nThen, based on the collected preference dataset Di, the reward function Ry can be updated through minimizing the cross-entropy loss between the predicted and the true preference labels [32]:\n$\\mathcal{L}_{R}(\\psi)=-\\mathbb{E}_{(\\sigma_{0}, \\sigma_{1}, y) \\sim D_{i}}[y \\log P_{\\psi}\\left(\\sigma_{1} \\succ \\sigma_{0}\\right)+\\left(1-y\\right) \\log P_{\\psi}\\left(\\sigma_{0} \\succ \\sigma_{1}\\right)]$.\n(3)\nAfter obtaining the reward function R, offline PbRL optimizes the policy by maximizing the expected discounted return J(\u03c0, R) like standard offline RL.\nModel-based reinforcement learning. Model-based RL learns the dynamics model to improve sample efficiency [36], [38], [39]. They used the offline dataset Doffline to estimate transition model T(s'|s, a). The transition model To parameterized by is typically trained via maximum likelihood estimation (MLE):\n$\\mathcal{L}_{T}(\\varphi)=-\\mathbb{E}_{\\left(s, a, s^{\\prime}\\right) \\sim D_{\\text {offline }}}\\left[\\log T_{\\varphi}\\left(s^{\\prime} \\mid s, a\\right)\\right]$.\n(4)\nIn practical implantation, the transition model is approximated by Gaussian distribution and the MLE method is employed to train Nr ensemble transition models {T\u03c6} [37]. The samples are generated through H-step rollouts. Here, we use the trained transition to generate more trajectory to achieve data augmentation."}, {"title": "IV. PROBLEM FORMULATION", "content": "The cost of collecting preference data is high since it needs real-time human feedback. Therefore, this paper aims to improve sample efficiency to learn accurate reward model from limited preference data and guarantee agent performance. For PbRL, the ideal form is that the learned reward function R from collected preference data can be consistent with true reward R*. Here, we define the function class as R = {R : S\u00d7A \u2192 R}. Then we assume the reward class R is realizable.\nAssumption 1 (Realizability). Let d(s, a) \u2208 \u2206(S \u00d7 A) be the arbitrary data distribution. Then, for any distribution d(s, a), infR\u2208R E(s,a)~d(s,a)[R*(s, a) \u2013 R(s,a)]2 < \u025br holds.\nBased on the above assumption, the optimal reward function can be obtained if the preference data can cover all data space. However, preference data tend to be limited, even less than hundred in real-world scenarios, such as rehabilitation robot field. Therefore, different from previous PbRL [3], [33], [35], this paper learns reward function from the limited dataset D\u2081 = {(\u03c3\u03bf, \u03c3\u2081, y)(\u2170)} \u2116\u21161. We train the transition model through Eq. (4) to generate more unlabeled preference data Du = {(\u03c3\u03c5, \u03c3\u03c4)(2)}1. The pseudo labels y for unlabeled dataset Du can be obtained through reward model trained on Di [3], that is\n$\\hat{y}\\left(\\sigma_{0}, \\sigma_{1}\\right)=1\\left[P_{\\psi}\\left(\\sigma_{1} \\succ \\sigma_{0} ; \\psi\\right)>0.5\\right]$,\n(5)\nwhere 1(\u00b7) is indicator function.The reward function is updated through collected labeled dataset Di and generated unlabeled dataset Du. Then, the reward model can be optimized through minimizing\n$\\mathcal{L}_{R}^{\\prime}(\\psi)=\\mathbb{E}_{(\\sigma_{0}, \\sigma_{1}, y) \\sim D_{i}}\\left[\\mathcal{L}\\left(\\left(\\sigma_{0}, \\sigma_{1}\\right), y\\right)\\right]$\n$\\quad+\\mathbb{E}_{(\\sigma_{0}, \\sigma_{1}) \\sim D_{u}}\\left[\\mathcal{L}\\left(\\left(\\sigma_{0}, \\sigma_{1}\\right), \\hat{y}\\right)\\right]$,\n(6)\nwhere $\\mathcal{L}\\left(\\left(\\sigma_{0}, \\sigma_{1}\\right), y\\right)=-(1-y) \\log P\\left(\\sigma_{0} \\succ \\sigma_{1} ; \\psi\\right)-y \\log P\\left(\\sigma_{1} \\succ \\sigma_{0} ; \\psi\\right)$. However, the pretrained reward model may generate incorrect pseudo-labels for unlabeled dataset, leading to noisy training and poor generalization [31]. Therefore, one key question is how to design a data selecting mechanism to improve prediction accuracy and guarantee training stability.\nThe another key aspect is the theory for offline PbRL. There are very few algorithms specifically designed for offline PbRL with strong guarantee, including generalization bound for reward model and safe improvement guarantee for policy learning. Therefore, another key question is to develop a systematic theory for offline PbRL, including generalization bound and improvement guarantee."}, {"title": "V. REWARD LEARNING", "content": "The reward learning involves two stages: pretraining based on collected labeled data Di and updating based on Di and unlabeled data Du during policy learning, which is essentially semi-supervised learning [26]. This section focuses on designing a data selecting function f(\u03c3\u03b1, \u03c3\u03c4) to ensure the quality of generated preference data, and explaining the factors that influence the generalization ability of reward model.\nA. Uncertainty-aware Pseudo-label Selection\nMotivated by the previous pseudo-labeling work [31], we select data from unlabeled dataset Du according two principles: confidence and uncertainty. The data with high confidence and low uncertainty can be chosen for reward training. High confidence refers that pre-trained reward model discriminates the preference of two trajectories into y with high probability p(\u03c3\u03bf, \u03c3\u2081, \u0177). Low uncertainty refers that the NR reward models (model ensembles) predicts with small variance \u03c4(\u03c3\u03bf, \u03c3\u03b9, NR). The probability confidence p(\u03c3\u03bf, \u03c3\u03b9, \u0177) and uncertainty variance \u03c4(\u03c3\u03bf, \u03c3\u03b9, NR) can be denoted as\n$p\\left(\\sigma_{0}, \\sigma_{1}, \\hat{y}\\right)=(1-\\hat{y}) P_{\\psi}\\left(\\sigma_{0} \\succ \\sigma_{1}\\right)+\\hat{y} P_{\\psi}\\left(\\sigma_{1} \\succ \\sigma_{0}\\right)$,\n$\\tau\\left(\\sigma_{0}, \\sigma_{1}, N_{R}\\right)=\\operatorname{Std}\\left\\{P_{\\psi}\\left(\\sigma_{0} \\succ \\sigma_{1}\\right)\\right\\}$,\n(7)\nwhere NR is the number of reward model, Std {P(\u00b7)} denotes the variance of output probability of NR reward models, and P(\u00b7) is the mean probability of NR reward models. Therefore, according to high confidence and low uncertainty, the f(\u03c3\u03bf, \u03c3\u03b9) can be denoted as:\n$f\\left(\\sigma_{0}, \\sigma_{1}\\right)=1\\left[p\\left(\\sigma_{0}, \\sigma_{1}, \\hat{y}\\right)>\\kappa_{p}\\right] \\cdot 1\\left[\\tau\\left(\\sigma_{0}, \\sigma_{1}\\right)<\\kappa_{\\tau}\\right]$,\n(8)\nwhere Kp and KT are thresholds of confidence and uncertainty respectively. f(\u03c3\u03bf, \u03c3\u03b9) = 1 denotes the generated data (\u03c3\u03b1, \u03c3\u03c4) is selected, and f(\u03c3\u03b1, \u03c3\u03c4) = 0 denotes the data is not selected. Then combing Eq. (6), the reward model can be optimized through minimizing\n$\\hat{\\mathcal{L}}_{R}(\\psi)=\\frac{1}{N_{l}} \\sum_{i=1}^{N_{l}} \\mathcal{L}\\left(\\left(\\sigma_{0}, \\sigma_{1}\\right)^{(i)}, y^{(i)}\\right)$\n$\\qquad+\\frac{1}{N_{u}} \\sum_{j=1}^{N_{u}} f\\left(\\sigma_{0}, \\sigma_{1}\\right)^{(j)} \\mathcal{L}\\left(\\left(\\sigma_{0}, \\sigma_{1}\\right)^{(j)}, \\hat{y}^{(j)}\\right)$,\n(9)\nwhere Nu is the number of generated dataset after screening, and LR(4) is the empirical risk. It contains two parts: labeled loss Li(\u03c8) and unlabeled loss Lu(4). Note that the label in unlabeled loss is pseudo-label, which may be different from the true label. Therefore, there is the gap between the unlabeled loss with pseudo-label Lu(4) and that with true label L'u(4). Before bounding this gap, we firstly give the below assumption without loss of generality.\nAssumption 2. For the pretrained reward model Ry, through the limited labeled dataset D\u2081 = {(\u03c3, \u03c3\u2081, y)(i)}1, if the pseudo label \u0177 is defined in Eq. (5), then the pseudo-labeling error for the unlabeled dataset Du = {(\u03c3\u03b1, \u03c3\u03c4)(3)} is smaller than n, i.e., \u2211j=11[\u1ef9 \u2260 y\u00b3]/Nu \u2264 \u03b7.\nNu\nNu\nProposition 1. Suppose that the loss L((\u03c3\u03bf, \u03c3\u03b9),y)) is bounded by \u03a9. Then, for any R\u2208 R, under Assumption 2 the following equation holds:\n$|\\mathcal{L}_{u}(\\psi)-\\hat{\\mathcal{L}}_{u}(\\psi)| \\leq \\eta \\Omega$.\n(10)\nThe proof of Proposition 1 can be found in Appendix A. This gap is mainly influenced by the term \u03b7. Through selecting high confidence and low uncertainty samples, the \u03b7 can be significantly reduced. Combining Eqs. (5) and (8), we find that if p(\u03c3\u03b1, \u03c3, \u0177) is small, the L((\u03c3\u03b1, \u03c3), \u0177) would become larger. Too large error can lead to unstable training and low performance. Therefore, through selecting mechanism f(\u03c3\u03c9, \u03c3\u03c4), a more accurate subset of pseudo-labels can be used in reward training, and through setting a higher confidence threshold for pseudo labeling, a lower prediction error and the stability of training can be guaranteed.\nB. Generalization Bound for Reward Model\nBefore developing generalization bound, we define the expected error LR(4) with respect to the reward model R(s,a; \u03c8): LR(\u03c8) = E(60,01,y)~D[L((50,01), y)], where D is data distribution. The reward model is trained through minimizing the empirical error LR(V) in Eq. (9). Through developing generalization error bound, we can analyse the factors that influence generalization ability.\nSimilar to the previous generalization bound works [29], Rademacher complexity, which measures the richness of a certain hypothesis space [19], is introduced firstly. The definition is given below:\nDefinition 1 (Empirical Rademacher complexity). Let F be a family of functions mapping from Z to R and S = {21,...,ZN} be a fixed sample of size Ns drawn from the distribution S over Z. The empirical Rademacher complexity of G for sample \u015c is defined as\n$\\mathfrak{R}_{S}(\\mathcal{F})=\\mathbb{E}_{\\sigma}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{N_{s}} \\sum_{i=1}^{N_{s}} \\sigma_{i} f(z)\\right]$,\n(11)\nwhere \u03c3 = (\u03c31,...,\u03c3\u03bd) are independent uniform random variables taking values in {\u22121, +1}.\nDefinition 2 (Induced Reward Function Families). Given for a space R of reward function R, the induced reward function families II(F) is defined as\n$\\Pi(\\mathcal{R})=\\{\\left(\\sigma_{0}, \\sigma_{1}\\right) \\rightarrow-\\log P\\left(\\sigma_{0} \\succ \\sigma_{1}\\right) \\mid R \\in \\mathcal{R}\\}$, (12)\nwhere P(\u00b7) is defined in Eq. (2) and II(R) is the union of projections of R onto each dimension.\nIn general, lower Rademacher complexity corresponds to better generalization performance. Then, based on the above definition and Proposition 1, we develop the new generalization bound for reward model trained through Eq. (9). The specific theorem is given below:\nTheorem 1. Let reward model be trained on the labeled dataset D\u2081 = {(\u03c3\u03bf, \u03c3\u03b9, y) (i)} N\u2081 and unlabeled dataset Du = {(\u03c3\u03b1, \u03c3\u03c4)(2)}. Then, for any 8 > 0, with probability at least 1 \u2013 8, under Assumption 1 and 2 the following holds for any reward function R\u2208 R,\nN\ni=1\n$\\mathcal{L}_{R}(\\psi) \\leq \\hat{\\mathcal{L}}_{R}(\\psi)+\\frac{\\eta \\Omega}{N}+4 \\mathfrak{R}_{N}(\\Pi(\\mathcal{R}))+3 \\sqrt{\\frac{\\log (2 / \\delta)}{2(N_{l}+N_{u})}}$,\nwhere D is the input combination of labeled and unlabeled dataset, i.e. {(\u03c3\u03bf, \u03c3\u03b9) (2) 1 \u039d\u03b9+\u039d.\nThe proof of Theorem 1 can be found in Appendix B. Theorem 1 indicates that the expected error LR(V) is bounded by empirical error \u00caR(V), pseudo-labeling error \u03b7\u03a9, Rademacher complexity and constant terms. The constant term can become small when the number of unlabeled data increases, but it may cause unstable training when labels are inaccurate. According to Proposition 1, empirical error and pseudo-labeling error can be reduced through selecting mechanism f(\u03c3, \u03c3\u03c4), thus the better generalization ability, that is tighter upper bound, can be achieved. Note that Theorem 1 has generality and can be applicable to methods that train reward model using pseudo-labels. The difference among various methods may lie in how they train reward model to improve the accuracy of pseudo-labels, that is reducing \u03b7. If one method fails to reduce the pseudo-labeling error n, the upper generalization bound would be looser than that of no data augmentation."}, {"title": "VI. POLICY LEARNING", "content": "This paper directly uses the proposed offline RL algorithms, such as CQL [4] and IQL [25], to perform policy learning. To solve the problem of high cost for labeling preference, we propose a offLine prEference-based RL with high Sample Efficiency algorithm (LEASE). This section aims to describe the details of LEASE and establish the theory for safe policy improvement guarantee.\nA. The Implementation Details of LEASE\nMost offline RL algorithms are based on Eq. (1). They proposed various algorithms to solve the problem of distribution shift. In offline PbRL setting, the reward is unknown for agent. The accuracy of reward model directly influence the performance of policy. Moreover, the unlabeled data are generated through the current policy interaction with learned transition model. Therefore, the reward model and policy influence each other. In practical implementation, the policy, Q-function, reward and transition model are parameterized by \u03c0\u03bf, Q\u03c9, Ry and To respectively.\nAlgorithm 1 gives pseudocode for LEASE. The goal of LEASE is to learn better reward model Ry from fewer labeled preference dataset Di and learn better policy \u03c0\u03b8 from offline dataset Doffline without interaction with environment. The key of LEASE is how to train reward model in policy learning process. Notably, the reward model only update once instead of updating constantly in this process. The reward update condition is set to when the number of unlabeled data in buffer reaches the maximum buffer capacity. The update time is influenced by rollout frequency and rollout batch size. The reward update condition, rollout length H and selecting mechanism f(01,02) are the important factors for reward model.\nB. Safe Policy Improvement Guarantee\nOffline RL aims to guarantee \u00a7-safe policy improvement over the behavior policy \u03bc (the policy used to collect offline dataset), i.e. J(\u4e93, R*) \u2265 J(\u03bc, R*) \u2013 \u00a7 [4]. This part develops theoretical guarantee of policy improvement for LEASE, that is giving the bound for J(\u00b5, R*)\u2212J(\uc288, R*), and further analyzes sample complexity when reward model is unknown. Firstly, we propose a new single-policy concentrability coefficient for PbRL based on state-action pair instead of trajectory like [18].\nDefinition 3 (Concentrability coefficient for PbRL). Concentrability coefficient CR(\u03c0) is used to measure how well reward model errors transfer between offline data distribution d and the visitation distribution d under transition T and policy \u03c0, defined as\n$C_{R}(\\pi)=\\sup _{R \\in \\mathcal{R}} \\frac{\\mathbb{E}_{(s, a) \\sim d^{\\pi}}[R^{*}(s, a)-R(s, a)]}{\\mathbb{E}_{(s, a) \\sim d^{\\mu}}[R^{*}(s, a)-R(s, a)]}$, (13)\nwhere R* (s, a) is the true reward model and coefficient CR(\u03c0) is upper bounded by ||d/d4||\u221e.\nNext, similar to [18], [20], we use \u025b-bracketing number to measure the complexity of reward function class R, which can be defined as\nDefinition 4 (\u03b5-bracketing number). The \u025b-bracketing number NR(8) is the minimum number of \u025b-brackets (l, u) required to cover a function class R, where each bracket (l, u) satisfies 1(\u03c3\u03bf,\u03c31) \u2264 PR(\u03c3\u03bf,\u03c31) \u2264 u(\u03c3\u03bf, \u03c3\u2081) and ||l - u||1 \u2264 \u025b for all R \u2208 R and all trajectory-pairs (\u03c3\u03bf, \u03c31), and PR(\u03c3\u03bf,\u03c3\u03b9) is the probability that segment \u03c3\u03bf is preferable to segment \u03c3\u03b9 defined in Eq. (2).\nTheorem 2. Under Assumption 1, for any \u03b4\u2208 (0,1], the policy learned by LEASE, with high probability 1 \u2013 \u03b4, satisfies that J(\u03bc, R*) \u2013 J(\u4e93, R*) is upper bound by\n$\\xi+\\frac{1+C_{R}(\\pi)}{1-\\gamma}\\left(\\sqrt{\\frac{4 C}{N L^{2}} \\log \\left(\\frac{N_{R}(1 / N)}{\\delta}\\right)}+ \\sqrt{\\frac{4 R_{\\max }}{N L} \\log \\left(\\frac{1}{\\delta}\\right)}\\right)$,\n(14)\nwhere C > 0 is the absolute constant defined in Eq. (2), N is the size of preference dataset, and L is the length of trajectory. The term & is performance gap depending on offline algorithm itself.\nThe proof of Theorem 2 can be found in Appendix C. The \u00a7 is constant when offline algorithm is determined. Therefore, the tighter bound can be achieved through reducing performance gap caused by reward model (see Proposition 3). It can be reduced to small value & with sample complexity of\n$\\begin{aligned}\\mathcal{N}=\\tilde{\\mathcal{O}}\\left(\\frac{4\\left(1+C_{R}(\\pi)\\right)}{(1-\\gamma)^{2}}\\left[\\sqrt{\\frac{C^{\\prime} \\log \\left(N_{R}(1 / N) / \\delta\\right)}{L^{2}}}+\\sqrt{\\frac{R_{\\max } \\log \\left(1 / \\delta\\right)}{L}}\\right]^{2}\\right)\\end{aligned}$.\n(15)\nwhere concentrability coefficient CR(7) can become smaller when the learned policy is close to behavior policy \u03bc. Bracketing number log(Nr(1/N)) measures the complexity of function class R and takes O(d) in linear reward model [18]. Notably, LEASE can learn accurate reward model through data augmentation under fewer N\u012b preference data (N\u2081 < N). Thus, the performance gap can become tighter under fewer data, that is LEASE can have higher sample efficiency compared with [18]. More discussion can be found in Section VIII-A."}, {"title": "VII. EXPERIMENTS", "content": "This section gives the details of experiments and focuses on answering the following questions:\n\u2022 Q1: How well the accuracy of reward model through data augmentation under fewer preference data?\n\u2022 Q2: How well does LEASE perform compared with offline PbRL baseline in standard benchmark tasks?\n\u2022 Q3: How the proposed selecting mechanism and parameter settings affect the performance of reward and policy?\nWe answer these questions on two distinct domains (mujoco and adroit) within the D4RL benchmark [22], covering several locomotion and manipulation tasks. The code for LEASE is available at anonymous.link.\nA. Experiment Setup\nExperimental Datasets. The offline dataset and preference dataset of experiments originate from [22] and [21], respectively. The offline datasets are based on the D4RL dataset [22]. We select medium and medium-expert two types dataset for Mujoco tasks, and human and expert for Adroit tasks. The difference between different dataset in certain task lies in the collected policy."}, {"title": "VIII. DISCUSSION", "content": "A. Discussion for Offline PbRL Theory\nIn offline RL field, high sample efficiency refers that the agent can achieve comparable performance under fewer data compared with the performance under large data. In this paper, the data refers to the preference dataset. The labeled preference dataset, each trajectory of length L, is collected through real-time human feedback under policy \u03bc, which demands tremendous human effort, thus the collected cost of preference data is higher than fixed offline data. The unlabeled dateset is generated through trained transition without real-time interaction under learned policy \u03c0\u00b9 at time t.\nThrough data augmentation, the sample efficiency can be significantly reduced. The performance gap caused by reward can be reduced to w under fewer labeled dataset. Zhan et al. [18", "sigma^{1}\\right)\\right": ""}, {"sigma^{1}\\right)\\right": {"R(sa)": "ref is an arbitrary trajectory distribution (usually set as \u03bc\u2081), and \u03bc\u03bf, \u03bc1 are behavior trajectory distribution."}}]}