{"title": "AI Governance in Higher Education: Case Studies of Guidance at Big Ten Universities", "authors": ["Chuhao Wu", "He Zhang", "John M. Carroll"], "abstract": "Generative AI has drawn significant attention from stakeholders in higher education. As it introduces new opportunities for personalized learning and tutoring support, it simultaneously poses challenges to academic integrity and leads to ethical issues. Consequently, governing responsible AI usage within higher education institutions (HEIs) becomes increasingly important. Leading universities have already published guidelines on Generative AI, with most attempting to embrace this technology responsibly. This study provides a new perspective by focusing on strategies for responsible AI governance as demonstrated in these guidelines. Through a case study of 14 prestigious universities in the United States, we identified the multi-unit governance of AI, the role-specific governance of AI, and the academic characteristics of AI governance from their AI guidelines. The strengths and potential limitations of these strategies and characteristics are discussed. The findings offer practical implications for guiding responsible AI usage in HEIs and beyond.", "sections": [{"title": "Introduction", "content": "AI applications in education (AIEd) have been at the forefront of discussions among education stakeholders, particularly in light of the advancements in Generative AI (GenAI). Undoubtedly, the continually improving performance of AI holds immense potential for enhancing educational experiences, enabling personalized learning, and automating administrative tasks (Baidoo-Anu and Ansah 2023; Chen, Chen, and Lin 2020). However, alongside these benefits, there are legitimate concerns about the potential negative impact of AI, especially GenAI. The fact that AI is trained with existing work and its ability to quickly generate content that may be unauthorized imitations impose fundamental challenges to the regulation of plagiarism and academic integrity (Dehouche 2021; Smits and Borghuis 2022). These challenges will potentially impact both faculty and students in their work and study. Specifically, for educators, the need to learn about AI and address it in teaching brings extra requirements for their knowledge and skills (Wu 2023). For students, the prevalence of AI also raises concerns about ethical issues, personal development, career prospects, and societal values (Chan and Hu 2023).\nHigher education institutions (HEIs) play a crucial role in technology innovation and its diffusion through creating knowledge, providing talent, and translating research into innovations (Gunasekara 2006). In the development and implementation of norms for AI governance, academic institutions responded by establishing centers for the study of AI governance, engaging in basic research, and influencing AI norms through academic expertise and collaborations (Chinen 2023; Mainzer 2022). While HEIs contribute to AI governance at a societal level, technology diffusion in universities itself can also be a complex process involving issues of power, legitimacy, and identity (Smith 2000). In the case of GenAI, due to the complex implications of encouraging or discouraging such an important technology, universities are urged to take action and devise strategies for guiding and regulating its usage among students, faculty, and staff (Chan 2023). Despite significant uncertainty about the pros and cons of incorporating GenAI in education, many universities have published policies and guidelines, aiming to promote responsible and beneficial AI usage. For example, the early review conducted by Moorhouse, Yeo, and Wan (2023) found that 23 out of the 50 top-ranking universities have developed publicly available guidelines by June 2023, which addressed the potential influence of GenAI on three main areas: academic integrity, advice on assessment design, and communicating with students. However, the authors pointed out that as these guidelines have been developed rapidly, it is likely that many of the suggestions have not been sufficiently tested. Similarly, Adams and Ivanov (2015) examined documents produced by 116 high research activity (R1) universities in the US, concluding that the majority of universities encourage the use of GenAI yet in a way that presents potential burdens for faculty and students and without much regard for ethical concerns.\nThe current study will contribute to this line of research by extending the focus beyond teaching and learning implications and analyzing the guidelines through the lens of AI governance. Prior studies on AI guidelines have provided an overview of how many HEIs are embracing GenAI usage and publishing AI guidelines, and what main topics are discussed. This study aims to further analyze the structure of AI guidelines from the organizational level and a community perspective. Specifically, we aim to illustrate the approaches HEIs take to promote responsible AI usage by answering the following research questions:\n1. What strategies of AI governance are demonstrated by"}, {"title": "Background", "content": "As AI technology continues to advance and to be widely adopted, the governance of its development and implementation has become a key issue that is widely concerned and discussed by various stakeholders (Erd\u00e9lyi and Goldsmith 2018; Thiebes, Lins, and Sunyaev 2021; Bucknall and Dori-Hacohen 2022). AI governance refers to the frameworks, policies, and practices designed to ensure that AI systems are developed, deployed, and managed in a way that aligns with ethical principles, legal requirements, and societal values. It encompasses the establishment of guidelines for responsible AI use, mechanisms for accountability, and processes for managing risks associated with AI technologies (Felzmann et al. 2020; Balasubramaniam et al. 2023; Rismani and Moon 2023; Henriksen, Enni, and Bechmann 2021). Organizations establish ethical guidelines for AI usage by engaging in a multifaceted process that involves the identification of core ethical principles, the development of practical frameworks, and the implementation of governance models that ensure adherence to these guidelines throughout the AI system's lifecycle (Janssen et al. 2020; Georgieva et al. 2022; Bessen, Impink, and Seamans 2022). The process often begins with the recognition of the need for ethical AI principles that sustain human values and rights (D\u00edaz-Rodr\u00edguez et al. 2023), acknowledging the complex and opaque nature of AI systems and their potential to impact fairness, accountability, and transparency (Akinrinola et al. 2024; Diakopoulos 2020). This recognition is supported by international and organizational efforts to publish principles of ethical AI, which aim to outline values and abstract requirements for AI development and deployment (Rees and M\u00fcller 2023). However, the effectiveness of these principles is contingent upon their translation into measurable and actionable guidelines that can be practically applied (Fell\u00e4nder et al. 2022).\nTo address the gap between ethical principles and their application, organizations are adopting frameworks like the hourglass model of organizational AI governance. This model emphasizes the need for governance at environmental, organizational, and AI system levels, connecting ethical principles to the AI system lifecycle to ensure comprehensive governance (Huriye 2023). These collective efforts underline the importance of a structured approach to AI governance, ensuring that AI technologies are developed and deployed in ways that uphold ethical standards and promote social good (Smuha 2019). HEIs also make prominent contributions to the process as research efforts are critical in assessing the impact of intelligent applications, minimizing harm, and promoting well-being and social good (Safdar, Banja, and Meltzer 2020). In addition, a comprehensive understanding of the impact of disruptive AI technologies on education and the development of frameworks to critical for society to make informed decisions about the use of generative AI tools (Khan 2024). This study aims to bridge the gap between theoretical principles and practical application, offering a detailed analysis of whether and how HEIs implement responsible AI governance frameworks."}, {"title": "Technology Diffusion in HEIS", "content": "Technology diffusion refers to the process by which new technologies are adopted and spread across different regions, sectors, and social groups, during which technology is transferred, implemented, and utilized, leading to widespread acceptance and integration into everyday practices (Rogers, Singhal, and Quinlan 2014). Technology diffusion in universities is a complicated process, which often faces multifaceted challenges rooted in both organizational and cultural aspects of HEIs (Hawawini 2011; Liu, Geertshuis, and Grainger 2020). One primary challenge is the rapid and massive development of technology, which requires continuous adaptation and innovation from HEIs to maintain their educational and academic quality (Christensen and Eyring 2011). This adaptation is contingent upon the effective diffusion of innovation models that consider the profile of human resources, technological conditions, organizational policies, documentation, and environmental dynamics (Ramdhani et al. 2021). Additionally, the adoption of new information and communication technology is hindered by issues such as incompatible technology with faculty's traditional teaching practices, inadequate faculty support, and insufficient plans for implementation (Dintoe 2019). Instructors face challenges in staying updated with the changing uses of technology in the classroom, necessitating training, funding, and alignment of perceptions to facilitate technology adoption (Baadel, Majeed, and Kabene 2017). The integration of technology in teaching and learning processes is influenced by technological, pedagogical, and organizational dimensions, with contextual factors playing a determinant role (Rodr\u00edguez-Abitia et al. 2020).\nRegarding the adoption of AI in HEIs, there is still a lack of sound evidence on the pedagogical impact of AI technologies, which raises questions about their ability to improve learning outcomes or facilitate effective pedagogical changes (O'Dea, O'Dea et al. 2023). Additionally, the ethical concerns associated with biased algorithms, which could adversely affect students if used in admissions or grading processes, represent a significant technological and ethical barrier (Slimi and Carballido 2023). Educators' perspectives and the social, psychological, and cultural factors influencing their trust and adoption of educational technology also play a role in the slow uptake of AI tools (Kizilcec 2024). The barriers to digital technology integration, including technophobia and the absence of planning, directly impact the adoption of AI in university teaching (Mercader 2020). Therefore, to effectively govern the implementation of AI tools, it is crucial to integrate robust communication practices. Al governance in HEIs involves not just creating policies but also ensuring they are clearly communicated and understood at all levels. This includes identifying key stakeholders responsible for crafting and delivering these messages, such as senior leaders or governance committees. Effective communication from authoritative figures"}, {"title": "Methods", "content": "The Big Ten Conference is the oldest collegiate athletic conference in the United States. While historically celebrated for its athletic prowess, its member institutions are also major research universities with large financial endowments and strong academic reputations. Established in 1896, the Big Ten has grown to include fourteen universities spread across the Midwest and Northeast. These institutions are known for their research and academic excellence, contributing significantly to advancements in science, technology, engineering, and mathematics (STEM) education. The list of the 14 universities, their locations, and enrollments are presented in Table 1 (On August 2, 2024, the conference expanded to 18 member institutions and 2 affiliate institutions. The additional 4 members and affiliate members were not included in this study).\nWe chose this list of institutions for the case study due to the following reasons. First, the Big Ten is known for its research contributions and scholarly output. They often have substantial funding and resources dedicated to technological advancement, including AI, making them influential in shaping research agendas and policies related to AI nationally and internationally. Studying their guidance on AI can provide insights into how leading research institutions envision the future of AI governance and ethical considerations. The Big Ten also shares similar profiles as large, research-intensive institutions with significant resources and academic influence. This homogeneity provides a level of consistency in comparing AI guidelines, making it easier to identify patterns and common themes within a similar institutional context. Focusing on this list limits the sample to a manageable number of institutions, which facilitates a more in-depth and detailed analysis of each university's AI guidelines and policies. Previous research has also utilized this sample to study campus recreation (Wilson, Guthrie, and Bopp 2020), ethnicity diversity (Bennett 2002), sexual harassment (Clair 1993), and transgender policy (Dirks 2016) in HEIs. Therefore, focusing on the Big Ten allows us to identify insights on AI governance that are interpretable and meaningful to other HEIs and society."}, {"title": "Data Collection", "content": "The data collection was conducted in March 2024. We define AI guidance as guidelines regarding AI usage published officially at the university level. Therefore, guidelines published by a specific college or a branch campus are not included in the analysis. To identify AI guidelines for each university, the primary author visited the 14 universities' official websites and conducted manual searches. The keywords 'AI', 'Generative AI', 'Guidance', 'Guideline', and 'Policy' are used in combination to conduct the search. Additionally, we used the keywords appended with the university name in Google Search and inspected the results returned on the first page to ensure comprehensive coverage in our search. After the initial round of data collection, another researcher repeated the search process independently to validate the results and check for missing information. All of the 14 universities have official guidelines related to AI. Only publicly accessible documents or websites were extracted for further analysis. The data collection and analysis process was reviewed and approved by the IRB office in the authors' home institution."}, {"title": "Data Analysis", "content": "Mind mapping technique (Wheeldon and Faubert 2009) and thematic analysis (Drisko and Maschi 2016) were employed to analyze the guidelines. First, the primary author read through all the collected data and created mind maps using the university as the base unit. Each mind map represents the structure of AI guidelines published by the university, including the specific unit that publishes the guideline and the organization of its content. The mind maps enable an overview of how guidelines are organized by each university, revealing commonalities and differences in AI governance. The initial mind maps were then reviewed collectively by all researchers to ensure the accurate representation of the original data. In the next round of data analysis, the primary author conducted a thematic analysis of the mind maps, systematically coding and categorizing the mind map nodes while referencing the original guideline text to maintain contextual accuracy. Specifically, topics and segments that represented the strategies and characteristics of the AI guidance were identified, categorized, and linked to broader themes within the research. This thematic analysis not only highlighted the core elements of AI guidance but also allowed for the identification of patterns, relationships, and emerging trends within the data, thereby providing a deeper understanding of the subject matter. The coding was then shared and reviewed by all researchers, who discussed the consistency in coding and resolved minor discrepancies. The codes were grouped into three major themes: the multi-unit governance of AI, the role-specific governance of AI, and the academic characteristics of AI governance. After the researchers completed the process, they collectively translated the themes and sub-themes into narratives. The mind maps of the 14 universities can be found in the supplementary materials."}, {"title": "Results", "content": "Instead of specific rules or suggestions for Al usage, our analysis focuses on the strategies taken by universities to guide AI usage and the common characteristics observed in these AI guidelines. The findings and insights are detailed in the following subsections."}, {"title": "Multi-unit Governance of AI", "content": "The adoption and management of new technologies in HEIS can be complex due to their diverse constituencies, including faculty, students, and staff, each with distinct needs and priorities. In the case of the Big Ten, multiple units have been involved in publishing AI guidelines for the university community, as shown in Table 2. While the advice offered by different units can still overlap and reference each other, some key differences entail the unique role of each unit in the organizational management and AI governance in HEIs.\nInformation Technology. Information Technology (IT) and similar departments in HEIs play a crucial role in managing network services, supporting various technologies and platforms, and resolving software and hardware issues encountered by students and staff (Volk and Jamous 2018). Six universities from the Big Ten have issued AI guidelines through their IT departments. These guidelines mainly focused on security and privacy risks associated with Gen AI tools and how to avoid them. Specifically, these risks are managed by IT through statements related to the following topics:\n1. Data-sharing policy. IT established a specific policy for data usage as the community interacts with AI tools. These statements guide people to input limited types of data into AI tools and prohibit the sharing of institutional data and other sensitive information, which is also in accordance with the university's existing data classification system:\nDo not share institutional data with this tool. Providing any personally identifiable information or university internal information, such as development code for systems hosting institutional data, is a violation of IU policy. -U6\nThe data classification can be found on other pages of the IT website, which usually categorize data into four levels: Public (Low Sensitivity), University Internal (Moderate Sensitivity), Sensitive/Restricted (High Sensitivity), and Restricted/Critical (Very High Sensitivity). Among the four levels, the Public data is considered safe to share without the need for further IT review. By recapitulating this data classification in Al guidelines, IT helps university users avoid undesired information leakage while using AI.\n2. Enterprise agreement. IT also provides information on AI tools that are considered more secure through an enterprise contract or agreement with the university. These tools are approved for data interaction classified up to and including the University Internal level. The most common tool with such an agreement is Microsoft Co-Pilot, due to part of the university's existing licensing with Microsoft services such as Office 365. Therefore, despite the agreement, IT still seeks to enhance risk avoidance through further qualifications of AI tools:\nSince Microsoft released this tool as part of existing licensing, it has not yet gone through the formal review process that is part of our standard procurement process. As with any such tool, caution is advised. -U4\nAs this statement shows, IT has a formal review process that ensures a tool or software meets the university's security requirements. The guidelines welcome faculty to contact IT to assess security attributes for a given implementation of AI.\n3. Trustworthy AI. While it's impossible for IT to comment on all AI tools available on the market, it has encouraged the community to use \"Trustworthy Al\" tools. The characteristics of \"Trustworthy AI\" are described using the existing National Institute of Standards and Technology's AI Risk Management Framework (see Figure 1):\nUW Madison faculty, staff, students and affiliates can help protect themselves and others by choosing tools and services that exhibit the NIST's characteristics of trustworthy AI. -U2"}, {"title": "Role-specific Governance of AI", "content": "The involvement of multiple units reveals another important characteristic of AI governance in HEIs: the need to address the concerns of different roles within the university community. In the case of Big Ten, four predominant roles merged from the AI guidelines: faculty (or instructor), student, staff, and researcher. Some guidelines apply to all members regardless of their roles, such as the data-sharing policy published by IT, but most of the guidelines are written with a specific role as the intended audience, as detailed below.\nFaculty. Delivering high-quality education is one of the most important objectives of universities and faculty play an indispensable role in this process. Therefore, it is unsurprising that most guidelines are written from a faculty's perspective:\nAt the TLTC, we look forward to helping you think creatively about your assessments and your specific learning outcomes to put authentic, relevant, student-centered learning at the forefront of your academic planning -U3\nAs this statement suggests, although some AI guidelines are written for faculty, their content is still student-centered, aiming to improve learning outcomes. Specifically, university guidelines often imply three tasks for the role of a faculty. First, the guidelines present plenty of resources for faculty to learn what AI is, what its benefits and limitations are, how it may impact assessment and student learning, how it may impact education equity, etc. Therefore, the first task is for the faculty to become knowledgeable about AI. Although there is no enforcement or specific requirements for what faculty's level of knowledge should be on these topics, the following statement shows universities' preferences for faculty to be more familiarized with AI:\nAl is quickly becoming an embedded element of the teaching and learning process that requires the acknowledgment and attention of instructors, instructional designers, and academic leaders. -U13\nSecond, faculty are encouraged to integrate AI into their pedagogical practices such as lecture preparation, assignment design, and brainstorming for active learning activities. On the one hand, the need to integrate AI is motivated by the increased productivity brought by AI, e.g., 'shortening the time instructors spend on creating course materials, coming up with examples and assignments, as well as making grading more efficient. (U12)' On the other hand, inappropriate use of Al among students can hamper the learning process, making 'AI-proof' teaching practices necessary:\nProbably the best way to guard against inappropriate use of Al-generated text is to redesign your assign-\nments, both the prompts themselves and the related processes. -U6\nThis concern also leads to the third task for faculty: providing guidance for students and supervising students' AI usage. The guidelines support faculty in creating syllabus statements, communicating expectations of AI usage with students, and discussing the implications with students. While instructors have the freedom to define acceptable AI usage in their classes, the guidelines have made recommendations regarding the detection of AI-generated content. Generally, universities discourage the use of AI detection tools, highlighting their high false-positive rates and emphasizing the need to focus on the learning process rather than the assignment product:\nThe available tools are simply not effective in providing the evidence needed to build an academic integrity case against a student. Our pedagogies should be built with critical AI literacy in mind, so it's important to think through what goals AI prohibition is going to meet and whether enforcement is how you want to spend your time and energy. -U14\nStudent. Students typically make up the largest population in the university community and AI guidelines are also student-centered, highlighting the need to promote student learning. However, as the regulation of students' use of AI has been primarily delegated to faculty, AI guidelines are often not written from the student's perspective. The limited guidelines targeting students echo those written for faculty, urging students to communicate with their instructors and seek suggestions and guidance. In addition, the guidelines highlight that students should thoroughly consider the consequences of their usage of AI both for themselves but also for the society:\nAs GenAI poses to be a revolutionary tool that can change the academic space and beyond, it is important for you to understand why and how you intend to use these new, powerful tools... Understand that your usage of GenAI-based tools can give you the means to better not just yourself, but also society as a whole, and there is an ethical responsibility towards doing so. -U7\nStaff. Staff can be involved with a broad range of administrative and operational tasks that support the university community. However, due to the variety of staff positions and their unique responsibilities, it is difficult to provide specific guidance for this role. U8 provided guidance for staff involved in university communication and marketing, demonstrating the importance of these activities for HEIs. In addition to examples of using AI operational tasks, writing, and editing, the guidance also provides examples where GenAI should not be used:\nAt this time, we advise AI should not be used in the creation of institution-specific content (e.g. leadership messaging) or information regarding the immediate health and safety of our community (e.g. updates and triage.)"}, {"title": "The Academic Characteristics of AI Governance", "content": "The multi-unit and role-specific characteristics of AI guidelines reflect the organizational complexity and multifaceted functionality of HEIs. However, regardless of the unit and role, AI governance in the Big Ten has generally incorporated academic characteristics, emphasizing the intention to empower the university community for informed decision-making. We summarize the characteristics as Educative and advisory, Flexible, and theSocratic method, as described below:\nEducative and Advisory Guidance. Except for the IT policies related to data sharing and privacy issues, most of the guidelines do not enforce or prohibit any actions for the university community. Instead, they focus on providing resources for people to learn more about AI and recommend possible behaviors to avoid risks and promote benefits:\nPurdue University continues to support the autonomy and choice of faculty and instructors to utilize instructional technology that best suits their teaching\nand learning environments. As such, there is no official university policy restricting or governing the use of Artificial Intelligence, Large Language Models or similar generative technologies. -U10\nAs this statement shows, the guidelines are written with the mindset to support the autonomy of its community to make the best use of the technology. In addition to being educative, the guidelines also make specific suggestions on appropriate or inappropriate usage of AI (see Figure 3). However, it should be noted with the educative and advisory characteristics, there also comes uncertainty regarding the use and management of AI in some scenarios. An obvious example is the use of AI detection tools. U12 concluded that \"We do not recommend using this detection tool as the basis for reporting a suspected case of academic dishonesty,\" which seems a consensus among the Big Ten. Yet, it remains unclear for faculty regarding what could be used as the basis for reporting academic dishonesty related to AI usage.\nFlexible Guidance. On the positive side, the uncertainty also accompanies the flexibility in AI guidelines, which encourages the community to experiment and explore the potential benefits of using AI. The flexibility is exemplified through the acknowledgment that AI is a rapidly evolving technology and that the guidelines will evolve as new information arises. In some cases, universities also make the process of developing AI guidelines an engaging and interactive process where different community members are encouraged to share their perspectives on the implications of GenAI. For instance, U14 leveraged the power of social media and created an online space for GenAI discussions among the university community:\nWelcome to our budding community, a space where we hope to see collaboration and knowledge exchange thrive. Here, you can both contribute and gain insights into the innovative ways in which our faculty, instructors, students, and staff are using GenAI tools to develop new teaching and learning methodologies. In addition, we hope that this platform will serve as a forum for thoughtful and respectful conversations to address the ethical complexities of GenAI. -U14\nMoreover, this flexibility is also demonstrated through multiple options provided for resolving AI-related issues. For instance, considering the course syllabus managing students' use of AI, universities often provide three types of sample statements allowing faculty to customize for their classroom setting, e.g., no restrictions, allow limited usage of ChatGPT, and prohibit the usage of ChatGPT.\nSocratic Method. The Socratic Method is a form of logical argumentation that promotes critical thinking, provoked by the continual probing questions of the teacher (Deli\u0107 and Be\u0107irovi\u0107 2016). While the AI guidelines are mostly static descriptions, some of its approaches resemble the Socratic Method by imposing questions on its readers. For some questions, the guidelines will provide possible answers or rationale, creating a FAQ section, but there are also questions that come with no answers, provoking the readers to think critically about the impact of GenAI:\nAs GenAI poses to be a revolutionary tool that can change higher education and beyond, it is important for you to understand why and how you intend to use these new, powerful tools. These are a few questions to consider and note that the answers to these questions will vary for each person. -U7\nAgain, the Socratic Method approach resonates with other academic characteristics, highlighting that the overall intention of AI guidance in universities is to improve AI literacy and raise awareness of responsible AI usage among its community rather than regulating its usage in a strict sense."}, {"title": "Summary", "content": "The strategies and characteristics of the AI guidelines can be summarized as the framework in Figure 4. The governance of AI is practiced through AI guidelines published by multiple units. Among them, the President & Provost recapitulates the content from other units and directs people's attention to the resources available. The AI Center can act as the one-stop shop for all guidelines. Other units tend to address the concerns of specific roles in the university community, including faculty, students, staff, and researchers, with guidance for students often delegated to faculty. Regardless of the role, community members are encouraged to participate in the discussion of the GenAI implications. Only the guidelines published by IT specify scenarios of prohibited use of AI. The rest of the guidelines tend to be educative, advisory, flexible, and Socratic, demonstrating the objective to improve the community's understanding of AI and empower them to use GenAI responsibly."}, {"title": "Discussion", "content": "In this section, we discuss the findings in relation to prior literature and their practical implications. First of all, the guidelines published by the Big Ten agree with the literature on the ethical development and deployment of AI: attempting to maximize the benefits of AI but also acknowledging the complex and opaque nature of AI systems and their potential limitations (Akinrinola et al. 2024; Diakopoulos 2020). They also demonstrate the recognition of the considerations of human values and rights in the community (D\u00edaz-Rodr\u00edguez et al. 2023). By contrast, the unique strategies and characteristics of AI guidelines in Big Ten reflect the challenges faced by HEIs when adapting themselves to the advancement of AI technology.\nSpecifically, the multi-unit governance of AI helps address the challenge of diverse stakeholders during technology diffusion in HEIs. By engaging different units in the process of Al governance, the university can consider more factors that impact the technology implementation such as technological infrastructure and human resources (Ramdhani et al. 2021). As our findings suggest, each of the units has a specific emphasis on roles and activities impacted by GenAI, contributing to more comprehensive guidelines addressing the needs of different stakeholders. Despite its advantages, the involvement of multiple units can result in more web pages and a more complicated information structure of the guidelines. Consequently, it is inevitable that the level of difficulty in finding information also increases due to the complicated structure (Tombros, Ruthven, and Jose 2005). For instance, a faculty member may need to access IT, Teaching & Learning, and University Libraries to find all the information they need, as data privacy, teaching implications, and research use of AI are all related to the role of faculty and they are addressed in different levels of detail by these units. Moreover, as our analysis shows, the content can also overlap between different units, leading to further efforts in comparing those guidelines and checking for discrepancies. Therefore, while the involvement of multiple units is beneficial, the publishing and rendering of AI guidelines may need to be simplified to make it more accessible to the university community.\nOne possible solution is to leverage the AI Center, i.e., create a website devoted to AI-related issues and make all AI guidelines accessible on this single site. In addition, strategies in website design could used to optimize the guidelines and reduce information load for readers (Chen 2018). Specifically, information seeking is easier when user needs are considered in the information architecture (Shih et al. 2012), suggesting the guidelines could be designed as role-oriented. Currently, the guidelines provide role-specific content that addresses the concerns of different roles. However, a few of them are organized in a role-specific way, making it difficult for students, faculty, and staff to find corresponding information. Organizing the guidelines in a role-oriented way may help the university community to comprehend the guidelines and put them into practice.\nAnother important issue regarding different roles is that faculty is the primary focus in most Al guidelines, with the responsibility for guiding students largely delegated to them. While guiding students through classroom-level management may be an effective way to maintain academic quality (Korpershoek et al. 2016) and addressing potential issues brought by AI, it should be noted that this responsibility may lead to an increased workload for faculty. Recent research has shown that faculty could complain about the increased workload due to the need to manage more AI-related academic integrity problems (Wu et al. 2024). This is especially"}, {"title": "Limitation", "content": "There are limitations in the current study that can be addressed through further research. First, while focusing on the Big Ten allowed us to identify meaningful patterns in AI guidance for HEIs, we recognize that the findings should not be generalized without considering cultural and socioeconomic differences among HEIs across the world. Specifically, HEIs in non-English-speaking countries and those in the Global South may face different challenges at the organizational level, and insights from the Big Ten may not be applicable in their scenarios. Future studies may seek to compare Al governance in universities with different cultural and societal backgrounds. Second, even though most AI guidelines are straightforward and clearly written, readers may still have different perceptions that misalign with the guidelines' intention. Investigating the perception of AI guidelines among different university members is important for assessing the effectiveness of AI guidelines in HEIs' \u0391\u0399 governance."}, {"title": "Conclusion", "content": "The current study presents a case study of AI guidance in the Big Ten universities. Through thematic analysis of different AI guidelines, we identify the multi-unit governance of AI, role-specific governance of AI, and the academic characteristics of Al governance in these universities. These strategies and characteristics reflect the university's intention to develop comprehensive guidelines for AI usage that both maintain the autonomy of its community and help them to take advantage of AI. However, the complicated information structure and flexibility in guidance may cause problems and confusion for the community. The findings provide practical"}, {"title": "Positionality Statement", "content": "The authors acknowledge their backgrounds and the potential biases that might be introduced when interpreting the findings of this study. The primary author's training spans multiple disciplines, including engineering, management, and educational research, and encompasses diverse roles such as student, instructor, and researcher in HEIs in the United States. Similarly, the research team, with its substantial experience in HEIs comparable to those in the Big Ten, brought a comprehensive perspective to the data analysis. All team members actively engaged in interpreting the findings and discussing the implications of the study. Despite this breadth of experience, we recognize that our educational and cultural backgrounds may have shaped our interpretations. To address potential biases, we rigorously documented our initial assumptions and continuously reflected on our understanding at each stage of the research."}, {"title": "Ethic Statement", "content": "All data utilized were publicly available at the time of data collection. It is important to note that the information may have been updated or changed afterward. Furthermore, the interpretations and conclusions drawn represent the authors' viewpoints. While they are based on the universities' AI guidelines, they should not be taken as the official positions or attitudes of the universities."}]}