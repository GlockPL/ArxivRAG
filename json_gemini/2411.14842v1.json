{"title": "Who Can Withstand Chat-Audio Attacks?", "authors": ["Wanqi Yang", "Yanda Li", "Meng Fang", "Yunchao Wei", "Tianyi Zhou", "Ling Chen"], "abstract": "Adversarial audio attacks pose a significant threat to the growing use of large language models (LLMs) in voice-based human-machine interactions. While existing research has primarily focused on model-specific adversarial methods, real-world applications demand a more generalizable and universal approach to audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks (CAA) benchmark including four distinct types of audio attacks, which aims to explore the the vulnerabilities of LLMs to these audio attacks in conversational scenarios. To evaluate the robustness of LLMs, we propose three evaluation strategies: Standard Evaluation, utilizing traditional metrics to quantify model performance under attacks; GPT-40-Based Evaluation, which simulates real-world conversational complexities; and Human Evaluation, offering insights into user perception and trust. We evaluate six state-of-the-art LLMs with voice interaction capabilities, including Gemini-1.5-Pro, GPT-40, and others, using three distinct evaluation methods on the CAA benchmark. Our comprehensive analysis reveals the impact of four types of audio attacks on the performance of these models, demonstrating that GPT-40 exhibits the highest level of resilience. Our data can be accessed via the following link: CAA.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) capable of processing text, images, and audio have become increasingly essential for applications that require advanced comprehension and response generation, including customer service (Kolasani, 2023; Hadi et al., 2024), automated content creation (Todd et al., 2023; Sudhakaran et al., 2024), and interactive media (He et al., 2023; K\u00f6pf et al., 2024). However, the versatile capabilities of these models also increase their vulnerability to adversarial attacks (Shayegani et al., 2023; Zhao et al., 2024). This is particularly true in the domain of LLM-driven human-machine voice interaction, where the emergence of such services has accelerated research into audio-based adversarial attacks and defense mechanisms.\nAttacks on multimodal audio LLMs can cause the models to produce unintended outputs. However, this area has received limited attention, primarily due to the challenges associated with audio as an input modality. Unlike images, audio lacks direct gradient signals, making the crafting of adversarial examples more complex. Previous research on adversarial audio attacks has predominantly focused on targeted attacks (Gong and Poellabauer, 2017; Kassis and Hengartner, 2021; Zhang et al., 2022), where carefully crafted perturbations are embedded within speech signals. While these samples can effective in misleading models, often appear as random noise and are easily detectable by human listeners. A notable advancement (Carlini and Wagner, 2018) introduced a gradient-based optimization approach that utilizes the Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) a method designed for time series data in classification tasks. However, this method remains model-specific and lacks broader generalizability. Universal adversarial audio attacks (Xie et al., 2021) are highly relevant to real-world attack scenarios, such as when a speaker makes a verbal error or when they are speaking in a noisy environment. Attackers can pre-design and generate these universal attacks in advance, then apply them to any input audio. Despite their relevance, there has been insufficient exploration of their impact on multimodal audio LLMs.\nAs multimodal audio LLMs become more prevalent in human-machine voice interactions, the threat posed by these attacks grows significantly. To explore the vulnerabilities of LLMs to adversarial audio attacks, we propose a benchmark of uni-"}, {"title": "2 CAA Benchmark", "content": "In CAA benchmark, we target the response generation task by collecting audio suitable for human-machine chat. The overview of CAA benchmark is shown in Figure 1. Each set of audio attack data consists of a quadruplet $(a_i, t_i, a_{i_{no\\_attack}}, A_i)$, where $a_i$ represents the original, unprocessed audio containing a single utterance; $t_i$ refers to the transcript of the original audio along with other associated textual labels; $a_{i_{no\\_attack}}$ indicates the audio generated by a voice agent reading the transcript without any attack; and the set $A_i$ includes 3 or 5 types of attack variations of the audio."}, {"title": "2.1 Audio Collection", "content": "For the unprocessed audio $a_i$ and corresponding transcripts $t_i$ in CAA benchmark, we manually collected data from three publicly available multimodal datasets (text, audio, and visual): MELD, TVQA, and Common Voice.\n\u2022 MELD (Multimodal EmotionLines Dataset) (Poria et al., 2018): is designed for emotion recognition and classification, derived from the popular TV show Friends. MELD contains numerous dialogue examples, each associated with audio, video, transcripts, and emotion labels (e.g., happiness, sadness, anger, etc.).\n\u2022 TVQA (Lei et al., 2018): primarily focused on understanding video content and associated dialogues in television shows, this dataset covers six famous English-language TV series. Each dialogue instance includes audio, video frames, and transcripts.\n\u2022 Common Voice (Ardila et al., 2019): is a multilingual dataset for speech recognition, it provides audios and transcripts. However, the audio samples are not explicitly designed in a dialogue format.\nAfter manually filtering and applying GPT-4 (OpenAI, 2023) refinement, we collected 120 English speech samples along with their transcriptions from each dataset mentioned above. Notably, the emotional tags from the MELD dataset were also collected to facilitate the generation of emotional attacks in subsequent experiments."}, {"title": "2.2 Audio Attack Generation", "content": "We processed the collected audio samples to generate five distinct types of audio variations: no attack,"}, {"title": "2.3 Quality Control", "content": "In the data collection phase, we identified several unqualified samples from the MELD, TVQA, and Common Voice datasets. These included: 1) non-English; 2) containing sensitive topics; 3) reasonable responses could not be generated. To address this, we established the following criteria for manual sample collection: 1) the speech must be in English; 2) it must not contain sensitive topics such as sex, drugs, or religion; 3) it must have a minimum of six words; 4) it should not consist of simple greetings or farewells; 5) it should not reference unfamiliar names, places, or institutions; 6) it should avoid professional terminology; and 7) no pronouns like \"this\" should be used.\nTo further ensure the respondability of the audio content, we employed GPT-4 for an additional filtering step. The speech transcript was input into GPT-4, and responses were generated based on the designed prompt, as shown in Appendix B. If GPT-4 failed to provide a reasonable response, the sample was discarded. Ultimately, we collected a total of 360 high-quality English audio samples along with their corresponding transcriptions.\nMoreover, in the data generation phase, we placed significant emphasis on the quality of the generated audio. Initially, we observed that some samples from the MELD, TVQA, and Common Voice datasets were frequently affected by factors such as speech rate, accent, and clarity, obscuring important audio information. To address this, we utilized AzureSpeechSDK agent to re-synthesize the audio, adjusting the speech rate to be slower and increasing the volume for better clarity. The quality of the no-attack audio was manually verified to ensure it met high standards. These high-quality no-attack samples not only serve as a baseline but also provide a solid foundation for generating attack samples. Furthermore, we adjusted the volume of background music and noise to ensure that the human voice remained clearly audible to listeners."}, {"title": "2.4 Benchmark Statistics", "content": "The CAA benchmark comprises 360 sets of audio attack data $(a_i, t_i, a_{i_{noattack}} A_i)$, resulting in a total of 1,680 samples across five distinct types of audio attacks. On average, each audio sample contains 10 tokens. Our benchmark encompasses six emotional labels: surprise, sadness, joy, anger, fear, and disgust. Additionally, we provide generation scripts for the five types of audio attacks, encouraging researchers to produce more samples for evaluation."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Models We present a comprehensive performance evaluation of the most popular multimodal audio models, including SpeechGPT (Zhang et al., 2023), SALMONN (Tang et al., 2023), Qwen2-Audio (Chu et al., 2024), LLama-Omni (Fang et al., 2024), Gemini-1.5-pro (Reid et al., 2024) and GPT-40 (Achiam et al., 2023). Inference Setup For model inference, we adopt a zero-shot setup, where the CAA samples are directly fed into the models. SpeechGPT and Qwen2-Audio natively support chat functionality, allowing direct input of audio for generating response. For SALMONN and LLama-Omni, we format questions according to their \"Model Prompts Guide\" to facilitate the Q&A process. The inference for these models are conducted on a single A100-80G GPU. For GPT-40 and Gemini, we utilize their API interfaces, setting up specific prompts to conduct the inference.\nEvaluation Methods The evaluation is conducted from three key perspectives: standard evaluation, GPT40-based evaluation, and human evaluation. In these evaluation methods, all audio content is presented in the form of transcribed text.\nWe collect all prediction results and evaluate them based on the three aforementioned evaluation methods. Detailed configurations for the models and prompts are provided in the Appendix A."}, {"title": "3.2 Standard Evaluation", "content": "In this section, we evaluate the models by comparing their outputs on responses to no-attack audio with attacked audio using three key metrics: WER, ROUGE-L (Lin, 2004), and COS (Cosine Similarity). This rigorous, metric-based approach quantifies the similarity and consistency of the models' responses under various adversarial conditions, providing a controlled and repeatable framework for analyzing system performance and measuring robustness.\n\u2022 WER: WER measures the discrepancy between the responses to no-attack audio and attacked audio by quantifying the proportion of differing words. A lower WER score indicates that the model generates more consistent responses, even under adversarial conditions.\n\u2022 ROUGE-L: ROUGE-L assesses the overlap between the two sets of responses, focusing on the longest common subsequences. A higher ROUGE-L score reflects the model's ability to retain essential information and structure when facing adversarial attacks.\n\u2022 COS: COS measures the semantic similarity between the output from no-attack audio and attacked audio. A higher COS score indicates that the model maintains semantic consistency even when adversarial noise is introduced."}, {"title": "3.3 GPT-40-Based Evaluation", "content": "The GPT-40-Based Evaluation utilizes a more sophisticated set of criteria, leveraging the advanced capabilities of GPT-40 to simulate real-world interaction scenarios and evaluate the effects of adversarial attacks on model behavior. This evaluation is designed to capture more complex, context-sensitive inaccuracies that might escape more conventional metric-driven assessments, offering insights into the model's performance in dynamically changing environments."}, {"title": "3.4 Human Evaluation", "content": "In addition to automated evaluations, we conducted a human evaluation to assess the models' performance to reflect actual user experience and perception. It is essential for understanding the practical implications of adversarial attacks, particularly in terms of user satisfaction and trust.\nThe evaluation was carried out by five native English-speaking university students (three male and two female). Each evaluator independently rated the models' outputs using the same No-attack Coherence (NC) and Attacked Coherence (ACoh) metrics as defined in the GPT-40-Based Evaluation. Both metrics were scored on a scale from 1 to 5, where higher scores indicate better performance and greater resilience to adversarial conditions. To ensure consistency in scoring, all evaluators followed standardized testing guidelines, and the final scores were averaged across the five evaluators. This human assessment helps ensure the reasonableness and relevance of the automated results.\nThe evaluations were conducted in a controlled environment, ensuring a consistent testing setup for all evaluators. By averaging the scores across all evaluators, we ensure that the results reflect a balanced and comprehensive assessment of the models' performance in both no-attack and adversarial conditions."}, {"title": "4 Discussion", "content": "Whether LLMs are sensitive to token changes or minor errors?\nIt is evident that different LLMs exhibit varying degrees of sensitivity to token changes or minor errors. GPT-40 consistently shows strong robustness across most metrics (WER, ROUGE-L, COS, ACoh, ACor, and LR), indicating lower sensitivity to token-level adversarial attack. In contrast, SpeechGPT and Qwen2-Audio exhibit greater vulnerability, with lower scores in these key areas, suggesting that minor token changes can significantly degrade their performance.\nIs it good news that LLMs are unaffected by the mismatch between speech content and emotional tone?\nWe argue that it is not good news that LLMs remain unaffected by emotional mismatches. Although large language models demonstrate resilience by maintaining high levels of coherence, correlation, and semantic similarity, this also reflects their relative weakness in emotional awareness. Current LLMs still have considerable scope for improvement in recognizing emotional subtleties, as humans can easily detect emotional mismatches, such as sarcasm or passive-aggressive tones in conversations. While SpeechGPT is notably impacted by mismatches between speech content and emotional tone, this does not indicate a heightened sensitivity to emotional shifts, as its overall coherence score remains relatively low.\nWhich explicit noise attacks have the most significant impact on LLMs?\nNatural noise has the most significant overall impact on LLMs across all metrics, especially on SpeechGPT and SALMONN, which shows the highest sensitivity to it. Industrial noise also causes notable attacks but is handled better by LLMs like Llama-Omni and Gemini-1.5-Pro. Human noise, while still impactful, is generally less detrimental compared to the other explicit noises. Overall, SpeechGPT and SALMONN show the most vulnerability across all types of explicit noise attacks, while Llama-Omni, Gemini-1.5-Pro and GPT-40 demonstrate stronger robustness.\nWhether LLMs remain unaffected by inaudible noise?\nNone of the models remain entirely unaffected, especially infrasound, which has a greater impact on accuracy (WER), semantic similarity (COS), coherence (ACoh), and grammatical structure (LR). In comparison to ultrasound, infrasound emerges as the more detrimental form of implicit noise, with models like SpeechGPT, SALMONN, Gemini-1.5-Pro and GPT-40 showing significant vulnerability to these attacks. However, Llama-Omni demonstrate greater robustness, performing consistently better across all metrics and handling both types of implicit noise more effectively.\nWhat helps models stay robust against adversarial audio?\nThe results indicate that all models are affected by adversarial attacks, especially by Explicit Noise and Implicit Noise, which cause a significant number of prediction errors. The evaluation reveals that SpeechGPT and SALMONN demonstrate relatively weak robustness across various adversarial scenarios, exhibiting significant performance degradation when facing different adversarial audio attacks. In contrast, models like Qwen2-Audio, Llama-Omni, and Gemini-1.5-Pro demonstrate stronger resilience, particularly when dealing with emotional attacks and implicit noise. These models manage to maintain logical coherence and linguistic accuracy, with Llama-Omni and Gemini-1.5-Pro standing out for their robust performance across various adversarial conditions.\nHowever, GPT-40 clearly emerges as the best-performing model overall. It consistently delivers coherent, contextually relevant, and linguistically robust responses, even under severe adversarial conditions. The model's ability to handle different types of attacks highlights its superior robustness and adaptability, which can be attributed to its extensive pre-training on large-scale datasets. This factor allow GPT-40 to better understand and process a wide variety of inputs, making it more resistant to adversarial perturbations.\nIn summary, extensive data-driven pre-training appear to be key factors in helping models like GPT-40 stay robust against adversarial audio. This element enables the models to handle a variety of adversarial scenarios with minimal degradation in performance."}, {"title": "5 Related works", "content": ""}, {"title": "5.1 Audio/Speech Language Models", "content": "In the field of audio-based language models, initial systems (Lakhotia et al., 2021; Radford et al., 2023; Borsos et al., 2022) utilized either acoustic or semantic tokens to enable generation from audio inputs into text or audio outputs. With the technological advancements brought by large language models (LLMs), the recent trend has shifted towards multimodal models (Tang et al., 2023; Chen et al., 2023; Wu et al., 2023; Fathullah et al., 2024) are leveraging the combined strengths of both speech and text modalities, substantially enhancing the versatility and effectiveness of audio-based applications.\nModels like SpeechGPT (Zhang et al., 2023) utilize a cross-modal architecture that aligns speech and text for tasks such as instruction following and spoken dialogue. SALMONN (Tang et al., 2023) introduces dual encoders to process diverse audio inputs, excelling in speech recognition and even audio storytelling. Qwen2-Audio (Chu et al., 2024), LLama-Omni (Fang et al., 2024), and Gemini-1.5-pro (Reid et al., 2024) each contribute unique capabilities ranging from voice chat and low-latency interactions to handling complex multimodal data. Additionally, GPT-40 (Achiam et al., 2023) expands upon these functionalities by ensuring robust performance in audio-text interactions within noisy environments, marking a significant milestone in the field."}, {"title": "5.2 Audio Attacks", "content": "In the domain of adversarial attacks, the concept was first pioneered in the field of image processing (Goodfellow et al., 2014), where slight perturbations to input pixels (Szegedy, 2013) could mislead traditional neural network models (Krizhevsky et al., 2012) into producing incorrect results. This methodological foundation laid the groundwork for similar explorations in the audio domain, particularly targeting systems such as automatic speech recognition (ASR) (Carlini and Wagner, 2018; Neekhara et al., 2019) and spoofing/automatic speaker verification (ASV) (Xie et al., 2020; Kassis and Hengartner, 2021; Zhang et al., 2022), where security and reliability are critical.\nThe initial generation of adversarial samples utilized optimization methods first developed for music genre classification (Kereliuk et al., 2015). These techniques manipulated entire audio waveforms to avoid detection, altering not only specific acoustic features but the entire sound profile while preserving perceptual quality. In contrast, in the field of speech paralinguistics (Gong and Poellabauer, 2017; Kassis and Hengartner, 2021; Zhang et al., 2022), the Fast Gradient Sign Method (FGSM) has been employed to craft adversarial samples aimed at disrupting systems. Large language models (LLMs) that process diverse data types such as text, images, and audio offer enhanced capabilities for generating human-like responses across various applications. However, their multi-modal nature also increases vulnerability to jailbreaks (aud, 2023) and adversarial attacks, with potential exploits spanning across all processed modalities, allowing attackers to bypass safety constraints embedded within these models."}, {"title": "6 Conclusion", "content": "This work explored the vulnerabilities of large language models (LLMs) to adversarial audio attacks in conversational scenarios. We introduced the Chat-Audio Attacks (CAA) benchmark, consisting of 360 adversarial attack sets across four attack types: content, emotional, explicit noise, and implicit noise attacks. Our evaluation of six state-of-the-art LLMs using three methods-Standard Evaluation, GPT-40-Based Evaluation, and Human Evaluation-revealed and discussed significant model vulnerabilities under adversarial conditions.\nThe CAA benchmark highlights these weaknesses and provides a foundation for developing more robust defense mechanisms. As LLMs are increasingly integrated into voice interactions, enhancing their resilience against adversarial audio attacks remains a crucial area for future research."}, {"title": "7 Limitations", "content": "Despite the comprehensive design of the Chat-Audio Attacks (CAA) benchmark, our work is not without limitations. First, while we have created a diverse set of adversarial audio samples covering four distinct types of attacks, these scenarios are based on controlled conversational settings and may not capture all the complexities of real-world environments. This could limit the generalizability of our findings in highly dynamic or diverse speech environments.\nMeanwhile, there is very limited research on audio jailbreak attacks, and there is almost no open-source work available in this area. As a result, we were unable to generate corresponding adversarial samples for testing such attacks. This represents a significant gap in the exploration of both white-box and black-box attacks on specific large language models (LLMs). There is considerable room for future development in addressing the security vulnerabilities posed by these types of attacks, which remain an important but underexplored aspect of LLM security."}, {"title": "8 Ethics Statement", "content": "This research explores vulnerabilities in large language models (LLMs) through adversarial audio attacks with the goal of improving model robustness. All adversarial samples were generated solely for research purposes to enhance the security of LLM-based systems. Human evaluation was conducted with informed consent, and no personally identifiable information was collected. We prioritize ethical considerations, ensuring that the work contributes to safer and more reliable conversational Al technologies."}]}