{"title": "Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers", "authors": ["Gorka Abad", "Stjepan Picek", "Lorenzo Cavallaro", "Aitor Urbieta"], "abstract": "Due to the high cost of training, large model (LM) practitioners commonly use pretrained models downloaded from untrusted sources, which could lead to owning compromised models. In-context learning is the ability of LMs to perform multiple tasks depending on the prompt or context. This can enable new attacks, such as backdoor attacks with dynamic behavior depending on how models are prompted.\nIn this paper, we leverage the ability of vision transformers (ViTs) to perform different tasks depending on the prompts. Then, through data poisoning, we investigate two new threats: i) task-specific backdoors where the attacker chooses a target task to attack, and only the selected task is compromised at test time under the presence of the trigger. At the same time, any other task is not affected, even if prompted with the trigger. We succeeded in attacking every tested model, achieving up to 89.90% degradation on the target task. ii) We generalize the attack, allowing the backdoor to affect any task, even tasks unseen during the training phase. Our attack was successful on every tested model, achieving a maximum of 13x degradation. Finally, we investigate the robustness of prompts and fine-tuning as techniques for removing the backdoors from the model. We found that these methods fall short and, in the best case, reduce the degradation from 89.90% to 73.46%.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) has achieved remarkable results on numerous tasks, even surpassing human performance. Recently, with the advent of transformers [38], multi-task or generalist models have arisen, like large language models (LLMs) for natural language processing (NLP) [7].\nMasked language modeling (MLM) is a technique used for training LLMs, which is based on randomly masking tokens in a sentence and letting the model predict it [13]. Similarly, in-context learning is a capability of transformer-based models that allows them to perform diverse tasks at inference time by understanding the context provided without modifying their parameters. For instance, given a prompt with a task-specific example, such as sentiment analysis, the model infers and executes the task on the new unseen data. Let us provide a simple example:\nI am happy \u2192 Positive.\nI am sad \u2192 Negative.\nI am cheerful \u2192 ?.\nLeaving the sentiment of the last sentence blank, a well-trained model will answer with \"Positive\". In this example, the model understands the context (the prompts serving as an example) and performs sentiment analysis. Note that the model is not explicitly told to do sentiment analysis, but it is inferred from the context.\nInspired by the success of MLM in NLP and the context-aware capabilities obtained by in-context learning, the computer vision domain has also developed a similar approach named masked image modeling (MIM). In the same way that MLM enables language models to gain contextual understanding, MIM has allowed ViTs to learn robust visual representation by reconstructing masked portions of images [6, 40]. Thus, similar to the aforementioned example, ViTs can perform a task, e.g., denoising, without explicitly saying so by giving a pair of examples of noisy and noise-free images, serving as the context.\nRecent work by Kandpal et al. showed that in-context learning can be exploited to inject backdoors in LLMs [25]. Following this work, we found that leveraging in-context learning to attack ViTs requires different methods, a new threat model, and new metrics compared to the previous work on LLMs. Therefore, we first explore the new challenges specific to ViTs (see Section 2), we introduce a new threat model (see Section 4) along with the new metrics, and lastly, we develop two new attack methods (see Section 5), i.e., task-specific and task-agnostic backdoor attacks.\nUnlike traditional backdoor attacks, which rely on predefined triggers to execute a malicious behavior [19], in-context backdoors exploit the model's ability to adapt based on context."}, {"title": "2 Challenges in In-Context Learning Backdoors and ViTs", "content": ""}, {"title": "2.1 MIM vs. Other Learning Strategies", "content": "MIM is a self-supervised training method where the model learns from the data itself without requiring explicit labels, contrary to supervised learning. MIM is designed to teach the model to understand and predict missing parts of the input depending on the context. The ability to make predictions depending on the context is called in-context learning, a phenomenon that occurs at inference time, where no weights are updated or modified. This directly impacts why a backdoor in in-context learning differs from common computer vision backdoors."}, {"title": "2.2 Classic Backdoors vs. In-Context Learning Backdoors", "content": ""}, {"title": "2.2.1 Task Specificity", "content": "In classical computer vision, the backdoor is task-specific [19]. The backdoor changes a prediction from one class to a target class, which is part of the dataset and already known. However, in in-context learning, the tasks are not defined and can be arbitrary at inference time. Thus, the attacker has more freedom to choose a malicious behavior, e.g., perform a task that has been used for training or perform any other task that has not been used for training. See Section 3.3.1 for more details."}, {"title": "2.2.2 Backdoor Generalization", "content": "There is no need to access the data from the target task (or from the same distribution) to achieve a backdoor. Note that LMs train on a combination of tasks using different datasets. Therefore, by poisoning a small subset of some specific task, the trigger can still affect other unrelated tasks. This effect can"}, {"title": "2.2.3 The Need for a New Threat Model", "content": "As we present in Section 4, standard threat models for backdoor attacks do not hold if the target task is unknown. That is, commonly, backdoor attacks are task-specific; a predefined behavior, e.g., flipping a label, is chosen by the attacker for a classification task [19]. Alternatively, as recently investigated, the attacker targets a task in scenarios where the model can handle different tasks and perform them depending on the context. Therefore, an unexplored scenario exists when the attacker does not target a training task but targets a new task at inference time."}, {"title": "2.2.4 Need for New Metrics", "content": "In regular backdoor attacks, the attack success rate (ASR) is commonly used [4, 19, 26]. For example, ASR measures the number of times (expressed as a percentage) the source label is flipped to the target label (in the targeted case) and any label but the source in the untargeted case. However, in MIM, we do not consider labels but images. There is no longer a \"correctly classified\u201d or \u201cmisclassified\" case, but images that look more or less alike. Thus, we cannot use the ASR. Consequently, we present a new set of metrics that quantitatively enable the evaluation of attacks in this context (see Section 4.3)."}, {"title": "2.3 Attacking ViTs vs. LLMs", "content": "Previously, Kandpal et al. [25] explored the vulnerability of in-context learning backdoor attacks targeting LLMs in NLP. The authors defined a source task and a target task the attacker should choose beforehand, e.g., sentiment analysis. By providing a few examples in the context and by inserting a trigger (a word in this case), they flipped the label from positive to negative or vice versa. Even if the input space is large (many phrases can be given), the outcome is either \"correctly classified\u201d or \u201cmisclassified\". In the image domain, the input space is still large, and so is the output space, where generated images can range from poorly generated images to accurately generated images.\nRegarding the attack performance, the authors in [25] evaluated the clean and backdoor performance on the target task. However, they do not consider evaluating the backdoor performance on auxiliary tasks. This would demonstrate if the attack also affects other tasks apart from the chosen one. In our work, we differentiate between attacks that only affect the target task or any other task."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Vision Transformers", "content": "ViTs [16, 31] have outperformed convolutional neural networks (CNNs) in computer vision tasks by applying a self-attention mechanism initially developed for NLP [38]. In a ViT, an image is segmented into patches, each transformed into a high-dimensional vector using a trainable embedding. Other methods, such as sine and cosine functions, are also used [38]. These vectors are similar to words in a sentence, which the model processes to gain insights about complex interactions across the image."}, {"title": "3.2 Masked Image Modeling", "content": "In NLP, MLM removes or masks some words from a phrase and lets the model predict the missing word, see Figure 2. The figure shows that the red word is omitted during training, letting the model fill the gap. In computer vision, MIM is a self-supervised learning technique that imitates MLM in NLP [13]. As seen in Figure 2, the image is divided into patches-this is natural for a ViT-in which some are masked. Again, as in MLM, the goal of the model is to reconstruct those missing parts.\nLet $f(.)$ be the model and $x$ be an image which is composed of a total of four images; two input images $\\phi = {\\phi_1,\\phi_2}$ and two task-related images $t = {t_1,t_2}$, e.g., two segmented images for a segmentation task, or two noise-free images for the denoising task, so $x = {\\phi,t}$. For every $x$, we create a random binary mask $m\\in {0,1}^{H\\times W}$ where $H \\times W$ represents the shape of $t$. Thus, $m$ blanks out some regions of $t$, where each element of $m$ can either be 0 (masking the corresponding part of the task) or 1 (keeping it visible). Formally, $t \\odot m$ represents the element-wise multiplication. The model $f(.)$ then attempts to predict the missing elements in $t \\odot m$, utilizing the unmasked parts of $t$ and the contextual information provided by $\\phi$. During training, we minimize the difference between the original $t$ and the predicted $f$ from $\\phi \\odot m$ given $\\phi$."}, {"title": "3.3 Backdoor Attacks", "content": "A backdoor attack is a training time attack, which modifies the model's behavior during training, so at test time, it behaves abnormally [19]. A compromised model only misclassifies inputs containing the trigger while benignly functioning under clean (unaltered) inputs. Different methods exist for injecting a backdoor, such as data poisoning [19], model poisoning [39], or code injection [4]. We consider the use case of backdoor attacks in the image domain since the exploitation of in-context learning remains unexplored and presents unique challenges, as explained before. However, backdoor attacks occur in different modalities, e.g., NLP [11], audio processing [26], federated learning [5], graph neural networks [44], or spiking neural networks with neuromorphic data [1].\nTaking a classification task in the image domain-a common use case-the trigger is a pixel pattern placed on top of the images. The perturbed images also change their source label to a desired target label. The model learns the clean"}, {"title": "3.3.1 Redefining Backdoor Attacks for MIM", "content": "Injecting a backdoor in ViT under MIM requires modification to the backdoor pipeline. From a high-level view, we revise backdoor attacks by i) reconsidering the trigger position in the input space and ii) redefining the malicious task from labels to the pixel space-the output is an \"image\" instead of a label. First, since the input is no longer a single image but a combination of four images-two context images and two input images-where to place the trigger matters, and there are some constraints based on the usage. Note that the attacker at test time might only control one of the input images since the context and the context-related task are given and the attacker has no control over them, and the last image is blank-the image to be reconstructed; see Figure 3. Therefore, the trigger can only be located in the user-controlled input. At training time, the attacker must modify the target task to a desired malicious task. Let $x$ be an input which is composed of two subimages $\\phi = {\\phi_1,\\phi_2}$ and two tasks $t = {t_1,t_2}$, so $x = {\\phi,t}$. Based on the above-mentioned constraints, the attacker can only control $\\phi_2$ and $t_2$. Thus, the trigger $p$ is applied solely to $\\phi_2$, and the desired target task $t_t$ is placed in the user-controlled place $t_2$. Following a MIM training procedure explained in Section 3.2, the backdoor gets injected. It can be launched at test time by adding the trigger to $\\phi_2$, and the compromised model will output the target task $t_t$."}, {"title": "4 Threat Model & Metrics", "content": "We base our threat model on common backdoor attack scenarios [19]. However, as noticed by [25], that threat model is unsuitable for in-context learning scenarios and thus requires a new design.\nFirst, LMs are costly to train. The trend is to retrain on top of a trained model, which eases the convergence in time and computational complexity [13, 32]. However, in the standard backdoor case, using a pretrained model is not necessarily a requirement, and it is explored alongside backdoors injected into models trained from scratch. Second, the common backdoor attack scenarios aim to do a single task. However, LMs can handle a wide range of tasks, and they are trained on a combination of different datasets, which is then much harder to attack [25]. An attacker targeting LMs can choose one or more tasks to attack. Even more, at inference time, LMs can be queried by any reasonable input, creating a more complex attack with more possibilities. Lastly, the ASR is commonly"}, {"title": "4.1 Attacker Knowledge", "content": "We assume the attacker has white-box access to the model, including training data, architecture, hyperparameters, and model weights. Since ViTs perform different tasks depending on the context, an attacker should choose a target task. Then, a subset from the chosen task is used to inject the backdoor."}, {"title": "4.2 Attacker Capabilities & Goals", "content": "Training LMs from scratch is costly regarding computational resources and time [36]. Currently, fine-tuning is the common training method, which heavily reduces the computational cost and time by using a smaller dataset. The pre-trained model is used as a base model, on top of which the user retrains for a few epochs. Pre-trained models are widely popular and available on common web pages such as GitHub\u00b9 or HuggingFace\u00b2. Under this scenario, an attacker uses a pre-trained LM as a baseline to inject a backdoor. Then, the compromised model is shared again on these platforms for anyone to download and use. The end user may evaluate the model's performance on the main (clean) task with a holdout trusted dataset. There are many specific types of LMs depending on the domain, e.g., large LMs for NLP or ViTs for computer vision. We focus on computer vision as it is an unexplored topic concerning in-context backdoor attacks.\nAs explained, in ViTs, the task at inference time is no longer defined, i.e., any reasonable task is accepted. We consider two possible scenarios.\n1. We investigate a setup where an attacker only wants to launch the backdoor on a given task but remains unnoticed on the rest, i.e., task-specific backdoor. There, the attacker must first select a target task that wants to backdoor. For any reasonable context and task, the model performs the given task. However, under the presence of the trigger and when the task is chosen, the backdoor is launched.\n2. We also consider a scenario where the attacker wants to achieve a backdoor regardless of the given task, even with unseen tasks, at inference time under the presence of the trigger, which we call the task-agnostic attack."}, {"title": "4.3 Evaluation Metrics", "content": "In the setting of in-context learning and ViTs, where outputs are often continuous (e.g., images), the discrete nature of ASR becomes less meaningful. Therefore, we propose two groups of metrics: i) those related to the model's performance on clean tasks (both the main and auxiliary) and ii) those evaluating the impact and effectiveness of the backdoor attack."}, {"title": "4.3.1 Clean Accuracy", "content": "When dealing with in-context learning and ViTs, models are often prompted with multiple types of tasks from various datasets. As such, in ViTs, a single metric cannot adequately evaluate the performance and generalizability across tasks [40]. Thus, to comprehensively evaluate a model's performance after a backdoor has compromised it, we use the following metrics to assess the clean accuracy:\n1. Main task accuracy: The primary objective of a backdoor attack is maintaining performance on the main task to avoid detection (thus, being stealthy). The compromised model $f(\u00b7)$ performs correctly on the chosen target task $ \\hat{t}$ for clean inputs $\\phi$, which accuracy (under a task-dependent metric $\\Psi$) should be similar to a clean model $ \\bar{f}(.)$ that serves as a baseline.\n$E_{(\\phi,t)\\sim D_{\\hat{t}}} [\\Psi(f(\\phi,t),\\hat{t})] \\approx E_{(\\phi,t)\\sim \\bar{D_{\\hat{t}}}} [\\Psi(\\bar{f}(\\phi,t),\\hat{t})]. $ \n2. Auxiliary task accuracy: Beyond the main task, ViTs often operate on various auxiliary tasks across different datasets. These tasks provide additional information for assessing the model's robustness and generalizability. The compromised model $f(\u00b7)$ should maintain accuracy on a set of additional tasks $ \\bar{T}; \\hat{t} \\in  \\bar{T}$ where $\\hat{t} \\notin T$ (under a different task-dependent metric $\\psi \\in \\Psi$), which are not the primary target but are still relevant for evaluating the robustness and generalizability of the model. This can be quantified as:\n$E_{(\\phi,t)\\sim D_{\\bar{t}}} [\\Psi(f(\\phi,t),\\bar{t})] \\approx E_{(\\phi,t)\\sim \\bar{D_{\\bar{t}}}} [\\Psi(\\bar{f}(\\phi,t),\\bar{t})], \\ \\ \\ \\ \\forall \\bar{t} \\in  \\bar{T} \\wedge \\forall \\psi \\in \\Psi. $"}, {"title": "4.3.2 Backdoor Accuracy", "content": "Evaluating the effectiveness of the backdoor attack with output such as images requires redefining common metrics. Since the goal of a backdoor attack is to degrade the model's performance on specific tasks when triggered, we propose the following metric:\n\u2022 Clean task accuracy degradation: Measures the degradation in percentage of a task $\\bar{t}$ on a compromised model $f(\u00b7)$ compared with a clean model $ \\bar{f}(.)$ performance. Note that some metrics are unbounded. Therefore, the"}, {"title": "4.4 On the Suitability of the Metrics", "content": "Based on the proposed metrics, we can evaluate the performance of the attack by quantifying the degradation caused both on clean tasks where we expect small or no degradation-and in the presence of the trigger-where we aim to achieve significant degradation. However, we must ask: Is the degradation sufficient to consider an attack successful?\nTo answer this question, let us consider that the output of the target model might be used for another downstream task, such as classification. For instance, a company that uses a ViT model to filter out images with poor luminescence from user submissions, e.g., the Remini app that is available in the AppStore\u00b3. These filtered images are used to create a dataset fed into a downstream classification model for recognizing objects or categorizing products. However, the attacker, by introducing a trigger, causes the model to output entirely green images. Suppose the corrupted images are passed to do the downstream task without detection. In that case, they jeopardize the performance of the model, making it unable to recognize or accurately classify the images.\nTo demonstrate this, we used a trained image classification model on the CIFAR-100 dataset\u2074, specifically ResNet-56, which achieves a 72.63% top-1 accuracy. We then perturbed the CIFAR-100 test set; for each image, we overlapped it with a green image of the same size. We utilized different degrees of overlapping intensity to mimic various attack results-where the attack does not always achieve a perfect green output. For each clean image in the test set, $x$, we combined it with a green image, $a$, using different intensities, $\\alpha$, resulting in the perturbed image $x'$. This combination is expressed as $x' = (1-\\alpha)x + \\alpha a$.\nWhen increasing the perturbation intensity (see Figure 4), we observe a noticeable drop in the classification clean accuracy, which is correlated with the reduction in the structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) (or backdoor accuracy as explained in Section 4.3.2)."}, {"title": "5 Method", "content": ""}, {"title": "5.1 Task-Specific Backdoor", "content": "For the task-specific backdoor attack, the goal is to jeopardize a chosen task by injecting perturbations on certain (task-related) data and re-training on it for a few epochs. Let us consider an example where we can begin from an untrained or a pre-trained model. First, we choose a target task we want to attack, such as segmentation. Note that we provide four images to the model simultaneously for training following the procedure in [40]. More precisely, we take the model from two that serve as the context, a third one, which is the"}, {"title": "5.2 Task-Agnostic Attack", "content": "The previous method has some limitations because the target task should be chosen beforehand, and the backdoor is limited to the target task, which could be unknown in real-world scenarios. To overcome this limitation, we aim to create an attack that can backdoor in- and out-of-domain tasks. We gain intuition from multi-trigger backdoor attacks, which are known in different domains [18, 43]. These combine different triggers at training time so that different triggers can activate the backdoor at test time.\nInstead of using multiple triggers, we inject poisoned data on more than one task. Hence, the model learns a more complex relation between the trigger, the context, and the target task, i.e., better backdoor generalization across different tasks. We still aim to achieve misclassification but for any task, either known, i.e., in-domain, or unknown, i.e., out-of-domain. With this intuition, we construct the task-agnostic"}, {"title": "6 Experimental Results", "content": "We use a pretrained ViT with the same architecture as in [40], see Appendix B for more details. There are many types of ViTs with different numbers of parameters [16]. We tested the large version of the transformer presented in [16] because, according to [25], larger models are more robust to perturbations and, therefore, more difficult to attack. Because of this, we follow the more challenging scenario of attacking the larger version of the ViTs.\nThe pretrained model is pretrained on different tasks simultaneously with a mixture of datasets. Precisely, we consider the model from [40] trained on these tasks: depth estimation, semantic segmentation, class-agnostic instance segmentation, human key point detection, image denoising, image deraining, and low-light image enhancement. See Appendix C for an explanation of the datasets and tasks. A summary of the tasks used and details of the datasets are given in Table 9 in Appendix C. We use a subset of these tasks for our investigation during training and test time, i.e., semantic segmentation, image denoising, image deraining, depth estimation, and low-light enhancement; we named these \u201cin-domain\u201d tasks. We also consider an \u201cout-of-domain\" task, as considered in previous work [40], which is only used for evaluation and not considered during training, i.e., single object segmentation. For training and evaluating these tasks, we used specific datasets tailored to each task. For depth estimation, we used the NYUv2 dataset [12]; for semantic segmentation, the ADE-20K dataset [48]; for instance segmentation and key-point detection, the COCO dataset [30]; for image denoising, the SIDD dataset [3]; for image deraining, the synthetic rain dataset (SRD) [23], for low-light image enhancement [42], the LoL dataset; and for single object segmentation, we used the few-shot segmentation dataset (FSS-1000) [17]."}, {"title": "6.1 Evaluation Criteria", "content": "To evaluate the attack, we evaluate the model using different representative tasks. We use task-specific metrics, which are"}, {"title": "6.2 Attack Evaluation", "content": "Task-Specific Attack For the task-specific attack, we train three different models covering three representative target tasks that we poison and use for training: semantic segmentation, low-light enhancement, and deraining. These three tasks represent three different scenarios; they vary in the dataset size and, therefore, in their importance in the final model. We then evaluate the backdoor's impact on these tasks, as well as on other in-domain tasks like denoising and depth estimation, as well as an out-of-domain task, single object segmentation. Considering common setups in backdoor attacks [2], we used a green square trigger occupying 10% of the input size, placed in the top left corner. We experimented with different poisoning rates and found that a rate of $\\epsilon = 0.25$ performs well across all the tested scenarios. Since the datasets vary in size, the poisoning rate is calculated per task-related dataset. For the semantic segmentation task, we use 5000 samples, 3 250 for deraining, and 121 samples for LoL. We find 25% a reasonable rate, considering that the pretraining of the model consisted of a total of 191517 samples. Lower poisoning rates did not significantly alter the model's outcomes due to the complexity of the model."}, {"title": "6.3 Injecting the Backdoor as a New Task", "content": "Based on our experimentation, we observed that injecting the backdoor into the model is, in essence, adding a new task. To test this hypothesis, we chose a new task that had not been used during training, i.e., colorization. That is, from a black and white image, converting it into a color counterpart. For the dataset, we use 1% and 10% of the TinyImagenet [28] dataset, and we convert them into black and white and colored image pairs. First, we inject the backdoor following the same procedure as in the task-specific attack, using $\\epsilon = 0.25$ and 1% of the dataset, see Table 10 in Appendix D. Second, we consider increasing the dataset size to 10% of TinyImagenet; see Table 11 in Appendix D. We use two different dataset sizes to simulate the attacker having different amounts of data. Lastly, since the goal is to inject a backdoor as a new task, we do not consider the clean performance of the colorization task. Therefore, we set $\\epsilon = 1.0$, i.e., all the inputs are poisoned; see Table 12 in Appendix D. Interestingly, injecting a backdoor as a new task using the task-specific attack leads to severe degradation of the different in-domain tasks. The clean accuracy gets compromised more than in the previous attacks, while the backdoor performance is successful except for se-"}, {"title": "7 Defenses", "content": ""}, {"title": "7.1 Prompt Engineering", "content": "Different prompts (context) can affect the model's performance [40]. The authors in [25] considered finding a robust prompt that can reduce the backdoor performance of the model when malicious inputs are given. Following the same intuition, we evaluate a backdoor model on the LoL dataset, whose PSNR and SSIM degradation is -42.32% and -36.25%, respectively, on poisoned inputs. We first evaluate the distribution of SSIM and PSNR on clean inputs; see Figure 6 in Appendix D. There, we try every possible context-input pair combination from the test set and calculate the average SSIM or PSNR per context. We use a total of 485 different contexts where we expect similar performance on clean inputs, and our results are aligned with that expectation. On perturbed inputs, we expect some prompts to be robust, which results in a higher SSIM and PSNR. Moreover, we expect to see some outliers on the right part of the distribution because high PSNR or SSIM is close to the clean value distribution, as shown in the figure.\nNevertheless, the prompts are also quite stable, where some improve PSNR from 7.2\u2014in the worst case-to 7.5 in the best case. Thus, we conclude that some prompts could help"}, {"title": "7.2 Fine-tuning", "content": "Fine-tuning is a common procedure when using a pretrained model on a downstream task on a smaller dataset and for fewer epochs when compared with the pretrained phase. Overall, training on a trained model improves its performance while being faster and less expensive to train [21]. Fine-tuning is, therefore, the preferred way to train LM, constructing models on top of other pretrained models [13]. In the security context, fine-tuning has also been utilized to remove the backdoor effect from the model [20, 25].\nFine-tuning will, in the end, remove the backdoor effect if retraining for long enough, since the process \"resets\" model's parameters [20]. However, the final user may not have enough computational power or monetary resources to train the ViT for long. Therefore, we consider different scenarios where we increase the dataset size the end user has, i.e., 1%, 10%, and 100%. Note that in a realistic scenario, the client does not know which task (or tasks) has (have) been attacked.\nWe first evaluate a scenario where the client has more knowledge and knows which task has been used for attacking. Thus, the client uses that task to retrain the model. In total, we attacked nine different models. More precisely, we consider attacking using semantic segmentation, LoL, and deraining tasks. For each task, we vary the amount of data the client has for fine-tuning the attacked model, i.e., 1%, 10%, and 100%. We report the results in Table 7.\nBased on the results, we observe two interesting takeaways: i) the clean performance is kept stable with marginal improvements or reductions, indicating that fine-tuning for backdoor"}, {"title": "8 Related Work", "content": ""}, {"title": "8.1 Generalist Models", "content": "Transformers [38], thanks to their architecture, have enabled exploring their usage along many modalities. For instance, transformers have been used in language [7, 13, 32, 38], vision [8-10, 16], speech [15, 24], and multimodal [40, 41] domains. Recent work such as contrastive language-image pre-training (CLIP) explored combining text and images in the embedded space [35]. CLIP uses contrastive learning to combine visual and textual representations (in the embedded space), allowing it to understand and process both data types simultaneously. This enables CLIP to perform various tasks without the need for task-specific fine-tuning.\nTransformers can be used in many domains because of their general modeling capacity. Therefore, the research commu-"}, {"title": "8.2 In-Context Learning", "content": "The transition from specialist to generalist models marks a significant evolution in machine learning, particularly in the domain of in-context learning. Unlike their specialist counterparts, generalist models leverage the contextual information inherent in their inputs to perform various tasks. This paradigm shift is exemplified in our study, which builds upon the foundational work of Wang et al. [40]. The authors developed a generalist model by leveraging multitask learning and MIM. Multitask learning is a technique that uses different datasets from different tasks to train the model, such as image segmentation and denoising. An example of this capability is demonstrated by SegGPT [41], a model designed to segment"}, {"title": "8.3 Backdoor Attacks", "content": "Backdoor attacks are a well-known threat in the DL community that aims to alter the model's behavior at test time under the presence of a trigger by different poisoning techniques during training time. The first backdoor attack, BadNets [19], compromised a computer vision classification model, which misclassified inputs under the presence of a square trigger. Since then, the research community has considered its application in different domains with different types of triggers [14, 33, 34].\nRegarding the domains, backdoor attacks have been considered in FL [5], graph neural networks [43], audio [26], and NLP [11] to name a few. In the domain of large models such as LLMs, backdoor attacks are also prominent [22, 29]. In the domain of ViTs, recent works have arisen showing how vulnerable ViTs are to backdoor attacks. Many backdoor attacks on ViTs follow the standard backdoor injection procedure from backdoor attacks in CNNs [37, 47]. Other works exploit unique features of ViTs to inject the backdoor. For instance, Yuan et al. [46] developed a universal trigger that drifts the attention of the model to the patches that contain the trigger. Yang et al. [45] explored adding an extra token to the model's input. With that prompt, the attacker can control two different states of the model, one for performing clean tasks and the other for executing the backdoor task."}, {"title": "9 Conclusions & Future Work", "content": "In-context learning is an ability of LMs that allows them to perform different tasks depending on how they are prompted. Recently, works on ViTs have exploited this property to develop models that can handle different tasks, from semantic"}, {"title": "A Ethics Considerations and Compliance with the Open Science Policy", "content": "Our work considers the threat of backdoor attacks for in-context learning and ViTs. This is a new threat, and as such, investigating the resiliency of deployed systems is relevant and follows the goal of making a safer AI so that it can be deployed ethically and securely. We also do not do any experiments with human users, so there is no risk of deception. Our experiments do not use live systems or violate terms of service. Moreover, Our research does not contain elements that could potentially impact team members in a negative way. To the best of our knowledge, our research follows all laws. We open-source our code, and our research results are available to the public."}, {"title": "B Model Architecture & Training Setup", "content": "We follow the model and the training details defined in [40"}]}