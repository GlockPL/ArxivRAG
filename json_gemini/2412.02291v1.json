{"title": "Conformal Symplectic Optimization for Stable Reinforcement Learning", "authors": ["Yao Lyu", "Xiangteng Zhang", "Shengbo Eben Li", "Jingliang Duan", "Letian Tao", "Qing Xu", "Lei He", "Keqiang Li"], "abstract": "Training deep reinforcement learning (RL) agents necessitates overcoming the highly unstable nonconvex stochastic optimization inherent in the trial-and-error mechanism. To tackle this challenge, we propose a physics-inspired optimization algorithm called relativistic adaptive gradient descent (RAD), which enhances long-term training stability. By conceptualizing neural network (NN) training as the evolution of a conformal Hamiltonian system, we present a universal framework for transferring long-term stability from conformal symplectic integrators to iterative NN updating rules, where the choice of kinetic energy governs the dynamical properties of resulting optimization algorithms. By utilizing relativistic kinetic energy, RAD incorporates principles from special relativity and limits parameter updates below a finite speed, effectively mitigating abnormal gradient influences. Additionally, RAD models NN optimization as the evolution of a multi-particle system where each trainable parameter acts as an independent particle with an individual adaptive learning rate. We prove RAD's sublinear convergence under general nonconvex settings, where smaller gradient variance and larger batch sizes contribute to tighter convergence. Notably, RAD degrades to the well-known adaptive moment estimation (ADAM) algorithm when its speed coefficient is chosen as one and symplectic factor as a small positive value. Experimental results show RAD outperforming nine baseline optimizers with five RL algorithms across twelve environments, including standard benchmarks and challenging scenarios. Notably, RAD achieves up to a 155.1% performance improvement over ADAM in Atari games, showcasing its efficacy in stabilizing and accelerating RL training.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) stands out as a prominent area within the artificial intelligence community, which promises to provide solutions for decision-making and control of large-scale and complex problems [1]. In recent years, RL has shown great potential in various challenging domains, including games [2], [3], robotics [4], [5], autonomous driving [6], [7], etc. However, training an RL agent with neural networks (NNs) is extremely unstable compared to other machine learning tasks since the inherent trial-and-error mechanism brings intractable uncertainty [8], [9]. Specifically, NNs in RL are prone to overfitting [10], overestimation [11], [12], and divergence [13], making it extremely important to guarantee the training stability.\nTraining an NN involves solving a nonconvex stochastic optimization problem, and gradient-based approaches are commonly employed to address it iteratively, whose stability plays a critical role in the convergence of RL. The history of gradient-based optimizers originated with the stochastic gradient descent (SGD) algorithm [14], which estimates the gradient using a small data batch at each step. Hinton et al. [15] introduced momentum and developed the stochastic gradient descent with momentum (SGD-M) algorithm, which accelerates convergence and mitigates local minima or saddle points. Nesterov [16] further incorporates lookahead gradients to update parameters, proposing the Nesterov accelerated gradient (NAG) algorithm, which achieves faster and more stable convergence. Duchi et al. [17] presented an adaptive gradient (AdaGrad) algorithm, which dynamically adjusts the learning rate for each parameter based on past gradients, making it suitable for sparse or noisy gradients. Hinton [15] developed the root mean square propagation (RMSprop) algorithm, which adapts the learning rate for each parameter using an exponential moving average of squared gradients instead of accumulating all past gradients, thereby preventing excessive drop in the learning rate. Ultimately, Kingma and Ba [18] combined the two algorithms above to propose the adaptive moment estimation (ADAM) algorithm, which is currently one of the most widely used optimizers for NNs. However, previous algorithms often solely rely on empirical and pragmatic approaches to expedite the convergence of NNs, lacking theoretical foundations, particularly in analyzing the optimization dynamics and investigating training stability.\nRecent studies underscore the importance of drawing parallels between gradient-based optimization algorithms and continuous-time dynamical systems [19], [20]. Analyzing optimization algorithms through their corresponding dynamical systems allows us to leverage extensive knowledge about these systems. This approach facilitates the examination of crucial properties such as stability and convergence rate that emerge during the optimization process. Furthermore, dissipative dynamical systems are highly desirable for optimization due to their natural convergence towards stationary points [21]. Among these systems, conformal Hamiltonian systems stand out, as their declining Hamiltonian (i.e., total energy) and symplectic"}, {"title": "II. PRELIMINARIES", "content": "This section briefly introduces the conformal Hamiltonian system and its integrator with the capacity of symplectic preservation.\nA. Conformal Hamiltonian system\nThe state of a conformal Hamiltonian system can be determined by a point in the phase space, $(q,p) \\in R^{2n}$, where $q = q(t) \\in R^n$ are the generalized coordinates, $p = p(t) \\in R^n$ are their conjugate momenta, $t \\geq 0$ is the time, and $n \\in N^+$ is the dimension of the system. the Hamiltonian $H : R^{2n} \\rightarrow R$ characterizes the total energy of the system:\n$H(q,p) = T(p) + U(q)$,\nwhere $T(p)$ and $U(q)$ represent kinetic and potential energy, respectively. A conformal Hamiltonian system is required to obey the following canonical equations:\n$\\dot{p} = -\\nabla_qH(q, p) - rp, \\quad \\dot{q} = \\nabla_pH(q,p)$,\nwhere $r > 0$ is a damping constant and we can rewrite (1) in matrix expression as\n$\\dot{z} = SVH(z) - rDz,$\nwhere\n$z = \\begin{bmatrix} q \\\\ p \\end{bmatrix}, \\quad S = \\begin{bmatrix} 0 & I \\\\ -I & 0 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 0 & 0 \\\\ 0 & I \\end{bmatrix},$\nand $I$ is the n-dimensional identity matrix. Canonical equations (2) define a continuous flow describing the evolution of the system as $\\varphi_t: \\varphi_t(z_0) = z(t)$, where $z_0 = z(0)$. Moreover, we use $C(z) = SVH(z)$ and $D(z) = -rDz$ to denote the"}, {"title": "III. CONFORMAL HAMILTONIAN PERSPECTIVE ON NEURAL NETWORK TRAINING", "content": "This section presents a novel perspective on viewing NN training as the evolution of a conformal Hamiltonian system. We present a universal framework for designing innovative conformal symplectic integrators as iterative updating rules for NNs, where the choice of kinetic energy plays a crucial role in governing the optimization dynamics. Classical kinetic energy enables accelerated gradient algorithms without limitations on parameter updating speed. In contrast, relativistic kinetic energy allows for adaptive gradient methods and imposes constraints on parameter updating speed to mitigate the impact of excessively large gradients.\nTraining an NN is typically formulated as a general noncon-vex stochastic optimization problem:\n$\\min_{\\theta \\in R^N} J(\\theta) = E_{x\\sim P} \\{ L(x,\\theta) \\}$,\nwhere $J$ is the objective function, $L$ is a nonconvex loss function, $P$ is the distribution of data $x$, and $\\theta \\in R^N$ are the trainable parameters.\nWhile convergence requires that $\\nabla J(\\theta) = 0$ and the updating speed of $\\theta$ approaches zero [15], this condition parallels that of a conformal Hamiltonian system converging to a stationary point, where $\\nabla U(q) = 0$ and momenta $p = 0$ [21]. Based on this analogy, the trainable parameters $\\theta$ of an NN can be seen as analogous to the generalized coordinates $q$ of a Hamiltonian system, with the objective function $J(\\theta)$ corresponding to the potential energy $U(q)$, i.e.,\n$\\theta = q, \\quad J(\\theta) = U(q), \\quad H(\\theta,p) = T(p) + J(\\theta)$.\nBuilding upon the analogy between NN training and Hamil-tonian dynamics, we present a novel framework to develop variable iterative updating rules for NNs, which involves two key steps:\n1) Specify the formulation of the kinetic energy $T(p)$ to construct different Hamiltonians $H$. This allows the embedding of different dynamics;\n2) Specify the conformal symplectic integrator to discretely approximate the system flow $\\varphi_t$. Different integrators will lead to different updating rules.\nThis analogy provides a promising avenue for embedding dynamical properties or physical principles into NN optimiza-tion. While the integrator primarily relates to the precision of discretization, the kinetic energy $T(p)$ directly shapes the dynamics governing optimization trajectories. Consequently, formulating appropriate kinetic energy becomes crucial in influencing salient algorithm behaviors such as stability and convergence rate. Our primary goal is to develop a new NN optimization algorithm that enhances learning stability and performance by determining a suitable expression for $T(p)$.\nB. Accelerated gradient from classical kinetic energy\nThe classical kinetic energy $T(p) = \\frac{\\|p\\|^2}{2m}$ is a common choice for developing NN optimizers. For instance, if we substitute the classical kinetic energy into (5), we can directly derive the following expressions from (3):\n$p_{k+1} = e^{-rh}p_k - h\\nabla J (\\theta_k), \\quad \\theta_{k+1} = \\theta_k + \\frac{h}{m}p_{k+1}$.\nWe further introduce a change of variables to make them more familiar within the optimization community:\n$v_k = \\frac{h}{m}p_k, \\quad \\alpha = \\frac{1 - e^{-rh}}{\\frac{h^2}{m} (1-e^{-rh})}, \\quad \\beta_1 = e^{-rh}$,\nwhere $v \\in R^n$ are the conjugate momenta of trainable parameters, $\\alpha > 0$ is the learning rate, and $0 < \\beta_1 < 1$ is the first-order momentum coefficient. By plugging (7) into (6), we obtain:\n$v_{k+1} = \\beta_1 v_k + (1 - \\beta_1) \\nabla J (\\theta_k), \\quad \\theta_{k+1} = \\theta_k - \\alpha v_{k+1}$,"}, {"title": "IV. RELATIVISTIC ADAPTIVE GRADIENT DESCENT", "content": "This section proposes the relativistic adaptive gradient (RAD) algorithm, which models a relativistic system consisting of multiple independent one-dimensional particles. RAD enables individual adaptivity for each trainable parameter, enhancing its efficiency on complex optimization problems like reinforcement learning (RL). Furthermore, to broaden RAD's applicability beyond its origin from deterministic dynamics, we incorporate exponential moving average and bias correction techniques, making it suitable for general nonconvex stochastic opti-mization problems. Finally, we discuss the close relationship between RAD and the adaptive moment estimation (ADAM) algorithm, shedding light on studying the inherent dynamics of other adaptive gradient methods.\nA. Individual adaptivity from multi-particle systems\nIn previous sections, we associate NNs with a system governed by a single n-dimensional particle, wherein all trainable parameters are treated as separate dimensions of the same particle. This results in limited individual adaptivity. As indicated by (10), the learning rate of each parameter $\\theta_i$ in $\\theta = [\\theta_1,\\theta_2,\\dots,\\theta_i,\\dots,\\theta_n]^T$ is uniformly controlled by the denominator $\\sqrt{\\delta^2 \\|v\\|^2 + 1}$ without individual adjustments. This deficiency results in slow convergence towards local optima during nonconvex stochastic optimization like RL.\nTo address the limitations of a single-particle system, we present an alternative perspective regarding NN optimization as the evolution of a system consisting of multiple independent one-dimensional particles. Under this view, each parameter $\\theta_i$ can be treated as a separate one-dimensional particle with position $q_i$ and momentum $p_i$. The optimization process for each trainable parameter then obeys the dynamics of its associated particle. Specifically, a system of equal-mass one-dimensional relativistic particles is described by the following Hamiltonian:\n$H(\\theta,p) = \\sum_{i=1}^n c\\sqrt{p_i^2 + m^2c^2} + J(\\theta)$,\nwhere the kinetic energy is composed of the individual kinetic energy of each particle $i$. Substitute this into (3), we can obtain the following updating rules for all $i \\in \\{1,2,\\dots,n\\}$:\n$p_{k+1,i} = e^{-rh}p_{k,i} - h [\\nabla J (\\theta_k)]_i, \\quad \\theta_{k+1,i} = \\theta_{k,i} + \\frac{hc}{\\sqrt{p_{k+1,i}^2 + m^2c^2}} p_{k+1,i}$,\nwhere the subscript $i$ represents the $i$-th element of the corresponding vector.\nNotably, the updating speed of each parameter $\\theta_i$ is adapted based on the individual momentum $p_i$. This feature endows NN training with per-parameter adaptivity, facilitating convergence under nonconvex stochastic optimization. By introducing the change of variables (7) and (9), we immediately receive the original updating rules of the first-order RAD:\n$v_{k+1,i} = \\beta_1 v_{k,i} + (1 - \\beta_1) [\\nabla J (\\theta_k)]_i, \\quad \\theta_{k+1,i} = \\theta_{k,i} - \\frac{\\alpha}{\\sqrt{\\delta^2 v_{k+1,i}^2 + 1}} v_{k+1,i}$.\nSince we model a multi-particle relativistic system, the learning rate of each parameter can be individually adjusted in terms of its learned second-order momentum $v^2$. Thus, each parameter can adaptively be updated at a different rate, which helps RAD converge fast under stochastic nonconvex settings.\nMoreover, the speed coefficient $\\delta$ is one critical hyperparam-eter that controls the strength of gradient normalization and the level of adaptivity, influencing the stability and convergence rate of RAD. According to (12), $|\\theta_{k+1,i} - \\theta_{k,i}| < \\alpha/\\delta$ is always true. Thus, the parameter updates are restricted as we primarily expected. The upper bound is inversely proportional to $\\delta$ or is proportional to $c$. The updating speed becomes"}, {"title": "V. CONVERGENCE ANALYSIS OF RAD", "content": "In this section, we will explore the convergence charac-teristics of RAD, focusing on how it performs in general nonconvex stochastic optimization as outlined in (4). We first state some assumptions as the fundamental basis for conducting convergence analysis.\nAssumption 1. The loss function $L$ is $L$-smooth, i.e., $\\exists L > 0, \\forall \\theta_1, \\theta_2$ and $x$, we have\n$\\|\\nabla L (x, \\theta_1) - \\nabla L (x, \\theta_2)\\| \\leq L \\|\\theta_1 - \\theta_2\\|$.\nAssumption 2. The loss function $L$ has gradient bounded on each coordinate, i.e., $\\exists M > 0, \\forall x,\\theta$ and $i$, we have\n$|[\\nabla L (x, \\theta)]_i| \\leq M$.\nAssumption 3. The variance of each coordinate of the stochastic gradient is bounded, i.e., $\\exists \\sigma_i > 0, \\forall \\theta$ and $i$, we have\n$E_{x\\sim P} \\{ ([\\nabla L(x, \\theta)]_i - [\\nabla J(\\theta)]_i)^2 \\} \\leq \\sigma_i^2$.\nGiven these assumptions, we can readily derive the following corollaries.\nCorollary 1. The objective function $J$ is $L$-smooth, such that\n$\\|\\nabla J (\\theta_1) - \\nabla J (\\theta_2)\\| \\leq L \\|\\theta_1 - \\theta_2\\|, \\quad \\exists L > 0,\\forall \\theta_1, \\theta_2$.\nCorollary 2. Each coordinate of the gradient of the objective function $J$ is bounded, such that\n$|[\\nabla J(\\theta)]_i| \\leq M, \\exists M > 0, \\forall\\theta, i$.\nCorollary 3. The variance of the stochastic gradient is bounded, such that\n$E_{x\\sim P} \\{ \\|\\nabla L(x, \\theta) - \\nabla J(\\theta)\\|^2 \\} < \\sum_{i=1}^n \\sigma_i^2 = \\sigma^2,\\forall\\theta$."}, {"title": "VI. NUMERICAL EXPERIMENTS", "content": "Building on our theoretical work, we conduct empirical tests on twelve benchmarks using the OpenAI Gym interface [35]. This includes one classical control task (CartPole-v1), six multi-body continuous-control tasks from MuJoCo (Ant-v3, HalfCheetah-v3, Walker2d-v3, Swimmer-v3, Hopper-v3, and Humanoid-v3) [28], four image-input discrete-control tasks from Atari games (Breakout-v4, Enduro-v4, Seaquest-v4, and SpaceInvaders-v4) [29], and one autonomous driving task using the IDSim simulator [36]. These experiments aim to validate three main hypotheses: 1) RAD stabilizes the training process of RL, thereby facilitating the attainment of high policy performance; 2) the speed coefficient $\\delta$ is critical to the training stability since it controls the updating speed of parameters; and 3) the symplectic factor $\\zeta$ plays an essential role in the adaptivity of effective learning rates.\nA. Experimental settings\nWe implement five widely used RL algorithms as testing grounds to evaluate the efficacy of RAD. These algorithms are: deep Q-network (DQN) [8], deep deterministic policy gradient (DDPG) [10], twin delayed DDPG (TD3) [4], soft actor-critic (SAC) [12], and approximate dynamic programming (ADP) [37]. Each of these is applied to different tasks. The comparison involves six mainstream baseline optimizers: ADAM, HB, DLPF, RGD, SGD, and NAG."}, {"title": "VII. CONCLUSION", "content": "This paper systematically investigates the optimization process of training an RL agent from a dynamic systems perspective. To improve the stability of RL training, we model NN optimization as the evolution of conformal Hamiltonian systems. By discretizing the system in a conformally symplectic manner, we develop iterative updating rules that incorporate symplecticity and long-term stability into the optimization process. Our primary contribution is the RAD algorithm, which simulates a relativistic system governed by multiple one-dimensional particles. RAD adapts and limits the updating for each trainable parameter individually, promoting stable and rapid convergence. Our findings demonstrate that RAD generalizes the widely used ADAM algorithm, providing insights into the inherent dynamics of other mainstream adaptive gradient algorithms. We establish RAD's convergence properties under general nonconvex stochastic optimization settings to ensure broader applicability. Experimental results confirm that RAD achieves state-of-the-art performance under default settings, highlighting its potential to stabilize RL training. Future work may explore the shadow dynamics of current mainstream methods to deepen understanding and enhance RAD's performance by integrating techniques from cutting-edge optimizers."}]}