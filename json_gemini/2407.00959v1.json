{"title": "Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving", "authors": ["Ran Tian", "Boyi Li", "Xinshuo Weng", "Yuxiao Chen", "Edward Schmerling", "Yue Wang", "Boris Ivanovic", "Marco Pavone"], "abstract": "The autonomous driving industry is increasingly adopting end-to-end learning from sensory inputs to reduce human biases in system design. Traditional end-to-end driving models, however, suffer from long-tail events due to rare or unseen inputs within their training distributions. To address this, we propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM's reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios. TOKEN effectively alleviates data scarcity and inefficient tokenization by leveraging a traditional end-to-end driving model to produce condensed and semantically enriched representations of the scene, which are optimized for LLM planning compatibility through deliberate representation and reasoning alignment training stages. Our results demonstrate that TOKEN excels in grounding, reasoning, and planning capabilities, outperforming existing frameworks with a 27% reduction in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios. Additionally, our work highlights the importance of representation alignment and structured reasoning in sparking the common-sense reasoning capabilities of MM-LLMs for effective planning.", "sections": [{"title": "1 Introduction", "content": "The autonomous driving industry is increasingly pursuing end-to-end learning from sensory inputs\nto reduce human inductive bias in system design [1, 2]. Despite the remarkable progress, end-to-end\nmodels inherently suffer from severe performance degradation in long-tail scenarios. For example,\nstate-of-the-art end-to-end autonomous driving planners often fail to navigate temporary construc-\ntion sites and react too aggressively to jaywalkers; even simple rule-based planners can significantly\noutperform high-capacity end-to-end models in these long-tail scenarios [3]. This motivates recent\nefforts to fine-tune Large Language Models (LLMs) into autonomous vehicle planners [4, 5, 6], aim-\ning to leverage the benefits of both high-capacity models and the common-sense reasoning abilities\nthat emerge from world-knowledge training.\nLLM-based planners, in their simplest form, depend on textual scene descriptions as prompts, mak-\ning their performance highly reliant on the quality and detail of these descriptions. Detailed prompts\nrequire extensive engineering and generate many tokens for the LLM to process. Conversely, our\nevaluations show that simple, heuristic prompts do not tap into the common-sense reasoning abil-\nities of LLMs due to insufficient scene understanding. As a result, Multi-Modal Large Language\nModels (MM-LLMs), which naturally integrate various data modalities beyond text, are emerging\nas promising foundations for developing autonomy stacks in autonomous vehicles.\nThe predominant approach is to leverage pre-trained encoders (typically pre-trained using visual-\ntext alignment) to extract features from the sensory inputs, followed by a querying transformer that\nuses latent queries to tokenize the features into dense latent tokens and feed them to the LLMs\n[7, 8, 9, 10, 11]. Training an effective scene tokenizer (encoder and querying transformer) often\nrequires billions of question-answer pairs (QAs), even for tasks that are much less complicated\nthan autonomous driving [12]. However, current MM-LLM datasets for autonomous driving typi-\ncally contain fewer than one million QAs [7, 13]. Consequently, these models often exhibit poor\nperformance in reasoning and planning tasks due to a lack of scene understanding and grounding\ncapability. The key challenge is to enable the scene tokenizer to extract informative and structured\ninformation that can unlock the common-sense reasoning ability of the LLM in a low-data regime.\nWe propose TOKEN (Fig. 1), a novel MM-LLM framework that utilizes object-centric tokenization\nto tokenize the world into a few object-level tokens to enhance the planning ability of autonomous\nvehicles, especially in long-tail scenarios. Our key insight is that object-level latent tokens, with\neach token representing a relevant object in the scene, are much more informative and easier\nfor the LLM to interpret compared to unstructured dense tokens. TOKEN not only produces a\ncondensed and semantically informed representation of the scene but also enables us to use a state-\nof-the-art end-to-end driving model as the pre-trained scene tokenizer, effectively alleviating both\nthe data scarcity and inefficient tokenization challenges present in current MM-LLM frameworks.\nIn Sec. 5.1, we compare TOKEN to alternative MM-LLM frameworks and demonstrate its supe-\nrior grounding, reasoning, and planning capabilities in a low-data regime. In Sec. 5.2, we compare\nTOKEN to the state-of-the-art end-to-end (SOTA) autonomous driving planner [2] and showcase its\nstrong performance in long-tail scenarios, including navigating around construction sites, executing\n3-point turns, resuming motion after a full stop, and overtaking parked cars through the oncoming\nlane. In Sec. 5.3, we compare TOKEN to the SOTA LLM-based planner [5] and demonstrate its su-\nperiority in long-tail scenarios. We further conduct an ablation study to highlight the importance of\nproper representation alignment and structured reasoning process alignment in effectively evoking\nthe common-sense reasoning ability of the LLM backbone for planning. To the best of our knowl-\nedge, we are the first to conduct an in-depth analysis to demonstrate the promising potential and\nnecessity of such alignment in effectively leveraging MM-LLM to mitigate long-tail challenges in\nautonomous driving."}, {"title": "2 Related Work", "content": "End-to-End Driving & Long-tail Event Mitigation. Traditional end-to-end autonomous driving\nmodels inherently suffer from performance degradation in long-tail scenarios [1, 2]. To mitigate\nthis issue, previous works focused on detecting such situations online [14, 15, 16, 17] and switching"}, {"title": "3 TOKEN Framework", "content": "We propose a novel MM-LLM framework, TOKEN, tailored for autonomous driving. It consists\nof three modules: a scene tokenizer that tokenizes the sensory inputs into object-level tokens, an\nadapter that aligns the object token's embedding space with the text embedding space, and an LLM."}, {"title": "4 Experimental Setup", "content": "4.1 Design Choice of the Scene-Tokenizer\nIn this work, we leverage the transformer-based end-to-end driving model, PARA-Drive [2], as our\nscene tokenizer to extract object-level tokens. PARA-Drive is a parallelized modular autonomous\nvehicle stack that encompasses a diverse set of modules for the co-training of bird's-eye view (BEV)\nfeatures from multi-view video input. Each module is a querying transformer that uses latent queries\nto attend to the BEV features and decode the corresponding task output. We pre-train PARA-Drive\non object-centric and scene-centric tasks, including mapping, object tracking, occupancy prediction,\nand motion prediction, as shown in Fig. 3. In object-centric tasks, each query produces a latent token\nthat encodes information about a specific scene object.\nSpecifically, the track query \\(Q_{track}\\) is trained to produce a token\\(\\mathbf{z}_{track} \\in \\mathbb{R}^{1\\times256}\\) that encodes object i's 3D bounding box and seman-\ntic category, the motion query \\(Q_{motion}\\) is trained to produce a token\\(\\mathbf{z}_{motion} \\in \\mathbb{R}^{1\\times256}\\) that encodes object i's potential dynamic behavior,\nand the map query \\(Q_{map}\\) is trained to produce a token \\(\\mathbf{z}_{map} \\in \\mathbb{R}^{1\\times256}\\)\nthat encodes the map element j's geometry and semantics (e.g., cross-\ning area) information. We concatenate the track token and motion to-\nken to constitute non-map object tokens: \\(\\mathbf{z}_{agent} = \\mathbf{z}_{track}\\mathbin{\\Vert} \\mathbf{z}_{motion}\\), and\ndirectly use the map token \\(\\mathbf{z}_{map}\\) as the map element token. Although\nwe don't use latent tokens from the occupancy prediction module, we\nstill include this module during training to fairly compare with PARA-\nDrive. We follow the same training procedure in [2] to train the scene\ntokenizer. All non-map element tokens share one MLP adapter and\nall map element tokens share another one."}, {"title": "4.2 Dataset Construction", "content": "To train and evaluate TOKEN, we construct a dataset based on the NuScenes dataset [28], including\nvisual question-answering (QA) pairs that span the full stack of autonomous driving development.\nWe illustrate a few examples of these QAs in Fig. 1; more can be found in App. A.\nPerception. We build perception QAs based on the DriveLM dataset [7], covering object semantics\nand dynamic behavior identification. Additionally, we create object-lane association QAs to enhance\nthe model's understanding of map elements and objects' topological relationships to the ego vehicle.\nBehavior Reasoning. Behavior reasoning QAs include two types of questions: 1) object level\nbehavior analysis questions where we ask the model to reason about whether an object is critical\n(i.e., is likely to influence the ego vehicle's planning) and provide the corresponding reason; 2)\nscene-level critical object grounding questions where we ask the model to predict the locations of\ncritical objects in the ego vehicle's local frame. Behavior reasoning QAs aim to enable the model to\nunderstand context-dependent critical objects, thereby connecting the object tokens with the LLM\nto simplify the scene for downstream planning.\nRoute-conditioned Hierarchical Planning. Different from previous works that use the relative\nposition of the ground-truth ego trajectory to define high-level commands (\"keep forward\" and \"turn\nright/right\"), we re-labeled the NuScenes dataset to use road-level navigation signals as high-level\ncommands, including: \"keep forward along the current road,\" \"prepare to turn right/left at the next\nintersection,\" \"turn right/left at the intersection,\" \"left/right U-turn,\" and \"left/right 3-point turn.\" We\nutilize chain-of-thought reasoning to align the model's planning process and guide it to progressively\ngenerate the driving plans in three steps. First, the model identifies the critical objects in the current\ndriving scene, including their categories and 2D locations in the ego frame. Next, it proposes the\ndesired behavior mode, detailing interaction plans with the critical objects (e.g., overtake) and lane-\nlevel decisions (e.g., left lane change). Finally, it generates a 3-second motion plan (6 waypoints)."}, {"title": "4.3 Training Strategy", "content": "We use LLaMA-2-7B [29] as our LLM backbone. We keep the scene tokenizer frozen and use a\nLow-Rank Adaptation (LoRA) module [30] to fine-tune the LLM. We train TOKEN in three stages:\npre-training, reasoning fine-tuning, and planning fine-tuning. During pre-training (\u201crepresentation\nalignment", "reasoning alignment\"), we train the\nadapter and LoRA together using the reasoning and planning QAs for 10 epochs with a learning rate\nof 1e-4. Finally, we train the adapter and LoRA together using just the planning QAs for another 10\nepochs to maximize the model's performance on planning, maintaining the learning rate at 1e-4.\"\n    },\n    {\n      \"title\"": "5 Experimental Results"}, {"content": "5.1 On the Value of Object-Centric Tokenization\nExperiment Design. We compare TOKEN against existing MM-LLMs to demonstrate its effec-\ntiveness and efficiency in scene understanding, grounding, and planning. Our experimental setup\nfocuses on the impact of 1) different tokenization schemes and 2) pre-training on driving data.\nBaseline. We compare TOKEN against (1) VILA-1.5 [31], which uses a trainable ViT to tokenize\nframes, (2) Video-LLaMA [22], which leverages a pre-trained and frozen ViT to extract visual\nfeatures and a querying transformer to produce visual tokens from the extracted features, and (3)\nBEV-TOKEN, a variant of TOKEN that directly uses dense bird's-eye view (BEV) features from\nthe same pre-trained PARA-Drive instead of sparse object-level tokens, similar to most MM-LLMs\n(e.g., Video-LLaMA). We follow the same training recipe for all models and use the same LLaMA-\n2-7B as the LLM backbone. For Video-LLaMA and VILA-1.5, we use the past four front view\nframes to render a 2-second video as input for each QA."}, {"title": "5.2 Generalization in Long-tail Driving Scenarios", "content": "We evaluate the planning performance of TOKEN against PARA-Drive in long-tail events. TOKEN\nuses the PARA-Drive as the scene-tokenizer, thus they share the same visual encoder and scene\ninformation, and only differ in the planner structure.\nLong-tail Events Construction. We manually inspected the NuScenes dataset and identified the\nfollowing long-tail scenarios for evaluation, each representing less than 1% of the training data: 1)\nexecuting 3-point turns; 2) resuming motion after a full stop; 3) overtaking parked cars through the\noncoming lane; and 4) navigating around construction sites. More details can be found in App. D.\nQuantitative Result. In Tab. 2, we summarize the quantitative evaluation for each long-tail scenario.\nTOKEN significantly outperforms PARA-Drive in terms of the quality and safety of predicted plans.\nSpecifically, TOKEN predicts more accurate turning maneuvers in 3-point turns (60% reduction in\nheading L2 Aveall), more effective motions after yielding (28% reduction in longitudinal weighted\nL2 Aveall), and safer plans when navigating around blocking vehicles and construction sites (100%\nand 67% collision rate reductions in the overtake and construction zone scenarios, respectively)."}, {"title": "5.3 On the Value of Alignment", "content": "We compare TOKEN with the SOTA LLM-based planner Agent-Driver [5]. Agent-Driver queries\ntext-based scene information using various tools and then fine-tunes GPT-3.5 into a motion planner.\nOur evaluation (Tab. 3) shows that TOKEN and Agent-Driver have similar overall performance, but\nTOKEN significantly outperforms agent-driver in long-tail scenarios with a much smaller model and\nless privileged information (details in App. F). We hypothesize that evoking common-sense reason-\ning in a LLM-powered planner requires proper alignment rather than just a larger LLM backbone.\nMotivated by this finding, we further conduct a detailed ablation study to shed more light on this."}, {"title": "6 Limitation & Future Work", "content": "A strength and limitation of TOKEN is using a pre-trained and frozen PARA-Drive model as the\nscene tokenizer. This allows for extracting informative tokens and controlled experiments. However,\nTOKEN's performance is tightly coupled with the quality of the pretrained tokenizer. We show a\nfailure case in App. I in which the critical object is not detected by the tracking querying transformer\nin PARA-Drive. Consequently, TOKEN is unable to understand the scene and generates incorrect\nbehavior. Further work will focus on co-training PARA-Drive to leverage the knowledge within the\nLLM to improve the scene tokenizer. Another limitation is outputting the trajectory plan as text,\nwith each digit as a separate token. This increases computational expense and complicates motion\ngeneration, leading to misalignment between predicted behavior and the motion plan (e.g., TOKEN\npredicts the correct behavior but incorrect motion in Fig. 6). Future work will focus on quantizing\nmotions into discrete LLM-understandable tokens [32] or using a dedicated trajectory decoder [33]."}, {"title": "Supplementary Materials", "content": "A Dataset Construction\nA.1 Examples of the QAs\nIn Fig. 8, we illustrate examples of QAs used in training TOKEN.\nA.2 Road-Level Navigation Signal\nPrevious works on VLM/LLM for autonomous vehicle planning often prompt the model with a\nhigh-level command based on the relative position of the ground-truth ego trajectory, including\n\"keep forward\" and \"turn right/left.\" However, these high-level commands are not only unrealistic\nbut also simplify the planning problem by removing the need for behavior planning. Therefore,\nwe re-labeled the NuScenes dataset to use road-level navigation signals as high-level commands,\nincluding \"keep forward along the current road,\" \"prepare to turn right/left at the next intersection,\"\n\"turn right/left at the intersection,\" \"left/right U-turn,\" and \"left/right 3-point turn.\"\nA.3 Interaction Mode Labeling\nWe use a combination of heuristics and manual labeling to annotate the interactions between the\nego vehicle and the other traffic agents. We first use two types of categorical modes to describe the\nlane-relationship between a traffic agent and the ego vehicle (agent-ego lane mode) and the relative\nmotion between a traffic participant and the ego vehicle (homotopy) [33]. Agent-ego lane mode at a\ntime step t encodes the topology relationship between the ego's current lane and the traffic agent's\nlane, including: LEFT, RIGHT, AHEAD, BEHIND, and NOTON, where NOTON describes that the\ntraffic agent is not on any derivable lanes in the scene (e.g., a parked vehicle in a parking lot). To\ncompute the agent-ego lane mode for each traffic agent, we follow [33] to first identify the lane on\nwhich each agent is located and then leverage the lane topology map to annotate the agent-ego lane\nmode. We project the agent's center to the lane polyline and use its relative position in the local\nFrenet frame to determine its lane association. Homotopies describe the relative motion between a\npair of agents shown in the video, including: [S, CW, CCW] (static, clockwise, counterclockwise).\nCombining agent-ego lane mode, homotopy, agent ground truth state information, and scene con-\ntext information (e.g., ego is located near an intersection) together, we can leverage heuristics to\nannotate the interaction. For example, within a 3-second horizon, a static object's agent-ego lane\nmode changes from AHEAD, to LEFT, to BEHIND, while the ego vehicle performs RIGHT-LANE-"}, {"title": "B Evaluation Protocol", "content": "In this section, we provide a detailed description about our evaluation protocol. In Section 5.1 of the\nmain text, we introduce the three variants of trajectory L2 error (the overall, turning, and progress\nerrors) and the collision rate used to evaluate the predicted motion plans. As noted in [2], different\nevaluation protocols used to compute these metrics can lead to significant metric variations. We use\nthe same evaluation protocol as described in [2] with one exception: we exclude samples where any\nfuture motion is missing near the end of a sequence (the frame masking strategy described in [2]).\nIncluding these partially invalid samples would significantly lower the L2 errors, as the L2 errors of\nthese invalid frames are set to zero."}, {"title": "C Additional Result: On the Value of Object-Centric Tokenization", "content": "In Tab. 5, we present the full quantitative evaluation of each model's performance in the planning\ntask. We observe that TOKEN significantly outperforms all baselines across all planning metrics."}, {"title": "D Long-tail Events Construction", "content": "We manually inspected the NuScenes dataset and identified the following long-tail scenarios for\nevaluation, each representing less than 1% of the training data: 1) executing 3-point turns; 2) resum-\ning motion after a full stop; 3) overtaking parked cars through the oncoming lane; and 4) navigating\naround construction sites.\nExecuting 3-point turns, which has one scene (scene-0778, frame 6-30) in the evaluation split and\nO scenes in the training distribution. We extracted 25 key frames from the scene in which the ego\nvehicle is performing the 3-point U-turn for evaluation to remove the noise from other nominal\nbehaviors in the scene.\nResuming motion after a full-stop, which includes 14 scenes in the training split and 8 scenes in\nthe evaluation split. We extract key frames from each scene where the ground truth (GT) motion plan\ncaptures the acceleration behavior after the full stop. This results in 70 key frames in the training\nsplit (0.28% of the total training samples) and 40 key frames in the evaluation split. The scenes\nand key frames in the training split are: scene-0208 (frame 25-29), scene-1023 (frame 21-25),\nscene-0067 (frame 24-28), scene-0159 (frame 4-8), scene-0185 (frame 26-30), scene-0262 (frame\n8-12), scene-0862 (frame 18-22), scene-0025 (frame 6-10) scene-0072 (frame 24-28), scene-0157\n(frame 12-16), scene-0234 (frame 4-8), scene-0423 (frame 6-10), scene-0192 (frame 14-18), and\nscene-0657 (frame 12-16). The scenes and key frames in the evaluation split are: scene-0921 (frame\n21-25), scene-0925 (frame 19-23), scene-0968 (frame 7-11), scene-0552 (13-17), scene-0917 (frame\n24-28), scene-0221 (frame 11-15), scene-1064 (frame 21-25), and scene-0331 (frame 8-12).\nOvertaking parked cars through the oncoming lane, which includes 14 scenes in the training split\nand 5 scenes in the evaluation split. We extracted key frames from each scene where the ground truth"}, {"title": "E Detailed Qualitative Result", "content": "In this section, we provide an in-depth analysis of the qualitative results shown in Sec. 5.2.\nExecuting a 3-point turn. During a 3-point turn, a vehicle makes a sharp left turn, backs up, and\nthen makes another left turn to complete the maneuver. In Fig. 4, we compare the motion plans\nfrom TOKEN and PARA-Drive. Despite receiving a \"3-point turn\" command, PARA-Drive predicts\nstraight movements at t 2s and t = 4s, likely due to the absence of such examples in its training\nset. In contrast, TOKEN understands the command and generates the correct turning behavior.\nWhen approaching the curb to stop and back up, both PARA-Drive and TOKEN predict forward\nmotions at t = 8s, likely due to the lack of 3-point turn examples in the dataset. At t = 10s, when\nthe vehicle has enough clearance, both models predict left-turn motions, with TOKEN's prediction\nmore closely aligning with the ground truth.\nOvertaking parked cars through the oncoming lane. Fig. 6 shows an example of qualitative\ncomparison between the motion plan from TOKEN and PARA-Drive at two constitutive time steps.\nAt t = 5s, PARA-Drive predicts a motion that collides with the blocking vehicle, while TOKEN\ninstructs the ego vehicle to decelerate to avoid the object. Interestingly, although TOKEN correctly\npredicts in language that the ego vehicle should decelerate and steer to the right, the motion plan only\nreflects the deceleration behavior. We hypothesize that this is caused by insufficient data that helps\nthe LLM associate low-level motion with mid-level behavior. When the ego vehicle straddles the\nlane divider and prepares to overtake, TOKEN instructs the ego vehicle to drive back to its original\nlane after overtaking, while PARA-Drive predicts a forward motion that straddles the lane divider.\nNavigating around construction sites. Fig. 7 shows an example of qualitative comparison between\nthe motion plan from TOKEN and PARA-Drive at two constitutive time steps. At t = 8s, TOKEN\ninstructs the ego vehicle to steer and bypass the traffic cones from the ego vehicle's right side, while\nPARA-Drive predicts a motion that collides with the blocking traffic cones At t = 12s, TOKEN\ninstructs the ego vehicle to steer to the right to keep forward along the current lane, while PARA-\nDrive predicts a motion that deviates from the lane center."}, {"title": "F Comparison with the SOTA LLM-based Planner - Agent-Driver", "content": "We compare TOKEN with the SOTA LLM-based planner Agent-Driver [5]. They have the following\ndifferences:\nScene representation. Agent-Driver queries text-based scene information using various tools (e.g.,\nobject detection, mapping, etc.) and uses the queried text-based information as an input prompt to\ninstruct the LLM to plan the ego vehicle's motion. TOKEN tokenizes the scene into a few object-\nlevel tokens. This makes TOKEN more efficient in terms of information density per-token (e.g.,"}, {"title": "GOn the Value of Alignment - Qualitative Results", "content": "In Fig. 9, we show a few qualitative comparisons between TOKEN and a variant of TOKEN trained\nwith representation alignment but without reasoning alignment. We can see that the predicted motion\nplans are more aligned the GT motion with reasoning process alignment."}, {"title": "H Additional Results", "content": "H.1 TOKEN with HD-map Information\nIn the main text, we use multi-view video as the sensory input. In this section, we include HD-map as\nan additional input to evaluate the performance of TOKEN. We utilize the CTT encoder described\nin [33] to fuse each traffic agent's past state history with each lane's ground truth center line and\nproduce a traffic agent token as an additional token for each traffic object. We use TOKEN +map\nto denote the variant of TOKEN with HD-map information and show its quantitative evaluation\nin Tab. 7. We can see that the additional map information and the past state history significantly\nimprove the planning performance in both evaluation split and long-tail scenarios.\nH.2 Ablation on the Effect of Structured Reasoning Process Alignment.\nOne of the unique features of TOKEN is that it reasons about semantically meaningful interactions\nwith identified critical objects (e.g., bypassing the blocking traffic cones). We hypothesize that this\nstructured reasoning process supervision enhances the model's planning performance by encour-\naging the model to understand the interactions between the ego vehicle and other traffic objects,\naligning more closely with how an expert reasons in the real world. To ablate the effect of structured\nreasoning process alignment, we removed it from the planning QAs' answer labels and used a sim-\nilar chain-of-thought reasoning and task planning method as in Agent-Driver (i.e., instructing the"}, {"title": "H.3 Few-Shot Learning.", "content": "To stress test TOKEN's few shot learning ability, we further remove 50% long-tail scenes from the\ntraining split and re-train PARA-drive and TOKEN and compare their performance. In Tab. 9, we\nshow the quantitative evaluation result in long-tail scenarios. We see that TOKEN only degrades\nslightly as opposed to PARA-Drive's significant performance degradation. For example, Traj L2\nAveall of PARA-Drive is degraded by 24% while TOKEN only experiences 9% degradation with\n50% long-tail scenes removed. The results indicate the superior few-shot learning ability of TOKEN."}, {"title": "I Failure Mode", "content": "One limitation of TOKEN is using a pre-trained and frozen PARA-Drive model as the scene tok-\nenizer, which makes the TOKEN's performance tightly coupled with the quality of the pretrained\ntokenizer. In Fig. 10, we illustrate a failure mode where the critical object (the motorcycle) is not\ndetected by the tracking querying transformer in PARA-Drive. Consequently, TOKEN assumes\nthe road is clear to proceed and fails to generate a motion that yields to the oncoming motorcycle.\nFurther work will focus on co-training PARA-Drive to leverage the knowledge within the LLM to\nimprove the scene tokenizer."}]}