{"title": "Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving", "authors": ["Ran Tian", "Boyi Li", "Xinshuo Weng", "Yuxiao Chen", "Edward Schmerling", "Yue Wang", "Boris Ivanovic", "Marco Pavone"], "abstract": "The autonomous driving industry is increasingly adopting end-to-end learning from sensory inputs to minimize human biases in system design. Traditional end-to-end driving models, however, suffer from long-tail events due to rare or unseen inputs within their training distributions. To address this, we propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM's reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios. TOKEN effectively alleviates data scarcity and inefficient tokenization by leveraging a traditional end-to-end driving model to produce condensed and semantically enriched representations of the scene, which are optimized for LLM planning compatibility through deliberate representation and reasoning alignment training stages. Our results demonstrate that TOKEN excels in grounding, reasoning, and planning capabilities, outperforming existing frameworks with a 27% reduction in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios. Additionally, our work highlights the importance of representation alignment and structured reasoning in sparking the common-sense reasoning capabilities of MM-LLMs for effective planning.", "sections": [{"title": "1 Introduction", "content": "The autonomous driving industry is increasingly pursuing end-to-end learning from sensory inputs\nto reduce human inductive bias in system design [1, 2]. Despite the remarkable progress, end-to-end\nmodels inherently suffer from severe performance degradation in long-tail scenarios. For example,\nstate-of-the-art end-to-end autonomous driving planners often fail to navigate temporary construc-\ntion sites and react too aggressively to jaywalkers; even simple rule-based planners can significantly\noutperform high-capacity end-to-end models in these long-tail scenarios [3]. This motivates recent\nefforts to fine-tune Large Language Models (LLMs) into autonomous vehicle planners [4, 5, 6], aim-\ning to leverage the benefits of both high-capacity models and the common-sense reasoning abilities\nthat emerge from world-knowledge training.\nLLM-based planners, in their simplest form, depend on textual scene descriptions as prompts, mak-\ning their performance highly reliant on the quality and detail of these descriptions. Detailed prompts\nrequire extensive engineering and generate many tokens for the LLM to process. Conversely, our\nevaluations show that simple, heuristic prompts do not tap into the common-sense reasoning abil-\nities of LLMs due to insufficient scene understanding. As a result, Multi-Modal Large Language\nModels (MM-LLMs), which naturally integrate various data modalities beyond text, are emerging\nas promising foundations for developing autonomy stacks in autonomous vehicles.\nThe predominant approach is to leverage pre-trained encoders (typically pre-trained using visual-\ntext alignment) to extract features from the sensory inputs, followed by a querying transformer that\nuses latent queries to tokenize the features into dense latent tokens and feed them to the LLMs\n[7, 8, 9, 10, 11]. Training an effective scene tokenizer (encoder and querying transformer) often\nrequires billions of question-answer pairs (QAs), even for tasks that are much less complicated\nthan autonomous driving [12]. However, current MM-LLM datasets for autonomous driving typi-\ncally contain fewer than one million QAs [7, 13]. Consequently, these models often exhibit poor\nperformance in reasoning and planning tasks due to a lack of scene understanding and grounding\ncapability. The key challenge is to enable the scene tokenizer to extract informative and structured\ninformation that can unlock the common-sense reasoning ability of the LLM in a low-data regime.\nWe propose TOKEN (Fig. 1), a novel MM-LLM framework that utilizes object-centric tokenization\nto tokenize the world into a few object-level tokens to enhance the planning ability of autonomous\nvehicles, especially in long-tail scenarios. Our key insight is that object-level latent tokens, with\neach token representing a relevant object in the scene, are much more informative and easier\nfor the LLM to interpret compared to unstructured dense tokens. TOKEN not only produces a\ncondensed and semantically informed representation of the scene but also enables us to use a state-\nof-the-art end-to-end driving model as the pre-trained scene tokenizer, effectively alleviating both\nthe data scarcity and inefficient tokenization challenges present in current MM-LLM frameworks.\nIn Sec. 5.1, we compare TOKEN to alternative MM-LLM frameworks and demonstrate its supe-\nrior grounding, reasoning, and planning capabilities in a low-data regime. In Sec. 5.2, we compare\nTOKEN to the state-of-the-art end-to-end (SOTA) autonomous driving planner [2] and showcase its\nstrong performance in long-tail scenarios, including navigating around construction sites, executing\n3-point turns, resuming motion after a full stop, and overtaking parked cars through the oncoming\nlane. In Sec. 5.3, we compare TOKEN to the SOTA LLM-based planner [5] and demonstrate its su-\nperiority in long-tail scenarios. We further conduct an ablation study to highlight the importance of\nproper representation alignment and structured reasoning process alignment in effectively evoking\nthe common-sense reasoning ability of the LLM backbone for planning. To the best of our knowl-\nedge, we are the first to conduct an in-depth analysis to demonstrate the promising potential and\nnecessity of such alignment in effectively leveraging MM-LLM to mitigate long-tail challenges in\nautonomous driving."}, {"title": "2 Related Work", "content": "End-to-End Driving & Long-tail Event Mitigation. Traditional end-to-end autonomous driving\nmodels inherently suffer from performance degradation in long-tail scenarios [1, 2]. To mitigate\nthis issue, previous works focused on detecting such situations online [14, 15, 16, 17] and switching"}, {"title": "3 TOKEN Framework", "content": "We propose a novel MM-LLM framework, TOKEN, tailored for autonomous driving. It consists\nof three modules: a scene tokenizer that tokenizes the sensory inputs into object-level tokens, an\nadapter that aligns the object token's embedding space with the text embedding space, and an LLM."}, {"title": "4 Experimental Setup", "content": "4.1 Design Choice of the Scene-Tokenizer\nIn this work, we leverage the transformer-based end-to-end driving model, PARA-Drive [2], as our\nscene tokenizer to extract object-level tokens. PARA-Drive is a parallelized modular autonomous\nvehicle stack that encompasses a diverse set of modules for the co-training of bird's-eye view (BEV)\nfeatures from multi-view video input. Each module is a querying transformer that uses latent queries\nto attend to the BEV features and decode the corresponding task output. We pre-train PARA-Drive\non object-centric and scene-centric tasks, including mapping, object tracking, occupancy prediction,\nand motion prediction, as shown in Fig. 3. In object-centric tasks, each query produces a latent token\nthat encodes information about a specific scene object.\nSpecifically, the track query $\\mathcal{Q}_{track}$ is trained to produce a token\n$z_{track} \\in \\mathbb{R}^{1\\times 256}$ that encodes object i's 3D bounding box and seman-\ntic category, the motion query $\\mathcal{Q}_{motion}$ is trained to produce a token\n$z_{motion} \\in \\mathbb{R}^{1\\times 256}$ that encodes object i's potential dynamic behavior,\nand the map query $\\mathcal{Q}_{map}$ is trained to produce a token $z_{map} \\in \\mathbb{R}^{1\\times 256}$\nthat encodes the map element j's geometry and semantics (e.g., cross-\ning area) information. We concatenate the track token and motion to-\nken to constitute non-map object tokens: $z_{agent} = \\left[z_{track}^i, z_{motion}^i \\right]$, and\ndirectly use the map token $z_{map}^j$ as the map element token. Although\nwe don't use latent tokens from the occupancy prediction module, we\nstill include this module during training to fairly compare with PARA-\nDrive. We follow the same training procedure in [2] to train the scene\ntokenizer. All non-map element tokens share one MLP adapter and\nall map element tokens share another one."}, {"title": "4.2 Dataset Construction", "content": "To train and evaluate TOKEN, we construct a dataset based on the NuScenes dataset [28], including\nvisual question-answering (QA) pairs that span the full stack of autonomous driving development.\nWe illustrate a few examples of these QAs in Fig. 1; more can be found in App. A.\nPerception. We build perception QAs based on the DriveLM dataset [7], covering object semantics\nand dynamic behavior identification. Additionally, we create object-lane association QAs to enhance\nthe model's understanding of map elements and objects' topological relationships to the ego vehicle.\nBehavior Reasoning. Behavior reasoning QAs include two types of questions: 1) object level\nbehavior analysis questions where we ask the model to reason about whether an object is critical\n(i.e., is likely to influence the ego vehicle's planning) and provide the corresponding reason; 2)\nscene-level critical object grounding questions where we ask the model to predict the locations of\ncritical objects in the ego vehicle's local frame. Behavior reasoning QAs aim to enable the model to\nunderstand context-dependent critical objects, thereby connecting the object tokens with the LLM\nto simplify the scene for downstream planning.\nRoute-conditioned Hierarchical Planning. Different from previous works that use the relative\nposition of the ground-truth ego trajectory to define high-level commands (\"keep forward\" and \"turn\nright/right\"), we re-labeled the NuScenes dataset to use road-level navigation signals as high-level\ncommands, including: \"keep forward along the current road,\" \"prepare to turn right/left at the next\nintersection,\" \"turn right/left at the intersection,\" \"left/right U-turn,\" and \"left/right 3-point turn.\" We\nutilize chain-of-thought reasoning to align the model's planning process and guide it to progressively\ngenerate the driving plans in three steps. First, the model identifies the critical objects in the current\ndriving scene, including their categories and 2D locations in the ego frame. Next, it proposes the\ndesired behavior mode, detailing interaction plans with the critical objects (e.g., overtake) and lane-\nlevel decisions (e.g., left lane change). Finally, it generates a 3-second motion plan (6 waypoints)."}, {"title": "4.3 Training Strategy", "content": "We use LLaMA-2-7B [29] as our LLM backbone. We keep the scene tokenizer frozen and use a\nLow-Rank Adaptation (LoRA) module [30] to fine-tune the LLM. We train TOKEN in three stages:\npre-training, reasoning fine-tuning, and planning fine-tuning. During pre-training (\u201crepresentation\nalignment\"), we disable LoRA and only train the adapter to enhance embedding space alignment\nbetween the scene and text tokens. We use only perception QAs (150k QAs) to train the adapter for\n5 epochs with a learning rate of 5e-4. In reasoning fine-tuning (\u201creasoning alignment\"), we train the\nadapter and LoRA together using the reasoning and planning QAs for 10 epochs with a learning rate\nof 1e-4. Finally, we train the adapter and LoRA together using just the planning QAs for another 10\nepochs to maximize the model's performance on planning, maintaining the learning rate at 1e-4."}, {"title": "5 Experimental Results", "content": "5.1 On the Value of Object-Centric Tokenization\nExperiment Design. We compare TOKEN against existing MM-LLMs to demonstrate its effec-\ntiveness and efficiency in scene understanding, grounding, and planning. Our experimental setup\nfocuses on the impact of 1) different tokenization schemes and 2) pre-training on driving data.\nBaseline. We compare TOKEN against (1) VILA-1.5 [31], which uses a trainable ViT to tokenize\nframes, (2) Video-LLaMA [22], which leverages a pre-trained and frozen ViT to extract visual\nfeatures and a querying transformer to produce visual tokens from the extracted features, and (3)\nBEV-TOKEN, a variant of TOKEN that directly uses dense bird's-eye view (BEV) features from\nthe same pre-trained PARA-Drive instead of sparse object-level tokens, similar to most MM-LLMs\n(e.g., Video-LLaMA). We follow the same training recipe for all models and use the same LLaMA-\n2-7B as the LLM backbone. For Video-LLaMA and VILA-1.5, we use the past four front view\nframes to render a 2-second video as input for each QA."}, {"title": "5.2 Generalization in Long-tail Driving Scenarios", "content": "We evaluate the planning performance of TOKEN against PARA-Drive in long-tail events. TOKEN\nuses the PARA-Drive as the scene-tokenizer, thus they share the same visual encoder and scene\ninformation, and only differ in the planner structure.\nLong-tail Events Construction. We manually inspected the NuScenes dataset and identified the\nfollowing long-tail scenarios for evaluation, each representing less than 1% of the training data: 1)\nexecuting 3-point turns; 2) resuming motion after a full stop; 3) overtaking parked cars through the\noncoming lane; and 4) navigating around construction sites. More details can be found in App. D.\nQuantitative Result. In Tab. 2, we summarize the quantitative evaluation for each long-tail scenario.\nTOKEN significantly outperforms PARA-Drive in terms of the quality and safety of predicted plans.\nSpecifically, TOKEN predicts more accurate turning maneuvers in 3-point turns (60% reduction in\nheading L2 Aveall), more effective motions after yielding (28% reduction in longitudinal weighted\nL2 Aveall), and safer plans when navigating around blocking vehicles and construction sites (100%\nand 67% collision rate reductions in the overtake and construction zone scenarios, respectively)."}, {"title": "5.3 On the Value of Alignment", "content": "We compare TOKEN with the SOTA LLM-based planner Agent-Driver [5]. Agent-Driver queries\ntext-based scene information using various tools and then fine-tunes GPT-3.5 into a motion planner.\nOur evaluation (Tab. 3) shows that TOKEN and Agent-Driver have similar overall performance, but\nTOKEN significantly outperforms agent-driver in long-tail scenarios with a much smaller model and\nless privileged information (details in App. F). We hypothesize that evoking common-sense reason-\ning in a LLM-powered planner requires proper alignment rather than just a larger LLM backbone.\nMotivated by this finding, we further conduct a detailed ablation study to shed more light on this."}, {"title": "6 Limitation & Future Work", "content": "A strength and limitation of TOKEN is using a pre-trained and frozen PARA-Drive model as the\nscene tokenizer. This allows for extracting informative tokens and controlled experiments. However,\nTOKEN's performance is tightly coupled with the quality of the pretrained tokenizer. We show a\nfailure case in App. I in which the critical object is not detected by the tracking querying transformer\nin PARA-Drive. Consequently, TOKEN is unable to understand the scene and generates incorrect\nbehavior. Further work will focus on co-training PARA-Drive to leverage the knowledge within the\nLLM to improve the scene tokenizer. Another limitation is outputting the trajectory plan as text,\nwith each digit as a separate token. This increases computational expense and complicates motion\ngeneration, leading to misalignment between predicted behavior and the motion plan (e.g., TOKEN\npredicts the correct behavior but incorrect motion in Fig. 6). Future work will focus on quantizing\nmotions into discrete LLM-understandable tokens [32] or using a dedicated trajectory decoder [33]."}]}