{"title": "st-DTPM: Spatial-Temporal Guided Diffusion Transformer Probabilistic Model for Delayed Scan PET Image Prediction", "authors": ["Ran Hong", "Yuxia Huang", "Lei Liu", "Zhonghui Wu", "Bingxuan Li", "Xuemei Wang", "Qiegen Liu"], "abstract": "PET imaging is widely employed for observing biological metabolic activities within the human body. However, numerous benign conditions can cause increased uptake of radiopharmaceuticals, confounding differentiation from malignant tumors. Several studies have indicated that dual-time PET imaging holds promise in distinguishing between malignant and benign tumor processes. Nevertheless, the hour-long distribution period of radiopharmaceuticals post-injection complicates the determination of optimal timing for the second scan, presenting challenges in both practical applications and research. Notably, we have identified that delay time PET imaging can be framed as an image-to-image conversion problem. Motivated by this insight, we propose a novel spatial-temporal guided diffusion transformer probabilistic model (st-DTPM) to solve dual-time PET imaging prediction problem. Specifically, this architecture leverages the U-net framework that integrates patch-wise features of CNN and pixel-wise relevance of Transformer to obtain local and global information. And then employs a conditional DDPM model for image synthesis. Furthermore, on spatial condition, we concatenate early scan PET images and noisy PET images on every denoising step to guide the spatial distribution of denoising sampling. On temporal condition, we convert diffusion time steps and delay time to a universal time vector, then embed it to each layer of model architecture to further improve the accuracy of predictions. Experimental results demonstrated the superiority of our method over alternative approaches in preserving image quality and structural information, thereby affirming its efficacy in predictive task.", "sections": [{"title": "I. INTRODUCTION", "content": "18F-fludeoxyglucose positron emission tomography (18F-FDG PET/CT) imaging is the most widely used and valuable molecular imaging technology in cancer clinic. The sensitivity of tumor diagnosis can reach 96% (83% to 100%), the specificity is 79% (52% to 100%), and the accuracy is 91% (86% to 100%) [1]. However, with the increasing clinical application of 18F-FDG tumor imaging, more and more false positive lesions have been found. Including inflammatory lesions (such as bacterial pneumonia, pulmonary abscess, aspergillosis) and granulomatous lesions (such as tuberculosis, sarcoidosis, histoplasmosis, Wegener granuloma, silicosis, etc.). These diseases make 18F-FDG PET/CT imaging difficult in the differential diagnosis of benign and malignant tumor lesions [2]. The main reason is that 18F-FDG is an analogue of glucose, which can be taken up by tumor cells or mononuclear macrophages, resulting in false positive results. Studies [3-5] have found that the peak time of 18F-FDG uptake by tumor cells and inflammatory cells is different. The peak time of 18F-FDG uptake by tumor cells is 3-4 hours after 18F-FDG injection, and the peak of uptake by inflammatory lesions is about 1 hour. Therefore, how to improve the efficiency of 18F-FDG PET/CT in the identification of benign and malignant tumors through the improvement of image acquisition methodology is a problem worth studying at present. Among them, delayed scan PET imaging (or dual-time PET imaging) technology is an effective method to improve the identification of benign and malignant tumors. The principle of delayed PET images acquisition is to take advantage of the different uptake or excretion levels of imaging agents in various tissues and cells at different time periods. A conventional image is collected about 1 hour after 18F-FDG injection, and then a delayed image is collected 2 hours after developer injection. Biological differences between tumors, inflammation, and normal tissues with high physiological uptake are identified by comparing changes in levels of imaging agent uptake or excretion over time periods. Currently, in many clinical studies of breast cancer, lung cancer, rectal and prostate cancer, etc., it has been found that standard uptake value (SUV) can significantly improve the accuracy of differentiating benign and malignant tumors through delayed PET images acquisition and quantitative analysis of lesions [6-10].\nHowever, in the field of PET delayed imaging research, there are some key challenges. 1) Increase the localized radiation dose for patients; 2) Prolonged patient stays in cory pose an increased clinical risk, particularly for critically ill patients, while also adding to the psychological burden. 3) Occupying a certain amount of time reduces machine turnover and total inspector count. 4) Extend staff working hours. 5) Raise staff radiation exposure levels. 6) Expand image storage capacity. These significant obstacles have led to a lack of research on PET delayed imaging prediction. At present, some studies have made progress in predicting the most appropriate time for delayed scanning, but they have not yet addressed the underlying issues [11, 12].\nWith the availability of extensive training datasets and substantial computational resources, the excellent performance of deep learning in the medical image domain has received increasing attention in recent years [13]-[15]. Generative adversarial network (GAN) is driven by adversarial learning of a pair of generator and discriminator. It presents many impressive performances across a series of image-to-image tasks such as image denoising [16], style transform [17], inpainting [18], and super-resolution [19]. Those GAN based image-to-image tasks are introduced by pix2pix. Image-to-image translation is the process of utilizing deep learning architectures in the field of computer vision to convert images from one domain to another by capturing shared correlations and feature representations between the two image types. For example, it can convert grayscale images to color images [20], translate satellite images into maps [21], or transform oil paintings into realistic photographs [22]. Additionally, diverse applications exist for this technology, including medical image reconstruction, precise image segmentation, advanced image enhancement, and intricate cross-domain image transformation [23, 24]. Surprisingly, we find that PET delayed imaging can also be formulated as an image-to-image conversion problem, where a model needs to be found to convert the first scan PET image into a delayed scan PET image. However, directly estimating a delayed PET image from the first scan also poses technical challenges, as some benign lesions such as inflammation and nodular granuloma can also have tracer uptake, complicating accurate diagnosis.\nIn the field of delayed imaging technology, few methods for generating delayed PET images have been reported. As far as we know, the generation of delayed PET images using deep learning methods has not been validated. In fact, due to the broader dynamic range of organ uptake and varying noise levels in scan results, the data distribution of PET images is more intricate than that of natural images. Notably, the work [25, 26] built a GAN model that includes a U-net generator and patch discriminator to obtain certain performance in the task which similar to ours. However, both models suffer from weak generator capabilities and shortcomings in adversarial learning, which hinder their ability to accurately predict PET images with long delays and achieve good performance on complex human structures.\nThe emergence of diffusion model [27]-[29] has brought generative model to a new stage and achieved remarkable results in some important fields. Among these, the denoising diffusion probability model (DDPM) [30] stands out as a representative model capable of enhancing information extraction from low-resolution PET images. To address the challenge of determining the timing for a delayed scan, we propose a novel diffusion model called spatial-temporal guided diffusion transformer probabilistic model for dual-time PET imaging (st-DTPM), aimed at alleviating clinical workload and enhancing diagnostic efficiency. The model is based on the U-net framework and combines the characteristics of CNN and pixel-wise transformer. It also adopts DDPM model under specific conditions to perform image diffusion. This study seeks to generate a delayed scan PET image by utilizing the first scan PET image via the st-DTPM model. Subsequently, we will assess the predictive accuracy of the st-DTPM model by comparing the generated results with the actual delayed PET images. Therefore, it can overcome the shortcomings of delayed scan PET imaging and make delayed PET images prediction more widely used in clinical practice.\nThe contributions in this paper are summarized as follows:\nDelayed PET imaging prediction based on spatial-temporal. Building upon the DDPM model, we concatenate noisy delayed scan PET images with original first scan PET images across channels to establish an image-to-image mapping. The similarity between two samples under different delays is taken as an additional constraint of the model. Furthermore, we embed time step and delay time as vectors into the model framework to further enhance predictive performance.\nDiffusion transformer probabilistic model. The architecture combines the advantages of CNN and transformer. We propose a conditional diffusion model based on U-net framework by mixing CNN and pixel-wise transformer, and embed different combinations of diffusion time step and delay time in both residual block and transformer block. Our approach is the first to apply deep learning to PET delayed imaging technology.\nThe remainder framework of this paper is outlined as follows. Section II briefly overviews some related works. Section III contains the key idea of the ST-DTPM approaches. The experimental settings shown in Section IV. Section V exhibits some experimental results. Section VI conducts a concise discussion and Section VII draws a conclusion for this work."}, {"title": "II. RELATED WORK", "content": "A. Denoising Diffusion Probabilistic Model\nDiffusion model [30-32] is a Markov chain that gradually infuses noise into pure images and then progressively removes noise, transforming Gaussian noise into generate images. The process of injecting noise into raw images can be represented as the forward diffusion process:\n$q(x_t | x_{t-1}) = \\mathcal{N} (x_t; \\sqrt{\\alpha_t}x_{t-1}, \\beta_tI)$  (1)\nhere $ \\beta_t $ and $ \\alpha_t $ represent the schedule of injecting noise into $ x_0 $ the variance $ \\beta_t \\in (0,1) $ increase with $ t \\sim uniform({1,\\dots,T})$. The definition of $ \\alpha_t =1-\\beta_t $, and $ \\overline{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i $, so $ \\alpha_t $ will decrease with t, and $ \\overline{\\alpha}_t $ will decrease to 0 as t becomes large. Eq. (1) represents one step in the diffusion process of Markov chain.\n$q(x_t | x_0) = \\mathcal{N} (x_t; \\sqrt{\\overline{\\alpha}_t}x_0, (1-\\overline{\\alpha}_t)I)$ (2)\nSince the parameters of the diffusion process are fixed, Eq. (1) can be extended to t steps, then we derive Eq. (2), which means get a noise image from a pure image. The $ x_t $ obeys a Gaussian distribution with mean $ \\sqrt{\\overline{\\alpha}_t}x_0 $ and variance $ 1-\\overline{\\alpha}_t $ for an arbitrary time step, it follows a normal distribution when t is large enough. The backward reconstruction process involves removing noise from a normal distribution using a Markov chain, which reverses the diffusion process by Bayes' theorem. Then the optimal mean $ \\mu_\\theta(x_t,x_0) $ will be:"}, {"title": "III. METHOD", "content": "A. Motivation\nThe PET delayed imaging technique is more flexible than traditional PET imaging methods. As depicted in Fig. 1, in order to obtain a more accurate understanding of the distribution of radioactive drugs in the body, the subject needs to undergo multiple scans at different time points after injection. Clinical doctors can evaluate the distribution changes of the radioactive drugs in the body by comparing the images at the two different time points, which can help determine the activity of lesions or tumors.\nSeveral studies have demonstrated that there is a certain correlation between the PET images of the first scan and the PET images of the delayed scan [37, 38]. Specifically, both scans involve PET imaging of tissue from the identical diseased site on the same subject that shares related information. Over time, tissue uptake of inflammation and other false positive lesions will gradually decrease, so the information contained will gradually decrease. The first scan provides information about molecular metabolism at the initial time point, while the delayed scan provides information about the time point of subsequent molecular changes. These temporal differences facilitate assessment of metabolic dynamics, such as tumor progression or treatment response.\nHowever, since the radioactive drug needs at least one hour to distribute throughout the body after injection, the second scan time point is often difficult to determine, which is a heavy task in practical applications and research. Therefore, we are contemplating if there are alternative methods to expedite the acquisition of the delayed scan PET images. Due to the broader dynamic range across various organs and the varying noise levels in different scan results, the data distribution of PET images is more intricate and possesses lower resolution compared to natural images. However, it is worth noting that PET delay imaging can also be formulated as an image-to-image conversion problem, where a model needs to be found to convert a PET image from the first scan to delay scan. Therefore, we can obtain more characteristic information by mining their prior information [39]. Diffusion model leverages the powerful generation capabilities to demonstrate their unique value in various fields. DDPM stands out as a representative model capable of enhancing information extraction from low-resolution PET images. Inspired by such methods, we try to generate delayed scan PET images using diffusion model methods. Therefore, we propose a novel spatial-temporal diffusion transformer probabilistic model (st-DTPM) to solve delayed scan PET imaging prediction problem.\nB. Spatial-Temporal Conditional PET Imaging Prediction\nIn this section, we introduce a diffusion model st-DTPM that follows the U-net framework. This model combines the advantages of CNNs and ViTs to excavate deeper feature information. Additionally, we propose a delay time embedding method based on dual-time PET imaging prediction, which utilizes the idea of position encoding in [35] to convert the time value into a general time vector. Drawing inspiration from [40], we propose a hybrid structure shown in Fig. 2. At each stage of the U-net down sampling, there are two blocks: a multi-CNN block and a pixel-wise transformer block, which will be discussed in detail in Section III.\u0421. Furthermore, to further enhance the prediction performance, we introduce the delay time embedding method, which is explained in following segment.\nDiffusion Model with Spatial Condition. The overall concept of forward-backward process and the deep denoising model is illustrated in Fig. 2. The training and generation of this approach are followed by DDPM, which gradually injects Gaussian noise at discrete time steps and attempts to mitigate noise using a deep denoising model to generate images from standard Gaussian noise. Since dual-time PET imaging prediction leverages early PET scan images to predict delayed PET scan images, the fundamental principle is related closely to the image-to-image methodology. Therefore, we need to achieve mapping between paired images. For this purpose, during training process, we adopt a simple approach follows [41, 42], using pairs of early scan PET images $ x_0 $ and noisy delayed scanned images $ x_t $ as model inputs. This method effectively facilitates the establishment of image-to-image mappings. We solve this task by adopting DDPM model for conditional image generation. The target of optimization in Eq. (4) can be reformulated as:\n$\\min {E_{\\varepsilon t}}{\\[ \\|\\varepsilon - \\varepsilon_\\theta (Concat(\\sqrt{\\overline{\\alpha_t}}x_0 + \\sqrt{1-\\overline{\\alpha_t}}\\varepsilon,x_t),t)\\|\\]}$ (5)\nwhere Concat is an operator that denotes the concatenation of pair images in channel dimension. In this way, a one-to-one image mapping can be achieved.\n$c = Concat(x_0,X_t)$ (6)\nwhere c denotes spatial condition that embeds to the model. In the context where of early scan PET image is used to as a condition for the denoising process, the backward processing proceeds as follows:\n$q(x_{t-1}|x_t, c) = \\mathcal{N} (x_{t-1}; \\mu_\\theta (x_t, c), \\sigma_t I)$ (7)\n$\\mu_\\theta (x_t, c) = \\frac{1}{\\sqrt{\\alpha_t}} (\\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha_t}}}\\varepsilon_\\theta(x_t, c))$ (8)\nIt can be seen from Eqs. (7) and (8) that conditional control is solely related to deep denoising models. Therefore, we use a method similar to [43, 44] which concatenate noisy images $ x_t $ and condition image $ x_0 $ on channels before every reconstruction step. To learn conditional diffusion models, such as class-conditional [45] or text-to-image [46] models, the conditional information is additionally incorporated into the noise prediction objective. Ultimately, the optimization target can be reformulated as:\n$\\min {E_{\\varepsilon , t}}{\\[ \\|\\varepsilon - \\varepsilon_\\theta (\\sqrt{\\overline{\\alpha_t}}c + \\sqrt{1-\\overline{\\alpha_t}}\\varepsilon,t,t_d)\\|\\]}$ (9)\nwhere $ t_d = st(x_0)-st(x_t) $ is the delay time interval that the difference between first scan time and delayed time scan.\nDiffusion Model with Temporal Condition. The diffusion model can be conceptualized as a cascade of denoising autoencoders. Each denoising autoencoder receives a noise image $ x_t $ as input and outputs its corresponding denoised image $ x_{t-1} $. The level of noise in the input image decreases incrementally as it passes through the autoencoder. Instead of employing a series of denoising autoencoders to handle multi-level noise, the diffusion model utilizes a universal denoising autoencoder. Consequently, it is essential to ensure that the model accurately discerns the noise level present in the current input image during the denoising process within the Markov chain, enabling precise completion of the denoising task. Our diffusion model framework is similar to DDPM [40, 47], which belongs to the discrete time diffusion model. The time step is set to 300, thus we denote time step $ t \\in {0,300} $.\nDelay time of dual-time PET imaging greatly affects the gap of radioactive quantification between first scan and delay scan. Based on this, we believe that delay time is a crucial factor affecting PET images. Therefore, we incorporate delay time as a temporal constraint within our model to further enhance prediction accuracy. Specifically, we embed the delay time using the same approach as the time step embedding in the diffusion model. Considering that the delay time interval $ t_d $ is a continuous variable and in order to transform it into a universal time vector, we need to convert it into a discrete time variable. Therefore, we ignore seconds and only use minutes to represent the delay time interval.\nIn order to unify the scale of two types of time information, we need to convert t and $ t_d $ into universal time vector $ T_s \\in \\mathbb{R}^{N \\times 1 \\times 1} $ and $ T_d \\in \\mathbb{R}^{N \\times 1 \\times 1} $, respectively. Then embed them into the multi-CNN block and pixel-wise transformer block in two different ways. Universal time vector converter is depicted in Fig. 3, it consists of sinusoidal position encoding follows [34] and bottleneck MLP. The sinusoidal position encoding uses the superposition of sine and cosine waves to transform discrete position or time into a vector $ T_d \\in \\mathbb{R}^{N} $. It is illustrated as follows:\n$PE_{(pos,2i)} = sin(pos/10000^{2i/N})$ (10)\n$PE_{(pos,2i+1)} = cos(pos /10000^{2i/N})$ (11)\nwhere N represents the length of the universal time vector, and pos represents the discrete time step of our input. The bottleneck MLP is built by dimension expansion linear layer and dimension compression linear layer, where the former expand dimension of $ T_s $ to $ T_s' \\in \\mathbb{R}^{4N} $, and the latter compression the dimension of $ SiLU(T_s') $ to $ T_s \\in \\mathbb{R}^N $.\nThere are two universal time vectors: the diffusion model time step vector $ T_s \\in \\mathbb{R}^{N \\times I \\times 1} $ and the delay time vector $ T_d \\in \\mathbb{R}^{N \\times I \\times 1} $, which need to be embedded into the model. To explore the interaction between two vectors, we have considered four combinations before embedding them into the model: 1) Each block (EC): alternately embed $ T_s $ and $ T_d $ into each block; 2) Linear adding (LA), adding them together and performing a linear projection i.e., linear($T_s +T_d$) . 3) Linear concatenation (LC): concatenating them and performing a linear projection i.e., linear(Concat($T_s,T_d$)); 4) Adding (AD): directly adding them i.e., $T_s +T_d$; Among them, the first approach combines effectiveness and efficiency. We will conduct a comprehensive analysis of the distinctions among these four embedding methodologies in ablation study.\nC. Denoising Network Combining CNN and Transformer\nAs illustrated in Fig. 2, the denoising module of the st-DTPM model consists of multiple stages. Each stage consists of the following two parts: (1) a cascade of residual block which obtains multi-scale visual feature and makes model possess inductive bias. (2) a pixel-wise transformer block which gives feature map long range relative among every pixel. PET images are characterized by their inherently blurred structure and the correlations between different regions. In one case, if a model composed solely of convolutional layers fails to capture the mutual influence of SUV values across different anatomical regions and cancerous tissues. In another case, when the model is built only with transformer, due to the lack of inductive bias and in the case of small data volume, relying solely on pixel correlations makes it difficult for the model to reconstruct the inherent human body shape in PET images. Therefore, we consider combining these two models to take full advantage of their strengths.\nOur proposed model integrates multi-scale information through multi-level sampling. It leverages convolutional layers to extract local structural details at each feature scale and employs transformer to effectively capture regional correlations. This hybrid approach enhances the model's capability to denoise PET images and reconstructs high-quality PET images, thereby improving diagnostic accuracy and clinical relevance.\nConvolution Layer. The module is followed by [48] which consists of convolution layer ( K = 3\u00d73, S = 1, P = 1), group normalization (Group = 4) and SiLU activation function. The operation between group normalization and the activation function serves as the time embedding mechanism, aiming to embed the time step from the denoising process of the diffusion model and the time interval condition from dual-time PET imaging into each block of the model. Specifically, we denote the input feature map as $ M \\in \\mathbb{R}^{C \\times H \\times W} $, where H, W and C represent the height, width, and channels of the feature map, respectively. Due to the hyperparameter setting of convolution layers, the shape of feature map is still keeping $ M \\in \\mathbb{R}^{C \\times H \\times W} $. Before the universal time vector is embedded in the convolution layer, we perform a linear projection that is denoted as $MLP(T_s, t \\in \\mathbb{R}^{2C \\times 1 \\times 1})$, then we chunk it into scale vector and shift vector, where scale, shift $ \\in \\mathbb{R}^{C \\times 1 \\times 1} $. Eq. (12) will embed time condition into residual block.\n$M_t = (scale+1) \\times M+ shift$ (12)\nPixel-wise Transformer Layer. The module follows ViT [35]. First, the given feature map $ x \\in \\mathbb{R}^{C \\times H \\times W} $ is passed through a flatten layer to obtain a pixel vector. Then, this vector undergoes a pixel-wise linear projection to generate a sequence of tokens $ x_s \\in \\mathbb{R}^{L \\times D} $, where L = H\u00d7W, D is hidden dimension of transformer block. Due to the small size of the original PET images, we adopt a pixel-based transformer, which can obtain correlations between each pixel without adding too much computational complexity. On this basis, we have added time condition to further improve its predictive accuracy. Specifically, the query Q, key K and value V of multi-head self-attention are redefined as Eqs. (13), (14) and (15).\n$Q = Concat(W_{qs}x +W_t \\times T)$ (13)\n$K = Concat(W_{ks}x +W_t \\times T)$ (14)\n$V = Concat(W_{vs}x +W_t \\times T)$ (15)\nwhere $ T $ is a universal time vector processed by a linear layer, whose length equals the length of the tokens. $ W_{qs} $, $W_{ks}$, $W_{vs}$ denote pixel-wise linear projection weights for Q, K and V. $ W_t $ represents temporal linear projection weights for universal time vector. The self-attention is computed as follows:\n$Attention = Softmax(\\frac{Q \\times K}{\\sqrt{D}} ) \\times V$ (16)\nBy injecting the time token into the attention calculation, each pixel in the feature map becomes correlated with the temporal condition. This correlation enables the model to adjust the denoised SUV with respect to time throughout the denoising process. After transformer block, the time token is discarded, the time token is discarded, reverting the feature map to its original arrangement. The temporal condition transformer is defined as:\n$x_t = MSA(W_{vs} \\times Concat(GN(x)+W_{t} \\times x_t))+x$ (17)\n$x_1 = MLP(GN(UnConcat(x,x_t)))+x$ (18)\nwhere MSA denotes multi-head self-attention which is described as Eq. (17), MLP and GN denote multi-linear projection and group norm, respectively.\nAlgorithm 1 and 2 provide a detailed explanation of the delayed PET image prediction process."}, {"title": "IV. EXPERIMENT", "content": "A. Datasets Details\nThe dataset used in this study is clinical data from 232 patients who undergo PET/CT routine examination and delayed 18F-FDG PET/CT scan at the affiliated hospital of Inner Mongolia medical university from January 2018 to December 2023, there are 122 cases of prostate cancer and 110 cases of lung cancer, and all patients' sensitive information is erased. All 233 patients have obvious lesions and are diagnosed through biopsy, surgical resection, or follow-up for more than 12 months.\nDuring the PET/CT data acquisition phase, the patient's fasting blood glucose level is controlled below 11.1 mmol/L before the scan. The patient needs to lie down and rest for 40-60 minutes, after inject 18F-FDG intravenously at a dose of 0.15 mci/kg. Then, empty the bladder, drink 500 ml of milk, and perform PET/CT imaging. During imaging, the patient is placed in a supine position with both arms raised to the top of the head, and the scanning range is from the skull to the upper middle femur. During CT scanning, CARE Dose4D is used as the current reference, with a reference current of 350mA, a voltage of 120 kV, and a reconstructed layer thickness of 5.0mm . Then, a PET scan is performed with a scanning speed of 1.7mm/s and FWHM 5.0, Ultra HD. The PET image reconstruction is performed using TureX+TOF, 2 iterations, and 21 subsets. Delayed scan is chest or pelvic PET/CT imaging performed again 60-120 minutes after the completion of trunk PET/CT acquisition. The remaining scanning parameters are the same as above.\nB. Experimental Setting\nEvaluation Metrics. To evaluate the quality of image reconstruction, one perceptual metric, Fr\u00e9chet inception distance (FID) and three typical metrics for evaluating image quality, including peak signal-to-noise (PSNR), structural similarity index (SSIM), and mean squared error (MSE) were employed to evaluate the performance of model. The PSNR and MSE are defined as follows:\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x_i})^2$ (19)\n$PSNR = 20 \\times lg(\\frac{MAX(x_i)}{\\sqrt{MSE(x_i)}})$ (20)\nwhere $x_i$ and $\\hat{x_i}$ denote reconstruction image and ground truth image, respectively. PSNR and MSE focus on pixel-wise matching between ground truth image and reconstruction image. Therefore, they can evaluate the details of the reconstructed image well, but only partially evaluate the structure of the reconstructed image. SSIM is formulated as follows:\n$SSIM(x, y) = \\frac{(2 \\mu_x \\mu_y + c_1)(2 \\sigma_{xy} + c_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}$ (21)\nit evaluates image quality in three aspects: brightness, contrast, and structure. However, both PSNR and SSIM can only evaluate the reconstructed images at the pixel level and statistical level, unable to truly reflect human observations of image quality. Dual-time PET imaging needs to play a role in clinical applications, so human perception of image quality is also an important evaluation metrics. Then we introduce FID, it is a Fr\u00e9chet distance between the hidden feature map of reconstruct images and ground truth images which performing a pretrained inception-V3 model [49]. The define of Fr\u00e9chet distance is described as follows:\n$FID^2 = \\|\\mu_r - \\mu_t\\|^2 +Tr(\\sigma_r +\\sigma_t - 2(\\sigma_r \\sigma_t)^{0.5})$ (22)\nwhere r and t denote the hidden feature map of reconstructed images and target images from pretrained inception-V3 model, $ \\mu $ denotes mean, $ \\sigma $ denotes covariance matrix and Tr denotes calculate the trace of a matrix. Inception-V3 model is a powerful image classification network that can effectively extract image features, thus the hidden feature of inception-v3 can be seen as a model of image perception, which can to some extent approximate human perception of images.\nImplementation Details. Our model was implemented by Pytorch framework and trained on an NVIDIA GeForce RTX 3060 Ti with 8G memory. The network training follows diffusion model manner for 300 timesteps and 1000 epochs using Adam optimizer which $ \\beta_1 = 0.5 $, $ \\beta_2 = 0.999 $, $ \\sigma_1 = 0.0001 $ with a batch size of 8. The number of stages of model is 5, the base dimension of each stage is 32, the number of CNN block and transformer block in each stage are both two."}, {"title": "V. RESULTS", "content": "To assess the capabilities of st-DTPM model, we conduct a comparative analysis against five prominent networks: DDPM, DiT, U-net, ResUnet, and Pix2Pix. We comprehensively analyzed the generation prediction capability of st-DTPM using two different datasets. The averaged quantitative results achieved are detailed in Table I in terms of four various evaluation metrics. Notably, st-DTPM achieves better results with higher PSNR and SSIM scores coupled with lower MSE and FID scores in both datasets. These findings underscore the ability of st-DTPM to generate delayed scan PET images of exceptional quality and precision, closely aligning with true delayed scan PET images. Meanwhile, in the comparison with DDPM generated results, we further confirm that the embedded transformer model and delay time variables can better capture the subtle changes and features in PET images, and the prediction results are more accurate. In contrast, DiT exhibits notably higher prediction errors, likely attributable to its diminished performance with limited datasets, necessitating extensive training for favorable outcomes. Besides, traditional generation models U-net, ResUnet and Pix2Pix are difficult to obtain the interaction between tissue structure and cancer tissue on the SUV values in different regions with only a single convolutional layer, so the generated images are blurry.\nFig. 5 presents compelling visual evidence through side-by-side comparisons of predicted delayed scan PET images alongside truly delayed images from three typical prostate cancer patients. It is worth to note that since the results of a single patient are presented here, we simply display the PSNR and SSIM of the images generated by each method."}, {"title": "VI. DISCUSSION", "content": "This work proposed a new dual-time PET delay imaging method named st-DTPM. The core idea of this method was to generate the PET images of the first scan through a network model, so as to realize the prediction of the delayed scan PET images. To match such a model, we leveraged the powerful generation capabilities of DDPM to generate delayed scan PET images. First, we used early scanned PET images and noisy delayed scanned images to train a deep noise reduction model to learn the prior information between the images. This process involved capturing complex spatial and temporal relationships in the data to improve the accuracy of predictions. Secondly, we embedded transformer structure to U-net denoising network inside DDPM to form a hybrid noise reduction model. At the same time, we converted the delay time variable and time step into a general vector and embed it into the network to further improve the prediction accuracy of the model.\nOur proposed st-DTPM model demonstrated better predictive accuracy compared to other generative models. To comprehensively assess its performance, we conducted comparative analyses across prostate cancer and lung cancer datasets, evaluating predictions generated by U-net, ResUnet, Pix2Pix, DiT, and st-DTPM. Specifically, when only using the U-net network, it failed to capture the mutual influence between tissue structures and cancerous tissues on the SUV values across different regions. Consequently, both U-net and ResUnet models produce less accurate predictions than st-DTPM. At the same time, employing transformer alone in the model construction lacked the necessary inductive bias to effectively reconstruct the intrinsic organizational features of PET images, particularly when data was limited. This limitation resulted in larger prediction errors for PET delay using DiT. By using the hybrid conditional diffusion model method of st-DTPM, we achieved significantly improved model prediction performance. This method enhanced accuracy by capturing intricate spatial-temporal relationships and effectively incorporated temporal information, thereby overcoming the limitations of other models.\nThe paired PET images from dual-time were not registered. However, st-DTPM still has accurate predictive performance. This demonstrates its robustness, as it can partially correct image position offsets while learning image mapping relationships.\nWhile the st-DTPM model has demonstrated excellent results in these comparative experiments, there are still notable limitations that require addressing. A primary concern is whether the inherent low resolution of PET images may introduce interference in delay prediction. This issue stems from the inherent limitations of PET imaging technology. Moving forward, we will focus on developing more accurate PET delay prediction methods to mitigate these challenges in future studies."}, {"title": "VII. CONCLUSION", "content": "In this work, we introduced a hybrid diffusion transformer model designed for predicting delayed scan PET images. Inspired by the DDPM architecture, our approach utilized early delayed scan PET images and delayed scan PET images with added noise as inputs, employing the hybrid diffusion transformer model to denoise and generate images from standard Gaussian noise. Experimental results demonstrated that the st-DTPM method achieved better performance metrics including higher PSNR and SSIM, lower MSE and FID compared to alternative models and exceled in generating high-quality delayed scan PET images."}]}