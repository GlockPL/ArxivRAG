{"title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents", "authors": ["Petr Anokhin", "Nikita Semenov", "Artyom Sorokin", "Dmitry Evseev", "Mikhail Burtsev", "Evgeny Burnaev"], "abstract": "Advancements in generative AI have broadened the potential applications of Large Language Models (LLMs) in the development of autonomous agents. Achieving true autonomy requires accumulating and updating knowledge gained from interactions with the environment and effectively utilizing it. Current LLM-based approaches leverage past experiences using a full history of observations, summarization or retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs a memory graph that integrates semantic and episodic memories while exploring the environment. This graph structure facilitates efficient associative retrieval of interconnected concepts, relevant to the agent's current state and goals, thus serving as an effective environmental model that enhances the agent's exploratory and planning capabilities. We demonstrate that our Ariadne LLM agent, equipped with this proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks on a zero-shot basis in the TextWorld environment. Our approach markedly outperforms established methods such as full-history, summarization, and Retrieval-Augmented Generation in various tasks, including the cooking challenge from the First TextWorld Problems competition and novel tasks like house cleaning and puzzle Treasure Hunting. The code for this work can be found at GitHub.", "sections": [{"title": "Introduction", "content": "Impressive language generation capabilities of large language models (LLMs) has sparked substantial interest in their application as core components for creating autonomous agents capable of interacting with dynamic environments and executing complex tasks. Over the past year, the research community has explored general architectures and core modules for such LLM agents [1, 2, 3, 4]. A crucial property of a general cognitive agent is its ability to accumulate and use knowledge. A long-term memory allows an agent to store and recall past experiences and knowledge, enabling it to learn from previous encounters and make informed decisions. However, the question of the best way to equip an agent with these capabilities remains open. Despite the constraints inherent in transformer architectures, contemporary methods enable LLMs to manage contexts encompassing millions of tokens [5]. However, this approach proves inefficient for agents required to maintain continuous interaction with their environment. Such agents must hold an entire historical context in memory to perform actions, which is not only costly but also limited in handling complex logic hidden in vast amounts of information. Research into alternative frameworks like Recurrent Memory Transformer [6, 7] and MAMBA [8] seeks to provide long-term memory solutions, though these models are still in their infancy.\nCurrently, the most popular solution for incorporating memory to LLM agents is the Retrieval-Augmented Generation (RAG) approach. RAG in a form of vector retrieval leverages an external database to enhance the model's prompt with relevant information. This technique is commonly used in memory architectures for LLM agents, often to recall specific observations or learned skills. However, it suffers from unstructured nature, greatly reducing the ability to retrieve related information, which may be scattered throughout the agent's memory. These limitations can be overcome by using knowledge graphs as database. This approach has also experienced a resurgence with the advent of LLMs [9]. However, for a robust memory architecture, integrating both structured and unstructured data is essential. In cognitive science, this integration parallels the concepts of semantic and episodic memory. Semantic memory encompasses factual knowledge about the world, whereas episodic memory pertains to personal experiences, which often contain richer and more detailed information. Though traditionally considered separate due to their distinct neurological representations, recent studies suggest these memory types are interconnected [10]. Semantic knowledge is built upon the foundation of episodic memory and subsequently provides a structured base for associative memory. This allows for the integration of various memory aspects, including episodic memories themselves.\nIn our research, we have developed a memory architecture called Ariadne's Graph (AriGraph), that integrates semantic and episodic memories within a memory graph framework. A knowledge graph represents a network of interconnected semantic knowledge, while episodic memories are depicted as episodic edges that can connect multiple relations within the graph. As an agent interacts with environment, it learns joint semantic and episodic world model by updating and extending knowledge graph based memory. This architecture not only serves as a foundational memory framework but also aids in environmental modeling, improving spatial orientation and exploration capabilities. For the general framework of our LLM agent called Ariadne, we employed pipeline of memory retrieval, planing and decision making inspired by neurocognitive models.\nFor evaluation of proposed methods we set up experiments to study two research questions.\nRQ1. Can LLM based agents learn useful structured world model from scratch via interaction with an environment?\nRQ2. Does structured knowledge representation improve retrieval of relevant facts from memory and enable effective exploration?\nWe evaluated our agent using tasks generated in the TextWorld environment [11]. Although our architecture can support visual modalities, our primary objective was to assess the agent's ability to plan and reason using accumulated knowledge, which for LLMs are still mainly in natural language domain. Text-based settings provide the flexibility to develop complex challenges that test these capabilities.\nExperimental results on three type of tasks such as navigation puzzles, cleaning and cooking demonstrate that our agent can effectively learn through interactions with environment and significantly outperforms other memory approaches for LLMs such as full history, summarization and RAG based on recency, importance, and relevance scores approach from [12]. We also show that our method outperforms existing reinforcement learning (RL) baselines."}, {"title": "AriGraph World Model", "content": "Memory graph structure. AriGraph world model $G = (V_s, E_s, V_e, E_e)$ consists of semantic $(V_s, E_s)$ and episodic memory $(V_e, E_e)$ vertices and edges (see Figure 2).\nAt each step $t$ agent receives observation $o_t$ and sends action $a_t$ back to the environment. The environment also returns rewards $r_t$ that are not visible to the LLM agent but are used to evaluate its performance. The agent continuously learns world model $G$ by extracting semantic triplets $(object_1, relation, object_2)$ from textual observations $O_t$.\n*   $V_s$ is a set of semantic vertices. Semantic vertices correspond to objects extracted from triplets.\n*   $E_s$ is a set of semantic edges. Semantic edge is a tuple $(v, rel, u)$, where $u, v$ are semantic vertices and $rel$ is a relationship between them. Semantic edges essentially represent triplets integrated in the semantic memory.\n*   $V_e$ is a set of episodic vertices. Each episodic vertex corresponds to an observation received from the environment at the respective step $v_e = O_t$.\n*   $E_e$ is a set of episodic edges. Each episodic edge $e_t = (v_e, E_t)$ connects all semantic triplets $E_t$ extracted from $o_t$ with each other and corresponding episodic vertex $v_e$. In other words episodic edges represent temporal relationship \"happened at the same time\".\nLearning AriGraph. Interaction with the environment can provide the agent with an information about the world to create new or update previously acquired knowledge. Given new observation $o_t$, LLM agent extracts new triplets as semantic vertices $V_t$ and edges $E_t$. To find already existing knowledge about the objects mentioned in $o_t$ a set of all semantic edges $E_{rel}$ incident to vertices $V_t$ is filtered out. Then outdated edges in $E_{rel}$ are detected by comparing them with $E_t$ and removed from the graph. After clearing outdated knowledge we expand semantic memory with $V_et$ and $E_t$. Episodic memory is updated by simply adding new episodic vertex $v_t$ containing $o_t$ and new episodic edge that connect all edges in $E_t$ with $v_e$. Episodic nodes store agent's past history and episodic edges connects all knowledge received at the same step. See appendix D for prompts used to extract new triplets and detect outdated knowledge.\nRetrieval from AriGraph. For successful decision-making and planning in a partially observable environment, the agent needs to be able to retrieve relevant knowledge. Retrieval from the AriGraph memory consists of two procedures: (1) a semantic search returns the most relevant triplets (semantic edges) and (2) an episodic search that, given extracted triplets, returns the most relevant episodic vertices $V_e$. The pseudo-code for the search is presented in the Algorithm 1."}, {"title": "Ariadne cognitive architecture", "content": "To test utility of AriGraph world modelling method we propose a cognitive architecture called Ariadne. Ariadne agent interacts with an unknown environment to accomplish a goal set by a user.\nThroughout this process, at each time step, the agent learns a world model, plans and executes actions. Ariadne has long-term memory stored as AriGraph and working memory containing information for current planning and decision making.\nGiven an observation the agent updates world model and retrieves semantic and episodic knowledge from AriGraph to working memory. Working memory is also populated with a final goal description, current observation, history of recent observation and actions. At the planning stage, Ariadne agent uses content of working memory to create new or update existing plan as a series of task-relevant sub-goals, each accompanied by a concise description. The planning module also evaluates the outcomes of actions based on feedback from the environment after each action at step $t - 1$, adjusting the plan accordingly.\nThe revised plan is added to the working memory which is accessed by the decision-making module, tasked with selecting the most suitable action aligned with the current plan's objectives. This module adheres to the ReAct [14] framework, requiring the agent to articulate the rationale behind an action before execution. Separation of planning from decision-making enables LLMs to focus on distinct cognitive processes. In text-based environments an agent selects an action from the list of valid actions. Our agent can also use graph specific function for navigation utilizing its memory module. It extends its action space with 'go to location' type commands and infers an optimal route to a target location using spatial relations stored in a semantic graph.\nOur tests and other studies [15, 16, 17, 18, 19] indicate that even advanced LLMs like GPT-4 struggle with spatial orientation and modeling environments through text alone. Thus, GPT-4 can accurately process only a limited set of relationships before it begins to struggle in generating correct paths or reasoning about spatial relations, often resulting in errors as the complexity of data increases. To overcome this limitation, the Ariadne agent utilizes its graph memory to keep track of previously visited locations and their interconnections, allowing it to infer unexplored paths if exploration is required by the plan. Details on implementation of exploration are provided in the Appendix B."}, {"title": "Experiments and Results", "content": "We compared Ariadne agent with alternative methods in a series of text based games involving spatial navigation, object collection and tool manipulation. All these games can be considered Partially Observable MDPs (POMDPs). Such games have long been benchmarks for researching agents capable of effectively remembering information and establishing long-term dependencies[20, 21, 22].\nThe primary objective is to unlock the golden locker and retrieve the treasure hidden within. The game consists of rooms each featuring a locker of a different color. Initially, the player finds a key in the first room, along with instructions detailing which specific locker this key can unlock. Each subsequent locker contains another key and a note directing the player to the next key's location, creating a chain of clues and discoveries leading to the golden locker. The easy variation has 12 rooms and 4 keys and hard one has 16 rooms and 5 keys, however second key is significantly harder to find. Agent receives 1 point for picking a key and 1 point for completing the game.\nThe goal is to clean the house that consists of 9 distinct rooms, each designated for a specific purpose, such as a pool, kitchen, etc. Each room contains items, some of which are misplaced, for example, a toothbrush found in the dining room. There are total of 11 misplaced items. The agent's objective is to tidy up the house by identifying items that are out of place and returning them to their appropriate locations. This task requires the agent to utilize reasoning skills to determine the proper locations for items and to effectively recall from memory where each item belongs, while simultaneously managing multiple tasks. Agent receives 1 point for picking up every displaced items, 1 point for placing an item in the right location and -1 for manipulation with correctly placed item. Our task is conceptually similar to the TextWorld Commonsense (TWC) benchmark[23]. However, while TWC primarily centers on logical reasoning within one or at most two locations, our emphasis is on environmental exploration and the testing of memory based on past observations. Consequently, we have substantially expanded the number of rooms and items in our setup.\nThe goal is to prepare and consume a meal. First an agent needs to find a recipe that provides detailed instructions, including required ingredients and specific preparation methods such as dicing, slicing, chopping, grilling, frying, and roasting. Medium difficulty task features 9 locations and 3 ingredients and hard task features 12 locations and 4 ingredients. The agent receives points for correctly selecting ingredients and executing the proper procedures. The game is considered lost if the agent performs any incorrect action or uses an inappropriate tool for a given ingredient. We enhanced the initial observation with instructions and provided explanations for specific actions to tailor the task for LLM models, particularly concerning the appropriate use of household kitchen appliances for different actions with ingredients. For instance, a BBQ should be used for grilling, while a stove is appropriate for frying (see Appendix D for prompt). This allows to test the agent's ability to remember and adhere to instructions.\nAdditionally, we tested our architecture on a variation of the cooking test from [24] to compare it with RL baselines. These tasks have 4 levels of difficulty, however, they are significantly simpler than our main tasks, having fewer locations, ingredients, and required actions. Level 1: 1 ingredient, 1 room, cutting. Level 2: 1 ingredient, 1 room, cutting + cooking. Level 3: 1 ingredient, 9 rooms, no processing. Level 4: 3 ingredients, 6 rooms, cutting + cooking. We tested or agent and raw GPT-4 with full history on 3 randomly generated environments for each task difficulty. As mentioned, we slightly adapted the task for the LLM models. While this adaptation should not impact the difficulty level, it's important to note that the environments for comparison between LLM and RL models were not 100% identical."}, {"title": "Baselines", "content": "As baselines we consider Ariadne architecture with the graph world model (sec. 2) replaced by one of the following alternatives: full history of observations and actions, iterative summarization, and a RAG implementation described in [12].\nThe simplest memory mechanism assessed involved retention of a complete history of observations and actions to inform decision-making at each step. Although this approach is prohibitively expensive for longer games, it is crucial for testing reasoning and planning capabilities of the current generation of LLMs over extended contexts in agentic settings.\nAn alternative to storing full history is to remember only what is necessary while discarding the rest. This might be implemented as an iterative summarization of all accumulated observations. This approach offers advantages due to its simplicity and low cost. However, it faces a critical limitation  the inability to recall any details not included in the summarized context.\nThe third baseline uses RAG featuring a scoring mechanism that incorporates recency, importance, and relevance [12]. Here, working memory receives history of 5 last observations and actions and 5 retrieved observations.\nFor RL baselines, we collect the best results reported by [24, 25, 26] for the GATA, LTL-GATA, and EXPLORER architectures on Cooking task with four difficulties levels from [24].\nTo estimate human human performance, we developed a graphical user interface (available at our GitHub), allowing volunteers to play medium versions of the Treasure Hunt, Cleaning, and Cooking. After collecting the data, we excluded sessions where the game was not completed."}, {"title": "Results", "content": "All our experiments were carried out with tasks created in the TextWorld environment [11]. We employed the gpt-4-0125-preview version of GPT-4 Turbo as a backbone LLM for our agent and baselines. We run our system and other LLM-based alternatives for five times on each task and report results from the three best out of five runs.\nResults presented on the Figure 4 and Figure 5 demonstrate a clear advantage of Ariadne agent with AriGraph over baseline memory implementation method. Ariadne with AriGraph successfully learns and utilize world knowledge for all three tasks.\nAll baseline agents fail to complete the treasure-hunting game (fig.4 A), with the highest achievement being an agent with full history access, which only manages to locate 3 keys. On the other hand AriGraph based agent successfully solves the task in about fifty steps. In more complex settings, where the second key is positioned at the farthest end of the map (fig.5 A), our agent successfully retrieves it and continues to finish the game, whereas the baseline agents struggle to locate even the second key.\nFor the cleaning task (fig.4 B), AriGraph enables the agent to effectively model the environment and strategically place collected items, completing the task more swiftly. It is noteworthy that, in this environment, episodic memory notably impeded the agent's progress. This phenomenon may be attributed to the agent's confusion caused by past memories contained the original locations of objects, leading to erroneous decisions regarding their return to these initial positions. The logs of the task completion support this hypothesis.\nIn cleaning task, maintaining a full history proved less beneficial likely due to the larger observations and faster filling of the context window as a result. RAG and summary alternatives show better results but still significantly lag in performance compared to proposed method.\nThe cooking task (fig.4 C, fig.5 B) presents the highest difficulty, because any error fails the whole task. All baseline agents either fail to gather the needed ingredients or make errors during meal preparation due to insufficient or improperly utilized information. In this task, episodic memory is particularly important, allowing the agent to recall useful observations such as the content of the recipe or cooking instructions. An agent without episodic memory manages to finish this task only once and score less overall points.\nGraph based knowledge representation makes possible implementation of efficient exploration policies which allows the agent to solve Treasure Hunt and Cooking tasks twice faster. This can be seen by comparing figures 4 A and 4 C. Comparison of results for easy and hard (fig.5 A-C, right bars) settings demonstrates that baselines stop learning at all but AriGraph approach scales surprisingly well. Adding objects and rooms in the tasks only slightly delay full task completion.\nWe also compared Ariadne with AriGraph world model on variation of cooking task from [24] with strong RL baselines (Fig.6). The task includes environments of four difficulty levels. Level 4 simplifies our task to merely locating the kitchen where all ingredients are stored. GPT-4 with full history manage to beat the first two levels, which involve a single room and ingredient, thus reveals that our agent completes the cooking game faster compared to top results of human players (fig. 7 C), performs comparable in the Treasure Hunt game (fig. 7 A) and under performs in the Cleaning game (fig. 7 B). Ariadne shows superior results compared to the average performance of human players across all games, particularly in more challenging settings such as cooking."}, {"title": "Related work", "content": "In this section, we can distinguish three main areas of focus related to our work: memory architectures, suggested for autonomous agents [27]; the integration of knowledge graphs with language models; performance evaluation of agents in text-based environments.\nVoyager [28], Ghost in the Minecraft [29] and Jarvis-1 [30] are advanced, open-ended LLM agents that show significantly better performance in Minecraft compared to earlier techniques. These agents feature memory capabilities through a library of learned skills, summaries of successful actions, and episodic memory with plans for successful task execution. However, they fall short in representing knowledge with semantic structure and depend heavily on the LLM's extensive Minecraft knowledge, or even access to the Minecraft wiki. Generative agents [12] mimic human behavior in multi-agent environments and were among the pioneers in introducing an advanced memory system for LLM agents, which we use as our baseline. This system records episodic memories and features a retrieval mechanism that prioritizes information based on recency, importance, and relevance. The importance is assessed by the LLM, while relevance is determined through vector retrieval. This approach works good for open ended behavior simulation, but falls short for the solution of specific task,"}, {"title": "Conclusions", "content": "In this paper, we introduced AriGraph, a novel knowledge graph world model designed for LLM agents. Given textual observations AriGraph learns from scratch both semantic and episodic memories in the form of knowledge graph. In our study we tested if LLM agent will be able to learn useful world model with our method and how AriGraph knowledge representation can help in goal directed behavior. To encapsulate and test different methods implementing memory about environment we propose a cognitive architecture called Ariadne. Ariadne is LLM based agent with memory, planning and decision-making capabilities. Our results demonstrate that combination of Ariadne with AriGraph was not only able to learn world models but also substantially outperformed full-history, summarization, and RAG implementations of memory in the TextWorld environments.\nIn Treasure Hunt, Cooking and Cleaning text games which require navigation and manipulation with objects distributed across multiple rooms the AriGraph agent demonstrates superior performance compared to alternatives which fail to accomplish task even in 'easy' task setting. Structured knowledge representation is beneficial for implementation of efficient exploration which significantly accelerates learning and task completion. As a result, the proposed learning method appears to scale surprisingly good with adding more objects and rooms to the tasks.\nLimitations and future work. Our framework stands to gain significantly from advancements in the reasoning capabilities of LLMs. The existing generation of models is still prone to hallucinations, occasionally follows instructions inadequately, and may struggle to fully integrate all available information when making decisions. Our approach is more cost-effective than relying on full history, particularly in lengthy tasks. However, it requires more queries to LLM and consumes more tokens compared to simpler methods such as RAG. In the paper, we propose that the principal concepts of Ariadne could potentially be generalized to domains beyond text games; however, this possibility requires additional research. Total cost for all experiments including baselines is 900 USD.\nIn our research, we laid the groundwork for developing a sophisticated memory system for an LLM agent. This system can be enhanced by incorporating additional types of modalities for observations, procedural memories and by applying a variety of methods for updating, reasoning, and conducting graph searches (for example, approaches from [52, 53, 36, 54, 55]), specifically through the integration of LLMs.\nContributions. All authors contributed to the study conception and design. PA, NS, AS, and DE contributed to method development and implementation. PA and NS conducted experiments. All authors engaged in discussing the methods and results throughout the experimental process. The first draft of the manuscript was written by PA, NS, and AS. All authors commented on previous versions of the manuscript. The final edit was performed by MB. MB and EB supervised this study. All authors read and approved the final manuscript."}, {"title": "Appendices", "content": "Pseudo-code for SemanticSearch function in AriGraph is listed in Algorithm 2. This algorithm is close to BFS search. The main difference is the use of retrieval mechanism in function EmbedAndRetrieve. Function EmbedAndRetrieve(E, q, w) use pretrained Contriever [13] model to compute embeddings for edges E and query q and then returns top w edges with a highest similarity score. Similarity score between edge e and query q is a dot product between their embedding. Most of the times when query for EmbedAndRetrieve is a semantic vertex it simply returns edges incident to this vertex, but also has ability to retrieve edges connected to vertices that are semantically close to the query vertex. For example semantic graph can contain separate vertices for \"grill\" and \"grilling\" that are not directly connected, so searching for \"gill\" can potentially return edge (\"bbq\", \"used for\", \"grilling\").\nBefore any planning or decision-making occurs, an auxiliary agent assesses the need for exploration based on a pre-established plan. Depending on this assessment, it either activates or deactivates exploration mode. Moreover, the graph agent extracts triplets containing information about exits, such as \"kitchen, has an unexplored exit, south,\" and triplets detailing spatial connections between locations like \"hall, east of, kitchen.\" Subsequently, simple graph-based methods can be employed to identify all exits from each room that the agent has detected but not yet explored. This information is then added to the working memory of the agent. Function for finding triplets corresponding to probable unexplored actions is listed in Algorithm 3. Current implementation use expert knowledge about what elements of the semantic graph can represent locations and exits between them.\nSince our Ariadne agent successfully passed both medium and hard difficulty levels without quality degradation, we decided to further increase the complexity of the hard cooking scenario. This was achieved by introducing closed doors, limiting the number of items in the inventory, and adding an extra ingredient to the recipe. The resulting environment became the most challenging cooking scenario that can be generated using TextWorld. Our agent was also tested in this environment, and subsequently, we compiled a diagram (Fig.8) depicting average success rates across different difficulty levels of the cooking tasks."}, {"title": "DLLM Prompts", "content": "The triplets denote facts about the environment where the player moves. The player takes actions and the environment changes, so some triplets from the list of existing triplets can be replaced with one of the new triplets. For example, the player took the item from the locker and the existing triplet \"item, is in, locker\" should be replaced with the new triplet \"item, is in, inventory\".\nSometimes there are no triplets to replace:\nSometimes several triplets can be replaced with one:\nEnsure that triplets are only replaced if they contain redundant or conflicting information about the same aspect of an entity. Triplets should not be replaced if they provide distinct or complementary information about entities compared to the new triplets. Specifically, consider the relationships, properties, or contexts described by each triplet and verify that they align before replacement. If there is uncertainty about whether a triplet should be replaced, prioritize retaining the existing triplet over replacing it. When comparing existing and new triplets, if they refer to different aspects or attributes of entities, do not replace them. Replacements should only occur when there is semantic duplication between an existing triplet and a new triplet.\nAnother example of when not to replase existung triplets:\nI repeat, do not replace triplets, if they carry differend type of information about entities!!! It is better to leave a tripplet, than to replace the one that has important information. Do not state that triplet needs to be replaced if you are not sure!!!\nIf you find triplet in Existing triplets which semantically duplicate some triplet in New triplets, replace such triplet from Existing triplets. However do not replace triplets if they refer to different things.\n\nINSTRUCTION:\nYou will be provided with sub-goals and reasons for it from plan of an agent. Your task is to state if this sub goals require exploration of the environment, finding or locating something.\nAnswer with just True or False.\n\n\nINSTRUCTION:\nYou are a planner within the agent system tasked with navigating the environment in a text-based game.\nYour role is to create a concise plan to achieve your main goal or modify your current plan based on new information received.\nMake sure your sub-goals will benefit the achivment of your main goal. If your main goal is an ongoing complex process, also put sub-goals that can immediately benifit achiving something from your main goal.\nIf you need to find something, put it into sub-goal.\nIf you wish to alter or delete a sub-goal within the current plan, confirm that this sub-goal has been achieved according to the current observation or is no longer relevant to achieving your main goal. Untill then do not change wording in \"sub_goal\" elements of your plan and their position in the plan. Only change wording in \"reason\" part to track the progress of completion of sub-goals.\nIf sub-goal was completed or confirmed to be no more relevant, delete it, replase it with new one or with lower priority sub-goals from the plan. Untill then keep the structure of sub-goals as it is. Create new sub-goals only if they will benifit your main goal and do not prioritize them over current sub-goals.\nIf your task is to obtain something, make shure that the item is in your inventory before changing your sub-goal.\nYour plan contains important information and goals you need to complete. Do not alter sub-goals or move them in hierarchy if they were not completed!\nPay attention to your inventory, what items you are carring, when setting the sub-goals. These items might be important.\nPay attention to information from your memory module, it is important.\nThere should always be at least one sub-goal.\n\n\nINSTRUCTION:\nYou are an action selector within an agent system designed to navigate an environment in a text-based game. Your role involves receiving information about an agent and the state of the environment alongside a list of possible actions.\nYour primary objective is to choose an action from the list of possible actions that aligns with the goals outlined in the plan, giving precedence to main goal or sub-goals in the order they appear (main goal is highest priority, then sub_goal_1, sub_goal_2, etc.). However, prioritize sub-goals that can be solved by perfroming single action in current situation, like 'take something', over long term sub-goals.\nActions like \"go to 'location'\" will move an agent directly to stated location, use them instead of \"go_west, type of actions, if the destination you want to move to is further than 1 step away.\"\nIn tasks centered around exploration or locating something, prioritize actions that guide the agent to previously unexplored areas. You can deduce which locations have been visited based on the history of observations and information from your memory module.\nPerforming same action typically will not provide different results, so if you are stuck, try to perform other actions or prioritize goals to explore the environment.\nYou may choose actions only from the list of possible actions. You must choose strictly one action.\n\n\nINSTRUCTION:\nYou are a guide within a team of agents engaging in a text-based game. Your role is to concisely yet thoroughly detail all the essential aspects of the current situation. Ensure that your summary aids in information extraction and facilitates the decision-making process by focusing on pertinent details and excluding extraneous information. Incorporate a strategic outlook in your narrative, emphasizing information integral to forming a tactical plan.\nAccurately relay the outcomes of previously attempted actions, as this is pivotal for shaping subsequent choices. Your account will form the sole basis on which the decision-making agents operate; thus, clarity and avoidance of potential confusion are paramount.\nBe judicious with your inferences, presenting only well-substantiated information that is likely to be of practical benefit. Your account should be succinct, encapsulated within a maximum of three paragraphs."}]}