{"title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents", "authors": ["Petr Anokhin", "Nikita Semenov", "Artyom Sorokin", "Dmitry Evseev", "Mikhail Burtsev", "Evgeny Burnaev"], "abstract": "Advancements in generative AI have broadened the potential applications of Large Language Models (LLMs) in the development of autonomous agents. Achieving true autonomy requires accumulating and updating knowledge gained from interactions with the environment and effectively utilizing it. Current LLM-based approaches leverage past experiences using a full history of observations, summarization or retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs a memory graph that integrates semantic and episodic memories while exploring the environment. This graph structure facilitates efficient associative retrieval of interconnected concepts, relevant to the agent's current state and goals, thus serving as an effective environmental model that enhances the agent's exploratory and planning capabilities. We demonstrate that our Ariadne LLM agent, equipped with this proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks on a zero-shot basis in the TextWorld environment. Our approach markedly outperforms established methods such as full-history, summarization, and Retrieval-Augmented Generation in various tasks, including the cooking challenge from the First TextWorld Problems competition and novel tasks like house cleaning and puzzle Treasure Hunting. The code for this work can be found at GitHub.", "sections": [{"title": "1 Introduction", "content": "Impressive language generation capabilities of large language models (LLMs) has sparked substantial interest in their application as core components for creating autonomous agents capable of interacting with dynamic environments and executing complex tasks. Over the past year, the research community has explored general architectures and core modules for such LLM agents [1, 2, 3, 4]. A crucial property of a general cognitive agent is its ability to accumulate and use knowledge. A long-term memory allows an agent to store and recall past experiences and knowledge, enabling it to learn from previous encounters and make informed decisions. However, the question of the best way to equip an agent with these capabilities remains open. Despite the constraints inherent in transformer architectures, contemporary methods enable LLMs to manage contexts encompassing millions of tokens [5]. However, this approach proves inefficient for agents required to maintain continuous interaction with their environment. Such agents must hold an entire historical context in memory to perform actions, which is not only costly but also limited in handling complex logic hidden in vast amounts of information. Research into alternative frameworks like Recurrent Memory Transformer"}, {"title": "2 AriGraph World Model", "content": "Memory graph structure. AriGraph world model $G = (V_s, E_s, V_e, E_e)$ consists of semantic $(V_s, E_s)$ and episodic memory $(V_e, E_e)$ vertices and edges (see Figure 2).\nAt each step $t$ agent receives observation $o_t$ and sends action $a_t$ back to the environment. The environment also returns rewards $r_t$ that are not visible to the LLM agent but are used to evaluate his performance. The agent continuously learns world model $G$ by extracting semantic triplets $(object_1, relation, object_2)$ from textual observations $O_t$.\n\\begin{itemize}\n    \\item $V_s$ is a set of semantic vertices. Semantic vertices correspond to objects extracted from triplets.\n    \\item $E_s$ is a set of semantic edges. Semantic edge is a tuple $(v, rel, u)$, where $u, v$ are semantic vertices and $rel$ is a relationship between them. Semantic edges essentially represent triplets integrated in the semantic memory.\n    \\item $V_e$ is a set of episodic vertices. Each episodic vertex corresponds to an observation received from the environment at the respective step $v_e = O_t$.\n    \\item $E_e$ is a set of episodic edges. Each episodic edge $e_t = (v_e, E_t)$ connects all semantic triplets $E_t$ extracted from $o_t$ with each other and corresponding episodic vertex $v_e$. In other words episodic edges represent temporal relationship \"happened at the same time\".\n\\end{itemize}\nLearning AriGraph. Interaction with the environment can provide the agent with an information about the world to create new or update previously acquired knowledge. Given new observation $o_t$, LLM agent extracts new triplets as semantic vertices $V_t$ and edges $E_t$. To find already existing knowledge about the objects mentioned in $o_t$ a set of all semantic edges $E_{rel}$ incident to vertices $V$ is filtered out. Then outdated edges in $E_{rel}$ are detected by comparing them with $E_t$ and removed from the graph. After clearing outdated knowledge we expand semantic memory with $V_{et}$ and $E_t$. Episodic memory is updated by simply adding new episodic vertex $v_t$ containing $o_t$ and new episodic edge that connect all edges in $E_t$ with $v_e$. Episodic nodes store agent's past history and episodic edges connects all knowledge received at the same step. See appendix D for prompts used to extract new triplets and detect outdated knowledge.\nRetrieval from AriGraph. For successful decision-making and planning in a partially observable environment, the agent needs to be able to retrieve relevant knowledge. Retrieval from the AriGraph memory consists of two procedures: (1) a semantic search returns the most relevant triplets (semantic edges) and (2) an episodic search that, given extracted triplets, returns the most relevant episodic vertices $V_e$. The pseudo-code for the search is presented in the Algorithm 1."}, {"title": "3 Ariadne cognitive architecture", "content": "To test utility of AriGraph world modelling method we propose a cognitive architecture called Ariadne. Ariadne agent interacts with an unknown environment to accomplish a goal set by a user.\nThroughout this process, at each time step, the agent learns a world model, plans and executes actions. Ariadne has long-term memory stored as AriGraph and working memory containing information for current planning and decision making.\nGiven an observation the agent updates world model and retrieves semantic and episodic knowledge from AriGraph to working memory. Working memory is also populated with a final goal description, current observation, history of recent observation and actions. At the planning stage, Ariadne agent uses content of working memory to create new or update existing plan as a series of task-relevant sub-goals, each accompanied by a concise description. The planning module also evaluates the outcomes of actions based on feedback from the environment after each action at step $t - 1$, adjusting the plan accordingly.\nThe revised plan is added to the working memory which is accessed by the decision-making module, tasked with selecting the most suitable action aligned with the current plan's objectives. This module adheres to the ReAct [14] framework, requiring the agent to articulate the rationale behind an action before execution. Separation of planning from decision-making enables LLMs to focus on distinct cognitive processes. In text-based environments an agent selects an action from the list of valid actions. Our agent can also use graph specific function for navigation utilizing its memory module. It extends its action space with 'go to location' type commands and infers an optimal route to a target location using spatial relations stored in a semantic graph.\nOur tests and other studies [15, 16, 17, 18, 19] indicate that even advanced LLMs like GPT-4 struggle with spatial orientation and modeling environments through text alone. Thus, GPT-4 can accurately process only a limited set of relationships before it begins to struggle in generating correct paths or reasoning about spatial relations, often resulting in errors as the complexity of data increases. To overcome this limitation, the Ariadne agent utilizes its graph memory to keep track of previously visited locations and their interconnections, allowing it to infer unexplored paths if exploration is required by the plan. Details on implementation of exploration are provided in the Appendix B."}, {"title": "4 Experiments and Results", "content": "We compared Ariadne agent with alternative methods in a series of text based games involving spatial navigation, object collection and tool manipulation. All these games can be considered Partially Observable MDPs (POMDPs). Such games have long been benchmarks for researching agents capable of effectively remembering information and establishing long-term dependencies[20, 21, 22].\nThe primary objective is to unlock the golden locker and retrieve the treasure hidden within. The game consists of rooms each featuring a locker of a different color. Initially, the player finds a key in the first room, along with instructions detailing which specific locker this key can unlock. Each subsequent locker contains another key and a note directing the player to the next key's location, creating a chain of clues and discoveries leading to the golden locker. The easy variation has 12 rooms and 4 keys and hard one has 16 rooms and 5 keys, however second key is significantly harder to find. Agent receives 1 point for picking a key and 1 point for completing the game.\nThe goal is to clean the house that consists of 9 distinct rooms, each designated for a specific purpose, such as a pool, kitchen, etc. Each room contains items, some of which are misplaced, for example, a toothbrush found in the dining room. There are total of 11 misplaced items. The agent's objective is to tidy up the house by identifying items that are out of place and returning them to their appropriate locations. This task requires the agent to utilize reasoning skills to determine the proper locations for items and to effectively recall from memory where each item belongs, while simultaneously managing multiple tasks. Agent receives 1 point for picking up every displaced items, 1 point for placing an item in the right location and -1 for manipulation with correctly placed item. Our task is conceptually similar to the TextWorld Commonsense (TWC) benchmark[23]. However, while TWC primarily centers on logical reasoning within one or at most two locations, our emphasis is on environmental exploration and the testing of memory based on past observations. Consequently, we have substantially expanded the number of rooms and items in our setup.\nThe goal is to prepare and consume a meal. First an agent needs to find a recipe that provides detailed instructions, including required ingredients and specific preparation methods such as dicing, slicing, chopping, grilling, frying, and roasting. Medium difficulty task features 9 locations and 3 ingredients and hard task features 12 locations and 4 ingredients. The agent receives points for correctly selecting ingredients and executing the proper procedures. The game is considered lost if the agent performs any incorrect action or uses an inappropriate tool for a given ingredient. We enhanced the initial observation with instructions and provided explanations for specific actions to tailor the task for LLM models, particularly concerning the appropriate use of household kitchen appliances for different actions with ingredients. For instance, a BBQ should be used for grilling, while a stove is appropriate for frying (see Appendix D for prompt). This allows to test the agent's ability to remember and adhere to instructions.\nAdditionally, we tested our architecture on a variation of the cooking test from [24] to compare it with RL baselines. These tasks have 4 levels of difficulty, however, they are significantly simpler than our main tasks, having fewer locations, ingredients, and required actions. Level 1: 1 ingredient, 1 room, cutting. Level 2: 1 ingredient, 1 room, cutting + cooking. Level 3: 1 ingredient, 9 rooms, no processing. Level 4: 3 ingredients, 6 rooms, cutting + cooking. We tested or agent and raw GPT-4 with full history on 3 randomly generated environments for each task difficulty. As mentioned, we slightly adapted the task for the LLM models. While this adaptation should not impact the difficulty level, it's important to note that the environments for comparison between LLM and RL models were not 100% identical."}, {"title": "4.2 Baselines", "content": "As baselines we consider Ariadne architecture with the graph world model (sec. 2) replaced by one of the following alternatives: full history of observations and actions, iterative summarization, and a RAG implementation described in [12].\nThe simplest memory mechanism assessed involved retention of a complete history of observations and actions to inform decision-making at each step. Although this approach is prohibitively expensive for longer games, it is crucial for testing reasoning and planning capabilities of the current generation of LLMs over extended contexts in agentic settings.\nAn alternative to storing full history is to remember only what is necessary while discarding the rest. This might be implemented as an iterative summarization of all accumulated observations. This approach offers advantages due to its simplicity and low cost. However, it faces a critical limitation  the inability to recall any details not included in the summarized context.\nThe third baseline uses RAG featuring a scoring mechanism that incorporates recency, importance, and relevance [12]. Here, working memory receives history of 5 last observations and actions and 5 retrieved observations."}, {"title": "4.3 Results", "content": "All our experiments were carried out with tasks created in the TextWorld environment [11]. We employed the gpt-4-0125-preview version of GPT-4 Turbo as a backbone LLM for our agent and baselines. We run our system and other LLM-based alternatives for five times on each task and report results from the three best out of five runs.\nResults presented on the Figure 4 and Figure 5 demonstrate a clear advantage of Ariadne agent with AriGraph over baseline memory implementation method. Ariadne with AriGraph successfully learns and utilize world knowledge for all three tasks.\nAll baseline agents fail to complete the treasure-hunting game (fig.4 A), with the highest achievement being an agent with full history access, which only manages to locate 3 keys. On the other hand AriGraph based agent successfully solves the task in about fifty steps. In more complex settings, where the second key is positioned at the farthest end of the map (fig.5 A), our agent successfully retrieves it and continues to finish the game, whereas the baseline agents struggle to locate even the second key.\nFor the cleaning task (fig.4 B), AriGraph enables the agent to effectively model the environment and strategically place collected items, completing the task more swiftly. It is noteworthy that, in this environment, episodic memory notably impeded the agent's progress. This phenomenon may be attributed to the agent's confusion caused by past memories contained the original locations of objects, leading to erroneous decisions regarding their return to these initial positions. The logs of the task completion support this hypothesis.\nIn cleaning task, maintaining a full history proved less beneficial likely due to the larger observations and faster filling of the context window as a result. RAG and summary alternatives show better results but still significantly lag in performance compared to proposed method.\nThe cooking task (fig.4 C, fig.5 B) presents the highest difficulty, because any error fails the whole task. All baseline agents either fail to gather the needed ingredients or make errors during meal preparation due to insufficient or improperly utilized information. In this task, episodic memory is particularly important, allowing the agent to recall useful observations such as the content of the recipe or cooking instructions. An agent without episodic memory manages to finish this task only once and score less overall points.\nGraph based knowledge representation makes possible implementation of efficient exploration policies which allows the agent to solve Treasure Hunt and Cooking tasks twice faster. This can be seen by comparing figures 4 A and 4 C. Comparison of results for easy and hard (fig.5 A-C, right bars) settings demonstrates that baselines stop learning at all but AriGraph approach scales surprisingly well. Adding objects and rooms in the tasks only slightly delay full task completion.\nWe also compared Ariadne with AriGraph world model on variation of cooking task from [24] with strong RL baselines (Fig.6). The task includes environments of four difficulty levels. Level 4 simplifies our task to merely locating the kitchen where all ingredients are stored. GPT-4 with full history manage to beat the first two levels, which involve a single room and ingredient, thus"}, {"title": "5 Related work", "content": "In this section, we can distinguish three main areas of focus related to our work: memory architectures, suggested for autonomous agents [27]; the integration of knowledge graphs with language models; performance evaluation of agents in text-based environments.\nVoyager [28], Ghost in the Minecraft [29] and Jarvis-1 [30] are advanced, open-ended LLM agents that show significantly better performance in Minecraft compared to earlier techniques. These agents feature memory capabilities through a library of learned skills, summaries of successful actions, and episodic memory with plans for successful task execution. However, they fall short in representing knowledge with semantic structure and depend heavily on the LLM's extensive Minecraft knowledge, or even access to the Minecraft wiki. Generative agents [12] mimic human behavior in multi-agent environments and were among the pioneers in introducing an advanced memory system for LLM agents, which we use as our baseline. This system records episodic memories and features a retrieval mechanism that prioritizes information based on recency, importance, and relevance. The importance is assessed by the LLM, while relevance is determined through vector retrieval. This approach works good for open ended behavior simulation, but falls short for the solution of specific task,"}, {"title": "6 Conclusions", "content": "In this paper, we introduced AriGraph, a novel knowledge graph world model designed for LLM agents. Given textual observations AriGraph learns from scratch both semantic and episodic memories"}, {"title": "Appendices", "content": "Pseudo-code for SemanticSearch function in AriGraph is listed in Algorithm 2. This algorithm is close to BFS search. The main difference is the use of retrieval mechanism in function EmbedAndRetrieve. Function EmbedAndRetrieve(E, q, w) use pretrained Contriever [13] model to compute embeddings for edges E and query q and then returns top w edges with a highest similarity score. Similarity score between edge e and query q is a dot product between their embedding. Most of the times when query for EmbedAndRetrieve is a semantic vertex it simply returns edges incident to this vertex, but also has ability to retrieve edges connected to vertices that are semantically close to the query vertex. For example semantic graph can contain separate vertices for \"grill\" and \"grilling\" that are not directly connected, so searching for \"gill\" can potentially return edge (\"bbq\", \"used for\", \"grilling\").\nBefore any planning or decision-making occurs, an auxiliary agent assesses the need for exploration based on a pre-established plan. Depending on this assessment, it either activates or deactivates exploration mode. Moreover, the graph agent extracts triplets containing information about exits, such as \"kitchen, has an unexplored exit, south,\" and triplets detailing spatial connections between locations like \"hall, east of, kitchen.\" Subsequently, simple graph-based methods can be employed to identify all exits from each room that the agent has detected but not yet explored. This information is then added to the working memory of the agent. Function for finding triplets corresponding to probable unexplored actions is listed in Algorithm 3. Current implementation use expert knowledge about what elements of the semantic graph can represent locations and exits between them."}, {"title": "C Difficulties Comparison", "content": "Since our Ariadne agent successfully passed both medium and hard difficulty levels without quality degradation, we decided to further increase the complexity of the hard cooking scenario. This was achieved by introducing closed doors, limiting the number of items in the inventory, and adding an extra ingredient to the recipe. The resulting environment became the most challenging cooking scenario that can be generated using TextWorld. Our agent was also tested in this environment, and subsequently, we compiled a diagram (Fig.8) depicting average success rates across different difficulty levels of the cooking tasks."}]}