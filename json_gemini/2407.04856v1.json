{"title": "Explorative Imitation Learning: A Path Signature Approach for Continuous Environments", "authors": ["Nathan Gavenski", "Juarez Monteiro", "Felipe Meneguzzi", "Michael Luck", "Odinaldo Rodrigues"], "abstract": "Some imitation learning methods combine behavioural cloning with self-supervision to infer actions from state pairs. However, most rely on a large number of expert trajectories to increase generalisation and human intervention to capture key aspects of the problem, such as domain constraints. In this paper, we propose Continuous Imitation Learning from Observation (CILO), a new method augmenting imitation learning with two important features: (i) exploration, allowing for more diverse state transitions, requiring less expert trajectories and resulting in fewer training iterations; and (ii) path signatures, allowing for automatic encoding of constraints, through the creation of non-parametric representations of agents and expert trajectories. We compared CILO with a baseline and two leading imitation learning methods in five environments. It had the best overall performance of all methods in all environments, outperforming the expert in two of them.", "sections": [{"title": "1 Introduction", "content": "One of the most common forms of learning is by watching someone else perform a task and, afterwards, trying it ourselves. As humans, we can observe an action being performed and transfer the acquired knowledge into our reality. In this respect, it is less challenging to achieve a goal in an optimal way by observing how an expert behaves; in the field of computer science, this is Imitation Learning (IL). Unlike conventional reinforcement learning, which depends on a reward function, IL learns from expert guidance, and is concerned with an agent's acquisition of skills or behaviours by observing a 'teacher' perform a given task.\nLearning from demonstration is the obvious approach for IL, requiring expert demonstrations, which are 'trajectories' including actions performed along the way to goal completion [12]. Such an approach uses the trajectories to learn an approximate policy that behaves like the expert. Learning from demonstration suffers from two significant drawbacks in practice: poor generalisation in environments with multiple alternative trajectories that achieve a goal, which is bound to occur when the dataset size increases, and the unavailability of data about the expert's actions. Learning from observation (LfO) overcomes these limitations by learning a task without direct action information via self-supervision, which increases generalisation [8]. This allows a model to learn from sample executions without action information, which would otherwise be unusable. LfO approaches often rely on techniques from classification to improve sample-efficiency [30] and generalisation [18]. Such agents require fewer expert trajectories, yielding more general approaches that are, hence, adaptable to unseen scenarios. However, these methods still fail to leverage some useful learning features, particularly the use of an exploration mechanism.\nSome existing work [5, 7] requires manual intervention in different stages of the process, e.g., the hard-coding of environment goals, which is not feasible in complex environments, such as robotic systems with multifaceted goals. Other work [7, 13, 30] is limited in that learning the environment dynamics depends strongly on previously collected samples that usually do not relate to how the environment dynamics operate under expert behaviour, such as random transitions, or prior knowledge of the dynamics of environments. In addition, maintaining self-supervision [7, 13] for an IL method is important since unlabelled data is more readily available, e.g., from sources that are not necessarily meant for agent learning.\nIn this paper, we propose a novel LfO approach to IL called Continuous Imitation Learning from Observation (CILO) that addresses the above issues. CILO (i) eliminates the need for manual intervention when using different environments by discriminating between policy and expert; (ii) requires fewer samples for learning by leveraging exploration and exploitation; and (iii) does not require expert-labelled data, thus remaining self-supervised. We evaluated CILO in five widely used continuous environments against a baseline and two leading LfO methods (see Section 4). Our results show that CILO outperformed all of the alternatives, surpassing the expert in two of five environments.\nCILO's new mechanisms are model-agnostic and applicable to a wider range of environment dynamics than those of the compared LfO alternatives. We argue that the new mechanisms can be readily incorporated into other IL methods, paving the way for more robust and flexible learning techniques."}, {"title": "2 Problem Formulation", "content": "We assume the environment to be an MDP \\( M = (S, A, T, r, \\gamma) \\), in which S is the state space, A is the action space, T is a transition model, r is the immediate reward function, and \\( \\gamma \\) is the discount factor [25]. Although in general an MDP may carry information regarding the reward and discount factors, we consider that this information is inaccessible to the agent during training, and the learning"}, {"title": "3 Continuous Imitation Learning from Observation", "content": "We address the need for manual intervention and for maintaining self-supervision in CILO through two key innovations: an exploration mechanism used when the action predictions are uncertain; and a discriminator to interleave random and current states to improve the prediction of self-supervised actions. CILO achieves this by employing three different models: (i) the inverse dynamic model M to predict the action responsible for a transition between two states \\( P(s_t, s_{t+1}) \\); (ii) a policy model \\( \\pi_\\theta \\) that uses the self-supervised labels \\( \\hat{a} \\) to imitate the expert \\( \\pi_\\psi \\) given a state \\( P(a \\mid s_t) \\); and (iii) a discriminator model D to discriminate between \\( \\pi_\\psi \\) and \\( \\pi_\\theta \\), creating newer samples for M.\nAlgorithm 1 provides an overview of CILO's learning process. First, CILO initialises all models with random weights and uses the"}, {"title": "3.1 Exploration", "content": "Exploration is vital for IL methods that use dynamics models to learn how the expert behaves. It enables policy divergence when the dynamics model is uncertain and increases state diversity, which helps the model approximate labelled transitions from unlabelled ones (expert). CILO borrows an exploration mechanism from reinforcement learning in continuous domains, in which each action in a policy consists of two outputs: the mean and standard deviation to sample from a Gaussian distribution. However, unlike traditional reinforcement learning, where a policy receives feedback in the form of the reward function, IL lacks this information. Thus, for a model M and parameters \\( \\theta \\) (\\( M_\\theta \\)), we employ the sampling mechanism in Eq. 4, where \\( \\pi \\) is the usual mathematical constant 3.14... and \\( \\varepsilon \\), as defined in Eq. 5, is used as standard deviation, where a is the ground-truth action (or pseudo-labels from M) and \\( \\hat{a} \\) is the action predicted by the model:\n\\begin{equation}\n\\tilde{a}_{M_{\\theta}} = \\frac{1}{\\varepsilon \\sqrt{2\\pi}} e^{-\\frac{\\||a - \\hat{a}\\||^2}{2\\varepsilon^2}}\n\\end{equation}\n\\begin{equation}\n\\varepsilon = ||a - \\hat{a}||^p\n\\end{equation}\nIn Eq. 4, M is either \\( M_{\\theta} \\) or \\( \\pi_\\theta \\), and \\( \\theta \\) are the parameters of the model updated for the epoch. Notice that when p = 1, the model M uses the absolute value between the predicted and ground-truth labels \\( ||a - \\hat{a}|| \\) and this allows for higher exploration.\nObservation 1. If \\( L \\) is a loss function that monotonically decreases a model's M error as it approximates the ground-truth function, eventually \\( ||a - \\hat{a}|| < 1 \\). If we then use \\( p > 1 \\) in Eq. 5, \\( \\varepsilon \\) will exponentially decrease.\nGiven all of the above, Eq. 5 offers a trade-off between exploration and exploitation. Since \\( \\varepsilon \\) is the standard deviation for the exploration function, as the model's predictions get closer to the ground-truth and pseudo-labels, the clusters will have lower variance because the exploration ratio is directly correlated to the model's error.\nIn Alg. 1, functions TRAINM (ln. 4) and BEHAVIOURALCLONING (ln. 6) use this adaptation to adjust the exploration ratio depending on how close the model's predictions are to the ground-truth (or pseudo-labels), in accordance with the standard deviation of the Gaussian distribution. This mechanism also has the benefit of not having to predict information beyond the agent's actions, such as standard deviation, instead obtaining this directly from the model's error. For deterministic behaviour, we can assume that the standard deviation for the model is 0 and use the model's output since sampling from a Gaussian distribution with average x and deviation 0 equals x."}, {"title": "3.2 Goal-aware function", "content": "Developing a goal-aware function may not be a trivial task. For environments with a well-defined goal, such as CartPole [1], which defines the goal to be balancing the pole for 195 steps, a goal-identification function could simply classify all trajectories that reach 195 steps as optimal. In this work, we formally define trajectories as:\nDefinition 1. A trajectory is a finite sequence of states \\( (s_1,..., s_n) \\) where for each \\( 1 < i < n \\), \\( s_{i+1} \\) is obtained from \\( s_i \\) via the execution of some action. We use the term \\( (\\tau_1^t, \\tau_2^t, ..., \\tau_d^t) \\) \\( \\in \\mathbb{R}^d \\) to denote the particular state \\( s_t \\) \\( (1 \\leq t \\leq n) \\) within the trajectory \\( \\tau \\).\nHowever, recall that in the context of IL, the agent has no access to the reward signal, and as environments grow in complexity, such a function becomes even harder to encode. By contrast, some environments have no prescribed goal. For example, the Ant environment requires the agent to walk as far as possible without falling, but with no defined cap on the number of time steps [23]. Thus, existing IL approaches [18, 7, 10] often rely on manually defined goal-aware functions, which have the benefit of dispensing with the alignment of the environment's goal. For example, we might define a specific trajectory as required in the Ant environment. Unless the agent reaches all points in this trajectory, our goal-aware function does not classify the episode as successful. However, this creates a degree of unwanted complexity in a learning algorithm and a cumbersome process as the number of environments grows. Yet, trajectories may carry relevant information for CILO since they approximate I's samples from \\( T^{\\pi_\\psi} \\) [7]. Therefore, CILO tries to classify trajectories that are close to \\( T^{\\pi_\\psi} \\) instead of successful ones.\nNevertheless, identifying whether samples are near \\( T^{\\pi_\\psi} \\) is also difficult. If we consider a stationary agent, we might discard samples that allow M to better predict transitions due to their distance to the \\( \\pi_\\psi \\) states alone. But, if we consider whole trajectories, it might be difficult to identify middling trajectories needed to close the gap between \\( T_\\theta \\) and \\( T^{\\pi_\\psi} \\), and better generalise [8]. Therefore, CILO needs a function that (i) simplifies comparisons between trajectories and (ii) allows M to receive suboptimal samples.\nFor the first problem, previous work [19] dealt with the issue of trajectory length by using the average of all states up to a point in time to account for the trajectory changes. Conversely, we use path signatures [2], which are fixed-length feature vectors that are used to represent multi-dimensional time series (i.e., trajectories). A path signature is computed by the function \\( \\beta \\) comprehensively defined in Section 3 of Yang et al. [29], succinctly summarised in the definition below (see Supplementary Material for more detail).\nDefinition 2. Let a trajectory \\( \\tau \\) of a countable length between [1, n] \\( (n \\in \\mathbb{N}) \\), where each state is a vector in \\( \\mathbb{R}^d \\) with dimensions indexed by a collection of indices \\( i_1, \\dots, i_k \\) \\( \\in \\{1,\\dots,d\\} \\). Let the recursively computed path signature \\( \\beta \\) for a trajectory \\( \\tau \\) for any \\( k \\geq 1 \\) and time t \\( (1 \\leq t \\leq n) \\) be:\n\\begin{equation}\n\\beta^{\\textit{i}_1,...,\\textit{i}_k}_{1,t} = \\int_1^t \\beta^{\\textit{i}_1,...,\\textit{i}_{k-1}}_{1,s} d \\tau^{\\textit{i}_k}_{s}.\n\\end{equation}\nThen, the signature of a trajectory \\( \\tau : [1,n] \\rightarrow \\mathbb{R}^d \\) is the collection of all the iterated integrals of \\( \\tau \\):\n\\begin{equation}\n\\beta(\\tau) = (1, \\beta(\\tau)_{1,n}^{\\textit{i}_1}, \\beta(\\tau)_{1,n}^{\\textit{i}_1,\\textit{i}_2}, \\dots, \\beta(\\tau)_{1,n}^{\\textit{i}_1,\\textit{i}_2,\\dots,\\textit{i}_k}),\n\\end{equation}\nwhere the zero-th term is conventionally equal to 1, and k is defined as the k-th level of the signature, which defines the finite collection of all terms \\( \\beta(\\tau)_{1,n}^{\\textit{i}_1,...,\\textit{i}_k} \\) for the multi-index of length k. For example, when k = d, the last term would be \\( \\beta(\\tau)_{1,n}^{d,d,...,d} \\)."}, {"title": "3.3 Sample efficiency", "content": "Besides approximating the expert policy, IL methods focus on efficiently using expert samples. This focus happens since expert samples are hard to obtain. Thus, creating more efficient methods, i.e., that require fewer samples, allows for more useable approaches. Some recent strategies [13, 30] minimise the number of required samples but depend on strong assumptions (see Section 4.2) or manual intervention for each new environment. For comparison, CILO uses 10 expert episodes \u2013 a number similar to Zhu et al. [30] and Kidambi et al. [13], but without requiring manual intervention for each environment. CILO relies on up-scaling \\( T^{\\pi_\\theta} \\) to increase the number of observations \\( \\pi_\\theta \\) sees before interacting with the environment. Although trivial, this strategy works because CILO is self-supervised and has an exploration mechanism. This strategy helps in two ways: (i) for each epoch all pseudo-labels differ in all transitions due to the exploration mechanism (Line 4, Algorithm 1); and (ii) increasing the number of samples \\( \\pi_\\theta \\) receives allows for more updates before sampling new experiences from the environment. By applying its exploration mechanism to each observation individually and sampling exploration values from a distribution, CILO ensures that each observation has unique action values, reducing the risk of misrepresenting the ground-truth action distribution."}, {"title": "4 Experimental Results", "content": "We compared CILO's results against three key related methods. Behavioral Cloning from Observations (BCO) [27], which is usually used as a baseline, and two of the most efficient LfO methods: Off-Policy Imitation Learning from Observations (OPOLO) [30], and Model-Based Imitation Learning From Observation Alone (MobILE) [13]. We experimented with five commonly used environments: Ant, Half Cheetah, Hopper, Swimmer, and Pendulum. Each method was run for 50 episodes, with the environment reset when the"}, {"title": "4.1 Implementation and Metrics", "content": "We used PyTorch to implement our agent and optimise the loss functions in Eq. 1-3 via Adam [15] and Imitation Datasets [9] to collect the expert data. As for the exploration mechanism in Eq. 5, we use p = 1 for \\( \\varepsilon \\) due to all environments actions being in the interval [-1, 1], and using \\( p > 1 \\) would significantly diminish the gap between predicted and ground-truth actions (as defined in Definition 1). In the supplementary material, we provide all learning rates and discuss hyperparameter sensitivity in more detail, but we note that CILO is not very sensitive to precise hyperparameters.\nWe evaluated all approaches using the Average Episodic Reward (AER) metric (Eq. 8) and use Performance (P) (Eq. 9). AER is the average accumulated reward for a policy \\( \\pi \\) over n number of episodes in t number of steps:\n\\begin{equation}\nAER(\\pi) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{t} r(\\mathbf{s}_{ij}, a(\\mathbf{s}_{ij})).\n\\end{equation}\nOn the other hand, P normalises between random and expert policies rewards, where performance 0 corresponds to random policy \\( \\pi_{\\varepsilon} \\) performance, and 1 is for expert policy \\( \\pi_{\\psi} \\) performance.\n\\begin{equation}\nP(\\pi) = \\frac{AER(\\pi) - AER(\\pi_{\\varepsilon})}{AER(\\pi_{\\psi}) - AER(\\pi_{\\varepsilon})}\n\\end{equation}\nNote that a negative value for P indicates a reward for the agent lower than a random agent's and a value higher than 1 indicates that the agent's reward is higher than the expert's. All results in Table 1 are the average and standard deviation in five different experiments. We do not report accuracy since achieving high accuracy does not necessarily translate into a high reward for the agent."}, {"title": "4.2 Results", "content": "We trained all methods using 10 expert trajectories. Table 1 shows how each method performed in the five environments. CILO had the best overall results in all environments. It consistently achieved results similar to the expert, surpassing it on Ant and Swimmer and achieving the maximum reward for the Pendulum environment. CILO's performance was close to the expert's in Hopper but a little lower in HalfCheetah \u2013 likely due to the higher standard deviation from the ground-truth actions in both environments. In the Swimmer environment, BCO and OPOLO achieved AER and performance similar to the expert, while CILO outperformed it by 0.29 points. The same happened in the Ant environment, where CILO surpassed the expert by \\( \\approx 484 \\) reward points. We hypothesise this is due to CILO's explorative nature and its ability to acquire new samples that the discriminator judges to come from the expert.\nComparing CILO to other methods, we see that OPOLO had the closest performance to CILO's in almost all environments. We attribute CILO's better performance than OPOLO's due to the fact that OPOLO's problem formulation assumes that the environment follows an injective MDP, which cannot be guaranteed with random seeds. For this work, we believe that it is more important for an agent to be able to correct its initial states into a successful trajectory than to be optimal in a single setting. Moreover, we notice that for the Pendulum environment, OPOLO only achieved the optimal reward when clipping the actions between [-1,1], which CILO does not require."}, {"title": "5 Discussion", "content": "In this section, we consider some key aspects of CILO's behaviour: (i) how CILO learns with different sample amounts; (ii) how it approximates predictions to the ground-truth actions of the expert; (iii) how similar each signature becomes to all trajectories over time; (iv) how different action distributions affect CILO; and (v) how I behaves over time."}, {"title": "5.1 Sample Efficiency", "content": "In order to understand CILO's sample efficiency, we experimented with three different amounts of expert episodes in the Ant environment. Ant provides an ideal setting due to its balanced learning complexity and shorter training times. Table 2 shows the AER and P results using 1, 10, and 100 trajectories. As expected, CILO does not achieve good results when using a single trajectory. This is because \\( \\pi_\\theta \\) has no information regarding different initialisation and trajectory deviations. This behaviour is intrinsic to behavioural cloning where, without sufficient information, the policy tends not to generalise [16]. Interestingly, CILO achieves 65 fewer reward points when using 100 trajectories than when it uses 10. We attribute this to the following: (i) when used in a LfO scenario, BC methods usually fail to scale according to the number of samples due to compounding error [26]; and (ii) increasing the number of expert samples decreases the deviation from \\( \\pi_\\psi \\) trajectories, resulting in overfitting and a worse \\( \\pi_\\theta \\).\nSince it achieves expert results for almost all environments, we do not consider this behaviour a limitation of CILO. Nevertheless, we hypothesise that using different strategies might result in an increase in performance when its data pool is increased. We also hypothesise that using incomplete or faulty trajectories might help CILO since it would not have so much data for all points in a trajectory, reducing overfit. Fine-tuning the exact number of expert trajectories requires some experimentation for each environment."}, {"title": "5.2 Ground-truth error over time", "content": "A concern for self-supervised IL methods is how to approximate pseudo-labels to ground-truth actions from the expert. However, approximating I samples to those from the experts is not always best. There might be samples that are between the experts' and Ipre that can help M smoothly close the gap between equally distributed and the ground-truth distribution [10]. Since CILO uses exploration to learn and this exploration mechanism relies on M's error, achieving lower error margins early might lead to less exploration and poorer results. It would therefore be better to have a consistent stream of new samples, to maintain the error marginally high but not a significant number of new samples since this could keep M's error too high or even collapse the network, i.e., updating all weights drastically and requiring a higher learning rate.\nTable 3 shows that using two different procedures and achieving two different policies with different error margins and weights yields"}, {"title": "5.3 Signature approximation over time", "content": "Given Definition 2, trajectories that are similar should be closer in the feature space, while those that do not share any states should be farther apart. Figure 2c shows the Manhattan distance between \\( \\pi_\\theta \\) and \\( \\pi \\) trajectories during the first 200 iterations. The difference is normalised between trajectories from random and expert agents. Hence, a difference greater than 1 means that the agent's signature path is farther from \\( \\pi_\\psi \\) than a random agent's. As expected, during early iterations, CILO produces episodes that are farther than the random agent since M has to learn state transitions before \\( \\pi_\\theta \\) can learn how to behave in the environment. We see similar behaviour from the discriminator D. In the initial iterations, it allows multiple trajectories to be appended to I due to its poor performance in discriminating between generated and expert trajectories. Once D learns to classify correctly, it is only 'fooled' by \\( \\approx 18\\% \\) of trajectories.\nAs \\( \\pi_\\theta \\) increases its performance, the distance between \\( \\pi_\\theta \\) and \\( \\pi_\\psi \\) signatures decreases. Similarly, D has a harder time distinguishing from expert and \\( \\pi_\\theta \\). We observe that D's results are as expected. By allowing these early trajectories to append into I, which had not achieved any goals, it allows M to learn from samples outside its randomly distributed ones. Since it only allows a few samples, M does not stop to predict actions due to skewed samples. But as D improves its classification performance, it forces \\( \\pi_\\theta \\) indirectly to be closer to the expert behaviour, therefore, achieving higher rewards. Using the gradient signal from D is likely to improve \\( \\pi_\\theta \\)'s performance further, but this adaptation would require the policy also to predict the next state, e.g., in a mechanism similar to the one used in Edwards et al.'s work [5]."}, {"title": "5.4 Effects of Gaussian exploration", "content": "Since we observed that CILO has a different behaviour for environments with different action distributions, we analyse Ant and HalfCheetah to understand the disparities between \\( \\pi_\\theta \\) and \\( \\pi_\\psi \\) action predictions. Figure 3 displays all distributions for 50 trajectories from the expert and trained policies. Note that for these actions, \\( \\pi_\\theta \\) is not using its exploration mechanism, that is, the policy is greedy. In all environments, the distribution from \\( \\pi_\\theta \\) actions differs from the expert ones. However, we observe that \\( \\pi_\\theta \\) actions have a higher intra-cluster variance than the expert ones. We believe this behaviour is due to CILO's exploration mechanism sampling from a Gaussian distribution, making it learn to have a higher variance around the average of an action (considering the error rate from Table 3). Therefore, the exploration mechanism makes it difficult to approximate distributions that do not follow this pattern, such as HalfCheetah.\nWe also note that CILO has more difficulty achieving better results in environments with sparse action distributions. If we compare Figures 3c and 3d, it is evident that CILO achieves actions near both limits, i.e., -1 and 1; however, it has a harder time predicting actions near the limit. In contrast, although both distributions from Figures 3a and 3b are unequal, we observe a more concentrated action cluster around 0, which helps \\( \\pi_\\theta \\) achieve better results. We see this behaviour as a limitation of CILO since selecting a new sampling distribution would require knowing beforehand how an expert behaves. However, we also hypothesise that training for a period without exploration and fine-tuning \\( \\pi_\\theta \\) with M's pseudo-labels would minimise this impact. Further training \\( \\pi_\\theta \\) with no exploration, we observe an increase in all environments, although not significantly."}, {"title": "5.5 I size over time", "content": "The use of the discriminator D allows CILO to start with fewer random samples since it appends samples on almost every iteration. However, increasing I on each epoch can create issues if the number of samples grows exponentially. Therefore, we plot in Figure 4 the size of I for each environment and epoch for the first 450 epochs. It is important to note that CILO usually reaches expert performance before its first 100 epochs. We observe that for most environments, CILO has a lower slope for appending IPos into its dataset. This behaviour is excellent since it means that CILO is less likely to create data pool sizes that would transform it to be inefficient. Furthermore, when we consider that in Torabi et al.'s work [27], \\( 5 \\times 10^5 \\) transitions are needed to learn the inverse dynamic model (\\( \\approx 50 \\) epochs), this behaviour allows for less preparation when learning an agent. Nevertheless, Figure 4 also present two other behaviours.\nFor the InversePendulum environment, CILO gets almost no sample variation when compared to the other methods in the early stages. However, after approximately 150 epochs, \\( \\pi_\\theta \\) yields trajectories"}, {"title": "6 Related Work", "content": "The simplest form of imitation learning from observation is Behavioral Cloning (BC) [20], which treats imitation learning as a supervised problem. It uses samples \\( (s_t, a, s_{t+1}) \\) from an expert consisting of a state, action and subsequent state to learn how to approximate the agent's trajectory to the expert's. However, such an approach becomes costly for more complex scenarios, requiring more samples and information about the action effects on the environment. For example, solving Atari requires approximately 100 times more samples than CartPole. Generative Adversarial Imitation Learning (GAIL) [11] solves this issue by matching the state-action frequencies from the agent to those seen in the demonstrations, creating a policy with action distributions that are closer to the expert. GAIL uses adversarial training to discriminate state-actions either from the agent or the expert while minimising the difference between both.\nRecent self-supervised approaches [27, 7] that learn from observations use the expert's transitions \\( (s_t, s_{t+1}) \\) and leverage random transitions \\( (s_t, a, s_{t+1}) \\) to learn the inverse dynamics of the environment, and afterwards generate pseudo-labels for the expert's trajectories. Imitating Latent Policies from Observation (ILPO) [5] differs"}, {"title": "7 Conclusions and Future Work", "content": "In this paper, we proposed Continuous Imitation Learning from Observation (CILO), a new LfO method combining an exploration mechanism and path signatures. CILO (i) does not require prior domain knowledge or information about the expert's actions; (ii) has sample efficiency superior or equal to the state-of-the-art LfO alternatives; and (iii) approximates (sometimes surpassing) expert performance. CILO achieves these results due to two key contributions. Firstly, the use of a discriminator paired with path signatures, allows CILO to acquire more diverse state transition samples while increasing sample quality. Secondly, the exploration mechanism, which uses the model's error rate to sample from a normal distribution, allows for a more dynamic exploration of the environment. As a result, the exploration ratio decreases as the model learns to approximate from the ground-truth labels. More importantly, these two innovations are completely model-agnostic, allowing them to be used in other IL methods without requiring major changes. We would argue that the innovations we proposed pave the way for IL models that generalise better and require less expert training data.\nOur next step is to investigate different exploration mechanisms to better fit the policy needs of specific environments. We would also like to experiment with different forms of adversarial learning to embed CILO's current discriminator into the policy loss function. Considering the path signatures are differentiable, it would be possible to backpropagate the gradients from the discriminator into the policy. This change would allow us to see if a direct signal from the enhanced loss function could improve the action prediction of the inverse dynamic model."}, {"title": "A Environments and samples", "content": "In this work, we experiment with five different environments. We now briefly describe each environment and how the expert samples were gathered. We used Stable Baselines 3 [22] coupled with RL Zoo3 [21] to gather expert samples and its weights loaded from HuggingFaces. We believe this will facilitate reproducibility by allowing future work to use the exact same experts. All expert results are displayed in Table 1 in Section 4.2 of the paper. We used a random sample pool of 50,000 states for all environments (partitioned into 35,000 states for training and the remaining 15, 000 for validation). It is important to note, that in each environment, a dimension d of these state vectors represents an internal attribute of the robot. Therefore, although they might share a similar number of dimensions, they may carry different meanings. Since I grows in size in each iteration, unlike Torabi et al.'s work [27], CILO does not rely on higher sample pools. Figure 5 shows a frame for each environment."}, {"title": "A.1 A note about the expert samples", "content": "During our experiments, we observed that not all experts are created equally. Although most experts trained or loaded from HuggingFace share similar results, the behaviour of each expert varies drastically. One could argue that humans also deviate for each trajectory, but using episodes with a more human-like trajectory (less hectic) yielded better results for all IL approaches. By presenting less hectic and more constant movements, we think each policy receives trajectories that vary more and generalise better. All samples used in this work are available in https://github.com/NathanGavenski/CILO."}, {"title": "A.2 Ant-v2", "content": "Ant-v2 consists of a robot ant made out of a torso with four legs attached to it, with each leg having two joints [23]. The goal of this environment is to coordinate the four legs to move the ant to the right of the screen by applying force on the eight joints. Ant-v2 requires eight actions per step, each limited to continuous values between -1 and 1. Its observation space consists of 27 attributes for the x, y and z axis of the 3D robot. We use Stable Baselines 3's TD3 weights. The expert sample contains 10 trajectories, each with 1,000 states consisting of 111 attributes. Ant-v2 shares distribution behaviour with InvertedPendulum-v2, and Hopper-v2, having action spaced in a bell-curve."}, {"title": "A.3 InvertedPendulum-v2", "content": "This environment is based on the CartPole environment from Barto et al. [1]. It involves a cart that can move linearly, with a pole attached to it. The agent can push the cart left or right to balance the pole by applying forces on the cart. The goal of the environment is to prevent the pole from reaching a particular angle on either side. The continuous action space varies between -3 and 3, the only one within the five environments outside of the 1 to 1 limit. Its observation space consists of 4 different attributes. We use Stable Baselines 3's PPO weights. The expert sample size is 10 trajectories, which consist of 10,000 states (with their 4 attributes) and actions (with a single action value per step). The invertedPendulum-v2 environment is the only one that has an expert with the environment's maximum reward. Therefore achieving P higher than 1 is impossible."}, {"title": "A.4 Swimmer-v2", "content": "This environment was proposed by [3]. It consists of a robot with s segments (s \u2265 3) and j = s 1 joints. Following [30], in our experiments we use the default setting s = 3 and j = 2. The agent applies force to the robot's joints, and each action can range from [-1,1] \u2208 R. A state is encoded by an 8-dimensional vector representing the angle, velocity and angular velocity of all segments. Swimmer distributions present the same distribution of HalfCheetah-v2 (centred around the lower and upper limits). We used Stable Baselines 3's TD3 weights. The expert sample contains 4 trajectories, with 1,000 states each plus actions for the j = 2 joints. The goal of the agent in this environment is to move as fast as possible towards the right by applying torque on the joints and using the fluid's friction."}, {"title": "A.5 Hopper-v2", "content": "Hopper-v2 is based on the work done by [6", "joints": "a torso at the top, a thigh in the middle, a leg at the bottom, and a single foot facing the right. The environment's goal is to make the robot hop and move forward (continuing on the right trajectory). A state consists of 1"}]}