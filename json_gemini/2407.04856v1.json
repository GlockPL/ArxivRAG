{"title": "Explorative Imitation Learning: A Path Signature Approach for Continuous Environments", "authors": ["Nathan Gavenski", "Juarez Monteiro", "Felipe Meneguzzi", "Michael Luck", "Odinaldo Rodrigues"], "abstract": "Some imitation learning methods combine behavioural cloning with self-supervision to infer actions from state pairs. However, most rely on a large number of expert trajectories to increase generalisation and human intervention to capture key aspects of the problem, such as domain constraints. In this paper, we propose Continuous Imitation Learning from Observation (CILO), a new method augmenting imitation learning with two important features: (i) exploration, allowing for more diverse state transitions, requiring less expert trajectories and resulting in fewer training iterations; and (ii) path signatures, allowing for automatic encoding of constraints, through the creation of non-parametric representations of agents and expert trajectories. We compared CILO with a baseline and two leading imitation learning methods in five environments. It had the best overall performance of all methods in all environments, outperforming the expert in two of them.", "sections": [{"title": "Introduction", "content": "One of the most common forms of learning is by watching someone else perform a task and, afterwards, trying it ourselves. As humans, we can observe an action being performed and transfer the acquired knowledge into our reality. In this respect, it is less challenging to achieve a goal in an optimal way by observing how an expert behaves; in the field of computer science, this is Imitation Learning (IL). Unlike conventional reinforcement learning, which depends on a reward function, IL learns from expert guidance, and is concerned with an agent's acquisition of skills or behaviours by observing a 'teacher' perform a given task.\nLearning from demonstration is the obvious approach for IL, requiring expert demonstrations, which are 'trajectories' including actions performed along the way to goal completion [12]. Such an approach uses the trajectories to learn an approximate policy that behaves like the expert. Learning from demonstration suffers from two significant drawbacks in practice: poor generalisation in environments with multiple alternative trajectories that achieve a goal, which is bound to occur when the dataset size increases, and the unavailability of data about the expert's actions. Learning from observation (LfO) overcomes these limitations by learning a task without direct action information via self-supervision, which increases generalisation [8]. This allows a model to learn from sample executions without action information, which would otherwise be unusable. LfO approaches often rely on techniques from classification to improve sample-efficiency [30] and generalisation [18]. Such agents require fewer expert trajectories, yielding more general approaches that are, hence, adaptable to unseen scenarios. However, these methods still fail to leverage some useful learning features, particularly the use of an exploration mechanism.\nSome existing work [5, 7] requires manual intervention in different stages of the process, e.g., the hard-coding of environment goals, which is not feasible in complex environments, such as robotic systems with multifaceted goals. Other work [7, 13, 30] is limited in that learning the environment dynamics depends strongly on previously collected samples that usually do not relate to how the environment dynamics operate under expert behaviour, such as random transitions, or prior knowledge of the dynamics of environments. In addition, maintaining self-supervision [7, 13] for an IL method is important since unlabelled data is more readily available, e.g., from sources that are not necessarily meant for agent learning.\nIn this paper, we propose a novel LfO approach to IL called Continuous Imitation Learning from Observation (CILO) that addresses the above issues. CILO (i) eliminates the need for manual intervention when using different environments by discriminating between policy and expert; (ii) requires fewer samples for learning by leveraging exploration and exploitation; and (iii) does not require expert-labelled data, thus remaining self-supervised. We evaluated CILO in five widely used continuous environments against a baseline and two leading LfO methods (see Section 4). Our results show that CILO outperformed all of the alternatives, surpassing the expert in two of five environments.\nCILO's new mechanisms are model-agnostic and applicable to a wider range of environment dynamics than those of the compared LfO alternatives. We argue that the new mechanisms can be readily incorporated into other IL methods, paving the way for more robust and flexible learning techniques."}, {"title": "Problem Formulation", "content": "We assume the environment to be an MDP \\(M = (S, A, T, r, \\gamma)\\), in which S is the state space, A is the action space, T is a transition model, r is the immediate reward function, and \\(\\gamma\\) is the discount factor [25]. Although in general an MDP may carry information regarding the reward and discount factors, we consider that this information is inaccessible to the agent during training, and the learning process does not depend on it. Solving an MDP yields a policy \\(\\pi\\) with a probability distribution over actions, giving the probability of taking an action a in state s. We denote the expert policy by \\(\\pi_\\psi\\).\nA common self-supervised approach to solving a task via IL uses an Inverse Dynamic Model \\(M_\\theta\\). \\(M_\\theta\\) uses a set of state transition samples \\((s_t, s_{t+1})\\) to predict the action performed in the transitions. By training \\(M_\\theta\\) to infer the actions in the state transitions, these approaches can automatically annotate all expert trajectories \\(T_{\\pi_\\psi}\\) with actions without the need for human intervention [27, 18, 7]. The agent policy \\(\\pi_\\theta\\) then uses these self-supervised expert-labelled states \\((s^\\pi, a)\\) to learn to predict the most likely action given a state \\(P(a | s^\\pi)\\). Torabi et al. [27] show that applying an iterative process in self-supervised IL approaches helps \\(\\pi_\\theta\\) achieve better performance. Initially, \\(M_\\theta\\) uses only single transition samples \\(I^{pre}\\) from \\(\\pi_\\theta\\) and its randomly initialised weights. At each iteration, these approaches use \\(\\pi_\\theta\\) to create new samples \\(I^{pos}\\) that are used to fine-tune \\(M_\\theta\\). However, using all transitions from \\(\\pi_\\theta\\) makes this iterative approach susceptible to getting stuck in local minima due to class imbalance from the \\(I^{pos}\\) data. Monteiro et al. [18] propose a solution that introduces a goal-aware function to sample from all trajectories at each epoch. This function does not require an aligned goal from the environment, hence it is up to the user to choose a desired goal. If \\(\\pi_\\theta\\) reaches this goal, the trajectory will be used. Finally, it is sensible to assume that \\(M_\\theta\\) is not well-tuned during early iterations and predicts mostly wrong labels. Therefore, Gavenski et al. [7] implement an exploration mechanism that uses the softmax distribution of the output as weights to sample actions proportionally to optimality from the model's prediction. As the confidence in the model increases, it predicts suboptimal actions less than the maximum a posteriori estimation. By exploring using the model's confidence, their approach can learn under the exploration and exploitation phases, helping \\(M_\\theta\\) to converge faster. Nevertheless, creating a handcrafted goal-aware function and using a softmax distribution as an exploration mechanism requires manual intervention and discrete actions. As a result, these methods become unsuitable for more complex environments where goal achievement is non-trivial to check."}, {"title": "Continuous Imitation Learning from Observation", "content": "We address the need for manual intervention and for maintaining self-supervision in CILO through two key innovations: an exploration mechanism used when the action predictions are uncertain; and a discriminator to interleave random and current states to improve the prediction of self-supervised actions. CILO achieves this by employing three different models: (i) the inverse dynamic model \\(M_\\theta\\) to predict the action responsible for a transition between two states \\(P(s_t, s_{t+1})\\); (ii) a policy model \\(\\pi_\\theta\\) that uses the self-supervised labels \\(\\hat{a}\\) to imitate the expert \\(\\pi_\\psi\\) given a state \\(P(a | s_t)\\); and (iii) a discriminator model D to discriminate between \\(\\pi_\\psi\\) and \\(\\pi_\\theta\\), creating newer samples for \\(M_\\theta\\).\nFirst, CILO initialises all models with random weights and uses the random initialised policy to collect random samples \\(I^{pre}\\) from the environment (Lines 1-2). The dynamics model uses these random samples to train in a supervised manner (Function TRAINM, Line 4). These samples are vital since they help \\(M_\\theta\\) learn how actions cause environmental transitions without expert behaviour-specific knowledge. TRAINM uses the loss from Eq. 1, where \\(\\theta\\) are the model's current parameters, S is the vector for state representations, A is the action vector representation, and t is the timestep.\n\\[\\mathcal{L}_{M}(I_S, \\theta) = \\sum_{t=1}^{|I_S|} |M_\\theta(s_t, s_{t+1}) - a_t| \\qquad (1)\\]\nWith the updated parameters \\(\\theta\\), \\(M_\\theta\\) predicts the self-supervised labels \\(\\hat{a}\\) to all expert transitions (\\(T^{\\pi_\\psi}\\) in Line 5). CILO then uses these expert labelled transitions to train \\(\\pi_\\theta\\) using behaviour cloning (Function BEHAVIOURALCLONING with Eq. 2) coupled with an exploration mechanism (Line 6).\n\\[\\mathcal{L}_{BC}(I) = \\sum_{(s_t, s_{t+1}) \\in I_S} |M_\\theta(s_t, s_{t+1}) - \\pi_\\theta(s_t)| \\qquad (2)\\]\nThe policy then generates new samples (\\(T^{\\pi_\\theta}\\)) that might help \\(M_\\theta\\) approximate the unknown ground-truth actions from the expert (Lines 7-8). Given all new samples, CILO generates path signatures \\(\\beta\\) [2] and uses D to classify signatures as from the expert or the agent. Line 9 updates the discriminator weights with the classification loss in Eq. 3, where \\(T_\\beta^E\\) are path signatures for all trajectories from expert and agent, C is for the source of the observation (expert and agent), y is the ground-truth label, and \\(\\hat{y}\\) is the source predicted by D.\n\\[\\mathcal{L}_D(T_\\beta^C) = - \\sum_{i=1}^{|T_\\beta|} \\sum_{j=1}^C y_{ij}\\log(\\hat{y}_{ij}), \\qquad (3)\\]\nSamples classified as expert by D are added to \\(I^{Pos}\\) (Line 9), which is then combined with the original \\(I^{pre}\\) to form \\(I_S\\) (Line 10). CILO uses this updated \\(I_S\\) in each iteration for a specified number of epochs (Line 4) or until it no longer improves (Lines 11-12), with an optional hyperparameter threshold.\nThe exploration mechanism allows CILO to deviate from its original action distribution according to the model certainty. This behaviour is helpful during early iterations when \\(M_\\theta\\) is unsure about which action might be responsible for a specific transition. Since random samples can be very different from the expert's transitions, we can assume that the model does not learn to recognise these transitions and generalises poorly. Here, we assume that the environment is stochastic, in that multiple actions might occur with a non-zero probability of transition between any pair of states. D's key objective is to discard trajectories that could result in \\(M_\\theta\\) getting stuck in bad local minima, and for instance, stop predicting specific actions (underfitting). Without a discriminator, it would be difficult to ignore signatures that differ considerably from the ground-truth without an environment-specific hyperparameter, hence reducing the method's generalisability. Finally, combining these mechanisms makes CILO more sample efficient, allowing for oversampling without misrepresenting the action distributions and overfitting. Figure 1 shows the CILO learning cycle in more detail with the different loss functions."}, {"title": "Exploration", "content": "Exploration is vital for IL methods that use dynamics models to learn how the expert behaves. It enables policy divergence when the dynamics model is uncertain and increases state diversity, which helps the model approximate labelled transitions from unlabelled ones (expert). CILO borrows an exploration mechanism from reinforcement learning in continuous domains, in which each action in a policy consists of two outputs: the mean and standard deviation to sample from a Gaussian distribution. However, unlike traditional reinforcement learning, where a policy receives feedback in the form of the reward function, IL lacks this information. Thus, for a model \\(M_\\theta\\) and parameters \\(\\theta\\) (\\(M_{\\theta^\\prime}\\)), we employ the sampling mechanism in Eq. 4, where \\(\\pi\\) is the usual mathematical constant 3.14... and \\(\\varepsilon\\), as defined in Eq. 5, is used as standard deviation, where a is the ground-truth action (or pseudo-labels from \\(M_\\theta\\)) and \\(\\hat{a}\\) is the action predicted by the model:\n\\[\\tilde{a}_{M_\\theta} = \\frac{1}{\\varepsilon \\sqrt{2\\pi}} e^{-\\frac{(a - M_{\\theta}(s))^2}{2\\varepsilon^2}} \\qquad (4)\\]\n\\[\\varepsilon = ||a - \\hat{a}||^p \\qquad (5)\\]\nIn Eq. 4, M is either \\(M_\\theta\\) or \\(\\pi_\\theta\\), and \\(\\theta\\) are the parameters of the model updated for the epoch. Notice that when p = 1, the model \\(M_\\theta\\) uses the absolute value between the predicted and ground-truth labels \\(||a - \\hat{a}||\\) and this allows for higher exploration.\nIf \\(\\mathcal{L}\\) is a loss function that monotonically decreases a model's \\(M_\\theta\\) error as it approximates the ground-truth function, eventually \\(||a - \\hat{a}|| < 1\\). If we then use \\(p > 1\\) in Eq. 5, \\(\\varepsilon\\) will exponentially decrease.\nGiven all of the above, Eq. 5 offers a trade-off between exploration and exploitation. Since \\(\\varepsilon\\) is the standard deviation for the exploration function, as the model's predictions get closer to the ground-truth and pseudo-labels, the clusters will have lower variance because the exploration ratio is directly correlated to the model's error.\nIn Alg. 1, functions TRAINM (ln. 4) and BEHAVIOURCLONING (ln. 6) use this adaptation to adjust the exploration ratio depending on how close the model's predictions are to the ground-truth (or pseudo-labels), in accordance with the standard deviation of the Gaussian distribution. This mechanism also has the benefit of not having to predict information beyond the agent's actions, such as standard deviation, instead obtaining this directly from the model's error. For deterministic behaviour, we can assume that the standard deviation for the model is 0 and use the model's output since sampling from a Gaussian distribution with average x and deviation 0 equals x."}, {"title": "Goal-aware function", "content": "Developing a goal-aware function may not be a trivial task. For environments with a well-defined goal", "as": "nHowever", "be": "n\\[\\beta^{i_1"}, {"title": "Sample efficiency", "content": "Besides approximating the expert policy, IL methods focus on efficiently using expert samples. This focus happens since expert samples are hard to obtain. Thus, creating more efficient methods, i.e., that require fewer samples, allows for more useable approaches. Some recent strategies [13, 30] minimise the number of required samples but depend on strong assumptions (see Section 4.2) or manual intervention for each new environment. For comparison, CILO uses 10 expert episodes \u2013 a number similar to Zhu et al. [30] and Kidambi et al. [13], but without requiring manual intervention for each environment. CILO relies on up-scaling \\(T^{\\pi_\\psi}\\) to increase the number of observations \\(\\pi_\\theta\\) sees before interacting with the environment. Although trivial, this strategy works because CILO is self-supervised and has an exploration mechanism. This strategy helps in two ways: (i) for each epoch all pseudo-labels differ in all transitions due to the exploration mechanism (Line 4, Algorithm 1); and (ii) increasing the number of samples \\(\\pi_\\theta\\) receives allows for more updates before sampling new experiences from the environment. By applying its exploration mechanism to each observation individually and sampling exploration values from a distribution, CILO ensures that each observation has unique action values, reducing the risk of misrepresenting the ground-truth action distribution."}, {"title": "Experimental Results", "content": "We compared CILO's results against three key related methods. Behavioral Cloning from Observations (BCO) [27], which is usually used as a baseline, and two of the most efficient LfO methods: Off-Policy Imitation Learning from Observations (OPOLO) [30], and Model-Based Imitation Learning From Observation Alone (MobILE) [13]. We experimented with five commonly used environments: Ant, Half Cheetah, Hopper, Swimmer, and Pendulum. Each method was run for 50 episodes, with the environment reset when the agent falls or after 1,000 steps. Each episode was run using random seeds to test the agent's ability to generalise."}, {"title": "Implementation and Metrics", "content": "We used PyTorch to implement our agent and optimise the loss functions in Eq. 1-3 via Adam [15] and Imitation Datasets [9] to collect the expert data. As for the exploration mechanism in Eq. 5, we use p = 1 for \\(\\varepsilon\\) due to all environments actions being in the interval [-1, 1], and using p > 1 would significantly diminish the gap between predicted and ground-truth actions (as defined in Definition 1). In the supplementary material, we provide all learning rates and discuss hyperparameter sensitivity in more detail, but we note that CILO is not very sensitive to precise hyperparameters.\nWe evaluated all approaches using the Average Episodic Reward (AER) metric (Eq. 8) and use Performance (P) (Eq. 9). AER is the average accumulated reward for a policy \\(\\pi\\) over n number of episodes in t number of steps:\n\\[AER(\\pi) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{t} r(s_{ij}, a(s_{ij})). \\qquad (8)\\]\nOn the other hand, P normalises between random and expert policies rewards, where performance 0 corresponds to random policy \\(\\pi_\\varepsilon\\) performance, and 1 is for expert policy \\(\\pi_\\psi\\) performance.\n\\[P(\\pi) = \\frac{AER(\\pi) - AER(\\pi_\\varepsilon)}{AER(\\pi_\\psi) - AER(\\pi_\\varepsilon)} \\qquad (9)\\]\nNote that a negative value for P indicates a reward for the agent lower than a random agent's and a value higher than 1 indicates that the agent's reward is higher than the expert's. All results in Table 1 are the average and standard deviation in five different experiments. We do not report accuracy since achieving high accuracy does not necessarily translate into a high reward for the agent."}, {"title": "Results", "content": "We trained all methods using 10 expert trajectories. Table 1 shows how each method performed in the five environments. CILO had the best overall results in all environments. It consistently achieved results similar to the expert, surpassing it on Ant and Swimmer and achieving the maximum reward for the Pendulum environment. CILO's performance was close to the expert's in Hopper but a little lower in HalfCheetah \u2013 likely due to the higher standard deviation from the ground-truth actions in both environments. In the Swimmer environment, BCO and OPOLO achieved AER and performance similar to the expert, while CILO outperformed it by 0.29 points. The same happened in the Ant environment, where CILO surpassed the expert by \u2248 484 reward points. We hypothesise this is due to CILO's explorative nature and its ability to acquire new samples that the discriminator judges to come from the expert.\nComparing CILO to other methods, we see that OPOLO had the closest performance to CILO's in almost all environments. We attribute CILO's better performance than OPOLO's due to the fact that OPOLO's problem formulation assumes that the environment follows an injective MDP, which cannot be guaranteed with random seeds. For this work, we believe that it is more important for an agent to be able to correct its initial states into a successful trajectory than to be optimal in a single setting. Moreover, we notice that for the Pen-dulum environment, OPOLO only achieved the optimal reward when clipping the actions between [-1,1], which CILO does not require."}, {"title": "Sample Efficiency", "content": "In order to understand CILO's sample efficiency, we experimented with three different amounts of expert episodes in the Ant environment. Ant provides an ideal setting due to its balanced learning complexity and shorter training times. Table 2 shows the AER and P results using 1, 10, and 100 trajectories. As expected, CILO does not achieve good results when using a single trajectory. This is because \\(\\pi_\\theta\\) has no information regarding different initialisation and trajectory deviations. This behaviour is intrinsic to behavioural cloning where, without sufficient information, the policy tends not to generalise [16]. Interestingly, CILO achieves 65 fewer reward points when using 100 trajectories than when it uses 10. We attribute this to the following: (i) when used in a LfO scenario, BC methods usually fail to scale according to the number of samples due to compounding error [26]; and (ii) increasing the number of expert samples decreases the deviation from \\(\\pi_\\psi\\) trajectories, resulting in overfitting and a worse \\(\\pi_\\theta\\). Since it achieves expert results for almost all environments, we do not consider this behaviour a limitation of CILO. Nevertheless, we hypothesise that using different strategies might result in an increase in performance when its data pool is increased. We also hypothesise that using incomplete or faulty trajectories might help CILO since it would not have so much data for all points in a trajectory, reducing overfit. Fine-tuning the exact number of expert trajectories requires some experimentation for each environment."}, {"title": "Ground-truth error over time", "content": "A concern for self-supervised IL methods is how to approximate pseudo-labels to ground-truth actions from the expert. However, approximating \\(I_S\\) samples to those from the experts is not always best. There might be samples that are between the experts' and \\(I^{pre}\\) that can help \\(M_\\theta\\) smoothly close the gap between equally distributed and the ground-truth distribution [10]. Since CILO uses exploration to learn and this exploration mechanism relies on \\(M_\\theta\\)'s error, achieving lower error margins early might lead to less exploration and poorer results. It would therefore be better to have a consistent stream of new samples, to maintain the error marginally high but not a significant number of new samples since this could keep \\(M_\\theta\\)'s error too high or even collapse the network, i.e., updating all weights drastically and requiring a higher learning rate.\nTable 3 shows that using two different procedures and achieving two different policies with different error margins and weights yields similar results for the error margins in both methods but not performance and AER. Figure 2a shows the learning results (error and performance) for \\(\\pi_\\theta\\), which uses weight decay and a scheduler during training, and Figure 2b shows \\(\\pi_\\theta\\), with no weight decay and a learning rate scheduler. We observe that both policies achieve a similarly consistent error margin. However, when comparing the average performances in a single episode, \\(\\pi_\\theta\\) achieves 29 more reward points with a lower variation. While using different strategies for classifying the action might help CILO with this behaviour, we use a similar topology to the one used by the models compared."}, {"title": "Signature approximation over time", "content": "Given Definition 2, trajectories that are similar should be closer in the feature space, while those that do not share any states should be farther apart. Figure 2c shows the Manhattan distance between \\(\\pi_\\theta\\) and \\(\\pi_\\psi\\) trajectories during the first 200 iterations. The difference is normalised between trajectories from random and expert agents. Hence, a difference greater than 1 means that the agent's signature path is farther from \\(\\pi_\\psi\\) than a random agent's. As expected, during early iterations, CILO produces episodes that are farther than the random agent since \\(M_\\theta\\) has to learn state transitions before \\(\\pi_\\theta\\) can learn how to behave in the environment. We see similar behaviour from the discriminator D. In the initial iterations, it allows multiple trajectories to be appended to \\(I_S\\) due to its poor performance in discriminating between generated and expert trajectories. Once D learns to classify correctly, it is only 'fooled' by \u2248 18% of trajectories.\nAs \\(\\pi_\\theta\\) increases its performance, the distance between \\(\\pi_\\theta\\) and \\(\\pi_\\psi\\) signatures decreases. Similarly, D has a harder time distinguishing from expert and \\(\\pi_\\theta\\). We observe that D's results are as expected. By allowing these early trajectories to append into \\(I_S\\), which had not achieved any goals, it allows \\(M_\\theta\\) to learn from samples outside its randomly distributed ones. Since it only allows a few samples, \\(M_\\theta\\) does not stop to predict actions due to skewed samples. But as D improves its classification performance, it forces \\(\\pi_\\theta\\) indirectly to be closer to the expert behaviour, therefore, achieving higher rewards. Using the gradient signal from D is likely to improve \\(\\pi_\\theta\\)'s performance further, but this adaptation would require the policy also to predict the next state, e.g., in a mechanism similar to the one used in Edwards et al.'s work [5]."}, {"title": "Effects of Gaussian exploration", "content": "Since we observed that CILO has a different behaviour for environments with different action distributions, we analyse Ant and HalfCheetah to understand the disparities between \\(\\pi_\\theta\\) and \\(\\pi_\\psi\\) action predictions. Figure 3 displays all distributions for 50 trajectories from the expert and trained policies. Note that for these actions, \\(\\pi_\\theta\\) is not using its exploration mechanism, that is, the policy is greedy. In all environments, the distribution from \\(\\pi_\\theta\\) actions differs from the expert ones. However, we observe that \\(\\pi_\\theta\\) actions have a higher intra-cluster variance than the expert ones. We believe this behaviour is due to CILO's exploration mechanism sampling from a Gaussian distribution, making it learn to have a higher variance around the average of an action (considering the error rate from Table 3). Therefore, the exploration mechanism makes it difficult to approximate distributions that do not follow this pattern, such as HalfCheetah.\nWe also note that CILO has more difficulty achieving better results in environments with sparse action distributions. If we compare Figures 3c and 3d, it is evident that CILO achieves actions near both limits, i.e., -1 and 1; however, it has a harder time predicting actions near the limit. In contrast, although both distributions from Figures 3a and 3b are unequal, we observe a more concentrated action cluster around 0, which helps \\(\\pi_\\theta\\) achieve better results. We see this behaviour as a limitation of CILO since selecting a new sampling distribution would require knowing beforehand how an expert behaves. However, we also hypothesise that training for a period without exploration and fine-tuning \\(\\pi_\\theta\\) with \\(M_\\theta\\)'s pseudo-labels would minimise this impact. Further training \\(\\pi_\\theta\\) with no exploration, we observe an increase in all environments, although not significantly."}, {"title": "\\(I_S\\) size over time", "content": "The use of the discriminator D allows CILO to start with fewer random samples since it appends samples on almost every iteration. However, increasing \\(I_S\\) on each epoch can create issues if the number of samples grows exponentially. Therefore, we plot in Figure 4 the size of \\(I_S\\) for each environment and epoch for the first 450 epochs. It is important to note that CILO usually reaches expert performance before its first 100 epochs. We observe that for most environments, CILO has a lower slope for appending \\(I^{Pos}\\) into its dataset. This behaviour is excellent since it means that CILO is less likely to create data pool sizes that would transform it to be inefficient. Furthermore, when we consider that in Torabi et al.'s work [27], 5 \u00d7 105 transitions are needed to learn the inverse dynamic model (\u2248 50 epochs), this behaviour allows for less preparation when learning an agent. Nevertheless, Figure 4 also present two other behaviours.\nFor the InversePendulum environment, CILO gets almost no sample variation when compared to the other methods in the early stages. However, after approximately 150 epochs, \\(\\pi_\\theta\\) yields trajectories more similar to the expert, which deteriorates D accuracy and increases \\(I_S\\) quite substantially. In this environment, we use this behaviour as a form of signal to stop since the reward does not improve. Figure 4 has an inset graph showing the first 150 epochs for the InvertedPendulum environment. In it, we observe during its first epochs, CILO appends samples in a lower rhythm.\nFor both HalfCheetah and Ant environments, we observe a linear pattern from the samples added into \\(I_S\\). This behaviour is not desired, resulting in a training procedure that takes around 3 and 1.5 times longer to finish than all the other environments for HalfCheetah and Ant. To mitigate this problem, CILO could implement a forgetting mechanism to get rid of some samples in each epoch either by random selection or using the chronological order of insertion. However, it should not keep its initial sample pool size, considering it has a smaller dataset and changing it could make \\(M_\\theta\\) susceptible to covariate shift. We hypothesise that adding samples up until an upper limit would be a better approach, eliminating samples from \\(I_S\\) in each epoch as needed to keep the pool size within the limit."}, {"title": "Related Work", "content": "The simplest form of imitation learning from observation is Behavioral Cloning (BC) [20], which treats imitation learning as a supervised problem. It uses samples \\((s_t, a, s_{t+1})\\) from an expert consisting of a state, action and subsequent state to learn how to approximate the agent's trajectory to the expert's. However, such an approach becomes costly for more complex scenarios, requiring more samples and information about the action effects on the environment. For example, solving Atari requires approximately 100 times more samples than CartPole. Generative Adversarial Imitation Learning (GAIL) [11] solves this issue by matching the state-action frequencies from the agent to those seen in the demonstrations, creating a policy with action distributions that are closer to the expert. GAIL uses adversarial training to discriminate state-actions either from the agent or the expert while minimising the difference between both.\nRecent self-supervised approaches [27, 7] that learn from observations use the expert's transitions \\((s_t, s_{t+1})\\) and leverage random transitions \\((s_t, a, s_{t+1})\\) to learn the inverse dynamics of the environment, and afterwards generate pseudo-labels for the expert's trajectories. Imitating Latent Policies from Observation (ILPO) [5] differs from such work by trying to estimate the probability of a latent action given a state. Within a limited number of environment steps, it remaps latent actions to corresponding ones. More recently, Off-Policy Learning from Observations (OPOLO) [30] uses a dual-form of the expectation function and an adversarial structure to achieve off-policy LfO. Model-Based Imitation Learning from Observation Alone (MobILE) [13] uses the same adversarial techniques, which rely on an objective discriminator coupled with exploration to diverge from its actions when far from the expert."}, {"title": "Conclusions and Future Work", "content": "In this paper, we proposed Continuous Imitation Learning from Observation (CILO), a new LfO method combining an exploration mechanism and path signatures. CILO (i) does not require prior domain knowledge or information about the expert's actions; (ii) has sample efficiency superior or equal to the state-of-the-art LfO alternatives; and (iii) approximates (sometimes surpassing) expert performance. CILO achieves these results due to two key contributions. Firstly, the use of a discriminator paired with path signatures, allows CILO to acquire more diverse state transition samples while increasing sample quality. Secondly, the exploration mechanism, which uses the model's error rate to sample from a normal distribution, allows for a more dynamic exploration of the environment. As a result, the exploration ratio decreases as the model learns to approximate from the ground-truth labels. More importantly, these two innovations are completely model-agnostic, allowing them to be used in other IL methods without requiring major changes. We would argue that the innovations we proposed pave the way for IL models that generalise better and require less expert training data.\nOur next step is to investigate different exploration mechanisms to better fit the policy needs of specific environments. We would also like to experiment with different forms of adversarial learning to embed CILO's current discriminator into the policy loss function. Considering the path signatures are differentiable, it would be possible to backpropagate the gradients from the discriminator into the policy. This change would allow us to see if a direct signal from the enhanced loss function could improve the action prediction of the inverse dynamic model."}, {"title": "Environments and samples", "content": "In this work, we experiment with five different environments. We now briefly describe each environment and how the expert samples were gathered. We used Stable Baselines 3 [22] coupled with RL Zoo3 [21] to gather expert samples and its weights loaded from HuggingFaces 4. We believe this will facilitate reproducibility by allowing future work to use the exact same experts. All expert results are displayed in Table 1 in Section 4.2 of the paper. We used a random sample pool of 50,000 states for all environments (partitioned into 35,000 states for training and the remaining 15, 000 for validation). It is important to note, that in each environment, a dimension d of these state vectors represents an internal attribute of the robot. Therefore, although they might share a similar number of dimensions, they may carry different meanings. Since \\(I_S\\) grows in size in each iteration, unlike Torabi et al.'s work [27], CILO does not rely on higher sample pools. Figure 5 shows a frame for each environment."}, {"title": "A note about the expert samples", "content": "During our experiments, we observed that not all experts are created equally. Although most experts trained or loaded from HuggingFace share similar results, the behaviour of each expert varies drastically. One could argue that humans also deviate for each trajectory, but using episodes with a more human-like trajectory (less hectic) yielded better results for all IL approaches. By presenting less hectic and more constant movements, we think each policy receives trajectories that vary more and generalise better. All samples used in this work are available in https://github.com/NathanGavenski/CILO."}, {"title": "Ant-v2", "content": "Ant-v2 consists of a robot ant made out of a torso with four legs attached to it, with each leg having two joints [23]. The goal of this environment is to coordinate the four legs to move the ant to the right of the screen by applying force on the eight joints. Ant-v2 requires eight actions per step, each limited to continuous values between -1 and 1. Its observation space consists of 27 attributes for the x, y and z axis of the 3D robot. We use Stable Baselines 3's TD3 weights. The expert sample contains 10 trajectories, each with 1,000 states consisting of 111 attributes. Ant-v2 shares distribution behaviour with InvertedPendulum-v2, and Hopper-v2, having action spaced in a bell-curve."}, {"title": "InvertedPendulum-v2", "content": "This environment is based on the CartPole environment from Barto et al. [1]. It involves a cart that can move linearly, with a pole attached to it. The agent can push the cart left or right to balance the pole by applying forces on the cart. The goal of the environment is to prevent the pole from reaching a particular angle on either side. The continuous action space varies between -3 and 3, the only one within the five environments outside of the 1 to 1 limit. Its observation space consists of 4 different attributes. We use Stable Baselines 3's PPO weights. The expert sample size is 10 trajectories, which consist of 10,000 states (with their 4 attributes) and actions (with a single action value per step). The invertedPendulum-v2 environment is the only one that has an expert with the environment's maximum reward. Therefore achieving P higher than 1 is impossible."}, {"title": "Swimmer-v2", "content": "This environment was proposed by [3]. It consists of a robot with s segments (s \u2265 3) and j = s 1 joints. Following [30], in our experiments we use the default setting s = 3 and j = 2. The agent applies force to the robot's joints, and each action can range from [-1,1] \u2208 R. A state is encoded by an 8-dimensional vector representing the angle, velocity and angular velocity of all segments. Swimmer distributions present the same distribution of HalfCheetah-v2 (centred around the lower and upper limits). We used Stable Baselines 3's TD3 weights. The expert sample contains 4 trajectories, with 1,000 states each plus actions for the j = 2 joints. The goal of the agent in this environment is to move as fast as possible towards the right by applying torque on the joints and using the fluid's friction."}, {"title": "Hopper-v2", "content": "Hopper-v2 is based on the work done by [6]. Its robot is a one-legged two-dimensional body with four main parts connected by three joints: a torso at the top, a thigh in the middle, a leg at the bottom, and a single foot facing the right. The environment's goal is to make the robot hop and move forward (continuing on the right trajectory). A state consists of 11 attributes representing the z-position, angle, velocity and angular velocity of the robot's three joints. We used Stable Baselines 3's TD3 weights and 10 expert episodes, each with 1,000 states and actions for the three joints. Each action is limited between [-1,1] \u2208 R."}, {"title": "HalfCheetah-v2", "content": "HalfCheetah-v2's environment was proposed in [28]. It has a 2-dimensional cheetah-like robot with two \"paws\". The robot contains 9 segments and 8 joints. Its actions are a vector of 6 dimensions, consisting of the torque applied to the joints to make the cheetah run forward (\"thigh\", \"shin\", and \"paw\" for the front and back parts of the body). All states consist of the robot's position and angles, velocities and angular velocities for its joints and segments. HalfCheetah-v2's goal is to run forward (i.e., to the right of the screen) as fast as possible. A positive reward is allocated based on the distance traversed, and a negative reward is awarded when moving to the left of the screen. We used Stable Baselines 3's TD3 weights. The expert sample size is 10 trajectories, each consisting of 1,000 states and actions. Each action is limited between the interval of [-1,1] \u2208 R."}, {"title": "Network Topology", "content": "We followed the same network topologies employed in the original works. Each model (\\(M_\\theta\\) and \\(\\pi_\\theta\\)) are MLP with 4 fully connected layers, each with 512 neurons, with the exception of the last layer whose size is the same as the number of environment actions, Table 4 displays the topologies alongside the input and output sizes of each layer. Following the implementation in [7], we used a self-attention module after the first and second layers. We experimented with normalisation layers during development, which did not increase the agents' results but helped with weight updates. Although we understand that having more complex architectures could increase our method's performance, for consistency we used the same original architecture to show that CILO achieves expert results and does not rely on the architecture. The implementation of our method can be found within https://github.com/NathanGavenski/CILO."}, {"title": "Training and Learning Rate", "content": "For training, we used a Nvidia A100 40GB GPU and PyTorch. Although we used this GPU, such hardware is not strictly required since CILO uses \u2248 2GB to train with a 1024 mini-batch size. The learning rates for \\(M_\\theta\\) and \\(\\pi_\\theta\\) are shown in Table 5. We note that CILO is robust to different learning rates for \\(\\pi_\\theta\\). However, \\(M_\\theta\\) is more sensitive since \\(I_S\\) changes at almost every iteration, assuming there is at least one agent's trajectory that D classifies as expert. Having a high learning rate can make \\(M_\\theta\\)'s weights update too harshly and result in CILO never learning how to label the \\(T^{\\pi_\\psi}\\) properly."}, {"title": "Computing the Path Signature", "content": "Given a trajectory and a function f that interpolates \\(\\tau\\) into a continuous map f: \\([1", "as": "n\\[\\int_1^n f(\\tau_t) dt = \\int_1^n \\sum_{i=1"}, 10, 1, 1, 1, 1, "tau", [1, "n] \\rightarrow \\mathbb{R}\\) is simply the increment of \\(\\tau\\):\n\\[\\int_1^n d\\tau_t = \\int_1^n 1 dt = \\tau_n - \\tau_1. \\qquad (11)\\]\nTherefore, by assuming that \\(\\beta\\) is a function of real-valued paths, we can define the signature for any single index \\(i_k \\in \\{1, \\ldots, d\\}\\) as:\n\\[\\beta^{i_k}_{1,n} = \\int_1^n d\\tau^{i_k}_t = \\int_{1\n    },\n    {\n      \"title\": \"Path Signatures\",\n      \"content\":", "In this work we rely on several path signature definitions to discriminate over agent and expert trajectories. In Section 3.2 of our paper, we briefly defined a trajectory \\(\\tau\\), in which each state is a vector in \\(\\mathbb{R}^d\\), and how to compute the path signature \\(\\beta(\\tau)^{i_1,\\dots,i_k}\\), where n is the length of the trajectory, and \\(i_1, \\dots, i_k \\in \\{1,\\dots,d\\}\\) (k > 1) are indices to elements in \\(\\mathbb{R}^d\\). Here, we provide some additional information on the process of computing a path signature and the intuition behind it."], {"title": "A Numerical Example", "content": "Let us consider a trajectory \\(\\tau\\) with two two-dimensional states \\(\\{\\tau_1, \\tau_2\\}\\), and the set of multi-indexes \\(W = \\{(i_1, \\ldots, i_k) \\mid k \\geq 1, i_1, \\ldots, i_k \\in \\{1,2\\}\\} \\), which is the set of all finite sequences of 1's and 2's. Given the trajectory \\(\\tau: [1, 10] \\rightarrow \\mathbb{R}^2\\) illustrated in Figure 7, where the path function for \\(\\tau\\) is computed according to the function:\n\\[\\tau_t = \\{\\tau_1, \\tau_2\\} = \\{5 + t, (5 + t)^2 \\mid t \\in [1, 10]\\} \\qquad (16)\\]\nFor the depth k desired, the computation of the signature would be computed as shown in Figure 8. For example, given that states in \\(\\tau\\) are two-dimensional (d = 2), the path signature for \\(\\tau\\) with depth k = 2 will have the \\(\\frac{d^{k+1}-1}{d-1} = \\frac{2^{3}-1}{2-1} = 7\\) terms in the vector \\(\\beta(\\tau)_{1,10} = [1, 9, 189, 40.5, 970.5, 730.5, 17860.5 ]."}, {"title": "Signature Properties", "content": "We now describe properties of path signatures that are most relevant to our work. The description is not comprehensive. We recommend the work from Yang et al. [29] and Chevyrev and Kormilitzin [2] for a more in-depth approach to path signatures.\nThis property relates to the fact that no two trajectories \\(\\tau\\) and \\(\\tau'\\) of bounded variation have the same signature unless the trajectories are tree-equivalent. In light of the invariance under reparametrisations [17], we note that path signatures have no tree-like sections to monotone dimensions, such as acceleration.\nThe second property refers to the product of two terms \\(\\beta(\\tau)^{i_1,\\dots,i_k}\\) and \\(\\beta(\\tau)^{i_1,\\dots,\\acute{i}_k}\\), which can also be expressed as:\n\\[\\beta(\\tau)_{1,n} \\cdot \\beta(\\tau)_{1,n} = \\beta(\\tau)_{1,n}^2 + \\beta(\\tau)_{1,n}^2 \\text{ or,} \\\\ \\beta(\\tau)_{1,n} \\cdot \\beta(\\tau)_{1,n} = \\beta(\\tau)_{1,n}^2 + \\beta(\\tau)_{1,n}^2. \\qquad (17)\\]"}, {"title": "Motivation for Signatures", "content": "Given the nature of deep learning methods operating on vectorial data, which requires the input data to be of a predetermined fixed length, many techniques, such as word embeddings (where a word is represented by a vector), are used to circumvent this length requirement. Moreover, imitation learning tasks, by definition, have to effectively represent expert demonstrations to capture relevant information for learning a desired behaviour. Path signatures provide a solution to represent sequential or trajectory-based expert demonstration in a principled and efficient manner.\nIn imitation learning, expert demonstration often takes the form of trajectories or sequences of states over time. Path signatures offer a way to encode these trajectories into high-dimensional feature representations that capture the expert behaviour in a geometric and analytic way. Furthermore, the uniqueness property ensures that essential information about the expert demonstrations is preserved in the path signature representation, enabling accurate discrimination over different trajectories' signatures. Lastly, path signatures provide a single hyperparameter (the number of desired collections k). By adjusting k, we can control the trade-off between representational quality and computational complexity, allowing for efficient learning and generalisation. However, we observe that increasing k leads to an exponential increase in the length of the signature, which imposes a limit to agents with limited computation resources."}, {"title": "Signature Time Complexity", "content": "We now briefly discuss the upper-bound complexity for computing path signatures and compare it to Pavse et al.'s work [19], which computes the averages of the trajectory states. Given Eq. 14 and Fig. 6, it should be easy to see that path signatures can be computed in time \\(O(t \\cdot d^k)\\), where t is the number of samples in a trajectory, d is the number of dimensions, and k is the depth/length of the signature. In contrast, Pavse et al.'s work [19] uses the average over the current and previous states. This does not work well when different trajectories average to the same value, but it is not an issue for signatures due to their uniqueness. Using averages is not an issue in Pavse et al.'s work (or in IRL in general), which computes an artificial reward signal at each timestep. However, this also quickly becomes costly because trajectories are traversed multiple times since the method computes the average of all sub-trajectories t times at each epoch (\\(O(d \\cdot t^3)\\)). The cost of computing signatures increases linearly with respect to the episode length, whereas the cost of computing the averages increases exponentially for Pavse et al.. Moreover, path signatures increase exponentially according to the number of dimensions d, which is constant for all environments. Therefore, the main parameter CILO has to be concerned about is the depth k, for which Fig. 9 provides a comparison in Ant-v2 the environment with the largest state representation. Fig. 9 shows that the cost of computing signatures is lower than that of computing averages for k=5 in episodes containing 1,000 timesteps (the total length for all MuJoCo environments used in this work)."}]}