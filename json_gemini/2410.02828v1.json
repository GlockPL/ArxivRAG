{"title": "PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI Systems", "authors": ["Gary D. Lopez Munoz", "Amanda J. Minnich", "Roman Lutz", "Richard Lundeen", "Raja Sekhar Rao Dheekonda", "Nina Chikanov", "Bolor-Erdene Jagdagdorj", "Martin Pouliot", "Shiven Chawla", "Whitney Maxwell", "Blake Bullwinkel", "Katherine Pratt", "Joris de Gruyter", "Charlotte Siska", "Pete Bryan", "Tori Westerhoff", "Chang Kawaguchi", "Christian Seifert", "Ram Shankar Siva Kumar", "Yonatan Zunger"], "abstract": "Generative Artificial Intelligence (GenAI) is becoming ubiquitous in our daily lives. The increase in computational power and data availability has led to a proliferation of both single- and multi-modal models. As the GenAI ecosystem matures, the need for extensible and model-agnostic risk identification frameworks is growing. To meet this need, we introduce the Python Risk Identification Toolkit (PyRIT), an open-source framework designed to enhance red teaming efforts in GenAI systems. PyRIT is a model- and platform-agnostic tool that enables red teamers to probe for and identify novel harms, risks, and jailbreaks in multimodal generative AI models. Its composable architecture facilitates the reuse of core building blocks and allows for extensibility to future models and modalities. This paper details the challenges specific to red teaming generative AI systems, the development and features of PyRIT, and its practical applications in real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative AI (GenAI) has increased in popularity over the past few years, since applications such as ChatGPT captured the zeitgeist of the new wave of GenAI developments. This disruptive and highly innovative technology has become more widespread and more easily accessible than ever before. The increased capabilities of these models have inspired the community to incorporate them into almost every domain, from healthcare [21] to finance [4] to defense [22].\nHowever, with these advances comes a new landscape for risk and harm. GenAI models are generally trained on huge datasets scraped from the Internet [10], and as such the models contain all the potentially harmful information available there, such as how to build a bioweapon, as well as all the biases, hate speech, violent content, etc. contained in these datasets [20]. When a company releases a product that uses GenAI, it inadvertently contains these potentially harmful capabilities and behaviors as an innate part of the model.\nAs with any rapidly advancing technology, the development of new tools and frameworks is crucial to manage and mitigate the associated risks. Generative AI systems in particular present unique challenges that require innovative approaches to security and risk management. Traditional red teaming methods are insufficient for the probabilistic nature and diverse architectures of these systems. Additionally, although there is a promising ecosystem of existing open-source GenAI tools, there is a dearth of tools grounded in practical application of GenAI red teaming.\nTo address these gaps, we introduce the Python Risk Identification Toolkit (PyRIT), a model- and platform-agnostic framework that allows red teamers to investigate novel risks, identify new jailbreaks, and run benchmarks on models. The converters, datasets, and orchestrators in PyRIT are specifically designed for the red teamer persona, and the modular structure makes it trivial to try a wide variety of attack combinations. Developing this framework in tandem with our operations ensures that PyRIT is grounded in real-world experience, enabling a more thorough and effective risk assessment process for generative AI systems.\nPyRIT's composable architecture enables the reuse of core components while allowing extensibility to new models and modalities in the future. The orchestration framework enables attacks with a wide range of complexity: from simply applying converters to input prompts and sending them off to a target to something as complex as using half a dozen generative AI models to compose and iteratively refine a multi-turn attack on a target. PyRIT is written in Python to make it more accessible to broader audiences and released as a library to encourage adoption by the industry.\nThe Microsoft AI Red Team (AIRT) has successfully utilized PyRIT in 100+ red teaming operations of GenAI models, including for Copilots and the release of Microsoft's Phi-3 models [1]. In this paper, we present PyRIT and share both a proof-of-concept (PoC) experiment and a real-world case study using PyRIT, demonstrating its practical applications for AI red teaming.\nThis paper is structured as follows:"}, {"title": "II. RELATED WORK", "content": "The rapidly increasing integration of AI into applications highlights the need for tooling for risk assessment. Traditional Al has open-source tools such as Counterfit [8] to assess AI security risks, Fairlearn to assess fairness in Al systems [23], and InterpretML to understand and explain AI morals [16]. The challenge with these tools and applications is that they lack support for attacks on the next generation of GenAI models.\nThough there are a wide variety of tools released to support various LLM use cases, all have different areas of focus or ranges of capabilities from PyRIT:\nGarak [7] provides a structured approach to probing LLMs for potential vulnerabilities. It was developed by NVIDIA and emphasizes a holistic method to specifically language model security, facilitating the exploration and discovery of issues. PyRIT, by contrast, supports a wide range of attack strategies, including single-turn and multi-turn attacks, against a wide range of multimodal models. It integrates with various Al services and platforms, allowing for comprehensive risk identification across different model types and modalities. Additionally, PyRIT has support for non-English converters which allow red teamers to probe models in other languages.\nAutoGen [24]: is an open-source framework designed to facilitate the development of applications with LLMs through multi-agent conversations. It enables the creation of customizable agents that interact using LLMs, human inputs, and various tools. AutoGen supports flexible conversation patterns with unified conversation interfaces and an auto-reply mechanism, which simplifies the creation of complex workflows. PyRIT, in contrast, focuses on adversarial testing and security assessment of generative AI models, while AutoGen is primarily aimed at enhancing the development and performance of LLM-based applications through multi-agent interactions.\nLangChain [6] is a software framework designed to simplify the creation of applications using large language models. It supports various use cases, including document analysis and summarization, chatbots, and code analysis. PyRIT, conversely, is dedicated to security risk identification and red teaming in generative AI systems. While LangChain offers tools for application development and deployment, PyRIT provides specialized components for conducting security assessments and identifying potential vulnerabilities in Al models\nSematic Kernel [15] is designed to streamline the development and management of AI applications and to enable the development and integration of AI agents. It focuses on simplifying the integration of various services and plugins to support AI workflows. PyRIT, conversely, is engineered for security assessment and red teaming, with a composable architecture that supports a wide range of attack strategies and modalities. While Semantic Kernel aims to enhance the operational efficiency of AI applications, PyRIT is tailored for probing and identifying security risks in generative AI systems.\nOverall, PyRIT's unique perspective and comprehensive feature set distinguish it from existing tools, making it an essential asset in the landscape of generative AI security."}, {"title": "III. METHODOLOGY", "content": "Initially developed as a set of one-off scripts, PyRIT has evolved into a reliable toolkit through continuous enhancements based on real-world red teaming exercises. PyRIT has been used by the Microsoft AI Red Team for 100+ red teaming operations of GenAI models and applications [14].\nWe made the conscious decision to implement PyRIT in Python due to its ease of use and accessibility to the community. The code repository is located on GitHub\u00b9 and is publicly released under an MIT license."}, {"title": "A. Components", "content": "Architecturally, PyRIT is composed of six different components, see Figure 1. Each component was developed to perform a unique and critical function to interact with GenAI models. The interfaces that each component exposes allow Responsible AI (RAI) engineers and red teamers to choose as many or as few of the components as they need when interacting with models.\n1) Memory: The memory component automatically stores all the interactions that users have with a GenAI system. This simplifies the development of new attack techniques, as it handles one of the most crucial aspects of an attack while letting the developer focus on the next message to be sent. Importantly, memory keeps track of the original value (e.g., \"Tell me how to build a bomb.\") as well as the value after applying converters (e.g., with Leetspeak converter \"T311 m3 how to bulld 4 bomb\"). This is particularly useful when applying non-trivial and non-deterministic converters, as it preserves the original value for inspection at a later point while capturing the converted value that is sent to a target. The same applies on the receiving end if target responses require deciphering or translating. Memory enables the analysis of repeated conversations and easy identification of conversations of interest.\nDuring and after operations, red teamers share output records with stakeholders or other red teamers. Memory provides simple ways to export conversations from its database (using DuckDB) to local files if needed. Alternatively, it is possible to store records centrally in a database in Azure, or to use the memory interfaces to implement custom logic to export conversations."}, {"title": "B. Extensibility", "content": "PyRIT is designed with extensibility at its core, allowing users to adapt and extend its functionalities to meet the evolving needs of security risk identification and red teaming in generative AI systems. Here are the key aspects of its extensibility:\n1) Flexible Attacks: PyRIT supports the implementation of various attack strategies, both single-turn and multi-turn. In single-turn attacks, a prompt is submitted and its response is evaluated in isolation. In multi-turn attacks, the red teaming bot engages in an ongoing interaction with the model, iterating through multiple prompts to achieve specific objectives. This flexibility allows users to tailor the attack methods to the specific system they are investigating.\nOut of the box, PyRIT supports many advanced adversarial techniques described in the literature:\nPrompt Automatic Iterative Reinforcement (PAIR). [5]\nTree of Attacks with Pruning (TAP) [13]\nGreedy Coordinate Gradient (GCG) [27]\nCrescendo: [19]\nBenchmarks: [9]\nSkeleton Key [18]\nGPTFuzzer [25]\nPersuasive Adversarial Prompts [26]\nMany-shot jailbreaking [3]\nPyRIT implements these adversarial attacks through the use of orchestrators. Executing these attacks involves instantiating an orchestrator, defining the prompt targets for both the target and adversary, and providing the necessary attack-specific parameters to run the adversarial technique. This approach lowers the barrier to entry for conducting adversarial ML attacks, enabling security researchers to evaluate the security of their models against state-of-the-art adversarial strategies. Moreover, PyRIT's modular architecture allows the community to develop custom attacks and easily extend PyRIT'S functionality.\n2) Open Source and Community-Driven Development:\nAs an open source project, we encourage community contributions to PyRIT. Users can extend its functionality by developing new components, improving existing ones, and sharing their enhancements with the broader community. This collaborative approach accelerates the development of robust AI security tools and fosters innovation.\nSince open-sourcing PyRIT, we have actively engaged with the community by accepting all external contributions and addressing every issue raised. As of July 1, 2024, PyRIT boasts"}, {"title": "IV. EXPERIMENT", "content": "To demonstrate the effectiveness of the attacker bot mode, we conducted a proof of concept using the chatbot Gandalf from Lakera [12]. Gandalf serves as an effective test bed for evaluating the capabilities and flexibility of the PyRIT framework. Designed to help users practice crafting prompts that can extract a password from the chatbot across ten progressively more difficult levels, Gandalf introduces additional countermeasures at each level, including stronger system prompts, block-lists, and input/output guards.\nTo evaluate the effectiveness of the Red Team Orchestrator in PyRIT, we developed targets and scorers tailored to Gandalf. The experimental setup involved configuring the following components within PyRIT:\n1) Target Endpoint: Gandalf was set as the target LLM.\n2) Red Team Bot: GPT-40 was the LLM powering the red team bot.\n3) Attack Strategy: A text description of the objective for the red team bot. In this case, the objective is to extract the password from the Gandalf (the target endpoint).\n4) Scorers: Custom scoring engines were implemented to evaluate the responses generated by Gandalf.\nWe used the red team orchestrator to probe Gandalf and extract the passwords for Levels 1-4. PyRIT successfully extracted the passwords by leveraging its self-reasoning capabilities, which keep track of conversation history to increase the likelihood of success in subsequent prompts.\nIn this setup, the PyRIT Red Team Orchestrator (RTO), powered by GPT-40, sends prompts to the Gandalf target. Internally, the RTO maintains a stateful conversation log in its memory to refine its prompts iteratively. In contrast, Gandalf is stateless and does not keep track of previous interactions, making it susceptible to persistent probing by the RTO.\nFor instance, in Level 1, the RTO was able to extract the password by sending a series of prompts designed to bypass Gandalf's initial defenses. An RTO conversation that extracts the password in a single-turn is shown below (the password is shown in red). A multi-turn conversation between RTO and Gandalf level 1 is shown in Appendix A."}, {"title": "B. AI Red Team Case Study", "content": "PyRIT was used extensively during a recent operation supporting the open-source release of Microsoft's Phi-3 series of models [1]. Prior to a model release, companies often engage in a 'break-fix' cycle, where red teamers probe the model for safety and security issues, and the GenAI team performs additional fine-tuning to address those issues, iterating the process as necessary, see Figure 3.\nFor the release, the team used PyRIT to probe and evaluate the safety of Phi-3 text and vision model release candidates, as well as to benchmark with a variety of state-of-the-art open-source models across a total of 15 harm categories. Sample categories include CBRN, phishing/cybersecurity, Election Critical Information (ECI), fairness/bias, hate speech, and sexual and violent content. For text, we also testing using two personas, low-skilled adversary and intermediate adversary, and two scenarios: single-turn and multi-turn. More information is given in Table II.\nMicrosoft's AI Red Team spent six weeks preparing datasets containing hundreds of prompts and prompt-image pairs and testing a variety of text and vision model candidates for release. PyRIT played an integral role in every part of this evaluation. For all evaluations, PyRIT was used to generate a wide variety of converted prompts from our base datasets.\nFor single-turn scenarios, PyRIT was used in \"bulk submission\" mode, where a dataset of prompts is sent to a model endpoint and the responses are collected and stored in PyRIT's memory object. The outputs are then scored using our custom self-ask scorers. Each scenario employed a scorer with custom instructions for that particular harm area. We then used PyRIT to generate scoring metrics and overview plots, highlighting current safety risks in the model as well as providing comparisons with the baseline models.\nFor multi-turn scenarios, the attacker bot mode was employed, wherein an attacker LLM was instructed to induce a specific type of output using various strategies. A scorer model was utilized at each turn to determine if the offending output had been successfully generated. All turns were automatically saved in memory for processing once the attack was complete.\nFigure 3 shows a subset of output from testing the Phi-3-mini text model. Safety post-training guided by AI red teaming led to a significant decrease in harmful content generation across all categories, including the 7 shown here."}, {"title": "V. DISCUSSION", "content": "The results of these experiments demonstrate PyRIT's capability to both 1) exploit known attack techniques at scale, and 2) explore the space of potential attacks effectively. For the Phi-3 operation, over a thousand prompts were generated, normalized, submitted for inference, and scored. These prompts spanned 15 known harm categories and used PyRIT converters and jailbreaks to carry out all attacks. For the Gandalf POC, the Red Team Orchestrator was able to navigate through Gandalf's security mechanisms, progressing through multiple levels by generating sophisticated prompts that bypassed the countermeasures. Key findings include:\n1) Prompt Diversity: By leveraging the converter component, the team was able to generate a wide array of prompt variations, increasing the likelihood of identifying successful attacks. This approach helped to identify brittle spots in the Phi-3 models and also proved effective in evading deny-lists and other input filters implemented in Gandalf.\n2) Efficiency in Prompt Generation: The automated nature of PyRIT allowed for rapid generation and testing of hundreds to thousands of prompts, significantly reducing the time required to identify vulnerabilities compared to manual testing.\n3) Adaptability: The dynamic prompt generation and multi-turn interactions enabled PyRIT to adapt to the target model's responses, refining prompts iteratively to achieve the desired outcome."}, {"title": "VI. FUTURE WORK", "content": "As the field of generative AI continues to mature, the tools for red teaming such systems must continue to improve and evolve. There are various areas of focus to enhance the functionality and capabilities of PyRIT. These include:\n1) Enhanced Reporting: Future versions of PyRIT will include improved reporting capabilities. This will involve the development of modules that can automatically generate detailed reports on the results of red teaming exercises, including metrics on attack success rates and identified jailbreaks.\n2) Expanded Multi-Modal Converter Library: We plan to significantly expand the library of multi-modal converters. This will involve developing new converters that can handle a wider range of input and output modalities, enhancing PyRIT's ability to conduct comprehensive adversarial testing across different types of generative Al models.\n3) Increased Number of Adversarial Algorithms: Future iterations of PyRIT will incorporate additional adversarial algorithms. We aim to integrate more advanced adversarial techniques to better identify risks in generative Al systems.\n4) Support for Emerging Generative AI Models: To ensure PyRIT remains a valuable tool in AI security, we will continuously update its framework to support new generative AI models as they emerge. This will involve regular updates to the target component."}, {"title": "VII. CONCLUSION", "content": "The rapid proliferation of Generative AI necessitates the development of robust, adaptable, and comprehensive tools for risk identification and mitigation. PyRIT stands out as a pioneering framework tailored to address the unique challenges posed by multimodal generative AI systems. Through its model- and platform-agnostic design, PyRIT provides red teamers with the flexibility to uncover novel risks and vulnerabilities across a wide array of AI models. The framework's composable architecture not only facilitates the reuse of core components but also ensures its adaptability to future models and modalities. Our PoC experiment and real-world case study underscore PyRIT's practical utility and effectiveness in enhancing AI red teaming operations. As the GenAI landscape continues to evolve, PyRIT is poised to play a crucial role in bolstering the security and reliability of these transformative technologies. Future work will focus on enhancing PyRIT's reporting capabilities, expanding its multimodal converters, and expanding the variety of adversarial algorithms and models it supports. By incorporating feedback from its deployment in diverse operational settings, we will continue fostering a community of users and contributors dedicated to advancing responsible AI practices."}, {"title": "VIII. DISCLAIMER", "content": "The information in this paper is for educational and research purposes only. The PyRIT framework aims to identify and"}]}