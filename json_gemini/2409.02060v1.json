{"title": "OLMOE: Open Mixture-of-Experts Language Models", "authors": ["Niklas Muennighoff", "Luca Soldaini", "Dirk Groeneveld", "Kyle Lo", "Jacob Morrison", "Sewon Min", "Weijia Shi", "Pete Walsh", "Oyvind Tafjord", "Nathan Lambert", "Yuling Gu", "Shane Arora", "Akshita Bhagia", "Dustin Schwenk", "David Wadden", "Alexander Wettig", "Binyuan Hui", "Tim Dettmers", "Douwe Kiela", "Ali Farhadi", "Noah A. Smith", "Pang Wei Koh", "Amanpreet Singh", "Hannaneh Hajishirzi"], "abstract": "We introduce OLMOE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLM0E-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMOE-1B-7B-INSTRUCT. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.", "sections": [{"title": "1 Introduction", "content": "Despite significant advances in Large Language Models (LMs) on various tasks, there remains a clear trade-off between performance and cost in both training and inference. High-performing LMs are inaccessible for many academics and open-source developers as they are prohibitively expensive to build and deploy. One approach to improve the cost-performance trade-off lies in using sparsely-activated Mixture-of-Experts (MoEs) [152]. MoEs have several experts in each layer, only a subset of which is activated at a time (see Figure 2). This makes MoEs significantly more efficient than dense models with a similar number of total parameters, which activate all parameters for every input [204]. For this reason, industry frontier models use MoEs including Gemini-1.5 [173] and reportedly GPT-4 [29].\nMost MoE models, however, are closed-source: while some have publicly released model weights [43, 78, 156, 176, 178], they offer limited to no information about their training data, code, or recipes (see Figure 1). While there have been prior efforts in making language modeling research fully accessible [18, 64, 88, 102, 192, 208], they have been largely limited to dense LMs. This comes despite MoEs requiring more openness as they add complex new design questions to LMs, such as how many total versus active parameters to use, whether to use many small or few large experts, if experts should be shared, and what routing algorithm to use. The lack of open resources and findings about these details prevents the field from building cost-efficient open MoEs that approach the capabilities of closed-source frontier models.\nTo address these issues, we introduce OLMOE, a fully open Mixture-of-Experts language model with state-of-the-art performance among similarly-sized models. In particular, we pretrain OLMOE-1B-7B for 5.1 trillion tokens with 6.9B total parameters, of which only 1.3B are activated for each input token. This leads to a similar inference cost as using dense models with around 1B parameters, such as OLMo 1B [64] or TinyLlama 1B [209], but requires more GPU memory to store its 7B total parameters. Our experiments show that MoEs train ~2\u00d7 faster than dense LMs with equivalent active parameters. In Figure 1, we show that OLMOE-1B-7B significantly outperforms all open 1B models and displays competitive performance to dense models with significantly higher inference costs and memory storage (e.g., similar MMLU scores to Llama2-13B, which is ~10\u00d7 more costly). Via instruction- and preference tuning, we create OLMOE-1B-7B-INSTRUCT, which we find exceeds various larger instruct models including Llama2-13B-Chat [181], OLMo-7B-Instruct (0724), and DeepSeekMoE-16B [42] on common benchmarks (MMLU, GSM8k, HumanEval, etc.).\nOur comprehensive set of controlled experiments highlights key design choices for MoEs (see Table 1) and LMs in general. One critical design decision for making MoEs performant is the use of fine-grained routing with granular experts [42]: we employ 64 small experts in each layer with 8 being activated. The choice of routing algorithm is also important: we find dropless [58] token-based routing [152] outperforms expert-based routing [218]. Our findings also include those that challenge prior work, such as the ineffectiveness of shared experts [42] and the limited benefits of sparsely upcycling a pretrained dense LM into an MoE [84] unless under small compute budgets. Finally, we analyze the routing behavior in OLMOE-1B-7B, finding that routing saturates early in pretraining, experts are rarely co-activated, and experts exhibit domain and vocabulary specialization.\nWe hope our fully open MoE facilitates more research and analysis to improve our understanding of these models. We release training code, intermediate checkpoints (every 5000 steps), training logs, and training data under open-source licenses (Apache 2.0 http://www.apache.org/licenses/ LICENSE-2.0 or ODC-By 1.0 https://opendatacommons.org/licenses/by/1-0/).", "sentences": [{"index": 1, "text": "Despite significant advances in Large Language Models (LMs) on various tasks, there remains a clear trade-off between performance and cost in both training and inference."}, {"index": 2, "text": "High-performing LMs are inaccessible for many academics and open-source developers as they are prohibitively expensive to build and deploy."}, {"index": 3, "text": "One approach to improve the cost-performance trade-off lies in using sparsely-activated Mixture-of-Experts (MoEs) [152]."}, {"index": 4, "text": "MoEs have several experts in each layer, only a subset of which is activated at a time (see Figure 2)."}, {"index": 5, "text": "This makes MoEs significantly more efficient than dense models with a similar number of total parameters, which activate all parameters for every input [204]."}, {"index": 6, "text": "For this reason, industry frontier models use MoEs including Gemini-1.5 [173] and reportedly GPT-4 [29]."}, {"index": 7, "text": "Most MoE models, however, are closed-source: while some have publicly released model weights [43, 78, 156, 176, 178], they offer limited to no information about their training data, code, or recipes (see Figure 1)."}, {"index": 8, "text": "While there have been prior efforts in making language modeling research fully accessible [18, 64, 88, 102, 192, 208], they have been largely limited to dense LMs."}, {"index": 9, "text": "This comes despite MoEs requiring more openness as they add complex new design questions to LMs, such as how many total versus active parameters to use, whether to use many small or few large experts, if experts should be shared, and what routing algorithm to use."}, {"index": 10, "text": "The lack of open resources and findings about these details prevents the field from building cost-efficient open MoEs that approach the capabilities of closed-source frontier models."}, {"index": 11, "text": "To address these issues, we introduce OLMOE, a fully open Mixture-of-Experts language model with state-of-the-art performance among similarly-sized models."}, {"index": 12, "text": "In particular, we pretrain OLMOE-1B-7B for 5.1 trillion tokens with 6.9B total parameters, of which only 1.3B are activated for each input token."}, {"index": 13, "text": "This leads to a similar inference cost as using dense models with around 1B parameters, such as OLMo 1B [64] or TinyLlama 1B [209], but requires more GPU memory to store its 7B total parameters."}, {"index": 14, "text": "Our experiments show that MoEs train ~2\u00d7 faster than dense LMs with equivalent active parameters."}, {"index": 15, "text": "In Figure 1, we show that OLMOE-1B-7B significantly outperforms all open 1B models and displays competitive performance to dense models with significantly higher inference costs and memory storage (e.g., similar MMLU scores to Llama2-13B, which is ~10\u00d7 more costly)."}, {"index": 16, "text": "Via instruction- and preference tuning, we create OLMOE-1B-7B-INSTRUCT, which we find exceeds various larger instruct models including Llama2-13B-Chat [181], OLMo-7B-Instruct (0724), and DeepSeekMoE-16B [42] on common benchmarks (MMLU, GSM8k, HumanEval, etc.)."}, {"index": 17, "text": "Our comprehensive set of controlled experiments highlights key design choices for MoEs (see Table 1) and LMs in general."}, {"index": 18, "text": "One critical design decision for making MoEs performant is the use of fine-grained routing with granular experts [42]: we employ 64 small experts in each layer with 8 being activated."}, {"index": 19, "text": "The choice of routing algorithm is also important: we find dropless [58] token-based routing [152] outperforms expert-based routing [218]."}, {"index": 20, "text": "Our findings also include those that challenge prior work, such as the ineffectiveness of shared experts [42] and the limited benefits of sparsely upcycling a pretrained dense LM into an MoE [84] unless under small compute budgets."}, {"index": 21, "text": "Finally, we analyze the routing behavior in OLMOE-1B-7B, finding that routing saturates early in pretraining, experts are rarely co-activated, and experts exhibit domain and vocabulary specialization."}, {"index": 22, "text": "We hope our fully open MoE facilitates more research and analysis to improve our understanding of these models."}, {"index": 23, "text": "We release training code, intermediate checkpoints (every 5000 steps), training logs, and training data under open-source licenses (Apache 2.0 http://www.apache.org/licenses/ LICENSE-2.0 or ODC-By 1.0 https://opendatacommons.org/licenses/by/1-0/)."}]}, {"title": "2 Pretraining and Adaptation", "content": "Pretraining architecture OLMOE is a decoder-only LM consisting of NL transformer [183] layers. The feedforward network (FFN) in dense models like OLMo [64], is replaced with an MoE module consisting of Ne smaller FFN modules called experts, of which a subset of k experts are"}, {"title": "5 Design choices for adaptation", "content": "Key decisions designning an MOE include determing number parameters and desing of eperts and choices for the routing argothrim and change the objectives.Experiments for these choises are in."}, {"title": "5 Design choises.", "content": "To determine the active and total number of parameters we use the MoE routing argothim."}, {"title": "4.1.3 Shared Experts", "content": "Dai et al. [39] propose training with a shared/fixed expert that is always used in addition to the routed experts. The intuition is to encourage the shared expert to learn common information and allow the other routed experts to learn more specialized knowledge. This should reduce redundancy among experts and thus lead to a better model as it can store more total information.\nIn Figure 6, we benchmark having a single shared and a single routed expert versus two routed ex-perts. While both settings lead to similar performance, sharing an expert performs slightly worse. Sharing an expert removes flexibility from the model and thus goes against the findings in \u00a74.1.2 suggesting that allowing for more expert combinations improves performance. Specifically, the two models in Figure 6 have $$\\binom{32}{4}$$= 35, 960 and $$\\binom{31}{3}$$ = 4, 495 possible combinations per layer. Thus, re-moving one of the routed experts and turning it into a shared one eliminates almost 90% of possible"}, {"title": "4.1.6 Load Balancing Loss", "content": "Shazeer et al. [152] propose the load balancing loss to penalize the model if it is unbalanced, i.e., if it routes all tokens to only a few experts. This is based on the observation that without such penalty, models tend to update only a select few experts in each layer [52, 17]. To compute the load balancing loss (LLB) we multiply the fraction of tokens $$f_i$$ routed to one expert $$E_i$$ with the total routing probability $$P_i$$ allocated to $$E_i$$ for one batch and sum it across the number of experts $$N_E$$:\n$$L_{LB} = \\frac{N_E}{\\mathcal{B}} \\sum_{i=1}^{N_E} f_i P_i$$"}, {"title": "5 MoE Analysis", "content": "By advancing open and cost-efficient models (\u00a71), OLMOE-1B-7B enables new research into LMs and MoEs. Making use of our released intermediate checkpoints, data, and code, we define and analyze four properties specific to MoEs: Router saturation (\u00a75.1), Expert co-activation (\u00a75.2), Domain specialization (\u00a75.3), and Vocabulary specialization (\u00a75.4)."}, {"title": "5.1 Router Saturation", "content": "We define router saturation as the proportion of expert activations at some intermediary checkpoint at time t that matches the expert IDs activated at some final checkpoint over the same dataset:\n$$Router \\: Saturation(t) = \\frac{1}{N} \\sum_{i=1}^N  |\\mathbb{E}_i^{(t)} \\cap \\mathbb{E}_i^{(T)}|$$"}, {"title": "5.2 Expert Co-activation", "content": "We define expert co-activation as the proportion of times two specific experts, $$E_i$$ and $$E_j$$, are simul-taneously activated out of the total number of activations of one of those experts:\n$$Expert \\: co\\text{-}activation(E_i, E_j) =  \\frac{N_{E_i,E_j}}{N_{E_i}},$$"}, {"title": "H Limitations and Future Work", "content": "We highlight four key limitations with this release of OLMOE-1B-7B. We look forward to ad-dressing these issues in future iterations of OLMOE.\nMore parameters OLMOE-1B-7B has 7B total parameters out of which 1B are activated for each input token. This small size makes OLMOE-1B-7B very cheap to use, yet we demonstrate in this work that it outperforms much more expensive models (Figure 1). However, using only 1B parameters for each input token also limits the capabilities of OLMOE-1B-7B as seen by its performance compared to models that use >7\u00d7 more parameters, such as Llama3.1-8B in \u00a73. While it may be possible that more parameters are not needed to match 8B models and beyond [80], in the short-term adding parameters is an easy way to improve the performance of OLMOE, at least allowing the model to utilize more than 1B parameters per input, possibly via recursion [45] or agentic workflows [185, 201]. Relatedly, changing the allocation of parameters to e.g. vocabulary versus non-vocabulary parameters is another avenue for improvement [170].\nMore data We train OLMOE-1B-7B for 5 trillion tokens, however, some recent dense models train significantly longer, such as Llama 3 with 15 trillion tokens [50]. To the best of our knowl-edge, there has been no large MoE that has been overtrained [57] as much as OLMOE-1B-7B. Specifically, taking the active parameters of OLMOE-1B-7B, our token multiplier [57] is around 5,000 (5T / 1B). There are likely benefits to training even longer, but to what degree overtraining is effective for MoEs and how it differs from dense models still requires more research [7].\nMultimodal OLMOE-1B-7B is a text-only large language model, thus it cannot take inputs or produce outputs in other modalities like images or audio. This limits its utility for the large variety of multimodal use cases of such models [74, 165, 27, 81, 118, 134, 14, 47, 50]. There has been early work on open multimodal MoEs [124, 97, 94, 155, 111, 193] and we look forward to making future versions of OLMOE a part of that.\nMultilingual We pretrain OLMOE-1B-7B on a predominantly English corpus and exclusively evaluate on English tasks. This may severely limit the usefulness of our model for research on non-English language models [107, 158, 222, 53, 163, 196]. While there has been work on training language-specific LMs [109, 55], it is more likely that as we add more data to build better future iterations of OLMOE we will mix in more non-English data due to data constraints [120]. This may make future OLMOE models perform better in non-English languages."}]}