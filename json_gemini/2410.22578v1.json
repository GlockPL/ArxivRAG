{"title": "Energy-Aware Multi-Agent Reinforcement Learning for Collaborative Execution in Mission-Oriented Drone Networks", "authors": ["Ying Li", "Changling Li", "Jiyao Chen", "Christine Roinou"], "abstract": "Mission-oriented drone networks have been widely used for structural inspection, disaster monitoring, border surveillance, etc. Due to the limited battery capacity of drones, mission execution strategy impacts network performance and mission completion. However, collaborative execution is a challenging problem for drones in such a dynamic environment as it also involves efficient trajectory design. We leverage multi-agent reinforcement learning (MARL) to manage the challenge in this study, letting each drone learn to collaboratively execute tasks and plan trajectories based on its current status and environment. Simulation results show that the proposed collaborative execution model can successfully complete the mission at least 80% of the time, regardless of task locations and lengths, and can even achieve a 100% success rate when the task density is not way too sparse. To the best of our knowledge, our work is one of the pioneer studies on leveraging MARL on collaborative execution for mission-oriented drone networks; the unique value of this work lies in drone battery level driving our model design.", "sections": [{"title": "I. INTRODUCTION", "content": "The easy deployment and high mobility have made drones prevalent nowadays. Due to the potential of enhancing the network coverage and improving the efficiency of executing tasks in complex and dangerous environments, drones have been used in various applications recently, such as using drones as mobile base stations in 5G networks [1] and mission-oriented drone networks for package delivery [2], border surveillance [3], forest fire monitoring [4], etc.\nAlthough drones can bring convenience in various scenarios, the limited battery capacity restricts the operation time, which is a challenging problem for researchers. With the current battery technology, drones can have approximately 20 to 40 minutes of operation time [5]. Researchers have explored two categories of approaches to overcome this challenge: battery or drone replacement and mission execution management. The first approach aims to design replacement mechanisms to let spare drones replace weary drones [6] or enable weary drones to recharge the batteries [7]. The mission execution management approach focuses on trajectory planning so that drones can efficiently finish their work before running out of power [8], [9].\nHowever, mission execution management for multi-drone systems is still underdeveloped, with only a few works using centralized algorithms [10]. In real life, a mission is usually composed of multiple tasks, such as inspection of the wind turbines in a wind farm [11]. The mission is composed of several tasks, and each is to examine a turbine. The energy spent on executing a task can vary depending on the task. Therefore, a single drone can finish some light tasks, while each of the other demanding ones may need multiple drones to execute cooperatively. Proper and robust execution management is necessary to efficiently finish the mission with the limited battery capacity and make drones more applicable in real life.\nThis paper proposes a collaborative execution model based on multi-agent reinforcement learning (MARL) and deep Q-network (DQN) [12] for mission-oriented drone networks to accomplish the assigned mission efficiently. We explore scenarios in which a mission comprises several independent tasks with various task lengths at different locations. A mission is considered to be completed when all of its tasks are finished. Each of these tasks needs one or more time steps to complete. A task with multiple time steps can have several discrete portions and can be co-finished by multiple drones, with each drone finishing a part of the task. Hence, a single drone may execute more than one task, and a task may be taken care of by more than one drone. Our study aims at collaborative task execution considering the battery level of drones and trajectory control, as less energy spent on travel from one task to another can let more energy be spent on task execution. To our best knowledge, our exploration is one of the pioneer studies focusing on collaborative mission execution while considering the energy consumption on travel using MARL for drone networks.\nThe rest of this paper is organized as follows. Section II reviews the related work and compares their studies with ours to show our contribution to this field in multiple aspects. Section III and IV provide a detailed description of the mission-oriented drone network model we used in this study and the design of our collaborative execution model. Section V describes our simulation environments, experiment design, simulation results. Section VI concludes and points out the future work of this study."}, {"title": "II. RELATED WORK", "content": "The methods proposed by existing studies can be categorized into two groups: methods without using machine learning and those leveraging machine learning. Most of these studies assume that tasks have binary lengths, which can be completed once drones pass through the locations where tasks are located. Therefore, these studies focus on optimizing trajectories to improve efficiency.\nAmong the methods in the first category, a common way to establish the trajectory optimization models is waypoint segmentation. Waypoint segmentation divides the trajectories based on different time intervals. Cobano et al. proposed RRT and RRT* algorithms to optimize the trajectories to address the Wireless Sensor Network (WSN) data collection problem defined by waypoint segmentation [13]. The proposed RRT and RRT* algorithms focus on collision avoidance. Wang et al. considered the same problem in a larger scale network [14]. They divided the aerial data collection into five procedures, including waypoint searching, and proposed the FPPWR algorithm to increase the efficiency of path planning while assuring the relatively short trajectories. Qin et al. also explored the large-scale WSNs intending to minimize the completion time without compromising the information collection quality [15]. They introduced a hovering point selection algorithm for appropriate waypoint selection and proposed a min-max cycle cover algorithm to allocate the waypoints and compute the trajectories of each drone. These waypoint segmentation methods require the consideration of multiple segments for different drones during calculation at each time step, which can be computationally expensive. Xia et al. proposed to use time segmentation to establish the trajectory optimization model to simplify the calculation [16]. Their proposed method reduces the computational complexity from $O(n^2)$ to $O(n)$ and is efficient for many drones.\nAll these studies discussed above focus on collision avoidance while optimizing the trajectories. The task execution is either not considered or finished when the drones pass through the waypoints of tasks due to the binary task length. However, in reality, a task execution may take a more extended period to finish than what it needs for a drone to pass through a waypoint. Li et al. consider the scenario that the tasks have non-binary length [10], similar to our proposed problem. They proposed the EATP model to accomplish all tasks energy efficiently. The advantage of these above non-machine-learning methods is that they guarantee an optimal or close-to-optimal solution. However, they usually require prior knowledge of the global information to calculate the paths and are less flexible to adapt to dynamic environments. Hence, they can be inefficient and time-consuming in reality applications.\nTo combat the challenges of the dynamic environment, researchers have adopted machine learning in their solutions. Reinforcement learning (RL) has been prevalently used due to its nature of learning through interaction with the environment [17]. Many researchers have employed RL to optimize drone networks' data sensing and transmission. The primary concern of Hu et al. is the probability of successful data transmission [18]. They employed Q-learning for trajectory control. To accelerate the Q-value convergence, they created an algorithm so that drones update their Q-functions based on the probability of successful data transmission. Fakhrul et al. concentrated on data freshness and energy efficiency [19]. They proposed using the DQN with experience replay model to maximize the energy efficiency of the trajectories while assuring data freshness. A 4-dimensional numerical value represents the state space of their model. Other researchers have approached the state space differently. Bayerlein et al. proposed a DDQN model with convolutional layers for multi-drone path planning [20]. A combination of centralized global and local map representations of the environment is used as the state space. Their simulation shows that this approach enables the agents to divide the tasks and cooperate effectively. MADDPG [21] is another framework applied to multi-drone target assignment and path planning problems. Qie et al. proposed the STAPP method based on MADDPG to solve the task assignment and path planning problem [22]. Their method can deal with the dynamic environments effectively as it only requires the information of the locations of drones, targets, and threat areas.\nSimilar to the methods discussed in the first category, those studies primarily focus on collision avoidance, trajectory planning, and task assignment. However, task lengths are out of consideration, so they didn't explore how to execute these non-binary tasks collaboratively. To bridge this gap, we propose a collaborative execution model driven by the battery level of drones, using MARL and deep Q-networks, which enables efficient mission execution. In summary, the contributions of this work are:\nUnlike the previous work, we consider a practical scenario where a mission consists of multiple tasks with non-binary lengths. Each task requires multiple time steps to finish. To create a robust and applicable model, we also formulate the problem so that each task location and length of a mission are unpredictable.\nWe leverage MARL and DQN to combat the challenge of the dynamic environment and solve the problems of collaborative execution. The drone battery level of all drones coupled with executed task portions drives the formulation of the reward function, enabling efficient cooperation. To our best knowledge, it is the first work that considers the constraints of battery capacity and task length in the model for drone networks.\nOur simulation study is presented in detail to demonstrate that our model is robust and applicable. We also explore the effect of specific hyperparameters on the performance of our model and point out the challenges of applying MARL and our future research directions."}, {"title": "III. SYSTEM MODEL", "content": "As illustrated in Fig. 1, we consider a set of trajectory points, $N = \\{N_0,...,N_n\\}$. The trajectory point $N_0$ is the location of the base station for drones, which is also the"}, {"title": "IV. PROPOSED COLLABORATIVE EXECUTION MODEL BASED ON MARL AND DQN", "content": "We propose a collaborative execution model based on MARL and DQN to address the limited battery capacity when executing the mission assigned to a drone network. Our model lets each drone have a DQN to find the best trajectory and task execution strategy to successfully co-finish the mission with other drones. All drones in this model share a reward function to collaborate with others fully.\nEvery DQN in our study consists of a policy network and a target network. These two networks have two fully-connected hidden layers using the rectified linear unit activation function. They also have a densely-connected output layer. The loss function used in this DQN model is the mean squared error, and we adopt the Adam algorithm to optimize speed and performance for training the model. We also combine our DQN with experience replay to break the dependency among the observations in the training process."}, {"title": "A. Action and State Space", "content": "Inspired by [24], we divide the space into a finite set of discrete trajectory points N in a grid pattern. Each time step $t, t \u2208 \\{0,...,T\\}$, a drone can travel from a trajectory point $N_{start}$ to another trajectory point $N_{end}$ on the same square, $N_{start} \u2208 N, N_{end} \u2208 N$. The maximum distance $d_{max}$ a drone can travel in a time step is the diagonal between two trajectory points located on the same square. When the travel distance $d(N_{start}, N_{end}) < d_{max}$, the drone will hover at the endpoint for the rest of the current time step. Fig. 2 shows a sample grid pattern with nine trajectory points. At any time step, a drone can travel from $N_4$ to any one of its eight adjacent trajectory points. It takes exactly one time step for a drone to travel from $N_4$ to any one of $\\{N_0, N_2, N_6, N_8\\}$. The drone will hover at the endpoint for the rest of the current time step if it travels from $N_4$ to any one of $\\{N_1, N_3, N_5, N_7\\}$.\nThe set of actions A available to the drones consists of ten actions, which can be grouped into three types. The first type is to let a drone move from one trajectory point to another, and the travel distance per time step for any action of this type fulfills $d(N_i, N_j) \u2264 d_{max}$. Eight actions belong to this type, allowing a drone to move to the adjacent trajectory points in eight different directions: up, down, right, left, upper left, upper right, bottom left, and bottom right. The first type results in a new trajectory point for a drone before the beginning of the next time step and consumes the drone's power for travel. The second type contains one action, letting a drone hover at the current trajectory point, $N_i$, if $N_i \u2208 \\{N_1, ..., N_n\\}$. If the current trajectory point is $N_0$, the drone stays stationary at the base station without consuming any energy. The second type doesn't update a drone's location but consumes the drone's power for hovering when the drone is not at the base station. The third type has one action, allowing a drone to execute the task at the trajectory point the drone currently locates while hovering there. This type doesn't change a drone's location but consumes the drone's energy for task execution and hovering. Each drone k chooses an action a from A at the beginning of each time step t and then updates its location and battery level $B_k$ accordingly.\nThe state space for the proposed collaborative execution model is a quintuple. Each time step t, the state space is denoted by Equation 7,\n$S_t = (L_{task}, L_{drone,t}, A, T, B),$ (7)\nwhere $L_{task}$ contains the locations of all K tasks, $L_{drone,t}$ contains the current locations of all K drones, A contains the current action of each of the K drones, $T$ contains the remaining time steps of each of the K tasks, $B$ contains the current battery levels of each of the K drones, and they are defined by the following Equations 8 to 12.\n$L_{task} = \\{l_{task} | l_{task} \u2208 N \\{N_0\\}, \u2200k \u2208 K\\},$ (8)\n$L_{drone,t} = \\{l_{drone,t} | l_{drone,t} \u2208 N, \u2200k \u2208 K\\},$ (9)\n$A = \\{a | a \u2208 A, \u2200k \u2208 K\\},$ (10)\n$T = \\{\u03c4 | 0 \u2264 \u03c4 < T_k, \u2200k \u2208 K\\},$ (11)\n$B = \\{B | B < B, \u2200k \u2208 K\\}.$ (12)"}, {"title": "B. Reward and Control Policy", "content": "At the beginning of each time step, every drone chooses an action that transmits the environment state from $S_t$ to $S_{t+1}$ and generates an immediate reward $R_{t+1}$. The reward is then used to form the control policy that drives the drones to accomplish the mission efficiently by optimizing the accumulated reward. We expect drones to execute tasks collaboratively while being aware of their battery levels. Hence, the reward is formulated based on the execution progress of all tasks and the remaining battery power of all drones. The reward function is shared among the drones, which encourages drones to collaborate as a team. The task execution progress per time step, E(t), is defined by Equation 13,\n$E(t) = \\sum_{k=1}^{K} (1 - \\frac{\u03c4_i}{T_i}),$ (13)\nand the reward function is defined by Equation 14,\n$R_{t+1} = \\begin{cases}\nE(t + 1) + \u03b3\u03bc_{t+1}, & \\text{if formulae 4 and 5 are true,} \\\\\nE(t + 1) - \u03b2\u03c9_{t+1}, & \\text{if formula 4 is true and formula 5 is false,} \\\\\nE(t + 1), & \\text{if formula 4 is false,}\n\\end{cases}$ (14)\nwhere $\u03bc_t$ measures the rate of the total amount of remaining energy of all drones at time step t, defined by Equation 15,\n$\u03bc_t = \\frac{\\sum_{k=1}^K B_k}{KB},$ (15)\n$\u03c9_t$ tells the number of drones that run out of power at the time step t, shown by Equation 16,\n$\u03c9_t = |\\{B_k | \u2200k \u2208 K, B_k < B_{return}\\}.|$ (16)\nand \u03b3 and \u03b2 are coefficients used to adjust the impact of $\u03bc_t$ and $\u03c9_t$ to achieve better energy efficiency. This reward function encourages drones to execute more tasks or more task portions per time step while reserving energy and discourages them from using up their battery for unnecessary travel or inefficient cooperation.\nThe action and state space coupled with the reward function are used in the proposed collaborative execution model, described as Algorithm 1. This proposed model lets each drone have its DQN and individually learn based on the share reward function and state space. We set a threshold, $batch\\_size \u00d7 \u03c8$, in this model to allow it to start learning from the past experience when there are enough samples stored in the replay memory with the consideration of avoiding unnecessary instability. In this model, another threshold, exploration rate \u0394, is to control the probability the model chooses an action based on its learning. If the probability is not larger than \u0394, the model randomly picks an action from A and investigates what occurs in the environment. Otherwise, the model selects the action with the highest Q-value for its present state. We will explore these two thresholds more in the next section."}, {"title": "V. PERFORMANCE EVALUATION", "content": "We adopt the base case parameters published in [23] to calculate the energy consumption rates in our simulation, as shown in Table I.\n$P_{facilities} = 3, P_{hover} = 4$, and\n$P_{forward} = 2.5$ for calculation simplification in our ex- periments. We set the battery capacity B = 1800 so that the drones would have enough energy to learn to execute tasks cooperatively. The DQNs of drones are built by using Tensorflow-Keras.\nThis study uses the success rate to measure the performance of the proposed energy-aware collaborative execution MARL model. The success rate is calculated as the ratio between the number of accomplished missions and the total number of missions during a specific period. The average accumulated reward of successful missions is adopted to measure the execution efficiency of completed missions. The accumulated reward is the sum of rewards of all steps of an episode. The average accumulated reward of successful missions is the mean of the accumulated rewards of episodes in which the mission has been successfully completed. A higher average accumulated"}, {"title": "A. Threshold Experiments", "content": "In this set of experiments, we set the initial exploration rate \u0394 to 0.5 and gradually decrement it by 3 \u00d7 10-6 over the experiments till it reaches 0.15. We compare the success rate at different exploration rates while varying \u03c8 following this series $\\{1, 2, 5, 10\\}$. We set the number of tasks K = 4, and the task length $T_i = 5, \u2200i \u2208 K$. The four task locations are shown as Fig. 3 and remain static in all experiments of this set.\nThe simulation results for this set of experiments are shown in Fig. 4. From Fig. 4 (a), we can observe that after learning from the first few episodes, the success rates keep compar- atively steady before decreasing with the exploration rate decrement. Although the success rates of different \u03c8 values are close, the performance is relatively higher when \u03c8 = 5. When \u0394 reaches 0.2, the success rate is still above 0.8 and the most relative to 0.9 when \u03c8 = 5. The differences between the average accumulated rewards of successful missions are more pronounced, as shown in Fig. 4 (b). When \u03c8 = 5, the average accumulated rewards of successful missions is higher than other \u03c8 values after the initial learning phase. Therefore, we set \u03c8 = 5 and let the exploration rate \u0394 start with 0.5 and gradually decrease to 0.2 in the other two sets of experiments."}, {"title": "B. Task Length and Location Experiments", "content": "We conducted three experiments in this set. The mission of each experiment is composed of four tasks, K = 4. The four tasks in the first experiment locate at the trajectory points indicated by Fig. 3, and each task has a random length between one and five time steps per episode, $1 < T_i \u2264 5, \u2200i \u2208 K$. The location of each task in the second experiment is set to a random potential task location indicated in Fig. 3, but every task has the same length per episode, five time steps, $T_i = 5, \u2200i \u2208 K$. The third experiment inspects the performance when both the location and length of each task are random per episode. The length of each task in the third experiment is a random value between one and five, $1 < T_i < 5, \u2200i \u2208 K$.\nFig. 5 shows the results of this set of experiments. From Fig. 5 (a), the success rate differences among the three experiments are subtle. Beyond the initial learning phase at the beginning of experiments, the success rates of the three experiments stay close to each other and gradually decrement to 0.8 when the exploration rate reduces to 0.2. We can learn that the task length and location don't impact the success rate significantly, even when each episode has a different set of tasks to be finished. Fig. 5 (b) indicates that the average accumulated rewards of successful missions are higher when the task locations are random. When exploration rate \u0394 < 0.3, the fixed task length over performs the random task lengths. We think the reason is two-fold: first, the random task location can let the proposed collaborative execution model learn to find a task more quickly, and second, the fixed task length is easier for the model to learn to collaboratively accomplish the mission than the random task length."}, {"title": "C. Task Density Experiments", "content": "We vary the number of tasks from two to ten in this study, and each task has a random length between one and five time steps, $1 < T_i < 5, \u2200i \u2208 K$, located at a random potential task location indicated in Fig. 3 per episode. We define task density as the ratio between the number of tasks and the grid size. The size of the grid remains the same in this study. Hence, the larger the number of tasks, the higher the task density.\nAs Fig. 6 illustrates, the task density affects the performance of the proposed collaborative execution model. In Fig. 6 (a), when the task density is very sparse, e.g., two tasks in the experimental area, the success rate is much lower than higher task densities, such as four or more tasks in the same area. When the task density is high, e.g., eight or more tasks in the experimental area, the success rate is nearly 100% regardless of the exploration rates. We believe that more tasks in the net- work area enable the proposed model to gain more experiences of locating tasks and learn more quickly in the early phase, allowing drones to select actions based on more accurate predictions built on those experiences. As shown in Fig. 6 (b), the average accumulated rewards of successful missions line up with the success rates. The average accumulated rewards of successful missions are more significant when the task density is higher, as our collaborative execution model learns better in the high task density scenarios."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We propose an energy-aware collaborative execution MARL model in this study to cope with the challenge of limited drone battery capacity in mission-oriented drone networks. We let each drone have its DQN and train it separately while sharing the same reward function across all drones to encourage them to cooperate more effectively. The battery levels of drones drive the reward function to stimulate them to learn to complete the mission collaboratively while being aware of their battery levels.\nWe conducted three sets of experiments in this study to investigate the impact of some hyperparameters of the pro- posed model, task length and location, and task density. Based on our simulation study, our proposed model can successfully accomplish the mission assigned to the drone network at least 80% of the time regardless of the task length and location the mission contains when the exploration rate is not less than 0.2 and the number of tasks in the experimental area is not less than four. The proposed model can achieve a 100% success rate regardless of the exploration rates in scenarios where the task density is not too sparse.\nOur next step is to evaluate our proposed model in larger grids and study the impact on its performance from the area covered by the mission. We also plan to extend the 2D space used in this study into 3D and inspect the proposed collab- orative execution model in the new 3D space. We can then examine the performance of the proposed model under more extensive action and state space. Moreover, investigating and formulating the relationship between the number of drones, battery capacity, task density, and task length is also on our to-do list. Last but not least, we are working on a variation of the proposed model, in which we let each drone use an individual reward that encourages the drone to finish tasks quickly and efficiently. Our study related to this variation is three-fold: first, whether the individual reward function will affect the performance of the mission-oriented drone networks; second, whether the individual reward function will result in competition among drones; and third, whether a competitive mission-oriented drone network will over-perform a collaborative mission-oriented drone network."}]}