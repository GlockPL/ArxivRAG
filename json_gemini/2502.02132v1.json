{"title": "How Memory in Optimization Algorithms Implicitly Modifies the Loss", "authors": ["Matias D. Cattaneo", "Boris Shigida"], "abstract": "In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as memory, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented (X. Chen et al., 2023).", "sections": [{"title": "Introduction", "content": "Many optimization methods used in deep learning are first-order methods with exponentially decaying memory. For example, adding \"momentum\" to gradient descent (GD) is a well-established practice to make training smoother and convergence faster (e. g. Krizhevsky, Sutskever, and G. E. Hinton (2012)). Adaptive methods such as Adam (Kingma and Ba, 2017), RMSProp (Tieleman, G. Hinton, et al., 2012), AdamW (Loshchilov and Hutter, 2019), and AdaFactor (Shazeer and Stern, 2018), which are commonly used to train large language models (Grattafiori et al., 2024; DeepSeek-AI et al., 2024; Chowdhery et al., 2023), all have exponentially decaying memory. Despite the popularity of such optimization methods, there is little theoretical knowledge about the implicit regularization memory introduces to them (potentially informing what regions of the loss space the method takes the iterates to, what minima they converge to, how such minima influence the generalization of the trained model, and so on). In this article, we introduce a general framework for identifying such regularization.\nWe study a general class of optimization algorithms described by the following iteration\n$\\theta^{(n+1)} = \\theta^{(n)} \u2013 hF^{(n)} (\\theta^{(n)}, ..., \\theta^{(0)}),$ (1)\nwhere $\\theta^{(n)} \\in \\mathbb{R}^d$ are the (evolving) parameters of the machine learning model, $\\theta^{(0)}$ is some initial condition, h is the step size or learning rate, and the functions $F^{(n)}$ map from (some subset of) $(\\mathbb{R}^d)^{n+1}$ to $\\mathbb{R}^d$ and are allowed to be different at each iteration. The right-hand side in Eq. (1) depends on the whole history of previous iterates, which means the algorithm has memory.\nFor many algorithms used in practice, dependence on the history comes in one specific form: by using what we call \"momentum variables\u201d, that is, exponential averages of some functions of the iterate $\\theta^{(n)}$ (usually, more specifically, functions of the loss gradient). We present five leading examples to illustrate this point."}, {"title": "Developing Intuition: Memory Regularizes GD with Momentum", "content": "We provide a heuristic explanation of our technique, considering the simplest algorithm with exponentially decaying memory: heavy-ball momentum GD (Example 1.1). As explained above, we would like to remove the dependence of the right-hand side in\n$\\theta^{(n+1)} = \\theta^{(n)} \u2013 h \\sum_{k=0}^{n} \\beta^{n-k}\u2207L(\\theta^{(k)})$ (5)\non the \u201cpast\u201d iterates $\\theta^{(n-1)}, ..., \\theta^{(0)}$, leaving only the dependence on the \"current\" iterate $\\theta^{(n)}$. Let us represent \"past\" iterates through the \"current\" one. First, write\n$\\theta^{(n-1)} = \\theta^{(n)} + h \\sum_{b=0}^{n-1}\\beta^{b}\u2207L(\\theta^{(n-1-b)})\n= \\theta^{(n)} + h \\sum_{b=0}^{n-1}\\beta^{b}\u2207L(\\theta^{(n)}) + O(h^{2}),$\nwhere the second equality relies on exponential averaging to replace historical iterates with $\\theta^{(n)}$, influencing only higher-order terms. Similarly,\n$\\theta^{(n-2)} = \\theta^{(n-1)} + h \\sum_{b=0}^{n-2}\\beta^{b}\u2207L(\\theta^{(n-2-b)})\n= \\theta^{(n-1)} + h \\sum_{b=0}^{n-2}\\beta^{b}\u2207L(\\theta^{(n)}) + O(h^{2}),$\n$= \\theta^{(n)} +h\\{\\sum_{b=0}^{n-1}\\beta^{b}+ \\sum_{b=0}^{n-2}\\beta^{b}\\}\u2207L(\\theta^{(n)}) + O(h^{2}),$\nwhere the last equality follows by inserting the expression for $\\theta^{(n-1)}$. Continue like this up to\n$\\theta^{(n-k)} = \\theta^{(n)} + h \\{\\sum_{l=1}^{k} \\sum_{b=0}^{n-l}\\beta^{b}\\}\u2207L(\\theta^{(n)}) + O(k^{2}h^{2}),$\nwhere the $k^{2}$ provides an estimate on the accumulation of error terms of order $h^{2}$.\nWe have now represented all the historical iterates through the current one. Combining it with Taylor expansion around $\\theta^{(n)}$ in Eq. (5), we obtain\n$\\theta^{(n+1)} = \\theta^{(n)} - h \\sum_{k=0}^{n} \\beta^{k}\\left\\{\u2207L(\\theta^{(n)}) + h\u2207^{2}L(\\theta^{(n)}) \\sum_{l=1}^{k} \\sum_{b=0}^{n-l} \\beta^{b}\u2207L(\\theta^{(n)}) + O(k^{2}h^{2})\\right\\}\n$\\theta^{(n+1)} = \\theta^{(n)} - h\\frac{1-\\beta^{n+1}}{1-\\beta}\u2207L(\\theta^{(n)})\n- h^{2} \\frac{\\beta[1-(2n+1)(1-\\beta)-\\beta^{2n+1}]}{(1-\\beta)^{3}}\u2207^{2}L(\\theta^{(n)})\u2207L(\\theta^{(n)}) + O(h^{3}).$"}, {"title": "General Theory: The Effect of Memory", "content": "The general form of an optimization algorithm with memory is given by Eq. (1). The only property of memory we use is that it (uniformly in n) decays exponentially fast, as made precise by Assumption 3.1 below. Openness and convexity of the domain of optimization D, that is, where all {$\\theta^{(n)}$} will be assumed to lie, are innocuous assumptions (e.g., $\\mathbb{R}^d$ is open and convex); we impose them to avoid technicalities with differentiation and Taylor expansion.\nAssumption 3.1 (Memory Decay). Let D be an open convex domain in $\\mathbb{R}^d$. Let {$F^{(n)} (\\theta^{(n)},..., \\theta^{(0)})$}$_{n=0}^{\\infty}$ be a family of functions $D^{n+1} \\rightarrow \\mathbb{R}^d$, two times continuously differentiable on their respective domains, such that for any $n \\in \\mathbb{Z}_{\\geq 0}, k_{1},k_{2} \\in \\{0, ..., n\\}, r, i, j \\in \\{1, ..., d\\}$,\n$|F^{(n)}| < \\gamma_{-1}$, (7)\n$\\left|\\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k_{1})}}\\right| < \\gamma_{k_{1}}$, (8)\n$\\left|\\frac{\\partial^{2} F^{(n)}}{\\partial \\theta^{(n-k_{1})} \\partial \\theta^{(n-k_{2})}}\\right| < \\gamma_{k_{1},k_{2}}$, (9)\nwhere $F^{(n)} = (F_{1}^{(n)}, ..., F_{d}^{(n)})$, and $\\gamma_{-1}, \\gamma_{k_{1}}$ and $\\gamma_{k_{1},k_{2}}$ are a families of positive reals (not depending on n) satisfying $\\sum_{k_{1}=1}^{\\infty} \\gamma_{k_{1}} < \\infty$ + $\\sum_{k_{1},k_{2}=1}^{\\infty} \\gamma_{k_{1},k_{2}} < \\infty$.\nIn the examples listed in the introduction, $F^{(n)}$ satisfies a more specific form that can be used to give more primitive conditions for Assumption 3.1. The following remark discusses this case.\nRemark 3.2 ($F^{(n)}$ as a function of momentum variables). Let {$g^{(n)}$}$_{l=1}^{L}$ be $L$ two times contin- uously differentiable functions $D \\rightarrow \\mathbb{R}^d$, uniformly bounded along with two derivatives, and let $\\Phi : (\\mathbb{R}^d)^{L} \\rightarrow \\mathbb{R}^d$ be a fixed two times continuously differentiable function, uniformly bounded along with two derivatives. Let {$\\beta_{l}$}$_{l=1}^{L}$ be fixed reals in [0,1), and {$b_{l}^{(n+1)}$}$_{l=1}^{L}$ be $L$ bounded nonnegative sequences of reals (for $n \\in \\mathbb{Z}_{>0}$). If the function $F^{(n)}$ has the specific form\n$F^{(n)} (\\theta^{(n)},..., \\theta^{(0)}) := \\Phi(m_{1}^{(n+1)},..., m_{L}^{(n+1)}), (10)\nwhere $m_{l}^{(n+1)} := b_{l}^{(n+1)} \\sum_{k=0}^{n} \\beta_{l}^{k}g_{l}^{(n-k)} (\\theta^{(n-k)}),$ (11)\nthen it satisfies Assumption 3.1 (Lemma D.1). In the full-batch case, the same holds true except $g_{l}^{(n)} = g_{l}^{(n)}$ are not allowed to depend on n."}, {"title": "Deriving the Memoryless Approximation", "content": "By Taylor expansion with the Lagrange remainder,\n$F^{(n)} (\\theta^{(n)},..., \\theta^{(0)}) \u2013 F^{(n)} (\\theta^{(n)}, ..., \\theta^{(n)})\n= \\sum_{k=1}^{n} (\\theta^{(n-k)} \u2013 \\theta^{(n)})^{T} \\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k)}} (\\theta^{(n)},..., \\theta^{(n)})\n+ \\frac{1}{2} \\sum_{k_{1},k_{2}=1}^{n} (\\theta^{(n-k_{1})} \u2013 \\theta^{(n)})^{T} \\frac{\\partial^{2} F^{(n)}}{\\partial \\theta^{(n-k_{1})} \\partial \\theta^{(n-k_{2})}} (\\zeta) (\\theta^{(n-k_{2})} \u2013 \\theta^{(n)})\n= \\sum_{k=1}^{n} (\\theta^{(n-k)} - \\theta^{(n)})^{T} \\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k)}} (\\theta^{(n)},..., \\theta^{(n)}) + O(h^{2}), (11)\nwhere $\\zeta$ is some point on the segment between $(\\theta^{(n)},..., \\theta^{(0)})$ and $(\\theta^{(n)},..., \\theta^{(n)})$; in the last step we used Eq. (9), $\\theta^{(n-k_{1})} \u2013 \\theta^{(n)} = O(k_{1}h)$, and $\\theta^{(n-k_{2})} \u2013 \\theta^{(n)} = O(k_{2}h)$.\nNext, write\n$\\theta^{(n-k)} - \\theta^{(n)}\n= \\sum_{s=n-k}^{n-1} (\\theta^{(s)} \u2013 \\theta^{(s+1)})\n= h \\sum_{s=n-k}^{n-1} F^{(s)} (\\theta^{(s)},..., \\theta^{(0)})\n= h \\sum_{s=n-k}^{n-1} F^{(s)} (\\theta^{(n)},..., \\theta^{(n)}) + O(k^{2}h^{2}),$\nwhere in the last step we used $F^{(s)} (\\theta^{(s)}, ..., \\theta^{(0)}) \u2013 F^{(s)} (\\theta^{(n)},..., \\theta^{(n)}) = O((n \u2212 s)h)$, which follows from Taylor expansion and Eq. (8). Insert this into Eq. (11) and use Eq. (8) again to continue:\n$F^{(n)} (\\theta^{(n)},..., \\theta^{(0)}) \u2013 F^{(n)} (\\theta^{(n)}, ..., \\theta^{(n)})$\n$= h \\sum_{k=1}^{n} \\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k)}} (\\theta^{(n)},..., \\theta^{(n)}) \\sum_{s=n-k}^{n-1} F^{(s)} (\\theta^{(n)},..., \\theta^{(n)}) + O(h^{2}).$\nWe conclude that the original numerical iteration can be rewritten as\n$\\theta^{(n+1)} = \\theta^{(n)} \u2013 h[F^{(n)} (\\theta^{(n)}) + M^{(n)} (\\theta^{(n)})] + O(h^{3}),$\nwhere the linear in h correction function is defined as $M^{(n)} = (M_{1}^{(n)},..., M_{d}^{(n)})$ with\n$M^{(n)} (\\theta) := h \\sum_{k=1}^{n} \\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k)}} (\\theta) \\sum_{s=n-k}^{n-1} F^{(s)} (\\theta). (12)$\nThe derivation of the memoryless iteration is now complete. To make it easier to apply it to our examples, we can write it more specifically as the following section shows."}, {"title": "$F^{(n)}$ as a Function of Momentum Variables", "content": "Specializing to the setup of Remark 3.2, and for any $s, n \\in \\mathbb{Z}_{\\geq 0}, F^{(s)} (\\theta) = \\Phi (\\bar{g}_{1}^{(s+1)} (\\theta), ..., \\bar{g}_{L}^{(s+1)} (\\theta)),$\nwhere $\\bar{g}_{l}^{(s+1)} (\\theta) := b_{l}^{(n+1)} \\sum_{k=0}^{n} \\beta_{l}^{k}g_{l}^{(n-k)} (\\theta),$\nand\n$\\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k)}} (\\theta)\n= \\sum_{l=1}^{L} \\sum_{i} \\frac{\\partial\\Phi}{\\partial m_{l;i}} (\\bar{g}_{1}^{(n+1)} (\\theta), ..., \\bar{g}_{L}^{(n+1)} (\\theta)) g_{l}^{(n-k)} (\\theta).$\nTherefore, in this special case, the correction term in the memoryless iteration (4) is given by, for r = 1, ..., d,\n$M^{(n)} (\\theta)\n= h \\sum_{l=1}^{L} \\sum_{i} b_{l}^{(n+1)} \\frac{\\partial\\Phi}{\\partial m_{l;i}} (\\bar{g}_{1}^{(n+1)} (\\theta), ..., \\bar{g}_{L}^{(n+1)} (\\theta))\n\u00d7 \\sum_{k=1}^{n} \\beta_{l}^{k}g_{l}^{(n-k)} (\\theta) \\sum_{s=n-k}^{n-1} \\Phi (\\bar{g}_{1}^{(s+1)} (\\theta), ..., \\bar{g}_{L}^{(s+1)} (\\theta)).$\nIn the full-batch case $g_{l}^{(s)} (\\theta) = g_{l}(\\theta)$, this can be simplified further. Let us assume $b_{l}^{(n+1)} \\rightarrow b_{l}$, where $b_{l}$ is constant in n. Then $\\bar{g}_{l}^{(n+1)} (\\theta)$ also become constant in n: specifically, they settle to $\\bar{g}_{l}(\\theta) := b_{l}(1 - \\beta_{l})^{-1}g_{l}(\\theta)$. Lemma D.2 then implies that the iteration becomes close to\n$\\theta^{(n+1)} = \\theta^{(n)} \u2013 h[\\Phi(\\bar{g}_{1}(\\theta^{(n)}), ..., \\bar{g}_{L}(\\theta^{(n)})) + M^{(n)} (\\theta)],$\nwith\n$M^{(n)} (\\theta)\n= h \\sum_{l=1}^{L} \\sum_{i} \\frac{b_{l}\\beta_{l}}{(1 \u2013 \\beta_{l})^{2}} \\frac{\\partial\\Phi}{\\partial m_{l;i}} (\\bar{g}_{1}(\\theta^{(n)}), ..., \\bar{g}_{L}(\\theta^{(n)}))\n\u00d7 \u2207$g_{l;i} (\\theta^{(n)})\\Phi(\\bar{g}_{1}(\\theta^{(n)}),...,\\bar{g}_{L}(\\theta^{(n)})).$\nCorrection terms for all examples are provided in Appendix C."}, {"title": "Approximation Bound", "content": "An argument similar to the derivation in Section 3.1 can be made to obtain the following result."}, {"title": "Application: Memory Anti-Regularizes AdamW but not Lion", "content": "We first study AdamW with memory by an application of Theorem 3.3 and Corollary 3.4. Neglecting coefficients decaying to zero exponentially fast, we have\n$\\theta^{(n+1)} = \\theta^{(n)} - h \\left[ \\frac{\\nabla L(\\theta^{(n)})}{\\sqrt{(\\nabla L(\\theta^{(n)}))^{2} + \\epsilon}} + M^{(n)}(\\theta^{(n)})\\right],\n\u2248sign(\u2207L(\\theta^{(n)}))\nwhere $M^{(n)}(\\theta)$ is given by\n$M^{(n)} (\\theta) = \\frac{h} {\\left[\\frac{\\beta_{1}(1 \u2013 \\beta_{1})^{-1} - \\beta_{2}(1 \u2013 \\beta_{2})^{-1}}{\\sqrt{(\\nabla L(\\theta))^{2} + \\epsilon}} + \\frac{\\epsilon \\beta_{2}(1 \u2013 \\beta_{2})^{-1}}{((\\nabla L(\\theta))^{2} + \\epsilon)^{3/2}}\\right] (\u2207||\u2207L(\\theta)||_{1,\\epsilon} + \\frac{1}{2}\u2207^{2}L(\\theta) \\theta)}.$ Here $||\u00b7||_{1,\\epsilon}$ is the perturbed one-norm defined as $||v||_{1,\\epsilon} := \\sum_{i=1}^{d} \\sqrt{v_{i}^{2} + \\epsilon}$. Taking $\\epsilon$ to zero, we can write this in the form of preconditioned gradient descent (with decoupled weight decay):\n$\\theta^{(n+1)} = (1 - h\\lambda)\\theta^{(n)} - h \\frac{\\nabla L(\\theta^{(n)})}{\\sqrt{|(\\nabla L(\\theta^{(n)}))|}},$\nwhere\n$\\bar{L}(\\theta) = L(\\theta) + h \\left[ \\frac{\\beta_{1}}{1 - \\beta_{1}} - \\frac{\\beta_{2}}{1 - \\beta_{2}}\\right]||\u2207L(\\theta)||_{1} + h\\lambda \\left[ \\frac{\\beta_{2}}{1 - \\beta_{2}}\\right](\u2207^{2}L(\\theta) \\theta).$\nis the modified loss. We see that the correction term $\\frac{\\beta_{1}}{1 - \\beta_{1}}||\u2207L(\\theta)||_{1}$, coming from exponentially averaging the gradients, penalizes the one-norm of the \u2207L, acting as a regularizer, but the correction term $-\\frac{\\beta_{2}}{1 - \\beta_{2}}||\u2207L(\\theta)||_{1}$, coming from exponentially averaging the squared gradient components, anti-penalizes this one-norm and acts as an anti-regularizer. Moreover, if $\\beta_{1} < \\beta_{2}$ (essentially always in practice), the anti-regularization is stronger than the regularization. Thus, memory anti-regularizes AdamW.\nConsider now Lion-K (Example 1.5). Neglecting terms going to zero exponentially fast as n \u2192 \u221e, the memoryless iteration is\n$\\theta^{(n+1)} = (1 \u2013 h\\lambda)\\theta^{(n)} \u2013 h[-\\nabla K(-\\nabla L(\\theta^{(n)})) + M^{(n)} (\\theta)],$\nwhere\n$M^{(n)} (\\theta) = -h \\frac{\\beta_{1}}{1 \u2013 \\beta_{2}} \u2207^{2}K(-\\nabla L(\\theta))\u2207^{2}L(\\theta) [\u2207K(-\\nabla L(\\theta)) \u2013 \\lambda \\theta].$"}, {"title": "Further Implications", "content": "We have taken a very general algorithm (1) and, assuming smoothness and that memory decays sufficiently fast, converted it to a memoryless iteration $\\theta^{(n+1)} = \\theta^{(n)} \u2013 h[F^{(n)} (\\theta^{(n)}) + M^{(n)} (\\theta^{(n)})] + O(h^{3})$. In this section we find an ODE in the form $\\dot{\\theta} = G_{h}(\\theta)$ whose continuous solution, with initial condition $\\theta(0)$, will approximate the memoryless iteration established in Section 3. Let us derive $G_{h}(\\theta)$ in the form of a power series $G_{1}(\\theta) +hG_{2}(h) + O(h^{2})$, where $O(h^{2})$ means \u201cterms of order at least two in h\". Relating the iteration number n of a discrete iteration and the time point t = nh on a continuous trajectory, we would like the continuous trajectory to satisfy the same one-step relation as the discrete iteration, up to O($h^{3}$):\n$\\theta((n + 1)h) = \\theta(nh) \u2013 h [F^{(n)} (\\theta(nh), ..., \\theta(nh)) + M^{(n)}(\\theta(nh))] + O(h^{3}).$\nIn fact, we will ensure it is true for nh replaced by any t:\n$\\theta(t + h) = \\theta(t) \u2013 h [F^{(n)} (\\theta(t),..., \\theta(t)) + M^{(n)} (\\theta(t))] + O(h^{3}). (13)$\nBut, using a Taylor expansion, and recalling that we are finding the trajectory satisfying $\\dot{\\theta}(t) = G_{h}(\\theta(t))$, hence $\\theta(t) = G_{h}(\\theta(t))\\dot{\\theta}(t)$, we have\n$\\theta(t + h) = \\theta(t) + h\\dot{\\theta}(t) + \\frac{h^{2}}{2}\\ddot{\\theta}(t)+O(h^{3})\n= \\theta(t) + h\\{G_{1}(\\theta(t)) + hG_{2}(\\theta(t)) + O(h^{2})\\}\n+ \\frac{h^{2}}{2}\\left\\{\\nabla G_{1}(\\theta(t))G_{1}(\\theta(t) + O(h)\\right\\} +O(h^{3})\n= \\theta(t) +hG_{1}(\\theta(t))\n+\\frac{h^{2}}{2} \\left\\{\\nabla G_{1}(\\theta(t))G_{1}(\\theta(t)\\right\\}-O(h^{3}).$\nIn order to match (13), we need to have\n$G_{1}(\\theta) = \u2013F^{(n)} (\\theta,..., \\theta),$\n$G_{2}(\\theta) = \u2013 M^{(n)} (\\theta)/h + \\frac{\\nabla G_{1}(\\theta) G_{1}(\\theta)}{2}.$\nSo, apart from the correction term coming from memory, the ODE $\\dot{\\theta} = G_{1}(\\theta) + hG_{2}(h)$ derived has another term\n$\\frac{h^{2}}{2} \\nabla G_{1}(\\theta)G_{1}(\\theta)$$\narising from the fact that the algorithm is discrete."}, {"title": "Mini-Batch Training", "content": "In specific cases, it is possible to identify the additional implicit regularization that is introduced to the algorithm by noise, if mini-batch training is used as opposed to full-batch. Assume that the form of $F^{(n)} (\\theta^{(n)}, ..., \\theta^{(0)})$ is given by\n$F^{(n)} (\\theta^{(n)}, ..., \\theta^{(0)}) = \\sum_{k=0}^{n} \\beta^{k}g^{(n-k)} (\\theta^{(n-k)}),$\nwhere the $g^{(n-k)}(.)$ functions are uniformly bounded along with their derivatives.\nThe correction term introduced by memory (12) is\n$M^{(n)} (\\theta)\n:= h\\sum_{k=1}^{n}\\frac{\\partial F^{(n)}}{\\partial \\theta^{(n-k)}} (\\theta,...,\\theta) \\sum_{s=n-k}^{n-1} F^{(s)} (\\theta,..., \\theta)\n$ = h\\beta \\sum_{k=0}^{n-1} \\beta^{k}\u2207g^{(n-1-k)} (\\theta) \\sum_{l=1}^{k+1} \\sum_{b=0}^{n-l} \\beta^{b}g^{(n-1-b)} (\\theta).$\nAssume that n is a (large) number of mini-batches in one epoch, and mini-batches are sampled randomly without replacement, with each permutation equally likely. Let us take the expectation of the correction term with respect to this randomness, that is, take the average over all re-orderings of (g(0),...,g(n)):\n$\\frac{1}{\\binom{n+1}{k+1}} \\sum_{\\pi k=0}^{n-1} \\sum_{k=1}^{k+1 \\sum_{l=1} n-l} \\beta \\sum_{k=0} \\beta^{k}\u2207g^{({\\pi}(n-1-k))}\\beta^{(n)} (\\theta) \\sum_{l=1} \\sum_{b=0}^{n-l} \\beta^{b}g^{({\\pi}(n-1-b))} (\\theta).$\nNote that $E \\nabla g^{(i)} (\\theta) g^{(j)} (\\theta)$ depends only on whether i = j or i \u2260 j. Therefore,\n$E[M^{(n)} (\\theta)]/h = C_{3}(\\beta)E [\\nabla g^{(1)} (\\theta)g^{(1)} (\\theta)]$\n+ $C_{4}(\\beta)E [\\nabla g^{(1)} (\\theta)g^{(2)} (\\theta)],$\nwhere $C_{3}(\\beta)$ and $C_{4}(\\beta)$ can be calculated as\n$C_{3}(\\beta) := \\beta \\sum_{k=0}^{n-1} \\beta^{k} \\frac{k+1}{n+1} \\rightarrow \\frac{\\beta}{n \\rightarrow \\infty} (1-\\beta)^{2} (1 + \\beta)},$\n$C_{4}(\\beta) := \\beta \\sum_{k=0}^{n-1} \\beta^{k} \\frac{k+1}{n+1} - C_{3}(\\beta) \\rightarrow \\frac{2\\beta^{2}}{n \\rightarrow \\infty} (1-\\beta)^{3} (1 + \\beta)}.$ We can simplify\n$E [\\nabla g^{(1)} (\\theta)g^{(2)} (\\theta)] = \\frac{1}{\\binom{n+1}{i\u2260j}} \\sum_{g \\nabla g^{(i}} (\\theta)g^{(j)} (\\theta)$\n= $g \\nabla (\\theta)g^{({\\nabla}_{0})} + o_{n}(1),$ where \\(0)} = \\frac{1}{\\binom{n+1}{k}} + g^{(j)} \\text { and } 0_{n}(1) g g(0)\nto 8 \n \n3 Proof 2-61)\nE \nl\\theta)\\\\theta)\n \\frac{\\mathbf{3}}{(0-\\\\right), \\text { 4}} \nt\t(1-\nThus, for large n we can write\nE\\\\h\nm\t\t(1)\n\n\\qquad\tC\t\\\\theta)\\\n\\qquad\\11. \\text -\\\n\\qquad"}, {"title": "Approximation Bound", "content": "An argument similar to the derivation in Section 3.1 can be made to obtain the following result."}]}