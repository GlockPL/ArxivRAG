{"title": "Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation", "authors": ["Haocheng Tang", "Jing Long", "Junmei Wang"], "abstract": "In this work, we introduce Auxiliary Discriminator Sequence Generative Adversarial Networks (ADSeqGAN), a novel approach for molecular generation in small-sample datasets. Traditional generative models often struggle with limited training data, particularly in drug discovery, where molecular datasets for specific therapeutic targets, such as nucleic acids binders and central nervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by integrating an auxiliary random forest classifier as an additional discriminator into the GAN framework, significantly improves molecular generation quality and class specificity.\nOur method incorporates pretrained generator and Wasserstein distance to enhance training stability and diversity. We evaluate ADSeqGAN on a dataset comprising nucleic acid-targeting and protein-targeting small molecules, demonstrating its superior ability to generate nucleic acid binders compared to baseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling strategy, ADSeqGAN also significantly improves CNS drug generation, achieving a higher yield than traditional de novo models. Critical assessments, including docking simulations and molecular property analysis, confirm that ADSeqGAN-generated molecules exhibit strong binding affinities, enhanced chemical diversity, and improved synthetic feasibility.\nOverall, ADSeqGAN presents a novel framework for generative molecular design in data-scarce scenarios, offering potential applications in computational drug discovery. We have demonstrated the successful applications of ADSeqGAN in generating synthetic nucleic acid-targeting and CNS drugs in this work.", "sections": [{"title": "1 Introduction", "content": "Non-supervised molecular generation has become a cornerstone of modern computational drug discovery, offering innovative approaches for designing novel compounds with desired properties. Over time, diverse methodologies have emerged, categorized by gener-\native objectives and molecular representations. For instance, models have been developed for molecular property optimization, probabilistic distribution learning, and site-specific design[1]. Molecular representations vary from simplified molecular-input line-entry system (SMILES) strings and molecular graphs to molecular fingerprints and 3D point clouds [2, 3, 4]. Meanwhile, neural network architectures like Word2Vec (W2V), sequence-to-sequence (Seq2Seq) models, transformers, graph convolutional networks (GCNs), graph attention networks (GATs), message-passing neural networks (MPNNs), and 3D-point networks have powered these advancements. Generative models such as recurrent neural network (RNN), generative adversarial networks (GANs), variational autoencoders (VAEs), adversarial autoencoders (AAEs), normalizing flows, and diffusion models have further diversified the toolkit for molecular design[2, 5, 6].\nAmong all the molecular representation, SMILES notations stand out due to their simplicity, widespread database availability, and extensive tool support. Their sequential representation makes them particularly amenable to natural language processing (NLP) techniques, which further reduce computational and storage costs. This positions SMILES-based approaches as highly advantageous for expanding compound spaces guided by molecular properties.\nGANs remain a classic and versatile class of generative models, offering key advantages over VAEs and Diffusion models. By avoiding assumptions of Gaussian priors, GANs are well-suited for datasets with non-Gaussian distributions. Additionally, GANs avoid maximum likelihood estimation (MLE), which, while stabilizing optimization, can constrain generative diversity[7]. Over the years, many GAN variants have been proposed to address specific challenges in sequence generation, including Sequence GAN (SeqGAN)[8] and Objective Reinforced GAN (ORGAN) [9]. SeqGAN leverages policy gradients to optimize sequence outputs, while ORGAN incorporates task-specific rewards to guide generation through reinforcement learning (RL). On the other hand, Auxiliary Classifier GANs (ACGANs)[10] incorporate class labels into the both generator and discriminator to stabilize training, while pre-trained GAN"}, {"title": "2 Related Work", "content": "Previous GAN-based models for SMILES sequence generation include SeqGAN and ORGAN. These foundational approaches were later extended with downstream networks such as ORGANIC[18], RANC[16] and ATNC[19], which tailored the generative process to specific application objectives.\nSince the introduction of GANs[20], advancements in architectures[10, 21, 22, 23, 24], training strategies [25], and objective functions[11, 17, 26, 27] have led to significant progress. Despite their success in image-based tasks, many methods have not yet been applied to GANs for sequence generation. In this work, we integrate these advancements including ACGAN, pre-trained discriminators, and WGAN objective function into a sequence-generation GAN framework, with a particular focus on molecular sequence generation.\nMolecules, due to their structured nature and extensive prior knowledge, are particularly well-suited for transfer learning. Descriptor-generation tools like RDKit[28] and OpenBabel[29] allow for the extraction of rich molecular features, which can be effectively transferred to unseen tasks, datasets, and domains. In our work, we leverage these transferable molecular property representations for unsupervised model training, enabling the generation of high-quality SMILES strings even with limited training data.\nGANs can amplify data to address data scarcity issue in molecule predicting task[30], outperforming traditional methods like Synthetic Minority Oversampling Technique (SMOTE). At present, data enhancement methods have not been used to generate molecules in GANs. For highly imbalanced datasets, we employ over-sampling to enrich minority classes."}, {"title": "3 Methods", "content": "Classic GANs consist of two parts: generator G, parameterized by \\u03b8g to produce sample Y and discriminator D, parameterized by \\u03b8d to distinguish synthetic data from real ones. The generator and discriminator of original GAN models are trained in alternation, following a minimax game:\n$\\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{Y \\sim p_{\\text {data }}(Y)}[\\log D(Y)]+\\mathbb{E}_{Y \\sim p_{G}(Y)}[\\log (1-D(Y))]$\\nHowever, these traditional GANs can not be directly applied to sequence generation tasks, so GAN+RL based SeqGAN and ORGAN were developed. Nonetheless, in our test task, these models struggled to learn enough features with limited real data. To solve these problems, we propose Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGANs). The process of applying ADSeqGANs to molecular generation is shown in Figure 1: Firstly, a hybrid database consisting of a small number of samples of the desirable class and samples of auxiliary classes is constructed. Next the molecule descriptors with strong classification ability are selected by logistic regression method as parameters for pre-training to get a classifier, which is then added to GAN training as auxiliary discriminators. At the same time,"}, {"title": "3.1 Formulation", "content": "N\nIn this work, we introduce a set of molecular features F and a set molecule classes C, used to train classifiers as aided discriminators $\\{D_{n}\\}_{n=1}^{N}$, through corresponding classifier function $\\{C_{n}\\}_{n=1}^{N}$. During discriminator training, only adversarial discriminator is updated and the other auxiliary classifiers are frozen. ac and bc are structural restrictions based on prior knowledge and certain molecule class c. For each auxiliary discriminator, the optimize function is:\n$\\min _{G} V\\left(D_{n}, G\\right)=\\sum_{C \\in C} \\mathbb{E}_{Y \\mid c \\sim p_{\\text {data }}(Y \\mid c)}\\left[\\log D_{n}(Y \\mid c)\\right]$\nwhere $D_{n}=a_{c} C_{n}(F)+b_{c}$\nThen the total training is to find the optimal solution of:\n$\\min _{G} \\max _{D} d_{0} V(D, G)+\\sum_{n=1}^{N} \\lambda_{n} V\\left(D_{n}, G\\right)$\nN\nwhere $\\sum_{n=0}^{N} \\lambda_{n}=1$\nIn this paper, we simply use pretrained random forest as a single auxiliary discriminator. Using more pretrained discriminators is computationally and memory-intensive and does not significantly improve the model's performance.\nFor discrete dataset, like SMILES strings of molecules, the sampling process is undifferentiable. One successful approach is to train Ge using a RL model via policy gradient [8, 9, 36]. Considering a full length sequence $Y_{1: \tau}=\\operatorname{string}\\left(t_{1} t_{2} \\ldots t_{\\tau}\\right)$, representing a discrete data, $Y_{1: t}$ is an incomplete subsequence belonging to $Y_{1: T}$. One can maximize the long term reward in the policy gradient process to mimic the expectation in eq(1):\n$J(\\theta)=\\mathbb{E}\\left[R_{T} \\mid S_{0}, \\theta\\right]=\\sum_{Y_{1} \\in \\mathbb{Y}} G_{\\theta}\\left(Y_{1} \\mid S_{0}\\right) \\cdot Q\\left(S_{0}, Y_{1}\\right)$\nwhere Ry is the reward for a complete sequence"}, {"title": "3.2 Implementation Details", "content": "with the length of T from the discriminator to generator, so is a fixed initial state at time 0, $y_{1}$ is the next token at time 1, $G_{\\theta}\\left(Y_{1} \\mid s_{0}\\right)$ is the policy that action a will be taken as state $s_{0}$ to get token $y_{1}$, and $Q(s, a)$ is the action-value function that represents the expected reward at state s of taking action a and following our current policy G$\\theta$ to complete the rest of the sequence. According to eq(2) and eq(3), we can decompose $Q \\left(s_{0}, Y_{1}\\right)$ as:\n$Q\\left(s \\mid Y_{1: T-1}, a \\mid y_{T}\\right)=\\lambda_{0} \\varphi, T+\\sum_{n=1}^{N} \\lambda_{n} R_{n, T}$\nHowever, the above equation is only for full sequence $Y_{1: T}$. We also want to get the initial state so and Q for partial sequences. s0 cannot directly be used, since the RL processes will consume more computational resources and may reduce the diversity of generated molecules. Instead, s is set as the initial state, directly sampling x tokens, as suggested by ORGAN. For the following generation steps, we perform M-time Monte Carlo search with the canonical policy G$\\theta$ to calculate Q at intermediate time steps. Thus we can evaluate the final reward when the sequence is completed:\n$M C_{G_{\\theta}}^{\\theta}\\left(Y_{1: t} ; M\\right)=\\left\\{Y^{1}, \\ldots, Y^{M}\\right\\}$\nwhere $Y^{m}=Y_{1: t}$ and $Y_{1: T}^{m}$ is stochastically sampled via the policy $G_{\\theta}$. Now Q(s, a) becomes Q(t):\n$\\left.\\begin{array}{ll}\\frac{1}{M} \\sum_{m=1}^{M}\\left(\\lambda_{0} \\varphi, T+\\sum_{n=1}^{N} \\lambda_{n} R_{n, T}\\right) & \\text { if } t<T \\\\\\lambda_{0} \\varphi, T+\\sum_{n=1}^{N} \\lambda_{n} R_{n, T} & \\text { if } t=T\\end{array}\\right\\}$\nFinally, according to SeqGAN, we can get an unbiased estimation of the gradient $J(\\theta)$:\n$\\nabla_{\\theta} J(\\theta) \\approx \\sum_{t=1, \\ldots, T} \\frac{1}{\\sum_{y_{t} \\in Y} G_{\\theta}\\left(y_{t} \\mid Y_{1: t-1}\\right)}\\left[\\nabla_{\\theta} \\log G_{\\theta}\\left(y_{t} \\mid Y_{1: t-1}\\right) Q(t)\\right]$\nGe is a RNN with long short-term memory (LSTM) cells for sequence generation, while D is a convolutional neural network (CNN) for text classification tasks[37]. Unlike SeqGAN and ORGAN, ADSeqGAN uses labeled start token as input, and different classes share the same RNN.\nTo avoid problems of GAN convergence like \"perfect discriminator\" and improve the stability of learning, we introduce the Wasserstein-1 distance, also known as earth mover's distance[38], to $D_{\\psi}$. In essence, Wasserstein distance turns the discriminator's classification task into a regression task, with the goal of reducing the distance between the real data distribution and the model distribution, which is more in line with the goals of RL."}, {"title": "4 Results", "content": "Here we conduct experiments to test ADSeqGAN in two representative scenarios: a moderately biased dataset consists of 4894 nucleic acid binders (NA) and 8191 protein targeting molecules (Pro) with high bioactivity, and an extremely biased dataset consisting of 548 CNS drugs and 7728 other small molecule drugs. Our objective is to prove that our model can generate functional drug molecules with only a small number of target samples while promoting synthesizability and drug-likeness, diversity and similar molecule length distribution. To estimate the quality of generated samples, score functions designed for synthesizability (SA)[42],quantitative estimate of drug-likeness (QED)[43, 44, 45] and Frechet ChemNet Distance (FCD) [46] are used, and we use fingerprint ECFP4[47] to compute Tanimoto similarity[48] for diversity evaluation.\nDuring training, every generator model was pretrained for 250 epochs based on MLE, and the discrminator was pre-trained for 10 steps. Unless otherwise stated, all GAN models use Wasserstein distance. Unless specified, is set to 0.2. MLE-based RNN, SeqGAN, RNN-pretrained RL, chemical language model (CLM)[14], MolGPT[49], MolGen[15] are selected for contrast experiments. All GAN-related models and RL-based models are set to be pretrained for 250 epochs of MLE-based Ge, and then 50 epochs for downstream training. Meanwhile, in pretraining steps of GANs, Dy is set to be pretrained for 10 steps. For other models, the training parameters are based on original papers. Notably, for all drug molecules in GAN and RL relative models, grammar restrictions"}, {"title": "4.1 Nucleic Acid Binder Generation", "content": "Nucleic acid-targeting drugs remain relatively scarce compared to their protein-targeting counterparts, despite the increasing recognition of nucleic acids as crucial therapeutic targets. In contrast to nucleic acid drugs, nucleic acid binders, though still not so many, are much more diverse and have been widely studied for their interactions with DNA and RNA. These binders play essential roles in regulating gene function and dying nucleic acids. Meanwhile, small molecules and protein-based drugs with high bioactivity have been extensively developed for protein targets, demonstrating well-characterized pharmacological properties.\nGiven the limited number of nucleic acid binders and the established chemical diversity of small molecules targeting proteins, it is of significant interest to explore a molecular generation model that integrates the chemical characteristics of both. The primary goal is to construct a generative model which is capable of learning the distinct yet complementary features of nucleic acid binders and bioactive small molecules that target proteins. By capturing the underlying chemical space shared between these two classes, such a model could facilitate the discovery of novel nucleic acid-targeting molecules with enhanced specificity and efficacy."}, {"title": "4.2 CNS Drug Generation", "content": "Despite the pressing need, the number of approved CNS drugs remains limited. One major obstacle in CNS drug development is the blood-brain barrier (BBB), a selective membrane that restricts the entry of many compounds into the brain, complicating the delivery of therapeutic agents. Recent advancements in artificial intelligence and machine learning offer promising avenues to overcome these hurdles by enabling the design of novel compounds with optimized properties for CNS activity, but the few samples are still a problem[60]. We demonstrate the potential of our small sample based generative model to expand the library of candidate molecules for CNS drugs."}, {"title": "5 Conclusion", "content": "In this study, we proposed ADSeqGAN, a novel sequence-based GAN framework incorporating auxiliary discriminators for small-sample molecular generation. By integrating a pretrained classifier as an additional discriminator, ADSeqGAN improves the generation of specific molecular classes, i.e., nucleic acid binders and CNS drugs. Our results demonstrate that ADSeqGAN outperforms traditional non-pretrained generative models in terms of molecular validity, diversity, and class specificity.\nThrough a combination of MLE pretraining generator, Wasserstein loss, and data augmentation techniques such as oversampling, ADSeqGAN effectively addresses mode collapse and enhances the generative process. The model exhibits strong label responsiveness, successfully differentiating nucleic acid binders from protein binders and generating CNS drugs at a significantly higher rate than standard approaches. Furthermore, docking experiments confirm the ability of ADSeqGAN in generating high-quality molecules, thus it has a great application in enlarging compound library for nucleic acid drug discovery.\nFuture work will focus on incorporating molecular scaffold information and SMILES syntax rules into the generation process to improve the success rate of valid molecule generation. We also aim to refine training strategies to further optimize molecular properties and integrate more advanced reinforcement learning techniques to enhance chemical space exploration."}]}