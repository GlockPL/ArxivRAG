{"title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking", "authors": ["Zekun Xi", "Wenbiao Yin", "Jizhan Fang", "Jialong Wu", "Runnan Fang", "Ningyu Zhang", "Jiang Yong", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "abstract": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.", "sections": [{"title": "Introduction", "content": "\u201cEducation is not the learning of facts, but the training of the mind to think.\"\nAlbert Einstein\nWriting is a continuous process of collecting information and thinking (Bean and Melzer, 2021). Recent advances in Large Language Models (LLMs) have demonstrated remarkable progress in machine writing such as open domain long-form generation (Liang et al., 2023; Yang et al., 2023; Zhao et al., 2024) or report generation on specific topics (Liu et al., 2018). To seek useful information, as shown in Figure 1, early attempts use Retrieval Augmented Generation (RAG) to expand new information on a given topic (Gao et al., 2024; Edge et al., 2024). However, vanilla RAG relies on a fixed set of search strategies (Ram et al., 2023), which lack diversity in generation, preventing a thorough exploration of the topic and resulting in a fragmented and incomplete understanding of the subject (Spink et al., 1998). To address this issue, STORM (Shao et al., 2024a) and Co-STORM (Jiang et al., 2024) have proposed a role-play approach designed to expand the perspective, which means collecting information from multiple perspectives, thus broadening the information space (Shen et al., 2023; Shanahan et al., 2023; Parmar et al., 2010). Yet these approaches are still being thought within the scope of one's own role, making it difficult to generate deep content and break through one's own knowledge boundaries(Ji et al., 2025). In particular, retrieved information often lacks depth, utility and redundancy, directly affecting the quality of generated articles, resulting in shallow, repetitive, and unoriginal outputs (Skarlinski et al., 2024).\nNote that humans can naturally avoid such pitfalls in the writing process. This phenomenon can be explained through the theory of reflective practice, a concept rooted in cognitive science (Osterman, 1990). According to this theory, human writers continuously reflect on previously gathered information and personal experiences, allowing them to reorganize, filter, and refine their cognitive framework. This process prompts writers to iteratively adjust their writing direction and mental pathways, ultimately allowing human authors to generate more profound, nuanced and original content (Bruce, 1978).\nMotivated by this, we propose OmniThink, a new machine writing framework that emulates the human-like cognitive process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they gradually deepen their understanding of complex topics to expand knowledge boundaries. By continuously reflecting on previously retrieved information, OmniThink can determine the optimal steps for further expansion. This expansion-reflection mechanism enables the dynamic adjustment of the retrieval strategies, fostering a more thorough and comprehensive exploration of relevant information. Once a diverse set of information has been gathered, OmniThink transitions to the stages of outline construction and article generation. This iterative thinking process leads to the production of articles of higher quality that contain a higher knowledge density of useful, insightful, and original content.\nWe evaluate OmniThink on the WildSeek datasets (Jiang et al., 2024) based on previous metrics as well as a new metric, named knowledge density. Experimental results demonstrate that OmniThink enhances the knowledge density of generated articles without compromising key metrics such as coherence and depth. Human evaluations and expert feedback further underscore the potential of our approach in addressing real-world challenges in the the generation of long-form articles. To conclude, our main contributions are as follows:\n\u2022 We propose OmniThink, a novel writing framework that emulates the human-like process of iterative expansion and reflection.\n\u2022 We propose a continuous expansion and reflection approach for open-domain article generation, which improves the information desitiy of the generated text.\n\u2022 We validate our approach through automatic and human evaluations, demonstrating its effectiveness in generating well-founded, high-quality long documents. Expert feedback further reveals new challenges in producing long-form documents that are both informative and contextually accurate."}, {"title": "Background", "content": "We focus on the task of open-domain long-form generation for machine writing, which involves retrieving information from an open domain and synthesizing it into a coherent article (Fan et al., 2019; Su et al., 2022; Quan et al., 2024). Given an input topic T, the target of open-domain long-form generation is to generate a long article A. The current standard approach involves two major steps (Zhang et al., 2019; Zheng et al., 2023): (i) Use a search engine S to retrieve information I = S(T) which is related to the topic T; (ii) Generate an outline O = Generate(I, T) based on the retrieved information I and input topic T. Finally, the article is generated using the outline, expressed as A = Generate(O,I)."}, {"title": "Revisiting Previous Methods", "content": "Previous works have made numerous efforts to improve the quality of open-domain long-form generation. Co-STORM (Jiang et al., 2024) introduces a user-participatory roundtable discussion in step (i) to enhance the diversity of the retrieved information. STORM (Shao et al., 2024a) proposes a questioning mechanism to improve the quality and relevance of the generated outlines in step (ii).\nAlthough substantial progress has been made in open-domain long-form generation, a persistent challenge remains: the generated content frequently suffers from excessive repetition and lacks substantial information. We present a case generated by STORM (Shao et al., 2024a) with GPT-4o as the backbone, as shown in Figure 2. In this article, the phrase \u201cAlphaFold was developed by Deep-"}, {"title": "Knowledge Density for the Article", "content": "Previous works mostly focus on whether the article is relevant and correct, but do not consider whether the article has sufficient depth(Li et al., 2024; Que et al., 2024; Liu et al., 2024). Many generated articles contain a lot of redundant information, which is very inconsistent with human writing. To address this, we introduce the Knowledge Density (KD) for the generated article, which is defined as the ratio of meaningful content to the overall volume of text (Xu and Reitter, 2017) as:\n$KD = \\frac{\\sum_{i=1}^{N} k_i U(k_i)}{L}$   (1)\nwhere N is the total number of atomic knowledge units identified within the document. The function U(ki) indicates whether the i-th unit information ki is unique. L represents the total length of the text. In this formula, the numerator represents the sum of unique units of atomic knowledge extracted from a long article. The denominator corresponds to the length of the article.\nNote that the value of the knowledge density metric lies in its ability to measure the reading cost of generated text from the perspective of information acquisition (Bovair and Kieras, 1991; Dos Santos and Mookerjee, 1993). Readers encountering low KD content often experience fatigue, frustration, or disengagement due to redundant or irrelevant details. In contrast, high-density content provides a streamlined experience, enabling efficient knowledge transfer.\nPrevious methods exhibit limited performance on the proposed KD due to the fact that the generated content in open-domain long-form generation"}, {"title": "OmniThink", "content": "As shown in Figure 3, we introduce a machine writing framework OmniThink, which emulates the human-like process of iterative reflection and expansion. OmniThink can be divided into three steps: Information Acquisition (\u00a73.1), Outline Structuring (\u00a73.2), and Article Composition (\u00a73.3)."}, {"title": "Information Acquisition", "content": "To acquire diverse and comprehensive information, OmniThink emulates the human learning process, progressively deepening its understanding of the topic through iterative Expansion and Reflection. As shown in Figure 4, we illustrate the specific process of Expansion and Reflection. This iterative process culminates in the construction of an information tree T, which organizes the retrieved information in a structured and hierarchical manner, and a conceptual pool P, which represents the LLMs' current understanding of the topic at time step m. Together, these components form the foundation of article generation.\nInitialization The interactive process begins with the initialization of a root node based on the input topic T. OmniThink first utilizes search engines, e.g., Google, or Bing, to retrieve information related to T, using the retrieved knowledge to construct the initial root node of the information tree Nr. This initial information in Nr is then organized and analyzed to form a preliminary conceptual pool Po, which serves as OmniThink's foundational understanding of the topic and guides subsequent expansion and reflection processes."}, {"title": "Expansion", "content": "At time step m, OmniThink analyzes all leaf nodes Lm = {N0, N1, . . ., Nn} of the information tree Tm. These leaf nodes are first stored in the conceptual buffer Pb, where each node is evaluated to determine if it requires further expansion. For nodes that need expansion, OmniThink uses the current conceptual pool Pm to identify areas for deeper expansion or suitable directions for expansion. For each leaf node Ni, OmniThink generates kn; sub-nodes, denoted as SUB(N) = {S0, S1,..., SkN; }, for expansion. Each sub-node represents a specific aspect or subtopic identified from the current node Ni. For each sub-node, OmniThink retrieves relevant information and stores it within the respective node, subsequently adding the sub-node to the appropriate position in the updated information tree Tm+1 as follows:\nTm+1 = Combine(Tm, SUB(No), . . ., SUB(Nn)))    (2)\nThis targeted retrieval process ensures that OmniThink collects comprehensive and in-depth content for each sub-node, thereby enriching the hierarchical structure of the information tree."}, {"title": "Reflection", "content": "In this phase, OmniThink reflects the newly retrieved information in all leaf nodes Lm+1 = {No, ...Nn}. The information retrieved from each leaf node is analyzed, filtered, and synthesized to distill the core insights Im+1 = {INS0, ..., INSn}. These refined insights are then incorporated into the conceptual pool Pm, which is continuously updated and enriched throughout the process as follows:\nPm+1 = Merge(Im+1, Pm)    (3)\nUsing the updated conceptual pool Pm+1, OmniThink further expands the leaf nodes of the information tree iteratively."}, {"title": "Outline Structuring", "content": "Outline is the core of an article, determining its content direction, structural hierarchy, and logical progression. To create an outline that is well-guided, clearly structured, and logically coherent, it is essential to have a comprehensive and in-depth understanding of the topic. In the previous section, OmniThink maintains a concept pool closely related to the topic, which essentially represents the boundaries and depth of the LLM's understanding of the topic. When generating the content outline, we first create a draft outline OD, and then ask the LLM to refine and link the content from the concept pool P, ultimately forming the final outline O = Polish(OD, P). Through this approach, the LLM is able to comprehensively cover the key points of the topic in the outline and ensure logical consistency and content coherence in the article."}, {"title": "Article Composition", "content": "After completing the outline O, we begin writing the content for each section S. At this stage, the LLM would work in parallel to write the content for each section. When writing the content of the section, we use the titles of each section and their hierarchical subsections to retrieve the most relevant K documents from the information tree by calculating the semantic similarity (Sentence-BERT (Reimers and Gurevych, 2019) embeddings). After obtaining the relevant information, the LLM is prompted to generate the section content with citations based on the retrieved information. Once all sections are generated, they will be concatenated into a complete draft article AD = {S1, .Sn}. Since these sections are generated in parallel and the specific content of other sections is not yet clear, we prompt the LLM to process the concatenated article, remove redundant information, and form the final article A = {S\u2081,..Sn}."}, {"title": "Experiments", "content": ""}, {"title": "Dataset and Baseline", "content": "We use WildSeek as evaluation dataset to verify the effectiveness of our method, following previous work (Jiang et al., 2024; Shao et al., 2024a). WildSeek collects and filters data related to the open-source STORM web application, with each entry consisting of a specific topic and a user's goal. We select representative baselines for comparison, including RAG, ORAG, and STORM (Shao et al., 2024a) and Co-STORM (Jiang et al., 2024). The baseline results are reproduced on the basis of STORM\u00b9."}, {"title": "Evaluation Setup", "content": "We employ both automatic and human evaluations to assess the generated long-form articles:\nAutomatic Evaluation. For automatic evaluation, we use Prometheus2 (Kim et al., 2024)2 to score articles on a scale of 0 to 5, evaluating Relevance, Breadth, Depth, and Novelty. Furthermore, we measure information diversity (Jiang et al., 2024) (cosine similarity differences between web pages) and knowledge density (discussed in detail in \u00a72.3) for information richness. Detailed procedures are provided in the Appendix C.\nHuman Evaluation. We randomly select 20 topics and compare articles generated by our method with those from the Co-STORM (the comprehensive best-performing baseline based on automatic evaluation), scoring them on the same four aspects. More details can be found in the Appendix D."}, {"title": "Implementation Details", "content": "We build OmniThink based on the DSpy framework (Khattab et al., 2023), and Appendix B contains the corresponding prompts we used. During generation, we set the temperature at 1.0 and top_p at 0.9. We use Bing's API with the parameter for the number of web pages returned per query set to 5. For the computation of knowledge density, we utilize Factscore\u00b3 with GPT-40-08-06 as the backbone to decompose atomic knowledge (Min et al., 2023). After decomposition, we proceed to use GPT-40-08-06 for the deduplication of the split atomic knowledge. To avoid the impact of search engine changes over time, all the results in our table"}, {"title": "Automatic Evaluation Results", "content": ""}, {"title": "Main Results", "content": "Article Generation. Table 1 presents the evaluation results on the WildSeek dataset employing GPT-40 and Qwen-Plus as backbones. Within the framework of four key grading criteria (Relevance, Breadth, Depth, and Novelty) OmniThink delivers exceptional performance across the board, with GPT-40 as its backbone, particularly distinguishing itself in the Novelty metric. This achievement can be credited to OmniThink's robust reflective capabilities, which enable it to extract and thoroughly explore novel insights from existing knowledge. When employing Qwen-Plus as the backbone, OmniThink's performance see a decline; however, it remains highly competitive.\nOmniThink's strength lies in its multifaceted and profound contemplation of retrieved information, which facilitates access to more profound layers of the external knowledge. This multi-perspective approach not only enriches the diversity of citation sources but also elevates the citation diversity level beyond that of other methodologies."}, {"title": "Ablation Study", "content": "As discussed in \u00a73.1, one of the main components of OmniThink is the introduction of dynamic expansion and reflection. We compare OmniThink with a version that does not implement dynamic expansion and reflection. As shown in Figure 5, the simplified version performs worse in various metrics related to article quality than the complete system, particularly in terms of Information Diversity and Novelty. This experiment demonstrates the powerful role of the dynamic expansion and reflection mechanism in enhancing information diversity and article novelty."}, {"title": "Expansion & Reflection Analysis", "content": "We provide a further analysis of how the expansion and reflection processes shape the various aspects of the final articles and contribute to its overall quality. Given the interdependent nature of expansion and reflection in OmniThink, it is impractical to assess their individual impacts in isolation. To address this challenge, we adopt an indirect yet systematic approach to evaluate their collective influence on the final articles' quality. During the information acquisition phase, we substitute the model used for expansion with a lower-performing model and measured the extent of performance decline in the generated article's metrics, which served as an indicator of the impact of the expansion process on these metrics. Similarly, the same approach is applied to assess the impact of the reflection process. Specifically, based on the experimental results for Qwen-Plus in Table 1, we replace the models used for the expansion and reflection processes from Qwen-Plus to Qwen2.5-7b-instruct (Team, 2024) and observe the decline in various evaluation results. This transition allows us to observe and document the subsequent changes in a range of evaluation metrics, providing insights into the expansion and reflection process's influence on the articles' overall assessment. We report the results in Figure 6.\nContinuous reflection expands knowledge boundaries. We observe that reflection is much more important than expansion with respect to novelty and informational diversity. Reflection endows the model with the capacity to not only re-evaluate and introspectively consider existing knowledge but also to integrate this information in a way that stimulates the emergence of a more diverse and expansive range of ideas. This process of deep introspection is essential, as it diversifies the narrative with a spectrum of insights, thereby laying the"}, {"title": "Thinking Depth Analysis", "content": "Our method has made numerous attempts to improve information retrieval, which are essentially scale-ups of the retrieved information. In this section, we discuss the impact of the quantity and depth of the retrieved information on the quality of the generation of articles. From Figure 7, we observe a rapid increase in the knowledge density and information diversity of the generated articles as the depth increases from 1 to 3. This indicates that as the depth increases, OmniThink can search for an increasing amount of diverse information on the Web and utilize this information in the generated articles. However, when the depth is raised to 4, the growth rate of knowledge density and information divinity slows down significantly. This may be because the available information on the subject approaches the search limit, making it difficult to retrieve more useful information on the topic."}, {"title": "Human Evaluation Results", "content": "To better understand the strengths and weaknesses of OmniThink, we engage 15 well-educated volunteers to conduct a human evaluation. In Figure 8, we present the results of human scoring. The findings indicate that OmniThink's average performance surpasses that of the current strongest baseline across various dimensions, with a notable 11% improvement in the Breadth metric compared to Co-STORM. However, in terms of the Novelty metric, although automated evaluation shows an 11% enhancement, human assessment reveals only"}, {"title": "Related Work", "content": ""}, {"title": "Information Seeking in NLP", "content": "Previous studies on information-seeking focused on designing question-answering (QA) systems (Wu et al., 2025). Early open-domain QA methods generally assumed that users could fulfill their information needs through a single query (Chen et al., 2017; Levy et al., 2021). Subsequent studies have recognized that, in real-world scenarios, users often struggle to satisfy their information needs with a single query (Chen et al., 2017; Levy et al., 2021). To address this limitation, researchers have explored multi sub-query retrieval methods, where a single query is decomposed into multiple sub-queries to retrieve distinct pieces of information (Mao et al., 2024; Chen et al., 2011; Peng et al., 2019). The information collected is then aggregated to provide a comprehensive answer. Building on these developments, recent advances in open-domain long-form generation require reasoning across multiple information sources (Fan et al., 2019; Ujwal et al., 2024; Wei et al., 2024; Tan et al., 2024). This line of open-domain long-form generatio underscores the importance of integrating information from multiple perspectives. For example, STORM introduces a retrieval paradigm that simulates multi-turn interactions from diverse perspectives, aiming to aggregate richer and more diverse information(Shao et al., 2024b). Similarly, Co-STORM employs a \u201croundtable discussion\" paradigm to further expand the diversity of information sources considered during retrieval (Jiang et al., 2024). Although these approaches have made significant advancements from multi-perspective and multi sub-query perspectives, they often fail to leverage the reasoning and introspective abilities of LLMs fully. Specifically, existing approaches do not fully exploit the potential of LLMs to dynamically adjust retrieval strategies and flexibly update information sources as the model's understanding of the topic deepens(Qin et al., 2024). Unlike previous works, we propose a reflection-based dynamic retrieval framework OmniThink, which facilitates a more comprehensive and contextually responsive retrieval process by enabling context-aware, self-reflective adjustments to retrieval strategies."}, {"title": "Machine Writing", "content": "Due to the high costs associated with manual writing, machine writing has garnered significant research interest in recent years (Zhou et al., 2023; Pham et al., 2024; Wang et al., 2024a,b). The emergence of LLMs and Retrieval-Augmented Generation (RAG) has opened new possibilities for automated writing (Liang et al., 2024; Balepur et al., 2023; de la Torre-L\u00f3pez et al., 2023). To ensure authenticity and real-time relevance, current RAG-based automated writing systems primarily rely on retrieved content to generate articles. For example, STORM (Shao et al., 2024a) introduces a role-playing question-and-answer approach to author Wikipedia-like articles, while Co-STORM (Jiang et al., 2024) proposes a user-participated information retrieval paradigm. Besides, AutoSurvey (Wang et al., 2024c) extends this framework into the domain of academic paper writing. However, these methods tend to overlook the issue of information diversity, which can result in outputs with limited practical value. Although these methods demonstrate notable advancements in specific domains, they often neglect the perspective of content utility, resulting in outputs with limited practical value. Unlike previous works, we propose a reflection-based dynamic retrieval framework OmniThink, which facilitates a more comprehensive and contextually responsive retrieval process by enabling context-aware, self-reflective adjustments to retrieval strategies. The proposed OmniThink adopts the concept of knowledge density, enhancing the informativeness and overall utility of generated text while maintaining its original quality."}, {"title": "Conclusion and Furture Work", "content": "We propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. Automatic and human evaluations demonstrate that OmniThink can generate well-founded, high-quality long articles. Our approach is model-agnostic and can be integrated with existing frameworks. In the future, we will explore more advanced machine writing methods that combine deeper reasoning with role-playing and human-computer interaction."}, {"title": "Limitations", "content": "Although the proposed OmniThink has demonstrated its advantages in both automatic and human evaluations, several limitations remain. Firstly, the current work is limited to search and text generation, while a vast amount of multimodal information in the open domain remains unused. Secondly, we have not considered personalized language styles in text production. As a result, the generated texts tend to be academic in nature, which may not be as suitable for general users' reading preferences. We plan to address these limitations in future work."}, {"title": "Implementation Details", "content": "We build OmniThink based on the DSpy framework (Khattab et al., 2023), and STORM. Appendix B contains the corresponding prompts we used. During article generation, we set the temperature at 1.0 and top_p at 0.9. The search engine employed is Bing's API, with the parameter for the number of web pages returned per query configured to 5. To retrieve information based on the outline, we use SentenceBERT (Reimers and Gurevych, 2019) embeddings to calculate cosine similarity, thereby retrieving the three most similar web pages each time. For the computation of knowledge density, we utilize Factscore\u2074 with GPT-40-08-06 as the backbone to decompose atomic knowledge (Min et al., 2023). After the decomposition, we proceed to use GPT-40-08-06 for the deduplication of the split atomic knowledge."}, {"title": "Full Prompts in OmniThink", "content": "In \u00a73, we introduce the specific process of OmniThink, which is implemented using zero-shot prompting based on GPT-40-2024-08-06. Lists 1, 2, 3, 4 and 5, respectively document the complete prompts for OmniThink's Expand, Reflect, Write Outline, Write Article, and Polish Article stages. These prompts are designed to guide the model through iterative stages of content generation, ensuring coherence and depth in the produced text. The structured process leverages dynamic adjustments based on intermediate outputs, reflecting a balanced integration of retrieval and generation capabilities. This systematic approach highlights OmniThink's ability to adaptively construct well-organized and contextually relevant articles across diverse topics."}, {"title": "Automatic Evaluation Details", "content": "To further ensure reliability, we conducted multiple evaluation rounds using different prompts covering various aspects of outline coherence, structural logic, and topic relevance. This multi-faceted evaluation helps mitigate potential biases and enhances the robustness of the scoring results."}, {"title": "Outline Evaluation", "content": "Since Prometheus2 (Kim et al., 2024) does not perform targeted optimization on the outline, we decided to use a more powerful model to score the"}, {"title": "Article Evaluation", "content": "Following Co-STORM (Jiang et al., 2024), we utilized the Prometheus-7b-v2.0 model for evaluation. Prometheus (Kim et al., 2024) is an open-source scoring model used to assess lengthy texts based on user-defined criteria. Its default temperature value is 1.0, and the top_p value is 0.9. Due to the model's limited context window, we exclude reference sections from the article evaluation and trim the input text to fewer than 2000 words to fit within the model's context window. This is consistent with STORM's approach (Shao et al., 2024a), where the shortest section is removed each time until the article length meets the specified requirement. The scoring criteria for article quality evaluation can be found in Listing 11."}, {"title": "Human Evaluation Details", "content": "The participants in the evaluation voluntarily provided their highest educational qualification to demonstrate their ability to impartially assess the article. As shown in Figure 9, all of our human evaluators have an undergraduate degree or higher, with 53% having a graduate degree. As discussed in \u00a76, to compare the merits of OmniThink and"}]}