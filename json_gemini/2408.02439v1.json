{"title": "Long Input Benchmark for Russian Analysis", "authors": ["Igor Churin", "Murat Apishev", "Maria Tikhonova", "Denis Shevelev", "Aydar Bulatov", "Yuri Kuratov", "Sergej Averkiev", "Alena Fenogenova"], "abstract": "Recent advancements in Natural Language Processing (NLP) have fostered the development of Large Language Models (LLMs) that can solve an immense variety of tasks. One of the key aspects of their application is their ability to work with long text documents and to process long sequences of tokens. This has created a demand for proper evaluation of long-context understanding. To address this need for the Russian language, we propose LIBRA (Long Input Benchmark for Russian Analysis), which comprises 21 adapted datasets to study the LLM's abilities to understand long texts thoroughly. The tests are divided into four complexity groups and allow the evaluation of models across various context lengths ranging from 4k up to 128k tokens. We provide the open-source datasets, codebase, and public leaderboard for LIBRA to guide forthcoming research.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive abilities in many NLP applications. Interacting with people through free-form text instructions, they serve as versatile tools for multiple scenarios, transforming the landscape of AI systems. One direction where LLM usage is developing rapidly includes tasks requiring long text processing, such as summarization and information extraction, where their applications alleviate the handling of long texts for humans.\nHowever, until recently, most LLMs had difficulties in handling long sequences of tokens and were only able to work with a limited context length of several thousand tokens. In recent years, new methods have enabled the models to increase their context significantly, empowering them to solve a new variety of tasks. This, in turn, and the community's demand for automatic systems solving such tasks at a good level has created a need for a"}, {"title": "2 Related Work", "content": "2.1 Long Context Large Language Models\nOne of the important tasks in the development of LLMs is to increase the length of the context that the model can understand. This problem has two key points: the complexity of calculations for long sequences and the ability of the model to extract important data in a long context. The solution of the first problem can be attributed to research on the effective processing of the self-attention as in Longformer (Beltagy et al., 2020), LongNet (Ding et al., 2023) and FlashAttention (Dao et al., 2022; Dao, 2023), using caches for previously calculated outputs such as Transformer-XL (Dai et al., 2019), Unlimiformer (Bertsch et al., 2024) and LongLLaMA (Tworkowski et al., 2024) or replacing it with another mechanism with more effective inference as in RetNet (Sun et al., 2023) and Mamba (Gu and Dao, 2023). The solution to the second problem is to improve positional encoding techniques such as ALiBi (Press et al., 2021) and RoPE-based approaches (Sun et al., 2022; Peng et al., 2023).\n2.2 Long Context Benchmarks\nUntil recently, most LMs had relatively small context lengths limited by a few thousand tokens. Thus, standard Natural Language Understanding (NLU) benchmarks (Wang et al., 2018, 2019; Shavrina et al., 2020) contained tasks within this size.\nEven today, many \u201cnew generation\u201d benchmarks created recently, such as HELM (Bommasani et al., 2023), MT-Bench (Zheng et al., 2023), and Russian-oriented benchmark MERA (Fenogenova et al., 2024) follow this pattern, limiting their tasks by relatively small context window size to simplify the evaluation procedure and reducing its cost.\nThe pioneers of long context processing benchmarks have been ZeroSCROLLS (Shaham et al., 2023), designed to test zero-shot model capabilities for NLU over long texts; L-eval (An et al., 2023), focused on a standardized evaluation methodology for long context LMs addressing two key aspects: dataset construction and evaluation metrics; and LongBench (Bai et al., 2023), the bilingual multi-task benchmark for long context understanding, comprising 21 tasks in English and Chinese. The tasks in LongBench can be divided into 6 big categories and cover key long-text application"}, {"title": "3 LIBRA", "content": "3.1 Benchmark Overview\nIn this section, we introduce LIBRA (Long Input Benchmark for Russian Analysis), a new benchmark for long context understanding in Russian, which includes 21 tasks for LLM evaluation. LIBRA aims to evaluate a large scope of LLMs, including pretrain models and models with supervised finetuning (SFT) with any system prompt that can be picked up.\nThe main purpose of the benchmark is to create a reliable instrument for the long context understanding evaluation, enabling the study of the model's ability to solve various tasks of different complexity with respect to the input context length. For this purpose, all tasks in the LIBRA benchmark are divided into 4 complexity groups, and the datasets have several subsets of various context lengths ranging from 4k up to 128k tokens. The latter makes it possible to explore the influence of the context length on the model results.\n3.2 Complexity group description\nIn this section, we describe each of the complexity groups of tasks.\nThe first complexity group (I) consists of tasks that require finding a short text fragment in long textual paragraphs containing irrelevant information. This group includes Passkey and PasskeyWithLibrusec datasets.\nThe second complexity group (II) includes tasks that require answering the question based on a relevant context. The following types of tasks are related to this group: question answering (QA) such as MatreshkaNames, MatreshkaYesNo, LibrusecHistory, ruTREC, ruSciFi, ruSciAbstractRe-"}, {"title": "3.3 Context Length Estimation", "content": "In the LIBRA benchmark, we divide all datasets into subsets of various context lengths. We measure context length in tokens; however, it may vary across different models and tokenizers. In our work, we used the fertility of tokenizers to distribute samples across different context lengths, which indicates the average number of tokens in which one word is tokenized. Thus, the average length in tokens for the text can be approximated by the number of words multiplied by the fertility number.\nFor the fertility approximation, we calculate the average fertility of the classic LLM tokenizers, which we further evaluate as baselines (see Subsection 4.1 for model description) on a complete list of datasets. The fertility of each model is shown in Table 2. The average fertility is 2.8. However, we decided to choose it with a margin so that the multilingual model with the highest fertility can be tested on the entire benchmark. As a result, we set the standard fertility to 3.\nFinally, using the selected fertility value, we divided all datasets into subsets of various context lengths ranging from 4k to 128k tokens. The resulting dataset sizes and the average sample context lengths are given in Table 3.\n3.4 Datasets\nThis section describes the datasets and data collection process in detail. We decided to create a combined benchmark that will include 1) transla-"}, {"title": "4 Evaluation Methodology", "content": "4.1 Baseline models\nWe evaluate 12 popular LLMs that feature long context capability, including GPT-4o, GLM4-9B-Chat (Zeng et al., 2022), ChatGLM2-6B-32k (Zeng et al., 2022), Saiga-LLaMA-3-8B, LLaMA-3-8B,\nLLaMA-3-8B-Instruct,\nLLaMA-2-7B-32K,\nLongAlpaca-7B,\nLongChat-7B-v1.5-32k, Mistral-7B-v0.1, Mistral-7B-v0.3, Mistral-7B-Instruct. A detailed information about the baseline models is given in Appendix C.\n4.2 Experimental setup\nSince the tasks themselves are long, in order not to go beyond the context window we fixed the evalua-"}, {"title": "5 Results", "content": "The baseline results with respect to context length are shown in Table 4 and with respect to tasks are shown in Tables 5, 6, 7. Detailed results for each model are given in Appendix D. Based on the obtained results we can draw the following conclusions for each group of tasks.\nGroup I The tasks from this group are relatively simple, and almost all models pass them well within their maximum input length. The only exception is the LongAlpaca-7B model.\nGroup II MatreshkaYesNo, turns out to be the most straightforward task in the group, which all models cope with naturally. The ruTPO and ruQuALITY tasks are of medium complexity; several models achieved good scores in them.\nThe classic QA task LibrusecHistory is effectively handled by modern models; however, the quality decreases with the input length increase (e.g. for ruSciAbstractRetrieval). Nevertheless, in some cases, a larger context is advantageous, as seen in ruTREC, where increasing the input length helps the model handle the task better because this task is designed in a few-shot format.\nThe most complex tasks in this group can be considered MatreshkaNames and ruSciFi. For the first, several models (e.g., ChatGLM2-6B-32k, LLaMA-2-7B-32K, and LongAlpaca-7B) show low results for any input length. ruSciFi with a 64K context is beyond the capabilities of most models. At the same time, the strongest models (GPT-4o and GLM4-9B-Chat) not only show promising results"}, {"title": "6 Conclusion", "content": "The rapid development of LLMs has posed new challenges for evaluating their ability to process long texts. To address this problem, we have introduced LIBRA. This benchmark evaluates LLM long context understanding abilities through 21 long-context textual tasks.\nThe tasks enable model evaluation across various context lengths ranging from 4k to 128k tokens based on the analysis of dataset context lengths of the models' tokenizers. Our contribution encompasses a benchmark methodology with open-sourced datasets of different lengths and domains, a codebase for model evaluation, and baseline solution scoring. The datasets are published under the MIT license, and the leaderboard is publicly accessible on HuggingFace."}, {"title": "Limitations", "content": "Although the LIBRA was created to solve the absence of the long context benchmark for Russian and provides significant advancements in evaluating language models with long contexts, it still has a number of limitations that need to be acknowledged.\nData Representation. The texts included in the benchmark are gathered from specific domains, which might not cover the full range of Russian language usage. This can raise concerns about data privacy, representation, and potential biases within the benchmark. It is important to consider that dialects, regional variations, and sociolects may not be adequately represented, potentially leading to biased performance metrics. As a result, models may excel in benchmark tasks but struggle with texts outside these domains, limiting their generalization ability. The corpus used for the benchmark may become outdated over time. New words, phrases, and usage patterns could emerge, making the benchmark less relevant for future model evaluations.\nMethodology limitations. When creating the datasets, we hypothesized that synthetically augmentation of the context length of the datasets, such as LibrusecHistory, would not affect the results. Our experiments show that these tasks are pretty challenging for many models. We made this methodological assumption due to the limitations of human data annotation; it is difficult for people to read large texts and concentrate enough to create questions and search for information within them. This data creation method may result in task errors, particularly when a newly extended text fragment contains conflicting information that could impact the answer. However, we found this approach acceptable due to the increased speed and cost-effectiveness.\nThe current methodology also restricts the number of tasks, and many of them are translated only due to the high cost of data creation.\nLength context. The benchmark focuses on evaluating long contexts, but the definition of \"long context\" can differ based on the application and the model. The chosen context lengths may not be ideal for all usage scenarios, and models could exhibit varying performance. In this paper, we have measured the average fertility of baseline model tokenizers on a full list of datasets from our benchmark to sample different contexts and analyzed the"}, {"title": "Ethical Considerations", "content": "The data used in the benchmark was created from open data sources. When annotating the data, we obtained transparent permission from all users and made efforts to maintain the confidentiality and anonymity of participants. As the benchmark develops, ongoing efforts are required to identify and minimize biases in the benchmark datasets and evaluation metrics. The benchmark does not currently contain the datasets covering the ethical or AI safety skill evaluation, but this is a space for future work."}, {"title": "A Data Annotation Details", "content": "The datasets LibrusecHistory, LibrusecMHQA, and ruSciAbstractRetrieval were created via the crowd-sourced platform.\nIn the LibrusecHistory, annotators were instructed to read a lengthy text and generate four questions based on the text and answer them. Guidelines were provided regarding the type of questions to ask: 1) Questions should be answerable using information present in the text 2) The questions must not be about widely known information but should be related to the text 3) Questions can cover various aspects such as character actions, appearance, thoughts, events, and scene descriptions 4) Logical deductions are not required to answer the questions 5) Each question should have a single, clear, unambiguous answer from the text.\nThe design of the dataset LibrusecMHQA project follows a similar structure to LibrusecHistory, but the question criteria were more complex. In this dataset, the questions were answered by expert editors rather than through crowd-sourcing. The main distinction in the criteria for annotators is the multi-hop questions, where simply reading the sentence containing the answer is insufficient. Instead, reading at least a paragraph of 2-5 sentences, or the entire relevant fragment, is necessary to gather information and generate a complete answer.\nThe ruSciAbstractRetrieval was collected by crowd-sourced annotators. These annotators were asked to read a long text annotation and briefly describe the contents. The criteria for the description were as follows: 1) The description must start"}, {"title": "B Dataset Examples", "content": "This section provides examples of the task format for the benchmark datasets. The exact prompts for the benchmark are not fixed. Here we provide prompts used in our experiments.\nPasskey: You are provided with a long text that contains the access key. Just remember the access key.\nContext: {context}\nYou only need to specify the access key in the response.\nQuestion: {input}\nAnswer:\nPasskey WithLibrusec: You are provided with a long text that contains the access key. Just remember the access key.\nContext: {context}\nYou only need to specify the access key in the response.\nQuestion: {input}\nAnswer:\nMatreshkaNames: You are provided with several dialogues. Remember the names of the people and the topics they talked about.\nContext: {context}"}, {"title": "C Detailed Model Information", "content": "The baseline model specifics are presented in Table 9."}, {"title": "D Detailed Model Results", "content": "This section presents the detailed results of model evaluation. The results are shown for the following models: GPT-4o (Table 10), GLM4-9B-Chat"}]}