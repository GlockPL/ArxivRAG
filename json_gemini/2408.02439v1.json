{"title": "Long Input Benchmark for Russian Analysis", "authors": ["Igor Churin", "Murat Apishev", "Maria Tikhonova", "Denis Shevelev", "Aydar Bulatov", "Yuri Kuratov", "Sergej Averkiev", "Alena Fenogenova"], "abstract": "Recent advancements in Natural Language Processing (NLP) have fostered the development of Large Language Models (LLMs) that can solve an immense variety of tasks. One of the key aspects of their application is their ability to work with long text documents and to process long sequences of tokens. This has created a demand for proper evaluation of long-context understanding. To address this need for the Russian language, we propose LIBRA (Long Input Benchmark for Russian Analysis), which comprises 21 adapted datasets to study the LLM's abilities to understand long texts thoroughly. The tests are divided into four complexity groups and allow the evaluation of models across various context lengths ranging from 4k up to 128k tokens. We provide the open-source datasets, codebase, and public leaderboard for LIBRA to guide forthcoming research.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive abilities in many NLP applications. Interacting with people through free-form text instructions, they serve as versatile tools for multiple scenarios, transforming the landscape of AI systems. One direction where LLM usage is developing rapidly includes tasks requiring long text processing, such as summarization and information extraction, where their applications alleviate the handling of long texts for humans.\nHowever, until recently, most LLMs had difficulties in handling long sequences of tokens and were only able to work with a limited context length of several thousand tokens. In recent years, new methods have enabled the models to increase their context significantly, empowering them to solve a new variety of tasks. This, in turn, and the community's demand for automatic systems solving such tasks at a good level has created a need for a thorough evaluation of LLM long context understanding.\nTo address this demand in English, several long context understanding benchmarks have been created recently with LongBench (Bai et al., 2023) and L-Eval (An et al., 2023) heading the list. However, the Russian language, at this point, lacks a fair instrument for transparent evaluation of long context understanding.\nOur work addresses this problem and presents a new benchmark, which we call Long Input Benchmark for Russian Analysis, or LIBRA, for the evaluation of LLM long context understanding abilities in Russian.\nThus, the contribution of our work can be summarized as follows:\n\u2022 we present a methodology for the evaluation of long-context abilities of LLMs for the Russian language;\n\u2022 we publicly release a set of 21 datasets of various skills and complexities in Russian which form the LIBRA benchmark;\n\u2022 we provide a codebase as long as the number of baseline solutions and public leaderboard."}, {"title": "Related Work", "content": "One of the important tasks in the development of LLMs is to increase the length of the context that the model can understand. This problem has two key points: the complexity of calculations for long sequences and the ability of the model to extract important data in a long context. The solution of the first problem can be attributed to research on the effective processing of the self-attention as in Longformer , LongNet  and FlashAttention  using caches for previously calculated outputs such as Transformer-XL , Unlimiformer  and LongLLaMA  or replacing it with another mechanism with more effective inference as in RetNet  and Mamba . The solution to the second problem is to improve positional encoding techniques such as ALiBi  and RoPE-based approaches .\nUntil recently, most LMs had relatively small context lengths limited by a few thousand tokens. Thus, standard Natural Language Understanding (NLU) benchmarks  contained tasks within this size.\nEven today, many \u201cnew generation\u201d benchmarks created recently, such as HELM , MT-Bench , and Russian-oriented benchmark MERA  follow this pattern, limiting their tasks by relatively small context window size to simplify the evaluation procedure and reducing its cost.\nThe pioneers of long context processing benchmarks have been ZeroSCROLLS , designed to test zero-shot model capabilities for NLU over long texts; L-eval , focused on a standardized evaluation methodology for long context LMs addressing two key aspects: dataset construction and evaluation metrics; and LongBench , the bilingual multi-task benchmark for long context understanding, comprising 21 tasks in English and Chinese. The tasks in LongBench can be divided into 6 big categories and cover key long-text application scenarios, including multi-document QA, single-document QA, summarization, few-shot learning, code completion, and synthesis tasks.\nHowever, the limitation of the long context benchmarks mentioned above is that they are mainly oriented at the English language (and the Chinese language for LongBench). As for the Russian language, there is an urgent need for a reliable system able to evaluate LLM long context understanding abilities. To address this problem, we propose LIBRA, which brings a methodology and 21 tasks for a long context understanding evaluation in Russian."}, {"title": "LIBRA", "content": "In this section, we introduce LIBRA (Long Input Benchmark for Russian Analysis), a new benchmark for long context understanding in Russian, which includes 21 tasks for LLM evaluation. LIBRA aims to evaluate a large scope of LLMs, including pretrain models and models with supervised finetuning (SFT) with any system prompt that can be picked up.\nThe main purpose of the benchmark is to create a reliable instrument for the long context understanding evaluation, enabling the study of the model's ability to solve various tasks of different complexity with respect to the input context length. For this purpose, all tasks in the LIBRA benchmark are divided into 4 complexity groups, and the datasets have several subsets of various context lengths ranging from 4k up to 128k tokens. The latter makes it possible to explore the influence of the context length on the model results."}, {"title": "Complexity group description", "content": "In this section, we describe each of the complexity groups of tasks.\nThe first complexity group (I) consists of tasks that require finding a short text fragment in long textual paragraphs containing irrelevant information. This group includes Passkey and PasskeyWithLibrusec datasets.\nThe second complexity group (II) includes tasks that require answering the question based on a relevant context. The following types of tasks are related to this group: question answering (QA) such as MatreshkaNames, MatreshkaYesNo, LibrusecHistory, ruTREC, ruSciFi, ruSciAbstractRetrieval and multiple choice QA tasks, which are presented by ruTPO and ruQuALITY.\nThe natural development of tasks from the second class of complexity are tasks with questions, the answers to which are not explicitly contained in the text but require the analysis of fragments of input data and the generation of an answer based on it. Such tasks in our classification belong to the third complexity group (III) and represent a multi-hop question answering (MHQA) type. This group includes the following tasks: ruBABILongQA1, ruBABILongQA2, ruBABILongQA3, ruBABILongQA4, ruBABILongQA5, LongContextMultiQ, LibrusecMHQA and ru2WikiMultihopQA.\nFinally, to the fourth complexity group (IV) belongs to the tasks that require understanding the whole context, solving mathematical problems, and QA tasks within complex domains. This group includes ruSciPassageCount, ruGSM100 and ruQasper datasets.\nIt should also be mentioned that we do not include code generation and analysis tasks in LIBRA as most of the software code in the world is written in languages based on English."}, {"title": "Context Length Estimation", "content": "In the LIBRA benchmark, we divide all datasets into subsets of various context lengths. We measure context length in tokens; however, it may vary across different models and tokenizers. In our work, we used the fertility of tokenizers to distribute samples across different context lengths, which indicates the average number of tokens in which one word is tokenized. Thus, the average length in tokens for the text can be approximated by the number of words multiplied by the fertility number.\nFor the fertility approximation, we calculate the average fertility of the classic LLM tokenizers, which we further evaluate as baselines (see Subsection 4.1 for model description) on a complete list of datasets. The fertility of each model is shown in Table 2. The average fertility is 2.8. However, we decided to choose it with a margin so that the multilingual model with the highest fertility can be tested on the entire benchmark. As a result, we set the standard fertility to 3.\nFinally, using the selected fertility value, we divided all datasets into subsets of various context lengths ranging from 4k to 128k tokens. The resulting dataset sizes and the average sample context lengths are given in Table 3."}, {"title": "Datasets", "content": "This section describes the datasets and data collection process in detail. We decided to create a combined benchmark that will include 1) translations of English datasets by using Google translator API, 2) adaptations to long input tasks in Russian and 3) entirely new datasets based on open data.\nWe decided not to generate samples using LLMs and instead used annotators to mark up the samples. This helps reduce bias from using models like GPT-4, which are also part of the assessment. However, it does have some drawbacks, as full annotation can be costly and time-consuming in certain cases.\nThe exact dataset format can be found in Appendix B.\nPasskey The Passkey is a synthetic QA dataset based on original passkey dataset from LongLLaMA's GitHub repository. The main idea of the task is to extract a relevant piece of code number from a long text fragment that was created by repeating short sentence template containing noise. The model must find this code among the irrelevant information.\nPasskey WithLibrusec The PasskeyWithLibrusec is a more complicated version of Passkey QA dataset, in which we use randomly selected texts from the Librusec dataset as noise to make this dataset more difficult for LLMs.\nruGSM100 The ruGSM100 dataset is a translation of gsm100 one from L-Eval. It contains 100 math problems to be solved using Chain-of-Thought in a few-shot mode. This dataset aims to evaluate the model's reasoning and logical skills in maths. The context for all tasks is a prompt of 16 examples with problem descriptions and answers.\nru2WikiMultihopQA The ru2WikiMultihopQA was created by translating the dataset 2WikiMultihopQA from LongBench, which consists of selected samples with a long context from the original multi-hop QA dataset 2WikiMultihopQA . This Wikipedia-based dataset tests reasoning skills by requiring a model to combine information from multiple texts to answer a question. The format of this dataset, which consists of up to 5-hop questions, makes it difficult for LLMs.\nruQasper The ruQasper was created by translating the Qasper dataset from LongBench, which consists of selected samples with a long context from the original questions answering dataset over academic research papers called Qasper . The goal of the task is to find the answer to the question in one of the parts of the article. The context for samples is drawn from scientific articles to make the task more difficult.\nruTREC The ruTREC was created by translating the TREC from LongBench. The dataset consists of selected samples with a long context from the original TREC . This dataset is a type of few-shot in-context learning, in which the model is given several examples to understand the context, and then it has to answer which topic the question relates to.\nruQuALITY The ruQuALITY was created by translating QuALITY from L-Eval, which consists of selected samples with a long context from the original multiple choice QA dataset called QUALITY . The model must find relevant information in the text and answer by choosing one of the four suggested options.\nruTPO The ruTPO was created by translating TPO from L-Eval. The original dataset in the L-Eval benchmark consists of 15 samples, that are sourced from the TOEFL Practice Online and the dataset TOEFL-QA . The TPO is a multiple-choice QA dataset, and, therefore, the model must find relevant information in the text and answer by choosing one of the four suggested options.\nruSciFi The ruSciFi was created by translating SciFi from L-Eval, which consists of selected samples with a long context from the original SF-Gram dataset, that contains thousands of science-fiction books, novels and movie information. The dataset aims to test the model's ability to follow contextual knowledge instead of parametric knowledge gained at the pretraining stage. The model needs to answer whether the information provided is true or false based on the information from the context and true or false based on the general world knowledge.\nMatreshkaNames To create this dataset, we utilized two sets: Matreshka and a Russian names dataset. The Matreshka dataset comprises brief interactions involving \u201cuser\u201d and \u201cbot\u201d roles, along with a brief description of the topic being discussed by each participant. To form longer contextual samples, we combined multiple interactions and replaced the names \"user\" and \"bot\" with the pull of names taken from the dataset of Russian names. Subsequently, we randomly selected a topic from the combined interactions and the name of the person discussing that topic. The dataset requires the model to identify the individual who discussed the selected topic.\nMatreshka YesNo The Matreshka YesNo is based on the two datasets: Matreshka and Russian names, similar to the MatreshkaNames dataset. Instead of predicting names in the MatreshkaNames, the model is supposed to indicate whether this topic was mentioned in the dialog. The dataset is balanced across answers.\nLongContextMultiQ The LongContextMultiQ is a multi-hop QA long context dataset for Russian that is based on data used for the MultiQ  dataset creation. The original MultiQ dataset is created by multi-hop dataset generation based on Wikidata and Wikipedia, and consists of samples with different length. We selected 200 samples from these generated sources with a long context for each context length.\nruBABILong We adapted the methodology from  to create the Russian Benchmark for Artificial Intelligence for Long (ruBABILong)-context evaluation. It contains five long-context reasoning tasks for QA using facts hidden among distractor facts and irrelevant background text. The ruBABILongQA1 task requires answering a question about a person's location using a single supporting fact. The ruBABILongQA2 and ruBABILongQA3 tasks introduce the challenge of differentiating subjects and objects, utilizing two and three supporting facts, respectively. The ruBABILongQA4 task tackles spatial reasoning through two-argument relations, while the ruBABILongQA5 task involves tracking multiple objects to solve the three-argument relation problem. Each task contains 100 samples, scaled to six sequence lengths from 4k to 128k. We obtained the task facts by translating the bAbI dataset , while the background texts were sampled using books from Librusec.\nLibrusecHistory This dataset was created in question-answering (QA) format using Librusec. Each sample in the LibrusecHistory dataset includes a text paragraph and a corresponding question. To create tasks with different input lengths, we initially selected large texts from various books in different domains and styles, divided them into fragments of several thousand tokens, and created the annotation (see Appendix A). These fragments and their respective questions and answers became the dataset's samples. Longer samples, with lengths up to 64,000 tokens, were created by supplementing these fragments with neighboring paragraphs from the original large text on both sides, resulting in longer inputs for the task.\nLibrusecMHQA This dataset was created in multi-hop Question Answering (QA) format, also using Librusec as a LibrusecHistory. The main difference between these datasets is that in the LibrusecMHQA dataset, the necessary information for the answer is distributed in several parts of the context, making the task more difficult and allowing us to evaluate the model's reasoning skills better. The generation procedure for samples of different lengths remains the same.\nruSciAbstractRetrieval The ruSciAbstractRetrieval is a QA dataset ideologically similar to the PassageRetrieval  dataset from LongBench, that aims to evaluate model's reasoning skills. Each element of the dataset consists of a summary description of the topic and a set text paragraphs created from abstracts of scientific articles from ruSciBench. The goal is to identify the paragraph where the specified topic is discussed. To create this dataset, we randomly choose some abstracts and generate descriptions of their topics using human annotators to acquire targets.\nruSciPassageCount The ruSciPassageCount dataset uses the basic idea of the original PassageCount from LongBench. This QA dataset requires the model to use the full context to solve the problem. To generate the data, we randomly select abstracts from the ruSciBench dataset. We then choose a number of repeats and an ID for the paragraph to repeat. Next, we add the remaining non-repeated paragraphs to the repeated paragraph until we reach the desired context length. The resulting sequence of paragraphs is randomly shuffled. The ground truth for each sample is the number of unique paragraphs."}, {"title": "Evaluation Methodology", "content": "We evaluate 12 popular LLMs that feature long context capability, including GPT-4, GLM4-9B-Chat , ChatGLM2-6B-32k , Saiga-LLaMA-3-8B, LLaMA-3-8B, LLaMA-3-8B-Instruct, LLaMA-2-7B-32K, LongAlpaca-7B, LongChat-7B-v1.5-32k, Mistral-7B-v0.1, Mistral-7B-v0.3, Mistral-7B-Instruct-v0.3. A detailed information about the baseline models is given in Appendix C."}, {"title": "Experimental setup", "content": "Since the tasks themselves are long, in order not to go beyond the context window we fixed the evaluation of tasks in zero-shot, except for tasks ruTREC and ruGSM100 in which the few-shot examples provided as a part of long context input. When the input length of the sample surpasses the maximum model context length, we truncate the input sequence from the right. The baselines were evaluated with greedy decoding (temperature = 1.0, num_beams = 1, do_sample = False) for reproducibility.\nFor each task, we fixed a natural language prompt unified for all the models (see Appendix B for the exact formulation). The prompts were estimated from an empirical analysis of the tasks through a series of experiments. However, it should be noted that further study of this subject is still required.\nWe run all the experiments on a double NVIDIA A100 GPU."}, {"title": "Results", "content": "The baseline results with respect to context length are shown in Table 4 and with respect to tasks are shown in Tables 5, 6, 7. Detailed results for each model are given in Appendix D. Based on the obtained results we can draw the following conclusions for each group of tasks.\nGroup I The tasks from this group are relatively simple, and almost all models pass them well within their maximum input length. The only exception is the LongAlpaca-7B model.\nGroup II MatreshkaYesNo, turns out to be the most straightforward task in the group, which all models cope with naturally. The ruTPO and ruQuALITY tasks are of medium complexity; several models achieved good scores in them.\nThe classic QA task LibrusecHistory is effectively handled by modern models; however, the quality decreases with the input length increase (e.g. for ruSciAbstractRetrieval). Nevertheless, in some cases, a larger context is advantageous, as seen in ruTREC, where increasing the input length helps the model handle the task better because this task is designed in a few-shot format.\nThe most complex tasks in this group can be considered MatreshkaNames and ruSciFi. For the first, several models (e.g., ChatGLM2-6B-32k, LLaMA-2-7B-32K, and LongAlpaca-7B) show low results for any input length. ruSciFi with a 64K context is beyond the capabilities of most models. At the same time, the strongest models (GPT-40 and GLM4-9B-Chat) not only show promising results but also improve the score with the length increase.\nGroup III For tasks from ruBABILong, an increase in context leads to worse results. ruBABILongQA2 and ruBABILongQA3 turn out to be significantly more complex than others, which coincides with results from .\nThe length of the context plays a significant role; with its growth, the quality immediately begins to decline for all but the strongest models.\nLibrusecMHQA turns out to be a complex dataset; the maximum quality of the models for solving this problem is only 50 for 8k tokens.\nGroup IV ruSciPassageCount is the most difficult task created from scratch. All models except GPT-40 handle it poorly, even with a 4K input length; the result's sensitivity to the context's size is high. Besides, all open models fail to cope with ruQasper for complex tasks and domains. A similar result is obtained when measuring the quality of solutions to mathematical problems from ruGSM100. Our conclusions are similar to those obtained in ; the only exception is the LLaMA-2 family of models, which performs worse in our experiments, most likely due to translating tasks into the less familiar Russian language.\nOverall, SFT models perform better than the pretrain once. In most cases, an increase in the input length negatively affects the capabilities of all models. The results indicate that our prior division of tasks into groups is highly correlated with their complexity."}, {"title": "Conclusion", "content": "The rapid development of LLMs has posed new challenges for evaluating their ability to process long texts. To address this problem, we have introduced LIBRA. This benchmark evaluates LLM long context understanding abilities through 21 long-context textual tasks.\nThe tasks enable model evaluation across various context lengths ranging from 4k to 128k tokens based on the analysis of dataset context lengths of the models' tokenizers. Our contribution encompasses a benchmark methodology with open-sourced datasets of different lengths and domains, a codebase for model evaluation, and baseline solution scoring. The datasets are published under the MIT license, and the leaderboard is publicly accessible on HuggingFace."}, {"title": "Limitations", "content": "Although the LIBRA was created to solve the absence of the long context benchmark for Russian and provides significant advancements in evaluating language models with long contexts, it still has a number of limitations that need to be acknowledged.\nData Representation. The texts included in the benchmark are gathered from specific domains, which might not cover the full range of Russian language usage. This can raise concerns about data privacy, representation, and potential biases within the benchmark. It is important to consider that dialects, regional variations, and sociolects may not be adequately represented, potentially leading to biased performance metrics. As a result, models may excel in benchmark tasks but struggle with texts outside these domains, limiting their generalization ability. The corpus used for the benchmark may become outdated over time. New words, phrases, and usage patterns could emerge, making the benchmark less relevant for future model evaluations.\nMethodology limitations. When creating the datasets, we hypothesized that synthetically augmentation of the context length of the datasets, such as LibrusecHistory, would not affect the results. Our experiments show that these tasks are pretty challenging for many models. We made this methodological assumption due to the limitations of human data annotation; it is difficult for people to read large texts and concentrate enough to create questions and search for information within them. This data creation method may result in task errors, particularly when a newly extended text fragment contains conflicting information that could impact the answer. However, we found this approach acceptable due to the increased speed and cost-effectiveness.\nThe current methodology also restricts the number of tasks, and many of them are translated only due to the high cost of data creation.\nLength context. The benchmark focuses on evaluating long contexts, but the definition of \"long context\" can differ based on the application and the model. The chosen context lengths may not be ideal for all usage scenarios, and models could exhibit varying performance. In this paper, we have measured the average fertility of baseline model tokenizers on a full list of datasets from our benchmark to sample different contexts and analyzed the models' results on our datasets across various context lengths. LMs with more parameters may inherently perform better, but this does not necessarily reflect improvements in long context understanding.\nData leakage is a critical concern for modern benchmarks because current models are trained on a significant amount of text from the Internet. Long context benchmarks are particularly risky, as their texts are based on web sources and books. This could potentially lead to data leakage and inaccurate evaluation. However, creating original long texts from scratch not found on the web is exceptionally costly. As a result, we use open sources to develop our benchmark, acknowledging the potential risks. Nevertheless, we firmly believe this will make a valuable contribution to the Russian community, as no long context datasets are currently available.\nEthical Considerations. The data used in the benchmark was created from open data sources. When annotating the data, we obtained transparent permission from all users and made efforts to maintain the confidentiality and anonymity of participants. As the benchmark develops, ongoing efforts are required to identify and minimize biases in the benchmark datasets and evaluation metrics. The benchmark does not currently contain the datasets covering the ethical or AI safety skill evaluation, but this is a space for future work."}, {"title": "Data Annotation Details", "content": "The datasets LibrusecHistory, LibrusecMHQA, and ruSciAbstractRetrieval were created via the crowd-sourced platform.\nIn the LibrusecHistory, annotators were instructed to read a lengthy text and generate four questions based on the text and answer them. Guidelines were provided regarding the type of questions to ask: 1) Questions should be answerable using information present in the text 2) The questions must not be about widely known information but should be related to the text 3) Questions can cover various aspects such as character actions, appearance, thoughts, events, and scene descriptions 4) Logical deductions are not required to answer the questions 5) Each question should have a single, clear, unambiguous answer from the text.\nThe design of the dataset LibrusecMHQA project follows a similar structure to LibrusecHistory, but the question criteria were more complex. In this dataset, the questions were answered by expert editors rather than through crowd-sourcing. The main distinction in the criteria for annotators is the multi-hop questions, where simply reading the sentence containing the answer is insufficient. Instead, reading at least a paragraph of 2-5 sentences, or the entire relevant fragment, is necessary to gather information and generate a complete answer.\nThe ruSciAbstractRetrieval was collected by crowd-sourced annotators. These annotators were asked to read a long text annotation and briefly describe the contents. The criteria for the description were as follows: 1) The description must start with the word \u201cDescribes\u201d. 2) It must be a single sentence, which can be complex. 3) The description should not exceed 30 words, including conjunctions, particles, and prepositions. 4) It should include the main general ideas identified in the abstract but should not include details.\nTraining examples were available for all projects. The contributions of human annotators are amassed and stored in a manner that ensures anonymity. The average hourly compensation exceeds the minimum wage per hour in Russia. Each annotator is informed about topics that may be sensitive in the data, such as politics, societal minorities, and religion."}, {"title": "Dataset Examples", "content": "This section provides examples of the task format for the benchmark datasets. The exact prompts for the benchmark are not fixed. Here we provide prompts used in our experiments.\nPasskey: You are provided with a long text that contains the access key. Just remember the access key.\nContext: {context}\nYou only need to specify the access key in the response.\nQuestion: {input}\nAnswer:\nPasskey WithLibrusec: You are provided with a long text that contains the access key. Just remember the access key.\nContext: {context}\nYou only need to specify the access key in the response.\nQuestion: {input}\nAnswer:\nMatreshkaNames: You are provided with several dialogues. Remember the names of the people and the topics they talked about.\nContext: {context}\nIn the answer, specify only the name of the interlocutor who spoke on the topic from the next question.\nQuestion: {input}\nAnswer:\nMatreshka YesNo: You are provided with several dialogues. Remember the names of the topics that the interlocutors talked about.\nContext: {context}\nIn the answer, you only need to specify 'Yes' if there was such a topic and 'No' if there was no such topic in the dialogues.\nQuestion: {input}\nAnswer:\nLibrusecHistory: You are given a long text in which you need to find the answer to the question.\nContext: {context}\nFind the answer in the text to the following question.\nQuestion: {input}\nAnswer:\nruTREC: Define the type of question below. Here are some examples:\nContext: {context}\nDefine the type of question below.\nQuestion: {input}\nAnswer:\nruSciFi: You are given a long text in which you need to find the answer to the question.\nContext: {context}\nYou need to answer the following question with one of the options: 'False [in the real world: False]', 'True [in the real world: False]', 'True [in the real world: True]' or 'False [in the real world: True]'.\nQuestion: {input}\nAnswer:\nruSciAbstractRetrieval: Below are a few paragraphs. Determine which paragraph the short description corresponds to.\nContext: {context}\nDetermine which paragraph the short description corresponds to. The response must contain the paragraph number.\nQuestion: {input}\nAnswer:\nruTPO: You are given a long text in which you need to find the answer to the question.\nContext: {context}\nYou will be given several answers to the question in the text; choose only one correct one and specify the letter A, B, C, or D.\nQuestion: {input}\nAnswer:\nruQuALITY: You are given a long text in which you need to find the answer to the question.\nContext: {context}\nYou will be given several answers to the question in the text; choose only one correct one.\nQuestion: {input}\nAnswer:\nLongContextMultiQ: You are given a long text where you need to find the answer to the question.\nContext: {context}\nFind the answer in the text to the following question.\nQuestion: {input}\nAnswer:\nLibrusecMHQA: You are given a long text where you need to find the answer.\nContext: {context}\nFind the answer in the text to the following question.\nQuestion: {input}\nAnswer:\nru2WikiMultihopQA: The answer to the question is based on the above excerpts.\nContext: {context}\nAnswer the question briefly, based on the above excerpts.\nQuestion: {input}\nAnswer:\nruBABILongQA1: I'm giving you a context with facts about the location of different people. You need to answer the question based only on information obtained from the facts. If the person was in different places, use the last location to answer the question.\nContext: {context}\nAnswer the question as briefly as possible.\nQuestion: {input}\nAnswer:\nruBABILongQA2: I'm giving you a context with facts about the location and actions of different people. You need to answer the question based only on factual information. If a person took an item in one place and went to another, that item is also in the second place. If a person leaves an item in the first place and moves to the second place, the item remains in the first place.\nContext: {context}\nAnswer the question as briefly as possible.\nQuestion: {input}\nAnswer:\nruBABILongQA3: I'm giving you a context with facts about the location and actions of different people. You need to answer the question based only on factual information. If a person took an item in one place and went to another, that item is also in the second place. If a person leaves an item in the first mets and moves to the second place, the item remains in the first place.\nContext: {context}\nAnswer the question as briefly as possible.\nQuestion: {input}\nAnswer:\nruBABILongQA4: I'm giving you a context with facts about the location and actions of different people. You need to answer the question based only on factual information.\nContext: {context}\nAnswer the question as briefly as possible.\nQuestion: {input}\nAnswer:\nruBABILongQA5: I'm giving you a context with facts about the location and actions of different people. You need to answer the question based only on factual information.\nContext: {context}\nAnswer the question as briefly as possible.\nQuestion: {input}\nAnswer:\nruSciPassageCount: Below are a few paragraphs. Read them and determine the number of unique paragraphs.\nContext: {context}\nDetermine the number of unique paragraphs. The answer must contain only one number.\nQuestion: {input}\nAnswer:\nruQasper: You are provided with a scientific article and a question.\nContext: {context}\nAnswer the question as briefly as possible, using a single phrase or sentence if possible. Don't give any explanations.\nQuestion: {input}\nAnswer:\nruGSM100: Examples of mathematical problems are given below. Think step by step and answer the question.\nContext: {context}\nThink step by step and answer the question.\nQuestion: {input}\nAnswer:"}, {"title": "Detailed Model Information", "content": "The baseline model specifics are presented in Table 9."}, {"title": "Detailed Model Results", "content": "This section presents the detailed results of model evaluation. The results are shown for the following models: GPT-40 (Table 10), GLM4-9B-Chat (Table 11), Mistral-7B-Instruct-v0.3 (Table 12), Mistral-7B-v0.3 (Table 13), LLaMA-2-7B-32K (Table 14), LongChat-7B-v1.5-32k (Table 15), ChatGLM2-6B-32K (Table 16), LongAlpaca (Table 17), LLaMA-3-8B-Instruct (Table 18), Saiga-LLaMA-3-8B (Table 19), LLaMA-3-8B (Table 20) and Mistral-7B-v0.1 (Table 21)."}]}