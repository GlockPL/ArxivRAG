{"title": "M-REWARDBENCH: Evaluating Reward Models in Multilingual Settings", "authors": ["Srishti Gureja", "Lester James V. Miranda", "Shayekh Bin Islam", "Rishabh Maheshwary", "Drishti Sharma", "Gusti Winata", "Nathan Lambert", "Sara Hooker", "Sebastian Ruder", "Marzieh Fadaee"], "abstract": "Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-REWARDBENCH, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-REWARDBENCH, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs' performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-REWARDBENCH dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.", "sections": [{"title": "1 Introduction", "content": "Reward models (RMs) are central to aligning state-of-the-art large language models with human preferences. They serve as an oracle that reflects preferred human values and enables steering language models towards safety, reasoning, and instruction-following capabilities (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022). As LLMS permeate daily life and are used worldwide, it is crucial to understand how their building blocks behave beyond resource-rich languages such as English or Chinese. This is especially important for reward models, as we aim for our LLMs to align with the values of a diverse global population rather than a specific subset.\nDespite their crucial role, reward model development and evaluation remain sparse, especially in multilingual contexts. This is partly due to the limited work extending preference alignment to multilingual settings (Aakanksha et al., 2024; Dang et al., 2024b). The few evaluations, to date, such as RewardBench (Lambert et al., 2024) and RMB (Zhou et al., 2024), are in English and do not cover tasks related to multilinguality such as translating from one language to another or answering user requests that involve cultural nuance. Hence, multilingual RM evaluation is still largely understudied.\nIn this work, we seek to fill this gap by curating resources and conducting a systematic evaluation of state-of-the-art reward models in multilingual settings. Our contributions are three-fold:\n\u2022 We bridge the resource gap (\u00a73) by curating the first massively multilingual preference evaluation dataset in 23 languages across 6 tasks called M-REWARDBENCH. Our language selection is diverse: containing 8 unique scripts, 8 language families, and 12 unique language subgroups.\n\u2022 We close the evaluation gap (\u00a75) by evaluating a wide range of both proprietary and open-source reward models on M-REWARDBENCH. We find that current reward models exhibit a large gap between English-only and non-English settings as shown in Figure 1 with a maximum drop of 13% in performance.\n\u2022 We provide analyses and insights (\u00a76) on how robust the current reward models are in a multilingual context and find that translation quality can have a positive effect on RM performance. We also extend these analyses to several linguistic dimensions, such as a language's resource availability, script, and family.\nWe publicly release all data and code associated with this work.\u00b9 We hope that releasing these artifacts will aid future research in multilingual model development and evaluation."}, {"title": "2 Reward Modelling", "content": "Preference learning and reward models Modern language models undergo a preference learning stage, during which an existing instruction fine-tuned model (IFT) is further aligned with human values and objectives by incorporating human feedback. This feedback comes in the form of preference data, where each instance is a (prompt, chosen, rejected) triple consisting of the prompt and a pair of ranked responses. Given a preference dataset, the objective of preference learning then is to maximize a reward function derived from these preference annotations. There are several ways to maximize this reward function: (a) explicitly training a separate reward model through sequence regression or a classifier based on the Bradley-Terry model (Bradley and Terry, 1952), and then using it to finetune an existing IFT model through techniques like PPO (Christiano et al., 2017; Ouyang et al., 2022) [Classifier RMs], (b) bypassing the reward modeling state by directly optimizing the policy on the preference data (Rafailov et al., 2024) [Implicit RMs], and (c) using generations from a language model to judge between answers (Zheng et al., 2024), and adopting it as a feedback mechanism similar to reward models (Yuan et al., 2024b; Li et al., 2023a) [Generative RMs].\nReward model evaluation RewardBench (Lambert et al., 2024) is a popular benchmark for evaluating reward models. It consists of 2,985 human-validated triples containing a prompt, the human-preferred response (chosen), and the non-preferred response (rejected). RewardBench evaluates RMs on chat, safety, and reasoning capabilities by comparing the RM's preferred response to the chosen answer. Reward models are evaluated via an accuracy metric, i.e., by inferring the raw score an RM assigns for the (prompt, chosen) and (prompt,\nrejected) pairs and then assigning a positive classification label if the preferred response is scored higher than the rejected one."}, {"title": "3 M-REWARDBENCH: A Multilingual Benchmark for Evaluating RMs", "content": "Our design philosophy for M-REWARDBENCH is to construct a benchmark that not only evaluates an RM's general-purpose capabilities in a single language but also assesses its performance on tasks that require multilingual knowledge. We achieve this by curating and translating instances from a wide array of available benchmarks for a specific task category."}, {"title": "General-purpose capabilities: Chat, Safety, Reasoning", "content": "To evaluate RMs on their general-purpose capabilities in another language, we first curate a set of prompts by translating RewardBench (Lambert et al., 2024) into 23 languages using the Google Translate API, which currently outperforms other translation systems for multilingual data (Xu et al., 2024; Liu et al., 2024; Lai et al., 2024, inter alia). After automatic translation, we conduct human evaluation of the translations and filter instances where the prompts contain several translation errors or English-specific concepts that may not exist or are difficult to translate into other languages. Appendix B shows an analysis of these instances.\nWe closely follow the same schema as RewardBench. As a result, the translated subsets of M-REWARDBENCH also contain categories for Chat, Chat-Hard, Safety, and Reasoning."}, {"title": "Multilingual capabilities: Translation", "content": "RewardBench doesn't specifically test for an RM's multilingual capabilities. To extend the evaluation suite towards that, we curated instances from MAPLE (Zhu et al., 2024). MAPLE is a human preference dataset for machine translation tasks that is derived from WMT20/21 test sets containing five translations per source text with each translation scored by human translators on a scale of 1 to 6. MAPLE covers four translation directions: German-to-English (de\u2192en), Chinese-to-English (zh\u2192en), English-to-German (en\u2192de), and English-to-Chinese (en\u2192zh).\nUsing the MAPLE dataset, we create two subsets: TRANSLATION-EASY and TRANSLATION-HARD. To build the TRANSLATION-EASY subset, we select the translation with the highest rating and treat it as the chosen response, and the translation with the lowest rating is selected as the rejected response. For the more challenging TRANSLATION-HARD subset, we randomly select two responses from the remaining three translations such that their ratings are close to one another, and treat the higher-scoring translation as the chosen response and the lower-scoring one as the rejected response.\nWe create 100 such chosen-rejected pairs for each of the two subsets in each of the four translation directions. To avoid noise in the chosen and rejected responses, we make sure that there is an absolute difference of at least 0.25 (5%) between the human scores for the chosen and rejected responses in the TRANSLATION-EASY subset. For the hard datasets, we increase this difference threshold to 0.50 (10%). To increase the diversity when constructing the triplets, we use the collection of 31 prompt templates from the original MAPLE dataset and randomly sample (with replacement) 100 templates that we then apply to the source texts to obtain the final prompts. This resulted in 100 \u00d7 2 instances for each of the four translation directions."}, {"title": "4 Experiment Details", "content": "Selecting reward models for evaluation We select 25 representative models with different parameter sizes ranging from 3 to 104 billion parameters. We also evaluate on different reward model types, encompassing Generative RMs like LlaMa 3.1 Instruct (Dubey et al., 2024) and Aya 23 35B (Aryabumi et al., 2024), Classifier RMs such as Eurus RM 7B (Yuan et al., 2024a) and T\u00fclu 2.5 13B RM (Ivison et al., 2024), and Implicit RMs trained using DPO such as Zephyr 7B (Tunstall et al., 2023) and T\u00fclu 2 DPO (Ivison et al., 2023).\nScoring metric We evaluate models via an accuracy score. For a given triplet (x, YC,REF, Yr,REF)"}, {"title": "5 Results", "content": "5.1 Evaluating state-of-the-art reward models\nImpact of RM type on English to Multilingual performance. First, we compare the RM performance on the English-centric RewardBench with their M-REWARDBENCH scores, as shown in Figure 1. Generative RMs occupy higher positions in the chart suggesting strong multilingual LLM-as-a-judge capabilities compared to other RM types. This also suggests that Classifier RMs and Implicit RMs may struggle more with multilingual generalization than generative RMs. The average performance drop seen for Generative RMs is 3%, while Classifier RMs and Implicit RMs both see an average drop of more than 8%. Similarly, the worst performing Generative RM sees a maximum drop of 6% while this number is more than 13% for both Classifier RMs and Implicit RMs.\nWhen studying the variance of scores, we observe that Generative RMs across different lan-guages have lower variance compared to other model types, suggesting that they have stronger alignment across languages. Finally, the strong correlation values between RewardBench and M-REWARDBENCH indicate that overall, models that excel on English tasks tend to perform better on multilingual tasks as well, though not at the same level.\nDrop in per-category performance from English to Multilingual benchmark. To understand the factors that affect the performance drop from English to Multilingual, we analyze the per-category performance difference of the top ten models. As shown in Table 3, we find that the Chat category, consisting of translated evaluation instances from AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al., 2024), suffers the most performance degradation for non-Generative RMs. All models show a decline in performance on our multilingual benchmark in the Chat-Hard category, with an average degradation of 5.96%. We observe the smallest decline in performance in the reasoning category, with an average decrease of 2.26%.\nLabel consistency across languages. Next, we examine the consistency of the models in labeling the same instances across different languages, using their English performance as the anchor for comparison."}, {"title": "5.2 Translation Task", "content": "The translation task is a completely new addition to this benchmark, introducing a fresh dimension to the evaluation of multilingual models. \nImpact of translation direction. In most cases, we find that RMs perform better when the task is scoring translations from English. This is particularly evident in the TRANSLATION-EASY subset, where most models exhibit higher performance in en\u2192xx compared to xx\u2192en. When we analyze the TRANSLATION-HARD subset, we observe a similar trend for translations from Chinese, but the opposite pattern emerges for German. Some models find it more challenging to select the better translation when the direction is from en\u2192de compared to de\u2192en.\nImpact of task difficulty. We observe that the difficulty of the tasks impacts performance across models. There is a consistent drop from easy to hard tasks across all language pairs. For instance, the gap between en\u2192zh (Easy) and en\u2192zh (Hard) for the GPT-4-Turbo model shows that the increased difficulty level significantly reduces accuracy. This trend is mirrored in the other direction where zh\u2192en (Hard) tasks typically score lower than zh\u2192en (Easy). Overall, models that perform well on easy tasks can struggle to maintain the same level of performance on harder translations, indicating the need for more sophisticated mechanisms to handle linguistic complexity and context ambiguity in challenging scenarios."}, {"title": "6 Analysis", "content": "In this section, we investigate how different multilingual aspects such as translation, linguistic dimensions (resource availability, language family, script), and native-speaker preferences relate to an RM's performance on M-REWARDBENCH.\n6.1 Impact of Multilingual Data Quality\nWe employ two different translation methods to compare the impact of the translation quality of the generated text on RM performance. Figure 4 illustrates the effect of translation quality on the performance of various reward models, grouped as Classifier RMs, Generative RMs, and Implicit RMS when tested on two versions of the multilingual benchmark translated using NLLB 3.3B and Google Translate.\nTranslation Quality Impacts RM Performance. We find that translation quality influences reward model performance across all model types. We compare the translations from two automatic translations, Google Translate and NLLB 3.3B, with the former being of higher quality (Xu et al., 2024; Liu et al., 2024; Lai et al., 2024, inter alia) and found a performance improvement of +1-3% when using a better automatic translator as shown in Figure 4.\nGenerative RMs achieve the highest scores. Among all models, Generative RMs (shown in purple) perform better across the board, with GPT-4 Turbo and GPT-40 leading with the highest scores: 83.5% (Google Translate) and 81.2% (NLLB). These results suggest that translation quality particularly benefits generative models, possibly due to their broader language understanding capabilities.\nSensitivity of Classifier and Implicit RMs. Classifier RMs exhibit a moderate performance gap between NLLB and Google Translate across most models. Implicit RMs exhibit the most noticeable disparity in performance, with certain models, like Mistral-2-7B-DPO and Zephyr-7B-Beta, showing weaker overall performance. The gap widens with Google Translate, where implicit RMs like BTRM Qwen-2-7B perform slightly better."}, {"title": "6.2 Language-specific analysis of RM performances", "content": "To understand if there are performance differences across the 23 languages in M-REWARDBENCH, we aggregate all the RMs' overall scores for each language. We find that the language with the highest-performing RMs is Portuguese (68.7%) while the lowest is Arabic (62.8%). To further understand this difference, we analyze RM performance across three linguistic dimensions, i.e., resource availability, language family, and language script, as shown in Figure 5 (full information for each language can be found in Table 6 in the Appendix).\nImpact of resource availability. We study the influence of resource availability on M-REWARDBENCH performance based on Joshi et al. (2020)'s classification: higher-numbered classes represent languages with more available resources for model training and evaluation. The trend demonstrates that RMs tend to perform better on data rich languages.\nImpact of language family. We find a noticeable variation in performance based on language family: Indo-European and Sino-Tibetan families, which include widely spoken languages such as English, Hindi, and Chinese, achieve the highest scores (\u2248 67.5%). We hypothesize that their strong performance aligns with the availability of ample training data and their presence in Class-5 resource availability. On the other hand, Afro-Asiatic and Turkic families score around 62.5%, reflecting the challenges models face with lower-resource languages, particularly those from underrepresented regions or understudied grammatical structures.\nImpact of script. Figure 5 (right) shows the impact of script type on M-REWARDBENCH performance. The data indicates that models perform best on Latin and Cyrillic scripts (closer to 67.5%), which are more prevalent in high-resource languages like English, Spanish, and Russian."}, {"title": "7 Related Work", "content": "Multilingual Preference Optimization Existing multilingual alignment methods typically rely on classifier RMs for RLHF or generative RMS for curating preferences in DPO. Lai et al. (2023) construct a synthetic preference dataset by translating an expanded version of the Alpaca dataset (Taori et al., 2023), generating model responses, and ranking back-translated outputs with ChatGPT. These ranked responses are then used to train a reward model for final RLHF training. She et al. (2024) focus on enhancing reasoning capabilities in LLMs for non-English languages through iterative DPO (Rafailov et al., 2024). Their method involves translating questions, generating multiple completions from the initial policy, and ranking these completions by calculating the perplexity of the English ground-truth target using NLLB-600M-distilled as a reward model (Costa-juss\u00e0 et al., 2022). Dang et al. (2024a) use Command-R as a reward model to align Aya-23-8B with RLHF. They evaluate both offline and online preference learning by translating ShareGPT into 23 languages and collecting completions from Command-R+ to curate multilingual preferences. However, none of the prior methods investigate the capabilities of classifier RMs or generative RMs in multilingual settings.\nLanguage model benchmarks on multilingual settings Several benchmarks were developed to test the multilingual capabilities of language models. These include MGSM (Shi et al., 2022), a translation of 250 math problems from GSM8K (Cobbe et al., 2021), X-Fact (Gupta and Srikumar, 2021), a multilingual fact-verification benchmark, and OpenAI's MMMLU,3 a translated version of the MMLU (Hendrycks et al., 2020) dataset. In addition, (Son et al., 2024) investigated LLM-as-a-judge and RM capabilities for Korean, and also found that LLMs have critical shortcomings in a language outside of English. M-REWARDBENCH aims to provide a comprehensive benchmark spanning 23 languages to test an RM's multilingual capabilities."}, {"title": "8 Conclusion", "content": "In this work, we conduct a systematic evaluation of reward models in multilingual settings. To achieve this, we construct a new multilingual evaluation benchmark called M-REWARDBENCH covering 23 diverse languages. This dataset addresses a significant gap in the field, where RMs have predominantly been assessed in English, leaving their performance in other languages largely unknown. Our evaluation of various open-source and closed-source RMs shows a significant difference in performance between English and non-English languages. We also show that translation quality and the availability of language resources are positively correlated with RM performance which further highlights the importance of having high-quality, diverse data for developing multilingual RMs.\nBy releasing M-REWARDBENCH to the community, we aim to help facilitate further research in multilingual reward modeling. We hope that our benchmark will serve as a valuable resource for developing RMs that are better aligned with human preferences of a global user base."}, {"title": "Limitations", "content": "Generalization to downstream DPO or policy model performance. Although we evaluated how different RMs perform on M-REWARDBENCH, it is unclear if high performance on M-REWARDBENCH correlates to high performance on downstream multilingual benchmarks. Meanwhile, Ivison et al. (2024) found that in the (English) RewardBench, improvements in RM performance do not necessarily translate to better downstream PPO performance. We leave this exploration for future work.\nImpact of automatic translations versus human-written translations. We did not explore whether the performance and ranking of reward models will change when human-written translations of the English dataset are used. Our analysis in \u00a76.1 shows that when using an automatic translator of high quality, the performance of RMs will also improve. We hypothesize that using Google Translate allows us to approximate human-quality translations in a scalable manner.\nEvaluating RMs on cultural preferences. Our analyses in \u00a7D show instances of preference inver-"}, {"title": "Ethics Statement", "content": "Some prompts in the Chat-Hard and Safety categories of M-REWARDBENCH may contain offensive prompts and responses. We advise users of this benchmark to exercise caution when browsing through the preference instances."}]}