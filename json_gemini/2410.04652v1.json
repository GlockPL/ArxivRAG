{"title": "Multimodal 3D Fusion and In-Situ Learning for Spatially Aware Al", "authors": ["Chengyuan Xu", "Radha Kumaran", "Noah Stier", "Kangyou Yu", "Tobias H\u00f6llerer"], "abstract": "Seamless integration of virtual and physical worlds in augmented reality benefits from the system semantically \"understanding\" the physical environment. AR research has long focused on the potential of context awareness, demonstrating novel capabilities that leverage the semantics in the 3D environment for various object-level interactions. Meanwhile, the computer vision community has made leaps in neural vision-language understanding to enhance environment perception for autonomous tasks. In this work, we introduce a multimodal 3D object representation that unifies both semantic and linguistic knowledge with the geometric representation, enabling user-guided machine learning involving physical objects. We first present a fast multimodal 3D reconstruction pipeline that brings linguistic understanding to AR by fusing CLIP vision-language features into the environment and object models. We then propose \"in-situ\" machine learning, which, in conjunction with the multimodal representation, enables new tools and interfaces for users to interact with physical spaces and objects in a spatially and linguistically meaningful manner. We demonstrate the usefulness of the proposed system through two real-world AR applications on Magic Leap 2: a) spatial search in physical environments with natural language and b) an intelligent inventory system that tracks object changes over time. We also make our full implementation and demo data available at (https://github.com/cy-xu/ spatially_aware_AI) to encourage further exploration and research in spatially aware AI.", "sections": [{"title": "1 INTRODUCTION", "content": "3D scene understanding of physical environments is crucial for context-aware augmented reality (AR). Research and industry endeavors have been steadily advancing the sensing and sensemaking capabilities of mobile computational platforms [15, 64]. Modeling and understanding basic geometric configurations such as room size, solid surfaces, and occlusions enable realistic virtual content placement and interactions [19, 45, 4]. A good semantic understanding and 3D segmentation can reveal the what and where of common objects in an environment to enable complex interactions and deeper blending of the virtual with the physical [21, 71, 37, 62, 61, 26]. Leveraging the power of recent large multimodal models (LMMs) and large language models (LLMs), we can even perform simple spatial and linguistic reasoning in complex real-world scenes [27, 57].\nIf we continue to push the envelope, what new forms of scene understanding and reasoning are in store for context-aware AR and its applications? Inspired by the unified visual and linguistic knowledge latent embeddings from recent neural vision-language models (e.g., OpenCLIP [10]) that enabled unprecedented world-sensing capabilities. We believe a promising holistic approach to probe this direction is to build a multimodal fusion pipeline that integrates the 1) geometric, 2) semantic, and 3) linguistic (vision-language) information of real-world scenes and objects into a unified 3D representation. To this end, we implemented a TSDF-based [11] 3D reconstruction and segmentation pipeline that fuses the deep linguistic features from RGB frames into the 3D representations of the physical space and individual objects.\nThe 3D fusion of neural vision-language features automatically enables linguistically meaningful spatial computing. While previous AR spatial search tasks are constrained by limitations of close-set detection or segmentation models, it is now possible to search for arbitrary objects, or even respond to abstract natural language queries in a physical space. We show in Figure 1 and Figure 4 Application 1 that a heat map responding to the query \"Things that might be dangerous to babies\" highlights the most probable areas through the AR headset. The physical environment, imbued with vision-language features, can provide valuable information about itself via AR interfaces.\nWe fuse context-relevant vision-language features into the 3D scene at the abstraction levels of scene voxel, vertex, and individually segmented physical object. Such CLIP-embedding-fused and semantically-indexed objects are more intelligent \"virtual twins\" than previous 3D geometric models. They become even more powerful when we add user-in-the-loop interactive machine learning controlled by AR interfaces. The user's interactions with physical objects provide valuable model steering instructions to train a"}, {"title": "2 RELATED WORK", "content": "Our work is broadly inspired and germane to topics in mixed reality, computer vision, machine learning, and HCI. In this section, we discuss related work in the areas of AR Scene Understanding, AR Scene Authoring, Physical Interaction in AR, Interactive and Online Machine Learning, Open-Vocabulary 3D Perception, and Version Control for Non-traditional Media."}, {"title": "2.1 Augmented Reality Scene Understanding", "content": "Scene understanding gives semantic meaning to reconstructed 3D models of the physical environment. This allows AR headsets to know not just the geometry but also the what and where of objects in the space, which is needed to unlock context-aware AR and interactions. SLAM++ [55] generates an object-level scene description relying on prior knowledge. However, the requirement of a library of known objects prevents this system from generalizing to arbitrary scenes. FLARE [19] creates AR object layouts that are consistent with the geometry of the physical environment. SnapToReality [45] helps align virtual content to real-world 3D edges and surfaces. Spatial mapping in Hololens [4] can infer semantic surfaces such as walls, floors, platforms, and ceilings.\nRecent leaps in computer vision, foundation models, large language models (LLMs), and large multimodal models (LMMs) provide new directions in tackling object-level scene understanding tasks. Chen et al. [9], PanopticFusion [44], and Panoptic MultiTSDFs [56] go beyond geometric-based AR by combining semantic segmentation with dense 3D reconstruction to achieve objectlevel 3D understanding, removing some of the constraints that SLAM++ was beholden to. Previous work has also explored characterizing scene context (such as location and orientation of objects) to support virtual content placement and realistic interactions with the physical environment [62], and automatic annotation of unprepared environments using machine learning to detect, recognize and segment objects intelligently [50].\nIn a more challenging open-vocabulary setting, Yoffe and Sharma proposed OCTOPUS and OCTO+ [69, 57] to automatically place arbitrary objects on the most suitable surface in AR. They chained a series of state-of-the-art ML methods to build a Mixture of Experts System, and used Segment Anything Model (SAM) [30] to identify individual objects, CLIP and clip-text-decoder [52, 46] to generate object labels, and ViLT [28], CLIPSeg [41], and Grounding DINO [39] to verify object guesses. Like many mixture of experts systems, they used LLMs or LMMs such as GPT-4, GPT-4V [47], and LLaVA [38] as the \"brain\" to reason about appropriate locations for object placement based on curated text and image inputs from various upstream models.\nCompared to their work, which focuses entirely on the question of how to place content in 2D image frame observations of 3D scenes, we tackle the much more general problem of embedding the semantic features within the 3D geometric representation, enabling additional levels of spatial reasoning."}, {"title": "2.2 Augmented Reality Scene Authoring", "content": "Authoring tools complement automatic scene understanding by allowing content creators or end users to assign semantic meaning or information to the reconstructed scene even in the absence of fully reliable automatic detection and segmentation. Early authoring systems such as Columbia's \"touring machine\" [15] relied on designers to manually attach information to the environment, whereas the follow-up MARS system utilized offline and online authoring tools [23]. Other AR applications [42, 60] have also explored the placement of virtual information on recognized real-world objects.\nImmersive authoring [31] in AR allowed users to parse objects from the reconstructed 3D model while moving around in the actual physical space. Interactive online systems such as SemanticPaint [65] and Semantic Paintbrush [43] continuously learn from the user's segmentation input to predict object labels for new unseen voxels as the user captures the environment. Huynh et al. proposed In-Situ Labeling [22] to facilitate more effective language learning in AR settings. SceneCtrl [71] and HoloLabel [6] provide user-in-the-loop scene editing and labeling.\nUnlike the above scene authoring tools that require careful interactive operations on voxels or meshes, this work has individual physical objects automatically segmented and labeled during the 3D reconstruction process. Users interact with the densely labeled scene at the object level for personalization by directly selecting individual objects."}, {"title": "2.3 Interacting with Physical Objects in AR", "content": "Going beyond geometric-based AR, recent context-aware AR focuses on novel interfaces that interact with virtual twins of physical objects in the environment, creating illusions that tightly blend the physical and the virtual. SceneCtrl [71] and Remixed Reality [37] enable manipulation of virtual versions of the physical environment. Annexing Reality [21] uses physical objects as proxies for virtual content to reduce the visual-haptic mismatch. RealitySketch [61] captures user sketchings to create virtual elements bound to physical objects, which dynamically respond to real-world changes. TransforMR [25] can replace real-world humans and vehicles with pose-aware virtual object substitutions to produce semantically coherent MR scenes. Kari et al. [26] demonstrated the concept of Scene Responsiveness which maintains visuotactile consistency in situated MR through visual illusions that hide, replace, or rephysicalize real objects with virtualized objects and characters.\nOur work is related to the above research as we provide novel interfaces for users to interact with the physical room, in our case through natural language and by tracking physical objects' changes over time. It differs from the above works in that we integrate deep vision-language features into the 3D models to automatically identify (segment and label) and remember individual objects for the application scenarios."}, {"title": "2.4 Interactive & Online Machine Learning for AR", "content": "Interactive ML and online ML are distinct learning paradigms that often go hand in hand in real-world interactive AR/MR applications. In these applications, training data becomes available in the form of a stream during the user's interaction with the environment.\nTo learn from previous user gestural and verbal input to predict the segmentation and object labels for new unlabeled parts of the 3D scan, SemanticPaint [65] proposed a streaming random forests algorithm that trains on voxel-oriented patches (VOPs), which are geometric and color features computed from raw TSDF volumes. Semantic Paintbrush [43] adopted the same VOPs as object features (RGB, surface normal vector, and 3D world coordinate) to train a similar streaming decision forest. ScalAR [51] also used a decision-tree-based algorithm to learn from the user input.\nUnlike previous interactive AR systems that trained decision trees on low-level features, this work utilizes deep vision-language models to generate semantic and linguistically meaningful deep latent features for the environment and individual objects. We propose a multimodal representation that considers an object's geometrics (voxels), appearance (RGB), and vision-language features (CLIP) to produce meaningful object graph representations that are robust against issues that low-level features suffer from, such as changing lighting conditions, orientation, and over-simplified semantics. We also propose our own version of interactive online ML, dubbed \"In-Situ Machine Learning\" (see Section 3.3), to refine and improve the performance of our automatic semantic segmentation and object recognition."}, {"title": "2.5 Open-Vocabulary 3D Perception", "content": "The attribute \"open-vocabulary\" describes a system that can recognize objects matching a free-text description, which may contain arbitrary natural-language descriptors (e.g., \"a chair whose color is somewhere between blue and green\") or abstract concepts (e.g., \"what can I use to prop open a door?\"). This is a much more flexible, intuitive, and ultimately more useful paradigm in many scenarios compared to the traditional computer vision approach of classifying objects into a pre-determined semantic taxonomy, which is inflexible and cannot be exhaustive.\nSeveral works have presented open-vocabulary systems for 2D image segmentation and understanding, either at the level of patches or entire images [52, 7, 24], while others have focused on dense, per-pixel representations [20, 33, 53]. These systems became possible because of the massive amount of paired images and text available on the internet that were used as training data for vision-language foundation models. In contrast, for 3D data, it is more difficult to directly develop the 3D-language connection, due to the lack of large datasets of paired geometry and text. To address this, a number of works have attempted to bootstrap 3D openvocabulary perception by distilling or otherwise lifting 2D openvocabulary models to operate on 3D data [27, 48, 73, 14, 67].\nOur system, specifically the Spatial Search with Natural Language feature, belongs to this latter category, lifting CLIP features into 3D by back-projecting them into a voxel grid, using a modification of the popular TSDF fusion algorithm. Our system is primarily differentiated by its design to support interactivity in AR. Most importantly, it operates with low latency, without requiring expensive components such as 3D convolutional neural networks or training neural radiance fields. This enables a smooth and familiar AR scanning workflow. In addition, our system builds an implicit surface mesh representation rather than relying on point clouds or density volumes. This is more amenable to downstream processing and rendering with the traditional graphics pipeline, meaning that it can easily be integrated into existing AR platforms and applications."}, {"title": "2.6 Version Control for Novel Media", "content": "One of our demonstration applications, intelligent object inventory, tracks certain object changes that are akin to the behaviors in version control systems. Software developers are most familiar with text-based version control systems (VCSs) such as Git [1] that keep track of changes in source code. The research community has explored version control interfaces and techniques on novel media other than text editing. Time-Machine Computing [54] tracks computer desktop states and allows users to visit a previous state. MeshGit [13] proposed a mesh edit distance to measure the dissimilarity between two polygonal meshes in 3D modeling workflows. SceneGit [8] tracks object-level element changes in a 3D scene as well as finer granularity changes at the vertex and face level. The Who Put That There system [35] records virtual objects' spatial trajectories from the user's direct manipulation in 3D VR scenes.\nVRGit [74] facilitates synchronous collaboration for manipulating and comparing VR object layouts immersively. While the system provides well-defined spatial versioning features, it operates on a library of predefined models because of its VR nature. Research with physical artifacts in mind, such as Catch-Up 360 [49] and works by Letter et al. [32] focused on the changes of a single object rather than room-size environments like in VRGit. AsyncReality [16] used external devices to volumetrically capture physical events for later immersive playback.\nOur work differs substantially from the above version control research in that physical objects change in ways different from source code or 3D models spatial translation, deformation of non-rigid objects, and appearance changes based on lighting conditions. Naive comparison between two mesh models only introduces counterproductive noises, even if they are perfectly aligned. This work, however, maintains object identity by relying on deep visionlanguage features embedded in the reconstructed 3D environment. We demonstrate one of the first intelligent inventory systems that automatically track basic object changes (missing/unchanged) in real-world environments in Section 4.2."}, {"title": "3 SYSTEM OVERVIEW", "content": "At the heart of our spatially aware AI system that informs the AR user interfaces in this paper is a custom 3D reconstruction pipeline with integrated vision-language fusion and 3D segmentation workflow. The pipeline starts with capturing a physical space a user walks around with an RGBD device that captures registered RGB images and depth maps to reconstruct the 3D model with geometric, semantic, and linguistic understanding (see Figure 1). Figure 2 shows the system overview. The accompanying video also demonstrates the interactive possibilities and flow of the system. We will discuss the design of the main components in this section."}, {"title": "3.1 Multimodal 3D Scene Model Fusion", "content": "Our multimodal scene volume is represented by a multi-channel voxel grid defined over the scene. The channels of this volume are organized into three components: geometry, language, and semantics. The geometric component is a single-channel TSDF volume produced using the typical TSDF fusion algorithm [11] according to the following running-average update rule:\n\n$D_{i+1}(x) = \\frac{D_i(x)W_i(x)+d_{i+1}(x)W_{i+1}(x)}{W_i(x)+W_{i+1}(x)},$\n\nwhere $D_i(x)$ is the accumulated TSDF estimate over all past views for voxel x at time i, and $d_i(x)$ is the TSDF estimate for voxel x from the current view at time i. w represents a per-view weight, and W is the total accumulated weight (we refer the reader to Curless &\nLevoy [11] for further details).\nWe then propose a simple mechanism to extend the scene volume with additional channels, which are populated by fusing feature vectors from image-aligned 2D feature maps as follows:\n\n$F_{i+1}(x) = \\frac{F_i(x)W_i(x)+f_{i+1}(x)W_{i+1}(x)}{W_i(x)+W_{i+1}(x)},$\n\nwhere $f_i(x)$ is a feature vector sampled from view i by perspective projection from voxel x, and F is the generated multi-channel feature volume. The main advantage of fusing features in this manner is that by averaging across views, we develop a more accurate multi-view feature and label estimate over time.\nWe leverage this extension to build the semantic and language components of the volume by fusing in two additional sets of 2D feature maps. The first one (object semantics) is a per-pixel class probability distribution, computed using the panoptic segmentation from k-means Mask Transformer [70]. The second one (language) is a per-pixel CLIP feature computed using OpenCLIP [10]. Since CLIP's feature output for a given image is only a single feature vector with no spatial dimensions, we tile each image into overlapping patches to produce a coarse 2D CLIP feature map. We then define a continuous CLIP feature across the image using bilinear interpolation. While the parameters can vary among capture devices and scenes, our setup resizes input frames to 1024 \u00d7 768 px and uses 2562 px patches with a stride of 128 px.\nFinally, the fusion process results in a per-voxel TSDF estimate, class probability distribution, and CLIP feature, that we use to support downstream applications (Figure 2 left). This process exhibits two properties that make it highly amenable to AR applications: 1) all three volume components are constructed using a running average update rule, so the process is incremental and can accept new input views at any time without needing to revisit earlier views; 2) no iterative optimization is required, leading to fast online reconstruction."}, {"title": "3.2 Post-Processing and Scene Manager", "content": "Following the multimodal 3D fusion, we perform three postprocessing tasks to support downstream applications.\n1) Mesh extraction. We run Marching Cubes [40] to extract a triangle mesh from the TSDF volume. This allows for convenient rendering and integration with existing AR graphics pipelines. Figure 1 shows various mesh visualizations rendered in AR headsets.\n2) 3D semantic segmentation. To create useful spatial awareness for AR, we are interested in going beyond per-voxel semantic information to delineate full objects that users can more easily select and manipulate. We therefore build on the class probability volume developed in Section 3.1 by first labeling each voxel with the class for which it has the highest predicted probability, and then segmenting consecutive volumes according to those labels using a custom 3D flood fill implementation. Similar to classic 2D floodfill algorithms that find connected regions on images [59], our 3D method clusters voxels of the same segmentation class in the 3D volume grid to parse individual objects in the user's surroundings, extracting the complete object boundary, shape, and identity, with no user intervention required (see Figure 2 semantic volume).\n3) Intelligent object inventory. During the object parsing process, the Scene Manager creates an object inventory by associating the per-voxel CLIP features with the individual objects. The scene manager, as shown in Figure 2, is the central communication hub that a) manages multiple versions of environment models, b) sends and receives data between the AR user interface via HTTP requests, and c) utilizes the in-situ machine learning to \"remember and reidentify\" unique objects for the intelligent object inventory.\nCompared to conventional 3D reconstruction focusing on user manipulation of the object model's geometric representations (e.g., mesh or volume), the proposed multimodal fusion associates each object's semantic/identity, metadata, and vision-language (CLIP) features with its geometrics, producing intelligent virtual twins. With CLIP features attached to objects, novel spatially-aware-AI interfaces are unlocked through interactive machine learning, which we will discuss in the next section. After these post-processing steps, users can easily interact with physical objects through the AR virtual pointer (see Figure 2 user input).\nSystem Performance. We host our system on a local server with a single NVIDIA RTX 3090 GPU (24GB of VRAM) to perform multimodal fusion and in-situ learning. The voxel size used in TSDF reconstruction is a key parameter that affects both the fusion quality and the object inventory. We present the post-processing performance in Table 1 for the two office test scenes shown in our paper and the accompanying video. A small voxel size like 2cm reconstructs the space at a high definition but comes at a much higher"}, {"title": "3.3  \"In-Situ\" Machine Learning", "content": "We propose a novel interactive and online machine learning concept called \"In-Situ\" Learning to improve AR experience in complex real-world environments. Since we have attached rich multimodal features to individual objects, one practical optimization objective for the in-situ learning model is to learn to remember and re-identify individual objects across different scans. We define in-situ learning as the process of encoding real-time data into a neural network, such that the network itself serves as both the knowledge container and decision-making unit for downstream tasks, e.g., as a probe to identify changes such as new or missing objects. This is related to AR works discussed in Section 2.4 and works that perform online neural scene encoding (Feng et al. NARUTO [17], Sandstrom et al. Point-SLAM [17]), but we introduce the additional temporal dimension to enable object tracking across multiple time points (room scans on different days) in the second application scenario.\nThe \"in-situ\" (Latin for \"in position\" or \"on site.\") nature of our learning concept is characterized by the following observations:\n1) As we live in a complex and constantly evolving world, the neural vision-language features that represent objects and contexts also change dynamically. These unlabeled training data are only generated at the moment when the user captures the physical space.\n2) Novel ground truth samples that guide the supervised learning (e.g., personalized object names and merged segments) are generated only when the user interacts with the environment. Unlike offline-collected large-scale datasets that present a universally accepted ground truth of our world [12, 36], the ground truth in in-situ learning varies among users and over time.\n3) Similar to typical interactive machine learning, the model's performance is evaluated by the user instead of by a fixed benchmark. It's at the user's discretion to decide if the model is sufficiently optimized, or else they can provide more training data by re-scanning the space or re-labeling incorrectly classified objects to fine-tune the model.\n4) Also similar to interactive machine learning, the model always immediately reflects individual user's latest annotations and preferences, which is in contrast to the typical batch incorporation of user feedback necessitated by large offline-trained models.\nTo accommodate the task of tracking physical objects, we convert an object's irregularly shaped volume representation to a graph representation to optimize for online machine learning. Unlike previous scene authoring AR work that trained on low-level TSDF features alone [65, 43, 51], the multimodal intelligent virtual twins allow us to create a novel graph representation that combines the geometric, semantic, and linguistic features for every object in the scene. As shown in Figure 3, we treat every voxel as a node pointing to the object's centroid, which converts an object's irregular voxel representation into a dense graph representation. For efficiency and data augmentation, we stochastically sample 30 voxels from the dense representation in each training iteration to generate a sparse graph, whose node attribute is the voxel location's OpenCLIP [10] vision-language feature, which we found sufficient to re-identify objects and reveal object changes (Section 4.2) without having to align the scene models for naive mesh comparison. Additional properties, such as RGB values, the geometric 3D vector pointing at the object's centroid, and relative spatial relationships to other objects can also be integrated as node or edge attributes based on specific task needs.\nIn other words, we turn a hard 3D object classification task into an easier graph classification task, which maintains its effectiveness even if the object or the environment changes dynamically (spatial translation, non-rigid deformation, varying lighting conditions). Additionally, the in-situ model is incrementally fine-tuned as the user provides new inputs from subsequent scans. Specifically, to learn the graph-based objects, we adopt a dynamic graph CNN [66] as the backbone of the in-situ model to train a graph classifier that predicts if the graph belongs to a class label previously trained on, or an unknown class (e.g., background objects not marked by user). A single in-situ model is trained for a specific space \u2013 it can be tailored by one user for personalization or shared between a group of users for collaboration and information exchange.\nTo summarize, in-situ learning's novelty lies in the \"just-in-time\" user-generated data and the evaluation metric that is based on user satisfaction. In real-world applications, system designers should choose the specific type of machine learning paradigm (e.g., supervised or self-supervised), the model architecture, and the training strategy that best supports the task at hand."}, {"title": "4 PROTOTYPE APPLICATIONS", "content": "In this section, we showcase two real-world Magic Leap 2 AR prototype applications to demonstrate the potential of the proposed multimodal 3D fusion pipeline and new tools and interfaces for users to interact with physical spaces and objects when used in conjunction with \"in-situ\" machine learning."}, {"title": "4.1 Spatial Search with Natural Language", "content": "We foresee a future of sophisticated real-virtual interactions, requiring deep understanding that goes beyond discrete objects with labels. By fusing 3D vision-language features into the 3D models, our system permits spatial search in a physical space using natural language. The user may issue complex, even abstract queries, for example, \"things that might be dangerous to babies.\" The system responds by highlighting matching regions in the user's surroundings to reveal potential falling hazards and objects that could trip a baby, as shown in Figure 4 (Application 1). Figure 6 illustrates more example queries and results.\nThis search capability is built on the language component of our scene volume, composed of a multi-channel CLIP feature volume. CLIP is key to this process as it embeds images and text into a shared feature space. Thus we can cast natural language search as building a map over the scene of the similarity between scene CLIP features and the CLIP embedding of a user-supplied text query. To enable this, we first resample the CLIP feature grid using trilinear interpolation to obtain a CLIP feature at each mesh vertex. We then compute the similarity of each vertex feature to the query feature, relative to a set of negative queries, following the search method of CLIP Surgery [34]. However, CLIP Surgery uses a long, fixed list of negative queries to identify redundant features that come at the cost of longer computation time and higher memory load, which exceed the acceptable limits for real-time interactive AR applications. We build the negative query list as the union of all class names extracted by our 3D semantic segmentation step. Our list is therefore shorter and more relevant, allowing us to produce heatmap outputs to the query similarity efficiently while filtering out noisy responses across the scene.\nOur Magic Leap 2 prototype application, as demonstrated in the accompanying video and Figure 4, overlays the response heatmap on top of the environment and physical objects via the optical-seethrough display to provide an immersive user experience. Leveraging this spatial search ability in AR applications can provide users with an enhanced understanding and navigation of unfamiliar spaces. It can also enable them to explore complex environments faster than they would manually."}, {"title": "4.2 Intelligent Object Inventory", "content": "We imagine an intelligent AI AR companion that keeps a \"temporal and spatial inventory\" of objects for real-world environments, helping users keep track of the objects in their space. Integrating geometric and semantic knowledge into the joint 3D space makes it possible to automatically parse individual 3D objects from the environment for an object-centric user interface. As shown in our accompanying video and Figure 4 Day 1, the \"magical\" instantaneous highlighting and selection of physical objects in optical-seethrough AR displays from any viewpoint provides an intuitive and direct interface for users to edit or personalize their space.\nOur main goal with this application is to show that when coupled with in-situ learning, the multimodal-feature-fused environment models can unlock novel spatially-aware AI user interfaces. For instance, having access to intelligent virtual twins of every physical object in the room makes it feasible to train a machinelearning model to \"learn and remember\" physical objects, maintain object identities, and track object changes without aligning any noisy unstructured mesh models. To this end, we introduce a basic intelligent inventory system to visually present one interpretation of object changes in real-world environments.\nWe briefly discussed in Section 2.6 that the concept of \"changes\" from text-based version control systems does not automatically translate to the spatial, morphological, or appearance changes in physical objects. While the \"true removal\" of an object or a paragraph of text is similar, naively comparing different scans of a space captured on different days yields counterproductive noise and does not maintain object identities. The ability to re-identify an arbitrary physical object is critical for effective user assistance. Learning to remember and re-identify an object relies on the in-situ learning model, which we introduced in Section 3.3. We will now discuss how it is used to reveal unchanged or missing objects in AR. This proof-of-concept AR demonstration does not yet constitute a full inventory system. Limitations are discussed in Section 5.\nWe offer three actions to collect user input to determine which objects to track and train on:\n1) Merge. Users can merge multiple mesh segments into one if a single object was fragmented during the 3D reconstruction process, e.g., false boundaries introduced by shadows. This change is picked up by in-situ learning, leading to the recognition of segmented parts as the same object in future scans of this space.\n2) Rename. Users can also customize the automatically generated object labels (e.g., \"bottle:2\" to \"Joe's thermos\") to improve their utility. This feature proves particularly beneficial in collaborative environments, such as a shared office, where it can help specify the ownership of items more clearly. In collaborative settings where multiple users might adjust the same space at different times, user-specified labels naturally reduce confusion. The accompanying video showcases an actual office setting in AR, where objects are distinctly tagged with their respective owner's names.\n3) Remember. Users can direct the system to track certain objects in the environment without further editing actions. The same objects should be re-identified with their current properties. This design provides a quick and easy way to collect \"positive samples\" that will be used to optimize the in-situ learning model for object classification.\nThe user's object-level personalization input provides the ground truth to guide the learning objective. The in-situ learning model is trained to classify arbitrary objects based on their neural features from randomly sampled sparse graphs, which improves robustness across different scans and avoids overfitting. Specifically, objects that are merged, renamed, or remembered are flagged as \"positive\" samples with a unique label and assigned a class index in the classifier's ground truth. Through various design experiments, we arrived at a training strategy that classifies all other \"non-personalized\" objects as a \"null\" class with an index of zero. We sample null class features from all other voxels that do not belong to any of the personalized objects. While it performs robustly for re-identifying objects, this strategy limits the ability to differentiate newly introduced objects from the null objects, which we will discuss in Section 5. The user triggers the training after they finish personalization. The training stops automatically after the model reaches its peak accuracy (over 95% in our demo scene) plus certain cooldown epochs. Other strategies, such as adaptive learning rate, can also be used when users label new objects and fine-tune a trained model. In our office demo scene, we set the cool-down epochs to 10 and the total in-situ model training takes less than 8 seconds."}, {"title": "5 DISCUSSION", "content": "More use cases. The ability to search in a physical space with natural-language queries has the obvious use case of quickly finding objects based on their description", "removal\" or \"modifications\" of physical objects. In particular, illusions similar to those described in [25, 26": "could be realized without the need to manually pre-mapping the environment.\nWhile working on our pipeline and prototype application", "reasons": "a) The most capable LLMs remain cloud-based", "5": "the requirement of network traffic and added latency add friction to real-world AR use cases. Our CLIP-based solution provides contextagnostic features that greatly benefit computational efficiency and flexibility for downstream tasks - the expensive scene reconstruction happens once to support unlimited numbers of queries and object learning. The CLIP features sufficiently support the example applications while relying only on local computation. Most importantly", "touring machine\" [15": ".", "choices": 1, "actual conversations with physical spaces\", and 2) limited the system's 3D object segmentation and object inventory feature to the 100 categories of common stuff and things defined in the COCO dataset [36, 29": ".", "30": "to identify object boundaries and then use LMMs such as LLaVA [3"}]}