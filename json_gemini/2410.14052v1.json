{"title": "FROM ISOLATED CONVERSATIONS TO HIERARCHICAL SCHEMAS: DYNAMIC TREE MEMORY REPRESENTATION FOR LLMS", "authors": ["Alireza Rezazadeh", "Zichao Li", "Wei Wei", "Yujia Bao"], "abstract": "Recent advancements in large language models have significantly improved their context windows, yet challenges in effective long-term memory management remain. We introduce MemTree, an algorithm that leverages a dynamic, tree-structured memory representation to optimize the organization, retrieval, and integration of information, akin to human cognitive schemas. MemTree organizes memory hierarchically, with each node encapsulating aggregated textual content, corresponding semantic embeddings, and varying abstraction levels across the tree's depths. Our algorithm dynamically adapts this memory structure by computing and comparing semantic embeddings of new and existing information to enrich the model's context-awareness. This approach allows MemTree to handle complex reasoning and extended interactions more effectively than traditional memory augmentation methods, which often rely on flat lookup tables. Evaluations on benchmarks for multi-turn dialogue understanding and document question answering show that MemTree significantly enhances performance in scenarios that demand structured memory management.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite recent advances in large language models (LLMs) where the context window has expanded to millions of tokens (Ding et al., 2024; Bulatov et al., 2023; Beltagy et al., 2020), these models continue to struggle with reasoning over long-term memory (Sun et al., 2021; Liu et al., 2024; Kuratov et al., 2024). This challenge arises because LLMs rely primarily on a key-value (KV) cache of past interactions, processed through a fixed number of transformer layers, which lack the capacity to effectively aggregate extensive historical data. Unlike LLMs, the human brain employs dynamic memory structures known as schemas, which facilitate the efficient organization, retrieval, and integration of information as new experiences occur (Anderson, 2005; Ghosh & Gilboa, 2014; Gilboa & Marlatte, 2017). This dynamic restructuring of memory is a cornerstone of human cognition, allowing for the flexible application of accumulated knowledge across varied contexts.\n\nA prevalent method to address the limitations of long-term memory in LLMs involves the use of external memory. Weston et al. (2014) introduced the concept of utilizing external memory for the storage and retrieval of relevant information. More recent approaches in LLM research have explored various techniques to manage historical observations in databases, retrieving pertinent"}, {"title": "2 RELATED WORK", "content": "Memory-Augmented LLMs Recent advancements in memory-augmented LLMs have introduced various strategies for enhancing memory capabilities. Park et al. (2023) developed LLM-based agents that log experiences as timestamped descriptions, retrieving memories based on recency, importance, and relevance. Similarly, Cheng et al. (2023) developed Selfmem with a dedicated memory selector. MemGPT (Packer et al., 2023) proposed automatic memory management through LLM function-calling for conversational agents and document analysis, providing a pre-prompt with detailed instructions on memory hierarchy and utilities, along with a schema for accessing and modifying the memory database. Zhong et al. (2024) introduced MemoryBank, a long-term memory framework that stores timestamped dialogues and uses exponential decay to forget outdated information. Additionally, Mitchell et al. (2022) proposed storing model edits in an explicit memory and learning to reason over them to adjust the base model's predictions. These methods represent common solutions for adding memory to LLMs, focusing on tabular memory storage and vector similarity retrieval (Zhang et al., 2024). However, as memory scales or information becomes dispersed across multiple entries, their unstructured representations reveal significant limitations.\n\nAnother line of work explores triplet-based memory. For example, Modarressi et al. (2023) proposed encoding relationships in triplets, and Anokhin et al. (2024) extended this approach to graph-based triplet memory for text-based games. While effective at encoding individual relations or scene graphs at the object level, these methods struggle with scalability and generalization to more complex data that do not fit neatly into a strict triplet format.\n\nStructured Retrieval-Augmented Generation Approaches To address the limitations of unstructured memory representations, recent advances have integrated structured knowledge into RAG models, enhancing navigation and summarization in complex QA tasks. Trajanoska et al. (2023) leveraged LLMs to extract entities and relationships from unstructured text to construct knowledge graphs. Similarly, Yao et al. (2023) proposed techniques to fill in missing links and nodes in existing knowledge graphs by utilizing LLMs to infer unseen relationships. Ban et al. (2023) identified causal relationships within textual data and represented them in graph form to enhance understanding of causal structures. In the context of retrieval-augmented generation, Gao et al. (2023) provided a comprehensive review of existing RAG methods, and Baek et al. (2023) utilized knowledge graphs as indexes within RAG frameworks for efficient retrieval of structured information.\n\nMore relevant to our work, RAPTOR (Sarthi et al., 2024) organizes text into a recursive tree, clustering and summarizing chunks at multiple layers to enable efficient retrieval of both high-level themes and detailed information. GraphRAG (Edge et al., 2024) constructs a knowledge graph from LLM-extracted entities and relations, partitioning it into modular communities that are independently summarized and combined via a map-reduce framework. While these methods effectively structure large textual data to improve retrieval and generation capabilities, they are limited to static corpora, requiring full reconstruction to integrate new information, and do not support online memory updates.\n\nIn this work, we propose a novel structured memory representation that overcomes these limitations by enabling dynamic updates and efficient retrieval in large-scale memory systems without necessitating full reconstruction."}, {"title": "3 METHOD", "content": "MemTree represents memory as a tree T = (V, E), where V is the set of nodes, and E \u2286 V \u00d7 V is the set of directed edges representing parent-child relationships. Each node v \u2208 V is represented as:\n\n$\\upsilon = [C_\\upsilon, e_\\upsilon, p_\\upsilon, C_V, d_\\upsilon]$\n\nwhere:\n\n\u2022 Cv: the textual content aggregated at node v.\n\u2022 ev \u2208 Rd: an embedding vector derived using an embedding model femb(Cv).\n\u2022 pv \u2208 V: the parent of node v.\n\u2022 CCV: the set of children of node v, with edges directed from v to each u \u2208 Cv."}, {"title": "3.1 \u039c\u0395\u039cORY UPDATE", "content": "The memory update procedure in MemTree is triggered upon observing new information (e.g., a new conversation). This procedure ensures that the tree structure dynamically adapts and integrates new data while maintaining a coherent hierarchical representation. The complete memory update process is outlined in Algorithm 1.\n\nAttaching New Information by Traversing the Existing Tree To integrate new information, we begin by creating a new node new with the textual content Cnew. Then we start tree traversal from the root node. At each node v, MemTree evaluates the semantic similarity between the new information Cnew and the children of the current node in the embedding space. This evaluation is performed by computing the embedding enew = femb(Cnew) for the new content Cnew and comparing it to the embeddings of the child nodes C(v) of the current node v using cosine similarity.\n\nThis similarity evaluation drives the following decisions:\n\n\u2022 Traverse Deeper: If a child node's similarity exceeds a depth-adaptive threshold \u03b8(dv), traversal continues along that path. If multiple child nodes meet this criterion, the path with the highest similarity score is chosen.\n\u2022 Boundary: When traversal reaches a leaf node, the leaf is expanded to become a parent node, accommodating both the original leaf node and new as children. The parent's content is then updated to aggregate both the original leaf node's content and the new information Cnew. The details of this aggregation process will be explained below.\n\u2022 Create New Leaf Node: If all child nodes' similarities are below the threshold \u03b8(dv), vnew is directly attached as a new leaf node under the current node.\n\nThe similarity threshold \u03b8(d) is adaptive based on the node's depth d, defined as:\n\n$\\theta(d) = \\theta_0\\lambda^d$,\n\nwhere \u03b80 is the base threshold, and \u03bb controls the rate of increase with depth. This mechanism ensures that deeper nodes, which represent more specific information, require a higher similarity for new data integration, thereby preserving the tree's hierarchical integrity. Further details, including specific parameter values, are provided in the Appendix A.1.3.\n\nUpdating Parent Nodes Along the Traversal Path Once new is inserted, the content and embeddings of all parent nodes v along the traversal path are updated to reflect the new information. This is achieved through a conditional aggregation function:\n\n$C'_v \\leftarrow \\text{Aggregate}(C_v, C_{\\text{new}} | n)$,\n\nwhere $C'_v$ is the updated content, and n = |C(v)| is the number of descendants of node v. The aggregation function, implemented as an LLM-based operation, combines the existing content Cv"}, {"title": "3.2 \u039c\u0395\u039cORY RETRIEVAL", "content": "Efficient and effective retrieval of relevant information is crucial for ensuring that MemTree can provide meaningful responses based on past conversations. Inspired by RAPTOR (Sarthi et al., 2024), we adopt the collapsed tree retrieval method, which offers significant advantages over traditional tree traversal-based retrieval.\n\nCollapsed Tree Retrieval The collapsed tree approach enhances the search process by treating all nodes in the tree as a single set. Instead of conducting a sequential, layer-by-layer traversal, this method flattens the hierarchical structure, allowing for simultaneous comparison of all nodes. This technique simplifies the retrieval process and ensures a more efficient search.\n\nThe retrieval process involves the following steps:\n\n1. Query Embedding: Embed the query q using femb(q) to obtain eq.\n2. Similarity Computation: Calculate cosine similarities between eq and all tree nodes.\n3. Filtering: Exclude nodes with similarity scores below a threshold \u03b8retrieve.\n4. Top-K Selection: Sort the remaining nodes by similarity and select the top-k most relevant nodes."}, {"title": "4 EXPERIMENTS", "content": "4.1 DATASETS\nWe evaluate the effectiveness of MemTree across various settings using four datasets: Multi-Session Chat, Multi-Session Chat Extended, QUALITY, MultiHop RAG. These datasets were selected to"}, {"title": "4.2 BASELINES", "content": "We compare MemTree with various baseline methods along with a naive baseline, which involves concatenating all chat histories and feeding them into a large language model (LLM):\n\n\u2022 MemoryStream: Park et al. (2023) proposes a flat lookup-table style memory that logs chat histories through an embedding table. The primary distinction between MemTree and this baseline is that MemTree utilizes a structured tree representation for the memory and models high-level representations throughout the memory insertion process.\n\u2022 MemGPT: (Packer et al., 2023) introduces a memory system designed to update and retrieve information efficiently. It uses an OS paging algorithm to evict less relevant memory into external storage. However, like MemoryStream, it does not format high-level representations.\n\u2022 RAPTOR: Sarthi et al. (2024) constructs a structured knowledge base using hierarchical clustering over all available information. The key difference between MemTree and this baseline is that MemTree operates as an online algorithm, updating the tree memory representation on-the-fly based on incoming knowledge, while RAPTOR applies hierarchical clustering on a fixed dataset.\n\u2022 GraphRAG: Edge et al. (2024) introduces a graph-based indexing approach designed to improve query-focused summarization and extract global insights from large text corpora. Like RAPTOR, GraphRAG assumes access to the entire corpus and applies the Leiden algorithm to identify community structures within the document graph. However, while MemTree expands its memory top-down to allow for efficient, online updates, GraphRAG generates community summaries in a bottom-up fashion, which is less suited for real-time adaptability."}, {"title": "4.3 IMPLEMENTATION DETAILS AND EVALUATION METRICS", "content": "Following previous work (Packer et al., 2023; Tang & Yang, 2024), we report the end-to-end question answering performance. Given each context-question-answer tuple, the experimental procedure involves four steps:\n\n1. Load the corresponding dialogue/history into the memory.\n2. Retrieve the relevant information from the memory based on the given query.\n3. Use GPT-4o to answer the query based on the retrieved information.\n4. Evaluate the generated answer using one of the following two metrics: 1) Use GPT-4o to compare the generated answer with the reference answer, resulting in a binary accuracy score; 2) Evaluate the ROUGE-L recall (R) metric of the generated answer compared to the relatively short gold answer labels, without involving the LLM judge.\n\nThe detailed prompts for steps 3 and 4 can be found in Appendix A.2. Other implementation details for MemTree can be found in Appendix A.1."}, {"title": "5 RESULTS", "content": "5.1 MULTI-SESSION CHAT\n15-round dialogue We present the MSC results in Table 1. For the naive baseline, directly providing the full history to GPT-4o yields the best result, achieving an accuracy of 96%. This outcome is expected, given that the entire dialogue consists of only 15 rounds and fewer than one thousand tokens. We also note that providing a summary of the chat history significantly drops performance to 35%, even for the more powerful GPT-4 Turbo model (Packer et al., 2023). This decline occurs because the summary may not cover the topics the query is addressing. To directly compare the performance of different memory management algorithms, we consider the setting where only the query and the retrieved information are provided to the LLM. In this scenario, MemTree surpasses both MemStream and MemGPT."}, {"title": "5.2 SINGLE-DOCUMENT QUESTION ANSWERING", "content": "Table 3 presents the accuracy of various models on the QuALITY benchmark. Llama-3.1 70B, which processes the full text in a single pass, achieves the highest overall accuracy at 65.1%. This superior performance is attributed to the dataset's relatively short length (5000 tokens), a trend also observed with the MSC dataset. Offline RAG methods such as RAPTOR and GraphRAG, designed for handling knowledge retrieval over longer contexts, achieve lower accuracies of 59.0% and 62.8%, respectively. The current online memory update method, MemoryStream, struggles with efficiently extracting memory key-value pairs, resulting in a significantly lower accuracy of 43.8%. In contrast, our method, MemTree, matches the offline performance of RAPTOR with a slightly higher accuracy of 59.8%, especially excelling on hard questions that demand deeper reasoning and comprehension. Moreover, MemTree retains the advantage of being an online method, allowing for continuous memory updates at minimal computational cost. Refer to Figure A.2 for a visualization of the results."}, {"title": "5.3 MULTI-DOCUMENT QUESTION ANSWERING", "content": "Table 4 summarizes the end-to-end performance of MultiHop RAG using various memory retrieval algorithms. All methods perform exceptionally well on inference-style questions, which focus on fact-checking based on a single document, consistently achieving over 95% accuracy. However,"}, {"title": "6 CONCLUSION", "content": "MemTree effectively addresses the long-term memory limitations of large language models by emulating the schema-like structures of the human brain through a dynamic tree-based memory representation. This approach enables efficient integration and retrieval of extensive historical data, as demonstrated by its superior performance on four benchmarks with different interactive contexts. Our evaluations reveal that MemTree consistently maintains high performance and demonstrates human-like knowledge aggregation by capturing the semantics of the context within its tree memory structure. This advancement offers a promising solution for enhancing the reasoning capabilities of LLMs in handling long-term memory."}, {"title": "Ethical Statement", "content": "In developing MemTree, we commit to ensuring that no private or proprietary data is mishandled during our experiments, and all data used for training and evaluation are publicly available. While our current research does not explicitly address principles such as transparency, responsibility, inclusivity, bias mitigation, or user safety, we recognize that recent advancements in these areas can be integrated into the memory learning component of our algorithm. We encourage the research community to engage with these ethical considerations as we strive to enhance our understanding and implementation of responsible AI practices."}, {"title": "Reproducibility Statement", "content": "We provide comprehensive details for reproducing our results in Section 4 and the Appendix, including our experimental setup, evaluation metrics, and implementation settings. The code and scripts used in our experiments will be made publicly available upon acceptance. All external libraries and dependencies required for reproduction are specified. Our method has been evaluated on both open-source and commercial models to demonstrate its applicability."}, {"title": "Disclaimer", "content": "This content is provided for general information purposes and is not intended to be used in place of consultation with our professional advisors. This document may refer to marks owned by third parties. All such third-party marks are the property of their respective owners. No sponsorship, endorsement or approval of this content by the owners of such marks is intended, expressed or implied."}, {"title": "A APPENDIX", "content": "A.1 MEMTREE DETAILS\nFurther details and parameter settings for our approach are outlined below. Unless otherwise specified, these settings are consistent across all experiments presented in the paper."}, {"title": "A.1.1 \u039cEMTREE ALGORITHM", "content": "The following outlines the algorithmic procedure for incrementally updating and restructuring the memory representation in MemTree. This approach ensures that new information is efficiently integrated into the existing memory hierarchy while dynamically adjusting based on content similarity and structural depth.\n\nParameters:\n\n\u2022 c: the textual content stored at a node or introduced as new information.\n\u2022 e: the embedding vector representing the content, generated by an embedding function femb.\n\u2022 v: a node in the memory tree, which contains content, embeddings, and connections to other nodes. Note that the root is a structural node and does not hold content.\n\u2022 d: the depth of a node in the tree."}, {"title": "A.1.2 AGGREGATE OPERATION", "content": "When new information is added, the content of parent nodes along the traversal path is updated through a conditional aggregation. This process combines the existing content of the parent node with the new content, factoring in the number of its descendants. The aggregation operation is implemented using the following prompt:\n\nYou will receive two pieces of information: New Information is detailed, and Existing Information is a summary from {n_children} previous entries. Your task is to merge these into a single, cohesive summary that highlights the most important insights.\nFocus on the key points from both inputs.\nEnsure the final summary combines the insights from both pieces of information."}, {"title": "A.1.3 ADAPTIVE SIMILARITY THRESHOLD", "content": "The adaptive similarity threshold ensures that deeper nodes, representing more specific information, require higher similarity for new data integration, while shallower nodes are more abstract and accept broader content. This mechanism preserves the tree's hierarchical integrity by adjusting selectivity based on the node's depth. The threshold is computed as:\n\n$\\text{threshold} = \\text{base\\_threshold} \\times \\exp\\left(\\text{rate} \\times \\frac{\\text{current\\_depth}}{\\text{max\\_depth}}\\right)$", "where": "\u2022 base_threshold = 0.4\n\u2022 rate 0.5\n\u2022 current_depth is the depth of the current node.\n\u2022 max_depth is the maximum depth of the tree."}, {"title": "A.1.4 RETRIEVAL", "content": "For the MSC experiment, the retrieval system returns the top k = 3 similar dialogues from 15-round conversations, with a context length of 1000 tokens for all models. In the MSC-E dataset, due to longer conversations, the retrieval returns the top k = 10 similar dialogues, with a context length of 8192 tokens to accommodate the models with full-chat history. This setting is similarly applied to the Multihop RAG and QUALITY experimenst, where longer contexts are required."}, {"title": "A.2 FURTHER EXPERIMENTAL DETIALS", "content": ""}, {"title": "A.3 DATASET STATISTICS", "content": "We summarize the dataset statistics in Table A.1 to provide a clear overview of the scale and complexity of the data used in our experiments. For the Multi-Session Chat (MSC) dataset, we worked with 500 conversation sessions, each consisting of about 14 rounds, allowing us to evaluate the model's ability to handle multi-turn dialogues. A memory representation was independently built for each session, capturing dialogues as the conversation progressed. In the extended version, MSC-E, we expanded the original dataset by generating an additional 70 sessions, each containing over 200 rounds of dialogue. For these longer sessions, a memory representation was similarly built for each session, but the increased number of rounds presented a greater challenge in managing long-term information across interactions. The QuALITY dataset, focusing on document comprehension, contains around 230 documents with an average of 5,000 tokens each. For each document, an independent memory representation was built to facilitate reasoning across the entire document. Lastly, MultiHop RAG includes 609 articles and over 2,500 multi-hop questions. A unified memory representation was constructed across all the news articles, enabling the model to retrieve and integrate information from multiple documents when answering complex multi-hop questions."}, {"title": "A.3.1 EVALUATION METRICS", "content": "Predicted Response Generation: To assess retrieval performance, we configure the LLM to generate a response to the query based solely on the retrieved content using the following prompt:"}, {"title": "A.4 MSC-E DATA GENERATION", "content": "Building on the MSC dataset from Packer et al. (2023), we extend each conversation to 200 rounds using the following iterative process. A sliding window of the most recent 8 turns is maintained, and for each step, the next 2 rounds of dialogue are generated using the prompt below. This approach allows for a natural progression of conversation while keeping the context manageable for the model:"}, {"title": "A.5 MSC-E QUERY GENERATION", "content": "To generate queries and ground-truth responses for evaluating memory retrieval quality, we apply the following prompt to subsets of the conversation history. The generated questions will help assess how effectively the memory captures and retrieves information from various points in the dialogue:"}, {"title": "A.6 FURTHER EXPERIMENTAL RESULTS", "content": "A.6.1 MSC-E: ACCURACY VS POSITION OF EVIDENCE\nWe present accuracy results on the MSC-E dataset, focusing on how performance varies based on the position of supporting evidence within the dialogue. This analysis demonstrates the model's ability to effectively retrieve and utilize information from different points in extended conversations, highlighting its robustness in scenarios where a memory component is essential for maintaining context."}, {"title": "A.6.2 QUALITY: PERFORMANCE VS QUESTION DIFFICULTY", "content": "The experiment is conducted on the QuALITY benchmark to evaluate model performance on questions of varying difficulty. Both single-pass and retrieval-augmented methods are tested, focusing on the comparison between online and offline memory representation approaches. Llama-3.1 70B, which processes the entire document in a single pass, serves as the baseline, while RAPTOR (Sarthi et al., 2024), GraphRAG (Edge et al., 2024), MemoryStream (Park et al., 2023), and MemTree (ours) are assessed for their ability to manage document comprehension with memory retrieval. Offline methods (RAPTOR and GraphRAG) that need to be rebuilt from scratch to incorporate new information are shaded in gray."}, {"title": "A.6.3 MULTIHOP RAG: ACCURACY VS QUERY TYPE", "content": "We present results across three query types: (1) Inference queries, requiring reasoning from retrieved information; (2) Comparison queries, which involve evaluating and comparing evidence within the retrieved data; and (3) Temporal queries, analyzing time-related information to determine event sequences. Here, we compare online and offline methods (shaded in gray). Note that offline methods must be rebuilt from scratch to incorporate new information and cannot support real-time memory updates like MemTree."}, {"title": "A.7 THEORETICAL JUSTIFICATION OF MEMTREE VIA ONLINE HIERARCHICAL CLUSTERING", "content": "In this appendix, we provide a theoretical justification for MemTree by connecting it to online hierarchical clustering algorithms, specifically the Online Top-Down (OTD) algorithm proposed by Menon et al. (2019). We demonstrate that MemTree aligns with this algorithm, inheriting its theoretical properties, which ensures efficient and effective hierarchical memory management in large language models (LLMs)."}, {"title": "\u0391.7.1 MEMTREE'S APPROXIMATION TO THE MOSELEY-WANG REVENUE", "content": "MemTree achieves an approximation to the optimal Moseley-Wang revenue (Section A.7.2) under a data separation assumption (Assumption 1), ensuring a structured and theoretically sound hierarchy formation. The following theorem summarizes this guarantee."}, {"title": "\u0391.7.2 BACKGROUND: ONLINE HIERARCHICAL CLUSTERING AND MOSELEY-WANG REVENUE", "content": "Hierarchical clustering organizes data into a nested sequence of clusters, capturing relationships at various levels of granularity. To evaluate the quality of such hierarchies, we utilize objective functions like the Moseley-Wang revenue function (Moseley & Wang, 2017), which measures how well similar data points are grouped together."}, {"title": "A.7.3 ALIGNMENT OF MEMTREE WITH THE OTD ALGORITHM", "content": "Both MemTree and the OTD algorithm adopt a top-down approach for integrating new data, utilizing hierarchical traversal and similarity-based decision-making at each node. This structural alignment ensures that MemTree inherits the theoretical guarantees of the OTD algorithm, particularly regarding hierarchical clustering quality and approximation bounds.\n\nIn MemTree, decisions are based on cosine similarity between embeddings, analogous to the similarity comparisons in OTD. Additionally, the content aggregation mechanism in MemTree plays a crucial role in preserving or enhancing intra-cluster similarity, ensuring that the embeddings of parent nodes reflect the collective content of their child nodes. Below, we summarize the traversal and insertion mechanisms of both algorithms to highlight their similarities:"}, {"title": "A.7.4 DATA SEPARATION ASSUMPTION AND DEPTH-ADAPTIVE THRESHOLD", "content": "The OTD algorithm's approximation guarantee relies on the data satisfying a B-well-separated condition."}]}