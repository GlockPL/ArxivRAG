{"title": "HALLUCINATION DETOX:\nSENSITIVE NEURON DROPOUT (SEND) FOR\nLARGE LANGUAGE MODEL TRAINING", "authors": ["Shahrad Mohammadzadeh", "Juan David Guerra", "Marco Bonizzato", "Reihaneh Rabbany", "Golnoosh Farnadi"], "abstract": "As large language models (LLMs) become increasingly deployed across vari-\nous industries, concerns regarding their reliability, particularly due to halluci-\nnations-outputs that are factually inaccurate or irrelevant to user input-have\ngrown. Our research investigates the relationship between the training process\nand the emergence of hallucinations to address a key gap in existing research that\nfocuses primarily on post hoc detection and mitigation strategies. Using models\nfrom the Pythia suite (70M-12B parameters) and several hallucination detection\nmetrics, we analyze hallucination trends throughout training and explore LLM\ninternal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel\ntraining protocol designed to mitigate hallucinations by reducing variance during\ntraining. SeND achieves this by deterministically dropping neurons with signifi-\ncant variability on a dataset, referred to as Sensitive Neurons. In addition, we de-\nvelop an unsupervised hallucination detection metric, Efficient EigenScore (EES),\nwhich approximates the traditional EigenScore in 2x speed. This efficient metric is\nintegrated into our protocol, allowing SeND to be both computationally scalable\nand effective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual ac-\ncuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.", "sections": [{"title": "INTRODUCTION", "content": ""}, {"title": "1.1 MOTIVATION", "content": "In the age of increasingly sophisticated and ever growing Large Language Models (LLMs), we see\nthese models being utilized in every industry imaginable. However, concerns surrounding their re-\nliability and safety have escalated due to their exploitation and the errors that arise when they are\nused by the general public. One of these concerning areas discovered by the scientific community\nis the phenomenon of hallucinations - LLMs producing content that may not align with real-world\nfacts, the user's input, or training data it has seen in the past (Huang et al., 2023a). In our research,\nwe target a specific field of hallucinations called confabulations. Confabulations are a type of hallu-\ncination where the LLM generates different responses given the same or similar inputs. This can be\nharmful when the generations alter between correct and factually incorrect responses.\nPrevious research has largely focused on identifying and addressing hallucinations in large language\nmodels (LLMs), but the impact of the training process on hallucinations remains under-explored"}, {"title": "1.2 RELATED WORK", "content": "The majority of research on hallucinations in language models has focused on detecting and mit-\nigating this phenomenon rather than explaining its underlying causes. Recent techniques can be\ncategorized into two main approaches: those that rely on output text or model probabilities at in-\nference time (Manakul et al., 2023; Joshi et al., 2017; Li et al., 2023) and those that utilize internal\nrepresentations or hidden layers of the model (Su et al., 2024; Chen et al., 2024; Kossen et al., 2024).\nWhile the former has demonstrated effectiveness, the latter offers deeper insights but often comes\nwith computational trade-offs. Additionally, methods like Reinforcement Learning with Human\nFeedback (RLHF) have gained traction for enhancing model reliability (Yu et al., 2024). However,\nmany of these post-hoc solutions enhance factual accuracy by layering algorithms atop pre-trained\nmodels, which can be inefficient. Our work addresses this gap by focusing on the internal dynamics\nof the model that contribute to hallucinations."}, {"title": "2 OSCILLATORY BEHAVIOUR VALIDATION", "content": "The training epochs of a transformer model can be vital in understanding the dynamics of how the\nmodel learns, particularly when trained on an unsupervised loss with stochastic gradient. Beyond\nthe model's architecture and the data itself, numerous factors influence the learning process such as:\nWhether the loss function penalizes the learner for factual mistakes it makes or if it primarily tries\nto force the model to memorize the data.\nWhile our paper does not aim to address the broader debate on whether language models truly\nunderstand language or rely heavily on memorization, our analysis of training dynamics through\nmultiple epochs shows that reducing stochastic gradient loss does not necessarily correspond to\nreducing hallucinations, verifying the results Li et al. (2024) showed for the oscillatory behaviour\nof LLMs in hallucination during training. This highlights the need for further investigation into the\nrelationship between optimization and factual accuracy in LLMs."}, {"title": "2.1 METHODS", "content": "In our study, we utilize Eleuther AI's Pythia and LMEval tools (Biderman et al., 2023; Gao et al.,\n2024a), to examine the development and evolution of LLMs throughout the training process. Pythia\ncomprises a suite of 16 LLMs, all trained on public data in the same sequential order, with sizes\nranging from 70 million to 12 billion parameters, 8 of which we use for our experiments. We used 20\nequally spaced training checkpoints from the start to the finish for our analysis. We chose Pythia as it\nis based on GPT-Neo X (Black et al., 2022), which shares a similar foundational architecture to other\nstate-of-the-art language models. Pythia's comprehensive package of models makes it particularly\nsuitable for our analysis, allowing us to conduct a thorough examination of the development and\nevolution of LLMs throughout the training process.\nThese models are evaluated at each checkpoint on a variety of hallucination/fact-checking metrics.\nTo do this, we leverage the HuggingFace Hallucination Leaderboard (Hong et al., 2024), which\noffers comprehensive benchmarks for our experiments. There are two main components to our\nevaluations: Summarization and Self-consistency. For summarization, models are evaluated under\nthe XSum dataset Narayan et al. (2018) where the model is given a dataset of BBC news articles\nand must give summaries of each article. A higher Rouge1 score on XSum means the data is\naligning better with the provided reference summary. SelfCheckGPT (Manakul et al., 2023) is used"}, {"title": "2.2 HOW DO THE ESTABLISHED ITERATIVE TRAINING PROCESSES INFLUENCE LLM\nHALLUCINATIONS?", "content": "The analysis of hallucination oscillations, as shown in Figure 1, indicates a consistent pattern across\ndifferent models: oscillations persist throughout training from the initial to the final checkpoint. This\nfinding highlights the uncertainty of halting training solely based on the convergence of training loss\nto its minimum to minimize hallucination. For example, in the evaluation plot of XSUM with the\n12B model (1b), the optimal value for the hallucination metric occurs at the 60000th checkpoint,\ndespite the model's unsupervised loss converging by step 140000, leading to the termination of\ntraining. This evidence challenges the notion that optimizing solely for unsupervised loss in SGD\nguarantees learning the most accurate representation of the data. This observation is seen more\ndrastically in 1a, where model size has no effect on the performance of SelfCheckGPT. Instead, we\nobserve exaggerated oscillatory behaviour within self-consistency, meaning that model size is even\nless effective at tackling the issue of confabulations. One potential solution to mitigate this issue\ncould involve incorporating a regularization term into the unsupervised loss based on a hallucination\ndetection metric discussed in Section 4."}, {"title": "2.3 HOW DOES MODEL COMPLEXITY AFFECT THE EMERGENCE OF HALLUCINATIONS\nTHROUGHOUT TRAINING?", "content": "An analysis of hallucination detection metrics reveals a diminishing rate of improvement with in-\ncreased model scaling, particularly up to the 12B parameter size (Figure 1b). This suggests that\nbeyond a certain point, even though there is improvement in the hallucinations, larger models do not\nsignificantly reduce hallucinations, indicating that scaling alone is not sufficient for building robust\nmodels. Instead, more refined approaches are needed to address the underlying variability in model\nbehavior. For the following experiments, we focus on the Pythia 1B model."}, {"title": "3 INTERNAL TRAINING DYNAMICS", "content": "Following our investigation of the oscillatory behaviour in training, we look into the internal states of\nthe Pythia 1B model to see what information we are able to extract. In doing so, we define a series"}, {"title": "3.1 SENSITIVE NEURONS", "content": "To start our analysis of the internal states, we convert the activation matrix of the model into a\nsentence embedding vector 3.1 which turns an $R^{n,m}$ activation matrix into a sentence embedding\nvector $a_k$ for input k with dimension $R^n$. Given its demonstrated success in hallucination detection\nby Su et al. (2024), we employ this sentence embedding extraction approach."}, {"title": "Definition 3.1 (Sentence Embedding Vector).", "content": "The Sentence Embedding Vector is a way to convert\nthe large $R^{n,m}$ activation matrix into a smaller, easier to manage vector with dimension $R^n$.\n$e_k = \\frac{1}{m} \\sum_{i=1}^{m} (H_i^{N-1} + \\tilde{H_i^{N-1}})$"}, {"title": "Definition 3.2 (Net Change Formula).", "content": "Let e denote the embedding of data point x at neuron i of\nthe contextual embedding after checkpoint/epoch t. Then we define the net change formula as\n$\\Delta_i = |e_i^t - e_i^{t-1}|$"}, {"title": "Definition 3.3 (Sensitive Neurons).", "content": "Indices of the contextual embedding for data point x which\nexhibit the highest net change across the last C checkpoints of training, indicating overall high\nvariability during this period. This is calculated by\n$V_i = Var(e_i) \\sum_{t=T-C+1}^{T} \\Delta e_i^t$\nwhere $V_i$ is the total variability during the last C checkpoints and the most Sensitive Neurons are\n$s = arg\\underset{1<i<N}{max} \\{V_i | V_i \\ge percentile(V, 100 - k)\\}$\nwhere N is the embedding vector size and k is the desired percentile threshold.\nThe above definition of Sensitive Neurons is then applied to LLM hallucinations through analyses of\nthe EigenScores. In their paper, Chen et al. (2024) define a new metric for detecting confabulations,\na subclass of hallucinations. They do this by calculating an EigenScore 3.4 based on determinant\ncalculations from multiple outputs of an LLM with a high-temperature setting (temperature set to\n0.5) to encourage the LLM to produce a variety of different outputs. They propose that if an LLM\nis set to hallucinate on that output, the generated texts will show higher semantic variability and\nproduce a higher EigenScore. This method achieves SOTA performance and is unsupervised as\nit only relies on the representations learned by the model. In the forthcoming sections, we will\nanalyze the correlation between the EigenScore of data points during training checkpoints and the\nmost Sensitive Neurons associated with them."}, {"title": "3.2 SENSITIVE NEURON IMPACT ON EIGENSCORES", "content": "To assess the correlation between Sensitive Neurons and other neurons in the embedding matrix\nof 10 generated outputs at a specific checkpoint, we conduct experiments aimed to determine if the\npresence of Sensitive Neurons indicates higher uncertainty and a greater likelihood of hallucinations.\nWe evaluate the Sensitive Neuron effect on the HELM dataset (Su et al., 2024), which includes\noutputs and internal states from six open-source LLMs based on inference over 50,000 Wikipedia\narticles, with human annotators labeling passages as factual or hallucinatory. To assess the impact\nof Sensitive Neurons on hallucination, we adapt the EigenScore method by applying it to sentence\nembeddings from the penultimate layer of EleutherAI's Pythia 1B model, focusing on checkpoints\nbetween 133,000 and 143,000 training steps, where embeddings are more stable and the model has\na higher degree of language understanding compared to initial checkpoints. We perform sensitive\nneuron dropout, removing the top 10% of Sensitive Neurons at each checkpoint, and compare the\nresults to a baseline where 10% of neurons are randomly dropped. Additionally, we analyze the\nimpact on hallucination-prone inputs versus non-hallucination-prone inputs to determine if Sensitive\nNeurons play a critical role during hallucination, without negatively affecting correct outputs."}, {"title": "3.2.1 WHAT IS THE EFFECT OF SENSITIVE NEURONS ON HALLUCINATION METRICS?", "content": "Since a reduction in the EigenScore metric can be used as a proxy to show the reduction in likeli-\nhood of hallucination and an increase in the uncertainty of the model about its response, we keep\nusing this metric in our investigations. We are able to show through our comparison of the baseline\nrandom neuron dropout and Sensitive Neuron dropout that Sensitive Neurons significantly reduce\nthe EigenScore metric and in turn, reduce the possibility of a confabulation implying\ntheir highly important role in determining model's certainty. Not only do we observe this in halluci-\nnatory outputs, we also observe a smaller reduction in EigenScore when applying this technique to\ncorrectly answered queries . This result indicates that our methodology has a significant\neffect on the uncertainty shown by an LLM. We observe that looking at the internal states of the\nmodel is an effective way to eliminate confabulating text generation in various model sizes."}, {"title": "3.3 EFFICIENT EIGENSCORE APPROXIMATION", "content": "To address the computational complexity of EigenScore calculations, particularly as LLM hidden\nlayer sizes increase and more generations occur at higher temperatures, we develop an approxi-\nmation method. This approximation, detailed in Algorithm 1, leverages the properties of Spectral\nDensity or Density of States (DOS) to estimate EigenScore without explicitly constructing the co-\nvariance matrix. While this approximation provides a general overview of EigenScore trends, it\nis important to note that the output scales differ: EigenScore ranges from whereas the ap-\nproximation, referred to as Efficient EigenScore (EES), outputs values between [-1,1]. Since the\nspectrum of the matrix is altered to make EES computable and operates on its own scale, EES can\nbe seen as a standalone metric for hallucination detection."}, {"title": "3.4 HOW DOES EFFICIENT EIGENSCORE APPROXIMATION SCALE COMPARED TO REGULAR\nEIGENSCORE?", "content": "The efficiency of EES is compared to that of the regular EigenScore calculation with respect to\nscaling matrix sizes. These tests are imperative to the application of our training protocol on LLMs\nin Section 4, as with increasing model size, we get ever larger matrix sizes to decompose for the\nEigenScore calculation. To elucidate these scaling effects, we conduct a grid search over two im-\nportant parameters: Matrix size and Moments used for EES calculation . The\ndifference between EES time in comparison to EigenScore when increasing the number of columns\nand rows is visualized using a moments value of 20. It is evident that EES provides a\nsignificant computational advantage when increasing the number of columns or rows. Remarkably,\nat matrix size R1e8, EES nearly halves the computation time of regular EigenScore calculation at\naround 4 seconds whereas EigenScore takes approximately 7 seconds to calculate. We can then de-\nduce that given a good enough approximation, EES provides a significant reduction in computational\ncomplexity as model and matrix size increase."}, {"title": "4 SENSITIVE NEURON DROPOUT (SEND)", "content": "Building on the findings from Section 3.2, and aiming to reduce variance in the factual uncertainty\nof LLMs during training, this section introduces SeND, an efficient and transferable framework for\ntraining LLMs. SeND integrates the EES method discussed in Section 3.3 to enhance computational\nefficiency while addressing variance in sensitive neuron behavior. By identifying sensitive neurons,\nwhich contribute to the oscillatory behavior of hallucinations during training, SeND deterministi-"}, {"title": "Algorithm 2 Sensitive Neuron Dropout", "content": "Require: $\\epsilon$ denotes the acceptable range for loss convergence and $\\delta$ denotes acceptable range for\nconfabulation (EES) convergence\n1: Initialize dataset with a% training $Y_t$ and (100 \u2013 a)% tracking $Y_s$\n2: while $Loss > \\epsilon$ and $EES > \\delta$ do\n\u25b7 Refer to Algorithm 1 for EES\n3:  for t in T do \u25b7 T denotes the number of epochs per sensitive neuron calculation\n4:   Train LLM for one epoch over $Y_t$\n5:   Record penultimate layer representations $R_t$ of LLM over $Y$\n6:  end for\n7:  for $t\\in T-1$ do\n8:   Calculate variability $V_i$ between $R_t$ to $R_{t+1}$ \u25b7 Refer to Equation 3\n9:  end for\n10:  Take average Variability $V_{avg} = \\frac{1}{N} + \\Sigma_{N. i=0}$\n11:  s = $K$ most sensitive neurons $\\in V_{avg}$ \u25b7 Refer to Equation 4\n12:  Drop neurons s for next T epochs\n13: end while"}, {"title": "4.1 SEND EXPERIMENT SETUP", "content": "To evaluate SeND, we use Eleuther AI's Pythia 1B model, continuing its training on specific datasets\nrather than restarting pretraining to maintain efficiency. We continue the training of the fully trained\nmodel on two datasets: HELM, consisting of Wikipedia text (Su et al., 2024), and MedHALT, a\nmedical dataset emulating real-world entrance exam questions (Pal et al., 2023). Due to the impor-\ntance of factual accuracy in the medical domain, MedHALT was chosen to assess SeND's impact on\nhallucination mitigation. Both datasets were tested in two sizes: 200 and 2,000 points (referred to as\n2k). SeND implements the EigenScore reduction technique from Section 3.2 and detects Sensitive\nNeurons using a 3-epoch window on a specialized hallucination tracking dataset. Sensitive Neurons\nin the penultimate layer are identified based on their variability across epochs and are deterministi-\ncally dropped for the subsequent 3 training epochs. This dropout process is repeated at each 3-epoch\ninterval until the training loss converges, effectively mitigating hallucination tendencies and refin-\ning the model. While we acknowledge that the number of subsequent epochs for dropout could be\nadjusted, we limit our approach to 3 for ease of computation."}, {"title": "4.2 PERFORMANCE OF SEND ON PYTHIA 1B", "content": "The results of training Pythia 1B on HELM and MedHALT 200 are illustrated in Figure 4. To vali-\ndate that the EES method accurately approximates the EigenScore metric; we compare the model's\nprogress during training (up to loss convergence) and assess whether the resulting graphs are sim-\nilar. These results are detailed in Appendix B.6. Upon confirming that EES provides a reliable\napproximation of the EigenScore, we proceed to compare the performance of Pythia 1B trained us-\ning standard training with SeND on HELM and MedHALT 2k. In the case of training\non HELM with the regular protocol, we observe results consistent with previous findings: while the\nmodel successfully reduces loss, it fails to optimize for hallucination, as evidenced by the increasing\nEES metric (green line in Figure 4a). Conversely, training with SeND reveals a consistent trajectory\ntoward reducing both EES and loss, as depicted by the blue line.\nTo assess the effectiveness of SeND in comparison to other state-of-the-art factuality metrics, we\nemploy the FactScore metric from Min et al. (2023), which quantifies the factual accuracy of con-\ntent generated by large language models (LLMs). The fact-checking is conducted using the HELM\ndataset on the final trained models (1B SeND and 1B Normal Training). A higher FactScore indi-\ncates improved factual precision. When evaluated on 100 data points from the HELM dataset, the\n1B SEND model achieves a FactScore of 0.07, whereas the 1B Normal Training model attains 0.05,\ndemonstrating a 40% improvement in factual accuracy, even during test time. This highlights the ef-\nficacy of SeND in enhancing the factual certainty of the model. Note that SeND is not a replacement\nfor post-hoc methods like RAG (Gao et al., 2024b), but rather to complement them."}, {"title": "5 CONCLUSION & FUTURE WORK", "content": "In this paper, we presented a protocol to refine the current training methods of LLMs based on exper-\niments showing oscillatory behaviour with respect to hallucinations throughout training (Figure 1).\nTo do this we used the internal states of LLMs, specifically the penultimate layer activations during\ninference on a specialized dataset. We present an initial method of reducing hallucinations based on\nthe principles of EigenScore metrics introduced by Chen et al. (2024). We showed empirically that\nour Sensitive Neuron detection method significantly reduces the EigenScore on inference of LLMs\nthroughout various stages of training (Figure 2). Following the success of the Sensitive Neuron\nmethod, we moved on to the application of a hallucination reduction method on training. We show\nthrough finetuning that we are able to fix the oscillatory behaviour initially seen throughout training\nand reduce the EES of finetuned models as shown in Figure 4 by modifying the internal mechanics\nof training with SEnsitive Neuron Dropout. At test time we achieve a 40% increase in FactScore\nperformance, verifying that SeND provides a substantial improvement to current training protocols.\nTo further advance our work, we plan to scale SeND to larger datasets and models, as current exper-\niments were limited by compute constraints with larger LLMs. Demonstrating SeND's effectiveness\non larger open-source models like Meta's LLaMA 3.1 (Dubey et al., 2024) will provide crucial ev-\nidence for organizations developing state-of-the-art LLMs to incorporate SeND into their training\nprotocols, ultimately improving model safety. Given that SeND targets variance reduction during\ntraining, we anticipate even greater gains on larger LLMs, where the higher inherent variance may\namplify the regularization effect and lead to more significant improvements."}, {"title": "A ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "A.1 DRASTIC EMBEDDING CHANGES LEADING TO SENSITIVE NEURONS", "content": "Looking at internal states of the model allows getting a deeper understanding of the dynamics that\ncould be leading to the oscillatory behaviour seen in Figure 1. To do this, we record the net change\n(Definition 3.2) between checkpoints of the penultimate layer where one checkpoint would be the\ncorrect answer and the next would hallucinate. This net change with respect to various different\ninput texts is plotted in Figure 5. It can be observed that there were specific embedding activations\nthat experienced drastically more change relative to the rest of the embeddings. This is the main\nsource of motivation to further define Sensitive Neurons (Definition 3.3)."}, {"title": "B EFFICIENT EIGENSCORE (EES) DERIVATION", "content": ""}, {"title": "B.1 BACKGROUND: CHEBYSHEV POLYNOMIALS", "content": "Chebyshev polynomials are a sequence of orthogonal polynomials in the interval [-1,1] \u2013 orthog-\nonality property shown in equation 8 \u2013 that are widely used in numerical analysis, approximation\ntheory, and other areas of applied mathematics. In this work, we are mainly concerned with the\nChebyshev polynomials of the first kind with the recurrence relation shown in equation 9. Note that\nthis recurrence could also be applied to matrices. Any function f defined in the interval [-1,1] can\nbe approximated with the Chebyshev expansion as shown in 10."}, {"title": "Background background", "content": "$\\int_{-1}^{1} \\frac{2}{(1 + \\delta_{0n})\\pi \\sqrt{1 - x^2}} T_m(x)T_n(x)dx = \\delta_{mn},$\nwhere $\\delta_{mn} = \\begin{cases} 1 & \\text{if } m = n, \\\\ 0 & \\text{if } m \\neq n, \\end{cases}$\n$T_0(x) = 1,$\n$T_1(x) = x,$\n$T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x),$ for $n \\ge 1.$\n$f(x) = \\sum_{n=0}^{\\infty} C_nT_n(x),$\nwhere $C_n = \\frac{2}{\\pi}\\int_{-1}^{1} \\frac{f(x)T_n(x)}{\\sqrt{1 - x^2}} dx$ for $n > 0,$\n$C_0 = \\frac{1}{\\pi} \\int_{-1}^{1} \\frac{f(x)}{\\sqrt{1-x^2}} dx.$"}, {"title": "B.2 BACKGROUND: DOS AND KPM", "content": "Let H be a symmetric matrix $H \\in R^{N\\times N}$ with an eigendecomposition $H = QAQ^T$, where $\\Lambda =\ndiag(\\lambda_1,..., \\lambda_N)$ and $Q = [q_1,\\cdots, q_N]$ is orthogonal. The spectral density induced by H is the\ngeneralized function:\n$\\mu(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} \\delta(\\lambda - \\lambda_i),$"}, {"title": "B.3 STOCHASTIC TRACE ESTIMATION ON EMBEDDING MATRIX", "content": "We are interested in computing the $d_m$ term of DOS relying solely on the embedding matrix E\ntherefore we need to rewrite $d_m$ as follows:\n$d_m = \\frac{1}{K N_z} \\sum_{i=0}^{N_z} z_j^T (E^T E) z_i$\nwhere $T_m$ can be computed using the Chebyshev polynomials of matrix $C = E^T E$.\n$T_0(E^T E)z_j = Iz_j = Z_j,$\n$T_1(E^T E)z_j = E^T Ez_j,$\n$T_{m+1}(E^T E)z_j = 2 E^T E T_m(E^T E)z_j \u2013 T_{m-1} (E^T E)z_j$\nEach term can be computed with a matrix-vector multiplication."}, {"title": "B.4 EES INTEGRAL CALCULATION", "content": "Given the orthogonality of the Chebyshev polynomials, we can simplify the integral mentioned\nin proposition 1. To approximate the EigenScore, we will expand log(\u5165) in terms of Chebyshev\npolynomials and use their orthogonality to simplify the integral.\nExpanding and Integrating\nTo approximate the integral:\n$\\frac{1}{K}\\int log(\\lambda) \\mu(\\lambda) d\\lambda$"}, {"title": "Background background", "content": "$\\mu(\\lambda) \\approx \\sum_{m=0}^{M} d_m T_m(\\lambda)$\nwhere:\n$T_m^* = w(\\lambda)T_m(\\lambda) = \\frac{2}{\\pi\\sqrt{1 - \\lambda^2}} \\frac{1}{(1 + \\delta_{0m})}T_m(\\lambda)$\nDistribute log(\u5165) in the integral:\n$\\frac{1}{K} \\int (\\log(\\lambda) \\sum_{m=0}^{M} d_m T_m(\\lambda)) d\\lambda = \\frac{1}{K} \\sum_{m=0}^{M} d_m \\int log(\\lambda) T_m(\\lambda) d\\lambda$\nEvaluate the Integral Using Orthogonality:\nTo simplify the integral,\n$\\int log(\\lambda) T_m(\\lambda) d\\lambda$\nFirst, express log(\u5165) as a series of Chebyshev polynomials:\n$log(\\lambda) = \\sum_{m=0}^{\\infty} c_m T_m(\\lambda)$\nThen:\n$\\int_{0}^{1} log(\\lambda) T_m(\\lambda) d\\lambda = \\int_{0}^{1} (\\sum_{m=0}^{\\infty} c_m T_m(\\lambda) T_m(\\lambda) )d\\lambda$\nNote: The lower bound of the integral is 0 as the matrix is defined in the spectrum [0, 1].\nUsing the orthogonality, we get:\n$c_m = \\int log(\\lambda) T_m(\\lambda) d\\lambda$\nSo the integral simplifies to:\n$\\frac{1}{K} \\sum_{m=0}^{\\infty} d_m c_m$"}, {"title": "B.5 EFFICIENT EIGENSCORE MOMENTS", "content": "Figure 6 presents the effect of using different moment values as the number of matrix rows increases\nwith respect to time. This is an important hyperparameter to tune as increasing the number of\nmoments on EES correlates to having a more accurate and representative approximation of the\nEigenScore. We observe that as moments in EES increase, the time to calculate EES increases.\nFrom this result, we conclude that selecting a moment value of under 50 would provide a balanced\ntrade-off between accuracy and calculation time."}, {"title": "B.6 EIGENSCORE AND EES TRAINING TRAJECTORIES", "content": "To demonstrate that our EigenScore approximation method, EES, is a good metric, we record the\nprogress of Pythia 1B finetuning on the HELM dataset using both EigenScore and EES hallucination\nperformance metrics (Figure 7). Albeit a different scale and window, the trajectories, magnitude and\nshape of the graphs are nearly identical while EES takes only 4 minutes to calculate and Eigen-\nScore takes approximately 8, an astounding 2x increase in compute speed. These results show that\nour metric closely resembles the target metric while greatly reducing the required computational\nresources."}]}