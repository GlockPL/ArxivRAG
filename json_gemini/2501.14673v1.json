{"title": "State Space Models for Extractive Summarization in Low Resource Scenarios", "authors": ["Nisrine Ait Khayi"], "abstract": "Extractive summarization involves selecting the most relevant sentences from a text. Recently, researchers have focused on advancing methods to improve state-of-the-art results in low-resource settings. Motivated by these advancements, we propose the MPoincareSum method. This method applies the Mamba state space model to generate the semantics of reviews and sentences, which are then concatenated. A Poincare compression is used to select the most meaningful features, followed by the application of a linear layer to predict sentence relevance based on the corresponding review. Finally, we paraphrase the relevant sentences to create the final summary. To evaluate the effectiveness of MPoincareSum, we conducted extensive experiments using the Amazon review dataset. The performance of the method was assessed using ROUGE scores. The experimental results demonstrate that MPoincareSum outperforms several existing approaches in the literature.", "sections": [{"title": "Introduction", "content": "Extractive text summarization is a fundamental Natural Language Processing (NLP) task that involves selecting the most salient sentences from a text while preserving its meaning. This process typically consists of two sequential steps (Zhou et al., 2018). The first step involves predicting the relevance of sentences using a deep learning model (Gu et al., 2022; Xie et al., 2022; Jia et al., 2020; Narayan et al., 2020). The second step ranks the sentences based on their relevance scores and selects the top-ranked ones.\nHowever, state-of-the-art models for extractive summarization are often data-hungry, with their complexity increasing in proportion to the large number of parameters. This makes extractive summarization particularly challenging in low- resource scenarios, such as those involving limited annotated datasets or computational resources. These limitations can negatively impact the language representation of sentences and reduce model performance. To address these challenges, several techniques have been proposed, including data augmentation, distant supervision (Liang et al., 2024), embeddings using pre-trained language models (Bajaj et al., 2021), domain adaptation of language models (Yu et al., 2021), and Meta-Learning (Chen et al.,"}, {"title": null, "content": "2021). Biljon et al. (2020) demonstrated that low- medium pre-trained language models with fewer parameters perform better on few-shot tasks.\nOne issue with transformer models is their inefficiency when dealing with long sequences. To alleviate this, Gu et al. (2024) proposed a State Space Model (SSM) with linear scaling in sequence length, which has shown improved performance in zero-shot tasks compared to attention-based models. Akbik et al. (2018, 2019b) demonstrated that powerful representations can be generated by concatenating high-resource embeddings from a general domain with low-resource embeddings from a target domain. Lange et al. (2020b) proved that domain- invariant representations can be generated by training embeddings on diverse domains using an adversarial discriminator to distinguish between embedding spaces. Meta-learning algorithms (Dou et al., 2019) aim to learn effective initializations for fine-tuning across various tasks with minimal training data.\nIn this paper, we frame the task of extractive summarization for the Amazon Review dataset (He and McAuley, 2016) in a low-resource setting as a binary classification of sentences and reviews, followed by paraphrasing the extractive summary to align with human summarization. Motivated by prior successes, our contributions are as follows:\n2 The semantics of sentences are generated using the State Space Model Mamba (Gu et al., 2024), which employs a selective scan algorithm to compress information, a HiPPPO initialization matrix to capture long-range dependencies, and a hardware-aware algorithm to accelerate computations.\n3 The generated semantics are further compressed using spectral clustering (Macgregor et al., 2023) and Poincar\u00e9 distance (Klein & Hilbert, 1999). The distances between embeddings and centroids are used as features.\n4 We explore parameter-efficient fine-tuning (PEFT) through LORA (Low-Rank Adaptation) (Hu et al., 2022), which integrates low-rank decomposition matrices into the model layers."}, {"title": "Related Work", "content": "State-of-the-art models in the extractive summarization research area heavily rely on the quality of sentence semantics. However, limited training datasets or computational resources can lead to a decline in performance. Recently, low- resource extractive summarization has gained increased attention. Yuan and colleagues (2023) applied disentangled representation learning to extractive summarization, separating context and pattern information (e.g., sentence position, n- gram tokens) to enhance generalization in low- resource settings. They proposed two loss functions for encoding and disentangling sentence representations into context and pattern components: 1) an adversarial objective, and 2) a mutual information minimization objective.\nTo reduce the need for large amounts of training data, Tang and colleagues (2023) introduced ParaSum, which reformulates extractive summarization as paraphrasing. This aligns the task with the self-supervised next-sentence prediction objective of pre-trained language models. Experimental results demonstrated the effectiveness of this approach in low-resource settings. Brazinskas and colleagues (2020) proposed a few-shot framework for abstractive opinion summarization, enabling successful cross-domain adaptation. In the first stage, a conditional transformer is trained to generate product reviews conditioned on summary-related review properties. In the second stage, they fine- tune a module to predict these summary properties. This framework outperforms many extractive and abstractive summarization models.\nIn this work, we build upon these advancements in enhancing sentence semantics to improve extractive summarization performance in low- resource settings. We propose using the State Space Mamba model, combined with Poincar\u00e9 compression and Spectral clustering, to generate sentence features. ur approach is evaluated using the Amazon review dataset."}, {"title": "Method", "content": "In this section, we describe the key components of our proposed model for classifying sentences as relevant or irrelevant for the final summary. First, we outline the main modules of the Mamba block:\n1) Recurrent State Space Model (SSM) through discretization, 2) HiPPO initialization on matrix A to capture long-range dependencies, 3) Selective"}, {"title": "Task Formulation", "content": "We formulate the extractive summarization as a binary classification of a sentence of being relevant or not, given its review. Then, we paraphrase the relevant sentences using BART, sequence-to-sequence model (Lewis et al., 2020).\n\u2022 Given a review of N sentences [S1, S2..., SN], we pass it, with each sentence, to the State Space Mamba model. Then, we collect the mean of the last hidden state as the representation of the review H, and the representation of the sentence Hs. Then, we concatenate them as Hrs and we apply Spectral clustering using Poincare distance to generate the review and the sentence features Frs."}, {"title": null, "content": "$\\F_{r s i}= d_{poincare}(H_{r s i}, C_{j})$ (1)\nwhere $d_{poincare}$ is the Poincare distance between the ith vector of the $H_{rs}$ embedding and the jth centroid.\n\u2022 Then, we apply a batch normalization and linear layer to predict the class label y for the sentence si:\n$y = w.H + b$ (2)\nwhere w is the weight vector, b is the bias and y is the final output.\n\u2022 After extracting the relevant sentences [s1, s2,..., sm], we paraphrase them using BART.\n$sum = BART(s_1) \u2295 ...\u2295 BART(s_m)$ (3)"}, {"title": "Selective Space Model", "content": "The most recent advances in extractive summarization are based on Transformer-based models. However, a major flaw of Transformers is the slow inference due to the self-attention computations when the sequence length of the input increases. To overcome this drawback, Mamba SSM (Gu et al., 2023) architecture parallelizes training as Transformers and performs inference that scales linearly with the sequence length. This can be an effective model for low-resource scenarios. Mamba SSM has the following properties:"}, {"title": "Recurrent SSM", "content": "A recurrent technique is used to compute a discretized version of the Mamba model. At each time step, the model calculates how the current input (Bxk) influences the previous state (Ahk-1) and then calculates the predicted output (Chk).\n$\\bar{A} = exp(A \\Delta)$\n(4)\n$\\bar{B} = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I)\u00b7 \\Delta B$ (5)\n$h_k = \\bar{A} h_{k-1} + \\bar{B} x_k$ (6)\n$Y_k = C h_k$ (7)\nA, B, and C are the parameters of the model.\nThis technique has the advantage of fast inference and the disadvantage of slow training."}, {"title": "HiPPO initialization on matrix A", "content": "The idea behind HiPPO (High order Polynomial Projection Operators) (Gu et"}, {"title": "Poincare Compression", "content": "The semantic vectors used by the Mamba model are dense and high-dimensional, which can slow training and reduce the model's generalizability in low-resource settings. To address this challenge, various compression techniques have been proposed. In this work, we aim to reduce the dimensionality of the Mamba-based embeddings and select the most meaningful vectors using Spectral clustering and Poincar\u00e9 distance, as outlined in the following algorithm."}, {"title": "Algorithm Poincare Compression", "content": "Input: embeddings {ei}1 and dimensionality d\ncentroids {c;}j=1\nfor i = 1,2,..., m do\nfor j = 1,2,..., n do\nhi,j = dpoincare(ei, Cj)\nend for\nend for\nOutput: Poincare-Spectral embeddings {h}i=m,j=n\nGiven Mamba-based embeddings, we apply Spectral clustering. Then, we compute the Poincare distance between each embedding vector and the centroids to generate the features. Spectral clustering and Poincare distance are described in the next sections."}, {"title": "Spectral Clustering", "content": "Spectral clustering (Ng et al.,2001) is robust for high-dimensional data as it uses the distance on a graph, which is more meaningful compared to Euclidean distance. Moreover, it is simple, fast, and effective.\nGiven a graph Gwith n vertices and k clusters, the spectral clustering algorithm consists of these two steps:\n1. Embed the vertices of G into Rk according to k eigenvectors of the graph Laplacian matrix\n2. Apply the k-means clustering algorithm to partition the vertices into k clusters."}, {"title": "Poincare Distance", "content": "Nickel et al (2017) have demonstrated that the embeddings in Poincare space outperform the embeddings in Euclidean space as they account for the hierarchal structure of the text. Motivated by this success, we embedded the Mamba-based semantics in the Poincare space.\nThe Poincare distance dpoincare(a, b) between two points a and b is given by this formula:\n$d_{poincare} (a, b) = 1 + \\frac{2|a \u2212 b|^2}{(1 \u2013 |a|^2)(1 \u2013 |b|^2)}$ (8)"}, {"title": "LORA (Low-rank adaptation)", "content": "Many research findings (Jukic et al.,2023) affirm the superiority of Parameter Efficient Fine Tuning (PEFT) over Full-Fine Tuning (FFT) in low- resource settings. Inspired by this, we apply LORA (Hu et al., 2021) to our proposed model to improve the performance of our extractive summarization in a low-resource setting. The LoRA technique freezes the pre-trained model weights and incorporates trainable rank decomposition matrices into the model's layers. This results in reducing the trainable parameters for the downstream tasks."}, {"title": "Paraphrasing", "content": "After selecting the relevant sentences in the review, we paraphrase them using the BART model (Lewis et al., 2020). It is a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by:"}, {"title": "Experiments and Results", "content": "To evaluate the strength of our proposed method, we have conducted several experiments using the Amazon review dataset."}, {"title": "Amazon Review Dataset", "content": "We used the Amazon customer review dataset (He and McAuley, 2016), which includes text reviews, summaries, user information, and product details. For our study, we sampled 136 product reviews for training and 73 for testing. The text reviews were split into individual sentences, which were then annotated as relevant or irrelevant. The annotation process was based on ROUGE scores and the semantic similarity between each sentence and its corresponding review.."}, {"title": "Experiments Settings", "content": "We performed our experiments using a P5000 GPU and an A4000 GPU, both equipped with 30 GB of RAM. The proposed model was implemented using HuggingFace's library, with the \"mamba-130m-hf\" version of the Mamba model and tokenizer. The maximum sequence length for the Mamba model was set to 128. We used the AdamW optimizer (Loshchilov et al., 2017) with a learning rate of 2e-5 and a weight decay of 0.5. A one-cycle learning rate scheduler (Smith et al., 2015) was employed to adjust the learning rate during training. To address overfitting in our low-resource setting, we applied a dropout rate of 0.5 and used BatchNorm1d for regularization. For LoRA configuration, we set lora_alpha to 32 and lora_dropout to 0.1.\nThe dataset was preprocessed by converting text to lowercase, removing URLs, digits, and punctuation, handling missing data via imputation, and normalizing the data before applying Spectral clustering. We evaluated our model using ROUGE-1, ROUGE-2, and ROUGE-L metrics. Each experiment was repeated multiple times, and we selected the best- performing model."}, {"title": "Summarization of Results and Analysis", "content": "Table 3 presents the empirical results of our proposed method on the Amazon review dataset, comparing it with several existing text summarization models from the literature. We report the ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) scores. As shown in the table, our method, MPoincareSum, outperforms several models, including Copycat (Brazinskas et al., 2020), MeanSum (Chu and Liu, 2019), and LexRank (Erkan and Radev, 2004), in terms of R2 and RL scores. This highlights the effectiveness of State Space models, Poincar\u00e9 compression, and Parameter-Efficient Fine-Tuning for extractive summarization in low-resource settings.\nHowever, FewSum (Bra\u017einskas et al., 2020) outperforms MPoincareSum in terms of R1 by 0.158. This can be attributed to their use of fluent and informative review summaries generated by a conditional transformer language model. In contrast, Amazon reviews are often unstructured and contain informal language, which may explain the performance gap."}, {"title": "Ablation Study", "content": "We conducted an ablation study to evaluate the impact of different components in our model. First, we experimented without the Poincar\u00e9 compression module, meaning the semantics of sentences and reviews were encoded using the SSM Mamba model without dimensionality reduction. The results in Table 4 show that removing the Poincar\u00e9 compression module leads to a significant drop in performance, indicating that compressing the SSM Mamba embeddings enhances the extractive summarization performance in low resource settings."}, {"title": "Conclusion", "content": "We presented the MPoincareSum method for extractive summarization in low-resource settings, applied to the Amazon review dataset. Our proposed approach involves extracting semantic features from reviews and sentences using a State Space Mamba model. These features are then concatenated, and we apply Spectral clustering to select the most meaningful ones. Next, we compute the Poincar\u00e9 distance between the embedding vectors and the centroids. A linear layer is then used to predict the relevance of each sentence. To reduce the model's parameters, we incorporate the LoRA technique. Finally, we paraphrase the relevant sentences using the BART sequence-to-sequence model to include them in the final summary.\nTo evaluate the performance of MPoincareSum, we conducted several experiments. The empirical results demonstrate the effectiveness of State Space models and Poincar\u00e9 compression for extractive summarization in low-resource settings, yielding competitive ROUGE scores.\nLooking ahead, we plan to further improve ROUGE scores in low-resource extractive summarization by experimenting with Mixture of Experts (MoE) models, which offer enhanced training efficiency without compromising inference performance"}, {"title": "Ethics Statement", "content": "Our research work uses Amazon reviews, a publicly available dataset. Our annotation process does not harm the intellectual property of the original authors of the dataset. The scientific artifacts used are available for research with licenses, including ROUGE, Hugging Face, and PEFT. Their use does not violate their intended use. Our task is a well-defined NLP task which is text summarization. Thus, there are no potential risks of this work."}]}