{"title": "CL-MFAP: A CONTRASTIVE LEARNING-BASED MULTIMODAL FOUNDATION MODEL FOR MOLECULAR PROPERTY PREDICTION AND ANTIBIOTIC SCREENING", "authors": ["Gen Zhou", "Sugitha Janarthanan", "Yutong Lu", "Pingzhao Hu"], "abstract": "Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers have turned to machine learning techniques to streamline the prediction and development of novel antibiotic compounds. While foundation models have shown promise in antibiotic discovery, current mainstream efforts still fall short of fully leveraging the potential of multimodal molecular data. Recent studies suggest that contrastive learning frameworks utilizing multimodal data exhibit excellent performance in representation learning across various domains. Building upon this, we introduce CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP) using three types of molecular data. This model employs 1.6 million bioactive molecules with drug-like properties from the ChEMBL dataset to jointly pretrain three encoders: (1) a transformer-based encoder with rotary position embedding for processing SMILES strings; (2) another transformer-based encoder, incorporating a novel bi-level routing attention mechanism to handle molecular graph representations; and (3) a Morgan fingerprint encoder using a multilayer perceptron, to achieve the contrastive learning purpose. The CL-MFAP outperforms baseline models in antibiotic property prediction by effectively utilizing different molecular modalities and demonstrates superior domain-specific performance when fine-tuned for antibiotic-related property prediction tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Bacteria play a pivotal role in a diverse array of diseases within the human body, serving as either the primary cause or a contributing factor. A promising and sometimes sole treatment for these diseases is antibiotics, a specialized class of drugs designed to target pathogenic bacteria. Despite advancements, a lack of antibiotics for many pathogenic bacteria persists, and antibiotic resistance allows bacteria to survive once effective treatments. Consequently, there is a pressing demand for the continual development of antibiotics. However, traditional antibiotic discovery faces two major issues: 1) it is extremely costly and 2) it is very time-consuming. Artificial Intelligence (AI) and Machine Learning (ML) methods can combat these issues and thus, have been employed over the past couple of years to aid in antibiotic discovery for a wide range of conditions. Deep learning (DL) tools including convolutional, recurrent, and graph neural networks have been leveraged to explore high-dimensional data and design compounds with desired antibiotic properties (Cesaro et al., 2023).\nLarge Language Models (LLMs) have increasingly stood out in recent years due to their exceptional performance, garnering widespread attention. As such, they have been implemented and fine-tuned to target pathogenic bacteria. For an LLM dedicated to the domain of antibiotic discovery, utilizing an extensive general molecular dataset for model training may not be a computationally cost-effective choice. By employing domain-specific training, the model can be taught to learn the unique characteristics, patterns, and nuances relevant to the field. Gu et al. (2021) support this assertion, arguing that for fields like biomedicine, which have a large amount of unlabeled text, pretraining a model from scratch yields greater benefits than continual pretraining of a general-domain LLM.\nContrastive learning, an effective method for utilizing large amounts of unlabeled data, has made significant progress in the field of ML in recent years. For antibiotic-related property prediction, contrastive learning significantly enhances model performance. Rather than relying on limited labeled molecular property data, this method leverages the vast amount of unlabeled molecular data available, helping identify patterns that contribute to a compound's properties. The resulting molecular representations are thus more robust as they include patterns that may be missed by traditional supervised learning approaches. This leads to more accurate predictions, better generalization to novel chemical spaces, and ultimately increases the success rate of identifying potential antibiotic candidates.\nIn this study, we introduce a novel approach to streamline antibiotic discovery by leveraging a contrastive learning framework with multimodal data to train a domain-specific LLM. We propose CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP). CL-MFAP integrates a transformer-based encoder with rotary position embedding for SMILES strings, a transformer-based encoder using a novel Bi-Level Routing Attention (BRA) mechanism for molecular graphs, and a multilayer perceptron for Morgan fingerprint embeddings. This model is pretrained on 1.6 million bioactive molecules with drug-like properties from the Chemical Database of Bioactive Molecules (ChEMBL) (Gaulton et al., 2011), a smaller, domain-specific dataset. Our comprehensive evaluation demonstrates that CL-MFAP outperforms baseline models trained on large-scale general datasets for antibiotic property prediction, while also exhibiting superior domain-specific performance when fine-tuned on targeted downstream tasks."}, {"title": "2 RELATED WORK", "content": "Transformers. Among the current mainstream LLMs, the most representative architecture is the transformer. A transformer is a DL architecture consisting of encoder and/or decoder components built around multi-head attention mechanisms. Different models in the transformer family use these components distinctively: Bidirectional Encoder Representations from Transformers (BERT) series based solely on the encoder (Devlin et al., 2018), the Generative Pre-trained Transformer (GPT) series based solely on the decoder (Radford & Narasimhan, 2018), and the Text-to-Text Transfer Transformer (T5) series utilizing both the encoder and decoder (Raffel et al., 2020). The architecture's core features are self-attention computation and positional encoding (Vaswani et al., 2017). The former is used to capture the semantic dependencies between the target word and the context and then determine its importance, while the latter understands the syntax and sequence information of the word by recording its position in the sequence. LLMs based on the transformer architecture have been widely proven to exhibit superior performance in capturing sequence semantics.\nLLMs for Molecular Property Prediction. LLMs have recently gained popularity in molecular property prediction due to their enhanced success. MoLFormer is a successful unsupervised transformer-based LLM that accurately captures sufficient chemical and structural information to predict a diverse range of chemical properties (Ross et al., 2022). ChemBERTa is a stack of bidirectional encoders that uses representations from transformers for molecular property prediction (Chithrananda et al., 2020) and is fine-tuned to better predict drug-target interactions (Kang et al., 2022). MolBERT is a self-supervised model, consisting of the bidirectional attention mechanism-based BERT architecture (Fabian et al., 2020). It is one of the most efficient pre-trained models for molecular property prediction that can be easily generalized to different molecular property prediction tasks via fine-tuning. All these examples of successful LLMs take in the structure of compounds in Simplified Molecular Input Line Entry System (SMILES) format for predictions.\nContrastive Learning Models for Molecular Representation Learning. As the field of drug development continues to advance, the integration and utilization of multimodal data have become essential for improving the performance of molecular property prediction LLMs. Contrastive learn-"}, {"title": "3 PROPOSED APPROACH", "content": "We designed a multimodal contrastive learning model based on molecular SMILES, Morgan fingerprints, and molecular graphs to comprehensively capture different chemical characteristics. The input data is SMILES representations, which describe the linear form of a molecule, including information about its composition, bond types, and functional groups, used to depict the overall connectivity of the molecular structure. From the SMILES representation, Morgan fingerprints and molecular graphs are constructed. Morgan fingerprints provide a quantitative representation of the molecule's features, encoding its structure as a high-dimensional binary vector that captures the presence and distribution of various substructures and functional groups. Specifically, a radius size of 2 was determined through ablation studies detailed in Appendix A.1 (Table A1), and a 2048-bit vector was chosen as it is large enough to minimize hash collisions (where different structural features map to the same bit) while being computationally efficient. The graph representation of a molecule describes its topology through nodes (atoms) and edges (chemical bonds), including details about the atom types, bond characteristics, and overall connectivity. Ablation studies were performed to validate the inclusion of these three modalities, detailed in Appendix A.2 (Table A2). Altogether, the model receives a widespread in-depth representation of each compound, allowing it to learn the specificities and patterns of the compounds that influence their antibiotic-related properties.\nFigure 1 illustrates the overall architecture of our model, which learns the three molecular feature modalities mentioned above through different embedding pathways. First, the model employs a transformer-based graph encoder with a novel bidirectional relation aggregation (BRA) mechanism"}, {"title": "3.1.1 ROTARY POSITIONAL EMBEDDING", "content": "Rotary Positional Embedding (RoPE) is an improved positional encoding method used in transformer models (Su et al., 2024). The rotation transformation effectively integrates positional information into each token and helps the model capture dependencies between distant tokens. Together, this preserves the relative position relationships between elements and improves prediction accuracy. This method is particularly suitable for processing molecular data with complex structural dependencies, as it improves the model's ability to understand sequential structural relationships. In the two-dimensional case, the formula for implementing rotary positional encoding through complex multiplication is as follows:\n$g(x_m, x_n, m - n) = Re [(W_q x_m) (W_k x_n)^*e^{i(m-1)\\theta}]$"}, {"title": "3.1.2 BI-LEVEL ROUTING ATTENTION", "content": "The Bi-level Routing Attention (BRA) mechanism is crucial, as it partitions the attention mechanism into two phases: an initial focus on global relationships followed by a more detailed scrutiny of local specifics. In conventional applications within computer vision, the BRA mechanism first identifies critical areas within an image and then focuses on local details. For instance, in an image featuring a dog, the model would initially identify the most prominent features, such as the dog's head, across the entire image, and then subsequently focus on local details such as the eyes and nose within the defined window.\nIn molecular graphs, diverse structural features are exhibited by different molecules, and these features significantly influence the functional performance of the molecules. For antibiotic molecules, complex cyclic structures represent a typical characteristic, the importance of which often surpasses other local structures in medicinal functionality, making precise understanding by the model crucial. In our model, through the Window-to-Window Attention mechanism of BRA, the model efficiently identifies and focuses on key structures and functional groups within the molecular graph that are central to functionality, such as cyclic structures. Concurrently, for peripheral structures or less likely"}, {"title": "Window-to-Window Level Routing.", "content": "This mechanism efficiently computes attention across regions of a feature map while considering local context. Beginning with a 2D feature map, $X \\in R^{H \\times W \\times C}$ a linear transformation is applied to create three tensors: Q (query), K (key), and V (value), as shown in Equation 4.\n$Q = XW_q, K = XW_k, V = XW_v$\nwhere $W_q, W_k$, and $W_v$ are the learnable projection weights, each of size $R^{C \\times C}$.\nTo perform window-to-window level routing, the feature map is divided into S x S non-overlapping windows, each containing feature vectors, resulting in reshaped Q', K' and V'. The window size S is set to 7, based on ablation studies explained in Appendix A.1 (Table A3). Within each window, the $Q', K'$, and $V'$ tensors are used to compute the average, resulting in $Q^w$ and $K^w$, which are the window-level representations for each non-overlapping window. These are then used to calculate the window-to-window score matrix (containing window-to-window attention scores) as shown in Equation 5.\n$A^w = Q^w (K^w)^T$\nIn the score matrix, each row contains the indexes of the top-k windows that are most relevant to the corresponding window."}, {"title": "Pixel-to-Pixel Level Attention.", "content": "For window I, its top-k relevant windows are scattered across the feature map. To gather these windows together, we use the following equation to collect $K^\\vartheta$ and $V^\\vartheta$:\n$K^\\vartheta = gather(K, I^w), V^\\vartheta = gather(V, I^w)$\n$K^\\vartheta$ and $V^\\vartheta$ represent the collected Key and Value tensors containing features from the top-k windows relevant to the current window I, respectively. For a given pixel j within a window I, the pixel will attend to all pixels in the top-k windows most relevant to window I. This ensures a fine-grained attention mechanism, allowing the model to refine feature representations at the individual pixel level."}, {"title": "3.1.3 MULTIMODAL CONTRASTIVE LEARNING", "content": "The advantage of a multimodal model lies in its ability to integrate information from different modalities, thus obtaining a more comprehensive understanding of molecular structure that enhances the robustness and generalization of the model. Contrastive learning is an approach that enhances feature learning by pulling similar pairs closer together while pushing dissimilar pairs apart. This approach significantly improves representation quality as it facilitates learning similarities and associations across different modalities. It also aids in the limited data issue commonly associated with antibiotic property discovery by leveraging the unlabeled molecular data available."}, {"title": "4 EXPERIMENTS", "content": "Environment. All implementations were conducted on the PyTorch platform using an NVIDIA A100 GPU. All models were trained using a learning rate of 1e-4, over 20 epochs, with batch size 8 and 4 worker processes. The Adam optimizer and gradient clipping were also applied during training, limiting the gradient norm to 1.0. For the bi-level routing attention, the window size is 7, the number of top k windows is 4, and there are 16 pixels per window and 8 attention heads.\nPre-trained CL-Models. To analyze the contribution of each component along the molecular graph embedding path-graph transformer encoder (GTE) and the newly introduced BRA\u2014as well as to test whether combining this GTE with a message-passing neural network (MPNN) can further enhance the model's ability to capture global information, we pre-trained five models within the overall framework of multimodal contrastive learning which differ in structural configurations along the graph embedding path. Aside from CL-MFAP, the other four models are labeled as Contrastive Learning Baseline 1-4 (CL-BL1-4). The labels and structures of all the models pre-trained under the multimodal contrastive learning framework are presented in Table 1."}, {"title": "Model Size.", "content": "Moreover, we measured the size of our models in terms of Params and FLOPs to further evaluate their performance and cost efficiency. Params refer to the number of trainable parameters in a model. This is directly related to the structure of the model, representing each learnable weight, including weights and biases in different layers. As such, it serves as a measure of the model's complexity and storage requirements (Han et al., 2024). FLOPs refer to the number of floating-point operations performed during a single forward pass of the model. This metric measures the computational complexity and cost of the model, providing insight beyond just the number of parameters. FLOPs are closely related to the model's inference speed and the computational resources required for its operation (Han et al., 2024)."}, {"title": "4.2 DOWNSTREAM PROPERTY PREDICTIONS", "content": "Datasets. Six datasets were used for downstream property prediction: MIC activity against E. coli (E. coli MIC) dataset curated from COADD database (Desselle et al., 2017), MIC activity against H. influenzae (H. influenzae MIC) dataset curated from ChEMBL database (Gaulton et al., 2011), BACE (Wu et al., 2018), Blood-Brain Barrier Penetration (BBBP) (Wu et al., 2018), Parallel Artificial Membrane Permeability Assay (PAMPA) (Siramshetty et al., 2021), and Bioavailability (Ma et al., 2008). All datasets were divided into 80% - 10% - 10% for training, testing and validation, respectively. More details can be found in Appendix A.1.\nBaseline Models. We selected MoLFormer, ChemBERTa-2, MolBERT, MolCLR, and FP-GNN as baselines to evaluate the performance of CL-MFAP. MoLFormer, a transformer-based model, was trained on 1.1 billion molecules from ZINC and PubChem databases (Ross et al., 2022). ChemBERTa-2, a BERT-based LLM with 12 encoders, was trained on 77 million PubChem compounds (Ahmad et al., 2022). MolBERT, another BERT-based model with 12 encoders, was trained on a smaller dataset of 1.6 million ChEMBL molecules (Fabian et al., 2020). MolCLR employs graph neural networks and contrastive learning with molecular augmentations, trained on 10 million PubChem SMILES (Wang et al., 2022). Finally, FP-GNN is a multimodal framework that combines molecular graphs and fingerprints for property prediction (Cai et al., 2022).\nMean Reciprocal Rank. To more intuitively evaluate the overall performance of each model across all downstream tasks, we employed the mean reciprocal rank (MRR) method, a statistical approach that synthesizes the rankings of all models on various downstream tasks (Wu et al., 2011). This method assigns a corresponding score to each model, with higher scores indicating superior overall performance. We first recorded the rank of each model's ROC-AUC metric in comparison to the other models for each task and then used the ranks to calculate the model's MRR value using the following equation:\n$MRR = \\frac{1}{Q} \\sum_{i=1}^{Q} \\frac{1}{rank_i}$"}, {"title": "4.3 RESULTS", "content": "The performance of CL-MFAP on downstream property prediction tasks was compared against all baselines. Using Area under the Receiver Operating Characteristic Curve (ROC-AUC) as the evaluation metric, the experimental results are summarized in Table 2. Notably, CL-MFAP outperforms all other baseline models on the E. coli MIC dataset (ROC-AUC: 0.854\u00b10.037), which is particularly relevant for antibiotic drug discovery as it predicts the antibacterial activity of compounds against E. coli. In addition, it performs second best on the H. influenzae MIC dataset (ROC-AUC:0.874\u00b10.015), with negligible difference from the best performing model, MoLFormer (ROC-AUC:0.876\u00b10.017). We noted similar performance for pre-trained chemical language models (CL-MFAP, MoLFormer, MolBERT, and ChemBERTa-2) that outperform models without pretraining (MolCLR and FP-GNN). Together, these results show the ability of CL-MFAP to exceed in antibacterial activity prediction, regardless of sample size. Thus, our model can also predict antibacterial activity for less studied bacterial strains with less data. On the remaining datasets, our model demonstrates consistently strong performance, ranking among the top 2 or 3 models, unlike"}, {"title": "4.4 ABLATION STUDIES", "content": "Overall Performance Ranking of CL-based Models. We compared the performance of five pre-trained CL models to verify the effectiveness of different components in the graph embedding path. We evaluated the performance of these pre-trained CL models on the downstream tasks using ROC-AUC (Table A5) and then ranked the performance of each model across all tasks based on these findings. As shown in Table 3, CL-MFAP outperforms the model variations in 5 out of 6 downstream tasks. To further assess model performance and cost efficiency, an MRR analysis of the overall model rankings was performed. The model size, represented via Params (Figure 3A) and FLOPS (Figure 3B), was plotted against the MRR score. CL-MFAP's top-left position in Figure 3A highlights its superior performance with fewer parameters.\nAblation study on the BRA. We conducted an ablation analysis on the contribution of BRA by comparing CL-MFAP vs. CL-BL3, and CL-BL1 vs. CL-BL2. The former compares the impact of BRA in the absence of MPNN, while the latter compares the effect of BRA when MPNN and GTE are used together. In both cases, models with BRA consistently outperform their counterparts (Table 3, Figure 3). Therefore, BRA plays a significant role in enhancing model performance.\nAblation study on the MPNN. The value of MPNN was also evaluated. As we initially hypothesized that introducing MPNN could help further capture comprehensive information (Cai et al., 2023), we introduced an MPNN path running parallel to GTE in the graph embedding process."}, {"title": "5 CONCLUSION", "content": "In this work, we present CL-MFAP, a novel multimodal contrastive learning framework. The model combines and compares molecular information from three modalities - SMILES, molecular graphs and fingerprints - to efficiently learn representations of molecules that improve its performance in predicting antibiotic-related properties. We also, for the first time, incorporate the BRA mechanism to enhance the quality of molecular representation learning. Experimental results demonstrate that CL-MFAP achieves outstanding performance in predicting drug molecule properties. In the future, we aim to integrate this model with other cross-domain potential modules and further refine its multimodal contrastive learning algorithm to enhance its generalization capabilities.\nAll code can be found at https://github.com/CLMFAP/CLMFAP."}, {"title": "A APPENDIX", "content": "We performed an additional ablation study to investigate the effect of the Morgan fingerprint radius size on CL-MFAP's predictive capabilities. CL-MFAP was tested with five fingerprint radius sizes (0, 1, 2, 3, 4, and 5) 1. As shown in Table A1, a radius of size 2 has the best overall performance, achieving the highest results in 5 of the 6 downstream datasets, proving that it is the best radius size for CL-MFAP."}, {"title": "Ablation study on Morgan Fingerprint Radius.", "content": "We removed each of the three data modalities from CL-MFAP individually and assessed its performance on downstream property prediction tasks to determine their individual impact. As shown in Table A2, We observe that removing either the SMILES or the molecular fingerprints results in a certain degree of performance decline. This suggests that both data modalities contribute approximately equally to the overall model performance, with the impact of removing Fingerprints being slightly greater than removing SMILES. However, when we remove the molecular graph modality, the model performance experiences a significant drop. This indicates that the primary contributor to our model's performance is the molecular graph, processed through the GTE integrated with the BRA mechanism, which aligns well with our assumptions."}, {"title": "Ablation study on Data Modalities.", "content": "We tested several different window sizes in our CL-MFAP to study their impact and determine the most optimal choice. Six distinct window sizes (2, 3, 5, 7, 9, and 11) were evaluated for CL-MFAP, and their performance on downstream property prediction tasks was assessed. As shown in Table A3, a window size of 7, representing a moderate configuration, achieved the best performance in 4 out of 5 tasks. In contrast, performance tends to decline when the window size is either too large or too small. This was predicted as when the window size is too small, the BRA mechanism is confined to focusing on highly local regions, overly emphasizing"}, {"title": "Ablation study on Window Size.", "content": "We performed an ablation study to investigate whether pretraining on the larger ChEMBL dataset improves model performance. CL-MFAP with and without ChEMBL pretraining was trained/finetuned on all downstream property prediction datasets. As shown in Table A4, in 5 of 6 tasks, dropping the pretraining slightly weakens model performance, although not significantly. This indicates that while pretraining enhances model performance and represents the ideal scenario, our algorithm and novel methodology are still able to achieve excellent results even without pretraining. In scenarios where cost-effectiveness is prioritized in training resource consumption, the model can handle the intended use cases to a similar extent without pretraining."}, {"title": "Additional ABLATION STUDIES", "content": "The choice of the downstream property prediction datasets was based on the availability of good quality data and biological relevance to antibiotic properties. The most relevant antibiotic property is antibacterial activity, and thus the E. coli and H. influenzae MIC datasets were curated from COADD (Desselle et al., 2017) ChEMBL (Gaulton et al., 2011), respectively, to analyze CL-MFAP's ability to predict antibacterial activity. The other datasets were obtained from trusted databases (MoleculeNet (Wu et al., 2018) and Therapeutics Data Commons (Huang et al., 2021)) and are commonly used in ML models to benchmark model performance in drug discovery."}, {"title": "A.2 DOWNSTREAM PROPERTY PREDICTION DATASETS", "content": "This dataset describes compound ability to inhibit Escherichia coli (E. coli). Obtained from COADD (Desselle et al., 2017), each compound has an associated Minimum Inhibitory Concentration (MIC) value, which represents the antibacterial activity against E. coli. The compounds were binarized as active (1) if MIC < 8 ug/mL and inactive (0) if MIC > 8 ug/mL. Size: ~100,000 compounds."}, {"title": "E. coli MIC Dataset.", "content": "This dataset describes the ability of compounds to inhibit Haemophilus influenzae (H. influenzae). Obtained from ChEMBL (Gaulton et al., 2011), each compound has an associated MIC value, which represents the antibacterial activity against H. influenzae. The compounds were binarized as active (1) if MIC < 4 ug/mL and inactive (0) if MIC > 4 ug/mL. Size: 3,341 compounds."}, {"title": "H. influenzae MIC Dataset.", "content": "This dataset from MoleculeNet (Wu et al., 2018) assesses compounds' binding ability for a set of inhibitors for \u1e9e-secretase 1. The compound is labeled active (1) if it is a potential inhibitor of B-secretase 1, 0 otherwise. Size: 1,512 compounds."}, {"title": "BACE Dataset.", "content": "This MoleculeNet (Wu et al., 2018) dataset assesses compounds' capacity to traverse the blood-brain barrier. The compound is labeled \"p\" if it can penetrate the barrier and \u201cnp\u201d if it cannot. Size: 2,038 compounds."}, {"title": "Blood-Brain Barrier Penetration (BBBP) Dataset.", "content": "This dataset evaluates compounds' permeability across the cell membrane based on the PAMPA assay. The compound is labeled 1 if it has high permeability, and 0 if it has low permeability. Size: NCATS set \u2013 2,035 compounds; Approved drugs set - 142 drugs (Siramshetty et al., 2021)."}, {"title": "Parallel Artificial Membrane Permeability Assay (PAMPA) Dataset.", "content": "This dataset contains the oral bioavailability of different drugs, defined as \"the rate and extent to which the active ingredient or active moiety is absorbed from a drug product and becomes available at the site of action\" (Chen et al., 2001). Size: 640 compounds (Ma et al., 2008)."}, {"title": "Bioavailability.", "content": "We primarily applied the Representation-Property Relationship Analysis (RePRA) method to evaluate CL-MFAP against its model variations and all baselines (MoLFormer, MolBERT, ChemBERTa-2, MolCLR, and FP-GNN). RePRA, a novel method introduced by Zhang et al. in 2023, draws inspiration from the concepts of Activity Cliffs (ACs) and Scaffold Hopping (SH) (Zhang et al., 2024). It assesses the quality of molecular representations extracted by pre-trained models and visualizes the relationship between these representations and molecular properties. RePRA generalizes ACs and SH from the structure-activity context to the representation-property context, defining an ideal relationship between molecular representations and their properties as a boundary condition. This condition drives the ACs and SH regions to a borderline state without observed data points, allowing for the calculation of ACs and SH thresholds based on these constraints. By using the detected ACs and SH, RePRA generates a map showing the distances between pairs of representations and molecular properties, thereby evaluating the quality of the representations."}, {"title": "A.3 EVALUATION OF CL-BASED MODELS ON DOWNSTREAM PROPERTY PREDICTION DATASETS", "content": "The RePRA map serves as a visualization tool for assessing the quality of molecular representations produced by a pre-trained model. Its x-axis denotes the similarity between the representations of a pair of target molecules, while the y-axis indicates the difference between the properties of this pair of molecules. Typically, a RePRA map is partitioned into four main regions, with shadowed ACs and SH zones that should ideally be avoided by the data points on the map."}, {"title": "RePRA Map.", "content": "This region is delineated by scenarios in which a pair of molecules showcases markedly different properties beyond the y-axis threshold of ACs, while their representations exhibit a noticeable similarity surpassing the x-axis threshold of ACs. A predominance of data points clustered in this area indicates that the model's representations are too similar to adequately capture the diverse range of molecular properties, thus indicating a limited ability of the pre-trained model to differentiate between molecular properties."}, {"title": "Activity Cliffs.", "content": "This region is characterized by instances where a pair of molecules exhibit fairly similar properties beyond the y-axis threshold of SH, yet their representations demonstrate a significant disparity surpassing the x-axis threshold of SH. A prevalence of data points clustered in this zone suggests that the model tends to generate highly various representations that correspond to a narrow range of similar molecular properties, indicative of subpar representation quality from the pre-trained model."}, {"title": "Scaffold Hopping.", "content": "Two evaluation scores, average deviation ($SAD$) and improvement rate ($SIR$), are derived from the RePRA Map to assess the performance of the models. $SAD$ quantifies the average deviation by considering the ratio of data points situated in ACs and SH, adjusting for noise points in the remaining ideal regions; a lower $SAD$ value indicates better performance. On the other hand, $SIR$ is computed by comparing the numbers of data points in ACs and SH between a standard baseline (ECFP) and the pre-trained model under evaluation. Again, a lower $SIR$ value signifies superior performance."}, {"title": "Evaluation Scores.", "content": "In addition to the RePRA map, a visualization of cosine similarities is also presented to analyze the distribution of similarities using CosineSim as a metric between pairs of molecules. This visualization aids in identifying if there are common substructures shared among most molecular pairs."}, {"title": "Visualization of Cosine Similarities.", "content": "For the RePRA measurement, we employed the Estimated SOLubility (ESOL) dataset, which includes the measured log solubility (mol/L) for 902 compounds (Niwa et al., 2009). The \"measured log solubility in mols per liter\" data from the ESOL dataset was utilized as labels for molecular properties. Initially, the distance between each pair of labels was computed, followed by calculating the distance between each pair of logits. These labels and logits were then collectively inputted into the RePRA algorithm to generate the map."}, {"title": "Datasets.", "content": "All models were evaluated using the RePRA test, with the scores presented in Table A6. For the $SAD$ parameter, it can be observed that the CL-MFAP model has the lowest result, indicating fewer noise data points with detected ACs and SH, which suggests a better representation-property relationship. For the $SIR$ parameter, the CL-MFAP model also has the lowest score, demonstrating an improvement in representation quality compared to the traditional ECFP method and indicating that CL-MFAP generates better representations compared to the other models. Since lower $SAD$ and $SIR$ scores jointly indicate superior molecular embedding and representation quality, it is unsurprising that the CL-MFAP model, enhanced by the BRA, excels in this test. Notably, all CL models utilizing GTE outperformed the baseline models, highlighting the inherent advantage of contrastive learning frameworks trained on multimodal data in effectively learning molecular representations."}, {"title": "Results.", "content": "Escherichia coli (E. coli) is a gram-negative bacterium commonly found in the gut microbiome of humans that is usually harmless. However, it can become pathogenic under certain conditions or pathogenic E. coli can be ingested and cause a variety of issues in humans. The issues can range from traveler's diarrhea and pneumonia (Mueller & Tainter, 2024) to playing a part in Inflammatory Bowel Disease (Martinez-Medina & Garcia-Gil, 2014). Although antibiotics exist for E. coli, many strains develop antibiotic resistance, thus showcasing the need for new antibiotic compounds effective against E. coli.\nIn this case study, we employ CL-MFAP to identify novel antibiotic compounds that are highly likely to be effective against E. coli."}, {"title": "A.4 REPRA - EVALUATION OF PRE-TRAINED MODELS", "content": "CL-MFAP was finetuned on Minimum Inhibitory Concentration (MIC) data against E.coli (Anti-E. coli Activity) described in Appendix A.2. Obtained from the COADD database, each compound has its associated MIC value, which represents the antibacterial activity, against E. coli. The compounds were binarized as active (1) if MIC < 8 ug/mL and inactive (0) if MIC > 8 ug/mL."}, {"title": "A.5 Escherichia coli CASE STUDY", "content": "Based on the finetuned CL-MFAP model, virtual screening was performed using the ZINC database. ZINC is a free database containing over 230 million commercially available compounds in ready-to-dock, 3D formats (Irwin et al., 2020). Due to its massive size, we used the ZINCK250k dataset (Basu, 2021), a subset of 250,000 compounds from ZINC. From this, 9389 compounds were identified with predicted activity 1 (predicted to be effective at inhibiting E. coli) with 100% probability and were chosen for further property testing."}, {"title": "Model Training.", "content": "For the 9389 compounds identified via virtual screening, their pharmacokinetic and ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) properties were predicted using ADMET-SAR (Yang et al., 2018). These properties allow us to identify compounds that have necessary molecular properties and are most likely to perform well as antibiotics. From this, we filtered to only include compounds that follow the Lipinski Rule of 5 (molecular weight < 500 Da, logP < 5, number of hydrogen bond acceptors"}, {"title": "Virtual Screening.", "content": "< 10, and number of hydrogen bond donors \u2264 5) with a maximum of 1 violation. In addition, their topological surface area"}]}