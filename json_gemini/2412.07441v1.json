{"title": "Reconstructing Deep Neural Networks: Unleashing the Optimization Potential of Natural Gradient Descent", "authors": ["Weihua Liu", "Said Boumaraf", "Jianwu Li", "Chaochao Lin", "Xiabi Liu", "Lijuan Niu", "Naoufel Werghi"], "abstract": "Natural gradient descent (NGD) is a powerful optimization technique for machine learning, but the computational complexity of the inverse Fisher information matrix limits its application in training deep neural networks. To overcome this challenge, we propose a novel optimization method for training deep neural networks called structured natural gradient descent (SNGD). Theoretically, we demonstrate that optimizing the original network using NGD is equivalent to using fast gradient descent (GD) to optimize the reconstructed network with a structural transformation of the parameter matrix. Thereby, we decompose the calculation of the global Fisher information matrix into the efficient computation of local Fisher matrices via constructing local Fisher layers in the reconstructed network to speed up the training. Experimental results on various deep networks and datasets demonstrate that SNGD achieves faster convergence speed than NGD while retaining comparable solutions. Furthermore, our method outperforms traditional GDs in terms of efficiency and effectiveness. Thus, our proposed method has the potential to significantly improve the scalability and efficiency of NGD in deep learning applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks have achieved unprecedented success in a myriad of complex tasks, establishing themselves as a cornerstone in the advancement of machine learning and artificial intelligence. Despite their success, the optimization of these networks remains a formidable challenge, critically influencing their performance and applicability. Traditional Gradient Descent (GD) methods, while being straightforward and computationally efficient, primarily leverage first-order information [1], often resulting in slow convergence rates and sub-optimal performance in the presence of highly non-linear and non-convex objective functions [2]. In contrast, Natural Gradient Descent (NGD) [3] emerges as a robust second-order optimization technique, offering faster and more effective convergence by considering the underlying geometry of the parameter space through the Fisher information matrix. However, the computational intensity required to calculate and invert this matrix poses significant challenges, rendering NGD impractical for training deep neural networks at scale.\nTo surmount these obstacles, we propose Structured Natural Gradient Descent (SNGD), a novel optimization framework that preserves the advantages of NGD while significantly reducing its computational burden. The cornerstone of SNGD is its ability to deconstruct the global Fisher information matrix into manageable, local Fisher matrices, thereby simplifying the overall computation and making it more amenable to deep network training. This methodological innovation paves the way for exploiting the optimization potential of NGD across a wider array of deep learning applications without being hindered by prohibitive computational costs. The theoretical innovation underpinning SNGD is articulated through a comprehensive paradigm that establishes a novel equivalence between the optimization of the original network via NGD and the application of a more computationally efficient Gradient Descent (GD) on a strategically transformed network. This transformation is achieved by reconstructing the deep network in such a way that aligns the traditional gradient descent's update direction with that of the natural gradient, albeit with significantly reduced computational complexity and storage demands. Central to this transformative process is the introduction of the local Fisher layer- a novel architectural element ingeniously designed to encapsulate the local curvature information of the loss function space. By integrating these local Fisher layers into the network, SNGD imposes a structured constraint on the transformation of model parameters, thereby ensuring that each gradient update is informed by a precise understanding of the local geometric properties of the loss landscape. This results in stable and fast gradient updates, leading to faster parameter convergence speed and better performance compared to conventional first-order optimizers. We evaluate the performance of SNGD across a spectrum of deep neural network architectures, including multi-layer Perceptron (MLP), convolutional neural networks (CNN), long short-term memory networks (LSTM), and residual networks (ResNet), leveraging diverse datasets like MNIST, CIFAR-10, ImageNet, and Penn Treebank. Experimental results demonstrate that"}, {"title": "II. RELATED WORKS", "content": "In this section, we focus on NGD and address numerical challenges in matrix root/inverse computations by drawing inspiration from the normalization method. Our work sheds new light on NGD and regularization methods."}, {"title": "A. Natural gradient descent", "content": "NGD was first proposed by Amari et al. [3] in 1998. The natural gradient is the direction of the fastest decrease of the error defined in the parameter space in Riemannian space. Amari's experiments show that NGD has a faster convergence rate than stochastic gradient descent (SGD). However, implementing NGD in deep neural networks is usually challenging due to the large corresponding Fisher matrix.\nTo simplify the calculation of natural gradients, Bastian et al. [4] considered every two layers of the neural network as a model, and the resulting information matrix is called the block information matrix. Martens et al. [5] proposed an effective method to implement the natural gradient algorithm called the approximate curvature of Kronecker coefficients (K-FAC), an approximate natural gradient algorithm for optimizing the cascading structure of neural networks. Zhang et al. [6] theoretically analyzed the convergence rate of NGD on nonlinear neural networks based on square error loss, determined two prerequisites for effective convergence after random initialization, and proved that although K-FAC can converge to global optimization as long as it satisfies the prerequisites. George et al. [7] proposed a novel approximate natural gradient method that is less computationally expensive than K-FAC. Their method uses diagonal variance on the basis of Kronecker eigenvectors. The characteristic of George and other methods is that diagonal variance is based on Kronecker eigenvectors. Bernacchia et al. [8] derived the exact expression of the natural gradient in deep linear networks, which exhibits morbid curvature similar to that in the nonlinear case. Given the complexity of deep neural networks, Sun et al. [9] decomposed a neural network model into a series of local subsystems, based on which they defined the relative Fisher information metric, reducing the complexity of the optimization calculation of the whole network model. Lin et al. [10] further addressed the computational difficulty of the Fisher matrix by using local parameter coordinates based on covariances of Gaussian and Wishart-based distributions. Yang et al. [11] designed randomized techniques for networks and low-rank approximations to reduce the computational cost and memory requirement of computing the natural gradient.\nOn a broader perspective, the computational challenge of Natural Gradient Descent (NGD) arises from its reliance on second-order statistics/derivation, a fundamental issue extending across various deep learning models. For instance, in Graph Neural Networks, Wang et al. [12] introduced bilinear mapping and attentional second-order pooling to address this problem. The work of [13] proposed an adaptive second-order tanning by adapting the multiple optimal learning factor MOLF [14] to develop efficient automated training for standalone MLP classifiers. In his substantial analysis of convex optimization, Lee et al. [15] tackled the bottleneck associated with the inversion matrix concerning second-order derivatives. They employed weighted analytic center frameworks and incorporated regularization terms into the objective function to constrain the amplitude of the Hessian."}, {"title": "B. Normalization method", "content": "Normalization techniques have been widely used in deep neural networks to improve training efficiency and generalization performance. In this subsection, we discuss the normalization method used in our work, which builds upon the idea that the effect of the natural gradient in the parameter space is similar to the whitening of the signal [16]. Our approach is to add additional layers to re-express the parameter space of each layer so that the optimization can perform gradient descent in Riemann space. We also provide an overview of other popular normalization methods as follows.\nLocal response normalization is proposed in AlexNet [17], while batch normalization (BN) [18] has become a popular choice for normalizing data along the batch dimension. However, BN has no significant effect when the batch dimension is small, and to avoid using it, new normalization methods have been proposed. layer Normalization (LN) [19] normalizes along the channel dimension, while instance normalization (IN) [20] performs a BN-like operation, but only for each sample. Weight normalization (WN) [21] recommends standardizing the parameter weights of each layer rather than the features. Group normalization (GN) [22] divides the channels into groups, not affected by the batch size, and spectrum normalization [23] is a new weight standardization technique used to stably train generative adversarial networks. Spatial adaptive normalization [24] is another approach that has advantages in terms of visual fidelity and alignment with the input layout when the semantic input layout is given. To improve the effectiveness of normalization, Singh et al. [25] proposed a novel Feature Wise Normalization approach that improves the generalization and superiority of the normalization method. Similarly, Fan et al. [26] proposed an adaptive standardization and rescaling normalization to learn standardization and rescaling statistics from data of different domains, which improves the model's generalization performance."}, {"title": "III. STRUCTURED NATURAL GRADIENT DESCENT (SNGD)", "content": "The structured natural gradient method (SNGD) is a technique that approaches the power of natural gradient descent (NGD) by reconstructing the original deep network and using traditional gradient descent (GD) optimization, which can make gradient updates more stable and faster compared to traditional first order GD. As shown in Fig. 1, our method utilizes decomposition to divide deep networks into subsystems hierarchically containing local Fisher layers and then use traditional GD to update the parameters of the reconstructed network. In this section, we first describe the relationship between NGD and GD, followed by the proposal of our SNGD and a description of how it can be applied to deep networks."}, {"title": "A. Background", "content": "NGD [3] is a second-order optimization method for training statistical models. The update rules for the model parameter vector w are as follows:\n\\begin{equation}\nw^{(k+1)}\\leftarrow w^{(k)} - \\alpha \\cdot G^{-1} \\cdot \\frac{\\delta l}{\\delta w^{(k)}}\n\\end{equation}\nwhere l is the loss function, $\\frac{\\delta l}{\\delta w^{(k)}}$ is the gradient of the loss function calculated in k-th iteration, a is the learning rate, G is the Fisher matrix, which can be regarded as the Riemann metric on the statistical manifold, and $G^{-1}$ is the inverse of the Fisher matrix.\nAssuming that the probability distribution of the model is p(x) of the input x, the Fisher matrix G is given by:\n\\begin{equation}\nG = E_{p(x|w)} {\\nabla_w log p(x|w)[\\nabla_w log p(x|w)]^T}\n\\end{equation}\nwhere $E_{p(x|w)}[\\cdot]$ denotes the expectation with respect to probability distribution p(x|w), $\\nabla_wlog p(x|w)$ is the conventional gradient of the log-likelihood, and $[\\nabla_wp(x|w)]^T$ is the transpose of the gradient matrix."}, {"title": "B. Relationship between the NGD and GD", "content": "We first discuss the relationship between NGD and GD and give a proposition.\na) Proposition 1: Let $w' = G^{\\frac{1}{2}} \\cdot w$. Then optimizing parameters w' based on GD is equivalent to optimizing parameters w based on NGD.\nb) Proof: This equivalence is derived by applying the chain rule of derivatives and exploiting the symmetry of the Fisher information matrix.\nFirst, the standard update rule of NGD is simplified to:\n\\begin{equation}\nw \\leftarrow w - a \\cdot G^{-1} \\cdot \\frac{\\delta l}{\\theta w}\n\\end{equation}\nIt is important to note that this equation assumes the existence of the inverse matrix $G^{-1}$ of the Fisher information matrix. However, in practice, the Fisher matrix is typically positive semi-definite and may not have a full-rank inverse. To address this issue, regularization or approximation techniques are commonly employed, which will be discussed in detail later.\nThen, we multiply both sides of Eq. (3) by $G^{\\frac{1}{2}}$ and obtain:\n\\begin{equation}\nG^{\\frac{1}{2}} \\cdot w \\leftarrow G^{\\frac{1}{2}} \\cdot w - a \\cdot G^{\\frac{1}{2}} \\cdot G^{-1} \\cdot \\frac{\\delta l}{\\theta w}\n\\end{equation}\n\\begin{equation}\nG^{\\frac{1}{2}} \\cdot w \\leftarrow G^{\\frac{1}{2}} \\cdot w - a \\cdot G^{-\\frac{1}{2}} \\cdot \\frac{\\delta l}{\\theta w}\n\\end{equation}\nNext, apply the chain rule of derivatives to transform $\\frac{\\delta l}{\\theta w}$ as follows:\n\\begin{equation}\nG^{\\frac{1}{2}} \\cdot w = G^{\\frac{1}{2}} \\cdot w - a \\cdot G^{-\\frac{1}{2}} \\cdot \\frac{\\delta l}{\\theta (G^{-\\frac{1}{2}}w)} \\cdot \\frac{\\delta (G^{-\\frac{1}{2}}w)}{\\theta w}\n\\end{equation}\nAfter taking the derivative $\\frac{\\delta (G^{-\\frac{1}{2}}w)}{\\theta w}$ of the above equation, we get:\n\\begin{equation}\nG^{\\frac{1}{2}} \\cdot w \\leftarrow G^{\\frac{1}{2}} \\cdot w - a \\cdot G^{-\\frac{1}{2}} \\cdot [G^{\\frac{1}{2}}]^T \\cdot \\frac{\\delta l}{\\theta (G^{\\frac{1}{2}} \\cdot w)}\n\\end{equation}"}, {"title": "C. Proposed method", "content": "Capitalizing upon proposition 1 above, we proposed a novel optimization method called structured natural gradient descent (SNGD). More specifically, the boost in efficiency brought by our method is built upon two elements: 1) Decomposing the network into subsystems, and 2) Efficient computation of the square roots of the Fisher matrix G addressing two computational challenges: 1) the cubic complexity involved in computing the Fisher matrix G, and 2) The matrix inversion.\n1) System decomposition: To address the challenge of computing the complex global Fisher information matrix during the training of deep neural networks with NGD, we employ a decomposition approach. Here, we partition the network into hierarchical subsystems, enabling the calculation of localized Fisher matrices, which can be calculated more efficiently. Each subsystem represents a parameterized network layer to which we append an additional sub-layer with a new normalization operator of $G^{-\\frac{1}{2}}$, and new weight parameters w', defined via the structural transformation over w' (Eq.11), and an activation function f.\n\\begin{equation}\nw = G^{-\\frac{1}{2}} \\cdot w'\n\\end{equation}\nThis above transformation allows for optimizing w' via traditional yet efficient GD optimization.\nLet us consider a n-layer deep neural network, as shown in Fig. 2. With the above decomposition, the network is composed of a sequence of subsystem layers. Specifically, the subsystem of the k-th layer (k \u2208 {1, 2, ..., n}) consists of a normalization sub-layer with a value of the negative square root of local Fisher matrix $G_k^{-\\frac{1}{2}}$, a sub-layer with new weight parameters $w_k'$ , and an activation function $f_k$ .\n2) Optimal computation of the square roots of Fisher matrix G: The local Fisher information matrix for each layer is determined as outlined below [9]:\n\\begin{equation}\nG\\leftarrow E(\\delta_f)E(xx^T)\n\\end{equation}\nwhere E() denotes the expectation, x represents the input of one layer, f is the activation function of this layer, and $\\delta_f$ is the derivative of the activation function, which means the effective learning area of the neuron.\nThe computational complexity of SNGD primarily lies in $G^{\\frac{1}{2}}$ and $G^{-\\frac{1}{2}}$. To mitigate this, we employ an iterative optimization technique. In Eq. (12), the main calculation cost lies in $E(xx^T)$, which takes $O(n^3)$. As $xx^T$ in the neural network corresponds to the Gram matrix, we approximate it using the Nystrom method [27]. This method adopts an iterative scheme to compute square roots Z of A (Eq. (13),\n\\begin{equation}\nF(Z) \\leftarrow Z^2 \u2013 A = 0\n\\end{equation}\nUsing the Denman-Beavers iteration method [28] with initial values $Y_0 = A$ and $Z_0 = I$, the iterative operations are defined as follows:\n\\begin{equation}\nY_{k+1} \\leftarrow \\frac{1}{2}(Y_k + Z_k^{-1}), Z_{k+1} \\leftarrow \\frac{1}{2}(Z_k + Y_k^{-1})\n\\end{equation}\nThrough Eq. (14), the matrices $Y_k$ and $Z_k$ can quickly converge to $A^{\\frac{1}{2}}$ and $A^{-\\frac{1}{2}}$. To further improve the efficiency, we modify the iterative Eq. (13) referring to the iterative method [29] to avoid inverse operations. The optimized iterative operations are as follows:\n\\begin{equation}\nY_{k+1} \\leftarrow \\frac{1}{2}Y_k (3I - Z_kY_k), Z_{k+1} \\leftarrow \\frac{1}{2} (3I - Z_kY_k) Z_k\n\\end{equation}\nBy offering an approximate solution to computing the inverse of G, this method enables us to derive the negative square root of the matrix entirely via matrix multiplication which complexity can go down to $O(n^{2.373})$ as compared to $O(n^3)$ for the matrix inversion [30]. This not only demands fewer memory and computational resources as a fundamental principle but also mitigates issues with numerical stability, particularly when the matrix is poorly conditioned."}, {"title": "D. SNGD Training of a deep neural network", "content": "After restructuring the network and adjusting its structure via the new normalization sub-layers, we use GD to optimize the new weight parameters, though retaining similar benefits as NGD. The computation of local Fisher layers in SNGD facilitates input regularization by leveraging $E(xx^T)$ and enables the identification of effective neurons by considering the variations in transformations through $E(\\delta_f)$. These local Fisher layers keep the data distribution stable and provide a curvature signal of the loss of space. Therefore, this new method has a significant effect on both accelerating network convergence and regularization, making gradient updates more stable and faster. The optimization procedure of deep neural networks using our proposed SNGD is depicted in Algorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "Networks. We evaluate the proposed method on several widely used networks as below.\nMLP: The MLP [31] has two fully connected hidden layers, each with 80 hidden units and ReLU as activation functions.\nVGG: The VGGNet [32] consists of convolution, pooling, and fully-connected layers. The kernel size is 3x3, and the max-pooling layer is of the size of 2x2 with a stride of 2. The activation functions after the convolution or fully-connected layer are all ReLU. VGG-16 is taken.\nResNet: ResNet-18 [33] is taken.\n1, 2, 3-layer LSTM [34] are both with 200 hidden units in each layer.\nDatasets and loss functions. We explore image classification on different datasets, including MNIST [35] and CIFAR dataset [31]. The MNIST and CIFAR-10 datasets for image classification consist of 10 different classes, with 28x28 gray images and 32x32 color images respectively, divided into a training set of 50,000 images and a test set of 10,000 images. Besides, to evaluate whether SNGD can generalize well to training on larger image recognition datasets, we also train ResNet on ImageNet [36] which contains 5247 classes and 3.2 million images. As for LSTM, we perform language modeling on the Penn Treebank dataset [37], a corpus consisting of over 4.5 million words of American English. The loss functions are all cross-entropy loss functions based on L2-regularized ($l_2 = 10^{-3}$).\nCompared optimizers. Due to SNGD being essentially a first-order normalization method, we perform extensive comparisons with traditional first-order network optimizers, such as SGD [38], Adam [39], and RAdam [40]. Additionally, we use the performance of KFAC [5] on MNIST as the baseline for the second-order optimizer to validate the effectiveness of our proposed SNGD. For these traditional first-order optimizers, we perform grid searches to choose the best hyperparameter and report the results using the best hyperparameter settings. For the learning rate, we search it among 100.0, 10.0, 1.0, 0.1, 0.01, 0.001, 0.0001. For image classification experiments, we set the learning rate of 0.1 for SGD with momentum and SNGD, 0.001 for Adam and RAdam. For SGD, we set the momentum as 0.9, which is the default for many networks such as ResNet. For other parameters, we set them as their default values in the literature [5], [38]\u2013[40]."}, {"title": "B. SNGD performance", "content": "In this subsection, we verify the performance of SNGD for the image classification task on MNIST and show the results of SNGD against second-order baseline KFAC and first-order optimizer SGD on MNIST.\nFig. 3(a) and (b) show the training loss and test accuracy for image classification task on MNIST, and we find that second-order algorithms, both SNGD and KFAC, perform far better than first-order optimizers SGD. From Fig. 3, we can see that our structured method converges faster than SGD while achieving higher test accuracy (97.6%) than KFAC (96.3%) and SGD (94.8%). The performance of SNGD greatly exceeds the original first-order optimizer SGD thanks to the advantages of the second-order optimization algorithm. Furthermore, our method is a more precise implementation of NGD than KFAC and takes into account the computational efficiency of the Fisher matrix, which leads SNGD to surpass the second-order optimizer baseline. However, generalizing this second-order optimization method baseline to large-scale deep networks is difficult to realize. So, in the following subsection, we compare well-tuned first-order optimizers with SNGD in various tasks."}, {"title": "C. Comparisons", "content": "Local Fisher layer versus batch norm layer: To demonstrate the effectiveness of key components of SNGD, namely the Local Fisher layer, we compare the Local Fisher layer to the Batch Norm layer with the same model and hyperparameters (as detailed previously) on MNIST, CIFAR-10, and Penn Treebank datasets. In Fig. 4(ac), we apply SNGD, SGD+BN, and SGD to train MLP on MNIST, VGG-16 on CIFAR-10, and 2-layer LSTM on Penn Treebank, respectively. For example, as shown in Fig. 4(a), the final loss of MLP trained with SNGD reaches 0.036 on the MNIST dataset, less than the final loss of 0.112 trained with SGD. Although the loss of SGD+BN is reduced to 0.094, it is still inferior to SNGD. Those results confirm the superiority of the proposed method and show that SNGD can accelerate the convergence of deep network models and achieve better performance than SGD while maintaining its computational simplicity.\n2) Comparison with traditional first-order GDs: We first demonstrate the effectiveness of SNGD in training different networks, including MLP, VGG-16, and ResNet, compared with traditional first-order optimizers, including SGD, Adam, and RAdam.\nMLP. As shown in Fig. 5(a), SNGD significantly outperforms other optimizers trained for 100 epochs using an MLP with two hidden layers on the CIFAR-10 dataset. However, traditional first-order optimizers reduce the loss at early steps but saturate quickly.\nVGG-16. Fig. 5(b) shows the training curves of traditional GD training VGG-16 on CIFAR-10 for 100 epochs. Overall, SNGD achieves the best performance with the lowest loss.\nResNet. Fig. 4(c) illustrates the training curve with the lowest loss when training ResNet-18 on CIFAR-10. This shows that SNGD also achieves the lowest loss on ResNet-18. These observations demonstrate the effectiveness of our SNGD approach, which generalizes better to training tasks than other stochastic first-order optimizers.\nWe further compare the training process of SNGD on the different datasets of 100 epochs and a batch size of 50 with SGD, Adam, and RAdam, respectively.\nMNIST. We train MLP on the MNIST dataset for 100 epochs, and the comparison of training and testing curves using different optimizers are shown in Fig. 6. The results show that SNGD significantly outperforms traditional stochastic first-order optimizers and adaptive methods, including SGD, Adam, and RAdam.\nCIFAR. As shown in Fig. 5, we compare the effectiveness of SNGD with other stochastic first-order methods in training MLP, VGG-16, and ResNet-18 on CIFAR-10. SNGD achieves a comparable convergence speed as adaptive methods such as Adam while achieving better accuracy than SGD and other methods without incurring excessive time cost.\nPenn Treebank. We conduct experiments with a 1,2,3-layer LSTM with several hidden units of 200 and report the perplexity on the test set trained with different optimizers in Fig. 7 and Table I. We can observe that on the simpler 1,2-layer LSTM model, the difference between different optimizers is not very obvious. But different optimizers have different optimization behaviors on more complex 3-layer LSTM models. Although Adam and RAdam converge slightly faster in the early stages, they quickly fall into local optimal solutions. At the same time, SNGD can quickly update the optimization direction, thus achieving the best final test complexity in language modeling tasks."}, {"title": "D. Generalization of the trained networks on test sets", "content": "We further evaluate the generalization ability of the trained MLP, VGG-16, and ResNet-18 on CIFAR10 with different optimizers. The Top-1 accuracy when the network converges is reported in Table II. Fig. 8 shows the highest accuracy verified. According to Table II, MLP, VGG-16, and ResNet-18 trained by SNGD all achieve the best Top-1 accuracy on the test set. These results show that SNGD is beneficial to improve the generalization ability of the trained network."}, {"title": "E. Computational time", "content": "To evaluate the computational efficiency of our proposed SNGD method, we compare it with traditional first-order optimizers using a 6-core Intel i9-8950HK CPU and an NVIDIA GeForce GTX 1080 GPU. Table IV shows the training time for MLP, VGG-16, and ResNet-18 with a batch size of 50 on CIFAR-10 per epoch using SNGD and traditional first-order optimizers. Our results indicate that SNGD achieves comparable training time to traditional first-order optimizers while exhibiting superior performance, as demonstrated in Table IV. We attribute this to the fact that most first-order optimizers, such as Adam, update parameters by computing the parameter update vector and assigning it to the network layer by layer. At the same time, our method employs the Gram matrix and Newton's method to accelerate the computation of the negative square root of the local Fisher information matrix, thereby retaining the natural gradient's accelerated convergence properties."}, {"title": "F. Resource Utilization and Efficiency", "content": "In this subsection, we further provide an empirical comparison of Gradient Descent (GD), Natural Gradient Descent (NGD), and our proposed Structured Natural Gradient Descent (SNGD) across multiple models and datasets. We analyze these optimization methods with respect to convergence time (Conv. Time), memory consumption (Mem), and GPU/CPU utilization, both average (Avg. Util) and peak (Peak. Util). Table V presents a synthesized overview of the comparative metrics.\nAs shown in Table V, the proposed SNGD demonstrates notable improvements in convergence times compared to NGD across all tested models. This increase in efficiency is especially significant in complex models such as VGGNET34/CIFAR and RESNET34/IMAGENET, where the high dimensionality of parameters can substantially slow down the optimization process. SNGD shows an advantageous balance, converging faster than NGD while avoiding the extensive memory utilization associated with this method. In terms of memory consumption, SNGD operates with a moderate overhead compared to GD but remains less demanding than NGD. This outcome indicates SNGD's capability to maintain a relatively low memory footprint while still benefiting from the structured approach to natural gradients. GPU/CPU utilization metrics provide insights into the computational behavior of SNGD. While average utilization for SNGD does not significantly deviate from GD and NGD, peak utilization suggests that SNGD can occasionally reach similar levels of resource demand as GD. This characteristic may imply that SNGD can intensify computational efforts when necessary, possibly contributing to its faster convergence times without sustaining high utilization throughout the optimization process.\nThe structured approach taken by SNGD, therefore, offers a compelling trade-off between computational efficiency and the convergence properties offered by NGD. By effectively managing the trade-offs between memory and computation, SNGD emerges as a robust alternative that leverages the benefits of both traditional GD and NGD. Future work should focus on validating these benefits in a broader range of applications and establishing the practical limits of the SNGD approach."}, {"title": "V. CONCLUSIONS", "content": "This paper has presented a novel optimization framework for training deep neural networks with Structured Natural Gradient Descent (SNGD). By reconstructing the network layer in a deep neural network, SNGD realizes natural gradient descent, accelerating network convergence and improving optimization effectiveness. Experimental results demonstrate that SNGD not only outperforms traditional GD optimization methods regarding convergence speed and accuracy on various training tasks, including MLP on MNIST, MLP, CNN, ResNet on CIFAR-10, ResNet on ImageNet, and LSTM on Penn Treebank, but also showcases its universal applicability and efficiency in resource utilization. Particularly, SNGD's capability to achieve faster convergence times across all tested models, as highlighted in our comparative metrics, underlines its superior efficiency. By maintaining lower memory consumption and dynamically managing GPU/CPU utilization, SNGD illustrates a significant advancement in optimizing deep learning computations. We believe that the SNGD method has the potential to expand the scope of natural gradient descent to encompass a wider array of deep networks, such as transformers, further enhancing its utility and effectiveness in the field of machine learning."}]}