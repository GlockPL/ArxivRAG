{"title": "Efficient and Accurate Memorable Conversation Model using DPO based on\nsLLM", "authors": ["Youngkyung Seo", "Yoonseok Heo", "Jun-Seok Koh", "Du-Seong Chang"], "abstract": "In multi-session dialog system, it is essential to\ncontinuously update the memory as the session pro-\ngresses. Simply accumulating memory can make\nit difficult to focus on the content of the conversa-\ntion for inference due to the limited input sentence\nsize. Therefore, efficient and accurate conversation\nmodel that is capable of managing memory to re-\nflect the conversation history continuously is neces-\nsary. This paper presents a conversation model that\nefficiently manages memory as sessions progress\nand incorporates this into the model to reflect the\nconversation history accurately with 3 methodolo-\ngies: SFT, DPO and DPO with SFT model. Our\nmodel using DPO algorithm shows an improve-\nment about 0.0591 of BERTScore in memory ac-\ncuracy, and the rate of responses reflecting the\nmemory increased as well. Also, response genera-\ntion performance enhanced about 4.292 in fluency,\n3.935 in coherence, and 2.896 in consistency. This\npaper describes a training method that yields bet-\nter performance than models with more than twice\nthe parameter size, even when the model size is\nsmaller. Thus, our model demonstrates efficiency\nnot only in terms of accuracy but also in resource\nutilization.", "sections": [{"title": "Introduction", "content": "The\nutilization of\nlarge-scale language models\n(LLMs) [Achiam et al., 2023] accompanied by external\nknowledge has garnered significant attention, as it can\novercome the limitations of reusability inherent in lower-\nlevel LLMs, which are often associated with high costs.\nIn practice, LLMs embody a vast amount of knowledge\nacross a wide array of topics, unparalleled by past stan-\ndards. Leveraging their significantly improved contextual\nunderstanding, LLMs can maintain consistent and extended\nconversations with users, addressing various user needs\nthrough a single integrated model. However, it is known\nthat LLMs are notably deficient in addressing issues related\nto knowledge they have not encountered during training\nor new information generated post-training. This shortfall\nunderscores the fundamental necessity of integrating neuro-\nsymbolic methodologies that leverage up-to-date information\nsources, such as knowledge graphs or structured databases,\nduring the inference stage of LLMs[Chen et al., 2022;\nMishra et al., 2022].\nThis paper focuses on the necessity of external knowl-\nedge in real industrial environments where small Large Lan-\nguage Models (sLLMs)[Abdin et al., 2024] are utilized as\nconversational agents. Contrary to the scaling law of lan-\nguage models[Kaplan et al., 2020], the recently highlighted\non-device Al research aims to provide reliable services to\nusers using small-scale language models in constrained en-\nvironments. Additionally, conversational agents must offer\npersonalized response services to individual users. As this\ncannot be achieved from the training phase alone, language\nmodels must be capable of utilizing individual user informa-\ntion as external knowledge. This user-specific information\nincludes not only meta-data like user preferences but also in-\nferable information from the history of past interactions with\nthe user.\nThis issue is designed for multi-session dialogue problems,\nand extensive research has been conducted in this area. Multi-\nsession dialogue assumes situations where conversations oc-\ncur continuously at regular intervals. A dialogue session is\ndefined as a multi-turn interaction between the user and the\nsystem from the initiation to the termination of a conversa-\ntion. Each dialogue session can range from a few minutes\nto several months apart. Therefore, the most crucial prob-\nlem in multi-session dialogue is ensuring that the system can\naccurately reference information from past dialogue sessions\nduring current interactions.\nRecent research has been directed towards continuously\nsummarizing and updating the history of previous dialogue\nsessions in the form of sentences, storing and updating them\nin external memory[Wang et al., 2024]. As multiple dia-\nlogue sessions progress, the information in external memory\nrequires updates. Information in memory may be deleted, up-\ndated with new content, or new information may be added\nover time."}, {"title": "Related Work", "content": "Another study [Wang et al., 2023] extracts key-value pairs di-\nrectly from layers, eliminating the need for separate storage\nor transformation. However, generating responses that reflect\nthese key-value pairs heavily depends on the model, requiring\na pre-trained model from the initial training phase.\nIn this paper, we propose a model that optimizes resource\nusage by managing memory through summarization without\nadditional transformations, using only Direct Preference Op-\ntimization (DPO)[Rafailov et al., 2024]. This approach sim-\nplifies memory management while maintaining high accuracy\nand efficiency."}, {"title": "Memory summarization", "content": "To conduct effective multi-turn or multi-session conversa-\ntions that accurately reflect the user's history, it is crucial\nnot only to extract relevant historical information but also to\nstore it efficiently. However, as conversations lengthen and\nthe number of sentences to manage increases, there is a need\nfor efficient processing methods. Previous research[Jang et\nal., 2023] has stored user utterances in a summary format,\nupdating the summary text as sessions progress. However,\nthis method can make memory management challenging due\nto the increasing length of the summaries.\nOther approaches have used hash tables[Liu et al., 2023a]\nor transformed summarization into formats like triplets[Kang\net al., 2022] for storage and updates. These transformations,\nhowever, can result in the loss of information regarding the re-\nlationships between contexts, leading to decreased accuracy."}, {"title": "Memory management", "content": "While managing memory is essential, the ability to retrieve\nand update relevant sentences in the conversation is equally\nimportant. Utilizing graph embeddings allows for efficient\nretrieval through hashing algorithms like LSH, but this ap-\nproach requires converting data into graphs and having a\nmodel capable of embedding these graphs. Managing mem-\nory through summarization by structuring it into short chunks\nin a list format simplifies updates. However, this method ne-\ncessitates clear criteria for replacing, adding, or deleting sen-\ntences, which typically involves training additional models\nlike NLI for effective management.\nBy employing prompts to let large language models\n(LLMs) make these decisions mechanically, memory updates\ncan be achieved without additional model training. This pa-\nper proposes an enhanced method that applies reinforcement\nlearning to improve the accuracy of memory updates within\ngiven dialogues. To address the challenge of reflecting con-\ntext switching, we automatically generate negative samples\nand use them with the Direct Preference Optimization (DPO)\nalgorithm to focus on the context.\nThis approach not only simplifies the process but also en-\nsures that memory updates are precise and contextually rele-\nvant, leveraging advanced techniques to overcome the limita-\ntions of previous models."}, {"title": "Methodology", "content": "In this section, we propose a memory-augmented framework\nusing small language models for consistent multi-session di-\nalogues. As shown in Figure 2, it consists of two modules:\nmemory management module and dialog generation module.\nThe primary purpose of the memory management module\nis to store speaker utterance information from previous con-\nversation sessions in a separate memory space for reference\nin future new conversation sessions. This module updates the\nmemory by referencing the contents of the current conver-\nsation session based on the speaker information updated up\nto the previous conversation session immediately after each\nconversation session ends. To achieve this, we propose a his-\ntory management method based on summary using a DPO-\ntuned SLM. The proposed method allows for managing the\nkey information of speaker utterances in the form of a set of\nshort sentences. This approach has the advantages of improv-\ning the accuracy of generated responses, maximizing learning\nefficiency, and facilitating easy monitoring through the man-\nagement of speaker information in a concise format."}, {"title": "Memory Management Module", "content": "In this module, the context of the dialog is summarized sep-\narately by speakers and stored in list format as memory to\nmanage memory efficiently at the end of the session's dialog.\nAs the session continues, in the case of multi-session conver-\nsations, the stored memory is updated with information nec-\nessary for the following conversations. When the memory is\nupdated, changes are easily deleted or replaced as it is orga-\nnized in a list format. This pipeline allows for more accurate\ninformation when generates the response."}, {"title": "Training Methods", "content": "We introduce a training scheme for the memory management\nmodule by only adopting the DPO algorithm, which is de-\nnoted as Method2. To make readers better understand, we in-\ntroduce different training schemes such as SFT and SFT with\nDPO."}, {"title": "Method1: SFT (Supervised fine-tuning)", "content": "Supervised fine-tuning is a machine learning technique where\na pre-trained model is further trained on a specific task using\nlabeled data. The pre-trained model, often trained on a large\ndataset for a related task, serves as a starting point, and then\nthe model's parameters are adjusted (fine-tuned) to better fit\nthe new task-specific data.\nIn supervised fine-tuning, the model's parameters are up-\ndated to minimize a predefined loss function, typically a mea-\nsure of the difference between the model's predictions and\nthe ground truth labels. This process allows the model to\nadapt its learned representations to the specific characteristics\nof the new task while leveraging knowledge gained from the\npre-training phase. Supervised fine-tuning is commonly used\nin transfer learning scenarios, where the pre-trained model's\nknowledge is transferred to new tasks, leading to improved\nperformance, especially when labeled data for the target task\nis limited."}, {"title": "Method2: DPO (Direct Preference Optimization)", "content": "Reinforcement learning is a machine learning paradigm\nwhere an agent learns to interact with an environment to\nachieve a goal by maximizing cumulative rewards. The agent\ntakes actions in the environment, observes the resulting state\ntransitions and received rewards, and learns a policy that\nmaps states to actions to maximize long-term rewards.\nSupervised fine-tuning is a form of supervised learning,\nwhere the model learns from labeled data with a predefined\nobjective, whereas reinforcement learning is a type of learn-\ning where the agent learns to interact with an environment to\nmaximize cumulative rewards through trial and error.\nIn particular, the method utilized in this paper aims to uti-\nlize DPO to learn which preference, either positive or nega-\ntive sample, should be selected, thereby reducing hallucina-\ntions and focusing on learning within the given conversational\ncontext. Through this approach, we aim to apply a learning\nmethod that can effectively understand the cause and effect\nof the context using simple pairwise comparisons, requiring\nfewer resources compared to SFT learning methods that label\ndata extensively."}, {"title": "Method3: SFT and DPO", "content": "Using a model trained with su-\npervised fine-tuning (SFT) to perform direct preference op-\ntimization (DPO) would likely yield performance enhance-\nment. The model enhanced through SFT might have already\nacquired rich knowledge from various datasets. This en-\nhanced model could improve performance when used in DPO\ntasks where additional training data is limited. It could main-\ntain pre-learned features while enhancing performance in pre-\ndicting human preferences. Also, DPO relies on human pref-\nerences rather than predefined reward functions. The model\nimproved through SFT might be more effective in predicting\nthese human preferences.\nTherefore, utilizing this model could result in more ac-\ncurate and efficient estimation of human preferences. This\ncould facilitate quicker adaptation to human preferences and\nenable faster decision-making processes."}, {"title": "Dialog Generation Module", "content": "Dialog generation module uses the memory generated from\nMemory management module and the context of the cur-\nrent session to generate responses to the user's dialog. In\nthis paper, we use Phi-3 as the base model. The example\nof prompt for the model is described in Figure 4. If the mem-\nory is well-structured, any language model can be used for\nthe base model of generating the conversation in dialog gen-\neration module. By combining the memory sentence list and\ndialog context to generate responses, the module is able to\nstore previous sessions' memory providing information more\nfocused on the user."}, {"title": "Experiment", "content": "In this section, we present the experimental results evaluating\nthe summarization and summarization update processes for\nmemory management. We describe examples of the prompts\nused for each evaluation or data generation and their respec-\ntive outcomes. The evaluations were conducted using both\nauto-evaluation with ChatGPT and human evaluation. Utiliz-ing these methods, we compare and discuss the performance\nof our model relative to other models."}, {"title": "Experiment Details", "content": "Dataset\nTo store dialogue in memory in a suitable summarization for-\nmat, it is essential to extract speaker information that can be\nutilized in subsequent sessions. Therefore, we utilize Multi-\nSession Chat(MSC, Xu et al.,2022a) dataset which has sum-\nmary information of each speaker as persona, and aggregated\nsummary information after the following sessions. To facil-\nitate continuous updates of the summarized information, we\nstructured the summaries into short sentences of 15 words\nor fewer, making them easier to manage than longer context\nsentences. To effectively train this approach, we designed\nprompts to create a dataset for Supervised Fine-Tuning (SFT)."}, {"title": "DPO negative sample", "content": "To apply Direct Preference Optimization (DPO), it is neces-\nsary to have pairs of negative and positive samples consist-\ning of rejected and selected samples. The reason for applying\nDPO in this paper is to accurately reflect the causal facts in the\nmemory. Previous research has used a method of generating\nnegative samples by altering some entities in given sentences\nto develop benchmarks for evaluating factuality [10]. There-\nfore, we used prompts to select sentences that accurately re-\nflect cause and effect, generating suitable negative samples\nfor reinforcement learning."}, {"title": "Evaluation Metric", "content": "In order to quantitatively compare the prediction sentences\nand target sentences pairwise, we assessed the quality of the\nsummary using the following metrics. The prediction sen-\ntences are those generated by the model, while the target sen-\ntences refer to the list of correct sentences that need to be\nstored in memory from a given session. Predicted summary\nsentences that exceed a certain threshold are considered Sim-\nilar sentences. Thus, the quality of the summary can be eval-\nuated based on how many of the target sentences are matched\nby the prediction sentences above a specific threshold. For\nevaluating the quality of the summary, we employed cosine\nsimilarity and BERTScore, which are commonly used to as-\nsess the similarity between two sentences. The metric is de-\nscribed in Figure 6.\nGiven the target dataset T = (t1, t2,t3) where each of the\nitem represents the answer summary of the memory, our goal\nis to evaluate the pair between the target and the given pre-\ndiction dataset P = (P1,P2, P3). Here, cosine similarity and\nBERTScore are used to calculate the similarity between the\neach pair of target and prediction. When the similarity value\nis more than 0.7 and 0.95 respectively, the sentences are ex-"}]}