{"title": "DIVE: Towards Descriptive and Diverse Visual Commonsense Generation", "authors": ["Jun-Hyung Park", "Hyuntae Park", "Youjin Kang", "Eojin Jeon", "SangKeun Lee"], "abstract": "Towards human-level visual understanding, visual commonsense generation has been introduced to generate commonsense inferences beyond images. However, current research on visual commonsense generation has overlooked an important human cognitive ability: generating descriptive and diverse inferences. In this work, we propose a novel visual commonsense generation framework, called DIVE, which aims to improve the descriptiveness and diversity of generated inferences. DIVE involves two methods, generic inference filtering and contrastive retrieval learning, which address the limitations of existing visual commonsense resources and training objectives. Experimental results verify that DIVE outperforms state-of-the-art models for visual commonsense generation in terms of both descriptiveness and diversity, while showing a superior quality in generating unique and novel inferences. Notably, DIVE achieves human-level descriptiveness and diversity on Visual Commonsense Graphs. Furthermore, human evaluations confirm that DIVE aligns closely with human judgments on descriptiveness and diversity\u00b9.", "sections": [{"title": "1 Introduction", "content": "Humans possess a cognitive ability to reason about the rich and complex stories beyond a given visual scene, based on their background commonsense knowledge. Visual commonsense reasoning is a key to this cognition-level visual understanding (Zellers et al., 2019), which helps humans comprehend the interactions around them. As research towards the human-level visual understanding of machines, visual commonsense generation (Park et al., 2020) has been introduced. This challenging task aims to generate textual commonsense inferences about potential antecedents and consequences, as well as the present intents of characters. Recent works on visual commonsense generation (Park et al., 2020; Xing et al., 2021) have progressed to develop vision-language models capable of generating more plausible and relevant inferences.\nDespite considerable efforts in visual commonsense generation, an important aspect of humans' innate cognitive ability has been overlooked in previous studies: humans can make descriptive and diverse inferences by capturing important, specific, and detailed information within a visual scene. This ability is necessary for making precise and informative inferences about various possible scenarios in the world, but it is lacking in existing models. Figure 1 illustrates a case where model-generated inferences still fall short of human-written infer-"}, {"title": "2 Related Work", "content": "Visual commonsense reasoning. With the goal of reasoning beyond visual recognition, the community has actively explored several visual commonsense reasoning tasks. Zellers et al. (2019) have proposed a visual commonsense reasoning benchmark to test if a model can identify an answer with rationale, given a question that requires a thorough understanding of images based on commonsense knowledge. Hessel et al. (2022) have proposed an abductive reasoning benchmark beyond literal image contents to evaluate the capacity of models to retrieve relevant inferences, localize evidence, and compare plausible inferences. Li et al. (2022a) have introduced a video QA benchmark that requires understanding of evidences and commonsense reasoning over time. Yu et al. (2022) have proposed an audiovisual commonsense reasoning benchmark focusing on physical knowledge, which requires an understanding on multi-sensory inputs. However, these benchmarks evaluate models in a question answering format, limiting the evaluation of the models' capability to generate commonsense inferences. To address this, Park et al. (2020) have"}, {"title": "3 Methodology", "content": "In this section, we introduce DIVE, a framework designed for descriptive and diverse commonsense generation. First, we propose a generic inference filtering method that balances the distribution of"}, {"title": "3.1 Generic Inference Filtering", "content": "Since the skewed distribution of VCG can cause vision-language models to favor generic inferences, we construct a balanced VCG based on our generic inference filtering method. We identify generic inferences based on their frequencies and how their related images are semantically concentrated, because a generic inference is expected to be frequent and associated with a broader range of images.\nGiven a visual commonsense graph $G = (I, E, P, C, R)$, where $I$, $E$, $P$, and $C$ denote sets of images, events, places, and commonsense descriptions, respectively, $R$ is the set of edges in the graph comprising visual commonsense inferences $R_{ij} = (I_i, E_i, P_i, r, C_j)$. Here $I_i \\in I, E_i \\in E, P_i \\in P, C_j \\in C$, and a reasoning type $r \\in \\{before, after, intent\\}$. Then, we measure the semantic concentration of images related to a commonsense description. Specifically, we measure the average cosine similarity of feature representations of the related images as follows:\n$S(C_j) = \\frac{\\sum_{x \\in G(C_j)} \\sum_{y \\in G(C_j)} sim(\\mathcal{F}(x), \\mathcal{F}(y))}{|G(C_j)|^2}$ (1)"}, {"title": "3.2 Contrastive Retrieval Learning", "content": "Although generic inference filtering can effectively reduce the skewness of the distribution, we observe that over-filtering negatively impacts the quality of models' inference results, primarily due to the reduction in the number of training examples. Additionally, it has been noted that a specifically designed training objective function to improve descriptiveness and diversity is beneficial, as the standard generative objective function may lead to generic generation (Li et al., 2016; Luo et al., 2018). These two observations underscore the need to develop novel training methods that improve descriptiveness and diversity in visual commonsense generation, used in conjunction with the filtering method.\nHere we propose a new contrastive retrieval learning method that encourages models to generate descriptive and diverse inferences about an image. The key motivation of our method is that models need to recognize detailed objects and interactions within images to generate descriptive and diverse inferences. Our method trains a model to retrieve the original image from which a given in-"}, {"title": "4 Experiments", "content": "In this section, we demonstrate the effectiveness of DIVE by comparing it with existing methods."}, {"title": "4.1 Experimental Setup", "content": "Dataset. We conduct the experiments on the VCG dataset (Park et al., 2020), which is a large-scale visual commonsense graph. We train models with DIVE on the filtered VCG training set. In addition, we evaluate models on the original VCG validation set, the unique VCG validation set, and the novel VCG validation set. The unique VCG validation set is a subset of the original set that consists of inferences with commonsense descriptions that appear once in the original set. The novel VCG validation set is a subset of the original set that consists of inferences with commonsense descriptions that do not appear in the training set. We expect that the unique and novel subsets predominantly contain specific inferences, since they exclude duplicate examples. For both subsets, we discard the inferences of images with fewer than five commonsense descriptions. The statistics of the dataset are reported in Appendix A.\nBaselines. We mainly compare our results with those of VisualCOMET (Park et al., 2020), KM-BART (Xing et al., 2021), and BLIP (Li et al., 2022b). VisualCOMET (Park et al., 2020) extends a pre-trained GPT-2 model (Radford et al., 2019) with 126 million parameters to incorporate visual and textual information. KM-BART is based on a pre-trained BART-base model (Lewis et al., 2020) with 141 million parameters and conducts additional pre-training with image captioning data. BLIP (Li et al., 2022b) is a pre-trained generative vision-language transformer with 247 million parameters. All the baselines are fine-tuned on VCG following the fine-tuning settings specified in their original papers.\nImplementation details. We fine-tune both vision-language BART and BLIP on VCG using"}, {"title": "4.2 Main Results", "content": "We first evaluate the descriptiveness and diversity in visual commonsense generation of vision-language models. In Table 1, we compare our DIVE models with state-of-the-art visual commonsense generation models on the original VCG validation set. We observe that our DIVE models outperform the baselines in all evaluation metrics for descriptiveness and diversity. Particularly, DIVE models reach human-level descriptiveness and diversity on the original VCG validation set. These results confirm that our DIVE framework effectively augments vision-language models with the capability for generating descriptive and diverse commonsense inferences, showing significant improvements over existing vision-language models. In addition, as shown in Tables 2 and 3, our DIVE models consistently outperform the baselines in terms of descriptiveness and diversity on the unique and novel VCG validation sets.\nTo evaluate the quality of visual commonsense"}, {"title": "4.3 Human Evaluation Results", "content": "We present human judgments on the plausibility, descriptiveness, and diversity of the generated inferences. We conduct human evaluations with workers from Amazon Mechanical Turk\u00b3.\nFollowing Xing et al. (2021), we generate 450 inferences with three different models, including DIVEBART, VisualCOMET (Park et al., 2020), and KM-BART (Xing et al., 2021), for pair-wise comparison. We generate five pairs of inferences for each inference type. For each example, we construct pairs of inferences generated by DIVE and one of the baselines, and sets of all inferences generated by each model. Then, we ask three annotators to choose a better inference based on the following three metrics: 1) plausible: which inference seems more plausible and reasonable to an image, 2) descriptive: which inference explains the image more informatively and specifically, and 3) diverse: which set of inferences seems more diverse in meanings and expressions."}, {"title": "5 Analysis", "content": "In this section, we conduct analyses of the components and results of DIVEBART."}, {"title": "5.1 Ablation Study", "content": "To better understand the contributions of each component in DIVE to performance improvements, we conduct ablation studies on generic inference filtering and contrastive retrieval learning. The results are shown in Table 8. We find that training models without our filtering method results in a significant degradation in the R@1 and Unique scores, which highlights that balancing the distribution of the visual commonsense resources is crucial for generating descriptive and diverse inferences. In addition, our contrastive retrieval learning method universally improves the three metrics when combined with the filtering method, showing its contributions to the improvements in generation quality, descriptiveness and diversity. Nevertheless, the contrastive retrieval learning method degrades the performance when applied alone. We speculate that this is because a wide range of images can be frequently sampled as negative ones if generic inferences are not eliminated, failing to meet the motivation of the method that trains models to recognize the detailed differences among similar images. This observation also shows that the components of DIVE are complementary to each other for the performance improvements."}, {"title": "5.2 Informativeness of Inferences", "content": "We analyze the amount of information contained in generated inferences by measuring word-level entropy (Mou et al., 2016). Figure 4 shows the distribution of generated inference on the original VCG validation set in relation to word-level entropy. The y-axis represents the ratio of the number of generated inferences for the corresponding interval of entropy in the x-axis. Each value k in the x-axis represents the interval between k\u22121.0 and k+1.0. We can observe that DIVE generates inferences with relatively high entropy, which implies the improvements in their informativeness."}, {"title": "5.3 Qualitative Analysis", "content": "We present qualitative examples of DIVE compared to baselines in Figure 5. It demonstrates that DIVE can generate more descriptive and diverse inferences compared to the baselines. As can be observed from the figure, DIVE effectively generates unique and novel inferences applicable to the given situations, utilizing specific expressions related to the image, such as \u201cdisturb\u201d, \u201cthirst\", etc. In contrast, the baselines frequently generate simple, generic, and seen descriptions. Interestingly, DIVE sometimes generates more descriptive and diverse inferences compared to human annotations like the Intent inferences in Figure 5 (a). This further implies that existing automatic evaluation can underestimate the scores of DIVE due to lexical differences from human annotations.\nDespite the promising results, DIVE generates some irrelevant inferences to the context even if the detailed information is explicitly given. This shows that vision language models still lack some commonsense knowledge and reasoning ability to accurately reason over recognized context.\""}, {"title": "6 Conclusion", "content": "We have presented DIVE to improve the descriptiveness and diversity of vision-language models in visual commonsense generation. We have proposed a generic inference filtering method to balance the skewed distribution of visual commonsense resources, based on the frequency and semantic concentration of images. In addition, we have proposed a contrastive retrieval learning method to promote the descriptiveness and diversity of vision-language models, by leveraging the structural infor-"}, {"title": "Limitations", "content": "While we have demonstrated that DIVE effectively improves the descriptiveness and diversity of generated inferences, there are some limitations that present promising avenues for future research. First, our filtering method leads to a loss of training data, which can limit the capabilities of vision-language models. We plan to investigate data augmentation methods for visual commonsense generation, such as utilizing external data (Xing et al., 2021) or data generated by foundation models (West et al., 2022). In addition, as the first work that explores the descriptiveness and diversity in visual commonsense generation, we have focused on the evaluations on VCG, which is the most representative dataset for visual commonsense generation. Nevertheless, the ability to capture detailed information from an image and to generate descriptive and diverse inferences would be significantly beneficial to various visual reasoning and generation tasks (Hessel et al., 2022; Zang et al., 2021; You et al., 2022), as well as reasoning tasks over audio and video (Yu et al., 2022; Li et al., 2022a). We thus plan to investigate the efficacy of DIVE on more diverse tasks and modalities. Finally, in Section 5.3, we have observed that DIVE occasionally generates irrelevant inferences to the given context possibly due to the lack of commonsense knowledge and reasoning ability. Future work could focus on enhancing the commonsense knowledge and reasoning ability in vision-language models (Li et al., 2022c; Han et al., 2023)."}]}