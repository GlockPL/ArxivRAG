{"title": "DIVE: Towards Descriptive and Diverse Visual Commonsense Generation", "authors": ["Jun-Hyung Park", "Hyuntae Park", "Youjin Kang", "Eojin Jeon", "SangKeun Lee"], "abstract": "Towards human-level visual understanding, visual commonsense generation has been introduced to generate commonsense inferences beyond images. However, current research on visual commonsense generation has overlooked an important human cognitive ability: generating descriptive and diverse inferences. In this work, we propose a novel visual commonsense generation framework, called DIVE, which aims to improve the descriptiveness and diversity of generated inferences. DIVE involves two methods, generic inference filtering and contrastive retrieval learning, which address the limitations of existing visual commonsense resources and training objectives. Experimental results verify that DIVE outperforms state-of-the-art models for visual commonsense generation in terms of both descriptiveness and diversity, while showing a superior quality in generating unique and novel inferences. Notably, DIVE achieves human-level descriptiveness and diversity on Visual Commonsense Graphs. Furthermore, human evaluations confirm that DIVE aligns closely with human judgments on descriptiveness and diversity.", "sections": [{"title": "1 Introduction", "content": "Humans possess a cognitive ability to reason about the rich and complex stories beyond a given visual scene, based on their background commonsense knowledge. Visual commonsense reasoning is a key to this cognition-level visual understanding (Zellers et al., 2019), which helps humans comprehend the interactions around them. As research towards the human-level visual understanding of machines, visual commonsense generation (Park et al., 2020) has been introduced. This challenging task aims to generate textual commonsense inferences about potential antecedents and consequences, as well as the present intents of characters. Recent works on visual commonsense generation (Park et al., 2020; Xing et al., 2021) have progressed to develop vision-language models capable of generating more plausible and relevant inferences.\nDespite considerable efforts in visual commonsense generation, an important aspect of humans' innate cognitive ability has been overlooked in previous studies: humans can make descriptive and diverse inferences by capturing important, specific, and detailed information within a visual scene. This ability is necessary for making precise and informative inferences about various possible scenarios in the world, but it is lacking in existing models. Figure 1 illustrates a case where model-generated inferences still fall short of human-written inferences in terms of descriptiveness and diversity. Existing models often ignore key details in a given scene, leading to the generation of similar and generic inferences such as \u201cwalk up to Person1\" and \"stand behind Person1\u201d that could happen in most contexts and provide minimal specific detail. In contrast, humans can create more descriptive and diverse inferences like \"work in the music store with Person1\u201d by considering the image's details, such as many instruments displayed behind and two men wearing employee ID cards.\nWe observe that this deficiency can be largely attributed to the skewed distribution of visual commonsense resources. Such resources, typically crowd-sourced, often involve many generic inferences as labels, because humans may not use detailed information from a given visual scene when annotating (Berg et al., 2012; Hessel et al., 2022). For example, more than 60% of images in Visual Commonsense Graphs (VCG) (Park et al., 2020) involve generic inferences as labels. As models repeatedly learn these generic inferences, they tend to generate inferences that vaguely describe a situation, failing to capture detailed information that specifies a given scene. This limitation restricts a deeper understanding of visual information by existing vision-language models.\nIn this paper, we introduce DIVE (Descriptive and dIverse Visual commonsense gEneration), a novel visual commonsense generation framework for improving the descriptiveness and diversity of commonsense inferences generated by vision-language models. Firstly, we construct a balanced visual commonsense graph from VCG, using a carefully designed filtering method that removes generic inferences, with a focus on the semantic concentration of images utilizing the representations of CLIP (Radford et al., 2021). Furthermore, we propose a new contrastive retrieval learning method that facilitates a model to recognize specific details of an image. To verify the efficacy of DIVE, we conduct experiments on VCG (Park et al., 2020) with popular generative models (Lewis et al., 2020; Li et al., 2022b). Our experiments verify that DIVE generates more descriptive and diverse inferences compared with state-of-the-art visual commonsense generation models. Notably, DIVE achieves human-level descriptiveness and diversity scores on VCG, significantly improving the generation quality of unique and novel inferences. We further conduct human evaluations on the plausibility, descriptiveness, and diversity of generated inferences, which confirm that DIVE aligns closely with humans' judgment of descriptiveness and diversity.\nOur main contributions are as follows,\n\u2022 We propose a novel framework for visual commonsense generation, called DIVE, which enhances the capability of vision-language models to generate descriptive and diverse inferences about visual scenes. To the best of our knowledge, this is the first work to address descriptiveness and diversity in visual commonsense generation, providing deep insights.\n\u2022 We develop generic inference filtering and contrastive retrieval learning methods to facilitate descriptive and diverse visual commonsense generation of vision-language models on the skewed visual commonsense resources.\n\u2022 Our extensive experiments verify that DIVE outperforms state-of-the-art visual commonsense generation models in terms of the descriptiveness and diversity on VCG."}, {"title": "2 Related Work", "content": "Visual commonsense reasoning. With the goal of reasoning beyond visual recognition, the community has actively explored several visual commonsense reasoning tasks. Zellers et al. (2019) have proposed a visual commonsense reasoning benchmark to test if a model can identify an answer with rationale, given a question that requires a thorough understanding of images based on commonsense knowledge. Hessel et al. (2022) have proposed an abductive reasoning benchmark beyond literal image contents to evaluate the capacity of models to retrieve relevant inferences, localize evidence, and compare plausible inferences. Li et al. (2022a) have introduced a video QA benchmark that requires understanding of evidences and commonsense reasoning over time. Yu et al. (2022) have proposed an audiovisual commonsense reasoning benchmark focusing on physical knowledge, which requires an understanding on multi-sensory inputs. However, these benchmarks evaluate models in a question answering format, limiting the evaluation of the models' capability to generate commonsense inferences. To address this, Park et al. (2020) have"}, {"title": "3 Methodology", "content": "In this section, we introduce DIVE, a framework designed for descriptive and diverse commonsense generation. First, we propose a generic inference filtering method that balances the distribution of"}, {"title": "3.1 Generic Inference Filtering", "content": "Since the skewed distribution of VCG can cause vision-language models to favor generic inferences, we construct a balanced VCG based on our generic inference filtering method. We identify generic inferences based on their frequencies and how their related images are semantically concentrated, because a generic inference is expected to be frequent and associated with a broader range of images.\nGiven a visual commonsense graph $G = (I, E, P, C, R)$, where $I, E, P$, and $C$ denote sets of images, events, places, and commonsense descriptions, respectively, $R$ is the set of edges in the graph comprising visual commonsense inferences $R_{ij} = (I_i, E_i, P_i, r, C_j)$. Here $I_i \\in I, E_i \\in E, P_i \\in P, C_j \\in C$, and a reasoning type $r \\in \\{\\text{before, after, intent}\\}$. Then, we measure the semantic concentration of images related to a commonsense description. Specifically, we measure the average cosine similarity of feature representations of the related images as follows:\n$$S(C_j) = \\frac{\\sum_{x \\in G(C_j)} \\sum_{y \\in G(C_j)} \\text{sim}(F(x), F(y))}{|G(C_j)|^2}$$,\nwhere $F(\\cdot)$ represents an image encoder, $G(C_j)$ is the set of images related to a commonsense description $C_j$. When calculating the similarity, we utilize the average of feature representations in the final hidden layer of CLIP (Radford et al., 2021). The measured value indicates how closely the related images lie in the feature space, that is, how specific an inference is.\nUsing this semantic concentration $S(C_j)$ and the frequency $|G(C_j)|$ of a commonsense description $C_j$, we identify and filter out generic inferences for each commonsense description. Figure 3 illustrates an example to measure the semantic concentration and frequency on visual commonsense graphs. Inspired by the frequency-based filtering method for words (Mikolov et al., 2013), we calculate the filtering probability $P_f(C_j)$ for a commonsense description $C_j$, which is defined as follows:\n$$P_f(C_j) = 1 - \\frac{t \\times S(C_j)}{\\sqrt{|G(C_j)|}}$$,\nwhere $t$ is a threshold. Finally, we deterministically filter $[P_f(C_j) |G(C_j)|]$ inferences between $C_j$ and related images with the lowest average similarity to the others."}, {"title": "3.2 Contrastive Retrieval Learning", "content": "Although generic inference filtering can effectively reduce the skewness of the distribution, we observe that over-filtering negatively impacts the quality of models' inference results, primarily due to the reduction in the number of training examples. Additionally, it has been noted that a specifically designed training objective function to improve descriptiveness and diversity is beneficial, as the standard generative objective function may lead to generic generation (Li et al., 2016; Luo et al., 2018). These two observations underscore the need to develop novel training methods that improve descriptiveness and diversity in visual commonsense generation, used in conjunction with the filtering method.\nHere we propose a new contrastive retrieval learning method that encourages models to generate descriptive and diverse inferences about an image. The key motivation of our method is that models need to recognize detailed objects and interactions within images to generate descriptive and diverse inferences. Our method trains a model to retrieve the original image from which a given inference is derived, within a set of similar images, in a contrastive manner. This approach is expected to facilitate models to identify the differences in detailed objects and interactions among similar images, thereby aligning with our motivation.\nFirst, we construct a set of similar images $H \\subset G(C_j)$ that share the same commonsense description $C_j$ as an inference. To identify similar images, we sample images that share the same inference result, based on our intuition that they have semantic similarity that leads to the same inference result. Subsequently, we identify a pair of an image and its corresponding commonsense description $(h_p, s_p)$, where $h_p \\in H$ and $s_p$ is a commonsense description uniquely related to $h_p$ among the images in $H$. We consider $h_p$ as a positive image and $s_p$ as a positive inference, while treating the other images $h_k \\in H$ as negative images. We then define an agreement function based on cosine similarity as follows:\n$$\\sigma(h, s) = \\text{exp}(\\text{sim}(V_h, T_s))$$,\nwhere $V_h$ and $T_s$ denote the feature representations of an image $h$ and a text $s$ from a vision-language model being trained, which are extracted by averaging the output feature vectors from the final layers of the encoder and decoder, respectively. It is worth noting that we obtain the image and text representations by averaging the projected representations from the image encoder and text decoder of a vision-language model, respectively. Finally, we define our contrastive retrieval loss as follows:\n$$L_{crl}(h_p, s_p, H) = -\\log \\frac{\\sigma(h_p, s_p)}{\\sum_{h_i \\in H} \\sigma(h_i, s_p)}$$.\nBased on the proposed method, we aim to train models to consider unique components that lead to specific inference results, rather than common components that lead to more generic inference results shared by multiple images.\nWe integrate our contrastive retrieval loss with the original language modeling loss of visual commonsense generation (Park et al., 2020). For a given image $h \\in H$, we can identify corresponding ground-truth inference $s = \\{w_1, w_2, ..., w_k\\}$ as a sequence of tokens. Then, the original loss is defined as follows:\n$$L_{org}(h, s) = -\\sum_{i=1}^{|s|} \\log P(w_i | w_{<i}, h)$$.\nThe final objective function is as follows:\n$$L(h_p, s_p, H) = L_{org}(h_p, s_p) + \\lambda L_{crl}(h_p, s_p, H)$$,\nwhere $\\lambda$ is a non-negative hyper-parameter for balancing the objective functions. It is noteworthy that we randomly select one inference for the loss calculation if multiple inferences are associated with an image $h_p$. We construct $H$ for each example in a batch and if a uniquely related commonsense description does not exist, we exclude the contrastive loss."}, {"title": "4 Experiments", "content": "In this section, we demonstrate the effectiveness of DIVE by comparing it with existing methods."}, {"title": "4.1 Experimental Setup", "content": "Dataset. We conduct the experiments on the VCG dataset (Park et al., 2020), which is a large-scale visual commonsense graph. We train models with DIVE on the filtered VCG training set. In addition, we evaluate models on the original VCG validation set, the unique VCG validation set, and the novel VCG validation set. The unique VCG validation set is a subset of the original set that consists of inferences with commonsense descriptions that appear once in the original set. The novel VCG validation set is a subset of the original set that consists of inferences with commonsense descriptions that do not appear in the training set. We expect that the unique and novel subsets predominantly contain specific inferences, since they exclude duplicate examples. For both subsets, we discard the inferences of images with fewer than five commonsense descriptions.\nBaselines. We mainly compare our results with those of VisualCOMET (Park et al., 2020), KM-BART (Xing et al., 2021), and BLIP (Li et al., 2022b). VisualCOMET (Park et al., 2020) extends a pre-trained GPT-2 model (Radford et al., 2019) with 126 million parameters to incorporate visual and textual information. KM-BART is based on a pre-trained BART-base model (Lewis et al., 2020) with 141 million parameters and conducts additional pre-training with image captioning data. BLIP (Li et al., 2022b) is a pre-trained generative vision-language transformer with 247 million parameters. All the baselines are fine-tuned on VCG following the fine-tuning settings specified in their original papers.\nImplementation details. We fine-tune both vision-language BART and BLIP on VCG using"}, {"title": "4.2 Main Results", "content": "We first evaluate the descriptiveness and diversity in visual commonsense generation of vision-language models. In Table 1, we compare our DIVE models with state-of-the-art visual commonsense generation models on the original VCG validation set. We observe that our DIVE models outperform the baselines in all evaluation metrics for descriptiveness and diversity. Particularly, DIVE models reach human-level descriptiveness and diversity on the original VCG validation set. These results confirm that our DIVE framework effectively augments vision-language models with the capability for generating descriptive and diverse commonsense inferences, showing significant improvements over existing vision-language models. In addition, as shown in Tables 2 and 3, our DIVE models consistently outperform the baselines in terms of descriptiveness and diversity on the unique and novel VCG validation sets.\nTo evaluate the quality of visual commonsense generation, we compare our DIVE models with the"}, {"title": "4.3 Human Evaluation Results", "content": "We present human judgments on the plausibility, descriptiveness, and diversity of the generated inferences. We conduct human evaluations with workers from Amazon Mechanical Turk.\nFollowing Xing et al. (2021), we generate 450 inferences with three different models, including DIVEBART, VisualCOMET (Park et al., 2020), and KM-BART (Xing et al., 2021), for pair-wise comparison. We generate five pairs of inferences for each inference type. For each example, we construct pairs of inferences generated by DIVE and one of the baselines, and sets of all inferences generated by each model. Then, we ask three annotators to choose a better inference based on the following three metrics: 1) plausible: which inference seems more plausible and reasonable to an image, 2) descriptive: which inference explains the image more informatively and specifically, and 3) diverse: which set of inferences seems more diverse in meanings and expressions.\nThe results are shown in Table 7. For plausibility, our DIVEBART model outperforms the state-of-"}, {"title": "5 Analysis", "content": "In this section, we conduct analyses of the components and results of DIVEBART."}, {"title": "5.1 Ablation Study", "content": "To better understand the contributions of each component in DIVE to performance improvements, we conduct ablation studies on generic inference filtering and contrastive retrieval learning. The results are shown in Table 8. We find that training models without our filtering method results in a significant degradation in the R@1 and Unique scores, which highlights that balancing the distribution of the visual commonsense resources is crucial for generating descriptive and diverse inferences. In addition, our contrastive retrieval learning method universally improves the three metrics when combined with the filtering method, showing its contributions to the improvements in generation quality, descriptiveness and diversity. Nevertheless, the contrastive retrieval learning method degrades the performance when applied alone. We speculate that this is because a wide range of images can be frequently sampled as negative ones if generic inferences are not eliminated, failing to meet the motivation of the method that trains models to recognize the detailed differences among similar images. This observation also shows that the components of DIVE are complementary to each other for the performance improvements."}, {"title": "5.2 Informativeness of Inferences", "content": "We analyze the amount of information contained in generated inferences by measuring word-level entropy (Mou et al., 2016). Figure 4 shows the distribution of generated inference on the original VCG validation set in relation to word-level entropy. The y-axis represents the ratio of the number of generated inferences for the corresponding interval of entropy in the x-axis. Each value k in the x-axis represents the interval between k - 1.0 and k + 1.0. We can observe that DIVE generates inferences with relatively high entropy, which implies the improvements in their informativeness."}, {"title": "5.3 Qualitative Analysis", "content": "We present qualitative examples of DIVE compared to baselines in Figure 5. It demonstrates that DIVE can generate more descriptive and diverse inferences compared to the baselines. As can be observed from the figure, DIVE effectively generates unique and novel inferences applicable to the given situations, utilizing specific expressions related to the image, such as \u201cdisturb\u201d, \u201cthirst\", etc. In contrast, the baselines frequently generate simple, generic, and seen descriptions. Interestingly, DIVE sometimes generates more descriptive and diverse inferences compared to human annotations like the Intent inferences in Figure 5 (a). This further implies that existing automatic evaluation can underestimate the scores of DIVE due to lexical differences from human annotations.\nDespite the promising results, DIVE generates some irrelevant inferences to the context even if the detailed information is explicitly given. This shows that vision language models still lack some commonsense knowledge and reasoning ability to accurately reason over recognized context."}, {"title": "6 Conclusion", "content": "We have presented DIVE to improve the descriptiveness and diversity of vision-language models in visual commonsense generation. We have proposed a generic inference filtering method to balance the skewed distribution of visual commonsense resources, based on the frequency and semantic concentration of images. In addition, we have proposed a contrastive retrieval learning method to promote the descriptiveness and diversity of vision-language models, by leveraging the structural information from visual commonsense graphs. Through extensive experiments on VCG, we have verified that DIVE is capable of generating descriptive and diverse inferences about visual scenes, significantly outperforming state-of-the-art visual commonsense generation models. Particularly, our human evaluations have confirmed that DIVE indeed captures specific visual information, leading to improvements in plausibility, descriptiveness, and diversity."}, {"title": "Limitations", "content": "While we have demonstrated that DIVE effectively improves the descriptiveness and diversity of generated inferences, there are some limitations that present promising avenues for future research. First, our filtering method leads to a loss of training data, which can limit the capabilities of vision-language models. We plan to investigate data augmentation methods for visual commonsense generation, such as utilizing external data (Xing et al., 2021) or data generated by foundation models (West et al., 2022). In addition, as the first work that explores the descriptiveness and diversity in visual commonsense generation, we have focused on the evaluations on VCG, which is the most representative dataset for visual commonsense generation. Nevertheless, the ability to capture detailed information from an image and to generate descriptive and diverse inferences would be significantly beneficial to various visual reasoning and generation tasks (Hessel et al., 2022; Zang et al., 2021; You et al., 2022), as well as reasoning tasks over audio and video (Yu et al., 2022; Li et al., 2022a). We thus plan to investigate the efficacy of DIVE on more diverse tasks and modalities. Finally, in Section 5.3, we have observed that DIVE occasionally generates irrelevant inferences to the given context possibly due to the lack of commonsense knowledge and reasoning ability. Future work could focus on enhancing the commonsense knowledge and reasoning ability in vision-language models (Li et al., 2022c; Han et al., 2023)."}]}