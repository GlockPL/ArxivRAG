{"title": "LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation", "authors": ["Eunsu Kim", "Juyoung Suk", "Seungone Kim", "Niklas Muennighoff", "Dongkwan Kim", "Alice Oh"], "abstract": "We introduce a novel evaluation paradigm for large language models (LLMs), LLM-as-an-Interviewer. This approach consists of a two-stage process designed to assess the true capabilities of LLMs: first, modifying benchmark datasets to generate initial queries, and second, interacting with the LLM through feedback and follow-up questions. In comparison to existing evaluation methods such as LLM-as-a-Judge, our framework addresses several limitations, including data contamination, verbosity bias, and self-enhancement bias. Additionally, we show that our multi-turn evaluation process provides valuable insights into the LLM's performance in real-world scenarios, including its adaptability to feedback and its ability to handle follow-up questions, including clarification or requests for additional knowledge. Finally, we propose the Interview Report, which offers a comprehensive reflection of an LLM's strengths and weaknesses, illustrated with specific examples from the interview process. This report delivers a snapshot of the LLM's capabilities, providing a detailed picture of its practical performance.", "sections": [{"title": "1 Introduction", "content": "With large language models (LLMs) becoming increasingly proficient in generating fluent free-form responses, it has become crucial to properly assess their capabilities and limitations (Liang et al., 2022; Chang et al., 2024). Recently, LLM-as-a-judge has emerged as a promising framework for automatic free-form response evaluation. Compared to traditional lexical matching-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) or embedding-based metrics (e.g., BERTScore (Zhang et al., 2019)), previous works on LLM-as-a-Judge have reported higher correlations with human judgments (Chiang and Lee, 2023; Zheng et al., 2023a; Dubois et al., 2024b). Despite its potential, the LLM-as-a-Judge framework faces several practical limitations that hinder its widespread adoption, primarily due to its static nature (Li et al., 2024a; Gu et al., 2024). First, using a fixed set of test inputs raises concerns about data contamination (Sainz et al., 2023; Zhou et al., 2023a; Oren et al., 2023b), where the evaluated models may achieve high scores on instances encountered during training. Second, single-turn interactions fail to thoroughly probe a model's true comprehension (Li et al., 2019; Wang et al., 2023; Kwan et al., 2024). For instance, the judge model can be confounded by superficial factors (e.g., preferring long responses) or exhibit high variance across multiple runs.\nIn this study, we propose a novel evaluation paradigm for LLMs, LLM-as-an-Interviewer. This approach is inspired by human interviews, where the process begins with common questions, but follow-up or different types of questions are often posed based on the individual's responses. We redesign this dynamic approach for LLM evaluation in two stages: 1) generating an initial interview question by adapting benchmark questions to create diverse challenges, and 2) providing tailored feedback and follow-up questions to better understand the models' unique characteristics."}, {"title": "2 Related Works", "content": "2.1 LLM-based Evaluation\nLLM-as-a-Judge is widely used for text evaluation, offering more human-aligned assessments than traditional methods like BLEU, ROUGE, and METEOR, while addressing the scalability and consistency limitations of human-based approaches (Gu et al., 2024). However, the LLM-as-a-Judge faces reliability issues, including self-enhancement bias, sensitivity to response length and order, and low self-consistency. To improve reliability, Li et al. (2023b) and Dubois et al. (2024b) aim to reduce biases related to ordering and length by adjusting positions or win rates. Additionally, several works, such as Wang et al. (2024b); Kim et al. (2024), train open-sourced critique models to address the inherent inconsistencies of closed LLMs, ensuring reproducibility.\nLLM-based evaluation is also used to simulate multi-turn interactions between evaluators and evaluated models in certain tasks. For instance, Wang et al. (2023) evaluates LLMs by simulating interactions between the user and the model, focusing on scenarios where the user provides feedback in tool-usage contexts. Additionally, Yu et al. (2024) simulate knowledge-focused dialogues in multi-round interactions to mitigate data contamination during evaluation. Li et al. (2023a) share a similar motivation to ours, as they are also inspired by human interviews. However, their approach is tailored to Conversational Question Answering tasks and focuses on generating new questions during evaluation.\nLLM-as-an-Interviewer is a generalized benchmark that mimics a multi-turn interview process to better assess a model's capabilities. Our framework enables the simulation of diverse interactions, such as giving feedback and asking follow-up questions, which align much more closely with how humans use models in real scenarios. Additionally, the increased number of interactions in our"}, {"title": "2.2 Data contamination in LLMs", "content": "LLMs are trained on large corpora that can be contaminated with benchmark data (Dodge et al., 2021; Soldaini et al., 2024), which can undermine the reliability of benchmarks (Zhou et al., 2023b). This has sparked growing interest in methods for contamination detection(Magar and Schwartz, 2022). Shi et al. (2024) and Oren et al. (2023a) propose methods to estimate the likelihood of text in LLM pretraining data using only API access, as is common for frontier models(OpenAI et al., 2024). This has led to more research on contamination (Yax et al., 2024; Deng et al., 2024; Jiang et al., 2024; Dekoninck et al., 2024b; Xu et al., 2024), including methods to evade detection by training on rephrased benchmark data (Dekoninck et al., 2024a).\nOverall, efforts to prevent contamination through detection have had limited success, prompting researchers to focus on contamination mitigation in benchmarks. This typically involves dynamic evaluation in one of two ways: (1) The evaluation data is dynamic, such as crawling new instances live (White et al., 2024; Jain et al., 2024) or generating new instances on the fly (Wang et al., 2024a; Li et al., 2024b); (2) The evaluator is dynamic, as in AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al., 2023b), where the same samples are reused, but evaluation is based on a dynamic LLM rating completions instead of ground-truth solutions. Some benchmarks, like the Chatbot Arena (Chiang et al., 2024), involve both dynamic data and evaluators, with users creating and rating responses on the fly.\nLLM-as-an-Interviewer is a benchmark that mitigates contamination through both dynamic evaluation data and a dynamic evaluator. The data is dynamic, as the LLM interviewer can rephrase questions and propose novel follow-ups. The evaluated model provides answers and shows its solving process, which the LLM interviewer then evaluates, making the evaluator dynamic as well. This setup makes it difficult to cheat via contamination. Additionally, LLM-as-an-Interviewer can run fully offline in minutes, enabling quick, low-cost evaluation, unlike Chatbot Arena, which requires human access to the model and can take days or weeks to yield statistically significant results at a higher cost."}, {"title": "3 LLM-AS-AN-INTERVIEWER", "content": "We introduce the LLM-as-an-Interviewer framework,2 simulating an interview scenario where one LLM dynamically evaluates another. In this section, we describe (1) the overall interview process (\u00a7 3.1) and (2) the Interview Report, which presents the outputs of our framework (S 3.2)."}, {"title": "3.1 Interview Process", "content": "The objective of the interview process is to evaluate LLMs by simulating various user-model interactions, similar to human interviews. Unlike typical LLM evaluations that focus on static, single-turn problems, human interviewers often modify questions, provide hints for incorrect answers, or ask follow-up questions based on the interviewee's responses to better assess their true abilities.\nInspired by these real interview scenarios, we divide the LLM evaluation process into two main stages: the Pre-Interview stage (preparing questions) and the Main Interview stage (asking questions). Algorithm 1 shows algorithmic implementations of the interview process.\n3.1.1 Pre-Interview Stage\nThis stage prepares seed questions for interviewing models by modifying queries from existing benchmark datasets, helping to avoid data contamination in LLM evaluations. It mirrors how human interviewers often tweak existing questions or introduce new challenges, especially since previous"}, {"title": "3.1.2 Main-Interview Stage", "content": "We divide the main interview stage into three key states, drawing on real interview processes (e.g., Google coding interviews) and the ability taxonomy from MT-Bench 101, which is based on real-world LLM application scenarios (Bai et al., 2024).\nProblem Solving The state in which the seed question is provided. The interviewee (model) may request clarification or additional information as needed.\nFeedback & Revising The state in which the interviewee solves the seed question and revises the solution based on the interviewer's feedback. This state closely resembles the most basic interaction scenario when users engage with the model in practice (Wang et al., 2023).\nFollow-up Question Answering The state where additional questions related to the solution or problem are posed. Follow-up questions assess the model's additional abilities or knowledge not covered by the initial question. Therefore, the type of follow-up question may be selected and customized based on the nature of the task. For example, in a reasoning task, the interviewer can request a rationale or clarification of previous responses. In an explanation task, the interviewer may ask whether the model knows additional facts that are not addressed earlier. A list of possible types of follow-up questions is provided in the Appendix A.2."}, {"title": "3.2 Interview-Report", "content": "The interview report, generated as a result of the interview process, includes 1) Performance Scores, 2) Error Analysis and Examples, and 3) Comprehensive Summary. The real examples of Interview-Report are in Appendix A.\nPerformance Scores We obtain three performance metrics through the interview process, which progresses across three main stages:\n1. Problem Solving Ability \u2013 measures how effectively the model solves the given problem at its n-th interaction with interviewer. Problem Solving Ability $(Score_{seed})$ at n-th interaction is defined as:\n$Score_{seed}@n = \\frac{\\sum_{p \\in P} score_n(p)}{P}$ ,\nwhere P is a set of all problems and $score_n(p)$ is the score of problem p at n-th interaction. If the model correctly solves an instance at n=i-1 with no further interactions, we set $score_{n=i} = score_{i-1}$.\n2. Response adaptability - represents how well the model adapts and revises its responses based on the interviewer's feedback or additional prompts. We compute this as the difference between the score of the final iteration, N, and the score of the first iteration:\nAdapt. = $Score_{seed}@N$ - $Score_{seed}@1$\n3. Follow-up Question Handling - assesses how well the model answers follow-up questions. We define Scorefollow as:\nScorefollow  = $\\frac{\\sum_{q \\in Q} score(q)}{|Q|}$\nwhere score(q) is the score of follow-up question q and Q is the set of follow-up questions.\nThese metrics reflect abilities crucial in real-world interactions with LLMs, where responding effectively to feedback and follow-up questions significantly influences user satisfaction beyond just answering the initial query. Our three scores capture these core skills, offering a clearer picture of LLM performance in real world scenarios.\nError Analysis and Examples A detailed breakdown of error types across multiple attempts, along with illustrative examples. These highlight common failure reasons and examples in the models, offering insights into their limitations and areas for improvement.\nComprehensive Summary A summary of the model's interview performance across all samples, offering general insights into its behavior during the interview process. It highlights what the model can and cannot do."}, {"title": "3.3 Generalizability of LLM-as-an-Interviewer Framework", "content": "In our experiment, we use MATH (Hendrycks et al., 2021) to represent tasks with deterministic answers"}, {"title": "4 Reliability of LLM-AS-AN-INTERVIEWER", "content": "4.1 Does LLM-AS-AN-INTERVIEWER Function as Intended?\nWe evaluate whether the current model can effectively perform the role of an LLM-as-an-interviewer. Specifically, four authors review each pair of responses\u2014one from the LLM-as-an-Interviewer and one from the interviewee\u2014and annotate whether the interviewer's response is appropriate. In this study, we evaluate GPT-4o, Llama-3.1-70B, and Llama-3.1-8B as interviewers, while using GPT-4o, GPT-3.5, Llama-3.1-70B and Llama-3.1-8B as interviewees to simulate the interview with models of varying capabilities.\nWe assess 40-150 samples for each stage and report the percentage of appropriate responses for each stage, along with the overall appropriate response rate. As shown in Table 1, GPT-4o consistently perform the closest to the intended behavior,"}, {"title": "4.2 Does LLM-AS-AN-INTERVIEWER Function Robust?", "content": "We assess the robustness of our framework across multiple interactions by repeating the experiment five times for each setting. Specifically, We calculate the standard deviation of these five runs for each combination of different interviewer temperatures(0 or 1) and different interviewee models. The interviewee's temperature remains fixed at 1. We use GPT-4o as the interviewer (the best-performing model in Table 1) and GPT-3.5, Llama-3.1-70b, and Llama-3.1-8b as the interviewees."}, {"title": "4.3 Does LLM-AS-AN-INTERVIEWER Possess Bias?", "content": "We evaluate whether LLM-as-an-Interviewer exhibits two biases-Verbosity and Self-enhancement bias-that are common in model-based evaluation (Zheng et al., 2023b).\nVerbosity Bias To analyze whether the LLM-as-an-Interviewer framework prefers longer answers, we analyze the correlation between answer length and score for each number of interactions with Interviewer. We use the long-form generation task, Depth-QA, for this analysis. As the Interviwer, we use GPT-4o and as the interviewees, we use seven different models.Figures 3 show that as the number of interaction increases, the linear correlation (r) between length and score weakens. Without any interaction with Interviewer (same as the LLM-as-a-Judge setting), we observe a statistically significant linear correlation among answer length and its score(r=.371, p<0.05). However, as the interactions progress, the p-value increases and the r-value decreases. This suggests that the interactions in our interview process help reduce verbosity bias, alongside the length control mechanism that adjusts win rates or scores (Dubois et al., 2024a).\nSelf-Enhancement Bias To examine whether interviewer models favor their own responses, we analyze score trends using GPT-4o, Llama-3.1-70B, and Llama-3.1-8B as both interviewers and interviewees. As shown in Figure 4, no significant self-enhancement bias is observed. Both Llama-3.1-8B and GPT-4o show little to no self-enhancement bias, generally giving themselves lower scores compared to other interviewers. Llama-3.1-70B consistently gives itself higher scores compared to evaluations by GPT-4o or Llama-3.1-8B, but this does not change its overall ranking. Interestingly, this trend weakens as interactions progress, with the score gap between GPT-4o (the top model) and Llama-3.1-70B increasing. A similar pattern is observed with the DepthQA dataset as discussed in Appendix B.1."}, {"title": "4.4 Effectiveness of Interview Report", "content": "A human study evaluating the effectiveness of the Interview Report will be conducted as future work."}, {"title": "5 Can LLM-AS-AN-INTERVIEWER Mitigate Data Contamination Issue?", "content": "This section demonstrates the efficiency of the LLM-as-an-Interviewer in addressing the dataset contamination issue through modification strategy during the pre-interview stage.\n5.1 Experimental Setting\nWe establish various settings to simulate real-world scenarios related to the model's training dataset. Specifically, we use two open-source models, OLMOE (Muennighoff et al., 2024) and Zephyr-7b (Tunstall et al., 2023), and fine-tune on different training sets. These settings allow us to evaluate both Uncontaminated Models\u2014which do not have access to the test set during training\u2014and Contaminated Models\u2014which are trained with exposure to the test set.\nUncontaminated Model Training As shown in the setting column of Table 3, we train uncontaminated models using three distinct configurations:\n1 Models trained exclusively on the train set of the target dataset (In-Distribution, TrainID).\n2 Models trained on both the TrainID and additional datasets from the same domain (Out-Of-Distribution, TrainOOD).\n3 Models trained solely on TrainOOD.\nContaminated Model Training For contaminated models, we employ four different configurations:\n4 Models trained exclusively on the test set of the target dataset (In-Distribution, TestID).\n5 Models trained on both the TestID and TrainID sets.\n6 Models trained on the TestID combined with an instruction-tuning (Instruct) dataset.\n7 Models trained on both TestID and TrainOOD.\nDataset We use the in-distribution test set(TestID) when testing the model, and MATH and DepthQA are used as in-distribution datasets. Since DepthQA is not pre-divided into train and test sets, we manually split it into 851 instances for the DepthQAtrain set and 848 instances for the DepthQAtest set. For other datasets, we"}, {"title": "5.2 Result", "content": "Table 3 presents the effectiveness of the modifying strategy in mitigating data contamination. The tables compare accuracy for both the original queries (Judge) and modified queries after implementing the modifying strategy(Interview.). Settings [1, 2, 3] represent uncontaminated models, serving as a proxy for the model's true ability on the task. The goal of the strategy is to bring the performance of contaminated models closer to these uncontaminated models."}, {"title": "6 Can LLM-AS-AN-INTERVIEWER Simulate Long-term Interactions Between Users and LMS?", "content": "In this section, we simulate the actual interview process using GPT-4o as the interviewer and various"}, {"title": "6.1 Feedback & Revising", "content": "The problem-solving section of Table 4 reflects the models' ability to incorporate feedback and revise their responses. Across all models, performance improves when feedback is provided for incorrect answers (Accseed@2, Accseed@3), compared to the initial accuracy (Accseed@1). Interestingly, as the models receive more feedback and iteratively revise their answers, the standard deviation (std) of scores across models decreases. This is because models that initially perform poorly (e.g., DeepSeek, GPT-3.5) show substantial improvement through interaction.\nAdditionally, there are cases where the rankings of models shift with repeated interactions. For example, while the Llama-3.1-70b model initially scores lower than Llama-3.1-8b or Qwen models in Accseed@1, its ranking improves as interactions progress. This result underscores the importance of our interactive evaluation framework, as conventional static evaluations (Accseed@1) fail to capture model performance in real-world scenarios where users iteratively interact with models to refine answers. Similar results are observed in DepthQA; however, as all models exhibit generally high performance, the trends are less pronounced compared to the MATH."}, {"title": "6.2 Followup Question Answering", "content": "The follow-up column of Table 4 presents the models' performance on follow-up questions in MATH. Overall, the models exhibit a trend of high accuracy on follow-up questions for problems they solved correctly (success column) and low accuracy for those they answered incorrectly(fail column). The examples of follow-up questions in each task can be found in Appendix D.2.\nMATH For MATH, the type of follow-up question differs depending on whether the model answers correctly or incorrectly. When the model provides the correct answer, follow-up questions typically ask for rationale questions, confirmation on the model's previous response (e.g., How did you determine Jasmine's rate of water consumption per mile?). In contrast, when the model answers incorrectly, follow-up questions ask clarification questions to understand the model's failure reasons. For example, the Interviewer either asks the model to reconsider its interpretation of the question (e.g., Could you explain what the problem means when it says \"Today a father's age is five times his son's age?) or probe the model's understanding of relevant concepts (e.g., What does it mean for a and b to be consecutive integers?)\nDepthQA The follow-up question for DepthQA typically targets facts mentioned in the reference solution but missing from the model's previous response(Additional Information Type). In addition, the model also asks for further examples or requests additional explanations in follow-up question. This approach partially addresses a key limitation of traditional factuality evaluation, which only evaluates the facts provided by the model (i.e., precision). By probing further with additional questions, we can assess the model's broader factuality, identifying missing or incomplete information. This is also similar to how users interact with models, requesting additional information when the model's answer is inadequate."}, {"title": "7 Conclusion", "content": "In this paper, we introduce LLM-as-an-Interviewer, a novel approach for evaluating LLMs through an interview process. Our analysis shows that our framework addresses common limitations found in current static evaluation methods. Specifically, it is robust to verbosity and self-enhancement bias, which are typical in the LLM-as-a-judge approach. Additionally, by dynamically modifying questions from existing benchmarks, our method effectively mitigates data contamination issues\u2014another significant challenge in LLM evaluation. Our multi-turn evaluation framework reveals various real-world abilities of LLMs, such as revising previous answers from feedback or responding to follow-up questions. We expect LLM-as-an-Interviewer to bring about a paradigm shift in the field of LLM evaluation."}, {"title": "A LLM-as-an-Interviewer", "content": "A.1 Implementation Details\nBuilding upon the interview framework described in \u00a7 3.1, this section explains our interview system implementation. The system operates as a two-agent dialogue with additional complexity in controlling the interviewer to conduct evaluations accurately and efficiently. Controlling interviewer is done in two stages: (1) keeping the interview state in track, and (2) using pre-written prompts for each state. While we implemented the interview process specifically for MATH and STEM domains, the framework can be easily adapted to other domains with minor modifications.\nPre-Interview: Interviewer model generates modified versions of seed questions through unclarification or paraphrasing\nFor Arithmetic Reasoning tasks (MATH), LLM Interviewer replaces numerical values in a query with unknown variables (e.g., x) or omits specific details. For example, \"If there were 5 chocolates and you ate 1, how many are left?\" becomes \"If there were 5 chocolates and you ate x, how many are left?\". This maintains the same reasoning level while preventing the direct application of memorized solutions, as seen in methods like those of Mirzadeh et al. (2024). For factuality-based long-form tasks (depth-QA), new problems are created by referencing ground truth solutions and questions from the original dataset to ensure relevance and diversity. Examples of these strategies are in Table 9.\nMain Interview: Consists of two primary components:\n1. Response Evaluation: Interviewer assesses interviewee responses using domain-specific criteria (mathematical correctness for MATH; factual accuracy and completeness for STEM) and identifies error sources.\n2. Follow-up Question Generation: Based on evaluation results, interviewer either probes deeper concepts (for correct responses) or provides targeted feedback to reveal misconceptions (for incorrect responses).\nInterview Report: The interview report, based on the detailed interaction logs from the Main Interview stage, is shown to the user at the end of the interview. This content for the report is explained"}, {"title": "A.2 Type of Follow-up questions", "content": "Rationale: A question aimed at understanding the reasoning or justification behind a decision, belief, or action.\nExample: What was your reasoning for choosing this particular approach to solve the problem?\nElaboration: A question requesting a more detailed explanation or further exploration of a topic or idea.\nExample: Could you elaborate on how this solution works in practice?\nClarification: A question designed to resolve ambiguity or ensure a statement or concept is fully understood.\nExample: When you refer to 'sustainability,' are you focusing on environmental or economic aspects?\nAdditional Information: A question seeking further details, background, or supplementary content beyond what has already been provided.\nExample: Are there any other factors we should consider before finalizing the plan?"}, {"title": "A.3 Adapting to New Tasks", "content": "To adapt a new task, one can modify the Main Interview process, specifically the response evaluation criteria and follow-up question generation strategy. This can be done by rewriting the pre-written prompts. Here are examples of adaptations for different domains:\nProgramming Interviews:\nResponse Evaluation: Code correctness, efficiency, and style\nFollow-up Generation: Edge cases, optimization opportunities, or alternative implementations\nLanguage Proficiency:\nResponse Evaluation: Grammar accuracy, vocabulary usage, and fluency\nFollow-up Generation: Complex linguistic scenarios or cultural context questions"}, {"title": "B Reliability of LLM-as-an-Interviewer", "content": "B.1 Self-Enhancement Bias\nFigure 5 shows the trend of self-enhancement bias in DepthQA. Similar to MATH(Figure 4), no significant self-enhancement bias is observed except for Llama-3.1-70b. Figure 6 illustrates the trends in MATH and DepthQA for follow-up questions. In MATH, as in previous results, Llama-3.1-70b consistently gives itself higher scores when grading its own responses compared to other models. However, Llama-3.1-70b also tends to give higher scores when grading other models, making it difficult to determine whether this is due to self-enhancement bias or because it generates easier follow-up questions compared to other models. In DepthQA, GPT-4 shows a similar pattern, giving itself higher scores when grading its own responses, but also assigning higher scores to other models when acting as the interviewer."}, {"title": "D Discussion", "content": "D.1 Models' Behavior as an Interviewee\nTable 10 shows the performance of 7 different models in DepthQA.\n\nD.2 Followup Question\nIn the following example, the follow-up question targets the n-th roots mentioned in the reference solution but missing from the model's previous response. This type of follow-up question is central to DepthQA. It partially address a key limitation of traditional factuality scoring, which only evaluates the facts the model provides (i.e., precision). By probing further with additional questions, we can assess the model's broader factuality, capturing missing or incomplete information."}, {"title": "DepthQA Follow-up Question Example", "content": "User: What is the significance of the roots of unity in mathematics and physics?\nSystem: The roots of unity are important in mathematics and physics because they have connections to many different areas of study. In mathematics, they are used in algebra, number theory, and complex analysis. In physics, they appear in wave functions, quantum mechanics, and signal processing. The roots of unity also have applications in computer science, cryptography, and engineering. Overall, they play a key role in understanding the behavior of periodic functions and symmetries in various mathematical and physical systems.\nFollow-up Question\nUser: Can you provide an example of determining the 6th roots of unity and specifying their arguments in radians?\nReference Solution\nThe roots of unity refer to the complex solutions of the equation $z^n = 1$, where n is a positive integer. These roots are regularly spaced around the circumference of the unit circle in the complex plane and hence, are of significant importance in many areas of mathematics and physics.\nThe term \"unity\" signifies the number 1, and \"roots\" refer to the solutions of an equation. So, the roots of unity are essentially the solutions to the equation that equal 1 when raised to the power n.\nEach n-th root of unity is given by the formula: $z = cos (\\frac{2\\pi k}{n}) + i sin (\\frac{2\\pi k}{n})$ for k = 0, 1, . . ., n \u2212 1. Here, cos and sin are the standard trigonometric functions, i refers to the imaginary unit, and is the mathematical constant Pi (approximately equal to 3.14).\nLet's say if you wanted to determine the 4th roots of unity, (omitted)"}]}