{"title": "LT-DARTS: An Architectural Approach to Enhance Deep Long-Tailed Learning", "authors": ["Yuhan Pan", "Yanan Sun", "Wei Gong"], "abstract": "Deep long-tailed recognition has been widely studied to address the issue of imbalanced data distributions in real-world scenarios. However, there has been insufficient focus on the design of neural architectures, despite empirical evidence suggesting that architecture can significantly impact performance. In this paper, we attempt to mitigate long-tailed issues through architectural improvements. To simplify the design process, we utilize Differential Architecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTS methods struggle to perform well in long-tailed scenarios. To tackle this challenge, we introduce Long-Tailed Differential Architecture Search (LT-DARTS). Specifically, we conduct extensive experiments to explore architectural components that demonstrate better performance on long-tailed data and propose a new search space based on our observations. This ensures that the architecture obtained through our search process incorporates superior components. Additionally, we propose replacing the learnable linear classifier with an Equiangular Tight Frame (ETF) classifier to further enhance our method. This classifier effectively alleviates the biased search process and prevents performance collapse. Extensive experimental evaluations demonstrate that our approach consistently improves upon existing methods from an orthogonal perspective and achieves state-of-the-art results with simple enhancements.", "sections": [{"title": "Introduction", "content": "Deep neural networks have demonstrated remarkable performance in image recognition tasks, yet they are predominantly utilized in settings where the data adhere to a balanced distribution. In reality, data distributions in many practical scenarios are long-tailed (Buda, Maki, and Mazurowski 2017), characterized by a few head classes with abundant samples and many tail classes with limited samples. It has been proved that conventional deep learning methods tend to exhibit sub-optimal performance in such a context. To address this challenge, the community has proposed numerous approaches including data augmentation (Li et al. 2021; Hong et al. 2022; Ahn, Ko, and Yun 2023), class re-sampling strategies (Liu et al. 2021; Bai et al. 2023), decoupling learning (Kang et al. 2019; Zhou et al. 2023) and so on. Nevertheless, most existing methods pay insufficient attention to neural architecture design, which is crucial for deep models to achieve optimal performance.\nIt is widely recognized that meticulous architectural design contributes to improved performance on balanced data distribution, and this consensus unsurprisingly extends to long-tailed data distribution. As depicted in Fig. 1, we conduct extensive training on several architectures and report their parameter sizes and performance. It can be observed that different backbones lead to various performances and an optimal architecture may achieve superior performance even with fewer parameters, which validates the feasibility of approaching the long-tailed problem from an architectural perspective. In terms of efficiently designing an architecture suited for long-tailed data, we leverage the power of differentiable architecture search (DARTS).\nDARTS (Liu, Simonyan, and Yang 2018) is an efficient neural architecture search method with minimal computational resource requirements. On balanced datasets, the architectures it discovers perform on par with those designed by experts. Unfortunately, as shown in Fig 2, directly applying DARTS on the long-tailed dataset fails to achieve satisfactory results. Even the latest improved version B-DARTS (Ye et al. 2022) underperforms compared to expert-designed networks, which deviation contrasts with the performance on balanced data distribution. Moreover, it is noteworthy that the integration of the re-balancing loss"}, {"title": "Background", "content": "DARTS constructs the neural network by stacking cells with the same architecture, where a cell is defined as a directed acyclic graph. The goal of search procss is to identify the operations (edges) utilized for connecting diverse features (nodes). To this end, DARTS makes use of continuous relaxation in the following form:\n$\\overline{o}(x) = \\sum_{o \\in O} \\frac{exp(a_o)}{\\sum_{o' \\in O} exp(a_{o'})}o(x)$\nwhere O is the search space and o(\u00b7) is the different operations selected from O. $\\overline{o}(\u00b7)$ is the mixed operation, and it will be replaced by the $o(\u00b7) = argmax_{o \\in O}a_o$ at the end of the search. a is a vector of dimension |O|, named architecture parameter, denotes the operation mixing weights. It can be optimized through the following steps:\n$\\min_{\\alpha} L_{val}(w^*(\\alpha), \\alpha)$\ns.t. $w^*(\\alpha) = \\underset{w}{\\operatorname{argmin}} L_{train}(w, \\alpha)$\nwhere w denotes the network parameters and $w^*(\\alpha)$ is the optimal network parameters w under the current architecture \u03b1. $L_{train}$ and $L_{val}$ denote the training and validation losses, respectively. The search process is based on gradient optimization of \u03b1 and w to find the optimal cell architecture."}, {"title": "Observation", "content": "Upon reviewing existing DARTS methods (Xu et al. 2019; Movahedi et al. 2022; Ye et al. 2023), we observe that while they make considerable progress in optimizing search strategies, they largely overlook improvements to the search space. This oversight likely results in their shortcomings on long-tailed data. Specifically, existing methods use dilated and separable convolutions as candidate operations, but these basic convolution operations may lack sufficient feature extraction capability, ultimately failing to achieve the desired performance. To verify this, we conduct a toy"}, {"title": "Components Exploration", "content": "We break down the entire convolution operation into various components, examining their effects on performance in the long-tailed context separately. As depicted in Fig. 3, our explorations span the examination of topology, the specific"}, {"title": "Operations Design", "content": "Based on these observations, we design two convolutional operations specifically tailored for long-tailed issues: LT-AggConv and LT-HierConv. LT-AggConv employs an aggregated convolution with a pre-activation approach, which uses multiple parallel convolutional operations and combines their outputs. This design benefits long-tailed scenarios by providing more refined feature representations for rare classes. LT-HierConv, on the other hand, uses hierarchical convolution with a post-activation approach, allowing the model to adaptively emphasize relevant features at various hierarchical levels, thereby enhancing classification accuracy. Both operations utilize the ReLU activation function and BatchNorm normalization and are based on a bottleneck architecture. Illustrations of these operations can be found in the appendix."}, {"title": "ETF: Hitting Two Targets with One Arrow", "content": "In this section, we propose using the Equiangular Tight Frame (ETF) classifier to enhance our method, effectively tackling both biased search and performance collapse issues."}, {"title": "Observation: A Biased Search", "content": "Previous studies have identified classifier bias in long-tailed model training (Kang et al. 2019; Nam, Jang, and Lee 2023). Similarly, in the long-tailed setting, we observe a gradual shift in the classifier during the search process. As shown in Fig. 7, on a balanced dataset, inter-class weight norms remain similar and show little change during the search process. In contrast, on long-tailed data, the differences in inter-class norms increase as the search progresses. Additionally, in the final epoch, the angles between weight vectors become more unevenly distributed on long-tailed data compared to balanced data. Such biased norms and angles may lead to a biased search, resulting in architecture favoring the head classes more. To address this issue, we introduce the Equiangular Tight Frame (ETF) classifier."}, {"title": "ETF classifier", "content": "According to the work of Papyan et al., in the terminal phase of neural network training, feature vectors collapse to their class means and the corresponding linear classifier weights converge in the same direction, leading to an equiangular tight frame (ETF). This phenomenon is referred to as neural collapse (NC) (Papyan, Han, and Donoho 2020), with a more detailed explanation provided in the appendix. Based on this, we propose an ETF classifier that ensures all weight vectors have equal norms, with the distances between them maximized and the angles equidistant. Formally, this can be defined as follows:\n$W = \\sqrt{\\frac{c}{d}} E_w CU (I - \\frac{1}{C} 11^T)$\nwhere $W \\in \\mathbb{R}^{d \\times C}$ and C represents the number of classes. $U \\in \\mathbb{R}^{d \\times C}$ is a matrix such that $U^T U = I$ and $I \\in \\mathbb{R}^{C \\times C}$ is the identity matrix. 1 is a C-dimensional vector with all elements equal to 1. $E_w$ is a hyperparameter that controls the magnitude of the weight norm."}, {"title": "Analysis", "content": "Since the ETF classifier has frozen unbiased weight vectors, it is undoubtedly well-suited for addressing the biased search problem caused by long-tailed issues. Additionally, we are surprised to find that it can also solve the performance collapse problem, which manifests as a rapid decline in accuracy during the later stages of DARTS search (Ye et al. 2022). Chen et al. suggest that smaller eigenvalues of the Hessian matrix during the search process can effectively prevent performance collapse (Chen and Hsieh 2020) and this observation explains why the ETF is effective in mitigating performance collapse:"}, {"title": "Theorem 1.", "content": "The ETF classifier can ensure the Hessian matrix has a smaller maximum eigenvalue."}, {"title": "Proof.", "content": "Given the loss L(w(\u03b1), \u03b1), where w(\u03b1) represents weights dependent on architecture parameters \u03b1. Decompose weights w into w = [wb, wc], where wb is the trainable backbone and wc is the fixed classifier. Consider the second-order Taylor expansion of L(w, \u03b1) around (w(\u03b1\u03bf), \u03b1\u03bf):\n$L(w(\\alpha), \\alpha) = L(w(\\alpha_0), \\alpha_0) + (\\alpha - \\alpha_0)^T \\nabla_\\alpha L(w(\\alpha), \\alpha) \\\\\n+ \\frac{1}{2} (\\alpha - \\alpha_0)^T \\nabla^2_\\alpha L(w(\\alpha), \\alpha) (\\alpha - \\alpha_0)$\nwhere the Hessian matrix $\\nabla^2L(w(\\alpha), \\alpha)$ can be abbreviated as H and decomposed as:\n$H = \\begin{bmatrix}\nH_{ww} & H_{w\\alpha}\nH_{\\alpha w} & H_{\\alpha \\alpha}\n\\end{bmatrix}$\nBreaking it further:\n$H = \\begin{bmatrix}\nH_{w_bw_b} & H_{w_bw_c} & H_{w_b\\alpha} \\\\\nH_{w_cw_b} & H_{w_cw_c} & H_{w_c\\alpha} \\\\\nH_{\\alpha w_b} & H_{\\alpha w_c} & H_{\\alpha \\alpha}\n\\end{bmatrix}$\nSince wc is fixed, the derivatives involving wc are zero:\n$H_{ww_c} = H_{w_c w_c} = H_{w_c \\alpha} = H_{\\alpha w_c} = 0$\nThis simplifies H to:\nH_{effective} = \\begin{bmatrix}\nH_{w_bw_b} & H_{w_b\\alpha} \\\\\nH_{\\alpha w_b} & H_{\\alpha \\alpha}\n\\end{bmatrix}$\nLeveraging the properties of block matrices, it follows that the simplification leads to a Hessian matrix with generally smaller maximum eigenvalues:\n$\\lambda_{max}(H_{effective}) < \\lambda_{max}(H)$\nIn summary, the introduction of the ETF classifier not only resolves the issue of biased classifiers through its inherent properties but also effectively mitigates performance collapse, thereby enhancing the performance of the architectures discovered during the search process."}, {"title": "Experiments", "content": "We evaluate the model's performance across four datasets of varying scales: CIFAR10-LT, CIFAR100-LT, Places-LT, and ImageNet-LT. Without loss of generality, we apply various imbalanced factors to CIFAR datasets. We compare our method with some state-of-the-art approaches including LDAM-DRW (Cao et al. 2019), BBN (Zhou et al. 2019), Balanced Softmax (Ren et al. 2020), MiSLAS (Zhong et al. 2020), LADE (Hong et al. 2020), GCL (Li, ming Cheung, and Lu 2021), RSG (Wang et al. 2021), ResLT (Cui et al. 2021), DisAlign (Zhang et al. 2021) and BGP (Wang et al. 2022). For the comparison subjects lacking experimental results, we reproduce them with their respective open-source codes."}, {"title": "Main Results", "content": "Firstly, we compare the performance of the best architecture discovered by LT-DARTS with ResNet-32 on the CIFAR-10/100-LT datasets. In order to further demonstrate the superiority of our architecture, ResNeXt and Res2Net are also included. The experimental results are shown in Table 2, and it is evident that the architectures uncovered by LT-DARTS exhibit heightened performance within a similar model size. Specifically, our architecture shows a 4% to 8% improvement over the basic ResNet baseline. Even when compared to more advanced architectures like ResNeXt and Res2Net, it still achieves at least a 1% accuracy advantage. Experiments across different datasets and imbalanced factors further validate the wide applicability of LT-DARTS.\nAdditionally, we compare our method with state-of-the-art methods in the long-tailed community. Our approach aims to find an optimal architecture, which is orthogonal to most existing long-tailed solutions, making it easy to combine with various methods for further performance enhancement. Table 3 presents our experimental results. In each block of this table (except the last), the first row represents existing SOTA methods with a manually designed ResNet backbone, the second row shows SOTA methods combined with DARTS, and the third row features SOTA methods combined with our approach. As illustrated in the first block, simply replacing the ResNet architecture in the baseline with LT-DARTS achieves superior performance compared to some existing solutions. It is important to note that the network is trained using the cross-entropy loss and does not incorporate any advanced rebalancing techniques.\nWhen examining the results within each block, it's clear that simply combining DARTS with existing long-tailed"}, {"title": "Benefits of the ETF Classifier", "content": "We first demonstrate that the ETF classifier overcomes the weight-biased issue. We present the weight norms and angles of the LT-DARTS classifier in Fig. 9. It shows that the weight norm is maintained at 1, aligning with the Ew = 1 in our method. The angle between any two weight vectors is approximately 96.4 degrees, which also aligns with the conclusion of Eq. (4) (detailed derivations are provided in the appendix). To highlight the role of the ETF classifier in alleviating performance collapse, we intuitively compare the performance differences with and without its use. As shown in Fig 8, unlike the significant performance drop in the later stages of the search observed with the trainable classifier (vanilla), our approach ensures continuous improvement in architecture performance throughout the search process. In"}, {"title": "Ablation Study", "content": "We conduct ablation experiments to validate the improvements in both the search space and search strategy. We take the vanilla DARTS as the baseline and first improve its search space by replacing two original convolution operations with the proposed LT-AggConv and LT-HierConv (Conv.). Next, we enhance our method by incorporating the ETF classifier (ETF cls.). The experimental results in Table 6 show that each improvement benefits the model's performance on the long-tailed dataset."}, {"title": "Related Work", "content": "Recently much effort has been devoted to deep long-tailed recognition. A branch of works. (Cao et al. 2019; Zhou et al. 2019; Hong et al. 2020; Zhang et al. 2021) tackle the long-tailed problem from the perspective of class re-balance,"}, {"title": "Differentiable Architecture Search", "content": "Differentiable Architecture Search (DARTS) has garnered significant attention in recent years because of its powerful performance and reduced computational requirements. Liu et al. pioneer the introduction of DARTS (Liu, Simonyan, and Yang 2018), allowing an efficient search of the architecture using gradient descent. Subsequently, a series of variations have been proposed to optimize the vanilla version. PC-DARTS (Xu et al. 2019) achieves higher efficiency by randomly sampling a subset of channels for operation search. R-DARTS (Zela et al. 2019) propose an early stopping criterion to make the searched model more robust. Fair-DARTS (Chu et al. 2019) relax the exclusive competition between skip-connections and achieve higher performance. Although these works have improved DARTS to some extent, their settings are still based on uniform data distribution. In this paper, we expose the limitations of current approaches in effectively handling long-tailed datasets and augment DARTS to enable it to effectively search for architectures that outperform manually designed ones in long-tailed environments."}, {"title": "Conclusion", "content": "Long-tailed data in real-world scenarios presents formidable challenges for recognition tasks. In this paper, we approach the issue from the overlooked perspective of architecture and opt to leverage differentiable architecture search to automate the network design. However, existing methods achieve unsatisfactory results on long-tailed datasets. Empirically, we observe limitations in the existing search space. To address this issue, we explore the architecture properties systematically and design two novel convolution operations, constructing a fresh search space. Moreover, we introduce an ETF classifier to further enhance the search strategy, mitigating the inherent bias search process. Extensive results demonstrate that our approach consistently improves existing methods and achieves state-of-the-art results with simple enhancements, providing a complementary solution to mitigating long-tailed problems."}]}