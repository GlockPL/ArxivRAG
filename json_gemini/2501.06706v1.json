{"title": "AIOPSLAB: A HOLISTIC FRAMEWORK TO EVALUATE AI AGENTS FOR\nENABLING AUTONOMOUS CLOUDS", "authors": ["Yinfang Chen", "Manish Shetty", "Gagan Somashekar", "Minghua Ma", "Yogesh Simmhan", "Jonathan Mace", "Chetan Bansal", "Rujia Wang", "Saravan Rajmohan"], "abstract": "AI for IT Operations (AIOps) aims to automate complex operational tasks, such as fault localization and root\ncause analysis, to reduce human workload and minimize customer impact. While traditional DevOps tools\nand AIOps algorithms often focus on addressing isolated operational tasks, recent advances in Large Language\nModels (LLMs) and AI agents are revolutionizing AIOps by enabling end-to-end and multitask automation.\nThis paper envisions a future where AI agents autonomously manage operational tasks throughout the entire\nincident lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps. Realizing this vision\nrequires a comprehensive framework to guide the design, development, and evaluation of these agents. To this\nend, we present AIOPSLAB, a framework that not only deploys microservice cloud environments, injects faults,\ngenerates workloads, and exports telemetry data but also orchestrates these components and provides interfaces\nfor interacting with and evaluating agents. We discuss the key requirements for such a holistic framework and\ndemonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps agents. Through evaluations\nof state-of-the-art LLM agents within the benchmark created by AIOPSLAB, we provide insights into their\ncapabilities and limitations in handling complex operational tasks in cloud environments.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid evolution of IT applications and services has led\nenterprises to increasingly depend on hyper-scale, cloud-\nbased systems. These systems are often distributed, em-\nploying architectures such as microservices and serverless\ncomputing, enabling scalability but also adding complexity\nand introducing new operational challenges. In such cloud\nenvironments, issues can cascade into large-scale outages.\nFor instance, an Amazon outage can result in losses of $100\nmillion in just one hour (Wolfe, 2018).\nTo address the challenges of managing incidents in such\ncomplex infrastructures, there is a movement towards the\nadoption of AIOps (Artificial Intelligence for IT Opera-\ntions), within the context of DevOps (Development and\nOperations). The ultimate goal of AIOps is to create au-\ntonomous self-healing clouds, where AI-driven approaches\ncan detect, localize, and mitigate faults with minimal hu-\nman intervention. Although such a concept has existed for\nover a decade (Li et al., 2012; Dai et al., 2009), the recent\nadvancements of AIOps and Large Language Model (LLM)\nagents have brought this vision closer to reality (Zhao et al.,\n2023; He et al., 2022; Ma et al., 2018; Zhang et al., 2018;\nGanatra et al., 2023; Somashekar et al., 2024; Zhang et al.,\n2024a; Chen et al., 2024). Large Language Model (LLM)\nagents (Mialon et al., 2023; Schick et al., 2024) integrate\nexternal tools to dynamically interact with their environ-\nment (Wei et al., 2022a), enabling them to autonomously\nmanage the entire incident lifecycle, as shown in Figure 1.\nTo realize this autonomous self-healing cloud vision, we\npropose a new paradigm called AgentOps (Agent for Opera-\ntions). In this paradigm, agentic approaches are not limited\nto isolated operational tasks but are capable of seamlessly\nmanaging multiple, cross-layer tasks across the entire op-\nerational stack. AgentOps represents an evolution where\nautonomous agents can make real-time decisions and end-\nto-end actions to ensure system reliability. This aligns with\nrecent advancements in AI, as highlighted by a post:\n\"State-of-the-art Al results are increasingly ob-\ntained by compound systems with multiple com-\nponents, not just monolithic models ... compound\nAl systems will likely be the best way to maximize\nAl results in the future\" \u2013 The Shift from Models\nto Compound AI Systems (Zaharia et al., 2024)\nAI-driven tools and benchmarks like WebArena (Zhou\net al., 2023), R2E (Jain et al., 2024b), HumanEval (Chen\net al., 2021), LiveCodeBench (Jain et al., 2024a), and SWE-\nbench (Jimenez et al., 2024) have significantly advanced the\n'Dev' side of DevOps by accelerating software development.\nHowever, progress in AI for 'Ops', particularly AgentOps,\nremains limited, due to the lack of high-quality benchmarks\nfor diverse, realistic scenarios. Addressing this gap requires\na framework that aids the design, development, and evalua-\ntion of AIOps agents within an interactive environment, a\nkey contribution of this paper.\nChallenges and contributions. Building a holistic bench-\nmark framework that can allow agents to interact dynam-\nically with the cloud poses several challenges. The first\nchallenge is to manage an evaluation flow that is generally\napplicable to diverse agents and clouds, powerful enough to\nevaluate agents by complex and realistic operational tasks,\nand valuable enough to provide different feedback or ob-\nservability, together with extensibility that make it possible\nto accommodate new tasks and agents by the users. While\nexisting tools address individual components of the AIOps\nevaluation, such as observability (He et al., 2023; Simonsson\net al., 2021), application suites (Gan et al., 2019; Zhou et al.,\n2021; Sriraman and Wenisch, 2018) and chaos engineer-\ning (Netflix, 2011; ChaosBlade Team, 2019; ChaosMesh\nAuthors, 2022), they lack the integration necessary to sup-\nport a unified AIOps evaluation.\nWe present AIOPSLAB, a holistic framework that can au-\ntomatically manage the entire end-to-end evaluation pro-\ncess for AIOps solutions. This involves deploying services,\nfault injection, workload generation, orchestrating the agent-\ncloud interaction, and analyzing results. Specifically, AIOP-\nSLAB features the Agent-Cloud Interface (ACI), a unified\ninterface that enables agents to interact with the cloud. ACI\nallows agents to communicate, take action, and receive feed-\nback, orchestrating these interactions to detect and resolve\nissues in dynamic and interactive environments.\nMoreover, a common challenge in operation benchmarks\nis the lack of realistic evaluation scenarios, as existing ap-\nproaches often rely on static datasets, such as system met-\nrics (Han et al., 2022; Jacob et al., 2020) that are typically\ntime series data, or on fixed question-answer format (Liu\net al., 2023). Such setups do not capture the dynamic, unpre-\ndictable, and evolving nature of real-world cloud environ-\nments, where workloads and incidents fluctuate over time.\nTo make matters worse, recent efforts on AgentOps (Wang\net al., 2023; Zhang et al., 2024a) use proprietary services\nand datasets. Furthermore, existing AIOps approaches and\ntheir benchmarks often focus only on isolated aspects of\nthe incident lifecycle, such as anomaly detection (Yu et al.,\n2024b) or fault localization (Sun et al., 2024). This lacks a\ncohesive framework to evaluate AIOps agents comprehen-\nsively. Moreover, it limits support for decision-making that\ncould assist in chaining algorithms or selecting the most\nsuitable agent for a given operation scenario.\nTo address these limitations, we designed a set of evaluation\nscenarios, referred to as problems, which replicates realistic\nincidents within the microservice system. AIOPSLAB's\nproblem pool is structured around a task-level taxonomy that\ncategorizes tasks of different problems across the incident\nmanagement lifecycle. Our approach ensures that evaluation\nscenarios go beyond simple performance or crash failures\n(that cannot be further analyzed or mitigated by the agents),\nincorporating fine-grained root causes to fully assess the\ndiagnostic and mitigation abilities of AIOps agents.\nImplementation. We developed AIOPSLAB, an inno-\nvative framework for building AgentOps benchmarks to\nevaluate LLM-based AIOps agents. AIOPSLAB utilizes\ntwo microservice applications from DeathStarBench (Gan\net al., 2019) as testbeds, along with their workload gen-\nerators. An extensible fault library, integrated with\nChaosMesh (ChaosMesh Authors, 2022), enables diverse\nfault injections into the system. A telemetry observer, in-\ncorporating Prometheus (Prometheus Authors, 2024) for\nmetrics, Jaeger (Jaeger Authors, 2024) for tracing, and File-\nbeat (Elasticsearch, 2024b) and Logstash (Elasticsearch,\n2024a) for logging, supports on-disk storage of telemetry\ndata, facilitating evaluations of both traditional AIOps al-\ngorithms and agentic solutions. We also integrate Helm\nand Kubernetes APIs into the AIOPSLAB's orchestrator\nimplementation.\nTo demonstrate the application of our framework in evalu-\nating LLM-based agents as the benchmark, we use AIOP-\nSLAB to create 48 problems as evaluation scenarios covering\ndifferent types of AIOps tasks, and register four agents from\ndifferent types on those problems. The agent registration is\nlightweight, with less than a hundred lines of code to im-\nplement. Our evaluation process reveals distinct challenges\nagents face across tasks.\nSummary. This paper makes the following contributions:\n\u2022 We unravel the requirements and challenges of achieving\na holistic framework that supports the design, develop-\nment, and evaluation of autonomous AIOps agents;\n\u2022 We develop a framework, AIOPSLAB, which can not only\ndeploy microservice cloud environments, inject faults,\ngenerate workloads, and export telemetry data but also\norchestrate these components and provide agent-cloud\ninterfaces for interacting with and evaluating agents.\n\u2022 We leverage the AIOPSLAB framework to construct a\nbenchmark suite with 48 problems across different AIOps\ntasks in an interactive environment and evaluate four\nLLM-based agents.\n\u2022 We provide a detailed analysis of the agents' performance\nand limitations by evaluating them on AIOPSLAB.\n\u2022 We will make AIOPSLAB publicly available.\u00b9"}, {"title": "2 AIOPSLAB", "content": "In this section, we discuss the design and implementation of\nAIOPSLAB and its components, as illustrated in Figure 2.\n2.1 Problem Definition\nTo support a wide range of evaluation scenarios (referred to\nas problems), which replicate realistic incidents within the\nmicroservice system, we first formalize an AIOps problem\n$P$ as a tuple: $P = (T, C, S)$, where $T$ represents a task, $C$\nrepresents a context, and $S$ represents the expected solution\n(oracle). The task $T$ defines the specific AIOps operation\nto be performed, categorized into four types: detection,\nlocalization, (root cause) analysis, and mitigation. We define\nthese tasks in Table 1. Each task type is associated with\nsuccess criteria and evaluation metrics. For instance, the\ndetection task employs Time-to-Detect (TTD) to measure\nthe time taken to detect a fault.\nThe context $C$ can be further formalized as a tuple: $C =$\n$(E, I)$, where $E$ is the operational environment in which\nthe problem occurs, and $I$ is the problem information used\nto describe the problem to the agent. The operational en-\nvironment includes the cloud service, the fault model, and\nthe workload model used to generate the problem, which\nis not shared with the agent. The problem information\ncomprises of information such as service descriptions, task\ndescriptions, and documentation about available APIs that\nis directly shared with the agent. It also subsumes indirect\ninformation (including logs, metrics, and traces observed in\nthe operational environment) that is queryable by the agent\nat runtime. Finally, $S$ is the expected outcome of the task,\nwhich is used to evaluate the agent's performance. The so-\nlution is typically problem and task-specific and is carefully\ndesigned for evaluation. Note that some problems, e.g., mit-\nigation tasks, can be solved in multiple ways. In such cases,\nAIOPSLAB evaluates the general state of the entire system,\ne.g., check whether all of the services are up and running,\nafter the problem is resolved, rather than solely on the tar-\ngeted resource where the fault was injected, because other\nservices or resources may have been inadvertently affected\nduring the mitigation process.\nExample 2.1. Consider the problem of localizing a Ku-\nbernetes target port misconfiguration in a social network\napplication. AIOPSLAB makes it easy to define this prob-\nlem in just a few lines by extending the LocalizationTask"}, {"title": "2.2 Orchestrator", "content": "AIOPSLAB's Orchestrator strictly enforces the separation\nof concerns between the agent and the service, using a\nwell-defined central piece, the Orchestrator. It provides a\nrobust set of interfaces that allow seamless integration and\nextension of various system components.\n2.2.1 Agent Cloud Interface\nA key responsibility of the Orchestrator is to provide a well-\ndefined interface for the agent to interact with the cloud\nenvironment. Typically, developers operate clouds and ser-\nvices with various programming (e.g., APIs, CLIs) and user\ninterfaces (incident portals, dashboards, etc.). However,\nexisting interfaces to the cloud are not well-designed for\nLLMs and agents. For instance, humans can reliably ig-\nnore irrelevant information, which can prove distracting for\nagents and hamper performance.\nThe ACI specifies (1) the set of valid actions available to\nthe agent, and (2) how the service's state is conveyed\nback to the agent as the observation of its actions.\nIn doing so, the ACI abstracts the cloud environment's\ncomplexity, simplifying the agent's decision-making pro-\ncess. The ACI is designed to be intuitive and easy to use,\nwith a concise list of APIs, each documented to ensure\nthat agents can make meaningful progress towards their ob-\njectives. Some APIs that AIOPSLAB provides by default\ninclude get_logs (fetch logs), get_metrics (fetch metrics),\nget_traces (fetch traces), and exec_shell (execute shell\ncommands after applying security policy filters).\n2.2.2 Session Interface\nAnother key responsibility of the Orchestrator is to manage\nthe lifecycle of the agent and the service. We implement the\nOrchestrator as a session-based system, where a Session\nis created for each instance of an agent solving a problem.\nAgents are registered with the Orchestrator, and a session\nstarts with simple API calls passing a unique problem iden-\ntifier. AIOPSLAB's design is highly flexible and inte-\ngrates with the growing LLM and agent framework space.\nOur only requirement is that the agent must implement a\nget_action method with the following signature: async\ndef get_action(state: str)-> str. It takes the service's\nstate as input from the Orchestrator and returns the next ac-\ntion the agent wants to take. Note that this could be a simple\nwrapper function around any existing agent framework."}, {"title": "2.2.3 Other Interfaces", "content": "Problem Initializers. As described in Section 2.1, each\nproblem is defined with a context $C$ which includes its\noperational environment. This environment is the service,\nfault, and workload conditions under which the problem\noccurs. Here, the Orchestrator deploys services and uses\ninfrastructure-as-code tools like (Helm, 2024) to deploy\nthe required cloud service for each problem. We describe\nservices already integrated into AIOPSLAB in Section 2.3.\nAs shown in Figure 2, to create realistic benchmark sce-\nnarios, the Orchestrator then interfaces with two entities:\n(1) a workload generator and (2) a fault generator. These generators introduce controlled service disruptions\nthat simulate live benchmark problems. As the workload\ngenerator, AIOPSLAB currently uses the wrk2 tool (Gan\net al., 2019), which supports several workload policies and\nalso replays industry workloads. However, the AIOP-\nSLAB is extensible to other workload generators. For fault\ngeneration, AIOPSLAB uses a custom fault library that in-\nstantiates faults across different levels of the system stack,\nsuch as application and virtualization. The library con-\ntains and extends to several fine-grained and parametric\nfaults that go beyond surface-level symptoms and engage\ndeeper into more complex resolution strategies. We describe\nthe fault library in detail in Section 2.4.\nProblem Evaluators. Finally, the Orchestrator plays a criti-\ncal role in evaluating the agent's performance on a problem.\nIt compares the agent's solutions against predefined suc-\ncess criteria and evaluation metrics specific to each task. AIOPSLAB supports several default and common metrics\nfor each task (e.g., Time-to-Detect for detection, number of\nsteps taken, and tokens produced by an LLM-powered agent\nsent to AIOPSLAB). Additionally, AIOPSLAB provides an\noptional qualitative evaluation of agent trajectories using\nLLMs-as-Judges (Zheng et al., 2024). Beyond that, all user-\ndefined evaluation metrics specific to the problem are run.\nFor instance, for the localization problem in Example 2.1,\nthe metric success is defined by the agent's submission\nmatching the fault microservice's name. Lastly, the Orches-\ntrator maintains comprehensive logs of all agent trajectories,\nincluding actions taken and resulting system states, facilitat-\ning detailed analysis and debugging. All of the evaluation\nresults will be automatically collected."}, {"title": "2.3 Cloud Services", "content": "AIOPSLAB deploys live microservice applications as cloud\nenvironments. AIOPSLAB is currently integrated with\nthe HotelReservation and SocialNetwork from DeathStar-\nBench (Gan et al., 2019). The SocialNetwork application\nhas 28 microservices, including Memcached, MongoDB,\nand Redis, that together implement several features of real-"}, {"title": "2.4 Task-oriented Fault Library", "content": "2.4.1 Task Taxonomy\nWe present a task-level taxonomy (Table 1) that categorizes\nthe tasks that AIOps agents should accomplish according\nto the different stages of the incident management lifecycle,\nwith progressively increasing complexity. In Table 1, a\nhigher level indicates a harder and more impactful task to\nevaluate agents.\nLevel 1 focuses on the preliminary identification of unusual\nbehavior within the system, for example, detecting a mal-\nfunctioning Kubernetes pod of a microservice. Also, users\ncan define more complex tasks or create sub-tasks. The root\ncause analysis task has both the system level and fault type\nprediction sub-tasks to be solved.\nTo instantiate problems across different task levels, we use\nfault injection to inject faults into the system, and construct\na problem pool for AIOPSLAB. We classify them into two\nmain types, symptomatic faults and functional faults, as\nshown in Figure 3.\n2.4.2 Symptomatic Faults\nSymptomatic faults, such as performance degradation and\ncrash failures, manifest as observable symptoms, such as\nincreased latency, resource exhaustion, or service outages.\nThese faults typically help to construct Level 1 and Level\n2 tasks in the taxonomy, which can create problems that\nevaluate AIOps approaches' detection and localization abil-\nity. These faults provide an overview of potential problems\nbut do not necessarily reveal the deeper, underlying root"}, {"title": "2.4.3 Functional Faults", "content": "Though there are many fault injection tools for testing the\nresilience of cloud systems (Marinescu and Candea, 2009;\nBanabic and Candea, 2012; Christakis et al., 2017; Zhang\nand Elbaum, 2012; Kingsbury, 2022; Pillai et al., 2014;\nAlquraan et al., 2018; Lu et al., 2019; Chen et al., 2020;\nLeesatapornwongsa et al., 2014; Gunawi et al., 2011; Ma-\njumdar and Niksic, 2018; Ju et al., 2013; Heorhiadi et al.,\n2016; Alagappan et al., 2016; Mohan et al., 2018; Sun et al.,\n2022; Canini et al., 2012), most of them focus solely on\ninjecting system symptoms. These coarse-grained faults can\nonly disrupt without modeling the underlying, fine-grained\nroot causes, e.g., misconfigurations or software bugs, and\nhence are unable to evaluate the capabilities of AIOps agents\nto diagnose and mitigate root causes.\nThe failure scenarios to evaluate AIOps agents across tasks\nmust go beyond simple performance or crash failures, and\nreflect realistic cases that challenge agents, where functional\nfaults come into play. Functional faults require approaches\nto not only detect (Level 1) and localize (Level 2) the failure\nbut also diagnose the root cause (Level 3), such as incorrect\ndeployment or operations, and apply the correct mitigation\nstrategies (Level 4). For instance, the fault in Figure 4\nrevokes the admin authentication for the MongoDB database\nof the geographic microservice (Mongodb-geo). Since the\nGeo service relies on its backend database, errors will appear\nduring its invocation."}, {"title": "2.5 Observability", "content": "AIOPSLAB is equipped with an extensible observability\nlayer to provide comprehensive monitoring capabilities.\nAIOPSLAB collects a wide array of telemetry data by its\ntelemetry collector, including (1) traces from Jaeger (Jaeger\nAuthors, 2024) detailing the end-to-end paths of requests\nthrough distributed systems, (2) application logs retrieved\nby Kubectl, or formatted and recorded by Filebeat (Elastic-\nsearch, 2024b) and Logstash (Elasticsearch, 2024a), and (3)\nsystem metrics monitored by Prometheus (Prometheus Au-\nthors, 2024). AIOPSLAB not only supports data collection\nduring the interaction with the LLM agent but can also ex-\nport the data offline to facilitate evaluating other traditional\nAIOps approaches. Besides, AIOPSLAB is designed to\ncapture information from other dimensions, e.g., codebase,\nconfiguration, and cluster information. Developers can also\ndesign and expose low-level system information (such as\nsyscall logs) to agents using AIOPSLAB's interface."}, {"title": "3 EVALUATION", "content": "This section begins by outlining the evaluation setup and\nmetrics employed within AIOPSLAB. We then delve into\nthe selected faults listed in Table 2, which serve as diverse\nevaluation scenarios within AIOPSLAB. Following this, we\nevaluate the performance of the AIOps agents solving these\nproblems, and then analyze the cost of the agents. We also\ndig into the reasons behind the performance differences to\nunderstand the challenges and potential agent improvements.\nNote that, all of the results are automatically collected and\nrecorded by the problem evaluators (Section 2.2.3).\n3.1 Evaluation Setup\nWe evaluate four LLM-based agents with AIOPSLAB. Note\nthat, for a fair comparison, we register the naive agent in\nAIOPSLAB without any fine-tuning or modifications. We\nuse GPT-3.5-TURBO and GPT-4-TURBO (Achiam et al., 2023)\nthat have access to only a secure shell as baselines (GPT-w-\nSHELL). In addition, we also evaluate the performance of\nREACT (Yao et al., 2023), which extends chain-of-thought\nreasoning (Wei et al., 2022b) by integrating reasoning and\nacting in an interleaved manner,\nAs for cloud operation-specific agents, we choose\nFLASH (Zhang et al., 2024b). FLASH employs a workflow au-\ntomation system that monitors execution status and decom-\nposes complex instructions into manageable, conditional\nsegments. It incorporates hindsight generation to learn from\npast interactions. As FLASH was not publicly available at\nthe time of writing, we develop a simplified version that\nretrospectively generates insights after each step.\nTo compare with other AIOps approaches specific to a cer-\ntain type of task, we evaluate three state-of-the-art, non-\nLLM-based AIOps algorithms on AIOPSLAB, using (multi-\nmodal) telemetry data as input. They are: MKSMC (\u00c7etin\nand Tasgin, 2020) for detection, RMLAD (Wang et al.,\n2020) and PDiagnose (Hou et al., 2021) for localization.\n3.2 Metrics\nCorrectness. This metric measures the accuracy of the\nagent's response to problems. It evaluates whether the agent\nsuccessfully detects, localizes, analyzes and resolves the\nproblems as expected.\nTime/Steps. These metrics evaluate the efficiency of the\nAIOps agent for each type of task. For example, Time-to-\nDetect (TTD) is the time elapsed from the occurrence of\na fault to its detection, and Time-to-Mitigate (TTM) is the\ntime taken from detection to complete mitigation of the fault.\nThe number of steps or actions taken to solve the problem\nis also recorded. Note that this is the number of times the\nagent interacts with the AIOPSLAB instead of the number\nof requests sent to the backend LLM.\nCost. We use the number of tokens, including both the input\ntoken and output tokens, generated by the agents/environ-\nment as an indicator of the cost.\n3.3 Problem Pool of AIOPSLAB Benchmark\nCurrently, AIOPSLAB benchmark consists of 48 problems\nin its problem pool. With six agents, we evaluate a total\nof 288 cases. Table 2 lists the faults used to instantiate the\nproblems. As shown in Table 2, all functional faults (in-\ncluding Fault 1-7) are used to create problems at all of the\nfour task levels; while the symptomatic faults (including\nFault 8-9) can only be used to create problems at the detec-\ntion and localization levels (Level 1 and Level 2). In the\ndetection-level task, the agents must identify the presence\nof faults in real-time. This task is a binary classification,\nwhere the agents have to respond either \"yes\" if a fault is\npresent or \"no\" on the contrary. The detection task (Level\n1) can be made more complex, e.g., by asking the agents\nto label the abnormal telemetry data; however, we keep it\nsimple here and leave the complex tasks to other levels. The\nlocalization (Level 2) task asks the agents to specify the\nexact location of the fault, usually a service or pod name\nin Kubernetes. The RCA task (Level 3) requires the agents\nto identify (1) the system layer the fault affects and (2) the\ntype of the fault, e.g., misconfiguration or operation error.\nThe mitigation task (Level 4) requires the agents to interact\nwith the environment to fix the fault with a series of actions,\nsuch as updating the configuration, or rollback to a previous\nversion, etc.\nMost faults enable users to extend and create new prob-\nlems easily by injecting the fault into other targets, such\nas services. For example, Fault 2 in AIOPSLAB can be"}, {"title": "3.4 Performance Results", "content": "The overall performance of the agents is summarized in\nTable 3, with task-specific results in Table 4. As illustrated\nin Table 3, FLASH achieves the highest accuracy among all\nagents. Although GPT-3.5-TURBO completes the tasks the\nfastest, it has the lowest accuracy at 15.25%.\nThe detection task, being a binary choice question, should be\nthe simplest task and the first step an AIOps agent performs.\nHowever, as shown in Table 4(a), only FLASH answers\nall the detection problems correctly. For localization task,\nagents are allowed to come up with a list of potential faulty\nservices as their answers (since there could be multiple faults\nhappenning in the system at the same time). To evaluate\ntheir accuracy, we consider both the top 1 and top 3 answers.\nIn Table 4(b), REACT performs best when evaluated using\nthe top 3 answers, but its accuracy drops when considering\nthe top 1. The RCA and mitigation tasks prove to be the\nmost challenging for the agents. GPT-3.5-W-SHELL fails to\nrecover any failure in its mitigation attempts.\nProblem difficulty differs across task levels. Despite show-\ning promise in addressing realistic operational tasks, none of\nthe agents consistently achieve high problem-solving accu-\nracy across four task categories in AIOPSLAB benchmark."}, {"title": "3.5 Influence of the Step Limit", "content": "We examine the impact of the maximum number of allowed\nsteps on the agent's performance, with the results shown\nin Figure 5. The step limit significantly affects the perfor-\nmance of certain agents. For instance, REACT and FLASH\nshow improved accuracy with more steps, with FLASH reach-\ning the highest accuracy of 59.32% when the step limit is\nset to 20. However, for GPT-3.5-TURBO, increasing the step\nlimit beyond 5 does not yield better performance but merely\nincreases the token consumption. Notably, the plateauing\nof accuracy after a certain number of steps indicates that\nself-repair with environment feedback can saturate quickly\nfor AIOps problems. On the contrary, in development tasks\n(Dev), such as code generation, feedback via various com-\npositional tools such as linters, type checkers, and test cases\nhelp agents continuously improve. This suggests the need\nfor (1) better task decomposition for AIOps problems using\nplanning, (2) improved feedback mechanisms for interme-\ndiate steps, and (3) solutions that go beyond environment\nfeedback and self-repair."}, {"title": "3.6 Agent Behavior: The Good, the Bad and the Gaps", "content": "We now delve into the behaviors of the agents and analyze\nthe good, the challenges, and opportunities for improve-\nment. In Table 4, we see that all agents perform better\nthan the traditional non-LLM AIOps methods in terms of\nthe problems for detection and localization tasks. Figure 6,\nshows the telemetry API usage patterns among agents. The\nget_logs API is the most frequently used API across all\nagents, then the get_metrics, and the get_traces APIs.\nHowever, agents also diverge in their patterns of API usage.\nFor example, FLASH does not use the get_traces API at\nall. We present the occurrences of other system commands\nfor each agent in Table 5. We next discuss the underly-\ning reasons and patterns contributing to the agents' poor\nperformance.\n3.6.1 Wasting steps on unnecessary actions\nWe observe that agents often waste steps on unnecessary\nactions, such as repeatedly calling the same API, generating\nnon-existent APIs, or spending excessive steps in multi-\nagent communication. Specifically, the GPT-3.5-W-SHELL\nagent often generates incorrect API commands in loops,\nleading to repeated errors in execution. For instance, set-\nting speaker_selection_method as round_robin allows ev-\nery agent to speak in every step, but this often prevents\ndecisive, efficient decisions, as agents repeatedly resort\nto telemetry APIs for more information. Even with the\nspeaker_selection_method set to auto, where the next\nspeaker is automatically chosen, a selected agent always\nspeaks ten times in a step without communication (with a\nmaximum of ten communication rounds per step).\n3.6.2 Overloaded information when consuming data\nTo dig deeper into the agent failure modes, we analyze the\ncorrelation between the agents' actions and the success or\nfailure of problem-solving, as well as the distribution of\nactions across steps. In Figure 7, we present the distribution\nof actions for both successful and failed cases. Agents tend\nto use get_metrics and get_traces APIs sparingly in suc-\ncessfully resolved problems, typically only when necessary.\nThis is understandable, as the metrics data, e.g., CPU and\nmemory usage have numerous values, which are hard to\ndirectly interpret, and trace data are descriptive records of\nthe system's dependencies, which are more comprehensi-\nble when visualized. However, agents may subsequently\nconsume these data with a cat command directly, which\ncan overwhelm the model's input context window and cause\ndistraction and more tokens to be consumed. Consequently,\nusing these telemetry APIs without careful consideration or\nanalysis can add more noise into the agents' reasoning, pos-\nsibly leading to token exhaustion. We expect more refined\ntelemetry data processing and filtering mechanisms to be\nimplemented in the agents to avoid this issue in the future.\n3.6.3 Invalid API usage\nWe notice that agents can struggle with improper formatting\nof API calls. For instance, GPT-3.5-W-SHELL consistently\ngenerates incorrect command formats (though the API name\nis correct), such as malformed parameters, and repeat the\nerror in subsequent steps. In many cases, GPT-3.5-W-SHELL\nrepeatedly responds with: \u201cI apologize for the error. Here is\nthe API call again:"}]}