{"title": "Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning", "authors": ["Hongyi Yuan", "Suqi Liu", "Kelly Cho", "Katherine Liao", "Alexandre Pereira", "Tianxi Cai"], "abstract": "Biomedical concepts, along with genomic features such as single-nucleotide polymorphisms (SNPs) and gene expression, are crucial for understanding the genetic and biomedical relation-ships in modern medicine. Genome-wide association studies (GWAS), expression quantitative trait loci (eQTL), and phenome-wide association studies (PheWAS) derived from biobank data provide valuable insights into the interplay between genomic and clinical features. However, harmonizing existing biological and clinical knowledge from different sources to enable a better understanding of diseases and treatments remains challenging. Variations in biomedical concept coding systems and differences in SNP selections across data sources create barriers to effective integration. To address this challenge, we introduce GENomic Encoding REpresentation with Language Model (GENEREL), a framework designed to bridge genetic and biomedical knowl-edge bases. What sets GENEREL apart is its ability to fine-tune language models to infuse biological knowledge behind clinical concepts such as diseases and medications. This fine-tuning enables the model to capture complex biomedical relationships more effectively, enriching the understanding of how genomic data connects to clinical outcomes. By constructing a unified embedding space for biomedical concepts and a wide range of common SNPs from sources such as patient-level data, biomedical knowledge graphs, and GWAS summaries, GENEREL aligns the embeddings of SNPs and clinical concepts through multi-task contrastive learning. This al-lows the model to adapt to diverse natural language representations of biomedical concepts while bypassing the limitations of traditional code mapping systems across different data sources. Our experiments demonstrate GENEREL's ability to effectively capture the nuanced relationships between SNPs and clinical concepts. GENEREL also emerges to discern the degree of related-ness, potentially allowing for a more refined identification of concepts. This pioneering approach in constructing a unified embedding system for both SNPs and biomedical concepts enhances the potential for data integration and discovery in biomedical research.", "sections": [{"title": "1 Introduction", "content": "Large biobanks, such as the UK Biobank , the Million Veteran Program , and All of Us , in conjunction with"}, {"title": "2 Related Works", "content": "Understanding the complex connections and interactions between biomedical concepts and genomic features has long been a central focus of biomedical research. The successful outcomes of GWAS over the past few decades  have provided valuable insights into the biological underpinnings of diseases, supported clinical decision-making, and facilitated drug discovery. However, GWAS typically concentrates on a specific trait of interest and lacks the ability to generalize to multiple or broader biomedical concepts.\nRepresentation Learning of Biomedical and Genomic Concepts Existing research learns embedding for biomedical and genetic concepts by using statistical learning algorithms such as factorization of co-occurrence and adjacency matrix , or random walk based graph learning . Recently, researchers also applied various graph neural networks along with link prediction or graph alignment objectives to generate embeddings . Despite variations in algorithms, all the aforementioned methods are based on the codified concepts from different coding systems such as ICD10 , CUI , and HPO . Different coding systems hinder the ability of the methods to generalize across different databases. Manually curated code mappings between systems are needed to enable multi-source learning, which is prone to human errors .\nBiomedical Language Models A wide range of pre-trained language models are employed to analyze biomedical and clinical language. These models are trained on various domain-specific corpora such as PubMed articles , clinical notes , and knowledge graphs . Masked language modeling , next token prediction , and contrastive learning  are the common techniques for adapting general language model to the biomedical domain. These language models have been shown to offer a more flexible and efficient method for processing biomedical knowledge ."}, {"title": "3 GENEREL", "content": "In this section, we first explain how GENEREL formalizes the task and models biomedical concepts and SNPs. We then detail the multi-task contrastive learning objective. Finally, we conclude by outlining the steps taken to extract the necessary training data from heterogeneous sources."}, {"title": "3.1 Modeling", "content": "Biomedical concepts are denoted by {ci}=1, where each ci is presented by a short text phrase or description, and genomic variant concepts are denoted by {gj}j=1 , where each gj is an indexed SNP along with the corresponding variant allele (e.g., rs2476601_A). We use a pre-trained language model denoted by M$ to map the biomedical concept to the dense embedding c e Rd,\nc = WpM(Ci) + bp,\nwhere & represents the trainable parameters in the language model, W and b compose the trainable linear layer to map the hidden state from the language model into any pre-defined dimension sizes of the shared embedding space, and the hidden state is extracted from the [CLS] position for each concept. For gj, since SNPs are independent concepts without any shared information, we use one-hot encoding for each gj and apply an embedding matrix Ey to generate dense representations"}, {"title": "3.2 Learning Objective", "content": "We consider three distinct modeling tasks in GENEREL: (1) the relatedness between biomedical concepts, (2) the relationship between biomedical and genomic concepts, and (3) the disambiguation of synonyms for each biomedical concept. For each task, we can formalize the training data into a collection of concept pairs:\nS \u2286 {(h, t) : h, t \u2208 {ci}\u222a{gj}}.\nAdditionally, each pair often has an associated weight that indicates the degree of relatedness. For instance, the strength of the association between a SNP and a trait can be quantified by the odds ratio or the regression coefficient. In GENEREL, we incorporate this information wh,t for a pair (h, t) if available; otherwise, we set wh,t = 1. Given the data pairs, we apply the contrastive loss to integrate the relatedness into our model. Specifically, we utilize the InfoNCE loss :\nLs = \u2211 Wh,tLInfoNCE (h, t)\n(h,t) ES\n=\n\u2211wh,t log\n(h,t) ES\nexp(sim(h, t)/\u0442)\n\u03a3\u0127ec exp(sim(h, t)/\u0442)\nwhere sim(,) is a similarity function, C is the set of conditional negative samples, and is the temperature parameter. In particular, we use the inner product of the embeddings as the similarity measure, i.e., sim(h,t) = (he,te). The implementation details of the InfoNCE loss may vary depending on the negative sampling schemes used . In GENEREL, we follow the implementation of CLIP  with a learnable temperature parameter."}, {"title": "3.3 Training Data", "content": "SNP collection We curate the common SNP collection from GWAS catalog and eQTL from GTEX [Lonsdale et al., 2013]. Both sources compile various traits and SNP associations from existing research. Since our framework prioritizes common SNPs, we exclude those SNPs associated with fewer than two traits in the GWAS catalog. For eQTL, we retain SNPs linked to the most prevalent tissues and with the largest proportions of variance explained (PVE) values. We also only include SNPs with A, T, C, or G as risk alleles, omitting those with complex variants like insertions or deletions. Finally, we merge the selected SNPs from the GWAS catalog and eQTL, resulting in 65,278 unique SNPs and 83,900 unique genomic concepts of SNPs paired with alleles.\nGWAS catalog and eQTL We pair selected SNPs with their associated traits and gene names provided by GWAS catalog or eQTL. To enrich the data, we use the trait phrases from both the description of the original studies and the mapped trait names from EFO coding system in GWAS catalog. We extract the betas or odds ratios from both sources to serve as wh,t, reflecting the association levels of the pairs. These beta or odds ratio values are inconsistent across studies due to varying units, which can differ by several orders of magnitude, leading to unstable training. To address this issue, we first group the pairs along with their values by both study and trait, and then normalize the values by dividing by the mean and truncating them at specified thresholds. All weights in the final dataset are in between 0 and 2.\nUK Biobank UK Biobank is a large-scale biomedical database containing participants' whole genome sequencing together with information concerning various aspects of health. It is challenging to incorporate biobank patient-level data into the contrastive learning process since the phenotype-genotype association is typically weak compared to cohort studies. To address this issue, we first utilize the correlation matrix between phenotypes and SNPs, adjusted for demographics such as gender and ethnicity. We then filter the pairs by applying a threshold to the absolute correlation values. This process can effectively identify the significantly associated phenotypes and SNPs from the patient-level data to construct high-quality training pairs. The correlation of each pair is also incorporated as the weight. We extract the EMIS cohort, consisting of 216,215 patients with 6,358 phenotypes and 61,455 SNPs. The phenotypes are defined using PheCodes, which group ICD codes into higher-level concepts, and SNOMED CT, which are mapped to UMLS Concept Unique Identifiers (CUIs). After processing, we have 467,026 pairs of associated concepts.\nPrimeKG To further enhance the relationships among biomedical concepts, we leverage PrimeKG, a biomedical knowledge graph that contains a comprehensive array of pairwise relationships across various entities, including diseases, drugs, genes, and phenotypes. During the training process, GENEREL primarily focuses on the biological knowledge related to diseases in PrimeKG to improve its understanding and representation of these concepts. We filter the relationships and concepts, leaving out the rare relational types and keeping only the concept types of gene/protein, disease, drug, effect/phenotype, molecular function and pathway.\nUMLS UMLS is an integrated biomedical terminology system that serves as a useful resource for developing language models in biomedical information extraction . UMLS concepts are organized as CUIs, which group synonymous terms that represent the same clinical concept. In our training, we use the 2020AB release of UMLS and extract synonymous term pairs as positive samples."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Training Setting", "content": "For multi-task training, we utilize the processed datasets as described in the previous section. Our language model utilizes the same architecture and initializes its weights from SapBERT . The weights and biases of the linear transformation and the SNP embeddings are initialized randomly. For both biomedical concept and SNP embeddings, we set the dimensions to 768. We collaboratively train GENEREL on all datasets for 25 epochs with a batch size of 512"}, {"title": "4.2 Evaluation", "content": "Our main evaluation focuses on two research questions:\nRQ1 Can the language model effectively encoder the biological information of the biomedical con-cepts?\nRQ2 Do the language model and SNP embedding matrix form a unified representation space?\nRQ1 For this question, we utilize associated pairs from two external biomedical knowledge bases, DisGeNET and DrugBank. DisGeNET is a platform that aggregates data on disease-associated genes and pathways from various databases and literature. We extract and sub-sample Disease-Gene and Pathway-Gene pairs from DisGeNET for evaluation. DrugBank, a key resource for pharmaceutical research, provides associations between drugs, indications, and genes. We evaluate GENEREL using Indication-Drug and Indication-Gene pairs from DrugBank. We have confirmed that the test pairs in DisGeNET and DrugBank do not overlap with the training data from PrimeKG through exact string matching.\nWe compare GENEREL to several biomedical language models, including BioBERT, Clinical-BERT, PubMedBERT, SapBERT, CODER, and KRISSBERT . We establish a baseline using a strong general embedding model, BGE . The area under the ROC curve (AUC) is evaluated for detecting related concept pairs by comparing them against ran-domly selected negative pairs. The similarity between concepts is measured using cosine similarity between their embeddings.\nRQ2 To address this question, we evaluate GENEREL on the task of detecting associations between biomedical concepts and SNPs. We employ a standard train-test split method, using the test set from the GWAS catalog as a benchmark. Additionally, we evaluate against a genome-wide"}, {"title": "4.3 Ablation Study", "content": "A key feature of the GENEREL framework is its incorporation of multi-task and multi-source training. To demonstrate the function of each training task, we conduct ablation experiments on"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Encoding the Relative Relatedness Levels", "content": "When modeling the relatedness between biomedical concepts and SNPs, an important fact is that the degrees of how SNPs influence concepts such as traits are different. In general, GWAS reflects this through the difference in the odds ratio or beta coefficients from regression. We consider and infuse this information into GENEREL through our weighted InfoNCE loss.\nHere we discuss to what extent the representation from GENEREL can encode the relative relatedness. Given an anchor trait, we pair two associated SNPs with relative differences in odds ratios to form a test sample. We also group these test samples by the difference in odds ratios in a monotonic manner. We conduct evaluation on the GWAS test split and the MVP database mentioned before."}, {"title": "5.2 Robustness to Synonyms", "content": "To test how robust our GENEREL framework is against synonyms in biomedical concepts, we construct two sub-sampled test sets from MVP and the GWAS catalog test split. We use the synonyms from previous research  for MVP and from UMLS for GWAS to substitute the original term phrases with synonyms."}, {"title": "5.3 Case Study", "content": "To further demonstrate the performance of our model, we visualize and compare the embeddings generated by GENEREL and PubMedBERT. Using t-SNE  to reduce the dimensionality to two, we create visual representations of the embeddings. Additionally, for GENEREL, we include embeddings for SNPs.\nAutoimmune diseases like type 1 diabetes, autoimmune thyroid disease, and rheumatoid arthri-tis affect a large portion of the population, making them a major public health concern and a frequent focus of research worldwide . These diseases have complex mechanisms, and studies have confirmed that type 1 diabetes and rheumatoid arthritis are linked to mutations in the PTPN22 gene . In Figure 2, the left plot shows that GENEREL effectively clusters the autoimmune diseases and related gene concepts into a localized group, whereas the embeddings from PubMedBERT (right plot) are more dispersed across the space. Additionally, for GENEREL, we highlight relevant SNPs and risk alleles connected to these"}, {"title": "6 Conclusion", "content": "In this paper, we proposed GENEREL, a framework that incorporates language models to encode the biomedical concepts from their phrases or descriptions, collaboratively with a broad set of com-mon SNPs. This design alleviates the framework's reliance on various coding systems to represent concepts and bypasses the limitations of traditional code mappings, facilitating learning across di-verse data sources. To that end, GENEREL is empowered with multi-task, multi-source contrastive learning tasks, infusing information from biomedical knowledge graphs, GWAS catalog, and patient-level data of different institutions. Through extensive evaluations, we quantitatively and qualita-tively demonstrate state-of-the-art performance in modeling the association between biomedical concepts and genomic variants and the capability of learning across data sources. GENEREL also shows to discern the degree of relatedness between concepts, allowing a more nuanced identification of associations. Overall, GENEREL presents a pioneering framework in joint representation learn-ing of genomic and biomedical concepts. It can facilitate and enhance the integration, discovery, and understanding of the biological mechanism in biomedical research."}]}