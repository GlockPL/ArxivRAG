{"title": "Enhancing LLM-Based Automated Program Repair with Design Rationales", "authors": ["Jiuang Zhao", "Donghao Yang", "Li Zhang", "Xiaoli Lian", "Zitian Yang"], "abstract": "Automatic Program Repair (APR) endeavors to autonomously rectify issues within specific projects, which generally encompasses three categories of tasks: bug resolution, new feature development, and feature enhancement. Despite extensive research proposing various methodologies, their efficacy in addressing real issues remains unsatisfactory. It's worth noting that, typically, engineers have design rationales (DR) on solution- planed solutions and a set of underlying reasons-before they start patching code. In open-source projects, these DRs are frequently captured in issue logs through project management tools like Jira. This raises a compelling question: How can we leverage DR scattered across the issue logs to efficiently enhance APR?\nTo investigate this premise, we introduce DRCodePilot, an approach designed to augment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt instruction. Furthermore, given GPT-4's constraints in fully grasping the broader project context and occasional shortcomings in generating precise identifiers, we have devised a feedback-based self-reflective framework, in which we prompt GPT-4 to reconsider and refine its outputs by referencing a provided patch and suggested identifiers. We have established a benchmark comprising 938 issue-patch pairs sourced from two open-source repositories hosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot achieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is utilized directly. Additionally, the CodeBLEU scores also exhibit promising enhancements. Moreover, our findings reveal that the standalone application of DR can yield promising increase in the full-match ratio across CodeLlama, GPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot initiative heralds a novel human-in-the-loop avenue for advancing the field of APR.", "sections": [{"title": "1 INTRODUCTION", "content": "Due to persistent tasks such as fixing bugs, introducing new features, and improving existing ones, software maintenance consistently ranks as the most time-consuming phase in the software life cycle, even accounting for up to 90% of total costs [43]. This phase is particularly challenging because it requires maintainers to possess a deep understanding of the large and complex codebase, to analyze, design, and implement modifications without introducing new issues. To expedite this process, various Automated Program Repair (APR) techniques have been proposed to lessen the burden [14, 63]. However, their effectiveness in addressing real-world problems remains suboptimal [19, 39].\nLet us reflect on the human process of software maintenance. In practice, engineers typically have an idea of the solution along with its associated considerations before beginning the actual code patching process. Within the open-source community, project management tools such as Jira are commonly employed to document this patching workflow. We depict this with the case study of addressing issue FLINK-32976, as presented in Fig. 1. The process is initiated by an issue report submitted by either engineers or users. Subsequently, contributors engage in discussions about the issue, possible solutions, and their respective advantages and drawbacks. An assignee then codes the solution, informed by these discussions. The final step involves the engineers submitting the patch to GitHub via a pull request.\nIt is evident that solutions and their accompanying arguments in the discussion, collectively termed the design rationale (DR) [1, 7, 12, 29, 54, 58?], play an indispensable role in manual program maintenance [8]. They furnish engineers with guiding principles for developing patch, thus reducing the incidence of flawed fixes [71]. This observation prompts us to inquire: How to effectively incorporate these design rationales to enhance the performance of current APR?\nIn our examination of the current state of APR research, we observed limited utilization of design rationale from discussions. Semantic search methods primarily focus on code similarity search, leveraging existing code repositories to find fixes that previously worked in similar contexts [17, 26]. Semantic-constraint [31, 67] and pattern-based [21, 23, 25, 30] methods further optimize the search process through manually defined rules or templates. These methods are ineffective in leveraging higher-level repair knowledge from human experience [14]. Learning-based models [14, 28, 35, 36, 61] learn empirical knowledge from a large number of defect repair samples to guide the repair process. Existing work [40] has trained deep learning models to generate code patches by adhering to high-level guidelines (e.g. solution description summarized from discussions); however, the models' performance has been limited. This may be attributed to three key factors: firstly, the inherent noise and complexity present in interleaved conversational data [3], making it challenging to obtain high-quality design rationale from discussions [41]; secondly, the intricate process of collaborative problem-solving represented by design rationale [71], which demands strong reasoning abilities [10]; and lastly, the models may lack the requisite background knowledge to properly articulate the technical solutions under discussion [2, 55].\nWe note that work have emerged for efficiently mining design rationale from issue logs [70], providing a solid foundation for leveraging design rationale. Moreover, large language models (such as GPT-4-turbo [6]) have the potential to better bridge the gap between abstract design rationale and concrete source code patches [63]: Some work has demonstrated that LLMs can integrate different types of software artifacts (e.g., bug reports) to better repair programs [37, 69]. Therefore, we aim to explore methods for efficiently integrating design rationale into APR based on LLM and existing tools. The previously analyzed challenges in utilizing design rationale may lead to LLMs experiencing reasoning failures and code hallucinations [55]. Fortunately, recent research has shown that LLMs can better handle complex problems through various types of feedback and self-correction mechanisms [16, 49]. In light of this, we introduce a feedback-based self-reflection framework to better empower LLM to apply design rationales.\nSpecifically, we introduce DRCodePilot, an approach that harnesses design rationale to drive automated patch generation. Initially, DRs are derived by extracting and pairing issue solutions with their corresponding arguments, leveraging the DRMiner tool[70]. Then GPT-4 pinpoints defective segments and generates patches by considering all DRs. For refining the patches, DRCodePilot collects feedback that encapsulates project-specific knowledge from two distinct sources:(1) reference patches generated by a fine-tuned CodeT5P [59], and (2) identifier replacement suggestions through a retrieval technique. Armed with this feedback, GPT-4 reflects on its initial answer, reasoning out final, refined patches.\nTo conduct an evaluation, we created a benchmark consisting of 938 issue-patch pairs by correlating Jira issues with their respective GitHub commits from two actively developed open-source projects: Flink and Solr. We selected five advanced code LLMs as baselines, including CodeLlama [45], StarCoder2 [34], CodeShell [66], GPT-3.5-Turbo, and GPT-4-Turbo (simplified as GPT-3.5 and GPT-4). The results of our experiments demonstrate the superior performance of our DRCodePilot. Notably, in terms of the number of full-match patches (those identical to the gold patches), our model achieved 109 full matches out of 714 samples in the Flink dataset and 18 out of 224 in the Solr dataset. This significantly surpasses the best-performing baseline model, GPT-4, which only managed 23 and 5 full matches respectively, making our model 4.7 and 3.6 times more effective. Furthermore, DRCodePilot improved CodeBLEU scores by 5.4% and 3.9% over GPT-4. Our findings also highlight the beneficial influence of DR, alongside reference patch and identifier feedback, on the quality of generated patches. Despite the imperfections of automated DR extraction by DRMiner, the sole application of these extracted DRs has proven to obviously enhance the full-match ratio across all baseline models impressively. We also showcase that patch quality can be further elevated with improvements in DR quality.\nOur main contributions are outlined as follows:\n\u2022 Framework: We introduce DRCodePilot, a novel feedback-based self-reflective framework that mimics human process in software maintenance. It begins by generating initial patches grounded in design rationale and refines them based on the feedback of reference patches and identifier recommendations. To our knowledge, DRCodePilot is the pioneering effort to merge design rationale with APR techniques.\n\u2022 Data: We collect up to 938 issue-patch pairs from two open-source projects and construct a dataset. We public the dataset and source code of DRCodePilot\u00b9 to facilitate the replication of our study and its application in extensive contexts.\n\u2022 Evaluation: We conduct experiments on our dataset to show that DRCodePilot achieves substantial improvements across different projects, especially a much higher full-match ratio. Ablation experiments further confirms the significant potential of applying design rationales in LLM-based software maintenance."}, {"title": "2 MOTIVATING EXAMPLE", "content": "In this section, we illustrate the potential impact of design rationale on program maintenance using a simple issue example, FLINK-329762 in Jira, as depicted in Figure 2.\nThe discussion of FLINK-32976 centers on a null pointer exception that occurs when starting a Flink standalone cluster with"}, {"title": "3 APPROACH", "content": "In this section, we discuss the design of DRCodePilot. DRCodePilot is designed to work in a realistic software development lifecycle, in which users submit issue reports to a software repository for bug fixing, feature addition or improvement, and the project maintainers have discussions about the issue before craft a patch to resolve it.\nThe DRCodePilot methodology unfolds in five strategic phases: 1) Design Rationale Acquisition (Section 3.1), where we replicate the existing work [70] to mine solutions and corresponding reasons for specific issues from Jira logs; 2) Defective Segment Location and Draft Patch Generation (Section 3.2), pinpointing flawed code areas and crafting initial patches via GPT-4, leveraging the gathered design rationales; 3) Reference Draft Generation (Section 3.3), employing a fine-tuned CodeT5P model against the full project repository to generate benchmarks, addressing GPT-4's long-context limitations; 4) Identifier Recommendation (Section 3.4), offering alternatives for the potential unsuitable identifiers in auto-generated segments; 5) Final Patch Generation (Section 3.5), directing GPT-4 to improve initial drafts based on reference patches and identifier suggestions."}, {"title": "3.1 Design Rationale Acquisition", "content": "The goal of this phase is to distill solutions and their respective arguments regarding specific issues, i.e., design rationales. Various methods have been proposed for extracting designs or solutions from diverse documents within the open-source community, such as mining issue-solution pairs from live chats [48], identifying question-answer dynamics [11], and uncovering design-centric discussions in pull requests [57], among others. For our purposes, we adopt the DRMiner approach [70], which specializes in deriving design rationales directly from issue logs using advanced LLMs.\nIn essence, it combines tailored prompt instructions with specific heuristic features to pinpoint design-related text and align solution-argument pairs. Its superior performance has been validated across 2092 sentences from 30 issues spanning three publicly available open-source systems."}, {"title": "3.2 Defective Segment Location and Draft Patch Generation", "content": "DRCodePilot identifies the defective segment within the buggy function and generates the initial patch using the advanced LLM, GPT-4. GPT-4 is selected for its exceptional performance across a range of natural language tasks and for its capability to generate code based on provided instructions [6].\nDRs extracted from issue logs with DRMiner are integrated into our prompt structure. As demonstrated in Figure 3, our prompt is composed of five sections:\n(1) An Instruction: This is crafted to guide GPT-4 in identifying the defective segment and generating a patch based on the issue"}, {"title": "3.3 Reference Patch Generation", "content": "Advanced general-purpose LLMs are adept at generating code snippets from provided prompts, yet they may fall short in crafting precisely correct patches due to a lack of specific knowledge about the particular projects. To counter this, we utilize CodeT5P [59], a smaller model amenable to fine-tuning within a project's context, to supply reference patches as feedback for GPT-4. This approach allows GPT-4 to reassess and refine its responses when necessary.\nOur methodology for generating reference patches is inspired by the principles set forth in [62]. As depicted in Step 3 of Figure 3, during the reference phase, we obscure the fault-ridden segments previously identified by GPT-4 in Step 2 using the marker <extra_id_0>. We selected CodeT5 due to its proficiency, having been pretrained on unimodal code corpora and bimodal code-text data, endowing it with considerable programming expertise [62]. To tailor it to the coding conventions of the target project, we further fine-tuned it on the project's codebase. Additionally, given the substantial resources required to train larger models, we opted for the 220M variant of CodeT5P in our study.\nFig. 4 shows the fine-tuning process. First, we build a training corpus from the project's codebase. For each Java file within this repository, we build an abstract syntax tree (AST). We then traverse through this AST to identify and extract the content of function nodes-termed \"methods\" in Java. To promote code conciseness, we exclude all comments and blank lines. Functions comprising less than three lines are also excluded, ensuring that only those with sufficient complexity are considered. The bodies of these methods are masked using <extra_id_0>, aligning with CodeT5P's pre-training protocols.\nTo augment the fine-tuning corpus, we create multiple masked instances for each method so that every line within a method body gets masked at least once across different samples. Specifically, we designate the number of lines to be masked, denoted as a, which"}, {"title": "3.4 Identifier Recommendation", "content": "Draft patches and reference patches may contain identifiers that are semantically similar but incorrect in the project context. As illustrated in Fig. 3, the reference patch incorrectly uses the get() call instead of the correct getString(). At this stage, we aim to offer identifier recommendations in these two kinds of patches as additional feedback to GPT-4 for further refinement.\nError-prone identifiers are detected based on a hypothesis: if an identifier appears solely in the generated patch and not elsewhere in the project code files, it is likely to be flawed. Considering the established observation that many solutions for bugs can be found within the same file [5], we source recommended identifiers just from the Java file containing the defect.\nThe core strategy for candidate identifier recommendation involves locating the most similar identifiers within code snippets that resemble the patch. Specifically, we divide the lengthy Java file into snippets matching the length of the given patch. We then calculate the CodeBLEU [44] score between each snippet and the patch, utilizing CodeBLEU due to its widespread application in measuring code similarity, which accounts for both semantic and structural similarities. From the three snippets that exhibit the highest similarity, we extract identifiers and evaluate the cosine similarity between the vectors of these candidate identifiers and"}, {"title": "3.5 Final Patch Generation", "content": "In the final step, GPT-4 adjusts its initial patches by incorporating feedback from Step 3 and Step 4. Particularly, the model's responses are reformulated into a User Assistant-dialogue format, complemented with the draft and reference patches along with suggested identifiers. Using this context, GPT-4 crafts the enhanced patch.\nThis refinement occurs during Step 5, as illustrated in Fig. 3. Here, our prompt carefully defines GPT-4's role as a proficient code-fixing expert. By doing so, the model is directed to tap into its vast reservoir of training data, enabling the provision of sophisticated, expert-level code rectifications [22]. Furthermore, it follows a deliberate \"follow my tips step by step\" chain-of-thought procedure [60] (i.e., creating an initial patch, incorporating feedback, and refining the solution), which facilitates a clear understanding of the troubleshooting progression.\nWithin the reference patch area, we supply exemplars of efficient solutions to steer GPT-4 toward compliance with established coding norms and best practices. GPT-4 can also discover and correct fine-grained errors more effectively based on identifiers recommendations, thereby improving the usability of the final generated patch. Additionally, we outline three distinct output directives to guarantee that the model's output aligns with the project's goals, ensuring the final code is not only structured but also easy to maintain."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "To evaluate the capabilities of DRCodePilot in resolving real life software issues, we answer the following research questions:\nRQ1 (Baseline Comparison): To what extent can DRCodePilot generate patches that effectively fix bugs?\nRQ2 (Ablation experiment): Are the injected design rationale and feedback-driven refinement effective in DRCodePilot?\nRQ3 (DR with other LLMs): What is the effect of integrating the extracted DRs into other LLMs, and does the quality of DRs matter?"}, {"title": "4.1 Research Questions", "content": "To evaluate the capabilities of DRCodePilot in resolving real life software issues, we answer the following research questions:\nRQ1 (Baseline Comparison): To what extent can DRCodePilot generate patches that effectively fix bugs?\nRQ2 (Ablation experiment): Are the injected design rationale and feedback-driven refinement effective in DRCodePilot?\nRQ3 (DR with other LLMs): What is the effect of integrating the extracted DRs into other LLMs, and does the quality of DRs matter?"}, {"title": "4.2 Data Preparation", "content": "Our experimental benchmark is composed of two critical elements: real-world issues enriched with pertinent design rationale discussions, and the corresponding human-crafted patches. While Defects4J [20] holds the title as the most prevalent benchmark in the APR domain, it lacks incorporation of developers' discussions. SWE-bench [19] has emerged as a benchmark that is rapidly gaining attention in the field. While it includes valuable developers' discussions, it unfortunately lacks critical meta-data labels such as the creator's name, comment participants, and comment indexing. This omission presents challenges for effectively extracting design rationales.\nTo address these deficiencies, we have developed our benchmark by selecting issues from the open-source systems Flink and Solr on Jira, ensuring comprehensive meta-data collection from issue logs, and correlating these with relevant GitHub pull requests (PRs) by tracing back through the provided issue links. Our focus on Flink and Solr stems from their status as emblematic large-scale software systems grappling with a wide array of problems.\nWe refine our dataset to only include instances where issues are closed and PRs are merged, which affords us the use of these human-written patches as verified ground truth. Harmonizing with Defects4J's approach, our filtration process scrutinizes patch files, guided by the parameters delineated in Table 1. Our priority lies with Java source file fixes that pivot on code logic-excluding unit tests, configuration files, and the like-and we narrow our scope to PRs altering solely one source file, with a maximum threshold set at 22 lines of change. This cut-off aligns with findings from Defects4J where 95% of patches adjust under 22 lines [52].\nThe data presented in Table 2 showcases the distribution of 'gold' patches based on the number of modified code lines within our benchmark. Notably, the majority of issues are resolved by altering fewer than 10 lines of code. This benchmark, inspired by Defects4J, includes issues that are, on average, less complex than those found in SWE-bench-judging by the scale of changes in lines and files. The rationale for this approach is to primarily investigate the extent to which design rationales embedded in developers' discussions can aid large language models in issue resolution. To this end, we have chosen to match the complexity level found in traditional Defect4J benchmarks. Looking ahead, we intend to tackle more challenging issues by incorporating design rationales from comments.\nFurthermore, there is a noticeable disparity in data volume between Solr (1.1k stars) and Flink (23.3k stars), which likely stems from the different levels of popularity the two projects enjoy on GitHub; lower popularity often results in fewer commits, issue reports, and subsequent fixes.\nIt is also important to highlight that we did not generate test cases for these patches. Although both projects feature comprehensive unit test suites, evolving dependencies can lead to some tests becoming unexecutable over time. Attempts to execute tests using post-merge main branch commits were largely unsuccessful, with only 13 samples passing. Additionally, to avoid unfair comparisons caused by patches appearing directly in comments, all code snippets in comments were replaced with [code] tags."}, {"title": "4.3 Experiment Setting", "content": "Evaluation Metrics. We employ two metrics to assess the effectiveness of the various APR approaches involved. First, we tally the number of full-match patches-those that are identical to the provided gold patches. We use this as the primary metric since our benchmark does not contain test cases.\nSecond, we apply CodeBLEU [44], a widely recognized metric for evaluating code generation quality that compares the semantic and syntactic precision of the generated code against gold code. It should be noted that CodeBLEU is better suited for assessing the quality of code at the level of functions or code snippets, due to its comprehensive analysis of components like syntactic accuracy, data flow, and logical structure, which typically span multiple lines of code. As per Table 2, it is observed that over half of the patches involve fewer than five lines of change. Therefore, we calculate CodeBLEU scores for the entire repaired function, meaning the original function wherein the flawed segment has been modified with the suggested patch.\nBaselines. The baseline methods selected in this experiment include four state-of-the-art large code models:\n\u2022 StarCoder2 [34]: Through a new training technique called Code-BERTa, StarCoder2 improves the model's generalization ability and understanding of programming languages without increasing the number of parameters. The model version used in this experiment is StarCoder2-15B.\n\u2022 CodeLlama [45]: Based on pre-training from LLaMA2 [56], CodeLlama enhances coding capabilities, better follows human instructions, and understands zero-shot tasks. The model version taken for this experiment is CodeLlama-7B.\n\u2022 CodeShell [66]: Using high-quality pre-training data, CodeShell fuses the core features of StarCoder [27] and LLaMA2, supports code-specific generation methods, and has a high-performance and easily extensible context window architecture. CodeShell has a model parameter count of 7B. It outperforms models with the same number of parameters on the HumanEval test.\n\u2022 GPT-3.5 (gpt-3.5-turbo-1106): We choose GPT-3.5 because it is free to use and is widely used as an effective aid to coding, and researchers can easily access and utilize it for experimental comparisons.\n\u2022 GPT-4 (gpt-4-1106-preview) : Successor to GPT-3.5, but with major enhancements in model size, training data, and capabilities, reportedly up to 1 trillion parameters\u2074. It is often considered to represent the highest level of the current large language models."}, {"title": "4.4 Baseline Comparison (RQ1)", "content": "Table 3 showcases the comparative evaluation of our design rationale-driven approach, DRCodePilot, against baselines that generate patches directly from buggy functions. We detail the number of full matches and their proportion relative to all samples. Our method outperforms the baselines across both systems in the benchmark, as indicated by the number of full-match instances and CodeBLEU scores. Among the baselines, GPT-4 leads, followed by GPT-3.5, CodeLlama, StarCoder2, and CodeShell.\nDRCodePilot significantly surpasses the best-performing baseline, GPT-4, in generating full matches. For instance, it achieves 109"}, {"title": "4.5 Ablation Study (RQ2)", "content": "Three types of knowledge play a crucial role in our DRCodePilot: design rationale for initial patch generation, reference patches and identifier recommendations for patch optimization. We aim to assess the contribution of each knowledge type to the quality of the final patch produced.\nTo accomplish this, we crafted three variants of DRCodePilot for our study: 1) DRCodePilot-DR, which omits design rationale from the prompt during the initial patch creation phase (removing Step 1 and changing Step 2 in Fig.3). 2) DRCodePilot-PF, which does not incorporate the generation of reference patches (removing Step 3 from DRCodePilot shown in Fig. 3). And 3) DRCodePilot-ID, which removes identifier recommendations from the process (excluding Step 4 from DRCodePilot illustrated in Fig. 3). All other experimental variables and procedures are kept consistent across the three variants.\nThese three variants were assessed using our benchmark, with the final results presented in Table 4. To better illustrate the comparative performance, we calculate the performance drop ratio for each variant relative to the original DRCodePilot across the two metrics. Specifically, if the metric value for DRCodePilot is denoted as \u03b1, and the corresponding value for one variant is \u03b2, then the performance drop ratio is computed as $\\frac{\\alpha-\\beta}{\\alpha}$. A positive ratio, indicated by a downward arrow and red font, signifies that excluding a certain type of knowledge has weakened the performance of that variant.\nGenerally, the absence of any knowledge type tends to diminish the model's performance, affecting both the count of full-match patches and CodeBLEU scores. Notably, the most pronounced performance decrease is observed with DRCodePilot-DR in terms of both the number of full matches and CodeBLEU metrics. The count of full-match patches plummeted by 81.65% and 94.44%, while the CodeBLEU scores fell by 9.17% and 4.87% across the two systems, respectively. This suggests that incorporating design rationales into the initial patch generation prompts can significantly improve patch quality, as they offer pertinent discussions that catalyze GPT-4's issue resolution capabilities. It's unsurprising that the decline in CodeBLEU is more modest than in full-match counts due to the relatively small proportion of changed lines within the entire defective functions.\nAdditionally, the greater reduction in full-match patches observed in Solr compared to Flink may suggest that design rationales assume a more crucial role in resolving complex issues, as evidenced by the larger amount of code line changes in Solr relative to Flink. We aim to further investigate this hypothesis in future work.\nThe second most noticeable performance decline is seen with DRCodePilot-ID, that is, upon removing identifier recommendations (Step 4), with drops of 3.67% and 5.55% in full-match counts, and 0.51% and 0.87% in CodeBLEU, respectively, for the two systems. These figures reveal that even an advanced code-focused LLM like GPT-4 can err in generating precise correct identifiers during issue repairs. However, our feedback-based, self-reflective prompt guides it towards re-evaluation and refinement of unsuitable identifiers.\nUpon reviewing the outcomes of DRCodePilot-PF, we observe that the count of exact-match patches remains constant in Flink and experiences a marginal increase in Solr. Concurrently, all CodeBLEU scores within these two systems exhibit a downturn. This indicates that GPT-4 does not merely replicate the reference patch but rather assimilates pertinent information from such feedback to refine its responses. Furthermore, we aim to investigate scenarios where project-specific reference answers contribute to the creation of superior quality patches. A thorough examination of the exact-match patches produced by both DRCodePilot and DRCodePilot-PF suggests that when design rationales behind issue resolutions are discernible, the sophisticated GPT-4 model demonstrates proficiency in generating high-caliber patches. In contrast, reference patches derived from simpler models can erode its confidence, leading to the displacement of exact matches. On the flip side, in cases where commentary on issues is scant, reference patches seem advantageous. To elucidate this phenomenon, we provide a representative example in Figure 5."}, {"title": "4.6 General Impact of Design Rationale (RQ3)", "content": "In this section, we aim to address three sub-questions. Firstly, our framework has thus far only utilized design rationales in conjunction with GPT-4. We are intrigued by the broader applicability of this approach in APR tasks. Specifically, we seek to understand the extent to which these design rationales may bolster the effectiveness of different models engaged in program repair (RQ3.1).\nSecondly, the design rationales we have employed were autonomously derived using the DRMiner methodology [70]. However, the evaluation outcomes presented in the originating paper indicate that the extraction process is not infallible, evidenced by inaccuracies and omissions. In light of this, our experiment sets out to ascertain the degree of improvement in program repair performance achievable through the integration of meticulously hand-annotated design rationales (RQ3.2).\nMoreover, given that the design rationales are gleaned from developers' comments, a natural question arises: can advanced general-purpose LLMs independently comprehend these comments and distill pertinent insights to facilitate issue resolution (RQ3.3)?\nTo explore the above sub-questions, we selected three baseline models capable of interactive prompting: CodeLlama, GPT-3.5, and GPT-4.\nSince expert annotations are involving, we randomly selected 61 issues that featured extensive comments from our benchmark. We focus on the quantity of comments because design rationales are often given in the comments, and we aim to assess the impact of design rationale quality. In other words, the presence of design rationales is a necessary condition for our issue selection.\nWe enlisted 16 participants with varied backgrounds, including two professors with over 12 years of academic experience in computer science; seven Ph.D. students and six postgraduate students engaged in both academic and industrial software research; along with one software engineer boasting more than six years of development experience at internet companies.\nThe annotation process involved independent analyses and joint discussions among three participants. Specifically, each issue was assigned to two of them, who would annotate the design rationales independently. Once the independent annotations were completed,"}, {"title": "5 RELATED WORKS", "content": "The design rationale encompasses the alternative design options considered and decided upon throughout the software life cycle [12, 33, 38], as well as the justifications for accepting or rejecting certain alternatives. It is widely applied to assist human engineers in development and design activities [4, 9, 15, 54, 58, 68]. Effectively documenting design rationale is critical to maintaining the long-term health and vitality of a project, allowing software developers to understand past decisions and continue to implement these core design ideas in future updates and maintenance.\nIn recent years, automatically mining design knowledge from various sources (such as emails, meeting notes, or open-source communities) has attracted widespread research interest [3, 24, 41, 46-48, 53, 57], as a substantial number of decisions actually occur in"}, {"title": "5.1 Design Rationale Extraction", "content": "The design rationale encompasses the alternative design options considered and decided upon throughout the software life cycle [12, 33, 38], as well as the justifications for accepting or rejecting certain alternatives. It is widely applied to assist human engineers in development and design activities [4, 9, 15, 54, 58, 68]. Effectively documenting design rationale is critical to maintaining the long-term health and vitality of a project, allowing software developers to understand past decisions and continue to implement these core design ideas in future updates and maintenance.\nIn recent years, automatically mining design knowledge from various sources (such as emails, meeting notes, or open-source communities) has attracted widespread research interest [3, 24, 41, 46-48, 53, 57], as a substantial number of decisions actually occur in"}, {"title": "5.2 Automatic Program Repair", "content": "Automated program repair (APR) technologies have exerted significant influence across various domains such as software engineering, system security, and artificial intelligence [14]. APR can broadly be classified into four categories: (1) Semantic search-based methods [17, 26] analyze the structure and contextual information of code to find and recommend repair solutions. (2) Semantic constraints-based methods guide the repair process by developing a set of constraint specifications, transforming program repair problems into constraint solving problems [31, 67]. (3) Pattern-based techniques reduce search space through template matching, which can be manually extracted or automatically mined to guide specific types of defect repairs [21, 23, 25, 30]. (4) Learning-based APR learn the experiential knowledge of program repair from a large number of repair samples, having the flexibility and scalability to follow complex instructions and handle types of defects [28, 35, 36, 41, 61, 62, 64]. Panthaplackel et al. obtain useful information from discussions (e.g. solution description) to enhance learning-based APR [40]. They fine-tuned a sequence-to-sequence model to generate the fixed code given varying input context representations.\nRecent work has explored applying code large language models for APR [27, 34, 45, 59, 66]. A prevalent paradigm involves formulating APR as a code generation task, and leveraging prompt engineering techniques to steer the model towards generating more effective patches[42, 63, 65]. One challenge of leveraging LLM is to overcome code hallucination, where LLM sometimes generate code that appears plausible but fails to meet the expected requirements or executes incorrectly [55]. Some studies suggest fine-tuning the LLM for program repair to enhance its effectiveness [13, 18, 51], but the cost is high. Other methods include providing feedback to the model based on external tools and guiding it to self-correct [16, 49], or providing useful information to the model such as bug reports [23, 37], retrieved API context [69], etc.\nIn this paper, we extract comprehensive design rationales (e.g. alternative solutions and arguments), to guide LLM in generating patches using zero-shot inference. To better generate feasible patches, we further devise a feedback mechanism by addressing potential technical information gaps in design rationales."}, {"title": "6 DISCUSSION", "content": "The principal threat to the construct validity in our study is associated with the choice of benchmark. Publicly available benchmarks, such as Defects4J [20] and SWE-bench [19], were not employed due to their lack of developer discussions or the meta-data labels of discussions -data critical for extracting design rationales. Accordingly, we constructed a new benchmark by aligning issues from Jira with their corresponding patches in GitHub. We believe that our custom benchmark will unlock novel possibilities for advancing design-guided program repair research.\nWe limited our evaluation to design rationales extracted solely from Jira, a single issue tracking system. Other platforms such as GitHub issues also host design discussions, which could affect the external validity of our findings. Given the analogous metadata structures between Jira and GitHub issues, we anticipate that our approach would be compatible. Nonetheless, further assessment across different platforms is warranted to validate this assumption.\nManual annotation often poses a threat to internal validity due to potential inconsistencies or biases. To mitigate such risks, we employed standard procedures involving independent annotations followed by collaborative discussions for consensus building, ultimately deciding outcomes through majority vote.\nAnother potential threat to internal validity stems from the content within developer comments. It is unsurprising that comments, or the design rationales derived from them, play a significant role if they contain explicit target code snippets. In practice, developer discussions are typically high-level and advisory in nature. Nonetheless, to minimize the impact of such rare occurrences, we ensure all code within the comments is obscured using a [code] token."}, {"title": "6.1 Threats to Validity", "content": "The principal threat to the construct validity in our study is associated with the choice of benchmark. Publicly available benchmarks, such as Defects4J [20", "19": "were not employed due to their lack of developer discussions or the meta-data labels of discussions -data critical for extracting design rationales. Accordingly, we constructed a new benchmark by aligning issues from Jira with their corresponding patches in GitHub. We believe that our custom benchmark will unlock novel possibilities for advancing design-guided program repair research.\nWe limited our evaluation to design rationales extracted solely from Jira, a single issue tracking system. Other platforms such as GitHub issues also host design discussions"}]}