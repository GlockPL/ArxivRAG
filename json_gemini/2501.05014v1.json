{"title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation", "authors": ["Oleg Sautenkov", "Yasheerah Yaqoot", "Artem Lykov", "Muhammad Ahsan Mustafa", "Grik Tadevosyan", "Aibek Akhmetkazy", "Miguel Altamirano Cabrera", "Mikhail Martynov", "Sausar Karaf", "Dzmitry Tsetserukou"], "abstract": "The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach. The code is available here: https://github.com/sautenich/uav-vla", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the field of aerial robotics has witnessed significant advancements, particularly in the development of unmanned aerial vehicles (UAVs) and their applications across various domains such as surveillance, agriculture, and disaster management [1]. As the complexity of missions increases, the need for effective communication between human oper-ators and UAVs has become crucial. Traditional methods of interaction often rely on complex programming or manual controls, which can be cumbersome and limit the accessibility of these technologies to a broader audience. Previous work in this domain has explored various approaches to enhance the traditional human-UAV communication. Most of the systems mainly focused on manual piloting and basic waypoint navi-gation [2], which required extensive training and experience.\nMore recent advancements have introduced automation and semi-autonomous UAV systems, allowing for improved mis-sion planning and execution. Transformer-based models [3] can generate outputs that represent actions for a robot. For instance, they can produce a set of positions for a robotic gripper in systems like OpenVLA and RT [4], [5], [6]. The works [7], [8], [9], [10] generates a sequence of movements as output, referred to as Visual Language Navigation (VLN) models.\nMany approaches to VLA and VLN necessitate extensive datasets that contain language instructions paired with se-quences representing the agent behavior in the environment. These models are typically restricted to the specific envi-ronments utilized during their training, lacking the ability to generalize to novel contexts. Furthermore, they exhibit limited understanding of the global scale and do not possess a comprehensive representation of the surrounding environment.\nOur research emphasizes the development of systems capable of generating path plans and executing actions solely based on linguistic instructions and open satellite data using only zero-shot capabilities of the powerful models without any model training.\nOur contributions are as follows:\n\u2022 We present a large-scale Vision-Language Action (VLA) system that generates complete path-action sets from a single text-based mission request, integrating textual inputs with satellite images.\n\u2022 We introduce the nano benchmark named UAV-VLPA-nano-30 aimed at fast measurements of the tasks solu-tions made by Vision Language Action systems at global scale.\n\u2022 We validate our system through the experiments on UAV-VLPA-nano-30, demonstrating performance comparable to human-level path and action generation."}, {"title": "II. RELATED WORK", "content": "The introduction of Vision Transformers (ViT) [11], [12] marked a significant advancement in the development of full-fledged models capable of processing and integrating multiple types of input and output, including text, images, video, and more. Building on this progress, OpenAI introduced models like ChatGPT-4 Omni [13], which can reason across audio, vision, and text in real time, enabling seamless multimodal interactions. To address the problem of objects finding in robotics applications, Allen Institute of AI introduced model Molmo, that can point the requested objects on an image [14]. The usage of the transformer-based models allowed the extensive developing of the new methods, benchmarks, and datasets for Vision Language Navigation tasks. Firstly, the problem of Aerial Visual Language Navigation was proposed by Liu et al. [15], where they introduced the Aerial VLN method together with AerialVLN dataset. In [9] Fan et al. described the simulator and VLDN system, that can support the dialog with an operator during the flight. Lee et al. [7] presented an extended dataset with geographical meta infor-mation (streets, squares, boulevards, etc.). The introduction of dataset was paired with the new approach for goal predictor. Zhang et al. [10] took a pioneering step by building a universal environment for embodied intelligence in an open city. The agents there can perform both VLA and VLN tasks together online. Gao et al. [16] presented a method, where a map was provided as a matrix to the LLM model. In that work was introduced the Semantic Topo Metric Representation (STMR) approach, that allowed to feed the matrix map representation into the Large Language Model. In [17] Wang et al. presented the benchmark and simulator dubbed OpenUAV platform, which provides realistic environments, flight simulation, and comprehensive algorithmic support.\nGoogle DeepMind introduced the RT-1 model in their study [5], wherein the model generates commands for robot opera-tion. The researchers collected an extensive and diverse dataset over several months to train the model. Utilizing this dataset, they developed a transformer-based architecture capable of producing 11-dimensional actions within a discrete action space. Building on the foundation of RT-1, the subsequent RT-2 model [6] integrates the RT-1 framework with a Visual-Language Model, thereby enabling more advanced multimodal action generation in robotic systems. The work of [18] and [19] highlights the potential of transformers and end-to-end neural networks to handle complex vision-language-action (VLA) tasks in real time."}, {"title": "III. DATA AND BENCHMARK", "content": "To estimate the overall success of the proposed system, we introduce a novel benchmark dataset UAV-VLPA-nano-30 to evaluate the effectiveness of the UAV-VLA. This bench-mark comprises 30 high-resolution satellite images collected from the open-source platform USGS EarthExplorer. Designed specifically for mission generation in aerial vehicles, the benchmark provides a standardized testbed to assess the UAV-VLA system's ability to interpret linguistic instructions and generate actionable navigation plans.\nThe benchmark spans diverse locations across the United States, including urban, suburban, rural, and natural environ-ments. These include: buildings (living houses, warehouses), sport stadiums, water bodies (ponds, lakes), transportation infrastructure (crossroads, bridges, roundabouts), fields, and parking lots. The benchmark satellite images were captured during the spring and summer seasons under daytime conditions, ensuring clear visibility and consistent lighting.\nThe satellite imagery has a resolution of approximately 1.5 meters per pixel, providing detailed visual representation of natural and man-made features. Each image spans an area of roughly 760 sq. meters, offering sufficient geographic coverage for mission generation tasks. Each of the image has a metadata (geographic location description), allowing the calculation of the identified points in latitude and longitude for flight plan generation."}, {"title": "IV. METHODOLOGY", "content": "In this paper, we present a novel UAV-VLA system that leverages Large Language Models (LLMs) and Vision Lan-guage Models (VLMs) for action prediction in aerial tasks. As shown in Fig. 1, the framework comprises three key modules: goal extracting GPT module, object search VLM module, and actions generation GPT module.\nThe process begins with a language instruction:\n$I = {l_1, l_2, ..., l_k}$,\nwhere I is the input prompt of length k, varying by task complexity. For instance: \"Fly around all the buildings at a height of 100 meters and come back.\"\nThe goal extracting GPT module parses I into a set of goals:\n$G = GPT(I) = {g_1,g_2,\u2026\u2026,g_n}$,\nwhere G contains goals derived from the instruction, tailored to the task.\nThe object search VLM module identifies these goals in the satellite image, producing processed points:\n$P_p = Molmo(G) = {[g_{1,1},g_{1,2}], [g_{2,1}, g_{2,2}], \u00b7\u00b7\u00b7, [g_{n,1}, g_{n,2}]}$\nThese points are transformed into global coordinates using metadata:\n$P_g = f(P_p) = {[lat_{1,1}, lon_{1,2}], ..., [lat_{n,1}, lon_{n,2}]}$,\nensuring accurate mapping to real-world locations.\nFinally, the actions generation GPT module uses Pg, mission details, and MAVProxy [20] to generate UAV actions:\n$A = GPT(P_g, [A_b]) = {A_1, A_2, ..., A_n}$"}, {"title": "V. EXPERIMENTS", "content": "This section evaluates the UAV-VLA system using the benchmark introduced in Section III, focusing on flight plan creation and a novel evaluation metric to assess system effec-tiveness."}, {"title": "A. Evaluation Metrics", "content": "The evaluation metric considers two aspects: the total length of the generated path and the error between each system-assigned point and the corresponding point in the human-generated trajectory (ground truth).\nTo compute the error, three methods were used to compare the system-generated and ground-truth trajectories. The Se-quential Method aligns points step-by-step in their respective order, providing a measure of sequential similarity but prone to cumulative errors over longer trajectories.\nDynamic Time Warping (DTW) [21] enables non-linear alignment by adjusting the trajectories through stretching or compressing sections, effectively measuring path similarity without strict sequence matching.\nK-Nearest Neighbors (KNN) matches each system-generated point to the nearest point in the ground truth based on spatial proximity, offering a general measure of accuracy without considering the order of points.\nThe error is quantified using the Root Mean Square Error (RMSE) calculated using Eq. 6:\n$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_n - \\hat{x}_n)^2 + (y_n \u2013 \\hat{y}_n)^2}$,\nwhere $x_n$ and $\\hat{x}_n$ are the system-generated and ground-truth points, respectively, and n is the total number of points."}, {"title": "B. System Setup and Procedure", "content": "The system was evaluated using the command described in Section IV: \"Create a flight plan for the quadcopter to fly around each building at a height of 100 m, return to home, and land at the take-off point\". The experiment was conducted on a PC with an RTX 4090 graphics card (24GB VRAM) and Intel Core i9-13900K processor. Due to memory constraints, the quantized Molmo-7B-D BnB 4-bit model [22] was used.\nWe compared flight plans generated by the UAV-VLA system with human-generated plans."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "The newly developed system has shown a general trajectory length of 77.74 km on the benchmark UAV-VLA-nano-30, which is 13.85 km, or 21.6%, longer than the ground-truth trajectory created by the experienced UAV pilot (Section III). In 7 out of 30 cases, or 23% of the cases, the UAV-VLA generated a trajectory path that was even shorter as can be seen in Fig. 5.\nAs shown in Table II, the sequential RMSE exhibited the largest mean error of 409.54 m per trajectory, which was expected due to its strict reliance on the sequential order of points. The Dynamic Time Warping (DTW) method demon-strated a reduced mean error of 307.27 m, highlighting its ability to account for temporal variations more effectively. The K-Nearest Neighbors (KNN) method resulted in the smallest mean error, as it disregards the sequence entirely and focuses solely on the spatial proximity of points.\nThe UAV-VLA system processes all benchmark images in approximately 5 minutes 24 seconds, 2 minutes for identifying required points using the object search VLM module and 3 minutes and 24 seconds for generating mission files with the actions generation GPT module. This is 6.5 times faster than the human-generated flight plans mentioned in Sec. III-B."}, {"title": "VII. DISCUSSION", "content": "This paper presents a novel approach for UAV Mission Generation on a global scale, enhancing flexibility and ac-curacy in mission planning. By addressing the limitations of traditional manual methods, this approach proves valuable in scenarios where manual intervention is inefficient. The main contributions of this work include:\n\u2022 The benchmark UAV-VLPA-nano-30, providing a stan-dardized framework for evaluating global-scale path plan-ning techniques.\n\u2022 A method to interpret natural language requests into ac-tionable flight paths, generating paths only 21.6% longer than human-created ones, showcasing its efficiency.\n\u2022 A new task for UAVs: language-based path planning, enabling autonomous execution of mission plans from natural language inputs.\nThis approach simplifies human-UAV interaction by en-abling direct communication via natural language, eliminating intermediate devices. Additionally, it lays the groundwork for robot-robot interaction, allowing autonomous mission gen-eration between robots. This innovation paves the way for seamless collaboration between UAVs, humans, and other robots in diverse environments."}, {"title": "VIII. FUTURE WORK", "content": "Future work will focus on creating a specialized dataset for training models in satellite map-based path planning. This dataset will enhance model precision and efficiency in mis-sion generation across various UAV applications. Additionally, we aim to develop an end-to-end model that autonomously generates mission plans from high-level goals, integrating action generation, path planning, and decision-making into a unified framework. This will represent a significant step towards achieving fully autonomous UAV mission planning adaptable to diverse environments and objectives."}]}