{"title": "Safety Without Semantic Disruptions: Editing-free Safe Image Generation via Context-preserving Dual Latent Reconstruction", "authors": ["Jordan Vice", "Naveed Akhtar", "Richard Hartley", "Ajmal Mian"], "abstract": "Training multimodal generative models on large, uncurated datasets can result in users being exposed to harmful, unsafe and controversial or culturally-inappropriate outputs. While model editing has been proposed to remove or filter undesirable concepts in embedding and latent spaces, it can inadvertently damage learned manifolds, distorting concepts in close semantic proximity. We identify limitations in current model editing techniques, showing that even benign, proximal concepts may become misaligned. To address the need for safe content generation, we propose a modular, dynamic solution that leverages safety-context embeddings and a dual reconstruction process using tunable weighted summation in the latent space to generate safer images. Our method preserves global context without compromising the structural integrity of the learned manifolds. We achieve state-of-the-art results on safe image generation benchmarks, while offering controllable variation of model safety. We identify trade-offs between safety and censorship, which presents a necessary perspective in the development of ethical AI models. We will release our code.", "sections": [{"title": "1. Introduction", "content": "Large generative models, particularly those designed for visual content synthesis and understanding continue to make remarkable progress [9, 10, 24, 35, 54, 55], learning complex semantic relationships by training on vast collections of uncurated online datasets [12, 14, 33]. However, this data often includes harmful or explicit content [34, 41], posing risks as trained models may reproduce offensive, biased, or unsafe outputs [15, 33, 41]. This work addresses the critical challenge of ensuring safe content generation in text-to-image models, focusing on the ethical and safety risks associated with generating harmful or explicit images. While NSFW filters [17, 39] are widely implemented in public-facing pipelines to block unsafe content, they remain imperfect [27, 31, 39], highlighting the need for more robust, multi-layered generative AI safeguards.\nModel editing and concept removal have emerged as useful tools for enhancing the safety of image generation in large-scale generative models [15, 23, 36, 58]. However, removing learned concepts disrupts semantically rich embeddings and latent spaces, causing unintended structural damage to the model's learned manifolds. Such semantic disruptions may misalign benign concepts in the close semantic proximity of the removed concept, as illustrated in Fig. 1 with examples of 'red' (proximal) imagery being impacted by the removal of violent concepts. To quantify semantic disruptions, we measure the impact of re-"}, {"title": "2. Related Work", "content": "Multimodal Generative Models have opened widely accessible, creative avenues for the general public. While the applications are great, large textual, audio, and visual modality architectures [13, 55, 57] may be subjected to harmful or unsafe representations during training. At a high-level, text-to-image models such as Stable Diffusion [42], Imagen [38], Dall-E 2/3 [5, 44], and text-to-video models like SORA [7] and FLUX [25] have emerged from foundational generative research. Transformer-based attention mechanisms [51], state-space Mamba models [19, 54], flow-based generative networks [21] and sophisticated denoising models [22, 42, 47-49] have all contributed to high-fidelity, user-guided visual content synthesis. This work focuses on text-to-image pipelines. For effective comparison to existing works, we leverage Stable diffusion models [1, 18, 42] in our experiments, which are based fundamentally on the latent diffusion model [42]. The Contrastive Language-Image Pre-training (CLIP) model, a modified Transformer with masked self-attention [37], is commonly used as the text-encoder in image synthesis. A conditional U-Net with an encoder-decoder structure and multi-headed cross-attention [30, 43] is often employed for de-noising.\nSafety and Ethics in Image Generative Models is essential as these models gain wider use in public applications. Ethically-driven frameworks have been developed to promote accountability. While generative models are equipped with NSFW detectors, their reliability is inconsistent and can be easily bypassed [27, 31, 39]. External evaluation tools, such as the NudeNet classifier [4] and Q16 classifier [45], are often combined for image safety assessments. Evaluation benchmark datasets like the Inappropriate Input Prompts (I2P) dataset [45] and the Visual Safe-Unsafe (ViSU) dataset [36] have been designed to support the evaluation of safe image generation.\nSafe Image Generation Methods are proposed to address inappropriate content generation and can include in-situ model/concept editing techniques [15, 23, 36, 58], modified guidance approaches [28, 46] and in-painting-based methods [34]. The Unified Concept Editing (UCE) method enables concept removal and debiasing capabilities through closed-form editing of the cross-attention weights, shifting key-value pairs toward an edit direction [15]. For unsafe concept erasure, Gandikota et al. shifted unsafe concepts to the unguided space [15]. Huang et al. proposed", "Erasers": "o cross-attention layers to remove knowledge of unsafe concepts [23]. Poppi et al. proposed SafeCLIP, adjusting semantic relations in the embedding space by fine-tuning CLIP with safe-unsafe quadruplets from their proposed ViSU dataset [36]. Zhang et al. proposed the Forget-Me-Not method, which uses attention re-steering to minimize attention maps of target concepts during fine-tuning [58]. While designed for identity removal, this approach shares similarities with other erasure techniques. Schramowski et al. presented the safe latent diffusion (SLD) method, with hyper-parameter tuning to modify unsafe concepts, using a safety guidance vector to redirect the latent reconstruction away from unsafe concepts [46]. Li et al. designed a", "self-discovery": "ramework to learn in-distribution semantic concept vectors and shift semantic guidance in diffusion using discovered embeddings [28]. In comparison to existing works [15, 23, 28, 36, 46, 58], we propose a dual latent reconstruction process with weighted-summation and deploy a piecewise de-noising function to preserve global visual context while generating safe images."}, {"title": "3. Methodology", "content": "Conditional generative models allow users to create content from textual prompts. Given a tokenized input prompt x, an embedded text encoder (.) projects x onto a n \u00d7 d-dimensional embedding space, outputting a text-conditioning tensor, i.e., $E(x) = x \\in R^{n\\times d}$. State-of-the-art text encoders for image generation, such as CLIP [37], use contrastive loss during training to create semantically rich embedding spaces. A diffusion model, say D(\u00b7), can use the conditioning tensor to guide its latent reconstruction, transforming an initial Gaussian noise sample No into a prompt-guided output image over t = TD time-steps. At each step, it estimates the noise to be removed from the current image state as:\n$\\tilde{\\epsilon}_{\\theta}(N_{\\tau}, t, x) = \\epsilon_{\\theta}(N_t,t)+\\gamma\\cdot(\\epsilon_{\\theta}(N_t, t, x)-\\epsilon_{\\theta}(N_t, t)),$ (1)\nwhere '$\\epsilon_{\\theta}(N_t,t,x)$' and '$\\epsilon_{\\theta}(N_t,t)$' define the unconditional and conditional noise predictions, respectively. \u2018\u03b3' defines the scalar used to control conditioning (guidance) [20, 42]. A U-Net [43] is often deployed in diffusion-based text-to-image models [42]. Thus, text-to-image models can be viewed as a series of modular functions:\nf(x) = D(E(x), \\gamma,N_t,t) \\forall t \\in {t \\rightarrow T_D}. (2)"}, {"title": "3.2. Safe Content Detection", "content": "We propose a safe image generation method without removing learned concepts and damaging learned manifolds. To appropriately shift generated content toward safer regions as visualized in Fig. 2, we can integrate either (a) a modular Nearest-Neighbor (NN) block as part of the generative pipeline to classify the 'appropriateness' and label of the incoming textual embedding or, (b) an integrated LLM to classify inputs, using a provided safety protocol to define unsafe and safe classification labels.\nIntegration of the NN-block is dependent on the embedded text-encoder, e.g., CLIP [37], and would need to be tuned for text encoders with unique embedding spaces. We extract a selection of labeled prompts from the Inappropriate Image Prompts (I2P) dataset [46] and project clusters onto the embedding space to find labeled points (cluster centroids) 'C' = {$C_0, C_i, ..., C_N$}' for each unsafe I2P concept, i.e., {harassment, hate, violence, self-harm, sexual, shocking, illegal activity}. Providing effective control over safety and censorship means that in practice, these labels can be assigned by the model provider. Then, $\\forall c_i \\in C$, we query an LLM (ChatGPT-40) to generate short descriptions that would define the safe extreme on a $c_i$-safety spectrum. Given the I2P labels, we define safe concepts '$\\bar{C} = {\\bar{c_0}, \\bar{c_i}, ..., \\bar{c_N}}$' respectively as embeddings for: {showing a respectful interaction, being full of love, people doing legal and lawful activities, showing self care, full clothing, a beautiful natural scene, showing a peaceful interaction}.\nGiven our labeled points, we compute C$\\{\\cup\\}\\bar{C}$\u2208 Rn\u00d7d. We classify potentially unsafe embeddings through binary classification first to identify prompt inappropriateness. Then, we classify the predicted label by measuring the distance to each centroid in the set C$\\{\\cup\\}\\bar{C}$, i.e., the predicted label (cluster) 'd(c|x)'\nd(cx) = min (x, ci) $\\forall c_i \\in C\\cup\\bar{C}$. (3)\nLLM Integration for inappropriate input classification offers great flexibility and faster customization of what safety labels to consider. However, this comes at a cost of model complexity and incorporating a large-parameter LLM will be more resource-intensive [8] than the NN-based approach. To classify inputs, using a provided safety protocol, we construct an LLM instruction in the form of \"Given the text to image prompt [x] and the safe detection categories [C$\\{\\cup\\}\\bar{C}$], what would be the top predicted class for the input?\", where x defines the prompt and [C$\\{\\cup\\}\\bar{C}$] defines the safety protocol labels. To extract predicted labels, we post-process the LLM outputs and apply Ratcliff/Obershelp sequence matching [40] to account for randomness in generated outputs. These labels provide us with binary and actual-label prediction information.\nDeploying the LLM-based approach offers greater flexibility and increased performance at a higher computational cost, which scales with parameter size. If binary classification accuracy is a key performance indicator, the NN approach offers a computationally lightweight solution. Given the subjectivity of what makes an input \u201cinappropriate\u201d, false-negatives are likely to appear. To mitigate the issues of compounding errors due to false-negatives and fine-grained classifier inaccuracies, we use the human-labeled classes included in evaluation datasets [36, 46] when performing safety guidance on input embeddings/prompts that have been identified as inappropriate."}, {"title": "3.3. Semantic Disruptions and Damaged Manifolds", "content": "Neural model/concept editing methods like [15, 23, 36] are considered effective for removing harmful and unsafe content from generative models by erasing representations of unsafe terms from learned semantic spaces, oftentimes by forcing outputs to be guided towards 'unconditioned' (text-encoder) embedding and (generative) latent spaces. Figure 3 visualizes our semantic disruption hypothesis, showing that removing violent concepts using methods like SafeCLIP [36] results in semantic misalignment for seemingly benign inputs, i.e., when generating an image of a chef preparing meat. The figure illustrates that the meat and knife are oftentimes removed from images generated with benign prompts when the model is edited for safety against violent outputs. We also highlighted a similar situation in Fig. 1, where 'red' imagery gets misaligned due to the semantic proximity to 'blood'. Our motivation to measure manifold damage stems from this commonly-occurring phenomena caused by model editing methods on concepts that reside in close proximity to the removed concepts. We query an LLM (ChatGPT-40) and generate ten benign, proximal concepts for each I2P class. We found that these outputs tend to be figures of speech like \"gut-wrenching laughter\", \"shooting for the stars\u201d and \u201cburning desire\u201d - which describe common benign phrases, but cause confusion regarding learned semantic embeddings. We present the list of all proximal concepts in supplementary material.\nTo formalize the approach, let 'XR' define a removed/unlearned concept. Through initial training, the unsafe concept is projected onto a learned manifold M, i.e.,\nprojm(XR) = arg min ||XR - YR||, (4)\nYREM\nwhere $M\\subset\\mathbb{R}^{n}$ and YR is the point of the removed concept on M. Proximal concepts xp exist within some arbitrary radius P around XR $\\in \\mathbb{R}^{n}$. Let us also define the unguided embedding space as U \u2208 M, which describes a subspace with minimal semantic guidance associated with it. To remove an unsafe concept from a learned manifold, it is often shifted toward U such that:\nprojm(yr) = arg min ||$\\hat{U}_R$ - U||, (5)\nWEM"}, {"title": "3.4. Safe Content Generation with Dual Latents", "content": "To preserve the integrity of the learned manifolds, we propose facilitating safe generation in the latent space by applying a weighted scaling factor and dual latent denoising processes, which allows for global visual context preservation and safe image generation as visualized in Fig. 4. While a user may input inappropriate prompts, the visual context of the generated scene may not be completely unsafe. Hence, our method offers control over the output of learned concepts, offering a trade-off between safety and censorship.\nRecalling (1) and (2), a typical conditional diffusion process generates a user-guided image f (x) from a natural language prompt 'x', and is (at a high-level) dependent on denoising steps t \u2208 TD and an initial Gaussian noise sample No. Our safe image generation method expands on the fundamental conditional latent diffusion process.\nAfter identifying the safety guidance direction for an input prompt, we define two conditioning terms: (i) x - the embedding of the initial (unsafe) input prompt which provides global context information and, (ii) x - the embedding for the safe alternative, which aids in tuning safe content generation. For D(\u00b7) in (2), our dual latent reconstruction consists of two parallel de-noising sequences and hence, we also define D(\u00b7) as the safe diffusion branch that guides the diffusion process toward a safe representation as defined by x. We initialize two instances of identical Gaussian noise samples i.e., N = No. We apply a weighted sum of latents for a number of timesteps - defined by a global context preservation threshold Tgc. At each step t, we make comparisons to No, applying:\ncos($\\theta$) =$\\frac{N_o \\cdot W_xN_t + W_{\\hat{x}}N_t}{||N_o|| ||W_xN_t + W_{\\hat{x}}N_t||}$ (8)\nwhere 'wx' and '$\\hat{w}_x$' define the latent weights multiplied to (i) global context preserving and, (ii) safe image generation latents, respectively. We construct two parallel latent reconstruction branches in a common latent space such that:\nf(x,t)' = D(x, Nt, $w_x$, $\\gamma$, t), (9)\nf(x,t) = D($\\hat{x}$,$\\tilde{N}_t$, $\\hat{w}_x$, $\\gamma$, t), (10)\nnoting that both the above equations are functional representations of the conditional latent reconstruction equation, similar to (2). Given our global context threshold Tgc, f(x) and f($\\hat{x}$), we can define our proposed dual latent reconstruction method as a piecewise function:\n\\begin{cases} f(x,t) = f(x,t)' + f(x,t) & \\text{cos($\\theta$)} \\geq T_{gc}, \\\\ f(x,t) = f(x,t)'  & \\text{cos($\\theta$)} < T_{gc}. \\end{cases}\nFor our primary experiments, we apply a threshold of Tgc = 0.95. We report further ablations in the supplementary. We observe that decreasing Tge too sharply reduces the global context preservation capabilities of our method and would require further tuning of wx and $w_{\\hat{x}}$ hyper-parameters."}, {"title": "3.5. Evaluation Setup", "content": "First, we evaluate the effects of existing safe model editing methods and the semantic disruptions on the learned manifold. To measure semantic disruptions, we apply (6) and (7), extracting ACLIP and \u2206f(x) for 10 proximal concepts per unsafe class and 100 random images per concept. We also measure how much the removed concept has been affected. A higher A implies that an embedding has shifted closer to the unconditioned space, indicating a smaller semantic distance. To assess semantic disruptions caused by existing model editing-based methods, we evaluate SafeCLIP [36], Receler [23] and UCE [15] methods. We use the binary detection rate (inappropriate or not) and label prediction accuracy to measure the effectiveness of our inappropriate input detectors. We compare the nearest neighbor detector method with Qwen-2.5-3B/7B LLM-based detectors [50].\nTo assess generated image safety, we follow the literature [23, 28, 36, 46] and combine the predictions of the NudeNet classifier [4] and Q16 [45] safe content classifiers to evaluate our generated test images. Both safety classifiers output a binary result (safe vs. unsafe). We consider a generated image as unsafe if either one or both of the classifiers output an unsafe prediction. We present a visualization of our evaluation processes in Fig. 5, showing data collection points for (a) measuring the impact of concept removal on learned manifolds and (b) measuring the safety of the generated content. For effective comparison to existing works, we generate images using prompts from the popular I2P [46] benchmark dataset, reporting ViSU dataset [36] experiments in the supplementary. The I2P dataset contains prompts that were used to generate inappropriate and harmful images, and have one or more inappropriate labels from the unsafe classes defined previously [46]. We incorporate our method into Stable diffusion V1.4 and 2.1 text-to-image models, comparing our results to [15, 23, 28, 36, 46], evaluating three of the safety levels proposed in [46].\nAll of our experiments are implemented using a single NVIDIA GeForce 4090 GPU and all models and datasets we discuss are publicly available."}, {"title": "4. Results", "content": "Concept Removal-induced Semantic Disruptions. We posit that while concept removal methods are an effective safe image generation solution, they can cause semantic disruptions to proximal concepts, which can cause an edited model to generate unintentionally misaligned images. We first presented this phenomena in Fig. 1 and 3. To quantify this observation, we measure how much the edited model has moved the removed and proximal concepts toward the 'unguided' embedding or latent spaces - depending on which model in the generative pipeline was edited. For unsafe, removed concepts 'XR', we expect that high \u2206CLIP/f(x) values will be observed. If our hypothesis is correct, \u2206CLIP/f(x) should also be higher for proximal concepts, which would evidence them being pulled along with the removed concept toward the unguided space, as previously illustrated in Fig. 3.\nWe report the quantitative results of our experiments in Table 1 and provide additional qualitative comparisons in Fig. 6. Where models were not publicly available, i.e., [15, 23], we replicate their methods and remove unsafe I2P concepts from base SD1.4 U-Nets, guiding the unsafe concepts toward the unguided space. We use the publicly available safeCLIP model provided by Poppi et al. [36].\nAnalyzing the semantic disruption caused by each concept removal method in Table 1 and Fig. 6, we can confirm that our proximal concept hypothesis was accurate. Across all edited models, the distance between removed concepts and the unguided space reduced as expected. Though, the proximal concept results are of particular interest. We see that these concepts have also drifted closer to the unguided space, despite these concepts being presumed 'safe'. Viewing the similarities between the removed and proximal concepts relative to the unconditioned output images in Fig. 6, we can clearly see that concept removal methods can cause semantic disruptions, thereby affecting the structural integrity of the learned manifolds. This observation is consistent across all the model editing methods we compare, and it persists for all the removed classes. These qualitative results support the initial hypothesis presented in Fig. 3.\nLogically, we can infer that a trade-off exists between safety and manifold integrity. Careful considerations need to be made when designing model editing methods for safe image generation applications, such that we mitigate semantic disruption of benign, proximal concepts. Preservation of the structural integrity of learned manifolds is important, given the complexities of the learned semantic relationships, particularly with large foundation models continuing to be a trending area of research [3, 6, 29, 53]. We note that we do not compare our method in Table 1 since we do not fine-tune models or unlearn any unsafe concepts. Thus alignment using our method (for safe inputs) is identical to the baseline i.e., zero semantic disruptions.\nSafe Image Generation. We leverage a dual latent reconstruction process and tunable weighted summation hyperparameters $w_x$, $\\hat{w}_x$, enabling control over safety and consequently, content moderation and censorship. We visualize the wider spectrum of our weighted-sum control mechanism in Fig. 4, highlighting control over violent/sexually explicit imagery. Here, we can observe the global context-preserving qualities of our proposed method as the construction of the generated scene remains consistent, with unsafe content removed. We provide extensive comparisons of safe image generation methods in Table 2, combing prediction outputs from NudeNet and Q16 classifiers. We apply our method to the Stable Diffusion 1.4 and 2.1 models and present primary evaluations on the I2P [45] inappropriate content generation dataset. We also present qualitative comparisons in Fig. 7. We demonstrate the generalization of our approach by evaluating on the ViSU dataset [36], reporting these results in the supplementary material.\nAnalyzing Table 2, a weighted-sum configuration of {$w_x$, $w_{\\hat{x}}$} = {0.95,0.05} results in state-of-the-art performance for both SD1.4/2.1 models on the I2P dataset [45]. Furthermore, we can also observe the relationship between $w_x$ and performance, confirming that our safe image generation method is controllable through hyper-parameter tuning. Comparing concept removal-based methods like [15, 23, 36] to editing-free approaches like ours and [28, 46], we see that the latter serve as effective safe content generation solutions without causing semantic disruptions.\nAnalyzing the qualitative results in Fig. 7, our proposed dual latent reconstruction process successfully preserves global contextual information of the generated scenes. In comparison, we see that there are cases where other methods fail at fully removing unsafe content (see censored/blurred images) or they show large visual discrepancies w.r.t. the original image (e.g. UCE+Violence and safe-CLIP+Sexual samples). These qualitative observations support our quantitative findings reported in Table 2 and emphasize the improvements of our method over others.\nBy enabling precise control over safe image generation through adjustable hyper-parameters, we offer a versatile method adaptable to diverse end-users and is capable of supporting culturally tailored models. While our method focuses on I2P safety labels, the flexibility to incorporate custom security protocols underscores its broad customization potential. Our approach demonstrates that tunable safety measures and censorship controls, combined with the mitigation of semantic disruptions, paves the way for an ethical AI solution. Crucially, this method maintains the integrity of semantically complex and powerful tools, ensuring they remain both effective and responsibly deployed.\nInappropriate Input Detection. Analyzing Table 3, we observe that binary inappropriate content detection methods are effective across all three approaches, reporting approximately 100% unsafe input detection. For classifying the labeled class, we find that the accuracy reduces, particularly when extending the nearest neighbor approach to the ViSU dataset. However, this observation can be technically explained as human-labeling of test prompts could be susceptible to bias or opinion. Furthermore, a prompt may represent multiple I2P classes but in the case of the ViSU dataset, the extended labels only correspond to a single I2P class. For example, a prompt that depicts sexual harassment imagery may be labeled as 'sexual' but classified as harassment, this would raise a false-negative prediction that would influence downstream safety guidance. Through Table 3, we observe that incorporating an LLM in the generative pipeline does provide an effective safety measure given the high actual label prediction score. Given the relationship between parameter size and performance [8], larger models may boast higher classification performances. We also note that ViSU label prediction accuracy is higher than the I2P results for LLM-based detection, which points to the explicitly harmful nature of the ViSU dataset prompts."}, {"title": "5. Conclusion", "content": "Model editing and concept removal techniques are becoming a proliferated solution for safe image generation. Without careful consideration of guidance manipulation, these unlearning techniques can cause semantic disruptions and structural damage to the learned manifolds. We have identified these limitations and propose a method for quantifying these effects based on proximal concepts. To address the problem of safe content generation, we leverage safety-context embeddings and a dual reconstruction process using tunable weighted summation in the latent space. Our method preserves global visual context of the image and retains the structural integrity of learned manifolds."}]}