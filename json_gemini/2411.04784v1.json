{"title": "Navigating Trade-offs: Policy Summarization for Multi-Objective Reinforcement Learning", "authors": ["Zuzanna Osika", "Jazmin Zatarain-Salazar", "Frans A. Oliehoek", "Pradeep K. Murukannaiah"], "abstract": "Multi-objective reinforcement learning (MORL) is used to solve problems involving multiple objectives. An MORL agent must make decisions based on the diverse signals provided by distinct reward functions. Training an MORL agent yields a set of solutions (policies), each presenting distinct trade-offs among the objectives (expected returns). MORL enhances explainability by enabling fine-grained comparisons of policies in the solution set based on their trade-offs as opposed to having a single policy. However, the solution set is typically large and multi-dimensional, where each policy (e.g., a neural network) is represented by its objective values.\nWe propose an approach for clustering the solution set generated by MORL. By considering both policy behavior and objective values, our clustering method can reveal the relationship between policy behaviors and regions in the objective space. This approach can enable decision makers (DMs) to identify overarching trends and insights in the solution set rather than examining each policy individually. We tested our method in four multi-objective environments and found it outperformed traditional k-medoids clustering. Additionally, we include a case study that demonstrates its real-world application.", "sections": [{"title": "1 Introduction", "content": "Multi-objective reinforcement learning (MORL) is a branch of reinforcement learning (RL) that aims to optimize multiple, often conflicting, objectives simultaneously. MORL has been applied in various domains, including water management ([6, 9]), autonomous driving ([16]), power allocation ([18, 33]), drone navigation ([32]), and medical treatment ([13, 17]). Unlike single objective RL that focuses on maximizing a single reward function, MORL requires balancing trade-offs among multiple goals. As a result, the output of an MORL algorithm is not a single policy, but a set of policies. Depending on the assumptions and the information available, the solution set can be a Pareto set, a coverage set, or a convex hull [10].\nA significant advantage of MORL is that it is inherently more explainable than single-objective RL. MORL outputs a rich set of policies (solutions), each offering distinct trade-offs with regard to the objectives. This multiplicity of solutions not only provides a comprehensive overview of the possible actions and their consequences but also facilitates a deeper understanding of how objectives interact and influence decision-making. Further, MORL avoids the need to collapse multiple goals into a single metric. Thus, decision makers (DMs) could be equipped with a nuanced and detailed understanding of the outcomes, thereby enabling them to make informed choices. This transparency in policy generation enhances the trustworthiness of decisions made using MORL in real-world problems.\nAlthough MORL makes trade-offs explicit, the DMs must still exert considerable cognitive effort to navigate and evaluate the options in the solution set. This process becomes more challenging if the DMs do not have clear preferences on the objectives. Further, as the number of objectives increases, the size of the solution set expands drastically, which makes comprehending the entire set of solutions impractical. Thus, although MORL holds considerable promise in enhancing the explainability of decisions, its practical application is hindered by the associated complexities. To fully leverage MORL'S potential, it is important to develop methodologies and tools to support the DM in exploring the policies in the solution set.\nThe main focus of current MORL research is on developing algorithms and benchmarking their performance (e.g., [2, 8, 34]). However, there is a dearth of tools that allow users (DMs) to comprehend the high-dimensional solution sets the MORL algorithms produce. Such tools facilitate the practical deployment of MORL systems by enabling DMs to analyze policy behavior, understand the trade-offs, and differentiate between policies. Without these decision-support tools, there is a gap in the practical applicability of MORL, hindering the translation of algorithmic advancements into real-world applications. To address this gap, we propose approaches for the post-analysis of trade-offs in the MORL solution sets.\nIn one-shot multi-objective optimization (MOO), e.g., engine design, a common method to convey a high-dimensional solution set involves clustering the solutions based on their trade-offs in the objective space (see [19] for a review on decision support approaches for traditional one-shot MOO). Clustering of solutions [5] in the objective space can streamline the decision-making process by reducing the dimensionality of the solution set, enabling DMs to focus on clusters of solutions rather than evaluating all solutions. Ulrich [29] extends the standard clustering approach by proposing Pareto-Set Analysis, an approach that identifies compact and well-separated clusters in both decision and objective spaces. This approach aids in identifying trends and key turning points in the data and it reveals the relationships between decision variables and objectives.\nWe extend the idea of Pareto-Set Analysis to MORL solution sets. Whereas clustering in the objective space is similar to one-shot MOO, finding clusters in the decision variable space is nontrivial for MORL. In MORL, a solution, policy, is described as a mapping from states to a probability distribution over actions. For problems with large (sometimes infinite) state or action spaces, the solution can be a neural network, making post-analysis in both spaces challenging.\nClustering in the objective space alone is inadequate, as different policies with similar trade-offs can behave differently. For example, two water management policies may yield similar expected returns but differ drastically in terms of disaster risk. Therefore, it's crucial to summarize and compare policy behaviors relative to their expected returns to understand how they align with or diverge from objectives.\nContribution. We address the challenge of decision support in MORL by proposing a method that explains the MORL solution set through clustering, considering both the objective and the behavior spaces. To do so, we show ways to effectively represent the behavior space of the policies by employing techniques from explainable reinforcement learning (XRL) field. Given that representation, we apply Pareto-Set Analysis (PAN) clustering algorithm [29] to find well-defined clusters in both spaces. We apply this method to four diverse MORL environments and show that it improves performance, compared to the traditional k-medoid clustering. We also use one of the environments, the multi-objective highway environment, to demonstrate the importance of considering not just the trade-offs (expected returns) but also policies' behavior when presenting information to the DMs. To the best of our knowledge, we are the first to tackle the challenge of explaining the solution set of MORL."}, {"title": "2 Background and Related Work", "content": "We review three lines of work we build on: 1) MORL, 2) Pareto Set Analysis, and 3) policy summarization."}, {"title": "2.1 Multi-Objective Reinforcement Learning (MORL)", "content": "MORL is used to solve problems where an agent should optimize multiple, potentially conflicting, objectives simultaneously over time. Each objective in MORL is represented by a separate reward function. These problems can be modeled as Multi-objective Markov Decision Processes (MOMDPs) with multiple reward functions. In MORL, a MOMDP is defined by (S, A, p, r, \u03bc, \u03b3), where:\n\u2022 S represents the state space.\n\u2022 A is the action space.\n\u2022 p(. s, a) is the probability distribution over next states given the current state s and action a.\n\u2022r:S\u00d7A\u00d7S \u2192 Rm is the multi-objective reward function with m objectives.\n\u2022 \u03bc is the initial state distribution.\n\u2022 \u03b3\u2208 [0, 1) is a discount factor.\nAn MORL policy, \u03c0 : S \u2192 A, maps states to actions to optimize outcomes across objectives. The action-value function for a policy \u03c0 for a given state-action pair (s, a) is an m-dimensional vector representing the expected return for each objective [10].\nA key concept in MORL is the Pareto frontier-a set of poli-cies whose multi-objective value functions are not dominated by any other policy. A policy's value profile is said to be Pareto dominant if it is better or equal in all objectives and strictly better in at least one objective compared to another policy's value function. The Pareto frontier thus consists of all policies that offer the best possible trade-offs among the objectives, where improving one objective cannot be achieved without worsening at least one other objective [30].\nAnother key concept is the Convex Hull, specifically in the context of linear utility functions. It defines the region where the weighted sum of objectives (as determined by the utility function) can be maximized. For a set of weights, which represent the relative importance of each objective, the optimal policy is one that maximizes this weighted sum. The Convex Hull, therefore, contains policies that are optimal for different combinations of weights, covering all linear preferences in the multi-objective framework [23].\nHypervolume is a popular metric for measuring the performance of MORL algorithms. It measures the volume (in the objective space) enclosed between the solution set generated by an algorithm and a reference point, which typically represents the worst possible values of objectives [31]. The metric provides a measure of how well a set of solutions covers the objective space, indicating the diversity and quality of the solutions with respect to the multiple objectives being optimized. A larger hypervolume generally indicates a diverse set which is also close to the ideal solution.\nGPI-PD. In our experiments, we employ the Generalized Policy Improvement-Prioritized Dyna (GPI-PD) algorithm [3] to train our agents. The algorithm leverages Generalized Policy Improvement (GPI) to establish prioritization schemes that enhance the efficiency of learning from samples. Through the utilization of active learning strategies, the algorithm enables the agent to (i) pinpoint the most promising objectives or preferences to focus on at any given time, accelerating the resolution of MORL challenges; and (ii) determine the most pertinent previous experiences to inform the learning of a policy tailored to a specific agent preference, utilizing an innovative approach inspired by Dyna-style MORL methods.\nGeneralized Policy Improvement (GPI) extends policy improve-ment by defining a new policy \u3160 that improves over a set of policies \u03a0, rather than a single one. For a given weight vector w \u2208 W, the GPI policy is \u03c0GPI(s; w) \u2208 argmaxa\u2208A max\u03c0\u2208\u03a0 qw(s, a). GPI ensures that the selected policy matches or surpasses the performance of any policy \u03c0\u03af \u0395 \u03a0 across all weight vectors w \u2208 W.\nDyna-style algorithms use a search control mechanism to sample experiences from the learned model for planning. They construct a set of policies \u03a0 with value vectors V approximating the CCS. Each iteration involves selecting a weight vector w \u2208 W, guided by GPI improvements, and learning a new policy \u03c0\u03c9 optimized for that weight vector. The algorithm employs GPI-based prioritization to efficiently determine relevant experiences for learning the optimal policy \u03c0\u03c9. GPI-PD learns a multi-objective dynamics model p to predict subsequent states and rewards from state-action pairs, facilitating Dyna updates to the action-value functions using simulated experiences."}, {"title": "2.2 Pareto-Set Analysis", "content": "Pareto-Set Analysis (PAN) is a clustering technique introduced in the field of MOO [29]. Unlike the traditional clustering approaches that focus solely on the objective space (e.g. k-means clustering), PAN identifies clusters that are compact and well-separated in both decision and objective spaces. Also, PAN does not focus on finding just one best partitioning of the solutions. Instead, it helps the DM to better understand the problem by revealing the relationship between different design choices and their outcomes in the objective space.\nPAN is flexible as it does not depend on the identification of specific design variables or feature vectors. The main prerequisite is a distance measure in each space. Since the clusters formed in the decision space may not align with those in the objective space, PAN formulates the clustering as a biobjective optimization challenge (partitioning to perform well in both objective and decision spaces). Thus, the PAN algorithm is a bi-objective evolutionary algorithm tailored for generating a set of effective partitionings.\nThe essence of PAN is its ability to optimize partitioning in both decision and objective spaces, utilizing two validity indices of the clustering: one for the objective space, V(f(C*), do), and another for the decision space, V(f(C*), dp). These indices evaluate the coherence of solutions within each cluster, guiding the algorithm toward partitionings that optimize the trade-offs between being compact in one space and well-separated in the other.\nPAN applies variation and selection processes, iteratively, to evolve a population of partitionings. Each partitioning is assessed based on its clustering quality in both spaces, marking a significant departure from conventional methods that prioritize objectives without considering the structural implications of solutions."}, {"title": "2.3 Policy Summarization Techniques in RL", "content": "We aim to adapt PAN clustering to MORL policy set. Unlike traditional one-shot MOO tasks that focus on one-time decisions, MORL involves sequential decision-making processes. In one-shot MOO, the decision space typically consists of vectors of decision variables that describe, e.g., a candidate design. In contrast, a MORL solution is a function, that maps states to actions, often represented by a neural network [3]. The objective space in MORL can be constructed by calculating the expected returns of each policy. However, the decision space, comprising sequences of decisions predicted by e.g. a neural network for given states, is complex. Consequently, our initial step is to create the decision space, which we refer to as the Behavior Space as it reflects the behavior of the policy. The goal is to summarize the behavior of each policy, enabling the calculation of similarity (or distance) between these summaries.\nTechniques from Explainable RL can be particularly valuable in this context. Specifically, we are using Highlights [4], a policy summarization method, which identifies the most important states for the particular policy and states in the close proximity of those states. These are combined into summary videos of agent behaviors; details can be found in Section 3.1. Other methods developed to summarize agent's behavior use computational models ([11], [14]) or state abstraction ([26], [27]). We decided not to use them as they produce summaries, which cannot be easily interpreted and analyzed for further computation of clusters."}, {"title": "3 Approach", "content": "Figure 1shows an overview of our approach, whose input is the set of policies generated by an MORL agent. Each policy, fi(x), is a mapping function (states to actions), and is represented in two spaces.\nObjective space: An objective vector (vector of objective values), Oi, computed as the expected returns over a number of episodes.\nBehavior space: A behavior matrix, Bi, of five Highlights (Section 3.1) states.\nWe measure the distances between two policies in the objective and behavior spaces, do and dB, via Euclidean and Frobenius distances, respectively (Section 3.2). These distances are used by PAN clustering (Section 3.3), a bi-objective clustering algorithm. PAN outputs a set of partitions (clustering), offering different trade-offs in terms of clustering quality in the two spaces. Each partition, Ci, represents a clustering (set of clusters) of the policies.\nThe key advantage of this method is its flexibility-it allows for the customization of partitioning based on different preferences or requirements. For instance, a decision maker may prioritize the analysis of policy behavior over the optimization of objectives, or vice versa. This adaptability ensures that the clustering process aligns with the decision maker's goals, facilitating a more targeted and insightful exploration of the solution space."}, {"title": "3.1 Policy Representation using Highlights", "content": "The Highlights algorithm [4] is designed to create a video summary of an agent's behavior through online simulations. It calculates the most significant states encountered by the policy, including those nearby. The algorithm uses a concept of state importance, which is judged based on how critical the choice of action in that state is to future rewards, as indicated by the agent's Q-values. This method of determining importance has been effective in identifying teaching moments in student-teacher RL, although it does have limitations, such as sensitivity to the number of possible actions.\nIn the Highlights approach, the state importance was as:\nI(s) = maxa Q\u03c0(s, a) \u2013 mina Q\u03c0(s,a), (1)\nwhere Qs,a) quantifies the expected return of taking action a in state s, following the policy \u3160 thereafter [28]. The original Highlights algorithm outputs videos of the most important transition for the policy. It also includes transitions proceeding and preceding the important states so that these snapshots can be used to show to the users.\nUtilizing the Highlights approach, we identify five most significant states for each policy (as in the original paper). We treat these five states as representative of policy behavior. Subsequently, each policy is represented by five states, which can be represented as a nfeat \u00d7 5 matrix, where nfeat is the number of features the state is represented with (we assume that it is possible to represent the state numerically). We also create video snapshots for later analysis.\nWe execute each policy fifty times to determine the set of five most important states for our analysis. During these runs, we assess the importance of each visited state based on Equation 1. We gather states from 50 executions of the policy, ensuring that each state is only recorded once by discarding any duplicates."}, {"title": "3.2 Distances between the Policies", "content": "Our goal is to cluster policies, considering both objective and behavior spaces. To do so, we define a distance measure for each space.\n\u2022 For the Objective Space, where policies are represented as objective vectors, we employ the Euclidean distance:\ndo (Oi, Oj) = \u03a3nobjx=1(Oxi - Oxj)2, (2)\nwhere Oi, Oi are objective vectors of policies i and j, respectively; and nobj is the number of objectives in the MORL problem.\n\u2022 In the behavior space, each policy is represented by a matrix with each column corresponding to a state. To calculate the distance between two matrices, we use the Frobenius distance:\ndB(Bi, Bj) = \u221a(\u03a3nfeatx=1 \u03a3nst=5y=1(Bxyi - Bxyj)2), (3)\nwhere Bi, Bj are behavior matrices of policies i and j, respectively; nfeat is the number of features a state is represented with; and nst is the number of states the policy is represented with (in our case, nst = 5)."}, {"title": "3.3 Bi-Objective Clustering using PAN", "content": "In our case PAN considers both the objective values of a policy and its behavior, represented by the five Highlights states.\nPAN clustering requires a cluster validity index, which measures how good the clusters are in a space. We employ the well-known Silhouette index [24]. Given a clustering C of N data points, the Silhouette index is:\nS(C) = 1N\u03a3Ni=1 s(i) (4)\nwhere s(i) is the Silhouette index for a single data point i and is calculated as:\ns(i) = b(i) \u2013 a(i)max{a(i), b(i)} (5)\nwhere a(i) is the average distance from the i to the other data points in the same cluster, and b(i) is the minimum average distance from the i to data points in a different cluster, minimized over all clusters. S(C) ranges from -1 to 1, where a high value indicates that the clusters are cohesive and well-separated.\nThe PAN algorithm seeks to minimize two objectives:\nminimize - (-S(Co) - 1) (6)\nminimize - (-S(CB) \u2013 1), (7)\nwhere S(Co) and S(CB) are Silhouette indices, measuring the clustering performance in objective and behavior spaces, respectively.\nWe represent a partitioning of the solution set in the PAN algorithm by a (variable length) list of clusters, where each cluster, in turn, is a list of indices of policies. Similar to the original paper, we also restrict that each partition contains at least two clusters and each cluster contains at least two policies.\nThe evolutionary algorithm starts with a random population of partitionings. Then, variation and selection operations are applied to the generated population as well as local optimization of the population in each generation (see [29]). The selection is conducted based on the hypervolume values, with reference point set to [2, 2] 2\nAs PAN operates as a standard evolutionary algorithm, it requires the configuration of parameters such as the number of generations, population size, and probabilities for various mutation operations. For every environment we tested the algorithm on, we did a parameter search over different combinations to achieve satisfactory performance. The parameter configuration as well as the convergence plots for each environment can be found in the appendix C [20]."}, {"title": "3.4 Algorithm Overview", "content": "Algorithm 1 shows the psuedo-code of our approach. Its input is a set of policies F = {f1,..., fi} and PAN parameters-the population size of the partitionings (n), number of generations to run the evolution for (g), and probabilities of mutation (pm), the union of two randomly chosen clusters (pu), recombination of parent clusters (pr) and split of two randomly chosen clusters (pm).\nFirst, the algorithm computes an objective vector for each policy as its expected returns and a behavior matrix for each policy using Highlights. Given these representations, the algorithm computes distances in objective and behavior spaces. Then, the algorithm performs PAN clustering. The final outputs a set of clusterings. Each clustering represents a set of clusters and the qualities of the clustering in objective and behavior spaces."}, {"title": "4 Experimental Setting", "content": "We conduct experiments in four multi-objective environments from MO-Gymnasium [1]: MO-Mountaincar (3 objectives), MO-Minecart (3 objectives), MO-Highway (3 objectives), MO-Reacher (4 objectives). Appendix C [20] provides a detailed description of the environments, with convergence plots. For brevity, we deep dive into MO-Highway as a case study to demonstrate the applicability and insights offered by our algorithm. We utilize the MO-Highway environment because it closely mirrors real-world scenarios, making it comprehensible for end-users. Additionally, this environment contains only 12 policies within the solution set, enabling a thorough analysis and understanding of the behavior exhibited."}, {"title": "5 Results", "content": "First, we show the performance, measured via hypervolume of the resulting set of clusterings (or partitionings) for all four environments, followed by a detailed analysis of the MO-Highway environment.\nTo benchmark the performance of PAN, we compare the clusterings identified by PAN with the clusterings identified via (iterative) k-medoids clustering [25] (similar to the original paper [29]). The k-medoids algorithm is similar to the k-means algorithm. Whereas k-means requires data points to be represented in an n-dimensional (typically, Euclidean) space, k-medoids only requires a matrix of dissimilarities between data points. This is useful for our setting since it allows any type of policy representation to be used.\nWe apply k-medoids several times for all possible cluster numbers. Each time, we cluster the solutions twice, once in behavior space and once in objective space, and check whether the optimized partitionings satisfy the constraints (i.e. containing at least two clusters, where each cluster contains at least two solutions). For all partitionings that satisfy the constraints, we calculate the silhouette index in decision space and in objective space. Finally, to compare the resulting population with PAN, we reduce the number of achieved solutions to the population size used with PAN, using PAN's selection procedure. We do not consider applying k-medoids to the combined behavior-objective space as the meaning of such a distance would be unclear.\nTable 1 shows that PAN performs comparably to k-medoids in the MO-Reacher and MO-Minecart environments. This similarity in performance can be attributed to k-medoids' focus on either behavior or objective space, potentially leading to better clustering outcomes that enhance the hypervolume by increasing the dispersion of the set of clusterings. In contrast, PAN tends to prioritize a balanced behavior-objectives performance, resulting in more moderate clusterings that do not significantly spread the set of clusters, thus achieving a lower hypervolume. However, in the other two environments evaluated, our method surpasses k-medoids, demonstrating its capability to achieve comparable or superior performance in a broader range of scenarios."}, {"title": "5.1 Detailed Analysis of MO-Highway", "content": "Solution Set. The solution set obtained from the MO-Highway environment consists of 12 policies. Although this is a smaller solution set than what is usually seen in real-world problems [22], it is sufficient to illustrate the challenges associated with exploring potential policies. After analyzing the expected returns of these policies (the trade-off plot can be found in the Appendix A [20]), it was evident that all the policies effectively mitigate collisions (one of the objectives). Thus, in the following analysis, we only focus on the trade-offs between two objectives-speed vs. staying on the right lane.\nObjective and Behavior Spaces We cluster the solution set separately in the objective and behavior space to evaluate whether and to what extent the two sets of clusters differ using k-medoids clustering. Figure 3 shows Sankey diagrams depicting the overlap between clusters in the two spaces. It is evident from these diagrams that the set of cluster differ substantially between the two spaces.\nTo quantify the difference between the two sets of clusters, we employ the Adjusted Rand Index (ARI). ARI adjusts the Rand Index, which measures similarity between two data clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the two clusterings. The values of the Adjusted Rand Index can range from 1 to 1, where 1 indicates perfect agreement between the two clusterings, O suggests random or chance agreement between the two clusterings, and a value less than O indicates that the observed clustering is less accurate than would be expected by random chance [12]. As Figure 3 shows, most ARI values are close to zero. This demonstrates that the clustering in objective and behavior spaces can lead to very different sets of clusters.\nPAN Clustering Having shown that the clusters in objective and behavior spaces differ substantially, we report the results of PAN clustering, which considers both spaces. The ten red dots in Figure 4 are the ten clusterings PAN found. Each of these clusterings considers a different trade-off between the clustering qualities (measured via Silhouette index) in the objective and behavior spaces.\nAs mentioned in the previous section, to benchmark the performance of PAN, we compare the clusterings identified by PAN with the clusterings identified via (iterative) k-medoids clusterings. Of the ten k-medoids clusterings we performed (Figure 3), we only consider four clusterings that adhere to the criteria of valid partitioning. The four blue rectangles in Figure 4 represent the four k-medoids clusterings. First, from the figure, it is evident that the clusterings represented by blue points is either good in the objective space or the behavior space, but there are no compromise clusterings. In contrast, PAN offers a wider range of clusterings covering the full spectrum.\nEach point in Figure 4 represents a partitioning of the solution set. Figure 5 shows how three different partitionings look like in the Objective Space. These partitionings represent 1) compromise clustering, 2) clustering with preference for behavior, and 3) clustering with preference for objective values, represented by Points X, B and O, respectively, in Figure 4. We show only two objectives (even though MO-Highway is a three-objective problem) as one of the objectives (Collision) does not offer any trade-offs.\nFinally, Figure 6 compares PAN clusterings with behavior-only and objective-only clusterings in three cases:\nCompromise clustering The top Sankey diagram shows a compromise clustering from PAN (Point X in Figure 4) in the middle, consisting of two clusters. The left and right columns show the clusters resulting from k-medoids clustering (k = 3) in objective and behavior spaces, respectively.\nClustering with preference for behavior The bottom-left Sankey diagram shows a clustering from PAN which gives behavior the highest preference (Point B in Figure 4), consisting of two clusters, in the middle. The left and right columns are chosen from k-medoids clustering (k = 2) similar to the above case.\nClustering with preference for objective values The bottom-right Sankey diagram shows a clustering from PAN which gives objectives the highest preference (Point O in Figure 4), consisting of three clusters, in the middle. The left and right columns are chosen from k-medoids clustering (k = 3) similar to the above cases.\nFigure 6 also shows ARI values comparing PAN clustering with behavior space clustering, and PAN clustering with objective space clustering. From the diagrams and ARI values it is evident that choosing an extreme clustering from PAN aligns better with the corresponding space (objective/behavior space) than the other space (behavior/objective space). In contrast, the compromise clustering balances alignment with both spaces. Eventually, which of these clusterings to choose depends on the preference of the decision maker."}, {"title": "6 Discussion", "content": "We analyzed the snapshot videos produced by the highlights algorithm [4], and observed distinct policy behaviors-e.g., a tendency to decelerate during lane changes, maintaining a short distance between vehicles, and overtaking with marked caution (keeping too much distance between cars in front)-that are not apparent solely from the trade-offs in the objective space. For comparative analysis, we selected two clusterings to observe variations in cluster configurations, as illustrated in Figures 5a and 5b visualized in the objective space.\nUnsurprisingly, Figure 5b shows that the clusters are well-defined in the objective space for the given clustering. When comparing two clusterings (Point X and Point O), we observe differences in solution distribution solely between Cluster 0 and Cluster 1, while the policies in Cluster 2 remain consistent across both clustering options.\nAll three policies in Cluster 2 are notably fast and exhibit similar objective values. Upon reviewing the highlight videos, it is evident that two of these policies display similar behaviors, characterized by speed and flexibility during overtaking maneuvers. However, the third policy consistently maintains a significant distance from the preceding vehicle, indicating a more cautious approach to overtaking, which in the real-world can be dangerous. As Point X maintains the homogeneity of clusters in both spaces, this is acceptable.\nIn Clusters 0 and 1, two policies (2 and 0) switch their cluster affiliations, reducing the homogeneity of the clusters in the objective space for Clustering X. Analysis of the highlight videos reveals that, for Clustering X, Cluster 1 comprises policies more inclined to overtake compared to Cluster 0. This suggests that while homogeneity in the objective space for Point X decreases, the categorization of policies based on their behavior improves.\nBy offering multiple clustering options, bi-objective clustering enables the selection of the most appropriate one for the problem. For instance, selecting a knee point partitioning-a point on the Pareto front balancing competing objectives where gains in one would mean disproportionate losses in another ensures consideration of both spaces in cluster formation. Alternatively, if the DM has a preference for the number of clusters or one space over another, a suitable clustering can be chosen accordingly while also conveying information about the representation's efficacy in both spaces. This approach not only enhances the DM's awareness but also provides comprehensive information to support informed and explainable decision-making.\nIn our proposed approach, DMs are presented with clusters derived from compromise clustering. Each cluster is characterized by the average trade-offs and behavior patterns of the solutions it contains. These patterns can be illustrated through highlight videos of each cluster's medoid or by identifying significant features within the highlights of the solutions in each cluster.\nFor the MO-Highway, we have analyzed these patterns through video examination; however, a more automated method is feasible through the assessment of feature importance derived from highlight's state representations. We elucidate the distinct behaviors and effectiveness of separation for each cluster, empowering DMs with a comprehensive understanding of each cluster's characteristics.\nDepending on the role of the DM-non-technical DM, technical DM (analyst), or developer-this method can be used for different purposes. For non-technical decision makers, we suggest displaying only representative policies from each cluster, like the cluster's medoid, to allow straightforward selections based on their preferences and avoid overwhelming them. They can then implement these directly or as a stochastic mixture of the cluster's policies [10]. For the technical DM, who is able to analyse solution set and understands how it was produced, it can help to identify trends within the clusters by analysing both spaces and the clusters. Additionally, regarding developers, this method holds the potential for enhancing interactive MORL algorithms, particularly during the preference-learning phase. By frequently soliciting DMs' preferences in an interactive manner, the algorithm can more accurately tailor its focus to areas of interest, thereby optimizing the learning process and outcomes."}, {"title": "7 Conclusions and Future Directions", "content": "We address post-analysis of MORL-generated multi-dimensional solution sets to elucidate trade-offs and policy behaviors for decision makers tasked with policy analysis and implementation. We employ clustering as a means of reducing the complexity of a solution set by developing a partitioning strategy effective in both behavior and objective spaces, optimizing for two indices related to these spaces.\nWe demonstrate that effective partitioning in one space does not always correspond to similar results in the other, especially in MORL contexts where policies with different behaviors produce similar outcomes. We address the bi-objective challenge of partitioning across behavior and objective spaces by using PAN clustering to optimize two validity indices without merging them into a single measure, allowing decision makers to visualize and prioritize trade-offs between the two spaces for informed strategy selection.\nWe identify multiple avenues for future work. A critical next step is conducting user studies to validate the efficacy of the proposed methods as well as the best approach to identify representative clustering. These studies can investigate the utility of different clustering techniques by comparing their performance and user perception. Specifically, it is important to examine how well users can make informed decisions with the aid of additional information like policy behavior and trade-off highlights. In practice, this means presenting participants with problem descriptions and policies characterized by trade-offs, enriched with clustering and behavioral information, to evaluate which format improves decision-making effectiveness. This user-centric approach is pivotal in ensuring that the theoretical advancements in MORL translate into practical tools that enhance decision-making processes in real-world scenarios. Additional to that, we acknowledge that parameter choices for Highlights can be tuned specifically to each environment, which in turn, can yield better PAN clusterings Another important direction is automating the analysis of highlight videos by detecting key features in highlighted states to enhance cluster descriptions, particularly as the number of policies grows. Finally, new methods for capturing policy behavior should be studied further. In this paper we focused on the clustering as a mean to reduce the solution set. However, there exist other approaches for redusing the size of the solution set, such as pruning methods [21], which should be explored further."}]}