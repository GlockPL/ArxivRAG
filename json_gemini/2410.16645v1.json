{"title": "Chatting with Bots: AI, Speech Acts, and the Edge of Assertion", "authors": ["Iwan Williams", "Tim Bayne"], "abstract": "This paper addresses the question of whether large language model-powered chatbots are capable of assertion. According to what we call the Thesis of Chatbot Assertion (TCA), chatbots are the kinds of things that can assert, and at least some of the output produced by current-generation chatbots qualifies as assertion. We provide some motivation for TCA, arguing that it ought to be taken seriously and not simply dismissed. We also review recent objections to TCA, arguing that these objections are weighty. We thus confront the following dilemma: how can we do justice to both the considerations for and against TCA? We consider two influential responses to this dilemma\u2014the first appeals to the notion of proxy-assertion; the second appeals to fictionalism and argue that neither is satisfactory. Instead, reflecting on the ontogenesis of assertion, we argue that we need to make space for a category of proto-assertion. We then apply the category of proto-assertion to chatbots, arguing that treating chatbots as proto-assertors provides a satisfactory resolution to the dilemma of chatbot assertion.", "sections": [{"title": "Introduction", "content": "Chatbots, and the large language models (LLMs) that serve as their engines, have become a central part of social, intellectual and business life. Students use them to construct essays, academics use them to draft grant applications, real estate agents use them to write advertising copy, magazine editors use them to generate content, and many millions of people relate to them as friends and romantic partners. What, exactly, is the nature of our interactions with LLM-powered chatbots?\nThese interactions are routinely described as \u2018conversations'. There is, however, a real question as to whether such talk should be taken literally, or whether we should treat \u2018chatbot conversation' on a par with other applications of psychological terminology (\u2018thinks', \u2018wants', etc.) to machines\u2014useful, but not strictly speaking true. Indeed, many would argue that our interactions with chatbots are no more genuine conversations than monopoly dollars are legal tender, and that interacting with a chatbot involves nothing more than simulating a conversation.\nThis paper examines the question of chatbot conversation by focusing on the case of assertion. Asserting (near synonyms are stating, affirming, or claiming) is one of the many things that we do with language\u2014it is a speech act. Our assertions range from the mundane (\"nice weather today\", uttered at a bus stop) to the consequential (\u201cit's cancer\", delivered by a doctor). According to what we call the Thesis of Chatbot Assertion (TCA), LLM-powered chatbots are also the kinds of things that can assert, and at least some of the output produced by current-generation chatbots qualifies as assertion. Showing that TCA is true wouldn't show"}, {"title": "Motivating the Thesis of Chatbot Assertion (TCA)", "content": "Ever since the development of ELIZA in the 1960s, we have been familiar with chatbots\u2014 computer programs that are able to produce natural language outputs on the model of ordinary human interaction. Although there is no serious reason to think that ELIZA is capable of assertion, things are notably different when it comes to current generation chatbots that are underpinned by LLMs (such as Claude or ChatGPT). Henceforth, by \u2018chatbots' we mean those that are powered by LLMs.\nThere are three reasons to take seriously TCA. First, not only are some chatbot outputs sources of information, being informative is arguably part of their proper function (Butlin,"}, {"title": "The Case Against Chatbot Assertion", "content": "This section presents four of the most prominent objections to TCA, leaving the task of evaluating them for later (Section 6).\n3.1 The understanding objection\nOne objection to TCA focuses on the fact that understanding is a precondition of assertion. If making an assertion requires understanding the sentences one utters, and chatbots fail to understand their outputs, then it follows that TCA is false."}, {"title": "Addressing the Dilemma", "content": "Let us take stock. We saw in Section 2 that there is a strong prima-facie case for treating chatbots as conversational partners, capable of making assertions. However, we have also just seen that there are multiple objections to TCA; each of these objections is significant, and their collective force goes a long way towards explaining why few theorists have endorsed TCA."}, {"title": "Proxy-Assertion", "content": "Perhaps the most influential attempt to \u2018split the difference' involves treating machine assertion as a case of proxy-assertion, an idea first proposed by Nickel (2013). Nickel suggests that machines can qualify as \u201cspeech actants to a substantial degree\u201d (p. 495), but he argues that \"ultimate responsibility for artificial speech does not lie with machines, but either with persons or companies, or with nobody at all\u201d (2013, p. 500). His model here is a situation in which a father sends his 8-year-old daughter to buy a bag of flour from the store. As Nickel tells the story, although the daughter speaks, she is not responsible for her speech; instead, that responsibility traces back to her father. It is the father who makes the relevant assertions (or, as the case may be, requests, questions, etc.) and the daughter is involved in those illocutionary acts as a mere proxy (Ludwig, 2018).\nBy treating chatbots as proxy-asserters, this account promises to evade the dilemma we noted above and do justice to both the \u2018pro' and \u2018con' cases. It seems to do justice to \u2018pro' case, for it recognises that interacting with chatbots does indeed involve genuine assertion. It also seems to do justice to the 'con' case for, by treating chatbots as mere proxies, it avoids the need to show that they can meet the various constraints associated with understanding, mental attitudes, mentalizing and normativity that we identified in Section 3. However, despite its promise, there are two problems with treating chatbots as proxy-asserters each of which is potentially fatal.\nFirstly, in typical cases of proxy assertion there is a clear candidate for the \"principal\u201d\u2014 the speaker whose assertion is carried out via the proxy. In Nickel's case, we can trace the illocutionary act back to the father. But in typical interactions with an LLM-driven chatbot, there is no clear candidate for the principal. As Freiman (2024, pp. 483\u2013484) notes, what is"}, {"title": "Fictionalism", "content": "A second attempt to evade the dilemma is advanced by Mallory (2023), who argues that interacting with chatbots involves a kind of (\u201cprop-oriented\u201d) make-believe. We are, in effect, treating the chatbots as a fictional character who has a range of linguistic capacities, including the capacity to make assertions.\nLike the proxy-assertion view that we've just considered, fictionalism also promises to \"split the difference\u201d between the 'pro' and \u2018con\u2019 views, incurring the benefits of both views without the costs. It promises to explain why interacting with chatbots has the phenomenological features that characterize genuine conversation, for episodes of pretending that one is (say) fighting a fire are designed to engender the sense that one is (in some sense) fighting a fire. At the same time, fictionalism doesn't have the commitments that adherence to"}, {"title": "Proto-Assertion and the Ontogenesis of Assertion", "content": "Few capacities have sharp, clearly-defined, boundaries. Think of walking. There is a point at which infants can only crawl, or perhaps walk only with assistance (e.g., an adult holding their hand or a 'walker'). Is a young child who is able to take a few hesitant steps a walker? The issue is moot. They are on their way to becoming a walker\u2014they are in the process of mastering the capacities required for walking\u2014but they are not yet, perhaps, a fully-fledged walker. We might think of them as a \u2018proto-walker'.\nAs with walking, so too with speech. There is a period in which the child has the capacity to understand and use a limited range of words, and to deploy those words in the service of illocutionary agency. For example, in response to the question \u201cWhat did you do today?", "zoo!": "Drawing on your background knowledge, you infer that the toddler went to the zoo. Has the toddler asserted that they visited the zoo? One's intuitions might be uncertain. On the one hand it looks as though what the toddler is doing is akin to what their older siblings (who are clearly capable of assertion) are doing when they say that they went to the zoo. Indeed, they look to be asserting that they went to the zoo in much the way that by saying \u201cbook!\u201d (in a certain context) they are asking for a book to be read to them. Using words to answer questions may not be a paradigm case of assertion (Alston 2000), but it is arguably a step along the path. At the same time, it's unclear whether toddlers meet the various conditions on assertion that we identified in Section 3. For example, it might be doubted whether toddlers understand the semantic content of their utterances. It might be doubted whether they are capable of the range of mental attitudes that are arguably required for assertion. It might be doubted whether they have the mentalizing capacities arguably required for assertion. And it might be doubted whether their speech is norm-guided and sanctionable in the relevant ways.\nIn fact, the situation is even more complicated than the foregoing suggests, for reflecting on the development of illocutionary capacities reveals that many of the central features of assertion themselves admit of gradations. Young children often have partial understanding of the expressions they utter a toddler might reliably utter \u201cdino!\u201d when pointing at a picture of a dinosaur, but they likely have an impoverished (and perhaps very confused) conception of a dinosaur. Exactly what mentalizing capacities young children have is a matter of ongoing debate (see Butterfill 2020; Carruthers, 2013; Lavelle, 2024; Perner, & Roessler, 2014), but there is little doubt that between early infancy and starting school there are radical changes in a child's capacities to understand, track, and appropriately respond to the mental states of its interlocutors. What about normativity? Rakoczy and Tomasello (2009) present empirical evidence that young children have a \u201crudimentary grasp", "that's not a dinosaur, that's a rhinoceros": "nWhere do these considerations leave us? One response is to insist that the challenge here is purely epistemic. Although we may not be able to figure out when children become illocutionary agents, the acquisition of illocutionary capacities has sharp, clearly-defined, boundaries. Before a certain developmental milestone children have no illocutionary capacities at all; following this milestone, they are fully-fledged illocutionary agents. Where exactly that developmental milestone occurs might be difficult to discern (and of course it need not fall at the same point for all children), but\u2014so this line of argument goes\u2014the very nature of"}, {"title": "From Toddlers to Transformers", "content": "In Section 2 we noted that a robust case can be mounted for TCA-the claim that chatbots are capable of assertion. Some of their linguistic outputs have the function of tracking the truth; the contrast between their modes seems to correspond to a contrast between illocutionary acts; and they produce (what appear to be) defences and explanations for their putative assertions, thus conforming to the behavioural profile of asserters to a significant degree. Against this, Section 3 reviewed a number of robust objections to TCA\u2014thus generating the dilemma of chatbot assertion. We suggest that this dilemma is best resolved by treating chatbots as proto- asserters.\nThe key point here is that many of the arguments against TCA are, on closer inspection, better construed as simply establishing the partial presence of those features.\nConsider first the understanding objection, according to which current chatbots lack the semantic understanding necessary for assertion. We are sympathetic to the claim that a full grasp of certain concepts requires sensorimotor engagement with the world, but the objection from understanding goes significantly beyond that claim.\nFor one thing, it's far from evident that sensorimotor capacities are required for grasp of all kinds of concepts. For example, understanding mathematical concepts (ADDITION, INFINITY), moral concepts (WRONG, PERMISSIBLE), and theoretical concepts (ENERGY, GENE) seem to be relatively independent of sensorimotor capacities. What's required, instead, is an adequate sensitivity to their inferential role, and that is something that LLM-driven chatbots may very well have (Piantadosi & Hill, 2022). Even with respect to concepts more closely tied to sensory input (such as RED or LOUD) we should grant that partial understanding might be available via sensitivity to their inferential role. For example, the blind can have partial grasp of RED and the deaf partial grasp of LOUD (Butlin 2023). This is not merely an \u2018in principle' argument, for research into the internal mechanisms of LLMs trained solely on text has revealed structural correspondences between their activation spaces and a variety of real world structures, suggesting that they may have acquired some degree of semantic understanding, in\nat least some domains (Abdou et al., 2021; Gurnee & Tegmark, 2023; Li et al., 2023; Nanda, Lee, & Wattenberg, 2023; Yildirim & Paul, 2024). Finally, many of the arguments against understanding in chatbots have been concerned with text-only systems whose inputs, outputs and training data consist solely of text. Thus, objections from the text-bound nature of chatbots evaporate in the face of the fact that most of the leading systems are now \u201cmulti-modal\"-\nprocessing text, audio and video inputs.\nAlthough LLMs clearly fall short of a full grasp of the words that they generate (Lake & Murphy 2021), a reasonable case can be made for thinking that they have a partial understanding of many concepts\u2014indeed, their grasp of many concepts may be significantly firmer than that of a young child.\nWhat about the attitudes objection? Do (current) chatbots lack the propositional attitudes (chiefly, beliefs and intentions) necessary for assertion?\nHere we would make two points. The first is that the possession of particular mental attitudes may not be an all-or-nothing matter. Do non-human animals have beliefs and intentions? What about very young children? Do delusions qualify as beliefs? Is self-deception intentional? Rather than restricting ourselves to only two answers to these questions (\u201cYes they do\u201d; \u201cNo they don't\u201d), many would argue that we ought to embrace a variety of \u201cin-between\u201d views, according to which these phenomena involve states that are, although not fully-fledged beliefs or intentions, importantly akin to these states (see e.g. Bayne & Hattiangadi, 2013; Schwitzgebel, 2001; Stich, 1979).\nSecond, although it is doubtful that chatbots have fully-fledged beliefs or intentions, they may have internal states which exhibit some (if not all) of the functional signature of beliefs. One line of evidence comes from probing studies which have found directions in\nmodels' internal activations at certain layers that correspond to the truth value of inputs (Azaria & Mitchell, 2023; Burns et al. 2024). Although other theorists have urged caution in interpreting such results (Harding, 2023; Herrmann & Levinstein, 2024; Levinstein & Herrmann forthcoming), even these authors take it as an open empirical question whether LLMs might have belief-like representations.\nPartly in response to these concerns, more recent studies have employed more careful methodology. Marks & Tegmark (2024) curated datasets to avoid generalisation issues present in previous studies, and also investigated the causal role of activity directions identified by probes. For two models from the LLaMA family, they found components of the models' internal activity (specifically, directions in activation space) which tracked the truth value of inputs, and established through intervention that the these components causally mediated outputs in appropriate ways: shifting activations along the identified directions caused the models to treat false statements as true, and vice-versa.\nThis is an active area of empirical research, but even from the current evidence, it seems likely that the LLMs underlying advanced chatbots have representations that are at least somewhat belief-like, and that these play a causal role in their production of linguistic outputs. Further, it is noteworthy that even those who have cautioned against na\u00efve ascriptions of beliefs to LLMs treat the concept of belief as involving a cluster of separable properties, each of which may allow for gradations. For instance, Herrmann & Levinstein (2024), who propose four criteria for LLM representations to count as beliefs, stress that \u201c[t]he satisfaction of these requirements come in degrees; in general, the more a representation satisfies these requirements, the more helpful it is to think of the representation as belief-like\u201d (p. 7). This is pertinent to the discussion at hand\u2500chatbots may turn out to be proto-believers, which lends further (if indirect) support to treating them as proto-asserters.\nThe question of whether LLMs have intentions has received less attention, but we would argue that this possibility should not be dismissed given the situation with respect to belief. More specifically, Todd and colleagues (2023) claim to have found evidence of \u201cfunction vectors\u201d in LLMs that appear to have a world-to-mind direction of fit. These are internal states that seem to serve as compact representations of abstractly-defined input-output functions (or tasks), such as translating a text from English to Spanish, identifying the capital city of a country, or finding the antonym of a word. By causally intervening on these vectors, one can trigger or interrupt the performance of a task, in a way that is robust across a wide range of input text, suggesting that these states may play some of the action-guiding functional role of intentions.\nWhat about the capacity of (current) chatbots to engage in the kinds of mentalizing tasks that are arguably required for assertion? Initial investigations suggested optimism about the theory of mind capabilities of LLMs (Kosinski, 2024), but more detailed studies have tended to show that current LLMs' ability to reason about the mental states of others (if that is even the correct framing) is at best brittle (Shapira et al., 2024; Strachan et al., 2024; Ullman, 2023).\nHowever, reflecting on the case of young children suggests that, while this might prevent full participation in the practice of assertion, we shouldn't take proto-assertion to require the same metarepresentational capacities (which are arguably missing in under-3s) as fully-fledged assertion. Perhaps it requires only some degree of sensitivity to the informational state and requirements of one's audience, which LLMs may indeed have.\nWhat, finally, of worries about normativity? One version of the normativity objection was that (on some views) engaging in the practice of assertion requires understanding the relevant norms, and that chatbots lack such understanding. We have already argued against blanket dismissal of understanding in chatbots, and there is reason to think that norms of assertion are not fundamentally beyond the grasp of chatbots; given that their behaviour is largely consistent with the norms of assertion (see Section 2), it is plausible that algorithms at least implicitly encode these norms\u2014and there may be independent reasons for requiring no more than implicit understanding of the relevant norms (Simion & Kelp 2018; Williamson 2000, p. 241; cf. Pagin 2016). Most importantly, as argued above, the claim that chatbots are proto-asserters requires only that they have partial understanding of the norms governing assertion (indeed, in this respect, they come much closer to the capacities of full-blooded asserters, compared with toddlers).\nA different version of the objection from the normativity came from Butlin and Viebahn (forthcoming). As we saw in Section 3.4, they argue that genuine assertion requires sanctionability, and that while (some) chatbots meet the first two conditions on sanctionability, they flout the third. However, in light of the preceding discussion, we suggest that this mixed result is best construed as further evidence that chatbots are proto-asserters, rather than for the conclusion that they are straightforwardly incapable of assertion: chatbots might not be sanctionable in the full sense that a human speaker is, but (analogously to children) we may assess them against our norms, and some may be appropriately responsive to correction.\nWhat should we say about Butlin and Viebahn's case of the self-calibrating thermometer (discussed in Section 3.4)? Given that this system also meets the first two conditions on sanctionability, one might worry that the account we have sketched would have to treat it as a proto-asserter and this would trivialise the concept. We share the intuition that such a system would be far less deserving of proto-asserter status than (say) a young child. But\""}, {"title": "Conclusion: At the Edge of Assertion", "content": "This paper has advocated for a new conception of chatbots, one which sees them as \u2018proto- asserters', located somewhere in the space between beings that are fully-fledged asserters and those that are non-asserters. Taking this approach to chatbots, we have suggested, does justice"}]}