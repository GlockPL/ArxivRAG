{"title": "Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training", "authors": ["Mingjia Shi", "Yuhao Zhou", "Ruiji Yu", "Zekai Li", "Zhiyuan Liang", "Xuanlei Zhao", "Xiaojiang Peng", "Tanmay Rajpurohit", "Ramakrishna Vedantam", "Wangbo Zhao", "Kai Wang", "Yang You"], "abstract": "Vision Mamba (e.g., Vim [54]) has successfully been in-tegrated into computer vision, and token reduction has yielded promising outcomes in Vision Transformers (ViTs). However, token reduction performs less effectively on Vi-sion Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a high loss of key knowledge and bad performance. This makes it not a good solution for en-hancing efficiency in Mamba. Token merging, which pre-serves more token information than pruning, has demon-strated commendable performance in ViTs. Nevertheless, vanilla merging performance decreases as the reduction ra-tio increases either, failing to maintain the key knowledge in Mamba. Re-training the token-reduced model enhances the performance of Mamba, by effectively rebuilding the key knowledge. Empirically, pruned Vims only drop up to 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in our main evaluation. We show how simple and effective the fast recovery can be achieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs of training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17 minutes, and Vim-S only drop 1.3% with 1.2\u00d7 (up to 1.5 \u00d7) speed up in inference.", "sections": [{"title": "1. Introduction", "content": "Vision Mambas (e.g., Vim [54]) have successfully intro-duced Mamba [13] into computer vision, achieving promis-ing results [28, 34, 48, 54]. The success of Vim and its follow-up works (Vims) are closely tied to the SSM model's efficient sequence processing.\nToken reduction is popular in model efficiency. The efficiency of DynamicViT [37] and its ilk [6, 26, 32] helps conducting effective Transformer with reduced to-"}, {"title": "2. Methodology", "content": ""}, {"title": "2.1. Preliminary: State Space Models", "content": "Structured SSM. Structured Sate Space Model (SSM) is a linear time-invariant system w.r.t. time t, whose discrete form is shown as follows:\n$h_t= Ah_{t-1}+ Bx_t, Y_t = Ch_t$ (1)\nwhere the state matrix A, the input matrix B, and the out-put matrix C are three learnable parameters, xt and yt are respectively input and output, and ht is the hidden state at time t. ht and ht-1 are simplified as h and h_ in follow-ing transfer equations and analyses. These diagonal plus low-rank structure are designed to compute sequence-to-sequence modules efficiently [7].\nSelective SSM. In Mamba [13], proposed Selective SSM change the linear time-invariant system into non-linear time-variant system with a design of a discrete non-linear"}, {"title": "2.2. Discussions", "content": "In this section, we propose explanations about the observa-tions in Fig. 1. The analyses are based on the difference between the Attention Block and SSM from the perspective of information transferring.\nThe key knowledge (i.e., specific and general knowl-edge) is reduced by token reduction, and further the rem-nant tokens and their imbalance lead to performance drop. As shown in Fig. 2, the essential causes are: 1) a large amount of general knowledge is irreparably reduced; 2) spe-cific knowledge keeping ratio is low and imbalanced. Fur-ther experiments support our theorem, if we shuffle token after reduction, only Mamba dropping, which means tokens in Transformer is not sensitive to its order of tokens' in-dexes. Moreover, token reduction's disruption to sequential dependency is one of the reasons for performance dropping, due to the knowledge embedded in the tokens' sequence.\nTakeaways. The main conclusions are as follows:\nMamba is more sensitive in pruning. Due to the marginal enrichment effects proposed in Theorem 1, we show that it's a higher chance to prune tokens containing more general knowledge in Mamba, according to Corollary 2.\nMerging performs better in Mamba. It's because merg-ing keeps more key knowledge by token keeping and filter-ing with similarity, according to Corollary 3.\nRe-training is a simple and effective solution. With most of the key knowledge still retained, the performance can simply recovered by re-training as shown in Tab. 1b."}, {"title": "2.3. Detailed Analyses and Related Theorems", "content": "In order to conduct fair comparison, the modules are as-sumed to be with the same information compression and extraction ability to get rid of the complicated impacts of the models' performance about scale of parameters, train-ing tricks and others. YT and YM are the output signals of the Attention Block and SSM respectively.\nAssumption 1. (Equal compressed information.) The out-put signals have the same entropy, both in general and in inference.\n$H(Y_T) = H (Y_M)$,\n$H(Y_T|X_t) = H(Y_M|X_t), \\forall t \\in [T]$. (3)"}, {"title": "2.4. Method", "content": "According to the aforementioned analyses, one of the main causes about Mamba's sensitivity and performance drop is the loss of key knowledge by token reduction. Therefore, in this section, we focus on the ways to effectively keep and recover the key knowledge in Mamba.\nAs discussed in Sec. 2.2 and Sec. 2.3, compared with pruning, merging remains the general knowledge in the reduction tokens by fusing similar tokens. As results in Tab. 1a, 1) merging keeps higher performance than prun-ing, especially in high reduction ratios. More given general knowledge restored, 2) re-training is simple but effective to recover the performance in Mamba with limited specific knowledge, as shown in Tab. 1b.\nConsistent with these facts empirically and theoretically, our proposal is that both token merging and re-training should be combined for key knowledge keeping and re-covering. Our framework, R-MeeTo (Re-training Merged Token), is therefore simple, compatible, and effective si-multaneously. The framework is simple but comprehensive presented as following, and more detailed implementation are in Appendix B."}, {"title": "3. Experiment", "content": ""}, {"title": "3.1. Settings", "content": "Datasets and models. We conduct all of our experiments on the ImageNet-1K [8] classification task and report top-1 accuracy (%). All images are augmented and resized to 2242 for evaluation. The baseline Mamba models comprise three variants of Vim [54]: Vim-Ti with 7 million parame-ters, Vim-S with 26 million parameters, and Vim-B with 98 million parameters. Following training techniques used in previous work [37, 45], all baseline models are initialized using pretrained weights [54].\nImplementation details. All experiments are conducted on a single machine equipped with 4 NVIDIA TESLA A100 40GB GPUs. During training, we use a batchsize of 128 with gradient accumulation performed over two steps, re-sulting in an effective total batchsize of 1024. Moreover, all models are trained with AdamW optimizer with a learning rate decaying from 2e-5 to le-6 using a cosine scheduler, and a weight decay of 5e-2. To ensure consistent FLOPs across R-MeeTo and other comparison methods, the token reduction ratio is set by default to 0.14 for Vim-Ti/DeiT-Ti and 0.31 for the other models. The blocks of even indexes except the 0th block are selected to merge. By default, to-kens' features (Xt) are merged in R-MeeTo, and are then reordered to preserve the original sequence order."}, {"title": "3.2. Comparative Experiments", "content": "Comparative designs. To validate both our theoretical claims and the practical effectiveness of R-MeeTo, we con-duct a comparison of top-1 accuracy and FLOPs against two state-of-the-art token pruning techniques in Mamba: Token Recognition [26] and Hidden State Alignment [51]. Addi-tionally, to assess the generality of R-MeeTo, we compare the performance of the re-trained VideoMamba (VideoM) [23] with token merged and the original pretrained one. Specifically, Vim-Ti and VideoM-Ti, due to their low ca-pacity, are re-trained with token merging for 30 epochs."}, {"title": "3.3. Faster Mamba in Minutes", "content": "We conduct re-training experiments for 3 epochs on Vim-Ti, Vim-S, and Vim-B for ImageNet-1K classification. The experiments are performed on a single machine equipped with 8 NVIDIA H100 GPUs. Additionally, we conduct the same experiments on two and four machines, each with 8 NVIDIA H100 GPUs, connected via InfiniBand [35]. This setup allows us to evaluate the scalability and per-formance of R-MeeTo across different hardware configu-rations. Specifically, gradient accumulation is performed over two steps, with the per-GPU batch sizes set as follows: Vim-Ti at 2304 = 1152 \u00d7 2, Vim-S at 1408 = 704 x 2, and Vim-B at 512 = 256 \u00d7 2. This configuration ensures optimal utilization of GPU memory for each model variant.\nWe report the wall time (in minutes) for each re-training in Tab. 3. As shown, all Vims are re-trained within 60 minutes. Notably, re-training the Vim-S model after token merging on 4 \u00d7 8 \u00d7 H100 requires less than 10 minutes. Give us minutes, we back a faster Mamba."}, {"title": "3.4. Ablation Study", "content": "Case study on token order: odd-even shuffle. We first conduct a case study to examine the impact of token order after token merging in R-MeeTo. Specifically, we re-train Vim and DeiT models on the ImageNet-1K for 3 epochs using R-MeeTo, comparing results with or without token shuffle after merging. The shuffle strategy is odd-even shuffle, for example, indexes from 0 to 3: [0,1,2,3] \u2192 {[0,2], [1,3]} \u2192 [0, 2, 1, 3]. It works in Transformer [2].\nAnalyses. The final top-1 accuracy comparison is presented in Tab. 4. As shown, token re-ordering has minimal im-pact on the performance of DeiT models during re-training and token merging, with no significant difference in top-1 accuracy when re-ordering is applied or omitted. In con-trast, maintaining token order substantially enhances per-formance for Vim models, with comparable improvements for Vims, respectively. These findings validate our theoret-ical claims and highlight the critical role of maintaining to-ken order in Vims, particularly for the smallest variant (i.e., Vim-Ti), underscoring its importance in token reduction.\nQuantitative study on token order: shuffle ratio. To further investigate the role of token order in Vims during token reduction, we first conduct experiments where only rs% of the tokens' features (Xt) are shuffled before each token reduction operation. Here, rs denotes the shuffle ra-tio, defined as the proportion of unordered features relative to the total number of features. Next, to explore how differ-ent level of shuffle ratios influence varying token reduction methods, we evaluate the performances of re-trained Vim-S models using both token pruning and merging. All models are re-trained for 3 epochs for consistency in comparison.\nAnalyses. Tab 5a shows the performance of training-free and re-trained Vims under different shuffle ratios. As ob-served, the performance of training-free Vims drastically declines as the randomness in the ordering of feature se-quences increases. This emphasizes the importance of to-ken order in the Vim architectures. This effect is particu-larly pronounced for Vim-Ti, likely due to its fewer num-"}, {"title": "4. Related Work", "content": ""}, {"title": "4.1. Vision Mamba", "content": "Mamba [13], based on SSM [11, 14, 31, 41], achieves a competitive performance with Transformer [46] with only linear complexity of #tokens. Recently, many works[5, 15, 17, 19, 23, 28, 33, 34, 38, 40, 48] explore the effectiveness of Mamba in computer vision.\nThe architectures of Mamba in computer vision have two main branches: 1) Vim series [19, 23, 54], design a more ef-fective bi-directional scanning block. 2) Vmamba [28] se-ries [28, 33, 34, 48], focus on cross-scan techniques. More-over, different types of data are employed, e.g., video [4, 18, 23, 24, 49], 3D [16, 25, 27, 52], multimodel [9, 24, 36, 40], motion sequence [53]. However, these works primarily fo-cus on mechanisms and data, lacking further optimization of existing models. Our proposal is simple and effective to accelerate Vision Mamba's inference with token reduc-tion, and then recover the caused performance drop by key knowledge rebuilding."}, {"title": "4.2. Token Reduction", "content": "Token pruning, as a popular strategy to reduce tokens, has already demonstrated great potential in accelerating Trans-formers in both natural language processing [12, 20, 21] and computer vision [10, 32, 37, 42, 50]. However, directly deleting tokens inevitably loses the information of pruned tokens. Merging [2, 6, 22, 26, 30, 39, 47], as an alterna-tive of tokens reduction, preserves the information from dis-carded tokens. As an example of merging in Transformers, ToMe [2] achieves inference acceleration without training.\nHowever, Mamba has fundamental differences from Transformers, making it challenging to apply methods from Transformers to Mamba. The difference comes from the sequence dependency of tokens in SSM.\nThe most related work is Hidden State Alignment [51], which designs a selective skipping mechanism to choose pruned tokens in Vims. This work focuses on pruning meth-ods in Mamba. Besides Hidden State Alignment [51], a limited number of works currently reveal the essential cause of performance dropping by token reduction. The method about merging and its usage in Mamba is even less. Our work provides analyses about the main causes, availability of merging, and further gives effective solutions for both accelerating and recovering pruned Mambas' performance."}, {"title": "5. Conclusion", "content": "In this paper, we propose three main observation about to-ken reduction in Mamba: 1) Mamba is more sensitive in to-ken reduction; 2) Merging keep more key knowledge than pruning; 3) re-training can recover the dropped performance simply and efficiently. From the perspective of information transfer, we analyses the main causes of the sensitivity and performance drop are the enrichment effect, the imbalance knowledge storage in Mamba, and the loss of key knowl-edge. Empirically, we provide verification about our theory and the compatibility and efficiency of the proposed recov-ering strategy, i.e. R-MeeTo, on token reduction."}, {"title": "A. Detailed Experiment Settings", "content": "Token reduction ratio. We use a reduction number of each layer r to control the whole token reduction ratio. Table 10 shows the relationship between the reduction of each layer and the token reduction ratio."}, {"title": "B. Implementation Details", "content": "The framework of R-MeeTo is shown in Algorithm 1, and the detailed modules are distributed in Algorithm 2, Algo-rithm 3 and Algorithm 4.\nOur R-MeeTo is consists of 2 main modules: token re-duction and re-training. The first module follows ToMe [2], but our implementation is on Mamba instead of Trans-former. We therefore propose our token merge method in the algorithms and simply use re-training to recover perfor-mance. The intuitions of each process are presented in the comments."}, {"title": "C. Analyses Details", "content": ""}, {"title": "C.1. Explanation", "content": "Tokens order. The token order means that before merge, we save the time t order of the tokens, and after merge, we sort the tokens in the original time t order (default setting)."}, {"title": "C.2. Theoretical Analyses", "content": "Proposition 2. (No dependency before in SSM's tokens.) In a SSM block, as a sequential components, its outputs Yo:t before time t do not have dependency on the inputs Xt at t. Thus, we have:\n$I(Y_M; X_t) = 0, \\forall t \\in [T]$.\nDefinition 1. (Mutual information) Mutual information be-tween 2 signals is defined with Shannon entropy H as:\n$I(X; Y) = H(X) \u2013 H(X|Y)$\n$= I(Y; X) = H(Y) \u2013 H(Y|X)$\nDefinition 2. (Interaction information's definition.) Inter-action information between 3 signals is defined with Shan-non entropy H as:\n$I(X; Y; Z) := H(X) + H(Y) + H(Z) + H(X, Y, Z)$\n$\u2013 H(X,Y) \u2013 H(X, Z) \u2013 H(Y, Z)$.\nLemma 1. (Equal total knowledge.) The amount of to-tal knowledge, compressed by Attention and SSM blocks from inputs Xt at time t, is the same if Assumption 1 holds.\n$I(Y_T; X_t) = I(Y_M; X_t),\\forall t \\in [T]$"}, {"title": "D. More Experiments", "content": ""}, {"title": "D.1. Experiments on Videos", "content": "Comparative experiment on VideoMamba. We perform experiments on the Kinetics-400 (K400) video dataset [3] to further evaluate our R-MeeTo.\nSettings. The clip is set to 8 frames per video. We set r=88 for VideoM-Ti/S/M. We apply the base learning rate of 2e-5 and re-training epochs of 30 for VideoM-Ti and 15 for VideoM-S/M. The top-1 accuracy and FLOPs are reported. We merge for every two blocks of the models. We merge 11 times in VideoM-Ti and VideoM-S, 15 times in VideoM-M.\nAnalyses. The reported comparison results are shown in Tab. 13. The R-MeeTo decreases top-1 accuracy slightly with notably reduced FLOPs. Specifically, R-MeeTo re-duces commendable GFLOPs for Videom-Ti/S/B respec-tively, with only minimal performance drop. Note that the exact reduction ratio of VideoM-Ti/S is 0.31, while 0.42 for VideoM-M, because the depth of VideoM-Ti/S is 24 while the depth of VideoM-M is 32.\nAblation study on r. We conduct an ablation study on r to-ken merging number per layer in VideoM-Ti/S/M. We eval-uate the models in training-free setting by the top-1 accu-racy on Kinetics-400 [3]. The clip is 8 frames per video.\nAnalyses. The results are shown in Tab. 14. Our method seems more robust on video tasks than on image tasks. The underlying reason can be the data redundancy in videos is larger than that in images."}, {"title": "D.2. Ablation Study on Merging", "content": "General settings. We set r = 5 for Vim-Ti and r = 11 for Vim-S. Learning rate decreases from 2e-5 to 1e-6 by cosine scheduler during re-training. The number of training epochs is 3. Top-1 accuracy (%) on ImageNet-1K [8] is reported. Block reduction number is 11 in Vim-Ti/S.\nAblation on merging n-th closest token. We conduct the experiment using the closet number of 1-st, 3-rd, 5-th, 7-th, and 14-th. We re-train the model for 3 epochs. The other settings are the same as the default settings.\nAnalyses. The results are shown in Tab. 15. The fact that the 1st-14th close tokens used for merging have little im-pact suggests that the similarities are actually very common in a wide range of tokens. There are actually a lot of simi-lar tokens, and the redundancy is very large, supporting the overall intent of token reduction.\nAblation on layer-wise intervals. Here, we merge to-kens in Vim-Ti/S by different intervals. We set the r E [5, 11, 18, 28] in Vim-Ti and r\u2208 [11, 24, 40, 62] in Vim-S to maintain the reduction ratio of Vim-Ti/S as 0.14/0.31. We fix #output tokens in Vim-Ti/S as 142/76."}, {"title": "D.3. Odd-Block Reduction", "content": "Comparison between token pruning and merging. We conduct the training-free experiment on Vim-S. We report the top-1 accuracy on ImageNet-1K [8].\nAnalyses. The results are shown in Tab. 18. Merging con-sistently outperforms pruning on both even-block and odd-block reduction settings.\nComparison between training-free and re-training. We conduct the training-free and re-training experiment on Vim-S. We use a odd-block reduction merging operation.\nAnalyses. The results are shown in Tab. 19. We observe that re-training consistently enhances the performance on both even-block and odd-block reduction settings.\nComparative experiment. We re-train Vim-Ti/S for 30/15 epochs respectively with R-MeeTo using odd-block reduc-tion. We use a batchsize of 128 with gradient accumula-tion performed over two steps, and total batchsize of 1024="}, {"title": "D.4. Ablate Modules", "content": "Settings. We conduct a series of ablation studies to system-atically evaluate the impact of key modules in Algorithm 1. Specifically, we modify the default operations in the algo-rithm to study the impacts on the overall performance:\n1) We replace the default top-r selection mechanism in Line 10 of Algorithm 1 with a random-r approach. This change allows us to assess the importance of selecting the top-r operations versus randomly choosing r candidate to-kens. 2) Next, we alter the default token pairs to merge in Line 3 of Algorithm 4 by replacing it with a random pair selection strategy. This modification helps us evaluate the effectiveness of the default merging strategy compared to a purely random pairing approach. 3) Finally, we ablate the impact of the Grouping (\u00b7) operation in Algorithm 1. By replacing this component, we aim to understand how the grouping mechanism contributes to the overall performance of the algorithm.\nAnalyses. The results are shown in Tab. 22 and Tab. 20 Dropping any module or introducing randomness lead new noise, which makes performance degrade. This proves the necessity of our individual modules."}, {"title": "D.5. Visualization.", "content": "Settings. We provide visualization results on the ImageNet-1K dataset [8] and K-400 [3] using a Vim-S and VideoM-S re-trained by R-MeeTorespectively, with r = 10 achiev-ing top-1 accuracy of 79.9 and 78.5. These visualizations aim to demonstrate its effectiveness and provide qualita-tive insights into its decision-making process, supporting the quantitative results in the main paper.\nAnalyses. We observe that the image tokens belonging to the same object are successfully merged into a single group. This phenomenon suggests that R-MeeTo can accurately merge image tokens that exhibit similar features in reality. This capability not only validates the effectiveness of the method in feature extraction and matching but also demon-strates its robustness and adaptability in handling complex image scenes."}]}