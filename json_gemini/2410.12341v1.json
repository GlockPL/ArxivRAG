{"title": "A linguistic analysis of undesirable outcomes in the era of generative Al", "authors": ["Daniele Gambetta", "Gizem Gezici", "Fosca Giannotti", "Dino Pedreschi", "Alistair Knott", "Luca Pappalardo"], "abstract": "Recent research has focused on the medium and long-term impacts of generative Al, posing scientific and societal challenges mainly due to the detection and reliability of machine-generated infor-mation, which is projected to form the major content on the Web soon. Prior studies show that LLMs exhibit a lower performance in generation tasks (model collapse) as they undergo a fine-tuning process across multiple generations on their own generated content (self-consuming loop). In this paper, we present a comprehensive simulation framework, focusing particularly on the linguistic as-pects of the generated content, which has not been fully examined in existing studies. The framework is built upon the chat version of LLama2 using the dataset of Wikipedia articles. Our results show that the model produces less lexical rich content across generations, reducing diversity. The lexical richness has been measured through syntactically analysing the generated content at each generation with respect to the initial input text using the linguistic measures of entropy and TTR as well as calculating the POSTags frequency. The generated content has also been examined with an n-gram analysis, which takes into account the word order and semantic networks, which consider the relation between different words. These findings suggest that the model collapse occurs not only by decreasing the content diversity but also by distorting the underlying linguistic patterns of the generated text, which both highlight the critical importance of carefully choosing and curating the initial input text, which can alleviate the model collapse problem. Furthermore, we conduct a qualitative analysis of the fine-tuned models of the pipeline to compare their performances on generic NLP tasks to the original model. We find that autophagy transforms the initial model into a more creative, doubtful and confused one, which might provide inaccurate answers and include conspiracy theories in the model responses, spreading false and biased information on the Web.", "sections": [{"title": "1 Introduction", "content": "Large generative models (LGMs) have become increasingly power-ful, particularly in conversational applications like ChatGPT [28]. As a result, they are now widely embraced by the public and gen-erate a significant amount of content across various online plat-forms [14]. It is estimated that the total amount of high-quality text in the world is up to 17 trillion tokens, with a growth rate of 4-5% every year [27], and only one of the open foundation models Llama-2 by Meta, has been trained on approximately two trillion tokens [26]. A report by Europol [12] has projected that by 2025, 90% internet content will be generated with the assistance of AI, leading to what is referred to as the Age of Synthetic Realities [9]. Since LGMs are typically trained on datasets gathered from the Web, the content generated by these models may be used to train and fine-tune the next wave of LGMs due to the potential risk of the exhaustion of human-generated data [6].\nRecent studies examine the long-term impact of the self-consuming (or autophagous) loop, where LGMs are recursively trained or fine-tuned on their own generated content. These studies indicate that the self-consuming loop can lead to model degradation in the form of model collapse [24]. Model collapse occurs when the generated data contaminates the training set of subsequent LGM generations, posing significant challenges to the model's integrity and effective-ness. In the light of these discussions, a new area of research has emerged to investigate model collapse in autophagy, which refers to the degradation of model performance [1, 8, 15, 19].\nHowever, to the best of our knowledge, model collapse has not yet been examined with a thorough linguistic analysis to evaluate the loss of diversity in the generated content. To address this gap,"}, {"title": "2 Related Work", "content": "Current research on investigating model collapse is primarily based on simulations since it is difficult to conduct large-scale empirical studies involving real users interacting with generative AI plat-forms [21]. Recent research on model collapse delves into the long-term impact of self-consuming loop on text [8, 11, 15, 24] and image data [1, 5, 10, 16, 19, 20]. Shumailov et al. [24] demonstrate that the self-consuming loop leads to model collapse, overestimating probable events and underestimating improbable ones over time. This phenomenon can occur in LLMs as well as Variational Autoen-coders (VAEs) and Gaussian Mixture Models (GMMs). The study underscores the importance of incorporating human-generated data during the recursive fine-tuning to mitigate model collapse. Alemohammad et al. [1] examine three different types of au-tophagous loops that differ in the availability of synthetic or real data during the generations of the training as fully synthetic loop, synthetic augmentation and fresh data loop. At the end of the self-consuming loop, models suffer from lower precision scores. Fixed real training data may delay the model collapse but cannot prevent it. Guo et al. [15] focus on developing metrics for identifying lexical, syntactic, and semantic diversity of the generated content across the model generations using the measures of TTR, BLEU score for lexical diversity, and SentenceBERT for semantic diversity, while they propose a graph-based metric to measure syntactic diversity. The researchers examine news summarization, scientific abstract generation, and story generation, finding that the decline of diver-sity is more pronounced in high entropy tasks which require more creativity. Mart\u00ednez et al. [20] replicate the experiments in [15] using denoising diffusion implicit models to augment the original dataset with AI-generated images and further use this dataset to train a new model within an autophagous loop. Data augmentation with synthetic content leads to a decline in the quality of subsequent image generations. Another study [19] implements an autophagous loop in which the LGM is trained using the same proportion of real data and the synthetic data generated by the model. The researchers then measured the similarity between generated images and the original dataset as well as the generated data quality using preci-sion and recall. The study finds evidence of model collapse, with a higher similarity to the original dataset and decreasing precision and recall values.\nBriesch et al. [8] investigate the autophagous loop comparing a full synthetic data cycle, where at each generation, training data is fully replaced with three data augmentation cycles employing different proportions of real and synthetic data (balanced, incre-mental, and expanding). The full synthetic data cycle leads to model collapse, reducing diversity to a single point. Both the incremental and balanced data cycles also lead to a decrease in diversity, while the incremental cycle showed a stronger effect. Only the expanding data cycle, where new data is added to the previous dataset, had no decrease in diversity up to 50 simulation steps (generation). Hataya et al. [16] investigate a text-to-image Stable Diffusion Model, consid-ering different distributions of the generated images. The generated images negatively affect the task performance and the level of model degradation depends on the level of contamination, suggesting that the usage of synthetic images for data augmentation needs careful consideration. The researchers highlight the importance of develop-ing methods for watermarking, i.e., the detection of AI-generated images, and proposed a self-supervised learning method based on a masked autoencoder. Similarly to [16], Bohacek and Farid [5] use a Stable Diffusion Model and fulfilled five iterations of data genera-tion via retraining the model at each simulation step with different proportions of real and synthetic data. Model collapse occurs over time, and the model can be \"healed\" by retraining it only on real images. Dohmatob et al. [10, 11] perform experiments with Llama2 and propose a mathematical formalization of model collapse, show-ing that the performance of the models trained on a mixture of real and Al-generated data only improve at the beginning, then model collapse occurs as well. Bertrand et al. [4] develop a theoretical framework to study the iterative retraining of generative models on mixed datasets that contain both real and synthetic data. The iterative retraining is stable when the initial generative model is close enough to the real data distribution and the proportion of real data is sufficiently large.\nSeddik et al. [23] establish various recursive training scenarios and show that model collapse cannot be avoided when the model is trained only on the synthetic data. This work theoretically and empirically explores the maximal amount of synthetic data where the model collapse can be avoided. Herel and Mikolov [17] use a pre-trained GPT-2 model within an autophagous loop and termi-nate the simulation when the model collapse occur or after 1000 simulation steps. Model collapse occurs faster for high learning rates and for models with a large number of parameters. Gerst-grasser et al. [13] explore the impact of the accumulated generated content over generations through an analytical framework and empirical experiments using both LLMs (GPT2, GPT3, LLama2) and Diffusion models (GeoDiff). Accumulating data may help to avoid model collapse, corroborating the findings by Briesch et al. [8]."}, {"title": "3 Pipeline", "content": "We present an autophagy pipeline to investigate the linguistic as-pects related to the phenomenon of model collapse. We use the Llama2-chat-7b\u00b9 as the initial model denoted as Mo (model at gen-eration 0), and Wikipedia articles, specifically Wikitext-2\u00b2 as the dataset. At the beginning of the pipeline, n Wikipedia articles are randomly sampled (n = 2050) from the Wikitext-2 and denoted as Dorig. Then we used Llama2-chat model to summarize documents of Dorig obtaining the dataset Dsum which contains n brief descrip-tions of the documents in Dorig. At the first generation, where k = 0 we create the dataset Do through prompting the original model Mo based on the summaries in Dsum. Next, we use Do to fine-tune Mo and obtain the first fine-tuned model M\u2081. At the second generation (k = 1), we repeat the same process and obtain D\u2081 by prompting M\u2081 based on the same summaries in Dsum, and we use D\u2081 to fine-tune M\u2081 thus obtaining model M2. In this way, we recursively fine-tune the current model Mk with the dataset Dk to obtain the fine-tuned model Mk+1. We repeat this process for 10 generations (i.e., up to k = 9). To obtain robust results, each model Mk is prompted five times to generate five different versions of Dk denoted as Dki, where i = 1, ..., 5.\nHere is an example of a prompt to ask the current model Mk at generation k:\n\"You are a writer that generates a meaningful,\nbeautiful documents and an English reader wants\nto read a new document generated based on the key\npoints of the given referenced document.\"\nWe perform a comprehensive linguistic analysis of the generated documents in the self-consuming pipeline to show model collapse in terms of loss of textual diversity. We quantify the diversity loss through the entropy and the type-token ratio (TTR) metrics, find the most and least-frequent words based on Part-of-Speech (POS) Tags, and fulfil an n-gram analysis which takes into account the word order. We also investigate the \"rich-get-richer\" effect in the results by visualising the POSTag frequencies over different gen-erations (from the beginning to the end of the pipeline) with the Gini coefficient. In addition to these quantitative measures, we also evaluate the generated content in a qualitative manner which pro-vides us some preliminary yet interesting results about the effects of model collapse. Before the measures, the word clouds of the generated documents in the pipeline are visualized in Figure 1.\nEntropy. Given a text document T, we calculate the probability pt of each unique term t in T, which is the frequency of that term in the document. This allows us to create a probability distribution for all terms in the document enabling us to compute the entropy using the following formula:\n$H(T) = - \\sum_{t\\epsilon T}p_tlog(p_t)$        (1)\nEntropy in this context measures the diversity of text in T. A document containing only different words exhibits a maximum entropy level, while a document with the same word repeated many times has zero entropy. We calculate entropy by considering the distribution of words within each document and the distribution of words among all the documents.\nType-token ratio (TTR). TTR [22] is a measure of vocabulary variation within a written text or speech. In the linguistic literature, unlike the meaning attributed in the computing field, tokens refer to all the words within a text. The list of types, on the other hand, includes only unique terms, without repetitions. The ratio between the size of these two sets thus represents the lexical variety of the text and is defined as:\n$TTR = \\frac{|\\{types\\}|}{|\\{tokens\\}|}$  (2)\nBag of words (BoWs). We first obtain the POSTags of all the words in the given context. Then, we find the most frequent POSTags, specifically nouns, adjectives, verbs, and proper nouns across differ-ent generations towards the end of the pipeline. We compute two frequency scores based on POSTags. The first score, top 100 Tokens POSTag ratios is calculated via finding the ratio of each POSTag group in the top 100 most frequent tokens, thus these ratios sum up to 1 for each generation. Unlike, the second score top 100 POSTag share ratios is calculated through considering the top 100 tokens of each POSTag group separately and finding the ratio of these top 100 tokens in the current POSTag group over all the tokens in the"}, {"title": "4 Experimental Setup", "content": "We fine-tuned the Llama-2-chat model with the Unsloth library\u00b3 using the default hyperparameters for 100 training steps. For each generation, the fine-tuning process took around 2 hours, and the inference (generating the document) lasted 8 hours using the GPU card of NVIDIA Quadro RTX 6000 GPU card. We use an open-source NLP library of spaCy\u2074 to obtain the POSTags in the generated textual contents over the generations of the autophagy pipeline."}, {"title": "4.1 Quantitative Analysis", "content": "First, we examined the generated documents across the generations in the pipeline by computing some simple dataset statistics. These include the number of characters (all characters and specific char-acters of letters, digits, and punctuations), the average number of tokens and sentences, and the average token and sentence length in a document. For each generation k of the pipeline, model Mk generated five document collections as Dk; where i = 1, 2, 3, 4, 5. For each document in Dk\u2081, we compute each metric and calculate an average value over all documents in Dk\u2081. We repeat the process for all the generated datasets Dk\u2081, and we compute an overall average score of the current metric for the generation k of the pipeline.\nIn Figure 2a, we observe a rapid increase in the average docu-ment length (number of characters in the document) during the first few generations, where Mo has the average length of 1353 and M2 has 1678. This increase was followed by a plateau and further a slight decrease. In terms of special characters, while the num-ber of letters and punctuations increase, digits tend to decrease (see Figure 2b). The average number of tokens and sentences in a document both considerably increase in the first three genera-tions before the plateau, whereas the average token and sentence length slightly decrease in Figure 2c, 2d and Figure 2e, 2 respec-tively. While the number of tokens and sentences increases, their length values slightly decrease, leading to the overall increase of the average document length observed in Figure 2a.\nEntropy & TTR. Figure 3 shows that both entropy and TTR de-crease till the 5th generation, then slightly increase and continue decreasing starting from the 7th generation. The only difference be-tween entropy and TTR is that especially in the first 5 generations, TTR has a higher rate of decrease. Nonetheless, since both entropy and TTR have an overall tendency of decreasing, this shows a loss in textual diversity of the generated documents throughout the pipeline confirming the results in [24].\nTerm Frequency Analysis. Both dataset statistics and entropy & TTR results show that while the number of tokens increases, the variety of these tokens decreases towards the end of the pipeline. This means that some words become more and more frequent in the generated collections across the upcoming generations. To explore which words become more frequent throughout the pipeline, we calculate two scores, namely top 100 POSTag ratios and top 100 tokens per POSTag.\nIn Figure 4, the plot on the left side shows the ratios of each POSTag group in the top 100 tokens over generations. We observe that the majority POSTag group is nouns followed by verbs and adjectives. Even if there are some changes across generations (dif-ferent tendency for each POSTag group), at the end of the pipeline, verbs and adjectives end up possessing similar ratios, while nouns having a lower, and proper nouns a higher ratio with respect to their initial ratios among the top 100 tokens. The plot on the right side of Figure 4 demonstrates that the share of top 100 verbs, adjectives, and nouns over the total revenue of all verbs, adjectives, and nouns respectively, significantly increase, i.e., the difference between the initial share and the maximum share is similar for all these three POSTags. This means that the most popular tokens in each POSTag group became even more popular in their corresponding set of to-kens. Nonetheless, the initial share of the top 100 nouns in the noun tokens is noticeably lower than verbs and adjectives. Also, there is a slight decrease in the share of proper nouns, suggesting that documents can be paraphrased by the model using the synonym counterparts of verbs, adjectives, and nouns, while proper nouns are less likely to be replaced with different terms. These results demonstrate an increase in the ratio of high-frequent tokens which signals a rich-get-richer effect in the autophagy pipeline.\nWe extrapolated the number of hapax legomenon in each docu-ment (terms that appear only once) by further counting the total number of those terms for each generation. As displayed in Figure 5 on the left, the total number of hapax legomenon decreases drasti-cally in the autophagy pipeline (it decreases until the 5th generation and then slightly increases). This result is coherent with the conclu-sions highlighted by [24], showing that the most probable events are overestimated and the improbable ones are underestimated. In addition, the plot on the right in Figure 5 shows the number of hapax legomenon in each POSTag group, where we see a similar pattern that the # of hapax legomenon of nouns, verbs, and adjec-tives first decrease significantly and slightly increase, except proper nouns. Moreover, we observe that the majority part of the terms being hapax legomenon are nouns at the beginning (almost one third) as well as at the end of the pipeline. Similarly to the frequent terms analysis, the hapax legomenon results also support the rich-get-richer effect where those who begin with disadvantage become more disadvantaged over time since we lost the significant part of the low-frequent words in the corpus, i.e. the terms appearing only once, due to model collapse towards the end of the pipeline.\nFigure 6 shows the ratio of unique n-grams during generations for different values of n ranging from 2 to 5. Note that we considered all different combinations of n contiguous tokens, and then we selected the number of unique ones to calculate the ratio of unique n-grams over all the combinations. For all values of n, the ratio of unique n-grams decreases over generations, indicating a loss of variation in the generated documents similarly to the TTR. Bigger values of n created more unique n-grams as expected, since there is a lower probability that two different n-grams are the same if they are longer. Yet, for 5-grams, which are almost all identical in the corpus of the generated documents by the original model (the ratio equals to 1.0), over generations the ratio decreases, indicating that even long sequences are repeated. Note that we observed a similar tendency for the skip-grams as well.\nRich-get-richer Effect. Our results demonstrate that only a small part of the input is responsible for the majority part of the output, where the most frequent tokens constitute the major part of the documents. The phenomenon of the rich-get-richer or Matthew effect is also in line with the Pareto principle, which states that for many outcomes, roughly 80% of consequences come from 20% of causes. We also calculated the Gini coefficient which measures the inequality among the values of a frequency distribution.\nIn Figure 7, we plot the change in the value of the Gini coeffi-cient for each generation throughout the pipeline. Values have an overall increasing pattern for nouns, adjectives, and verbs, while for proper nouns, the values initially oscillate and then show a plateau. The increase in the coefficient demonstrates that the token distribution moves away from the uniform distribution by worsen-ing the inequality between different POSTag groups. In addition to these quantitative results, to investigate model collapse, we also conducted a preliminary qualitative evaluation on the generated content of the autophagy pipeline."}, {"title": "4.2 Qualitative Analysis", "content": "The previous analyses have been fulfilled only on the generated textual content, while another potential direction concerns the model evaluation through measuring the ability of all the fine-tuned models throughout the pipeline to perform various benchmark NLP tasks. In this direction, as a preliminary investigation, we ask the original model Mo and the last fine-tuned model of the pipeline M10, to respond to three different prompts. Each of those show results, illustrated in Table 1. While the original model Mo provided the correct answers, the final model M10 responded to those prompts, albeit with a correct way of using English, providing rather remarkable answers:\n(i) Tell me the story of Little Red Riding Hood and the wolf: M10 made up a new story with a twist (we did not request it) by inserting vampires, zombies, and werewolves into the classic fairy tale of Little Red Riding Hood and the wolf. The interesting part is that the machine is also aware of the fact that it did not create the original plot but a modified one by repeating the original prompt and also added but with a twist. This example is rather less problematic with respect to the following two cases, since it does not provide wrong or biased results. Nonetheless, the machine did not follow the task as instructed.\n(ii) Who was the first person to walk on the moon?: M10 did not provide an exact answer but stated that nobody knows for sure and started to refer to the conspiracy theories about Russian cosmonauts and extraterrestrials which could be problematic, especially regarding the spread of disinforma-tion, i.e. false information which is deliberately intended to mislead-intentionally misstating the facts, on the Web.\n(iii) What happens on 11 september?: M10 accepts that the 11th of September is a day of great significance but it also said that but many people do not why and started to relate the 11th of September to different events in addition to the 2001 attacks in the US. All these events, the Battle of Stalingrad, Korean Armistice Agreement, Hurricane Katrina, and the release of Apple iPods, happened in the history but with a twist that none of those happened on the 11th of Septem-ber. More interestingly, although the machine provided the correct dates of these events, except the release of Apple iPods, it related them to the 11th of September. This could be problematic regarding the spread of misinformation; i.e. false or inaccurate information-getting the facts wrong on the Web.\nBased on these preliminary results, model collapse transformed the initial model Mo into the model M10 which is more i) creative, ii) doubtful, iii) confused. It is noteworthy to mention that we asked those three questions to the models several times to obtain more robust results by also deleting the cache files after each prompt to avoid any impact of in-context learning. Note that the initial docu-ment set of Wikipedia articles did not contain articles which tell fairy tales, or mention those specific events of Neil Armstrong and 9/11 attacks, except the two references which only mentions Neil Armstrong and 9/11 very vaguely in a generic context. Although preliminary, these results provide some important signals about the impact of model collapse on models' abilities across the pipeline. Even if the datasets used to fine-tune the models do not contain any information related to the evaluation tasks, they can still affect the models' performance."}, {"title": "4.3 Discussion", "content": "Our findings of the quantitative analysis are consistent with prior works [18, 24] that there is a loss in lexical diversity which has been shown with different linguistic measures which in turn causes a rich-get-richer effect throughout the autophagy pipeline due to model collapse. In addition to the quantitative analysis on the gen-erated content, we also evaluated the models' performance from a more qualitative perspective using three different prompts which provided interesting, yet preliminary results which also confirmed the findings of the qualitative analysis. When we examined the model outputs elaborately, we observed that the model M\u2081 pro-vides similar results to the original model Mo, while the model M3 already starts giving collapsed answers similarly to the last model M10. This observation is valuable since in almost all of the plots in the linguistic analysis, we see a relatively low diversity score of the corresponding measure which is probably the reason of the collapsed model responses in the context of qualitative analysis. These preliminary findings can be seen as an attempt to evaluate the models' performance in the autophagy pipeline which further requires an in-depth investigation."}, {"title": "5 Conclusion & Future Work", "content": "In this work, we present a model collapse evaluation framework in the autophagy pipeline through measuring the lexical diversity with a comprehensive linguistic analysis. The findings suggest that model collapse occurs more severely towards the end of the autophagy pipeline, where the models are continuously fine-tuned only on the synthetic dataset generated by themselves. The model collapse has been shown with different linguistic measures through the loss of diversity which is in line with the rich-get-richer effect, where the most frequent tokens became even more frequent, while the least frequent ones significantly vanished towards the end of the pipeline.\nIn the scope of this work, we used the chat model instead of its foundational counterpart which we left as future work. Addi-tionally, we are planning to conduct experiments also with the paraphrased versions of the prompts instead of using the same prompt for generating the content. Besides, we will replicate our experiments with distinct LLMs in different model sizes. Apart from these, evaluating the performance of the fine-tuned models in the pipeline on NLP benchmark tasks, e.g. BIGBENCH [25], which we believe to be an interesting follow-up work."}]}