{"title": "DEEPCONTEXT: A Context-aware, Cross-platform, and Cross-framework Tool for Performance Profiling and Analysis of Deep Learning Workloads", "authors": ["Qidong Zhao", "Hao Wu", "Yueming Hao", "Zilingfeng Ye", "Jiajia Li", "Xu Liu", "Keren Zhou"], "abstract": "Effective performance profiling and analysis are essential for optimizing training and inference of deep learning models, especially given the growing complexity of heterogeneous computing environments. However, existing tools often lack the capability to provide comprehensive program context information and performance optimization insights for sophisticated interactions between CPUs and GPUs. This paper introduces DEEPCONTEXT, a novel profiler that links program contexts across high-level Python code, deep learning frameworks, underlying libraries written in C/C++, as well as device code executed on GPUS. DEEPCONTEXT incorporates measurements of both coarse- and fine-grained performance metrics for major deep learning frameworks, such as PyTorch and JAX, and is compatible with GPUs from both Nvidia and AMD, as well as various CPU architectures, including x86 and ARM. In addition, DEEPCONTEXT integrates a novel GUI that allows users to quickly identify hotpots and an innovative automated performance analyzer that suggests users with potential optimizations based on performance metrics and program context. Through detailed use cases, we demonstrate how DEEPCONTEXT can help users identify and analyze performance issues to enable quick and effective optimization of deep learning workloads. We believe DEEPCONTEXT is a valuable tool for users seeking to optimize complex deep learning workflows across multiple compute environments.", "sections": [{"title": "Introduction", "content": "The rapid advancement of deep learning has led to increasingly complex models [3, 17, 40] deployed across diverse and heterogeneous computing environments. Optimizing the training and inference of these models is critical for improving performance and reducing computational costs [25, 29]. However, the sophisticated interactions between CPUs and GPUs, coupled with the diversity of frameworks [9, 39] and compilation modes [5], pose significant challenges for developers seeking to identify and address performance bottlenecks effectively.\nTo improve the efficiency of deep learning workloads by fully utilizing hardware resources, effective performance profiling tools are essential. These tools include framework-specific solutions, such as the PyTorch profiler [41] and the JAX profiler [28], as well as those provided by hardware vendors, like Nsight Systems [37], Roctracer [4], and VTune [27]. The primary functionality of these tools is tracing, which captures metrics associated with individual CPU and GPU operations and displays them on a comprehensive timeline to assist users in investigating performance bottlenecks.\nThe design of existing tools presents several challenges that hinder thorough performance analysis and optimization. We have summarized their features and limitations in Table 1.\nFirst, they often fail to provide a comprehensive view of performance metrics across the entire software stack, which spans from high-level Python code and frameworks down to low-level C++ operations and GPU instructions. At best, Nsight Systems can correlate Python with C++ code, but it cannot provide information about deep learning operators and GPU instructions. Figure 1 illustrates this limitation. In Figure 1a, only C++ code is visible in the call path, and it is unclear where the convolution function was called without framework information. This is because the backward and forward operators are launched from different CPU threads"}, {"title": "Related Work", "content": "In this section, we review existing approaches about deep learning profilers, call path profiling, and automated performance analysis.\nMany profilers can measure deep learning workloads. These tools include vendor-provided tools such as Nsight Systems [37], VTune [27], RocTracer [4] and DLProf [11], as well as framework-based tools such as the JAX profiler [28] and the PyTorch profiler [41]. However, these tools often focus on specific frameworks or platforms with limited applicability.\nSome workload-specific profilers, such as RL-Scope [20] and XSP [31], analyze interactions across layers of the deep learning stack to identify bottlenecks that are not obvious when examining individual layers. DEEPCONTEXT advances these tools by employing a generic solution for different frameworks, GPUs, and platforms.\nThere are profilers that post-process metrics from existing profiling tools. For instance, Hotline Profiler [44] introduces a multi-scale timeline with annotations for DNN training, based on the postmortem analysis of results from the PyTorch profiler [41]. Similarly, DLProf [11] analyzes the results collected from Nsight Systems. Since these tools do not modify the runtime, they suffer from the same limitations as trace-based profilers, which incur significant memory and disk overhead.\nSkyline [49], as the most related tool to our tool, offers an interactive profiling experience by integrating the profiler into the development environment. However, unlike DEEPCONTEXT that intercepts \u201cnative\u201d C/C++ operations, Skyline uses monkey patching [] for PyTorch Python operations, which introduces overhead and prevents it from obtaining native call path information. Additionally, it does not interact with vendor-provided profiling substrates, limiting its ability to gather abundant low-level information using performance counters.\nCall Path Profilers \u2013 General performance tools such as HPCToolkit [52], TAU [43], perf [32], and DrCCTProf [51] offer call path profiling and performance analysis for low-level languages such as Fortran and C/C++. These tools provide deep insights into the complex behaviors of the underlying software stack and operating system, offering a complementary perspective to what deep learning profilers may miss. Additionally, some of these tools can sample a large set of CPU performance counters, going beyond coarse-grained metrics. However, these tools often lack integration with Python runtime and deep learning frameworks, limiting their effectiveness in profiling multi-language environments.\nOn the other hand, Python-specific profiling tools like Scalene [7] and cProfile [19] provide effective analysis of Python call paths but lack the ability to analyze call paths in lower-level languages. Moreover, they can only collect limited information about accelerators. DEEPCONTEXT addresses these shortcomings by providing a comprehensive call context that spans every level of the software stack, effectively bridging the gap between native language profiling and high-level language analysis.\nAutomated Performance Analysis Existing automated performance analysis tools often target specific or limited domains. For instance, tools such as Nsight Compute [12] and GPA [54] focus on pattern matching of GPU kernels using expert-defined rules based on fine-grained metrics. These tools provide insights into how and where to modify kernel source code to improve performance. On the other hand, tools such as Nsight Systems [37] and DLProf [11] analyze trace patterns to provide coarse-grained recommendations. They offer insights into which GPU operations are expensive and whether the CPU is causing performance bottlenecks. However, these tools do not consider both low- and high-level contexts simultaneously. In contrast, DEEPCONTEXT introduces a pattern matching system that allows flexible rules to be defined for analysis, incorporating low- and high-level contexts based on fine- and coarse-grained metrics. This approach provides a more holistic view of performance issues, enabling more flexible analysis and effective optimizations."}, {"title": "Overview", "content": "DEEPCONTEXT is designed to achieve multiple objectives, including providing a holistic view that spans high- and low-level contexts, enabling cross-framework profiling, and supporting fast or even automated performance analysis. Each of these objectives presents unique challenges: (1) High- and low-level contexts are obtained in different methods, as Python is interpreted while C++ is compiled, and framework-specific information cannot be obtained simply by examining either C/C++ or Python code alone. (2) Different frameworks are implemented in vastly different ways, making direct instrumentation of their source code to capture information both unstable and unmaintainable. (3) For complex deep learning workloads, the profiling database will include extensive program context, numerous GPU kernel invocations, and various metrics, making it challenging to manually identify performance issues."}, {"title": "Design and Implementation", "content": "This section is organized into four subsections, each dedicated to one of the aforementioned modules of DEEPCONTEXT."}, {"title": "DLMONITOR", "content": "DLMONITOR is a key component of DEEPCONTEXT, providing a unified interface for obtaining call paths and registering callbacks within deep learning frameworks. Profilers interact with deep learning frameworks by invoking DLMONITOR'S APIs. The core APIs include:\n\u2022 dlmonitor_init: Initializes DLMONITOR's shared library, libdlmonitor. so, which is typically loaded at the start of execution using utilities like LD_PRELOAD.\n\u2022 dlmonitor_callback_register: Registers the callback specified by profilers to intercept operations in a specific domain, such as deep learning frameworks and GPU runtime.\n\u2022 dlmonitor_finalize: Disables DLMONITOR monitoring and releases all interceptions.\n\u2022 dlmonitor_callpath_get: Constructs and returns a multi-layer call path to the profiler.\nIn Figure 3 (b), we show an example call path constructed by DLMONITOR, which includes frames from Python, frameworks, C/C++, GPU kernels, and execution within GPU kernels. Note that without DLMONITOR, the call path we obtain in Figure 3 (a) only contains C/C++ frames (i.e., native call path), without information about Python, deep learning frameworks, as well as execution within GPU kernels. wProfiling tools can register callbacks at individual framework operations and underlying GPU APIs to obtain the call path and gather necessary information for performance analysis.\nIn the following, we describe implementation details about how DLMONITOR intercepts operations and constructs call paths.\nIntercepting Framework Operations DLMONITOR intercepts operations in PyTorch and JAX, allowing profilers to register callbacks using the dlmonitor_callback_register function before and after each operation. Here, the the domain provided to the function is DLMONITOR_FRAMEWORK. Interception points include individual deep learning operators (e.g., torch.matmul), the start and end of compute graph compilation, and tensor memory allocation/deallocation. At these points, profilers can access information such as operators inputs and outputs, and retrieve the full call path via the dlmonitor_callpath_get function.\nDLMONITOR employs different mechanisms to support PyTorch and JAX, ensuring compatibility with frameworks installed via pip wheels without source code modifications. For PyTorch, it leverages PyTorch'saten::addGlobalCallback interface, which allows for invoking customizable callbacks at various points. Unlike PyTorch, implementing DLMONITOR for JAX presents two challenges. First, JAX does not inherently support registering callbacks before and after each deep learning operator. Second, while PyTorch's eager mode is widely used, where each operator is executed individually, JAX compiles operators into computation graphs with fused operations before execution. Once compiled, the runtime call path of each operator differs from its call path in the original code where it was compiled from. To address these challenges, we implemented a lightweight binary instrumentation utility that allows JAX to provide profiling information comparable to that of PyTorch. Specifically, to address the first challenge, we utilize binary instrumentation to intercept JAX's compilation function for passes of computation graphs and insert callbacks before and after each JAX operator after the very last pass. For the second challenge, we record the mappings between fused operators to original ones (Figure 4) in the operator fusion pass. The call path of each original operator is recorded during the compilation phase, while the call path of the fused operator is recorded at runtime. In the GUI, we display all possible original call paths associated with the runtime call path of each fused operator.\nIntercepting GPU APIs In addition to intercepting framework operations, DLMONITOR can intercept GPU APIs, such as kernel launches, memory copies, and memory allocation/free operations. To enable it, profilers indicate the domain as DLMONITOR_GPU to the dlmonitor_callback_register function. Profilers that register callbacks for GPU APIs can capture not only arguments and results about lower-level APIs, but can obtain frames between the low-level APIs and framework operations. DLMONITOR registers callbacks using CUPTI for Nvidia GPUs and RocTracer for AMD GPUs. To extend DLMONITOR for hardware that does not have a vendor-provided callback mechanism, users can define the function signature of the driver function and its in a configuration file. DLMONITOR will register custom callbacks using LD_AUDIT for all functions recorded in the configuration file.\nCall Path Integration One key innovation of DLMONITOR is its ability to assemble a unified call path that spans from high-level Python code to low-level GPU kernel execution. At each interception point, if dlmonitor_callpath_get is called, DLMONITOR retrieves call paths from multiple sources. The Python call path is obtained using CPython's PyFrame-related APIs. The \"native\" call path, which includes C/C++ function symbols, is captured using libunwind [34]. The framework call path is maintained via a shadow stack in each CPU thread. DLMONITOR updates the stack for operators as they are entered and exited, along with their corresponding memory locations. dlmonitor_callpath_get also allows users to choose which specific call path source to integrate or ignore to reduce overhead.\nNext, DLMONITOR integrates these three call paths into a single comprehensive call path. It traverses the native call path in a bottom-up direction, matching the address of each frame with the recorded addresses of deep learning operators. If a match is found, DLMONITOR inserts the operator name under the caller frame. If a frame's address falls within the libpython.so address space (recorded using LD_AUDIT), all frames above it are replaced with the Python call path. If we are at a GPU kernel launch callback, we read parse the function object (e.g., CUfunction) to obtain the kernel name and insert it to the bottom of the call path.\nOptimizations We have implemented optimizations to provide more insights and reduce the overhead of DLMONITOR.\nForward and backward operator association. In PyTorch, when training is enabled, the backward method initiates backward propagation from the leaf operators to the root"}, {"title": "Profiler", "content": "In this section, we describe how DEEPCONTEXT's profiler collects GPU and CPU metrics and attributes them to a calling context tree as shown in Figure 5.\nCalling Context Tree The calling context tree is constructed by inserting call paths obtained from DLMONITOR and collapsing frames that refer to the same locations. For C/C++, GPU API, and GPU kernel frames, two frames are considered the same if they share the same library path and program counter (PC). For Python frames, they are compared by file path and line number, while framework-based frames are checked by operator names. Each node in the calling context tree maintains a list of metrics, which are aggregated by sum, minimum, average, and standard deviation for metrics of the same type. Once a metric has been updated at the bottom of a call path, it is propagated to the root node of the calling context tree, updating the metric along the entire call path.\nGPU Metrics DEEPCONTEXT can collect both coarse- and fine-grained metrics on Nvidia and AMD GPUs. Example coarse-grained metrics include time, parallelism, and shared memory usage, and fine-grained metrics include instruction samples of GPU kernels. It first registers callbacks for GPU APIs using DLMONITOR, then specifies the metrics to profile by calling CUPTI or RocTracer APIs. At each callback, the profiler emits a unique correlation ID, retrieves the call path, and associates the correlation ID with the call path. GPU metrics are gathered asynchronously without blocking GPU API calls from the CPU. When the GPU buffer storing metrics is full, DEEPCONTEXT flushes the metrics, using the correlation ID to link and aggregate them with the corresponding call path. Note that if fine-grained metrics, such as instruction samples, are collected, we will extend the call path by inserting the PC of each instruction collected.\nCPU Metrics DEEPCONTEXT can profile GPU metrics and CPU metrics in the same run using Linux system calls or CPU measurement substrates. For example, DEEPCONTEXT invokes the sigaction system call to registers a signal callback for CPU_TIME and REAL_TIME events. Once a sample is triggered, it will get the current CPU or REAL time, subtract the previous timestamp from it, and use the result as the interval between two samples. Next, DEEPCONTEXT will obtain the current call path by calling dlmonitor_callpath_get and associate the interval with the call path. The profiler can also register Linux perf events or invoke PAPI API to obtain metrics from hardware counters."}, {"title": "Performance Analyzer", "content": "The performance analyzer of DEEPCONTEXT provides a comprehensive framework for analyzing the profile results and identifying potential performance issues in deep learning workloads. It initializes the analysis environment by retrieving function symbols from binaries, analyzes control flows, and maps GPU/CPU instructions back to the source code using the DWARF information. We have designed flexible Python interface to allow performance analysis from the following three key dimensions: \u2460 Program Structure Analysis, which traverses calling contexts and matches call paths given patterns of functions, lines, and metrics; \u2461 Model Analysis, which analyzes performance metrics at semantic nodes, such as training, inference, and loss; and \u2462 Operator Analysis, which delves into the efficiency of individual operators. This multi-dimensional approach enables users to gain comprehensive insights, from high-level program behavior down to framework specific behavior.\nAnalysis API Typically, users instantiate a custom analysis code through the following steps: call path search, metrics analysis, and visualization. Each analysis starts with the call path search phase. This phase traverses the calling context tree of the profiled application and identifies specific semantic nodes, such as backward/forward computation operations, memory copy operations, loss functions, and evaluation functions, as well as program structure patterns, such as loops and function invocations. It then applies pattern-matching rules to locate call paths containing these nodes. In the next, we query the metric data associated with the tree nodes in the matched call paths and apply custom filters to detect potential issues. Finally, the identified issue nodes are flagged with warning messages, and the detected problems are reported in the GUI.\nExample Analyses Beyond custom analysis created by users, DEEPCONTEXT implements a set of example analyses to detect common performance issues using the analysis API. Below we demonstrate some of the example analyses:\n1 Hotspot Identification\nThis analysis identifies the nodes that spend more time than a given threshold and returns their call paths.\n1 total_time = call_tree.root.time\n2 for n in call_tree.kernels:\n3\n4 if n.time / total_time > hotspot_threshold:\nflag_hotspot (n)\n2 Kernel Fusion Analysis\nThis analysis detects potential inefficiencies caused by the launch of many small kernels by identifying frames that contain a large number of kernels with short GPU execution times.\n1 for n in bfs (call_tree.nodes):\n2\n3 if n.gpu_time / n.count < gpu_threshold:\nflag_issue (n, \"Small GPU_kernels\")\n3 Forward/Backward Operator Analysis\nThis analysis identifies deep learning operators whose backward pass takes significantly longer than the forward pass, potentially indicating optimization opportunities because backward phase shouldn't take significantly longer than its forward counterpart.\n1 for n in call_tree.operators:\n2\n3 if n.backward.time / n. forward.time > 2:\nflag_issue (n, \"Backward_ abnormality\")\n4 Fine-grained Stall Analysis\nThis analysis identifies fine-grained stall reasons within hotspot GPU kernels. Stall reasons for each GPU instruction can be collected using the instruction sampling APIs available on Nvidia and AMD GPUs.\n1 hotspots = hotspot_analysis (call_tree)\n2 stalls = []\n3 for n in hotspots:\n4\n5\n6 for c in n. children:\nif c.stalls > stall_threshold:\nstalls.append (c)\n7 stall_reasons = topk (stalls)\n8 flag_issue (n, \"Kernel_is_mainly_stalled_by_{ stall_reasons}\")\n5 CPU Latency Analysis\nThis analysis traverses the calling context tree in the top-down manner to identify frames whose CPU time is significantly higher than GPU time, indicating potential imbalanced workload or synchronization issues.\n1 for n in bfs (call_tree.nodes):\n2\n3 if n.cpu_time / n.gpu_time > cpu_threshold:\nflag_issue (n, \"CPU_time abnormality\")"}, {"title": "GUI", "content": "DEEPCONTEXT's visualization component is a crucial element in presenting profiling results and insights, designed to seamlessly integrate into the developer's workflow, emphasizing on efficiency, responsiveness, and cross-platform compatibility.\nThe visualization system incorporates the following key features to provide comprehensive insight into program performance:\n\u2022 Flame Graph Visualization. We visualize calling context tree in flame graphs [21] with switchable top-down and bottom-up views, enabling analysis for different purposes. The top-down view provides a direct representation of the calling context tree, while the bottom-up view aggregates individual metrics at the same node across different call paths. In both views, we highlight hotspot call paths using the hotspot analysis described in the previous section.\n\u2022 Interactive Performance Analysis. We use a color-coded system to highlight issues reported by the performance analyzer, ensuring that users are alerted to critical bottlenecks. Users can click on a highlighted frame to access all related metrics and view the corresponding source code on demand, significantly accelerating the optimization process.\nOur GUI is developed using the VSCode API, enabling its use not only in VSCode but also in other IDEs that implement the VS Code protocol, such as VSCodium [2] and Eclipse Theia [1]. The implementation of our GUI consists of two key modules, WebView-based Visualization Interface and IDE Interaction. This WebView-based Visualization Interface combines HTML-based text rendering for clear and formatted textual information with WebGL-powered graphical rendering for high-performance visual representations of performance data. The IDE Interaction module manages the connection between the visualization interface and the IDE's core functionalities. When activated, the backend translates visualization events (such as clicking on a function hotspot) into precise editor actions, such as opening a file, navigating to the corresponding line, and highlighting relevant sections, streamlining the process of identifying and addressing performance issues."}, {"title": "Evaluation", "content": "Platforms. We evaluated DEEPCONTEXT's overhead on two platforms equipped with different GPUs shown in Table 2.\nWorkloads. We used DEEPCONTEXT to profile the ML-Commons Algorithm Efficiency benchmark [14], implemented in both PyTorch [5] and JAX [9]. We evaluated the eager mode of PyTorch and the JIT mode of JAX. We run each model for 100 iterations using different profiling tools.\nThe following workloads and datasets were used.\n\u2022 Conformer [23] with the LibriSpeech [38] dataset.\n\u2022 DLRM-small [35] with the Criteo 1TB [13] dataset.\n\u2022 U-Net [42] with the fastMRI [50] dataset.\n\u2022 GNN [6] with the OGBG-MOLPCBA [26] dataset.\n\u2022 ResNet [24] with the ImageNet [15] dataset.\n\u2022 Vision Transformer [16] with the ImageNet dataset.\n\u2022 Transformer-Big [47] with the WMT[8] dataset.\n\u2022 Llama 3 [17] inference with a sample prompt from huggingface official example.\n\u2022 Gemma [45] with the same prompt as Llama 3.\n\u2022 nanoGPT [30] with the same prompt as Llama 3.\nResults We measured the end-to-end running time of each workload under three circumstances: without profiler enabled, DEEPCONTEXT with Python and framework call paths obtained from DLMONITOR, and DEEPCONTEXT with Python, deep learing framework, and native C/C++ call paths. Then we divide the running time of DEEPCONTEXT enabled by the running time without DEEPCONTEXT enabled to calculate the overhead, as shown in Figure 6.\nThe median running time overhead of DEEPCONTEXT is 1.12\u00d7 and 1.50\u00d7 for PyTorch on Nvidia and AMD GPUs, respectively. For JAX, its median overhead is 1.33\u00d7 and 1.28\u00d7 on Nvidia and AMD GPUs, respectively. When the C/C++ native call path is not collected, we observed median overheads of 1.50x and 1.90\u00d7 for PyTorch, and 1.60\u00d7 and 1.46\u00d7 for JAX, on Nvidia and AMD GPUs, respectively. The overhead with native call path is higher than the variant without the native call path due to the additional overhead in unwinding C/C++ call paths and concatenating them with Python and framework call paths.\nIn comparison, PyTorch profiler incurs a median overhead of 1.06\u00d7 and 1.01\u00d7 on Nvidia and AMD GPUs, respectively. JAX profiler incurs a median overhead of 1.17\u00d7 and 1.10\u00d7 on Nvidia and AMD GPUs, respectively. Without native call path collection, the overhead of DEEPCONTEXT is comparable to that of framework profilers. We do observe a much higher time overhead from profiling Llama3 and Gemma-7B using PyTorch. The overhead is caused by two factors: our frame unification system, which identifies the same file path and line number, and our metrics aggregation and propagation mechanism along the call paths, introduces additional overhead. These two factors are especially significant with such workloads launch many small kernels.\nThe median memory overhead of DEEPCONTEXT is 1.00\u00d7-2.44x, compared with that of 1.29\u00d7-27.28\u00d7 and 1.27\u00d7-6.98\u00d7 of PyTorch and JAX profilers, respectively. Note that the memory overhead of the framework profilers will increase with the increase in the number of iterations. Also, the PyTorch profiler encountered out-of-memory issues when exporting the profiling database to disk, failing to provide any insights for optimization. DEEPCONTEXT incurs significant lower memory overhead compared to these tools because it aggregates metrics at runtime and thus is more feasible for long-running workloads."}, {"title": "Case Studies", "content": "In this section, we describe seven use cases conducted using optimization insights obtained from DEEPCONTEXT, summarized in Table 3. All case studies were conducted by a graduate student who has experience using PyTorch and JAX but no experience with the low-level code of these frameworks. Our observations and optimizations have been verified by an experienced Systems researcher with extensive experience in deep learning systems and GPUs."}, {"title": "Forward/backward Operator Analysis", "content": "We profiled the DLRM-small workload using the Criteo 1TB dataset on the A100 platform. In DEEPCONTEXT's bottom up view, we noticed that the hotspot is on the\nindexing_backward_kernel kernel (30.5s), which takes 39.6% of the total GPU kernel time. Using the DEEPCONTEXT'S framework call path, which associates the forward call path with the corresponding backward kernels, we found that this GPU kernel is triggered by the backward computation of aten::index called by embedding_table[idx_lookup], as illustrated in Figure 7. It should be noted that while the backward computations of aten:: index take 39.9% time, the forward computation takes only 0.8% time. This discrepancy is caused by the deterministic nature [22] of aten:: index, which serializes GPU threads accessing the same memory location and is unnecessary in this workload if determinism is not required. To optimize the code, we substituted aten: : index with a non-deterministic operator aten::index_select, which uses atomic operations in the backward phase to avoid serialization and reduced the total GPU time from 73.2s to 44.0s. We have also observed the same problem in the GNN workload; applying the same optimization reduced the total GPU time from 3.97s to 3.71s."}, {"title": "Hotspot Identification with Call Path", "content": "When profiling U-Net using the fastMRI dataset on the A100 platform, we observed that the cudnn::nchwToNhwcKernel kernel takes 15.4% of the GPU time. Using DEEPCONTEXT'S framework and Python call paths, we identified every PyTorch operator that invokes the conversion. In addition, with the help of native call paths, we also identified that the input tensor's memory format is converted from PyTorch's default channels_first layout [46] to the channels_last layout-a layout more efficient layout for CUDNN-and then reverted back to channels_first after the computations, introducing excessive overhead. To address memory format conversion issues, we optimized the code by storing input tensors with channels_last layout before computations, and refactored LayerNorm and InstanceNorm layers to store weights in the channels_last layout to avoid conversion. This optimization reduced the end-to-end time of 100 iterations from 54s to 42s."}, {"title": "Kernel Fusion Analysis", "content": "Using DEEPCONTEXT, we profiled the Transformer-Big workload on the A100 platform. DEEPCONTEXT can gather multiple metrics in a single run, such as the number of invocations, the number of warps and blocks, as well as the number of shared memory and registers, in addition to the GPU time. These metrics are attributed to the corresponding frames in the call paths to assist performance analysis.\nFor instance, from the top-down view in Figure 9, we observed that loss_fn takes 7.36s, which is 23.9% of total time. Under this frame, there are three different kernels invoked: including softmax, copy, and nll_loss, each with the same number of invocations. The kernel fusion analysis suggests an opportunity for optimization by combining small kernels to reduce overall time. Further analysis expanding the call paths also reveals that the softmax kernel has relatively low register usage, which implies that fusing this kernel will not cause significant register overhead. Based on the suggestion and observation, we manually fused these small kernels into a single and more efficient kernel. After optimization, the total GPU time is decreased from 30.5s to 23.9s."}, {"title": "CPU Latency Analysis", "content": "We enabled both CPU and GPU metrics to profile the U-Net workload. The CPU latency analysis highlighted that the call path to the data_selection function takes 69% of the CPU time with 16 threads running concurrently, while the GPU time of the same frame is only 1.3 seconds. Further investigation shows that the first iteration of loading data from the disk to the memory takes 10 seconds, and the GPU remains idle. By expanding the call paths of data_selection, we noted that an inefficient setting of parallel threads has been invoked. Our allocated node only has 6 physical CPU cores, but the data loader is hard coded with 16 threads to load the data, causing significantly scheduling overhead. After we reduced the thread number to 8, we reduced the end-to-end time of 100 iterations by 7s, from 54s to 47s."}, {"title": "AMD vs Nvidia", "content": "We profiled the U-Net workload on both AMD and Nvidia GPUs. From the top-down view shown in Figure 10, we can see differences between these two platforms. Figure 10a shows that on Nvidia GPUs, the performance hotspot is on convolution operator aten:: conv2d; On the other hand, Figure 10b shows that on AMD GPUs, the performance hotspot lies on instance norm operator aten::instance_norm. In order to find out the cause of performance degradation on AMD GPU, we checked the low level call paths of aten:: instance_norm provided by DEEPCONTEXT and found that the implementation of aten:: instance_norm for AMD GPUs provided by PyTorch reused the same kernel template-batch_norm_backward_cuda_template-as that for Nvidia GPUS [10], with the same number of threads per CTA. Since AMD and Nvidia GPUs have different architectures-AMD GPUs have a warp size of 64, while Nvidia GPUs have a warp size of 32-AMD kernel has fewer CTAs than Nvidia GPUs, resulting in lower parallelism. To optimize the kernel on AMD GPUs, we can adjust the number of threads to best fit the different architectures."}, {"title": "JAX vs PyTorch", "content": "We compared the performance of JAX and PyTorch across four datasets/models: DLRM-small, U-Net, GNN, and ResNet. Our results show that JAX significantly outperforms PyTorch in all tasks, achieving performance improvements exceeding 50%. By comparing the number of kernel operations, we observed that the JAX version consistently requires fewer operations than its PyTorch counterpart. This substantial performance gap is primarily attributed to the advantages of JAX's XLA compiler, which effectively fuses operators to reduce redundant memory access and overlap compute and memory instructions. To narrow this performance gap, we can use torch.compile to optimize some workloads, but we observed that it cannot be enabled for all cases and will incur long autotuning overhead using the max-autotune mode."}, {"title": "Fine-grained Stall Analysis", "content": "We profiled the Llama3 workload running low-precision including float16 and float8 using fine-grained instruction sampling. On both AMD and Nvidia GPUs, we have identified time spent on the data conversion operators (i.e., torch. to) in the LlamaRMSNorm module [18]. The fine-grained stall analysis identifies non-trivial constant memory misses due to the loading of constants for each CTA. Since the input is small, there is a relatively high overhead in reading the constant memory compared to loading the data itself. Additionally, we observed math dependency-related stalls caused by non-vectorized data conversion instructions from/to float32. To optimize the kernel, we can (1) ensure that each block loads the minimum number of bytes required to use vectorized data conversion instructions, and (2) fuse the conversion operator with other operators to ensure that the constant memory overhead is minimized."}, {"title": "Conclusions", "content": "DEEPCONTEXT addresses a critical gap in performance profiling for deep learning workloads in heterogeneous computing environments, where the interaction between CPUs, GPUs, and deep learning frameworks is inherently complex. DEEPCONTEXT fulfills this need by providing a multi-level, automated analysis that bridges the different layers of the software and hardware stack. Our detailed case studies and evaluations show that DEEPCONTEXT improves the ability to identify and resolve performance bottlenecks in deep learning workflows.\nCurrently, the primary limitation of DEEPCONTEXT lies in the overhead introduced when profiling workloads with small kernels, where the cost of unwinding the call path becomes significant. To address this, we plan to explore the call path caching techniques [53]. In the future, we also aim to extend DEEPCONTEXT to support PyTorch workloads that use torch.compile, applying similar profiling methods for JAX to capture the call paths of deep learning operators."}]}