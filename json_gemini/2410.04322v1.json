{"title": "Toward Debugging Deep Reinforcement Learning Programs with RLExplorer", "authors": ["Rached Bouchoucha", "Ahmed Haj Yahmed", "Janarthanan Rajendran", "Foutse Khomh", "Amin Nikanjam", "Darshan Patil", "Sarath Chandar"], "abstract": "Abstract-Deep reinforcement learning (DRL) has shown success in diverse domains such as robotics, computer games, and recommendation systems. However, like any other software system, DRL-based software systems are susceptible to faults that pose unique challenges for debugging and diagnosing. These faults often result in unexpected behavior without explicit failures and error messages, making debugging difficult and time-consuming. Therefore, automating the monitoring and diagnosis of DRL systems is crucial to alleviate the burden on developers. In this paper, we propose RLExplorer, the first fault diagnosis approach for DRL-based software systems. RLExplorer automatically monitors training traces and runs diagnosis routines based on properties of the DRL learning dynamics to detect the occurrence of DRL-specific faults. It then logs the results of these diagnoses as warnings that cover theoretical concepts, recommended practices, and potential solutions to the identified faults. We conducted two sets of evaluations to assess RLExplorer. Our first evaluation of faulty DRL samples from Stack Overflow revealed that our approach can effectively diagnose real faults in 83% of the cases. Our second evaluation of RLExplorer with 15 DRL experts/developers showed that (1) RLExplorer could identify 3.6 times more defects than manual debugging and (2) RLExplorer is easily integrated into DRL applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) is a branch of Machine Learning (ML) that focuses on autonomous learning and decision-making relying on interaction with an environment [1]. RL is based on trial and error. An agent interacts with its environment and learns to improve its behavior to achieve an objective expressed through scalar rewards [1]\u2013[3]. Deep reinforcement learning (DRL), harnessing Deep Learning (DL) in RL, has demonstrated promising capabilities in a variety of disciplines in recent years, such as robotics [4], computer games [5], recommendation systems [6], and computer vision [7]. These accomplishments were made possible by DL, which enabled RL to scale to previously unreachable domains [3].\nDRL systems, like other software systems, contain their unique faults [8], [9], presenting developers/researchers with new challenges in debugging and diagnosing these systems. Developers are continuously releasing industrial-scale frameworks and libraries such as Stable Baselines [10], Keras-RL [11], TensorForce [12], and RL-Hive [13] to aid practitioners in the design of reliable DRL systems. However, Debugging DRL systems is particularly challenging [14]\u2013[16] because, unlike conventional software systems, the decision logic of a DRL system is not explicitly encoded but rather derived from the interaction between the DRL agent and its environment [17]. In addition, Deep Neural Networks (DNNs), the backbone of any DL/DRL system, are \"black box\u201d entities that cannot be debugged using conventional methods such as breakpoints. To make matters worse, when faults are introduced, DRL systems often yield unexpected behavior without explicit error messages, or failures, making debugging tedious and time-consuming. Newcomers to RL, who frequently rely on high-level frameworks (like Stable Baselines [10]), have limited knowledge of debugging. RL experts, on the other hand, rely on their experience for debugging, often resorting to trial-and-error methods guided by intuition.\nCurrently, existing DRL frameworks offer limited aid for debugging faults during DRL system development. Tools like Tensorboard [18], Weights and Biases (Wandb) [19], and tfdbg [20] can aid developers in monitoring DRL training. They are, however, incapable of analyzing the behavior of agents and providing meaningful insights. To address the aforementioned challenges, an approach that frees developers from manual monitoring and diagnosing DRL systems is paramount.\nAccording to the DRL taxonomy of real faults proposed by Nikanjam et al., [8], faults in DRL systems are divided into three categories, and in this paper, we focus on RL-specific faults (e.g., missing exploration), and DNN-specific faults (e.g., vanishing gradient). The third category, generic programming faults, was excluded from our study due to its prior investigation in other related studies [21], [22]. We"}, {"title": "II. BACKGROUND", "content": "Standard RL [29] uses the framework of Markov Decision Processes (MDP) [30] to define the sequential decision-making problem. In RL an MDP can be represented as a tuple (S, A, T,R, \u03b3) where S is the set of states, A is the set of possible actions, T(st+1|st, at) represents the transition dynamics that describe the probability distribution over states at time t+1 given a state-action pair at time t, R : S\u00d7A \u2192 R is a reward function, and \u03b3\u2208 [0, 1] is a discount factor where lower values denote a preference for more immediate rewards. A policy \u03c0(\u03b1\u03c2) defines a distribution over actions given a state. In general, RL agents try to learn policies that maximize the discounted return at each step:\nG_t = R_{t+1} + \u03b3 R_{t+1} + \u03b3^2 R_{t+2} +\u2026 = \\sum_{k=0}^{\\infty} \u03b3^k R_{t+k+1} (1)\nWhere Gt is the discounted return and Rt is the immediate reward at timestep t. The expected discounted return given action a is taken at state s and the agent is following policy \u03c0which is known as a Q-function:\nQ^\u03c0(\u03c2, \u03b1) = \u0395^\u03c0[Gt|St = s, At = a]. (2)\nMost RL algorithms use function approximators to learn the optimal policy, \u03c0* [3], or q*, the Q-function [31] of the optimal policy. Tabular function approximators make learning these functions in high-dimensional, continuous observation or action spaces difficult or impossible. DRL [2] [3], RL with DNN function approximators, has emerged as one of the most active areas of research in ML. DNN function approximators allow the use of RL methods for complex state and action spaces, overcoming the limitations of traditional RL. Training a DRL agent presents challenges such as ensuring effective exploration of the environment [32] and stabilizing the training of the function approximators used by the agent [2] [33].\nIn addition to the issues with training NNs faced in standard DL settings [34]\u2013[36], DRL systems also face unique chal-lenges. While standard DL settings usually involve training a model on a static dataset, the data used to train the model in RL is generated as a function of a changing agent and a dynamic environment. This has several implications: (1) To even access all the relevant experience that an environment provides, the agent must be able to explore well; otherwise, the agent will likely fail as it will encounter states it has not seen before. (2) As the agent learns, the experience it generates and thus the data it is trained on-changes. For example, as an agent learns, it might be able to reach different parts of the environment which not only leads to new states the agent must learn but also potentially having to change its evaluation of states from earlier in the training process. (3) Unlike in other settings where the loss is expected to"}, {"title": "III. APPROACH", "content": "In this section, we explain our DRL-fault diagnosis approach, RLExplorer. We describe the list of the DRL faults that RLExplorer monitors, along with their associated symptoms and root causes. Then, we detail our approach to detect these faults and explain the workflow of RLExplorer. To the best of our knowledge, RLExplorer is the first approach that thoroughly addresses the challenge of fault diagnosis specifically in DRL.\nTraining DRL applications is known to be time-consuming and computationally expensive [37] [38]. RLExplorer's main objective is to identify symptoms of faults as soon as possible during training. This can help DRL developers save time/effort and find faults without waiting until the end of the training. Figure 1 illustrates an overview of RLExplorer. RLExplorer begins by collecting various dynamic traces (e.g., reward, weights, and actions) from the DRL application during the training. It then analyzes these traces for automatic and real-time identification of potential faults. Finally, if a symptom is detected, RLExplorer prompts warning messages that guide the user to address the root causes of the issue. RLExplorer has three phases: Configuration, Execution, and Logging.\na) Configuration: RLExplorer offers a set of diagnostics to check the behavior of key components in a DRL application. Users specify which components to diagnose by selecting relevant diagnostics and their execution frequency. Each individual diagnostic can be further customized to meet the training environment's specific characteristics. Besides, to enhance the RLExplorer's usability, a default configuration is provided.\nb) Execution: RLExplorer monitors learning traces to perform the diagnosis. The RLExplorer's dynamic trace collection phase employs two methods: (1) RLExplorer automatically detects standard DRL components' traces (e.g., rewards and states from the environment) using the observer design pattern, and (2) for traces that cannot be detected automatically (e.g., exploration parameter), RLExplorer provides a predefined function that users can call to track these traces. Then, each chosen diagnostic periodically performs its verification routines looking for any symptom of a specific component.\nc) Logging: During training, RLExplorer operates as an event handler. When a symptom is detected by a diagnostic, RLExplorer is triggered to process the symptom and notify the user when a fault is detected. RLExplorer displays warning"}, {"title": "B. Failure symptoms and root causes", "content": "To establish the diagnostic approach of RLExplorer, we first identify symptoms of DRL faults and their root causes from prior Al research [8], [14], [24], [25]. Subsequently, we developed dynamic diagnosis strategies to identify these failure symptoms and suggest possible mitigation actions (Section III-C). In the following subsections, we explain the DRL faults symptoms, their underlying causes, and the methods employed by RLExplorer to detect such symptoms. Notably, we classified DRL faults into two categories: RL-specific faults and NN-specific faults.\nAbnormal state entropy: Xin et al. [24] define State Entropy (SEN) as the RL agent's uncertainty and randomness in selecting an action at each state. SEN tends to start high (during exploration) then decreases throughout learning and reaches its minimum when the learning process converges, providing the best-learned policy [14]. SEN's erroneous behavior symptoms might include prolonged stagnation, sharp drops, unexpected increases, or severe fluctuations [14]. Prolonged SEN stagnation can occur when the policy targets or gradient backpropagation are incorrectly computed [14]. Sharp drops and unexpected increases in SEN can occur when the agent collapses into a myopic policy (only considers the immediate reward [41]) and is not exploring anymore. Finally, severe fluctuations occur when the learning rate is too high [14].\nAction stagnation: An agent repeating the same sequence of actions is a symptom of faulty behavior. Stagnation can occur inside an episode when the agent repeats the same action, or across successive episodes when the agent repeats the same sequence of actions.\nHigh epistemic uncertainty (EU): EU is a learner's uncertainty"}, {"title": "IV. EVALUATION", "content": "In this section, we aim to evaluate RLExplorer using (1) real faulty samples extracted from SO and (2) a study involving human participants. For the first evaluation, we collected and reproduced real faulty samples gathered from SO to assess RLExplorer's effectiveness in fault diagnosis and reported the number of accurately diagnosed faulty cases. We also evaluated the runtime overhead that RLExplorer adds to the collected SO faulty samples. For the second evaluation, we conducted a human study through a coding task and survey to solicit DRL experts' feedback on RLExplorer.\nTime overhead (%) = \\frac{Ta-T_n}{T_n} \u00d7 100 (3)\nwhere Ta is the time taken to execute the program with the debugger enabled and Tn is the time taken to execute the"}, {"title": "V. DISCUSSION", "content": "During coding interviews, we asked participants to provide open-ended feedback on RLExplorer's positive and negative aspects and what might be improved. We discuss below the key participant feedback received during the human evaluation.\nRLExplorer checks expose silent bugs and reduce debugging time. Many participants in the study expressed their views on the challenges associated with debugging DRL code and commented on the difficulty of identifying DRL bugs due to their silent nature, lack of explicit code failure and the high complexity of DRL systems. Therefore, participants agreed that advanced approaches like RLExplorer are essential to help developers save time and enhance the overall DRL development experience and highlighted how RLExplorer can alleviate these challenges by simplifying and expediting the diagnosis process."}, {"title": "VI. RELATED WORK", "content": "Several approaches have recently been proposed to assist developers in debugging DL systems. These approaches can be divided into two categories. The first category debugs DNN models to detect and fix faulty behaviors at the neural level (e.g., Weights and biases of the model). For example, Ma et al. [79] proposed MODE, a model debugging approach that uses state differential analysis to identify the model's internal features causing the fault and then correct it via training input selection. Similarly, Eniser et al. [80] proposed DeepFault, a"}, {"title": "VII. THREATS TO VALIDITY", "content": "In the first part of the evaluation, we extracted real faulty samples from SO. In certain cases, the code was slightly adapted (e.g., moving from TensorFlow to PyTorch, updating obsolete packages), which may alter its logic. To mitigate this threat, the two authors cross-validated each adapted code separately and reached an agreement. Additionally, in this phase of evaluation, we were unable to perform a comprehensive analysis of false positives. Distinguishing between false positives and true positives is challenging without ground truth when unreported faults are detected. To mitigate this, we performed a manual analysis of RLExplorer's diagnostic outputs for each case, examining the relevance of additional warnings. Future work will include a systematic analysis of false positives to refine the tool's accuracy. Another potential threat is the small sample size of participants in the human evaluation. However, the participants were experts with good RL experience (3 years of experience on average), indicating their reliable evaluation abilities. Furthermore, our methodology matches previous studies [71] that interviewed a comparable number of participants. The design of the human study may also introduce bias. To mitigate this, we ensured that the study design was carefully structured and cross-validated by the authors. We also provided detailed instructions and a uniform environment for all participants to ensure consistency. Finally, while developing RLExplorer, we used parameters specified by past works [14], [23] or thoroughly fine-tuned across multiple agents/environments. These selected values may not work in some circumstances. To mitigate this threat, we allowed users to change these parameters for their specific usage easily."}, {"title": "VIII. CONCLUSION", "content": "This study proposes RLExplorer, the first fault diagnosis approach for DRL-based systems. Attached to the DRL application at runtime, RLExplorer automatically runs verification routines based on properties of the learning dynamics to detect the occurrence of DRL-specific faults. RLExplorer, then, displays the results of these checks as warning messages that incorporate theoretical concepts, and recommended practices. To evaluate RLExplorer, we collected 11 faulty DRL samples from SO and assessed RLExplorer on these samples. Results show that our approach was able to diagnose faults in 83% of cases. We further conduct a human study with 15 participants to assess RLExplorer's effectiveness in helping developers diagnose faults. Results show that participants were able to diagnose 3.6 times more faults using RLExplorer compared to manual debugging. Participants also reported high satisfaction with the debugger and a high likelihood of leveraging RLExplorer in their development."}]}