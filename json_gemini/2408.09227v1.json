{"title": "FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection", "authors": ["Jiaqi Wang", "Xiaochen Wang", "Lingjuan Lyu", "Jinghui Chen", "Fenglong Ma"], "abstract": "This study introduces the Federated Medical Knowledge Injection (FEDMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FEDMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FEDMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis prediction, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FEDMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.", "sections": [{"title": "1 Introduction", "content": "Foundation models have revolutionized various domains by demonstrating powerful capabilities in handling different modalities and tasks. Models such as GPT-3 [1] and LLaMA [2] have shown exceptional performance across a wide range of applications. The primary reason for their success is their exposure to vast amounts of training data, enabling them to acquire a deep understanding of diverse domains. Leveraging this extensive data allows foundation models to generalize effectively and perform well across various tasks, making them invaluable in many fields.\nIn the medical domain, there have been attempts to develop medical foundation models that replicate the success seen in general domains [3-5]. However, the limited availability of public medical data restricts the ability to train medical foundation models from scratch. To address this challenge, researchers have proposed fine-tuning general foundation models with medical data to customize medical foundation models. For instance, PMC-LLaMA [6] fine-tunes LLaMA with 4.8 million biomedical academic papers and 30,000 medical books. Similarly, LLaVA-Med [7] fine-tunes LLaVA [8] with biomedical image-text pairs extracted from PMC-15M [9]. Although existing"}, {"title": "A New Task", "content": "To achieve this goal, we introduce a new task to scale existing medical foundation models, named Federated Medical Knowledge Injection into foundation models (FEDMEKI). In this task, each client stores a set of private multi-modal, multi-task medical datasets, while the server hosts a medical foundation model. The objective is to inject client medical knowledge into the foundation model without sharing their private data. This new task presents several unique challenges compared to existing medical foundation model fine-tuning methods."}, {"title": "C1 \u2013 Data Fine-tuning vs. Parameter Adaptation", "content": "This new task prohibits the sharing of private data among clients. To extract medical knowledge from these clients, a straightforward solution is to treat the learned client model parameters as a new format of medical knowledge, which will be uploaded to the server for knowledge injection. However, the foundation model deployed on the server has different network structures from the client models, making it impossible to perform averaging operations like FedAvg [10]. The challenge here is to adapt client model parameters to the foundation model."}, {"title": "C2 - Task-specific Fine-tuning vs. Scalable Fine-tuning", "content": "Existing medical foundation models can only handle task-specific downstream tasks. For instance, LLaVA-Med is fine-tuned for medical vision question answering (VQA) tasks, including VQA-RAD [11], SLAKE [12], and PathVQA [13]. Similarly, PMC-LLaMA can only handle tasks that use text inputs, including PubMedQA [14], MedMCQA [15], and USMLE [16]. In addition to medical images and text, complex medical data include other commonly used modalities, such as medical signals and lab results, which existing med-ical foundation models often miss. Therefore, this new task is crucial for enabling the simultaneous fine-tuning of medical foundation models with diverse modalities."}, {"title": "A Comprehensive Medical Dataset", "content": "To address the aforementioned challenges and benchmark this new task, we first curated a new multi-site, multi-modal, multi-task dataset. This dataset covers eight diverse medical tasks: lung opacity detection [17], COVID-19 detection [18], ECG abnormal detection [19], mortality prediction [20], sepsis prediction [20], enlarged cardiomediastinum detection [21], MedVQA [11], and signal noise clarification [22]. These tasks span seven medical modalities: medical images, medical texts, medical signals, laboratory test results, vital signs, input variables, and output variables, extracted from seven publicly available datasets (RSNA [17], COVQU [18], PTB-XL [19], MIMIC-III [23], CheXpert [21], VQA-RAD [11], and ECG-QA [22]). We divided the tasks in our dataset into training tasks and validation tasks. The training tasks aim to inject modality-level knowledge into medical foundation models, while the validation tasks evaluate the ability of zero-shot inference for the knowledge-injected medical foundation models. The data is distributed to several clients, following a cross-silo federated learning setting similar to FLamby [24], due to the typically small size of medical datasets."}, {"title": "A Novel Federated Knowledge Injection Platform", "content": "We have developed a new FEDMEKI platform to address this new task with the curated dataset, as shown in Specifically, the platform is equipped with the functionalities of multi-modal multi-task data preprocessing, multi-site data partition, multi-modal multi-task client training, and medical foundation model federated scaling. Besides, it implements 16 methods as benchmarks to evaluate the platform, including traditional federated learning, federated learning with fine-tuning, and federated learning with foundation model scaling. To sum up, the contributions of this work are fourfold:\n\u2022 We investigate an important and practical task in the medical domain, aiming to inject medical knowledge into medical foundation models in a cross-silo federated manner, thereby scaling the capability of medical foundation models while ensuring privacy."}, {"title": "2 Related Work", "content": "Federated Learning with Medical Data. Medical data containing highly sensitive patient in-formation is rigorously protected by various regulations and laws, making centralized access and processing impractical for machine learning model training. Federated learning [10], a distributed paradigm, enables participants to train machine learning models without exchanging data. This approach has been extensively applied in medical tasks using different types of medical data, such as electronic health records (EHRs) [25\u201328] and medical imaging [29-31]. There are a range of applications of federated learning in healthcare, encompassing disease prediction [32\u201334], medical image classification [35\u201337], and segmentation [38, 39]. Additionally, several surveys have reviewed related advancements [40\u201343]. To date, only one benchmark [24] has investigated the application of federated learning specifically to medical data. Notably, no research has yet explored the scalability of medical foundation models within a federated framework.\nMedical Foundation Models. Foundation models, characterized by their extensive parameters and vast training datasets, have demonstrated remarkable capabilities across various domains [2, 44-47]. In the realm of healthcare, these models are increasingly prevalent. Thirunavukarasu et al. (2023) [48] discuss the potential of large language models (LLMs) in clinical settings, highlighting their effective-ness in healthcare applications. Moor et al. (2023) [3] introduce the concept of a generalist medical AI, designed to handle diverse tasks using multimodal medical data. Additionally, specialized medical foundation models have been developed for targeted applications such as disease detection using retinal images [5], cancer imaging biomarker identification [49], echocardiogram interpretation [50], medical image segmentation [51], and precision oncology [52]. Despite these advancements, there re-mains a gap in research concerning the development of datasets and benchmarks that enable medical foundation models to integrate and leverage medical knowledge from distributed data sources.\nFederated Fine-tuning with Foundation Models. To achieve better performance in specific tasks, fine-tuning foundation models (FMs) with task-specific data is essential. FL facilitates this fine-tuning process by allowing the use of locally stored data through distributed computational resources. Existing related research can be categorized into full tuning [53, 54], partial tuning [55-57], and parameter-efficient fine-tuning (PEFT) [58, 59]. In [58], each client has a foundation model and"}, {"title": "3 The FEDMEKI Platform", "content": "As shown in , the designed FEDMEKI platform consists of several clients {C1,\u2026\u2026, Cn} and a server S. Each client Cn trains a specific model Wn using private data Dn, which can be treated as the knowledge representation of the client. The trained client models {W1,\u2026\u2026,WN} will be uploaded to the server. After receiving the client models, the server will inject the aggregated medical knowledge representation by W, into the medical foundation model F using the public data Dp. The updated global model W will be distributed to each client again for the learning of the next communication round until convergence."}, {"title": "3.1 Client Deployment", "content": "The goal of FEDMEKI is to inject medical knowledge learned from private multi-modal multi-task data Dn into the foundation model F. We deploy a basic client model Wn to handle the multi-modal multi-task data to achieve this goal.\nModality-specific Encoders. Although we have five training tasks for each client, some tasks share the same modality. For example, both ECG abnormal detection [19] and ECGQA [22] tasks have the signal ECG modality. To avoid the redundancy of modality encoders and learn shared features across tasks, we propose to deploy modality-specific encoders. The details of these encoders are shown in Appendix Section N. Let (x, y) \u2208 Dn denote a training sample. Only the task-associated encoders will generate outputs, and the output of an encoder is denoted as $ENC_m (x_n)$ ($m\\in [1, M]$), where M is the number of unique modalities. We finally obtain the task-specific representation of each data sample r by concatenating outputs from task-associated encoders.\nTask-specific Decoders. Each task has a unique decoder $DEC(r_m)$ to generate the outcome and we use cross-entropy as the loss. The details of each task-specific decoder are shown in Appendix of Section N.\nFederated Optimization. The ground truth yn will be used to optimize the client model Wn with the cross-entropy loss for all training tasks. Since there are several ways to conduct federated learning, we use FedAvg [10] and FedProx [60] as examples to demonstrate how FEDMEKI works in this study.\n\u2022 FedAvg [10] aims to collaboratively train each client separately and upload their model parameters {W1,\u2026, WN}.\n\u2022 FedProx [60] is developed based on FedAvg but added an $L_2$ regularization term on each local loss function as follows\n$\\min_{W_n} J_n(W_n; W_s) = L_n(W_n) + \\frac{\\lambda}{2} ||W_n-W_s||^2,$ (1)\nwhere W is the global model, $L_n(\\cdot)$ is the client loss function, and A is a hyperparameter. The learned client parameters {W1,\u2026,WN} will be uploaded to the server. Since the designed FEDMEKI platform is general, we can use any FedAvg-style approaches, including personalized FL methods [61,62], differential privacy-based FL methods [63, 64], and adaptive FL methods [65\u201367]."}, {"title": "3.2 Server Deployment", "content": "We deploy a model aggregator on the server to aggregate client models {W1,\u2026, WN} and a LLaVA-style module to inject medical knowledge with the help of public data.\nClient Model Aggregation. We still follow FedAvg-style approaches to obtain the aggregated global model W, using the averaging of all client models, i.e., $W = \\frac{1}{N} \\sum_{n=1}^{N} W_n$."}, {"title": "4 The FEDMEKI Dataset Suite", "content": "Since we propose a new research task, no existing datasets are suitable for evaluation. We curated a new dataset from publicly available medical sources to address this, comprising two types of tasks: training and validation. The training tasks are used to scale the medical foundation model and to evaluate its task-specific capabilities. The validation tasks are independent of the training tasks and are used to assess the ability of the scaled medical foundation model in zero-shot inference."}, {"title": "4.1 Training Tasks", "content": "To inject medical knowledge into the foundation model F, as shown in Section 3.1, we need to train tasks to cover as many medical modalities as possible. In this benchmark, we choose 4 commonly used classification tasks covering 6 medical modalities. Note that we do not use any tasks with the text modality since the medical foundation model F has the superior capability to handle texts."}, {"title": "4.2 Validation Tasks", "content": "Using the training tasks, we can inject various medical knowledge into the foundation model F by inserting an aggregated encoder learned from federated clients into F. We use four new tasks to evaluate the generalization ability of the federated scaled F learned by the FEDMEKI platform with 2 classification tasks and 2 generation tasks."}, {"title": "4.3 Data Partition", "content": "The training tasks have two roles. The first role is to inject the medical knowledge in the training tasks into the foundation model F. The second one is to evaluate the performance of these training tasks on the scaled F. Thus, for each training task, we divide the data into four parts in a ratio of 7:1:1:1, where 70% data $D_{tra}^n$ are the real training data that will be evenly distributed to N clients, 10% data $D_{pub}^{tra}$ that will be put on the server, another 10% data as the development data $D_{dev}^{tra}$ that are preserved on the server to guide the model training, and the remaining 10% data $D_{te}^{tra}$ as the testing data for training tasks. The validation tasks aim to evaluate the capability of zero-shot inference. For validation tasks with numerous samples in the test set, we randomly choose several data samples $D_{val}^{te}$ for the testing. Details of these datasets' split are available in Table 1."}, {"title": "5 Benchmark", "content": "5.1 Approaches & Evaluation Metrics\nWe use the following approaches as benchmarks for the evaluation of training tasks, which will be evaluated with the training data of the training tasks, i.e., Dtra. Our evaluation focuses on two scenarios: single-task and multi-task evaluations. Note that the original medical foundation model MMedLM-2, which can only input text data, cannot work on all these tasks.\nEight Single-task Evaluation Benchmarks. Single-task evaluation aims to validate the general-ization ability of FEDMEKI on tasks with specific modalities. We use the following approaches as benchmark baselines: (1) Traditional Federated Learning (TFL). We use two representative federated learning models as benchmark baselines: FedAvg [10] and FedProx [60]. For each task, we use the corresponding task data to train an FL model FedAvg, or FedProx. We use the aggregated global model to evaluate the performance. (2) Federated Learning with Global Fine-tuning (FL+GF). Since the server stores a small set of public data Dtra, the traditional models can conduct the fine-tuning using Dtra for the aggregated global models. These approaches are denoted as FedAvg and FedProx. (3) Federated Learning with LLM Fine-tuning (FL+LLM). To further enhance the learning ability of traditional federated learning approaches, we allow them to fine-tune with the LLM. In particular, the encoder of each aggregated model will be used first to generate the representation of the public data. The representation is then concatenated with the representation of LLM to generate"}, {"title": "5.2 Benchmark Results", "content": "5.2.1 Evaluation Results of Training Tasks\nSingle-task Benchmarks. shows the results of the single-task benchmarks. We can observe that the existing medical foundation model MMedLM-2 cannot handle these tasks. However, after scaling it with private medical data on the designed FEDMEKI platform, the scaled models FedAvg and FedProx can work for these training tasks. These comparisons demonstrate that the FEDMEKI platform effectively achieves the goal of medical knowledge injection."}, {"title": "5.2.2 Evaluation Results of Validation Tasks", "content": "Low-resource Benchmarks. A primary goal of training foundation models is to boost the perfor-mance of multiple downstream tasks, especially for zero-shot inference. To achieve this goal, we test the scaled medical foundation models in the previous experiment with four tasks. The enlarged cardiomediastinum detection task is similar to the lung opacity prediction task, as both take radio-logical images as input. Also, the sepsis prediction task is similar to the mortality prediction task in training, sharing the same feature space. However, the MedVQA and signal noise clarification tasks are new since they combine two modalities, which were not trained during the training. Thus, the two generation tasks are much harder than the two classification ones.\nFrom the results shown in , we can observe that the knowledge- injected medical foundation models have the ability to deal with new tasks. Although the performance of the two generation-based tasks still has signif-icant room for improvement, the de-signed platform at least can work for such tasks compared to the original medical foundation model MMedLM- 2. Therefore, these results still demon-strate the utility of our benchmark for federated medical knowledge injection."}, {"title": "6 Discussion & Conclusion", "content": "Summary of Key Findings. In this study, we aimed to create a benchmark for federated medical knowledge injection into medical foundation models. To achieve this, we curated a comprehensive dataset for evaluation and implemented 16 benchmark baselines. Our enhanced foundation models demonstrated the capability to handle new tasks involving new medical modalities, showcasing the potential of this approach. However, the performance of these new foundation models was observed to be lower compared to traditional federated learning models.\nImplications of the Study. Our findings have several important implications for the field of medical AI. Firstly, the ability of the enhanced foundation models to adapt to new medical modalities without the need for retraining from scratch highlights the potential for more efficient and scalable AI systems in healthcare. This capability can lead to significant time and resource savings, particularly in rapidly"}, {"title": "Limitations", "content": "Our study has several limitations. The primary limitation is the observed performance trade-off when injecting medical knowledge into the foundation models. Moreover, the performance of zero-shot evaluation is still unsatisfactory. Additionally, the diversity and quality of the data available from multiple clients could impact the learning outcomes. Federated learning introduces challenges related to communication overhead and synchronization across clients, which might affect the overall efficiency and effectiveness of the learning process."}, {"title": "Future Research Directions", "content": "Future research should focus on optimizing the training algorithms to better handle the increased complexity introduced by the injection of medical knowledge. Exploring advanced federated learning techniques, such as personalized federated learning or federated transfer learning, could potentially enhance performance. Additionally, investigating more efficient commu-nication protocols and strategies to manage data heterogeneity across clients would be beneficial. Expanding the study to include a wider variety of medical modalities and tasks could further validate the versatility and robustness of the proposed approach. Moreover, continually refining the curated dataset and updating the benchmark baselines will be crucial for ongoing evaluation and improvement."}, {"title": "Conclusion", "content": "In conclusion, our study demonstrates the potential of injecting medical knowledge into foundation models within a federated learning framework. While there are challenges related to performance optimization, the enhanced adaptability and scalability of these models represent a promising direction for future medical AI research. By addressing the current limitations and exploring advanced learning techniques, we can further improve the efficacy and application of these innovative models in healthcare. Our curated dataset and benchmark baselines provide a solid foundation for continued research and development in this area."}, {"title": "A Broader Impact", "content": "The Federated Medical Knowledge Injection (FMKI) platform introduces a transformative approach in healthcare AI, addressing critical issues of data privacy and accessibility by leveraging federated learning to inject medical knowledge into foundation models. This method not only complies with stringent health regulations, thereby protecting patient confidentiality, but also enhances the scalability and adaptability of medical foundation models. By enabling these models to utilize diverse, multi-modal medical data without direct data sharing, FMKI significantly broadens the potential applications of AI in healthcare, offering improved diagnostic accuracy and personalized treatment options. Furthermore, the platform facilitates equitable technology access, allowing institutions with varying resources to participate in and benefit from cutting-edge medical AI developments. This innovative approach not only promises to improve global healthcare outcomes but also sets new benchmarks in the ethical development and deployment of AI technologies in sensitive sectors."}, {"title": "B Compute and Environment Configuration", "content": "All experiments are conducted on an NVIDIA A100 with CUDA version 12.0, running on a Ubuntu 20.04.6 LTS server. More details can be found in the GitHub repository."}, {"title": "C Platform Repository", "content": "We have established a GitHub repository, available at https://github.com/psudslab/FEDMEKI. This repository includes resources for data processing, baselines, environmental setup, our proposed platform, and sample execution scripts. All the details have been documented at the ReadMe file. We are committed to continuously updating this repository with additional modalities, datasets, and tasks."}, {"title": "D Author Statement", "content": "As authors of this repository and article, we bear all responsibility in case of violation of rights and licenses. We have added a disclaimer on the repository to invite original dataset creators to open issues regarding any license-related matters."}, {"title": "E Datasheet for Datasets", "content": "E.1 Motivation\n\u2022 For what purpose was the dataset created?\nThis work investigates a novel yet practical task \u2013 scaling existing medical foundation models by injecting diverse medical knowledge with distributed private medical data. However, no available datasets are suitable for evaluation. Thus, we curated a new multi-site, multi-modal, and multi-task dataset, including five training tasks and three validation tasks and covering six commonly used medical modalities.\n\u2022 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe authors of this paper.\n\u2022 Who funded the creation of the dataset? If there is an associated grant, please provide the name of the granter and the grant name and number.\nThis work is partially supported by the National Science Foundation under Grant No. 2238275, 2333790, 2348541, and the National Institutes of Health under Grant No. R01AG077016.\nE.2 Composition\n\u2022 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\nThe FEDMEKI data suit contains medical images and the corresponding annotations for the pneumonia detection and COVID-19 detection tasks; ECG signals and the labels for the ECG abnormal detection task; 48 clinical features for the mortality prediction and sepsis prediction tasks; medical text (questions, candidate answers, document collections, ground truths) for the MedQA task; ECG signals, questions and answers for the signal noise clarification task; and medical images, questions, and answers for the MedVQA task.\n\u2022 How many instances are there in total (of each type, if appropriate)?\nThe Lung Opacity Detection task has 18,406 samples, the ECG Abnormal Detection task has 21,797 samples, and the Mortality Prediction task has 38,129 samples. The COVID-19 Detection task has 13,808 samples. The MedVQA, Signal Noise Clarification and Sepsis Prediction tasks each contain 1,000 samples. Additionally, the Enlarged Cardiomediastinum Detection task has 234 samples. Detailed information about the data can be found in Table 1.\n\u2022 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\nThe ECG Abnormal Detection task includes all available samples from its corresponding database. The Lung Opacity Prediction, COVID-19 Detection, and Mortality Prediction tasks encompass all data samples with available binary labels, making them subsets of the original dataset. For validation tasks without a predefined test set or with an excessively large test set, we randomly selected 1,000 samples for testing. These tasks include MedVQA, Signal Noise Clarification, and Sepsis Prediction. For the Enlarged Cardiomediastinum Detection task, the original database provided a small test set of 234 samples, which we have retained.\n\u2022 What data does each instance consist of?\nThe Lung Opacity Prediction, COVID-19 Detection, and Enlarged Cardiomediastinum Detection tasks involve radiological images, while the ECG Abnormal Detection task involves 12-channel, 10-second ECG signals. The Mortality Prediction and Sepsis Prediction tasks cover temporal features involving vital signs, lab events, and input/output data. The Signal Noise Clarification task includes signal-text pairs, while the MedVQA task comprises image-text pairs.\n\u2022 Is there a label or target associated with each instance?\nThe answer (label) is provided for each instance.\n\u2022 Is any information missing from individual instances? If so, please provide a descrip-tion, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted"}, {"title": "E.3 Collection process", "content": "\u2022 How was the data associated with each instance acquired?\nWe directly used the original data instance to curate our own dataset.\n\u2022 What mechanisms or procedures were used to collect the data (e.g., hardware appara-tuses or sensors, manual human curation, software programs, software APIs)?\nWe mainly used Python scripts to collect, process and label the data.\n\u2022 If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\nThe random sampling involved in this study relies on specific seed (42), thus becomes deterministic.\n\u2022 Who was involved in the data collection process (e.g., students, crowd workers, con-tractors), and how were they compensated (e.g., how much were crowd workers paid)?\nThe data collection process was fully performed by the study's authors.\n\u2022 Over what timeframe was the data collected?\nN/A\n\u2022 Were any ethical review processes conducted (e.g., by an institutional review board)?\nN/A.\n\u2022 Does the dataset relate to people?\nYes.\n\u2022 Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\nAll data are collected through open-source database without interaction with individuals."}, {"title": "E.4 Preprocessing/cleaning/labeling", "content": "\u2022 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucket-ing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\nYes. The preprocessing on MIMIC-III data follows existing work [20].\n\u2022 Was the \"raw\" data saved in addition to the preprocess/cleaned/labeled data (e.g., to support unanticipated future uses)?\nN/A.\n\u2022 Is the software that was used to preprocess/clean/label the data available?\nPreprocessing, cleaning, and labeling are done via Python."}, {"title": "E.5 Uses", "content": "\u2022 Has the dataset been used for any tasks already?\nNo.\n\u2022 Is there a repository that links to any or all papers or systems that use the dataset?\nNo.\n\u2022 What (other) tasks could the dataset be used for?\nWhile the dataset is curated for research on federated medical knowledge injection problem, other studies concerning developing centralized medical foundation model can also leverage the dataset.\n\u2022 Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nN/A.\n\u2022 Are there tasks for which the dataset should not be used?\nN/A."}, {"title": "E.6 Distribution", "content": "\u2022 Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\nNo.\n\u2022 How will the dataset be distributed?\nThe preprocessing code is available at https://github.com/psudslab/ FEDMEKI/tree/main/data_preprocess. Users can download corresponding dataset and utilize the preprocessing scripts for generating the final dataset used in this study.\n\u2022 Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\nThe dataset is released under MIT License.\n\u2022 Have any third parties imposed IP-based or other restrictions on the data associated with the instances?\nNo.\n\u2022 Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?\nNo."}, {"title": "E.7 Maintenance", "content": "\u2022 Who will be supporting/hosting/maintaining the dataset?\nThe authors of this paper.\n\u2022 How can the owner/curator/manager of the dataset be contacted(e.g., email address)?\nContact the first authors (jqwang@psu.edu and xcwang@psu.edu).\n\u2022 Is there an erratum?\nNo.\n\u2022 Will the dataset be updated (e.g., to correct labeling erros, add new instances, delete instances)?\nIf any corrections are required, our plan is to upload an updated version of the dataset with comprehensive explanations for the changes. Furthermore, as we broaden our QA scope, we will consistently update the dataset with new QA templates/instances.\n\u2022 If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\nN/A\n\u2022 Will older versions of the dataset continue to be supported/hosted/maintained?\nPrimarily, we plan to maintain only the most recent version of the dataset. However, under certain circumstances, such as significant updates to our dataset or the need for validation of previous research work using older versions, we will exceptionally preserve previous versions of the dataset for up to one year.\n\u2022 If others want to extend/augment/build on/contribute to the dataset, is there a mecha-nism for them to do so?\nContact the authors of this paper."}, {"title": "F Training Task \u2013 Lung Opacity Detection", "content": "F.1 Task Description\nIn the United States, pneumonia keeps the ailment on the list of top 10 causes of death in the country. The task is to locate lung opacities on chest radiographs. In this challenge [17], 18,406 images are annotated as either Lung Opacity or Normal, providing a basis for extracting the binary classification task. The task is to develop an algorithm to detect visual indicators of pneumonia in medical images. Specifically, the algorithm needs to identify and localize lung opacities in chest radiographs.\nF.2 License and Ethics\nThis dataset is permitted to access and utilize these de-identified imaging datasets and annotations for academic research, educational purposes, or other commercial or non-commercial uses, provided you adhere to the appropriate citations.\nF.3 Access and Preprocessing\nThe resource is available to access via the official website at https://www.rsna.org/rsnai/ ai-image-challenge/rsna-pneumonia-detection-challenge-2018 and Kag- gle at https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/ overview. It includes dataset description, annotations, and mapping from RSNA image dataset to original NIH dataset. The data is organized as a set of patient IDs with corresponding im- age class annotations, including \"No Lung Opacity/Not Normal,\" \"Normal,\" and \"Lung Opac- ity.\" We collected images labeled as either \"Normal\" or \"Lung Opacity\" and formulated the problem as a binary classification task. The code for preprocessing is available at https: //github.com/psudslab/FEDMEKI/tree/main/data_preprocess.\nF.4 Data Samples\nWe provide a random data sample from the dataset and visualize it in"}, {"title": "G Training Task \u2013 COVID-19 Detection", "content": "G.1 Task Description\nThis task challenges the model to assess whether X-ray images display symptoms of Covid-19, thereby evaluating the model's proficiency in interpreting medical imagery. For this purpose, we employ the COVQU dataset [18].\nG.2 License and Ethics\nThe licensing and ethical compliance adhere to the regulations established by the original datasets.\nG.3 Access and Preprocessing\nThis dataset can be accessed via the link at https://www.kaggle.com/datasets/ tawsifurrahman/covid19-radiography-database. This dataset, featuring COVID- 19, normal, and other lung infection categories, is being released incrementally. The initial release comprised 219 COVID-19, 1,341 normal, and 1,345 viral pneumonia chest X-ray (CXR) images. The first update expanded the COVID-19 category to include 1,200 CXR images. In the second update, the collection was further enlarged to include 3,616 COVID-19 positive cases, along with 10,192 normal, 6,012 lung opacity (non-COVID lung infection), and 1,345 viral pneumonia images, complete with corresponding lung masks. We selected normal and COVID-19 positive images to formulate this task as a binary classification problem.\nG.4 Data Samples\nWe provide a random data sample from the dataset and visualize it in"}, {"title": "\u0397 Training Task \u2013 ECG Abnormal Detection", "content": "H.1 Task Description\nElectrocardiography (ECG) is a crucial diagnostic tool for assessing a patient's cardiac condition", "https": "physionet. org/content/ptb-xl/1.0.3/ or via the terminal by wget -r -N -c -np https://physionet.org/files/ptb"}]}