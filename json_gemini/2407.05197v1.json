{"title": "A Generalized Transformer-based Radio Link Failure Prediction Framework in 5G RANS", "authors": ["Kazi Hasan", "Thomas Trappenberg", "Israat Haque"], "abstract": "Radio link failure (RLF) prediction system in Radio Access Networks (RANs) is critical for ensuring seamless communication and meeting the stringent requirements of high data rates, low latency, and improved reliability in 5G networks. However, weather conditions such as precipitation, humidity, temperature, and wind impact these communication links. Usually, historical radio link Key Performance Indicators (KPIs) and their surrounding weather station observations are utilized for building learning-based RLF prediction models. However, such models must be capable of learning the spatial weather context in a dynamic RAN and effectively encoding time series KPIs with the weather observation data. Existing works fail to incorporate both of these essential design aspects of the prediction models. This paper fills the gap by proposing GenTrap, a novel RLF prediction framework that introduces a graph neural network (GNN)-based learnable weather effect aggregation module and employs state-of-the-art time series transformer as the temporal feature extractor for radio link failure prediction. The proposed aggregation method of GenTrap can be integrated into any existing prediction model to achieve better performance and generalizability. We evaluate GenTrap on two real-world datasets (rural and urban) with 2.6 million KPI data points and show that GenTrap offers a significantly higher F1-score (0.93 for rural and 0.79 for urban) compared to its counterparts while possessing generalization capability.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of modern networking applications such as Industry 4.0, intelligent transportation systems, health informatics, and augmented/virtual reality (AR/VR) necessitates high network bandwidth, robust reliability, and fast communication speeds [1]. Fifth-generation (5G) cellular networks aspire to accommodate these applications by satisfying various service level objectives (SLOs) through the use of millimetre-wave (mmWave) spectrums (24GHz to 100GHz). For instance, teleoperated driving systems can schedule human takeover if SLOs are not satisfied [2]. However, a 5G radio access network (RAN) requires deploying a denser array of base stations to communicate over mmWave radio as it traverses short distances. Moreover, these links suffer from distortion and attenuation due to weather phenomena like precipitation, humidity, temperature, and wind [3], [4]. Thus, mobile operators must have predictive maintenance of RANs connections to support the above real-time applications.\nA group of works investigated this correlation of mmWave radio and weather conditions using operator-provided real data [5], [6], [7]. Our initial investigation on the same dataset [6] reveals the same findings and motivates us to revisit the current reliable 5G RAN systems. These works developed learning-based failure prediction schemes due to the availability of radio key performance indicators (KPIs) from radio stations and respective weather attributes. The automated systems also reduce human intervention, CAPEX, and OPEX. Semih et al. propose a branched LSTM architecture to process both temporal and spatial features and offer better performance compared to models considering only temporal data processing [6]. Islam et al. introduce a comprehensive data preprocessing pipeline and use an LSTM-autoencoder model to predict link failures based on the reconstruction loss [7]. Finally, Agarwal et al. deploy decision trees and random forest classifiers to predict upcoming five-day radio link failures [5].\nThese works demonstrate the influence of weather attributes on radio communication and incorporate them into their developed models. However, the solutions suffer from few critical limitations. Specifically, these models cannot capture long-term dependencies due to vanishing gradients issue of LSTM [8]. Also, they do not weigh the importance of different elements in a sequence during predictions [9]. Another critical design aspect is correctly associating each link to the surrounding weather stations that affect the link. However, the existing approaches deploy heuristics to associate radio sites with weather stations. For instance, [5], [6], and [7] use the closest weather station, the aggregated k nearest stations, and the maximum of minimum distances between radio and weather sites, respectively. Also, these heuristic-based solutions cannot generalize well on a dynamic topology, which may occur if providers selectively turn on/off radio stations for efficient resource utilization [10]. Overall, existing LSTM-based solutions suffer from learnability and generalizability critical for predictive RAN maintenance.\nThis work fills the above gaps and proposes GenTrap, a novel radio link failure prediction framework capable of efficiently learning both the spatial (radio and weather stations association) and temporal (time-series features) context of RAN and surrounding weather stations. Specifically, GenTrap leverages a Graph Neural Network (GNN) to dynamically learn the effect of weather attributes from surrounding stations on radio links and realize a generalized model for unseen radio links. This GNN module can be incorporated into existing architectures for better performance and generalizability. In addition, the time series Transformer can encode complex temporal dependencies by learning which time point in the past to focus on for predicting future link failures [11]. Thus, the novelty of GenTrap includes developing a learnable architecture for capturing weather effects and applying transformers in encoding time series radio and weather data for radio link failure prediction.\nThe GNN weather effect learnable module dynamically assigns k nearest weather stations to each radio link, i.e., allows each link to learn from different values of k. Then, the time series transformer module encodes each pair of historical radio link KPI and the assigned weather station observations. An average pooling operation on the transformer output captures the temporal dependencies among features for each link. A GNN max aggregation over these vectors generates latent representations for spatio-temporal correlations between links and their associated weather stations. The static features of the link are processed using a feed-forward network. Finally, the feed-forward output is concatenated with the spatio-temporal latent vector and passed through a final feed-forward network for link failure prediction the next day. We evaluate GenTrap over two sets (urban and rural) of open-source real-world data provided by the top Turkish telecommunications company, Turkcell [12], [6]. The datasets consist of KPIs from both regular and failed links and surrounding weather stations (details in Section IV). The contributions of this paper are the following:\n\u2022 We propose the GenTrap framework that introduces a generalized weather effect learnable GNN aggregation module and incorporates the state-of-the-art time-series transformer to capture spatio-temporal context efficiently.\n\u2022 To the best of our knowledge, we are the first to propose a novel aggregation module that can also improve existing deep learning-based failure prediction models.\n\u2022 We rigorously experiment using two real-world data sets to show the superiority of GenTrap in performance and generalizability compared to its counterpart. Specifically, it offers an F1 score of 0.92 and improves the F1 score of LSTM+ from 0.63 to 0.70, incorporating the proposed aggregation module.\n\u2022 We share the GenTrap prototype code [13] for reproducing, adapting, and extending the proposed framework."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "This section offers the necessary background to understand the propose work and associates that knowledge to motivate the need for developing GenTrap.\n5G RAN. 5G has gained popularity due to its support for emerging applications that require high bandwidth, high reliability, and low latency [14]. It uses mmWaves to achieve these breakthroughs at the cost of coverage size and higher penetration loss [15]. 5G deployment takes advantage of small cells that collect user traffic and communicate it to radio sites/stations (RS) to mitigate these drawbacks. The radio sites use 5G radio links (RL) to communicate with each other and the core network, which connects users to the Internet [16]. These links are usually surrounded by numerous weather stations (WS) that can provide weather context at the radio links [6]. An overview of this type of deployment is depicted in Fig. 1. We use radio site KPIs along with the weather station data to predict radio link failures for the upcoming day. Thus, providers can take the necessary precautions for critical services.\nImbalance dataset. The percentage of radio link failures, however, is less compared to the normally operating ones, which creates imbalanced data to be processed. For instance, the Turkcell dataset [6] that we use has 0.3% and 0.06% failures in rural and urban deployments, respectively [6]. Thus, deep learning models trained on such a highly imbalanced dataset leads to good performance only on the majority class [17]. Typical approaches for handling such datasets are to use random undersampling of the majority class [6] and SMOTE oversampling of the minority class [7]. Undersampling balances class distributions by randomly removing majority class instances, which may remove informative data points [18]. On the other hand, SMOTE oversampling takes each minority class sample and generates synthetic examples along the line segments by joining k nearest neighbours [19]. However, some of the limitations of SMOTE include generating noisy samples and introducing bias due to sub-optimal neighbours selection [20].\nWe use the weighted cross entropy loss function to overcome the limitations of undersampling and oversampling as data points are neither sampled nor generated [21], [22], [23]. It deals with the imbalanced data by incorporating prior probabilities into a cost-sensitive cross-entropy error function [22]. The regular cross-entropy function is symmetrical. Also, the error reduction for both classes occurs at the same logarithmic rate. Thus, in the case of imbalanced data, the majority class will have a larger influence on the total loss as the overall error is minimized regardless of class. We use the following weighted cross entropy loss function to mitigate the effects of class imbalance in the dataset.\n$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m -y^{(i)} log(\\hat{y}^{(i)})\\lambda - (1 - y^{(i)}) log(1 - \\hat{y}^{(i)})$  (1)\nHere, $J(\\theta)$ and $m$ are the total loss and number of samples, respectively. $y$ is the ground truth ($y = 1$ for failure) and $\\hat{y}$ is the model prediction.\nGraph Neural Network (GNN) aggregation. We can deploy a learning-based failure prediction scheme on the balanced data by incorporating weather impact on radio links. For instance, existing works incorporate surrounding weather station information - by using derived features, optimal distance, and closest station data to capture their spatial context [6], [7], [5]. However, the optimal number of closest weather stations can vary across links even in the same deployment;"}, {"title": "III. RELATED WORK", "content": "This section presents two groups of related works for GenTrap: learning-based failure prediction approaches in 5G and GNN-based aggregation methods that capture spatial correlations.\nLearning-based failure prediction. Khunteta et al. [29] and Boutiba et al. [30] introduced the LSTM network to capture temporal feature correlations to predict link failures, but they do not consider weather effects. Other works filled the gap and utilized both historical radio link KPIs and weather observation data - similar to the dataset used in our approach. For example, Agarwal et al. [5] combined individual link features with the closest weather station measurements and proposed Random Forest as the classifier. Aktas et al. [6] utilized a branched architecture with LSTM and feed-forward network to capture temporal and categorical feature dependencies, respectively. Islam et al. [7] exploited the advantage of the reconstruction capabilities of LSTM-autoencoder by training their model on normal operational data and flagging data points with high reconstruction error as a failure during testing. These approaches rely on LSTM's ability to extract useful information. Still, they cannot weigh the importance of elements in a time series sequence. They fail to capture all possible influences among time series variables [9]. Recently, transformer models have demonstrated promising results in time series forecasting [28], as they can capture long-range dependencies, focus on important elements in a sequence and learn from all possible dependencies [11]. We take advantage of the time-series transformer model and propose a branched architecture that performs graph aggregation over each link's surrounding weather stations to achieve the best performance."}, {"title": "IV. DATASET DESCRIPTION", "content": "The performance of GenTrap is evaluated over two sets of real-world open-source data from a renowned telecommunication provider, Turkcell [6]. The dataset comprises radio link configuration and key performance indicator (KPIs) data, coupled with time-aligned weather station observations of two distinct deployments, urban and rural, where the time range is between January 2019 to December 2020 and January 2019 to December 2019, respectively. Because of privacy concerns, some configuration parameters and performance data (e.g., equipment name, link IDs, etc.) of the radio links are anonymized without loss of information. Also, the actual GPS location of these stations is not provided; instead, there are pairwise relative distances among these sites. A detailed description of the data tables is provided below.\nrl-sites. This data contains radio site identifiers and site-specific parameters such as height and clutter class - surrounding environment at the site, e.g., open urban, open land, dense tree area, etc. The same radio site can have multiple radio links as each uses different links to communicate with different sites.\nrl-kpis. Presents daily radio link KPIs, where important ones include severally error second, error second, unavailable second, block bit error, etc. and link-specific configuration parameters such as card type, modulation, frequency band, etc. Each link is uniquely identified with a pair of radio site-id and mini link-id.\nmet-stations. This data encompasses unique weather station numbers and station-specific parameters such as height and clutter class information. The surrounding environment can take clutter class values such as dense trees, open land, airports, etc. These features provide the spatial characteristics of the weather stations.\nmet-real. Provides hourly historical weather observations (e.g., temperature, humidity, precipitation, etc.) that are aggregated daily to align with the radio link KPI data. These features capture the temporal properties of the weather stations.\nmet-forecast. This data provides the upcoming five-day weather forecast (snow, rain, scattered clouds, etc.), humidity, temperature, wind speed, etc. for each day. The maximum and minimum predictions are also provided for forecast features (e.g., temperature and humidity).\ndistances. Contains pairwise relative distances between all radio sites and weather stations, where the distances are considered in units.\nThe above datasets contain similar features in both urban and rural deployments but with different numbers of radio sites, radio links, and weather stations (Table I).\nImpact of weather on radio links. We try to understand the relationship between surrounding weather station data and radio link failure through an initial investigation. In Fig 3, we plot precipitation recorded from the closest weather station of a radio link and observe that failure occurs during peak rainfall. This demonstrates heavy precipitation has impact on 5G communication channels."}, {"title": "V. GENTRAP ARCHITECTURE", "content": "This section first presents the GenTrap architecture. Then, we illustrate the integration of GNN-based spatial context capturing in existing LSTM+ and LSTM-Autoencoder models.\nA. GenTrap\nFig. 4 presents the GenTrap architecture, which maps time-series sequences to a probability vector. Specifically, the prediction system takes the radio link KPIs and surrounding weather station observations as inputs and generates the link failure probability vector for the following day. The system deploys 1 GNN aggregation over variable weather stations, 2 a time-series transformer, and 3 a GNN max aggregation function to generate an embedding vector that captures the given radio link and its relevant weather station context. In parallel, 4 a feed-forward network processes one hot encoded static feature and generates a latent representation to capture link and weather station configuration parameters. Then, these two context feature vectors are concatenated and fed to 5 another feed-forward network to produce the final output vector with two elements: the probability of link failure on the following day and the probability of no failure. We divide GenTrap into three main modules: GNN aggregation, generalized transformer branch, and feed-forward branches, which are presented below.\n1) Learnable GNN Aggregation: This is the key component of GenTrap in learning the spatial context among radio and weather stations to realize a generalized model. The GNN aggregation scheme is presented in Algorithm 1 that has two components (highlighted in Fig. 4): 1 GNN Variable Number of WS + RL and 3 GNN Max Aggregation. In the first component, for each mini-batch of m links, we pick a value k where k can range from one to the maximum number of closest weather stations (Line 2). Thus, links in mini-batches can be associated with different numbers of closest weather stations. Specifically, we consider a variable number of weather stations for each link (Line 5). We iterate over its k closest weather stations (Line 6) and concatenate the KPI feature vector with the corresponding time-aligned weather station observation vectors to generate k WS + RL vectors (Line 7) for the chosen radio link. Then, we use the Transformer module (presented below) to convert them into context-aware representation vectors (Line 8). In the second component, we take this output and perform the global average pooling to create k temporal embedding vectors that capture the time series dependencies over historical link and weather data. We do a max aggregation (Line 13) across these vectors to get our final node embedding vector ($L_m$ NodeEmbd) for a radio link. Similarly, we calculate node embedding vectors for all m links.\n2) Transformer Branch: The transformer module goes over the time series vectors as part of the GNN variable weather station consideration process and generates radio link plus weather station embedding vectors that capture the influence of each element on every other element of the time series sequence.\nSpecifically, it receives radio link (RL) and weather station (WS) time-series data as input. The RL time series includes 9 features (e.g., severe error seconds, available time, bbe, etc.), whereas the weather station time series includes 7 features (e.g., temperature, humidity, precipitation, etc.). These features are available for each day, and we add the time step as an additional feature as a positional encoding scheme for the transformer. We can describe time series vectors for a radio link as L = {$L_1, L_2, L_3, \u2026., L_t$}, where $L \\in R^{tX9}$ and t ranges from 1 to the total number of days. While weather station vectors can be represented as W = {$W_1,W_2,W_3, ..., W_n$} in ascending order of distance from L, where n is the total number of weather stations in a deployment. Each weather station has a time-series data, $W_1 = {W_1, W_1, W_1, ..., W_1 }$; where W\u2208 $R^{nXtX7}$.\nIn Fig. 4, WS1 + RL represents the previous five-day feature vectors of one radio link and its first closest weather station time-series data, whereas WS2 + RL represents the same radio link and second closest weather station data. These (WS+RL) vectors are the concatenation of 9 link and 7 weather features along with 1 time-step number column, giving us the input tensor to transformer module, which is of shape = (batchsize,3X5X17). The transformer module has a multi head attention with 4 heads, each of size 32, and the two 1D convolution filters are of size 32 and 17, respectively (Fig. 2). The output shape of the transformer is (batchsize, 3X5X17), the same as the input shape. We perform global average pooling across the time dimension to capture temporal dependencies for each WS+RL pair, giving us output (batchsize, 5,17) - denoted by different colors for different pairs for each pair. Lastly, we choose the max function as our aggregator to perform an element-wise max operation across the embedding vectors to get one feature vector (batchsize, 17). Thus, the generalized transformer module generates one feature vector to capture the variable number of closest weather station effects on each radio link.\n3) Feed forward Branches: We use the generalized transformer module to handle time series radio links and weather station data. On the other hand, we deploy a feed-forward network to encode static radio links and weather station features (Fig. 4). Radio link features (e.g., modulation type, frequency band) and weather station features (e.g., clutter class, weather day) are categorical features. We perform one hot encoding of these categorical features and pass them to a feed-forward network with two layers: 32 and 17 neurons. The output vector from the static branch is concatenated with the output vector from the generalized transformer branch to get a representation vector (batchsize, 34) that captures both temporal and static dependencies. We feed the concatenated vector to another feed-forward network with two layers: 16 and 2 neurons. A Softmax layer gets the final probability vector for link failure. We train our model with weighted categorical cross-entropy loss and Adam optimizer. We take the maximum of the 2 probability scores during inference to make a binary prediction for each input.\nB. LSTM+\nFig. 5 presents the LSTM+ architecture, and Fig. 6 shows the LSTM+ with augmented GNN-based dynamic weather station aggregation. The LSTM+ architecture uses two separate branches to process time series and static features. The temporal dependencies of radio links and derived weather station features are captured using 4 LSTM layers. In contrast, the configuration parameters are one-hot encoded and processed by a feed-forward network similar to the ones used in GenTrap. These two output vectors are concatenated and fed to another feed-forward network to get the final probability score vector.\nC. LSTM-AutoEncoder\nFig. 7 and Fig. 8 present the LSTM-Autoencoder and its integration with the GNN aggregation, respectively. The LSTM-Autoencoder is only trained on normal links to encode the input sequence to a latent representation and decode it back to the output sequence [34]. The encoder and decoder LSTM consists of two LSTM layers with decreasing (from 32 and 24) and increasing (24 and 32) number of neurons."}, {"title": "VI. RADIO LINK FAILURE PREDICTION", "content": "This section presents the link failure prediction workflow (Fig. 9) deploying the above learning models. The process consists of three components: data preprocessing, model training and validation, and model testing. In brief, the data preprocessing step consists of cleaning raw data, correlating links with weather stations, handling missing values, encoding categorical features, and performing a time series split. Then, the next part focuses on training and validating the existing LSTM+ and LSTM-autoencoder models along with the proposed GenTrap. Lastly, we test the performance of these approaches on unseen real-world link KPIs and weather observations, which is presented in Section VII.\nA. Data Preprocessing\nThe data preprocessing consists of the following steps.\nData preparation. The effectiveness, precision, and intricacy of machine learning tasks are significantly influenced by calibrating the training data [35]. Our initial investigation revealed that there are inconsistent values in weather station and radio link data (e.g., unexpected string values both in radio and weather data). These inconsistencies lead to erroneous or impossible data transformation for the subsequent steps, e.g., casting features to proper data types. Thus, we first tackle these inconsistencies, e.g., by removing the data samples if a numerical feature contained unexpected string values. After handling inconsistent values, we cast all numerical and categorical features to the floating and the string data type, respectively. In this problem, we consider daily data for both radio links and weather stations.\nReal weather data alignment. Our dataset has data from different entities (e.g., weather stations and their observations, radio sites and their link performance data). In order to merge weather observations with radio link KPIs, their temporal frequencies need to be maintained. Radio site KPIs and real weather realizations are collected in the chosen dataset over daily and hourly time intervals, respectively. We use the standard mean aggregation [6] to transform hourly realizations into daily weather data to align historical weather realizations with radio link KPIs.\nData imputation. The majority of statistical and machine learning algorithms lack robustness in handling missing values, thereby being susceptible to the impact of incomplete data [36]. We calculate the percentage of missing values for each feature in our dataset. Some features from historical radio link KPIs and real weather station data have a high percentage of missing values. We use a simple heuristic of dropping features with missing values of 20% or higher. Also, some numerical features suffer from missing segments over time, but the data can be reliably interpolated if the percentage of missing values is under 15% [37]. Thus, we deploy time series linear interpolation to impute missing numerical KPIs and historical weather observations [38].\nData Merging. We need to use historical KPIs and weather data to predict following-day link failure. Thus, we append a label column in the KPIs table, representing the next-day link status. Also, each radio site can have multiple links, so we merge the KPI features with the corresponding site features by matching the site id. Weather station features are also merged with weather observation data similarly.\nTackling data imbalance. We use the weighted cross-entropy loss function to tackle the data imbalance, which incorporates prior probabilities into a cost-sensitive cross-entropy error function. Unlike traditional cross-entropy, this weighted approach accounts for the imbalanced nature of the data, giving a larger influence to the majority class while minimizing overall error. The loss function puts the prior minority to majority class ratio \u5165 (0.003 for rural and 0.0006 for urban) into the regular cross entropy (Eq. 1). In rural deployment, this ensures that both classes have an equal influence because when y = 0 for a non-failure instance, the remaining term (1 \u2013 $y^\\text{(i)}$) log(1 \u2013 $\\hat{y}\\text{(i)}$) contributes \u5165 = 0.3 percent to the loss. Similarly, when y = 1 for a failure instance, the remaining term -y\u00b2 log(y) contributes (1 \u2212 ) = 99.7 percent to the loss.\nTime series split. Cross-validation (CV), a widely adopted method for assessing algorithm generalizability in classification and regression, has been extensively studied by researchers [39]. Our dataset contains time series numerical values both for radio link KPI features and weather station observations [6]. In the case of time series data, where the underlying process evolves over time, this can undermine the fundamental assumptions of cross-validation, which assumes that the data is independent and identically distributed. The temporal nature of time series data introduces dependencies and patterns that must be appropriately accounted for in the evaluation process [40]. Therefore, we use rolling origin [37] method to compare our framework with previous works.\nThe evaluation involves sequentially moving values from the validation set to the training set while changing the forecast origin accordingly. This way, folds with increasingly more train data are produced. It is also known as n-step-ahead evaluation, where n is the forecast horizon. The approach fits our use case because RLF prediction systems will be retrained or fine-tuned in the real world with new data as they become available. We create the 5 folds by first sorting the data across time and splitting them into the first 70% train, the next 20% validation and the last 10% test set to create the fold with the largest training data. For instance, in the urban deployment - data ranging from January 2019 to December 2020 - this results in train, validation, and test sets containing January 2019 to April 2020, May 2020 to September 2020, and October 2020 to December 2020 data, respectively. This completes the first fold, and subsequent folds are created by offsetting the splits by 10%. Thus, the second fold would contain the first 60% as the train, the next 70%-80% as the validation, and the next 80%-90% as the test data. (Fig. 10). Similarly, we create the rest of the folds for both deployments."}, {"title": "VII. EVALUATION OVER REAL-WORLD DATA", "content": "This section presents the evaluation results of GenTrap over two real-world datasets: urban and rural. We also implement and evaluate the existing LSTM+ and LSTM-autoencoder models on the same datasets for a fair comparison. Next", "metrics": "precision", "follows": "Precision = $\\frac{TP}{TP+FP}$, Recall = $\\frac{TP}{TP+FN}$, and F1score = $\\frac{2Precision Recall}{Precision+Recall}$. We report the average of failure and non-failure precision, recall, and F1-score.\nWe perform all the experiments on a machine with Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, 32 GB memory, and Nvidia Quadro RTX 8000 GPU with 50GB VRAM. The OS and GPU versions were Ubuntu 20.04.6 LTS and CUDA 11.7, respectively.\nB. Performance comparison of different models\nTable II presents the performance of GenTrap along with LSTM+, LSTM-autoencoder, and their GNN aggregation-capable variants. We report F1 scores with their corresponding precisions and recalls on the 5-fold test data while predicting radio link failures for the following days of chosen days. The results confirm that GenTrap significantly and consistently outperforms all existing approaches with the best F1 scores of 0.93 and 0.79 for rural and urban deployment, respectively.\nThe inferior performance of LSTM+ and LSTM Autoencoder can be attributed to two factors. First, these existing models deployed heuristics instead of learning the weather station association using GNN. Secondly, LSTM considers equal weights for all previous day data. It does not have any internal mechanism to give more priority to important days, e.g., feature values from recent days or important weather events. Another shortcoming of LSTM is its context window, which is limited to the previous context, thus facing the issue of vanishing gradients. This constraint can be a limitation when capturing complex dependencies with a long sequence span. On the other hand, GenTrap deploys GNN-based node aggregation for efficient spatial context capture. Furthermore, the transformer uses a self-attention mechanism to focus on the most relevant elements of the input sequence. Also, it has a larger context window to better understand the relationships between feature values that are far apart in the time sequence.\nThese advantages from GNN aggregation and transformer time series encoding lead to GenTrap's superior performance.\nFig. 11 presents the distribution and variability of F1-scores of different models using box and whiskers plots. The box represents the interquartile range (IQR), which contains the middle 50% of the data. GenTrap scores are more concentrated in the middle with less variability, while scores from other approaches are more spread out. Having a lower variability makes GenTrap more reliable in real-world applications than previous approaches like LSTM+ and LSTM Autoencoder, which have greater variability. In the LSTM+ plot, we also observe outliers that lie beyond the whiskers, making this approach less credible.\nWe also perform a One-way ANOVA test to determine whether the GenTrap results are statistically significant. We achieve a p value of 0.003, which proves the result is statistically significant at the 0.05 level.\nC. Performance improvement using GNN aggregation\nThe purpose of this evaluation is to show the benefits of having a learnable spatial context capture ability using GNN aggregation compared to existing heuristic-based context capture. Table II presents the performance of GenLSTM+ and GenLSTMAE, i.e., LSTM+ and LSTM-Autoencoder augmented with GNN aggregation. GenLSTM+ significantly outperforms the heuristic-based LSTM+. In particular, GenLSTM+ offers an F1 score of 0.85 and 0.70 in rural and urban, respectively, compared to 0.71 and 0.65 in LSTM+. In the case of GenLSTMAE, we observe a similar improvement, i.e., an increase of F1 score from 0.54 to 0.58 on average across the data splits for the rural deployment. Note that the LSTM-Autoencoder result of 0.60 has a worse performance than the reported one from previous work [7", "41": [42], "11": [43]}]}