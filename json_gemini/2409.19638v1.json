{"title": "BadHMP: Backdoor Attack against Human Motion Prediction", "authors": ["Chaohui Xu", "Si Wang", "Chip-Hong Chang"], "abstract": "Precise future human motion prediction over sub-second horizons from past observations is crucial for various safety-critical applications. To date, only one study has examined the vulnerability of human motion prediction to evasion attacks. In this paper, we propose BadHMP, the first backdoor attack that targets specifically human motion prediction. Our approach involves generating poisoned training samples by embedding a localized backdoor trigger in one arm of the skeleton, causing selected joints to remain relatively still or follow predefined motion in historical time steps. Subsequently, the future sequences are globally modified to the target sequences, and the entire training dataset is traversed to select the most suitable samples for poisoning. Our carefully designed backdoor triggers and targets guarantee the smoothness and naturalness of the poisoned samples, making them stealthy enough to evade detection by the model trainer while keeping the poisoned model unobtrusive in terms of prediction fidelity to untainted sequences. The target sequences can be successfully activated by the designed input sequences even with a low poisoned sample injection ratio. Experimental results on two datasets (Human3.6M and CMU-Mocap) and two network architectures (LTD and HRI) demonstrate the high-fidelity, effectiveness, and stealthiness of BadHMP. Robustness of our attack against fine-tuning defense is also verified.", "sections": [{"title": "I. INTRODUCTION", "content": "Human motion prediction is a sequence-to-sequence task where future motion sequences are predicted based on observed historical motion sequences. Accurate forecasting of future human poses is crucial for the success of various applications, such as human-robot interaction and collaboration (HRI/C) [1], [2], human tracking [3], autonomous driving [4], and rehabilitation [5].\nVarious advanced neural network architectures have been explored for this task, including recurrent neural networks (RNNs) [6], [7], [8], graph convolutional networks (GCNs) [9], [10], [11], generative models [12], [13], [14], [15], and Transformers [16], [17]. Despite the extensive research into deep learning based human motion prediction, the vulnerability of these models to potential attacks has not been sufficiently explored. To date, only one evasion attack [18] has been investigated on convolution-based human motion prediction. Hence, there exists a significant gap in understanding the robustness of human motion prediction models against other forms of malicious attacks.\nBackdoor attack is an emerging form of data-poisoning attack targeting deep neural networks (DNNs), where the adversary subtly alters a small subset of training samples by embedding a trigger into the input data and substituting the corresponding outputs with predefined targets. During training, the victim model inadvertently learns both the intended tasks and a strong association between the trigger and the target output. At the inference stage, the model behaves as a benign model under normal conditions but consistently produces the predefined target output when the trigger is present in the input.\nMost existing backdoor attacks focus on image classification tasks [19], [20], [21], [22], [23], while a few have been extended to other tasks [24], [25], [26], [27], [28]. However, backdoor attacks on human motion prediction models remain uncharted. A successful backdoor attack in this domain poses safety hazards that may lead to grave consequences. For instance, in the scenario of HRI/C, a robot equipped with a poisoned model may inaccurately predict human motions, leading to erroneous decisions and potentially hazardous outcomes in subsequent time steps. The main challenges of launching a backdoor attack on human motion prediction are as follows: 1) Due to the unique data format of human motion samples (spatial and temporal 3D joint positions), existing data-poisoning techniques are not directly applicable for generating the poisoned samples for such task; 2) To avoid detection by the model trainer, the poisoned training samples of human motion sequences must remain smooth and natural. This means that the clean samples need to be subtly manipulated to ensure that the fundamental physics principles of human body dynamics are not violated.\nIn this paper, we propose a novel backdoor attack to human motion prediction task dubbed BadHMP. The main contributions of our work are summarized as follows:\n\u2022 To the best of our knowledge, this is the first work that investigates the vulnerability of human motion prediction models to backdoor attacks;\n\u2022 Two new types of backdoor triggers (\"rigid\" and \"soft\") and two types of targets (\"jammed\u201d and \u201cmoving\") dedicated to human motion prediction are designed, resulting in four distinct poisoning strategies;\n\u2022 Two novel evaluation metrics, Clean Data Error (CDE) and Backdoor Data Error (BDE), are proposed to assess the attack performance on human motion predictors;\n\u2022 Extensive experiments are conducted on two popular benchmark datasets (Human3.6M and CMU-Mocap) and two widely used model architectures (LTD and HRI) to attest the robustness of our proposed attack.\""}, {"title": "II. RELATED WORKS", "content": "Due to their good performance in sequence-to-sequence prediction tasks, RNNs have been extensively studied for human motion prediction. In the first RNN-based approach [6], an Encoder-Recurrent-Decoder model was used for motion prediction. Subsequently, a Structural-RNN [7] was developed to manually encode the spatial and temporal structures. To achieve multi-action predictions using a single model, a residual architecture for velocity prediction was proposed in [8]. Numerous RNN-based methods [29], [30], [31], [32] have since emerged, aiming to further enhance the prediction performance.\nThe GCN-based motion prediction method was first introduced in [9] by employing the Discrete Cosine Transform (DCT) to encode the spatial dependency and temporal information of human poses. This approach was further refined [10] by capturing similarities between current and historical motion contexts. Inspired by the success of GCN in modeling dynamic relations among pose joints [9], various GCN-based prediction methods [11], [33] have been developed for more complex spatio-temporal dependencies over diverse action types. Recently, the attention mechanism [16], [17] of Transformers [34] has been leveraged to capture the spatial, temporal, and pairwise joint relationships within motion sequences. Without resorting to complex deep learning architectures, a lightweight multi-layer perception combined with DCT and standard optimization techniques can achieve excellent performance with fewer parameters [35]. Sampling from deep generative models [12], [13], [14], [15] trained over large motion-capture dataset has also been devised for more realistic and coherent stochastic human motion prediction.\nBackdoor attacks train the victim model with poisoned training data to embed a malicious backdoor that can be activated at test time to cause the victim model to misbehave. The most popular attack dates back to BadNets [19], where a small number of training samples are stamped with a tiny fixed binary pattern (a.k.a trigger) at the right bottom corner and relabeled to a target class. The backdoor feature is learnt by the DNN classifier during the training process. Since then, a series of improved backdoor attacks have been developed, employing techniques such as image blending transformations [36], steganography [21], warping transformations [20], and adaptive optimizations [37], to enhance the stealthiness of the backdoor trigger, thereby evading detection by the model trainer. An even more inconspicuous branch of clean-label backdoor attacks [23], [22], [38] can achieve target misclassification by hiding the backdoor triggers into the training images without altering their labels. While most existing backdoor attacks have been developed for image classification tasks, some successful attacks have also been reported in other task domains, such as speech recognition [24], [25], graph classification [26], [27], crowd counting [28], and point cloud classification [39], [40].\nDefenses against backdoor attacks on computer vision and natural language processing models can be broadly categorized into two groups: detection and mitigation. Detection methods [41], [42], [43] focus on identifying whether the training dataset or the trained model has been embedded with a backdoor, while mitigation techniques [44], [45], [46], [47] aim to purify the poisoned training dataset or sanitize the victim model to reduce the success rate of backdoor activation by the triggered samples without compromising the prediction accuracy of benign samples."}, {"title": "III. THREAT MODEL", "content": "Given a history motion sequence $X_{1:N} = [X_1, X_2,..., X_N]$ that is composed of N consecutive frames of human poses, the human motion prediction model $f_\\theta$ parameterized by $\\theta$ aims to forecast the future T frames of poses as $X_{N+1:N+T} = [X_{N+1}, X_{N+2},..., X_{N+T}]$, where each pose $x_i \\in \\mathbb{R}^{K\\times 3}$ consists of 3D coordinates of K joints. Let $D_{tr}$ and $D_{ts}$ denote the training and test datasets, respectively. The training process aims to solve the following optimization problem:\n$\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{X \\sim D_{tr}} [L_m(f_\\theta(X_{1:N}), X_{N+1:N+T})] ,\\\\\n= \\arg \\min_\\theta \\mathbb{E}_{X \\sim D_{tr}} [ \\sum_{n=1}^{T}\\sum_{j=1}^{K} ||\\hat{X}^{(j,n)}_{N+1:N+T} - X^{(j,n)}_{N+1:N+T}||_2^2 ]$  (1)\nwhere $\\hat{X}_{N+1:N+T} = f_\\theta(X_{1:N})$ and $X_{N+1:N+T}$ denote the predicted and ground-truth poses of future T time steps, respectively. $\\hat{X}^{(j,n)}$ represents the predicted j-th joint position at frame n, and $x^{(j,n)}$ is its corresponding ground turth. $L_m$ denotes the Mean Per Joint Position Error (MPJPE) [48] in millimeter, which is the most widely used metric for 3D pose error evaluation."}, {"title": "A. Attack Scenario", "content": "Following the threat model of backdoor attacks on image classification models [19], [20], [23], we assume that the attacker is a malicious third party who provides the training set to the model trainer. In this scenario, the attacker is allowed to poison $p\\%$ of samples of the clean training set $D_{tr}$ before the training stage, where $p = (N_{poison}/N_{train}) \\times 100\\%$ is commonly referred to as the injection ratio. However, the attacker has no knowledge of or access to the training pipeline, including the model architecture, optimization algorithm, training loss, etc. Additionally, manipulating the well-trained victim model is also not permitted. The backdoor sample generation process can be expressed as $\\tilde{X} = G(X)$, where $G(.)$ denotes the poisoning function that will be elaborated in Sec. IV, and X is the poisoned sample."}, {"title": "B. Attacker's Goals", "content": "The attacker aims to launch a high-fidelity, effective and stealthy backdoor attack on the victim model $f_{\\theta'}$, with $\\theta'$ being the poisoned parameters.\nFidelity. The victim model $f_{\\theta'}$ is expected to perform normally as a benign model $f_\\theta$ when fed with clean test samples to prevent the backdoor attack from being noticed by the model trainer. To assess this, we define a Clean Data Error (CDE) metric as follows:\n$CDE(f) = \\mathbb{E}_{X \\sim D_{ts}} [L_m(f(X_{1:N}), X_{N+1:N+T})]$.  (2)\nThe CDE of the victim model should be comparable to that of the benign model, i.e., $|CDE(f_{\\theta'}) - CDE(f_\\theta)| \\leq \\epsilon$, with $\\epsilon$ being a samll positive threshold.\nEffectiveness. The victim model should produce incorrect sequences dictated by the attacker on triggered inputs at test time. We define the Backdoor Data Error (BDE) metric to evaluate the effectiveness of backdoor activation as:\n$BDE(f) = \\mathbb{E}_{X \\sim \\tilde{D}_{ts}} [L_m(f(X_{1:N}), \\tilde{X}_{N+1:N+T})]$,  (3)"}, {"title": "IV. THE PROPOSED ATTACK", "content": "Our attack consists of three steps: localized history sequences poisoning, global future sequences poisoning, and sample selection."}, {"title": "A. Localized History Sequences Poisoning", "content": "A body pose is composed of five parts: torso, left arm, right arm, left leg, and right leg. On the N-frame input sequences, only several connected joints in a fixed limb are manipulated for backdoor trigger embedding, while the remaining joints' positions are unchanged. This is to ensure that the semantic meaning of the input sequences is not damaged. For instance, a \u201cwalking\u201d sample can be poisoned by controlling the \"left arm\" to do a predefined motion like staying rigid or drawing a circle. In this paper, we select m joints on the left arm for poisoning, denoted as $J_p = \\{j_1, j_2,\\dots, j_m\\}$. The joint of the left shoulder is $j_0$.\nTwo types of triggers are proposed: rigid and soft.\nRigid trigger creates a condition where the joints on one arm (the non-dominant left arm is chosen for convenience) form a predefined pose and remain relatively still in the 3D space, with movements closely tied to $j_0$. Hence, the left arm appears \"frozen\" during the input time steps. Given a source sample $X^{src}$ randomly selected from the clean test set, the left arm pose at the first frame is grafted onto the N input frames of the clean training sample X. However, instead of directly grafting the left arm, a scaling transformation is first applied to $X^{src}$, producing $X^{src}_{Scale} = Scale(X^{src}, X)$, where the bone lengths match those in X but the directions remain the same as in $X^{src}$. This step avoids the disproportionate grafted left arm. The grafting operation is then performed as:\n$\\tilde{X}^{(j,n)} = \\begin{cases} X^{src(j,1)} + [X^{(j_0, n)} - X^{src(j_0,1)}], & \\text{if } j \\in J_p, \\\\ X^{(j,n)}, & \\text{otherwise,} \\end{cases}$ (7)\nwhere $1 \\leq n \\leq N$. The rigid trigger embedding process can be regarded as adopting a predefined left arm pose to the training input sequences without changing the bone lengths.\nSoft trigger also grafts the left arm pose from the source sample to the training sample. However, unlike the rigid trigger, it involves a frame-by-frame basis left arm pose grafting throughout the input time steps. Consequently, the left arm in the poisoned sample exhibits a predefined motion naturally and flexibly. This operation can be formulated as:\n$\\tilde{X}^{(j,n)} = \\begin{cases} X^{src(j,n)} - X^{src(j_0,n)} + X^{(j_0,n)}, & \\text{if } j \\in J_p, \\\\ X^{(j,n)}, & \\text{otherwise.} \\end{cases}$  (8)\nSince the left arm motion is extracted from a real test sample, its smoothness and naturalness are guaranteed. Generally, the soft trigger is stealthier than rigid trigger because the manipulated left arm continues to move flexibly, thus is less likely to draw attention. However, poisoning the victim model with the rigid trigger makes it easier for the attacker to activate the embedded backdoor at the inference stage - the attacker simply needs to ensure that the left arm maintains a predefined pose for N frames to induce the incorrect prediction."}, {"title": "B. Global Future Sequences Poisoning", "content": "On the contrary, when poisoning the future sequences, we globally modify the 3D coordinates of all the K joints. There are also two types of targets: jammed and moving.\nJammed target can be simply generated by replicating the last frame of the poisoned input sequences for the future T frames. This results in the whole sequence appearing to be jammed starting from frame N. The poisoning process is as follows:\n$\\tilde{X}_{N+1:N+T} = [\\tilde{X}_{N}, \\tilde{X}_{N},..., \\tilde{X}_{N}]$.  (9)\nDuring training, the victim model efficiently learns such a distinguishing \"jammed\" feature from the poisoned future sequences and associates it with the backdoor trigger feature (rigid or soft) in the input sequences. Consequently, the trained victim model will output \"jammed\" frames whenever it encounters the backdoor trigger at the inference stage.\nMoving target is embedded by applying fixed trajectories to all K joints. We first extract trajectories of all joints from the future sequences of the source sample, and subsequently add these trajectories to the last frame of the poisoned input sequences to generate the moving target as follows:\n$\\tilde{X}^{(j,n)} = X^{(j,N)} + [X^{src(j,n)} - X^{src(j,N)}], j \\in \\{1,2,...,K\\}$,  (10)\nwhere $N+1 \\leq n \\leq N + T$. Note that the source sample used for trajectories extraction can be either the same as or different from the one used for rigid/soft trigger generation, and scaling transformation is not required for the moving target generation.\nThe type of backdoor trigger and target for sample poisoning can be selected arbitrarily. Consequently, there are four possible poisoning strategies: rigid-jammed (R-J), rigid-moving (R-M), soft-jammed (S-J), and soft-moving (S-M)."}, {"title": "C. Sample Selection", "content": "Since the attacker can only poison a small fraction of the training set, it is necessary to select the top $p\\%$ of clean samples that are most suitable for poisoning.\nSpecifically, the attacker first poisons the entire clean training set to acquire the fully poisoned training set. For each pair of clean and poisoned samples, the absolute values of their differences in Acc, jerk, and BLC are calculated and weighted to measure the stealthy loss $L_s$ of the poisoned sample as follows:\n$L_s = |Acc(X) - Acc(\\tilde{X})| + \\lambda_1 |Jerk(X) - Jerk(\\tilde{X})| + \\lambda_2 |BLC(X) - BLC(\\tilde{X})|$, (11)\nwhere $\\lambda_1$ and $\\lambda_2$ are positive weighting factors that balance the contributions of different terms. Finally, the top $p\\%$ of poisoned samples with the smallest $L_s$ are selected to replace their counterparts in the clean training set $D_{tr}$ to generate the poisoned dataset $\\tilde{D}_{tr}$."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "The proposed backdoor attack is evaluated on two benchmark motion capture (mocap) datasets, including Human3.6M (H3.6M) [48] and CMU-Mocap (CMU).\u00b9\nH3.6M is the most widely used large-scale dataset for human motion prediction, comprising 3.6 million 3D human poses. It includes motion sequences of 7 actors (S1, S5, S6, S7, S8, S9, S11) performing 15 distinct actions, such as walking, eating, smoking, and phoning. The human skeleton is composed of 32 joints expressed by exponential maps. We convert these representations to 3D coordinates and use the remaining 22 joints after removing 10 redundant joints. Similar to previous works [9], [10], [18], the sequences are down-sampled to 25 frames per second (fps) with a sampling rate of 2. S5 and S11 are employed as the test and validation datasets, respectively, while the remaining 5 subjects serve as the training dataset. To balance different actions with different sequence lengths and avoid high variance, we take 256 random samples per action for test as in [10], [35].\nCMU contains 8 categories of actions where the 38-joint skeletons are also originally presented by exponential maps. Like H3.6M, these presentations are converted to 3D coordinates. The training and test datasets are divided according to the configuration of [9]. CMU is evaluated on 25 joints, with 256 random samples per action.\nOur attack is model agnostic, which assumes that the adversary has no knowledge about the victim model's architecture. To demonstrate the generalizability of our proposed attack, we evaluate its performance on two model architectures: LearningTrajectoryDependency (LTD) [9] and HistoryRepeatItself (HRI) [10].\nThe model is trained to predict both short-term (0 to 500 ms) and long-term (500 to 1000 ms) future human motions. The input length N is fixed at 50 frames. The output length T is set to 25 for both H3.6M and CMU datasets. The default injection ratio p is set to 10%. We use the Adam optimizer [51] and a batch size of 256 for training. The network is trained for 50 epochs, with the learning rate initially set to 0.01 and decayed by a factor of 0.96 every two epochs. For evaluation, we measure the CDE and BDE of the model at 80 and 400 ms for short-term prediction, and 560 and 1000 ms for long-term prediction."}, {"title": "B. Evaluation", "content": "To evaluate the fidelity of the poisoned model and the effectiveness of backdoor activation, we train a benign model and a victim model on $D_{tr}$ and $\\tilde{D}_{tr}$, respectively, and measure their prediction performance on both clean and poisoned test sets. The dataset used for the evaluation is H3.6M, and the model architecture is LTD. All poisoned training/test samples are generated by the soft-moving (S-M) poisoning strategy.\nThe benign model demonstrates excellent prediction performance on clean test samples, with an average CDE of approximately 12 at 80 ms. As prediction time increases, the CDE gradually rises, which is expected because the prediction errors accumulate over time. In contrast, the benign model's BDE is significantly higher than the CDE because it does not learn the backdoor features during training. As a result, it fails to produce the attacker expected predictions when trigger-embedded input sequences are encountered.\nThe CDE of the victim model remains very close to that of the benign model across all actions and evaluation time steps. This indicates that the \"fidelity\u201d criterion is met, as the victim model behaves like a normally trained model when processing clean test samples. However, for test samples generated by the specific poisoning strategy, the BDE of the victim model is significantly lower compared to the CDE. Additionally, the BDE accumulates much more slowly than the CDE over time. Specifically, the victim model's average CDE and BDE are 12.2 and 3.5 at 80 ms, and 119.4 and 6.8 at 1000 ms, respectively. These results show that the embedded backdoor can be successfully activated at test time, causing the victim model to accurately produce the target sequences as intended by the attacker, even for long-term predictions. Thus, our attack also fulfills the effectiveness criterion.\nThe attack performances in other cases across various datasets, model architectures, and poisoning strategies are also evaluated. Due to the page limit, we only report the average CDE and BDE of these experiments in Table II. All victim models trained on the poisoned H3.6M dataset have high fidelity and the target sequence can be effectively activated, regardless of the model architectures and poisoning strategies used. On the other hand, the attack effectiveness on the CMU dataset varies across different cases. Specifically, the \"soft\" trigger achieves a lower BDE compared to the \"rigid\" trigger. We hypothesize that the feature of the \"soft\" trigger is more easily learned by the victim model during training. Nevertheless, the BDE measured on the CMU dataset is consistently and significantly lower than the corresponding CDE, indicating that the attack is satisfactorily effective across all cases.\nThe semantic meaning of the predicted future sequences is altered and the motion is no longer identified as \"soccer\u201d. Notably, the prediction error accumulates more rapidly over time with clean input sequences. This observation aligns with the findings from the quantitative results provided in the tables. They demonstrate that the victim model exhibits lower BDE than CDE, and the embedded backdoor can be easily and effectively activated during inference.\nTable III reports the average Acc, Jerk, and BLC for paired clean and poisoned samples. The column \"Diff.\" shows the absolute difference between clean and poisoned samples. The 10% of clean training samples are either 1) randomly selected from the training set or 2) selected using the \"sample selection\" step for poisoning. The results indicate that the differences in Acc, Jerk, and BLC between clean and poisoned samples are smaller when the sample selection step is applied. Nevertheless, even without this step, randomly poisoned samples remain sufficiently stealthy. This is due to our carefully designed backdoor trigger and target, which ensure smooth and natural transitions, thereby avoiding significant changes in acceleration, jerk, and bone length.\nThe default injection ratio is set to 10% for all the experiments presented earlier. To investigate the effect of injection ratio on attack performance, we trained multiple victim LTD models on datasets poisoned with different ratios, specifically $p \\in \\{2\\%,5\\%,8\\%, 10\\%, 15\\%\\}$. The results show that the CDE of poisoned models is insensitive to changes in $p$. Even at an injection ratio of 15%, the CDE of the victim model remains very close to that of the benign model, thus further confirming that the victim model maintains a high fidelity even if it is heavily poisoned.\nMoreover, the BDE gradually decreases as $p$ increases, which is expected because a higher proportion of poisoned training samples allows the victim model to better learn the association of features between the backdoor trigger and the target sequence. The 10% default injection ratio provides a good balance of effectiveness (low BDE) and stealthiness (low injection ratio) on both datasets."}, {"title": "VI. CONCLUSIONS", "content": "This paper proposed BadHMP, the first backdoor attack targeting human motion prediction task. Two types of triggers and two types of targets are designed to generate smooth and natural poisoned samples. We also introduced a sample selection strategy to further improve the stealthiness of poisoned training samples, making them harder to be detected by the model trainer. The prediction fidelity of the poisoned model to benign input sequences, the activation success rate of target sequences, and the smoothness and naturalness of the trigger sequences of BadHMP are comprehensively evaluated by objective quantitative metrics on two datasets and two model architectures to validate its compliance with the fidelity, effectiveness and stealthiness criteria."}]}