{"title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management", "authors": ["Yi Xiong", "Hao Wu", "Changxu Shao", "Ziqing Wang", "Rui Zhang", "Yuhong Guo", "Junping Zhao", "Ke Zhang", "Zhenxuan Pan"], "abstract": "The expanding context windows in large language models (LLMs) have greatly enhanced their capabilities in various applications, but they also introduce significant challenges in maintaining low latency, particularly in Time to First Token (TTFT). This paper identifies that the sharp rise in TTFT as context length increases is predominantly driven by queuing delays, which are caused by the growing demands for GPU Key-Value (KV) cache allocation clashing with the limited availability of KV cache blocks. To address this issue, we propose LayerKV, a simple yet effective plug-in method that effectively reduces TTFT without requiring additional hardware or compromising output performance, while seamlessly integrating with existing parallelism strategies and scheduling techniques. Specifically, LayerKV introduces layer-wise KV block allocation, management, and offloading for fine-grained control over system memory, coupled with an SLO-aware scheduler to optimize overall Service Level Objectives (SLOs). Comprehensive evaluations on representative models, ranging from 7B to 70B parameters, across various GPU configurations, demonstrate that LayerKV improves TTFT latency up to 11x and reduces SLO violation rates by 28.7%, significantly enhancing the user experience", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) has ushered modern applications into a new era, characterized by significant advancements across various domains such as coding assistants [26], conversation [1], and planning [11]. A critical feature of LLMs is their context window, which is rapidly expanding to enable advanced analysis of extensive documents, effective problem-solving within large codebases, and customized content generation based on detailed instructions [10]. Notable examples include Anthropic's Claude-3 [5], Google's Gemini-1.5 [10], and UC Berkeley's Large World Model (LWM) [21], all of which support a context window of up to 1 million tokens.\nThe increasing context length introduces challenges in maintaining smooth live interactions, as the user experience in LLM serving is directly impacted by token generation latency. Specifically, various metrics can be used to measure Service Level Objectives (SLOs). The most critical SLO metrics are Time to First Token (TTFT), which measures the latency from request arrival to the generation of the first token, encompassing both queuing delay\u00b9 and prefill latency; and Time Per Output Token (TPOT\u00b2), defined as the average time between consecutive tokens for the same request. However, as context length increases, TTFT becomes dramatically prone to violating SLO requirements. This is clearly demonstrated by an experiment where the prompt length was increased from 128 to 16k tokens while keeping the output length fixed at 512 tokens."}, {"title": "2 Background and Motivations", "content": ""}, {"title": "2.1 Preliminary", "content": ""}, {"title": "2.1.1 The Process of LLM Inference", "content": "Most of the popular LLMs [28, 37] are built upon the decoder-only transformer architecture [38]. These models consist of stacked transformer layers, each containing an attention mechanism and a feed-forward network (FFN). The attention layers facilitate token interactions within a request, while the FFN processes tokens individually. During each iteration, given the preceding tokens, the model predicts the next token.\nTo prevent redundant computations, LLM inference stores the keys and values of all attention layers from preceding tokens in GPU memory, referred to as the KV cache, which can be frequently reused for subsequent token generation. This optimization splits the generation process into two phases: the prefill phase and the decoding phase.\nIn the prefill phase, all input tokens are processed in parallel to generate the initial output token. The ability to process input tokens concurrently in this phase typically results in high computational demands, except for requests with short prompts. Since the computational complexity of attention mechanisms scales quadratically with input length, while that of FFNs scales linearly, the computation time in the prefill phase generally grows superlinearly with input length. In contrast, the decoding phase only produces the key-value cache for the newly generated output token."}, {"title": "2.1.2 Existing LLM Serving Systems", "content": "The compute utilization in serving LLMs can be improved by batching multiple requests. Because the requests share the same model weights, the overhead of moving weights is amortized across the requests in a batch. For LLMs that have variable-sized input and output, the granularity of batching has a huge impact on system throughput and serving latency. If scheduling is performed at the request granularity, executing a batch of requests with different input prompt lengths requires padding tensors to the maximum length and waiting for the request with the longest output to finish. Iteration-level batching strategy, originally proposed by BatchMaker [9] for non-transformer-based sequence-to-sequence models, performs batching at token granularity. ORCA [45] extends this approach to support the LLM workload: whenever a request finishes an iterative decoding step, the scheduler checks whether it has reached the end of a sequence and can leave the batch, making room for requests to start their computation immediately.\nFor each request, the model performs iterative generation until either the special end-of sentence token (EOS) is emitted or the preconfigured maximum decoding length is reached."}, {"title": "2.2 Motivation", "content": "As depicted in Figure 1, due to its superlinear increase in latency, TTFT increasingly struggles to meet SLO requirements as the context length grows. Notably, this surge is predominantly driven by queuing delay, rather than the more widely discussed prefill latency. We conducted an in-depth analysis of the reasons behind the sharp increase in average queuing delays as the context length extends and visualized in Figure 2.\nFirstly, Figure ?? illustrates the current phase, where the system leverages PagedAttention GPU kernel to handle decoding iterations with non-contiguous stored KV caches. PagedAttention reserves a significant portion of GPU memory for KV blocks, intended to store future KV cache entries. To determine the amount of memory to allocate for KV blocks, the system profiles the available GPU memory during initialization based on the maximum configured input size. During this process, a fixed proportion (e.g., 90%) of the remaining memory-after accounting for model parameters and activations-is reserved for KV blocks. As context window becomes longer, maximum input configurations correspondingly expands, resulting in greater activation memory usage during profiling. Consequently, the GPU memory for KV blocks decreases. This figure deliberately displays a small number of KV blocks to highlight that the capacity of KV blocks can be significantly limited.\nSecondly, Figures. ?? and ?? illustrate scenarios where requests of varying lengths are queued for scheduling. Existing serving systems are stateless across requests. In other words, they de-allocate all the cache slots used by a request as soon as it finishes. Therefore, within the iterative batch processing approach, the system allows new requests to initiate the prefill stage earlier if sufficient KV blocks are available, thus reducing queuing delays. To determine whether a new request can be inserted, the system compares the total KV blocks required for its prefill stage with the currently available KV blocks. As a result, if requests have shorter prompts, they can be scheduled immediately, as indicated by the green segment. However, if requests have longer prompts, they must wait until KV blocks are released, necessitating at least one request to be fully completed first. This process can be time-consuming, which consequently results in subsequent requests remaining queued.\nIn summary, the significant rise in average queuing delays is caused by the increasing allocation requirements for KV cache, which come into conflict with the limited number of GPU KV cache blocks."}, {"title": "3 Desgin", "content": "To mitigate queuing delays caused by limited GPU KV block resources when handling long prompts, our core idea is to refine the granularity of the KV cache to a layer-wise level, rather than retaining the entire KV cache of the prompt in GPU KV blocks. By implementing layer-wise KV block allocation and KV cache offloading, the demand for GPU KV blocks is reduced, which in turn facilitates the scheduling of new requests. This reduction in queuing delays directly contributes to the optimization of the TTFT SLO.\nHowever, concrete design of this idea is nontrivial, as an imprudent approach could result in TPOT SLO violations or a decrease in the number of queries per second (QPS). Specifically, inserting prefill stage during the current decoding stages can optimize the TTFT of queued requests, but this may lead to an increase in TPOT for requests currently being decoded. Therefore, determining how to optimize TTFT while satisfying the TPOT SLO is a key design consideration. Furthermore, the system's QPS is closely tied to the efficient utilization of computing resources. If computation is stalled due to additional PCIe communication, QPS may decrease, negatively impacting overall system throughput. Therefore, managing PCIe communication effectively without compromising computational efficiency is another crucial design consideration.\nBased on the above analysis, we design LayerKV, a simple and lightweight plug-in for existing LLM inference service systems that effectively optimizes TTFT while ensuring compliance with TPOT SLO and maintaining QPS of systems. The overall architecture of LayerKV is illustrated in Fig. 3. The SLO-aware Scheduler directly addresses the first key design consideration, determining whether and how many requests' prefill stages can be scheduled earlier, ensuring the optimization of average TTFT without compromising the TPOT SLO of requests that still need to generate tokens. Moreover, the LayerKV Execution Engine and layer-wise KV cache offloading are key components that handle computation and communication processes, while the KV Cache Manager, Block Allocator, and Cache Engine manage the logical and physical aspects of the layer-wise KV cache. These components address the second design consideration, ensuring that LayerKV's design optimizes TTFT with almost no negative impact on QPS."}, {"title": "3.1 SLO-aware Scheduler", "content": "The primary task of the SLO-aware Scheduler is to determine the maximum number of prefill phases that can be scheduled without violating the TPOT SLO of requests currently in the decoding phase. This is achieved by analyzing both the historical and future states of all decoding requests. For any given request in the decoding phase, the decision-making process considers the historical decoding time and the number of tokens already decoded, as well as the projected number of tokens and time required for future decoding stages.\nFor any given request $i$ in the decoding phase, the historical states include the decoding time already spent $T^{past}_i$ (including time waiting for decoding) and the number of tokens already generated $N^{past}_i$, while the future entails the estimated number of tokens still required $N^{future}_i$ and the expected remaining decoding time $T^{future}_i$. $T^{past}_i$ and $N^{past}_i$ are accessible through direct monitoring, whereas $N^{future}_i$ and $T^{future}_i$ require predictive estimation. Similar to the approach in latest work [31], the prediction of the complete generation length $N_{max}$ can be framed as a multi-class classification problem to ensure prediction accuracy. Specifically, the predicted complete generation length can be divided into multiple percentile ranges, and a model predicts which range the output sequence length corresponding to the request falls into. Under this prediction approach, $N^{future}_i$ is conservatively estimated by subtracting $N^{past}_i$ from the lower bound of the predicted generation length range. Naturally, $N^{future}_i$ is constrained to positive integers. The expected remaining decoding time $T^{future}_i$ is simply estimated using the current TPOT multiplied by $N^{future}_i$.\nAt this point, for any request in the decoding phase with an SLO target that requires TPOT to be less than $T_{tpot}$ seconds, the maximum allowable duration for scheduling the prefill of new requests, $T_{allow\\_prefill}^i$ can be calculated as follows:\n$T_{allow\\_prefill}^i = T_{tpot} \times (N_i^{past} + N_i^{future}) - (T_i^{past} + T_i^{future})$ (1)\nGiven the set of requests currently in the Request Queue ${q_1, q_2, ...} \\in Q$, the prefill stages for requests from $q_1$ to $q_n$ can be scheduled as long as the following condition is met:\n$\\sum_{k=1}^{n} T_{prefill}^{q_k} < min_i T_{allow\\_prefill}^{i}$ (2)\nThe prefill time $T_{prefill}$ for each request $q_k$ can be estimated using the following formula:\n$T_{prefill} = a \\times seqlen \\times \\frac{2 \\times N_{param} + 2 \\times seqlen \\times n_{hidden}}{FLOP \\ per \\ second \\ of \\ device}$ (3)\nwhere $seqlen$ denotes the sequence length of the prompt; $n_{param}$ and $n_{hidden}$ denote the model's total number of parameters and hidden layer size, respectively; $a$ is an empirical correction factor derived from profiling data that adjusts the theoretical estimate to more accurately reflect the observed prefill times under real-world conditions.\nAlgorithm 1 summarizes the process through which the SLO-aware Scheduler makes request scheduling decisions based on TPOT SLOs."}, {"title": "3.1.1 Layer-wise KV Blocks Allocation", "content": "Once the SLO-aware Scheduler determines the scheduling of prefill stages for specific requests, these requests are subsequently by the LayerKV Execution Engine. The processing specifically involves layer-wise KV block allocation and KV cache offloading, enabling the limited GPU KV blocks to support more incoming requests.\nA critical consideration in layer-wise KV block allocation is determining the minimum number of layers that must be retained within the GPU KV blocks to ensure that computation time fully overlaps with offloading communication time, thereby maintaining QPS. Suppose a model consists of $L$ layers. For this model's KV cache, at least $x$ layers are retained on the GPU, while the remaining $L-x$ layers are offloaded to the CPU. The offloading is performed asynchronously during the computation. The prefill time exhibits a super-linear relationship with the sequence length, as shown in Eq. 4, while the offloading time, which scales linearly with sequence length, can be estimated as follows:\n$T_{offload} = \\beta \\times seqlen \\times \\frac{2(L - x) \\times d_{heads} \\times n_{heads} \\times f_{precision}}{PCIe \\ Bandwidth}$ (4)\nwhere $d_{heads}$ is the dimensionality of each attention head, $n_{heads}$ refers to the number of attention heads, and $f_{precision}$ specifies the numerical precision format, $\\beta$ is an empirical correction factor. To fully conceal the PCIe communication overhead, the condition $T_{offload} \\le T_{prefill}$ must be satisfied. Based on this condition, the $x$ can be determined. It can be noted that $x$ is closely linked to the length of the requested prompt, as well as the model's architecture and hardware setup. When the prompt is long, $x$ can be zero, allowing all KV cache layers to be offloaded to the CPU without occupying GPU KV blocks. Conversely, when the prompt is short, $x$ is greater than zero, requiring at least $x$ KV cache layers to remain in GPU memory, as their communication overhead cannot yet be fully overlapped. Note that the minimum number of reserved layers in a GPU KV block does not imply that these KV caches must remain in GPU memory for an extended duration. They also can be offloaded to the CPU, freeing up GPU memory during stages when PCIe are relatively idle. Here, GPU KV blocks can be regarded as a special send buffer.\nCertainly, keeping certain layers of the KV cache in the GPU until they are used offers clear advantages, which can be considered free prefetching. However, if GPU KV resources become insufficient after multiple inference phases, this may block or preempt other requests, adversely affecting system QPS-an outcome we aim to avoid. Therefore, we introduce a strategy to evaluate whether further offloading of these $x$ layers is required based on system resource availability. Concretely, we propose a state transition equation to proactively anticipate the status of GPU KV blocks across several stages. The equation is defined as:\n$Avail(t + 1) = Avail(t) + Released(t) - Allocated(t)$ (5)\nwhere $Avail(t)$ and $Avail(t + 1)$ represent the number of free KV blocks at the beginning of stages $t$ and $t + 1$, respectively. $Released(t)$ and $Allocated(t)$ denote the KV blocks released and allocated at time $t$. First, the initial value of $Avail(t)$ can be based on the current number of available GPU KV blocks. Second, $Released(t)$ is defined as the quantity of GPU KV blocks freed by sequences that have concluded at the current time. To estimate which sequences will finish, the multi-class prediction model discussed in \u00a7 3.1 is utilized, with the median of the predicted sequence length range serving as a rough estimate. Third, the estimation of $Allocated(t)$ considers both the number of sequences at time $t$ and the KV cache scheduled for prefill or decoding. We conservatively assume that each sequence requires one additional KV block, and for decoding, any new request in the running queue will be included in the batch if the available blocks are sufficient. As for the KV blocks required for prefill, are the variables that need to be controlled.\nWhen the available GPU KV blocks fall below the preset threshold, indicating resource insufficiency, the retained $x$ layers of KV cache will be offloaded to the CPU. We prioritize offloading the most recently processed requests, starting with $x/2$ layers. If this proves insufficient, the full offloading will be executed."}, {"title": "3.1.2 Layer-wise KV Cache Management", "content": "In the design of LayerKV, layer-based KV cache management is a critical strategy that optimizes system resource utilization by alternating caching of KV layers between the GPU and CPU. However, to effectively manage the mapping between these KV cache layers and their corresponding devices, an additional table is required to store the mapping information for each KV cache layer and its assigned device.\nSpecifically, we determine the number of layers to retain on the GPU by considering available device memory and the total token count of the request, which guides the execution of LayerKV offloading. The offloaded layers are evenly distributed across the model's layers. For example, in an 8-layer model, if 4 layers of KV cache are kept on the GPU, we retain the 1st, 3rd, 5th, and 7th layers on the GPU, while the 0th, 2nd, 4th, and 6th layers are offloaded to the CPU. This approach allows computation to overlap with transmission overhead, as only the KV cache for the 0th layer must be offloaded before the attention operation in the 2nd layer begins, enabling data transfer during the computation of the 0th and 1st layers. Moreover, since block location information varies between layers, we extend the block table, which records the block ID and storage location for each request. We add layer-wise information to each block, indicating the indices of the layers where the KV cache is retained on the GPU and the indices of the layers stored on the CPU."}, {"title": "3.1.3 Layer-wise KV Cache Offloading on Multi-GPUs", "content": "When the model weights of a LLM exceed the capacity of a single GPU, multiple GPUs are typically deployed using tensor parallelism, where both model weights and KV cache are distributed across GPUs. During the forward pass of each layer, both the computation of Attention and FFN require an all-reduce operation. On GPU nodes equipped with NVLink, this all-reduce operation transfers data via NVLink, which does not interfere with LayerKV swapping between the CPU and GPU. However, on GPU nodes without NVLink, the all-reduce operation transfers data over PCIe, which is also used by LayerKV. This leads to PCIe contention, as the all-reduce operation is on the critical path of end-to-end inference latency and directly impacts system throughput.\nTo mitigate PCIe contention, LayerKV implements a mechanism that checks PCIe usage before initiating swapping. If PCIe is already in use, the swapping operation is delayed for a portion of the all-reduce latency before checking again. This check mechanism ensures that LayerKV swapping is not launched during an ongoing all-reduce operation. Additionally, to further alleviate contention, the swapping data is divided into smaller units, and the check mechanism is applied to each subunit, reducing interference with the ongoing all-reduce operation. Together, these methods significantly reduce PCIe contention."}, {"title": "4 Implementation", "content": "LayerKV is implemented based on the widely adopted LLM inference framework vLLM [18]. To ensure the SLO requirements for TTFT and TPOT are met, the scheduler orchestrates batch requests for both the prefill and decode stages. Prior to each scheduling event, the runtime records the queuing time, progress, and predicted sequence length for each request. This information is then provided to the scheduler to make decisions for each stage.\nFor KV cache memory management, LayerKV allocates a single PyTorch tensor during initialization to store physical KV cache, rather than assigning a separate tensor for each layer. This approach allows flexible logical allocation for partial layers within a request, which is essential for layer-wise KV cache management. To expedite the transfer of KV cache between the CPU and GPU, a dedicated CUDA stream is introduced, enabling computation and data transfer to occur concurrently. Additionally, a CPU thread handles the transfer of KV cache from pinned memory to pageable memory, preventing delays in GPU computation caused by KV transfers and copies.\nIn the prefill stage, the h2d (host-to-device) transfer of KV cache is initiated immediately after KV computation for each layer, overlapping with the computation of the same layer. In contrast, during the decode stage, KV cache is transferred layer-by-layer from host memory to GPU memory."}, {"title": "5 Evaluation", "content": "In this section, we evaluate the performance of LayerKV with state-of-the-art solutions on various LLM models with different real-world workloads. The evaluation shows LayerKV outperforms the current state-of-the-art system in terms of TTFT under the same TPOT SLA requirements."}, {"title": "5.1 Experimental Setup", "content": "Models. We use Llama-2-7B [36], Yi-34B-200K [44], and Llama-3.1-70B as the LLM model in our evaluation. As these models are widely used in academic and industry, and have different model size and the longest request length targeting different application scenarios. In addition, Yi-34B-200K and Llama-3.1-70B supports memory efficient attention technique, grouped-query attention (GQA) [4], which saves KV memory footprint.\nTestbed. We evaluate LayerKV on servers each with eight NVIDIA L20 48GB GPUs, 64 CPU cores, 2048 GB of host memory. The PCIe is used to connect GPUs and CPUs, and each two GPUs share one PCIe connection. We use PyTorch 2.4.0, CUDA 12.2, vLLM 0.5.5 for our evaluation. All experiments are conducted on this server, with the number of GPUs adjusted based on model requirements: 1 GPU for Llama2-7B, 2 GPUs for Yi-34B, and 4 GPUs for Llama3.1-70B. The degree of tensor parallelism is set to 1, 2, and 4, respectively.\nWorkloads. We employed fixed-length inputs to intuitively demonstrate system performance across different context lengths, while also incorporating a popular real-world dataset, ShareGPT [2], to simulate practical service scenarios. The dataset, collected from real conversations with ChatGPT, has been widely utilized in prior research [18, 42, 49]. Due to the limited context window of ChatGPT-3.5, the sequence length in this dataset ranges from 4 to 2.3K tokens.\nBaselines. We compare LayerKV with the following state-of-the-art LLM serving systems: vLLM [18]\u00b3: It is one of the most popular LLM serving systems."}, {"title": "5.2 End-to-End Performance", "content": ""}, {"title": "5.2.1 Performance Comparison Under Varying Context Lengths", "content": "Figure 4 presents a comparison of performance between LayerKV and vLLM across varying context lengths, with the request arrival rate of 1 req/s. The top three line plots indicate that as the context length increases, vLLM's TTFT escalates sharply, while LayerKV experiences a more gradual rise, with the performance gap widening up to an order of magnitude. This outcome aligns with LayerKV's core design principles, as previously discussed.\nIn contrast, the lower three bar charts show that throughput of both systems naturally decrease with increasing context length. At this request arrival rate, the throughput of the two systems is nearly identical."}, {"title": "5.2.2 Performance Comparison Under Varying Degree of Parallelism", "content": "We further investigate the impact of degree of parallelism (DoP) on the Yi-34B-200K model, as illustrated in Figure 5. With increasing DoP, computational capacity scales proportionally, and larger GPU memory alleviates resource contention. Despite these improvements, LayerKV consistently achieves notable TTFT reductions. Additionally, the increased DoP further narrows the marginal throughput gap between LayerKV and vLLM."}, {"title": "5.2.3 Performance Comparison Under Varying Request Arrival Rates", "content": "Figure 6 compares the performance of LayerKV and vLLM across different request arrival rates using the ShareGPT dataset. As the request arrival rate increases, vLLM's TTFT rises sharply, particularly at higher rates, where queuing delays lead to significant latency spikes. In contrast, LayerKV effectively controls TTFT, maintaining relatively low latency even under heavy loads. In this case, LayerKV achieves up to an 11x reduction in TTFT latency.\nFurthermore, under low load conditions, the system's throughput scales proportionally with the increase in request arrival rate. However, once the arrival rate exceeds a certain threshold, the system enters an overburdened state, causing throughput to reach a bottleneck. At this time, due to the need to swap portions of the KV cache from the CPU during the decoding phase, LayerKV's throughput is marginally lower than vLLM's. However, LayerKV mitigates this by maximizing the number of layers retained on the GPU through layer-wise KV block allocation, limiting the throughput gap consistently maintaining it below 3%"}, {"title": "5.2.4 SLO Violation Rate Comparison Under Varying Request Arrival Rates", "content": "We further explore the impact of varying request arrival rates on the SLO violation rate using the Llama2-7b model. Specifically, the TTFT SLO is set to 3000 ms and the TPOT SLO to 200 ms for each request. A violation is recorded if either of these thresholds is exceeded.\nFigure 7 presents the experimental results. It is evident that as the request arrival rate reaches 6 requests per second, VLLM begins to exhibit significant SLO violations due to a sharp increase in TTFT. LayerKV consistently maintains a violation rate 17.7-28.7% lower than vLLM.\nFurthermore, we observe that without the SLO-aware Scheduler, LayerKV encounters increased TPOT, resulting in some SLO violations. For instance, at a request arrival rate of 5.5, its performance is occasionally inferior to vLLM. However, with the integration of the SLO-aware Scheduler, this issue is effectively mitigated."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 KV Cache Optimization", "content": "In this section, we further explore works related to KV cache memory optimization, which can be broadly categorized into algorithm-level optimizations and system-level optimizations.\nAlgorithm-level optimizations aim to reduce KV cache memory requirements. Common techniques include KV quantization [13, 15, 24], window attention [43, 46], KV pruning [8, 35], and activation-shared attention [4, 32]. These methods seek to lower memory usage by compressing the cache or eliminating unnecessary KV entries, but they often come with a certain degree of accuracy loss.\nSystem-level optimizations focus on increasing available memory capacity. A straightforward approach is to utilize multiple GPUs or offload KV caches to CPU memory or even disk when GPU memory is insufficient. Various parallel strategies partition memory demands across token-wise [6, 22, 40], model-wise [25, 48], operator-wise [29, 34], and stage-wise [30, 49] levels, effectively distributing memory pressure. However, these strategies typically require multi-GPU or distributed environments and are not applicable to single-device setups. On the other hand, well-established offloading methods [19, 33] are more suitable for offline scenarios, where the primary focus is on optimizing system throughput rather than SLO metrics. In contrast, LayerKV maintains lossless precision, supports both single- and multi-GPU setups, and simultaneously accounts for SLO requirements."}, {"title": "6.2 LLM Serving Systems", "content": "Table 1 compares LayerKV with state-of-the-art LLM serving systems. vLLM proposes a PagedAttention mechanism to minimize GPU memory fragmentation, boosting batch size and token generation throughput. DistServe [49] divides prefill and decode execution to different GPUs in order to avoid performance interference of the two computation stages. DeepSpeed-FastGen [14] and Sarathi [3] decompose prompts into small chunks and combine them with decode tokens, which improves responsiveness and tail latency.\nCompared to previous approaches, LayerKV manages the KV cache through a more flexible layer-wise method, expanding the memory management space. Layer-wise offloading significantly reduces queuing delays and improves TTFT. The SLO-aware scheduling carefully optimizes both TTFT and TPOT SLO."}, {"title": "7 Conclusion", "content": "To address the significant challenge of increasing Time to First Token (TTFT) in LLM serving under large context lengths, we have developed LayerKV, which introduces layer-wise KV block allocation, management, and offloading for fine-grained control over system memory. LayerKV effectively reduces queuing delays by optimizing GPU KV cache usage without requiring additional hardware or compromising output quality. Our comprehensive evaluations on models ranging from 7B to 70B parameters across single and multiple GPUs demonstrate that LayerKV achieves up to an 11x reduction in TTFT latency and reduces Service Level Objective (SLO) violation rates by 28.7%, significantly enhancing user experience."}, {"title": "8 Future Work", "content": "Looking ahead, there are several directions to further enhance the capabilities of LayerKV. First, we plan to enhance LayerKV by integrating KV cache quantization techniques to further optimize memory efficiency. Quantizing KV caches will enable LayerKV to support larger models and longer context lengths by reducing resource consumption. Moreover, we aim to extend LayerKV by incorporating prefill-decoding disaggregation to further minimize queuing delays and improve system throughput. By decoupling the prefill and decoding stages, we can achieve greater flexibility in resource allocation and more precise adherence to SLO requirements. These features will be released in future versions."}]}