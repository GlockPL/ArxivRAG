{"title": "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "authors": ["Zheng Hui", "Zhaoxiao Guo", "Hang Zhao", "Juanyong Duan", "Congrui Huang"], "abstract": "In different NLP tasks, detecting harmful content is crucial for online environments, especially with the growing influence of social media. However, previous research has two main issues: 1) a lack of data in low-resource settings, and 2) inconsistent definitions and criteria for judging harmful content, requiring classification models to be robust to spurious features and diverse. We propose Toxicraft, a novel framework for synthesizing datasets of harmful information to address these weaknesses. With only a small amount of seed data, our framework can generate a wide variety of synthetic, yet remarkably realistic, examples of toxic information. Experimentation across various datasets showcases a notable enhancement in detection model robustness and adaptability, surpassing or close to the gold labels. We release the generated data at Github upon acceptance.", "sections": [{"title": "1 Introduction", "content": "The 21st-century digital realm presents vast connectivity and information exchange opportunities alongside the challenge of widespread harmful content like cyberbullying, hate speech, and misinformation, impacting individuals and communities negatively. As such, the development of effective mechanisms for detecting and mitigating harmful content is of paramount importance.\nThe emergence of the Transformer-based model led to the development of complex models capable of identifying toxic content with remarkable accuracy. However, the effectiveness of these models hinges on the quality and diversity of the datasets used for their training. Traditional datasets, often curated manually, tend to lack the breadth required to cover the multifaceted nature of harmful content. Consequently, while these models excel at recognizing explicit instances of toxicity, they struggle when faced with subtler forms.\nNumerous datasets containing harmful content are commonly sourced from platforms like Twitter or online forums. Nevertheless, many of these datasets suffer from significant class imbalances regarding specific types of toxic language. Training a compact model tailored for detecting specific harmful information or opting for a smaller model due to resource or latency constraints often necessitates extensive human-labeled data. However, such data are scarce in numerous downstream tasks and costly to annotate. These smaller or low-resource datasets also encounter challenges, such as limited linguistic diversity and the risk of overfitting. Privacy concerns arise regarding the use of social media data obtained without user consent in research experiments. Moreover, there is the issue of dataset decay, particularly with online messages, including abusive ones, which tend to be deleted over time.\nRecent advancements in large language models (LLMs) such as GPT-3 have spurred researchers to delve into their potential for synthetic data. Although LLM-based data augmentation often excels in enhancing model performance, there are mixed findings regarding whether LLM-generated synthetic data can effectively help model training to achieve performance levels comparable to models trained on real-world data with human annotation.\nLLMs have also shown their potential in generating synthetic data for harmful information detection. However, the work utilizing LLMs in this area remains limited and the existing works are relatively outdated. Additionally, the effectiveness of LLMs in this area is constrained due to their struggles with subjective tasks and the amplification of biases inherent in human-annotated training data. These biases arise from the varying standards used for 'gold' labels in annotation, and LLMs often fail to capture the complex nuances and diversity of human communication in tasks with high subjectivity.\nTo address these issues, we introduce the ToxiCraft Framework that enhances the robustness and reduces bias in synthetic data by incorporating a diverse set of training examples and employing advanced bias detection algorithms. The ToxiCraft Framework is specifically designed to produce higher quality synthetic data that better reflect human judgements, making models trained on this data more effective and equitable for real-world applications."}, {"title": "2 Related Work", "content": "Early studies established classifiers to detect harmful information using neural network models or word embedding methods. In recent years, models based on the Transformer architecture have demonstrated remarkable capabilities, prompting researchers to explore further. Rajput et al. (2021) conducted research on the ETHOS hate speech detection dataset, comparing classifiers' performance in hate speech detection by replacing or integrating word embeddings (fastText, GloVe, or FT + GV) with BERT embeddings. Aluru et al. (2020) contrasted simple models (such as LASER embeddings with logistic regression) and BERT models in scenarios with scarce and abundant linguistic resources. Lin et al. (2024) generated explanations through multimodal debates between LLMs, enhancing the transparency and explainability of harmful meme detection. da Silva Oliveira et al. (2024) validated the effectiveness of ChatGPT in identifying harmful Spanish-language speech. Our Toxicraft framework extends these developments by leveraging LLMs not only for direct detection but also for enhancement of the training data pool through synthetic data generation. Unlike methods that rely solely on existing data, Toxicraft enriches the dataset, which is particularly critical in the ever-evolving domain of online content where new forms of harmful expression continuously emerge."}, {"title": "2.1 Harmful Information Detection", "content": ""}, {"title": "2.2 Large Language Models", "content": "Transformer architecture has revolutionized the field of NLP. Large Language Models (LLMs) based on the Transformer architecture are pre-trained on extensive corpora, resulting in models with vast parameter scales and exceptional learning capabilities. The BERT model utilizes the bidirectional encoder from Transformer to process input text, generating rich context-aware word embeddings, with variants including ALBERT, RoBERTa, and DeBERTa. OpenAI have launched the GPT series of large language models based on Transformer's decoder, including GPT-2, GPT-3, Instruct-GPT, and GPT-4. These large language models are garnering increasing research interest, not only exhibiting outstanding performance in a broad range of tasks in the natural language understanding sector-such as sentiment analysis , text classification but also demonstrating remarkable capabilities in natural language generation tasks like summarization, translation and question answering."}, {"title": "2.3 Data Synthesis", "content": "Traditional data synthesis methods have employed techniques ranging from synonym replacement to token-level manipulations, as exemplified by the works of. These methods, while useful, offer limited contextual understanding and diversity. The advent of translation models and masked filling brought improvements in maintaining semantic consistency, yet they still fall short in generating the context-rich synthetic data required for complex tasks such as harmful content detection. In contrast, Toxicraft leverages the latest advancements in LLMs for data synthesis, transcending the limitations of earlier approaches by producing contextually nuanced and varied synthetic instances. Unlike methods that require fine-tuning on annotated data, which incurs substantial human labor costs Toxicraft efficiently synthesizes data without intensive manual effort. In comparison to zero-shot frameworks like ZEROGEN , SuperGen , and PROGEN, which generate datasets from scratch or suffer from low information content and redundancy, Toxicraft's approach is designed to yield high-quality synthetic data that is both diverse and relevant to the seed data, thus ensuring its applicability to real-world tasks. In the few-shot learning domain, while Yu et al. (2023) rely on diverse attribute prompts and PromDA along with MSP utilize soft prompts for data diversity and optimization, Toxicraft differentiates itself by not only enhancing the diversity but also focusing on the generation of synthetic data that closely aligns with the intricacies of harmful content. The integration of attribute prompts within Toxicraft's framework ensures that the synthesized data captures a broad spectrum of harmful content, effectively addressing both the volume and variety required for robust model training."}, {"title": "3 Methodology", "content": "In this section, we commence by defining the problem statement and subsequently introduce the ToxiCraft framework. ToxiCraft is designed as a versatile and effective few-shot learning framework that operates through dataset generation."}, {"title": "3.1 Problem Statement", "content": "In the context of online content moderation, particularly the detection of harmful information, the problem can be modeled as a supervised classification task. Given an input content sequence x, represented as tokens $x = [x_1, x_2,..., x_{|x|}]$, the goal is to predict a binary output y, where $y \\in \\{0,1\\}$ indicates whether the content is harmful (y = 1) or not (y = 0). The dataset D consists of pairs of content sequences and their corresponding labels:\n$D = \\{(x^{(i)},y^{(i)})\\}_1^N$\nwhere N represents the number of examples in the dataset. The major challenge in this domain is the scarcity of labeled examples of harmful content, denoted as Dh:\n$D_h = \\{(x^{(i)},y^{(i)}) \\in D : y^{(i)} = 1\\}$\nDue to the sparse nature of Dh, there is a need for effective methods to augment the dataset, especially enhancing the representation of the minority class, which is harmful content in this scenario.\nTo address the limitations posed by scarce harmful content samples, we propose a data augmentation method - ToxiCraft that involves synthesizing new content instances using LLMs. Toxicraft utilizes LLMs to augment the dataset with synthetic instances, enhancing the representational diversity of harmful content. The data augmentation function, defined by:\n$Aug(x) = \\{x' : x' = \\Phi(LLM, x, \\epsilon), \\epsilon \\sim P(\\epsilon)\\}$\nHere, $\\Phi$ denotes the Toxicraft framework which is described in Section 3.2. And Toxicraft uses selected seed data to guide the synthesis, ensuring that the generated content is relevant and varied. $\\epsilon$ represents a noise vector drawn from a probability distribution P, introducing variations in the synthesized outputs to prevent overfitting to the seed samples. Seed data S for Toxicraft are selected from Dh based on a random sampling process, which aims to capture a diverse array of harmful content types. The selection function S(d, n) picks n samples from d using a probability distribution tailored to emphasize less represented content:\n$S(D_h, n) = \\{(x^{(i)}, y^{(i)}) : x^{(i)} \\in D_h,$\n$i \\in \\text{random indices from } D_h\\}$\nwhere n is the number of seed data chosen to guide the LLM in data generation. Details on the selection process and the impact of seed data size are provided in Section 4. The augmented dataset D' is then:\n$D' = D \\cup \\{(x', y) : x' \\in Aug(x), S(D_h,n) \\in D_h\\}$\nAfter that, we employ a smaller model, e.g., BERT, fine-tuned on both D and D'. The training objective is to minimize the loss function $\\mathcal{L}(\\theta)$ across the augmented dataset:\n$\\mathcal{L}(\\theta) = \\frac{1}{|D'|} \\sum_{(x,y)\\in D'} l(h_\\theta(x), y)$\nwhere l represents a loss function appropriate for binary classification tasks, such as cross-entropy. The evaluation of the Toxicraft framework using the smaller model involves several key metrics:\nEffectiveness: Measured by the accuracy and Macro F1-score of the model on a held-out test set T, not seen during training or augmentation:\n$\\text{Macro F1-Score}(\\theta) = \\frac{1}{K} \\sum_{k=1}^{K} F1_k(\\theta)$\nwhere $F1_k(\\theta) = 2 \\times \\frac{\\text{Precision}_k (\\theta) \\times \\text{Recall}_k(\\theta)}{\\text{Precision}_k(\\theta) + \\text{Recall}_k(\\theta)}$ and K is the number of classes, which in this binary classification context, K = 2."}, {"title": "3.2 Toxicraft Framework", "content": "In the Toxicraft framework, we employed LLMs as agents to analyze seed data containing harmful information.\nAttribute Extraction The LLMs' task is to identify the attributes within this data to determine if the content is harmful. This concept was inspired by the research of Yu et al. (2023); however, unlike their approach, we did not manually filter the attributes generated by GPT. Instead, we retained these attribute tags generated by the model itself and cataloged all specific groups or themes indicating harm in the seed data, collectively termed the 'Harmful Themes Index'. The advantage of automated attribute tagging is its capacity to process and analyze data rapidly and on a large scale, significantly enhancing processing efficiency. To further enhance the accuracy of generated attributes and the robustness of the model, we introduced a new step where LLM provides a confidence score when generating attributes. Based on this confidence score, we decided whether to retain the attribute using a simple random function.\nEven with a high confidence score, an attribute may be randomly discarded by a small chance, which could increase the robustness of the decision-making process, reduce the risk of cumulative errors, and avoid over-reliance on the model's singular judgment. In this process, we considered the possibility of errors in automated attribute tagging but decided not to take model-based corrective measures. Instead, we chose to maintain a simple and efficient system to quickly respond to and update changes in the dataset. Although this approach is simple, it can maintain a certain level of accuracy and consistency without sacrificing efficiency.\nToxiCraft Prompt Next, we added manually-set indicators to this data with Attribute Extraction by GPT (We call this process as \"Toxicraft Prompt\") and input 10% of the data extracted randomly from the seed data pool together with \"Toxicraft Prompt\" to GPT. Manually-set indicators include the intensification or weakening of tone, whether to increase swear words, whether to use irony, country and time. Each value will be randomly selected or masked. Thus, it is possible that none of the manually-set indicators are added, but at most only one answer will be selected from each indicator. For example, the country could be randomly set as the United States and the year as 2023.\nCAE and TSR According to the research by Pavlopoulos et al. (2020) and , high-quality context can help better detect harmful data. Considering the inherent length limitations of the dataset (primarily from a harmful dataset on Twitter before 2017 with a limit of 140 characters (about 20-35 words) and after 2017 with a limit of 280 characters), we innovatively generated context-based preceding and succeeding text. We employed a technique called 'Contextual Anchoring Enhancement', using dropout to randomly abandon the preceding or succeeding text or retain them all to enhance the model's robustness. Furthermore, we also checked the generated data and used GPT to evaluate their quality. Among the result data, we selected the top-performing 10%, applied 'Thematic Style Refinement', transforming the themes recorded in the 'Harmful Themes Index', and added them to the seed data. We repeated these steps until generating M results controlled by hyperparameters, while the number of data generated per batch using 10% of seed data K was also controlled by hyperparameters. The total ToxiCraft framework genrated data are N.\nThrough this carefully designed series of steps, Toxicraft effectively utilizes the Chain of Thought (COT) concept to gradually generate high-quality training data. Additionally, we compared the dataset generated by Toxicraft with the dataset generated only using simple COT prompts, and detailed experimental results will be presented in Section 4."}, {"title": "3.3Robustness:", "content": "Evaluated by the model's performance consistency across variations in the test sets, specifically those generated by different noise vectors $\\epsilon$ within the Toxicraft framework:\n$\\text{Robustness}(\\theta, \\epsilon) = Var_\\epsilon(l(h_\\theta(x_\\epsilon), Y))$\nwhere $x_\\epsilon$ are instances generated with specific perturbations, and Var represents variance, measuring performance variability. We also provide a pseudocode in Appendix A."}, {"title": "4 Experiment Setup", "content": "In this section, we outline the data employed in our experiments. We first introduce the four datasets containing harmful information in Section 4.1.1. Following this, we consolidate these datasets and present statistics on the unified dataset in Section 4.1.2."}, {"title": "4.1 Datasets and processing", "content": ""}, {"title": "4.1.1 Harmful information Datasets", "content": "Automated Hate Speech Detection (AHSD) described by Davidson et al. (2017), comprises 24,802 tweets extracted using a hate speech lexicon. These tweets are manually categorized by CrowdFlower workers into three distinct groups: hate speech, offensive but not hate speech, and neither, with an intercoder agreement of 92%.\nTowards the Political Opponent (HTPO) described by Grimminger and Klinger (2021b) encompasses 3,000 tweets related to the 2020 U.S. Presidential election, annotated for both stance detection (favorable or against candidates) and the presence of hateful language, enabling nuanced analysis of political discourse and sentiment.\nHate Speech and Offensive Content Identification (HASOC) described by Mandla et al. (2021), is part of the Hate Speech and Offensive Content Identification track, consists of 17,657 tweets in Hindi, German, and English, annotated for hate speech and offensive content with three classification levels: presence of hate or offensive content, type of offense (hate, offensive, or profane), and the nature of the insult (targeted or untargeted).\nCall me sexist, but (CMS) described by Samory et al. (2021), consists of 6,325 instances drawn from Twitter and includes a comprehensive annotation for expressions of sexism, where each entry is independently labeled by five coders into categories based on content and phrasing, such as sexist, maybe-sexist, civil, and uncivil.\nSadly, it's important to mention many datasets such as AIRAIT (Waseem and Hovy, 2016), only their tweet IDs and their labels were publicly available. Regrettably, a substantial portion of tweets (>= 40% positive datas) were inaccessible through the X API."}, {"title": "4.1.2 Data processing and Splits", "content": "Our data preprocessing pipeline prioritizes identifying harmful content in English. Thus, we exclude any tweets in languages other than English and verify language using a Roberta-based language identifier. Additionally, to prevent overlap between similar datasets, we remove near-duplicated entries through normalization, ignoring duplicate entries based on their lemmatized forms and removing URLs and mentions. The majority of collected datasets focus on hate speech, and in some cases, offensive speech, which we consider harmful. For datasets with a binary hate classification task or a more detailed classification like CMS, where all \"hate\" subclasses are treated as one, we categorize them as harmful. Datasets focusing on specific types of hate speech, such as sexism, are also considered harmful. Moreover, datasets containing offensive speech are likewise classified as harmful in our case. Finally, in datasets like AHSD and HASOC, where a distinction between hate, offensive, or profound speech exists, we include entries labeled as hate speech or offensive, considering them as harmful. Entries labeled as normal or not-hateful are categorized as not harmful."}, {"title": "4.2 Baselines", "content": "We contrast our method with various baseline approaches, including those that utilize LLM-powered data augmentation techniques.\nAll Gold Labels: This method utilizes the entire gold-labeled training dataset for training the models. It serves as a baseline by providing comprehensive data exposure, which allows the models to learn from a complete range of examples in the dataset.\nSeed Gold Labels with In-context Learning: This approach expands the seed data by generating new data instances from the seed data samples, where samples for in-context learning and target-context selection are randomly picked. It tests the model's ability to generalize from an enhanced but limited dataset, simulating training under resource-constrained conditions.\nToxiCraft: Our proposed method that enhances seed data with synthetically generated data, expanding the training dataset by adding new examples that mirror the characteristics of the seed data. This method not only expands the quantity of available training data but also diversifies the types of training examples. It aims to overcome data scarcity and improve model robustness by broadening the training scenarios."}, {"title": "4.3 Implementation Details", "content": "We employed GPT-4 as our agent model, for generations, in our experiment, we set N to be 1000, K to be 100, and M to be 3. For smaller-scale classification tasks, we utilized two language models of a reasonable size: BERT-base and RoBERTa-base as downstream models. The implementations provided by Hugging Face were utilized for training and evaluating all language models. We fine-tuned the learning rate, warm-up steps, number of epochs, batch size, and other hyperparameters for each model. For BERT, we use learning rate=1.8282 \u00d7 10-5, epoch=3, batch=4, warm-up step=30.And for ROBERTa, we use learning rate=1.1856 \u00d7 10-5, epoch=3, batch=4, warm-up step=30. All fine-tuning experiments were conducted using A40 and GTX 4080TI GPU, and the results are reported in terms of macro-F1 score."}, {"title": "5 Experimental Results", "content": "Main Results We conduct experiments on two different data augmentation scenarios and report the results of training data augmentation in Table 2 and the LLM generated successful rate results in Table 3. As shown in table, using the complete gold-labeled dataset, generally sets a high benchmark, notably on the AHSD and CMS datasets, indicating that comprehensive data access typically results in better model performance. Conversely, \u2018In-context Learning with Seed' shows a performance increase with larger seed sizes, but significantly underperforms with smaller seeds (50, 100), indicating challenges in training models with limited data. Our Toxicarft substantially outperforms or comparable with all baselines across different settings demonstrating the effectiveness of our approach. Notably, our approach, which enhances seed data with synthetically generated labels, demonstrates substantial performance gains, especially with larger seeds (150, 200). These results approach or even surpass those from training with all gold labels, highlighting the potential of synthetic data to effectively supplement training datasets and enhance learning outcomes. The lower performance across all methods in the HTPO dataset, which focuses on political harmful content, suggests that such content poses additional complexities and nuances that are challenging for models to learn. Our framework not only improves as more seed data is available but also illustrates that well-generated synthetic data can serve as a robust tool for augmenting training datasets, thus potentially reducing reliance on extensive manually labeled data. In term of models, ROBERTa generally outperforms BERT, especially in higher data regimes (e.g., \"Toxicraft with Seed Data\" at seed 200 across multiple datasets). This could be attributed to ROBERTa's more robust pre-training, which might be better at handling the nuances introduced by synthetic data augmentation.\nAnalysis on generated success rate The high success rate of 78.6% for the AHSD dataset suggests that GPT-4 excels in generating data that closely matches the characteristics and complexities of this particular dataset. It's important to note that both the success rate and the macroF1 generated from a small amount of seed data for AHSD are high, the possibility of a data breach cannot be ruled out. Conversely, the lower success rate of 52.2% for the HTPO dataset, which focuses on political harmful content, highlights the challenges faced in synthetic data generation when dealing with nuanced and sensitive content. It's possible that the GPT series may have been specifically tuned for political topics, which could explain the difficulties encountered in generating synthetic data for this dataset."}, {"title": "Analysis on Robustness", "content": "Figure 3 reveals Toxicraft's robust ability to generate synthetic data that enhances model generalization across datasets. In the AHSD to HASOC evaluation, we observe that as the seed size increases, the performance of models trained on Toxicraft synthetic data rises significantly, peaking at a Macro F1 score of 67 for the 200 seed size. This surpasses the gold label performance for the AHSD dataset, which stands at a Macro F1 score of 54. Similarly, for the HASOC to HTPO evaluation, the upward trend continues with the performance of the 200 seed size reaching a Macro F1 score of 58, again outperforming the gold label performance, benchmarked at 56. These results not only demonstrate Toxicraft's capacity to produce high-quality synthetic data that captures the complexity of harmful content but also its effectiveness in adapting to different datasets. The ability to exceed gold label performance suggests that Toxicraft could potentially reduce the need for extensive labeled datasets, particularly in specialized or sensitive areas like political content, where data acquisition is challenging."}, {"title": "6 Conclusion", "content": "In this study, we introduced Toxicraft, a framework that effectively enhances data augmentation for harmful content detection tasks in low-resource settings. Toxicraft utilizes Large Language Models to synthetically expand seed datasets, overcoming the diversity and volume limitations of conventional augmentation methods. Our extensive evaluations demonstrate that Toxicraft significantly improves model robustness, outperforming or closed to baselines trained with gold label datasets. This work contributes to the ongoing efforts to develop data-efficient and adaptable models for sensitive content moderation, and sets a foundation for future research on leveraging synthetic data generation in various domains."}, {"title": "7 Future Work", "content": "Future research for Toxicraft will focus on three main objectives: enhancing multilingual capabilities, refining the seed data selection process, and reducing reliance on high-cost LLMs.\nMultilingual Data Generation: Future efforts will explore translating harmful content from English into other languages while preserving both the local nuances and the target language characteristics.\nOptimizing Seed Data Selection: Another area of focus will be to refine the methodology for selecting seed data. We aim to develop strategies to identify the most diverse and representative examples within a dataset. One proposed method is to analyze word embeddings and select seed data points whose vectors are the least similar, ensuring a broad coverage of the data space.\nCost-effective Model Exploration: We will also continue our efforts to identify more cost-effective and transparent alternatives to GPT-4, such as the Mistral model, for generating high-quality synthetic samples of harmful content. These steps will potentially lower the barriers to accessing sophisticated data augmentation techniques and contribute to the advancement of in content moderation."}, {"title": "8 Limitations", "content": "In our investigation of the Toxicraft framework, we have recognized a few limitations that warrant further attention: (1) While our approach, which centers around GPT-4, has shown proficiency in generating synthetic data for harmful content detection, its performance on niche or underrepresented content types is yet to be fully understood. The adaptability of Toxicraft to a wider array of nuanced domains remains an area for exploration.\n(2) The efficacy of Toxicraft relies partly on the availability of initial seed data of high quality. Obtaining such data can pose challenges, particularly in highly specialized or sensitive contexts. Future work will need to address strategies for seed data selection in scenarios where gold standard labels are scarce or non-existent. (3) The utilization of models like GPT-4 brings about concerns related to accessibility and reproducibility. The proprietary nature of such models and potential licensing restrictions may limit widespread adoption and independent verification of the results."}, {"title": "9 Ethical Consideration", "content": "The generation of synthetic data for harmful content detection necessitates careful ethical considerations. It involves handling potentially sensitive or offensive material, and there is a responsibility to ensure that such data does not perpetuate harm or bias. Rigorous validation processes and ethical oversight are essential to prevent the reinforcement of such biases in synthetic data. Collaboration with subject matter experts and ethicists will be critical to navigate these challenges effectively and responsibly in future iterations of Toxicraft."}]}