{"title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?", "authors": ["Yifei Li", "Junbo Niu", "Ziyang Miao", "Chunjiang Ge", "Yuanhang Zhou", "Qihao He", "Xiaoyi Dong", "Haodong Duan", "Shuangrui Ding", "Rui Qian", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Conghui He", "Jiaqi Wang"], "abstract": "Temporal Awareness\u2014the ability to reason dynamically based on the timestamp when a question is raised\u2014is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.", "sections": [{"title": "1. Introduction", "content": "Large Vision Language Models (LVLMs) [27, 35, 47, 60] and Video-LLMs [23, 34, 57] have shown remarkable progress, achieving impressive scores on existing benchmarks [11, 12, 24]. Recent works, such as VideoLLM-online [5] and Flash-VStream [58], have pioneered J.A.R.V.I.S\u00b9-like real-world video assistants by integrating pre-trained vision encoders [40] with LLMs [9, 46]. However, a critical question remains: How far are current state-of-the-art models from achieving human-level online video understanding?\nDespite the existence of dozens of evaluation benchmarks in video understanding, there remains a significant domain gap between these evaluations and real-world video understanding tasks. Early evaluations [19, 53, 55] are largely based on video understanding and retrieval datasets [2, 54], assessing models through coarse-grained QA tasks, such as \"Q: Who is dancing? A: Man\". These QAs predominantly focus on short videos with fixed question types and lack temporal indispensability [11]. Subsequent works [12, 24, 61] attempt to address these limitations by extending video temporal length and incorporating more diverse tasks and video sources. E.T.Bench [30] advances this further by exploring inherent temporal information in videos and evaluating fine-grained temporal event detection capabilities. However, all the aforementioned works are limited to offline settings, where models have access to all video frames when answering queries. While these models exhibit impressive performance on offline video understanding benchmarks, a substantial gap remains between their demonstrated capabilities and the requirements of a real-world assistant or autonomous agent.\nA pioneering benchmark, VStream-QA [58], represents one of the earliest efforts to evaluate streaming understanding, leveraging video sources from Ego4d [15] and MovieNet [17]. Meanwhile, StreamingBench [26], a most recent work, expands the scope by evaluating Video-LLMs on a larger scale in streaming scenarios. However, three primary evaluation categories of StreamingBench primarily target the leverage of existing visual inputs to respond to incoming queries immediately, resulting in an incomplete portrayal of streaming perception.\nIn this work, we propose that effective online video understanding requires simultaneous capabilities to trace back past information, perceive the going-on, and forward active responding simultaneously. Given a query during a streaming video, a Video-LLM must determine whether to respond immediately using past and ongoing information or wait until sufficient evidence has been accumulated. We refer to this as the Video Chain-of-Time thinking process (Figure 3), inspired by the Chain-of-Thought reasoning in LLMs [49].\nWe introduce OVO-Bench (Online-VideO-Benchmark) to evaluate Video-LLMs' online video understanding capabilities. The benchmark comprises 644 videos from diverse sources, including curated datasets and web videos, spanning 7 major domains (Sports, Video Games, Ego Centric, etc.) with durations ranging from minutes to half an hour. Using a hybrid approach combining semi-automated MLLM generation and human curation, we created 2814 high-quality samples (Meta-Annotations) with precise event timestamps. These Meta-Annotations are organized into 12 tasks across three categories: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding, reflecting the human video understanding process illustrated in Fig. 3. Notably, the proposed Forward Active Responding marks the first evaluation that requires models to continuously adapt their responses to ongoing visual input for online video understanding.\nBuilding on the human-reviewed meta-annotations, we develop an evaluation pipeline that queries Video-LLMs densely along temporal axes to simulate continuous information processing. For Backward Tracing and Real-Time Visual Perception, we adopt multiple-choice evaluation, converting videos into segments from start to query time to accommodate offline models. With this approach, we explore the potential of explicitly leveraging state-of-"}, {"title": "2. Related Works", "content": "Video Large Language Models. Video Large Language Models (VLLMs) can process a video by treating it as a sequence of video frames. Projects like VideoChat [23], Video-LLaMA [57], and Video-ChatGPT [34] project the CLIP-ViT [41] embeddings of selected video frames through a Multi-Layer Perceptron (MLP) projector into the LLM embedding space, then concatenate these embeddings with text embeddings for enhanced video understanding. However, the context length of MLLMs limits their effectiveness in understanding long videos [23, 34], as longer videos require more frames and a longer context length. To address this limitation, two major approaches have been developed: compressing video features and selecting critical frames.\nIn the realm of feature compression, Chat-UniVi [21] merges similar visual tokens through clustering techniques. MovieChat [43] and MA-LLM [16] employ a memory bank to store a fixed number of video tokens by iteratively merging the most similar tokens. ST-LLM [29] and MovieChat [43] reduce video tokens to 32 using a pre-trained Q-Former from BLIP2 [10]. LLaMA-VID [25] takes a more radical approach, compressing each frame into a content token and a context token.\nOn the other hand, frame selection methods aim to identify the most representative frames. VideoStreaming [39] utilizes a small LLM to select critical video clips, while FlashVstream [58] employs a clustering method to choose representative frames for high-resolution processing. LongVU [42] leverages question embeddings to select question-related frames, thereby enhancing video understanding.\nBenchmarks for Video Understanding. Traditional video benchmarks, e.g., MSVD-QA [53], MSRVTT-QA [53], and ActivityNet-QA [55], predominantly consist of short videos, typically ranging from 1 to 2 minutes in duration. These datasets are meticulously annotated with corresponding questions and ground truth answers. GPT-4 [35] is employed to assess the accuracy of the answers by comparing them against the provided questions and ground truth responses. However, these benchmarks primarily focus on evaluating short, static video scenes. Hence, new benchmarks designed to test causal and temporal understanding, e.g., NExT-QA [52], TemporalBench [3], and AutoEval-Video [7] are proposed.\nTo gauge the capabilities of models on long-duration videos, benchmarks like EgoSchema [33] covering over 5,000 egocentric videos with an average length of 3 minutes have been introduced. In contrast, Video-MME [12], LVBench [48], and LongVideoBench [51] feature videos spanning from 20 minutes to over an hour, evaluating a broad spectrum of video understanding capabilities. HourVideo [4] stands out with egocentric videos extending up to 2 hours, accompanied by more than 12,976 multiple-choice questions. Unlike these offline video benchmarks, our proposed OVO-Bench is designed to evaluate online, interactive video understanding.\nOnline Video Understanding. Traditional offline video understanding methodologies primarily focus on accessing entire video sequences to facilitate prediction tasks. Conversely, online video understanding demands models to process video streams sequentially, making decisions based on current and past information. This approach is particularly well-suited for scenarios where future data is unavailable, such as in embodied intelligence, autonomous driving, and augmented reality applications. Among online video understanding methods, FlashVStream [58] employs a clustering method to select representative frames, enabling MLLMs for real-time interactions. LIVE [5] introduces a comprehensive framework for learning in video streams, which includes a training objective, data generation schema, and an inference pipeline tailored for online video understanding."}, {"title": "3. OVO-Bench", "content": "In this section, we present the construction process of our OVO-Bench. We start with a detailed introduction to the three different modes of online video understanding, followed by a comprehensive description of the data collection and annotation procedures. A statistical report of our proposed benchmark is displayed at the end of this section."}, {"title": "3.1. Online Video Understanding Mode Taxonomy", "content": "Online video understanding aims to equip real-world, always-on agents with the ability to receive and process video inputs continuously, which closely mimics the human visual perception process. We categorize online video understanding into three distinct problem-solving modes: (1) Backward Tracing, (2) Real-Time Visual Perception, and (3) Forward Active Responding. Given a user-provided text query Qto at the current time to and a streaming video input $X_{(-\\infty,+\\infty)}$, these modes are formally defined as follows:"}, {"title": "1. Backward Tracing:", "content": "$R_{to} = P(Q_{to}, X_{(-\\infty,-T]})$"}, {"title": "2. Real-Time Visual Perception:", "content": "$R_{to} = P(Q_{to}, X_{(-T,to]})$"}, {"title": "3. Forward Active Responding:", "content": "$R_{(to,+\\infty]} = P(Q_{to}, X_{(to,+\\infty)})$\nin which T represents a threshold that defines the boundary for recent times, and R denotes the model's response. The first two modes, Backward Tracing and Real-Time Visual Perception, involve collecting visual information from past and current timeframes respectively, and are expected to give immediate responses. In contrast, Forward Active Responding requires the model to withhold a response until sufficient future information becomes available to ensure a confident answer. Based on these distinctions, we have meticulously designed tasks tailored to each mode to effectively evaluate the performance of Video-LLMs across these diverse capabilities."}, {"title": "3.1.1. Backward Tracing", "content": "Memory, particularly long-term memory, is a crucial aspect of human intelligence. In video understanding systems, this capability involves recalling and reasoning about past events. We focus on the following three tasks to evaluate this capability:\n1. [EPM] Episodic Memory: Backtrack and retrieve key moments from past video inputs.\n2. [ASI] Action Sequence Identification: Identify the correct sequence of human actions in the video streams.\n3. [HLD] Hallucination Detection: Ask questions irrelevant to existing video inputs."}, {"title": "3.1.2. Real-Time Visual Perception", "content": "Accurate real-time perception of visual content is crucial, as actions undertaken in the present shape future outcomes. In various real-world scenarios, an immediate and precise understanding of ongoing visual inputs is essential. We propose six critical categories that constitute the foundational capabilities for effective real-time visual perception:\n1. [STU] Spatial Understanding. Reason over the spatial relationships between objects occurring in nearby frames.\n2. [OJR] Object Recognition. Recognize the objects appearing in the current frames.\n3. [ATR] Attribute Recognition. Identify the characteristics or properties of objects, such as color, texture, and size that appear in nearby frames.\n4. [ACR] Action Recognition. Recognize and interpret the actions being performed by individuals in the current frame.\n5. [OCR] Optical Character Recognition. Recognize and interpret characters that appear within the frame.\n6. [FPD] Future Prediction. Forecast the most probable subsequent phase of the current scene, including changes in object states, actions, and other dynamic elements."}, {"title": "3.1.3. Forward Active Responding", "content": "Transitioning from passive reception to active perception is essential for advanced video understanding systems. Existing benchmarks primarily focus on the aforementioned two understanding modes, where Video-LLMs are required to respond immediately based on available information. In contrast, we introduce the Forward Active Responding mode, which allows the model to adjust its responses based on forthcoming visual inputs. We devise four task dimensions to evaluate the models' active responding abilities:\n1. [REC] Repetition Event Count. Respond when a repetitive event occurs again, including both high-frequency repetitive actions over short durations and semantically long-term repetitive occurrences of certain events.\n2. [SSR] Sequential Steps Recognition. Respond when a certain procedure or sequence of actions has transitioned to another stage.\n3. [CRR] Clues Reveal Responding. Delay responding until sufficient information or clues are provided."}, {"title": "3.2. Benchmark Construction", "content": "Under the taxonomy guidelines above, we make our first step by collecting video data and annotations from existing datasets and crawling data from the web to increase diversity. As our proposed evaluation pipeline highly relies on the accurate timestamp annotations of the referred events in the constructed prompt, the scarcity of event-level timestamps in existing datasets [50][32][38] promotes the design of our highly efficient meta-data generation pipeline 3. Raw annotations with coarse timestamps are then refined by humans to ensure accuracy. Our final questions and options for evaluation are constructed using our rule-based pipeline based on these human-refined meta-annotations. All QA samples undergo manual inspection before being included in the final test set."}, {"title": "3.2.1. Video and Annotation Collection", "content": "Video Source Selection. We follow existing benchmarks [30][24] by exploiting high-quality customized video datasets, and enrich our diversity by utilizing self-crawling videos from different domains. (1) Human-annotated Video Dataset. Our main consideration for utilizing organized datasets is to alleviate the labor-intensive source video collection process. Specifically, we include QA-Ego4D[1] and OpenEQA[31] for the [EPM] task, STAR[50], YouCook2[62], CrossTask [64], HiREST[56], and COIN[44] for the [ASI] task, Perception-Test[38] and Thumos [20] [13] for the [REC] task, COIN[44] for the [SSR] task, MovieNet[17] for the [CRR] task, and Ego4D[15] for tasks under Real-Time Visual Perception. All samples are selected from val or test sets to avoid potential data leakage. (2) Web-crawling Videos. To further extend the diversity of our benchmark, we follow the existing practice [11][26] of crawling source videos from YouTube.\nMeta-Annotations Collection. We employ three approaches to collect our meta-annotations which contain event-level timestamps: (1) Existing Annotation Repurposing. For human-annotated datasets with accurate event-level timestamps [1][44] [15], we explicitly take advantage of these labels and reconstruct them to our final prompt. (3) Semi-Automatic Generation. For datasets that provide video-level QA pairs without complete temporal localization, including [32][50][38][20][13], we prompt temporal-sensitive Video-LLMs like Gemini-1.5[45] to provide coarse-grain timestamps which fit the event referred in question and answer. For tasks under the Real-Time Visual Perception scenario, timestamps are given during our automatic QA construction process, which will be illustrated in 3.2.2. We then perform meticulously inspect all collected source videos and the corresponding meta-annotations to ensure precision."}, {"title": "3.2.2. Prompt Generation", "content": "Question and Answer Generation. Besides carefully selecting QA pairs from existing datasets to fit into our proposed tasks, we also adopt a highly efficient automatic question and answer generation pipeline, particularly for the Real-Time Visual Perception scenario. We randomly sample short clips from original long-form videos and then leverage GPT-40[18] to select potential candidates and construct questions and corresponding answers using human-refined prompts. Human-proposed questions are also adopted as a part of these tasks to alleviate possible LLM preferences. For the novel [CRR] task, even the strongest Video-LLMs/MLLMs like Gemini-1.5-Pro struggle to construct desired problems. Volunteers are then recruited to provide QA pairs under our guidance.\nOptions Generation and Selection. We adopt multiple-choice questions as testing forms for Backward Tracing and Real-Time Visual Perception scenarios. However, as revealed in [6], the naively designed options of a multi-choice form query can cause information leakage about answers. We propose to generate options using a carefully designed rule-based and visually grounded transformation of correct answers, bringing misleading information from original videos to increase difficulty. Specifically, we prompt Video-LLMs with original QA pairs and corresponding video clips to generate visual-related options. A careful human review is then conducted to further ensure the options' effectiveness. All options are shuffled after human review to avoid potential preference bias.\nPrompting Offline-Models for Simulated Online Understanding. With the significant performance gap between main-streaming powerful offline Video-LLMs [45][36][47] and existing online models [5] [58], one natural question is made: Is it effective to prompt offline models directly for online video understanding? For the Real-Time Visual Perception setting, we make human curation to the original question to include implies about the real-time query scenarios, for example, by using sentence patterns like What is/What am I or containing words like Now/Currently. We made another intuitive attempt to prompt offline models to solve tasks under our novel Forward Active Responding scenario, which asks for a continuous adapting capability. Specifically, we devise a multiple-triggering densely query and evaluation pipeline, allowing the model to decide whether"}, {"title": "4. Experiments", "content": "This section presents comprehensive experiments and in-depth analyses of OVO-Bench."}, {"title": "4.1. Models and Evaluation Strategies", "content": "We evaluate four existing types of models: (1) Offline Multimodal Models, including GPT-40 [37], Gemini-1.5-Pro [45], Qwen2-VL [47], LLaVA-NeXT-Video [28], LLaVA-OneVision [27], InternVL-V2 [8] and LongVU [42], (2) Online Multimodal Models, including Flash-VStream-7B [58] and Videollm-Online[5] (3) Blind LLMs, including GPT-4-turbo [35]. (4) Human Agents. To ensure a fair comparison of model performance, we adhere to the principle of consistency by maintaining the same number of frames or frames per second (fps) across all models.\nConsidering the limitations on input video length for existing offline Video-LLMs, we adopt specialized video input methods tailored to such models. Specifically, we segment the video into clips based on the timestamps of the questions.For instance, for a question Qi posed at timestamp ti, we extract the video clip Video[0: ti] as the visual input. This approach simulates a streaming question-answering scenario in online video understanding."}, {"title": "4.2. Main Results", "content": "Table 1 reports the performance of eleven models under different settings on OVO-Bench, including the Real-Time Visual Perception, Backward Tracing, and Forward Active Responding. Our evaluation brings several important findings, as follows:\nOffline Video-LLMs' video understanding capabilities can be effectively transferred to real-time video understanding. The results demonstrate that offline Video-LLMs, despite being designed for offline processing, perform competitively in Real-Time Visual Perception tasks. This suggests that the advanced video comprehension abilities developed in offline settings are transferable and can enhance performance in certain online scenarios, thereby partially bridging the gap between offline and online video understanding.\nCurrent Video-LLMs lack temporal prioritization when handling VQA tasks. Existing Video-LLMs do not prioritize real-time temporal information when answering questions, leading to an inability to accurately locate the correct scene when multiple misleading scenes matching the question appear in the video stream, as shown in Fig2. Even the best current proprietary models achieve only 54.49% and 66.97% on [STU] and [ACR] tasks, respectively, which represents a significant gap compared to Human Agents.\nA powerful LLM backbone is the key to achieving high-performance video understanding. As shown in Table 1, the Qwen2-VL-72B model significantly outperforms its smaller counterpart, Qwen2-VL-7B, across all evaluated metrics. The larger model's superior architecture allows it to excel in Real-Time Visual Perception with an average score of 65.81%, compared to 60.65% for the 7B variant. Additionally, Qwen2-VL-72B demonstrates enhanced capability in Backward Tracing tasks, achieving a score of 62.87%, notably higher than the 48.58% scored by Qwen2-VL-7B. These results highlight the importance of a robust and powerful LLM backbone in effectively processing and understanding complex video data, underscoring the need for more powerful architectures to achieve high performance in video understanding tasks.\nHallucinations are prevalent in Video-LLMs. The [HLD] in Table 1 measures hallucinations in Video-LLMs [59], indicating that hallucinations are a significant issue, particularly in open-source and online models. Proprietary models like Gemini 1.5 Pro perform better in managing hallucinations, yet there remains a notable gap compared to human performance(52.69% vs. 91.37%). This problem arises due to the models' inability to fully comprehend complex visual and temporal contexts, leading to errors in interpretation and response. Addressing hallucinations is crucial for improving the reliability and accuracy of Video-LLMs in real-world applications."}, {"title": "4.3. Comparison between online Video-LLMs and offline Video-LLMs", "content": "Models like Gemini 1.5 Pro and Qwen2-VL-72B, representative of offline Video-LLMs, demonstrate strong performance across various tasks, as shown in Fig5. Specifically, Gemini 1.5 Pro achieves the highest average score among these models. This superior performance suggests that offline models, despite not being designed for online or real-time processing, can effectively comprehend and process complex visual information when provided with sufficient computational resources and pre-processing time. Their architectures typically allow for processing the entire video sequence holistically, leveraging global context and detailed temporal information, which enhances their temporal understanding and reasoning capabilities.\nIn contrast, Flash-VStream-7B, representing online Video-LLMs, shows comparatively lower performance in real-time perception tasks compared to offline models. This model is designed to process video in a streaming manner, handling inputs frame by frame with strict latency constraints to achieve real-time responsiveness. The performance gap highlights a potential trade-off between real-time processing capabilities and the depth of visual understanding."}, {"title": "4.4. Forward Active Responding", "content": "We include our evaluation pipeline design for our proposed Forward Active Responding. While our high-quality human-annotated queries and clues lay an ideal testbed for future real-world online understanding models, existing naively designed online video models usually collapse in our evaluation process. We made our initial attempts to leverage our multiple-triggering query pipeline to prompt offline VideoLLMs to perform online video understanding thinking schema and further explore their potential in always-on visual perception.\nEvaluation Pipeline and Metrics. As illustrated in Fig.6, We propose to query the Video-LLMs densely along the temporal axes, particularly around the interested events. Our main concerns are twofold: 1) Encourage models' timely finding of the right clues, and 2) Avoid any possible hallucination before the right clue appears. For the [REC] task, larger counting numbers are awarded. Based on this, we proposed our designed scoring metrics for the three tasks in the Forward Active Responding.\nOffline Models for Online Video Understanding. Despite their promising performance on the Backward-Tracing and Real-Time Visual Perception, in which the models are given full information for making confident responses, our preliminary results show that even state-of-the-art offline models like Gemini-1.5-Pro, fails to capture the linguistic information of ongoing querying, showing limited understanding of online video content."}, {"title": "5. Conclusion and Future Work", "content": "In this work, we introduced OVO-Bench, a comprehensive benchmark designed to assess online video understanding capabilities of Video-LLMs across three critical modes: Backward Tracing, Real-Time Visual Perception, and Forward Active Responding.We anticipate that OVO-Bench will serve as a valuable resource for the research community, guiding the development of Video-LLMs toward practical, real-world applications. By highlighting current limitations and providing a platform for rigorous evaluation, we hope to inspire future research dedicated to advancing online video understanding and achieving human-level comprehension in artificial intelligence systems."}]}