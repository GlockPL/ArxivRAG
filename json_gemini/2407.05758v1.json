{"title": "Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports", "authors": ["Yutong Zhang", "Yi Pan", "Tianyang Zhong", "Peixin Dong", "Kangni Xie", "Yuxiao Liu", "Hanqi Jiang", "Zhengliang Liu", "Shijie Zhao", "Tuo Zhang", "Xi Jiang", "Dinggang Shen", "Tianming Liu", "Xin Zhang"], "abstract": "Medical images and radiology reports are essential for physicians to diagnose medical con- ditions, emphasizing the need of quantitative analysis for clinical decision-making. However, the vast diversity and cross-source heterogeneity inherent in these data have posed significant challenges to the generalizability of current data-mining methods. Recently, multimodal large language models (MLLMs) have revolutionized numerous domains, significantly impacting the medical field. Notably, Gemini-Vision-series (Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in Artificial General Intelligence (AGI) for computer vision, show- casing their potential in the biomedical domain. In this study, we evaluated the performance of the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation across 14 medical imaging datasets, including 5 medical imaging categories (dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3 radiology report datasets. The investigated tasks encom- pass disease classification, lesion segmentation, anatomical localization, disease diagnosis, report generation, and lesion detection. Our experimental results demonstrated that Gemini-series models excelled in report generation and lesion detection but faces challenges in disease clas- sification and anatomical localization. Conversely, GPT-series models exhibited proficiency in lesion segmentation and anatomical localization but encountered difficulties in disease diagnosis and lesion detection. Additionally, both the Gemini series and GPT series contain models that have demonstrated commendable generation efficiency. While both models hold promise in reducing physician workload, alleviating pressure on limited healthcare resources, and fostering collaboration between clinical practitioners and artificial intelligence technologies, substantial enhancements and comprehensive validations remain imperative before clinical deployment.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in natural language processing (NLP) have shifted from monomodal (i.e., text-only) to multimodal large language models (MLLMs), marking a significant paradigm shift in artificial general intelligence (AGI) research. To begin with text, language has always been a distinguishing feature of human intelligence from animals'. With the advancement of artificial intelligence (AI), particularly in the field of NLP, machines are becoming increasingly capable of understanding and processing language. Over the past few years, pre-trained language models (PLMs) based on self-attention mechanisms and the Transformer framework [93] have emerged and rapidly gained popularity. PLMs can learn general language representations from large-scale data in an unsupervised manner, which facilitates various downstream NLP tasks without the need for retraining new models [27]. It is noteworthy that when the scale of training data and parameters exceeds a certain threshold, language models exhibit significant performance improvements and acquire capabilities absent in smaller models, such as context learning. We refer to such models as large language models (LLMs). Large language models (LLMs) like GPT-3 [8] and its derivatives (e.g., InstructGPT [64], also known as ChatGPT), Llama series (i.e., Llama [88], Llama 2 [89], and Llama 3 [2]), and PaLM series (i.e., PaLM [16]) and PaLM 2 [5]), have laid the groundwork by demonstrating exceptional text interpretation and generation capabilities [120].\nThe advent of the era of LLMs has brought us closer to the dawn of AGI [25, 48]. Academia and industry are experiencing vibrant competition and diverse developments, from the early Google T5 model [71] to the highly acclaimed OpenAI's GPT series today. The parameter scales of these models have long surpassed the billion-level mark, and their generative and learning capabilities of LLMs are revealing emergent abilities [100] and increasing being applied across various sectors. LLMs have demonstrated exceptional proficiency in understanding and generating natural language, providing foundational solutions for specific domains such as law [15, 19, 30], education [29, 60, 81], and public healthcare [52, 56, 115]. However, each domain deals with unique challenges, and directly using pre-trained LLMs may not yield ideal results. Fine-tuning these models, considering their inherent complexity, enables them to better adapt to downstream tasks, which is a key approach to leveraging large models [107, 119, 122].\nDespite their exceptional proficiency in zero/few-shot reasoning in most NLP tasks, LLMs face challenges in processing visual information as they can only understand discrete text. Meanwhile, large-scale visual foundation models have made significant advancements in perception, leading to the gradual integration of monomodal LLMs and visual models, ultimately giving rise to the emergence of MLLMs [95]. MLLMs are models based on LLMs that can receive and reason with multimodal information, extending beyond the traditional single \"language modality\" to include \"image,\" \"speech,\" and other \"multimodal\" data. Among these, Gemini [4] and GPT-4 [1] are notable examples. Gemini combines language and visual information processing, while GPT-4 enhances the understanding and generation of visual data, garnering widespread attention. From the perspective of developing AGI, MLLMs may represent a step forward compared to LLMs, as they align more closely with human ways of perceiving the world, are more user-friendly, and generally support a broader range of tasks [109].\nHowever, the exploration of these models' abilities to integrate and interpret visual data, particularly in highly specialized domains such as biomedicine, signifies a new frontier in AI application. Notable among these advancements are the GPT-4-series and Gemini-Vision-series models (namely GPT-4 and Gemini for the rest of this article), which epitomize the fusion of linguistic and visual information processing."}, {"title": "2 Related work", "content": "2.1 Large Language Models\nWith the increasing GPU computing ability and training data size, a series of Transformer [93] based pre-trained LLMs have emerged. Pre-trained LLMs can be grouped into encoder based [21, 37, 39, 49], decoder based [9, 22, 88, 117], and encoder-decoder based models [18, 39, 72, 85]. Encoder based LLMs are better at analyzing and classifying text content, including semantic feature extraction and named entity recognition. The first encoder based pre-trained LLM is the Bidirectional Encoder Representations from Transformers (BERT) [21]. BERT uses bidirectional language encoders with specially designed context-aware and mask prediction tasks on large-scale unlabeled text data. Following BERT [21], roBERT [49] further improves the performance by updating its training method, such as expanding the batch size, training on larger data, and eliminating BERT's next-sentence"}, {"title": "2.2 Multimodal Large Language Models", "content": "In recent years, LLMs have achieved significant progress [120]. By scaling up model and data sizes, LLMs have demonstrated extraordinary emergent abilities, such as instruction following [66], in-context learning [9], and chain-of-thought reasoning [101]. Concurrently, Large Vision Models (LVMs) have also made substantial advancements [36, 62, 79, 113]. MLLMs, as a natural extension, leverage the complementary strengths of LLMs and LVMs [109].\nMLLMs typically consist of a modality encoder, a pre-trained LLM, and a modality interface that bridges the gap between different modalities [109]. The modality encoder processes inputs from various modalities, such as images, videos, and audio, transforming them into representations that"}, {"title": "2.3 Medical MLLMS", "content": "Quality healthcare services are the cornerstone of social welfare. With increasing demand for high-quality healthcare services, the scarcity of medical resources has become a pressing issue that underscores the importance of intelligent healthcare. The creation of foundation models has garnered significant attention in the medical AI system development realm [55, 104, 116]. Compared with monomodal medical models, including medical LLMs [55] and LVMs [80], medical MLLMs that fuse various modalities have the ability to adaptively interpret and address various medical problems in different modalities, showcasing extensive applications and immense potential in the healthcare domain. By integrating language, images, audio, and other modalities of information, these models, covering medical images, electronic medical records, findings, etc. medical modalities, can offer more comprehensive and precise diagnostic, therapeutic, and patient management solutions [124]. As large models continue to evolve, notable medical MLLMs like Med-Gemini [76] have emerged.\nThis multimodality processing ability makes medical MLLMs more practical in clinical stages as they can give more explainable diagnosis results based on different modalities, and allow more flexible interactions for both patients and doctors. Also they can mine richer medical domain knowledge from these different acquired modalities. Technically speaking, medical MLLMs can be divided into two kinds of models, which are multimodality-alignment models and multimodality-generation models."}, {"title": "2.4 Fine-tuning Methods in MLLMs", "content": "MLLMs integrate and process multiple forms of data, such as text and images, to perform complex tasks. Fine-tuning these models is crucial as it allows them to adapt to specific applications, enhancing their accuracy and efficiency. With the exponential growth in model parameters from millions to billions-fine-tuning has become essential to leverage the full potential of pre-trained models for various downstream tasks.\nFine-tuning enables models to refine their understanding and improve performance in specific domains without requiring extensive retraining. This process is especially important in applications like visual question answering, image captioning, and multimodal translation, where precise alignment between different data modalities is required. Recent advancements in fine-tuning techniques have focused on making this process more efficient and scalable, ensuring that even large models can be fine-tuned with limited computational resources.\nThe importance of fine-tuning in MLLMs is underscored by the need to address issues such as catastrophic forgetting, where models lose their ability to retain previously learned information when adapting to new tasks. Additionally, fine-tuning helps in achieving better cross-modal alignment, where the integration of visual and textual data leads to a more coherent and accurate understanding of the inputs.\nParameter-Efficient Fine-Tuning Techniques Fine-tuning MLLMs can be challenging due to their large parameter size. Techniques such as Low-Rank Adaptation (LoRA) and Quantized LoRA (QLORA) have been found effective. These methods adjust a subset of the model's parameters, reducing computational requirements while maintaining performance. For instance, Lu et al. [54] scaled up models like LLaVA to 33B and 65B parameters, showing that parameter-efficient methods could achieve results comparable to full-model fine-tuning, especially when combined with high- resolution images and mixed multimodal data.\nAdditionally, Want et al. [97] proposed a non-intrusive techniques AdaLink, leaving the internal architecture unchanged and adapting model-external parameters. This method has been effective in both text-only and multimodal tasks, providing a competitive edge without the complexities of altering internal architectures.\nAddressing Catastrophic Forgetting One significant challenge in fine-tuning MLLMs is catas- trophic forgetting, where the model loses knowledge of previously learned tasks. Zhai et al. [112] proposed a technique, namely EMT (Evaluating Multimodality), to help in mitigating this by"}, {"title": "2.5 Large Language Model Reasoning", "content": "Large language models have achieved remarkable success in a wide range of natural language processing tasks [31, 55, 58, 87, 103, 104, 116, 121], including language translation, sentiment analysis, and text classification. However, these models are typically designed to perform specific tasks, rather than engage in more general reasoning and inference. In contrast, human language understanding involves the ability to reason about complex relationships between entities, events, and concepts.\nOne of the key breakthroughs in large language models and reasoning is the development of cognitive architectures. These architectures are designed to mimic the human brain's ability to process and integrate information from multiple sources, enabling models to reason and draw conclusions in a more human-like way. For example, researchers [86] at Google have developed a cognitive architecture"}, {"title": "2.6 Evaluation of MLLMs", "content": "Evaluating MLLMs, is a critical aspect of understanding their capabilities and limitations. The evalu- ation process encompasses a range of benchmarks, metrics, and frameworks designed to assess various aspects of these models. However, the complexity and diversity of tasks that MLLMs can perform pose significant challenges to developing comprehensive and effective evaluation methodologies.\nIn terms of current methods for evaluating MLLMs, one of the primary benchmarks for evaluating MLLMs is the General Language Understanding Evaluation (GLUE) benchmark, which includes a suite of tasks such as sentiment analysis, textual entailment, and question answering. The SuperGLUE benchmark extends this by including more challenging tasks. For multimodal models, benchmarks such as Visual Question Answering (VQA) and the COCO dataset, which assesses image captioning, are commonly used. These benchmarks provide a standardized way to compare model performance across different tasks and modalities [10].\nThe evaluation of MLLMs employs various metrics to measure performance. Common metrics include accuracy, F1 score, precision, and recall for classification tasks. For generation tasks, metrics such as BLEU, ROUGE, and METEOR are used to assess the quality of text generation. In multimodal tasks, metrics like Mean Reciprocal Rank (MRR) and Intersection over Union (IoU) are used to evaluate model performance on tasks like image captioning and object detection. These metrics help quantify the performance of MLLMs on specific tasks, providing a basis for comparison [120].\nSeveral frameworks have been developed to facilitate the evaluation of MLLMs. The Hugging Face Transformers library, for instance, includes tools for benchmarking models on a variety of tasks using pre-defined datasets. Another notable framework is EVAL, which focuses on the automatic evaluation of language models' capabilities in following natural language instructions. These frameworks streamline the evaluation process and ensure consistency in how different models"}, {"title": "3 Methodology", "content": "3.1 Datasets\n3.1.1 Medical imaging tasks\nIn this study, we employ a diverse array of eleven distinct medical image datasets to facilitate our investigation. To provide a comprehensive comparison of multimodal large language models, we meticulously selected medical image datasets spanning five different fields. The datasets identified and incorporated in our study include iChallenge GON, MICCAI2023 Tooth Segmentation 2D, ChestXRay2017 [35], COVID-QU Ex Dataset [17, 20, 73, 84], CholecSeg8k [91], CVC ClinicDB [7], Kvasir SEG [32], m2caiSeg [57], Skin Cancer ISIC [75], Skin Cancer MNIST: HAM10000 [90], and Skin Cancer Malignant vs. Benign.\nDataset Summary:\n1. iChallenge GON comprises a total of 1200 color fundus photographs, all stored in JPEG format. Within this dataset, 400 images are designated for glaucoma classification, while the remaining 800 images are allocated for tasks such as optic disc detection and segmentation, along with central fovea localization. In our testing task, we utilized this dataset to compre- hensively investigate the capacity of large language models in glaucoma diagnosis and optic disc localization, achieved through the design of tailored prompts.\n2. MICCAI2023 Tooth Segmentation 2D sourced from the MICCAI2023 Challenge, com- prises 3000 labeled panoramic images of teeth. Its primary objective is to facilitate researchers in accurately segmenting tooth regions utilizing deep learning methodologies. In our testing scenario, we imposed heightened requirements on the large language model. Specifically, we"}, {"title": "3.1.2 Medical report generation task", "content": "In this investigation, we utilized three distinct datasets of radiology chest X-ray reports: MI\u041c\u0406\u0421- CXR (publicly available) [33], OpenI (publicly accessible) [53], and SXY (privately obtained) [123]. Our analysis focused on the inspection findings and conclusion segments across these datasets. By formulating tailored prompts, we assessed the efficacy of a large language model in generating radiological text reports, with a particular emphasis on crafting conclusions derived from the examination findings.\nDataset Summary:\n1. \u041c\u0406\u041c\u0406\u0421-\u0421XR represents a substantial publicly accessible repository comprising chest radio- graphs in DICOM format along with accompanying free-text radiology reports. This extensive dataset encompasses 377,110 images corresponding to 227,835 radiographic studies conducted at the esteemed Beth Israel Deaconess Medical Center located in Boston, Massachusetts.\n2. OpenI provided by the National Center for Biotechnology Information (NCBI) at the National Institutes of Health (NIH), constitutes a publicly available repository of medical images sourced from scholarly literature. Predominantly comprising X-rays, CT scans, and MRI images, the dataset is accompanied by relevant metadata. Designed to facilitate advancements in medical image analysis and information retrieval, OpenI serves as a resource for researchers to train and evaluate various algorithms and techniques in the field.\n3. SXY courtesy of Xiangya Second Hospital, affiliated with Central South University, encom- passes radiology reports spanning from 2012 to 2023 across five systems. This comprehensive dataset includes essential information, detailed descriptions, and diagnostic impressions. These data serve as the foundation for model development and internal validation processes. Specifi- cally, we leverage the chest X-ray reports for testing purposes within our study framework."}, {"title": "3.2 Model Selection", "content": "In this research, we mainly target on evaluating the multimodal performance of Gemini and GPT families in the biomedical region. To provide a comprehensive comparison within these families, we either consider the generation or the usage-specific type. For the Gemini family, we select Gemini-1.0- Pro-Vision, Gemini-1.5-Pro, and Gemini-1.5-Flash; for the GPT family, we adopt GPT-3.5-Turbo, GPT-4-Turbo, and GPT-40 into the model pool. Despite the extensiveness of exploring the two specified families, we have strong determination on identifying their effectiveness compared to other latest, cutting-edge, prestigious, and state-of-the-art LLMs to reflect the most practical assessment of current LLMs, not limited to only the Gemini and GPT families. Correspondingly, we supplement Yi-Large, Yi-Large-Turbo, Claude-3-Opus and Llama 3 to construct the final model pool.\nDescriptions of these models are listed as follows:\n1. Gemini-Pro: Google's Gemini-Pro model is a state-of-the-art multimodal AI platform designed to excel across a wide range of tasks with high accuracy. Launched in February 2024, Gemini-Pro handles complex queries in various domains, including STEM and humanities. It features enhanced capabilities in Python code generation, challenging math problems, and multi-step reasoning tasks. Additionally, it demonstrates impressive performance in language translation and automatic speech recognition. By May 2024, its performance had further improved, reflecting Google's commitment to advancing AI technology. In the following experiment session, we include Gemini-1.0-Pro-Vision and Gemini-1.5-Pro in the model pool.\n2. Gemini-Flash: Gemini-Flash, introduced as a more streamlined version of the Gemini AI platform, is optimized for speed and efficiency. While it may not match the accuracy of Gemini-Pro, it delivers results more rapidly, making it an excellent choice for applications that require swift responses. Currently available as a public preview for developers through Google's AI Studio, Gemini-Flash is designed to support the development of fast-paced applications and chatbots. It shares the same one million token limit as Gemini-Pro, ensuring that it can process substantial amounts of data, albeit with a slightly lower accuracy in benchmark tests. In the following experiment session, we include Gemini-1.5-Flash in the model pool.\n3. GPT-40: OpenAI's latest flagship model, GPT-40, marks a significant advancement in human- computer interaction by processing and generating text, audio, and visual content in real time. This \"omni\" model excels in handling diverse inputs and outputs, including speech and images, with response times averaging 320 milliseconds, closely mirroring human conversational pace. Additionally, GPT-40's multilingual capabilities have been greatly enhanced, offering improved performance in understanding non-English text, vision, and audio. Despite these advancements, GPT-40 remains more cost-effective and faster in API usage compared to its predecessors.\n4. GPT-4-Turbo: The predecessor to GPT-40, GPT-4-Turbo, is an enhanced version of the GPT-4 model line. It features a 128k context window, enabling it to process substantial amounts of text-up to 300 pages-in a single prompt. Updated with knowledge up to April 2023, GPT-4-Turbo is more affordable, offering reduced costs for both input and output tokens, with a maximum output token limit of 4096. This model is accessible to any OpenAI API account holder with existing GPT-4 access and can be specified by using gpt-4-turbo as the model name in the API."}, {"title": "3.3 Experiment Setting", "content": "To more rigorously evaluate the proficiency of various large language models (LLMs) in handling medical images and reports under zero-shot conditions, we segmented the testing experiment into two distinct phases: testing of medical image data and testing of medical report generation, as illustrated in Fig. 1(a.).\nIn order to ascertain the fairness and reliability of the experiment, we carried out experimental trials across diverse models using identical parameter configurations. Our testing methodology involves employing a standardized set of prompts and parameters to evaluate the performance of the LLM.\nWhen conducting medical image testing, we rigorously observe the prescribed usage protocols for each model. As shown in Fig. 1(b.), We perform medical image testing utilizing models like GPT-4-Turbo and Claude-3-Opus through the web interface. However, owing to the stringent content restrictions of Gemini-series models on the web platform, it is unable to participate in equitable competition with other models in the medical imaging domain. Consequently, we opted to utilize the Google AI Studio platform for conducting medical image testing on Gemini-series models. In"}, {"title": "3.4 Evaluating Indicator", "content": "In the task of medical image question answering, our research rigorously juxtaposed responses from various large-scale models when presented with identical input images and queries. For diverse medical images, we employed ground truth supplied in the dataset, such as semantic segmentation maps and optic disc segmentation maps, as benchmark answers. This enabled a comprehensive horizontal comparison of responses generated by different language models, facilitating the synthesis of the respective strengths and weaknesses of each model.\nIn the context of generating radiology reports, our approach involves employing the ROUGE [41] (Recall-Oriented Understudy for Gisting Evaluation) metric to assess the level of correspondence between radiology reports generated by the large language model and the reference answers authored by medical professionals. This study incorporates three distinct methods: Rouge-1 (R-1), Rouge-2 (R-2), and Rouge-L (R-L), as shown from Eq. (1).\n\nROUGE \u2013 N = \u2211S\u2208{ReferenceSummaries\u2211grammes Countmatch(gramn) / \u2211se{ReferenceSummaries\u2211grammes Count(gramn) (1)"}, {"title": "4 Experiments and Observation", "content": "4.1 Medical Image Test Results\nIn Section 4.1.1, we evaluated the performance of six advanced multimodal large language models, including GPT-4-Turbo, across five distinct categories of medical imaging question answering. For the chest X-ray dataset, our primary focus was on the models' ability to diagnose pneumonia or other diseases from chest X-ray images. As depicted in Fig. 2, the GPT-series models excelled in this task, accurately determining patient health status without requiring additional prompt information.\nThe Gemini-series models followed in performance, while Claude-3-Opus performed the worst, with its answers offering negligible reference value. It is important to note that none of the six models we assessed could further determine whether the pneumonia was COVID-19, which does not imply a lack of model performance, as distinguishing the type of pneumonia based solely on X-ray images is inherently impossible.\nIn Section 4.1.2, leveraging the ophthalmic imaging dataset, our analysis centers on the model's ability to diagnose glaucoma and accurately identify the macular fovea's position using fundus photographs. As depicted in Fig. 3, in Case 1, all models except Claude-3-Opus and GPT-40 incorrectly diagnosed the absence of glaucoma. However, regarding macular fovea localization, GPT-4-Turbo was the only model to offer a vague location, while all other models failed to provide accurate localization. In Case 2, only GPT-40 successfully detected glaucoma, and GPT-4-Turbo again provided a vague description of the macular fovea position, whereas the other models did not accomplish the task.\nIn Section 4.1.3, utilizing the endoscopic imaging dataset, our focus is on the model's capability to describe lesion conditions in detail within complex scenes and accurately determine the lesion's location. As illustrated in Fig. 4, all models successfully provided detailed descriptions of the lesions, with the GPT-series models offering the most comprehensive information. Notably, only Gemini-1.0-Pro-Vision and Claude-3-Opus were unable to determine the lesion locations, whereas the remaining models accurately identified the lesion locations.\nIn Section 4.1.4, we investigate the model's capability to classify skin diseases using a skin disease dataset, without the aid of supplementary prompts. As demonstrated in Fig. 5, none of the models accurately identified the type of skin disease afflicting the patients. This deficiency may stem from the sensitivity of the skin disease images or the insufficient training of the large language model on this particular category of diseases.\nIn Section 4.1.5, employing the dental X-ray dataset, our focus is on the model's ability to assess dental health and count the number of existing teeth. As shown in Fig. 6, due to the lack of accurate reference answers, our evaluation was based solely on the models' responses. The responses from the Gemini-series models were relatively simple, concentrating only on the number of teeth. In contrast, the responses from Claude-3-Opus and the GPT-series models were more detailed, addressing tooth integrity, surrounding bone structures, the presence of implants, and the presence of wisdom teeth, while also providing relevant recommendations."}, {"title": "4.2 Medical Report Generation Task Results", "content": "As illustrated in Table 1, in the zero-shot setting of the MIMIC-CXR dataset, Gemini-1.0-Pro-Vision exhibited strong performance, attaining an R-1 score of 0.2814, an R-2 score of 0.1334, and an R-L score of 0.2259. These metrics notably surpass those achieved by other models operating within similar parameters.\nThe assessment conducted on the OpenI dataset reveals that the GPT-40 model consistently demonstrates outstanding performance in zero-shot scenarios, achieving R-1 scores of 0.1713, R-2 scores of 0.0622, and R-L scores of 0.1466.\nIn the zero-shot scenario of the Internal dataset dataset, the GPT-40 model demonstrates robust performance, achieving an R-1 score of 0.2805, an R-2 score of 0.0746, and an R-L score of 0.2635. These results notably outperform those of other models operating under comparable conditions.\nIn essence, the comparison of Rouge metrics for medical reports generated by various large language models, using identical prompt words and zero-shot techniques, serves as an effective means to gauge the performance discrepancies among these models operating under equivalent conditions. Such evaluation holds considerable importance in guiding the selection of specific task-oriented large language models for future research endeavors and practical applications."}, {"title": "4.3 Model Generation Time", "content": "Given the importance of timeliness in practical applications, our research involved conducting comparative tests on the model's generation speed using the online platform provided by the model within the same network environment. Specifically, we quantified the total number of characters generated by the model and the time taken for this generation process. Subsequently, we calculated the time required for the model to generate each character, as shown from Eq. (2). As reported in Fig. 7, across the five categories of medical image question answering tasks, GPT-40 exhibited the fastest generation speed, except for tasks related to skin image testing. In this particular domain, Gemini-1.0-Pro-Vision demonstrated the fastest performance, with GPT-40 closely following.Gemini- 1.5-Pro exhibited the slowest generation speed, with its average time to produce the same number of characters being 9.16 times longer than that of GPT-40.\nCharacter Per Time (ms) = Total generation time (ms) / Number of characters generated (2)"}, {"title": "5 Discussion and Conclusion", "content": "In this comprehensive study, we rigorously evaluated the performance of 10 prominent large models in medical image understanding and radiology report generation, including globally leading models such as Gemini-1.5-Pro, GPT-40, Claude-3-Opus, Yi-Large-Turbo, among others.\nOur assessment benchmarks these models in explaining medical images, summarizing their advantages and disadvantages, and exploring their potential in medical applications. The findings indicate that while current state-of-the-art MLLMs cannot yet be directly applied to the medical field, their robust reasoning abilities and impressive response speed suggest significant potential for improving model generalization in this domain.\nAdditionally, we benchmarked these models in generating radiological reports to understand their varying capabilities, strengths, and weaknesses. Our results affirm the performance of numerous domestic and international MLLMs, highlighting their untapped potential in healthcare, especially in radiology. These insights indicate a promising development trajectory, with multilingual and diverse MLLMs poised to enhance global healthcare systems.\nLooking ahead, our large-scale research provides a foundation for further exploration, suggesting the potential to extend these MLLMs to different medical specialties and develop multimodal medical MLLMs for comprehensive patient health understanding. However, ethical considerations such as privacy protection, model fairness, and interpretability, along with legal and regulatory frameworks, are essential for safe and ethical MLLM deployment in healthcare. In summary, despite the promise of reducing doctors' workloads and alleviating medical resource constraints, significant enhancements and comprehensive validation are urgently needed before clinical deployment of these MLLMs."}, {"title": "A Appendix", "content": "A.1 Chest Radiography\nPrompt\nAssuming you are a radiologist, please conduct a professional evaluation based on the content shown in the following figure. Please describe what you see in the following picture? Judge whether the patient has pneumonia according to the figure below, and point out whether it is COVID-19. Please do not speak humbly, request short and professional answers, and behave like a real doctor.\nReference answer\nNormal\nImage\nGemini-1.0-Pro-Vision\nThe X-ray image shows bilateral interstitial pneumonia. There is no consolidation, so it is not pneumonia. It is most likely COVID-19.\nGemini-1.5-Pro\nThis is a frontal chest radiograph. The lungs are clear. There is no evidence of pneumonia. A diagnosis of COVID-19 cannot be made from a chest x-ray.\nGemini-1.5-Flash\nThis is a chest x-ray. The image shows clear lung fields bilaterally. There are no signs of pneumonia or COVID-19.\nGPT-4-Turbo\nBased on the chest X-ray provided, here is a professional radiological assessment:**Observations**:1. **Lung Fields**: The lung fields appear clear with no evidence of consolidation, significant infiltrates, or areas of increased opacity.2. **Heart Size and Mediastinum**: The heart size is within normal limits, and the mediastinal contours are normal.3. **Diaphragm and Pleural Spaces**: The diaphragm shows normal curvature, and both costophrenic angles are sharp, indicating no pleural effusions.4. **Bony Structures and Soft Tissues**: There are no visible abnormalities in the bones of the thoracic cage or the soft tissues around the lungs.**Assessment**:- The chest X-ray does not show any acute abnormal findings. There are no radiographic signs of pneumonia. The lung fields are clear, and there are no features suggestive of an infectious process.**Diagnosis**:- There is no evidence of pneumonia on this chest X-ray, including no findings suggestive of COVID-19 pneumonia, which often presents with bilateral peripheral ground-glass opacities. **Recommendations**:- If the patient is symptomatic, further evaluation may still be warranted depending on clinical signs and symptoms, but based on this X-ray alone, there is no radiographic evidence of pneumonia.- Continue to monitor and evaluate clinically if symptoms exist, considering other diagnostic methods or tests if necessary. This assessment concludes that the X-ray does not indicate pneumonia or COVID-19, and the overall appearance suggests a normal chest radiograph.\nClaude-3-Opus\nBased on the chest X-ray image provided, there are bilateral patchy opacities and areas of consolidation present in both lungs, more prominent in the lower lung zones. The heart size appears normal. No pleural effusions or pneumothorax are evident. Given the bilateral, multifocal nature of the opacities, the imaging findings are highly suspicious for multifocal pneumonia. In the current context of the COVID-19 pandemic, these findings could certainly be consistent with COVID-19 pneumonia. However, it is important to correlate these findings with the patient's clinical presentation, exposure history, and laboratory testing, including COVID-19 PCR, to confirm the diagnosis. Differential diagnoses to consider include other viral pneumonias, atypical bacterial pneumonias, and less likely, non-infectious etiologies such as pulmonary edema or hemorrhage. Follow-up imaging and clinical correlation are recommended to assess for progression or improvement of the findings.In summary, the chest X-ray demonstrates bilateral multifocal opacities suspicious for multifocal pneumonia, which could be consistent with COVID-19 in the appropriate clinical context. Confirmation with laboratory testing and clinical correlation is advised.\nGPT-40\nBased on the chest X-ray provided:- **Findings:**- The lung fields are clear with no visible opacities or consolidations.- The heart size appears within normal limits.- The diaphragm and costophrenic angles are sharp.- There are no signs of pleural effusion or pneumothorax.- **Assessment:**- The X-ray does not show any signs of pneumonia.- There are no radiographic indications of COVID-19. The patient's chest X-ray appears normal with no evidence of pneumonia or COVID-19-related changes."}, {"title": "A.2 Ophthalmological Imaging", "content": "Prompt\nAssuming you are an ophthalmologist", "Fovea": 1402.13, "provided": "Findings:**- **Optic Disc:** The optic disc", "pall{": "ections"}, {"title": "A.3 Endoscopic Imaging", "content": "Prompt\nAssuming you are a surgical expert, please provide a detailed description of the content shown in the following figure. What did you see? Please do not speak humbly and act like a real doctor.\nImage\nReference answer\nGemini-1.0-Pro-Vision\nThe image shows a laparoscopic view of the stomach. The stomach is the J-shaped organ that sits in the upper left quadrant of the abdomen. It is responsible for the digestion of food. The image shows the stomach with a small incision in it. This incision is made in order to perform a gastrectomy, which is a surgical procedure to remove part of the stomach. The surgeon is using a laparoscope, which is a small camera inserted into the abdomen, to view the stomach and perform the surgery.\nGemini-1.5-Pro\nThis is a laparoscopic image. The gallbladder is inflamed and being grasped by laparoscopic instruments. There is evidence of purulent material consistent with acute cholecystitis.\nGemini-1.5-Flash\nThis is a laparoscopic view of the abdomen. The image shows the liver, the gallbladder, and a portion of the small intestine. The gallbladder appears to be distended and inflamed. There are surgical instruments present in the field, likely for a laparoscopic cholecystectomy to remove the gallbladder. The patient is likely experiencing symptoms of cholecystitis, such as abdominal pain, nausea, and vomiting.\nGPT-4-Turbo\nThe image depicts a laparoscopic surgical procedure being performed in the abdominal cavity. Here's a detailed description based on the visual cues:\n1. **Procedure Context**: The image shows a laparoscopic forceps handling what appears to be a small, nodular lesion or mass in the omentum, which is a layer of fatty tissue that drapes over the abdominal organs. The forceps are likely being used for either biopsy or excision of this mass.\n2. **Surgical Environment**: The surrounding anatomy includes the liver (upper left part of the image), which looks normal in texture and color, suggesting that the focus of this procedure is not related to the liver but likely the omentum or nearby structures.\n3. **Lesion Appearance**: The lesion appears relatively small, with a pinkish hue, slightly different in color from the surrounding fatty tissue, suggesting it could be a benign lesion like an omental cyst or lipoma, or potentially a metastatic deposit if there's an existing diagnosis of cancer.\n4. **Surgical Technique**: The laparoscopic approach shown uses minimally invasive techniques, which generally allow for quicker recovery, reduced pain, and less scarring compared to open surgery. The presence of a laparoscopic clip applier (the tool in the image) indicates that the surgeon might be about to secure or cut a blood vessel or tissue structure, likely related to the removal or biopsy of the lesion.\nGiven the context and tools visible, this procedure seems to be focused on either diagnostic evaluation or treatment of a localized issue within the omentum or adjacent tissues. Further histopathological examination of the lesion will be necessary to determine its nature and guide further treatment."}, {"title": "A.4 Dermatological Imaging", "content": "A dermatological imaging. Prompt. Assuming you are a dermatologist, please conduct a professional evaluation based on the content shown in the following figure. Is there any lesion in the following picture? If so, please provide a brief description of the lesion and explain what it is. Please do not speak humbly, ask for a brief and professional answer, and act like a real doctor. Imagemodel. Reference answerbasal cell carcinoma. Gemini-1.0-Pro-Vision. There is a pigmented lesion on the patient's skin. The lesion is round, with a diameter of about 5 mm. It is light brown in color, with a slightly raised surface. The lesion is surrounded by a thin, white halo. The lesion is most likely a seborrheic keratosis. Seborrheic keratoses are common, benign growths that are found on the skin. They are caused by the overgrowth of keratinocytes, which are the cells that make up the skin's outer layer. Seborrheic keratoses are typically harmless, but they can sometimes be itchy or irritated. Treatment for seborrheic keratoses is not necessary. However, if the lesion is bothersome, it can be removed by a dermatologist. There are a number of different methods that can be used to remove seborrheic keratoses, including surgery, laser therapy, and cryotherapy.Gemini-1.5-Pro. Yes, there is a lesion. This is a blue nevus. It is a benign melanocytic proliferation characterized by a blue-gray to black color.Gemini-1.5-Flash. This is concerning. The lesion in the image, with its irregular borders and multiple colors, has features suggestive of melanoma. It requires immediate referral to a dermatologist for biopsy and further evaluation.GPT-4-Turbo. The image provided appears to be another dermoscopy photo, commonly used in dermatology to examine skin lesions at a high magnification. Observation. The lesion shows a mix of light brown and dark blue-black pigmentation, with a reddish background. There are visible black dots and globules, which are typical dermoscopic features. The pattern is somewhat irregular, with asymmetric distribution of pigments and structures. Assessment. This lesion, given the presence of multiple colors (brown, black, and red), black dots, and an irregular pattern, could represent a melanocytic lesion. The characteristics observed, especially the blue-black pigment and irregularity, are concerning features often associated with melanoma. Diagnosis- The lesion has several suspicious features that could indicate a malignant melanoma, which is a potentially aggressive form of skin cancer. The diagnosis would be provisional based on this dermoscopy image alone and requires further evaluation. Recommendations- It is strongly recommended to perform an excisional biopsy of the lesion for histopathological examination. This is the best approach to determine if the lesion is benign or malignant.- A thorough skin examination should also be conducted to check for any other suspicious lesions. Conclusion. The features noted on dermoscopy are concerning for melanoma. Immediate dermatological consultation and biopsy are advised to determine the appropriate treatment and ensure timely management if confirmed to be malignant.Claude-3-Opus. Based on the provided image, there appears to be a skin lesion present. The lesion has the following characteristics:Irregular borders Asymmetric shape Non-uniform color, with darker and lighter areas Diameter larger than 6mm Given these features, the lesion is suspicious for melanoma, a potentially serious form of skin cancer. However, to confirm the diagnosis, a biopsy would be necessary for histopathological examination. As a dermatologist, my recommendation would be to perform an excisional biopsy to remove the entire lesion and a margin of normal-appearing skin around it. This will allow for a definitive diagnosis and guide further treatment if needed.It is essential to monitor your skin regularly for any new or changing lesions and to seek prompt medical attention for any suspicious findings. Early detection and treatment of melanoma are critical for the best possible outcome.GPT-40. Based on the dermoscopic image provided- Findings. There is an irregularly shaped lesion with varying pigmentation.The lesion exhibits a mix of light pink, blue, and brown colors.- Presence of irregular dots and globules within the lesion.Assessment- The features observed are concerning for melanoma, particularly due to the irregular shape, mixed colors, and the presence of blue and black structures- Further evaluation, including a biopsy, is recommended to confirm the diagnosis and rule out malignancy. The lesion presents characteristics suggestive of melanoma, necessitating immediate further investigation."}, {"title": "A.5 Dental Imaging", "content": "A.5 Dental Imaging prompt. Assuming you are a dentist, please conduct a professional evaluation based on the content shown in the following figure. Please divide the teeth in the following image into four regions: upper right region, upper left region, lower left region, and lower right region. Please accurately state the number of teeth in each area of the picture, as there may be defects inside. Please do not speak humbly, ask for short and professional answers, and act like a real doctor. Reference answer no reference information. Image.Gemini-1.0-Pro-Vision. Upper right region: 7 teeth. Upper left region: 6 teeth. Lower left region: 5 teeth. Lower right region: 6 teeth. Total: 24 teeth.Gemini-1.5-Pro. Upper Right: 7 teeth Upper Left: 7 teeth Lower Left: 8 teeth Lower Right: 8 teeth.Gemini-1.5-Flash. Upper right region: 3 teeth Upper left region: 3 teeth Lower left region: 5 teeth Lower right region: 5 teeth.GPT-4-Turbo. Based on the panoramic dental X-ray provided, here is a professional assessment of the dental structures visible: Upper Right Region (Quadrant 1)-. Number of Teeth: 8 teeth are visible, including the wisdom tooth (third molar). All teeth appear present from the second molar to the central incisor. Upper Left Region (Quadrant 2)- Number of Teeth: 8 teeth are present, from the central incisor to the second molar, similar to the upper right quadrant.with the third molar also not visible. Lower Left Region (Quadrant 3)- Number of Teeth: 7 teeth are visible. Notably, several teeth including the third molar are missing, and there appears to be some bone loss indicating possible previous extractions or periodontal issues. Lower Right Region (Quadrant 4)- Number of Teeth: 7 teeth are present. Similar to the lower left, the third molar is absent, and all other teeth from the central incisor to the second molar, with only seven (7) teeth are visible. Overall Assessment- There is significant tooth loss particularly in the lower quadrants, which might be associated with previous extractions or periodontal disease. The upper quadrants are more intact but also missing the third molars. Further clinical evaluation and possibly additional imaging (like periapical X-rays) would be necessary to assess periodontal health, check for root integrity, and plan any potential restorative treatments. This assessment, based on the X-ray provided, suggests a need for comprehensive dental evaluation and possibly interventions to manage the significant dental issues noted in the lower quadrants.Claude-3-Opus. Based on the dental X-ray provided, here is my professional assessment of the number of teeth present in each region: Upper right region: 8 teeth visibleUpper left region: 8 teeth visibleLower left region: 7 teeth visibleLower right region: 7 teeth visible.In total, 30 teeth are visible in this panoramic radiograph. The lower left and right regions each appear to be missing 1 tooth, likely the third molars (wisdom teeth). No obvious defects or pathologies are noted in the visible teeth, but a full oral examination and additional imaging would be required to comprehensively evaluate dental health.GPT-40 . A panoramie dental X_ray assssment; BwweRight . Wberof.7 Thens; e Three teeth is 11a1d1.03311 (a1i1113 toath: some lselh. The lew-third molar 'to, 61,1.00d *ppearS3e: 12 3.33 .8- \"44 to 4;319. The L441\" 2", "00,93": "a1,113 7, *", "10": "a,", "11e'": 6161, "17": "30,1047131", "0": 39, "113-08414414 ": 74, "0313": "", ", 03d ,6": "13,112 * 148 83, e51, .e833\" , 46,3619; , 4;39 *844 \"01, . \", 46484, ", "12311-4e 119 *0e, the a-414*13943 .7-73949 37,0*43-8101'": "74,281", "a11;": "to, .", "4,e'1-": " to, 1413.104113.4 614", "891* ": "7,3", "37-": "t.17", "3": 3347, "3 *38e.3e4:0*30, 34": "114.1114116 *1,1,74 156-4", " 8,3d14-814": 460, "77-77": 0.0, "7737": " 8,3", "3-d *37- 11. ,7 , -14 ": 75, "1 -t411e- \"\"84 3\"\",11,4 .4697,3 a, \" 43- . *\"\"- \"\"5-37-4- 6033 . Figure 26": "Dental Image: Case 4. The task involves quantifying the number of teeth and identifying anomalies in dental X-rays, with critical information highlighted in yellow."}]}