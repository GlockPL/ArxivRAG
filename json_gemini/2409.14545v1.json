{"title": "Why Is Anything Conscious?", "authors": ["Michael Timothy Bennett", "Sean Welsh", "Anna Ciaunica"], "abstract": "We tackle the hard problem of consciousness taking the naturally-selected, self-organising, embodied organism as our starting point. We provide a mathematical formalism describing how biological systems self-organise to hierarchically interpret unlabelled sensory information according to valence and specific needs. Such interpretations imply behavioural policies which can only be differentiated from each other by the qualitative aspect of information processing. Selection pressures favour systems that can intervene in the world to achieve homeostatic and reproductive goals. Quality is a property arising in such systems to link cause to affect to motivate real world interventions. This produces a range of qualitative classifiers (interoceptive and exteroceptive) that motivate specific actions and determine priorities and preferences. Building upon the seminal distinction between access and phenomenal consciousness, our radical claim here is that phenomenal consciousness without access consciousness is likely very common, but the reverse is implausible. To put it provocatively: Nature does not like zombies. We formally describe the multilayered architecture of self-organisation from rocks to Einstein, illustrating how our argument applies in the real world. We claim that access consciousness at the human level is impossible without the ability to hierarchically model i) the self, ii) the world/others and iii) the self as modelled by others. Phenomenal consciousness is therefore required for human-level functionality. Our proposal lays the foundations of a formal science of consciousness, deeply connected with natural selection rather than abstract thinking, closer to human fact than zombie fiction.", "sections": [{"title": "1 Introduction", "content": "Why is anything conscious?\u00b9 Both biological and other physical systems process information, yet it seems that humans consciously experience in addition to merely process information. Why? Living organisms are constantly processing self- and world-related information to secure survival in an ever-changing world. Human bodies share with all other physical systems the property of being instantiated in time and space, (e.g., our body occupies a given position and volume in space at a given time). Yet, unlike physical systems, biological, living systems are dissipative systems using energy to self-organise in the face of entropic decay and environmental perturbation [1, 2]. Originally formalised in the field of cybernetics [3, 4] the notion of self-organisation has been subsequently applied to various disciplines including physics [5], biology [6, 7] and neuroscience [8-10]. Self-organization is typically defined as the spontaneous emergence of spatiotemporal order or pattern-formation processes in physical and biological systems resulting from interactions of its components with the environment [11-13].\nInterestingly, much self- and world related information processing goes on behind the scenes, or \"in the dark\" so to speak, that is, without being constantly present to our conscious minds. But why doesn't all information processing go in the dark?\nThis question is the subject of long-standing debates across disciplines [14]. For example, one highly influential view is that consciousness has two aspects [14]. The first is functional, by which we mean the ability to access and communicate information [15]. How exactly information processing is linked to consciousness is the \"easy problem\" of consciousness [2]. The second aspect is \"what it is like\" to consciously experience information processing, or phenomenal consciousness [1, 2, 15-17]. This doesn't mean just global states like being awake, but more specific local states like smelling a cup of coffee. These local contents or \"qualia\" are characterised by what it is like to be in them [14]. It is unclear the extent to which functional and phenomenal aspects are independent. David Chalmers has influentially suggested that it may be possible to construct a \"zombie\" which acts in every way like a person but has no qualia [15]. For example, a thermostat certainly detects heat and so processes information, but there presumably is not anything it is like to be a thermostat. Hence the question \"why is anything conscious\" may be understood as \"why is there sometimes a qualitative aspect to information processing?\". This is the \"hard problem\" of consciousness.\nThe hard problem has sparked a substantial body of work and a detailed discussion of these debates [14, 18] lies beyond the scope of our paper. Rather, in what follows we build upon the useful distinction between lower and higher order theories of information processing in relation to conscious experiences.\nFor example, higher order thought theory (HOT) [19, 20] holds that the information of which a conscious being is aware are higher order \"meta-representations\" of lower order \"local\" mental states. Lower order states may include emotions and perceptions, while higher order meta-representations reflect upon those. The link between the two"}, {"title": "2 Pancomputational Enactivism Definitions", "content": "For convenience of reference, we have placed all definitions here. We have aimed to make the gist of the paper understandable without the math, hence the reader can either skip this section or refer back to it if needed. Many of the definitions have been adapted from a variety of preceding work [32, 34, 35, 42-44]. They are referred to in the body of the paper when they become relevant, in the order in which they appear here.\nDefinition 1 (environment).\n\u2022 We assume a set \\( \\Phi \\) whose elements we call states.\n\u2022 A declarative program is \\( f \\subseteq \\Phi \\), and we write P for the set of all declarative programs (the powerset of I).\n\u2022 By a truth or fact about a state \\( \\phi \\), we mean \\( f \\in P \\) such that \\( \\phi \\in f \\).\n\u2022 By an aspect of a state \\( \\phi \\) we mean a set \\( \\eta \\) of facts about \\( \\phi \\) s.t. \\( \\phi \\in \\eta \\). By an aspect of the environment we mean an aspect \\( \\eta \\) of any state, s.t. \\( \\eta \\neq \\emptyset \\). We say an aspect of the environment is realised by state \\( \\phi \\) if it is an aspect of \\( \\phi \\).\nDefinition 2 (abstraction layer).\n\u2022 We single out a subset \\( v \\subseteq P \\) which we call the vocabulary of an abstraction layer. The vocabulary is finite.\n\u2022 \\( L = \\{l \\subseteq v : \\cap l \\neq \\emptyset\\} \\) is a set of aspects in v. We call \\( L_v \\) a formal language, and \\( l \\in L_v \\) a statement.\n\u2022 We say a statement is true given a state iff it is an aspect realised by that state.\n\u2022 A completion of a statement x is a statement y which is a superset of x. If y is true, then x is true.\n\u2022 The extension of a statement \\( x \\in L_v \\) is \\( E_x = \\{y \\in L_v : x \\subseteq y\\} \\). Ex is the set of all completions of x.\n\u2022 The extension of a set of statements \\( X \\subseteq L_v \\) is \\( E_X = \\bigcup_{x \\in X} E_x \\).\n\u2022 We say x and y are equivalent iff \\( E_x = E_y \\).\n(notation) E with a subscript is the extension of the subscript.\n(intuitive summary) \\( L_v \\) is everything which can be realised in this abstraction layer. The extension \\( E_x \\) of a statement x is the set of all statements whose existence implies x, and so it is like a truth table. Intuitively a sensorimotor system is an abstraction layer. Likewise, a computer (each statement asserting a state of the computer, or a part thereof).\nDefinition 3 (v-task). For a chosen v, a task a is a pair \\( (I_a, O_a) \\) where:\n\u2022 \\( I_a \\subset L_v \\) is a set whose elements we call inputs of a.\n\u2022 \\( O_a \\subset E_{I_a} \\) is a set whose elements we call correct outputs of a."}, {"title": "3 Back to Foundations", "content": "Rather than presupposing local states or an abstraction layer, we must start from very basic first principles to formalise all conceivable environments [32, 43] in definition 1. Where there are things, we call those things an environment. Where things differ, we have different states of that environment. Hence, we begin by formalising the environment as a set of contentless global states \\( \\Phi \\). We don't assume there is any internal structure to the states contains, but rather define declarative \"programs\" in terms of relations between these irreducible, contentless states. The set P contains all such programs. The powerset of P is every aspect of the environment, because every aspect of any conceivable environment must be set of such declarative programs, which is a subset of P.\nAxiom 1: When there are things, we call these things the environment.\nAxiom 2: Where things differ, we have different states of the environment.\nUniversality Claim: Axioms 1 and 2 hold for every conceivable environment.\nWe don't need to concern ourselves with the internal structure of environment states beyond this. Declarative programs return true or false, so a declarative program is a \"fact\" if it is true, and the current state of the environment is the set of all \"facts\". Truth is determined with respect to states. If time is one way in which things differ, then there is only one state at a time (dimensions are implicit). The only meaning these programs have is how their truth values relate in different environmental states. Hence this is a representationless form of pancomputationalism [30].\n3.1 Natural Selection and Embodiment\nWe then assume a process of natural selection. This produces embodied organisms."}, {"title": "3.1.1 Self-Organising Systems as Self and World Constraints", "content": "Both snowflakes and human bodies are self-organising systems, yet only the latter are regarded to display conscious experiences. What exactly in the self-organisation model of a body make it radically different from the snowflake? To tackle this question, we need to introduce the notion of self- and world constraints.\nGiven that we have an environment, and an abstraction layer that implies an embodied formal language, we can talk about computation with inputs and outputs by just treating everything as embodied statements embedded and enacted within the pancomputational environment. Given an input i, the set of all possible outputs is the extension \\( E_i \\) of that input. This is because if i is realised by the environment, then the environment is constrained to only those states that realise i, which constrains what other statements can be realised. We can use this fact to talk about \"policies\" as embodied constraints on behaviour. A policy is a statement whose extension constrains outputs, like a causal intermediary in machine functionalism [47]. If behaviour is \"motivated\", then statements have valence determined by natural selection and"}, {"title": "3.1.2 Inference", "content": "Every statement in L, implies a constraint, because there are only so many outputs that can be expressed by a body at the same time as any given statement. That is what the extension of a statement represents. If i is a statement which is true, then the only possible statements a body can express is \\( E_i \\). As a result, a body might be express a statement \\( \\pi \\) (meaning \\( \\pi \\) is true), and \\( \\pi \\) would then constrain the body to only correct outputs \\( O_\\mu \\subset E_{I_\\mu} \\) if \\( O_\\mu = E_{I_\\mu} \\cap E_\\pi \\). We call a constraining statement a policy. A policy constrains outputs given inputs. A correct policy is one that constrains outputs to only correct outputs. For the sake of intuition, think of \"correct\" as \"fit\" according to natural selection (although this need not always be the case). This is more formally expressed in definition 4."}, {"title": "3.1.3 Learning", "content": "Learning, in computational terms, tends to be understood as the process of modelling the program which caused data. Constructing a \"world\" model. Here, we just need to explain how an organism gets \\( \\mu \\) from h. We don't need a world model, but a way"}, {"title": "4 Relevance Realisation Through Causal Learning", "content": "The enactive process of relevance realisation can be framed as policy learning. Natural selection prefers more adaptable organisms, so we assume it optimises for organisms that optimise for weaker policies. We call this weak policy optimisation (WPO). Policies determine how inputs are mapped to outputs. Interpretation. Fit policies must correctly predict causes of valence. Hence by constructing fit policies, an organism must realise what is relevant. A weaker policy implies all the more specific versions of itself, meaning those that more tightly constrain outputs by having a smaller extension. Hence policy learning implies a lattice of policies that vary in weakness. Every fit policy is a \"causal identity\" for something, for example an object like a \"food\" or a more abstract concept like \"pain\".\nFor convenience, we'll now expand upon the concept of organism given in definition 6. All organisms must have preferences (will make some decisions and not others), regardless of how those arise. All organisms must have policies that reflect those prefer-ences, and every policy implies tasks, so we can define preferences as an binary relation over tasks. Correct or incorrect choice of policy affects the organism's existence and survival."}, {"title": "5 Relevant Causal Identities", "content": "The extension of fit behaviour is an extensional definition of what an organism is compelled to want by natural selection. Likewise, an intension of fit behaviour would be any policy that results in fit behaviour, delivering the organism what it is compelled to want. Indeed, this was how the formalism of tasks we employ originated, to formalise"}, {"title": "5.1 Causal Learning", "content": "In accord with [42, def. 6] we assume a vocabulary \\( v_a \\) belonging to an organism we'll denote a. A \"cause\" in the context of this formalisation is not a variable but a statement \\( l \\in L_{v_a} \\) in that formal language. The raincoat example would involve obs, rain \\( \\in L_{v_a} \\) such that:\nobs  \"Alice put on a raincoat\" and rain  \u201cIt rained\u201d\nobs and r have truth values in accord with the definition of sensorimotor language. As we did with the example involving variables we assume the organism has concluded \\( p(r | o) = 1 \\) from passive observation, the naive interpretation of this being that rain can be triggered by intervening to put a raincoat on Alice. In the case of passive obser-vation, the statement obs = \"Alice put on a raincoat\" is true. However, the statement which is true in the case of intervention not only obs, but \\( i \\in L \\) such that obs \\( \\subset int \\) and:\nint \u201cAlice is wearing a raincoat because of Bob's actions\u201d\nIn other words, so long as \\( c \\neq int \\), the intervention can be differentiated from the passive observation. We formally define this in definition 10.\nThis being the case, any set \\( c_{int} \\cap obs \\) could be used to identify the party undertaking the intervention, which is why c is referred to as a \"causal identity\". It distinguishes the intervention int from the passively observed effect obs, like reafference in living organisms. However, the above only considers one intervention. A weaker or more general causal identity would be one that is shared by more intervention. For example, we might have two different interventions \\( a_1 \\) and \\( a_2 \\), with the observed effects \\( C_1 \\subset a_1 \\) and \\( C_2 \\subset a_2 \\), and causal identities \\( a_1 \\subset C_1 = l_1 \\) and \\( a_2 \\subset C_2 = l_2 \\). If \\( l_3 = l_1 \\cap l_2 \\neq \\emptyset \\), then \\( i_3 \\) is a causal identity that is present in two interventions.\nBeing a question of classification, we can express a causal identity in relation to a v-task, for which the causal identity is a policy (expanding upon [35, def. 10]), giving"}, {"title": "5.2 Ascribing Intent To Other Objects", "content": "The \"do\" operator is necessary to discern the difference between an event one has caused, and that same event passively observed. However, what is passive observation if not the result of an intervention by something other than oneself? The distinc-tion between \"intervention\" and not is misleading. Everything is an intervention. The question is not \"is this an intervention\" but \"by whom was this intervention made?\".\nTo illustrate this point we return to Alice, Bob and her raincoat. Earlier, we arrived at the following graph in which Bob's intervention was given by int.\nHowever, what if a third person Larry puts the coat on Alice? Surely Bob can observe this, and so Bob's observation of Larry's intervention is \\( v \\in L_v \\), such that obs \\( \\subset v \\). To account for this, Bob can construct a causal graph as below (with b. representing Bob and l. representing Larry).\nBob's causal identity for himself \\( c_b \\subset b. int - obs \\) only represents the intervention by himself. However, now we can see that Bob must also construct a causal identity \\( c_l \\) for Larry, where the \\( c_l \\subset l. int - obs \\). More generally, for an organism a with sensorimotor language \\( L_{v_a} \\) to construct a causal identity for an object b, it must first be the case that a is affected by b [42], to satisfy the incentive precondition for causal identity. It depends on motive, or valence. Recall that to be affected is formally defined in definition 9.\nAssume an organism a is affected by b given inputs INT, and not affected given inputs OBS. To then attribute the contents of INT to one specific entity, there must be something in common between the members of INT caused by b that is not shared by any member of OBS caused by something else (in other words it must be at least possible for a to discern the existence of b). The contents of INT are \"interventions\" by b and by learning c, a corresponding causal identity, a can discern the existence of b. This is not to say that b decides (chooses an output) or is even alive. What is important is that b affects a, making it possible to discern when these interventions are a consequence of b's existence."}, {"title": "5.3 Preconditions", "content": "There are certain preconditions for the existence of a causal identity corresponding to INT and OBS.\nFirst, the vocabulary \\( v_a \\) of an organism a must be large and expressive enough to ensure that observations are distinguishable from interventions (in other words, the causal identity must be a subset of the vocabulary). We must have sufficient scale. Second, it must be in the organism's interest to make this distinction.\nInference is only possible if some states are preferable to others. It is a value judgement. As Hume pointed out, one cannot derive what \"ought\" to be from a statement of what \"is\". Natural selection provides a notion of what ought to be, by eliminating anything which ought not.\n1. The scale precondition requires v contain the causal identity.\n2. The incentive precondition is that fitness demands the causal identity."}, {"title": "5.4 Realising Lower Order States And Higher Order Meta Representations", "content": "Causal identities are not constrained to other organisms, but we use other organisms for intuition. This is because the causal identity for an object is a policy predicting its behaviour. This makes intuitive sense when that object is a self-organising system with goals. However, it also applies to inanimate objects like rocks. A rock still has behaviour, but it does not have intent. Yet by constructing a policy representing the rock's intent, one may predict what the rock will do (e.g. damage a tree when thrown). That ability to predict is what matters. The weaker the causal identity, the more pervasive the \"identity\" in the sense of being part of more interventions. Learning is not just about constructing policies but joining them together into weaker, more abstract policies. By learning policies that correctly identify the causes of valence at different levels of abstraction, the organism engages in relevance realisation.\nImportantly, embodiment imposes severe limitations in the form of a finite vocab-ulary. In definition 6 we formalise this for the sake of explanation, formalising an organism o with a set of policies the organism has realised. Each and every policy implies a v-task. A v-task is a triadic relation between inputs, outputs and policies which resembles Peircean semiosis [42, 58] of sign, referent and interpretant. Hence, in definition 7 we formalise this as a \u201cprotosymbol\u201d systems, for the organism o. Interaction is defined in terms of choosing an output, which means there is inevitably a policy and a policy implies a v-task, so definition 6 includes a preference order over tasks, which is used in definition 8 to formalise the interpretation of inputs (in terms of constraints, rather than an algorithm). In particular, the organism must always act according to an interpretation, and some interpretations imply others. Related as they are in a lattice, protosymbols are analogous lower order states and higher order meta representations. Tasks exist in a \"generational hierarchy\". They are not mutually exclusive. Higher level tasks are more general, and have fewer policies because only"}, {"title": "6 Multi-Layered Self-Organisation", "content": "In an interactive setting", "causal-identity": "or one's self-related information processing are con-structed [35]. This lets us frame the construction of embodied selves in developmental [60] and evolutionary terms. We turn to this discussion now.\n6.1 The First Order Self\nA first order self (1ST henceforth", "do": "perator [45].\nA 1ST order self formally defined in definition 13. It serves as the locus of self-related information processing and experience [35]", "self\" that is part of the present inter-action. But also, to anticipate future planned interactions (for example, an insect navigating its environment [38]), and recollections of past interactions (it is a subset of all those relevant \"statements\" in the formal language).\nNatural selection prefers efficiency. The absence of a single, centralised causal identity can create inefficiencies14. Decentralised or asynchronous control might be advantageous in some circumstances (e.g. an ant colony), but not others (e.g. in an individual human body). An organism that has a distributed control system might have a \"self\" for each part of that system, and perhaps the co-ordination of parts might seem to suggest the existence of a centralised \"self\". Redundancy might be use-ful in some circumstances, but there is also a cost\n    },\n    {": "itle", "6.2 The Second Order Selves": "content\": \"Survival may demand organism a infer b's prediction of a's interventions (to see one's self as if through another's eyes [42]). This is called a second order self (2ND hence-forth). We argue that if access conscious contents are available for communication in the human sense, then they must be communicable in the Gricean sense [62, 63]. Grice argued that communication is about the inference of intent. If person a and b are talking, then the meaning \\( m_a \\) of what a says is whatever a intends b under-stand. The meaning \\( m_b \\) that b understands is whatever b thinks a wants b to think. b has understood what a means if \\( m_b \\) approximates \\( m_a \\). This can happen only if a can predict with reasonable accuracy what b thinks a thinks, and b can predict what a thinks b will think upon hearing an utterance. In other words, both a and b must have 2ND order selves that are good approximations. Yes, there are other aspects to communication.\nHowever, here we are talking about consciousness. Access conscious contents are those available for reasoning and report. It follows15 that access conscious contents must in principle be communicable in the sense Grice described.\nAs such, we argue contents available for communication can only be the con-tents of 2ND order selves, which means only an organism with 2ND order selves can be considered to have access consciousness. 2ND order selves also explain atten-tion and self-awareness. An organism can have many 2ND order selves because they depend upon who or what the organism is interacting with, just as the availability of information depends on context.\nIntuitively, where a 1ST order self might allow one to observe a cat and form plans regarding causal interactions with the cat, a 2ND order self would allow one to be consciously aware of the cat for the purpose of reasoning and report. One can know of the cat, and that another organism knows of the cat, but a 2ND order self is insufficient to be aware that one is aware of the cat16.\nMore formally using the notation given in definition 15, assume a and b are organ-isms that evolved to accurately predict one another's behaviour. Assume a constructs a causal identity \\( c_a \\) to predict b given input \\( i_a \\in I_{\\mu_a} \\), of which a second order self \\( c_{ha} \\) is part. Likewise, b constructs \\( c_a \\) to predict a given input \\( i_b \\in I_{\\mu_b} \\), of which \\( c_{ab} \\) is part. What is important here is that each organism's intent is to some extent inferred by the other, and that fact inevitably changes the sorts of policies that will be \"fit\". For exam-ple, second order self means each knows the other can anticipate manipulation, which means the optimal policy will often be to have rather than feign intent that aligns"}, {"title": "6.2.1 The Third Order Selves", "content": "We can continue scaling WPO indefinitely. 3RD and higher order selves can explain function. For example, meta self-awareness [39] is the awareness that one is self-aware. If self-awareness stems from 2ND order selves, then it follows that meta self-awareness requires 3RD order selves. Formally a third order self for a reflecting off \\( c_{ha} \\) lets b is \\( c_{haba} \\). It is a's prediction of b's prediction of a's prediction of b's prediction of a."}, {"title": "7 The What and Why of Consciousness", "content": "Up to now we have developed the conceptual toolbox that one can use to dissolve the hard problem. We started with the basic observation that self-organizing systems such as the human bodies constantly process information to maintain oneself in the face"}, {"title": "8 From Rocks to Einstein: The Hierarchy of Being", "content": "To illustrate how our argument applies in the real world we describe stages of con-scious organism. Each stage follows from scaling up supply and demand for WPO,"}, {"title": "Stage 0: Unconsciousness", "content": "Stage zero may be understood as the consciousness we are willing to assign to a rock: more exactly, the lack of consciousness so assigned. While some assert a thesis of panpsychism and claim everything, even a solitary hydrogen atom is conscious, we take rocks as expressing a baseline example of things in the universe that are not conscious at all. They do not sense. They do not think. They do not act. They have no 1ST order self.\n\u2022 Example: A rock."}, {"title": "Stage 1: Hard Coded", "content": "Stage one refers to adaptations \"hard-coded\" or hard-wired by natural selection. This is the sort of preset behaviour that allows complexity to persist [66] in a reasonably consistent environment.\n\u2022 What: Hard-coded adaptations. Habituation and sensitization.\n\u2022 How: The extension of fit behaviour is learned by natural selection and hard-coded into the organism as a policy (in DNA, form, the local environment etc).\n\u2022 Why: If the environment never changes, it makes more sense to just hard-code survivable behaviour.\n\u2022 Example: Single-celled protozoan."}, {"title": "Stage 2: Learning", "content": "Stage two introduces learning. To learn an organism must store, classify and order historical examples by valence. There is not something it is like to be stage two, because there is no locus of \"self\". A biological example of such a decentralised nervous system is the cubozoan box jellyfish Tripedalia cystophora. Even Tripedalia cystophora was recently shown to be capable of associative learning [40]. An entirely distributed control system can \"learn\". Likewise, stage two is exemplified by the nematode C. elegans [67, 68], which has a centralised nervous system and exhibits some ability to adapt with experience. However, the absence of a \"self\" prohibits cause and effect reasoning, which as others have already pointed out must limit spatial, navigational abilities [38]. When starved C. elegans exhibit \u201cincreased locomotion and dispersal in a random, rather than directed, search\" [38, 69, 70], whereas something like a bee or"}, {"title": "Stage 3: 1ST Order Self", "content": "This is where phenomenal (i.e. experiential) consciousness begins, with a 1ST order self. In biological terms this also implies reafference, which others have argued is the key to subjective experience, albeit for different reasons than what we have argued here [36-38]. They identified a housefly as a good example of where subjective experience may begin, and we concur. We also hold this is where an organism might be said to have intent. Intuitively, the policy that motivated behaviour is the intent of that behaviour. The \"weaker\" a policy is, the more behaviours it motivates. Conversely, if one's actions share anything in common it is the intent that motivated them. The more diverse the actions, the more \"general\" or \"high level\" the intent they share. For example, the action of eating tends to involve the intent of satisfying hunger. If a policy is implied by all of an organism's behaviour, then it might have motivated that behaviour. While this is an unusual form of words, we contend that an organism is such a policy.\nA stage three there is a self to feel simple hunger and pain. However, communication as described by Grice would be impossible [63], because a stage three organism has no self-awareness, to represent another's perception of their intent [42]. Nor would a stage three organism be able to conceive of its own death, or shame, because it cannot conceive of itself.\n\u2022 What: A 1ST order self. Reafference. Phenomenal consciousness.\n\u2022 How: Embodiment in which intervention is not identical to observation.\n\u2022 Why: Accurate prediction of consequences of interventions. For example, a fly must distinguish between having moved, and the environment having moved, to navigate.\n\u2022 Example: Housefly."}, {"title": "Stage 4: 2ND Order Selves", "content": "Stage four is the 2ND order self, and importantly this is where we hold access con-sciousness begins because it is where information is available for report in the Gricean sense. In other words, access consciousness follows phenomenal consciousness. The"}, {"title": "9 Unifying Lower and Higher Order Theories of Consciousness", "content": "Our earlier research argues that assuming lower order states is like assuming an abstraction layer in a computer [35, 44]. The behaviour of software is determined by the abstraction layer that \"interprets\" it. At the most foundational level software is"}, {"title": "10 Conclusions and Outlook: Why Nature Does Not Like Zombies", "content": "In this paper we provided a mathematical formalism uniting lower order states and higher order meta-representations, dissolving the hard problem of consciousness. Specifically, we described a multilayered formalism illustrating how biological self-organising systems become phenomenally conscious when they construct a 1ST order self. A human lacking a 1ST order self could not perform causal reasoning needed to adapt as humans evidently can [45].\nAs previous research pointed out [32], the computer metaphor with the seminal distinction between hardware and software is a simplification. Rather, software is a state of hardware. Nature privileges efficiency over abstract simplification. Compare the vast quantities of both training data and energy required by a language model, to the small quantities humans need to solve a problem. Biological systems are more efficient, because they are adaptive at a every level [24, 44]. Hence, rather than trying to explain the mind in the abstract like software, we started at the level of the embodied organism.\nOne direct consequence of our approach is that it places the phenomenal qual-ity of conscious experience before access consciousness. We have shown a consistent definition of access consciousness implies 2ND order selves, which imply a 1ST order self. Phenomenal consciousness arises first, access comes later. Unlike panpsychism, we don't believe rocks are conscious but solely those self-organising systems that need to adapt, motivated by valence, while keeping track of the self. Consciousness is a necessary adaptation, and a zombie is impossible because some behaviour cannot be achieved without consciousness.\nThere remain unanswered questions regarding whether the mere presence of 1ST, 2ND and 3RD order selves is sufficient for consciousness, but we hold they are at least necessary. Their absence should guarantee a system is not conscious. This serves to resolve questions about the consciousness of artificially intelligent systems, such as large language models. Such models are neither optimised to construct weak rep-resentations, nor have any incentive to construct selves, being passive mimics [42] of human behaviour. Future research should attempt to identify 2ND and 3RD order selves in biological systems. This will help establish levels of consciousness in different organisms. Merker, Barron and Klein have already nicely demonstrated the existence of a 1ST order self in humans and insects. Future research may also consider training organoids and artificial systems in a manner that should cause them to construct 1ST, 2ND and 3RD order selves. This may help whether what we have described is suffi-cient to achieve the outward behaviour of consciousness and what else, if anything, may be required to engineer it. Any difference between equivalent synthetic biological"}]}