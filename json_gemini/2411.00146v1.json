{"title": "Responsibility-aware Strategic Reasoning in Probabilistic Multi-Agent Systems", "authors": ["Chunyan Mu", "Muhammad Najib", "Nir Oren"], "abstract": "Responsibility plays a key role in the development and deployment of trustworthy autonomous systems. In this paper, we focus on the problem of strategic reasoning in probabilistic multi-agent systems with responsibility-aware agents. We introduce the logic PATL+R, a variant of Probabilistic Alternating-time Temporal Logic. The novelty of PATL+R lies in its incorporation of modalities for causal responsibility, providing a framework for responsibility-aware multi-agent strategic reasoning. We present an approach to synthesise joint strategies that satisfy an outcome specified in PATL+R, while optimising the share of expected causal responsibility and reward. This provides a notion of balanced distribution of responsibility and reward gain among agents. To this end, we utilise the Nash equilibrium as the solution concept for our strategic reasoning problem and demonstrate how to compute responsibility-aware Nash equilibrium strategies via a reduction to parametric model checking of concurrent stochastic multi-player games.", "sections": [{"title": "1 Introduction", "content": "Strategic decision-making and reasoning in multi-agent systems (MAS) operating in a dynamic and uncertain environment is a challenging research problem. Formalisms such as ATL [3] and PATL [10] provide important frameworks for reasoning about strategic decision making in MAS. These formalisms have also found notable applications in the field of multi-agent planning [36,23,33,17,12]. On the other hand, recently, the surge in research focused on the trustworthiness of autonomous systems has elevated the concept of responsibility as a crucial aspect in AI [24,18]. Consequently, the notion of a responsibility-aware agent has emerged [38,25,15]. In this context, a self-interested agent is assumed to consider not only its own reward but also its responsibility with respect to the tasks assigned to it.\nThis paper explores the problem of strategic reasoning where agents are aware of causal responsibility [11,32], which captures the impact of an agent's or coalition's actions or inaction on outcomes. Analysing causal responsibility enables a balanced distribution of rewards or penalties among agents based on their contribution to outcomes\u00b9 . This provides a way for ensuring fairness among agents."}, {"title": "2 Parametric Model of Stochastic Game", "content": "In this section, we introduce the model utilised in this paper. We begin with the conventional model of a concurrent stochastic multi-player game (CSG) [34,28]. Then, we define the corresponding parametric stochastic MAS, which captures the dynamics of a given CSG.\nDefinition 1. A concurrent stochastic multi-player game (CSG) is a tuple G = (Ag, S, so, (Acti)i\u2208Ag, d, Ap, L) where:\nAg = {1, ..., n} is a finite set of agents;\nS is a finite non-empty set of states;\nSo \u2208 S is the initial state;\nActi is a finite set of actions for i. With each agent i and states \u2208 S, we associate a non-empty set Acti(s) of available actions that i can perform in s. Write ActAg = Act\u2081 \u00d7\u00b7\u00b7\u00b7 \u00d7 Actn.\n\u03b4 : S \u00d7 ActAg \u2192 Dist(S) is a probabilistic transition function;\nAp is a finite set of atomic propositions;\nL:S \u2192 2Ap is the state labelling function mapping each state to a set of atomic proposition drawn from Ap.\nWe augment CSGs with reward structures of the form r = (rs,ra), where rs: S \u2192 Ris the state reward and ra: ActAg \u2192 R is the action reward. In this paper, we consider cumulative rewards [28], that is the sum of payoffs accumulated during the run until a specific point.\nIn this work we assume that players have memoryless strategies. Let Acti(s) be the set of actions available for player i in state s. The set of all memoryless strategies of i from s can be encoded by a set of variables V = {Xi,0, ..., Xi,j}. Intuitively, the value of x \u2208 V corresponds to the probability of some action a being chosen by i in some state s. Let Vi = Uses V. A memoryless (mixed) strategy for i in G thus corresponds to an evaluation of such a set of variables represented by a function Ci : Vi \u2192 QVi.\nDefinition 2. Given a CSG G = (Ag, S, so, (Acti)i\u2208Ag, \u03b4, \u0391p, L), we construct the corresponding parametric stochastic multi-agent system (PSMAS) as a tuple M = (Ag, S, so, V, \u0394, Ap, L), where:\nAg = {1, ..., n} is a finite set of agents;\nS is a finite non-empty set of states;"}, {"title": "3 The Logic PATL+R", "content": "We introduce PATL+R, a variant of PATL that incorporates quantified reward and responsibility formulae. In this paper, we specifically consider bounded path semantics."}, {"title": "3.1 PATL+R over finite (bounded) paths", "content": "Definition 13. Let M = (Ag, S, SL, V, A, Ap, L). The syntax of PATL+R is made up of state formulae and path formulae represented by & and 4, respectively.\n\u00a2 ::= a | \u00ab\u03c6 | \u03c6\u2227\u03c6 | \u3008A\u3009Pxp[V] | \u3008A\u3009Rxq[\u25ca\u2264k$] | (A)Dd [CAR\u03af,\u03c0(\u03c8)] | \u3008A\u3009D<d[CPRi,\u03c0(\u03c8)]\n\u03c8 ::= \u039f\u03a6 | $U<k\nHere a \u2208 Ap is an atomic proposition, AC Ag is a set of agents, (A) is the strategy quantifier, i \u2208 A is an agent, < \u2208 {<, <, >, >}, p\u2208 [0,1], q, d\u2208 R are probability, reward and responsibility degree bounds, respectively, and k\u2208 N is a time bound.\nNote that a PATL+R formula is defined relative to a state; path formula only appear within the probabilistic operator (A) Pp [], the reward operator (A)Rq [<k], and the responsibility degree operator (A)Dd [i], where Vi denotes the responsibility operator CARi,\u03c0(\u03c8) or CPRi,\u03c0(\u03c8). The formula (A)Pop [V] expresses that the coalition A has a strategy such that the probability of satisfying path formula isp when the strategy is followed. The formula (A)Rq [\u25c7<k$] expresses that the coalition A has a strategy such that the expected rewards of satisfying path formula\u25ca<kisq when the strategy is followed, where \u25c7<k$ = true U<k . The formula (A)Dd [Vi] expresses that the degree of responsibility of i of satisfying path formula & under coalition A is d."}, {"title": "3.2 Model checking PATL+R", "content": "As PATL+R extends PATL, which operates within a branching-time logic framework, the fundamental model checking algorithm shares a basic structure similar to those used in CTL (see [5] for details). This algorithm operates through the recursive computation of the set Sat(), which represents states satisfying the formula & within the model. Given a state formula 4, the algorithm recursively evaluates the truth values of its subformulae d' across all states, starting from the propositional formulae of & and following the recursive definitions of each modality. The problem of model checking a PSMAS w.r.t. arithmetic term comparison (excluding responsibility operators introduced in this paper) is similar to PATL and rPATL (see [10,28] for details). Therefore, we focus on evaluating responsibility degree formulae Es,GA (D[i])."}, {"title": "4 Finding Stable Joint Plans", "content": "In this section, we discuss how to compute stable joint plans. To this end, we first introduce the notion of a utility function, which intuitively is a function that considers both reward/payoff and a responsibility degree.\nDefinition 15. Given M = (Ag, S, s\u0131, V, \u2206, Ap, L), a joint plan \u03c0, we define the payoff valuation function of i \u2208 Ag as the expected payoff of Hist(50):\nV(80) = \u03a3V(\u03c1)\n(1)\n\u03c1\u03b5Hist\u03c0 (80)\nExample 6. The expected payoff of \u03c0\u2081 of A1 given in Example 3 can be computed as:\nVA(so) = 2 \u00b7 (1 \u2212 x1)x2 + 1 \u00b7 x1(1 - x2)\nDefinition 16. Given M = (Ag, S, Si, V, A, Ap, L), an outcome y, and a joint plan \u03c0, we define the responsibility valuation function of i \u2208 Ag as the responsibility degree of Hist\u3160(50):\nR(50) DAg[CARi,\u03c0(4)] + \u03b8\u00b7 DAg[CPRi,\u03c0(4)]\n(2)\nwhere O is a Lagrange coefficient to adjust the weight of CAR degree and CPR degree."}, {"title": "4.1 Nash equilibrium for responsibility and utility", "content": "Definition 18. Given M = (Ag, S, so, V, \u2206, Ap, L), for each agent i, and Ag\\i,\na plan \u03c0\u00b2 is a best response w.r.t. utility functions if it is the set\nUBR\u00bf(Ag\\) {\u03c0\u00b2 | max(x(V, Ag(80), R\u00b2, \u03c0\\ (80))}\n\u03c0\nA joint plan is considered a mixed Nash equilibrium (NE) if it belongs to the best response sets \u03c0\u2208 uBR\u00bf(\u03c0Ag\\i) for all i \u2208 Ag in M.\nComputing expected payoff for reachability V. The computation of the payoff valuation function, as given in (1), involves calculating the reachability rewards, using the formula:\n\u0395\u03c3(A)R[<k$]) = E(p\u00b2({p\u2208 Histm(s, 0A) | p\u3151\u043c \u25ca\u2264k$})\nwhere (p) = \u03a3\u03ba\u03bf (ra(pa(i)) + rs(ps(i))), k\u00f8 = min{k,k'} and k' \u2264 k s.t. ps(k') = \u03c6."}, {"title": "5 Conclusions and Future Works", "content": "We have developed an approach to multi-agent strategic reasoning with responsibility-aware agents. We introduced PATL+R, a logic that can be used"}]}