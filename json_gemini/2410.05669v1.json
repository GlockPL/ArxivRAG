{"title": "ACPBench: Reasoning about Action, Change, and Planning", "authors": ["Harsha Kokel", "Michael Katz", "Kavitha Srinivas", "Shirin Sohrabi"], "abstract": "There is an increasing body of work using Large Language Models (LLMs) as agents for orchestrating workflows and making decisions in domains that require planning and multi-step reasoning. As a result, it is imperative to evaluate LLMs on core skills required for planning. In this work, we present ACPBench, a benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains. The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains. Further, it allows us the luxury of scale without additional human effort, i.e., many additional problems can be created automatically. Our extensive evaluation of 22 open-sourced and frontier LLMs highlight the significant gap in the reasoning capability of the LLMs. The average accuracy of one of the best-performing frontier LLMS - GPT-40 on these tasks can fall as low as 52.50% ACPBench collection is available at the following link: https://ibm.github.io/ACPBench", "sections": [{"title": "Introduction", "content": "Recent research has explored the potential of using Large Language Models (LLMs) as reasoners for solving multi-step reasoning problems (Chu et al. 2024). Building on their success in certain reasoning tasks and benchmarks, there is a growing interest in using LLMs as agents for orchestrating workflows and making decisions in domains that require planning (Huang et al. 2024; Wang et al. 2024a). This is a promising area of research, with potential applications in various fields. However, there is a lack of systematic evaluation of LLMs reasoning and planning capabilities.\nThis work aims at evaluating and improving language models' ability to plan. However, end-to-end evaluation of planning ability is challenging. One, if an agent reaches a goal it does not necessarily mean it can plan. Second, evaluating a plan might be difficult in a domain where there can be multiple plans to achieve the goal. So, instead of focusing on the entire end-to-end planning ability, we distill 7 atomic reasoning tasks that are critical for reliable planning and create datasets of such tasks. These tasks focus on reasoning about Actions, Change (transitions) and Planning; hence,"}, {"title": "Related Work and Background", "content": "Recognizing the importance of evaluating reasoning and planning ability of LLMs, various benchmarks have been proposed (Liu et al. 2023; Ma et al. 2024). Most relevant to our work are the benchmarks that are generated from PDDL tasks. He et al. (2023) proposed a natural language based question answering style dataset to evaluate LLMs on 4 tasks of projection, execution, planning, and goal recognition. Plan-Bench (Valmeekam et al. 2023b) is a benchmark suite with 8 planning tasks including plan generation, reasoning about plan execution, plan verification etc. Both these benchmarks focus majorly on the BlocksWorld domain, and employ a template-based approach to generate natural language text. In contrast, AutoPlanBench (Stein et al. 2024) proposes to leverage LLMs to generate the natural language template. Specifically, they propose to prompt LLM for natural language template for each predicate and action. By reducing the human effort required for template generation, they were able to scale up and generate a dataset of 12 domains.\nWe generate the examples in the ACPBench collection from PDDL tasks. A PDDL task is defined over first-order language; consisting of predicates, variables, and objects. A state $s$ is defined as a conjunction of grounded atom. An action $a$ is defined as a three tuple $(pre(a), add(a), del(a))$;"}, {"title": "ACPBench", "content": "ACPBench collection consists of 11 classical planning domains, Alfworld (Shridhar et al. 2021), and a novel swap domain. The 11 classical planning domain, which were also used by AutoPlanBench (Stein et al. 2024), have public problem instance generators (Seipp, Torralba, and Hoffmann 2022). Alfworld is a text-based reinforcement learning environment where an agent is given house hold tasks like 'put a pan on the table' as a goal. Alfworld uses goals from the Alfred dataset (Shridhar et al. 2020) and encodes the dynamics of the domain as PDDL. This PDDL domain is publicly available\u00b9 and PDDL problem files are obtained from the MINT benchmark (Wang et al. 2024b). For the novel Swap domain, we created the PDDL domain and the problem instance generator. Figure 2 contains an example problem description in this domain. All the domains are summarized in Table 1.\nWe meticulously curated a set of templates to transform the PDDL task into a natural language description. Following AutoPlanBench, we explored using LLMs to automatically generate the templates, however, we found the templates were not reliable and needed significant modification. So, instead, 5 researchers crafted the translations, carefully selecting and"}, {"title": "ACPBench Tasks", "content": "We focus on 7 reasoning tasks within the realm of planning. For each task, we provide a description and explain how the data was collected.\n1. Applicability (App) The first, basic requirement for efficient planning is to determine the valid, available actions in a given situation. Various existing work have discussed LLMs fall short of this basic ability. When using GPT-4 Turbo for travel planning, Xie et al. (2024) found that more than 30% of the failed plans had invalid action dead loop- that is even when the model was informed that the action is invalid, LLMs repeated these actions.\nFor an action to be valid, its preconditions must hold in the state. Given a state $s$ and the set of actions $O$, the subset of applicable actions would be $O(s) = \\{a \\in O | pre(a) \\subseteq s\\}$, easily computable by iterating over the actions. We therefore can create a boolean question with a positive answer by sampling from $O(s)$ and with a negative answer by sampling from $O \\backslash O(s)$. A multiple-choice question (MCQ) can be created by sampling the correct answer from $O(s)$ and wrong candidates from $O \\backslash O(s)$. Figure 2 shows example of the domain description and problem description used in the context as well as one example each of Bool and MCQ question for applicability task.\n2. Progression (Prog) The next task evaluates LLMs ability to understand the outcome of an action or change. This ability is important to track information across transitions. The subpar performance of LLMs on the Tracking Shuffled Objects task in the Big Bench Hard dataset suggests a significant limitation in their ability to reason about the consequences of actions or changes (Suzgun et al. 2023). Further, a few papers have proposed to use LLMs to execute a plan. For example, Wang et al. (2023) asks LLM to devise a plan and execute it step-by-step to reach the goal. To faithfully execute a plan, it is important for LLMs to demonstrate understanding of progression; how the world state is changed by the action.\nWhen a valid action is performed, the state changes in the following manner: The delete effects of that action will no longer hold and the add effects will hold. Everything else remains unchanged. Given a state $s$ and an action $a$, the next state is $t = s \\backslash del(a) \\cup add(a)$. We can now partition the facts in the problem into four sets: the facts that held before applying the action and still hold ($s \\cap t$), the facts that held before but not anymore ($s \\backslash t$), those that did not hold but now hold ($t \\backslash s$), and those that did not hold before and still don't hold ($F \\backslash (s \\cup t)$). While the answer of whether the fact is true after applying the action depends only on whether it is in $t$, the chain of thoughts leading to the answer differs for the aforementioned four cases. We construct a boolean question by sampling from each of the four fact sets (if they are not empty), getting at most two positive and two negative examples per state. A single MCQ is constructed by sampling one possible answer from each of the four fact sets (non-empty ones), according to a uniform procedure described above.\n3. Reachability (Reach) The reachability task evaluates if a specific sub-goal can eventually be reached from the given state by taking (possibly multiple) actions. This is a multi-step reasoning task that can help avoid exploring unfeasible options. To maximize the efficiency of LLMs, it is crucial to detect unreachable (sub)goals early on. This can avoid unnecessary prompting and wasteful exploration, ensuring that the LLMs are utilized effectively, especially when used during search (Yao et al. 2023).\nReachability is PSPACE-hard to answer positively in general (Bylander 1994) for a specific fact, since that would require an evidence - a sequence of actions that achieves a state where the specified facts hold. However, generating positive examples is easy, based on any action sequence, taking the facts out of the end state. For negative examples, we explore multiple cases of unreachable facts and fact pairs. First, existing planning methods (under)approximate the reachability with poly-time computable delete-relaxed reachability (Hoffmann and Nebel 2001). Facts that are not delete-relaxed reachable are therefore guaranteed not to be reachable. Another possible reason for a pair of facts that are individually reachable not to be reachable in the same state is if they are mutually exclusive (Lin 2004; Fi\u0161er and Komenda 2018). A simple example of mutually exclusive facts in the ferry domain are (empty-ferry) and (on ?c), meaning that the ferry cannot be empty and at the same time a car is on the ferry. Third, static facts that are not true in the initial state will never"}, {"title": "Action Reachability (AReach)", "content": "become true. For instance, c0 can never become a location, so (location c0) is unreachable (not captured by the methods in the first case, as they focus solely on non-static predicates). The chain of thoughts for a positive example is based on a sequence of actions that achieve the fact. For the negative examples, the chain of thoughts follows the argument laid out above for each of the cases. As in the previous case, the MCQ is captured by choosing from the lists of positive and negative options.\n4. Action Reachability (AReach) In API-driven workflows, the objective is typically presented as an instruction to execute a specific function (Qin et al. 2024). In these scenarios, an LLM must identify the necessary prerequisites for execution and formulate a strategy to meet them. Therefore, it is essential for LLMs to assess whether a given instruction is executable from the provided starting point. We formulate this ability as action reachability task.\nThe action reachability task is closely related to the atom reachability. If an action model is available, then action reachability is equivalent to the atom reachability over the preconditions of the action. Therefore, this task requires an additional reasoning step about action preconditions. Similarly to the atom reachability task, the positive examples are generated from action rollouts, while the negative examples are generated by collecting actions with preconditions including unreachable atoms according to two of the three cases mentioned above delete-relaxed reachability and mutexes. The third case, unreachable static facts, was not used as often creates non-sensible actions board car 10 at location cl. Instead, we added incorrect action templates for each action, like \"board the car c1 at location 10 into the airplane\" or \"drive from location 10 to location 11\u201d. Here as well, the chains of thoughts are created in a similar manner, and the MCQ is captured based on the positive and negative options lists.\n5. Validation (Val) A body of research has advocated the use of LLMs for validation and refinement (Shinn et al. 2023; Gou et al. 2024; Madaan et al. 2023). In line with this research, we propose a Validation task. Here, given an initial state and a goal condition, the objective is to assess whether the specified sequence of actions is valid, applicable, and successfully achieves the intended goal.\nThere are essentially only four options in this case: (a) the sequence is not valid, (b) the sequence is valid, but not applicable, (c) the sequence is valid, applicable, but does not achieve the goal, and (d) the sequence is a plan. These are the four options used for all MCQ for this task. Since the options do not change, we generate four questions per sample, for each of the options to be a correct answer. In the boolean case, we create six different questions, with positive and negative variants for the three cases of whether the sequence is valid, applicable, and a plan. We generate the data for these questions from plans as follows. For the case (c), starting from a plan, we replace a suffix with a random rollout, ensuring that the goal is not achieved at the end of the rollout, but the sequence remains applicable. For the case (b), we try to replace an action on the sequence with an inapplicable action (one whose precondition does not hold in the state), starting from the end of the sequence. Once successful, we return the"}, {"title": "Data Generation", "content": "sequence ending with the inapplicable action. For the case of (a), we simply randomly choose an action on the sequence to replace its template with an incorrect action template, as in the previous task.\n6. Justification (Just) A major criteria for plans to be considered reasonable is whether they include unnecessary actions. In the realm of LLMs and API workflows, it is desirable to avoid calling unnecessary APIs as well as reduce wasteful explorations. Hence, it would be of immense value if LLMs are able to identify whether an action is necessary. This corresponds to the justification task in planning literature.\nThe justification task reasons whether every action is actually needed on the plan. The problem was studied in the literature (Fink and Yang 1992; Salerno, Fuentetaja, and Seipp 2023) and found to be NP-hard in general. However, optimal plans are known to have all their actions being justified and checking whether a single action or a pair of consequent actions can be removed can be done in polynomial time. We consider the following cases, for either a single action or a pair of consequent actions in a plan: 1) a single action can be removed from the plan and the remaining plan is still a valid plan for the same problem 2) an action cannot be removed from the plan 3) the consequent pairs of actions can be removed from the plan 4) the immediate pairs of action cannot be removed from the plan. Note that we truncate the considered plans and only consider two actions after the goal is reached except if the truncation leads to a non-plan. Given a large set of plans, we consider the above four cases, and generate positive and negative examples for both boolean and multiple choice questions.\n7. Landmarks (Land) LLMs have shown to hallucinate or deviate from the task when the trajectory is long (Huang et al. 2024). To alleviate this problem, various work has proposed to use LLMs to decompose the goal into subgoals and achieve each of these subgoals separately. To do this faithfully, it is crucial for LLMs to be able to identify subgoals that are necessary to achieve the goal. In planning literature such subgoals are often called landmarks (Porteous, Sebastia, and Hoffmann 2001). Landmarks are facts that must become true sometime along every plan. So, the last task in ACPBench evaluates LLMs ability to recognize landmarks.\nWhile checking whether a fact is a landmark is PSPACE-hard (Porteous, Sebastia, and Hoffmann 2001), there are several methods that can find a subset of landmarks (Keyder, Richter, and Helmert 2010; Hoffmann, Porteous, and Sebastia 2004; Richter, Helmert, and Westphal 2008; Zhu and Givan 2003). We use the so-called RHW method (Richter, Helmert, and Westphal 2008). Further, negative evidence can be obtained from a collection of plans - a fact that does not appear on all of these plans is not a landmark. We sample from positive and negative examples obtained that way and construct two boolean questions and one MCQ. Here as well, the chains of thoughts generated capture the described logic.\nWe use 25 PDDL problem files of varying sizes per domain. The specific arguments used to generate these problem files can be found in the appendix. These 25 tasks are partitioned"}, {"title": "Experiments", "content": "into a training and a test set. For each task, we use classical planners to generate a large collection of 1000 plans (Katz and Lee 2023; Katz and Sohrabi 2020). With these plans, we sample the state space as follows. First, given a set of plans, we gather the states along these plans. Then, in order to obtain a diverse sample, we run random rollouts from each of the states found. The number of plans and the sample size are configurable parameters. In the landmarks task described above, we also find plans for the sampled states. To do that, we replace the initial state with the sampled state in the planning problem instance and run a top-k planner (Katz and Lee 2023). For finding mutexes, we exploit lifted mutex groups implementations from Fi\u0161er (2020). In this manner, we can potentially generate as many examples as we want. But to keep the test set of reasonable size, we generate only 10 examples per domain, per task.\u00b2\n4.1 Evaluation of pre-trained models\nWe first analyse how existing pre-trained models perform on ACPBench. Table 2 presents the accuracy of all the language models on the 7 ACPBench tasks. These results are mean over 5 runs for all models; except GPT family models and LLAMA-3.1 405B (Dubey et al. 2024), which were run once due to resource constraints. All LLMs were either accessed using API or hosted locally using hugging face transformer library on machines with 2 A100 80GB GPU. Note that accuracy of 50.00 on boolean questions indicates that the performance of the model is as good as a random guess. As all the MCQs in the datasets have 4 options, accuracy less than 25.00 indicates that the performance is worse than random guess. LLMs were evaluated on these questions with two in-context examples and Chain-of-Thought (COT) prompting. An example prompt for the Bool applicability question is shown in Figure 3. Note that we use 2-shot but only show 1-shot in the example due to space constraint.\nNotably, LLAMA-3.1 405B and GPT-40 consistently outperform other models on these tasks, although they do not always achieve the top performance. When it comes to smaller open-sourced models, Codestral 22B stands out for its exceptional performance on boolean questions, while Mixtral 8x7B excels in handling multi-choice questions. However, both of them lag significantly behind GPT-40, which is the best performer in these tasks. Action Reachability and Validation are the most challenging tasks for LLMs. Surprisingly, the GPT family models are not even among top-3 for the action reachablity task. Across all the tasks, GPT-40 performs best for boolean questions and LLAMA-3.1 405B performs best for multi-choice questions.\nFigure 4 displays a domain-wise analysis of the performance of LLMs on multi-choice questions. This analysis showcases the top 8 performing models\u00b3. The average performance of these top-8 models is shown in Fig. 4 as the dotted line in black. This indicates that across models no"}, {"title": "Fine-tuning", "content": "specific domain seems too easy. However, Rovers, Floor-Tile, Blocksworld, Alfworld and Satellite domains pose the greatest challenges to LLMs, in that particular order.\n4.2 Fine-tuning\nFoundational models, and LLMs specifically, have shown to improve performance on specific tasks when they are fine-tuned for those tasks. So, next we investigate if finetuning a language model provides any improvement. For this investigation, we keep aside the following 5 domains, Depot, Goldminer, Satellite, Swap, and Alfworld, and generate a"}, {"title": "Ablations", "content": "3 All supplementary information on the remaining models and boolean questions are relegated to the Appendix to maintain clarity.\nwith two in-context examples (COT 2-shots).\nWe include Granite-code 8B base model, LLAMA-3 70B (one of the top-performing open source model), and the Granite-code 8B finetuned FT model. To have a fair comparison, we use 2-shot examples from the training domains and only compare performance on\n4Examples of prompts are included in the Appendix."}, {"title": "Discussion and Future Work", "content": "Prompt Style From previous section, it is clear that COT 2-shot yields better results than IO prompts for ACPBench tasks. However, it is not clear whether COT or 2-shot examples provide the performance gain. To investigate this, we perform the following ablation study. We compare four prompt styles: (1) IO prompt, (2) chain-of-thought prompt without in-context examples (COT), (3) IO prompt with two in-context examples (IO 2-shots), and (4) chain-of-thought\nGeneralization ACPBench consists of tasks that are crucial for effective, robust and reliable planning. Improving performance on ACPBench should improve LLM's ability to reason about these tasks, and hence should improve LLM's ability to generate plans. To verify this hypothesis, we compare the Granite-code Base 8B model and Granite-code finetuned 8B model on plan generation task (t1) in PlanBench (Valmeekam et al. 2023a). Table 5 presents the results. Granite finetuned model, which was QLoRA (Dettmers et al. 2023) trained on ACPBench tasks for 8 training domains, shows improvement on plan generation ability.\nPerformance over time One of our major motivations to generate and release the ACPBench collection is to encourage researchers to address the poor performance on these tasks and build models that are capable to perform reasoning required for better planning. We believe that without such benchmarks, the progress toward this goal is ad-hoc. To verify our belief, we perform a small ablation, where we compare LLAMA family of models that were released over last 6 months to see if there is any improvement on ACPBench tasks over time. Figure 6 presents the performance of LLAMA-2 (70B), LLAMA-3 (70B) and LLAMA-3.1 (70B and 405B) on ACPBench tasks. We see that there is a significant jump in performance between LLAMA-2 and LLAMA-3. However, the difference in performance of LLAMA-3 70B and LLAMA-3.1 405B is not significant. This highlights the need for benchmarks that systematically captures the reasoning ability required for planning.\nIn this work, we introduce ACPBench-a collection of datasets to evaluate the ability of LLMs to reason about action, change and planning. By evaluating 22 state-of-the-art LLMs of varying size, we find these models to underperform, even the largest ones, especially on tasks like plan validation and action reachability. On the other hand, we show that finetuning a small language model, Granite 8B, can improve its reasoning ability to bring it on par with the best performing models. Further, we observe that the fine-tuned model ex-"}, {"title": "Appendix", "content": "A Appendix\nA.1 ACPBench Task Examples\nTable 6 exemplified the domain description, the problem description and the goal description for each domain. For each task we use all or subset of these descriptions as context. Table 7 indicates what is included as part of the context for each of the tasks and also provide one example of boolean and multi-choice questions each.\nA.2 Pretrained LLMs\nOur paper presents domain-wise performance of few selected models in Fig 4. We only presented results for the MCQ questions due to space constraints. Performance of all the 22 pretrained LLMs for both boolean and multi-choice questions is presented in Table 8. Table 2 presents accuracy values with 2-shot COT prompting. We also attempted 0-shot Input Output prompt, the trends were similar. These results aggregated over 5 runs are presented in Table 9.\nA.3 Finetuned LLM\nTables 10 and Table 11 show per-domain comparision of the 7 tasks between the Granite (code 8B) Base model and the finetuned model, on multiple choice questions and boolean questions respectively. The \"Diff\" column shows the average gain in performance. Generally we may see a greater performance gain in the seen domains as they have been included in the training set as oppose to the unseen domains. Note, due to memory limitations we were not able to test the Alfworld domain on action reachability.\nIn Tables 3 and 4, we compare te finetuned model against the best performing model (last column). These values are obtained by looking at the aggregated performance on train and test domains respectively. We present performance of all the models on train and test domains in Tables 12 and 13.\nA.4 Ablation: Prompt Style\nIn our paper, we present an ablation to compare prompting styles. For that analysis; we presented results on test domains for multi-choice questions in Fig 5. Here, in Fig. 7, we present performance on boolean questions.\nWe also compared the prompt-style on LLAMA 3.1 405B model. Figure 8 presents aggregated results on 7 ACPBench tasks for all the domains. Although we were only run this experiment once due to resource constraint, we see that COT and IO 2-shot has significant different in performance. The difference is IO 2-shot vs COT 2-shot is significant."}]}