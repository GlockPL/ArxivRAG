{"title": "Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation", "authors": ["Ahmad S\u00fcleyman", "G\u00f6ksel Biricik"], "abstract": "Large-scale text-to-image (T2I) diffusion models have demonstrated an outstanding performance in synthesizing diverse high-quality visuals from natural language text captions. Multiple layout-to-image models have been developed to control the generation process by utilizing a broad array of layouts such as segmentation maps, edges, and human keypoints. In this work, we present ObjectDiffusion, a model that takes inspirations from the top cutting-edge image generative frameworks to seamlessly condition T2I models with new bounding boxes capabilities. Specifically, we make substantial modifications to the network architecture introduced in ContorlNet to integrate it with the condition processing and injection techniques proposed in GLI-GEN. ObjectDiffusion is initialized with pretraining parameters to leverage the generation knowledge obtained from training on large-scale datasets. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model achieves an AP50 of 46.6, an AR of 44.5, and a FID of 19.8 out-performing the current SOTA model trained on open-source datasets in all of the three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding abilities on closed-set and open-set settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple objects of different sizes and locations.", "sections": [{"title": "1. Introduction", "content": "Image generation is one of the most advanced research sub-fields of the relentlessly evolving Generative Artificial Intelligence (GenAI) and Deep Learning (DL). Thanks to its transformative practical applications, Generative Adversarial Networks (GANs) [14] have succeeded in drawing considerable attention both in academia and industry. Diffusion-based powerful image generation models [1, 38, 50, 53] are capable of synthesizing stunningly detailed realistic images of high-quality that capture diverse visual concepts.\nOne major challenge that hinders the widespread adoption of generative AI in general and image generation models in particular is the difficulty of customization. Customization is an active research topic that aims to steer the image generation process towards producing controllable content that adheres to user-specified requirements. While text prompts [32, 41] are the most common type of input, it is deemed inadequate in cases that necessitate a high degree of content and style control in the synthesized image. Particularly, the image caption falls short in providing a precise description of the sizes, colors, shapes, textures, spatial alignments, and interactions between the objects present in the synthesized image. This limitation prompted computer vision researchers to adopt and integrate more effective types of control formats, in conjunction with the naive caption to achieve a greater degree of controllability over the output scenes. Segmentation maps [76], normal maps [64], depth maps [48], edge maps [4, 70], human keypoints [5], bounding boxes [49], user scribbles, cartoon drawings, image styles or a combination of them are among the well-established conditioning modalities that can be leveraged to tailor the image generation.\nIn this work, we present ObjectDiffusion, a conditional image generative framework that augments text-to-image models (T2I) [36, 47] by incorporating new control capabilities. ObjectDiffusion can condition T2I models on object detection [49] annotations. That is, in addition to the text prompts, we condition T2I models on object labels and their corresponding bounding boxes. The object labels are not limited to a predefined set of categories, rather they can be in open-ended text format, for example, \"a stripped pink full face motorcycle helmet\". ObjectDiffusion consists of two pretrained networks. The first is a locked text-to-image Stable Diffusion (SD) [50] model pretrained on"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Diffusion Models", "content": "In 2015, Sohl-Dickstein et al. [58] established the theoretical foundation of the probabilistic diffusion model, a framework that works by iteratively adding random noise to the ground-truth images until all the structure is lost, and then learns how to denoise the perturbed images to restore their original structures through recursive runs of noise prediction neural network $\\epsilon_{\\theta}(x_t,t)$ over T time-steps. Alternative to noise prediction, score-based [59] generative models $s_{\\theta}(x)$ sample new instances by estimating the gradients of the log probability distribution of the data $\\nabla \\text{log }P_{data}(x)$. Diffusion models and score-based generative models can be formalized under a unified framework [60].\nDhariwal et al. [8] demonstrated that probabilistic diffusion models [58] have outperformed the prominent GANS [14] both in quality and diversity metrics. Rombach et al. [50] presented Stable Diffusion (SD), a remarkable work that denoises the image in latent space in order to reduce the computational and memory requirements of operating in pixel-level space."}, {"title": "2.2. Control Layout Overview", "content": "Conditional models [28, 35, 45, 68, 72, 73, 75] empower image generative models, for the most part text-to-image models, to produce controllable outputs by extending them to accept single or multiple external layout maps, generally called conditions. Scribbles and sketches are simple and vague types of conditions that can be created conveniently by users. Segmentation maps [76], on the other hand, offer a granular control over the image content by dictating a pixel-level condition map of the image lattice. Hough lines [15] and Canny edges [4] can condition generative models to capture subtle texture and fine-grained details that can not be specified via bounding boxes and segmentation maps. Keypoints [5] is a format that allows creating images from skeletal structures that likely depict human postures. Advanced types of control formats include normal maps [64] and depth maps [48] both of which challenge the generative models by widening the conditioning task scope to integrate information about the perceived depth of images."}, {"title": "2.3. Conditional Image Generative Models", "content": "T2I models utilizes image caption to guide the generation process. Although simple and powerful, text prompt alone is insufficient for tailoring the generation process into creating customized outputs. To address this limitation, various layout-to-image systems [21, 29, 61, 63, 71, 74] have been proposed to use more advanced conditional modalities to enable a fine-grained control. Conditional models render complex images with style and content complying with the control layout. PITI [67] is a layout-to-image diffusion model that is trained from scratch on conditional tokens. Training generative models from scratch is inefficient and entails a very high training cost. T2I-Adopters [35] are light-weight neural network blocks borrowed from NLP [19] for facilitating fine-tuning huge T2I models on various types of guiding inputs like depth, segmentation, keypoints, sketch, and color maps. Some diffusion-based methods like ReCo [72] fine-tune a pretrained text-to-image model on bounding box and open-ended text information, however this continuous learning may cause a catastrophic loss of generation knowledge [20, 52].\nTo circumvent this, GLIGEN [28] proposed to freeze the weights of a pretrained SD model [50] and insert trainable gated self-attention layers [65] in the attention blocks"}, {"title": "2.4. Training-Free Conditional Methods", "content": "Traditional conditional models require computationally expensive training on a dataset of image-control pairs. Training-free [11, 12, 27, 29, 61, 71, 74] methods circumvent this by performing the conditioning on pretrained T2I models during the sampling process eliminating the requirement for training or fine-tuning. BoxDiff [69] achieves the positional alignment effect by manipulating specific regions of the cross-attention map that correspond to the constraining boundary box regions. Guidance-based methods can achieve control by designing task-specific guidance procedures. For example, Layout Control Guidance [6] applies forward and backward guidance losses on the attention map to steer the sampling towards generating object-level control. Universal Guidance [3] offers a solution to avoid training a network on noisy data by leveraging available pretrained models like semantic segmentation and object recognition neural networks. The goal is to carefully design and minimize the loss function $l(c, f (z_t))$ between the guidance function f (\u00b7) of the latent image z and the control module c.\nDespite their success, training-free control approaches prolong sampling time and decrease perceptual quality, and as a result, they are usually used in combination with regular training-based conditional models."}, {"title": "3. Method", "content": "For controllable image generation, we introduce ObjectDiffusion, a framework that takes inspirations from two remarkable conditional models GLIGEN [28] and Control-Net [73] to seamlessly augment open-source text-to-image generation diffusion models on control layout. Concretely,"}, {"title": "3.1. Entity Grounding Layout", "content": "Due to its simplicity and effectiveness in controlling the generation process, we focus on conditioning diffusion-based models on object detection annotations. The object detection serves as a condition to ground text-to-image frameworks into rendering specific objects in specified regions of the synthesized images. In particular, our control layout consists of a set of grounding tokens, each of which is composed of one-to-one pairs of semantic and spatial information. The semantic instruction is a textual description of the object, for example, \"A shiny luxury red sports car\". Each object in the image can be defined in an open-ended text format [72], rather than being limited to a predefined set of labels like COCO categories [30]. For spatial instruction, each entity is attributed by a single 2D rectangular box which determines the location of the object in the image. The box is represented in the form of an object detection rectangular box, that is, a list of two points needed to construct the detection box $[x_1, y_1, x_2, y_2]$ where $(x_1,y_1)$ is the upper-left point and $(x_2, y_2)$ is the lower-right point. While the objects of the image are defined by the control layout, the overall image outline is specified by a single global im-"}, {"title": "3.2. Fusing Conditional Entities Overview", "content": "Processing Grounding Semantic Tokens: Conditional semantic tokens are strings of varying length consisting of single or multiple words. These object-level text captions are processed similar to the regular image caption. Specifically, they are encoded using a pretrained CLIP ViT-L/14 [46] text encoder. The CLIP encoder denoted as $\\text{Etext}(e)$ embeds the entity e into a tensor feature of length 768.\nProcessing Grounding Spatial Tokens: We embed the spatial information into an expressive periodic tensor via a Fourier embedding [34] block. The mathematical representation of Fourier transformation $F(\\cdot)$ of a box b = $[x_1, y_1, x_2, y_2]$ is expressed as the following:\n$F_i(b) = [\\text{sin } (f_i \\cdot x_1), \\text{cos } (f_i \\cdot x_1),...,\\\\ \\text{sin } (f_i \\cdot y_2), \\text{cos } (f_i \\cdot y_2)]$\n$F(b) = [F_0(b), F_1(b), ..., F_{M-1}(b)],$\\\nwhere i $\\in$ {0, 1, 2, . . ., \u041c \u2013 1}\nwhere M is the number of frequencies, and $F_i(b)$ is Fourier transformation at the $f_i$ frequency. The output of embedding F(b) is 4 (box coordinates) * 8 (frequency) * 2 (sin & cos) = 64-dimensional tensor per bounding box. The Fourier embedder does not define any trainable parameters.\nConcatenating Conditional Features: For each of the grounding entity, we first concatenate the encoded text entity $\\text{Etext}(e)$ with the embedded positional information F(b) into a single tensor of size 832. Then we feed this tensor as an input to a Multilayer Perceptron (MLP) [43] consisting of three fully connected linear layers of sizes (832, 515, 768) and a Sigmoid-weighted Linear Unit (SiLU) [10] :\n$\\text{o}_i = \\text{MLP } (\\text{Etext}(e), F(b))$\n$\\text{o} = [\\text{o}_1, \\text{o}_2, ..., \\text{o}_N]$, where i $\\in$ {1,2,...,N}\nThe MLP transforms the concatenated grounding feature vectors from 832 to a transformer vector of of standard size 768. Assuming that the maximum number of entities allowed per image is a hyperparameter N, the final control vector o has the size (N, 768). Figure 1 illustrates the processing of grounding information and caption.\nIntegrating Conditional Features: To inject the extracted control features o into the standard transformer blocks of LDM [50], we follow GLIGEN and insert a new gated self-attention SA (\u00b7) layer between the self-attention and the cross-attention of the transformer block [2]. The gated self-attention layer takes both the intermediate latent visual features v and the external control features o as inputs and integrates them as shown in the equation:\n$v = v + \\text{tanh}(y) \\cdot SA ([v, o])$"}, {"title": "3.3. Model Architecture", "content": "Our model architecture is illustrated in Figure 2. We start with a pretrained Stable Diffusion (SD) [50] model that enables conditioning image generative diffusion models on image captions but does not support object detection data defined in the Section 3.1 as a grounding input. We lock the whole SD model and clone the encoder blocks and middle block, but not the decoder blocks, from the pretrained model into a trainable network [73]. This allows us to leverage the generation knowledge learned from training on a massive dataset. To equip our model with conditional capabilities, we introduce some modifications to the copied network structure. In particular, we insert gated self-attention (GSA) layers, implemented by $\\text{tanh}(y)$ [9], into each of the attention blocks of the trainable network [28]. We call our modified trainable network GroundNet because it empowers ObjectDiffusion to be grounded on the detection layout by mapping the grounding features into the baseline model. GroundNet is connected to the SD baseline via zero-convolution layers [37], implemented as 1\u00d71 zero-initialized convolutional layers. Both the gating mechanism of the GSAs and the zero-convolution layers are important to prevent the noise caused by the new grounding features from damaging the weights of the trainable GroundNet. This is especially important in the initial training phase where noise can harm the pre-training generation knowledge.\nObjectDiffusion is similar to ControlNet in that they both freeze the pretrained SD model and fine-tune a parallel network to adapt to a new control layout. Also, both use zero-convolutional layers to connect the trainable network with the frozen baseline network. However, ObjectDiffusion and ControlNet differ in various aspects. First, ControlNet fine-tunes an exact copy of the locked SD blocks, while ObjectDiffusion modifies the cloned SD blocks with a gated self-attention mechanism which enables the integration of the grounding features. Second, ControlNet adds the extracted condition features to the visual features and injects the concatenated features into the first trainable SD encoder only. In contrast, ObjectDiffusion implements a multi-scale control injection, where the control features are injected into each of the encoder layers and the middle layer of GroundNet. The multi-scale control injection strategy reinforces the control signal and is shown to produce a superior output compared with the standard single injection [75]. Finally, ControlNet processes the control input using an auxiliary convolutional network. Conversely, ObjectDiffusion processes the condition input via CLIP text encoder and Fourier Embedder as laid out in Section 3.2. It is worth noting that the object detection layout presented in Section 3.1 is not among the supported control layouts by the original ControlNet."}, {"title": "3.4. Learning Objective", "content": "Diffusion models [58] learn the distribution of the training data by maximizing the log likelihood. Concretely, given a noised version of an image $x_t$, equivalent to $z_t$ in latent space, and the number of steps t at which the original image $x_o$ was distorted, a diffusion neural network is trained to predict the amount of noise $\\epsilon$ added to the image [18]. Accordingly, the training objective is a weighted variational bound that can be interpreted as a simple L2 loss function between the noise added during the diffusion process and the noise predicted by the model:\n$\\min L_{DM} = E_{x,t,\\epsilon ~N(0,1)} [||\\epsilon - f_{\\theta} (x_t, t)||^2]$\nwhere $\\epsilon$ is the ground-truth noise, $\\theta$ is the learnable model parameters, and $f_{\\theta}$ is the predicted noise.\nWe condition the latent diffusion models $f_{\\theta} (z_t,t)$ on two external inputs, the text caption c and object detection tokens g expressed as:\n$g = [(\\text{e}_1, b_1),......,(\\text{e}_n,b_n)]$\nwhere $e_i$ and $b_i$ are the semantic information and the positional information, respectively, corresponding to the ith grounding object.\nAs discussed in Section 3.3, our proposed framework consists of two networks. The first is a frozen T2I diffusion network $f_{\\theta} (z_t, t, c)$ [50] which conditions the model in text caption e but not in detection tokens g. The second is a trainable GroundNet $f_{\\theta'} (z_t, t, c, g)$ which conditions both on caption c and detection tokens g. All components of this module are trainable. The overall noise prediction of our model can be formalized as follows:\n$f_{\\theta,\\theta'} (z_t, t, c, g) = f_{\\theta} (z_t, t, c) + Z (f_{\\theta'} (z_t, t, c, g))$\nwhere Z denotes the zero-convolutional layers. For simplicity, we do not show the weights of Z and assume that the weights of the zero-convolution layers are contained within the weights \u03b8'.\nOur learning objective minimizes the L2 loss function that measures the difference between the noise $\\epsilon$ added to the latent image in the forward process and the noise predicted by the model $f_{\\theta,\\theta'} (z_t, t, c, g)$:\n$\\min L = E_{z,t,c,g,\\epsilon ~N(0,1)} [||\\epsilon - f_{\\theta,\\theta'} (z_t, t, c, g)||^2]$"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Dataset: we fine-tune and evaluate our model on Common Object in Context (COCO) [30] dataset. We use the COCO2017 training split of annotated 118K images for fine-tuning purpose and use the COCO2017 validation split of 5k annotated images for evaluation. The median image ratio is 640x480, and each image in the COCO2017 dataset has 5 captions and an average of 7 objects labels and bounding boxes, classified into 80 simplified categories.\nPreprocessing Pipeline: We directly resize with Bicubic [22] interpolation to the desired image size 512\u00d7512 pixels, and unlike GLIGEN we do not crop the image at all. The Figure 3 illustrates the difference between our image preprocessing and GLIGEN preprocessing. We randomly flip the image 50% of the time and drop the object grounding annotations when their bounding box area counts for less than 1% of the image area. Additionally, we only keep the 30 objects that have the largest areas. The bounding boxes are converted from COCO format $[x_{min}, y_{min}, w, h]$ to the format $[x_{min}, y_{min}, x_{max}, y_{max}]$.\nInitialization: Initializing our model weights from pretrained models enables us to capitalize on and build upon the extensive knowledge acquired through prior training. We initialize the text-to-image model using Stable-Diffusion-v-1-4 [7] due to its compatibility with GLIGEN which allows for a fair comparison to the SOTA model. Stable-Diffusion-v-1-4 is extensively trained on high-resolution subset of the large-scale LAION-5B [54] dataset. We initialize GroundNet from GLIGEN's diffusers-generation-text-box checkpoint [13] which is pretrained on manually annotated object names and bounding boxes from the Object365 [56], Flickr [42] and Visual Genome (VG) [25] datasets in addition to object names and bounding boxes extracted via GLIP [26] from the SBU [39], and CC3M [57] datasets but with no fine-tuning on the COCO [30] dataset. We freeze the pretrained CLIP ViT-L/14 [46] text encoder during the whole training process. ObjectDiffusion contains 1.32 billion parameters, 460.8M of them are trainable parameters and 860M frozen parameters.\nImplementation Details: We fine-tune ObjectDiffusion for 100k iterations with a learning rate 5 \u00d7 10-5 on a single NVIDIA QUADRO GV100 32 GB GPU using Adam [23] optimizer. We set the warmup iterations to 4k and apply no weight decay. We randomly choose 1 out of the 5 available COCO captions. To ensure that the training is robust, we randomly drop the caption and randomly drop the condition input, each with a 10% independent probability.\nWe utilize some techniques to overcome the limitations in GPU memory and computational capacity. First, we implement the Automatic Mixed Precision (AMP) [33], a smart quantization technique that assigns multiple precisions to different modules of the same model according to their tolerance of low precision. AMP delivers a 109% speedup in training. Second, we employ Gradient Accumulation [24], that is, we train with a batch size of 4 but update the model weights with the normalized accumulated gradients once every 4 iterations. Gradient Accumulation allows us to quadruple the batch size from 4 to 16 which smooths the learning curve.\nEvaluation Benchmarks: We use the COCO2017 evaluation set to quantitatively and qualitatively assess Object-"}, {"title": "4.2. Quantitative Results", "content": "Table 1 details the Average Precision (AP, AP50, AP75) [40] metrics and Average Recall (AR) [40] metric between the YOLOv8m [49] predictions and ground truth attributes. ObjectDiffusion achieves 27.4, 46.6 28.2, and 44.5 in AP, AP50, AP75, and AR, respectively.\nOur model surpasses the state-of-the-art model [28] in AP50 score by more than 10% and obtains competitive AP and AP75 scores surpassing most of the layout-to-image models including LostGAN-V2 [62], LAMA [29], TwFA [71], and GLIGEN [28]. Our high AP scores indicate that ObjectDiffusion succeeded in learning from ground information and was able to synthesize images in a high alignment with the control input. Our model was able to increase the zero-shot AP score of GLIGEN (zero-shot) from 19.1 to 27.4 and the score of the AP50 from 30.5 to 46.6 which indicates a successful training on the COCO2017 [30] dataset. The results also show that our approach circumvents the problem of \"knowledge forgetting\" that typically hinders fine-tuning layout-to-image models.\nIn Table 2 we calculate the AP and AP50 score values per class and report the first three classes with the highest AP values and the first three classes with the lowest AP values. The classes \u201ccat\u201d, \u201cbear\", and \"dog\" are the top classes where our model was able to ground with a very high precision. On the other hand, \u201cbook\u201d, \u201chair drier\", and \"sports ball\" are the classes that confound our model the most. Unexpectedly, the effectiveness of grounding an object category does not correlate with the category frequency in the dataset. For example, the category \u201cbook\u201d is overrepresented in the COCO2017 dataset, however it has a very low AP value. Similarly, the \"cat\", \"dog\", and bear categories are underrepresented in the COCO2017 dataset, but they have the highest AP values.\nTable 3 shows that our model achieves the highest reported AR score 44.5 only 5 points behind the upper bound value. The high AR value suggests that ObjectDiffusion rarely overlooks objects which is a common issue that text-to-image models suffer from.\nFinally, to measure the quality of our model generation, we calculate the FID [17] scores in Table 4. ObjectDiffusion achieves a FID score of 19.8 outperforming the state-of-the-art model GLIGEN (fine-tuned), GLIGEN, and the GLIGEN (zero-shot) by 6%, 8% and 27%, respectively. The improvement in FID score proves the efficiency of our proposed training architecture in learning the generation capabilities for high-quality image synthesis.\nIt's worth noting that our evaluation is limited to models trained on publicly available datasets."}, {"title": "4.3. Qualitative Results", "content": "We visually demonstrate the capabilities of ObjectDiffusion to ground on boxes and entities. We test our model under both closed-set vocabulary entity names and open-set vocabulary entity names. Our qualitative testing is designed to encompass various entities such as animals, persons, vehicles, home stuffs, and sports in indoors and outdoors contexts. To verify our model ability to generate satisfactory images from challenging requirements we vary the bounding box sizes, the bounding box locations, the number of objects per image, and the length of object names.\nClosed-set Setting Evaluation: Figures 6 to 10 display representative images under closed-set vocabulary setting. All closed-set images are synthesized with COCO2017 validation set annotations. Each figure showcases a specific general concept like people, animals, vehicles, and rooms.\nFigure 6 various photo-realistic locations from a standard home showing the ability of ObjectDiffusion to synthesize high-quality, precise images from complex layouts. For example, the layout of the image (c) in the first row of Figure 6 is an object-intense visual of living room furniture. That is, the layout contains 19 size-varying bounding boxes labeled with 8 entity classes such as \u201ccouch\", \u201cchair\", \"vase\", and \"tv\".\nFigure 7 shows a collection of land vehicles, railway vehicles, and watercraft. The second row of Figure 7 demonstrates our model's capabilities in adapting to bounding boxes with varying aspect ratios. ObjectDiffusion successfully accommodated the \u201cmotorcycles\" and the \u201cpersons\u201d in each of the three instances in respect to the grounding entity locations and sizes. Additionally, the output images adhered to the image caption, that is, placing the \"motorcycle\" on a paved track in the left image, on a dirt track in the middle image, and on a highway in the right image.\nIn Figure 8 we list some synthesized images of people in formal and casual situations. ObjectDiffusion renders photo-realistic portraits with authentic facial appearance. We notice that in image (c) of Figure 8 the man is wearing sunglasses but is missing the black hat specified in the prompt. We think that the model may not have seen enough suit wearing men with hats on their heads. We also notice, in the same image, that the \"tie\" is not 100% positioned within the box boundaries. We also observe that the hands of people in the (b) and (c) images are not perfect. These constraints are inherited from the pretrained text-to-image model weights that we initialized our model from.\nWe exhibit instances of domestic and wild animals in Figure 9 where we demonstrate artificial images of dogs, cats, horses, and bears. The high grounding precision of qualitative examples of different kinds of animals are in line with the high AP scores obtained in animal categories reported in Table 2.\nThe last examples of visuals generated under closed-set vocabulary settings are collected in Figure 10. This figure depicts images of teddy bears, vases, food, and sport activities.\nObjectDiffusion can ground instances from a single grounding box or multiple grounding boxes, like in the \"teddy bear\" instances. We can utilize image prompt as a global condition that describes the overall surroundings and background. For example, the images (b) and (c) in Figure 10 show a \"teddy bear\" laying on a wall and \"teddy bears\" sitting on a sofa. In some cases, our model may overlook the colors or the materials of the conditional objects, like the color of the \"vase\u201d in image (f). ObjectDiffusion grounding ability is not limited to static objects. The last row of Figure 10 illustrates that our model can actually render objects in dynamic moving scenes like skiing and surfing in (j), (k) and (l) examples.\nOpen-set Setting Evaluation: We evaluate ObjectDiffusion performance in grounding the generation process on entity names that are not present in training data. To do so, we manually craft the caption, regional, and semantic grounding inputs using instance names outside the 80 COCO2017 classes. In Figure 11 we provide three examples per annotation.\nThe first row (a) demonstrates image examples of a modern house in Scandinavian style. We can use the category names from the 80 COCO categories but with modifier attributes preceding the name, such as a \u201ccomfy couch\", a"}, {"title": "4.4. Limitations and Failures", "content": "ObjectDiffusion is able to synthesize high quality scenes, however sometimes struggles in capturing the intricate fine-grained details of faces, hands, and feet [51]. Faces are highly detailed, complex structures that exhibit a high degree of variability. Image (a) in Figure 5 is an example of a young man with distorted face and hands.\nSimilar to the limitation in face synthesis, ObjectDiffusion manifests an imperfection in rendering images that contains text. We notice that our model repeatedly failed in rendering these images, even though our fine-tuning dataset contains an abundance of images featuring the embedded text. We can notice from the image (b) in Figure 5 our model failed in overlaying a short single-word text on the stop sign.\nObjectDiffusion may overly attend to the control signals, that is, the bounding boxes, which may lead to an unconventional placement of entities. For example, the image (c) in Figure 5 shows a \u201cdog\u201d sitting on top of a \u201cvehicle\u201d. While the objects are rendered in the correct regions specified via the bounding boxes, the legs of the \u201cdog\u201d are not touching the \u201ctruck\u201d, resulting in an absurd scene."}, {"title": "5. Discussion and Conclusion", "content": "In this work, we have taken inspiration from the two prominent works, ControlNet and GILGEN to propose a new model ObjectDiffusion. Our model learns to ground T2I models on entity name and bounding box annotations. ObjectDiffusion exceeds the current state-of-the-art GLIGEN (fine-tuned) trained on open-source datasets in AP50 and AR metrics. Our model also scores higher than the top-scoring layout-to-image baselines. In the FID quality metric, our model surpasses the top scores of both specialized layout-to-image and diffusion-based models conditioned on COCO2017 object detection annotations. Qualitative evaluation demonstrates the ability of ObjectDiffusion to synthesize diverse high-quality, high-fidelity images that seamlessly adhere to the conditional inputs."}]}