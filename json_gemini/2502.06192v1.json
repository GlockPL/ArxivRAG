{"title": "Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation", "authors": ["Guanglong Sun", "Hongwei Yan", "Liyuan Wang", "Qian Li", "Bo Lei", "Yi Zhong"], "abstract": "Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact \"student\" model from a large \u201cteacher\" model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named spacing effect in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD). We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNS (e.g., the performance gain is up to 2.31% and 3.34% on Tiny-ImageNet over online KD and self KD, respectively).", "sections": [{"title": "1. Introduction", "content": "Knowledge distillation (KD) is a powerful technique to transfer knowledge between deep neural networks (DNNs) (Gou et al., 2021; Wang & Yoon, 2021). Despite its extensive applications to construct a more compact \u201cstudent\" model from a converged large \"teacher\" model (aka offline KD), there have been many recent efforts using KD to promote generalization of the model itself, such as online KD (Zhang et al., 2018; Zhu et al., 2018; Chen et al., 2020) and self KD (Zhang et al., 2019; Mobahi et al., 2020). Specifically, online KD simplifies the KD process by training the teacher and the student simultaneously, while self KD involves using the same network as both teacher and student. However, as these paradigms can only moderately improve learning performance, how to design a more desirable KD paradigm in terms of generalization remains an open question.\nCompared to DNNs, biological neural networks (BNNs) are advantageous in learning and generalization with specialized adaptation mechanisms and effective learning procedures. In particular, it is commonly recognized that extending the interval between individual learning events can considerably enhance the learning performance, known as the spacing effect (Ebbinghaus, 2013; Smolen et al., 2016). This highlights the benefits of spaced study sessions for improving the efficiency of learning compared to continuous sessions, and has been described across a wide range of species from invertebrates to humans (Beck et al., 2000; Pagani et al., 2009; Menzel et al., 2001; Anderson et al., 2008; Bello-Medina et al., 2013; Medin, 1974; Robbins & Bush, 1973).Taking human learning as an example, the spacing effect could enhance skill and motor learning (Donovan & Radosevich,\n1999; Shea et al., 2000), classroom education (Gluckman\net al., 2014; Roediger & Byrne, 2008; Sobel et al., 2011),\nand the generalization of conceptual knowledge in chil\ndren (Vlach, 2014).\nInspired by biological learning, we propose to incorporate\nsuch spacing effect into KD (referred to as Spaced KD, see\nFig. 1) as a general strategy to promote the generalization\nof DNNs (see Fig. 2). We first provide an in-depth the\noretical analysis of the potential benefits of Spaced KD.\nCompared to regular KD strategies, the proposed Spaced\nKD helps DNNs find a flat minima during stochastic gra\ndient descent (SGD) (Sutskever et al., 2013), which has\nproven to be closely related to generalization. We then\nperform extensive experiments to demonstrate the effective\nness of Spaced KD, across various benchmark datasets and\nnetwork architectures. The proposed Spaced KD achieves\nstrong performance gains (e.g., up to 2.31% and 3.34% on\nTiny-ImageNet over regular KD methods of online KD and\nself KD, respectively) without additional training costs. We\nfurther demonstrate the robustness of the space interval, the\ncritical period of the spacing effect, and its plug-in nature to\na broad range of advanced KD methods.\nOur contributions can be summarized as follows: (1) We\ndraw inspirations from the paradigm of biological learning\nand propose to incorporate its spacing effect to improve\nonline KD and self KD; (2) We theoretically analyze the\npotential benefits of the proposed spacing effect in terms\nof generalization, connecting it with the flatness of loss\nlandscape; and (3) We conduct extensive experiments to\ndemonstrate the effectiveness and generality of the proposed\nspacing effect across a variety of benchmark datasets, net\nwork architectures, and baseline methods."}, {"title": "2. Related Work", "content": "Knowledge Distillation (KD). Representative avenues of\nKD can be generally classified into offline KD, online KD,\nand self KD, based on whether the teacher model is pre\ntrained and remains unchanged during the training process.\nOffline KD involves a one-way knowledge transfer in a two\nphase training procedure. It primarily focuses on optimizing\nvarious aspects of knowledge transfer, such as designing the\nknowledge itself (Hinton et al., 2015; Adriana et al., 2015),\nand refining loss functions for feature matching or distri\nbution alignment (Huang & Wang, 2017; Asif et al., 2019;\nMirzadeh et al., 2020b). In contrast, online KD simplifies\nthe KD process by training both teacher and student simul\ntaneously and often outperforms offline KD. For instance,\nDML (Zhang et al., 2018) implements bidirectional distilla\ntion between peer networks. For self KD, the same network\nis used as both teacher and student (Zhang et al., 2019; Das\n& Sanghavi, 2023; Mobahi et al., 2020; Zhang & Sabuncu, 2020; Yang et al., 2019; Lee et al., 2019). In this paper,\nthe self KD we refer to is the distillation between different\nlayers within the same network (Zhang et al., 2019; Yan\net al., 2024; Zhai et al., 2019). However, existing methods\nfor online KD and self KD often fail to effectively utilize\nhigh-capacity teachers over time, making it an intriguing\ntopic to further explore the relationships between teacher\nand student models in these environments.\nAdaptive Distillation. Recent studies have found that the\ndifference in model capacity between a much larger teacher\nnetwork and a much smaller student network can limit dis\ntillation gains (Liu et al., 2020a; Cho & Hariharan, 2019;\nLiu et al., 2020b). Current efforts to address this gap fall\ninto two main categories: training paradigms (Gao et al.,\n2018) and architectural adaptation (Kang et al., 2020; Gu &\nTresp, 2020). For instance, ESKD (Cho & Hariharan, 2019)\nsuggests stopping the training of the teacher early, while\nATKD (Mirzadeh et al., 2020a) employs a medium-sized\nteacher assistant for sequential distillation. SHAKE (Li &\nJin, 2022) introduces a shadow head as a proxy teacher for\nbidirectional distillation with students. However, existing\nmethods usually implement adaptive distillation by adjust\ning teacher-student architecture from a spatial level. In con\ntrast, Spaced KD provides an architecture- and algorithm\nagnostic way to improve KD from a temporal level.\nFlatness of Loss Landscape. The loss landscape around\na parameterized solution has attracted great research atten\ntion (Keskar et al., 2016; Hochreiter & Schmidhuber, 1994;\nIzmailov et al., 2018; Dinh et al., 2017; He et al., 2019).\nA prevailing hypothesis posits that the flatness of minima\nfollowing network convergence significantly influences its\ngeneralization capabilities (Keskar et al., 2016). In general,\na flatter minima is associated with a lower generalization\nerror, which provides greater resilience against perturba\ntions along the loss landscape. This hypothesis has been\nempirically validated by studies such as (He et al., 2019).\nAdvanced advancements have leveraged KD techniques to\nboost model generalization (Zhang et al., 2018; Zhao et al.,\n2023; Zhang et al., 2019). Despite these remarkable ad\nvances, it remains a challenging endeavor to fully under\nstand the impact of KD on generalization, especially in\nassessing the quality of knowledge transfer and the efficacy\nof teacher-student architectures."}, {"title": "3. Preliminaries", "content": "In this section, we first present the problem setup and some\nnecessary preliminaries of KD. Then we describe the spac\ning effect in biological learning and discuss how it may\ninspire the design of KD."}, {"title": "3.1. Problem Setup", "content": "We describe the problem setup with supervised learning of\nclassification tasks as an example. Given N training samples\n$\\mathcal{D}_{\\text{train}} = \\{(x_i, y_i)\\}_{i=1}^N$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$, the\nneural network model $f_\\theta(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^C$ with parameters $\\theta\\in\\mathbb{R}^P$ is optimized by minimizing the empirical risk over $\\mathcal{D}_{\\text{train}}$\nand evaluated over the test dataset $\\mathcal{D}_{\\text{test}}$. Using the SGD\noptimizer (Sutskever et al., 2013), $f_\\theta(\\cdot)$ is updated for each\nmini-batch of training data $B_t = \\{(x_i, y_i) \\in \\mathcal{D}_{\\text{train}}\\}_{i\\in \\mathcal{I}_t}$,\n$\\mathcal{I}_t \\subseteq \\{1,2,... N\\}$:\n$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\mathcal{B}} \\sum_{i \\in \\mathcal{I}_t} \\nabla_\\theta L_i(\\theta_t),$\nwhere $L_i(\\theta) = l_{\\text{task}}(f_\\theta(x_i), y_i)$ is a task-specific supervision loss. $\\eta$ and $\\mathcal{B} = |\\mathcal{I}_t|$ denote the learning rate and batch\nsize, respectively. KD supports various kinds of interac\ntion between multiple neural networks. The teacher-student\nframework we refer to here consists by default of a teacher\nnetwork $g_\\phi(\\cdot)$ and a student network $f_\\theta(\\cdot)$, where the flow\nof knowledge transfer is often one-direction: the learning of\n$f$ is guided by the output of $g$, but not vice versa. The loss of\nstudent network $f$ in KD is bi-component as a weighted sum\nof task-specific and distillation loss ($l_{\\text{task}}$ and $l_{\\text{KD}}$), where a\nhyperparameter $\\alpha$ controls the impact of teacher guidance:\n$\\mathcal{L}_{\\text{KD}}(\\theta, \\phi) = (1 - \\alpha)l_{\\text{task}}(f_\\theta(x_i), y_i) + \\alpha l_{\\text{KD}}(f_\\theta(x_i), g_\\phi(x_i)).$\nIn many applications, the teacher network $g$ is often different from and much larger than the student network to obtain\na more compact model. Meanwhile, there is an increasing\nnumber of efforts to implement KD to improve generaliza\ntion for one particular architecture, where the teacher and\nstudent may share a common framework but differ in the\nrandom seeds for initialization. Some KD methods even\ntreat different parts within one single network as teacher and\nstudent. Below we describe two representative methods:\nOnline KD. Though traditional KD assumes the teacher\nnetwork $g$ as a pre-trained and powerful model, there ex\nist scenarios where obtaining such a teacher is costly or\nimpractical. Online KD is proposed to learn from an on\nthe-fly teacher network, allowing for dynamic adaptation\nduring student training. In online KD, the updating of $g$ is\naligned with $f$ for every mini-batch $B_t$ with $|\\mathcal{I}_t|$ (see Alg. 1\nin Appendix A.10) 1:\n$\\phi_{t+1} = \\phi_t - \\frac{\\eta}{\\mathcal{B}} \\sum_{i \\in \\mathcal{I}_t} \\nabla_\\phi L^{(\\text{teacher})}(\\phi_t)$\n$= \\phi_t - \\frac{\\eta}{\\mathcal{B}} \\sum_{i \\in \\mathcal{I}_t} \\nabla_\\phi l_{\\text{task}}(g_{\\phi_t}(x_i), y_i).$\nThe design of an online teacher is quite demand-oriented,\nit could be simply a copy of the student network (Li et al.,\n2022b; Wu & Gong, 2021). But to maintain a valid knowl\nedge gap between student and teacher, they are often ini\ntialized using different random seeds in practice. Besides,"}, {"title": "3.2. Spacing Effect in Biological Learning", "content": "Originally discovered by (Ebbinghaus, 2013), the biological\nspacing effect highlights that the distribution of study ses\nsions across time is critical for memory formation. Then, its\nfunctions have been widely demonstrated in various animals\nand even humans (see Sec. 1). Many cognitive computing\nmodels have proposed the concept of spaced learning and\ndescribed its dynamics, positing an optimal inter-trial inter\nval during memory formation (Landauer, 1969; Peterson,\n1966; Wickelgren, 1972). These studies motivate us to fur\nther investigate if a proper space interval could benefit KD\nof possible data variability across training batches. Here we\nprovide more detailed explanations of the interdisciplinary\nconnections:\nIn machine learning, KD aims to optimize the parameters\nof a student network with the help of a teacher network by\nregularizing their outputs to be consistent in response to\nsimilar inputs. As shown in a pioneering theoretical analy\nsis (Allen-Zhu & Li, 2020), KD shares a similar mechanism\nwith ensemble learning (EL) in improving generalization\nfrom the training set to the test set. In particular, online KD\nperforms this mechanism at temporal scales, and self KD\ncan be seen as a special case of online KD. In comparison,\nthe biological spacing effect can also be generalized to a\nkind of EL at temporal scales, as the brain network processes\nsimilar inputs with a certain time interval and updates its\nsynaptic weights based on previous synaptic weights, which\nallows for stronger learning performance at test time (Pagani\net al., 2009; Smolen et al., 2016).\nThe proposed Spaced KD draws inspirations from the bi\nological spacing effect and capitalizes on the underlying"}, {"title": "4. Spaced KD", "content": "In this section, we describe how Spaced KD is implemented\ninto online KD and self KD, and include a pseudo code\nfor each in Appendix A.10. We then theoretically analyze\nthe benefit of the proposed spacing effect in improving\ngeneralization."}, {"title": "4.1. Incorporating Spacing Effect into KD", "content": "By applying spaced learning in the pipeline of KD, more\nprecisely in the context of online KD, we implement a pro\ncess of alternate learning between teacher and student. The\nteacher network updates itself several steps in advance, and\nthen it helps the student network train on the same set of\nbatches. Formally, we define a hyperparameter Space Inter\nval denoted as $s$ to represent the gap between the teacher's\nand student's learning pace. Spaced KD is described as\nfollows (see Fig. 1):\n1.  First, we train the teacher $g_{\\phi_t}(\\cdot)$ for $s$ steps (from $B_t$\nto $B_{t+s-1}$) according to the learning rule in Eq. 3,\nobtaining an advanced teacher $g_{\\phi_{t+s}}(\\cdot)$ identical to\nthat of online KD.\n2.  Then, we freeze the parameters $\\phi_{t+s}$ of our teacher $g$,\nand start to transfer knowledge from it to the student\n$f_\\theta(\\cdot)$ that lags behind over the same batches of training\ndata $B_t \\sim B_{t+s-1}$:\n$\\theta_{t+s} = \\theta_t - \\frac{\\eta}{\\mathcal{B}} \\sum_{j=t}^{t+s-1} \\sum_{i \\in \\mathcal{I}_j} \\nabla_\\theta \\mathcal{L}_{\\text{KD}}(f_{\\theta_i}, \\phi_{t+s}),$\nwhere $\\mathcal{L}_{\\text{KD}}$ is the same as Eq. 2 but using fixed teacher\nparameters $\\phi_{t+s}$.\nIntrinsically, Spaced KD is a special case of online KD. The\nmain difference that sets Spaced KD apart from online KD\nis the less frequent updates of the teacher network, which\nprovides a relatively stable learning standard for the student\nnetwork and potentially contributes to its better generaliza\ntion ability than the online setting. In practice, we initialize\nthe teacher in Spaced KD using the same random seed as\nthe student. To take a closer look, we theoretically illus\ntrate the impact of the proposed spacing effect on KD with\nstep-by-step mathematical derivations in the next section."}, {"title": "4.2. Theoretical Analysis", "content": "To understand why Spaced KD might provide better gener\nalization than online KD, we analyze the Hessian matrix of\nthe loss function for the student network in both scenarios.\nThe Hessian matrix plays a crucial role in understanding\nthe curvature of the loss landscape. In literature, various\nmetrics related to the Hessian matrix have been adopted to\nevaluate the flatness of a loss minimum after training con\nvergence, reflecting the generalization ability of the trained\nmodel (Krizhevsky et al., 2009; Blanc et al., 2020; Damian\net al., 2021; Zhou et al., 2020). Here we choose the Hes\nsian trace as a representative for convenience. A smaller\nHessian trace indicates a flatter loss landscape, which has\nalso been proved to be related to the upper bound of test set\ngeneralization error.\nSetup. For simplicity we set the dimension of class space\nas $c = 1$, and the extension of $c > 1$ is straightforward. Let\nthe mean square error (MSE) be the task-specific loss. The\nKD loss characterizes the distance between two distributions\n$\\hat{y}$ and $y$: $l_{\\text{task}}(\\hat{y}, y) = l_{\\text{KD}}(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2$.\nHessian Matrix. For KD loss at the $i$-th data sample that\nfollows Eq. 2, the Hessian matrix at a point $\\theta$ of student\n$f_\\theta(\\cdot)$ with respect to its teacher $g_\\phi(\\cdot)$ can be calculated as\nthe second-derivative of the empirical risk $\\mathcal{L}_{i}^{\\text{KD}}(\\theta, \\phi) =$\n$\\frac{1}{N} L_i^{\\text{KD}}(\\theta, \\phi)$. It could be easily verified that:\n$H_\\phi(\\theta) = \\nabla_\\theta^2 \\mathcal{L}^{(\\text{KD})}(\\theta, \\phi)$\n$= \\frac{1}{N} \\sum_{i=1}^N [\\nabla_\\theta f_\\theta(x_i) \\nabla_\\theta f_\\theta(x_i)^T + \\beta(i, \\theta, \\phi)\\nabla_\\theta f_\\theta(x_i)],$\nwhere $\\beta(i, \\theta, \\phi) = (1 - \\alpha)(f_\\theta(x_i) - y_i) + \\alpha(f_\\theta(x_i) - g_\\phi(x_i))$, and in fact $\\nabla_\\theta \\mathcal{L}^{(\\text{KD})}(\\theta, \\phi) = \\beta(i, \\theta, \\phi)\\nabla_\\theta f_\\theta(x_i)$.\nAt arbitrary time stamp $t$ during the supervised training\nprocess, the teacher model's parameters for student $\\theta_t$ in\nonline KD is $\\phi_t$. In Spaced KD it should be $\\phi_{k(t)}$ with\n$k(t) = (\\lceil t/s \\rceil)s$ where $\\lceil \\cdot \\rceil$ denotes ceiling operation. Notice that for online KD, the loss function constantly changes\ndue to the update of the teacher, but when we focus on the\nloss curve for a particular $\\phi$, the differentiability of $\\mathcal{L}_i^{(\\text{KD})}$\nare preserved, allowing us to continue the discussion.\nDefinition 4.1 (Local linearization.). Let $\\theta^*$ be a local mini\nmizer of loss function w.r.t $f_\\theta(\\cdot)$, we call the local lineariza\ntion of $f_\\theta(\\cdot)$ at $\\theta$ around $\\theta^*$ as: $f_{\\theta}(x) = f_{\\theta^*}(x) + (\\theta - \\theta^*, \\nabla_\\theta f_{\\theta^*}(x))$.\nFor both teacher and student networks, this linearized model\nin Def. 4.1 provides an applicable approximation of the local\ndynamic behavior around a converged point. We denote\n$\\phi^*$ and $\\theta^*$ as the local minimizer of teacher and student,\nrespectively. Without loss of generality, we assume that"}, {"title": "5. Experiment", "content": "In this section, we first describe experimental setups and\nthen present experimental results."}, {"title": "5.1. Experimental Setups", "content": "Benchmark. We evaluate the proposed spacing effect on\nboth ResNet-based architectures (He et al., 2016) such as\nResNet-18, ResNet-50 and ResNet-101, and transformer\nbased architectures (Dosovitskiy et al., 2020) such as DeiT\nTiny (Touvron et al., 2021) and PiT-Tiny (Heo et al.,\n2021). We consider four commonly used image classifi\ncation datasets: CIFAR-100 (Krizhevsky et al., 2009), Tiny\nImageNet, ImageNet-100, and ImageNet-1K (Russakovsky\net al., 2015). CIFAR-100 is a well-known image classifica\ntion dataset of 100 classes and the image size is 32 \u00d7 32.\nTiny-ImageNet consists of 200 classes and the image size is\n64 \u00d7 64. ImageNet-100 and ImageNet-1K contain 100 and\n1000 classes of images, respectively, and the image size is\n224 x 224.\nImplementation. For ResNet-based architectures, we use\nan SGD optimizer (Sutskever et al., 2013) with 0.9 momen\ntum, 128 batch size, 80 epochs, and a constant learning\nrate of 0.01. For KD-related hyperparameters (Zhang et al.,\n2019), we use a distillation temperature of 3.0, a feature\nloss coefficient of 0.03, and a KL-Divergence loss weight of\n0.3. For transformer-based architectures, we use an AdamW\noptimizer (Loshchilov & Hutter, 2017a) of batch size 128\nand epoch number 300 (warm-up for 20 epochs). Besides,\na cosine learning rate decay policy (Loshchilov & Hutter,\n2017b) is utilized with initial learning rate 5e - 4 and final\n5e-6, following the training pipeline of previous works (Liu\net al., 2021; Li et al., 2022a; Sun et al., 2024).\nFor Spaced KD, we manually control a sparse interval s\nin terms of epochs, which is proportional to the total num\nber of samples in the training set (e.g., s = 0.5 denotes\nhalf of the training set). To avoid potential bias, the train\ning set is shuffled and both teacher and student receive\nthe same data flow. In online KD, the teacher employs\nthe same network architecture as the student if not spec\nified, distilling both response-based (Hinton et al., 2015)\nand feature-based (Adriana et al., 2015) knowledge. In\nself KD, the teacher is the deepest layer of the network\nand the students are the shallow layers along with auxiliary\nclassifiers (Zhang et al., 2019). Specifically, ResNet-based\narchitectures consist of 4 blocks so 3 students correspond to\nthe three shallower blocks. The number of students for trans\nformers depends on the network depth, namely, 11 in our\nsetup. Auxiliary alignment layers and classifier heads are\nutilized to unify the dimensions of feature and logit vectors\nproduced by students from different depths for distillation.\nUnless otherwise specified, all results are averaged over\nthree repeats."}, {"title": "5.2. Effectiveness and Generality of Spacing Effect", "content": "Overall Performance. Our proposed Spaced KD outper\nforms traditional online KD (see Tab. 1) and self KD (see\nTab. 2) across different datasets and networks. The per\nformance of different intervals can be seen in Fig. 2 and\nTab. 6. Compared to vanilla online KD and self KD, the\nenhancement of accuracy is 2.14% on average, with moder\nate variations from a minimum of 1.19% on ResNet-101/\nCIFAR-100 to a maximum of 3.44% on ResNet-101 / Tiny\nImageNet. For the larger dataset ImageNet-1K, our Spaced\nKD improves the performance for ResNet-18 and ViT net\nworks by up to 5.08% (see Tab. 7, Tab. 8 of Appendix A.4).\nTeacher-Student Gap. Considering that capacity gaps\nbetween teacher and student for their different architec\ntures or training progress would affect distillation gains\n(see Sec. 2), we further evaluate various teacher-student\npairs across model sizes and architectures, and Spaced KD\nremains effective in all cases (see Tab. 9 and Tab. 10 in\nAppendix A.5). Interestingly, if we train the teacher ahead\nof the student by s steps at the beginning and then distill\nits knowledge to the student maintaining a constant training\ngap, there is no significant improvement over the online\nKD (see Tab. 11). This indicates the particular strength of\nSpaced KD, which applies in the later stage rather than the\nearly stage.\nDifferent KD Losses. To evaluate generality, we imple\nment Spaced KD with representative loss functions, such\nas L1, smooth L1, MSE (reduction=mean), MSE (reduc\ntion=sum), and cross-entropy. As shown in Tab. 3, Spaced\nKD applies to different loss functions with consistent im\nprovements."}, {"title": "5.3. Extended Analysis of Spacing Effect", "content": "Sensitivity of Space Interval. Through extensive inves\ntigation (see Fig. 2 and Tab. 6 in Appendix A.2), the space\ninterval s is relatively insensitive and s = 1.5 results in con"}, {"title": "5.4. Generalization of Spaced KD", "content": "Flat Minima. To verify whether Spaced KD could con\nverge to a flat minima, we conduct experiments to ob\nserve the model robustness that reflects the flatness of\nloss landscape around convergence, following previous\nworks (Zhang et al., 2018; 2019). We first train ResNet\n18/50/101 networks on CIFAR-100 with traditional online\nKD (w/o) and our Spaced KD (w/1.5, the interval is 1.5\nepochs). Then Gaussian noise is added to the parameters\nof those models to evaluate their training loss and accuracy\nover the training set at various perturbation levels, which\nare plotted in Fig. 4. The results show that the model trained\nwith Spaced KD maintains a higher accuracy and lower loss\ndeviations than naive KD under gradient noise level. Fur\nthermore, after applying this interference, the training loss\nof the independent model significantly increases, whereas\nthe loss of the Spaced KD model rises much less. These\nresults suggest that the model with Spaced KD has found\na much wider minima, which is likely to result in better\ngeneralization performance.\nNoise Robustness. In addition to manipulating network\nparameters, we conduct an extra experiment to evaluate the\nmodel's generalization ability to multiple transformations\nthat create out-of-distribution images. Specifically, we apply\n6 representative operations of image corruption (Michaelis\net al., 2019) to the images of the CIFAR-100 test set. The\ntest accuracy at noise intensity 1.0 is recorded in Tab. 5 and\nresults of other intensity levels can be found in Tab. 12 of\nAppendix A.8. It is clear that in most cases with different\ncorruption types and network architectures, our proposed\nSpaced KD helps the student network resist noise attacks,\nwhich reflects its superior robustness to unseen inference\nsituations. Besides, we test robust accuracy using a represen\ntative adversarial attack method called BIM (Kurakin et al.,\n2017), and our Spaced KD is more robust across different ar\nchitectures (see Tab. 13 in Appendix A.9). The above results\nempirically offer evidence for the generalization promotion\nbrought by the spacing effect."}, {"title": "6. Conclusion", "content": "In this paper, we present Spaced Knowledge Distillation\n(Spaced KD), a bio-inspired strategy that is simple yet ef\nfective for improving online KD and self KD. We theoreti\ncally demonstrate that the spaced teacher helps the student\nmodel converge to flatter local minima via SGD, resulting in\nbetter generalization. With extensive experiments, Spaced\nKD achieves significant performance gains across a variety\nof benchmark datasets, network architectures and baseline\nmethods, providing innovative insights into the learning\nparadigm of KD from a temporal perspective. Since we also\nreveal a possible critical period of spacing effect and provide\nits potential theoretical implications in DNNs, our findings\nmay offer computational inspirations for neuroscience. By\nexploring more effective spaced learning paradigms and\ninvestigating detailed neural mechanisms, our work is ex\npected to facilitate a deeper understanding of both biological\nlearning and machine learning.\nAlthough our approach has achieved remarkable improve\nments, it also has potential limitations: Our results suggest a\nrelatively insensitive optimal interval (s = 1.5) for Spaced\nKD, yet remain under-explored its theoretical foundation\nand an adaptive strategy for determining it. Additionally, our\nresults indicate that the timing of Spaced KD is important.\nThe effectiveness of adaptive adjusting the space interval\nand the timing of distillation remains to be validated and\nanalyzed in subsequent research. In future work, we would\nactively explore the application of such spacing effect for\na broader range of scenarios, such as curriculum learning,\ncontinual learning, and reinforcement learning."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Proof of Theorem 4.4", "content": "Proof. For a general KD loss", "0*": "n$\\text{Tr"}, "H(\\theta^*)) = \\frac{1}{N} \\sum_{i=1}^N [\\|\\nabla_\\theta f_{\\theta^*}(x_i)\\|^2 + \\beta(i, \\theta^*, \\phi)\\text{Tr} (\\nabla f_{\\theta^*}(x_i))"], "be": "n$\\mathbb{E}[\\text{Tr}(H^{(\\theta^*)}))] = \\mathbb{E}[\\frac{1}{N} \\sum_{i=1}^N [\\|\\nabla_\\theta f_{\\theta^*}(x_i)\\|^2 + \\alpha\\Delta_\\phi \\nabla_{\\phi}g_{\\phi^*}(x_i)\\text{Tr} (\\nabla f_{\\theta^*}(x_i))],$\n$\\mathbb{E}[\\text{Tr}(H^{(\\phi)}(\\theta^*))] = \\mathbb{E}[\\frac{1}{N} \\sum_{i=1}^N [\\|\\nabla_\\theta f_{\\theta^*}(x_i)\\|^2 + \\alpha \\Delta_{\\phi_{k}} \\overline{\\nabla_{\\phi_{k}} g_{\\phi^*}(x_i)}\\text{Tr} (\\nabla f_{\\theta^*}(x_i))].$\\"}