{"title": "CONTINUAL SELF-SUPERVISED LEARNING CONSIDERING MEDICAL DOMAIN\nKNOWLEDGE IN CHEST CT IMAGES", "authors": ["Ren Tasai", "Guang Li", "Ren Togo", "Minghui Tang", "Takaaki Yoshimura", "Hiroyuki Sugimori", "Kenji Hirata", "Takahiro Ogawa", "Kohsuke Kudo", "Miki Haseyama"], "abstract": "We propose a novel continual self-supervised learning method\n(CSSL) considering medical domain knowledge in chest CT im-\nages. Our approach addresses the challenge of sequential learning\nby effectively capturing the relationship between previously learned\nknowledge and new information at different stages. By incorporat-\ning an enhanced DER into CSSL and maintaining both diversity and\nrepresentativeness within the rehearsal buffer of DER, the risk of\ndata interference during pretraining is reduced, enabling the model\nto learn more richer and robust feature representations. In addi-\ntion, we incorporate a mixup strategy and feature distillation to fur-\nther enhance the model's ability to learn meaningful representations.\nWe validate our method using chest CT images obtained under two\ndifferent imaging conditions, demonstrating superior performance\ncompared to state-of-the-art methods.", "sections": [{"title": "1. INTRODUCTION", "content": "Self-supervised learning (SSL) has gained significant attention for\nits ability to reduce the annotation costs of large-scale datasets [1].\nSSL methods typically involve two phases: pretraining and fine-\ntuning. During pretraining, a model learns feature representations\ndirectly from unlabeled data. In the fine-tuning stage, these learned\nrepresentations are refined using a smaller labeled dataset [2, 3].\nFor large-scale medical datasets, annotations require expertise from\nphysicians, which is both time-consuming and labor-intensive [4,5].\nAs a result, there has been active research into medical SSL methods\nthat work with limited labeled data from various modalities such as\nX-rays [6, 7], computed tomography (CT) [8, 9], and magnetic reso-\nnance imaging (MRI) [10, 11]. These studies have shown promising\nresults of SSL for medical image analysis.\nIn the medical field, a variety of imaging modalities, including\nX-rays, CT, and MRI, are utilized. Due to the inherent dimensional\ndifferences between these modalities, such as 2D for X-rays and\n3D for CT and MRI, recent researchers have explored SSL meth-\nods that pretrain across multiple modalities, simultaneously [12, 13].\nHowever, these studies have reported no significant improvement in\nclassification accuracy when pretraining involves multiple modali-\nties [14], primarily due to insufficient handling of the distinct data\ndistributions, which causes interference during the pretraining pro-\ncess.\nRecently, continual self-supervised learning (CSSL) [14] has\nbeen introduced to address data interference across modalities by\nconsidering differences in data distribution. CSSL preserves the di-\nversity of data distribution during pretraining, allowing the model\nto maintain rich feature representations for later fine-tuning. A ma-\njor challenge in CSSL, is catastrophic forgetting, where new knowl-\nedge overwrites previously learned information [15]. To address this\nproblem, a rehearsal-based approach from continual learning is em-\nployed. This approach stores a portion of past data and features in a\nrehearsal buffer, enabling the model to retain and revisit prior knowl-\nedge during subsequent learning [16, 17]. Among these approaches,\ndark experience replay (DER) [16] is particularly effective in miti-\ngating catastrophic forgetting during continual pretraining on diverse\nmodalities.\nMedical imaging relies on various domain-specific images de-\npending on the equipment and imaging conditions [18, 19]. As a\nresult, several researches have focused on medical SSL methods that\ncan pretrain on multiple domains simultaneously [20, 21]. These do-\nmains capture the same anatomical region, whereas the modalities\ncapture different anatomical regions. Therefore, these domains tend\nto have more similar data distributions compared to different modali-\nties. However, prior studies often overlook differences in data distri-\nbution across domains, leading to data interference. In CSSL across\nmultiple domains, it is crucial to maintain not only the diversity of\ndata distributions but also their representativeness. To address this is-\nsue, we focus on capturing the relationship between past knowledge\nfrom earlier stages and future knowledge from subsequent stages.\nThis paper proposes a novel CSSL method that leverages medi-\ncal domain knowledge in chest CT images. Specifically, our method\nincorporates an enhanced DER that ensures both diversity and rep-\nresentativeness in the rehearsal buffer by accounting for differences\nin data distributions across sequential learning stages. The enhanced\nDER allows us to minimize data interference during pretraining\nand enables the model to learn richer feature representations across\nmultiple domains. Additionally, we integrate a mixup strategy and\nfeature distillation to further improve representation learning. We\npretrain using chest CT images obtained under two different imag-\ning conditions and conduct evaluation on another open CT image\ndataset. Through extensive experiments, our method consistently\noutperforms other approaches.\nOur contributions are summarized as follows.\n\u2022 We propose a novel CSSL method that effectively addresses\ndata distribution shifts during pretraining in chest CT images\nacross two domains.\n\u2022 By incorporating an enhanced DER into the CSSL method,\nit prevents data interference caused by the impact of differ-"}, {"title": "2. CONTINUAL SELF-SUPERVISED LEARNING METHOD\nCONSIDERING MEDICAL DOMAIN KNOWLEDGE", "content": "The proposed method employs a three-stage CSSL approach to mit-\nigate catastrophic forgetting and reduce data interference between\ntwo domains in chest CT images. In the first stage, self-supervised\nlearning is performed using the initial dataset D\u2081 from one domain\nof chest CT images. In the second stage, selected images from D1\nare stored in the rehearsal buffer, ensuring that both diversity and\nrepresentativeness are maintained. In the third stage, continual self-\nsupervised learning is applied using the next dataset D2 from an-\nother domain and the obtained rehearsal buffer. After completing the\nCSSL process, the model is fine-tuned with labeled data for down-\nstream tasks, such as classification. Figure 1 shows an overview of\nthe proposed method."}, {"title": "2.1. Stage 1: Self-supervised Learning on Dataset D1", "content": "The first stage of pretraining begins with training model M1 us-\ning the first domain dataset D1. The Masked Autoencoders (MAE)\nmethod [22] based on masked image modeling is used to learn the\ninput data's feature representations. In this process, each image with\nC channels is divided into n patches of resolution (V, V), and us-\ning a masking rate r, a subset of m = n \u00d7 r patches is randomly\nmasked. The n m unmasked patches are converted into token se-\nquences by the tokenizer TM\u2081 and are passed through the encoder\n\u03a6\u039c\u2081 to generate feature representations. The decoder M\u2081 then re-\nconstructs the masked patches based on these representations and\nthe embeddings of the masked patches from the original image. The\nmodel is trained to minimize the mean squared error (MSE) between\nthe original masked patches Xm and the reconstructed patches Ym\nas follows:\n$L_{MSE} = \\frac{1}{m\\times V^2 \\times C} ||Y_m - X_m||^2$.   (1)\nAt the end of the first stage, the model M\u2081 is trained, capturing\nthe rich feature representations of the first domain dataset D1. This\ntrained model will also be used for the third stage of CSSL, which\ninvolves both D\u2081 and the second domain dataset D2."}, {"title": "2.2. Stage 2: Storing Image Samples in Rehearsal Buffer", "content": "The second stage involves the selection of images stored in the re-\nhearsal buffer, which plays a critical role in capturing shifts in data\ndistribution between stages, helping to mitigate catastrophic forget-\nting. In the second stage, we propose an enhanced DER that uses\nk-means sampling to account for the distributions of the domain\ndatasets D\u2081 and D2, ensuring that the stored samples from D\u2081 are\nboth diverse and representative. Figure 2 provides an overview illus-\ntration of this stage.\nLet N1 and N2 represent the number of images in domain\ndatasets D\u2081 and D2, respectively. Using the parameter a to deter-\nmine the sampling ratio, the number of clusters is set to K = N\u2081\u03a7\u03b1.\nThe dataset D1 is first clustered into K classes based on embeddings\ngenerated by the pretrained model M1, with each cluster denoted\nas ai, where i\u2208 {1,2,..., K}, and the feature vector of each\ncluster center represented as pi. The combined dataset of D\u2081 and"}, {"title": "2.3. Stage 3: Continual Self-supervised Learning on Dataset D2", "content": "Following the SSL pretraining of model M\u2081 in the first stage, model\nM2 is also pretrained in the third stage using the MAE method. In\nthis stage, model M2 is trained using both the second domain dataset\nD2 and the image samples stored in the rehearsal buffer B that ob-\ntained from the second stage. We incorporate two critical techniques\nin this stage: the mixup strategy and feature distillation, both de-\nsigned to enhance the model's ability to retain knowledge from the\nfirst stage while learning new representations from the second do-\nmain.\nThe mixup strategy is applied to augment the image samples\nstored in the rehearsal buffer B, further improving the diversity of\nthe data used for training. Let S denote the batch size, C the number\nof image channels, and (H, W) the image resolution. A batch of\nimages b \u2208 $R^{S\\times C \\times H \\times W}$ is drawn from B, duplicated, and shuffled\nto create a new batch b'. The mixed batch bmix is calculated as\nfollows:\n$b_{mix} = b + (1 - \\lambda) \\odot b'$.(3)\nHere, X \u2208 $R^{S\\times C \\times H \\times W}$ is a mask, where each element takes a\nrandom value in the range [0, 1), determining the extent to which"}, {"title": "3. EXPERIMENTS", "content": "3.1. Datasets and Settings\nWe constructed two subsets of chest CT images from the J-MID\ndatabase based on mediastinal and lung window settings, and used\nthem as D\u2081 and D2 for pretraining. The number of images N\u2081 and\nN2 were 31,256 and 26,403, respectively. All of these images are\ngrayscale and have a resolution of 512 \u00d7 512 pixels. For fine-tuning\nand evaluation, we trained the model using the SARS-CoV-2 CT-\nScan Dataset [23] and validated it by performing a COVID-19 classi-\nfication task. The SARS-CoV-2 CT-Scan Dataset contains a total of\n2,481 chest CT images labeled into two classes: COVID-19 positive\nand negative. Out of these, 1,986 images were used for fine-tuning\nand 495 images were used for evaluation. In the pretraining of MAE,\nthe masking ratio was set to r = 0.75, and the ViT-B [24] was used\nas the encoder. In each pretraining stage, a warm-up strategy was\nused during the first 40 epochs to gradually raise the learning rate\nfrom 0 to 0.00015, after which it was reduced to 0 in the following\ntraining using a cosine schedule. In k-means sampling, the parame-\nters a and \u1e9e, which determine the sampling ratio, were set to 0.01\nand 0.05, respectively. Additionally, the number of clusters K and\nthe number of samples T in the rehearsal buffer B were set to 312\nand 1,562, respectively. The parameters 1, 2, and 3, which de-\ntermine the number of images to be acquired, were set to 6, 3, and\n1, respectively. The AdamW optimizer [25] was used in the fine-\ntuning phase, and the learning rate was set to 0.00005. We took SSL\non the dataset for 300 epochs at each pretraining stage, followed\nby fine-tuning on the dataset for 80 epochs. As evaluation metrics,\ntwo-class classification accuracy (ACC), the area under the receiver\noperator curve (AUC), and F1 score (F1) were used.\nThe effectiveness of the learning order of domains in the pro-\nposed method was also verified by swapping D\u2081 and D2 and con-\nducting continual pretraining accordingly. Furthermore, to examine"}, {"title": "3.2. Results and Discussion", "content": "From Table 1, we can observe that the proposed method outper-\nforms all the comparative methods. From the three results of the\nMAE method, it was confirmed that pretraining with two domains\nimproves accuracy compared to using a single domain. Addition-\nally, as the proposed method outperforms the MAE method that si-\nmultaneously pretrains on two domains in evaluation metrics, it was\nconfirmed that the proposed method can reduce data interference be-\ntween domains compared to pretraining on multiple domains simul-\ntaneously. Furthermore, the experimental results show that when\nthe model is pretrained on the domains in the same order, the pro-\nposed method outperforms MedCoSS in the evaluation metrics. This\nconfirms that the proposed method, which takes data distribution\nbetween stages into account, is effective in reducing data interfer-"}, {"title": "3.3. Ablation Studies", "content": "In the proposed method, we conducted an ablation study in which\nwe varied the parameters Y1, Y2, and 3, which determine the ratio\nof images acquired in the rehearsal buffer. This allows us to examine\nthe optimal parameters that consider the differences in data distribu-\ntion between stages to ensure diversity and representativeness within\nthe rehearsal buffer. The ablation results are shown in Table 2. In the\nproposed method, which performs continual pretraining in the order\nfrom D\u2081 to D2, it is shown that the highest accuracy in the eval-\nuation metrics is achieved when the parameters Y1, Y2, and 3 are\nset to 6, 3, and 1, respectively. Furthermore, it is observed that pri-\noritizing the storage of samples that are closer in data distribution\nbetween domains in the rehearsal buffer, rather than those that are\nfarther apart, tends to be more effective. It was confirmed that fo-\ncusing on representativeness when storing samples in the rehearsal\nbuffer is effective. This is likely because the domains capture the\nsame anatomical regions, leading to similar data distributions and\na tendency for the data distributions between domains to be closer.\nOn the other hand, when Y1, Y2, and 3 are set to 8, 1, and 1, re-\nspectively, the accuracy in the evaluation metrics decreases. This\nsuggests that it is preferable to maintain not only representativeness\nbut also diversity."}, {"title": "4. CONCLUSION", "content": "In this paper, we presented a novel CSSL method that incorporates\nmedical domain knowledge from chest CT images. By effectively\ncapturing the differences in data distribution between two chest CT\nimage domains and maintaining diversity and representativeness in\nthe rehearsal buffer, our method enhances the representation learn-\ning process in CSSL. Additionally, we introduced a mixup strategy\nand feature distillation to further refine feature representations. Ex-\ntensive experiments with chest CT images demonstrated that our ap-\nproach outperforms state-of-the-art methods."}]}