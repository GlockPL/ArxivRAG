{"title": "YOLOv12: Attention-Centric Real-Time Object Detectors", "authors": ["Yunjie Tian", "Qixiang Ye", "David Doermann"], "abstract": "Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms.\nYOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRV2: YOLOv12-S beats RT-DETR-R18/RT-DETRv2-R18 while running 42% faster, using only 36% of the computation and 45% of the parameters. More comparisons are shown in Figure 1.", "sections": [{"title": "1. Introduction", "content": "Real-time object detection has consistently attracted significant attention due to its low-latency characteristics, which provide substantial practicality [4, 17, 24, 28]. Among them, the YOLO series [3, 24, 28, 29, 32, 45\u201347, 53, 57, 58] has effectively established an optimal balance between latency and accuracy, thus dominating the field. Although improvements in YOLO have focused on areas such as loss functions [8, 35, 43, 44, 48, 67, 68], label assignment [22, 23, 34, 59, 69], network architecture design has remained a critical research priority [24, 28, 32, 57, 58]. Although attention-centric vision transformer (ViT) architectures have been proven to possess stronger modeling capabilities, even in small models [20, 21, 25, 50], most architectural designs continue to focus primarily on CNNs.\nThe primary reason for this situation lies in the inefficiency of the attention mechanism, which comes from two main factors: quadratic computational complexity and inefficient memory access operations of the attention mechanism (the latter being the main issue addressed by FlashAttention [13, 14]). As a result, under a similar computational budget, CNN-based architectures outperform attention-based ones by a factor of ~ 3\u00d7 [38], which significantly limits the adoption of attention mechanisms in YOLO systems where high inference speed is critical.\nThis paper aims to address these challenges and further builds an attention-centric YOLO framework, namely YOLOv12. We introduce three key improvements. First, we propose a simple yet efficient area attention module (A2), which maintains a large receptive field while reducing the computational complexity of attention in a very simple way, thus enhancing speed. Second, we introduce the residual efficient layer aggregation networks (R-ELAN) to address the optimization challenges introduced by attention (primarily large-scale models). R-ELAN introduces two improvements based on the original ELAN [57]: (i) a block-level residual design with scaling techniques and (ii) a redesigned feature aggregation method. Third, we make some architectural improvements beyond the vanilla attention to fit the YOLO system. We upgrade traditional attention-centric architectures including: introducing FlashAttention to conquer the memory access issue of attention, removing designs such as positional encoding to make the model fast and clean, adjusting the MLP ratio from 4 to 1.2 to balance the computation between attention and feed forward network for better performance, reducing the depth of stacked blocks to facilitate optimization, and making use of convolution operators as much as possible to leverage their computational efficiency.\nBased on the designs outlined above, we develop a new family of real-time detectors with 5 model scales: YOLOv12-N, S, M, L, and X. We perform extensive experiments on standard object detection benchmarks following YOLOv11 [28] without any additional tricks, demonstrating that YOLOv12 provides significant improvements over previous popular models in terms of latency-accuracy and FLOPs-accuracy trade-offs across these scales, as illustrated in Figure 1. For example, YOLOv12-N achieves 40.6% mAP, outperforming YOLOv10-N [53] by 2.1% mAP while maintaining a faster inference speed, and YOLOv11-N [28] by 1.2% mAP with a comparable speed. This advantage remains consistent across other scale models. Compared to RT-DETR-R18 [66] / RT-DETRv2-R18 [40], YOLOv12-S is 1.5%/0.1% mAP better, while reports 42%/42% faster latency speed, requiring only 36%/36% of their computations and 45%/45% of their parameters.\nIn summary, the contributions of YOLOv12 are two-fold: 1) it establishes an attention-centric, simple yet efficient YOLO framework that, through methodological innovation and architectural improvements, breaks the dominance of CNN models in YOLO series. 2) without relying on additional techniques such as pretraining, YOLOv12 achieves state-of-the-art results with fast inference speed and higher detection accuracy, demonstrating its potential."}, {"title": "2. Related Work", "content": "Real-time Object Detectors. Real-time object detectors have consistently attracted the community's attention due to their significant practical value. The YOLO series [3, 9, 24, 28, 29, 32, 45-47, 53, 54, 57, 58] has emerged as a leading framework for real-time object detection. The early YOLO systems [45-47] establish the framework for the YOLO series, primarily from a model design perspective. YOLOv4 [3] and YOLOv5 [29] add CSPNet [55], data augmentation, and multiple feature scales to the framework. YOLOv6 [32] further advance these with BiC and SimCSPSPPF modules for the backbone and neck, alongside anchor-aided training. YOLOv7 [57] introduce E-ELAN [56] (efficient layer aggregation networks) for improved gradient flow and various bag-of-freebies, while YOLOv8 [24] integrate a efficient C2f block for enhanced feature extraction. In recent iterations, YOLOv9 [58] introduce GELAN for architecture optimization and PGI for training improvement, while YOLOv10 [53] apply NMS-free training with dual assignments for efficiency gains. YOLOv11 [28] further reduces latency and increases accuracy by adopting the C3K2 module (a specification of GELAN [58]) and lightweight depthwise separable convolution in the detection head. Recently, an end-to-end object detection method, namely RT-DETR [66], improved traditional end-to-end detectors [7, 33, 37, 42, 71] to meet real-time requirements by designing an efficient encoder and an uncertainty-minimal query selection mechanism. RT-DETRv2 [40] further enhances it with bag-of-freebies. Unlike previous YOLO series, this study aims to build a YOLO framework centered around attention to leverage the superiority of the attention mechanism.\nEfficient Vision Transformers. Reducing computational costs from global self-attention is crucial to effectively applying vision transformers in downstream tasks. PVT [61] addresses this using multi-resolution stages and downsampling features. Swin Transformer [39] limits self-attention to local windows and adjusts the window partitioning style to connect non-overlapping windows, balancing communication needs with memory and computation demands. Other methods, such as axial self-attention [26] and criss-cross attention [27], calculate attention within horizontal and vertical windows. CSWin transformer [16] builds on this by introducing cross-shaped window self-attention, computing attention along horizontal and vertical stripes in"}, {"title": "3. Approach", "content": "This section introduces YOLOv12, an innovation in the YOLO framework from the perspective of network architecture with attention mechanism.\n3.1. Efficiency Analysis\nThe attention mechanism, while highly effective in capturing global dependencies and facilitating tasks such as natural language processing [5, 15] and computer vision [19, 39], is inherently slower than convolution neural networks (CNN). Two primary factors contribute to this discrepancy in speed.\nComplexity. First, the computational complexity of the self-attention operation scales quadratically with the input sequence length $L$. Specifically, for an input sequence with length $L$ and feature dimension $d$, the computation of the attention matrix requires $O(L^2d)$ operations, since each token attends to every other token. In contrast, the complexity of convolution operations in CNNs scales linearly with respect to the spatial or temporal dimension, i.e., $O(kLd)$, where $k$ is the kernel size and is typically much smaller than $L$. As a result, self-attention becomes computationally prohibitive, especially for large inputs such as high-resolution images or long sequences.\nMoreover, another significant factor is that most attention-based vision transformers, due to their complex designs (e.g., window partitioning/reversing in Swin transformer [39]) and the introduction of additional modules (e.g., positional encoding), gradually accumulate speed overhead, resulting in an overall slower speed compared to CNN architectures [38]. In this paper, the design modules utilize simple and clean operations to implement attention, ensuring efficiency to the greatest extent.\nComputation. Second, in the attention computation process, memory access patterns are less efficient compared to CNNs [13, 14]. Specifically, during self-attention, intermediate maps such as the attention map (QKT) and the softmax map ($L \\times L$) need to be stored from high-speed GPU SRAM (the actual location of the computation) to high bandwidth GPU memory (HBM) and later retrieved during the computation, and the read and write speed of the former is more than 10 times that of the latter, thus resulting in significant memory accessing overhead and increased wall-clock time\u00b9. Additionally, irregular memory access patterns in attention introduce further latency compared to CNNs, which utilize structured and localized memory access. CNNs benefit from spatially constrained kernels, enabling efficient memory caching and reduced latency due to their fixed receptive fields and sliding-window operations.\nThese two factors, quadratic computational complexity and inefficient memory accessing, together render attention mechanisms slower than CNNs, particularly in real-time or resource-constrained scenarios. Addressing these limitations has become a critical area of research, with approaches such as sparse attention mechanisms and memory-efficient approximations (e.g., Linformer [60] or Performer [11]) aiming to mitigate the quadratic scaling."}, {"title": "3.2. Area Attention", "content": "A simple approach to reduce the computational cost of vanilla attention is to use the linear attention mechanism [49, 60], which reduces the complexity of vanilla attention from quadratic to linear. For a visual feature $f$ with dimensions $(n, h, d)$, where $n$ is the number of tokens, $h$ is the number of heads and $d$ is the head size, linear attention reduces the complexity from $2n^2hd$ to $2nhd^2$, decreasing the computational cost since $n > d$. However, linear attention suffers from global dependency degradation [30], instability [11], and distribution sensitivity [63]. Furthermore, due to the low-rank bottleneck [2, 10], it offers only limited speed advantages when applied to YOLO with input resolution of 640 x 640.\nAn alternative approach to effectively reduce complexity is the local attention mechanism (e.g., Shift window [39], criss-cross attention [27], and axial attention [16]), as shown in Figure 2, which transforms global attention into local, thus reducing computational costs. However, partitioning the feature map into windows can introduce overhead or reduce the receptive field, impacting both speed and accuracy. In this study, we propose the simple yet efficient area attention module. As illustrated in Figure 2, the feature map with the resolution of $(H, W)$ is divided into $l$ segments of size $(\\frac{H}{l}, W)$ or $(H, \\frac{W}{l})$. This eliminates explicit window partitioning, requiring only a simple reshape operation, and thus achieves faster speed. We empirically set the default value of $l$ to 4, reducing the receptive field to $\\frac{1}{4}$ of the original, yet it still maintains a large receptive field. With this approach, the computational cost of the attention mechanism is reduced from $2n^2hd$ to $n^2hd$. We show that despite the complexity $n^2$, this is still efficient enough to meet the real-time requirements of the YOLO system when $n$ is fixed at 640 (it increases n if the input resolution increases). Interestingly, we find that this modification has only a slight impact on performance but significantly improves speed."}, {"title": "3.3. Residual Efficient Layer Aggregation Networks", "content": "Efficient layer aggregation networks (ELAN) [57] are designed to improve feature aggregation. As shown in Figure 3 (b), ELAN splits the output of a transition layer (a 1 \u00d7 1 convolution), processes one split through multiple modules, then concatenates the all the outputs and applies another transition layer (a 1 \u00d7 1 convolution) to align dimensions. However, as analyzed by [57], this architecture can introduce instability. We argue that such a design causes gradient blocking and lacks residual connections from input to output. Furthermore, we build the network around the attention mechanism, which presents additional optimization challenges. Empirically, L- and X-scale models either fail to converge or remain unstable, even when using Adam or AdamW optimizers.\nTo address this problem, we propose residual efficient layer aggregation networks (R-ELAN), Figure 3 (d). In contrast, we introduce a residual shortcut from input to output throughout the block with a scaling factor (default to 0.01). This design is similar to layer scaling [52], which is introduced to build a deep vision transformer. However, applying layer scaling for each area attention will not conquer the optimization challenge and introduce slowdown on latency. This shows that the introduction of the attention mechanism is not the only reason for convergence but the ELAN architecture itself, which verifies the rationale behind our R-ELAN design.\nWe also design a new aggregation approach as shown in Figure 3 (d). The original ELAN layer processes the input of the module by first passing it through a transition layer, which then splits it into two parts. One part is further processed by subsequent blocks, and finally both parts are concatenated to produce the output. In contrast, our design applies a transition layer to adjust the channel dimensions and produces a single feature map. This feature map is then processed through subsequent blocks followed by the concatenation, forming a bottleneck structure. This approach"}, {"title": "3.4. Architectural Improvements", "content": "In this section, we will introduce the overall architecture and some improvements over the vanilla attention mechanism. Some of them are not initially proposed by us.\nMany attention-centric vision transformers are designed with the plain-style architectures [1, 18, 19, 21, 25, 51], while we retain the hierarchical design of the previous YOLO systems [3, 24, 28, 29, 32, 45-47, 53, 57, 58] and will demonstrate the necessity of this. We remove the design of stacking three blocks in the last stage of the backbone, which is present in recent versions [24, 28, 53, 58]. Instead, we retain only a single R-ELAN block, reducing the total number of blocks and contributing to optimization. We inherit the first two stages of the backbone from YOLOv11 [28] and do not use the proposed R-ELAN.\nAdditionally, we modify several default configurations in the vanilla attention mechanism to better suit the YOLO system. These modifications include adjusting the MLP ratio from 4 to 1.2 (or 2 for the N- / S- / M-scale models) is used to better allocate computational resources for better performance, adopting nn.Conv2d+BN instead of nn.Linear+LN to fully exploit the efficiency of convolution operators, removing positional encoding, and introduce a large separable convolution (7 \u00d7 7) (namely position perceiver) to help the area attention perceive position information. The effectiveness of these modifications will be validated in Section 4.5."}, {"title": "4. Experiment", "content": "This section is divided into four parts: experimental setup, a systematic comparison with popular methods, ablation studies to validate our approach, and an analysis with visualizations to further explore YOLOv12.\n4.1. Experimental Setup\nWe validate the proposed method on the MSCOCO 2017 dataset [36]. The YOLOv12 family includes 5 variants: YOLOv12-N, YOLOv12-S, YOLOv12-M, YOLOv12-L, and YOLOv12-X. All models are trained for 600 epochs using the SGD optimizer with an initial learning rate of 0.01, which remains the same as YOLOv11 [28]. We adopt a linear learning rate decay schedule and perform a linear warmup for the first 3 epochs. Following the approach in [53, 66], the latencies of all models are tested on a T4 GPU with TensorRT FP16.\nBaseline. We choose the previous version of YOLOv11 [28] as our baseline. The model scaling strategy is also consistent with it. We use several of its proposed C3K2 blocks (that is a special case of GELAN [58]). We do not use any more tricks beyond YOLOv11 [28].\n4.2. Comparison with State-of-the-arts\nWe present a performance comparison between YOLOv12 and other popular real-time detectors in Table 1.\nFor N-scale models, YOLOv12-N outperforms YOLOv6-3.0-N [32], YOLOv8-N [58], YOLOv10-N [53], and YOLOv11 [28] by 3.6%, 3.3%, 2.1%, and 1.2%"}, {"title": "4.4. Speed Comparison", "content": "Table 4 presents a comparative analysis of inference speed across different GPUs, evaluating YOLOv9 [58], YOLOv10 [53], YOLOv11 [28], and our YOLOv12 on RTX 3080, RTX A5000, and RTX A6000 with FP32 and"}, {"title": "4.5. Diagnosis & Visualization", "content": "We diagnose the YOLOv12 designs in Tables 5a to 5h. Unless otherwise specified, we perform these diagnostics on YOLOv12-N, with a default training of 600 epochs from scratch.\nAttention Implementation: Table 5a. We examine two approaches to implementing attention. The convolution-based approach is faster than the linear-based approach due to the computational efficiency of convolution. Additionally, we explore two normalization methods (layer normalization (LN) and batch normalization (BN))"}, {"title": "5. Conclusion", "content": "This study introduces YOLOv12, which successfully adopts an attention-centric design that traditionally is considered inefficient for real-time requirements, into the YOLO framework, achieving a state-of-the-art latency-accuracy trade-off. To enable efficient inference, we propose a novel network that leverages area attention to reduce computational complexity and residual efficient layer aggregation networks (R-ELAN) to enhance feature aggregation. Furthermore, we refine key components of the vanilla attention mechanism to better align with YOLO's real-time constraints while maintaining high-speed performance. As a result, YOLOv12 achieves state-of-the-art performance by effectively combining area attention, R-ELAN, and archi-"}, {"title": "6. Limitations", "content": "YOLOv12 requires FlashAttention [13, 14], which currently supports Turing, Ampere, Ada Lovelace, or Hopper GPUs (e.g., T4, Quadro RTX series, RTX20 series, RTX30 series, RTX40 series, RTX A5000/6000, A30/40, A100, H100, etc.)."}, {"title": "7. More Details", "content": "Fine-tuning Details. By default, all YOLOv12 models are trained using the SGD optimizer for 600 epochs. Following previous works [24, 53, 57, 58], the SGD momentum and weight decay are set to 0.937 and 5 \u00d7 10-4, respectively. The initial learning rate is set to 1 \u00d7 10-2 and decays linearly to 1 \u00d7 10-4 throughout the training process. Data augmentations, including Mosaic [3, 57], Mixup [71], and copy-paste augmentation [65], are applied to enhance training. Following YOLOv11 [28], we adopt the Albumentations library [6]. Detailed hyperparameters are presented in Table 7. All models are trained on 8\u00d7 NVIDIA A6000 GPUs. Following established conventions [24, 28, 53, 58], we report the standard mean average precision (mAP) on different object scales and IoU thresholds. In addition, we report the average latency in all images. We recommend reviewing more details at the official code: https://github.com/sunsmarterjie/yolov12.\nResult Details. We report more details of the results in Table 6 including $AP_{50:95}^{val}$, $AP_{50}^{val}$, $AP_{75}^{val}$, $AP_{small}^{val}$, $AP_{medium}^{val}$, $AP_{large}^{val}$."}]}