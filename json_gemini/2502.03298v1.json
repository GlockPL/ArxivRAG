{"title": "MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters", "authors": ["Amin Dada", "Osman Alperen Kora\u015f", "Marie Bauer", "Amanda Butler Contreras", "Kaleb E Smith", "Jens Kleesiek", "Julian Friedrich"], "abstract": "While increasing patients' access to medical documents improves medical care, this benefit is limited by varying health literacy levels and complex medical terminology. Large language models (LLMs) offer solutions by simplifying medical information. However, evaluating LLMs for safe and patient-friendly text generation is difficult due to the lack of standardized evaluation resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks. We use this dataset to evaluate various LLMs on patient-oriented question-answering. Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models, while automated metrics correlate with human judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the development of LLMs to enhance patient understanding and ultimately improve care outcomes.", "sections": [{"title": "1 Introduction", "content": "Access to health documents empowers patients and improves medical care (Greene and Hibbard, 2012; Lye et al., 2018; Ross and Lin, 2003). These documents, however, often use language too complex for patients to understand (Paasche-Orlow et al., 2005), and physicians have no time to simplify documents in a patient-friendly manner (Ammenwerth and Sp\u00f6tl, 2009).\nThis gap between healthcare providers and patients can be bridged by large language models (LLMs) (Ali et al., 2023; Jeblick et al., 2024; Zaretsky et al., 2024; Eisinger et al., 2025). Through their ability to simplify medical information, LLMs can enhance the access to health documents and ultimately improve patient care. However, assessing and comparing LLMs in their ability to generate safe and patient-friendly text remains challenging due to the lack of benchmarks and publicly available resources. Strict privacy regulations surrounding clinical data limit dataset accessibility, thereby impeding the development of open benchmarks for evaluating LLMs in medical contexts.\nTo address this issue, we developed MeDiSumQA. MeDiSumQA is a novel, patient-oriented question-answering (QA) dataset, a format especially suitable to improve patient understanding of clinical documents (Cai et al., 2023).\nIn this paper, we describe how we created, curated, and evaluated MeDiSumQA, crafting a standardized resource for future benchmarking. By making this task openly available to researchers, we support broader development and testing of LLMs for healthcare applications, helping address challenges of time constraints and health literacy."}, {"title": "2 Related Work", "content": "While several clinical QA datasets exist (Pampari et al., 2018; Lehman et al., 2022; Soni et al., 2022; Bardhan et al., 2022; Dada et al., 2024b; Kweon et al., 2024), none, to the best of our knowledge, are explicitly designed for patient-oriented use.\nPrior research has explored medical text simplification, but did not focus on helping patients understand clinical documents in a QA format. Aali et al. (2024) developed a public dataset that converts MIMIC hospital course summaries into concise discharge letters. Campillos-Llanos et al. (2022) created a Spanish dataset for simplifying clinical trial texts, demonstrating the importance of multilingual resources. Trienes et al. (2022) focused on making pathology reports more understandable for patients, though their dataset remains private and does not address everyday clinical questions. Similarly, while Ben Abacha and Demner-Fushman (2019)'s MeQSum dataset transforms consumer health questions into brief medical queries, it lacks strong clinical focus."}, {"title": "3 Methods", "content": null}, {"title": "3.1 Dataset Generation", "content": "In the MIMIC-IV dataset (Johnson et al., 2023), some discharge summaries conclude with a discharge letter that summarizes key information and follow-up instructions in patient-friendly language. We used these discharge letters as the foundation for generating QA pairs in the following manner (Figure 1):\nFirst, we identified discharge summaries containing discharge letters by searching for the string\u00b9 that indicates the start of a discharge letter. We split each discharge letter into sentences using Meta's Llama-3-70B-Instruct (Dubey et al., 2024), which proved more accurate than traditional sentence splitters like NLTK, especially when handling irregular formatting and placeholders introduced by anonymization. To ensure accuracy, we prompted the LLM to preserve the original sentence structure and wording, which we subsequently verified by confirming that each processed sentence could be matched exactly with its source in the original discharge letter via exact string matching.\nAfterwards, we fed these sentences into an LLM to generate matching questions from a patient's perspective. The LLM was allowed to reformulate the answer to match the question, but was instructed to reference the source sentence. We then checked these references to confirm that no information from the source document was altered. Since the answers are directly derived from the discharge letters written by medical professionals, this method maintains both medical accuracy and patient-friendly language. All mentioned prompts are listed in Appendix A.\nThe resulting QA candidates were then manually reviewed by a physician who selected high-quality examples based on the following criteria:\nFactual correctness Question-answer pairs had to be logically connected. Answers that did not match their questions (e.g., \"What medication should I avoid taking due to a possible allergy?\" - \"You were prescribed ibuprofen\") were excluded.\nCompleteness Answers had to be complete. Partial answers (e.g., \"What medications were started for me?\" - \"You were started on Vancomycin 1gm IV every 24 hours\" when additional antibiotics were prescribed) were discarded.\nSafety Answers needed to be safe. Potentially harmful instructions (e.g., \"Take Coumadin 3"}, {"title": "3.2 QA Categories", "content": "In MeDiSumQA, we identified six QA categories:\n\u2022 Symptoms & Complications\n\u2022 Procedures & Tests\n\u2022 Diagnosis\n\u2022 Treatment & Hospital Course\n\u2022 Medications\n\u2022 Post-Discharge Care & Follow-Up\nTo assign each QA pair to one of these categories, we used Meta's Llama-3.3-70B-Instruct (Dubey et al., 2024)."}, {"title": "3.3 Evaluation", "content": "We evaluated the following models on MeDiSumQA: Mistral-7B-Instruct-v0.1 (Jiang et al., 2023), Meta-Llama-3-8B-Instruct, Meta-Llama-3.1-8B-Instruct (Dubey et al., 2024), and four biomedical models derived from previously mentioned general-purpose language models: BioMistral-7B (Labrak et al., 2024), Llama3-Med42-8B (Christophe et al., 2024), Llama3-Aloe-8B-Alpha (Gururajan et al., 2024), and Meditron3-8B (OpenMeditron, 2024). We evaluated model performance on the MeDiSumQA dataset through automatic and manual assessments to ensure a comprehensive analysis."}, {"title": "3.3.1 Automatic Evaluation", "content": "We evaluated the models using established similarity metrics that capture both n-gram overlap and semantic similarity. The temperature was set to 1.0 for all models. Due to the long input length, the models were prompted with a one-shot example."}, {"title": "3.3.2 Manual Evaluation", "content": "To complement the automatic evaluation, we manually assessed 100 generated answers from two models: Mistral-7B-Instruct-v0.1, a lower-scoring model, and Meta-Llama-3.1-8B-Instruct, a higher-scoring model. For each model, we sorted the answers by the average similarity score across all automatic metrics. We then divided them into five equal-sized bins, with the lowest 20% placed in bin 1, the next 20% in bin 2, up to bin 5 containing the highest 20%. We then sampled ten predictions from each bin.\nThe answers were rated by a physician on five critical aspects:\n\u2022 Factuality: Accuracy of medical information, rated on a scale from 1 to 5.\n\u2022 Brevity: Conciseness of the response, rated on a scale from 1 to 5."}, {"title": "4 Results", "content": null}, {"title": "4.1 MeDiSumQA Description", "content": "Initially, we generated 500 QA pairs, which were reduced to 416 pairs after manual curation. Figure 3 shows three examples of the resulting QA pairs. Analysis of the QA categories in MeDiSumQA show a fairly even distribution across most categories (Figure 2). Treatment & Hospital Course make up the largest portion at 22.4%. Procedures & Tests, Medications, Symptoms & Complications, and Diagnosis each range between 17.1% and 20.9%. Post-Discharge Care & Follow-Up questions are notably underrepresented at only 4.8%."}, {"title": "4.2 Automatic Evaluation", "content": "Automatic evaluation across different LLMs reveals varying performance on MeDiSumQA (Table 1).\nMeta-Llama-3.1-8B-Instruct performed best among all tested metrics, achieving the highest scores despite being a general-domain model without specific biomedical adaptation.\nComparing biomedical-adapted models with their general-domain counterparts reveals mixed results. Some biomedical adaptations showed only marginal improvements over their base models: BioMistral-7B marginally outperformed its base model Mistral-7B-Instruct-v0.1 with a small increase of 0.45 points, while Llama3-Med42-8B showed a similar pattern with a slight improvement of 0.52 points over Meta-Llama-3-8B-Instruct. However, several biomedical adaptations performed notably worse. Most striking is the case of Llama3-Aloe-8B-Alpha, which showed a substantial decrease of 9.28 points compared to its base model Meta-Llama-3-8B-Instruct. Similarly, Meditron3-8B exhibited a considerable decline of 2.43 points relative to Meta-Llama-3.1-8B-Instruct."}, {"title": "4.3 Manual Evaluation", "content": "Manual comparison of Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.1 across factuality, brevity, patient-friendliness, relevance, and safety revealed differences between the lower and higher scoring models (Figure 4).\nIn terms of factuality, Llama-3.1-8B-Instruct demonstrated consistently high performance, maintaining scores above 4.0 across all bins, with minimal variation. In contrast, Mistral-7B-Instruct-v0.1 showed a gradual improvement from bin 1 (score 2.5) to bin 5 (score 4.3).\nIn the brevity metric, both models showed improved scores in higher bins. Llama-3.1-8B-Instruct maintained generally higher brevity scores throughout, starting at approximately 4.0 in bin 1 and reaching nearly 5.0 in bin 5. Mistral-7B-Instruct-v0.1 displayed more variable performance,"}, {"title": "5 Discussion", "content": "Here, we introduce MeDiSumQA, a benchmark dataset designed to evaluate the ability of LLMs to answer clinical questions in a patient-friendly manner. By combining automatic and manual evaluations, our study provides insights into the strengths and limitations of LLMs for patient-oriented question answering, thus narrowing the gap between complex medical information and safe patient communication."}, {"title": "5.1 Characterization of the Dataset", "content": "MeDiSumQA provides a diverse and structured set of patient-oriented QA pairs derived from discharge summaries, covering key medical topics relevant to patient care. The category distribution of MeDiSumQA indicates comprehensive coverage across six major domains, with a particular emphasis on in-hospital care, medical interventions, and treatment courses. This suggests that the dataset aligns closely with the most immediate concerns patients may have after hospitalization, such as understanding their diagnosis, medications, and follow-up care.\nWhile the dataset captures essential aspects of patient education, Post-Discharge Care & Follow-Up is underrepresented. This imbalance may reflect the structure of discharge summaries themselves, which tend to focus more on inpatient treatment rather than long-term care guidance. Expanding MeDiSumQA to include additional post-discharge documentation, such as outpatient follow-up notes or rehabilitation plans, could improve MeDiSumQA's ability to support patient education beyond hospital stays."}, {"title": "5.2 Automatic Evaluation", "content": "MeDiSumQA requires LLMs to perform multiple skills simultaneously. Models must comprehend discharge summaries to understand patient cases, extract relevant details about hospital stays, and present this information in patient-friendly language. The discharge summaries are notably long, averaging 3,245.66 tokens with a standard deviation of 1,419.91, which is a significant challenge for LLMs due to the need for effective long-context reasoning (Li et al., 2024a). Furthermore, models must possess comprehensive medical knowledge and understanding of clinical guidelines to provide accurate follow-up advice. This complex task therefore evaluates an LLM's ability to integrate comprehension, information extraction, clear communication, and medical expertise in a patient-centered context."}, {"title": "5.3 Correlation of automatic and manual Evaluation", "content": "When comparing automatic with manual evaluation, our results show that calculated metrics like ROUGE and BERT Score correlate well with human judgment. Higher automated metric scores consistently corresponded to higher manual ratings and preferences, particularly for higher-scoring predictions. Conversely, answers from lower-performing models were rarely preferred by physicians and were sometimes deemed unsafe. This correlation between manual scores and physicians' assessments validates that LLMs can be well assessed in their capability to answer medical questions in a"}, {"title": "5.4 Data Contamination", "content": "If evaluation datasets overlap with an LLM's training data, benchmark validity of these datasets is compromised due to data contamination (Li et al., 2024b; Deng et al., 2023). Such contamination can cause models to memorize rather than generalize, artificially inflating their performance. Although it is possible that some LLMs in our study have encountered parts of the MIMIC-IV dataset, this is unlikely since MIMIC-IV requires authentication for access.\nA broader concern for datasets is intentional benchmark manipulation, when models are deliberately trained on evaluation datasets, which compromises dataset reliability. One solution is to generate datasets using private, inaccessible data. To facilitate this, we offer our dataset generation pipeline as open-source, allowing hospitals and other organizations to create confidential benchmarks from their own clinical reports. By releasing our MeDiSumQA code publicly, we enable others to develop independent datasets and conduct robust LLM evaluations using private medical data."}, {"title": "5.5 Limitations", "content": "Despite its strengths, MeDiSumQA presents challenges. The dataset primarily focuses on English-language discharge summaries, limiting its applicability to multilingual settings. Additionally, while automated metrics such as ROUGE and BERT Score provide valuable insights, our manual assessments reveal that these do not always align with human judgment, particularly in terms of brevity and relevance. Future research should explore more robust evaluation methods that incorporate real-world patient feedback."}, {"title": "5.6 Outlook", "content": "We make MeDiSumQA available to the public, which offers an opportunity for widespread adoption in the medical AI community, enabling robust evaluations of models based on their ability to generate accurate, patient-friendly responses. This transparency can drive improvements in patient-centered AI by ensuring models are assessed against expert-validated benchmarks.\nDuring manual evaluation, some model-generated answers were preferred over the ground truth. This presents an opportunity to refine the dataset by incorporating high-quality model-generated responses, with physicians selecting the most appropriate answers. As this approach could introduce bias toward LLMs used in the selection process, future versions of MeDiSumQA could involve multiple independent reviewers to ensure broader generalizability.\nLastly, expanding the dataset by applying our pipeline to a larger set of discharge summaries in different languages would enable use cases beyond single-language few-shot evaluation, including fine-tuning models for improved patient-oriented applications. Making the dataset more diverse and scalable will help develop safer, more effective AI-driven healthcare solutions."}, {"title": "6 Conclusion", "content": "MeDiSumQA represents another step toward enhancing patient understanding of medical documents by providing benchmarks to assess LLMs in answering medical questions in a patient-friendly manner. By evaluating models on both automated and human-centered metrics, our study demonstrates that automatic metrics correlate well with human judgment while also highlighting the potential of general-purpose LLMs in patient education. By making MeDiSumQA accessible on PhysioNet, we aim to foster further research into the applicability of LLMs for patient-oriented question answering and encourage advancements in this field. We hope that MeDiSumQA will serve as a valuable resource for the development of more patient-friendly AI systems, ultimately bridging the gap between complex medical information and safe, effective patient communication."}, {"title": "A Prompts", "content": "Figures 6 and 7 show the prompts we use to split the discharge letter into sentences and generate question-answer pairs. For the question-answer generation we include a one shot example. Figure 8 shows the prompt we use to evaluate LLMs on MeDiSumQA."}]}