{"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "authors": ["Anwoy Chatterjee", "HSVNS Kowndinya Renduchintala", "Sumit Bhatia", "Tanmoy Chakraborty"], "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling errors, alteration of wording or the prompt template. However, while assessing the quality of an LLM, the focus often tends to be solely on its performance on downstream tasks, while very little to no attention is paid to prompt sensitivity. To fill this gap, we propose POSIX a novel Prompt Sensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering a more comprehensive evaluation of LLM performance. The key idea behind POSIX is to capture the relative change in log-likelihood of a given response upon replacing the corresponding prompt with a different intent-preserving prompt. We provide thorough empirical evidence demonstrating the efficacy of POSIX in capturing prompt sensitivity and subsequently use it to measure and thereby compare prompt sensitivity of various open-source LLMs. We find that merely increasing the parameter count or instruction tuning does not necessarily reduce prompt sensitivity whereas adding some few-shot exemplars, even just one, almost always leads to significant decrease in prompt sensitivity. We also find that alterations to prompt template lead to the highest sensitivity in the case of MCQ-type tasks, whereas paraphrasing results in the highest sensitivity in open-ended generation tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/POSIX.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are pre-trained on enormous amounts of text data using the next-token-prediction objective and they can perform a variety of NLP tasks via \u201cprompting\" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). However, LLMs have been found to be surprisingly sensitive even to the smallest of variations in prompts that do not significantly alter its meaning \u2013 such as wording, prompt template or even minor spelling errors \u2013 so much so that prompt engineering, which is a process of iteratively tuning prompts to elicit desired responses, has become a widespread practice (Reynolds and McDonell, 2021).\nDespite prompt sensitivity being a crucial aspect for assessing the usability of an LLM, standard evaluation benchmarks such as MMLU (Hendrycks et al., 2021) or BBH (Suzgun et al., 2022) focus predominantly on performance metrics like exact match, leaving prompt sensitivity sidelined. Similarly, the model cards and blog posts announcing the most commonly used LLMs often do not contain prompt sensitivity analysis at all (AI@Meta, 2024). However, from a user-centric perspective, models with low prompt sensitivity are generally preferred over highly prompt-sensitive ones, even if both perform similarly on standard benchmarks. This is largely because a real-world user may not always be able to formulate the \"optimal\" prompt everytime. Moreover, a universally applicable optimal prompt that can work across different model architectures may not even exist. Therefore, it becomes essential to develop a dedicated procedure to systematically evaluate and quantify the sensitivity of LLMs towards intent-preserving (or, intent-aligned) variations in prompts.\nWhile the exploration of the topic of quantifying prompt sensitivity is limited, there exist a few works which study prompt sensitivity and attempt to quantify it. For instance, the HELM benchmark (Liang et al., 2023) implements various kinds of perturbations to the prompts, such as typos and misspellings, and reports the exact match score of"}, {"title": "2 Related Work", "content": "Sensitivity of LLMs to Prompt Variations. The in-context learning ability of LLMs (Brown et al., 2020) makes them highly versatile, enabling them to perform a wide range of tasks through prompting, often without the need for further fine-tuning (Radford et al., 2019; Raffel et al., 2020; Gao et al., 2021). However, the robustness of in-context learning is often questioned (Weber et al., 2023), with many studies showing that the output from LLMs is heavily dependent on aspects like the selection and ordering of in-context examples (Liu et al., 2022; Su et al., 2022; Lu et al., 2022; Zhao et al., 2021), choice of input labels (Min et al., 2022), or the phrasing of instruction given in the prompt (Gu et al., 2023; Sun et al., 2024). Apart from these aspects, LLMs are also observed to be highly sensitive to slight modifications in the structure or wordings of the prompts, even though their semantic meaning remains the same. Many prior works (Arora et al., 2022; Leidinger et al., 2023; Sclar et al., 2023; Voronov et al., 2024; Mizrahi et al., 2024) have studied this issue of sensitivity of LLMs to minor alterations in the input prompt.\nFew of these works (Leidinger et al., 2023; Mizrahi et al., 2024; Voronov et al., 2024) have also called for extending the evaluation benchmarks they argue that instead of evaluating on a single instance of a prompt, the benchmarks should include multiple variants for each prompt to account for the divergence in behaviour of the models to prompt variations. While for most existing benchmarks like MMLU (Hendrycks et al., 2021) or BIG-bench (Srivastava et al., 2022; Suzgun et al., 2022), the performance is reported for a single template of the prompts, the LMentry (Efrat et al., 2023) benchmark uses three templates for each task in it and reports the average performance over them. Also, recently, Polo et al. (2024) proposed PromptEval, a method to facilitate efficient evaluation of LLMs on any benchmark with multiple prompt templates under a limited budget. Zhu et al. (2023) introduced Promptbench for evaluating the robustness of LLMs to variations done in prompts with adversarial intent \u2013 they observed that almost all LLMs lack robustness towards adversarial prompts. They quantified robustness using Performance Drop Rate, which measures the relative drop in performance when perturbations are introduced into the prompt.\nIn this work, we also advocate for benchmarks with multiple variations of the same prompt. However, instead of relying on measures based on performance alone for measuring sensitivity or robustness, we argue the need for a comprehensive measure that can capture the prompt sensitivity of LLMs effectively.\nPrompt Engineering. Due to such extensive variation in the performance of LLMs on slight modifications in input prompt, it is crucial to query LLMs with the optimal prompt to get the desired output. Prompt engineering is the practice of crafting tailored prompts for input to the LLMs to guide them towards the intended responses. Though for real-world use cases, users often perform prompt engineering manually, prior studies have also proposed ways to automate this process (Deng et al., 2022). Another method for obtaining a better input prompt is Meta-prompting (Reynolds and McDonell, 2021; Zhou et al., 2023; Ye et al., 2024), which aims to improve a prompt iteratively with the help of an LLM itself through further prompting. Furthermore, for tasks involving reasoning, Chain-of-Thought prompting (Wei et al., 2023) has been observed to be very effective.\nFor designing a few-shot prompt, the choice of in-context examples plays a crucial role in the performance of LLMs. While existing studies (Liu et al., 2022; Min et al., 2022) show that examples which are semantically similar to the input work the best, in some cases selecting diverse examples can be beneficial (Su et al., 2022; Min et al., 2022). To maximize the potential of LLMs, it is therefore crucial to be aware of the best practices for prompting."}, {"title": "3 POSIX: Prompt Sensitivity Index", "content": "Based on the notion of sensitivity introduced in Section 1, we will first define which intent-preserving variations we consider in this work and describe POSIX in detail, including a brief description of the design choices involved."}, {"title": "3.1 Preliminaries", "content": "Definition 3.1 Any two prompts $x_1$ and $x_2$ are said to be intent-aligned despite variations in their wording or template or inclusion of minor spelling errors, if they are designed to elicit responses from a language model based on the same underlying goal, intent or meaning.\nDefinition 3.2 A set of prompts $X = \\{x_i\\}_{i=1}^{N}$ is said to be an intent-aligned prompt set if and only if for all $1 \\leq i \\neq j \\leq N$, $x_i \\in X$ and $x_j \\in X$ are intent-aligned."}, {"title": "3.2 Defining the Prompt Sensitivity Index", "content": "Definition 3.3 Let $X = \\{x_i\\}_{i=1}^{N}$ be an intent-aligned prompt set and $Y = \\{y_i\\}_{i=1}^{N}$ be the set of corresponding responses generated by a language model $M$, i.e., $y_i$ is the response generated by $M$ when prompted using $x_i$. The sensitivity of the model $M$ on $X$ is defined as\n$\\psi_{M,X} = \\frac{1}{N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1 \\atop y_j \\in Y}^{N} |\\log \\frac{P_M(y_j|x_i)}{P_M(y_j|x_j)}|$\nwhere $N$ is the cardinality of $X$, $L_{y_j}$ is the number of tokens in $y_j$, $P_M(y_j|x_i)$ is the probability of the model $M$ generating the response $y_j$ given the prompt $x_i$, and $P_M(y_j|x_j)$ is the probability of the model $M$ generating the response $y_j$ given the prompt $x_j$.\nDefinition 3.4 (POSIX) Given a language model $M$ and a dataset $D = \\{X_i\\}_{i=1}^{M}$ of $M$ intent-aligned prompt sets ($X_i$'s), the prompt sensitivity index (POSIX) for the language model $M$ on the dataset $D$ is defined as\n$POSIX_{D,M} = \\frac{1}{M} \\sum_{i=1}^{M} \\psi_{M,X_i}$"}, {"title": "3.3 What does $\\psi_{M,X}$ Capture?", "content": "As mentioned briefly in Section 1, if we have two intent-aligned prompts $x_i$ and $x_j$ and the corresponding responses $y_i$ and $y_j$ generated by an LLM $M$, we would essentially like to capture how different are $P_M(y_i|x_i)$ and $P_M(y_i|x_j)$ (and also similarly for $P_M(y_j|x_j)$ and $P_M(y_j|x_i)$). In order to make the sensitivity measure comparable across different intent-aligned prompt sets as well as across models, we need to remove the dependence on the overall scale of the model's probability distributions. Therefore, we consider the ratios $\\frac{P_M(y_j|x_i)}{P_M(y_j|x_j)}$, which are immune to scale. And, since we only need to look at relative change in probabilities by replacing $x_i$ with $x_j$ or vice-versa, we convert the probability ratio to the logarithmic scale and also use the absolute value on top of it. Furthermore, in order to accommodate for arbitrary response lengths, we use length normalization for each term in the summation.\nWe now look at conceptual explanations for why $\\psi_{M,X}$ incorporates four properties listed in Section 1 while deferring the empirical evidence of the same to Section 5.1."}, {"title": "3.3.1 POSIX and Response Diversity", "content": "While not explicitly evident in the expression of $\\psi_{M,X}$, response diversity contributes indirectly to higher $\\psi_{M,X}$. Say if two responses, $y_i$ and $y_j$, are significantly different, then their log-likelihoods (given a prompt) are likely to be significantly different i.e., the terms in summation, $|log \\frac{P_M(y_j|x_i)}{P_M(y_j|x_j)}|$ become large, thereby leading to greater $\\psi_{M,X}$ overall."}, {"title": "3.3.2 POSIX and Response Distribution Entropy", "content": "By response distribution entropy, we mean the entropy of the distribution of response frequencies, i.e., how many times each unique response appears in $Y$. A higher entropy indicates the tendency of the model to generate divergent responses more often consequently, the magnitude of the log-likelihood ratios in the summation of Definition 3.3 will tend to be high, resulting in an uptick in the value of $\\psi_{M,X}$ with increase in response distribution entropy."}, {"title": "3.3.3 POSIX and Semantic Coherence", "content": "When the responses to intent-aligned prompts are semantically similar, i.e., the average cosine similarity between their embeddings is high, then intuitively the model is less sensitive. This is also captured by $\\psi_{M,X}$ because if $x_i$ and $x_j$ both generate semantically similar responses $y_i$ and $y_j$, then the probability of generating $y_j$ typically does not differ significantly for the two intent-aligned prompts $x_i$ and $x_j$ and so in such cases, the individual terms in the summation are low, leading to a lower $\\psi_{M,X}$ overall.\nRemark: It may so happen that the responses to two intent-aligned prompts $x_i$ and $x_j$ are semantically equivalent but involve very different word choices and still $P_M(y_j|x_j)$ and $P_M(y_j|x_i)$ can"}, {"title": "3.3.4 POSIX and Variance in Confidence", "content": "Consider the case where all responses to the prompts in $X$ are exactly the same, i.e., all $y_j$'s are the same. In such a case, should we call the model to be not sensitive at all? Upon a closer look, we can realize that the model would still be considered as sensitive if there is a notable variation in the likelihood of generating the response with a change in the input prompt. As evident from Definition 3.3, our proposed index $\\psi_{M,X}$ directly measures how divergent the log-likelihoods are for"}, {"title": "4 Experimental Setup", "content": "To analyse the effectiveness of POSIX in capturing various facets of sensitivity as described in Section 3 and to quantify and compare the prompt sensitivity of various LLMs using it, we experiment on Massive Multitask Language Understanding benchmark, or MMLU (Hendrycks et al., 2021), for classification tasks (posed as MCQ questions), and on Alpaca (Taori et al., 2023) for open-ended generation tasks. We also include Big Bench Hard, or BBH (Suzgun et al., 2022) as an additional dataset in Appendix E. MMLU contains about 14,000 prompt-response pairs from 57 different domains. For open-ended generation task, we sample 5,000 questions from Alpaca.\nWe consider a total of eight LLMs from three"}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Evaluating the Efficacy of POSIX", "content": "We now empirically investigate if POSIX incorporates the four factors described in Section 3 by looking at correlation plots of POSIX with those factors. For the plots, we combine data from all types of prompt variations and all models listed in Section 4. Additionally, for computing the cosine similarity between generated responses, we use an off-the-"}, {"title": "5.2 Effect of Instruction Tuning on Sensitivity", "content": "Table 1 presents the POSIX values of various models obtained for different variation types on MMLU and Alpaca (Please refer to Appendix E for results on BBH). We observe that chat or instruct versions of the models are generally less sensitive than the corresponding base models in the case of template variations on MMLU, with Mistral being the only exception. However, in the other categories of variations, the instruct versions tend to be more sensitive than their base models. Especially for open-ended generation tasks, i.e., on Alpaca, the higher sensitivity of instruct versions is even more pronounced. Note that this implies that instruction tuning does not necessarily improve model sensitivity. Figure 2 shows the distribution of POSIX values of all models for the Mixture of variation types (please refer to Appendix H for the other variation types). We clearly observe the instruct models to have higher sensitivity than the base ones, more so on Alpaca. Furthermore, the Mistral models seem to have the least divergence in sensitivity between the base and instruct variants whereas the OLMO models have the most disparity.\nAs the base models undergo both instruction tuning and alignment on human preferences to obtain the instruct versions, the above observations are a cumulative effect of instruction tuning and alignment procedures. To disentangle their consequences and study the effect of only instruction tuning on sensitivity, we separately fine-tune LLaMA-2 7B and Mistral 7B base models on the entire FLAN dataset (Wei et al., 2022). We call these fine-tuned models Llama-2-7b-FLAN and Mistral-7B-FLAN, respectively. Their POSIX values are reported in Table 3. In most cases, the chat version is better than FLAN-only models in terms of sensitivity, except in case of prompt template for MMLU, where FLAN-only models significantly outperform the chat versions. We hypothesize that this is due to the nature of the FLAN dataset, which contains a huge focus on MCQs and various prompt"}, {"title": "5.3 Impact of Model Scale on Sensitivity", "content": "To study the effect of model scale on prompt sensitivity, we experiment with 1B and 7B variants of OLMO as well as 7B and 13B variants of Llama-2. Figure 3 and Figure 4 show the variations in the value of POSIX with model scale for prompt variants from Mixture of variation types, on both MMLU and Alpaca (Please refer to Appendix E for results on BBH). We observe that the 7B model is significantly more sensitive compared to the 1B model on the MMLU dataset; however, they are comparable in the case of the Alpaca dataset, in the case of OLMo. Similarly, even in the case of Llama-2, a 13B model is not guaranteed to always have lesser prompt sensitivity than a 7B model. This only re-emphasizes the fact that accuracy and sensitivity are two separate aspects and higher accuracy does not necessarily imply better sensitivity and vice-versa. The POSIX values of OLMO-1B for all variation types are reported in Table 6 of Appendix C, and that of Llama-2-13B (base and chat) models are reported in Table 7 (for MMLU) and Table 8 (for Alpaca) of Appendix D."}, {"title": "5.4 Few-shot Exemplars and Sensitivity", "content": "Table 2 consists of POSIX values computed for the few-shot setting in the case of the MMLU dataset. The key finding is that adding few-shot exemplars, even if it just a single example can significantly boost the robustness of LLMs towards variations in prompts. Although, adding even more few-shot examples might yield diminishing gains, i.e., when compared to the value that a single example adds, the additional value of a second or third few-shot exemplar is not that much prompt sensitivity either remains about the same or slightly decreases."}, {"title": "5.5 Impact of Various Variation Categories", "content": "From Table 1, it can be observed that prompt template is the most sensitive variation type in the case of MCQs, and paraphrases are almost always the most sensitive variation type in the case of Alpaca (OLMo being the only exception). Moreover, the"}, {"title": "6 Conclusions", "content": "We introduced POSIX - a novel prompt sensitivity index, as a reliable measure of sensitivity of LLMs towards intent-preserving variations in prompts such as spelling errors, prompt templates, and alterations in the wording. We presented thorough empirical analysis for the efficacy of POSIX in capturing prompt sensitivity and subsequently used it to measure and compare multiple open-source LLMs, revealing some interesting observations such as prompt template is the most sensitive variant type for MCQ tasks and paraphrasing is the most sensitive variant type for open-ended generation tasks, and also that parameter count or instruction tuning do not necessarily decrease prompt sensitivity of the models. These findings highlight the nuanced behaviour of LLMs towards prompt variations, underscoring the importance of considering prompt sensitivity index for their holistic evaluation."}, {"title": "Limitations", "content": "While POSIX has its own advantages, like the ability to work across different kinds of prompt variations and tasks, including open-ended generation with arbitrary response lengths, one of the main limitations of POSIX is its computational complexity. POSIX needs $O(MN^2)$ log-likelihood comparisons if $M$ is the total number of prompts in a dataset under consideration and $N$ is the number of variations per prompt. Nevertheless, POSIX is very effective in incorporating various facets of prompt sensitivity."}, {"title": "Ethical Considerations", "content": "Since we use open-source large language models and open-source datasets like MMLU and Alpaca, our work encompasses all the corresponding considerations of those works. Although, our method would be expected to largely benefit the community by providing a reliable way to evaluate sensitivity of large language models towards variations in prompts. While attempting to paraphrase the prompts in MMLU using GPT-3.5-Turbo, quite a few prompts have been flagged as either violent or biased, etc. Most of them were from the moral_scenarios split of MMLU. We made sure to remove these from our analyses."}, {"title": "A Code and Data", "content": "Our code that we used for generating the variants of prompts and computing the sensitivity metric is open-sourced at https://github.com/kowndinya-renduchintala/POSIX. The code development utilized open-source tools, primarily relying on the HuggingFace library for inference, with PyTorch as the underlying framework. Both PyTorch and HuggingFace are licensed under permissive licenses, with PyTorch under the BSD license and HuggingFace under the Apache 2.0 license."}, {"title": "B Prompt Templates", "content": "The original template used for experiments on MMLU is:\nQ:{}\\n(A){} (B){} (C){} (D){}\\nA:\nThe other 20 prompt templates used for the experiments with template variations on MMLU are listed in Table 4.\nFor experiments on the Alpaca dataset which contains open-ended questions, the default template used is: Q:{}\\nA: . The other 20 prompt templates used for the experiments with template variations on Alpaca are listed in Table 5."}, {"title": "C Sensitivity of OLMo-1B", "content": "Table 6 reports the POSIX values of OLMO-1B model for different variation types on MMLU and Alpaca."}, {"title": "D Sensitivity of Llama-2-13B", "content": "Table 7 reports the POSIX values of Llama-2-13B models (base and chat variants) for different variation types on MMLU and Table 8 reports it for Alpaca dataset."}, {"title": "E Sensitivity on the BBH dataset", "content": "Table 9 reports the POSIX values on the Big-Bench Hard (BBH) dataset. Additionally, Figure 5 depicts prompt sensitivity of various models on BBH dataset in the form of box plots and Figure 6 depicts the effect of varying model scale on prompt sensitivity for BBH dataset. Please note that we only consider one kind of variation - prompt template - for this dataset. This is because in the case of BBH, many tasks like boolean_expressions, date understanding, geometric_shapes or dyck_languages, even minor spelling errors are not intent-preserving and paraphrasing would not be possible in many cases such as numerical expressions. For this dataset, we sample 2700 samples randomly from 23 tasks."}, {"title": "F Examples of generated paraphrases", "content": "Please note that we open source all the data at https://github.com/kowndinya-renduchintala/POSIX. Although, following are some examples of generated paraphrases for few open-ended questions from alpaca data:\n\u2022 Original Question: How much do you know about Buddhism?\nWhat is your awareness of Buddhism?\nWhat is your level of expertise on Buddhism?\nAre you familiar with the principles of Buddhism?\nWhat is your level of familiarity with Buddhism?\nCan you share your knowledge of Buddhism with me?\n\u2022 Original Question: Explain the concept of cognitive biases.\nInterpret the idea of cognitive biases\nExpound on the concept of cognitive biases\nElaborate on the concept of cognitive biases\nExplicate the concept of cognitive biases\nSpell out the notion of cognitive biases\n\u2022 Original Question: Describe the best way to store fresh berries.\nProvide instructions on how to store fresh berries for maximum freshness.\nOffer advice on how to best store fresh berries.\nElaborate on the best way to store fresh berries to maintain their freshness.\nDetail the optimal way to keep fresh berries fresh for longer.\nElaborate on the proper way to store a bunch of fresh berries."}, {"title": "G Efficacy of POSIX for Open-ended Generation", "content": "Figure 7 shows the correlation of POSIX with the four factors listed in Section 1 for a combination of all types of prompt variations in Alpaca, depicting the effectiveness of POSIX in successfully capturing the nuances of prompt sensitivity."}, {"title": "H Distribution of POSIX Values for All Variation Types", "content": "Figure 8 and Figure 9 depict the distribution of the values of POSIX for all models and variation types in MMLU and Alpaca, respectively."}]}