{"title": "Bootstrap SGD: Algorithmic Stability and Robustness", "authors": ["Andreas Christmann", "Yunwen Lei"], "abstract": "In this paper some methods to use the empirical bootstrap approach for stochastic gradient descent (SGD) to minimize the empirical risk over a separable Hilbert space are investigated from the view point of algorithmic stability and statistical robustness. The first two types of approaches are based on averages and are investigated from a theoretical point of view. A generalization analysis for bootstrap SGD of Type 1 and Type 2 based on algorithmic stability is done. Another type of bootstrap SGD is proposed to demonstrate that it is possible to construct purely distribution-free pointwise confidence intervals of the median curve using bootstrap SGD.", "sections": [{"title": "1 Introduction", "content": "Bootstrap is an effective statistical method to improve the generalization of machine learning models by resampling. The basic idea is to first use sampling with replacement to produce B bootstrap samples from the original training data set. Then, for the b-th bootstrap sample (b = 1,..., B), a learning algorithm is applied to produce a model $h^{(b)}$. Finally, an aggregation method is used to combine these local models for prediction. There exists a huge literature on bootstrap, and we refer to [7-9] and the references therein.\nDepending on the aggregation method, we have different types of bootstrap methods. In this paper, we consider three types of bootstrap methods for stochastic gradient descent (SGD) methods, which become the workhorse behind the success of many machine learning applications and have received a lot of attention [14, 22, 24]. For bootstrap of Type 1, we take an average of weight parameters and use the model associated to the averaged weight parameter for prediction. For bootstrap of Type 2, we first use each local model for prediction, and then take an average of these predicted outputs as the final prediction. For bootstrap of Type 3, we also first predict by each local model but then use the highly robust median of these predicted output values to obtain pointwise confidence intervals for the median and pointwise tolerance intervals.\nDue to the resampling and the aggregation scheme, bootstrap can be useful to improve the stability and robustness, which intuitively means the ability to withstand perturbations and outliers, which are both common in many applications and in routine data. We say an algorithm is stable if the output of the algorithm is not sensitive to the perturbation of a training dataset [1, 14, 20, 27]. Pioneering stability analysis rigorously shows the improvement of stability by bootstraping [10]. However, their analysis treated multiple copies of a training example as a single example, which is not the choice in practice. Furthermore, bootstrap is also useful to do inference such as building confidence intervals for some parameters of interest [7]. This inference is useful to understand how reliable the prediction is [4, 12].\nIn this paper, we study the algorithmic stability and robustness of bootstrap SGD. Our main contributions are summarized as follows."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Algorithmic Stability", "content": "Let $P$ be a probability measure defined over a sample space $Z = X \\times Y$, where $X$ is an input space and $y$ is an output space. Let $S = (z_1,..., z_n) \\in Z^n$ be a training dataset of size $n$, based on which we aim to build a function $h: X \\rightarrow Y$ for prediction. We assume the prediction function $h$ is indexed by a parameter $w \\in W$, where $W$ is a normed space and has the role of a parameter space. Let $l: R \\times R \\leftrightarrow R^+$ be a loss function, and we denote $l(h_w(x), y)$ the loss suffered by using $h_w$ to do prediction on $z = (x, y)$. For brevity, we denote $f(w; z) := l(h_w(x), y)$. The behavior of a model on training and testing is then measured by the empirical and population risk as follows\n$F_S(w) := \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_i), \\quad F(w) := E_z[f(w; z)],$\nwhere $E_z[\\cdot]$ denotes the expectation with respect to (w.r.t.) the distribution of $z$. We often apply a (randomized) learning algorithm $A$ to (approximately) minimize the empirical risk, and we denote by $A(S)$ the model derived by applying $A$ to the dataset $S$. We will assume in this paper - if not otherwise mentioned - that $W$ is a separable Hilbert space and that the (measurable) learning algorithm maps into a separable Hilbert space. Examples are of course $R^d$ or separable reproducing kernel Hilbert spaces. The relative behavior of $A(S)$ as compared to the best model $w^* = \\arg \\min_{w \\in W} F(w)$ is referred to as the excess population risk $F(A(S)) - F(w^*)$, which can be decomposed as follows\n$F(A(S)) - F(w^*) = F(A(S)) - F_S(A(S)) + F_S(A(S)) - F_S(w^*) + F_S(w^*) - F(w^*)$.\nWe refer to the first term $F(A(S)) - F_S(A(S))$ and the third term $F_S(w^*) - F(w^*)$ as the generalization gap, as they measure the difference between training and testing. We refer to the second term $F_S(A(S)) - F_S(w^*)$ as the optimization error as it measures the suboptimality of $A(S)$ as measured by the training error. The optimization error is a central concept in optimization theory and has been extensively studied in the literature [24]. The generalization gap is a central concept in statistical learning theory, which is closely related to the stability [16, 23, 26, 32] and robustness [3, 15, 29, 31] of the learning algorithm. In this paper, we will leverage the algorithmic stability to study the stability of boostrap algorithms. We first introduce several popular stability concepts. We denote $S \\sim \\tilde{S}$ if they are neighboring datasets, i.e., $S$ and $\\tilde{S}$ differ by a single example. Let $\\epsilon > 0$ and $|\\cdot |_2$ denote the $l_2$ norm."}, {"title": "3 Algorithmic Stability of Bootstrap SGD", "content": ""}, {"title": "3.1 Stability Analysis for Bootstrap SGD of Type 1", "content": "Let $S = (z_1,..., z_n)$ be a neighbouring dataset of $S$, i.e., $S$ and $\\tilde{S}$ only differ by a single example. We will assume in this paper, if not mentioned otherwise, that the learning algorithm is permutation invariant. Hence without loss of generality, we can assume that $S$ and $\\tilde{S}$ differ by the last example, i.e.,\n$z_i = \\tilde{z}_i, \\text{ if } i < n$.\nWe assume $z_1,..., z_n, \\tilde{z}_n$ are independently drawn from the same distribution. The proofs of results in this subsection are given in Section 7.1."}, {"title": "3.2 Stability Analysis for Bootstrap SGD of Type 2", "content": "In this section, we consider stability and generalization bounds for bootstrap SGD of Type 2. The following theorem gives the stability bounds. The proofs of results in this subsection are given in Section 7.2."}, {"title": "4 Distribution-free Confidence and Tolerance Intervals for Bootstrap SGD of Type 3", "content": "It is well-known that distribution-free confidence intervals for quantiles and distribution-free tolerance intervals can be constructed by certain intervals, where the endpoints are defined by order statistics,"}, {"title": "5 Numerical Example for the 3 Types", "content": "In this section, we present experimental results to show the behavior of bootstrap SGD with different types.\nFor illustration purposes only, let us demonstrate how the bootstrap methods behave in a one dimensional toy example. The data set is generated as follows. The sample size is $n = 1000$. The"}, {"title": "6 Discussion", "content": "In this paper three methods to use the empirical bootstrap approach for SGD to minimize the empirical risk over a separable Hilbert space were investigated from the view point of algorithmic stability and statistical robustness. In Type 1 and Type 2 one simply computes the average from the bootstrap approximations which yield $h_S^{(1)} = h_w$, where $w = \\frac{1}{B} \\sum_{b=1}^B w_{T+1}^{(b)}$, and $h_S^{(2)} = \\frac{1}{B} \\sum_{b=1}^B h_{w_{T+1}^{(b)}}$, respectively. These two types of bootstrap SGD have the property that the estimated functions $h_S^{(1)}$ and $h_S^{(2)}$ are both elements of the Hilbert space. In Type 3, one computes the pointwise median of the bootstrap approximations which yields $h_S^{(3)}$(x) = median$_{1 \\leq b \\leq B} \\{ h_{w_{T+1}^{(b)}}(x) \\}, x \\in X$. Our results show that these bootstrap SGD methods have some desirable algorithmic stability and robustness properties if the loss"}, {"title": "7 Proofs", "content": ""}, {"title": "7.1 Proofs for Bootstrap SGD of Type 1", "content": "The following lemma establishes the non-expansiveness of gradient operator $w \\rightarrow w - \\eta \\nabla l(w)$, which was established by Hardt et al. [14]."}, {"title": "7.2 Proofs for Bootstrap SGD of Type 2", "content": ""}]}