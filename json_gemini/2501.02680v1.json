{"title": "From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering", "authors": ["Wenran LI", "Xavier F. Cadet", "David Medina-Ortiz", "Mehdi D. Davari", "Ramanathan Sowdhamini", "Cedric Damour", "Yu Li", "Alain Miranville", "Frederic Cadet"], "abstract": "Protein design with desirable properties has been a significant challenge for many decades. Generative artificial intelligence is a promising approach and has achieved great success in various protein generation tasks. Notably, diffusion models stand out for their robust mathematical foundations and impressive generative capabilities, offering unique advantages in certain applications such as protein design. In this review, we first give the definition and characteristics of diffusion models and then focus on two strategies: Denoising Diffusion Probabilistic Models (DDPM) and Score-based Generative Models (SGM), where DDPM is the discrete form of SGM. Furthermore, we discuss their applications in protein design, peptide generation, drug discovery, and protein-ligand interaction. Finally, we outline the future perspectives of diffusion models to advance autonomous protein design and engineering. The E(3) group consists of all rotations, reflections, and translations in three-dimensions. The equivariance in the E(3) group can maintain the physical stability of the N-Ca-C frame of each amino acid as much as possible, and we reflect on how to keep the diffusion model E(3) equivariant for protein generation.", "sections": [{"title": "1. Introduction", "content": "For decades, protein engineering and protein design tasks have been regarded as NP-hard optimization problems,the algorithmic challenges continue to persist despite advancements in computational methods. (Mukhopadhyay, 2014; Pierce & Winfree, 2002). As the number of residues increases from 75 to 200, the number of conformations increases from O(n75) to O(n200), where n is the average number of rotamers per position. Researchers have been working to explore effective methods to bridge the sizeable gap. Due to their ability to learn complex patterns for large datasets, deep learning approaches have been applied to various tasks such as protein structure prediction, sequence design for specific functions, and de novo protein design (DNPD) (Watson et al., 2023). Generative modeling is a subfield of ML that focuses on developing algorithms capable of generating new data samples that resemble the data distribution from a given training dataset. Successful applications of generative modeling have highlighted the potential of protein design by modeling the probability distribution of protein sequences. Techniques such as variational autoencoders (VAE) and generative adversarial networks (GAN) have been employed on generation problems for protein sequences and structures (Rossetto & Zhou, 2019; Tucs et al., 2020). Alternatively, diffusion models have given amazing results for image, audio, and text synthesis, while being relatively simple to implement. Diffusion models are related to stochastic differential equations (SDEs), making their theoretical properties particularly intriguing. These models have shown significant advantages in modeling complex distributions and have thus gained traction in protein engineering (Tang et al., 2024a). Using their mathematical foundations, diffusion models offer a promising framework for addressing challenges in protein design.\nBuilding on these foundations, a diffusion probabilistic model (Kloeden et al., 1992) uses a parameterized Markov chain trained by variational inference. This approach enables the generation of samples that align with the data distribution within finite time, providing a structured and efficient mechanism for generative tasks. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. Diffusion models address key challenges faced by other generative approaches: they overcome the difficulty of accurately matching posterior distributions in VAEs, mitigate the instability arising from the adversarial training objectives in GANs, and excel in protein generation tasks, particularly in producing structures with improved atom stability (Chen et al., 2024a; Tang et al., 2024b; Li et al., 2024a).\nThe concept of equivariance (Batzner et al., 2022) arises naturally in machine learning of atomistic systems: physical properties have well-defined transformation properties under translation, reflection, and rotation of a set of atoms. Several reviews on the application of diffusion modeling to the generation of biomolecules have been published (Norton & Bhattacharya, 2024; Guo et al., 2023b; Zhang et al., 2023b; Goles et al., 2024); they have surveyed some diffusion models that can address various bioinformatics problems, such as denoising cryo-EM data, single-cell gene expression analysis, and protein design (for details, see Appendix. I.). However, most reviews have not discussed the common mathematical features and the importance of equivariance properties.\nThe motivation for this work is to provide advanced and comprehensive insights into the development, evaluation, and comparison of diffusion models, explaining the advantages and disadvantages of these approaches compared to other generative models, and the future directions and perspectives of diffusion models to assist the protein design.\nThe main contributions of this review include:\n\u2022 An accessible introduction to the fundamentals of diffusion models and equivariance.\n\u2022 A fairly detailed overview of the applications of diffusion models in biomolecule design (for more details, see Appendix. A.).\n\u2022 A discussion on the future development of diffusion models to assist in biomolecule design.\nThis work explores the generation of different biomolecules through diffusion models, emphasizing protein design."}, {"title": "2. Theoretical preparation", "content": "This section introduces two common diffusion models, DDPM and SGM, to lay the foundation for the following sections. In addition, we give the concepts of symmetry and equivariance and the relationship between them. The relationship between the molecular structure and the model is also revealed."}, {"title": "2.1. Diffusion models", "content": "A diffusion model is a deep generative model based on two stages: a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data are gradually perturbed over several steps by adding Gaussian noise. In the reverse phase, a model restores the original input data by learning to reverse the diffusion process step by step.\nIn the discrete form, for a sufficiently large time $T > 0$, $t \\in \\{0, 1, ..., T\\}$, with the random variable $x_0 \\in \\mathbb{R}^n$, where n is the dimension, the forward process iteratively adds isotropic Gaussian noise to the sample. The Gaussian transition kernel is set as:\n$q(x_t|x_{t-1}) = \\mathcal{N}(\\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t \\mathbb{I}),$ (1)\n$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1}),$ (2)\nwhere the $ \\beta_t $ are chosen according to a fixed variance scheme (Song et al., 2022; Croitoru et al., 2023; Rombach et al., 2022). Noisy data $x_t$ can be sampled directly from $x_0$:\n$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon,$ (3)\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\mathbb{I})$ and $\\alpha_t = \\prod_{s=1}^t(1 - \\beta_s)$.\nWhile the reverse process, starting from noise $x_T \\sim \\mathcal{N}(0, \\mathbb{I})$, aims to learn the process of denoising:\n$p_\\theta(x_0) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t);$ (4)\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_\\theta(x_t,t)),$ (5)\ni.e. to learn $p_\\theta(x_{t-1}|x_t)$ using a model with hyperparamters $\\theta$. Here\n$\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} = \\sigma_\\theta(x_t, t)\\Big),$\nthe DDPM aims to approximate $\\epsilon$ using a parametric model structured as $ \\sigma_\\theta $. The objective function can be written as follows:\n$\\theta^* = \\arg \\min_{ \\theta } \\mathbb{E}_{x_0, t, \\epsilon}[||\\epsilon - \\sigma_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)||^2].$\nIn the continuous form (Kingma et al., 2023), the following stochastic differential equation (SDE) (Kloeden et al., 1992) has the same transition distribution $q(x_t|x_0)$ as in equation (2) for any $t \\in [0,T]$:\n$dx = f(t)x dt + g(t)dw_t,$\nwhere $w_t$ is the standard Wiener process, $f(t)$ is a drift term that typically describes a time-dependent scaling of the data, and $g(t)$ is a scalar function known as the diffusion coefficient.\n(Song et al., 2020) indicated that the following time reversal SDE and probability flow ordinary differential equation (ODE) preserve the marginal distribution for $x_T \\sim p_\\theta(x_T)$:\n$dx = [f(x,t) - g(t)^2\\nabla_x \\log p_\\theta(x)]dt + g(t)dw_t,$ (6)\n$dx = [f(x, t) - \\frac{1}{2}g(t)^2 \\nabla_x \\log p_\\theta(x)]dt$ (7)\nwhere $w_t$ is the reverse Wiener process, $ \\nabla \\log p_\\theta(x) $ is the Stein score. Score-based generative models learn the gradient of the probability distribution rather than the distribution itself, i.e,\n$\\theta^* = \\arg \\min_{ \\theta } \\mathbb{E}_{x_0, t, \\epsilon} [||s_{t,\\theta}(x_t) - \\nabla_{x_t} \\log p_\\theta(x_t|x_0)||^2].$\nFurther descriptions about diffusion models are provided in Appendix. B."}, {"title": "2.2. Geometric symmetry and equivariance", "content": "Geometric symmetry and equivariance are related concepts in mathematics and machine learning, especially when dealing with transformations like rotations, translations, and reflections.\nDefinition 2.1. (Symmetry) (Cohen et al., 2021) Let X denote the input space, Y the label space, and w the weight space, let $f: X \\times W \\rightarrow Y$ denote a model. A transformation $g: W \\rightarrow W$ is a symmetry of the parameterization if\n$f(x, gw) = f(x, w)$\nfor all $x \\in X$ and $w \\in W$\nDefinition 2.2. (Equivariant) (Bronstein et al., 2021) Let $p_g: X \\rightarrow X$ be a set of transformations on X for the abstract group $g \\in G$. We say a function $f: X \\rightarrow Y$ is equivariant to g if there exists an equivalent transformation on its output space $p'_g : Y \\rightarrow Y$ such that:\n$f(p_g(x)) = p'_g(f(x)).$\nSymmetry typically refers to the static properties of shapes, patterns, or systems. It is used to describe the geometric conformation in proteins. Equivariance, on the other hand, refers to the dynamic relationships between input and output under transformations. Most of the models discussed in this paper are equivariant models."}, {"title": "3. Diffusion model for protein generation", "content": "This section discusses the generation of protein sequence and structure separately."}, {"title": "3.1. Sequence Generation", "content": "Sequence generation models usually regard amino acids as the word, input them to language models for feature extraction first, then input them to diffusion models for generation.\nTaxDiff (Zongying et al., 2024) combines the denoise transformer with the diffusion model to learn taxonomically guided over the space of protein sequences and thus fulfills the requirements of downstream tasks in biology. EvoDiff (Alamdari et al., 2023b) presents order-agnostic autoregressive diffusion models (DAOMs) and discrete denoising diffusion probabilistic models (D3PM) to generate highly realistic, diverse and structurally plausible proteins.\nDPLM (Wang et al., 2024c) initially trained with masked language models (MLMs), then continuously trained with the diffusion objective, demonstrates a strong generative capability for protein sequences. DPLM-2 (Wang et al., 2024b) is a multimodal protein foundation model that extends DPLM to accommodate both sequences and structures, where foundation models are large deep learning neural networks that have changed the way data scientists approach machine learning. For assessing the feasibility of the sequences, (Zongying et al., 2024) used OmegaFold (Wu et al., 2022) to predict their corresponding structures and calculate the average predicted Local Distance Difference Test (pLDDT) across the entire structure, which reflects OmegaFold's confidence in its structure prediction for each residue on sequences level. We compare the pLDDT of the models mentioned above in Table 1. We can see from Table 1 that the pLDDT score of the sequences sampled by DPLM-2 is close to that of DPLM. This score suggests that DPLM-2 largely retains its sequence generation capability inherited from sequence pre-training in DPLM.\nEvolutionary scale modelling (ESM) (Lin et al., 2023) is a class of language models applied to the generation of protein sequences. ForceGen (Ni et al., 2024) develops a pLDM by combining the ESM Metagenomic Atlas (Lin et al., 2023), a model of the ESM family, with an attention-based diffusion model (Ni et al., 2023) to generate a protein sequence and structure with non-mechanical properties."}, {"title": "3.2. Structure Generation", "content": "Generating a backbone is a difficult task because a backbone should fulfill the following three criteria:\n\u2022 Physically realizable: We can find the sequence that folds into the generated structure (Martin et al., 2008).\n\u2022 Functional: We aim for conditional sampling under diverse functional constraints without retraining (Mandell & Kortemme, 2009).\n\u2022 Generalizability: We hope that the model has multiple application scenarios (Murphy et al., 2012).\nFor the above criteria, we introduce some models that in our opinion best meet the standards in order, and discuss the effects of these models."}, {"title": "3.2.1. PHYSICALLY REALIZABLE MODEL: DIFFUSION ON SE(3) GROUP", "content": "SE(3) is the notation for the special Euclidean 3D group that includes translational and rotational isometric transformations and keeps the volume constant (see more details in Appendix. D). This mathematical framework is particularly relevant for modeling molecular systems, where maintaining spatial invariance is crucial for accurate predictions.\nBuilding on this principle, RFDiffusion (Watson et al., 2023) repurposes RoseTTAFold (Baek et al., 2021) to perform reverse diffusion. The SE(3)-equivariance of RoseTTAFold underpins RFDiffusion's ability to respect these isometric transformations during the generative process. RFDiffusion has also been effectively applied in the design of peptide binders (V\u00e1zquez Torres et al., 2024; Liu et al., 2024). By fine-tuning RoseTTAFold All-Atom (RFAA) (Krishna et al., 2024), a neural network for predicting biomolecular structures, to diffusion denoising tasks, RFDiffusionAA generates folded protein structures surrounding the small molecule from random residue distributions. ProteinGenerator (Lisanza et al., 2023) is a sequence space diffusion model based on RoseTTAFold that simultaneously generates protein sequences and structures. The success rate of ProteinGenerator in generating long sequences that fold to the designed structure is lower than RFDiffusion, this may reflect the intrinsic difference between diffusion in sequence and structure spaces.\nFrameDiff (Yim et al., 2023) is a diffusion model in the Lie group (Watson et al., 2022) SE(3) for the generation of protein backbones. It's forward process is,\n$\\dT(t) = \\begin{bmatrix}0, \\frac{1}{2}R^{3N}x(t)\\end{bmatrix}dt +  \\begin{bmatrix}dB^{(3)}_0, PdB^{R^{3N}}\\end{bmatrix},$\nwhere $P \\in R^{3N\\times 3N}$ is the projection matrix removing the center of mass$\\sum_{i=1}^{N} \\sum_{j=1}^{N} X_n$, and $(T = (R(t), X(t))_{t>o}$ is a stochastic process on SE(3) with invariant measure $\\mathcal{N}(0, Id) \\otimes \\mathcal{NU}(SO(3)) = $, t\u22650 push-forward by P. The backward process $(T_t)_{t\\in[0,T_F]} = ([R_t, x(t)])_{t\\in[0,T_F]}$ is given by\n$\\dR(t) =  \\nabla_{log} P_{T_F^{-1}}(T(t))dt + dB^{(3)}_{(t)},$ (8)\n$\\dX(t) = P\\{X(t) +  \\nabla_{log} P_{T_F^{-1}} (T(t))\\}dt +PdX.$ (9)\nThis model applies Invariant Point Attention (IPA) (Jumper et al., 2021) to keep the updates of residues in coordinate space that are SE(3)-invariant."}, {"title": "3.2.2. MODEL WITH STRONG FUNCTIONALITY", "content": "Protein design projects often involve complex and composite requirements that vary over time. Chroma (Ingraham et al., 2023) explores a programmable generative process with custom energy functions, which aims to make the generated protein have desired properties and functions, such as symmetry, substructure, shape and semantics."}, {"title": "3.2.3. MODEL WITH GENERALIZABILITY", "content": "AlphaFold3 (AF3) (Abramson et al., 2024) exhibits strong generalizability and versatility, extending beyond protein generation to handle diverse molecular tasks, including ligand and RNA structure prediction. AlphaFold2 (AF2) (Jumper et al., 2021) is a highly accurate protein structure prediction model. Its two important components, Evoformer and IPA, have been widely used in other models. AlphaFold3 replaces its Structure Module part with a Diffusion module. The component of the Diffusion module, Diffusion Transformer (Peebles & Xie, 2023), shows great generative ability."}, {"title": "4. Diffusion model for peptide generation", "content": "Peptides have aroused great interest due to their potential as therapeutic agents (Wang et al., 2022). Currently, there are several reviews (Wan et al., 2022; Ge et al., 2022; Goles et al., 2024) that summarize the application of generative models to peptides. Here, we focus on peptide generation by diffusion models.\nFor the design of peptide sequences, ProT-Diff (Wang et al., 2024d) combines a pre-trained protein language model (PLM) ProtT5-XL-UniRef50 (Elnaggar et al., 2020) with an improved diffusion model to generate de novo candidate sequences for antimicrobial peptides (AMPs). AMP-Diffusion (Chen et al., 2024c) uses PLM ESM2 (Lin et al., 2023) for latent diffusion to design AMP sequences with desirable physicochemical properties. This model is versatile and has the potential to be extended to general protein design tasks. Diff-AMP (Wang et al., 2024a) integrates thermodynamic diffusion and attention mechanisms into reinforcement learning to advance research on AMP generation. Sequence-based diffusion models complement structure-based approached by aiding in sequence-to-function or optimizing sequence design for structural goals.\nFor peptide structure design, Pepflow (Abdin & Kim, 2023b) trains the diffusion model to generate the peptide structure and then uses E(3)-equivariant graph neural networks (EGNN) to perform conformational sampling. This model can generate a variety of all-atom conformations for peptides of different lengths, and comparative experiments were performed with AF2 and ESM-fold.\nFor the co-design of peptides, PepGLAD (Kong et al., 2024) proposes geometric latent diffusion model combining with receptor-specific affine transformation to do the full-atom peptide design. MMCD (Wang et al., 2024e) completes the co-generation of structure and sequence for both antimicrobial and anticancer peptides. It also uses EGNN for the structure generation part."}, {"title": "5. Small molecule generation", "content": "Molecules live in physical 3D space, there is a high need to better understand the design space of diffusion models for molecular modeling. The topic of generating molecules using diffusion models is equivalent to the following question: How to generate attributed graphs using diffusion models? To answer this question, there are two main challenges:\n\u2022 Complex dependency: Dependency between nodes and edges.\n\u2022 Non-unique representations: Order of the nodes is not fixed.\nFor the first challenge, diffusion models need to define the atomic positions $x_i \\in \\mathbb{R}^3$ and the atomic types $a_i = \\{C,N,O, ...\\}$ and specify independent forward processes for each data type,\n$p_t(x_t|x_0) = \\mathcal{N}(x_t|a_t x_t, \\sigma_t \\mathbb{I}),$ (10)\n$p_t(a_t| a_0) = \\mathcal{N}(a_t|a_t a_t, \\sigma_t \\mathbb{I}),$ (11)\nIf $G_t = (x_t, a_t)$, then $p_t(G_t| G_0) = \\mathcal{N}(x_t|a_t G_t, \\sigma_t \\mathbb{I})$, and continuously forward process represented as\n$dG_t = f_t(G_t)dt + g_t(G_t)dw_t,$\nThe reverse-time diffusion process is represented as:\n$\\begin{cases}dx_t = [f_{1,t}(x_t) - g_{1,t}\\nabla_{x_t} log p_t(G_t)]dt + g_{1,t}dw_1, \\\\da_t = [f_{2,t}(x_t)-g_{2,t}\\nabla_{a_t} log p_t(G_t)]dt + g_{2,t}dw_2.\\end{cases}$ (12)\nwe use $s_x(G_t), s_a(G_t)$ to approximate $ \\nabla_{x_t} log p_t(G_t), \\nabla_{a_t} log p_t(G_t) $ respectively, and train the neural network to jointly approximate the score functions of the"}, {"title": "5.1. Permutation equivariant", "content": "A model is called equivariant to permutation if its permute input is equivalent to permute output.\nGDSS (Jo et al., 2022) is a novel permutation equivariant one-shot diffusion model. It can generate valid molecules by capturing the node-edge relationship. CDGS (Huang et al., 2023a) incoporates discrete graph structures into a diffusion model. It is permutation equivariant and implicitly defines the permutation invariant graph log-likelihood function.\nDiGress (Vignac et al., 2023a) is also a permutation equivariant architecture with a permutation invariant loss. The main difference from GDSS is that DiGress defines a diffusion process independent of each node and edge. DiGress achieves better performance than GDSS on QM9 dataset (Ramakrishnan et al., 2014) with simpler architecture. JODO (Huang et al., 2023b) proposes a diffusion graph transformer to generate 2D graph and 3D geometry molecule generation. Without extra graph structural and positional encoding, JODO-2D is comparable to, or better than, DiGress in most metrics."}, {"title": "5.2. Diffusion model on SE(3) group for molecule", "content": "We have discussed the important role of the SE(3) equivariant model in protein structure generation before, here we discuss its application in molecule generation.\nGeoDiff (Xu et al., 2022) integrates the diffusion model with graph neural networks (GNN) to generate stable conformations, the difference being that the GNN is SE(3)-invariant. SubGDiff (Zhang et al., 2024) incorporates subgraphs into the diffusion model to improve molecular representation learning. With 500 steps, SubGDiff achieves much better performance than GeoDiff with 5000 steps on 5 out of 8 metrics, which implies that it can accelerate the sampling efficiency.\nBoth Target Diff (Guan et al., 2023) and DiffBP (Lin et al., 2024a) propose a target-aware molecular diffusion process with a SE(3)-equivariant GNN denoiser. The training and sampling procedures in TargetDiff are aligned in non-autoregressive and SE(3) equivariant."}, {"title": "5.3. Models based on EGNNS", "content": "We consider the rotation, reflection, and translation group in R\u00b3, abbreviated as E(3). Since biomolecular structures align with elements in the E(3) group, E(3)-equivariant neural networks are effective tools for analyzing molecular structures and properties.\nE(3) Equivariant diffusion model (EDM) (Hoogeboom et al., 2022) Learns a diffusion model that is equivariant to translation and rotation. It operates on continuous and categorial features to generate molecules in 3D space. DiffLinker (Igashov et al., 2024) leverages EDM and develops diffusion models for molecular linker design.\nCGD (Klarner et al., 2024) can consistently generate novel, near-out-of-distribution (near-OOD) molecules with desirable properties. CGD also applies to EDM for material design following the setup of GaUDI (Weiss et al., 2023), which can discover molecules better than existing ones. SILVR (Runcie & Mey, 2023) combines ILVR (Choi et al., 2021) and EDM to do fragment merging and linker generation.\nBy building point-structured latent codes with invariant scalars and equivariant tensors, GeoLDM (Xu et al., 2023) can effectively learn latent representations while preserving roto-translational equivariance. It also circumvents the limitations of EDM on irregular training surfaces. SubDiff (Yang et al., 2024) performs"}, {"title": "6. Protein-ligand interaction", "content": "DiffDock (Corso et al., 2023b) uses an equivariant graph neural network in a diffusion process, and predicts the 3D structure of how a molecule interacts with a protein . DockGen (Corso et al., 2024) improves upon DiffDock by scaling up the training data and model size, as well as integrating a synthetic data generation strategy based on extracting side chains from real protein structures as ligands. It is faster and better suited for bootstrapping.\nDiffDock-PP (Ketata et al., 2023) learns to translate and rotate unbound protein structures into their bound conformations. DiffDock-site (Guo et al., 2023a) is a novel paradigm that integrates the precision of the point site for identifying and initializing the docking pocket. It notably outperforms DiffDock in several metrics. Its DiffDock-site-P variant stands out by integrating the pretrained DiffDock for refining ligand attributes. By introducing discrete latent variables to DiffDock, DisCo-Diff (Xu et al., 2024) improves performance on molecular docking and can also synthesise high-resolution images.\nFABind (Pei et al., 2024) takes independent message passing, cross-attention update, and interfacial message passing together, to build a fast and accurate protein-ligand binding model. FABind+ (Gao et al., 2024a) is enhanced by introducing Huber loss in dynamic pocket Radius Prediction and permutation loss in Docking structure prediction.\nNeuralPlexer (Qiao et al., 2023) incorporates essential biophysical constraints and a multi-scale geometric deep learning system for the diffusion process. For generating the ligand-specific protein-ligand complex structure, a deep equivariant generative model named DynamicBind (Lu et al., 2024) is employed. DynamicBind predicts the ligand-specific protein-ligand"}, {"title": "7. Discussion", "content": "Diffusion models have already demonstrated their advantages over previous traditional and machine learning approaches by setting new state-of-the-art results in numerous problems. In addition, some basic models have also been frequently used in protein generation recently, such as EGNN, RoseTTAFold, IPA and ESM; these models have derived some new models. Here, we highlight several landmark models:\n\u2022 The IPA in AlphaFold2 satisfies the property of SE(3) equivariant, but was replaced by the diffusion transformer in AlphaFold3. Therefore, Alphafold3 does not satisfy the properties of an equivariant.\n\u2022 The reverse diffusion in RFDiffusion is composed of RoseTTAFold. This model inherits the good properties of RoseTTAFold, making the generated model physically realizable.\n\u2022 FrameDiff is the first model to introduce SE(3) manifolds into protein structure generation problems. The properties of the SE(3) group provide a mathematical basis for the expression of structural information.\n\u2022 As a better type of SE(3) equivariant, E(3) equivariant is widely used in the generation of small molecules. The most successful example so far is EDM.\n\u2022 DiffDock is the first model to introduce the use of diffusion models in the molecular docking task, and its performance is very close to traditional methods. Several works proposed different modifications to its framework.\nDue to the large size and complexity of protein structures, most current protein models can only satisfy SE(3) equivariance but do not have as good properties as E(3) equivariance. How to establish a diffusion model in the E(3) group to complete protein production is a topic we can study in the future.\nWhile progress in the field has demonstrated that diffusion models can accelerate early-stage drug discovery, challenges remain in adapting such workflows to real-world discovery campaigns:\n\u2022 Addressing synthesizability is an ongoing challenge, because many proposed ideas may not have known synthetic routes, and a chemist can only triage a function of proposed ideas.\n\u2022 Despite various widely adopted evaluation metrics, measuring and comparing the performance of diffusion models remains a major challenge given the lack of ground-truth and universal metrics.\n\u2022 Complex dynamics. Cohesive models tend to be static and ignore the fact that proteins and ligands are amphipathic, which is a factor that should be considered when analyzing protein functions.\n\u2022 Protein structure prediction models typically predict static structures as seen in PDB, not the dynamical behavior of biomolecular systems in solution.\nWhat are potential directions the community could consider exploring further?\n\u2022 RFDiffusion and ProteinGenerator, which adapt the diffusion model with the traditional model, RoseTTAFold, have done a variety of tasks, such as peptide binder generation, motif-scaffolding, and sequence-structure codesign. We can explore more applications of these two models.\n\u2022 There are so few models in the area of diffusion models for peptide design that similar diffusion models for protein design can probably be extended to design peptides.\n\u2022 Traditional models are more analytical and closely match the physical properties of proteins. We can use them for more fruitful tasks such as protein-nucleic acid and protein-ligand interactions.\n\u2022 Can ETNN deformations of EGNN (Battiloro et al., 2024) and NequIP (Batzner et al., 2022) be applied to the generation of molecules? Can EGNN be used to study peptide structures?"}, {"title": "8. Conclusion", "content": "This review comprehensively summarizes the application of the diffusion model for bioengineering. It captures the progression of AI model architectures, highlighting the emergence of E(3) equivariant GNN (EGNN) and diffusion models as game changers in recent work. Diffusion Models are particularly promising generative frameworks."}, {"title": "A. Model overview", "content": "This section presents the glossary and mindmap for the models."}, {"title": "A.1 Glossary", "content": "We describe the terms that are important to our review.\nTerms\nSE(3)-\nequivariance\nAtom stability\nperformance\nDescription\nGiven an input point cloud P, and a random rotation matrix R,\nthe network N satisfies N(PR) = N(P)R. This kind of network will\nkeep the physical structure.\nAtom stability is calculated as the ratio of atoms exhibiting correct\nvalency, while molecule stability reflects the function of the generated\nmolecules in which each atom maintains stability.\nMap the protein sequence to a word-probability latent space. using a pretrained protein language model (pLM) and train a diffusion\nmodel to learn the map between sequence representations.\nDesignable backbones have optimal secondary structure configurations\nwith favored tertiary structure symmetries such that they are physically\nrealizable with the 20 natural AAs.\n\nProtein\n\nA.2. Mindmap for models\nFig. 5 is a overview of models and the relationships among the models."}, {"title": "A.3. Model list", "content": "Models mentioned in this review have been implemented as open-source tools. We list their task, input, output,\ndataset for training, data size, and code link in Table 3. There are 16 models for protein design, 7 models for\npeptide generation, 24 models for small molecule generation, and 9 models for protein-ligand interaction, i.e., 56\nmodels in total. This table may help users with their research problems and help developers further improve\nthem."}, {"title": "B. Extension of Diffusion Models", "content": "We only introduced the representation of the diffusion model in the main text. In this part, we supplement its derivation, application, and improvement."}, {"title": "B.1. Loss function of DDPM", "content": "Definition B.1. (KL divergence) (Li et al.", "lower\nbound)": "n$-L_{VAE} = log"}]}