{"title": "EasyJudge: an Easy-to-use Tool for Comprehensive Response Evaluation of LLMS", "authors": ["Yijie Li", "Yuan Sun"], "abstract": "Recently, there has been a growing trend of employing large language models (LLMs) to judge the quality of other LLMs. Many studies have adopted closed-source models, mainly using GPT-4 as the evaluator. However, due to the closed-source nature of the GPT-4 model, employing it as an evaluator has resulted in issues including transparency, controllability, and cost-effectiveness. Some researchers have turned to using fine-tuned open-source LLMs as evaluators. However, existing open-source evaluation LLMs generally lack a user-friendly visualization tool, and they have not been optimized for accelerated model inference, which causes inconvenience for researchers with limited resources and those working across different fields. This paper presents EasyJudge, a model developed to evaluate significant language model responses. It is lightweight, precise, efficient, and user-friendly, featuring an intuitive visualization interface for ease of deployment and use. EasyJudge uses detailed datasets and refined prompts for model optimization, achieving strong consistency with human and proprietary model evaluations. The model optimized with quantitative methods enables EasyJudge to run efficiently on consumer-grade GPUs or even CPUs. We also provide detailed analysis and case studies to further reveal the potential of our method.", "sections": [{"title": "1 Introduction", "content": "The evaluation of response quality from large language models (LLMs) has been a central concern within the research community. As the instruction-following capabilities of LLMs continue to evolve, a more comprehensive and precise evaluation of their responses becomes particularly crucial."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Evaluation Based on Reference Texts", "content": "Traditional model-free scoring methods like BLEU and ROUGE were widely used but have limitations in evaluation reliability. Recent model-based methods, such as BERTScore, BLEURT, and BARTScore, improve evaluation by capturing semantic-level information. EasyJudge visually compares responses using metrics like ROUGE, BLEU, and BERTScore, offering users a more comprehensive and intuitive evaluation of models."}, {"title": "2.2 LLM-Based Text Evaluation", "content": "Recent research has shifted towards using large language models as evaluators, employing GPT-4 or fine-tuned Judge LLMs to assess the text quality generated by other models. Recent studies have shown that ChatGPT can outperform crowd-sourced workers in text annotation tasks. Using closed-source models like GPT-4 for evaluation poses challenges, including high costs, privacy risks, and limited control. Fine-tuned open-source Judge LLMs, such as PandaLM, AUTO-J, PROMETHEUS, JudgeLM, and Eval-Instruct, have been developed to overcome this. These models offer cost-effective, reliable evaluation solutions, addressing issues like data leakage, evaluation bias and adapting to diverse tasks. They collectively advance LLM evaluation by integrating subjective criteria, enhancing multimodal and dialogue tasks, and providing alternatives to closed-source models.\nHowever, current LLM-as-Judge research typically only provides fine-tuned LLMs, lacking an intuitive and user-friendly visualization interface specifically optimized for LLM evaluation. This presents challenges for users who seek a simple and efficient one-stop solution for evaluating individual responses or entire texts. Additionally, users are unable to intuitively access evaluation results from these models. A comparison between EasyJudge and these evaluation models is provided in Table 1."}, {"title": "3 System Overview", "content": "This section provides a detailed overview of the EasyJudge system. As shown in Figure 1, EasyJudge consists of three key components:"}, {"title": "3.1 Data Processing Module", "content": "We collect real-world interaction data between humans and Large Language Models to create the initial Instruction Dataset. A classifier is then trained to categorize this instruction data. Additionally, GPT-4 is employed to expand the instructions through prompts. Multiple open-source large models are then used to generate responses to the instruction data. Finally, GPT-4 is invoked with carefully designed prompts that include detailed evaluation criteria to produce evaluation results. The data is then integrated for use in the subsequent model fine-tuning."}, {"title": "3.2 Evaluation Model Training", "content": "This is the core of EasyJudge, where the LLaMA-3-8b base model is fine-tuned using the multi-scenario, multi-criteria instruction data obtained from the data processing phase. The result is the POINTWISE model for direct evaluation and the PAIRWISE model for pairwise comparison evaluation. Next, model merging techniques are applied to integrate the performance of both models. Finally, model quantization techniques are used to optimize the model."}, {"title": "3.3 User-Friendly Interface", "content": "The evaluation process of model responses is designed to be transparent, offering users an intuitive interface with several key features. These include selecting models, adjusting model parameters, configuring evaluation scenarios, and customizing evaluation criteria. Additionally, the interface provides a clear visualization of the evaluation results. For example, it displays the outcomes of pairwise comparisons and direct scoring in a straightforward manner, offers detailed feedback, and presents multi-dimensional score references to help users better understand the evaluation process."}, {"title": "4 Implementation Details", "content": "To better understand the evaluation process within EasyJudge, this section will explain the implementation of three key issues."}, {"title": "4.1 Data Processing", "content": ""}, {"title": "4.1.1 Definition of Evaluation Scenarios and Criteria", "content": "To ensure a more accurate and context-relevant evaluation of LLM responses, EasyJudge, based on prior research, categorizes evaluation scenarios into 50 distinct types, which are further summarized into nine broader categories: text generation and writing, information extraction and analysis, mathematics and logical reasoning, code tasks, QA, reasoning and judgment, role-playing and conversation, basic NLP tasks, and a default type. It is intuitive to understand that the evaluation criteria for responses in different scenarios, such as code generation and writing a project proposal, should vary significantly. Therefore, EasyJudge customizes evaluation criteria for each of the 50 distinct scenarios. Each category includes 8-10 evaluation criteria, totalling 134 unique criteria divided into four main categories: Basic, Style, Content, and Format."}, {"title": "4.1.2 Dataset Construction", "content": "High-quality datasets are crucial for effectively fine-tuning large language models to serve as evaluation judges. However, existing datasets and prior research often lack sufficient diversity and detailed evaluation criteria. To address these issues, EasyJudge introduces a new dataset that includes various seed tasks across different evaluation scenarios, comprehensive answers from multiple open-source LLMs, scoring results from a teacher LLM across various criteria dimensions, and detailed reasoning behind each evaluation.\nEasyJudge extracts 15k seed tasks from a large. A classification model is then used to categorize the instructions based on the scenario definitions described in section 4.1.1. For scenarios with limited instructions, GPT-4 is employed to supplement them using the self-instruct method. The prompt template used for this process is provided in Figure 3. To enhance the diversity of the dataset, we aggregate responses from multiple open-source LLMs, including but not limited to LLaMA, Alpaca, and Vicuna. Next, we combine the LLM-generated responses with reference answers to create an answer set. For PAIRWISE tasks, two responses from different open-source models are randomly selected from the answer set for the same instruction. An advanced teacher model, GPT-4, is then used to assign detailed scores and provide thorough reasoning for the comparison. For POINTWISE tasks, a response from an open-source model is randomly selected from the answer set for a given instruction. The advanced teacher model, GPT-4, then assigns detailed scores and provides comprehensive reasoning for the evaluation. To ensure robust and comprehensive judgments, we utilized detailed prompt templates, the specifics of which are provided in Figure 4 and Figure 5. The prompt contains critical inputs such as the scenario, evaluation criteria, instruction, and response to be evaluated, along with the evaluation requirements and output format. Including these details ensures the model produces clear, comprehensive, and accurate evaluation results."}, {"title": "4.2 Evaluation Model Fine-Tuning", "content": "The data required to train the EasyJudge model is constructed by integrating the datasets mentioned in section 4.1.2. The training data follows the Alpaca fine-tuning format, which consists of four components: instruction, input, output, and system. The instruction includes the task to be evaluated and its corresponding response, evaluation requirements and the output format; the input is left empty by default, the output contains the scores and reasoning provided by the teacher model GPT-4, and the system includes the scenario and evaluation criteria. The data templates are in Figure 6.\nTo reduce positional bias in PAIRWISE comparisons, EasyJudge applies a simple data augmentation technique. According to the judgelm, For each pairwise training sample, the order of the two responses in the input is randomly swapped. Additionally, to enhance the model's ability to handle unknown responses, EasyJudge randomly drops the reference for each data point."}, {"title": "4.3 User-Friendly Interface Development", "content": "To make the evaluation process of large language models more intuitive and user-friendly, we developed a Streamlit-based interface. Using the Streamlit framework, we created a transparent and responsive interface. This interface not only supports data upload and parameter adjustments but also allows users to select evaluation scenarios dynamically through radio buttons. The evaluation results are presented in a rich format, including text and graphical representations, ensuring that users can easily interpret the meaning of the model's output. This design significantly reduces the operational complexity of EasyJudge and enhances user experience, enabling even non-expert users to perform advanced model evaluations efficiently. This intuitive interface and the model's efficient computation capabilities can meet diverse evaluation needs, particularly in resource-constrained environments."}, {"title": "5 Demonstration Scenarios", "content": ""}, {"title": "5.1 Diversity Classification", "content": "Figure 2 provides a screenshot of the EasyJudge user interface, through which users can perform large model response evaluations by following these steps:\nStep 1 (Task Configuration): As shown in Figure 2-1, the configuration interface guides users through the initial setup. Users begin by selecting the evaluation task type, which includes single response direct scoring (POINTWISE) and pairwise response ranking (PAIRWISE). The system automatically selects the appropriate prompt template based on the chosen scoring strategy. After selecting the task, another essential configuration allows users to adjust the EasyJudge model parameters, such as temperature, Top-p, and max_length, according to their specific needs to achieve optimal evaluation results.\nStep 2 (Scenario and Criteria Configuration): Next, users can select specific task scenarios and criteria tailored to their evaluation data, a crucial advantage of EasyJudge's highly customizable system. The evaluation criteria configuration is shown in Figure 2-2. Once users select a task scenario, EasyJudge conducts a multi-dimensional evaluation based on the specific criteria. Alternatively, users can opt not to select a scenario where EasyJudge will evaluate the model response using the default scenario. The default scenario encompasses ten standard evaluation criteria suitable for most task evaluations. For more customized evaluation results, users can manually select the criteria. EasyJudge currently offers 40 evaluation criteria across four main categories for tailored evaluation. If custom criteria are selected, the system will automatically bypass scenario selection.\nStep 3 (Data Upload): As shown in Figure 2-3, EasyJudge provides two methods for data upload. Suppose a user is evaluating a single data instance. In that case, they can sequentially copy the instruction, Model 1's response, Model 2's response, and the reference answer into the corresponding input fields, then click the submit button to initiate the evaluation. To evaluate multiple data instances, users must upload a JSON/JSONL file containing the evaluation data in Alpaca format. After uploading the file, users can click the submit button to start the evaluation process. The interface for single data evaluation in the POINTWISE mode is shown in Figure 8.\nStep 4 (Results Display): In this step, EasyJudge presents the final evaluation results, providing users with clear evaluation information that helps them intuitively understand the quality of the responses. Taking PAIRWISE evaluation as an example (Details on POINTWISE evaluation can be found in Appendix C.), as shown in Figure 2-4, the top of the page displays the final evaluation results. At the same time, the middle section presents the Detailed Evaluation Feedback from the EasyJudge model. At the bottom, three charts (labeled as Figures A, b, and c) are provided: Figure A shows the scores of Response A and Response B across different evaluation criteria dimensions and compares the two responses in each dimension. Figure b compares Response A with the reference text, displaying evaluation results based on traditional metrics, including ROUGE, BLEU, BERTScore, BLEURT, and BARTScore. Figure c compares Response B with the reference answer evaluated using traditional metrics. Finally, if users choose to upload a JSON/JSONL file, then they can finally download a JSON file containing the evaluation result data by clicking the \"Download\" button."}, {"title": "6 Conclusion", "content": "This paper introduces EasyJudge, an innovative tool for evaluating LLMs with advanced models, offering a customizable interface and precise, multi-dimensional assessments. It enhances efficiency through model quantization, enabling use on consumer-grade GPUs and CPUs. Future research plans include integrating new technologies to extend EasyJudge's capabilities to evaluate multimodal models, Retrieval-Augmented Generation (RAG), and intelligent agents, contributing to advancing Artificial General Intelligence (AGI) while improving evaluation accuracy and practicality across diverse scenarios."}]}