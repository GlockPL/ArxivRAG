{"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "abstract": "Knowledge editing allows for efficiently adapting large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on single-language or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; Yang et al., 2024a; Guo et al., 2025) have shown strong capabilities in natural language understanding, generation, and reasoning (Wei et al., 2022; Zhong et al., 2023a; Peng et al., 2023; Lu et al., 2024; Zhao et al., 2023). However, as world knowledge evolves, LLMs need methods to update outdated information efficiently. Knowledge editing (Yao et al., 2023) allows modifications to specific knowledge while preserving un-"}, {"title": "2 Preliminary", "content": "Knowledge editing selectively modifies in-scope knowledge while preserving out-of-scope behavior. Given a base LLM $p_\\theta$ and an edit descriptor $(x_e, y_e)$, where $x_e$ is the modification description and $y_e$ is the corresponding answer, the edited model should adhere to four key properties:\nReliability evaluates accuracy on edit descriptors:\n$E_{(x_e,y_e)\\sim X_e}1 [arg \\max_y p_\\theta(y|x_e) = y_e]$                                                      (1)\nGenerality assesses the precision of semantically rephrased examples:\n$E_{(x_{par},y_e)\\sim X_{par}} 1 [arg \\max_y p_\\theta(y|x_{par}) = y_e]$\nLocality ensures that out-of-scope inputs remain unchanged:\n$E_{(x_e,y_e)\\sim O_e}1[P_\\theta(Y|x_e) = P_\\theta(Y|\\tilde{x_e})]$ (3)\nPortability measures the ability to transfer updated knowledge to related queries:\n$E_{(x_e,y_e)\\sim I}1[arg \\max_y p_\\theta(y|x_e) = Y_e]$                                                      (4)"}, {"title": "2.2 Cross-Lingual Knowledge Editing", "content": "Cross-lingual knowledge editing extends monolingual knowledge editing by requiring a multilingual LLM $P_{m\\theta}$ to propagate knowledge from a source language to a target language. Given an edit descriptor in the source language $(x^s_e, y^s_e)$, the goal is to maximize:\n$E\\limits_{(x,y)\\sim X\\atop {x_t=I_t(x),y=I_t(y)}} 1[arg \\max_y p_{m\\theta} (y|x_t) = y_t]$\n$E\\limits_{(x,y)\\sim O\\atop {x_t=I_t(x),y=I_t(y)}} 1[P_{m\\theta}(Y|X_t) = P_{m\\theta} (Y|\\tilde{X_t})]$\nHere, $x_t, y_t$ are the edit descriptors in the target language $t$, and $I_t(\\cdot)$ translates the source language input into the target language. Cross-lingual knowledge editing also requires cross-lingual comprehension, ensuring that updates in the source language lead to consistent responses in the target language."}, {"title": "3 Methodology", "content": "To achieve the democratization of knowledge, we propose the Cross-Lingual Knowledge Democracy Edit (X-KDE) framework, as shown in Figure 2. This framework enables LLMs to adapt to evolving"}, {"title": "3.1 XE-IT Stage: Learning to Edit", "content": "The goal is to enable the model to leverage knowledge edits in the source language while preserving the unchanged information. To meet the requirements for cross-lingual editing, we carefully constructed a high-quality dataset and employed XE-IT to fine-tune the model."}, {"title": "3.1.1 Dataset Construction", "content": "Data Sources. Our goal is to enable the model to use edit descriptors effectively while maintaining unrelated information. We use several widely used knowledge editing datasets, including ZsRE (Levy et al., 2017), HalluEditBench (Huang et al., 2024a), RIPPLEEDITS (Cohen et al., 2024), WikiBio (Hartvigsen et al., 2024), MQUAKE (Zhong et al., 2023b), and COUNTERFACT (Meng et al., 2022a), to build our training data. These datasets provide edit descriptors paired with QA pairs. To mitigate data leakage, we randomly sample and translate subsets for training."}, {"title": "Sample Generation", "content": "Existing datasets often feature straightforward QA pairs, which limit the model's comprehension ability. To address this, we use Deepseek (Liu et al., 2024) to generate complex in-scope and out-of-scope query-answer pairs. This method enhances training data quality and model comprehension."}, {"title": "Quality Control", "content": "To ensure relevance, we use Deepseek to assess the quality of in-scope query-answer pairs. Samples are scored based on Syntactic Structure, Lexical Richness, and Edit Consistency, with low-quality samples filtered out and replaced by higher-scoring ones."}, {"title": "Translation Process", "content": "We use large language models, i.e., Deepseek\u00b9 to translate the generated data from English to Chinese."}, {"title": "Parallel Data Construction", "content": "Our dataset follows a parallel structure (Figure 2(b)), with the in-scope section guiding LLMs when to use updated knowledge and the out-of-scope section minimizing the impact on unrelated knowledge. The dataset includes both monolingual and cross-lingual sections, where the source language contains edit descriptors and the target language contains queries and answers. Further details about our dataset are pro-"}, {"title": "3.1.2 Fine-Tuning", "content": "Thanks to the flexible parallel structure, we can adaptively select the source and target languages to satisfy specific needs. We create a large-scale cross-lingual dataset and compute loss based only on the answer tokens. The model generates answers in the target language given source-language edit descriptors and target-language queries."}, {"title": "3.2 TL-PO Stage: Preference Optimization", "content": ""}, {"title": "3.2.1 Preference Scoring", "content": "After the XE-IT phase, the model has initially acquired the ability for cross-lingual knowledge editing. However, when faced with queries in the target language, the model may still make mistakes, such as generating responses in the source language, producing surface-level transliterations, or failing to follow target language patterns. To address this, we use a multilingual translation model to compute \"alignment\" scores, favoring responses aligned with the target language."}, {"title": "3.2.2 Alignment Optimization", "content": "We aim for the model to generate answers in the target language with higher likelihood than in the source language: $p_\\theta(y|x) > P_\\theta(y|\\tilde{x})$. To achieve this, we employ ORPO, a state-of-the-art preference optimization method. We collect flawed outputs $(\\Upsilon)$ and the ground truth in the target language $(\\Upsilon_w)$, then optimize the objective function:\n$L_{ORPO} = E_{(x,yw,y1)} [L_{XE-IT} + \\lambda \\cdot L_{OR}]$\nwhere $L_{XE-IT}$ is the XE-IT loss and $L_{OR}$ maximizes the likelihood ratio between the preferred response and the less preferred one.\n$L_{XE-IT} = \\frac{1}{m}\\sum_{t=1}^{m} \\log P_\\theta(y_t|x, y1x)$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Baselines. We chose the following methods as baselines: (1) FT-L (Meng et al., 2022a) fine-tunes a specific layer of the feed-forward network to maximize the likelihood of target tokens; (2) FT-M (Zhang et al., 2024) fine-tunes the same feed-forward network layer as FT-L. Additionally, it masks the original text and applies cross-entropy loss on the target answer; (3) ROME (Meng et al., 2022a) employs causal mediation analysis to identify the target area for editing, and then updates the parameters of the feed-forward network layers;\nBackbones. We select two public models as backbones, including LLaMA2-Chat-7B (Touvron et al., 2023) and Qwen2.5-instruct-7B (Yang et al., 2024a). These models are widely used in chatbot applications, the former excels in English, while the latter demonstrates strong multilingual abilities."}, {"title": "4.2 Results of Single Fact Editing", "content": "Table 1 and Table 2 demonstrate the main results of single fact editing, which focus on single editing cases. From these results, we can find several significant observations:\nX-KDE outperforms other methods in the cross-lingual setting by a significant margin. As shown in Table 1, when edited in English, it is evident that our method brings average performance improvements of 6.76%, compared to LTE. In particular, in the cross-lingual setting, our method achieves further performance gains in portability, which demonstrates that our method not only captures surface-level changes in wording but enables the LLM to effectively internalize the knowledge edited in the source language and apply it to the target language."}, {"title": "4.3 Results of Mass Fact Editing", "content": "In the previous section, we introduced the results of single-fact editing. However, real-world scenarios are often more complex, requiring simultaneous or sequential edits to multiple pieces of knowledge. Therefore, in this section, we conduct comprehensive experiments using X-KDE alongside several methods that support mass editing (FT-L, FT-M, MEMIT, and LTE) on LLaMA2-Chat-7B in both batch-edit and sequential-edit settings, and then present the corresponding results.\nX-KDE can process thousands of edits simultaneously. In line with the single-edit procedure, we evaluate both English and Chinese edits separately."}, {"title": "4.4 Results of General Tasks", "content": "A series of studies have demonstrated that knowledge editing can influence model performance across various scenarios (Yang et al., 2024b; Li et al., 2023b; Gu et al., 2024). To investigate whether our method impacts the model's capabilities in unrelated domains, we conducted tests across a range of fields. Given that the cross-lingual knowledge editing task typically involves two languages, we use English and Chinese as representative examples. Multiple benchmarks are selected in these two languages, covering tasks such as com-"}, {"title": "5 Analysis", "content": ""}, {"title": "5.1 Are both stages of X-KDE indispensable?", "content": "We examine the significance of the two stages in the X-KDE method through our ablation experiments, as shown in Table 4. Focusing solely on the improvement in performance metrics, Stage 1 undoubtedly plays a decisive role in our method, achieving significant gains (up to +25.64% average score) compared to the untrained baseline model. This stage enables cross-lingual knowledge editing via in-context learning, providing a substantial boost in model performance. While Stage 2 appears to offer a smaller improvement (a 2.25% average gain), a closer analysis highlights its practical importance. Stage 2 is particularly effective in adjusting the model's output style to align with the target language, addressing issues such as incorrect language mixing (e.g., code-switching) or failure"}, {"title": "5.2 Does every composition of the training data matter?", "content": "In this section, we focus on the composition of the training data. As shown in Table 5, the absence of any specific segment of the training data leads to a noticeable decline in editing performance, whether in monolingual or cross-lingual settings. Excluding either monolingual or cross-lingual training data causes a sharp drop in performance in the corresponding areas. When the in-scope data is omitted, the model tends to retain its original knowledge, resulting in reduced reliability, generality, and portability."}, {"title": "5.3 Why choose ORPO as the preferred optimization method?", "content": "We evaluate several popular preference optimization methods using a held-out dataset from our training data, specifically direct policy optimization (DPO, Rafailov et al., 2023), contrastive preference optimization (CPO, Xu et al., 2024), Kahneman-Tversky Optimization (KTO, Ethayarajh et al., 2024), and odds ratio preference optimization (ORPO, Hong et al., 2024) on the Bi-ZsRE benchmark. Based on these results, we adopt ORPO as the default optimization method for the second phase of our approach, as it provides the most significant improvement."}, {"title": "6 Related Work", "content": "Knowledge Editing The task of knowledge editing was introduced by (Sinitsin et al., 2020) to update specific knowledge while preserving unrelated information. Current methods fall into two paradigms: preserving or modifying the model's parameters. (1) Preserving LLMs' parameters involves auxiliary models or extra parameters. SERAC (Mitchell et al., 2022) uses a counterfactual model to update knowledge without altering model parameters.\nCross-Lingual Knowledge Editing Nearly all language models are multilingual, making it crucial to enhance instruction-following capabilities across different languages (Zan et al., 2024) and enable synchronized cross-lingual knowledge updates (Xu et al., 2022; Wang et al., 2023a). \nLLM Alignment LLM alignment (Gabriel, 2020) ensures that LLMs' behaviors align with human values. Techniques such as supervised fine-tuning (SFT) (Wei et al., 2021; Wang et al., 2023e;"}, {"title": "7 Conclusion", "content": "In this paper, we present the Cross-Lingual Knowledge Democracy Edit (X-KDE) framework, which facilitates knowledge editing across languages in large language models (LLMs). By integrating Cross-lingual Edition Instruction Tuning (XE-IT) and Target-language Preference Optimization (TL-PO), X-KDE efficiently transfers knowledge from a source language to a target language while maintaining strong performance in monolingual settings.\nFuture research will explore applying X-KDE to other domains and optimizing its efficiency."}, {"title": "Limitations", "content": "While our work presents promising results, there are a few limitations to consider. First, due to computational constraints, we validate X-KDE on models with up to 7B parameters. Evaluating larger models, such as those exceeding 70B parameters, could provide more robust insights. Second, while our method has been effective in multilingual settings, its application to additional domains, such as finance or law, remains unexplored. Future research will focus on scaling X-KDE and extending its applicability to other fields."}]}