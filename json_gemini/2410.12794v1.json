{"title": "Disaggregating Embedding Recommendation Systems with FlexEMR", "authors": ["Yibo Huang", "Zhenning Yang", "Jiarong Xing", "Yi Dai", "Yiming Qiu", "Dingming Wu", "Fan Lai", "Ang Chen"], "abstract": "Efficiently serving embedding-based recommendation (EMR) models remains a significant challenge due to their increasingly large memory requirements. Today's practice splits the model across many monolithic servers, where a mix of GPUs, CPUs, and DRAM is provisioned in fixed proportions. This approach leads to suboptimal resource utilization and increased costs. Disaggregating embedding operations from neural network inference is a promising solution but raises novel networking challenges. In this paper, we discuss the design of FlexEMR for optimized EMR disaggregation. FlexEMR proposes two sets of techniques to tackle the networking challenges: Leveraging the temporal and spatial locality of embedding lookups to reduce data movement over the network and designing an optimized multi-threaded RDMA engine for concurrent lookup subrequests. We outline the design space for each technique and present initial results from our early prototype.", "sections": [{"title": "1 INTRODUCTION", "content": "Embedding-based Recommendation (EMR) models, widely used in e-commerce, search engines, and short video services, dominate AI inference cycles in production datacenters, such as those at Meta [12, 28]. They process user queries using both continuous and categorical features, transforming categorical features into dense vectors via embedding table lookups, and finally, combining them with the continuous features for neural network (NN) scoring.\nServing EMR models at scale leads to pressing memory requirements [32] as embedding tables can grow to terabytes in size, accounting for over 99% of model parameters [4]. In practice, we need to partition an EMR model and distribute it across multiple monolithic servers with a mix of GPUs, CPUs, and DRAM [28, 33, 45, 54]. On a specific server, existing advances propose to decouple the embedding lookup from the NN computation and use DRAM for embedding store [30]. Recent work [45] further enhances this approach by employing an embedding cache on GPUs to optimize lookup performance. However, this monolithic approach has limitations in scalability and total cost of ownership (TCO) in practice. Recommendation workloads need a mix of resources-memory for embedding store and GPUs for NN computation, and this mixture varies across models and evolves over time. Monolithic servers that provision resources in a fixed portion is hard to achieve both performance and cost efficiency. Recent studies show that it can lead to idle resources and wasted costs of up to 23.1% [24].\nA promising approach to achieve performant and cost-efficient large EMR model serving is to disaggregate embedding storage and NN computation into independent servers. Specifically, using CPU-based servers to store embedding tables in memory while utilizing GPU nodes for NN computations. These components are interconnected via high-speed networks, such as remote direct memory access (RDMA) [1, 9, 40]. This decouples the memory and GPU resources and allows them to scale independently, improving the total resource efficiency and reducing the TCO. Disaggregation also increases system robustness, as failures are isolated to individual components.\nHowever, disaggregating EMR model serving raises novel networking challenges. First, remote embedding lookup involves extensive data transmission over the network. For example, an 8-byte categorical feature index could generate a returned embedding vector with hundreds or even thousands of float values [37, 55]. Worse still, each lookup needs to query multiple such indices, and each batch contains up to thousands of lookups [18, 33]. This can be efficiently handled by local GPU memory with high memory bandwidth in a monolithic design. However, decoupling embedding storage and computation shifts this pressure to the network, with a much smaller bandwidth, potentially causing network bottlenecks. On the other hand, intensive data transmission imposes stringent performance requirements on the network layer. Unfortunately, today's RDMA systems [7, 48] are not designed for EMR disaggregation. For instance, the single-thread RDMA I/O models that are commonly used in regular applications [5, 22] will suffer from high software queuing latency for EMR serving. The recent design on disaggregated EMR systems [24] mainly focuses on resource provisioning but overlooks the above networking challenges.\nIn this paper, we design an optimized disaggregated EMR system called FlexEMR. FlexEMR optimizes the disaggregation by proposing two classes of techniques to tackle the challenges discussed above. The first set of techniques explores the temporal and spatial locality of embedding lookup. While existing works [45] implement embedding caches on GPUs to leverage temporal locality, we observe that such caches could compete with NN computation for limited GPU memory, and propose mechanisms to dynamically adjust caching strategy"}, {"title": "2 OVERVIEW", "content": "In this section, we provide background on EMR model serving, describe the motivation and challenges for disaggregated EMR serving, and present an overview of our solutions."}, {"title": "2.1 Background: EMR models", "content": "An EMR model handles two types of input features: categorical features (sparse) representing discrete categories or groups and continuous features (dense) representing measurements or quantities that are continuous in nature [33]. For example, in a video recommendation system, categorical features could include video IDs, genres, or user IDs, while continuous features could include user age or watch time. The categorical features often have very high cardinality, as each feature can consist of millions of instances (e.g., numerous specific IDs for users in feature \"user IDs\"). EMR models convert these high-dimensional categorical features into dense vector representations via embedding tables.\nFigure 1 illustrates the key components and workflow of a representative EMR model. It takes candidate items as input, including both categorical and continuous features from upstream. For those accessed instances (e.g., IDs) in a batch, EMR retrieves their associated embeddings (dense vectors), which will be aggregated into a single fixed-size embedding vector through pooling operations such as sum or average. Meanwhile, the continuous features are processed by a bottom neural network (bottom NN) which is typically a multilayer perceptron (MLP) to generate high-dimensional dense vectors. The feature interaction process combines the dense vectors from categorical and continuous input features through operations such as element-wise multiplication or concatenation. The combined result is fed into a top neural network (top NN) to compute user-item scores for top-k ranking. The items with the highest scores are presented to the user."}, {"title": "2.2 Motivation: Disaggregated EMR serving", "content": "State-of-the-art EMR models consist of hundreds of sparse features, each associated with an embedding table with potentially millions of embedding rows [10, 34]. Indeed, production-level EMR models could have TB-level embedding tables (e.g., Meta uses 50TB DLRM model [32]).\nThe large-size embeddings have presented significant challenges for EMR serving because they cannot be stored on a single GPU. Therefore, EMR embeddings are often partitioned and scattered across multiple servers, each server has a combination of GPUs, CPUs, and DRAM. Considering that EMR workloads require two distinct types of resources-large memory for embedding storage and GPUs for NN computation-researchers propose decoupling them for better flexibility. Specifically, this approach leverages DRAM and CPUs for embedding storage and lookup, while utilizing GPUs on the same servers for NN computation. As a further improvement, an embedding cache is employed in GPU memory to cache the \"hot\" entries to optimize the lookup performance [45].\nThe embedding-NN decoupling enables more flexible EMR serving, but doing that on monolithic servers has several limitations. Monolithic servers provision GPU, CPU, and DRAM resources in fixed proportions, but the demands for these resources by EMR workloads can evolve across models and change over time due to varied recommendation workloads. Scaling up the whole server for the most demanding resource or the peak workload will lead to low resource utilization and waste of costs. A recent study [24] has found that fixed resource provision on monolithic servers can result in wasted costs of up to 23.1%. Therefore, more resource- and cost-efficient EMR serving are urgently needed.\nBuilding upon the trend of disaggregation in datacenters [6, 13], a promising solution is to fully disaggregate the embedding layer and dense NN compute into network-interconnected CPU embedding servers and GPUs (rankers), respectively."}, {"title": "2.3 Key research challenges", "content": "Disaggregated EMR serving involves a large volume of data movements over the network. Typically, given some categorical feature indices as input, the ranker first fetches all corresponding embedding vectors from remote embedding servers and then performs feature pooling operations. As a result, the network bandwidth between embedding servers and rankers becomes a major bottleneck (as shown in Figure 2), presenting several domain-specific challenges.\n(1) Contention in GPU memory. To reduce data movement over the network, existing works [17, 21, 23, 25, 27, 29, 46, 47] attempt to cache frequently accessed embedding entries in GPU memory. However, we observe that embedding cache is far from a perfect solution. Using precious GPU memory for caching could significantly reduce serving throughput, especially when the NN model size and request batch size are large. Essentially, NN inference also requires a large amount of GPU memory, and the existing caching strategy could cause serious resource contention between the two tasks.\n(2) Large-scale fan-out pattern. Remote embedding lookup generates large-scale fan-out subrequests. For example, an 8-byte categorical feature index could generate a returned embedding vector with hundreds or even thousands of bytes in dimension size. Moreover, each lookup needs to query multiple such indices, and each batch contains up to thousands of lookups. Unlike local memory, the network bandwidth is significantly lower. Hence, issuing hundreds or thousands of concurrent batched embedding lookups can lead to severe network contention and degraded performance.\n(3) RDMA engine efficiency. RDMA is commonly used for remote data access. Most existing RDMA applications employ single-threaded RDMA I/O models, which send out RDMA read requests to different target machines using one thread. This leads to extended queuing latency in our scenario. We need to design a more efficient RDMA I/O engine capable of handling concurrent embedding lookup requests and results, while effectively re-balancing skewed workload patterns across distributed embedding servers."}, {"title": "2.4 Our solution: FlexEMR", "content": "In this paper, we propose FlexEMR\u2014an EMR serving system that aims at addressing the aforementioned challenges. Figure 3 illustrates the envisioned system architecture. At a high level, the rankers initiate embedding lookups. Each lookup contains multiple subrequests, which firstly go through an adaptive embedding cache on rankers serving as a lookup fast path for reduced latency. The requests are then sent to embedding servers via a set of optimized RDMA engines. Once the corresponding embedding vectors are found, FlexEMR initiates a hierarchical pooling process to retrieve results without causing network contentions.\nOur design is primarily based on two sets of techniques. First, we reduce embedding movement over the network by exploiting temporal and spatial locality across embedding lookups and subrequests. Temporal locality means that (i) a non-negligible portion of embeddings (e.g., 10%~15%) are the most frequently accessed in a period (i.e. hot embeddings [10, 11, 46]), and (ii) some subrequests often appear together in the same lookup (i.e. embedding co-occurrence [49]). Existing work has leveraged temporal locality to design embedding caches on GPUs, but we argue that the caching design should be dynamically adjusted to avoid GPU memory contention. Spatial locality means that multiple embedding tables/shards often co-locate in the same embedding server, so many subrequests in a lookup will be sent to the same destinations. As such, we propose to push-down lightweight pooling operations onto embedding servers. This leverages"}, {"title": "3 FLEXEMR DESIGN", "content": "In this section, we outline a potential system design and optimizations. We first highlight the design of an adaptive caching mechanism and a hierarchical EMB pooling architecture in section 3.1. Next, in section 3.2, we discuss how multi-threaded RDMA further optimizes the embedding lookup."}, {"title": "3.1 Locality-enhanced disaggregation", "content": "3.1.1 Adaptive EMB caching. A common practice to reduce embedding lookup latency is to leverage the temporal locality across requests and cache \u201chot\u201d lookup or pooling results in GPU memory [45]. However, because the embedding caches share GPU memory with NN computation, an enlarged cache inevitably leads to a smaller batch size for NN computing due to GPU memory contention, thereby degrading overall throughput. In this work, we explore an approach to adaptively adjust the size of cache: when the system is overloaded, FlexEMR reduces cache size automatically to preserve overall throughput; otherwise, it expands the cache to improve latency.\nTracing temporal dynamics. The first step towards an adaptive caching strategy is to capture the workload temporal dynamics (Figure 5). In reality, the ranker often uses a task queue to receive batches of requests from upstream, then feeds them into downstream EMR models. FlexEMR could monitor the size of these batches, then apply a sliding window algorithm to determine whether the system is under high load.\nAdjusting cache size. Once a decision is made to enlarge or shrink cache size, we need to consider how to enforce these actions accordingly. This involves two sub-tasks: Firstly, we need to determine the updated cache size. Our observation here is that, given the incoming batch size and the EMR model architecture, it is possible to build a model to estimate the memory size required by NN computation. The ideal cache size is the difference between GPU memory capacity and the parts reserved for NN. The second task is to swap embeddings into or out of GPU memory For the swap in action, FlexEMR could initiate RDMA reads from the ranker to asynchronously fetch the hot embeddings from embedding servers in a transparent manner. For the swap out action, FlexEMR should remove part of embedding cache lines based on a LRU algorithm, and free up the corresponding GPU memory.\n3.1.2 Hierarchical EMB pooling. Apart from temporal locality, spatial locality is also prevalent in EMR serving systems. In a disaggregated architecture, embedding tables are placed onto a set of remote embedding servers. Given an embedding lookup request from the ranker, a typical workflow is shown in Figure 4(a): First, the ranker sends sub-requests to remote embedding servers and asks them to return corresponding embedding vectors. The ranker then aggregates these results through pooling operations. This communication leads to extensive embedding movement over the network and increased latency.\nHierarchical pooling leveraging spatial locality. We seek to reduce the embedding movements between the ranker and embedding servers for higher throughput under bounded latency. Our finding here is that the CPUs in embedding servers could be utilized to perform partial pooling operations. If an embedding server contains multiple required vectors (i.e. spatial locality), then it could aggregate them first before sending to the ranker. Motivated by this finding, we envision a hierarchical pooling architecture, as shown in 4(b). For each embedding lookup, FlexEMR first invokes embedding server CPUs to perform partial pooling whenever possible, then asks the ranker to retrieve their outputs and perform global pooling to obtain the final results. Unlike existing works [57], FlexEMR is the first to explore parallel operator push-downs (i.e., pushing pooling operations down to embedding servers). This design could potentially generalize to other workloads with large-scale fan-out patterns.\nRouting table for identifying co-located embeddings. An important question here is how to identify the embedding spatial locality among embedding servers-i.e., given as input a set of sparse feature indices, we need to identify which indices are co-located at where. A na\u00efve solution is to maintain a routing table storing the pairs. It then queries all corresponding embedding"}, {"title": "3.2 EMB lookup with Multi-threaded RDMA", "content": "We next discuss how to optimize the RDMA I/O engine for remote embedding lookup. Given a batch of embedding lookup requests, each containing a large amount of fan-out subrequests, rankers in FlexEMR require an efficient RDMA I/O engine to forward subrequests to remote embedding servers. Since the completion time of an embedding lookup is dominated by the slowest subrequest, the overall pipeline is very sensitive to tail latency. The single-threaded RDMA IO model used for most existing RDMA applications [5] becomes a major bottleneck, since it incurs high queuing latency overhead between the embedding and transport layers.\nA promising solution is to use multi-threaded RDMA, where RDMA connections to embedding servers are assigned to multiple I/O threads, and embedding subrequests are distributed to these connections according to corresponding destinations. However, na\u00efvely using multi-threaded RDMA introduces non-negligible contentions due to limited RNIC parallelism resources (e.g., user access regions [43]): Figure 8 (left) shows that it can lead to up to 62% throughput drop in our microbenchmark.\nContention-free multi-threaded embedding lookup. To understand the root cause of contentions under concurrent lookup subrequests, we delve deep into the architecture of multi-threaded RDMA. As shown in Figure 6, we find that each RDMA engine contains a dedicated I/O thread, and each thread encompasses multiple RDMA connections. The RNIC parallelism units are allocated to each newly created connection in a round-robin manner, resulting in a one-to-many mapping between RDMA parallelism units and connections. However, the I/O threads for remote embedding lookup are not aware of such mappings, thereby enforcing multiple RDMA connections belonging to different I/O threads to access the same parallelism unit simultaneously. To coordinate different I/O threads, each parallelism unit must implement a complex locking mechanism, which could introduce significant performance overhead.\nTo solve this problem, we envision a mapping-aware multi-threaded RDMA engine, capable of transparently generating one-to-one mapping between I/O threads and RNIC parallelism units, as shown in Figure 6 (right). The key-enabling technique is the resource domain feature provided by RDMA [2, 31], which exposes the mapping between connections and RNIC parallelism units to the application layer. As such, FlexEMR could ensure that all connections assigned to the same parallelism unit are allocated to the same RDMA engine, thus preventing contention from concurrent threads. Essentially, in the cluster initialization stage, FlexEMR firstly creates RDMA connections between embedding servers and rankers, then identifies their resource domains. Since there is a static mapping between the resource domain and parallelism unit, FlexEMR could subsequently aggregate connections into different RDMA engines according to the resource domain, so that each RDMA engine points to a dedicated parallelism unit.\nLive connection migratation among RDMA engines. Another common problem in practice is skewed subrequest patterns. Connections to different embedding servers might experience vastly different utilization rate, which leads to imbalanced loads among RDMA engines. Since an RDMA engine can manage multiple connections used for different embedding servers, a strawman solution is to live-migrate connections in overloaded engines to under-utilized engines: Periodically, FlexEMR traces the number of queued subrequests in each connection. When a connection becomes overloaded, FlexEMR selects the least loaded RDMA engine and initiate the migration process. However, this workflow brings back the RDMA multi-thread contention problem, because the migrated connection still points to the old parallelism unit. FlexEMR aims at a live migration strategy without RDMA contention concerns. The key idea is to re-associate the migrated connection with the resource domain used by the new RDMA engine. Notably, FlexEMR detaches the connection from the old domain, and then attaches it to the resource domain of the new one.\nFast credit-based flow control with RDMA QoS. FlexEMR pipelines pooling computation and remote embedding lookup"}, {"title": "4 PRELIMINARY RESULTS", "content": "Even though FlexEMR is still work-in-progress, we showcase initial evidence around its major components, including adaptive embedding caching (\u00a73.1.1) and multi-threaded RDMA embedding lookup (\u00a73.2). We use the popular MLPerf framework [36] and a set of production-scale embedding lookup traces released by Meta [37] to synthesize inference workloads. Our testbed includes two interconnected Intel Xeon servers each equipped with 32 CPU cores, 128GB memory, and a 100Gbps Mellanox RDMA NIC. One of them is equipped with a Nvidia A100 GPU with 80GB of memory.\nNa\u00efve caching leads to GPU contentions. To understand the benefit of adaptive EMB caching, we analyze a pure GPU caching-based solution on a representative RMC2 model [10, 19]. For the GPU caching baseline, we vary the size of EMB caches and observe the changes on supported batch sizes. As Figure 7 shows, as we increase the GPU cache size, the caching-based solution has to settle with smaller batch size due to contention on GPU memory capacity, resulting in decreased inference throughput and wasted GPU compute cycles. FlexEMR on the other hand aims to achieve the highest batch size through an adaptive embedding caches, as proposed in \u00a73.1.1, mitigating memory contention in most scenarios.\nFlexEMR outperforms na\u00efve RDMA-based embedding lookup in efficiency. Next, we compare the lookup performance of a na\u00efve multi-threaded RDMA baseline against our FlexEMR prototype. As Figure 8 illustrates, with mapping aware multi-threading, FlexEMR achieves higher throughput than baseline by up to 2.3x. Moreover, FlexEMR achieves 35% lower latency on credits transmission, which further reduces possible congestion between rankers and embedding servers. This demonstrates the importance of an efficient multi-threaded RDMA engine (\u00a73.2)."}, {"title": "5 RELATED WORK", "content": "Many existing works treat EMR as generic deep learning models and adopt GPU-centric approaches for their deployment [28, 33, 45, 54], leading to under-utilized GPU resources. Recent projects apply a variety of caching mechanism[17, 21, 23, 25, 27, 29, 46, 47] to speed up embedding lookups. However, these solutions suffer from low cache hit rate in production environments [19]. Specialized hardware such as FPGAs has also been explored to enhance recommendation systems [14, 20, 53], but we strive for a generic solution with commodity hardware. Compression [3, 8, 41, 50] and sharding [38, 39, 42, 54, 56] are common optimizations to embedding table lookup. These works are complementary to ours, as the proposed techniques can be integrated seamlessly into FlexEMR for further improved performance. DisaggRec [24] proposed a similar disaggregated memory system. However, the resource distribution is fixed and determined through an exhaustive search. This approach introduces overhead and fails to capture serving dynamics."}, {"title": "6 CONCLUSION & FUTURE WORK", "content": "Embedding-based recommendation (EMR) model serving consumes the majority of AI inference cycles in production datacenters due to its unique embedding-dominated characteristics and stringent service-level objectives. However, prior serving systems for EMR models struggle to achieve high performance at low cost. We propose FlexEMR, a fast and efficient system that disaggregates embedding table lookups from NN computation. FlexEMR uses a set of locality-enhanced optimizations atop a multi-threaded RDMA engine to ensure performance and resource efficiency. We envision FlexEMR to improve user experience of Internet-scale recommendation services while driving down costs for their providers. Furthermore, we believe this paradigm can benefit other ML workloads, including large language models (LLM) [26, 52], multimodal models [16], and mixture-of-expert (MoE) [35, 51], which we will also investigate in future works."}]}