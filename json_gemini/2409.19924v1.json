{"title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability", "authors": ["Kevin Wang", "Junbo Li", "Neel P. Bhatt", "Yihan Xi", "Qiang Liu", "Ufuk Topcu", "Zhangyang Wang"], "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their ability to per- \nform complex reasoning tasks, but their effectiveness in planning remains underexplored. In this \nstudy, we evaluate the planning capabilities of OpenAI's ol models across a variety of bench- \nmark tasks, focusing on three key aspects: feasibility, optimality, and generalizability. Through \nempirical evaluations on constraint-heavy tasks (e.g., Barman, Tyreworld) and spatially complex \nenvironments (e.g., Termes, Floortile), we highlight ol-preview's strengths in self-evaluation and \nconstraint-following, while also identifying bottlenecks in decision-making and memory manage- \nment, particularly in tasks requiring robust spatial reasoning. Our results reveal that ol-preview \noutperforms GPT-4 in adhering to task constraints and managing state transitions in structured \nenvironments. However, the model often generates suboptimal solutions with redundant actions \nand struggles to generalize effectively in spatially complex tasks. This pilot study provides foun- \ndational insights into the planning limitations of LLMs, offering key directions for future research \non improving memory management, decision-making, and generalization in LLM-based planning.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have sig- \nnificantly changed the landscape of arti- \nficial intelligence, achieving impressive re- \nsults in various language-related tasks, such \nas chatbots, math, and coding, etc. One \nof the areas, that remains yet to be fully \nclaimed by LLMs, is the use of language \nagents for planning in the interactive physi- \ncal world [Huang et al., 2022a,b, Singh et al., \n2023, Lin et al., 2023]. Previous scrutiny \n[Liu et al., 2023, Valmeekam et al., 2024a, \n2023, 2024b] pointed out that despite pos- \nsessing advanced inference reasoning tech- \nniques like Chain-of-Thought (CoT) [Wei \net al., 2022] and Tree-of-Thought (ToT) \n[Yao et al., 2024], LLMs still struggle in \nmaking success plans without relying on ex- \nternal tools, such as a PDDL planner [Liu \net al., 2023, Lyu et al., 2023]. \nThe recent release of OpenAI's o1 model [OpenAI, 2024], trained with reinforcement learning to \nnaturally employ chain-of-thought reasoning, has reached new heights in problem-solving, particularly \nin mathematics and code generation. This suggests potential for planning, a seemingly related area. \nRecent research [Valmeekam et al., 2024a] was the first to evaluate the success rates of ol and other \nLLMs on Blocksworld and its variants from PlanBench [Valmeekam et al., 2024b], demonstrating that \no1 has significantly enhanced its capabilities, extending the boundaries of what LLMs can accomplish"}, {"title": "2 Planning Ability Evaluation: Three Perspectives", "content": "We propose evaluating the planning abilities of language model agents from three key perspectives: \nfeasibility, optimality, and generalizability. These categories represent distinct but interconnected \naspects of planning, each addressing a critical dimension of the agent's capability to effectively handle \ntasks in diverse environments. By dividing planning abilities into these three perspectives, we ensure \na comprehensive evaluation, where each aspect plays a significant role in overall performance. The \nmotivation for this division lies in the varied challenges planning entails, from basic execution to \nadvanced optimization and adaptation across new contexts.\nFeasibility Feasibility assesses whether the agent can produce a viable plan to achieve the goal, often \nreferred to as success rate in previous works [Liu et al., 2023]. A plan must not only be executable but \nalso ensure goal completion under real-world constraints. This category directly measures the agent's \nability to operate within the problem's rules, ensuring that each generated plan is valid and practical \nin real-world settings. Feasibility can be further divided into three key components:\n1. Ability to create feasible steps Each step in a plan must be executable within the system, \nadhering to constraints specific to the problem domain. Constraints might include physical limi- \ntations, action order requirements, or other domain-specific rules. In route planning, for instance, \ncertain zones may be inaccessible, while in operational planning, tasks may have dependencies \nthat must be respected. We term failures related to this issue as \"Inability to Follow Prob- \nlem Rules\" (IR). This occurs when a plan violates domain constraints, often because language \nmodels, trained on natural language data, misinterpret problem-specific constraints: some steps, \nalthough violating the physical constraints of the problem, may still seem logically plausible \nwithin a natural language framework, leading the agent to mistakenly adopt them. Such errors \nbecome more frequent as the complexity or number of rules increases, revealing a need for more \nsophisticated validation mechanisms beyond natural language reasoning.\n2. Ability to generate a feasible plan Even if individual steps are valid, the overall plan \nmay still fail to achieve the intended goal. The agent might not generate a coherent sequence of \nactions, leading to dead ends or random exploration. This issue, termed \"Inability to Generate \na Feasible Plan\" (IP), grows more prominent in complex tasks requiring advanced reasoning. \nStronger models like ol, which demonstrate superior reasoning capabilities, tend to perform \nbetter, as they provide more thorough analysis and structured plans.\n3. Ability to understand the problem Feasibility also hinges on correctly interpreting the \nproblem's initial and goal states. Even with valid steps and an overall plan, misinterpreting \nthe starting conditions or the desired end state can result in errors. Such failures, termed \n\"Misinterpretation of Goal State\" (MG), are common when plans require deep reasoning over \nmultiple steps. This issue, however, can be mitigated in models with stronger reasoning abilities.\nOptimality While feasibility ensures that a plan can be successfully executed, optimality concerns \nhow efficiently the plan achieves its goal. In many real-world scenarios, a feasible plan is not enough; \nthe plan must also be resource-efficient, minimizing unnecessary actions, time, and cost. In this \ncontext, optimality refers to whether the language agent can generate the most efficient plan, avoiding \nredundant steps or suboptimal decisions that waste resources."}, {"title": "3 Experiments on Planning Benchmarks", "content": "We assess the planning capabilities of GPT-4, 01-mini, and ol-preview as discussed in Section 2. \nThe overall comparison in outlined in Table 1. Next, we will explore each task in detail to carefully \nanalyze the language agents' abilities across various planning tasks. The results afor feasibility, op- \ntimality, and generalizability are presented in figures 2, 3, and 4, respectively. For each task, errors \nare highlighted in red, while the reasons for these errors are indicated in orange. The specific action"}, {"title": "3.1 Barman", "content": "Task description In this task, a robot barman is tasked with preparing a series of drinks by manipu- \nlating drink dispensers, shot glasses, and a shaker. The robot, equipped with two hands, must perform \na variety of actions such as grasping containers, filling/refilling shot glasses, pouring ingredients, shak- \ning cocktails, and cleaning or emptying containers. Each action comes with strict preconditions for \nexample, the robot can only grasp a container when one hand is free, and shaking a cocktail is only \npossible when the shaker contains exactly two ingredients. Successfully completing this task requires \nprecise sequencing of actions, where adhering to these constraints is crucial to avoid mistakes."}, {"title": "3.2 Blocksworld", "content": "Task description This planning task involves multiple blocks arranged on a table, where the goal \nis to move from an initial configuration to a pre-specified goal configuration. The robot arm, which \ncan hold only one block at a time, must execute a series of actions such as picking up, putting down, \nstacking, and unstacking blocks to achieve the desired arrangement. The challenge lies in determining \nthe correct sequence of these actions, while adhering to constraints that dictate how blocks can be"}, {"title": "3.3 Grippers", "content": "Task description This task involves a team of robots equipped with two grippers, capable of moving \nbetween rooms and manipulating objects. The robots have three primary actions: moving from one \nroom to another, picking up objects, and dropping them. Each action is constrained by the robot's \ncurrent location and the status of its grippers, meaning that a robot can only pick up an object if \nits gripper is free and can only drop an object in a specific location once it is carrying one. Effective \nplanning requires coordinating these actions while adhering to these constraints to accomplish the goal \nof manipulating objects across different rooms.\nAnalysis In this domain, both ol-mini and ol-preview significantly outperformed GPT-4, particu- \nlarly in success and optimality rates. GPT-4 managed a 70% success rate but only a 20% optimality \nrate, indicating frequent suboptimal action sequences. In contrast, o1-mini achieved both higher suc- \ncess and optimality rates, at 80% for each. o1-preview performed even better with a 90% success rate, \nthough its optimality rate dropped slightly to 70%. Figure 7b showcases an example where GPT-4 \ngenerates a suboptimal solution by including an unnecessary relocation of the robot, which adds re- \ndundant steps to the plan. Conversely, ol-mini, through additional reasoning, eliminated unnecessary \nmovements, leading to an optimal solution.\nHowever, ol-preview exhibited a shortcoming related to the MG error. In one instance, depicted \nin Figure 7a, the robot's initial state already coincided with the goal state both balls were already"}, {"title": "3.4 Floortile", "content": "Task description In this task, a team of robots is responsible for painting a grid of floor tiles in \nblack and white. Each robot can move in four directions, switch the color of its spray gun, and paint \ntiles directly in front of or behind them. The main challenge is that robots can only paint tiles that \nare currently unpainted and cannot move onto tiles that have already been painted. This creates a \ncomplex constraint, requiring careful planning of movements and actions to achieve the desired tile \npattern without the robots trapping themselves or each other. The task demands strategic coordi- \nnation between movement and painting actions, ensuring the robots follow the rules while efficiently \ncompleting the grid pattern.\nAnalysis In this domain, all models-GPT-4, 01-mini, and ol-preview-failed to solve the test cases, \nbut the reasons for their failures varied. For GPT-4 and o1-mini, 90% of their failures stemmed from \nthe IR error. Specifically, both models frequently violated the rule that robots can only paint tiles \ndirectly in front or behind them, instead attempting to paint the tile on which they were standing. \nThis rule violation was a common source of errors as the models struggled to keep track of the task \nconstraints while moving and painting simultaneously.\nOn the other hand, ol-preview showed a notable improvement in this regard, with only 30% of \nits failures caused by IR. ol-preview's internal self-evaluation mechanism allowed it to better track \nthe rules and adjust its actions accordingly. For instance, when it initially attempted to paint the \nwrong tile, it was able to reevaluate the action and correct itself by following the task constraints. \nHowever, despite these improvements in rule adherence, ol-preview encountered other errors, such \nas rule confusion. In some cases, it misinterpreted which tiles could be painted or made invalid \nassumptions about the sequence of movements. While its chain-of-thought reasoning helped it self- \ncorrect in some cases, it was ultimately unable to solve the task entirely, as seen in Figure 8."}, {"title": "3.5 Termes", "content": "Task description The Termes task requires controlling a robot to construct structures by moving \nbetween different positions and manipulating blocks. The robot can move horizontally, vertically (up \nand down), and is tasked with placing or removing blocks at neighboring positions that match in"}, {"title": "3.6 Tyreworld", "content": "Task description This task involves replacing flat tyres on vehicle hubs with intact, inflated tyres. \nThe process requires the use of tools such as a wrench, jack, and pump, and the agent must follow \nspecific actions to manipulate the tyres, nuts, and tools. There are 11 predefined actions, including \nopening and closing the boot, fetching and storing tools, loosening and tightening nuts, jacking up and \ndown hubs, removing and installing wheels, inflating tyres, and securing or undoing nuts. Success in this \ntask depends on executing these actions in the correct sequence while satisfying specific preconditions, \nsuch as using the wrench to loosen the nuts before removing the wheel or tightening the nuts only \nafter lowering the jack.\nAnalysis ol-preview generated correct plans for all test problems, significantly outperforming both \nGPT-4 and o1-mini, which failed to complete all but the simplest cases. The primary issue for GPT-4 \nand ol-mini was their frequent failure to follow the required action sequences. For instance, com- \nmon errors included \"loosening the nuts after jacking up\" or \"tightening the nuts before jacking \ndown\"-critical mistakes that would prevent successful tyre replacement. Figure 10 provides an ex- \nample where failing to follow these mechanical constraints leads to incorrect plans.\nWe also evaluated the models' generalization capabilities within this domain, revealing that while \nol-preview performed well on structured tasks, its success rate dropped from 100% to 20% when \nthe actions and tools were replaced with random symbols. Despite the symbols being abstract, the \nunderlying action constraints and logic remained unchanged. This sharp decrease in performance, as \nillustrated in Figure 11, suggests that while ol-preview excels in rule-based planning with familiar \nsymbols, it struggles to generalize when the problem context becomes more abstract."}, {"title": "4 Discussion", "content": "The primary limitation of this study stems from the relatively small dataset used in our empirical \nevaluations. While the experiments provide a foundational understanding of the ol model's planning \ncapabilities, broader insights into its generalizability and robustness can only be derived with more"}, {"title": "4.1 Empirical Limitations", "content": "The primary limitation of this study stems from the relatively small dataset used in our empirical \nevaluations. While the experiments provide a foundational understanding of the ol model's planning \ncapabilities, broader insights into its generalizability and robustness can only be derived with more"}, {"title": "4.2 Model Performance vs. Problem Complexity", "content": "Our analysis reveals a strong correlation between the complexity of the problem and the performance \nof the ol model. We empirically examine each problem along two dimensions of complexity: action \ncomplexity and spatial complexity, as illustrated in Figure 12.\nSpecifically, the Floortile and Termes tasks highlight the challenges ol faces in environments with \nhigher spatial and rule-based complexity. In Floortile, the task is set in a two-dimensional world, \nwhere robots must follow strict painting rules while navigating a constrained grid. In contrast, Ter- \nmes involves a three-dimensional setting, introducing additional layers of complexity due to vertical"}, {"title": "4.3 Constraint Following and State Management", "content": "One key finding of this study is the ol model's improved \nability to follow constraints and manage states, especially \nin comparison to GPT-4. ol-preview's self-evaluation \nmechanism, which allows the model to check and cor- \nrect its actions during plan generation, was particularly \neffective in tasks like Blocksworld and Tyreworld. In these \ntasks, ol-preview demonstrated a higher success rate in \nadhering to complex rules, such as the preconditions for \nusing a wrench or jack in Tyreworld, while avoiding the \nrule violations that plagued GPT-4 and ol-mini. How- \never, this ability to follow constraints deteriorates in more \ncomplex environments like Termes, where the need for \nprecise spatial reasoning and multi-step manipulation of- \nten leads to rule violations and misinterpretations of task \ngoals. This points to a potential limitation in the model's \nstate management when dealing with more abstract prob- \nlem spaces, and may call for explicit integration of neu- \nrosymbolic methods Yang et al. [2024a]."}, {"title": "4.4 Optimality and Redundancy in Planning", "content": "Optimality remains a significant challenge for the ol models, as demonstrated across tasks like Blocksworld \nand Floortile. While ol-preview often generated feasible plans, it frequently failed to produce optimal \nsolutions, leading to redundant actions and inefficiencies. For example, in Blocksworld, o1-preview \nadded unnecessary steps to the plan, reducing its overall efficiency despite reaching the correct goal \nstate. This suggests that while the model can understand and follow constraints, it struggles with \ndecision-making related to resource minimization and action optimization. The ability to reason about \noptimality is crucial for real-world applications, where minimizing steps and resources is often as impor- \ntant as achieving the correct outcome. Enhancing this aspect of ol's reasoning mechanism-perhaps \nby incorporating more advanced cost-based decision frameworks would be a valuable area for future \nresearch. Additionally, we observed that all three models exhibited some hallucination, including the \nassumption of non-existent rules. For instance, the ol-preview model in the grippers assumed that it \ncould only move to adjacent numbered rooms, whereas the actual rule specifies that the move action \ncan proceed to any room. Although model could still generate feasible plan, but that also hinder its \nability to generate optimal plan."}, {"title": "4.5 Generalization and Adaptability", "content": "Another promising outcome of this study is ol-preview's demonstrated ability to generalize across tasks \nwith consistent rule structures, as seen in Grippers. In these cases, ol-preview outperformed GPT- \n4 by effectively adapting its learned strategies to new environments. In these scenarios, ol-preview \nconsistently outperformed GPT-4 by effectively adapting its learned strategies to new environments. \nThe ol model attempted to imbue meaningless symbols with natural language meaning to aid problem- \nsolving, as seen in Figure 11. Additionally, ol-preview's self-evaluation capabilities enabled it to \nmaintain reasonable adherence to constraints, with only minor deviations, compared to GPT-4, which \noften fails to grasp the goal. While ol-preview's generalizability surpases previous GPT4 model, \nparticularly in structured, low-dimensional tasks, there is still substantial room for improvement in \nenabling these models to adapt to more dynamic, high-dimensional, and abstract problem spaces."}, {"title": "5 Conclusion", "content": "Our study offers a pilot evaluation of the planning capabilities of OpenAI's o1 models, providing new \ninsights into their strengths and limitations. By systematically evaluating their feasibility, optimality, \nand generalizability across diverse planning tasks, we have uncovered key areas where ol-preview \ndemonstrates promising advancements as well as significant challenges that remain to be addressed."}, {"title": "5.1 Summary of Findings", "content": "The findings of our experiments can be summarized from four key perspectives:\n1. Understanding the Problem: ol-preview demonstrated an improved ability to grasp the task \nrequirements and constraints, particularly in well-defined, rule-based environments like Barman \nand Tyreworld. This was largely due to its self-evaluation mechanism, which allowed for more \naccurate state tracking and constraint adherence. However, more evidence is needed to establish \nwhether these improvements translate to better reasoning capabilities in more abstract settings.\n2. Following Constraints: Across most tasks, ol-preview showed a superior capacity to follow \ntask-specific constraints compared to GPT-4. However, this ability weakened as the complexity \nof spatial reasoning and state transitions increased, as seen in Termes. This suggests that \nwhile constraint following is a relative strength of the ol model, more work is needed to handle \nenvironments with higher-dimensional state spaces.\n3. State and Memory Management: One of o1-preview's key advantages over previous models \nis its ability to remember and manage multiple states effectively within a plan, which contributed \nto its higher success rate in certain tasks. However, as problem complexity increased, the model's \nstate management became less reliable, particularly in tasks involving spatial reasoning across \nmultiple dimensions. This implies a potential bottleneck in the model's memory and decision- \nmaking processes.\n4. Reasoning and Generalization: While ol-preview showed some promise in its generalization \nability, particularly in structured environments like Grippers, its performance in more abstract \ntasks like Termes revealed substantial limitations. The model struggled with reasoning under \nconditions where actions and outcomes were less directly tied to the natural language represen- \ntation of the task, highlighting an area for future improvements."}, {"title": "5.2 Opportunities for Improvement", "content": "We posit several key areas where future iterations of LLM-based planners can be improved:\n\u2022 Optimality and Resource Utilization: Developing more sophisticated decision-making mech- \nanisms that minimize redundant actions and optimize resource usage will be crucial for making ol \nmodels more applicable to real-world planning tasks. This could involve incorporating cost-based \nreasoning or learning from expert demonstrations to achieve more optimal plans.\nAdditionally, Retrieval-Augmented Generation (RAG) methods could offer a potential solution \nby providing real-time, low-cost external memory updates, especially when tasks rely on large \nknowledge bases encoded in natural language text. However, RAG's effectiveness hinges on the \naccuracy and efficiency of its retrieval algorithms, which may introduce further challenges.\n\u2022 Generalization in Abstract Spaces: While ol-preview shows promise in generalizing across \nstructured environments, its performance in tasks with more abstract and complex rule sets \nremains suboptimal. Future work should focus on enhancing the model's ability to generalize \nin high-dimensional and spatially dynamic environments, potentially through improved memory \nmanagement Yang et al. [2024b] and abstraction mechanisms Zheng et al. [2024].\nEnhancing the model's decision-making and memory management capabilities, particularly for \nspatially complex tasks, will be essential for improving both optimality and generalizability in \nfuture iterations of LLM-based planning models."}, {"title": "\u2022 Handling Dynamic and Unpredictable Environments:", "content": "Handling Dynamic and Unpredictable Environments: Many real-world planning prob- \nlems involve dynamic environments with unpredictable elements. Testing the ol models in such \nsettings would provide valuable insights into their robustness and adaptability, especially when \nrules or constraints change during execution."}, {"title": "\u2022 Improving Constraint Adherence through Self-Evaluation:", "content": "Improving Constraint Adherence through Self-Evaluation: One recurring issue across \nmultiple domains is the models' inability to follow task-specific constraints accurately. Intro- \nducing more robust self-evaluation mechanisms could help LLMs better verify their own outputs \nbefore finalizing decisions, potentially catching mistakes like rule violations. Techniques such \nas multi-stage validation or symbolic verification Yang et al. [2024a], where models cross-check \ntheir proposed actions against the task constraints, could significantly reduce the incidence of \nconstraint-related errors."}, {"title": "\u2022 Leveraging Multimodal Inputs:", "content": "Leveraging Multimodal Inputs: To enhance the model's understanding of spatial and physi- \ncal reasoning tasks, future LLM-based planners could benefit from integrating multimodal inputs \nsuch as visual data, 3D environments, or sensor information Sun et al. [2024]. By incorporating \nnon-textual data, planners would be better equipped to handle complex tasks, such as robotic \nmanipulation or navigation, where purely text-based reasoning might miss critical spatial rela- \ntionships or physical constraints."}, {"title": "\u2022 Scalability to Complex Multi-Agent Planning:", "content": "Scalability to Complex Multi-Agent Planning: Many planning tasks, particularly in \nrobotics and logistics, require coordination between multiple agents Wu et al. [2023]. Extend- \ning LLM-based planners to effectively handle multi-agent systems would be an important step \nforward. This could involve developing strategies for decentralized planning, where each agent \ngenerates its own plan based on local knowledge, while still cooperating to achieve a shared goal."}, {"title": "\u2022 Incorporating Human Feedback for Continuous Learning:", "content": "Incorporating Human Feedback for Continuous Learning: One way to improve both \noptimality and generalization is by incorporating continuous learning through human feedback. \nInteractive feedback loops, where human users provide corrective signals or suggestions during \nplan execution, could help models refine their decision-making and better adapt to new situations \nor tasks that deviate from their training data.\nIn conclusion, while ol-preview represents a notable advancement in LLM-based planning, signifi- \ncant challenges remain, particularly in terms of optimizing plans, generalizing to more abstract tasks, \nand managing state complexity. Future research should aim to build on these insights to create more \nrobust, efficient, and adaptable planning agents capable of handling the diverse range of challenges \npresented by real-world planning problems."}]}