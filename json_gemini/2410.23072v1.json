{"title": "CNN Explainability with Multivector Tucker Saliency Maps\nfor Self-Supervised Models", "authors": ["Aymene Mohammed Bouayed", "Samuel Deslauriers-Gauthier", "Adrian Iaccovelli", "David Naccache"], "abstract": "Interpreting the decisions of Convolutional Neural Networks (CNNs) is essential for un-\nderstanding their behavior, yet explainability remains a significant challenge, particularly\nfor self-supervised models. Most existing methods for generating saliency maps rely on\nground truth labels, restricting their use to supervised tasks. EigenCAM is the only notable\nlabel-independent alternative, leveraging Singular Value Decomposition to generate saliency\nmaps applicable across CNN models, but it does not fully exploit the tensorial structure of\nfeature maps. In this work, we introduce the Tucker Saliency Map (TSM) method, which\napplies Tucker tensor decomposition to better capture the inherent structure of feature maps,\nproducing more accurate singular vectors and values. These are used to generate high-fidelity\nsaliency maps, effectively highlighting objects of interest in the input. We further extend\nEigenCAM and TSM into multivector variants\u2014Multivec-EigenCAM and Multivector Tucker\nSaliency Maps (MTSM)\u2014which utilize all singular vectors and values, further improving\nsaliency map quality. Quantitative evaluations on supervised classification models demon-\nstrate that TSM, Multivec-EigenCAM, and MTSM achieve competitive performance with\nlabel-dependent methods. Moreover, TSM enhances explainability by approximately 50%\nover EigenCAM for both supervised and self-supervised models. Multivec-EigenCAM and\nMTSM further advance state-of-the-art explainability performance on self-supervised models,\nwith MTSM achieving the best results.", "sections": [{"title": "1 Introduction", "content": "Convolutional Neural Networks (CNNs) demonstrate exceptional performance across various computer vision\napplications (Redmon et al., 2016; Krizhevsky et al., 2017; Liu et al., 2021). These networks can be trained in\na supervised manner to tackle classification or regression tasks (He et al., 2016) or in a self-supervised manner\nfor image segmentation (Walsh et al., 2022; Kirillov et al., 2023), face recognition (Boutros et al., 2022), and\nlearning image embeddings (Bardes et al., 2022). However, their inherent black-box nature poses significant\nchallenges, particularly in medicine (Cruciani et al., 2021; Bouayed et al., 2022) and biometrics (Kim & Cho,\n2021), where explainability is crucial for trust and adoption.\nNumerous works have attempted to address the challenge of explaining CNN decisions, proposing diverse\nmethods that can be broadly categorized into feature attribution methods and Class Activation Map (CAM)\nmethods. Feature attribution methods involve either estimating a locally interpretable model (Ribeiro et al.,"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Class Activation Maps", "content": "Class Activation Maps (CAMs) (Zhou et al., 2016) are local explainability methods for CNN based models.\nThey help visualize the regions of the input image that significantly contribute to the CNN's prediction by\nanalyzing the intermediate results. Formally, CAMs are defined as the sum of the feature maps resulting\nfrom the last convolutional layer of a CNN F\u2208 RC,H,W, weighted by a vector w \u2208 RC as follows:\n$$CAM = \\sum_{i=1}^C w_iF_i.$$\nAs observed from Equation 1, the weight vector w is the only unknown to determine\u00b2. Various works propose\ndifferent methods to infer w (Chattopadhay et al., 2018; Wang et al., 2020; Muhammad & Yeasin, 2020;\nZhang et al., 2024). In Section 3.2, we provide an overview of the different CAM based methods with a focus\non the EigenCAM method (Muhammad & Yeasin, 2020) which utilizes the SVD (Trefethen & Bau, 1997)."}, {"title": "2.2 Matrix Decomposition and SVD", "content": "Matrix decomposition involves factorizing a matrix M\u2208 Rm,n into a product of k matrices to simplify compu-\ntations and reveal underlying structures (Trefethen & Bau, 1997). Singular Value Decomposition (SVD) (Tre-\nfethen & Bau, 1997) is a prominent matrix decomposition method, which expresses a matrix M as M = UEVT.\nMatrices U \u2208 Rm,m and V\u2208 Rn,n consist of singular vectors and form a basis for the rows and columns\nof M, respectively. The matrix \u03a3\u2208 Rm,n is a diagonal matrix containing singular values on the diagonal,\nrepresenting the information and variance encoded per direction of the new basis (Trefethen & Bau, 1997).\nHowever, the SVD is exclusively applicable to matrices therefore can not be applicable to higher dimensional\ndata in the form of tensors without the need to undergo a matricization operation. This ultimately results in\na loss of relationship information between the elements of flattened modes. Consequently, tensor algebra\nintroduces a multitude of decompositions which operate directly in the tensor space hence solving the\nrelationship disruption problem."}, {"title": "2.3 Tensors and Tucker Decomposition", "content": "A tensor T\u2208 Rn1,n2,\u2026,nk an algebraic object that can be thought of as a multi-dimensional array that general-\nizes matrices to higher dimensions, allowing for more complex data representations. Consequently, a multitude\nof operations on matrices can be extended to tensors. One such extension is tensor decomposition (Kolda &\nBader, 2009). Among the most well known tensor decompositions we find the CANDECOMP/PARAFAC (CP)\ndecomposition (Kiers, 2000; Bove, 2010) and the Tucker tensor decomposition (Sheehan & Saad, 2007; Cheng\net al., 2023). As noticed by Kolda & Bader (2009), CP decomposes a tensor as a sum of rank-one tensors\nwith no additional constraints, whereas the Tucker tensor decomposition is a higher-order form of principal\ncomponent analysis. Hence, the Tucker decomposition provides sorted singular vectors according to singular\nvalues as opposed to the CP decomposition. This property is of importance to the modeling of our proposed\nTSM methods and motivates our choice of the Tucker decomposition over the CP decomposition or any other\ndecomposition (See Section 4).\nFocusing on the Tucker tensor decomposition, it decomposes a tensor Tas :\n$$T = C \\times_1 A^{(1)} \\times_2 A^{(2)} \\times ... \\times_k A^{(k)}.$$"}, {"title": "3 Related work", "content": ""}, {"title": "3.1 Feature attribution methods", "content": "Feature attribution methods are explainability techniques which assign a weight to each feature of the input,\nsuch as a pixel in an image (Lundberg & Lee, 2017). One such feature attribution method is the Guided\nBackpropagation method (Springenberg et al., 2015), which propagates positive gradients of the classification\nloss function to the input. However, to address the gradient saturation and the stochastic nature of gradients,\nmethods like Layer-wise Relevance Propagation (LRP)(Bach et al., 2015) and DeepLFT (Shrikumar et al.,\n2017) were proposed. These methods calculate a relevance score for the output of each neuron j as a weighted\nsum of all the neurons it is connected to. This operation is then back-propagated till the input, resulting\nin a heatmap showcasing the importance of each region to the network's output (Montavon et al., 2019).\nHowever, Lundberg & Lee (2017) notices that the latter methods require domain-specific knowledge, which is\ndataset-specific, and therefore introduces Deep SHapley Additive exPlanations (Deep SHAP) (Lundberg &\nLee, 2017). Deep SHAP uses Shapley values (Ichiishi, 1983) to calculate the contribution of each feature in\nthe network's input. Despite its strong theoretical backing, Deep SHAP's computational complexity poses\nchallenges for large datasets, making it less practical for real-time applications, even with approximation\nmethods (Aas et al., 2021). Moreover, all the stated methods highlights certain pixels which does not produce\nspatially human-interpretable coherent maps. LIME (Ribeiro et al., 2016) solves the latter problem and\nhighlights contiguous regions of the input. This is achieved by perturbing input data and create a locally\ninterpretable model around the predicted classification, relying on the model's output class.\nAll feature attribution methods are inherently label-dependent, as they calculate saliency maps based on\na specified target class in different ways. The reliance of feature attribution methods on label information\nprevents their application to self-supervised models, where no ground truth labels are available. In contrast,\nour proposed methods, TSM and MTSM, operate in a label-independent manner and produce visually\ninterpretable saliency maps by highlighting larger, contiguous regions of the input. Consequently, in our\nexperiments we exclusively evaluate the performance of feature attribution methods on classification models."}, {"title": "3.2 Class activation map methods", "content": "Building on feature attribution techniques, Class Activation Maps (CAMs) (Zhou et al., 2016) offer a\nvisual interpretation of model predictions, particularly in computer vision tasks. They have also found\napplication in different domains, such as anomaly detection (Kimura et al., 2020), disease identification\nand localization (Khan et al., 2019; Bouayed et al., 2022). CAM methods can be broadly categorized into\nlabel-dependent and label-independent methods. Below is a brief description of each family of CAMs."}, {"title": "3.2.1 Label-dependent class activation maps", "content": "Label-dependent class activation maps rely on the label information associated with the input. This\nlabel information is combined with the output of the network to estimate a loss and backpropagate the\nloss information to infer the weight vector w. This information can be in the form of gradients as in\nGradCAM (Selvaraju et al., 2017) and XGradCAM (Fu et al., 2020), or the impact of each feature map\non the correct classification as in ScoreCAM (Wang et al., 2020) and AblationCAM (Desai & Ramaswamy,"}, {"title": "3.2.2 Label-independent class activation maps", "content": "Label-independent class activation maps rely on the decomposition of the feature map tensor to infer the\nweight vector w. To the best of our knowledge, EigenCAM (Muhammad & Yeasin, 2020) is the only\nmethod in this family. EigenCAM flattenes the feature map tensor into a matrix, and after applying the\nSVD (Trefethen & Bau, 1997), the first right singular vector is used as the weight vector. Despite EigenCAM's\ngood performance, the matricization step disrupts spatial relationships between dimensions, captures a limited\namount of variance in the first singular vector and fails to utilize the full potential of tensor algebra, leading\nto a less accurate weight vector and a lower quality saliency map. To address these shortcomings, in the\nfollowing Section 4 we propose the TSM method, which leverages Tucker tensor decomposition for more\naccurate weight vector estimation and improved saliency maps. Moreover, since both EigenCAM and TSM\nrely only on one singular vector, in Section 5 we propose an extension of both methods to harness all the\nsingular vectors and singular values."}, {"title": "4 Tucker Saliency Maps", "content": "The Tucker Saliency Map (TSM) method is a local explainability approach. It involves generating saliency\nmaps in the form of heat maps highlighting significant regions in the input images of a CNN related to\nthe task at hand. To accomplish this, firstly the feature map tensor outputted by a convolutional layer is\nretrieved. Then, a weighted sum of this tensor is performed along the channels mode with the weight vector\nweighting each feature map estimated through the Tucker tensor decomposition. Consequently, it can be\nnoticed that TSM is applicable to both supervised and self-supervised CNN models as it does not require\ninformation regarding the loss but only requires access to the feature map tensor. Figure 1 provides an\noverview of the proposed method, visually summarizing its key components and process.\nIn detail, given a feature map tensor F\u2208 RC,H,W retreived as the output of a convolutional layer, we firstly\nperform its Tucker tensor decomposition :\n$$F = C \\times_1 A^{(1)} \\times_2 A^{(2)} X_3 A^{(3)}$$\nWith C being the core tensor, A(1) \u2208 RC,C, A(2) \u2208 RH,H, and A(3) \u2208 RW,W are the matrices containing the\northonormal singular vectors for each dimension. The choice of Tucker tensor decomposition among all tensor\ndecomposition methods is strategic, as it accurately identifies the direction of highest variance, a feature\ncrucial for weighting feature maps effectively (Kolda & Bader, 2009).\nSecondly, inspired by the work of Muhammad & Yeasin (2020), we consider the first singular vector i.e. the\none associated to the largest singular value, A(1) of the matrix A(1) as the weight vector w \u2208 RC. This can\nbe seen as projecting the feature map tensor on the direction with the highest variance. As a result, the\nobtained saliency maps highlighting more of the regions important to the model's output hence more accurate"}, {"title": "5 Multivector EigenCAM and Multivector Tucker Saliency Maps", "content": "Figure 3 represents the distribution of the first five singular values in both SVD decomposition and Tucker\ndecomposition divided by the sum of all singular values per tensor. From this figure we notice that for both\nthe SVD and Tucker decompositions singular values other than the first one encapsulate a significant amount\nof variance. Consequently, EigenCAM and TSM only convey a portion of the total information encoded in\nthe feature map tensor. To this end, in this section we introduce Multivector EigenCAM and Multivector\nTucker Saliency Maps (Multivec-EigenCAM and MTSM) explainability methods. These methods generate a\nsaliency map per singular vector in the SVD or the Tucker tensor decomposition. Then, a weighted sum\nof the generated saliency maps is performed. The weights of each saliency map are inferred based on the\nsingular values such as, given \u03c3i the singular values associated with the i-th singular vector (extracted either\nfrom the SVD or the Tucker decomposition), the weight are given by \u03c3i/\u03c31 where \u03c31 is the largest singular value.\nWe provide an overview of the proposed MTSM method in Figure 1.\nFormally, given a feature map tensor F \u2208 RC,H,W, an operator D(\u00b7) encapsulating the SVD or Tucker tensor\ndecomposition and returning the ordered singular values \u03c3\u2208 R and their corresponding singular vectors for\nthe channels dimension V\u2208 Rr,C:\n$$\\sigma, V = D(F)$$\n$$Mutivec-* = norm\\left(\\sum_{i=1}^r \\frac{\\sigma_i}{\\sum_{j=1}^C \\sigma_j}V_iF \\right)$$\nThis formulation of the Mutivec-* can be seen as first generating the EigenCAM or TSM, since the weight of\nthe first saliency map is one, than adding additional information to it which is generated using the rest of the\nsingular vectors. Consequently, the produced saliency maps are richer and more precise as demonstrated\nquantitatively and qualitatively in Section 6.\nThe differentiating point between uni-vector methods to its multi-vector ones in performance would depend\non the amount of variance encoded by the singular vectors other than the first one. If these vectors do not\nencode a large amount of variance the performance does not improve significantly (See the Tucker tensor\ndecomposition in Figure 3(a) and the quantitative results in Table 2). However, if they encode a large\namount of information, harnessing these vectors would result in significant performance improvement (See\nFigure 3(b)). Consequently, multi-vector saliency map methods in the worst scenario output the same result"}, {"title": "6 Experiments", "content": "In this section we start by presenting the metrics we use to evaluate explainability methods based on saliency\nmaps both on supervised classification models and self-supervised representation learning models. Then, we\npresent, discuss and compare the experimental results of our proposed methods compared to methods in the\nliterature. Moreover, in Appendix A we present Python code for the implementation of our proposed methods\nand in Appendix B we lay out our experimental setup. Appendices D, E and F put forward additional\nqualitative results for the different tested supervised and self-supervised CNN models."}, {"title": "6.1 Evaluation metrics", "content": ""}, {"title": "6.1.1 Supervised learning metrics", "content": "To evaluate our proposed methods on supervised models, we opt for the two conventionally used metrics, the\nAverage Drop and Average Increase proposed in the work of Chattopadhay et al. (2018) as they align with\nhuman interpretation and favor saliency maps with contiguous regions.\n\u2022 Average Drop (AD) This metrics quantifies the answer to the question \"Did we hide in the image\ninformation which is relevant to the target class ?\". To do so, the AD calculates the mean drop in\nclassification confidence for the target class via the following formula :\n$$AD(%) = \\frac{1}{N} \\sum_{i=1}^N \\left[\\frac{p_i - o_i}{p_i}\\right]_+ \\cdot 100.$$\n\u2022 Average Increase (AI)This metric measures if the saliency map has removed unnecessary informa-\ntion from the input image, hence removing noise and improving the model's confidence in the target"}, {"title": "6.1.2 Self-supervised learning metrics", "content": "Since, to the best of our knowledge, no framework exists for the evaluation of saliency map based explainability\nmethods on self-supervised CNN models, we propose one. The proposed framework relies on four metrics;\nnotably the Average Drop (AD), the Average Increase (AI) (Chattopadhay et al., 2018), the Mean Squared\nError (MSE), and the mean Intersection over Union (mIoU) metric (Jaccard, 1901).\n\u2022\n\u2022 Average Drop (AD) and Average Increase (AI) (Chattopadhay et al., 2018) To calculate the\nAD and AI, we estimate the saliency map from the self-supervised model, then evaluate the quality of\nthe saliency map using pretrained supervised classification model and the formulae described in the\nprevious Section 6.1.1. We detail the pretrained classification models used for each self-supervised\nmodel in Appendix B.4.\nMean Squared Error Since self-supervised models output an encoding for each input image, we\ncalculate the mean squared error between the encoding zi of the image i and the encoding zi of the\nimage i masked with the inferred saliency map. If the saliency map is able to isolate the important\nregions in the image which are encoded, there should be no difference between the encodings. This\nmetric is to be minimized and we formulate it as follows:\n$$MSE = \\frac{1}{N} \\sum_{i=1}^N \\left[z_i - z_i'\\right]^2.$$\n\u2022\nMean Intersection over Union (Jaccard, 1901) Under the hypothesis that the saliency map\nrepresents a segmentation highlighting the most important parts in an image, we calculate the mean\nIntersection over Union segmentation metric using the ground truth segmented images in the Pascal\nVOC dataset (Everingham et al., 2010). Formally, we calculate this metric using the following\nformula:\n$$B = \\begin{cases}\n1 & \\text{if } \\textit{saliency\\_map} < T \\\\\n0 & \\text{else}\n\\end{cases}$$\n$$mIoU(B, S) = \\frac{|B \\cap S|}{|B \\cup S|},$$\nwhere B is the pixel-wise binarized saliency map according to a threshold T, S is the ground truth\nbinary segmentation mask and |B\u2229S| (respectively, |B\u222aS|) represents the area of intersection (re-\nspectively, union) between the saliency map and the ground truth segmentation S. Moreover, to be\nthorough, we study the impact of the performance of the different proposed explainability methods as\na function of the threshold parameter T on a multitude of models. The obtained results are reported\nin Appendix C. We note that this metrics is to be maximized."}, {"title": "6.2 Results and discussion on supervised classification models", "content": "We evaluate and compare the TSM, MTSM and Multivec-EigenCAM method to a multitude of saliency map\nextraction methods on the pretrained VGG16 (Simonyan & Zisserman, 2014), Resnet50 (He et al., 2016) and\nConvNext (Liu et al., 2022) models. For the evaluation, we calculate the AD and AI on the 50,000 validation\nimages of the ImageNet dataset. The obtained quantitative and qualitative results are presented in Table 1\nand Figure 4 respectively."}, {"title": "6.3 Results and discussion on self-supervised models", "content": "In this section, we present the quantitative and qualitative results obtained by the methods we propose. We\nanalyse, discuss and compare them only to the results obtained by the EigenCAM method (Muhammad\n& Yeasin, 2020) as it is the only principled method which can be applied in this context\u00b3. In this section,\nwe calculate the AD, AI and MSE metrics on the 50,000 validation images of the ImageNet dataset (Deng\net al., 2009) whereas the mIoU metric is calculated on the 1,449 validation images of the Pascal VOC\ndataset (Everingham et al., 2010). The obtained quantitative and qualitative results are presented in Table 2\nand Figures 5, and 6.\nFrom Table 2, we observe the superior performance of Tucker decomposition based saliency map methods (i.e.\nTSM and MTSM) compared to the SVD based saliency map methods (i.e. EigenCAM and Multivec-\nEigenCAM). Additionally, similar to our observation on the supervised classification model, TSM acheives a\n50% improvement in performance on all metrics compared to EigenCAM. This is attributed to the benefits of\nusing the most suitable mathematical tools to manipulate tensors notably the Tucker tensor decomposition.\nAlso, the use of the absolute value in the TSM and MTSM methods brings forward negative contributions\nwhich are of interest. Consequently, through the used metrics we can conclude that compared to EigenCAM,\nTSM is able to :\n\u2022\nRetain more of crucial information in the image which is relevant to image classification.\n\u2022\nProduce saliency maps aligned with segmentation masks.\n\u2022\nConceal notably less input regions vital to the self-supervised model.\nAdditionally, we notice that the multi-vector methods improve on the performance of TSM. Also, Multivec-\nEigenCAM has a significant performance jump compared to EigenCAM as opposed to the performance\ngap between TSM and MTSM. This can also be explained by our previous argument on the supervised\nclassification models which links back to the amount of variance encoded by the first singular vectors. However,\nfor the VicRegL ConvNext model we notice a slight drop in performance going from TSM to MTSM. The\nreason for this is illustrated in Figure 3(a). We can see that a large portion the variance is captured by the\nfirst singular vector around 30% to 40%. Consequently, the rest of the singular values do not bring significant\ninformation. Combined with the weight assigned to these vectors, their impact on the final saliency map is\nminimal at best or adds noise to it.\nFigures 5, and 6 qualitatively confirm the quantitative results of Table 2. These figures illustrate a superior\ncoverage of important objects in the saliency maps generated by TSM compared to those produced by\nEigenCAM. For instance, we notice TSM providing better coverage of the man in the stadium and the\naudience behind the gymnast. Lastly, the visual representations in Figure 6 underscore the qualitative findings"}, {"title": "7 Conclusion", "content": "In this work, we introduce the TSM method, a label-independent CNN explainability method harnessing\ntensor algebra notably Tucker tensor decomposition to produce high quality and fidelity saliency maps of\nCNN's input regions. TSM is a principled method, not restricted to supervised classification CNN models\nand yet achieves competitive quantitative and qualitative results compared to label-dependent methods.\nMoreover, compared to the only CNN label-independent method namely EigenCAM, TSM allows for a more\naccurate estimation of singular vectors and values resulting in more accurate saliency maps. Furthermore, we\nextend the EigenCAM and TSM method to the Multivec-EigenCAM and MTSM which take profit of all\nthe singular vectors and values further improving the explainability of supervised and self-supervised CNN\nmodels. We test all the label-independent methods on various supervised classification CNN models via\nestablished metrics and on self-supervised CNN models through an evaluation procedure we propose. When\ncomparing EigenCAM to TSM, our results indicate a 50% improvement in quantitative results on all tested\nmetrics and models. Multivec-EigenCAM and MTSM further improve on the performance of their base\nmethods with varying extents depending on the amount of variance encoded by each singular vector. Our\nqualitative results showcase that EigenCAM can be misleading when used as an explainability method. This\nis because it highlights fewer regions of the input in the saliency map compared to our proposed methods\nwhich are more reliable since they highlight all the significant regions.\nIt worth noting that TSM and MTSM come with increased computational costs, requiring approximately 2.5\u00d7\nmore processing time than EigenCAM due to the Tucker tensor decomposition. Despite this complexity,\nthe significant improvements in explainability and the methods' effectiveness in interpreting self-supervised\nmodels justify the additional computational burden. Future work will focus on optimization techniques to\nreduce execution time, ensuring that these methods remain efficient."}]}