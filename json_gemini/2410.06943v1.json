{"title": "AUTOFEEDBACK: AN LLM-BASED FRAMEWORK FOR EFFICIENT\nAND ACCURATE API REQUEST GENERATION", "authors": ["Huanxi Liu", "Jiaqi Liao", "Dawei Feng", "Kele Xu", "Huaimin Wang"], "abstract": "Large Language Models (LLMs) leverage external tools primarily through generating the API\nrequest to enhance task completion efficiency. The accuracy of API request generation significantly\ndetermines the capability of LLMs to accomplish tasks. Due to the inherent hallucinations within the\nLLM, it is difficult to efficiently and accurately generate the correct API request. Current research\nuses prompt-based feedback to facilitate the LLM-based API request generation. However, existing\nmethods lack factual information and are insufficiently detailed. To address these issues, we propose\nAutoFeedback, an LLM-based framework for efficient and accurate API request generation, with a\nStatic Scanning Component (SSC) and a Dynamic Analysis Component (DAC). SSC incorporates\nerrors detected in the API requests as pseudo-facts into the feedback, enriching the factual information.\nDAC retrieves information from API documentation, enhancing the level of detail in feedback. Based\non this two components, Autofeedback implementes two feedback loops during the process of\ngenerating API requests by the LLM. Extensive experiments demonstrate that it significantly improves\naccuracy of API request generation and reduces the interaction cost. AutoFeedback achieves an\naccuracy of 100.00% on a real-world API dataset and reduces the cost of interaction with GPT-3.5\nTurbo by 23.44%, and GPT-4 Turbo by 11.85%.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are becoming increasingly intelligent and autonomous, progressing towards solving\nreal-world pragmatic tasks [1][2][3][4][5]. By combining the impressive intention comprehension ability with external\ntools, such as Application Programming Interfaces (APIs) [3][4][6] and code interpreters[7][8], LLMs have further\nextended their application scenarios.\nNotwithstanding substantial advancements in tool-augmented LLMs applied in research assistant [9][10], and software\ndevelopment [11][12]. The potentiality inherent in the integration of LLMs with external tools remain predominantly\nunderexplored. Even one of the existing state-of-the-art models (GPT-4 [13]) still struggle to use tools to solve\nreal-world problems[3][4][14]. One of the primary problems is that the LLM is susceptible to hallucinations [15] -\ngenerating plausible yet ungrounded information. This poses great challenges in using external tool that usually requires\nstandardized format input and accurate parameter values.\nTo reduce the hallucinations, fine-tuning [2][6][16] on particular datasets has been proven to be an effective approach.\nWhile this approach typically works for specific tools within the dataset, it is hard to generalize, not to mention the high\nlabor costs associated with dataset construction.\nOther existing research provides linguistic prompts to stimulate the planning and reasoning ability of LLMs [17]. For\nexample, task demonstrations[18], self-thoughts[19], self-examination [20][21] are used as static feedback (no direct\ninteraction with external environment), to complete user's task. However, instead of providing factual information such\nas API call format error and runtime execution error, static feedback predominantly comes from the output of LLMs,\npotentially exacerbating the phenomenon of hallucinations. Further, methods interacting with external environment\nprovide dynamic feedback to facilitate the task completion process [22][23]. However, such feedback lacks sufficient\ninformation, resulting in less accuracy in guiding the LLM.\nWe use a real-world example to illustrate the deficiency in dynamic feedback, as shown in Figure 1. The LLM assists\nusers in planning their driving routes through the route_planning API, but it incorrectly reverses the order of latitude\nand longitude, causing failure API execution. And the API response only contain an \"info_code: 20000\" as a brief\nmessage, which is imprecise for LLMs to fix API error.\nTo alleviate these issue, we first investigate the API request generation of different LLMs on four datasets, and\nsummarize the following four main error types as the factual information in the feedback. We provide motivating study\nfor each error in Section 2:\n1) E1: NO_API: API requests are not generated or the generated is failed to be parsed due to incorrect format.\n2) E2: API_NAME_MISMATCH: It could be a formatting mismatch between the generated API name with\nthe API documentation, or a mismatch with the user instruction.\n3) E3: PARAMETER_INVALID: The generated output contains invalid parameter names which doesn't\nappear in the API documentation.\n4) E4: INPUT_MISMATCH: The types of parameter values are mismatched with the API documentation, or a\nmismatch with the user instructions."}, {"title": "2 MOTIVATING SYUDY", "content": "In this section, we provide detailed motivating examples to illustrate the four errors mentioned in Section 1."}, {"title": "2.1 API Request Generation", "content": "Taking user's instructions as input, LLMs generate API requests based on the API documentation. However, due to\nthe hallucinations, the LLM has difficulty in generating correct API requests and cannot meet user requirements. We\nconducted research on various errors of the generated API request across four datasets. API-Bank [14] is a famous\ntool-using synthetic dataset with 2138 APIs, 3146 samples and 1000 domains which determines the functionality of the\nAPIs, such as healthcare and fitness; Materials Project API (MP-API) is a code dataset on Chinese instructions with\n26 APIs and 379 samples; ToolAlpaca-single and -mix datasets [6] are open-source datasets of real-world API tools.\nThe general information about datasets is shown in the Table 1, and We will provide other detailed descriptions about\ndatasets in the experiment section."}, {"title": "2.2 Error Analysis", "content": "The results of the API error cases are shown in Figure 2. We categorize errors based on their location in API requests,\nas the structural information contained within the error categories is factual, which could be subsequently fed back to\nthe LLM. The generated API request is compared with the ground true to determine its error type, and the order of our\njudgement process is E1->E2->E3->E4. The results indicate that all of LLMs are prone to produces incorrect output in\nthe API request generation.\nTo increase factual information in the error categories, we further decompose these major errors into sub-errors. If the\nLLM is unable to generate the correct API format, it is classified as E1. With regards to E2, we found the sources fall\ninto the following three main dimensions: E2.1 Selection Error: calling other candidate API name; E2.2 Literal Error:\nincorrect letter case, or different naming methods (camel method, underscore method) in API name; E2.3 Semantic\nError: semantically similar to the correct API name. Similar to E2, the sources of E3 also come from these three\ndimensions: E3.1. Selection Error: calling parameter name in other candidate API; E2.2 Literal Error: incorrect\nletter case, or different naming methods in parameter name; E2.3 Semantic Error: semantically similar to the correct\nparameter name. It is difficult for rule-based methods to locate the error types of value, and we come up with E4.1:\nType Error: The parameter type is inconsistent with the API documentation.\nFigure 2 shows that the main source of API errors is actually E2.1, where LLMs misinterpret user intent and choose\nother candidate APIs. API requests generated by general LLMs (LLaMA-2-7B and GPT-3.5 Turbo) are difficult to\nparse correctly (E1). There are some formatting of the API name existing in LLaMA-2-7B, such as E2.2 and E2.3. The\nerror of invalid parameter names (E3) are relatively few. The other errors in E4 take up a large portion, as there is no\nground true for parameter value, making it difficult to accurately classify error types.\nThese predefined error types would be used as pseudo-facts for feedback, which greatly diminish the hallucinations of\nthe LLM in API request generation. In the experiment section, this point will be illustrated by comparing the effects of\ndifferent granularity error types on static feedback."}, {"title": "3 METHODOLOGY", "content": "Our Framework, AutoFeedback contains two low-coupled and complementary components: Static Scanning Component\n(SSC) and Dynamic Analysis Component (DAC). We combine it with the origin process (gray arrows) of LLM calling\nAPI, creating two feedback loops, as illustrated in Figure 3. To avoid the dead loop, we set the maximum number of\nfeedback as a hyperparameter.\n1. Static feedback (green arrows). The LLM takes the API documentation and user instruction as input, parses its\ngenerated output to get the API request. SSC perform a local static scanning to determine if any error exists in the API\nrequest. If none, proceed with sending the request; if any, conduct error detection to locate sub-errors, and give the\nprecise static feedback to the LLM.\n2. Dynamic Feedback (purple arrows). After the API request is sent to the server and executed, the user assesses\nwhether the returned API response meets his requirement. For those that failed, the DAC retrieves their detail error\nmessage in the API documentation and combines with the API response as dynamic feedback.\nComponent Input: The input of SSC includes user instruction, API documentation (including API name and its\ndescription, the type and description of its required parameter, exception specification), and the API request in the\nformat APINAME(key1=value1, key2=value2,...) generated by the LLM; The input of the DAC additionally includes\nthe API response from the API server.\nOur framework also provides support for human-machine collaboration, recording all information in feedback as the\nlogs to assist users in solving tasks."}, {"title": "3.2 Static Scanning Component", "content": "The LLM suffers from serious hallucinations [15][17], which lead to difficulties in generating API requests with\nstandardized format input and accurate parameter values, further constraining their ability to accurately leverage\nexternal tools. To implements lexical constraints on the output of LLMs, some work restrict the decoder [27][28] to\nmodify the probability distribution of output which often result in significant performance losses. Other work activate\nthe self-reflection ability in LLMs through elaborate prompts [20][21][29]. But these approaches only provide static\nfeedback from the output of LLMs, which may include hallucinations and lack of the factual information.\nSSC categorizes errors into predefined types in Section 2, providing the factual information about the error location and\nerror cause, significantly increasing the validity and accuracy of static feedback."}, {"title": "3.2.1 \u0391\u03a1I Retriever.", "content": "In Section 2, a major fraction of the cases in E2 are mistakenly selected other candidate APIs (E2.1). To deal with this\nproblem, we use a deep learning model as an API retriever M, which could calculate the semantic similarity between\ntwo texts.\n\\(M(text1, text2) = S\\),, \\(S \u2208 [0, 1]\\).\nThe model encodes the user instruction and the API description in documentation into two embeddings, and calculates\ntheir relevance with embedding similarity. Then, we ranks APIs in documentation based on the similarity score between\nthe description and the instruction. Finally, SSC list the top k scoring APIs (generally set k to 1) as relevant APIs set R,\nwhich is subset of the API documentation. The API names in R are consider to meet user needs, and will be provided\nfor Error Dection sub-component."}, {"title": "3.2.2 Error Detection.", "content": "The LLM output may not contain API requests or cannot be parsed, which could be summarized as E1. For other\ngenerated API requests, SSC perform automated error detection as follow:\nAPI name (E2.1, E2.2, E2.3). First SSC determines whether the API requests r\u00b9 generated by the LLM is in R. If not\n(r\u00b9 \u2209 R), the API name is incorrect (E2). And the next is to identify the error source. If the API name appears in the\nAPI documentation, the sub-error type is E2.1. This is considered to be a misunderstanding of user intent by the LLM,\nwhich resulted in selecting other APIs incorrectly. Because of the different in code styles, the LLM incorrectly generates\nthe name of the underscore (e.g. user_login), whose real value should be the hump method (userLogin). SSC changes all\nAPI names to lowercase and remove characters other than alphanumeric by regular expression (r\u2019[^a-zA-Z]'). If the\nAPI r\u00b2 in API documentation and r\u00b9 have the same name after modification, the sub-error type is E2.2. The detection\nfor E2.3 is primarily aimed at LLMs making up non-existent and fake APIs in hallucinations. For example, the user\ninstruction is \"I'm trying to find out how much aspirin is left.\", and the generated fake API is \"find_aspirin_number()\"\nwhile the real API is \"list_medicines(name='aspirin')\". We also use M to detect semantic similarity scores between\nthe name of r\u00b9 and the name of candidate APIs in API documentation. If there is an API r\u00b3 that causes the score to\nexceed the threshold (generally set to 0.5), the sub-error type is E2.3. Other situations are the other error in E2.\nAPI parameter name (E3.1, E3.2, E3.3). When r\u00b9 \u2208 R, SSC next verifies the correctness of the API parameter name.\nUnlike the API name, we already know the correct parameter names, as they are provided in the API documentation. If\nthe parameter name in r\u00b9 does not exist in the corresponding description in the API documentation, which serves as\nE3. We consider this incorrect API parameter as p\u00b9. If the name of p\u00b9 appears in the parameter list of other APIs in\nthe document, the sub-error type is E3.1. SSC perform the same operation on API parameter names as in E2.2 and\nE2.3 for API name. The sub-error type is E3.2 when p\u00b9 matches the parameter p\u00b2 in other API after modification. If\nthe similarity score between parameter p\u00b3 in API documentation and the name of p\u00b9 is greater than the threshold, it is\nconsidered E3.3. Other situations are the other error in E3.\nAPI parameter value (E4.1). Due to the great flexibility in parameter values, we could only make preliminary\njudgments based on the parameter type. SSC parses all parameter values in r\u00b9 and compare them with the standardised\ndata types in the API documentation. The sub-error type is E4.1 when the match of the parameter value v\u00b9 is incorrect\nand the corresponding parameter serves as pr. In some case, there are no errors in after static scanning, but existing\nduring the calling process, or the calling results still cannot meet the user's requirements, it is the other error in E4.\nReturn values. SSC performs error detection in ascending order of number (e.g. E1->E2, E2.2->E2.3,), and once an\nerror is detected, return the corresponding value to enter static feedback."}, {"title": "3.2.3 Static Feedback.", "content": "SSC receives the generated API request as input, and with the assistance of the API retriever, conducts error detection,\nresulting in static error. Then, SSC selects the appropriate prompt template based on the error type and completes it\nbased on other return values to generate the static feedback. As illustrated in Figure 4), the template is roughly divided\ninto five parts, with the granularity of feedback becoming finer sequentially. Firstly, we declare that there is an error in\nthe API request; Secondly, inform the error location and content; Thirdly, eliminate sources of errors that have already\npassed detection (for E2.3, E2.1 and E2.2 supposed to be excluded); Then, locate the source of the error and provide\nsuggestions for correction; Finally, request the LLM to regenerate API requests. The static feedback of some errors\nmight be missing certain parts. For instance, in E1, due to the first detection, there is no third part."}, {"title": "3.3 Dynamic Analysis Component", "content": "DAC focuses on the fact that many APIs return insufficient information (e.g. error code), after execution at the\nserver, rather than detailed error messages (as shown in Figure 1). In this case, the user have to continue querying\nthe API documentation to find a detailed explanation of the corresponding error code, which is user-unfriendly and\ninconvenient. DAC automates the retrieval process, increases the detail level of dynamic feedback, and facilitates to\ncorrect corresponding errors in the API request."}, {"title": "3.3.1 Document Retrieval.", "content": "DAC retrieves the details of the corresponding error from API documentation. We take the Retrieval Augmented\nGeneration (RAG)[23] approach to extract the error-related message.\nThe relevant code example is shown in the Figure 5. We first split the API documentation to obtain all document content\nrelated to the API (including functional descriptions, exception specification, etc.). Then we split these contents into\nsentence granularities, and use the embed model to vectorize sentences. Next, we places semantically similar sentences\ninto the same data block, achieving semantic chunking and vectorization. This is equivalent to building a vector indexed\ntext database, where vectorized sentences serve as indexes.\nDuring the retrieval process in Figure 6, DAC uses the API name (e.g. route_planning) to match corresponding chunk\nvectors. Then use the same embed model to vectorize user query, and find for the most similar chunk vector as an\nindex (implemented by approximate nearest neighbor algorithm), to retrieve the corresponding text from the database\nconstructed earlier. Finally, a more detailed error message is obtained, such as the \"Longitude precedes latitude\", instead\nof \"info_code:20000\" in Figure 1."}, {"title": "3.3.2 Dynamic Feedback", "content": "The LLM uses its own internal representations to produce outputs which is not grounded in the external world, while\nit's crucial for the tool-augmented LLM to interaction with external tools, which could mitigate the possibility of the\nfact hallucinations. Therefore, our framework integrates ReAct algorithm[22] into the DAC, which allows LLMs to\nobserve the API response to incorporate factual information into reasoning. Simultaneously, we combined the error\nmessage from the Document Retrieval sub-component with the API response to form the dynamic feedback.\nThe overall process of the Dynamic Feedback is shown in the algorithm 1. The server executes the sent API request r\nand obtrains the corresponding API response P. The next step is to assess whether the requirement of user have been\nmet or if the maximum feedback count N has been reached. Returns P directly if the condition is satisfied. Otherwise,\nit enters the dynamic feedback loop: DAC first retrieves the corresponding error message in the doc; The API request\nr serves as the action and API response and error message are combined as the observation in ReAct, which are fed\ninto LLM together with Record; Newly generated API request and the thought will be stored with the action and the\nobservation as a feedback record for the next feedback. Finally, we assess on the server's new execution result."}, {"title": "4 EXPERIMENTS", "content": "We evaluate AutoFeedback using four research questions:\nRQ1: Does the AutoFeedback improve the accuracy of API request generation and effectively address four typical\ntypes of errors?\nRQ2: Does AutoFeedback reduce the interaction cost of generating accurate API request with LLM?\nRQ3: What is the performance contribution of each component in the framework?\nRQ4: What challenges AutoFeedback face in practical applications?"}, {"title": "4.1 Experimental Settings", "content": "To verify the effectiveness of AutoFeedback, our experimental settings comprises two scenarios: single-API calling\n(1,2,3) and multi-API planning (4)."}, {"title": "4.1.1 Scenarios and Datasets", "content": "1) API-Bank[14]. It is a well-known synthetic API dataset designed for tool-using LLMs.\n2) MP-API2. It is a code dataset built on a Python library (Materials Project), measuring the ability of LLMs to\ncall materials chemistry related tools, annotated by human experts.\n3) ToolAlpaca-single[6]. We conducted task decomposition for each sample in the real-world API dataset\nToolAlpaca, to ensure a single API request could complete the sub-task.\n4) ToolAlpaca-mix[6]. Due to API inaccessibility or other reasons, the number of available APIs in the original\ndataset is reduced. We construct the ToolAlpaca-mix by mixing the remaining functioning APIs in its simulated\ndataset.\nIn the first scenario, the model addresses the user's problem by calling a single API request. However, in realistic\napplication scenarios, LLMs generally are supposed to plan how to combine multiple API calls to solve problems.\nThe general information about four datasets in shown in Table 1. The average number of minimum API requests\nrequired in ToolAlpaca-mix is 1.38, the rest are single API datasets. ToolAlpaca-mix has the highest average number\nof parameters per sample at 4.51, while the rest in descending order are MP-API for 3.80, API-Bank for 2.18,\nToolAlpaca-single for 1.99."}, {"title": "4.1.2 Experimental Subject", "content": "To validate the effectiveness and generality of AutoFeedback, we have selected four widely used and advanced LLMs.\nConsidering different usage, the models for our experiments include open and closed source, non-fine-tuning and\nfine-tuning models.\n1) LLaMA-2-7B[25]. It is one of most popular LLMs in the open-source community.\n2) Mistral-v0.2-7B[26]. It is one of the best performance models in the open-source community at the same scale.\n3) ToolAlpaca-7B[6]. It is the same architecture as LLama-2-7B, fine-tuned on the ToolAlpaca training dataset\n[6].\n4) GPT-3.5 Turbo[24]. Published by Open AI, it is broadly used closed-source LLM in commercial applications.\n5) GPT-4 Turbo[13]. Published by Open AI, it is currently one of the most powerful and intelligent LLMs."}, {"title": "4.1.3 Evaluation Metrics", "content": "1) Accuracy. The accuracy is calculated by dividing the number of API request samples that meet user require-\nments by the total number of samples. Due to the lack of API servers for API Bank and MP-API, we consider\nthe generated API request that is identical to the standard answers as meeting user requirements. For the\nToolAlpaca-single and -mix, we followed the approach in [6] which passes API requests and responses to\nGPT-4 for evaluation.\n2) Process Correctness. For the scenario of multiple API calls (ToolAlpaca-mix), the call unrelated to the task\ncould lead to unnecessary cost. We use it to measure the optimality of API request sequences. Same as [6],\nwe employ GPT-4 to assess whether the API requests generated by the LLM are consistent with the standard\nanswer.\n3) Overhead. Existing commercial LLMs all charge for the number of tokens generated, and [28] uses the token\nnumber to denote the generated overhead. We further measure the interaction overhead using \\(\\frac{tokeni}{Accuracy}\\), which indicate the average overhead required for 1% improvement in accuracy.\nWe used the same evaluation prompt in the paper [6] for Accuracy and Process Correctness. In addition, we selected\nthree experts to evaluate the results under the same conditions to demonstrate the effectiveness of GPT-4 evaluation."}, {"title": "4.1.4 Experiment Environment", "content": "We implemented AutoFeedback using PyTorch. We run all experiments on Ubuntu 20.04. The experiments were\ncarried out on a machine with one Intel(R) Core(TM) i9-10850K CPU @ 3.60GHz, 32 GB main memory and one\nNVIDIA GeForce RTX 3090. The default API Retriever and the embed model in AutoFeedback are lightweight\nsentence-transformers. For all experiments the maximum number of static feedback was set to 3, and the dynamic one\nwas set to 2."}, {"title": "4.2 Experimental Results and Analysis", "content": "RQ1: Does the AutoFeedback improve the accuracy of API request generation and effectively address four\ntypical types of errors?\nWe evaluated the most important metric - Accuracy acorss a wide range of datasets and LLMs, illustrated in Table 3. The\n\"Base\" means the base performance without feedback. Since API-Bank and MP-API dataset do not have corresponding\nserver to return responses, we have only made static feedback for them.\nThe result demonstrates that AutoFeedback significantly improves the accuracy of various types of LLMs across all\nexperimental datasets, reaching a maximum of 40.52%. In particular, GPT-4 Turbo with AutoFeedback addressed nearly\nall problems in two real dataset scenarios, achieving 100% on ToolAlpaca-single and 97.76% on ToolAlpaca-mix;\nGPT-3.5 Turbo, attained 83.56% accuracy on API-Bank, 61.48% on MP-API, and also solved almost all tasks on\nToolAlpaca-single with 97.41% accuracy and 94.03% on ToolAlpaca-mix; LLaMA-2-7B obtained 66.59% accuracy on\nAPI-Bank compared to 47.87% for the Base; for ToolAlpaca-single, it solved 75.00% of the problems, far exceeding\nthe Base of 34.48%; similarly on ToolAlpaca-mix, it improved from 32.83% to 61.19%. In addition, Mistral-v0.2-7B\nincreased its accuracy from 70.18% to 79.69% on API-Bank; as a fine-tuned LLM, ToolAlpaca-7B also achieved\naccuracy of 87.93% and 89.55% for ToolAlpaca-single and ToolAlpaca-mix, respectively, which is higher than the\n68.97% and 64.18% at \"Base\".\nTo find out whether AutoFeedback accurately solves the typical errors, we used the API-Bank dataset as an instance to\nstudy the distribution of error types for GPT-3.5 Turbo. Meanwhile, to present the necessity of the feedback method, we\nfirst selected a non-feedback method as a comparison:\n1) LMQL[30] is the state-of-the-art language model programming method which improves the specification of\ncontent generated by LLMs via constraints.\nThe LMQL method only needs to predict the values in square brackets \"[]\": APINAME(key1=[value1], key2=[value2],\n......).\nThen, to present the importance of detailed and factual feedback, we compared AutoFeedback with two representative\nand publicly available LLM coarse-grained feedback approaches:\n2) DTG[21] is a novel prompting framework. The LLM is self-reflective about its own generation through few\ndemonstrations, detecting errors and correcting them.\n3) Reflaxion [29] is the state-of-the-art reinforce language agents framework. The external environment gives\nbinary feedback to the LLM-based agent, i.e. success or failure.\n4) AutoFeedback without Error Detection, (AutoFeedback w/o ED). Removing Error Detection sub-\ncomponent in AutoFeedback, only gives feedback on relatively coarse-grained errors in API request, which\nare E1, E2, E3, E4.\nThe result is shown in the Table 4. Compared to all baselines, AutoFeedback achieved optimal performance (83.57%).\nApart from the slightly higher percentage of 0.86% on El compared to the best Reflaxion's 0.73%, its proportion\nacross all categories of issues remained the lowest, effectively mitigating a spectrum of problems. Furthermore, with\nthe granularity of feedback from coarse-to-fine, performance also gradually improves. For instance, Reflaxion gives\nbinary feedback, AutoFeedback w/o ED gives coarse-grained feedback, and AutoFeedback gives fine-grained feedback.\nRelevant data for other models is the same tendency, and could be found in Appendix ??.\nAs a further step, we understand that the resolution of sub-errors at a more fine-grained level, using LLaMA-2-7B\nas an example, is shown in the Figure 7. AutoFeedback achieved the largest error decline rate on all types of errors\nwhich we could give accurate feedback. While DTG performed poorly and even added errors on E2.1, E2.2, E3.3. The\nreason for this discrepancy lies in DTG's lack of demonstrations in the initial feedback iteration, resulting in inaccurate\nself-detection error types. In contrast, the error types in AutoFeedback contain factual information. Similar to Table 4,\nthe finer the feedback granularity, the more the error rate decreases.\nThese experiments indicate that an increased provision of factual details through static feedback correlates with an\nenhanced precision in the generation of API requests by the LLM."}, {"title": "RQ2: Does AutoFeedback reduce the interaction cost of generating accurate API request with LLM?", "content": "We present the accuracy of ToolAlpaca-single and the interaction overhead in Table 5. Although the average number of\ntokens for \"Static & Dynamic\" feedback is the highest, due to its vastly elevated accuracy, the resulting interaction\noverhead is instead actually the smallest. For example, AutoFeedback has reduced the interaction overhead of GPT-3.5\nTurbo from 13.01 to 9.96, a decrease of 23.44%, GPT-4 Turbo from 10.97 to 9.67, resulting in 11.85%.\nThe interaction overhead of three iterations of the LLMs on API-Bank is illustrated in Figure 8. It could be observed that\nAutoFeedback maintains the lowest interaction cost under the same round of iterations on all LLMs. It is noteworthy\nthat the interaction overhead tends to grow with the number of iterations increases. However, for highly intelligent\nLLMs such as GPT-3.5 Turbo, the overhead does not increase significantly."}, {"title": "RQ3: What is the performance contribution of each component in the framework?", "content": "We first report the results of ablation study about the SSC and DAC components in Table 3. Compared to the \"Base\",\nboth \"Static\" and \"Dynamic\" feedback could improve accuracy, and their combination further enhance performance, far\nexceeding the effect of using them alone.\nNext, regarding the Error Detection sub-components of SSC, we found that the absence of the sub-component in\nAutoFeedback (\"AutoFeedback w/o ED\") prevents it from accurately resolving errors in API requests, resulting in a\nperformance degradation of 2.55% in Table 4."}, {"title": "RQ4: What challenges AutoFeedback face in practical applications?", "content": "1. API Documentation. The most important prerequisite for AutoFeedback to operate successfully in real-world\napplication is to have a standardised, descriptive and comprehensive API documentation. It is required that the\ndocumentation contains a correct description of the API functionality and its parameters. It is challenging for the fact\nthat the API documentation is in a high-speed iterative updating process. For example, we have found in practice that\nsome of the API tools in ToolAlpaca [6] have been lost due to a variety of issues, such as service inaccessibility, or API\nparameter change. The documentation is supposed to be updated in real-time as much as possible.\n2. API Retriever. Additionally, the selection of API retriever is also crucial, which is closely related to the resolution of\nthe E2. In practical applications, we could utilize text information retrieval algorithm[31] or fine-tune a pre-trained\nmodel, such as BERT[32].\n3. Feedback Failure. We found that with increasing feedback iterations, the accuracy is no longer increasing. Feedback\ndoes not improve the accuracy of API calls indefinitely, and the accuracy bottleneck remains closely related to model\nperformance. When the set maximum number of feedback is reached but the problem remains unresolved, manual\ncollaboration is required. In our experiments, GPT-4 Turbo cannot solve an sample in ToolAlpaca-mix, which is\nillustrated in Figure 9."}, {"title": "4.3 Evaluation Effectiveness", "content": "For the API-BANK and MP-API datasets, due to the lack of API servers, we compared them with the standard answers\nin the dataset to determine their correctness. This is a rigorous and effective evaluation method. However, API requests\nthat are inconsistent with standard answers in real-world scenarios could still meet the user requirement. Therefore,\nfor Toolalpaca-single and -mix datasets, we used the LLM as an agent to better evaluate the quality of API request\ngeneration.\nAlthough the LLM struggles to generate API requests, it is advisable to evaluate the results. As these are two tasks with\ndifferent contexts, the evaluation task only requires the LLM to play the role of the user without requiring excessive\nprofessional knowledge. This method has been widely applied [6][33][34][35] and is considered to be more aligned\nwith human preferences. We perform API parsing on the LLM output to avoid potential bias caused by direct evaluation\nby GPT-4. Simultaneously, we invited three human experts to rate the experimental results to validate the effectiveness\nof the LLM evaluation."}, {"title": "5 THREATS TO VALIDITY", "content": "We have identified the following two major threats to validity:\n(1) Unpredictable API format: The process of generating API requests in LLM is inherently uncertain. Even with\nspecified formats being set in prompts", "interrupt": "Due to ambiguity in user instructions and API documentation or limitations in the\nLLM itself, AutoFeedback struggles solving problems in some cases, and could trap in an infinite feedback loop.\nIn response, we have set up a mechanism for human collaboration. When reaching the preset maximum number of\nfeedback iterations, AutoFeedback will break the loop to wait for the user to manually intervene, and"}]}