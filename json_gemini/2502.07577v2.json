{"title": "Automated Capability Discovery via Foundation Model Self-Exploration", "authors": ["Cong Lu", "Shengran Hu", "Jeff Clune"], "abstract": "Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce AUTOMATED CAPABILITY DISCOVERY (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers both surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. All code and evaluation logs are open-sourced at https://github.com/ conglu1997/ACD.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs; Gemini Team, 2024; OpenAI, 2024b; Touvron et al., 2023), trained on internet-scale datasets, have revolutionized natural language processing by demonstrating strong general-purpose capabilities. These \"Foundation Models\" (FMs; Bommasani et al., 2021) display exceptional performance on tasks requiring common- sense knowledge (Talmor et al., 2019), reasoning (Wei et al., 2022), and comprehension (Chang et al., 2024), enabling applications ranging from conversational agents (Brown et al., 2020) to code generation (Gauthier, 2024). Recently, agentic systems powered by foundation models have even shown the capacity to propose and investigate new scientific ideas (Lu et al., 2024c) and provide ever-better agentic systems (Hu et al., 2024). However, identifying unknown abilities or failure modes in FMs remains a major challenge, especially because such knowledge is crucial to ensuring both safe deployment and maximizing real-world utility.\nTraditional evaluation techniques-centered around human- created benchmarks (BIG-bench authors, 2023; Chen et al., 2021a; Cobbe et al., 2021; Dua et al., 2019; Hendrycks et al., 2021a;b; Phan et al., 2025; Zellers et al., 2019)-are labor- intensive to create and limited by predefined categories, often failing to capture the full spectrum of a model's capac- ities or unearthing surprising new behaviors pre-deployment. Moreover, as models become more advanced, they may sat- urate or overfit these benchmarks, so those metrics may not reflect broader performance gains. Users also commonly encounter unique use cases and failure modes not covered by benchmarks. While frequently updating or creating new test suites (Phan et al., 2025; White et al., 2024) attempts to address these issues, continually devising new tasks is expensive, not model-specific and will fail to probe the 'un- known unknowns' (things that benchmark creators do not think to include). This underscores the need for scalable, efficient evaluation methods that are cheap and require mini- mal overhead to keep pace with rapidly evolving foundation models (Bowman et al., 2022).\nWe introduce AUTOMATED CAPABILITY DISCOVERY (ACD), a framework that augments existing evaluation approaches by automating the discovery of a foundation model's capabilities and failure modes. It designates one model as a scientist to systematically propose open-ended tasks for a subject model, which could be itself or a different foundation model (Section 4). Concretely, ACD instructs the scientist to propose interesting new challenges (Faldor et al., 2024; Lu et al., 2024c; Pourcel et al., 2024b; Shah et al., 2024a; Zhang et al., 2024a;b), asks the subject to at-"}, {"title": "2. Background", "content": "2.1. Open-ended Discovery Algorithms\nOpen-ended algorithms (Stanley & Lehman, 2015; Stan- ley et al., 2017) aim to continuously generate novel and diverse artifacts (Hughes et al., 2024) within a search space, rather than focusing on a fixed objective. These algorithms emulate human creativity by autonomously exploring new artifacts, increasingly supported by large foundation models that can encode intrinsic notions of \u201cinterestingness\u201d (Fal- dor et al., 2024; Lu et al., 2024b; Zhang et al., 2024b). They have been applied to evolving novel robot morphologies in code (Lehman et al., 2022), generating new reinforcement learning environments (Faldor et al., 2024; Wang et al., 2019; 2020), discovering novel loss functions (Lu et al., 2024a) and agentic systems (Hu et al., 2024), and investigat- ing scientific hypotheses (Lu et al., 2024c).\nGenerally, these algorithms maintain and update an archive A of discovered artifacts. At iteration t, they sample a new artifact at from a foundation model M conditioned on a sub- set Ct-1 of previously discovered artifacts, typically limited in size for computational feasibility. The generated artifact at is evaluated for novelty (e.g., via embedding-based simi- larity), and then added to the archive if sufficiently different from others in A. ACD adapts these principles to systemati- cally reveal a foundation model's capabilities, treating each novel capability or failure as a generated \u201cartifact\"."}, {"title": "3. Related Work", "content": "Open-Ended Discovery with Foundation Models. The field of open-endedness (Stanley, 2019) aims to continually discover diverse and novel artifacts forever. Recent methods leverage the generative capabilities and vast prior knowledge of foundation models (FMs) to accelerate this process (Fal- dor et al., 2024; Hu et al., 2024; Lehman et al., 2022; Zhang et al., 2024b) by harnessing a foundation model's intrin- sic notion of interestingness (Faldor et al., 2024; Hu et al., 2024; Lu et al., 2024b; Zhang et al., 2024b) to construct the next proposal, analogous to human innovation. No- table examples include ELM (Lehman et al., 2022) which evolves novel robot morphologies; OMNI-EPIC (Faldor et al., 2024), which automatically designs novel environ- ments for reinforcement learning (RL) agents; DiscoPOP which discovers new loss functions for preference optimiza- tion algorithms (Lu et al., 2024a); ADAS (Hu et al., 2024), which evolves novel designs for LLM-based agentic sys- tems; and The AI Scientist (Lu et al., 2024c), which seeks to automate the entire scientific process by proposing novel ideas, conducting experiments, and writing a scientific paper summarizing the results.\nAutomated Evaluation of Foundation Models. Recent research also investigates automated evaluation of FMs, moving beyond static, human-designed test suites. Rainbow Teaming (Samvelyan et al., 2024) applies Quality-Diversity algorithms (Mouret & Clune, 2015; Pugh et al., 2016) to find novel adversarial attacks that stress-test FMs for safety. Similarly, works like (Jiang et al., 2024; Pavlova et al., 2024; Zheng et al., 2024; Zhou et al., 2024) automate the red team- ing (probing a system for weaknesses) process. These works expand the comprehensiveness of existing safety checks but do not have the ability to create new tasks that might reveal completely unknown FM capabilities. Other techniques generate new debate topics and evaluate FMs through multi- round debate between them (Zhao et al., 2024), discover open-ended programming challenges (Pourcel et al., 2024a), or devise visual recognition and reasoning tasks from a col- lection of visual assets (Zhang et al., 2024a). Meanwhile, (Shah et al., 2024b) produces challenging math problems from existing datasets and human-in-the-loop supervision. However, the generated tasks in these works tend to focus on a restricted domain, which fails to provide an overview of a model's abilities across a wide array of skills and limits the discovery of surprising capabilities of FMs."}, {"title": "4. Automated Capability Discovery", "content": "Given a foundation model we wish to evaluate (the subject), AUTOMATED CAPABILITY DISCOVERY (ACD) designates another foundation model as a scientist to propose new tasks and then evaluate how well the subject model performs. The scientist and subject could be the same model or different, but in either case, they are both foundation models, so we refer to this as \u201cfoundation model self-exploration.\" By iteratively refining tasks to uncover interesting or surprising outcomes, ACD aims to automate much of the process of revealing a model's capabilities. Below, we outline the key stages of ACD. (See Appendix B for the full ACD prompts.)\n4.1. Definition of Task Families\nWe adopt a simplified version of the METR Task Stan- dard (METR Task Standard Team, 2024), an established format for packaging tasks to evaluate foundation models. In particular, ACD instructs the scientist to define and gener-"}, {"title": "5. Empirical Evaluation", "content": "We now demonstrate ACD's performance in discovering diverse capabilities across several foundation models, in- cluding GPT-40 (OpenAI, 2024b), Claude Sonnet 3.5 (An- thropic, 2024), and Llama3-8B (Llama Team, 2024). First, we provide an in-depth examination of GPT-40 acting as both scientist and subject, followed by experiments with dif- ferent scientist-subject pairings and cross-model analyses. We run our algorithm for 5000 generations for all evalua- tions. Further details on hyperparameters and evaluation protocols appear in Appendices C and G.\n5.1. Case Study and Human Evaluation on GPT-40\nWe begin by analyzing ACD with GPT-40 serving as both scientist and subject. In Figure 2, we visualize all discov- ered tasks by embedding each task's description in a 2D t-SNE (Van der Maaten & Hinton, 2008) plot, grouped by HDBSCAN (McInnes et al., 2017). From these 5000 gen- erations, we discover 1330 interestingly new tasks, which fall into 25 distinct clusters (Table 4 in Appendix E.1). The tasks span wide-ranging areas, including puzzle-solving and creation (e.g., Sudoku, logic riddles, custom word puzzles), code generation and debugging, advanced math, creative writing, and legal text interpretation. We provide many ex- amples from our evaluations, spanning cryptography, code generation, memory-based logic, advanced mathematics, legal queries, puzzle design, and creative writing in Ap- pendix E.3.\nHere, we examine a few in detail. Figure 1 (right) high- lights three surprising tasks discovered by ACD that reveal GPT-40 sometimes fails at seemingly trivial operations. For instance, it incorrectly computes a sequence of three arith-"}, {"title": "6. Report Generation", "content": "Once tasks and evaluations have been collected, ACD can automatically compile a Capability Report summa- rizing each discovered capability, highlighting consistent successes, failures, and key insights about the subject model. This mirrors recent developments where foundation models have been used for extensive scientific writing (Lu et al., 2024c; Steinruecken et al., 2019; Wang et al., 2024).\nThe advantage is twofold: (1) The resulting report serves as a compact overview of discovered capabilities and failure"}, {"title": "7. Safety Considerations", "content": "Secure Execution and Containerization. All code gen- erated by our system for defining and evaluating tasks is executed within containerized environments. This approach prevents unauthorized network access, restricts access to the host machine's filesystem, and mitigates other poten- tially unsafe behaviors. Our methodology adheres to widely adopted community standards for secure code generation and execution (Chen et al., 2021b; Hu et al., 2024; Jimenez et al., 2024), ensuring that any inadvertent or harmful com- mands are effectively sandboxed. Furthermore, we explic- itly instruct ACD not to access the internet or the filesystem, and static analysis confirms that there are no such attempts (e.g., no 'os' system calls are present). These measures substantially reduce the likelihood of deploying dangerous code.\nSafety Advantages of AUTOMATED CAPABILITY DIS- COVERY. By design, ACD systematically uncovers both surprising successes and unanticipated failure modes in foundation models. Identifying unexpected or emer-"}, {"title": "8. Conclusion and Limitations", "content": "We have introduced AUTOMATED CAPABILITY DISCOV- ERY (ACD), a framework in which one foundation model, acting as a scientist, autonomously discovers and evaluates the capabilities of another subject model, thereby reducing the need for manual task design. Through systematic ex- ploration and automated evaluation, ACD reveals a wide range of surprising capabilities and unexpected failures in the foundation models it evaluates, such as the GPT and Llama models. Human evaluation of GPT-40 tasks confirms that most automatically generated tasks are coherent and that self-assessment aligns well with human judgments, al- beit with a slight positive bias. Future work could focus on improving the automated judge, for instance by using more sophisticated or even learned agentic systems (Hu et al., 2024). A further path for automation could be enhanc- ing the selection of examples in our Capability Reports to match the quality of the manually curated highlights (Ap- pendix E.2). With better filtering and scaling, we envision being able to entrust larger portions of the model evaluation process to ACD, greatly enhancing AI safety.\nAlthough our experiments focused on single-turn, text-based tasks, future extensions could target more complex agentic tasks or incorporate multimodal inputs and outputs (Zhang et al., 2024a). A particularly exciting target for ACD is the new (at the time of writing) class of powerful yet un- derexplored reasoning models (e.g., OpenAI's o1 (OpenAI, 2024a) or DeepSeek's r1 (DeepSeek-AI, 2025)). ACD could play a significant role in systematically discovering and characterizing a range of behaviors in these emerging models. Conversely, these improved models could act as much more effective scientists, enabling ACD to perform even more detailed analyses of existing systems. Finally, the tasks generated by ACD could also represent an inter- esting way to generate new challenges for models to solve themselves (Colas et al., 2023; Schaul, 2024), potentially facilitating model self-improvement via open-ended (Faldor et al., 2024; Stanley et al., 2019; Zhang et al., 2024b) and AI-generating algorithms (Clune, 2019)."}]}