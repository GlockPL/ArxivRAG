{"title": "Automatic Voice Identification after Speech Resynthesis using PPG", "authors": ["Thibault Gaudier", "Marie Tahon", "Anthony Larcher", "Yannick Est\u00e8ve"], "abstract": "Speech resynthesis is a generic task for which we want to synthesize audio with another audio as input, which finds applications for media monitors and journalists. Among different tasks addressed by speech resynthesis, voice conversion preserves the linguistic information while modifying the identity of the speaker, and speech edition preserves the identity of the speaker but some words are modified. In both cases, we need to disentangle speaker and phonetic contents in intermediate representations. Phonetic PosteriorGrams (PPG) are a frame-level probabilistic representation of phonemes, and are usually considered speaker-independent. This paper presents a PPG-based speech resynthesis system. A perceptive evaluation assesses that it produces correct audio quality. Then, we demonstrate that an automatic speaker verification model is not able to recover the source speaker after re-synthesis with PPG, even when the model is trained on synthetic data.", "sections": [{"title": "1. Introduction", "content": "Nowadays media monitors and journalists have to deal with huge content streams from all over the world in different languages. In this context, SELMA project\u00b9 aims to develope a voice-over framework which will generate a speech signal targeting the voice of a specific journalist/presenter from input translated text. One option is to use a Text-to-Speech (TTS) system to generate the speech signal corresponding to the translated text expressed by the target voice. However, despite many recent developements, the signal synthesized with such an approach does not necessarily correspond to what an editor wants. Therefore, there is a need for the creation of a speech resynthesis framework which enables an expert to directly modify an existing audio file. To do so, the expert needs to control different aspects of the speech signal generation through the use of an interpretable representation. For instance, EditTTS [1] uses text as a representation that permits control over the linguistic content, and [2] uses Phonetic PosteriorGrams (PPG) as a finer representation to edit rhythmic or phonetic contents. PPG, as a time-vs-phoneme representation representing posterior probabilities of phonetic classes at the frame level, is the representation we investigate in this paper.\nPPG has the advantage of disentangling phonetic and rhythmic information, and thus giving control to our expert on these two aspects independently [2, 3]. PPG has been used for the task of voice conversion [4] (modifying speaker). Such representation also embeds speaker accent, thus allowing to perform"}, {"title": "2. Related work", "content": "All the speech generation tasks are generally divided into two steps: one feature predictor generates a mel-spectrogram from the input, and then a vocoder turns the mel-spectrogram into audio. Among the existing neural vocoders, WaveNet [7] and WaveGlow [8] have been extensively used with Tacotron2, but the recent HifiGan [9] which provides faster training and inference, is now the most used one. Also, some end-to-end systems tend to appear, that embed both steps without relying on an internal signal-related representation of speech, such as Vits [10]\nSpeech generation covers a great variety of tasks (among"}, {"title": "2.1. Speech generation from text", "content": "The first task that comes in mind is Text-to-Speech (TTS), where a written sentence must be generated with a target voice. Some systems, such as Tacotron2 [11] or FastPitch [12] have been developped for this task in particular. The easiest case for this task is where only one target voice is used: the voice from the training set. Blizzard Challenge [13, 14] is a TTS challenge, where multiple teams are given one or multiple tasks and a database, and these teams compete to provide the \"best\" synthetic samples for the tasks. The evaluation is done using multiple listening tests in different conditions to order the participants. Each edition of the challenge uses a different database, which can differ on language (French, Spanish, Mandarin...) or in contents (children book).\nHandling multiple voices, or using a speaker representation as a voice target, are harder versions of this task, which is done by other systems. This TTS task is usually evaluated with MOS scores, using different questions to capture the opinion of listeners on some precise aspects of speech, such as naturalness or speaker similarity to the target voice. Some research is also done in the direction of MOS prediction from audio, for example in VoiceMOS Challenge [15]."}, {"title": "2.2. Speech generation from audio", "content": "Voice conversion is a speech resynthesis task, in which a target sentence uttered by a source speaker is given, along with a target speaker. The goal is then to generate the target sentence with the target voice, with minimal changes to the aspects which are not the speaker. Evaluation for this task can either be done by running a perceptive test, or by using automatic metrics. Similarly to the Blizzard Challenge for TTS, the Voice Conversion Challenge is a recurring challenge for Voice Conversion systems. This challenge has been run every two to three years, starting in 2016. It provided different tasks over the years to explore different aspects of speech, such as cross-lingual Voice Conversion in 2020 [16] or Singing Voice Conversion in 2023 [17].\nAccent conversion is closely related to Voice Conversion. This task consists of changing the accent in the sentence, for example from a non-native to a native accent, without changing the identity nor the words from the original sentence. In FAC-via-PPG [2], it is done by using PPG as input for a speech synthesis system, here Tacotron2, to modify the pronounciation of the sentence.\nSpeech anonymization is a variant of Voice Conversion. In this case, the goal is not to generate a specific voice, but instead to not be able to identify the source speaker [18], without interfering with linguistic or prosodic elements.\nSpeech edition is the last task that we will present. This task consists of, given an audio corresponding to a sentence and a change to do to the audio (the easiest case being replacing a word by another), generating the same audio with the modification taken into account. This is done by changing a part of the input text by EditSpeech [19], and by changing content or shifting pitch by EdiTTS [1].\nThese four tasks, where the criteria can differ from one task to another (speaker similarity, keeping/removing some linguistic elements...) are examples of Speech Resynthesis. Compared to TTS, the evaluation could require to compare two audios on some aspects of speech. This evaluation can be done through perceptive testing or by using automatic metrics in different feature spaces."}, {"title": "2.3. Speaker verification and evaluation", "content": "Depending on the targeted task, an evaluation of the similarity of the speakers from two samples can be necessary. As an example, Voice Conversion aims at maximizing the similarity of the synthetized speaker with a target speaker, but Speech Anonymization suppresses the original speaker identity while maintaining the linguistic content. Perceptive evaluation is often done with a Speaker Similarity MOS, where participants to the listening test are asked whether they think that the two presented speakers are the same or not. Automatic evaluation usually relies on Speaker Verification systems, such as ResNet [20] and Ecapa-TDNN [21]. These systems are trained to produce similar embeddings for audios coming from the same speaker, and different embeddings if the speakers are different, regardless of the linguisic contents of the audios. The input features can be acoustic features, such as MFCC or Mel-Spectrograms, or features extracted by a pretrained model, such as WavLM [22]."}, {"title": "2.4. Datasets", "content": "Datasets used for speech generation are usually audiobooks. The most common for mono-speaker high-resource English synthesis is LJSpeech [23]. This dataset contains 13100 audio segments of 1 to 10 seconds, for a total of 24 hours. When we want to learn a finite small number of voices, MAILABS [24] provides, in its English subset, a vast amount of audio for 4 speakers, with 40 to 70 hours per speaker. This gives the ability to train one-hot speaker-encoded synthesis systems. This dataset also contains high quantity of audio in 8 other languages, that would give the ability to train speech synthesis systems in these languages.\nLibriSpeech [25] is another dataset, frequently used for many different tasks, such as speech recognition or speaker verification. This dataset contains 960 hours of audio in total, of 2 500 different speakers.\nLibriTTS [26] is a subset of LibriSpeech used for Text-to-"}, {"title": "3. Phonetic PosteriorGrams and speech synthesis", "content": "Phonetic PosteriorGrams (see example Figure 3) are a frame-level representation of speech, which gives a probability of presence of phonemes at each timeframe. This representation can be interesting to give fine-grained control over represented speech to users (see Figure 1), since it disentangles different high-level features, such as pronunciation or rhythm. It also conveys more information than one-hot encoding of phones, since the confusion between two classes can be interpreted as different ways to realize a same phoneme. But some other information might be present in this representation, and this paper aims to look for speaker identity information in a PPG.\nOur PPG are extracted from the same model as in [2], which is a Kaldi generalized maxout network [27] trained to mimic a GMM-HMM model representing 5,816 sub-phone units, which are then grouped into 40 phone classes for English speech. 100 PPG frames are extracted per second."}, {"title": "3.1. Phonetic PosteriorGrams (PPG)", "content": "Phonetic PosteriorGrams (see example Figure 3) are a frame-level representation of speech, which gives a probability of presence of phonemes at each timeframe. This representation can be interesting to give fine-grained control over represented speech to users (see Figure 1), since it disentangles different high-level features, such as pronunciation or rhythm. It also conveys more information than one-hot encoding of phones, since the confusion between two classes can be interpreted as different ways to realize a same phoneme. But some other information might be present in this representation, and this paper aims to look for speaker identity information in a PPG.\nOur PPG are extracted from the same model as in [2], which is a Kaldi generalized maxout network [27] trained to mimic a GMM-HMM model representing 5,816 sub-phone units, which are then grouped into 40 phone classes for English speech. 100 PPG frames are extracted per second."}, {"title": "3.2. Speech synthesis from PPG (PPG2Mel)", "content": "Speech generation systems traditionnaly take in input a sequence of characters or phonemes. PPG are a representation of audio, pretty similar to one-hot encoding of phonemes. Thus, traditional systems can be easily adapted with PPG as an input. We follow a similar approach as described in [2] and [4], which involves training a Tacotron2 system [11] using PPG as input, rather than text.\nThe training should be easier compared to text input because PPG provide a frame-level representation and convey precise timing information. The only modification made to the Tacotron2 architecture are in the first layer of the encoder, where the Character Embedding layer is replaced with a linear layer that maps the 40 phone classes to a 512-sized hidden representation."}, {"title": "3.3. Mel-Spectrogram to Speech", "content": "Since the PPG2Mel system generates a Mel-spectrogram, we need to convert it back to the time domain. To achieve this, we employ WaveGlow, a neural vocoder described in [8]. Our vocoder was trained on the LJSpeech dataset with the default configuration, except for the sampling rate, which we set to 16kHz instead of 22.05kHz to match the sampling rate of other audio versions. We used the implementation provided by Nvidia, available on their GitHub repository\u00b2."}, {"title": "3.4. Perceptive evaluation of synthetic speech", "content": "Speech reynthesis would serve no purpose if it results in a degradation of the synthesized speech quality. Therefore, our initial objective is to assess the quality of the speech generated by the system detailed in Section 3.2. In order to ensure that our speech quality remains on par with other speech synthesis methods, we compare our PPG2Mel model with samples generated by a Tacotron2 system that was trained on textual input.\nThis test does not aim to compare our speech resynthesis system to other on any aspect (naturalness, voice similarity...), since the system we use is not the main contribution of this article, but is an existing system from the literature. The goal of this test is only to make sure the provided audio is of sufficient quality to study the eventual presence of a speaker.\nDifferent audio versions are shown to the listeners, and are summarized in Figure 2:\n\u2022 Natural audio: Original audio from LJSpeech dataset, resampled to 16kHz to match the other audios. This will give us the opinion of listeners about natural audio, which is the upper bound for our systems.\n\u2022 Vocoder audio: We extracted mel-spectrograms from original audios, and fed it to the vocoder we use. This gives us the degradation induced by the vocoder, that our synthesis systems will not be able to avoid."}, {"title": "3.5. Results of perceptive evaluation", "content": "Results are reported in Table 1. We exclude the introduction steps and take into account all the other answers, including those coming from participants who did not complete all the steps. From these results, we can conclude that using PPG as an input for speech synthesis does not degrade audio quality compared to TTS. We also see that a large part of the degradation in audio quality comes from the vocoder, which means that a better training setup or the use of another vocoder could benefit to speech quality of both TTS and PPG2Mel systems.\nWe are aware tht our results are below similar MOS reported in the litterature. However, we notice that even the natural audio is not evaluated with good score. This states that our participants were particularly strict during the evaluation process compared to state of the art MOS evaluations."}, {"title": "4. Speaker verification experiment", "content": "Now that we confirmed the correct audio quality of the resynthesis, we want to perform source speaker verification after PPG-based speech re-synthesis. Since our goal is not voice conversion, the synthetic samples are not required to sound like the target speaker.\nIn this section, our objective is to determine whether a Naive Automatic Speaker Verification (ASV) system, trained on natural speech, can successfully identify the source speaker after resynthesis (Q1). Subsequently, we employ synthetic data to train an Informed ASV system, designed to recognize the source speaker after resynthesis. We investigate the extent to which this Informed ASV system can recognize the source speaker in samples synthesized with a target voice but also from natural speech samples in order to evaluate the mismatch between natural and synthetic speech (Q2). The Informed ASV is supposed to learn how to discriminate speakers in a feature space adapted to the target voice. In case the synthesis process completely hides the source speaker, we expect strong degradations with both Naive and Informed models. However, in case the synthesis process only partially hides the source speaker, we expect strong degradatation for the Naive system, and lower degradation for the Informed one. Finally, we investigate how much both Naive and Informed systems are able to link the target speaker identity from natural samples and samples synthesized with its target voice (Q3)."}, {"title": "4.1. Data and notations", "content": "This experiment is realized upon 3 different datasets. The first one is the English section of M-AILABS 4, a read speech corpus based on LibriVox. We used two speakers, E. Klett, denoted Speaker A (female), and E. Miller, denoted Speaker B (male), as described in Table 2, row 1. Each of these speakers provided 30 to 45 hours of speech data, which we divided into training, validation, and test sets. Throughout this experiment, those two speakers consistently served as the target speakers. This means that all synthetic samples used in this experiment were converted to one of these two voices. We trained two mono-speaker PPG2Mel A and PPG2Mel B models, one for each speaker A and B (Fig. 1).\nLibriSpeech-test-clean [25] subset is our speaker verification enrollment and test sets. It contains 40 gender-balanced speakers (~ 8 min speech), denoted Speaker 1 to 40, as shown in Table 2, row 2. These speakers are the source speakers we want to recognize with ASV systems before/after resynthesis.\nWe create two synthetic versions of this subset, using PPG2Mel A and PPG2MelB, shown in row 3 of Table 2.\nFinally, VoxCeleb1&2 [5, 6] datasets are used to train ASV models (row 4 of Table 2).\nThe two mono-speaker PPG2Mel A and PPG2Mel B models are used to convert all samples from VoxCeleb towards a target speaker, randomly chosen among A and B (row 5 of Table 2. The speaker labels for training remain the same as in the original dataset, even if they now have a different voice.\nNatural samples are denoted \\(N_{source}\\), where source is within {A, B, 1\u201340}. Synthetic samples are denoted \\(R_{source}^{target}\\), where source is as previously mentionned and target is either A or B depending on the PPG2Mel model used. We will not mention the speakers of VoxCeleb datasets."}, {"title": "4.2. ASV models", "content": "Both Naive and Informed ASV models use an ECAPA-TDNN [21] architecture fed with input features obtained by processing the speech samples with a WavLM-large pretrained model and trained using an AAM loss. 256-dimension x-vectors are extracted.\nThe Naive ASV model is trained on the original (natural) VoxCeleb1&2 development data.\nThis model achieved 1.57% EER on VoxCeleb-o after 4 days of training on one RTX8000 GPU. The Informed ASV model is trained on the synthetic version (target speaker A or B) of VoxCeleb1&2 data described in the previous section. The best version of this system is obtained after 1 day on the same architecture and only obtains 20% of EER on the synthetic version of the VoxCeleb-o task."}, {"title": "4.3. Experiments and results", "content": "Table 3 summarizes the different set up and results obtained for the 5 speaker verification experiments. Each experiment is described with corresponding enrollement/test pairs. Experiments come with Equal Error Rates (EER) calculated with both Naive and Informed ASV models. For all tests in experiments (1), (2) and (3), the speaker reference labels are the one from speakers 1-40 while in experiments (4) and (5) the labels are A or B. For example, in exp (3), enrollement is done with natural samples from speakers 1-40, while test is done with synthetic samples from speakers 1-40 converted with PPG2Mela. Target pairs are 1-1, 2-2,... 40-40, while impostor pairs are 1-2, 1-3, ... 39-40.\nThe first experiment (1) ensures that our naive model achieves correct results. To do so, we want to recover the speaker from natural speech. As we could expect, the naive model gives good results (EER=1.98%) since it is the task it has been trained on, and the informed model introduces a strong degradation (EER=29.44%), showing that there is a mismatch between the training data used for this model and the test data.\nExperiment (2) tests naive and informed models on synthetic speech. We compare samples from the speakers 1 40, which were all synthetized using PPG2Mela (\\(R_{1-40}^A\\)), and we want to see if our models are able to recognize those which come from the same source speaker. The results show that both naive and informed ASV mocels are unable to discriminate source speaker in the synthetic speech space (EER > 49%). This answers to the question Q1: the naive model is not able to recognize the source speaker after re-synthesis (EER=49.46%)\nOne hypothesis is that source speaker identities have been hidden during re-synthesis. We can see that the informed model better recognizes speakers in the natural space (EER=29.44%, exp. (1)) than in the synthetic space (EER=49.80%, exp. (2)). During training, the informed model hardly converged, but it seems that the few it learnt enables to discriminate speakers in the natural space only (as the task is easier). The answer to the question Q2 is: even an informed model is not able to recognize the source speaker after re-synthesis.\nExperiment (3) assesses the ability of both models to make the link between the speakers in the natural space and the same speakers in the synthetic space. To do so, we use natural LibriSpeech dataset as our enrollment dataset and the synthetic version of the same dataset, using PPG2Mela, as our test data. We see that both naive and informed models are unable to recognize the 40 source speakers after re-synthesis. We conclude that our PPG approach is indeed able to hide source speaker identity, even to an informed ASV. Any source speaker acoustic clues which could help model to retrieve their identity is not detected after re-synthesis.\nExperiment (4) and (5) measure if source speakers 1 - 40 converted in A synthetic space are closer to their natural version (resp. synthetic version converted in B) than to the natural speech of A. From experiment (4), we conclude that the speaker identity of the samples from source speaker 1 40, re-synthesized with target speaker A, does not correspond to the identity of A, thus confirming the fact that we are not doing voice conversion. However, the results show that re-synthesized"}, {"title": "5. Conclusion", "content": "Our first experiment aims to ensure that the PPG2Mel model we trained produced audio of a correct quality. The perceptual study that we performed confirmed that we were achieving the same quality as our TTS baseline, and that the vocoder was producing most of the quality loss.\nWe trained a naive ASV system on natural speech and an informed ASV system on synthetic speech to try to recover source speaker information hidden by speech synthesis. Our experiments show that even if the naive model achieves state of the art results on natural speech, neither naive nor informed ASV systems are able to retrieve source speaker information which would come from the PPG. This implies that source speaker acoustic clues are not detected by the models in re-synthetized speech. Also, we showed that both ASV models are not able to link target speakers from natural and synthetic samples.\nThese speaker identification results show that the amount of speaker information that goes from the PPG to the synthesized sample is small enough to permit the use of PPG in tasks such as Voice Conversion or Speech Edition.\nFuture work would include using a better vocoder, and running a speaker similarity perceptive evaluation of our systems, to compare our approach to Voice Conversion models and to the results obtained through automatic speaker verification.\nSince PPG do not convey acoustic speaker clues, we advocate for their use in speech edition, as a speech controllable representation without biasing the ouput towards source speaker."}, {"title": "6. Acknowledgements", "content": "This work was performed using HPC resources from GENCI-IDRIS (Grants 2022-AD011012565 and AD011012527). This project has also received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sk\u0142odowska-Curie grant agreement No 101007666. This paper was partially funded by the European Commission through the SELMA project under grant number 957017."}]}