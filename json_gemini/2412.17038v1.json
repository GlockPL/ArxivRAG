{"title": "ErasableMask: A Robust and Erasable Privacy Protection Scheme against Black-box Face Recognition Models", "authors": ["Sipeng Shen", "Yunming Zhang", "Dengpan Ye", "Xiuwen Shi", "Long Tang", "Haoran Duan", "Ziyi Liu"], "abstract": "Abstract-While face recognition (FR) models have brought remarkable convenience in face verification and identification, they also pose substantial privacy risks to the public. Existing facial privacy protection schemes usually adopt adversarial examples to disrupt face verification of FR models. However, these schemes often suffer from weak transferability against black-box FR models and permanently damage the identifiable information that cannot fulfill the requirements of authorized operations such as forensics and authentication. To address these limitations, we propose ErasableMask, a robust and erasable privacy protection scheme against black-box FR models. Specifically, via rethinking the inherent relationship between surrogate FR models, ErasableMask introduces a novel meta-auxiliary attack, which boosts black-box transferability by learning more general features in a stable and balancing optimization strategy. It also offers a perturbation erasion mechanism that supports the erasion of semantic perturbations in protected face without degrading image quality. To further improve performance, ErasableMask employs a curriculum learning strategy to mitigate optimization conflicts between adversarial attack and perturbation erasion. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the state-of-the-art performance in transferability, achieving over 72% confidence on average in commercial FR systems. Moreover, ErasableMask also exhibits outstanding perturbation erasion performance, achieving over 90% erasion success rate.\nIndex Terms-Facial privacy, Adversarial example, Face recognition", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the advancement of deep learning, face recognition (FR) technology has become deeply integrated into daily life [1], [2]. Users frequently share personal facial images on social media platforms such as Twitter, Facebook, and LinkedIn, which provides great convenience for trusted authorities (TAs) in performing face verification. However, this convenience comes with significant privacy risks. Attackers can match these images with existing biometric databases to accurately identify individuals, facilitating serious criminal activities [3]. As a result, there is an urgent need for a robust scheme that can address the misuse and threats posed by FR technology while effectively preserving the usability of the information for TAs.\nExisting adversarial examples [4]-[9] against FR models can serve as a feasible solution. However, they suffer from the following challenges: 1). Existing schemes exhibit weak transferability towards strictly black-box FR models.\nSome transferable adversarial example schemes have been proposed in recent year [10]-[14], which simply employ ensemble attack for all surrogate models. Unfortunately, this often leads to overfitting of the surrogate white-box models [15], resulting in weak transferability. In ensemble attacks, models often prioritize fitting the more easily attackable surrogate models before gradually adapting to the relatively harder-to-attack ones. This observation indicates an imbalance in the optimization process. Consequently, even if the ensemble attack loss converges, the model tends to overfit to the surrogate models and fails to learn robust and general features. This raises the question: Can the model learn more robust and general features if it can learn in a stable and balancing attack strategy? 2). The adversarial performance permanently disrupts face verification of trusted authority for forensics and authentication. The most obvious obstacle encountered by existing schemes is their permanently damage to the identifiable information present in the source faces. This severely hinders the ability of TAs to accurately perform face verification. Reversible Adversarial Examples (RAEs) can serve as one of potential strategies for TA's demands [16]\u2013[19]. Most of them adopt a reversible data hiding strategy to fulfil recovery of original images [16], [17], [20]. However, RAEs exhibit several limitations. They fail to address the robustness of recovery that cannot tackle with various image process methods in real-world scenarios and overlook facial identity protection, leaving a gap in facial privacy protection scenarios. Additionally, existing RAEs typically combine pixel-level adversarial perturbations with separate data hiding techniques. This raises the question: Would it be possible to design semantic perturbations in facial privacy protection scenarios that are inherently self-erasable?\nTo resolve the questions mentioned above, we present a novel facial privacy protection scheme called ErasableMask, which utilizes facial attributes as conditions to generate transferable and erasable adversarial examples with strong robustness to tackle with various image process methods. Specifically, ErasableMask solves the above question through the following strategies:\nTo tackle with question 1. Existing studies have demonstrated that auxiliary learning can serve as a promising balancing strategy, which enhances generalization of the primary task by learning additional relevant features obtained in the sharing of features with auxiliary tasks [21], [22]. This motivates us to think about whether the model can learn a more transferable attack through this stable and balancing strategy, where attacking one specific surrogate FR model serves as a primary task and fine-grained fine-tuned the model with feedback from other surrogate FR models as auxiliary tasks. But there exists a question of how to transfer the strategy learned from the primary task to the auxiliary tasks, while also allowing the auxiliary tasks to provide beneficial feedback. Recently, meta-learning has been utilized to improve the optimization compatibility between different tasks [23]\u2013[25]. As a result, we introduce a novel meta-auxiliary attack to learn more robust and general features that boost transferability. Specifically, we split surrogate FR models into a primary task and several auxiliary tasks before each attack, then use the performance of auxiliary tasks in meta-test to further fine-grained fine-tuned optimization direction of the primary task in meta-train, which is a form of meta-learning with double gradient.\nTo tackle with question 2. One direct method to enable self-erasable capability is deploying a restorer and allowing it to be tightly coupled with the generator and trained end-to-end with it. However, there exists a problem that the clean-domain information in protected faces is severely damaged, resulting in a difficulty for restorer to obtain enough information that facilitate perturbation erasion. This inspires us to consider whether we can retain enough clean-domain information that can guide perturbation erasion. As a result, we employ a clean-domain information injection strategy which embeds additional source face information via a copy of generator to improve perturbation erasion performance. Moreover, the adversarial generation and erasion of semantic perturbations are a set of conflicting optimization objectives. And to tackle with real-world transmission standards, ErasableMask also needs to hold robustness both in adversarial attack and perturbation erasion, which can be fulfilled by a differential noise pool [26]. Directly deploying this noise pool will undoubtedly increase the difficulty of training for this conflicting task. This prompts us to think about making the ErasableMask generator and restorer acquire adversarial, erasable, and robust capabilities step by step to mitigate conflicts. Thus, we adopt a three-stage curriculum learning [27] strategy. Specifically, we allow ErasableMask to solely focus on attribute modification strategy in stage 1 and deploy the noise pool in the last two phases (introducing robust adversarial example in stage 2 and robust perturbation erasion in stage 3).\nTo the best of our knowledge, ErasableMask is the first self-erasable facial privacy protection scheme. As shown in Fig. 1, ErasableMask strikes a balance between facial privacy protection and information usability. Compared with existing works in Table. I, ErasableMask supports self-erasable ability and strong robustness both in attack and recovery. Overall, ErasableMask makes the following contributions:\n1) We design a novel erasable adversarial example scheme that retains clean-domain information during the generation and can leverage the information embedded to erase semantic perturbations in protected faces.\n2) ErasableMask introduces a novel meta-auxiliary attack, which employs a balancing and stable attack strategy to learn more general features that boost transferability against black-box FR models.\n3) ErasableMask employs a curriculum learning strategy to mitigate optimization conflicts between adversarial and perturbation erasion performance, thereby enabling the acquisition of transferability and perturbation erasion capability step by step.\n4) Through extensive experiments on CelebA-HQ and FFHQ, ErasableMask demonstrates state-of-the-art performance against black-box models both in transferability (over 72% confidence on average in commercial FR systems), perturbation erasion (nearly 90% erasion success rate in black-box scenarios), and robustness (little decrease in performance against both black-box and white-box image process methods)."}, {"title": "II. RELATED WORK", "content": "Many studies have shown that deep neural networks (DNNs) are highly vulnerable to adversarial examples. Depending on the adversary's knowledge of the target model, these schemes can be categorized into black-box and white-box attacks [28], [29]. White-box attacks and query-based black-box attacks [6], [32], which rely on knowledge of the target models, are limited in practical scenarios. Therefore, this paper mainly considers transfer-based black-box attacks [33]\u2013[36], which aims at improving transferability without the knowledge of the target model. Although adversarial examples demonstrate extraordinary potential in attacking classification model, the unerasable adversarial performance inevitably prevent their applications in privacy protection scenarios.\nRecently, a special type of adversarial example, Reversible adversarial examples (RAEs) have been of increasing interest. RAEs can mislead classification model and conduct recovery of origin images, which can server a promising solution for privacy protection. Most of them adopt additive perturbations along with reversible data hiding method [16], [17], [19], [20], [37]. They either consider black-box attacks against classification models [16]\u2013[18], or a gender protection scenario [19]. However, there is a gap both in facial privacy protection scenarios, where RAEs cannot support robust recovery that resist image process methods, and the self-erasable ability of semantic perturbations."}, {"title": "B. Adversarial Example against Face Recognition", "content": "Existing attacks against FR systems primarily fall into two categories: patch-based [5], [7] and perturbation-based [4], [6], [28], [29], [33], [38] attacks. Patch-based attacks, such as Adv-Hat [5], Adv-Makeup [15], SOP [39], and SMAP [40], add noticeable patches to specific regions of facial images. However, this method significantly degrades image usability. In the other category, perturbation-based adversarial attacks can introduce more imperceptible noise on facial images. Moreover, most schemes against FR models tend to employ ensemble attacks simply without an in-depth insight in how to improve transferability. Recently, GMAA [31] improves black-box transferability by introducing a set of the target face with different poses, thus transforming the discrete target domain into a manifold one, which is more likely a data augmentation trick. Sibling-Attack [30] uses attacking AR models as an auxiliary task, generating additive adversarial perturbations through multitask optimization. The strategy of Sibling-Attack is enlarging search space, and this additive perturbation will inevitably introduce noticeable noise on facial images. However, existing schemes lack considerations on how to improve transferability with an in-depth insight into the inherent relationship between FR models. Moreover, all of them cannot support recovery of source facial images, which hinders the authentication and management of TAs."}, {"title": "III. PROBLEM STATEMENT", "content": "In general, there are two kinds of adversarial examples against FR models, including untargeted (dodging) and targeted (impersonation) attacks. Numerous facial privacy protection schemes have been proposed [11], [12], [41], [42] aiming at impersonation attacks. As a result, in this section, we formulate a facial privacy protection scenario based on impersonation attacks and provide a comprehensive description of the abilities and goal of users, trusted authority, and unauthorized hackers."}, {"title": "A. Users' Ability and Goal", "content": "Users possess several facial images $x_{cov} \\in \\mathbb{R}^{h \\times w}$, where $\\mathbb{R}$ denotes the real number field, and $h, w$ denotes the height and width of the $x_{cov}$ respectively. Users have no knowledge about the parameters or architecture of malicious FR models but can leverage a locally held facial privacy protection model to process $x_{cov}$, resulting in a protected image $x_{adv}$. In general, the protected face generation process can be expressed as the following formula:\n$\\min_{x_{adv}} L_{adv} = D(FR(x_{adv}), FR(x_{target}))$,\n$S.t.||x_{adv}, x_{cov}||_p \\leq \\delta,$\nwhere $D(\u00b7)$ denotes a distance function. $FR(\u00b7)$ denotes a DNN-based FR model. $x_{adv}$ denotes protected faces, $x_{cov}$ denotes source face and $x_{target}$ denotes target face. $||x_{adv}, x_{cov}||_p \\leq \\delta$ is utilized to quantify the visual similarity between $x_{adv}$ and $x_{cov}$, where $|| ||_p$ means $L_p$ norm."}, {"title": "B. Trusted Authorities' Ability and Goal", "content": "Trusted Authorities (TAs) represent platform managers, and need to conduct face verification in forensics scenarios. TAs can obtain the clean example $x_{rec}$ through the restorer and conduct face verification through the FR model they hold. TAs are totally honest entities and would not deliver restorer and $x_{rec}$ to UHs or other Users. Therefore, we redefine facial privacy protection scenario in erasion perspective. We present a formula for recovery example $x_{rec}$.\n$\\max_{x_{rec}} L_{erasion} = D(FR(x_{rec}), FR(x_{target}))$,\n$S.t.||x_{cov}, x_{rec}||_p < \\delta$.\nIn the process of erasion, we expect the erasion operation to eliminate the adversarial strength towards $x_{target}$ to the greatest extent. We quantify the visual similarity between $x_{rec}$ and $x_{cov}$ through the function $||x_{rec}, x_{cov}||_p$."}, {"title": "C. Unauthorized Hackers' Ability and Goal", "content": "Unauthorized Hackers (UHs) can obtain facial images from online platforms and conduct malicious face verification. Based on their ability to defend against adversarial examples, we define the following two types:\nProfessional Unauthorized Hackers (PUHs): In this case, PUHs are experts in adversarial examples and face recognition, possessing the ability to fine-tune FR models and resist adversarial threats.\nRegular Unauthorized Hackers (RUHs): In this case, They can only acquire FR models trained on large-scale data from the internet, or use commercial FR models."}, {"title": "IV. METHODOLOGY", "content": "As shown in Fig. 2, we built the ErasableMask framework using a conditional GAN architecture [43]. ErasableMask consists of a generator $G$, which is composed of an encoder $G_{enc}$, a perturbation encoder $E_{adv}$, and a decoder $G_{dec}$. An restorer $R$, and a discriminator $D$, which has two components including an attribute classifier $D_{att}$ and a discriminator $D_{G}$."}, {"title": "A. Visual Identity-preserved Facial Attribute Manipulation", "content": "As input, the generator $G$ first receives a clean domain facial image $x_{atta}$ with n-bit attribute label $atta \\in \\{0,1\\}^n$ and a n-bit attribute label $att_b \\in \\{0,1\\}^n$ to generate a protected example $x_{atta}^{adv}$. The discriminator $D_{G}$ learns to distinguish the clean domain image $x_{atta}$ from the protected face $x_{atta}^{adv}$. Correspondingly, the generator $G$ learns to generate protected faces that match the real image distribution to deceive $D_{G}$.\n$L_{D} = -log D_{G}(x_{atta}) - log (1 - D_{G}(x_{atta}^{adv}))$,\n$L_{G} = -log (D_{G}(x_{atta}^{adv})).\nTo ensure that $x_{atta}^{adv}$ retains the correct attributes in face, we employ the attribute constraint loss to effectively disentangle facial attribute information in high-dimensional feature space. By constraining $x_{atta}^{adv}$ with the $L_{att}$ in attribute classifier $D_{att}$, we reduce the semantic difference between $x_{atta}^{adv}$ and the target attributes $att_b$. The loss function can be formulated as:\n$L_{att} = \\sum_{i=1}^{n} -atta_i log D_{att}^{i}(x_{atta}) -\n(1 - atta_i)log(1 - D_{att}^{i}(x_{atta})),\n\\sum_{i=1}^{n} -attb_i log D_{att}^{i}(x_{atta}^{adv}) -\n(1 - attb_i)log(1 - D_{att}^{i}(x_{atta}^{adv})).\nThe encoder $G_{enc}$ must ensure that the generated image retains as much of the facial content information from the source image $x_{atta}$ as possible, as the latent variable $z$ needs to enable the decoder $G_{dec}$ to reconstruct attribute-independent details for any attribute conditions. To achieve this, the reconstruction loss $L_{rec}$ is introduced.\n$L_{rec} = || G_{dec}(G_{enc}(x_{atta}), atta) - x_{atta} ||_1,$\nwhere $|| ||_1$ represents the $L_1$ loss."}, {"title": "Algorithm 1 Meta-auxiliary Attack", "content": "Input: $x_{atta}$ (source image); $G(\u00b7)$ (adversarial example generator); $FR_i \\in \\{FR_1, FR_2... FR_K\\}$ (surrogate FR models);\nOutput: best model parameters;\n1: Initialization: $L_{FR_{i}}\\{0\\} = L_{FR_{i}}\\{1\\} = 1$;\n2: for i \u2208 [0, Epoch] do\n3:  for $x_{atta} \\in Dataset$ do\n4:   $x_{atta}^{adv} \\leftarrow G(x_{atta}, att_b)$;\n5:   for p\u2208 [1,\u2026\u2026,K] do\n6:   Calculate $L_{FR}^{pri}$ with Eq. 9;\n7:   $\\theta'_E = \\theta_E - lr \u00b7 \\nabla_{\\theta_E} L_{TRA}(\\theta_E)$;\n8:   for q\u2208 {1,..., K}\u2229q \u2260 p do\n9:   $x_{atta}^{'adv} \\leftarrow G(x_{atta}^{'adv}, 0, attr_b)$;\n10:   end for\n11:  Calculate $L_{FR}^{pri}$ with Eq. 10;\n12:   end for\n13:  Calculate $L_{adv}$ and update $E_{adv}$ with Eq. 12;\n14:  end for\n15:  Calculate $w_i(t)$ with Eq. 11;\n16: end for\n17: return best model parameters;"}, {"title": "B. Meta-auxiliary Attack for Semantic Perturbations", "content": "Unlike pixel-level perturbations, ErasableMask achieves a more natural and realistic result for $x_{atta}^{adv}$ by introducing semantic perturbations, as illustrated in Fig. 3. Specifically, $G_{enc}$ and $G_{dec}$ are well-trained under the constraints in section IV-A to generate naturally realistic attribute-modified images. On this basis, ErasableMask introduces a perturbation encoder $E_{adv}$, which is initially set with parameters identical to $G_{enc}$. During training, $E_{adv}$ applies semantic perturbations by fusing its each layer's outputs with each layer of $G_{enc}$'s output. Denoting the i-th layer of $G_{enc}$ as $Gene$ and the i-th layer of $E_{adv}$ as $E_{adv_i}$, where i \u2208 {1,\u2026\u2026,n} for an encoder with n layers, the operations to the i-th layer of $G_{enc}$ can be represented as:\n$f_{ti} = G_{enc}(f_{ti-1})$,\n$perb_i = E_{adv}(perb_{i\u22121})$,\n$f_{si} = \\beta \u00b7 f_{ti} + (1 \u2212 \\beta) \u00b7 perb_i$.\nSpecifically, both $G_{enc}$ and $E_{adv}$ take the clean example $x_{atta}$ as their input. The decoding process of $G_{dec}$ is:\n$z_{adv} = [f_{s1}, f_{s2},\u2026\u2026,f_{sn}]$,\n$x_{atta}^{adv} = G_{dec}(z_{adv}, att_b)$.\nTo avoid $x_{atta}^{adv}$ differing from $x_{atta}$ largely, a perturbation loss $L_{perb}$ is introduced.\n$L_{perb} = max (\u03c3_1, ||x_{atta}^{adv} \u2212 x_{atta}\u2032||_2)$,\nwhere \u03c3\u2081 \u2208 (0,+\u221e), and $|| ||_2$ represents the $L_2$ norm. In ErasableMask, \u03c3\u2081 is set to 30.0. Particularly, $x_{atta}\u2032$ can be generated from $x_{atta}\u2032 = G_{dec}(G_{enc}(x_{atta}), att_b)$.\nTo enable $x_{atta}^{adv}$ to demonstrate adversarial performance, we need to employ a transferable attack strategy. However, the existing ensemble attack fails to learn general and robust features due to its unbalanced and unstable learning process. (weak transferability in our experiments). Motivated by this, we introduce a more stable and balancing attack strategy called meta-auxiliary attack, to fine-grained fine-tuned the optimization direction of $E_{adv}$ via additional beneficial features from other surrogate models during each attack. The algorithm is demonstrated in Alg. 1. Specifically, before attacking a specific surrogate FR model, we split K white-box surrogate models $\\{FR_i i \\in \\{1,\u2026\u2026,K\\}\\}$ into 1 primary task and k \u2212 1 auxiliary tasks (All surrogate FR models take turns as primary task). For the convenience of description, we take surrogate model $FR_A$ as an example, the rest surrogate models $\\{FR_i i \\in \\{1,\u2026, K\\} \u2229 i \u2260 A\\}$ denote auxiliary tasks. Thereafter, we conduct meta-train in primary task:\n$L^{pri}_{FRA} = 1 - Cos [FRA(x_{target}), FRA(x_{atta}^{adv})]$.\nThen we conduct meta-test in auxiliary tasks to perform fine-grained fine-tuning in optimization direction.\n$x_{atta}^{'adv} = G_{dec} (G_{enc} (x_{atta}), E(x_{atta}), att_b)$,\n$\\theta_E' = \\theta_E - lr \u00b7 \\nabla_{\\theta_E} L_{TRA}(\\theta_E)$,\n$L_{FRA}^{LTRA} = \\frac{\\sum_{i=1}^{K}n_i\u2260A \\sum_{i=1}^{ni} A w_i(t)}{(K-1)} (1 - Cos[FRA(x_{target}), FRA(x_{atta}^{'adv})])$,\nwhere we also employ a self-adaptive parameter $w_i$ to balance the contributions of different surrogate models. Via this, it can prevent easily attackable surrogate models from dominating during training. We can obtain $w_i$ from the following.\n$rate(t) = \\frac{Mean (L_{FR_{i}} \\{t-1\\}) i\u2208 \\{1,2... K\\}}{Mean (L_{FR_{i}} \\{t - 2\\})}$,\n$w_i(t) = \\frac{exp(rate_i(t))}{\\sum_{k=1}^{K}exp(rate_k(t))}$,\nwhere $Mean (L_{FR_{i}}\\{t\\})$ is the mean of $L_{FR_{i}}$ in t-th epoch. Finally, the adversarial loss of ErasableMask is:\n$L_{adv} = max(\\frac{1}{K} \\sum_{i=1}^{K}(L_{FR_{i}}^{pri} + L_{FR_{i}}), \\epsilon)$,\nwhere $\\epsilon$ is used to adjust the perturbation intensity."}, {"title": "C. Perturbation Erasion based on Information Injection", "content": "The reconstruction of protected faces is an ill-posed problem, as a single protected face $x_{atta}^{adv}$ can potentially be reconstructed to numerous $x_{atta}$. Moreover, since the semantic perturbations in feature space permanently damage identifiable information in source faces, this makes it challenging for the restorer R to obtain enough information for perturbation erasion. To address this, ErasableMask injects clean-domain information during generation to maximize the amount of source information needed by R.\nSpecifically, a copy of $G_{dec}$ is used by introducing an additional branch as shown in Fig. 3, where $x_{atta}$ is used as input to obtain the latent variable $z_{clean} = G_{enc}(x_{atta})$. Subsequently, the tuples $(z_{clean}, att_b)$ and $(z_{adv}, att_b)$ are input into $G_{dec}$, where feature fusion is performed after each layer's output and is controlled by weight \u03b3. The restorer R is initialized with the same parameters and structures as G and is optimized utilizing the loss $L_{era}$:\n$x_{atta}^{rec} = R(x_{atta}^{adv})$,\n$L_{era} = \\sum_{n=1}^{N} ||x_{atta}^{rec} (n) \u2212 x_{cov} (n)||_2,$"}, {"title": "D. Robustness Enhancement and Curriculum Learning", "content": "In real-world facial privacy protection scenarios, online social platforms process uploaded images to comply with file storage and transmission standards. This processing significantly affects ErasableMask's adversarial and erasion performance. Therefore, after generating protected faces $x_{atta}^{adv}$, ErasableMask incorporates a noise pool to enhance its robustness. The noise pool includes JPEG compression (QF = 50), Gaussian noise (Var = 0.003), and Resizing transformations (1/4).\nIn the initial training stage, introducing all losses makes it challenging to learn an effective generation capability. To address this issue, we introduce a curriculum learning strategy that allows the model to gradually acquire generation and perturbation erasion capabilities over three stages.\nIn the first stage, we aim to develop a well-trained encoder $G_{enc}$ and decoder $G_{dec}$. The loss function at this stage is:\n$L_1 = \u03bb_{rec}L_{rec} + \u03bb_{att} L_{att} + \u03bb_{G}L_{G}.$\nIn the second stage, we aim to ensure that $L_{adv}$ does not impair the model's ability to produce natural images. Thus, in this phase, we fix the parameters of the encoder $G_{enc}$ and decoder $G_{dec}$, then introduce $E_{adv}$, which has the same structure and is initialized with the parameters of $G_{enc}$, to generate semantic perturbations. Additionally, we introduce R at this stage, initializing it with the parameters of the encoder $G_{enc}$ and decoder $G_{dec}$. This stage allows $E_{adv}$ and the R to be tightly coupled and can be trained end-to-end. The loss function of stage 2 is:\n$L_2 = \u03bb_{att} L_{att} + \u03bb_{rec} L_{rec} + \u03bb_{G} L_{G}\n+\u03bb_{adv} L_{adv} + \u03bb_{era} L_{era} + \u03bb_{perb}L_{perb}$.\nSince erasing semantic perturbations and obtaining a non-adversarial example $x_{atta}^{rec}$ from $x_{atta}^{adv}$ is a challenging task, the third stage focuses on independently training the R to develop its ability to map from the adversarial domain to the clean domain. The loss function of stage 3 is:\n$L_3 = L_{era}$."}, {"title": "V. EXPERIMENT", "content": "1) Implementation details and Dataset: To maintain all losses on the same scale, we set the parameters $\u03bb_{att}, \u03bb_{rec}, \u03bb_{G}, \u03bb_{adv}, \u03bb_{era}$, and $\u03bb_{perb}$ to 10, 150, 1, 200, 150, and 1 respectively. ErasableMask is trained with 200 epochs in the first stage, 100 epochs in the second stage, and 50 epochs in the third stage. The learning rate is set to 0.00002. We selected high-resolution datasets: CelebA-HQ [44], a high-quality facial image dataset containing approximately 30,000 images with a resolution of 512 \u00d7 512, and FFHQ [45], a high-quality face dataset with over 70,000 images. To simulate real-world applications, we first use MTCNN [46] to extract and preprocess the facial regions within images. Due to computational constraints, each image in the dataset is downscale to a resolution of 256 \u00d7 256. All experiments are conducted on RTX3090 GPU 24 GB\u00d71.\n2) Practical Evaluation Metrics for Facial Privacy Protection: We use Attack Success Rate (ASR) to evaluate the ability of facial privacy protection.\n$ASR = \\frac{\\sum_{n=1}^{N} 1 (cos[FR(x^{adv}), FR(X_{target})] > \u03c4_1)}{N}$\nWhere, 1 (.) denotes indicator function, ASR indicates that facial privacy protection is considered successful when the similarity between $X_{adv}$ and $X_{target}$ exceeds the threshold \u03c4\u2081. In this paper, T\u2081 was set to 0.01 times the False Acceptance Rate (FAR) of the FR model.\nTo evaluate ErasableMask's performance of erasion, we propose the Erasion Success Rate (ESR) as a metric.\n$ESR = \\frac{\\sum_{n=1}^{N} 1(cos[FR(x^{rec}), FR(X_{target})] < \u03c4_2)}{N}$\nESR indicates that ErasableMask has successfully reconstructed a clean example that does not disrupt face verification when the cosine similarity between $x_{rec}$ and $X_{target}$ is less than 0.1 times the False Acceptance Rate (FAR) of the FR model.\n3) Competitors: To verify the performance of ErasableMask's black-box transferability, we implemented several benchmark adversarial attack schemes, including FGSM [28], PGD [29], Sibling Attack [30], AdvFaces [4], Semanticadv [9], and GMAA [31]. FGSM and PGD are well-known schemes due to their strong adversarial capabilities, and we set the perturbation strength to 4/255, considering noticeable noise introduced by additive perturbations. AdvFaces, Sibling Attack, Semanticadv and GMAA are recent schemes that generate adversarial examples specifically targeting FR models. 4) Target models: Following [11], we conducted extensive experiments and selected six FR models as white-box models: IR152 [47], IRSE50 [48], Facenet [49], Mobileface [50], ArcFace [50] based on IResNet100 [51], and CosFace [52] based on IResNet100. Three of these models were used as white-box models for training, while the remaining three were used as black-box models for evaluation. Subsequently, Face++ [53], Tencent [54] and Aliyun [55] were chosen as black-box commercial FR models to serve as target models. To evaluate transferability against black-box robust FR models, we fine-tuned ArcFace and CosFace on the public dataset LFW using adversarial training [56], achieving 99.28% and 99.35% accuracy on LFW dataset, respectively."}, {"title": "B. Comparison on Offline FR models", "content": "We benchmark against prior work [11], [12], [31] in Table. II, concentrating on evaluating ErasableMask's transferability against black-box FR models. ErasableMask almost achieves optimal ASR among all competitors, surpassing all face-based schemes. Different from existing schemes, ErasableMask also shows strong transferability against FaceNet, with 30% higher than the second optimal. Compared with existing facial protection schemes, ErasableMask offers a restorer that can perform perturbation erasion and obtain a clean-domain example. The perturbation erasion performance is also demonstrated in Table. II. The ESR results against all black-box FR models are almost close to 90%."}, {"title": "C. Comparison on Commercial FR systems", "content": "Another scenario is that unauthorized hackers choose commercial FR systems, like Aliyun, Tencent, and Face++. To simulate this, we employ commercial FR systems for face verification. We randomly select 100 examples for each scheme, then upload them and target face to platforms. We calculate the mean confidence rate and the results are demonstrated in Table. III. The mean confidence of ErasableMask is 9.79%, 7.54%, and 21.34% higher than the second higher scheme against Aliyun, Face++ and Tencent, respectively."}, {"title": "D. Comparison on Robust FR Models", "content": "In this section, we evaluate all competitors and ErasableMask against fine-tuned robust FR models. The results are demonstrated in Table. IV. ErasableMask outperforms all competitors with 79.78% against ArcFace and 81.33% against CosFace, respectively. Traditional gradient-based schemes exhibit the poorest performance against fine-tuned FR models. However, Sibling Attack employing attacking AR model as an auxiliary task exhibits higher robust performance than other gradient-based schemes. Among all face-based competitors, AdvFaces shows the highest ASR, mainly because AdvFaces generates additive perturbations rather than semantic perturbations. That is to say, semantic perturbation results in a poorer robustness compared with additive perturbation. However, ErasableMask overcomes the drawbacks of semantic perturbation, with over 70% higher than others."}, {"title": "E. Robustness Analysis", "content": "We evaluate robustness via applying JPEG(QF = 50), Gaussian noise(Var = 0.003), Resizing transformation(1/2), Median Filter(Kernel = 5), Random Rotate(angle E [-30,30]), and Central Crop(224) to the protected faces in Table. V. ErasableMask demonstrates excellent protection and erasion capability in terms of ASR and ESR across. It outperforms almost all competitors in both white-box and black-box image processing methods, with rather high ASR. While it also shows excellent erasion capability in JPEG, Gaussian noise, Resizing transformation, Median Filter. Even though the erasion performance against Random Rotate and Central Crop shows less effectiveness, we think it is reasonable that TAs will not apply these methods to disrupt face verification."}, {"title": "F. Visualization and Image Quality", "content": "Fig. 4 demonstrates the protected faces generated by various schemes. Compared with gradient-based schemes, ours has no obvious pixel-level adversarial noise. Furthermore, we use L1 loss, Mean Square Error (MSE), Frechet Inception Distance (FID) [57] and Learned Perceptual Image Similarity(LPIPS) [58] as metrics to evaluate the quality of the protected faces. we compare our protected faces with attribute-modified images without adversarial perturbations for fair comparison. The comparison results are shown in Table. VI. the results indicate that the quality of ErasableMask is similar or less than other schemes with much higher transferability."}, {"title": "G. Ablation Study", "content": "1) Without Meta-auxiliary Attack: In this section, we removed the meta-auxiliary attack while keeping other conditions unchanged. During the experiment, we observed that the adversarial loss Ladu rapidly declined, and concurrently, the quality of protected faces significantly dropped. The model quickly identified the local optimum for the three white-box models, thereby demonstrating strong adversarial capabilities"}, {}]}