{"title": "Analysis and Visualization of Linguistic Structures in Large Language Models:\nNeural Representations of Verb-Particle Constructions in BERT", "authors": ["Hassane Kissane", "Achim Schilling", "Patrick Krauss"], "abstract": "This study investigates the internal representations of verb-particle\ncombinations within transformer-based large language models (LLMs),\nspecifically examining how these models capture lexical and syntactic\nnuances at different neural network layers. Employing the BERT\narchitecture, we analyse the representational efficacy of its layers for\nvarious verb-particle constructions such as \u201cagree on\u201d, \u201ccome back\u201d, and\n\"give up\". Our methodology includes a detailed dataset preparation from\nthe British National Corpus, followed by extensive model training and\noutput analysis through techniques like multi-dimensional scaling (MDS)\nand generalized discrimination value (GDV) calculations. Results show that\nBERT's middle layers most effectively capture syntactic structures, with\nsignificant variability in representational accuracy across different verb\ncategories. These findings challenge the conventional uniformity assumed\nin neural network processing of linguistic elements and suggest a complex\ninterplay between network architecture and linguistic representation. Our\nresearch contributes to a better understanding of how deep learning models\ncomprehend and process language, offering insights into the potential and\nlimitations of current neural approaches to linguistic analysis. This study\nnot only advances our knowledge in computational linguistics but also\nprompts further research into optimizing neural architectures for enhanced\nlinguistic precision.", "sections": [{"title": "1. Introduction:", "content": "Recent neural network models have proven successful in most natural language\nprocessing tasks. This success is due to the large amounts of data on which neural network\nmodels are trained. These models have provided answers to several questions regarding\nlanguage processing and have contributed to solving many issues, such as machine\ntranslation (MT) and automatic speech recognition (ASR). However, with the advent of\nneural network models, research questions have arisen regarding their abilities to deal\nwith language. Questions such as: How much linguistic competence do neural network\nmodels have? What kind of linguistic knowledge is acquired by a neural network model?\nThese questions aim to examine the linguistic abilities that neural networks possess,\nstarting from phonological interactions, and syntactic tagging, to semantic and abstract\nrepresentation.\nA major area of interest has focused on the ability of deep neural networks to learn pre-\ndefined linguistic concepts such as parts-of-speech tags and semantic tags (Dalvi et al.,\n2022). Some studies are attempting to explore in which part of the network specific\nlinguistic knowledge is encoded. For example, Wilcox et al. (2018) reported that\nRecurrent Neural Networks (RNN) language models can learn about empty syntactic\npositions. Jawahar et al. (2019) investigated the linguistic structure learned by BERT:\nPre-training of Deep Bidirectional Transformers (Devlin et al., 2019) and found that the\nlanguage model can capture the rich hierarchy of linguistic information in the lower"}, {"title": "1.1. Linguistic Knowledge and the Distributional Models of Language:", "content": "Linguistic knowledge or knowledge of language is a topic that has been approached\nmostly in early and late generative frameworks (Chomsky, 1968: p.ix; Chomsky, 1981)\nand, recently, in usage-based approaches such as construction grammar (Chaves, 2019;\nLeclercq, 2023). It is a term that refers to the ability of humans to \"know a language,\" and\nthis ability is different from the process of \"having memorized a list of messages\" (Rizzi,\n2016). With this understanding of linguistic knowledge from the perspective of\ngenerative theory, this concept would refer to the grammar that includes the general\nprinciples and processes that allow the native speaker to construct and evaluate sentences\nin their language and decide on sentence acceptability. Therefore, some questions have\nbeen asked to answer the nature of linguistic knowledge: What is linguistic knowledge?\nHow is it acquired? The second question focuses on how much of our language\nknowledge is decided by experience and how much is determined by a predefined mental\nprocess (Haegeman, 1994).\nLinguistic knowledge is an umbrella term that includes components such as linguistic\nlevels. When we talk about knowing a set of words of a certain language, that's \"lexical\nknowledge\". Knowing how those words are structured with each other and put together"}, {"title": "1.2. Linguistic Levels and Corresponding BERT layers:", "content": "The model's Initial Layers usually process the sequence form. Rogers et al. (2020) state\nthat the first BERT layer processes the combination of token, segment, and positional\nembeddings. Early layers to the fourth layer in the BERT-base, contain more information\nabout linear word order. However, as the processing moves up the layers, an emergent\nunderstanding of more hierarchical sentence structure appears.\nMiddle Layers are specified in syntactic information processing. Studies by Hewitt and\nManning (2019), Goldberg (2019), and others have found these layers (e.g., layers 6-9 in\nBERT-base and 14-19 in BERT-large) most effective in capturing syntactic structures,\nlike tree depth and subject-verb agreement. Jawahar et al. (2019) have approved the\nfindings in these studies, which have shown that phrasal representations learned by BERT\nin the middle encode a rich hierarchy of linguistic information.\nLate Layers: During fine-tuning for specific tasks, these layers undergo the most change.\nHowever, restoring lower layers to their pre-trained states does not significantly affect\nmodel performance, indicating a certain robustness."}, {"title": "1.3. Internal activations of the model:", "content": "Within natural language processing and neural models' interpretation, BERT\nrepresents one of the state-of-the-art transformer-based language models. The success of"}, {"title": "1.4. Construction Grammar as a Theoretical Framework for Neural Language\nModels Interpretability:", "content": "Recent developments in cognitive-functional linguistics and usage-based linguistics\nsuggest that language structure emerges from language use, with the essence of language\nbeing its symbolic dimension, with grammar being derivative (Tomasello 2003: 5).\nConstruction grammar is one of the usage-based language acquisition approaches (Perek,\n2023). From this point, it provides a corpus-based analysis of the process of language\nacquisition; which means there is no assumption that the children have an innate language\nfaculty to process and acquire language. instead, they acquire language because they are\nliving in a linguistic community. From this principle, grammar would be considered a\ndynamic system of form-meaning pairings, that is based on the domain-general\nacquisition by children, in which the linguistic categories emerge by processing large\namounts of linguistic data (Diessel, 2013).\nConstructional approaches to language description have their roots in Fillmore's early\nwork from 1968. From there, a variety of approaches emerged with the shared proposals\nthat (a) grammar is directly related to function (grammar has meaning) and does not\ndepend on transformations and derivation (structure), and (b) constructions are learned\npairings of form and meaning related to one another in a constructional network, in which\nthe relationships between and among constructions are captured via a \u201cdefault inheritance\nnetwork\u201d (Goldberg, 2013).\nFrom this principle, construction grammar shares the same principles with embedding-\nbased models that represent linguistic knowledge as vectors in high dimensional space\nwhich they are trained on large corpora to handle linguistic tasks. Therefore, the\nunderstanding of linguistic structures in those models emerges from language use, in"}, {"title": "1.5. Linguistic observations of the defined constructions:", "content": "How many lexical units are considered while dealing with verb-particle\ncombinations? The answer to this question is unclear. We might declare that any attested\ncombination of traits represents a new sense, or we could pick a few features and say that\nspecified combinations belong to unique senses. Given the feature space, both selections\nare somewhat arbitrarily made, and this suggests that the theory does not regard unique\nsenses or lexical items as first-class language structures. This might be a result of the\ncontextual modulation principle."}, {"title": "1.5.1. Agree on, Agree with, Agree to, and Agree that:", "content": "The verb \"agree\" serves as the main lexical unit, its meaning is emerged from its\nform, not modified by the following prepositions or conjunctions. Thus, it has a one\nlexeme that it belongs to when it is sorted in an English dictionary with only one meaning,\nand the following prepositions or conjunctions are related with the verb in a syntactic\nrelation depending on the phrase structure requirements. Since the verb \u201cagree\" is a single\nlexical unit, its meaning is not influenced by the preposition or conjunction that follows\nit. Moreover, it might have a single representation in NLP systems when it is investigated\nin as a specific token in an NLP model."}, {"title": "1.5.2. Come in, Come out and Come back:", "content": "The verb \"come\" is combined with different adverbial particles (in, out, back) to\nform phrasal verbs with distinct meanings. Thus, the full combination is considered a\ndependent form-meaning construction, which should be sorted as a single lexeme in the\nEnglish dictionary based on its meaning. Each combination of \"come\" with a different\nadverbial particle forms a distinct phrasal verb with its meaning. Therefore, since the NLP\nmodels are context-sensitive, the verb \"come\" may have multiple representations in NLP\nsystems, each indicating the specific adverb that follows it. For example, come_in,\ncome_out, come_back. However, the representations may appear similar in some ways,\nregarding the degree of semantic transparency of the phrasal verbs in this category; where\nthe combination might correspond to the composition of the verb and the following\nparticle which is named literal phrasal verbs."}, {"title": "1.5.3. Give in, Give out, Give up:", "content": "The verb \"give\" forms phrasal verbs with different particles (in, out, up), each\nstanding for a unique meaning. Similar to \u201ccome\u201d, each combination of \"give\" with a\ndifferent particle forms a distinct phrasal verb with its meaning. Thus, the different forms\nof the verb \"give\" may have more distinct representations in NLP systems because of its\nidiomatic nature, each indicating the specific particle that follows it. For instance, the\nfollowing verbs give_in, give_out, and give_up have different meanings and have no\nparticipation of the verb or the particle in the meaning of the full combination.\nIn summary, while \"agree\" might have a single representation in NLP systems\nwith features indicating the accompanying preposition or conjunction, \"come\" and \"give\"\nmay have multiple representations, each reflecting the specific adverbial particle that\nmodifies their meaning. This distinction arises from the different nature of these\nconstructions: \"agree\" operates more as a unitary verb with different argument structures,"}, {"title": "1.6. Constructional Approach to verb-particle combinations:", "content": "Lipka (1972) defined the category of phrasal verbs, which he calls \u201cverb-particle\nconstruction\" as \"in English (and also in German) can be regarded as a particular surface\nstructure shared by a large number of lexical items with various word-formative and\nsemantic structures.\u201d In simpler terms, this definition says that phrasal verbs in English\n(and German) follow a recognizable pattern in their construction (verb + particle). Despite\nhaving a wide range of combinations and meanings, they share a common format that\nmakes them identifiable as phrasal verbs. This pattern includes the way they are formed\n(verb combined with a particle) and their often-idiomatic meanings.\nWithin the framework of Lexical-Functional Grammar (LFG), a lexicalist,\nconstraint-based grammatical approach that shares a lot of fundamental tenets with\nConstruction Grammar. (CxG) (Findlay, 2023), and Booij (2001) argue against the\nmorphological interpretation of phrasal verbs (called them Separable Complex Verbs\n(SCVs)), instead positioning them as syntactic constructs. He proposed that phrasal verbs\nare originated from resultative small clauses, but undergo grammaticalization. This\nprocess transformed the particles from clause elements into productive aspectual markers.\nThe development of phrasal verbs exemplifies how syntactic surface structures can\ntranscend their role as mere outputs of syntactic rules. They can evolve into their entities,\nultimately resembling lexical idioms with open slots.\nBoas (2003: 32, 236, 280), in his work on resultatives, focuses on the resultative\naspect of the phrasal verbs, without taking into consideration the word order alternation.\nHe provides the idea that the particle acts as the modifier of the preceding verb and is part\nof its semantic frame, but it is not part of the verb's syntactic frame. In his approach, the"}, {"title": "1.7. This Study:", "content": "Despite their successes in natural language processing and other areas of artificial\nintelligence, recent deep neural networks remain difficult to explain and are still\nconsidered to be black-box models (El Zini & Awad, 2022). Here, we examine the\ninternal representations created by transformer-based models at different levels of the\nnetwork and assess their language expertise using appropriate extrinsic linguistic\nconstructs. Following Belinkov et al. (2020), regarding the interpretation of neural\nmodels and linguistic-level representations within their internal activations and in\nconnection with the aim of our study, which is the examination of the representation of\nverb-particle combination, we seek answers to the following questions: (i) Do the internal\nrepresentations capture lexical semantics? (ii) How accurately is the phrase-level\nstructure captured within the internal representations of individual words? Which layers\nin the architecture capture each of these linguistic phenomena? (iii) Can the internal\nactivations be explained and meet the usage-based theories about verb-particle\ncombinations like the unity of phrasal verbs and the syntactic compositionality of\nprepositional verbs?"}, {"title": "2. Methods:", "content": null}, {"title": "2.1. Dataset creation and pre-processing:", "content": "We collected natural language text data for training our model from the British\nnational corpus, with queries to search the target construction (target verb + target\nparticle) preceded by 10 tokens and followed by the other 10 tokens. The complete text\ndata consists of a total of 995 samples. The number of samples representing each specific\nverb construction is (agree on: 100; agree to: 100; agree that: 100; agree with: 100; come\nback: 99; come in: 99; come out: 99; give in: 99; give out: 93; give up: 100, give away:\n100)."}, {"title": "2.2. BERT architecture:", "content": "This section introduces (BERT) Bidirectional Encoder Representations from\nTransformers (Devlin et al., 2019) and its implementation for the study. BERT is a\ntransformer-based model, following the architecture proposed by Vaswani et al. (2017).\nThe BERT model employed a transformer design with 12 layers for the base model and\n24 for the large model. Each block in the base model has 12 attention heads, while the\nlarge model has 16 attention heads and a hidden dimension of 768 in the base and 1024\nin the large model."}, {"title": "2.3. Multi-dimensional scaling:", "content": "The neural model output for each target token is a vector representation of 768\ndimensions, which is not visualizable. Therefore, we used a frequently used statistical\ntechnique for generating low-dimensional data from the high-dimensional embeddings.\nThis method is multi-dimensional scaling (MDS) (Torgerson, 1952; Cox & Cox, 2008).\nIn the context of our study, we use it to represent the selected constructions in a dataset\nas points in a multidimensional space so that close similarity between constructions in the\ndataset corresponds to close distances between the corresponding points in the\nrepresentation. MDS is a productive implanting strategy to visualise high-dimensional\npoint clouds by anticipating them onto a 2-dimensional plane. Moreover, MDS has the\ndefinitive advantage that it is parameter-free and all shared separations of the focuses are\nprotected, in this manner preserving both the global and local structure of the primary\ndata (Surendra et al., 2023). MDS is an effective approach for visualising high-\ndimensional data by representing patterns as points in space and dissimilarities as\ndistances between points.\nThe data representation may be visualised as a series of point clusters by colour-\ncoding each projected data point in a data set based on its label. For example, MDS has\nalready been used to visualize word class distributions in various linguistic corpora\n(Schilling et al., 2021a), hidden layer representations (embeddings) of artificial neural\nnetworks (Schilling et al., 2021b; Krauss et al., 2021), the structure and dynamics of\nrecurrent neural networks (Krauss et al., 2021), brain activity patterns during pure tone\nor speech perception (Schilling et al., 2021; Krauss et al., 2018), or even during sleep\n(Krauss et al., 2018)."}, {"title": "2.4. Generalized discrimination value:", "content": "We used the generalized discrimination value (GDV) to calculate cluster separability\nas published and explained in detail in Schilling et al, (2021). Briefly, we consider N\npoints $X_{n=1..N} = (x_{n,1},..., x_{n,D})$, distributed within D-dimensional space. A label $I_n$\nassigns each point to one of L distinct classes $C_{l=1..L}$. In order to become invariant against\nscaling and translation, each dimension is separately z-scored and, for later convenience,\nmultiplied with $\\frac{1}{2}$:\n\n$s_{n,d} = \\frac{1}{2} \\frac{x_{n,d} - \\mu_d}{\\sigma_d}$\n\nHere, $\\mu_d = \\frac{1}{N}\\Sigma_{n=1}^{N} x_{n,d}$ denotes the mean, and $\\sigma_d = \\sqrt{\\frac{1}{N} \\Sigma_{n=1}^{N}(x_{n,d} - \\mu_d)^2 }$ the standard\ndeviation of dimension d. Based on the re-scaled data points $s_n = (s_{n,1},..., s_{n,D})$, we\ncalculate the mean intra-class distances for each class $C_l$\n\n$d(C_l) = \\frac{2}{N_l(N_l - 1)} \\Sigma_{i=1}^{N_l-1} \\Sigma_{j=i+1}^{N_l} d(s^{(l)}_i, s^{(l)}_j)$,\n\nand the mean inter-class distances for each pair of classes $C_l$ and $C_m$\n\n$d(C_l, C_m) = \\frac{1}{N_l N_m} \\Sigma_{i=1}^{N_l} \\Sigma_{j=1}^{N_m} d(s^{(l)}_i, s^{(m)}_j)$.\n\nHere, $N_k$ is the number of points in class k, and $s^{(k)}_i$ is the $i^{th}$ point of class k. The quantity\nd(a, b) is the Euclidean distance between a and b. Finally, the Generalized\nDiscrimination Value (GDV) is calculated from the mean intra-class and inter-class\ndistances as follows:\n\nGDV = $\\frac{1}{\\sqrt{D}} \\Sigma_{l=1}^{L} \\frac{1}{L} d(C_l) - \\frac{2}{L(L - 1)} \\Sigma_{l=1}^{L-1} \\Sigma_{m=l+1}^{L} d(C_l, C_m)$"}, {"title": "2.5. Code implementation:", "content": "We collected the internal representations by passing the samples of text through the\nmodels. The process was implemented with Python, using PyTorch (Paszke et al., 2019)\nfrom HuggingFace's Transformers library (Wolf et al., 2020). Mathematical operations,\nlike GDV calculations, were performed them with NumPy (Harris et al., 2020) and sci-\nkit-learn (Pedregosa et al., 2011) libraries. Visualizations and MDS were realised with\nMatplotlib (Hunter, 2007)."}, {"title": "3. Results:", "content": "The neural model has different representations for linguistic constructions across its\nlayers. We projected the model outputs for all linguistic constructions through the verbs'\nword embedding vectors using MDS (Figure 1 and 2). The GDV across layers of the\nneural network is illustrated for different verb constructions for each category: \u201cagree\u201d\nverbs, \u201ccome\u201d verbs, and \u201cgive\u201d verbs, and also for the value for all inputs. Consistent\nwith the approach for analysing construction category representations, the GDV is used"}, {"title": "4. Discussion:", "content": null}, {"title": "Linguistic explanations:", "content": "Considering the construction grammar framework, and the theoretical analysis of\nthe phrasal verbs and verb-preposition combinations. We discuss the given results in\nterms of two approaches, the first is the lexical unity of phrasal verbs and the non-\nunity in verb-preposition combinations. The second is the representation of the\nconstructions according to their organization in the constructicon network. As for the\nfirst approach, following Herbst and Sch\u00fcller (2008: 120), we assumed the tokens\ninvolved in phrasal verb sequences to be represented in isolated spaces since they are\nalready representing single lexical units in the lexicon (different representations of\nthe token Give according to the phrasal verb it belongs to, and the same for the token"}, {"title": "Models Capabilities:", "content": "The analysis of linguistic representations within transformer-based models, such as\nBERT, highlights substantial differences across linguistic levels. Our study demonstrates\nthat verb-particle combinations exhibit varying degrees of representation accuracy within"}, {"title": "5. Bibliography:", "content": null}]}