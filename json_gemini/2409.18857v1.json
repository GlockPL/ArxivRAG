{"title": "MITIGATING SELECTION BIAS WITH NODE PRUNING\nAND AUXILIARY OPTIONS", "authors": ["Hyeong Kyu Choi", "Weijie Xu", "Chi Xue", "Stephanie Eckman", "Chandan K. Reddy"], "abstract": "Large language models (LLMs) often show unwarranted preference for certain\nchoice options when responding to multiple-choice questions, posing significant\nreliability concerns in LLM-automated systems. To mitigate this selection bias\nproblem, previous solutions utilized debiasing methods to adjust the model's input\nand/or output. Our work, in contrast, investigates the model's internal representa-\ntion of the selection bias. Specifically, we introduce a novel debiasing approach,\nBias Node Pruning (BNP), which eliminates the linear layer parameters that con-\ntribute to the bias. Furthermore, we present Auxiliary Option Injection (AOI),\na simple yet effective input modification technique for debiasing, which is com-\npatible even with black-box LLMs. To provide a more systematic evaluation of\nselection bias, we review existing metrics and introduce Choice Kullback-Leibler\nDivergence (CKLD), which addresses the insensitivity of the commonly used met-\nrics to imbalance in choice labels. Experiments show that our methods are robust\nand adaptable across various datasets when applied to three LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of large language models (LLMs) has revolutionized artificial intelligence applications,\nparticularly in the domain of natural language processing. These models have demonstrated out-\nstanding performance across a variety of use cases, including chatbots, machine translation, text\ngeneration, data annotation, etc. Their ability to answer questions with high precision has opened\nup new avenues for automated systems.\nDespite their remarkable abilities, LLMs suffer from the se-\nlection bias problem that often occurs in answering multiple-\nchoice questions (MCQs). When selecting the answer for an\nMCQ, many LLMs prefer the choices in a given position (e.g.,\nthe last choice), or with a specific choice symbol (e.g., (A)\nor (3)) (Zheng et al., 2024; Wei et al., 2024; Pezeshkpour &\nHruschka, 2024). This phenomenon degrades model perfor-\nmance.\nMany previous works have attempted to explain this phe-\nnomenon and/or propose diverse ways to mitigate selection\nbias. While there are a few works focused on either modi-\nfying the input format (Li et al., 2023b; Robinson et al., 2023)\nor calibrating the output probabilities (Zheng et al., 2024; Reif\n& Schwartz, 2024; Wei et al., 2024), to the best of our knowl-\nedge, no embedding or parameter-level investigation has been\nperformed. Because selection bias originates from internal\nparameter-level computations, it is crucial to explore how the\nLLM embeddings contribute to the bias in their output re-\nsponses."}, {"title": "2 SELECTION BIAS IN LLMS", "content": "Although LLMs are most often used for text generation, some tasks involve responding to multiple-\nchoice questions (MCQs). For example, LLMs are increasingly used to annotate data samples, a task\nthat requires selecting the best choice from several options. When responding to MCQs, however,\nLLMs suffer from selection bias, which is the model's inclination to prefer a choice option bound\nwith a specific symbol or located in a certain order. In this section, we formally define selection\nbias (\u00a7 2.1) and discuss when and where the signs of selection bias are observed (\u00a7 2.2)."}, {"title": "2.1 SELECTION BIAS PROBLEM", "content": "Selection bias refers to a model's tendency to se-\nlect options in a given position or with a given\nsymbol among MCQ choices, regardless of the\ncorrectness of its choice. This includes the model's\na priori preference of a certain choice symbol,\nand its inclination to favor the choice presented at\na specific ordering position (Zheng et al., 2024).\nWe take a holistic view of this selection bias and\nformally define it as the discrepancy between the\nmodel response to the original choice ordering and\nthe expected model response over all the choice\norderings. That is, a model has no selection bias\nwhen reordering the MCQ answer choices leads to\nno change in the model predictions."}, {"title": "2.2 MOTIVATING ANALYSES", "content": "While selection bias is a prevalent problem in querying the large language model (LLM), it is im-\nportant to properly capture when and where the bias occurs. Here, we provide two simple analyses\nthat motivate the design of our debiasing methods.\nSelection bias is amplified when the model is incorrect. Figure 3(a) shows the frequency of\nchoices of the four response options on the ARC-Challenge dataset (Clark et al., 2018) using Llama-\n3-8B-Instruct (Meta, 2024) and Bloomz-7b1 (Muennighoff et al., 2023). We manipulated the test\ndataset to include all possible orderings of the MCQ choices. Thus, the bars should be at 0.25. How-\never, the models prefer answer choices 'D' and 'A', respectively. These preferences are stronger for\nincorrect responses than correct ones, suggesting that the models' uncertainty or ignorance amplifies\nthe selection bias.\nSelection bias stems from the final decoder layers. To identify the source of selection bias, we\ninvestigate the difference between the correct and incorrect sample embeddings. Specifically, we ex-\nplore the discrepancies within a single sample by permuting the sequence of choices in the question.\nThe difference between the embeddings within the choice-permuted set removes the sample-specific\nsemantic information while the pure effect of the selection bias remains in the difference.\nAccordingly, we first retrieve the intermediate embeddings of an LLM by computing the t-th token\nembedding from the l-th decoder layer as $z_{l,t} = f_l(x_A)_t$, where $f_l$ is the LLM decoder up to the\nl-th layer and $x_A$ is the input with answer choices A. For brevity of notation, let $z \\in \\mathbb{R}^d$ be the\nembedding from an arbitrary layer and token location. Then, we quantify the selection bias by com-\nputing the embedding difference between the correct and incorrect questions within the permutations\nof A. That is, the bias vector b for a sample x is defined as\n$b_x = \\frac{1}{n_-}\\sum_{i=1}^{n_-} z_-^{(i)} - \\frac{1}{n_+}\\sum_{i=1}^{n_+} z_+^{(i)}$\nwhere $z_-$ is the embedding vector of the choice-permuted questions that the model answered incor-\nrectly, and $z_+$ is from the correctly answered questions. Also, $n_-$ and $n_+$ correspond to the number\nof incorrect and correct questions, respectively. To balance the number of correct and incorrect\nsamples, we use the vector sets ${z_-, z_+}$ only when $1 \\le n_+/n_- < 2$. Then, we average the bias"}, {"title": "3 METHODS", "content": "Motivated by our findings that the selection bias is (1) amplified when the model response is in-\ncorrect, and (2) represented in the final decoder layer, we introduce two methods for debiasing\nthe model predictions: Bias Node Pruning (BNP) and Auxiliary Option Injection (AOI). As the\nnames suggest, BNP drops nodes in the final output layer that contribute to the selection bias, and\nAOI utilizes an auxiliary \u201cI don't know\" option to eliminate bias induced by ignorance."}, {"title": "3.1 BIAS NODE PRUNING", "content": "As shown in \u00a7 2.2, the average bias vector $b \\in \\mathbb{R}^d$ is most prominent in the final layer, and the\nselection bias materializes in the final output projection parameters, $W \\in \\mathbb{R}^{d \\times |V|}$, where V is\nthe vocabulary set. To mitigate the selection bias problem induced by the linear layer, we prune\nthe parameters in W that contribute to the bias. In choosing which parameters to prune, we gain\nintuition by approximating a biased model, F, as\n$F(x) \\approx (D(x_A) + b) \\cdot W,$\nwhere D is a conceptual LLM decoder with zero selection bias, and b is the average bias vector de-\nfined in equation 2. Then, $b \\cdot W$ is the factor that contributes to the selection bias, and removing the"}, {"title": "3.2 AUXILIARY OPTION INJECTION", "content": "Because selection bias is more likely when a model is incorrect, we hypothesized that providing an\n\"I don't know\" (IDK) option would reduce selection bias. The auxiliary option $o_{aux}$ is applied as\n$A := A \\cup {o_{aux}}$\n$\\hat{a} = arg\\underset{a \\in A}{max} P(\\hat{y} = a | x_A),$\nwhere A is the set of answer choices, and $x_A$ is the input question with choices A. How we retrieve\nthe probability for each choice a will be later discussed in the implementation details in \u00a7 5 and\nAppendix A.2. Further analyses on AOI will be provided in \u00a7 6.2."}, {"title": "4 EVALUATION", "content": "There is no consensus in the literature on how to measure selection bias. Here, we first review two\nselection bias metrics, Standard Deviation of Recalls (RStd) and Relative Standard Deviation (RSD),\nwhich evaluate the consistency of performance across choices. By scrutinizing their limitations, we\npropose Choice Kullback-Leibler Divergence (CKLD), which is a novel distribution-based bias\nmetric.\nDefinition 1. (Standard Deviation of Recalls) is the standard deviation of the class-wise recall:\n$RStd = \\sqrt{\\frac{1}{k}\\sum_{i=1}^k (r_i - \\bar{r})^2},$\nwhere k is the number of choices, $r_i$ is the recall of the i-th class, and $\\bar{r}$ is the arithmetic mean of $r_i$\nvalues (Zheng et al., 2024).\nDefinition 2. (Relative Standard Deviation) is the class-wise accuracy standard deviation normal-\nized by the overall accuracy:\n$RSD = \\frac{\\sqrt{\\sum_{i=1}^k (s_i - \\bar{s})^2}}{\\bar{s}},$\nwhere k is the number of choices, $s_i$ is the accuracy of the i-th class, and $\\bar{s}$ is the mean accuracy\naveraged across classes (Croce et al., 2021; Reif & Schwartz, 2024).\nWe empirically show how these performance-based metrics, RStd and RSD, behave across different\ndata characteristics. We constructed synthetic 4-way MCQ datasets by varying the choice selection\nratio under different ground-truth ratios. For instance, in the third column of Figure 5, labeled \"'A'\nLabel Ratio = 0.55\", answer choice 'A' is the correct choice in 55% of the samples and the rest are\nlabeled 'B', 'C', or 'D' 15% of the time, respectively. To simulate realistic predictions, we have the\nmodel render correct predictions half of the time, and predict with respect to the choice selection\nratio (i.e., 'A' selection rate) for the other half. For example, if 'A' Selection Rate is 0.4, each choice\nwill be sampled with respect to $P(A) = 0.4$ and $P(B) = P(C) = P(D) = 0.2$ half of the time,\nand will predict the correct answer for the other half. With this set up, the selection bias metrics\nshould be lowest at the 'A' Label Ratio, shown with a vertical dashed line in Figure 5."}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate our Bias Node Pruning (BNP) and Auxiliary Option Injection (AOI) in\nvarious settings. We demonstrate the effect of our methods in \u00a7 5.1 and show that AOI can debias\nblack-box models in \u00a7 5.2.\nDatasets and Models. We evaluate our method on three multiple-choice question answering data\ntest sets, ARC-Challenge (Clark et al., 2018), MMLU-Redux (Gema et al., 2024), and Common-\nsenseQA (Talmor et al., 2019). To retrieve the average bias vectors (equation 2), a separate set of\nout-of-bag samples is used. Further dataset details are provided in Appendix A.1. For the models, we\nmainly evaluate our approach on Llama-3-8B-Instruct (Meta, 2024), Mistral-7B-Instruct-v0.2 (Jiang\net al., 2023), and Bloomz-7b1 (Muennighoff et al., 2023).\nImplementation Details. As discussed in Section \u00a7 4, we employ RSD and CKLD to measure\nselection bias and assess the debiasing performance of our approach. We use Accuracy and the\nweighted F1 score for question answering performance evaluation. In predicting the answers from\nLLMs, we follow previous works (Zheng et al., 2024): we select the choice symbol (e.g., A, B,\nC, D) with the highest probability. For BNP, we prune 32 nodes for Llama-3 and Mistral, and 128"}, {"title": "5.1 MAIN EXPERIMENTS", "content": "BNP + AOI consistently improves base model performance by reducing selection bias. Table 1\nshows the performance of our methods with three LLMs and MCQ datasets. For all models and\ndata sets, BNP and/or AOI increased accuracy and F1 score and decreased RSD and CKLD. It is\nespecially noteworthy that Llama-3's accuracy on ARC-Challenge improves from 52.3% to 65.3%\nwhen both BNP and AOI are applied; an outstanding 24.9% increase.\nOur method can be applied together with other debiasing and decoding methods. For fur-\nther insight, we compare our methods with other debiasing and decoding approaches: Chain-of-\nThought (CoT; Wei et al. (2022)), In-Context Learning (ICL; Brown et al. (2020)), and Decoding by\nContrasting Layers (DoLa; Chuang et al. (2023)). For CoT, we follow the implementation of Ope-\nnAI Evals (OpenAI) by first prompting with \"Let's think step by step\", and then using the generated\nexplanation to regenerate the final prediction. In the case of ICL, we take one question from the\ntraining set to retrieve N! choice-permuted questions, where N is the number of choices. Then, we\nrandomly select three questions from the choice-permuted pool and create demonstrative examples\nfrom them, where the LLM agent always answers the choice-permuted questions correctly. Con-\ncrete prompt formats and details are provided in Appendix A.3. These baseline methods can be\nused along with our debiasing methods. Both question answering and debiasing improve when our\nmethods are applied together (Table 2), even achieving the best performance in collaboration with"}, {"title": "5.2 BLACK-BOX SETTINGS", "content": "Several state-of-the-art models are black-box and their parameters are not open to the public. In\nthese cases, BNP is not feasible, leaving AOI as the only available technique for debiasing the model,\nusing text outputs for prediction. For this reason, we devise a comparative experiment where only\nAOI is applied to the models. For white-box models Llama-3, Bloomz, and Mistral, we compute the\nJaccard similarity between each choice option and the generated text to select the choice with the\nhighest similarity score, instead of the probability-based answer selection method used in our main\nexperiments. This approach simulates a black-box setting with the white-box models. Moreover,\nwe extend our experiment to Claude-3 Haiku and Sonnet models (Anthropic, 2023), which are\nclosed-source black-box models. In Table 3, AOI generally improves black-box model performance\n(accuracy and F1) and reduces selection bias (RDS and CKLD)."}, {"title": "6 ANALYSES", "content": "In this section, we provide in-depth analyses on the mechanism and efficacy of our methods: Bias\nNode Pruning (\u00a7 6.1) and Auxiliary Option Injection (\u00a7 6.2). The qualitative findings from our\nexperiments are discussed in \u00a7 6.3."}, {"title": "6.1 ANALYZING BIAS NODE PRUNING", "content": "BNP is not sensitive to the number of nodes pruned. Figure 6(a) reveals how the performance\nmetrics change as the number of pruned nodes varies. Regardless of the number of nodes pruned\nfrom 8 to 128, our method improves the base performance (dashed lines in the figure) by great\nmargins. While our method is robust to the amount of nodes pruned, searching for the adequate\nlevel of pruning may achieve better debiasing performance on the downstream task. Full list of the\nfigure is provided in Appendix D.2.\nThe average bias vector can be generalized across datasets. The average bias vector represents\nthe direction of selection bias in the embedding space. If the bias vector captures pure information\nabout selection bias, it should generalize across datasets. To test this hypothesis, we used the bias\nvector from one dataset on another. Figure 6(b) shows a heatmap of the improvement in each"}, {"title": "6.2 ANALYZING AUXILIARY OPTION INJECTION", "content": "Content of the auxiliary option matters. Table 4: AOI with arbitrary option content on the\nOur experiments above used \u201cI don't know\u201d as MMLU-Redux dataset.\nthe auxiliary option, but other options are also\npossible. We conducted an experiment where\nwe substituted it with \"I know the answer\". In\nTable 4, 'Arbitrary' refers to this auxiliary op-\ntion. For Llama-3 and Bloomz, the inclusion\nof the arbitrary auxiliary option improves per-\nformance and reduces selection bias relative to\nthe baseline (Table 4), but the \"I don't know\"\n(Ours) performs better in most cases. With the\nMistral model, however, the arbitrary options\ndegrades model performance and increase selection bias. A full table with other datasets and more\nablation experiments are in Appendix E"}, {"title": "6.3 QUALITATIVE EVALUATION", "content": "Impact on choice distributions. In Figure 7, we show how the distribution of the selected answer\nchoices changes when we introduce BNP and AOI. In all three datasets, the distribution becomes\nmore uniform when BNP and/or AOI are applied, indicating lower levels of selection bias. More\nqualitative examples are provided in Appendix F.\nQualitative examples. In addition to disclosing the distributional effect, we provide below the\nqualitative question-response examples of Llama-3 and Bloomz on the ARC-Challenge dataset. As\nin Figure 3(a), Llama-3 often showed a preference for choice 'D', regardless of the order of choices.\nOur method successfully corrects such errors. Bloomz, on the other hand, showed a preference for\nchoice 'A'. Again, our methods corrected the model's response."}, {"title": "7 RELATED WORKS", "content": "Selection Bias. The large language models' tendency to favor choices in a certain order or with\na specific symbol has been discussed in many previous works. Some of the works investigated\nthe skewed pattern of responses for MCQs (Zheng et al., 2024; Wei et al., 2024; Pezeshkpour &\nHruschka, 2024), emphasizing that selection bias is a critical problem. Many works have approached\nthis problem by calibrating the output probabilities (Wang et al., 2023; Zheng et al., 2024; Reif\n& Schwartz, 2024; Wei et al., 2024; Pezeshkpour & Hruschka, 2024; Wang et al., 2024; Balepur\net al., 2024; Li & Gao, 2024; Gupta et al., 2024), while others change the way queries are input (Li\net al., 2023b; Robinson et al., 2023). Additional approaches include debiasing the LLM through\ndistillation training (Liusie et al., 2024) and training the model to enforce its multiple choice symbol\nbinding (MCSB) property (Xue et al., 2024). While parameter pruning methods are often used for\nefficient deep learning (Srinivas & Babu, 2015; Han et al., 2016; Zhu & Gupta, 2017; Molchanov\net al., 2019; 2022) or to have the LLM unlearn certain factual knowledge (Liu et al., 2024; Pochinkov\n& Schoots, 2024), parameter pruning has rarely been discussed for debiasing. Thus, our Bias Node\nPruning is a novel approach in the context of the selection bias.\nAuxiliary Options. Inclusion of the \"I don't know\" option can improve the quality of data col-\nlected in surveys (Schuman & Presser, 1996) but does not meaningfully impact the labels assigned\nby annotators (Beck et al., 2022). Recent research has drawn attention to the similarities between\nsurveys, labeling tasks, and model responses to MCQs (Tjuatja et al., 2023; Eckman et al., 2024;\nChen et al., 2024) Further research into LLMs' response behavior would benefit from incorporating\ninsights from the survey science domain: see the discussion in (Eckman et al., 2024)."}, {"title": "8 CONCLUSION", "content": "When LLMs answer MCQs, selection bias is a critical problem. Previous research has predomi-\nnantly focused on modifying the LLM's input and/or output. In contrast, we uncover the internal\nsource of the bias by scrutinizing the embedding-level discrepancies introduced by this bias. Build-\ning on these insights, we propose Bias Node Pruning (BNP) and Auxiliary Option Injection (AOI).\nAdditionally, we address the limitations of existing performance-based evaluation metrics by in-\ntroducing a new distribution-based metric, Choice Kullback-Leibler Divergence (CKLD), which ad-\ndresses the insensitivity of prior metrics to imbalance of choice labels. Our approach improved MCQ\nanswering performance by reducing the level of selection bias across widely used MCQ datasets\nusing both open-source (white box) and closed-source (black-box) models. BNP and AOI work\nalongside other debiasing/decoding methods to improve the base performance of Llama-3 by up to\n33.8% on the ARC-Challenge dataset. We also conducted in-depth analyses to better understand the\neffect of each component, along with case studies to provide qualitative insight. Overall, our method\nprovides a novel intuition in scrutinizing the internal source of selection bias, and also provides a\nnew approach in debiasing LLMs."}, {"title": "A FURTHER EXPERIMENTAL DETAILS", "content": "A.1 DATASETS\nWe experiment on three datasets: ARC-Challenge (Clark et al., 2018), MMLU-Redux (Gema et al.,\n2024), and CommonsenseQA Talmor et al. (2019). We also provide the ground-truth choice ratios\nin the test dataset in Table 5.\nARC-Challenge is a dataset from the AI2 Reasoning Challenge, which contains grade-school\nlevel multiple-choice science questions. Among the 'Challenge' and the 'Easy' sets, we use the\nformer set with 1.17K test and 1.12K training questions. The training questions are used to extract\nthe average bias vectors.\nMMLU-Redux is a dataset derived from the original Massive Multitask Language Understand-\ning (MMLU) (Hendrycks et al., 2021) dataset, which comprises multiple-choice questions from 57\ndifferent branches of knowledge. Gema et al. (2024) discovered that this original version contains\nnumerous errors, and curated the dataset to have 3,000 manually re-annotated questions across 30\nsubjects in the original MMLU dataset. In the case of MMLU-Redux, there is no training set avail-\nable. So we utilize the validation set from the original MMLU dataset to pre-compute the average\nbias vectors.\nCommonsenseQA is a dataset of multiple-choice questions that require commonsense knowledge\nto respond. The dataset questions are extracted using the knowledge graph, ConceptNet (Speer et al.,\n2017), which consists of 9.74K training and 1.22K validation questions. We use the training set to\nretrieve the average bias vectors and evaluate on the validation set."}, {"title": "A.2 IMPLEMENTATION DETAILS", "content": "Here, we detail how we retrieve model predictions and list hyperparameters used for each model-\ndataset experiment."}, {"title": "A.3 BASELINES", "content": "In this section, we provide further details on how the debiasing baselines in Table 2 are designed.\nChain-of-Thought (CoT) first generates the model response that includes explanations by\nprompting with \"Let's think step by step\" as follows.\nSystem Prompt: You are an AI assistant that answers multiple choice questions. Please think\nstep by step and respond with capitalized alphabet(s) that correspond to the correct answer.\nUser: { question }.\nAssistant: Let's think step by step.\nUsing the explanation that is generated with the prompt, we query the LLM once more with\nSystem Prompt: You are an AI assistant that answers multiple choice questions. Please think\nstep by step and respond with capitalized alphabet(s) that correspond to the correct answer.\nUser: { question }.\nAssistant: Let's think step by step. { explanation }. So the correct answer is\nand identically use the first token output probability distribution to retrieve the predictions. Note\nthat the actual prompt format depends on the model and the template above is a generic form.\nIn-Context Learning (ICL) takes one question out-of-bag sample and retrieve N! choice-\npermuted questions, where N is the number of choices. Then, three of the choice-permuted ques-"}, {"title": "A.4 METRICS", "content": "In this section, we provide a full list of selection bias metrics, including RStd, RSD, our CKLD, and\nother existing metrics that were not discussed in the main paper. We taxonomize the metrics into\nthree groups: brute-force evaluation, performance-based evaluation, and distribution-based evalua-\ntion.\nA.4.1 BRUTE-FORCE EVALUATION\nBrute-force evaluation metrics utilize all possible choice permutations to retrieve the metric value.\nSince we need to infer the output for each of the choice-permuted questions, the computation in-\ncreases by a factor of N!, where N is the number of choices in the question. Here, we list two\nbrute-force evaluation metrics, Proportion of Plurality Agreement (PPA) and Permutation Sensitiv-\nity (PS), and one semi-brute-force metric that additionally computes only the reverse-order permu-\ntation, Fluctuation Rate (FR).\nDefinition 1. (Proportion of Plurality Agreement) is the proportion of the plurality choice among\nall possible choice orderings of a multiple-choice question:\n$PPA = \\frac{1}{|X|}\\sum_{i=1}^{|X|} (\\frac{max_{n}}{N!}\\sum_{j=1}^{N!} \\mathbb{1} (y_j = o_n)),$\nwhere X is the set of test samples, N is the number of choices in each question, n is the index of the\nchoices, $y_j$ is the choice content of the j-th choice-permuted sample prediction, and $o_n$ is the n-th\nchoice content. (Robinson et al., 2023)\nDefinition 2. (Permutation Sensitivity) is the expected divergence in output probability distributions\nof the choice-permuted questions:\n$PS = E_{\\sigma_i, \\sigma_j}[d(P(\\cdot | q, A_{\\sigma_i}); P(\\cdot | q, A_{\\sigma_j})],$"}, {"title": "B COMPLEXITY OF BIAS NODE PRUNING", "content": "Bias Node Pruning is a two-step process that includes the (1) average bias vector computation,\nand (2) node pruning. The first phase utilizes M out-of-bag samples with N choices. This step\nrequires computing the outputs of N! choice-permuted questions, translating to a complexity of\nO(N! M). Once we retrieve the average bias vector, we use it to compute the top-k nodes that\nactivate selection bias (equation 4). This is also a one-time process whose node-pruned parameters\nare applied throughout all test-time inference tasks. The complexity of inference itself is identical to\nthe original model without Bias Node Pruning, which is proportional to the number of test samples\nevaluated."}, {"title": "C PROOF OF CKLD'S LABEL RATIO SENSITIVITY", "content": "We want to prove that CKLD is minimized when the prediction has no bias towards a certain choice,\nand matches the ratio of ground-truth labels. From the CKLD definition (equation 15) of\n$CKLD = \\sum_{i=1}^k p_i log \\frac{p_i}{q_i},$\nlet $q_i = p_i r_i$, where $r_i$ is the selection bias multiplier applied to the ground-truth choice ratio for\neach i = 1, ..., k. As we want to find out when CKLD is minimized, we formulate the objective as\nfollows:\n$minimize \\sum_{i=1}^k p_i log \\frac{p_i}{q_i}$\ns.t. $q_i = p_i r_i$ and $\\sum_{i=1}^k p_i r_i = 1$.\nBy rewriting this as a Lagrangian function L,\n$L(r_1,..., r_k, \\lambda) = \\sum_{i=1}^k p_i log \\frac{p_i}{p_i r_i} + \\lambda(\\sum_{i=1}^k p_i r_i - 1)$\n$= -\\sum_{i=1}^k p_i log r_i + \\lambda (\\sum_{i=1}^k p_i r_i - 1),$\nwhere $\\lambda$ is the Lagrangian multiplier, we take the partial derivative of each variable as:\n$\\frac{dL}{dr_i} = -\\frac{p_i}{r_i} + \\lambda p_i = 0$\n$\\frac{dL}{d\\lambda} = \\sum_{i=1}^k p_i r_i - 1 = 0.$\nThen, from equation 19,\n$r_i = \\frac{1}{\\lambda},$\nand by substituting this to equation 20, we get\n$0 = \\sum_{i=1}^k p_i \\frac{1}{\\lambda} - 1$\n$\\frac{1}{\\lambda} = 1.$\nTherefore, the objective is minimized when $\\lambda = 1$, which translates to $r_i = 1$ (. equation 21). This\nis equivalent to saying that CKLD is minimized when $q_i = p_i r_i = p_i$, i.e., when the prediction ratio\nmatches the actual label ratio and there is no selection bias towards a certain choice."}, {"title": "D MORE EXPERIMENTS AND ANALYSES", "content": "Here, we provide further experiments and analysis results that were not included in the main\nmanuscript. In \u00a7 D.1, we demonstrate an extended experiment result on another dataset. In \u00a7 D.2,\nan extended list of figures of Figure 6 (a) is provided.\nD.1 FURTHER EXPERIMENTS ON HELLASWAG DATASET\nBeyond the three datasets tested in our main paper in Table 1, we disclose results on another widely\nused benchmark dataset, HellaSwag (Zellers et al., 2019). HellaSwag is a commonsense natural\nlanguage inference (NLI) dataset that contains 4-way MCQ samples that asks the model to select\nthe option that best ends the given sentence. The experimental results are in Table 6. Bloomz is not\nincluded in the table because the model failed to reasonably respond to most of the questions."}, {"title": "E DIFFERENT AOI SETUP", "content": "In this section, in addition to all three dataset ablation studies on the content of auxiliary options in\n\u00a7 6.2, we provide further ablation study results on the number and location of the auxiliary options.\nMore auxiliary options have mixed effects on performance. We find that controlling the number\nof auxiliary options has a notable impact on performance. That is, we tried adding multiple auxiliary\noptions, all with the same \u201cI don't know\" content. In most cases in Table 7, adding more auxiliary\noptions did not help improve performance (see n-Choices AOI). Interestingly, however, both the\nquestion-answering and debiasing performance of Llama-3 significantly improved when using more\noptions. This seems to be a peculiar property of Llama-3 that we can enhance its performance by\nsimply adding multiple auxiliary options.\nLocation of the auxiliary option does not decide performance. The location of the auxiliary\noption is another factor to consider. In our main experiments, we have appended the \"I don't know\"\noption to the end of the choice list. In comparison, we try placing it in the first choice option (i.e.,\nwith choice symbol 'A'), corresponding to 'First Choice AOI' in Table 4. Overall, there were mixed"}]}