{"title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution", "authors": ["Yuxiang Wei", "Olivier Duchenne", "Jade Copet", "Quentin Carbonneaux", "Lingming Zhang", "Daniel Fried", "Gabriel Synnaeve", "Rishabh Singh", "Sida I. Wang"], "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data-the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-40. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.", "sections": [{"title": "1 Introduction", "content": "The application of large language models (LLMs) to software engineering (SE) tasks has received significant attention in recent years, with researchers exploring their potential to automate various complex SE tasks, such as library-level and complex code generation , real-world bug/issue resolution , and software testing. Among these tasks, SWE-bench -a benchmark for solving real-world software issues has emerged as a focal point of research efforts, and researchers have proposed various agentic or pipeline-based methods to push LLMs' real-world issue solving capability. However, most current techniques depend on powerful proprietary LLMs like GPT-40 or Claude-3.5-Sonnet , where advancements are driven more by enhanced prompting strategies than by improvements in the underlying LLM.\nWith the release of DeepSeek-R1 , reinforcement learning (RL) using rule-based rewards has become a crucial technique for enhancing the reasoning capabilities of LLMs across various downstream tasks, including coding and mathematics. However, their effectiveness in SE tasks remains limited , and their substantial total parameter size (671B) poses challenges for researchers attempting to train them. For mathematics, the reward is generally defined as whether the answer predicted by the LLM can exactly match the ground truth. While in coding, existing RL research typically utilizes execution feedback as the reward signal and is limited to competitive programming tasks, where code is self-contained and easily executable. This is challenging to apply to real-world SE tasks due to the execution cost and lack of executable environments. Meanwhile, previous research relied on proprietary teacher models and has focused primarily on supervised fine-tuning (SFT), which, as we demonstrate in our paper, is less effective and less generalizable.\nTo address these limitations, we propose SWE-RL, the first RL method to improve LLMs on SE tasks by directly using rule-based rewards and software evolution data-the record of entire software lifecycle, including all code snapshots, changes, and events like PRs and issues. As shown in Figure 1, we begin by curating a comprehensive dataset of GitHub pull requests (PRs), which is then transformed into the seed dataset for RL. Each data item includes an issue, the corresponding code context, and the oracle patch merged by the PR. During RL, the policy LLM is tasked with solving a given issue through reasoning and producing the code changes. The code changes are then converted into a consistent patch format for reward calculation. If the response is incorrectly formatted, the reward will be -1; otherwise, the reward is a similarity score (between 0 and 1) of the predicted and the oracle patch calculated by Python's difflib.SequenceMatcher . Notably, we provide the complete content of each file in the input prompt, which implicitly teaches the model to reason about the precise fault locations before suggesting repair edits.\nApplying SWE-RL to Llama-3.3-70B-Instruct , our model Llama3-SWE-RL-70B solves 41.0% of the issues in SWE-bench Verified , a human-verified subset of SWE-bench, with Agentless Mini, our pipeline-based scaffold built upon Agentless , featuring simplifications to match our RL process and enhancements for scaling. This performance is comparable to leading proprietary LLMs like GPT-40 and state-of-the-art among medium-sized LLMs with less than 100B total parameters. Our ablation studies demonstrate that Llama3-SWE-RL-70B significantly outperforms its Llama baseline. Additionally, we developed a competitive supervised fine-tuning (SFT) model from Llama-3.3-70B-Instruct using synthetic data generated in the Magicoder style to enhance the chain-of-thought process, employing the same seed as SWE-RL. We show that Llama3-SWE-RL-70B, trained with SWE-RL solely for solving issues, not only surpasses the SFT model in SWE-bench but also excels in other out-of-domain (OOD) tasks, including function-level coding , practical code generation with library use , code reasoning , mathematics , and general language understanding . In these OOD tasks, Llama3-SWE-RL-70B even outperforms Llama-3.3-70B-Instruct, whereas the SFT model results in decreased performance.\nIn summary, our contributions are as follows:"}, {"title": "2 SWE-RL", "content": null}, {"title": "2.1 Raw pull request data curation", "content": "Figure 2 provides a high-level overview of our process for curating the raw PR dataset for Llama3-SWE-RL. In the following paragraphs, we detail each step of the curation process. During data processing, we exclude all the repositories used by SWE-bench to prevent data contamination.\nGitHub events and clones. The goal of this stage is to recover all pull request details that human developers can inspect on GitHub. To achieve this, we need two sources of information: (1) all events that occur within a PR and (2) the source code of a repo before the changes introduced by the PR are merged. We derive all GitHub events from GHArchive , which contains all activity events data from GitHub. Our collection includes all GitHub events from Jan 1, 2015 to Aug 31, 2024.\nTo obtain source code, since pull requests often occur at different commit stages of a repository, we opt to use git clone to retrieve the entire repository with its commit history, rather than relying on the GitHub API to download specific code snapshots. Eventually, we successfully cloned and processed 4.6M repositories.\nPR data aggregation. These collected events and git clones are disparate entities that require further processing before they can be used for training purposes. At this stage, we focus on each PR individually and aggregate all pertinent information associated with it. This includes mentioned issues, user discussions, review comments, initial code contents, and subsequent commits and code changes.\nTo start, we keep only merged PRs and gather all related conversational events for each PR, sorting them in chronological order. Next, using the base_commit and head_commit hashes of a PR, we retrieve the contents of all modified files indicated by its patch at the merge base of the two commits. The reasoning behind this approach is that many PRs aim to merge back into the main branch, which may have undergone changes since the PR was created. By considering the merge base as the actual starting point for a developer working on the PR, we can more accurately understand the context of the changes. We save all intermediate commits and code changes made between the merge base and the head commit. Additionally, we extract the complete patch that represents the cumulative changes from start to finish. Finally, we scan each aggregated PR to identify patterns that resemble issues, and associate the matched issues with the corresponding PR. In the end, we have 24M aggregated PR instances.\nRelevant files prediction. Currently, each pull request includes only the code files that have been modified. In our earlier experiments, we noticed that this approach let LLMs learn a bias: the model consistently generated edits for every code file presented and was unable to handle noisy files presented in the context. This issue was also mentioned in the finetuning experiment discussed in the SWE-bench paper. To"}, {"title": "Data filtering", "content": "GitHub PRs can be quite noisy, so we implemented various filtering strategies to eliminate potentially harmful PRs. In designing these filtering rules, our goal is to maximize the recall of high-quality PRs while permitting a certain level of noise. First, we remove the bot-generated PRs whose title, description, or username contains keywords \u201c[bot]\u201d, \u201cdependabot\u201d, \u201crenovate\u201d, \u201cbump\u201d, or \u201cautomerge\u201d. Also, we remove PRs with empty changes or with extremely large number of changes (e.g., in some PRs, the developer uploaded a directory of data files by mistake). Additionally, we implemented a more fine-grained set of filters used in CodeLlama to examine each code change hunk. We then removed any PRs where all code changes were flagged by these filters. For example, this can exclude PRs with only lock file changes or version updates. Finally, this gives us around 11M unique PR instances."}, {"title": "2.2 Reward modeling", "content": "To prepare the initial dataset for RL, we extract high-quality PR seeds from the raw dataset we collected. These seeds are selected based on specific heuristics. For example, a PR instance should include at least one linked issue, the issue should describe a bug-fixing request, and the code changes should involve programming files. For each seed, we extract the issue descriptions and code context, including all changed files and some relevant but unchanged files. These are converted to input prompts for the policy LLM. We also take the oracle patch from each seed, which will be used for reward calculation.\nWe bootstrap the policy LLM with the prompt template shown in Figure 3. Assuming that the LLM has generated a rollout \u03c4 given an issue and its code context, the reward function is defined as follows:\n\\(R(\\tau) =\\begin{cases}\n-1, & \\text{if the format is wrong,}\\\\\n\\text{compare}(\\text{patch}_{\text{pred}}, \\text{patch}_{\text{gt}}), & \\text{otherwise.}\n\\end{cases}\\)   (1)\nHere, patchpred denotes the patch corresponding to the code changes generated by the policy LLM if the format of \u03c4 is correct, and patchgt means the oracle patch for this issue."}, {"title": "2.3 Aha moments and generalized reasoning capabilities", "content": "\"Aha moments\" on software engineering. With the application of SWE-RL, we observe \u201caha moments\" where Llama3-SWE-RL exhibits emergent reasoning skills. To our knowledge, this is the first study demonstrating the existence of such aha moments in the realm of real-world software engineering tasks, confirming the findings of DeepSeek-R1 , which mainly focuses on competition coding and math. As shown in Figure 4, through RL, Llama3-SWE-RL-70B can allocate more thinking time to reflect on its initial assumptions during the issue-solving process. This behavior emerges naturally from the model's interaction with RL, rather than being explicitly programmed."}, {"title": "3 Evaluation", "content": null}, {"title": "3.1 Experimental setup", "content": "Training configs. Llama3-SWE-RL-70B is trained on top of Llama-3.3-70B-Instruct using SWE-RL for 1,600 steps with a 16k context window. We use a global batch size of 512, sampling 16 rollouts from each of the 32 problems in every batch. For every global step, a single optimization step is performed.\nScaffolding. We have developed Agentless Mini on top of Agentless as the underlying scaffold. Different from Agentless's multi-step localization, Agentless Mini focuses solely on file-level localization, delegating detailed reasoning to the repair step by providing the entire file contents in the input. This enables Llama3-SWE-RL to do more reasoning during SWE-RL and simplifies the RL process to focus on only one issue-solving task. Despite this simplification, Llama3-SWE-RL can still seamlessly complete other pipeline steps, benefiting from out-of-domain generalization through RL. Furthermore, while Agentless only employs one reproduction test per issue for reranking, Agentless Mini can use multiple reproduction tests, which has proven effective in our scaling analysis (\u00a73.4). More details about Agentless Mini can be found in Appendix A.\nEvaluation setup. We conduct evaluation on SWE-bench Verified , a subset of SWE-bench with 500 human-verified problems that can more reliably evaluate AI models' capability in solving real-world software issues. In the main evaluation (\u00a73.2), we generate 500 patches for each problem using a 1.0 temperature, and use the top 30 reproduction tests for execution and reranking. Ultimately, only the highest-ranked patch for each issue will be submitted for SWE-bench evaluation to calculate the pass@1 score.\nSFT baseline. To understand the advantages of SWE-RL, we also trained an SFT baseline, named Llama3-SWE-SFT-70B, for experiments in \u00a73.3, \u00a73.4, and \u00a73.5. It is trained on top of Llama-3.3-70B-Instruct using a mixture of synthetic code editing data, Llama 3 coding SFT data, and Llama 3 general SFT data. The synthetic data is generated using an approach inspired by Magicoder , where high-quality PR data serves as the seeds for creating chain-of-thoughts and subsequent editing, as well as serving as the oracle for filtering. Llama-3.3-70B-Instruct is employed for this generation process. The seed PR dataset is identical to the one utilized for RL. In contrast to our RL model, which only requires a seed PR dataset to trigger the RL loop, the SFT baseline needs synthetic data generation for chain-of-thoughts and additional data mix to ensure the dataset diversity and model generalizability. More details are explained in Appendix B."}, {"title": "3.2 Main results", "content": "Table 1 presents the pass@1 results on SWE-bench Verified for models that utilize open-source scaffolds, with models categorized by their size. From the table, we observe that Llama3-SWE-RL-70B achieves state-of-the-art results among small and medium-sized language models (<100B) by resolving 41.0% of the issues. Additionally, all other open-source baselines we compare, such as Lingma-SWE-GPT , SWE-Gym , and SWE-Fixer , include distilled outputs from GPT-40 or Claude-3.5-Sonnet in the training data. In contrast, Llama3-SWE-RL is trained solely with publicly available data through our reinforcement learning technique SWE-RL, without relying on any proprietary LLMs in the pipeline. Llama3-SWE-RL also sets a new record for Llama-based methods on SWE-bench."}, {"title": "3.3 Baseline comparison", "content": "To understand how much SWE-RL improves LLMs in solving sofware issues, we compare Llama3-SWE-RL with the corresponding Llama-3 and SFT baseline in Table 2, using Agentless Mini as the underlying scaffold. In this experiment, we also evaluate on SWE-bench Verified but focus on the models' repair ability. To achieve this, we provide oracle files in the context and let the model generate a single repair edit using greedy decoding, without incorporating additional pipeline steps such as localization and test generation. The table reveals that the base Llama-3.3 model struggles to produce correctly formatted code edits, even when using a 20-sample majority voting approach, where outputs with incorrect formats are pre-filtered. With SFT, most code edits generated by the language model are correctly formatted, and the repair performance shows significant improvement. However, Llama3-SWE-RL-70B demonstrates even greater enhancement in repair capabilities, although its format accuracy is slightly lower than that of the SFT version. This indicates that SWE-RL aids the LLM in better reasoning about issue solving and code editing."}, {"title": "3.4 Scaling analysis with more samples", "content": "Agentless Mini supports scaling both the number of repair samples and the number of generated reproduction tests. The difference in the number of samples may affect the reranking accuracy. In this section, we evaluate how the final pass@1 performance on SWE-bench Verified scales with the two factors.\nFigure 5 shows that increasing both the number of repair samples and test samples enhances performance on SWE-bench. Notably, for repair samples, there is a significant score increase from 33.6 to 40.0 when the sample size is expanded from 20 to 160. However, beyond 160 samples, the improvement trend begins to plateau, with scores only rising slightly from 40.0 to 41.0 as the sample size increases to 320 and 500. Although the impact of adding more reproduction test samples is less obvious, there is still a gradual score improvement from 38.8 to 41.0 as the number of test samples increases up to 20. There is no difference between 20 and 30 test samples, suggesting a performance saturation point has been reached."}, {"title": "3.5 Generalizability of RL", "content": "Surprisingly, we have identified additional aha moments where Llama3-SWE-RL acquires general reasoning abilities that are transferrable to various out-of-domain tasks, such as function-level code generation and mathematics, although RL is applied exclusively to software issue solving. Figure 4 demonstrates that Llama3-SWE-RL is capable of reasoning through self-reflection, exploring alternative approaches, and solving complex problems by breaking them down into smaller subtasks. In \u00a73.5, we demonstrate that Llama3-SWE-RL improves over even more out-of-domain tasks, including library use, code reasoning, and general language understanding."}, {"title": "3.6 Reward ablation", "content": "According to Equation (1), the reward design of SWE-RL allows different instantiations of the compare function. Throughout the paper, we adopt the sequence similarity between the predicted and the oracle patch, which is a continuous value from 0 to 1. We denote this type of reward as continuous. It is natural to compare this continuous reward with a discrete reward, where the compare function outputs 1 if the predicted and oracle patches exactly match each other, and 0 otherwise. We trained a variant of Llama3-SWE-RL with the discrete reward function, using the same training setup as in the continuous reward case.\nAs shown in Figure 6, while the discrete and continuous reward functions lead to similar format accuracy, the continuous reward is more effective in enhancing the repair performance. From the training dynamics, we can see that discrete rewards grow slower than continuous rewards. Additionally, the average discrete reward remains approximately zero upon the completion of training, meaning it struggles to obtain patches exactly matching the oracles. This is because real-world patches are highly diverse and often cannot be easily matched. The continuous reward function better captures partial correctness and incremental improvements, allowing the model to learn more nuanced and effective repair strategies."}, {"title": "4 Related work", "content": null}, {"title": "4.1 Language models for software engineering", "content": "Large language models (LLMs), trained with billions to trillions of code tokens, have demonstrated outstanding performance in a wide range of coding tasks, including code generation, code optimization , program repair , and software testing. Initially, researchers primarily focused on single-shot code generation tasks, such as function-level , class-level , and repository-level code completion. However, with the rapid development of LLMs, the performance on many popular single-shot code generation benchmarks like HumanEval , MBPP , and EvalPlus has become saturated. Since the development of SWE-bench , which requires solving real-world GitHub issues, researchers start to work on improving LLMs' real-world issue-solving capability and have designed various scaffolds for SWE-bench. Two general types are (1) agentic scaffolds , where an LLM drives the decision-making process based on its past actions and observations through tool-based interaction with the environment; and (2) pipeline-based scaffolds , where an LLM goes through human-defined stages to solve a given issue. Generally, agentic methods are more general but require strong instruction-following and capable LLMs to drive the autonomous process, and can be computationally intensive due to multi-round interactions. In contrast, pipeline-based approaches are more specialized but efficient, with a focus on LLMs' pure code editing capability. Therefore, we designed our minimalist pipeline-based scaffold, Agentless Mini, to focus on the enhancements of Llama3-SWE-RL's core code editing capabiltiy."}, {"title": "4.2 Training software agents", "content": "While existing scaffolds have successfully leveraged proprietary language models to tackle real-world software engineering tasks, open models typically yield subpar results in these settings. Moreover, the most effective approach to enhancing real-world software engineering capabilities through training remains unclear. Recently, researchers have begun exploring the possibility of training open LLMs specifically for software engineering tasks, aiming to improve performance on benchmarks such as SWE-bench. For instance, Lingma-SWE-GPT introduces 7B and 72B model variants that build on top of Qwen2.5-Coder-7B and Qwen2.5-72B-Instruct , using an iterative development-process-centric approach. SWE-Gym presents the first open training environment for software engineering agents, significantly improving the performance of Qwen2.5-Coder's 7B and 32B variants on SWE-bench. More recently, SWE-Fixer finetunes the Qwen2.5 base series, resulting in a 7B code retriever and a 72B code editor focused on efficient issue resolution, achieving notable best@1 improvements. Notably, all these works incorporate distilled samples from either GPT-40 or Claude-3.5-Sonnet in their training data and are built upon Qwen2.5 models. Their training objectives are all based on supervised finetuning. On the contrary, Llama3-SWE-RL is based on Llama 3 and trained through reinforcement learning (RL) using SWE-RL. The seed dataset for RL is sourced exclusively from publicly available repositories, allowing Llama3-SWE-RL to self-improve its issue-solving capabilities through the RL inscentive. Remarkably, Llama3-SWE-RL achieves the best performance among these models with a 41.0% solve rate on SWE-bench Verified , demonstrating for the first time that LLMs can already effectively address real-world issues through RL on real-world software artifacts."}, {"title": "5 Conclusion", "content": "We introduce SWE-RL, the first reinforcement learning (RL) approach to improve language models (LLMs) on software engineering tasks using software evolution data (e.g., PRs) and rule-based rewards. The resulting model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified, a human-verified collection of"}, {"title": "Limitations", "content": "Despite the promising results, our approach has several limitations. First, our reward implementation compares the sequence similarity between the predicted and oracle patch rather than their semantic equivalence. This may prevent the policy LLM from exploring alternative, functional equivalent solutions. Additionally, in Agentless Mini, the localization process is simplified to mapping repository structures to file paths, which lacks comprehensive context. Moreover, as a pipeline-based approach, Agentless Mini divides all steps into distinct inference stages. This \"external structure\" prevents the model from learning through interaction feedback and hinders its ability to consider the entire problem holistically. Furthermore, our approach requires a substantial sampling budget to achieve optimal results, which may be impractical for projects with high execution costs."}, {"title": "Future work", "content": "In the future, we plan to address these limitations by integrating agentic reinforcement learning into our framework. This will enable the model to independently learn localization and repair strategies without relying on external structures. Additionally, we will incorporate execution to allow the model to interact directly with the repository environment. We will also focus on improving the sample efficiency during inference, with the goal of achieving equal or better performance with fewer samples. Ultimately, we aim to enhance the practicality of SWE-RL, paving the way for more powerful, reliable, and fully open-source solutions for active GitHub issue resolution."}, {"title": "A Agentless Mini", "content": "In addition to a model proficient in code editing, effectively tackling software engineering tasks, such as those found in SWE-bench, also requires a robust scaffold. Agentless is one of the state-of-the-art scaffolds for SWE-bench at the time of writing. Building upon Agentless with various simplifications and enhancements, we developed Agentless Mini, a framework that prioritizes straightforward component decomposition, parallelization, and scalability. With Agentless Mini, each step's inference or execution compute can be independently scaled to enhance SWE-bench performance. In Figure 7, we present a detailed illustration of Agentless Mini's working principles. The following paragraphs will elaborate on each step and highlight the differences from Agentless.\nLocalization and repair. For localization, we employ a prompting-based approach that enables the model to predict relevant file paths based on a given issue and the repository's structure. Unlike Agentless, which involves two additional detailed steps to identify related elements and files, as well as a separate embedding model, Agentless Mini simplifies the process. It generates multiple samples of potentially problematic files from the model and consolidates them into unique sets for repair.\nDuring the repair phase, the LLM is conditioned on the full content of the files to predict search/replace edits. We generate multiple repair samples from different location sets, ensuring a comprehensive exploration of the patch search space.\nReproduction tests generation and selection. Agentless samples reproduction tests for patch selection. Initially, multiple reproduction tests are generated based on an issue description, and one majority sample is selected after filtering. These tests must have distinct logic to output \"Issue reproduced\" and \"Issue resolved\" when the issue is reproduced or resolved, respectively. They are filtered based on whether they correctly output \"Issue reproduced\" when executed in the original codebase. Agentless Mini enhances this pipeline with two key improvements. First, instead of relying solely on the issue description, the model retrieves a relevant test file to guide test generation. Additionally, rather than selecting just one majority sample, Agentless Mini allows for the selection of multiple top test samples based on voting results. In our evaluation, using more test samples has proven beneficial for reranking (\u00a73.4).\nRegression tests selection. We select regression tests in the same manner as Agentless. Initially, we gather all passing tests for each issue by executing the code before any modifications. This step does not require model inference and needs to be performed only once. Subsequently, an additional, optional inference step is conducted to filter out passing tests that are supposed to fail after the issue is fixed. The rest of the tests will be marked as regression tests.\nReranking. Agentless Mini utilizes both regression and reproduction tests for reranking. For each issue, every"}, {"title": "B Synthesizing supervised-finetuning data", "content": "Figure 8 shows our method of generating synthetic supervised-finetuning (SFT) data. The data generation pipeline is inspired by Magicoder, where the OSS-Instruct technique generates high-quality code instruction data from open-source seed snippets. We apply a similar methodology to generate both fault localization and code editing data using high-quality PR seeds.\nCollecting high-quality seed PRs. To begin with, we extract high-quality PR seeds from the raw dataset we collected, as detailed in \u00a72.1. These seeds are chosen based on specific heuristics. For example, a PR instance should include at least one linked issue, the issue should describe a bug fix request, and the code changes should involve programming files.\nLocalization and editing data synthesis. We adopt Llama-3.3-70B-Instruct for data synthesis. For localization data, we prompt the model with the issue description, repository structure, and the paths of edited and relevant files as hints. We then ask the model to identify the relevant files for modification or review by generating a thought process, followed by a prioritized list of file paths. During filtering, we ensure that the model's response includes all files that are genuinely edited in the PR, and these files should be prioritized in the ranking.\nSimilarly, in terms of code editing, we synthesize code edits for a given issue, in search/replace format, by providing the ground-truth PR and patch as guidance to Llama-3.3-70B-Instruct. During the filtering process, we ensure that all search/replace blocks adhere to the correct format and that all search paths and blocks can be accurately matched within the input files' context.\nSFT baseline. As discussed in \u00a73.1, we meticulously constructed Llama3-SWE-SFT-70B as a strong SFT baseline. This SFT baseline is trained on a 16k context window for 2B tokens, where the training data consists of a mix of the aforementioned synthetic localization and editing data, as well as coding and general SFT datasets from Llama 3."}, {"title": "C Midtraining on large-scale PR data", "content": "In addition to our main technique, SWE-RL, we are exploring orthogonal ways to improve language models (LLMs) on real-world software engineering. In this section, we explain how we can directly utilize the raw PR collection through continued pretraining (midtraining) to empower small LLMs, such as Llama-3.1-8B, to achieve a high resolve rate (31.0%) on SWE-bench Verified.\nData packing design. In pretraining, documents are often concatenated into a long sequence and split by the context window, avoiding padding and boosting efficiency. However, Ding et al. highlights"}, {"title": "Stable training and annealing", "content": "We divide the midtraining process into two stages: a stable training stage and an annealing stage, as outlined in MiniCPM and Llama 3. We utilize a trapezoidal learning rate scheduler, also known as Warmup-Stable-Decay. In this approach, the learning rate remains constant during the stable training stage after an initial linear warmup, followed by a linear decay during the annealing stage. During the stable training stage, we exclusively use midtraining data. In the annealing stage, we incorporate additional datasets, including the synthetic SFT data (Appendix B) for localization and editing, as well as some coding data and general-purpose dialogs from Llama 3."}]}