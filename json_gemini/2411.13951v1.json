{"title": "A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series", "authors": ["Lucas Correia", "Jan-Christoph Goos", "Thomas B\u00e4ck", "Anna V. Kononova"], "abstract": "Benchmarking anomaly detection approaches for multivariate time series is challenging due to the lack of high-quality datasets. Current publicly available datasets are too small, not diverse and feature trivial anomalies, which hinders measurable progress in this research area. We propose a solution: a diverse, extensive, and non-trivial dataset generated via state-of-the-art simulation tools that reflects realistic behaviour of an automotive powertrain, including its multivariate, dynamic and variable-state properties. To cater for both unsupervised and semi-supervised anomaly detection settings, as well as time series generation and forecasting, we make different versions of the dataset available, where training and test subsets are offered in contaminated and clean versions, depending on the task. We also provide baseline results from a small selection of approaches based on deterministic and variational autoencoders, as well as a non-parametric approach. As expected, the baseline experimentation shows that the approaches trained on the semi-supervised version of the dataset outperform their unsupervised counterparts, highlighting a need for approaches more robust to contaminated training data.", "sections": [{"title": "1 Introduction", "content": "As the digitisation of industrial processes progresses, more and more data is recorded. Ensuring this data is representative of the process is important, as downstream tasks like modelling or optimisation can be negatively impacted by incomplete or contaminated data. For tasks that require system behaviour modelling, data deviating from the norm is hence undesired, and we speak of anomalous behaviour. Recorded data manifests itself in many forms depending on the application and domain, one form being time series. Examples of real-world time series applications are diverse, ranging from cardiology [24] and server metrics monitoring [31] to water systems [22, 1, 38] and unmanned aerial vehicles [39]. Note that, we use time series and sequences synonymously throughout this paper."}, {"title": "", "content": "Time series are signals that represent a property or feature of a dynamic system as a function of time, usually sampled at a fixed rate. An arbitrary time series $S$ can be univariate, i.e. $S \\in \\mathbb{R}^T$, or multivariate, i.e. $S \\in \\mathbb{R}^{T\\times d}$, where $T$ refers to the number of discrete time steps and $d$ to the number of features in the time series. More specifically, univariate time series solely possess a temporal correlation, i.e. along the time axis, whereas multivariate time series can also contain correlation along the feature axis."}, {"title": "", "content": "Detecting anomalous behaviour in time series is referred to time series anomaly detection, which can be split into two main areas: continuous- and discrete-sequence [10], where the former is the most common type present in public datasets. It is defined as detecting anomalies in a process that runs for a continuous time period without breaks. Typically, the test subset $D_{\\text{test}}$ in the dataset $D$ in a continuous-sequence problem consists of a singular multivariate time series composed of multiple nominal and anomalous sub-sequences, i.e. $D_{\\text{test}} = \\{S_1\\}$. In this work, we use nominal as a synonym for normal or anomaly-free to avoid confusion with Gaussian distributions. Discrete-sequence anomaly detection, in contrast, is defined as detecting anomalies in $N$ chunks of processes that happen independently of each other, such as automotive test benches, where several tests may occur sequentially but are not temporally contiguous and hence provide a time series for each test, i.e. $D_{\\text{test}} = \\{S_1, ..., S_n, ..., S_N\\}$. Here, the testees, i.e. the test subjects, are not monitored over a continuous period of time, but are instead monitored solely during each process chunk. Automotive testing is not the only use for discrete-sequence anomaly detection, however. Another discrete-sequence problem could also include the analysis of the flight behaviour of an aeroplane, where the time while it is docked is irrelevant and may not be recorded. Therefore, datasets for discrete-sequence anomaly detection consist of several nominal and anomalous time series, where a given anomalous time series may be entirely anomalous or only partly."}, {"title": "", "content": "Depending on the system, the time series data may also feature variable states, meaning the recorded signals appear slightly different if certain external conditions change but are still considered nominal. One example of a variable-state system is a battery, where the voltage response to current changes depending on states like the battery temperature and the battery state of charge (SoC). A problem involving such a system requires the distinction between behaviour changes due to different states and behaviour changes due to an anomaly, further complicating detection."}, {"title": "", "content": "In addition to that, detecting anomalous behaviour in a timely manner is also advantageous because the source of anomalous behaviour may bring about damage to said system. Such problems where the detection delay plays a role are referred to as online time series anomaly detection."}, {"title": "", "content": "Analogous to types of learning, there is supervised, semi-supervised, and unsupervised anomaly detection. Supervised anomaly detection is essentially imbalanced binary time series classification and is only rarely found in the literature. This is most likely because, in real-world use cases, possible anomaly types are rarely known a priori. In addition to that, labelling data is expensive, which is why unsupervised and semi-supervised anomaly detection are more relevant in both literature and the real world. Unsupervised anomaly detection is independent of any labels, i.e. any available data for model training contains both anomalous and nominal time series, and it is not known which is which [7]. In contrast, semi-supervised anomaly detection can be considered a more relaxed setting, where anomalous time series are absent from the training subset [7]. In the real world, semi-supervised problems still require some labelling to ensure an entirely nominal training subset, which is not always given."}, {"title": "", "content": "Recently, time series anomaly detection has mostly been attempted using deep learning, with simpler statistical methods remaining largely unexplored. Several researchers [36, 3, 28] have hence raised concerns on whether deep learning-based methods are warranted for the complexity of publicly available datasets and even challenge any anomaly detection performance claims made because such datasets are deemed as flawed [36]. The lack of a standardised procedure to benchmark approaches on a high-quality dataset is considered a fundamental issue, which prevents any tangible and objective advances in the field."}, {"title": "", "content": "Our contribution is a novel, non-trivial, and high-quality dataset consisting of multivariate time series for online anomaly detection, named the Powertrain Anomaly Time series benchmark (PATH) dataset. While primarily aimed at unsupervised anomaly detection, we provide versions for semi-supervised anomaly detection and time series generation and forecasting as well. Despite the data being generated using simulation, the electric vehicle simulation model is strongly motivated by the real world and is therefore complex and variable-state."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Publicly Available Datasets", "content": "Over the last few years, five benchmark datasets have emerged as by far the most popular, with at least one of them being cited in the vast majority of publications on multivariate time series anomaly detection. A summary of the properties of these datasets is shown in Table 1."}, {"title": "", "content": "The MSL [19], SMAP [19], and SMD [31] have already been thoroughly analysed by Wu and Keogh [36], who point out several issues with the datasets. The first issue observed in the datasets is triviality, defined by being solvable using so-called one-line code, such as the moving standard deviation over a subset of the dataset features. Moreover, all of them suffer from what Wu and Keogh [36] have called unrealistic anomaly density, meaning that they have sub-sequences with a very high anomaly share and hence do not match the assumption that anomalies are rare events. In addition to that, Wu and Keogh [36] suspect possible mislabelling present in the MSL dataset. They base their suspicion on the fact that the dataset contains sub-sequences with static behaviour in an evidently dynamic channel, which is labelled as nominal. Many of the issues pointed out by Wu and Keogh [36] can also be extended to the SWaT and WADI datasets, as discussed by Correia et al. [10]."}, {"title": "", "content": "Certain real-world applications like automotive testing present complexities previously unseen in public datasets. As outlined by Correia et al. [11], such applications comprise much more diverse discrete-sequence datasets, owed to the presence of both highly dynamic and mostly static features, as well as variable states. The presence of variable states leads to features exhibiting a different pattern depending on the time it is observed. In the context of automotive testing, an example of such a feature would be the state of charge of a battery, which discharges with time and hence shows different behaviour for the same test done twice in a row."}, {"title": "", "content": "Hence, we construct a new high-quality dataset that is non-trivial and reflects real-world complexity."}, {"title": "2.2 Doubts Regarding Applicability of Deep Learning", "content": "Recently, there has been growing doubt on whether deep learning (DL) algorithms are definitively the better choice for time series anomaly detection. For the purpose of this publication, classical methods refer to all approaches not based on DL, including non-parametric and statistical approaches, as well as simpler machine learning methods like clustering."}, {"title": "", "content": "Wu and Keogh [36] claim that the superiority of DL in anomaly detection is assumed to be a given, despite a lack of clear evidence for the need for DL. They stress that existing classical methods should be considered, given their generally simpler and faster nature."}, {"title": "", "content": "To investigate the comparative performance of classical methods and DL-based methods, Audibert et al. [3] analyse a variety of different models on five of the most popular benchmark datasets, shown in Table 1. They conclude that, across the datasets considered, there is no algorithm that dominates all the other ones, arguing that there is no reason to omit classical methods from benchmarking."}, {"title": "", "content": "Rewicki et al. [28] also conduct a comparative study of classical and DL-based methods, though on the UCR Anomaly Archive benchmark proposed by Wu and Keogh [36], which exclusively contains univariate time series and therefore lacks correlations between channels present in multivariate time series. They conclude that classical methods perform better than their DL counterparts, although this is to be expected given the simpler, univariate nature of the dataset."}, {"title": "", "content": "While the findings and doubts of the above-mentioned are valid, they are limited by the lack of large, high-quality multivariate datasets. In this paper, we purposefully include results from a state-of-the-art classical method to find out whether doubts on DL are still justified for extensive and complex real-world datasets. See Section 4 for results and discussion."}, {"title": "3 Proposed Dataset", "content": ""}, {"title": "3.1 Simulation Model", "content": "To create an extensive and diverse dataset, we propose to use a physically-inspired model from which we can generate data using simulation. MathWorks offers reference models for a variety of dynamic systems, one of which is the full electric vehicle (FEV) model [5] from the powertrain block set in Simulink. This choice is based on our familiarity with the domain, as generating data blindly without any background may lead to systematic errors. The FEV model offered by MathWorks consists of six main subsystems: the drive cycle block, the driver block, the environment block, the controllers block and the vehicle block. The topology of the FEV model is illustrated in Figure 1."}, {"title": "", "content": "To represent system behaviour, $d_p = 16$ signals are chosen to be logged during simulation and are summarised in Table 2. We chose these signals based on domain knowledge, with the goal of picking the features that are most representative of powertrain behaviour."}, {"title": "", "content": "The drive cycle block of the FEV model defines the target vehicle speed and features a series of real-world drive cycles, i.e. profiles depicting the target vehicle speed as a function of time. From the list of speed profiles available in the original FEV model [5], a subset of them is eliminated due to their unrealistic nature, e.g. high linearity or duplicity, as many cycles are present as sub-sequences in others. Our analysis shows that, for example, the presence of the FTP72 drive cycle within FTP75 or the presence of the LA92Short drive cycle within LA92. In addition to that, drive cycles aimed at heavy vehicles, like trucks or buses, are also eliminated. The resulting subset of drive cycles chosen for simulation contains 33 different speed profiles of varying length, shown in Table 3, along with their lengths in seconds. Some drive cycles may be designed for specific types of powertrains such as diesel ones, but given that they only represent vehicle speed profiles, there is no reason why they cannot be driven by a vehicle with an electric powertrain, like the one modelled in this case."}, {"title": "", "content": "The driver block of the FEV model regulates the dynamic system to maintain the target speed. Its inputs are the target vehicle speed and the actual vehicle speed, and its outputs are the acceleration and deceleration control commands, index 12 and 13 in Table 2, respectively, which are fed into the controllers block of the FEV model. This block takes said accelerator and brake pedal commands stemming as well as vehicle states like actual vehicle speed, electric motor speed and battery signals to calculate request signals for the powertrain, like the required electric motor torque and the brake signal, as well as battery management system signals like the battery SoC, index 5 in Table 2. Electric vehicles are capable of regenerative braking, meaning, the electric motor is used to decelerate the vehicle by acting as a generator, thereby charging the battery if it is not already fully charged."}, {"title": "", "content": "Following is the vehicle block of the FEV model, which outputs how the vehicle reacts to any inputs and contains the electric plant subsystem and the drivetrain subsystem. Both take inputs from the controllers block, including the battery SoC, and the environment block, as well as from each other, as shown in Figure 2. The electric plant subsystem outputs the electric motor torque, the battery current and power, the battery temperature and the cooling pump and refrigerator powers, which correspond to indices 2, 6, 7, 14, 15 and 16 in Table 2, respectively. The electric motor torque is input into the drivetrain subsystem, which in turn outputs the electric motor speed, and front and rear axle torques, forces and speeds, corresponding to indices 1, 3, 4, 8, 9, 10, 11 in Table 2, respectively. The motor speed is also fed back into the electric plant model, completing the control loop. Both subsystems also contain further subsystems within them which uncover the causal relationships between their respective signals, and diving as deep as the lowest abstraction level of the model is outside the scope of this paper. Readers interested in more detail can refer to the Simulink model available in the provided repository. By default, the battery model features a static temperature model, however, to increase system complexity, a dynamic temperature model is added to the FEV model. The model used is the EV Battery Cooling System [15], also from MathWorks."}, {"title": "", "content": "The environment block of the FEV model dictates environmental conditions that affect the longitudinal dynamics of the FEV model. Parameters like atmospheric pressure, wind speed, road grade and coefficient of friction can be set within this subsystem."}, {"title": "", "content": "By default, the signals are logged at a sampling frequency of 10 Hz and the solver used is the differential algebraic equations' solver for Simscape (daessc). Physical simulations may encounter numerical under- and overflow, which slow down simulations drastically. To avoid this, a timeout counter of one hour is set in place to skip the current simulation if triggered. Simulation time depends on the length of the drive cycle, however, for the computer hardware used simulations never take longer than 20 minutes, and hence one hour is considered sufficient for problem-free simulations."}, {"title": "3.2 Dataset Generation", "content": "To generate a dataset that is not only extensive but also diverse, 100 simulations have been undertaken for each of the 33 drive cycles, each with random initial battery temperatures and battery SoCs. At this stage, all model properties have been left as default, and hence all simulation results have been considered nominal. For each simulation, the two states (battery temperatures and battery SoCs) have been sampled from uniform distributions $\\mathcal{U}(10\u00b0C, 30 \u00b0C)$ and $\\mathcal{U}(10\\%, 100\\%)$, respectively, to ensure no bias is introduced. Sampling from uniform distributions also eliminates and reduces the effectiveness of simple threshold and control chart methods because the battery temperature and state of charge, but also, by extension, other channels, exhibit nominal but high deviation from the average behaviour. As a precautionary measure, drive cycles with a minimum SoC value lower than 5% have been removed, as very low values have been observed to result in abnormal behaviour. After simulation, $N_n = 3273$ highly diverse and unique nominal time series have been collected. For illustration purposes, a nominal time series is plotted in Figure 3."}, {"title": "", "content": "For the generation of anomalous time series, six types of anomalies have been considered. Some types can occur as both sub-sequence anomalies and sequence anomalies [10], while others only in sequence anomaly form, due to simulation model limitations. To better distinguish the two anomaly forms, we refer to sequence anomalies as whole-sequence anomalies henceforth. The distribution of sub-sequence and whole-sequence anomalies across the different anomaly types is shown in Table 4. Anomalies are caused by changing certain model properties prior to simulation, ensuring that any observed anomalous behaviour results from simulation rather than manual tampering of the data, like in the UCR dataset [12], which eliminates any bias. We ran all simulations with a fixed seed of 1."}, {"title": "", "content": "For the first kind of anomaly, we turn the regenerative braking off, which leads to visibly different motor and axle torques, as well as battery current and power, as these can no longer assume negative values. When regenerative braking is off, the battery SoC now has an exclusively negative gradient as it is no longer recharged via regenerative braking, and hence it decreases at a faster rate. The brake pedal is also used more to compensate for the missing braking motor torque. For each of the cycles, this anomaly type is simulated in two different ways: without regenerative braking from the beginning and from a random point in time within the drive cycle. This random point in time is sampled from a uniform distribution $\\mathcal{U}(0.2T, 0.8T)$, where $T$ denotes the temporal length of the drive cycle in question, see Table 3. This statistical distribution is used for all sub-sequence anomaly types. One of the anomalous time series for the CADC130 drive cycle and its control counterpart are plotted in Figure 4."}, {"title": "", "content": "In the case of the next anomaly type, we introduce a headwind of 5 ms\u207b\u00b9 to the model. This headwind acts as a force on the frontal area of the vehicle and needs to be overcome to maintain the target vehicle speed by using the accelerator pedal more than the norm, which leads to higher motor and axle torques and therefore axle forces. The higher motor torque requires a higher battery current and power, which also causes accelerated discharging. Like previously, this anomaly type is simulated for each drive cycle, both from the beginning and from a random point in time within the cycle. One of the anomalous time series for the CLTCPassenger drive cycle and its control counterpart are plotted in Figure 5."}, {"title": "", "content": "Following that, we reduce the displacement of the cooling pump by 10% to simulate another anomaly type. Evidently, this change leads to a higher battery temperature as the cooling capacity is reduced. This reduction is also visible in the pump power. Like with the previous two anomaly types, this anomaly type can start from the beginning and from a random point in time within the cycle. One of the anomalous time series for the CUEDCDieselME drive cycle and its control counterpart are plotted in Figure 6."}, {"title": "", "content": "For the next anomaly type, we reduce the requested motor torque value output by the powertrain control module by 10%. As a response to the change, the driver model requests a higher acceleration pedal value and consequently a different brake pedal values as well. This anomaly type can also start from the beginning and from a random point in time within the cycle. One of the anomalous time series for the FTP75 drive cycle and its control counterpart are plotted in Figure 7."}, {"title": "", "content": "In the next case, we increase the loaded wheel diameter by 10% which, for the same target vehicle speed, leads to a lower motor and axle angular velocity. Furthermore, a larger wheel diameter leads to higher motor and axle torques, which are achieved using higher accelerator and brake pedal values. Here, the wheel diameter also has an effect on the battery temperature, which, depending on its absolute magnitude, may also affect the cooling system. Due to model limitations, this anomaly can only be simulated for whole-sequence anomalies. One of the anomalous time series for the HUDDS drive cycle and its control counterpart are plotted in Figure 8."}, {"title": "", "content": "The last anomaly is recorded after increasing the driver response time by a factor of 4. This is one of the more subtle anomalies types, but manifests itself in all channels, except for the battery temperature and cooling. Like for the wheel diameter anomaly, this anomaly can only be simulated for whole-sequence anomalies. One of the anomalous time series for the LA92 drive cycle and its control counterpart are plotted in Figure 9."}, {"title": "", "content": "To ensure that the different anomaly types actually lead to anomalous behaviour, we run control simulations with the same initial battery states but with otherwise nominal model properties. Given the uniform distribution from which the battery temperature is sampled from, half of the simulated anomaly types start with a battery temperature below 20\u00b0C. In these cases, the battery will naturally heat up as it is being used and hence the cooling system does not play a role. Therefore, in the case of the reduced cooling pump displacement, often no anomalous behaviour can be observed because the simulated anomaly is identical with the corresponding control simulation. For these cases, the simulated anomaly is discarded."}, {"title": "", "content": "Finally, this results in $N_a = 284$ successful anomalous simulations, where $N_a = N_{ss} + N_{ws}$. Hence, the entire dataset $D$ consists of $N_n+N_{ss}+N_{ws} = 3557$ unique (nominal and anomalous) multivariate time series, with an anomalous sequence ratio of $284/3557 \\approx 8\\%$. $D$ is then shuffled and divided into three separate folds for cross-validation, which corresponds to 2/3 and 1/3 split training and test subsets, respectively. Formally, the training subset $D_{\\text{train}} = \\{S_1, ..., S_m, ..., S_M\\}$ then consists of $M = 2371$ multivariate time series on average, where $S_m \\in \\mathbb{R}^{T_m\\times d_D}$, where $T_m$ is the number of time steps in sequence $S_m$. Likewise, the test subset $D_{\\text{test}} = \\{S_1, ..., S_n, ..., S_N\\}$ consists of $N = 1186$ multivariate time series on average, where $S_n \\in \\mathbb{R}^{T_n\\times d_D}$, where $T_n$ is the number of time steps in sequence $S_n$. For benchmarking purposes, we suggest the users use the prescribed training and test split to ensure comparable results."}, {"title": "", "content": "To add further complexity and to reflect real-world properties, we undertake some post-processing. First, we trim the beginning of each time series in $D$ by random amounts so that time series representing the same drive cycle are rarely in sync. The amount by which a given time series is trimmed is sampled from uniform distribution $\\mathcal{U}(0, 0.1T)$. This artefact can happen in the real world and means that, for the same drive cycle, any given time step is not comparable across different sequences, eliminating the viability of simple statistical methods such as control charts. In addition to that, we add noise sampled from Gaussian distribution $\\mathcal{N}(0, 0.01\\sigma_D)$ to further move the obtained data towards the real world, where $\\sigma_D$ is the feature-wise standard deviation of the dataset."}, {"title": "3.3 Usability of the Dataset", "content": "Clearly, both $D_{\\text{train}}$ and $D_{\\text{test}}$, as specified previously, contain nominal and anomalous time series, though in an unsupervised setting the labels for $D_{\\text{train}}$ should be disregarded. This is because the dataset is aimed at unsupervised time series anomaly detection, which requires approaches especially robust to contaminated training data."}, {"title": "", "content": "We believe the underlying properties of the dataset can be useful in other research areas too. The same $D_{\\text{train}}$ and $D_{\\text{test}}$ subsets can also be used for imbalanced time series classification if the labels are considered. Additionally, we provide a number of different subset variations for other tasks. For semi-supervised anomaly detection, we provide a clean $D_{\\text{train}}$ with on average $M = 2182$ nominal time series and the same labelled $D_{\\text{test}}$ in the dataset, where clean refers to the absence of anomalous sequences in the subset. Furthermore, for time series forecasting or generation, we supply clean versions of both $D_{\\text{train}}$ and $D_{\\text{test}}$, where $M = 2182$ and $N = 1091$ on average, respectively. Despite being targeted at online time series anomaly detection, the PATH dataset can just as well be used in offline time series anomaly detection."}, {"title": "4 Baseline Results on the Dataset", "content": ""}, {"title": "4.1 Methodology", "content": "The evaluation metrics used to quantify anomaly detection performance by Correia et al. [11] are adopted, as they provide a parameter-free way to quantify online anomaly detection performance in an interpretable way."}, {"title": "", "content": "In this work, we consider OmniAnomaly [31], TCN-AE [34], SISVAE [21], LW-VAE [16], TSADIS [32], and TeVAE [11] when conducting experiments. The hyperparameters for each approach are set as specified in the respective publication, though early stopping is applied to all that require a training procedure. Early stopping is parameterised such that the respective reconstruction error is monitored and training is stopped once validation loss has stopped decreasing for 250 epochs."}, {"title": "", "content": "The validation subset $D_{\\text{val}}$ is obtained by further splitting $D_{\\text{train}}$ and hence is also unlabelled. As future work may require a validation subset, it is left to the individual to extract it from the training subset if needed. The test subset $D_{\\text{test}}$ should be the same as the one provided to ensure comparable results."}, {"title": "", "content": "As mentioned in Section 3 the simulation signals are sampled at 10 Hz by default, however, to reduce the computational load in our experiments, we downsample the data to 2 Hz with a low-pass filter with a cut-off frequency of 1 Hz, as it is consistent with the Whittaker-Nyquist-Shannon theorem [30]. This downsampling procedure is considered as part of the approaches tested and is optional for any future work, which may alternatively use the raw time series data or perhaps even correlation matrices [33, 37]."}, {"title": "", "content": "To bring all channels to a common magnitude, the dataset features are z-score normalised."}, {"title": "", "content": "Finally, we segment the time series data into fixed-length sub-sequences, also referred to as windows. The rationale for using windows instead of full-length sequences is that the dynamics present in the time series data tend to occur quickly and only influence the data for a brief duration. Modelling entire variable-length sequences is possible, but it would lead to inefficient use of the model's learning capacity, as it would have to maintain information over unnecessarily long periods. By focusing on windows that are just long enough to capture the existing dynamics in the data, model training should be more effective. To determine the optimal window length at 2 Hz, especially to capture the slowest dynamics present in a signal, we perform an autocorrelation analysis [11] for each drive cycle and for every feature within those cycles, yielding a window size of 256 time steps. In the literature, the window size is often treated as a hyperparameter [16, 35, 32] or provided without reasoning [27, 8, 21, 34, 13]. However, it is not possible to tune hyperparameters outside a supervised setting, and therefore such methods might not be applicable in real-world settings. In contrast, finding a suitable window length using autocorrelation is completely unsupervised. TSADIS takes window size as a hyperparameter before calling, hence a window size of 256 time steps is also used. To map the individual windows back to continuous sequences, mean-type reverse-windowing [11] is used where applicable."}, {"title": "4.2 Reproducibility and Benchmarking Considerations", "content": "While perhaps sounding similar, repeatability, reproducibility, and replicability are defined differently according to the Association for Computing Machinery [2]."}, {"title": "", "content": "Several position papers [14, 26, 25, 18, 4, 20, 29] call for greater attention to be paid to reproducibility and replicability in computer science. Additionally, some conferences focus on reproduction, like the Machine Learning Reproducibility Challenge [23], or make specific calls for reproducibility and replicability papers, like the European Conference on Information Retrieval [6]. To enable future work to reproduce the results in this paper, we aim to be as transparent as possible by providing publicly available, clean and thoroughly commented source code for all experiments and the Simulink model under https://github.com/lcs-crr/PATH, as is suggested in literature [26, 25, 29]."}, {"title": "", "content": "The seed for random operations has an impact on model training, given that processes like sampling and weight initialisation rely on it. To increase robustness of the results and to eliminate the possibility of the results owing to a specific fold and seed combination rather from the characteristics of the model [29], all three folds are trained on seeds 1 through 5, yielding 15 different combinations. The final result is then given as the average of the 15 different combinations. As mentioned, TSADIS does not require training, and hence its results are simply the average over all three folds."}, {"title": "", "content": "In case future work aims to replicate the results of this paper, we encourage deviating from the experimental setup outlined in this paper [17], though, as Bartz-Beielstein et al. [4] point out, there is no definition for how different an experimental setup needs to be for results to be considered replicable. Using a different set of seeds, splitting the dataset into different folds, using different implementations of the approaches or even by using different software and hardware are some of the variables that could be changed in the setup, for example. In the case of replicability, the above-mentioned changes should not change the outcome [18]. Moreover, it is just as important that future work provides the same level of transparency regarding the experimental setup and documentation."}, {"title": "", "content": "It should be noted that the test subset $D_{\\text{test}}$ is often not available in the real world, so we strongly discourage approaches performing supervised threshold search using the labelled test data in $D_{\\text{test}}$."}, {"title": "", "content": "Furthermore, there is no way to stop future research from performing hyperparameter tuning using $D_{\\text{test}}$, hence any results obtained for this dataset should be considered as the theoretical maximum anomaly detection performance achievable by the model, not as a realistic anomaly detection performance observable in the real world."}, {"title": "", "content": "We run all simulations that generate the PATH dataset on a workstation equipped with an Intel Xeon Gold 6234 CPU running Windows 10 Enterprise LTSC version 21H2 with MATLAB version 23.2. The framework used for model training is TensorFlow 2.15.1 and TensorFlow Probability 0.23 on Python 3.10 on a workstation running Ubuntu 22.04.5 LTS, equipped with two Nvidia RTX A6000 GPUs. All work involving TSADIS is done in a separate environment with the latest compatible Python version of 3.9. Further information on library versions used can be found in the requirements.txt file in the repository."}, {"title": "4.3 Results and Discussion", "content": "To provide baseline results for the version of the PATH dataset for unsupervised anomaly detection, we test several approaches. The corresponding results are shown in Table 5."}, {"title": "", "content": "First, it is evident that there is a large gap between the unsupervised and theoretical best threshold results. The unsupervised threshold is a rudimentary estimation based on the unlabelled validation subset $D_{\\text{val}}$ [11"}]}