{"title": "Deep Learning and Data Augmentation for Detecting Self-Admitted Technical Debt", "authors": ["Edi Sutoyo", "Paris Avgeriou", "Andrea Capiluppi"], "abstract": "Self-Admitted Technical Debt (SATD) refers to circumstances where developers use textual artifacts to explain why the existing implementation is not optimal. Past research in detecting SATD has focused on either identifying SATD (classifying SATD items as SATD or not) or categorizing SATD (labeling instances as SATD that pertain to requirement, design, code, test debt, etc.). However, the performance of these approaches remains suboptimal, particularly for specific types of SATD, such as test and requirement debt, primarily due to extremely imbalanced datasets. To address these challenges, we build on earlier research by utilizing BiLSTM architecture for the binary identification of SATD and BERT architecture for categorizing different types of SATD. Despite their effectiveness, both architectures struggle with imbalanced data. Therefore, we employ a large language model data augmentation strategy to mitigate this issue. Furthermore, we introduce a two-step approach to identify and categorize SATD across various datasets derived from different artifacts. Our contributions include providing a balanced dataset for future SATD researchers and demonstrating that our approach significantly improves SATD identification and categorization performance compared to baseline methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Technical debt (TD) is a software development metaphor that represents the extra cost of change as a result of choosing a quick and limited solution over an optimal approach that would, however, take longer to implement [1], [2]. Like financial debt, TD has to be paid back with interest on higher future costs during maintenance and evolution [3].\nIn practice, TD is not always visible to all development project stakeholders. In most cases, it is only known to a small number of developers, particularly to those who wrote the code containing the debt. To resolve this issue, numerous researchers have proposed various techniques to detect TD and subsequently make it explicit. Most of these prior works have concentrated on detecting TD through static source code analysis [4], [5], [6].\nWhile static code analysis can effectively detect TD in the form of violations of coding rules, dependencies, or smells, another kind of TD is admitted by the developers themselves, known as self-admitted technical debt (SATD). SATD refers to circumstances where developers use code comments, issues, pull requests, or other textual artifacts to explain why the existing implementation is not optimal [7]. For example, developers frequently use terms, e.g., \u2018TODO' or 'fixme', when admitting the existence of TD in source code comments. The key benefit of SATD is that, unlike approaches utilizing static code analysis to detect proxies of TD (such as dependencies or smells), it represents TD per se as stated by developers who are familiar with the code [8]. Furthermore, the approaches grounded in source code analysis encounter significant challenges with false positives [9], wherein numerous source code components are identified as problematic despite not being perceived as such by developers. This is primarily because these methods solely depend on the structure of the source code for detecting code smells, disregarding crucial factors such as developer feedback, project domain, and contextual information surrounding the detection of code smells [10].\nThe detection of SATD in the literature has, in general, pursued two main directions:\n1) A binary classification (i.e., identification) of a software development artifact, into either \u2018SATD' or \u2018Not-SATD\u2019. This is the simplest classification, but it still lists all SATD items and allows the development team to understand how much SATD exists in the system [11].\n2) A multi-type classification (i.e., categorization) of artifacts using labels related to the software development activities (e.g., 'requirement debt', 'design debt', 'code debt') [12], [13], [14]. Recognizing the development activities where SATD occurs has a long-term advantage: each specific type of technical debt requires different repayment strategies [15], thus categorizing the specific types of SATD, and monitoring their evolution, facilitates their repayment. Furthermore, identifying specific types of SATD is essential for developers to understand the distribution of the different types better [16] as well as the effects of the different types on maintenance efficiency [17].\nThe most recent attempts at detecting SATD have included various software artifacts to label different types of SATD effectively [11], [18], [19], this has translated into a multi-dimensional problem, where specific artifacts are starting to be used in the detection of specific types of SATD. Recently, Li et al. [20] proposed an automatic SATD detection approach across multiple sources: source code comments, issue trackers, commit messages, and pull requests. However, the performance of such approach (i.e., F1-score) needs to be improved to categorize SATD more accurately; for example, the"}, {"title": "II. BACKGROUND", "content": "Previous studies have demonstrated that technical debt (TD), while impairing future maintenance and evolution, is ubiquitous and unavoidable in software development processes [23], [24]. Developers must spend additional time and effort building new features or fixing bugs to either pay off the debt (e.g., through refactoring) or pay the interest on it.\nResearchers have focused on detecting TD for over a decade, as detection is the first step in managing it [25]. Various approaches have been proposed since TD can lower software quality, pose long-term risks, and require to perform redesign and refactoring. Unlike TD detection methods which only focus on the source code, SATD detection has many advantages because SATD items are directly and intentionally recognized by developers, e.g., through code comments [10]. Furthermore, SATD represents the debt per se as it was introduced by the developer, in contrast to debt identified by static source code analysis, which is only a proxy [26].\nFor almost a decade, SATD detection from source code comments has rapidly evolved, whether it is the identification of SATD and Not-SATD (binary class) [10], [27], [28] or categorization of specific types of SATD (multi-class) [14], [17], [29], [30]. Furthermore, researchers have also been successful in identifying the existence of SATD from other artifacts, namely issue tracking systems, commit messages, and pull requests [11], [13], [18], [31].\nRecently, Li et al. [20] proposed an approach to categorize specific types of SATD (i.e., code/design, requirement, documentation, and test debt) by mining four different artifacts, namely source code comments (CC), issues section (IS), pull section (PS), and commit messages (CM). Their approach achieved an average F1-score of 0.611. Furthermore, they have stated that their approach still struggles to categorize test and requirement debt. In our work, we attempt to enhance performance by optimizing the categorization of both test and requirement debt, as well as the other SATD types.\nIn a recent literature review [32], 68 papers were analyzed concerning the detection of SATD. It was found that BiLSTM and BERT were the two deep learning architectures that achieved the highest F1-score values for identifying and categorizing SATD, respectively. Despite previous research indicating the superior performance of both architectures, they were found to struggle with imbalanced data. Therefore, in this study, we propose a two-step approach by leveraging the capabilities of BiLSTM and BERT, integrated with AugGPT data augmentation, to effectively address the challenge posed by imbalanced data for SATD identification and categorization.\nThe datasets used to produce those performance comparison results were mostly sourced from two main references: Maldonado et al. [10], and Guo et al. [27]. Their code comments datasets are summarized as follows: the first is a multi-class SATD dataset containing five specific types of SATD ('design debt', 'requirement debt', \u2018defect debt', 'documentation debt', and 'test debt') plus the 'Not-SATD' label, where instances were labeled as not containing any SATD. The imbalance of these labels is noticeable: for example, 'design debt' contains 2,703 instances, but the 'documentation debt' is only 54. The second dataset [27] contains two labels (SATD and Not-"}, {"title": "III. STUDY DESIGN", "content": "The objectives of this study are to identify SATD and categorize its types when considering various artifacts. The methodology shown in Figure 1 contains 6 parts:\n\u2022 (Part i Dataset) - We consider four artifacts to detect SATD: source code comments (CC), issues section (IS), commit messages (CM), and pull section (PS). Our approach can be extended to any data source that might potentially contain traces of SATD. See Subsection III-B.\n\u2022 (Part ii - Data Augmentation) We employ an LLM data augmentation strategy to address data imbalances. The specific label instances will be augmented to balance their examples and avoid skewness in the representation of the categories. Data augmentation is applied exclusively to the training set to prevent data leakage. See Subsection III-C.\n\u2022 (Part iii Preprocessing) We treat all software artifacts with text preprocessing and feature extraction. Text preprocessing is used to eliminate noise in the data. Meanwhile, feature extraction is utilized to enable an algorithm to learn from training data with pre-defined features before it is fed into a deep learning architecture. See Subsection III-D, and III-E.\n\u2022 (Part iv Identification) The first of our two-step approach is a binary classification of SATD: we identify whether the four sources of artifacts contain either SATD or Not-SATD instances. See Subsection III-F.\n\u2022 (Part v - Categorization) \u2013 The second step in our two-step approach only considers the instances that contain SATD, as identified in the previous step: by utilizing BERT, we categorize SATD types, namely code/design debt (C/D), documentation debt (DOC), test debt (TES), and requirement debt (REQ). See Subsection III-G.\n\u2022 (Part vi - Evaluation) \u2013 To evaluate the effectiveness of the proposed approach, we run experiments on a publicly available dataset provided by Li et al. [20]. The F1-score is employed to evaluate the performance of the proposed approach. We employ a stratified train-validation-test split for each of the artifacts, allocating 80% of the data for training, 10% for validation, and 10% for testing. See Subsection III-H.\nA. Research Questions\nOur work is based on three research questions (RQs):\nRQ1: How does the proposed approach impact the overall performance in the identification and categorization of SATD? Motivation: this RQ aims to determine whether the two-step approach can enhance the performance of baseline approaches from the literature.\nRQ2: How does augmenting datasets to address highly imbalanced data impact the overall performance?\nRQ3: What are the most indicative keywords that represent each source of artifacts and specific types of SATD? Motivation: detecting and categorizing SATD through highly specific keywords can be enhanced to lead to a more comprehensive understanding of SATD and its impact on software development. Since existing approaches have limitations in identifying SATD (i.e., low F1-scores), researchers can categorize SATD patterns and trends using keywords across different artifacts. In addition, the identified keywords can be used to improve the explainability of our model's prediction results.\nTo answer these RQs, we follow the approach presented in Fig. 1. Below, we will provide a more comprehensive elaboration on each of its steps.\nB. Training and testing datasets\nThe datasets from Maldonado et al. [10] and Guo et al. [27] indicate that instances can simply be identified as SATD and Not-SATD but can also be categorized into specific types of SATD. These two research datasets have been used so far a total of 34 times by other researchers to identify SATD and to evaluate machine learning algorithms, as reported in [32]. However, these datasets focus on SATD detection by only using source code comments.\nTo evaluate our approach, we utilize the dataset provided by Li et al. [20], derived from various artifacts (see Table I). This dataset includes 5,000 commit messages and 5,000 pull requests from 103 Apache open-source projects, as well as 4,200 issues from 7 open-source projects using two issue tracking systems (Jira and Google Monorail). Each instance was manually annotated to identify whether it was Not-SATD or SATD, and further categorized into specific types of SATD. Additionally, they analyzed 23.6M source code comments, 1.3M commit messages, 3.7M issue sections (including individual issue summaries, descriptions, or comments), and 1.7 million pull request sections (including summaries, descriptions, or comments) from 103 open-source projects to"}, {"title": "C. Data Augmentation", "content": "Based on our examination of the dataset in Table I, we observed significant imbalances in the distribution of specific SATD types. For instance, in the dataset derived from CC artifacts, DOC debt comprises only 54 rows, accounting for 1.99% compared to C/D debt and 0.09% compared to Not-SATD. Similar imbalances exist in datasets from issue trackers, pull requests, and commit messages.\nThe situation of extremely imbalanced data is a typical challenge that needs to be addressed when detecting SATD, particularly for categorizing the specific SATD types [28]. The issue of class imbalance can significantly affect how SATD instances are classified [17]. Furthermore, it is challenging for deep learning architectures to categorize correct SATD types since they learn semantic information from a limited number of data points. As a result, dealing with the class imbalance and increasing the number of data in the minor class is necessary.\nUndersampling and oversampling are common approaches to resolving imbalance issues [33], but both have shortcomings [34]. Undersampling reduces samples from large classes to match smaller classes, potentially deleting crucial information and limiting sample variety while decreasing training samples. Oversampling balances data by repeatedly selecting samples from small classes, which does not enhance sample variety and often leads to overfitting [35].\nData augmentation, another key method to address imbalance, involves generating new samples based on existing ones. This not only increases the number of samples in small classes but also enhances sample diversity. Traditional techniques include synonym replacement and random operations [36], while recent methods like back-translation and word vector interpolation have been explored [37], [38]. However, these approaches still struggle with maintaining accuracy and diversity and often require human annotation [36]. Therefore, in this study, we utilized AugGPT [39], based on ChatGPT-3.5, as a powerful data augmentation strategy to address the issue of imbalanced classes. AugGPT demonstrates high faithfulness and compactness in preserving accuracy compared to other augmentation methods [39]. This technique generates text by probabilistic sampling from a language's lexicon [40], effectively enhancing dataset diversity and addressing the challenges posed by imbalanced classes in SATD detection.\nIn this paper, AugGPT was employed to produce a supplementary training dataset with the primary objective of generating paraphrased text for each data point while ensuring the original meaning remained unchanged. Preserving the meaning is crucial to avoid potential mislabeling. According to Dai et al. [39], single-turn and multi-turn dialogues are two design prompts for the data augmentation process. This study formulated the prompt instruction through experimentation and trial-and-error, guided by the principles outlined in the multi-turn prompt dialogue as proposed by them [39].\nAs depicted in Fig. 2, we also added context (e.g., specifying \"commit message from GitHub\") and a persona (i.e., a programmer) to ensure the generated augmentation data closely resembled the original text [41]. This approach effectively augmented the dataset, enhancing its diversity and introducing linguistic variation."}, {"title": "D. Text Preprocessing", "content": "Developers often use their own style and preferences when adding comments to source code, issue trackers, commit messages, and pull requests, resulting in diverse text formats. These texts provide valuable information, such as reasons for code changes, progress updates on group efforts, or documentation of modifications [43]. Since machine learning and deep learning algorithms require structured text, preprocessing is essential to reduce data noise.\nThis study employed standard preprocessing procedures, including data cleansing, duplicate removal, lowercase conversion, tokenization, stop word removal, punctuation removal, and lemmatization. Additional steps included excluding short words (two letters or fewer), removing numbers, URLs, and non-ASCII characters, and eliminating extra white spaces."}, {"title": "E. Feature Extraction", "content": "Feature extraction using word embeddings refers to the process of utilizing pre-trained word embeddings to represent words in a text or document as numerical features that can be used for various machine learning tasks [44].\nInstead of training word embeddings from scratch, pre-trained embeddings like GloVe [45] and BERT [46] are often used to extract meaningful word representations. These embeddings predict a word's context based on its neighboring words or reconstruct the word from its context, encoding semantic information and enabling algorithms to understand and infer language. Therefore, this study uses GloVe embeddings for BiLSTM and BERT embeddings for BERT architectures.\nAfter data augmentation, text preprocessing, and feature extraction, we employ two architectures: BiLSTM (the Identification in Fig. 1) is used to identify SATD or Not-SATD artifacts (binary classification), while BERT (the Categorization) is utilized to categorize specific types of SATD (multi-class classification). After these steps, we will evaluate and compare how well each architecture performs."}, {"title": "F. SATD Identification with BiLSTM", "content": "According to the findings from a prior study performed to identify the optimal algorithms to identify SATD [32], BiLSTM [47] is the most effective deep learning architecture and has been routinely utilized by researchers for this purpose. Leveraging the results of past research as the first step of our proposed approach, we utilize the BiLSTM architecture to identify whether an item from a specific artifact should be considered as either SATD or Not-SATD.\nThe model's initial layer is an embedding layer, configured with a vocabulary size, an embedding dimension, and a pre-trained embedding matrix. The core architecture consists of multiple stacked BiLSTM layers. The first BiLSTM layer has 128 units and is accompanied by a Dropout layer with a rate of 0.3 to reduce overfitting. A Batch Normalization layer follows to stabilize and accelerate training by normalizing activations. Next, two additional BiLSTM layers are stacked, each with a Dropout layer at a 0.3 rate. The second BiLSTM layer has 64 units, while the third returns to 128 units. The final Bidirectional LSTM layer, with 128 units, consolidates the learned features. The training was monitored for validation loss and halted early when the minimum validation loss was achieved."}, {"title": "G. SATD Categorization with BERT", "content": "A pre-trained language model, specifically the BERT-base-uncased implemented in the Transformers library, was utilized for the purpose of categorizing SATD. BERT-base was selected based on its established superiority in categorizing SATD compared to other machine learning and deep learning models [32]. Additionally, the BERT-base demonstrates comparatively lower computational resource requirements in terms of time and memory when compared to the BERT-Large model.\nThe BERT-base-uncased model was pre-trained using a vast collection of English text with a self-supervised approach. The classifier consists of a sequential combination of layers: a linear layer mapping the BERT output to the hidden layer, a ReLU activation function to introduce non-linearity, and another linear layer mapping the hidden layer to the output layer. To optimize the pre-trained BERT-base-uncased model (768 hidden units, 12 layers, and 110M parameters) for categorizing SATD, we used CrossEntropyLoss as the loss function for the multi-class classification task. The model was trained using the AdamW optimizer, a modified version of the Adam optimizer that incorporates weight decay [48].\nTo further optimize our model's performance, we fine-tuned several hyperparameters. Specifically, we used a batch size of 32, determining the number of samples processed in a single iteration. The ReLU activation function was employed to enhance the model's learning and prediction capabilities. We set the learning rate to 5e-5, guiding the model on how quickly to adjust its internal parameters during training. Additionally, we selected an AdamW optimizer with an epsilon value of 1e-8 to ensure stability and efficiency in the optimization process. These fine-tunings collectively enhanced our model's performance.\nExperiments were conducted on all datasets, including CC, IS, PS, and CM artifacts. The deep learning algorithms were implemented using the PyTorch library and trained on NVIDIA Tesla V100 GPUs."}, {"title": "H. Model Evaluation", "content": "We use the F1-score ($2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$) to evaluate the performance of our approach because it is computed as the harmonic mean of precision and recall. This makes it more sensitive to extreme values than other metrics like the arithmetic mean, which can be helpful when dealing with imbalanced datasets."}, {"title": "IV. RESULTS", "content": "This section provides a detailed analysis of the outcomes from our proposed two-step approach. For RQ1, we utilize the BiLSTM and BERT architectures without data augmentation for SATD identification and categorization. To address RQ2, we apply the BiLSTM and BERT architectures with data augmentation techniques for these tasks. The classification focuses on the four SATD types presented by [20]. Finally, we elaborate on the most representative SATD keywords to address RQ3.\nA. RQ1: SATD identification and categorization without data augmentation (base)\nTo demonstrate the significance of our approach in identifying SATD and in categorizing various types of SATD as measured by F1-score, in this subsection, we will first discuss the results of BiLSTM and BERT without data augmentation.\nTo our knowledge, no previous studies have automated the identification of SATD (binary classification) on various artifacts. To establish baselines for comparison, we used studies that employed the datasets from [10], [27], which solely examine CC artifacts. We utilized and modified the source code provided by [17], [27], [28], [49] to enable its execution for identification and categorization across IS, PS, and CM artifacts. These modifications included adjusting the class numbers, standardizing the preprocessing steps, and standardizing the split ratio for training and testing. Therefore, to showcase the effectiveness of our approach, we compared it against several existing approaches: Natural Language Processing (NLP) [10], the pioneering SATD detection; and Matches task Annotation Tags (MAT) [27], a straightforward approach primarily applied to CC artifact. Additionally, we evaluated the performance of eXtreme Gradient Boosting+Synthetic Minority Oversampling Technique (XGBoost+SMOTE) [28], which incorporates oversampling techniques; eXtreme Gradient Boosting+Easy Data Augmentation (XGBoost+EDA) [17], which integrates data augmentation strategies; LightGBM [49], which has been optimized for handling sparse data; JSD-GAN [50], which employs deep learning data augmentation method, namely Generative Adversarial Network (GAN); and BiLSTM for SATD identification across all artifacts.\nIn terms of SATD categorization, our study compared the proposed approach with XGBoost+SMOTE [28], XGBoost+EDA [17], JSD-GAN [50], MT-Text-CNN [20], and BERT. XGBoost+SMOTE and XGBoost+EDA were chosen as baselines because both use data augmentation approaches and are capable of performing categorization tasks. Meanwhile, the JSD-GAN model represents the latest approach for SATD categorization.\nIn the identification task (see Table V), concerning CC artifacts, BiLSTM (base) achieves the second position relative to JSD-GAN, with respective scores of 0.875 and 0.910. Additionally, BiLSTM (baseline) exhibits superior performance compared to all other artifacts by attaining Macro average F1-scores of 0.748, 0.668, and 0.797 for IS, PS, and CM artifacts, respectively. These findings align with F1-score outcomes reported in studies utilizing datasets provided by [10] and [27].\nRegarding SATD categorization, the results presented in Table VI show that BERT outperformed XGBoost+SMOTE, XGBoost+EDA, JSD-GAN, and MT-Text-CNN in categorizing C/D, DOC, and TES debts (excluding commit messages). However, BERT did not achieve better results for REQ debt categorization. In terms of the Macro-averaged F1-score, BERT surpassed the baseline methods, except in the CM artifact. Specifically, BERT achieved Macro-averaged F1-score values of 0.656 (compared to 0.619 with MT-Text-CNN) for CC artifacts, 0.719 (0.453) for IS artifacts, 0.455 (0.441) for PS artifacts, and 0.448 (0.475) for CM artifacts.\nOverall, based on the findings presented in Tables V and VI, it appears that BiLSTM architecture demonstrates only a slight enhancement in the identification task, whereas BERT architecture exhibits notably superior performance, albeit not universally across all SATD types in terms of F1-score results."}, {"title": "B. RQ2: SATD identification and categorization with AugGPT", "content": "In the identification task, BiLSTM+AugGPT demonstrates superior performance compared to standard BiLSTM in terms of F1-score. As illustrated in Table V, BiLSTM+AugGPT achieves Macro-averaged F1-score values of 0.939 (compared to 0.875 with regular BiLSTM) for CC artifacts, 0.878 (0.748) for IS artifacts, 0.862 (0.668) for PS artifacts, and 0.880 (0.797) for CM artifacts. The Macro-averaged F1-score shows that BiLSTM+AugGPT not only improves SATD detection but also maintains a balanced performance across both classes.\nThe second step of our approach uses the BERT+AugGPT, so we compared the F1-score of the MT-Text-CNN [20] and other aforementioned baselines. This second step aims to properly categorize specific types of SATD (i.e., C/D, DOC, TES, and REQ).\nAs demonstrated in Table VI, based on Macro-averaged F1-score, the overall findings suggest that BERT+AugGPT exhibits a notably stronger performance in comparison to MT-Text-CNN [20] for SATD categorization across all sources of artifacts. More specifically, the BERT+AugGPT outperforms the XGBoost+SMOTE [28], XGBoost+EDA [17], JSD-GAN [50], MT-Text-CNN [20], and BERT in categorizing all the specific types of SATD in all datasets. It maintained high F1-scores consistently, with the highest Macro Avgs. for CC (0.882), IS (0.899), PS (0.876), and CM (0.847)."}, {"title": "C. RQ3: Keywords for SATD identification and categorization", "content": "Understanding how SATD keywords are used across different artifacts can reveal valuable insights into their similarities and differences. Keywords offer a lightweight method for identifying and categorizing SATD from various sources, aiding developers in comprehending technical debt (TD) [17]. For instance, keywords can generate reports that identify SATD types or highlight areas likely to contain SATD.\nThere are different methods for extracting keywords. Potdar et al. [7] manually summarized 62 common SATD keyword patterns for identifying SATD in source code comments. However, this manual approach is time-consuming and labor-intensive, making it challenging to encompass all possible SATD patterns.\nRecently, Li et al. [20] employed a backtracking approach to retrieve keywords by categorizing the most significant features using a trained Text-CNN. Their method requires a backtracking mechanism to map these features back into the architecture structure to discover keywords [51]. However, backtracking can incur high computational costs due to its tendency to explore numerous partial solutions before arriving at a comprehensive solution [52].\nConsequently, we employed a lightweight technique called KeyBERT [53] for keyword extraction and extraction-based summarization. KeyBERT utilizes BERT embeddings to identify keywords and keyphrases closely related to a given document. We used the original dataset provided by Li et al. [20] for keyword extraction, rather than the augmented dataset, to maintain data accuracy, avoid potential bias, and ensure the credibility of the extracted keywords. The search for SATD and Not-SATD examples in the artifacts is based on categorizing keywords strongly associated with the presence of SATD. Table VII summarizes the top SATD keywords extracted from the four artifacts. This table shows the most representative keywords identified by KeyBERT for various types of SATD in the artifacts. The complete list of keywords with similarity scores is available in the online repository [22].\nWe noticed that certain keywords (underlined in Table VII) were not uncovered by Li et al. [20]. These additional keywords seem to have been missed potentially due to the relatively low F1-score (0.611) achieved by their approach. Our approach can thus serve as a valuable supplement, automatically identifying SATD and highlighting keywords or phrases that aid in this identification. The top 10 keywords for each specific SATD type extracted using KeyBERT are listed in Table VIII and can be considered as patterns. Since C/D debt items outnumber other debt types, they are the most prevalent type of SATD across various sources. Consequently, the keywords associated with C/D debt predominantly overlap with the top keywords listed in Table VII."}, {"title": "V. DISCUSSION", "content": "In the next subsections, we discuss the implications of our findings for both categories of stakeholders.\nA. Implications for researchers and practitioners\nUtilizing deep learning approaches and data augmentation techniques like AugGPT for identifying and categorizing SATD has significant implications for developers and researchers. The effectiveness of identifying and categorizing SATD is influenced by both the data augmentation technique and the algorithms employed. This is evident when comparing methods such as XGBoost+SMOTE, XGBoost+EDA, JSD-GAN, MT-Text-CNN, and BERT. BERT excels in categorizing C/D and DOC debt, and TES debt (except in the CM artifact). Notably, the data augmentation technique has a greater impact, as seen in the superior results of BERT+AugGPT compared to BERT alone. Thus, alongside optimal deep learning architectures, prioritizing dataset quality is crucial for addressing data imbalances in SATD identification and categorization.\nImproved identification and categorization of SATD instances, as demonstrated by our approach, enable developers to pinpoint problem areas in the code base and fix them, enhancing overall software quality. Although our approach outperforms baselines and state-of-practice methods, we recommend that software practitioners provide clear and standardized annotations in each artifact to ensure appropriate label quality using the right keywords (see results of RQ3).\nIn software development, 'TODO' and 'fixme' are keywords commonly used within source code to indicate tasks or issues that need to be addressed later. These keywords act as reminders for developers, highlighting areas of the code that require further attention or work. As presented in Table VII, our experimentation identifies these two keywords as the most prevalent within CC artifacts. Unfortunately, there is a lack of standardization or universally agreed-upon keywords for other artifacts. Consequently, we strongly advocate that practitioners consider using keywords in the IS, PS, and CM artifacts when introducing short-term solutions (i.e., SATD) into a project due to unavoidable circumstances. Developers are encouraged to utilize clear keywords based on each artifact to facilitate the subsequent categorization of SATD items.\nAdditionally, SATD identification datasets commonly exhibit class imbalance, where Not SATD items significantly outnumber SATD items. Addressing this imbalance is crucial for improving performance. We recommend that researchers contribute to developing and utilizing various techniques, such as transfer learning and few-shot learning. Furthermore, collaboration with software engineering experts and domain specialists is essential for contextual understanding of SATD and optimizing identification models.\nFinally, the approach shown in Subsection III-C is general enough to be expanded outside the scope of SATD identification and categorization and utilized in other fields of Software Engineering research. Imbalanced datasets pose challenges for deep learning models, often resulting in strong performance for majority classes but weakness in minority classes. AugGPT, as demonstrated in our study, effectively augments minority class instances, diversifies the sample space, mitigates imbalance, and enhances overall model performance."}, {"title": "B. Threats to Validity", "content": "Construct validity. Threats to construct validity pertain to the extent to which operational measures accurately represent what is being investigated, following the research questions. In this study, we employ a widely used evaluation metric for imbalanced datasets, namely the F1-score, to evaluate the performance of our proposed method. We believe there is little threat to construct validity, as previous studies also use it to evaluate the performance [10], [18], [20], [54].\nReliability. The reliability threat may come from the datasets, namely the source code comments originally from Maldonado et al. [10] and the rest from Li et al. [20]. It is worth noting that the datasets were labeled manually, introducing the possibility of personal biases. However, it is encouraging to find that the results of manual classification performed by different individuals, as reported in the prior studies, demonstrate a high degree of consistency. The calculated Cohen's Kappa coefficients, which reached +0.81 and +0.74, further support this consistency and indicate that the data sets possess a reasonable level of reliability that can help mitigate concerns about potential inconsistencies caused by manual classification.\nExternal validity. While our experiments used publicly available datasets, the generalizability of our approach to other projects and programming languages remains uncertain, which may limit its broader applicability. However, we used diverse datasets from various open-source projects, covering different contributors, SLOC, comments, and SATD types. Since our method processes natural language text from code comments, issue trackers, pull requests, and commit messages, these artifacts remain relevant across different languages.\nFurther research is needed to validate our approach with a larger, more diverse dataset. Moreover, as our training data comes from open-source projects, there are limitations in generalizing results to industry projects, which may handle technical debt differently. While our findings may extend to similar open-source projects, SATD documentation practices in industry could vary."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This study aims to automatically identify and categorize SATD in software artifacts, such as source code comments, issues section, pull requests, and commit messages. Our approach employs BiLSTM for initial identification and BERT for subsequent categorization. Analysis of the datasets revealed a significant imbalance in SATD types, addressed through AugGPT for data augmentation, which markedly improves F1-score performance over baselines. Our method provides a robust framework for comprehensive SATD identification and categorization across varied sources and also offers insights into key indicative keywords for each artifact and SATD type.\nThere are several potential research topics for future research. The current study addresses class imbalance using a large language model (LLM)-based data augmentation strategy. Future research could explore alternative methods of dealing with imbalanced data in SATD detection, such as few-shot learning or transfer learning to further improve detection accuracy, particularly for rare SATD categories. Additionally, we encourage investigating the use of other LLMs, such as Gemma-7B, Vicuna-13B, Falcon 180B, and LLaMa 3-8B, to address data imbalance challenges. Finally, investigating keywords and patterns from several software artifacts to enhance understanding and explainability of SATD represents an interesting research direction."}]}