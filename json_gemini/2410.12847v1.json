{"title": "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "authors": ["Yu-Chen Lin", "Wei-Hua Li", "Jun-Cheng Chen", "Chu-Song Chen"], "abstract": "Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method attributed to its remarkable performance with few updated parameters on various large-scale pretrained Language Models (PLMs). Traditionally, each prompt has been considered indivisible and updated independently, leading the parameters increase proportionally as prompt length grows. To address this issue, we propose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT). In our method, we refer to the concept of product quantization (PQ), allowing all soft prompts to share a set of learnable codebook vectors in each subspace, with each prompt differentiated by a set of adaptive weights. We achieve the superior performance on 17 diverse natural language tasks including natural language understanding (NLU) and question answering (QA) tasks by tuning only 0.3% of parameters of the PLMs. Our approach also excels in few-shot and large model settings, highlighting its significant potential. Our code is available on GitHub.", "sections": [{"title": "1 Introduction", "content": "With the blooming of large language models, Parameter Efficient Fine-Tuning becomes an effective solution to leverage the power of pretrained language models (LMs). Among various approaches, Prompt Tuning (PT) has been recognized for its simplicity and efficacy by adding tokens in front of the inputs. Though prompting pretrained LMs with specific or human-designed instructions makes model transferable to downstream tasks, additional effort is needed for elaborating the prompts as the output produced are often sensitive to them. To address this issue, learning the prompts becomes a solution. Prompt tuning (Lester et al., 2021), Prefix tuning (Li and Liang, 2021) and P-tuning (Liu et al., 2022b) replace explicit instructions with continuous prompt embeddings and provide flexibilities for the pretrained models to adapt themselves with superior performance. Following the concept, ATTEMPT (Asai et al., 2022), MPT (Wang et al., 2023), DePT (Shi and Lipani, 2024), and TPT (Wu et al., 2023) demonstrate the capability of learnable PT in both single and multitask training scenarios.\nHowever, previous studies often treat the prompts as independent units in learning. Though the learned prompts can be further clustered for noise filtering (Bhardwaj et al., 2022), the parameters needed for training are not reduced since learning occurs before clustering. In this work, we introduce a method that represents the prompt based on a set of learnable codewords. All prompts share a codebook with N codewords. Compared with updating the prompts independently and thus preventing word embeddings from sharing information with each other, our codebooks are sharable across all prompts in a downstream task, making codebooks' parameters size independent of the prompt length.\nIn addition, our approach does not follow the common practice of regarding each prompt as inseparable. When treating a prompt as an indivisible word embedding, we may overlook the possibility that, say, certain words may align with other words in the first half of the embedding and match different words in the second half. To tackle this issue, we adopt the idea of product quantization (PQ) (Jegou et al., 2010) by dividing a prompt's word embedding into several subsections and construct a codebook for each subsection. In the past, PQ is effective for approximate nearest-neighbor search (Jegou et al., 2010; Yu et al., 2018) and neural network compression (Wu et al., 2016). However, if we directly apply PQ to the learned parameters, their amount will not be lowered for training. Hence, we simply follow PQ's concept where the codebooks are subsection-specific, and provide a set of learnable codewords for each subsection."}, {"title": "2 Related Work", "content": "Parameter-efficient fine-tuning enhances the capabilities of pretrained LMs by updating a small set of parameters. The approach varies, such as training extra modules (Houlsby et al., 2019; Sung et al., 2022) or modifying specific parts like biases or attention weights (Zaken et al., 2022; Hu et al., 2021). Among these, as Prompt Tuning (PT) is popular for its simplicity and effectiveness, we focus on PT."}, {"title": "2.1 Prompt Tuning Methods", "content": "This track focuses on enhancing the quality and efficiency of prompts. Schick and Sch\u00fctze (2021) and Brown et al. (2020) incorporate manually crafted instructions into the input sequence to provide task-specific guidance helping steer the model's output. When the instructions are well-designed, models with frozen parameters exhibit excellent performance. However, additional effort is required for human adjustment since the output is sensitive to the prompts. To address the issue, Wang et al. (2021) and Gao et al. (2021) further generate hard prompt templates by model automatically. Nonetheless, optimizing discrete prompts is challenging. Thus, Prompt Tuning (Lester et al., 2021), Prefix tuning (Li and Liang, 2021), and P-tuning (Liu et al., 2022b) turn prompts into continuous vectors, known as soft prompt, which are prepended to the word embeddings. The learnable prompts are trained with the pretrained LMs frozen. By turning discrete prompts to a continuous space, the optimization can be achieved by a simple gradient descent. Recently, Su et al. (2021) and SPOT (Vu et al., 2022) explore the advantages of initializing prompts by pretrained ones from other tasks. They demonstrate that learning prompts on one or more source tasks, and subsequently utilizing these learned prompts as initializations for a target task, is notably effective. ATTEMPT (Asai et al., 2022), MPT (Wang et al., 2023) and TPT (Wu et al., 2023) further design various architectures for multitask transfer learning. On the other hand, DePT (Shi and Lipani, 2024) focuses on reducing the training and inference time by decomposing prompt as a shorter one and a low-rank matrix added on word embeddings. Nevertheless, earlier approaches treat prompt as monolithic units, causing the number of trainable parameters to increase linearly with to the prompt length. In contrast, our method introduces a shared codebook in each subspace, which remains unaffected by the prompt length and facilitates information sharing among different prompts."}, {"title": "2.2 Quantization in NLP", "content": "Vector quantization (VQ) is a related technique to PQ which is widely employed in NLP. VQ provides an effective discretization of latent sentence representations, making it especially suitable for NLP tasks due to the inherently discrete nature of text, as demonstrated in Van Den Oord et al. (2017), Roy et al. (2018), Roy and Grangier (2019), Mercatali and Freitas (2021) and Angelidis et al. (2021). VQ is also used in PT. Bhardwaj et al. (2022) initially train a contextualized prompt for each input and cluster them using VQ to reduce variance.\nHowever, in the previous approaches, the number of parameters remains substantial since the training of original representations occurs before clustering. Different from these methods, we introduce learnable codebooks and adaptive weights which enable end-to-end training, thereby maintaining parameter efficiency throughout the process."}, {"title": "3 Methodology", "content": "We first give a preliminary of PT for downstream tasks and PQ, and then present our method."}, {"title": "3.1 Prompt Tuning for Downstream Tasks", "content": "Given a pretrained LM with parameters \u03b8, we want to transfer it to a target task with the training data $D = \\{(x_j, y_j)\\}|_{j=1}^{|D|}$. We first map $x_j$ to a sequence"}, {"title": "3.2 Review of PQ and Method Motivation", "content": "VQ is known as the process of mapping a vector x to the closest codeword $c^*$ in a codebook $C = \\{C_1, C_2, ..., C_N\\}$ containing N codewords. As an extension, PQ divides the vector $x \\in \\mathbb{R}^d$ into K subspace, $x = [x_1, x_2, ..., x_K]$, with d = tK and $x_k \\in \\mathbb{R}^t$. Each subspace possesses a codebook $C_k$ which contains $N_k$ codewords of dimension t for k = 1..K. PQ thus exploits the Cartesian product of the codeword sets,\n$C = C_1 \\times C_2 x ... \\times C_K$, (2)\nto encode the vector x. The total number of codewords becomes $N = \\prod_{k=1}^{K}N_k$ for entire space. When K = 1, PQ degenerates to VQ.\nPQ has the advantage of enabling more codewords for the representation of x by consuming fewer parameters. Eg., if $N_k$ is the same for all k, PQ can take the storage cost of only $O(tKN_k) = O(dN_k)$ to provide the codewords amount of $(N_k)^K$. For VQ, however, only $N_k$ codewords are provided under the same storage cost, or"}, {"title": "3.3 Proposed Method \u2013 ACCEPT", "content": "Previous methods often view each prompt as a single and indivisible word embedding, independent to other prompts. We suppose that tokens can share the same characteristics in a more fine-grained dimension. Our method leverages the concept of PQ and partition embedding space into K smaller subspaces. The k-th subspace has a codebook $C_k = \\{c_1^k, c_2^k,..., c_r^k\\}$ containing r codewords of dimension t, with t = d/K. Specifically, the total K codebooks are shared across all prompts.\nRemember that there is a set of trainable prompts $P = \\{P_1, P_2, \u2026, P_m\\}$ ($p_i \\in \\mathbb{R}^d$) for a downstream task in PT. Similarly, we divide each $p_i$ into K sub-prompts $p_i = \\{p_i^1, p_i^2,..., p_i^K\\}$ ($p_i^k \\in \\mathbb{R}^t$). We assign a group of weights, $W_i = \\{w_{i1}^k, w_{i2}^k, ..., w_{ir}^k\\}$"}, {"title": "4 Experiments", "content": "We present the experimental results and comparisons to other approaches in this section."}, {"title": "4.1 Experimental Settings", "content": "Datasets and Tasks. Following previous works, we evaluate our method on 13 NLU tasks and 4 QA tasks, including (1) MNLI (Williams et al., 2018), QQP, QNLI (Demszky et al., 2018), SST-2 (Socher et al., 2013), STS-B (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), RTE (Giampiccolo et al., 2007) and CoLA (Warstadt et al., 2019) from GLUE (Wang et al., 2018) benchmark; (2) MultiRC (Khashabi et al., 2018), BoolQ (Clark et al., 2019), WiC (Pilehvar and Camacho-Collados, 2018), WSC (Levesque et al., 2012) and CB (De Marneffe et al., 2019) from SuperGLUE (Wang et al., 2019) benchmark; (3) MRQA 2019 Shared Task (Fisch et al., 2019), including Natural Questions (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), SearchQA (Dunn et al., 2017) and NewsQA (Trischler et al., 2016). We use SciTail (Khot et al., 2018) additionally for few-shot learning.\nBaselines. We compare the proposed approach with various PEFT baselines including: (1) Fully fine-tuning (FT), where all the parameters of"}, {"title": "4.2 Results on NLU and QA Tasks", "content": "In Tab. 1, we compare the performances and the number of parameters during training of the proposed method with various methods on GLUE and SuperGLUE benchmarks. As can be seen, our method outperforms previous PT methods by a large margin, especially on MRPC, RTE and COLA datasets of the GLUE benchmark, while consistently improving on other datasets such as MNLI, QQP, etc. Similar results can be found on the SuperGLUE benchmark. Our method achieves a great improvement on the Bool, WiC and CB datasets, while also yielding promising performances on MultiRC and WSC. It is worth noting that our method surpasses previous PEFT methods exploiting much more tunable parameters such as Adapter, and also outperforms FT by 3.0% and 4.4% on the average performances of GLUE and SuperGLUE with only 0.3% parameters tuned. We further visualize the average performances against the number of trainable parameters for each method in Fig. 2. Our approach achieves the highest average accuracy while using the fewest parameters, making it more suitable for both performance and parameter efficiency than the others.\nBesides having favorable results on the NLU tasks above, the proposed method also achieves nice performances on QA tasks. Tab. 2 demonstrates that our method achieves a 4.2% improvement on the average of MRQA 2019 Shared Task than PT with fewer parameters, further reducing the performance gap between FT and PT methods.\nTo conclude, the proposed method achieves state-of-the-art performances on the challenging GLUE/SuperGLUE benchmarks and MRQA 2019 Shared Task with fewer trainable parameters, highlighting its efficiency and effectiveness."}, {"title": "4.3 Results on Few-shot Adaptation", "content": "Following Gao et al. (2021), Asai et al. (2022), Wang et al. (2023), Wu et al. (2023), Shi and Lipani (2024), we conduct the experiments with a limited number of training samples available on the BoolQ, CB, and SciTail datasets to verify the capability of ACCEPT in resource-limited scenarios. The experimental process involves initially training prompts on the intermediate tasks (e.g., MNLI) followed by transferring them to the target datasets with 4, 16, or 32 randomly sampled instances. In"}, {"title": "4.4 Ablation Study", "content": "Learnable Codebook and Subdivision. To demonstrate the effectiveness of ACCEPT, we first conduct an ablation study of PQ, utilizing the shared learnable codebook and prompt embedding subdivision, with the prepended and added prompt tunings. Tab. 4 shows that by sharing the learnable codebook among prompts, there is a noticeable performance improvement over the original architectures. Moreover, by dividing prompt embeddings into more fine-grained pieces, the performances are further enhanced. The results reveal the efficacy of PQ by subdividing the prompt embedding space.\nDifferent Granularity of Subdivision. We further study on the impact of using different sub-"}, {"title": "Prompt Initialization", "content": "We further analyze how initialization affects the performance. We conduct three initialization settings, (1) Random initialization: Both the codebooks and weights are initialized with a random Gaussian Distribution; (2) Intermediate task initialization: SPOT (Vu et al., 2022) has shown that initializing prompts with the pretrained weights from the tasks of a similar nature can benefit the training of the target task. (3) Target task initialization: By first pre-training the codebooks and the weights of SCPP and SCAP respectively on the target task, both of them are then served as the initialization of ACCEPT. Tab. 10 shows that our method achieves better performances than PT and DePT with all three strategies, revealing the robustness and effectiveness of ACCEPT. Moreover, intermediate task initialization strategy yields the best performances. We conjecture that the pretrained codebooks and weights from intermediate task of a similar nature helps the target task transfer more easily, providing additional knowledge and surpasses the performances of random or target task initialization. Detailed results are provided in Appendix C.1.\nPrompt Length.We evaluate the impact of different prompt lengths (m) on model performance and training time, as shown in Fig. 4. The experiments are conducted on the MRPC and STS-B datasets with m values of {0, 20, 40, 60, 80, 100}, while maintaining the same level of training parameters across all settings. The results indicate that as m increases, the training time also rises. Notably, our approach achieves peak accuracy in both datasets with m = 60, making it our optimal choice for the prompt length setting."}, {"title": "5 Conclusion", "content": "In this paper, we present ACCEPT, a novel prompt tuning method based on product quantization. As compared with other PT methods, the proposed method allows versatile and efficient prompt learning by subdividing prompt embeddings and computing each subprompt with the linear combination of learnable codewords and weights. Extensive experiments demonstrate that ACCEPT achieves outstanding performance across various NLP tasks. Furthermore, we also show the proposed approach is capable of being effectively adapted to billion-parameter models and achieves decent results.\nWhile we currently use all codewords for linear combination, we aim to explore sparse representations in the future work. Besides, we plan to extend our research scope by applying ACCEPT to a wider range of tasks with a more diverse set of LLMs."}, {"title": "Limitations", "content": "While our extensive experiments across 17 datasets highlight the effectiveness of ACCEPT, it's important to acknowledge some additional considerations. Our method introduces some extra hyperparameters, such as determining the optimal sub-dimension t, which requires some extra computational efforts. Moreover, ACCEPT involves managing two distinct learning rates for SCPP and SCAP. Additionally, due to the significant resource requirements of the models with tens of billions of parameters, our experiments were conducted on a limited number of datasets. Future work will aim to explore ACCEPT on a broader range of datasets and larger models to further validate its efficacy."}, {"title": "Appendix", "content": "A Experimental Setting\nWe use PyTorch\u00b9, huggingface transformers\u00b2 and huggingface PEFT\u00b3 to implement our work. GLUE4, SuperGLUE and MRQA 2019 Shared Task are downloaded from huggingface dataset. We use the original T5 checkpoint rather than the LM-adapted 1.1 version (Lester et al., 2021). We modified codes based on DePT's repository7. We mainly cite the experiment results from Wu et al. (2023) and Shi and Lipani (2024). We typically use m = 60 for the length of SCPP, and set the maximum sequence length I to 256, which also corresponds to the length of SCAP (except using 348 for MultiRC following Shi and Lipani (2024)). We partition SCPP and SCAP into K = 24 and K = 2 subsections, respectively. The associated r is calculated by the equation $rd + rmK < md$ for each model with dimension d. As for the experiments using the Llama-2-7B model, we modified codes based on Petals' repository. We use a learning rate of 3e-3 for SCPP and 5e-5 for SCAP. The weight decay is le-2 and 1e-3, respectively, with a batch size of 32.\nB Task and Dataset Details\nWe list the detailed information, including numbers of training, evaluation and testing samples, task types and evaluation metrics of each dataset which has been used in our experiments in Tab. 11. We utilize a diverse range of datasets covering various NLU tasks, including Natural Language Inference (NLI), Paraphrase Detection, and Sentiment Analysis. Additionally, we explore different types of Question Answering (QA) tasks, such as extractive and boolean QA. The effectiveness and generalizability of ACCEPT are demonstrated across these tasks in Tab. 1 and Tab. 2.\nC More Details of Experiments"}]}