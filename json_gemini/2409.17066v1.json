{"title": "VPTQ: EXTREME LOW-BIT VECTOR POST-TRAINING QUANTIZATION FOR LARGE LANGUAGE MODELS", "authors": ["Yifei Liu", "Jicheng Wen", "Yang Wang", "Shengyu Ye", "Li Lyna Zhang", "Ting Cao", "Cheng Li", "Mao Yang"], "abstract": "Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.\nIn this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8\u00d7 increase in inference throughput compared to SOTA.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown excellent performance across various complex tasks as their sizes increase. However, the enormous weight of LLMs poses significant challenges for efficient inference and practical deployment. This size reduction significantly affects memory capacity and hard-disk storage and requires substantial bandwidth for inference. Weight-only quantization is a mainstream model compression technique that effectively reduces the model's size by representing floating-point numbers with fewer bits."}, {"title": "2 Background and Motivation", "content": ""}, {"title": "2.1 Post Training Quantization in LLM", "content": "Post-Training Quantization (PTQ) aims to decrease model weight size by simplifying the numerical representation and seeking to maintain the model's accuracy without retraining the model. We can formulate PTQ as the following optimization problem:\n$\\arg \\min E[L(X, W + \\triangle W) \u2013 L(X, W)]$\n$\\approx \\triangle W^T \\cdot g(W) + \\frac{1}{2} \\triangle W^T W. H(W) \\triangle W$\nwhere the original model weights $W$, quantized weights $\\hat{W}$, and $\\triangle W = \\hat{W} \u2013 W$ represent the weight quantization error. The loss of the model task is $L$. The optimization object is to minimize the impact of model quantization on the model task, which means minimizing the expected deviant of loss function.\nPTQ typically employs a concise and accurate method for analyzing the above optimization problem: Second-Order Optimization. Following a Taylor series expansion, this method breaks down the optimization goal into first-order, second-order, and higher-order terms. $g(W)$ and $H (W)$ represent the gradient and Hessian of task loss $L$, respectively. It often assumes that the model has already reached local optimal before model quantization, which means that the first-order term is nearly zero. Higher-order terms exert a minor effect on the optimization goal, and we typically disregard interactions among weights between different layers. Consequently, we can simplify the optimization problem by focusing on optimizing the second-order term, and define the following optimization problem:\n$\\arg \\min_{\\triangle W} \\triangle W^T \\cdot H(W) \\cdot \\triangle W,$\ns.t.\n$\\triangle W = 0$                                                                                                                                                                                                                                                                                                                                                                                                                          (1)\nThe objective of optimization problem is to minimize the second-order error in model quantization, subject to the constraint that the change in model weights is as minimized as possible, i.e., $\\triangle W = 0$."}, {"title": "2.2 Vector Quantization in Neural Networks", "content": "VQ is a key method for efficient lossy data compression. Its objective is to reduce the distortion by mapping high-dimensional original data to a lower-dimensional space represented by a lookup table (Eq. 2). VQ maps original vectors ($W'$) from the vector space to a finite set of vectors, which is commonly referred as a codebook(lookup table, C). Each vector in the original space approximates the closest vector (centroid $C_i$), in the codebook.\n$\\arg \\min_{i \\epsilon k} || v - C_i ||^2, v \\epsilon W'$\n(2)\nVQ indicates the nearest centroid $C_i$ that minimizes the Euclidean distance between the input vector $v$ in the lookup table. The optimization problem aims to find the index $i$ that results in the smallest distance between $v$. Thus, each input vector is represented by the most similar centroids, thus minimizing total distortion.\nRecent research has explored the use of VQ for model weight quantization . These studies attempt to compress the embedding layer, the convolution layer, and the classification layer of neural networks using VQ. Figure 1 illustrates an example of applying VQ to compress model weights on a weight matrix. For a weight matrix $W$ with dimensions $M \\times N$, we reshape $W$ into vectors of length $v$ as $W'$ (step \u2460). The number of reshaped vectors should be $\\frac{M \\times N}{v}$. Next, we employ k-means or other clustering algorithms to build a codebook (step \u2461). The constructed codebook contains $k$ centroid vectors, each with $v$ dimensions. Applying the VQ algorithm directly often does not yield an acceptable accuracy. Typically, PTQ algorithms adjust the model index and centroid to enhance the accuracy of the quantized model (step \u2462).\nDuring model inference, each operator in the model first dequantizes the original weight matrix from the lookup table (codebook) by index and centroid. Unlike scalar quantization, VQ keeps the index and centroid in quantized weight. The equivalent compression ratio of VQ can be formulated as: total original model bits / (codebook bits + index bits). The equivalent quantization bitwidth is as: original bit width/compression ratio. For example, a 4096 \u00d7 4096 FP16 weight matrix with vectors of length $v$ = 8 and 256 centroids, the compression ratio is (16 \u00d7 4096 \u00d7 4096)/(8\u00d7256+\n8 \u00d7 4096 \u00d7 4096/8) = 15.9. The equivalent bitwidth is 1.0001 bit."}, {"title": "2.3 Vector Quantization in LLMs", "content": "While VQ has been applied to weight quantization, the following significant challenges persist when quantification of LLM.\nThe number of parameters in LLMs is enormous, which requires quantizing the model using lightweight methods to avoid excessive resource consumption.\nQuIP# introduces an incoherence processing using the randomized Hadamard transform for the weight matrix before VQ."}, {"title": "3 Vector Post-Training Quantization", "content": ""}, {"title": "3.1 VPTQ Algorithm", "content": "VPTQ leverages Second-Order Optimization and solves the optimization problem Eq.1 to achieve extreme low bit quantization. Assume that a weight matrix is $W\\in R^{M\\timesN}$, and a Hessian matrix collected from the current layer is $H\\in R^{M\\timesM}$. We denote the q-th column of the weight matrix as $W_{:,q}$. The quantized column $\\hat{W}_{:,q}$ can be represented as the transpose of concatenated centroid vectors\n$\\hat{W}_{:,q} = (C_0, C_1, ..., C_{M/v})^T$.\nWhen the weight matrix of the model is large, we can first split the weight matrix into multiple groups. Each group has its own independent codebook. This method allows us to flexibly divide the weight matrix into several submatrices ($W_{:,q:q+(M/\\text{group num})}$) equal to the group number. For clarity, we describe only one group in the following algorithm description.\nUnlike GPTVQ, we quantize each column of the matrix independently, which we refer to as Channel-Independent Second-Order Optimization. It greatly simplifies the complexity of VQ in Second-Order Optimization. GPTVQ, on the other hand, quantizes v columns of the matrix ($\\hat{W}_{M,v}$) at once, leading to larger errors and more complex transformations for problem optimization.\nWe use Lagrange Method to transform the optimization problem 1 into an unconstrained optimization problem. The Lagrangian function $L(\\triangle W)$, and $\\lambda$ is the Lagrangian multiplier:\n$L(\\triangle W) = \\triangle W^T H(W)\\triangle W + \\lambda\\triangle W$\nThe dual function $g(\\lambda)$ can be represented as:\n$g(\\lambda) = \u2013\\frac{1}{4} H^{-1} \\lambda^2 \u2013 \\frac{1}{2} (\\hat{W}_{:,q} \u2013 W_{:,q})$\nDifferentiating g(x) with respect to $\\lambda$ and setting it to 0,\n$g'(\\lambda) = \u2212H^{-1}\u03bb \u2013 (\\hat{W}_{:,q} \u2013 W_{:,q}) = 0$\nwe can find that when $\u03bb^T=H^{-1} \\frac{(\\hat{W}_{:,q}-W_{:,q})}{H_{qq}}$, the problem reaches an optimal solution.\nBy substituting $\u03bb^T$ into the optimization problem, we find that to minimize the error introduced by quantization, we need to minimize the impact on the Lagrangian function. Therefore, we can transform the quantization problem into minimizing:\n$\\triangle L(\\triangle W) = \\sum_i \\frac{||v \u2013 C_i||^2}{H_{qq}^{-1}}$\nWe find that when quantizing a column vector each time, we only need to consider minimizing $\\sum||v \u2013 C||^2$, which is to find the closest centroid in Euclidean Distance. It precisely aligns with the optimization of VQ. Moreover, since VPTQ quantizes the weight matrix column by column, $H_{qq}$ is constant when quantizing each column, so we do not need to consider Hessian when finding the centroid.\nAfter quantizing a column of weight matrix, we needs to update the current quantization error to the unquantized part through:\n$\\triangle W = \\frac{(W_{:,q} - \\hat{W}_{:,q})}{H_{qq}^{-1}}$\nIt will transform current quantization errors to the following unquantized columns. Since GPTVQ quantizes v columns at the same time, and quantization error can only spread to other unquantized columns when all v columns have been quantized. It will lead to more errors accumulating in the quantization, resulting in a decrease in model accuracy.  Algorithm 1 provides a detailed description of the steps to solve the optimization problem and quantize the weights according to the above analysis.\nCompared with GPTQ, VPTQ employs vector representations in the quantization, which choose the vector closest to the original matrix to represent the original data. Moreover, since GPTVQ quantizes multiple columns simultaneously, making the propagation of quantization errors to unquantized columns more challenging."}, {"title": "3.2 Optimization in VPTQ", "content": ""}, {"title": "3.2.1 Hessian-Weighted Centroid Initialization", "content": "VTPQ algorithm requires the initialization of centroids in the codebooks prior to quantization. Properly initializing centroids can reduce quantization errors and improve model accuracy.  \nWe can transform the optimization object by leveraging the cyclic property of matrix traces and the Hadamard product. We refine the optimization objective as:\n$\\triangle W^T\\triangle W\\odot H = \\sum{h_{i,i}||\\triangle W:,i||^2}$\n$+ \\sum_{i=0}^{n-1} \\sum_{j=0,j\\neq i}^{n-1} hij (\\triangle W:i W:,j)$\nBecause the Hessian matrix is predominantly diagonal, we can prioritize optimizing the first term through centroid initialization."}, {"title": "3.2.2 Residual Vector Quantization", "content": "We enable Residual Vector Quantization (RVQ) in VPTQ. RVQ improves vector quantization (VQ) by breaking down the compression of a weight matrix into two (or more) stages. Each stage further compresses the residual error from the previous quantization stage:\n$Q(v_{\\text{res}}) = \\arg \\min || (v_{\\text{res}} \u2013 Q(v)) \u2013 C_{\\text{res}} ||^2$\nUnlike GPTVQ, VPTQ enables RVQ, which quantizes VQ quantization error using a separate lookup table for better representation and quantization."}, {"title": "3.2.3 Outlier Elimination", "content": "Recent studies on quantization in LLM have consistently observed a significant presence of outliers in activation Outliers typically result in large values in the diagonal elements of the Hessian matrix.  \n$Q(v_{outlier}) = arg \\min || V_{outlier} \u2013 C_{outlier}||^2$\nFurthermore, VPTQ flexibly partitions the weight matrix and uses a separate outlier lookup table to quantify matrix tiles most affected by outliers."}, {"title": "4 End to end Quantization Algorithm", "content": "In this section, we will detail the end-to-end model quantization algorithm (Algorithm 2). The algorithm takes the original model, vector length v, centroid number k, and Hessian matrices H as inputs. \nIn each layer, we first quantize the weight of each Linear Operator (matrix multiplication of input and weight). If we enable the outlier option, the algorithm first selects outlier columns."}, {"title": "5 Experiments and Evaluations", "content": ""}, {"title": "5.1 Settings", "content": "Algorithm Baseline We focus on weight-only quantization.\nModels and Datasets We benchmark accuracy on LLaMA-2 , LLaMA-3 families , and Mistral. Following previous work , we report perplexity on language modeling tasks (WikiText-2 , C4 ). We also employ lm-eval-harness to perform zero-shot evaluations on common sense QA benchmarks (PIQA , HellaSwag , WinoGrande , ARC ).\nBaselines For LLaMA-2 and Mistral models, we compare VPTQ against GPTQ, GPTVQ, DB-LLM, QuIP# and AQLM. To account for the different overheads resulting from varying codebook constructions, we provide results with comparable bit widths to facilitate a fair comparison."}, {"title": "5.2 Accuracy Evaluation", "content": "Results on LLaMA-2 model: We compare VPTQ with QuIP#, AQLM, GPTVQ, DB-LLM and GPTQ on LLaMA-2 model. First, we discuss the results of 2 bit quantization. As shown in Table 2, GPTQ, as a scalar quantization method, performs poorly with unusable accuracy.  Therefore, we primarily focus on comparing VPTQ with the state-of-the-art 2 bit quantization methods QuIP# and AQLM which both choose longer vector lengths.\nResults on LLaMA-3 and Mistral model:"}, {"title": "Inference throughput and quantization cost", "content": "Table 2, the \u2018toks/s' column indicates the number of tokens gener-ated per second during the decode phase of inference.  Therefore, our inference throughput for the 7B and 13B models is 1.6-1.8\u00d7 faster than AQLM."}, {"title": "6 Conclusion", "content": "In this paper, we propose Vector Post-Training Quantization (VPTQ), a novel approach to achieving extremely low-bit quantization of LLMs by Vector Quantization. Through the application of Second-Order Optimization, we have formulated the LLM Vector Quantization problem and directed the design of our quantization algorithm. By further refining the weights via Channel-Independent Second-Order Optimization, we have enabled a more granular VQ.\nVPTQ also includes a brief and effective codebook initialization algorithm, achieved by decomposing the optimization problem. We have extended VPTQ to support residual and outlier quantization, which not only improves model accuracy but also further compresses the model size.\nOur experimental results demonstarte the effectiveness and efficiency of VPTQ."}, {"title": "7 Limitations", "content": "Related researches on PTQ have adopted end-to-end model finetuning after the PTQ phase.\nDue to GPU resource constraints, we cannot fine-tune larger models (70B) for longer iterations and more tokens."}, {"title": "A Appendix: All Experiments Results", "content": ""}, {"title": "A.1 Supplementary Explanation for Main Results Table 2", "content": "Table 2 shows our main results.\nDB-LLM Since they did not open source their code,\nGPTQ We reproduce the 2-bit results using the official GPTQ repository.\nGPTVQ They do not release their 2-bit quantized model.\nAQLM Their 1.97-bit LLaMA-2 13b model has not been open-sourced, so we are unable to measure its inference throughput."}, {"title": "A.2 All Experiments Results", "content": "In this section, we present all our experimental results, including the perplexity of the quantized model on different context lengths in two datasets, Wikitext2 and C4,\nTable 4 displays all results of Llama2 at 2 bits quantization."}, {"title": "B Quantitative Analysis of Quantization Parameter Settings", "content": "Quantization configuration The quantization parameters of all VPTQ 2bit models are shown in Table 8.\nLayer-wise finetuning parameters  We train each layer for 100 iterations."}, {"title": "C Ablation Study", "content": "Table 10 shows results from LLaMA2-13b on wikitext2 and c4 (sequence length=4096) under different quantization parameters."}, {"title": "C.1 Parameter Description", "content": "When performing N% outlier elimination, N% of outliers will be quantized using a codebook with a vector length of vo and ko centroids."}, {"title": "C.3 Channel-Independent Optimization", "content": "Row #4 with channel-independent optimization shows a perplexity decrease of 1 compared to row #5 without it,indicating that channel-independent second-order optimization effectively mitigates quantization error accumulation."}, {"title": "C.4 Outlier Elimination", "content": "Rows #4, #8, #9, and #10 represent the results for eliminating 0%, 1%, 2%, and 5% outliers, respectively."}, {"title": "C.5 Finetuning", "content": "Rows #4, #11, and #12 show results for without any finetuning, with layer-wise finetuning, and with end-to-endfinetuning, respectively."}, {"title": "C.6 Group Number", "content": "Rows #14 #15 #16, and #17 show the quantization results when 99% of parameters are divided into 1, 2, 4, and8 groups, respectively. Each group has its own independent codebook."}, {"title": "C.7 Higher Bitwidth", "content": "Rows #18 and #19 represent the results for 3-bit and 4-bit quantization, respectively. Compared to the FP16 results inrow #1, 4-bit vector quantization incurs almost no loss."}, {"title": "D Inference Evaluation", "content": ""}, {"title": "D.1 Throughput Measurement Process", "content": "We follow the throughput measurement method used in AQLM . During the prompt phase, weprovide 1 token and then have the model generate 256 tokens, calculating the generation time for each output token todetermine the throughput in tokens per second (tok/s)."}, {"title": "D.2 Our Dequantization Implementation", "content": "Our dequantization implementation is divided into two phases. In the first phase, which handles prompts with relativelylong sequences, we restore the quantized weights (index and centroid, etc.) to FP16 and then call \u2018torch.matmul'. In the second phase, during decoding, we fuse the dequantization and GEMV operations into QGemv, eliminating therepetitive reading and writing of FP16 weights."}]}