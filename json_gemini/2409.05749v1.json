{"title": "ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL", "authors": ["Safwen Naimi", "Wassim Bouachir", "Guillaume-Alexandre Bilodeau"], "abstract": "To extract robust and generalizable skeleton action recognition features, large amounts of well-curated data are typically required, which is a challenging task hindered by annotation and computation costs. Therefore, unsupervised representation learning is of prime importance to leverage unlabeled skeleton data. In this work, we investigate unsupervised representation learning for skeleton action recognition. For this purpose, we designed a lightweight convolutional transformer framework, named ReL-SAR, exploiting the complementarity of convolutional and attention layers for jointly modeling spatial and temporal cues in skeleton sequences. We also use a Selection-Permutation strategy for skeleton joints to ensure more informative descriptions from skeletal data. Finally, we capitalize on Bootstrap Your Own Latent (BYOL) to learn robust representations from unlabeled skeleton sequence data. We achieved very competitive results on limited-size datasets: MCAD, IXMAS, JHMDB, and NW-UCLA, showing the effectiveness of our proposed method against state-of-the-art methods in terms of both performance and computational efficiency. To ensure reproducibility and reusability, the source code including all implementation parameters is provided at https://github.com/SafwenNaimi.", "sections": [{"title": "I. INTRODUCTION", "content": "Human activity recognition (HAR) is used in areas like video surveillance, sports analysis, robotics, and health monitoring. It aims to analyze videos containing humans to identify the performed actions. Despite significant progress, robustly recognizing a wide range of human activities remains difficult due to various factors present in realistic environments, such as large variations in how the same action can be performed under different viewpoints and by different persons, and the recording conditions that may not be favorable due to occlusions.\nTraining HAR methods in a fully supervised manner requires large datasets with accurate annotations, which are time-consuming and costly to prepare in some crowded scenarios. To overcome this issue, self-supervised methods have been proposed to learn from data without the need for labels [1]\u2013[5]. Those pre-trained representations generated from unlabeled data can then be fine-tuned by training on a smaller labeled dataset to create an efficient model to solve a specific task. The first approaches have been focusing on learning representations by solving pretext tasks [1], [2], while more recent works adopt a contrastive learning framework [3], [4]. The goal of contrastive representation learning is to create an embedding space where comparable sample pairs are close together and dissimilar sample pairs are far away. Bootstrap Your Own Latent (BYOL) [5] follows this contrastive objective and achieves very good results in computer vision tasks without using negative pairs, by relying on two neural networks that interact and learn from each other.\nBy capitalizing on self-supervised learning, we propose a novel human activity recognition method addressing the limited data availability problem. The proposed method is based on human skeletons to mitigate the impact of visual appearance variation. We first detect humans in the scenes and estimate their poses with skeletons over temporal windows. This provides a skeletal action representation focused on discriminative posture and gesture cues, rather than appearance details, like clothing. To capture the most salient features of these skeletons, a Selection-Permutation strategy is applied to the skeleton joints. We then introduce a lightweight convolutional transformer-based model to capture the relationships between skeleton joints, both spatially and temporally, which we called ReL-SAR (Representation Learning for Skeleton Action Recognition). The convolutional layers specialize in extracting local spatial features to discern the spatial hierarchies within human poses and ensure robustness to variability in appearance thanks to their spatial inductive bias. Then, the transformer component allows to model temporal dependencies and dynamic evolution of poses, leveraging the complementary strengths of transformer and convolutional layers. To overcome the need for large amounts of data, ReL-SAR is pre-trained with BYOL. We show that representations learned with BYOL can effectively boost the performance.\nThe main contributions of this paper are as follows: 1) We propose a lightweight convolutional transformer-based model that we call ReL-SAR, to learn efficient representations for"}, {"title": "II. RELATED WORK", "content": "Skeleton action recognition uses skeleton data, which is robust to changes in appearance or environment, to analyze human actions by focusing on joint movements. Early research, such as Fernando et al. [6], utilized handcrafted features and hidden Markov models, while subsequent studies like Du et al. [7] adopted LSTM-based recurrent neural networks to capture temporal dynamics of movements. However, these methods often overlooked spatial relationships among joints. More recent approaches, such as those by Liu et al. [8] and Yang et al. [9], have incorporated both pose and motion information using two-stream RNN architectures and encoded pose images, respectively. Choutas et al. [10] developed PoTion, a method that aggregates joint heatmaps colored by relative time into a fixed-dimension representation for CNN-based action classification. Despite the spatial focus of CNNs, their limited capacity in modeling long-range temporal dependencies led to the exploration of Transformers, which excel in managing long-range dependencies through self-attention mechanisms, as noted by Dosovitskiy et al. [11]. This has motivated the integration of CNNs and Transformers to leverage the strengths of both models in recognizing human actions. Self-supervised learning has evolved significantly in the field of action representation from skeleton sequences. Initial approaches focused on pretext tasks, with Zheng et al. [1] utilizing a recurrent encoder-decoder GAN for learning representations, and Kundu et al. [12] employing a variational autoencoder to capture the human pose manifold, subsequently mapping actions as trajectories within this space. Transitioning to contrastive learning, recent efforts such as Lin et al. [3] integrated it into a multi-task learning framework (MS2L), aiming for more general representations. Thoker et al. [13] explored cross-contrastive learning between graph-structured and sequence-structured data to retain both modality-invariant and specific features. Li et al. [14] developed SkeletonCLR and CrosSCLR, leveraging a momentum contrast framework to enhance skeleton sequence representations. These methods traditionally use positive and negative sample pairs to train networks for distinguishing relevant pairings. However, the BYOL approach in ReL-SAR focuses only on positive pairs, which offers benefits like improved stability, better memory efficiency, and superior representation quality without the need for designing negative pairs."}, {"title": "III. PROPOSED METHOD", "content": "We motivate and explain our proposed ReL-SAR model in this section, covering pose extraction, our Selection-Permutation strategy, details of our convolutional transformer architecture, and the application of BYOL for representation learning."}, {"title": "A. Human Detection and Pose Estimation", "content": "Humans are first detected to establish regions of interest for subsequent processing. We use YOLOv5x [15] for this purpose. We then perform pose estimation. A recent study emphasizes the need for precise estimation of pose keypoint coordinates and their significance in achieving accurate HAR [16]. Therefore, we use ViTPose [17], as our method for pose estimation. Our choice of ViTPose is due to its lightweight decoder and Vision Transformer architecture, which have demonstrated state-of-the-art performance on the MS COCO dataset. We input to ViTPose the bounding box coordinates obtained with YOLOv5x corresponding to the detected human in a given frame t, and it returns a pose tensor Vt. For a sequence of frames, the ViTPose model generates a set of pose tensors as a skeleton sequence."}, {"title": "B. Input skeleton pre-processing and Selection-Permutation strategy", "content": "For skeleton-based HAR, previous studies have employed a methodology involving a limited number of long temporal skeleton sequences as input [18], [19]. These studies utilize a straightforward matching mechanism based on confidence scores to link skeletons into long sequences that cover the entire video. However, in ReL-SAR, we propose to sample a large number of short temporal skeleton sequences as input for better input consistency. ReL-SAR is therefore not dependent on the video length and does not necessitate zero-padding. By using multiple short video sequences, we can handle inputs with different lengths. The ViTPose model outputs 25 joint locations in the form of (x, y) coordinates, along with a confidence score, resulting in a 25 \u00d7 3 tensor. The pose tensors generated by ViTPose capture the spatial information of the body pose keypoints across an entire video sequence of length T. We employed a set X containing pose tensors"}, {"title": "C. Spatio-temporal feature extraction", "content": "The model employed in ReL-SAR for spatio-temporal feature extraction is illustrated in Figure 2. Firstly, an input skeleton sequence X goes through a 1D ConvNet encoder consisting of two blocks, each containing a convolutional 1D layer with F filters, K kernels, and a SeLU activation followed by a Batch-Normalization layer and a MaxPooling layer to extract spatial embeddings. The resulting spatial embeddings are then mapped to a dimension $D_{model}$ using a linear projection to obtain a sequence embedding $s_i$ that will serve as input to the Transformer encoder. We add an extra learnable token to the sequence embeddings following Dosovitskiy [11]. The role of this token is to aggregate information from the entire sequence. We refer to this token as the class token [CLS] for consistency with previous work [20], even though it is not attached to any label nor supervision in our case as in [21]. Moreover, positional information is incorporated into the sequence via a learnable positional embedding tensor $p_i \\in \\mathbb{R}^{D_{model}}$ added to all token representations, providing positional information that enables the model to effectively process sequential data. Our Transformer encoder comprises (L) layers, each consisting of alternating multi-head self-attention and feed-forward blocks (H). After each block, dropout, layer normalization, and residual connections are systematically applied. Each feed-forward block is a multi-layer perceptron with two layers and GeLU non-linearity. The initial layer serves to increase the dimensionality from $D_{model}$ to $D_{mlp} = 4 \\cdot D_{model}$ while incorporating the activation function. Conversely, the second layer reduces the dimensionality, restoring it from $D_{mlp}$ to $D_{model}$. The multi-head QKV self-attention mechanism (MSA) relies on a trainable associative memory using key-value vector pairs. Specifically, for the l-th layer of the Transformer encoder and the h-th attention head, the computation of queries (Q), keys (K), and values (V) is expressed as:\n$Q = XW_Q, K = XW_K and V = XW_V$                                               (1)\nwhere $W_Q$, $W_K$ and $W_V$ belong to $\\mathbb{R}^{D_{model} \\times D_h}$, $D_h$ being the dimension of the attention head. For each self-attention head (SA) and every element within the input sequence, a weighted sum is performed over all the corresponding values in (V). Finally, the outputs from all attention heads are concatenated, and they undergo a linear projection to revert to the original dimension $D_{model}$. This attention mechanism operates in the time domain, enabling the creation of a comprehensive spatio-temporal feature embedding representation by establishing connections across different time windows."}, {"title": "D. Bootstrap Your Own Latent (BYOL) for representation learning", "content": "In terms of modern self-supervision learning counterparts of BYOL, they use negative sampling and image (dis)similarity (SimCLR [22], MoCo [23], DINO [21]). They are strongly dependent on the tedious use of augmentation methods. To apply BYOL, we use two neural networks having the same Convolutional Transformer architecture as presented in the previous section, referred to as online and target networks (see Figure 3). The online network is defined by a set of weights $\\theta$, and the target network uses a different set of weights $\\xi$. First, BYOL produces two augmented views, $x_i \\equiv t(x)$ and $x_j \\equiv t'(x)$, from an unlabeled skeleton sequence input X by applying respectively data augmentations t and t'. Then, the online network outputs a representation $y_\\theta$, a projection $z_\\theta$, and a prediction $q_\\theta (z_\\theta)$ from the first view $x_i$. On the other hand, the target network outputs $y_\\xi$ and the target projection $z_\\xi$ from the second view $x_j$. Finally, the following mean squared error between the L2-normalized predictions $q_\\theta (z_\\theta)$ and target projections $z_\\xi$ is calculated:\n$L_{\\theta, \\xi} \\triangleq || q_\\theta (z_\\theta) - \\bar{z_\\xi}||_2^2 = 2 - 2 \\cdot \\frac{(q_\\theta (z_\\theta), z_\\xi)}{||q_\\theta (z_\\theta)||_2 ||z_\\xi||_2}$                                               (2)\nwhere $( , )$ denotes the inner product. To make the loss symmetric, $L_{\\xi, \\theta}$ is computed by feeding $z_\\theta$ to the online network and $x_i$ to the target network. The final loss is defined as:\n$L^{BYOL} = L_{\\theta, \\xi} + L_{\\xi, \\theta}$                                                                                       (3)\nAt each training step, BYOL minimizes this loss function with respect to $\\theta$ only, but $\\xi$ is a slowly exponential moving average:\n$\\theta : \\xi \\leftarrow \\tau \\xi + (1 - \\tau) \\theta,$                                                                                (4)\nwhere $\\tau$ is a target decay rate. Empirical evidence demonstrates that the combination of adding the predictor to the online network and representing the target network as an exponential moving average of the online network encourages richer representations while avoiding collapsed solutions.\nWe incorporate light data augmentations in the process of training BYOL to learn representations that are semantically relevant for action recognition [5]. Transforming the skeleton input sequence creates variations that minimize overfitting and improve generalization for stronger performance. Our data augmentations used for BYOL are:\n*   Noise: This transformation adds a random noise (or jitter) to joint coordinates, forcing the model to learn features invariant to minor skeleton variations.\n*   Scaling: This transformation changes the magnitude of the input sequence through multiplying with a random scaling factor, making the model more robust to variations in scale and offset of the skeleton sequence.\n*   Vertical Flip: This transformation applies a randomized vertical flipping to the input sequences, presenting the model with new temporal dynamics while maintaining feature continuity."}, {"title": "E. Action Classification", "content": "After BYOL training, we obtain two encoders that capture meaningful spatio-temporal features: $f_\\theta$ derived from the online network and $f_\\xi$ derived from the target network. As in Grill et al. [5], only the encoder of the online network is kept as the final model. With the embeddings from the online encoder, we include a simple linear classifier $f(.)$ on top of the encoder to perform action recognition. Unlike other contrastive-based self-supervised skeleton action recognition methods [24], [25], which verify the model via linear evaluation protocol, we apply an end-to-end fine-tuning and focus on the evaluation of learned feature hierarchies for the skeleton action recognition tasks."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conducted an extensive evaluation of ReL-SAR on several publicly available datasets for human activity recognition (HAR). We will describe the datasets and metrics used, and then present the implementation details followed by a detailed description and discussion of our experiments."}, {"title": "A. Datasets and Evaluation Metric", "content": "In our experiments, we evaluated the proposed approach on the following datasets, with a particular focus on their limited size: Multi-Camera Action Dataset (MCAD) [26]: It consists of 14,298 action samples, covering 18 action categories; INRIA Xmas Motion Acquisition Sequences dataset (IXMAS) [27]: It contains 12 action categories with 1800 action samples; Joint-annotated Human Motion DataBase (JHMDB) [28]: It consists of 960 video sequences depicting 21 actions; Northwestern-UCLA (NW-UCLA) [29]: It contains 1484 action samples covering ten action classes. We adopt the same evaluation benchmark in [29]; cameras 1 and 2 capture samples as training data and the rest as testing data. On all the datasets, classification accuracy is used as an evaluation metric."}, {"title": "B. Implementation and Training Details", "content": "All experiments were performed using an Nvidia GeForce RTX 3060 GPU. We implemented ReL-SAR using TensorFlow. For human detection, YOLOv5x [15] with pre-trained weights from MS COCO is employed. For human pose estimation, we used the ViTPose model [17], specifically, the huge model trained on MS COCO. In our implementation, the specific value of T is dataset-dependent. We selected it based on the length of the videos to ensure sufficient coverage of important action information. We chose T= 30 for MCAD, IXMAS, and NW-UCLA datasets, and T= 12 for JHMDB dataset. We have set the parameters of our Convolutional Transformer as follows: F= 192, K= 3, H= 3, L= 6, and $D_{model}$= 192 with F and K being the number of filters and kernel size respectively of each Conv1D layer. L is the number of Transformer layers, and H is the number of heads.\n1) BYOL pre-training: We trained BYOL with unlabeled skeleton sequences for 100 epochs with a batch size of 64. We used stochastic gradient descent (SGD) with a weight decay of 1e-4 and a momentum of 0.9. We opted for a cosine decay learning rate schedule with an initial rate of 1e-2 and 1000 decay steps.\n2) Fully Supervised baseline training: The fully supervised baseline corresponds to our convolutional transformer architecture without fine-tuning or freezing BYOL encoder. We trained this model for 500 epochs with a batch size of 128. The AdamW optimization algorithm is employed with a step drop of the learning rate $\\lambda$ to 1e-4 at a fixed percentage (80%) of the total number of epochs. A linear warmup was performed for the first 200 epochs, representing 40% of training. Weight decay of 1e-4 and label smoothing of 0.1 were used for regularization."}, {"title": "C. Evaluating the Effectiveness of Learned Feature Hierarchies", "content": "To assess the quality of the learned representations, we conducted an analysis evaluating the performance of various fine-tuning scenarios in Table I. In the fully fine-tuning setting, we first trained BYOL with unlabeled skeleton sequences and then fine-tuned with the linear classifier in a supervised manner. In the frozen settings, we preserve the weights of the specified early convolutional layer learned with BYOL, and only fine-tune the weights of the later, more task-related layers.\nThis analysis aimed to determine whether the features extracted from various layers trained in a self-supervised manner with BYOL exhibited differences in quality compared to their fully supervised counterparts with respect to the end-task performance, and if so, which layer should be utilized for this purpose. Table I provides action recognition accuracy results. Fine-tuning the entire model after applying BYOL yielded improvements over the supervised baseline across all four datasets. This highlights the ability of BYOL to achieve more representative feature extraction. Freezing the first Conv1D layer obtained from BYOL and fine-tuning the remainder yielded optimal performance on the MCAD, JHMDB, and NW-UCLA datasets. We achieved a 2.09% action recognition accuracy boost over the supervised baseline on NW-UCLA dataset, from 93.18% to 95.27%, when freezing Conv1D Layer1 of BYOL. This suggests that the low-level information captured by BYOL generalizes more effectively compared to the supervised baseline. Interestingly, freezing Conv1D layer 2 resulted in lower performance compared to freezing Conv1D layer 1 across all datasets. This could be attributed to the second layer representations becoming overly specific to data augmentations. However, freezing Conv1D layer 2 still demonstrated improvements in performance and generalization compared to the supervised baseline. These findings underline the ability of BYOL to learn more general skeleton features in the absence of explicit labels."}, {"title": "D. Semi-Supervised Evaluation", "content": "Under the semi-supervised setting, we initially pre-trained the online encoder $f_\\theta$ of BYOL with unlabeled skeleton sequences. We then fine-tune the linear classifier $f(.)$ with a small ratio of action annotations in a supervised fashion. We compare ReL-SAR with other state-of-the-art methods in the same semi-supervised setting for NW-UCLA dataset. Results are given in Table II. Following Yang et al. [34], we derived labeled data by uniformly sampling 1, 5, 10, 15, 30, and 40 videos from the training set. As Table II shows, ReL-SAR performs better than the state-of-the-art on the NW-UCLA dataset when less training data is available."}, {"title": "E. Impact of the proposed Selection-Permutation strategy", "content": "We showcase the advantages of our input skeleton structure, highlighting improvements in HAR. In ReL-SAR, we used a large number of short skeleton sequences of length T as input for skeleton action recognition. To investigate the effect of sequence length T, we tested multiple combinations of sequence length and number of joints. Results of the different combinations are given in Figure 4. Interestingly, we find that the highest performances are obtained when using our Selection-Permutation strategy with J= 15 except for T= 5. Using this strategy, we achieve the best result (93.92% accuracy) with T= 30. We also observe that with the increasing size of sequence, the performance of each model increases until a certain length, after which the recognition accuracy slightly decreases. This suggests an optimal balance between sequence length and joint number for achieving peak performance in skeleton action recognition."}, {"title": "F. Comparison to the State-of-the-Art", "content": "To demonstrate the benefits of ReL-SAR, we conducted comparisons with state-of-the-art methods on four datasets: IXMAS, MCAD, JHMDB, and NW-UCLA. The results are summarized in Tables III, IV, V and VI, respectively. Top-1 Accuracy denotes the action recognition accuracy, Param denotes the number of parameters, and FLOPs denotes the Floating point Operations per Second. Best result is bolded. Second Best result is in italics. ReL-SAR is ranked first in action recognition accuracy on MCAD, IXMAS, and JHMDB,"}, {"title": "V. CONCLUSION", "content": "We introduced ReL-SAR, a lightweight convolutional transformer model for skeleton-based action recognition. It relies on a Selection-Permutation strategy to obtain informative joint-level inputs and a self-supervised pre-training with BYOL for learning robust low-level skeleton features without labels. The experiments on four datasets demonstrate that ReL-SAR achieves superior action recognition performance while being computationally efficient. Overall, ReL-SAR provides an accurate yet lightweight approach for skeleton-based action recognition, making it suitable for deployment on limited-resource devices."}]}