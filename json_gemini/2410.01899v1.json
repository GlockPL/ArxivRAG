{"title": "The potential of LLM-generated reports in DevSecOps", "authors": ["Nikolaos Lykousas", "Vasileios Argyropoulos", "Fran Casino"], "abstract": "Alert fatigue is a common issue faced by software teams using the DevSecOps paradigm. The overwhelming number of warnings and alerts generated by security and code scanning tools, particularly in smaller teams where resources are limited, leads to desensitization and diminished responsiveness to security warnings, potentially exposing systems to vulnerabilities. This paper explores the potential of LLMs in generating actionable security reports that emphasize the financial impact and consequences of detected security issues, such as credential leaks, if they remain unaddressed. A survey conducted among developers indicates that LLM-generated reports significantly enhance the likelihood of immediate action on security issues by providing clear, comprehensive, and motivating insights. Integrating these reports into DevSecOps workflows can mitigate attention saturation and alert fatigue, ensuring that critical security warnings are addressed effectively.", "sections": [{"title": "1 Introduction", "content": "The adoption of DevSecOps pipelines through incorporating security tools for Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST) into the DevOps workflows, is a strategy aimed at identifying and mitigating bugs and vulnerabilities early in the development lifecycle, before the deployment. Nevertheless, many of the available security tools frequently produce inaccurate results or irrelevant results (e.g., false positives and negatives) , requiring the software teams to manually assess such outputs. For example, poor quality of scanning results and limitations related to the outputs of security tools hinder rapid deployments, as manual and time-consuming tasks that require security expertise are needed to address these issues [13]. Thus, despite the advantages of security scanning tools, developers often encounter significant challenges in effectively managing the influx of alerts and warnings generated by these workflows, especially if there are multiple tools involved, often providing overlapping results. This phenomenon, known as alert"}, {"title": "2 Background", "content": "DevSecOps is a recent paradigm shift in software engineering, aimed at integrating security practices into the DevOps framework. DevOps, initially focused on improving collaboration between development and operations teams, has evolved to include security, leading to the emergence of DevSecOps. This transformation is driven by the need to address security concerns that have become increas-"}, {"title": "2.1 DesvSecOps", "content": "DesvSecOps is a recent paradigm shift in software engineering, aimed at integrating security practices into the DevOps framework. DevOps, initially focused on improving collaboration between development and operations teams, has evolved to include security, leading to the emergence of DevSecOps. This transformation is driven by the need to address security concerns that have become increas-"}, {"title": "2.2 LLM Report Generation", "content": "LLMs have the ability to process large amounts of data and generate human-like text, and they have proven their value in domains such as healthcare and legal services. LLMs can offer diagnostic suggestions, legal advice, and even write comprehensive reports summarizing large contexts, bridging gaps where human expertise may be limited or unavailable. In the domain of medical diagnosis, LLMs have shown promise in analyzing multimodal data combining medical images and symptom descriptions to provide diagnostic reports . Moreover, in the context of advice-seeking, studies indicate that LLM-generated advice can influence user behavior ."}, {"title": "3 Methodology", "content": "In this section, we present our approach to report generation with LLMs. For input, we opt for the LAZARUS AI Secret Scanner tool, a novel language-agnostic secret scanning solution presented in , capable of identifying hardcoded credentials in source code, either human-generated (passwords), or machine-generated (API keys/secrets, tokens, etc.). Different than popular SAST tools with similar functionality, such as TruffleHog\u00b3 which follow the conventional approach to secret detection, i.e. looking for high entropy strings or using regular expressions, LAZARUS AI Secret Scanner employs an ensemble of two text convolution neural network models based on TextCNN architecture , namely the Context Model and the Password Model. The Context Model is trained to classify source code snippets surrounding specific seed elements (including names of methods, variables, constants, etc.) that can be potentially relevant for various authentication contexts, including databases, mail servers, automation, web services and more. The Password Model is trained to classify strings extracted for the code snippets previously identified as potential passwords, machine-produced secrets (including API keys, JWT tokens, etc.), or ordinary strings. From experiments conducted in a large-scale dataset, LAZARUS AI Secret Scanner has shown compelling performance in terms of both correctly identifying credentials and the type of authentication context they appear in."}, {"title": "4 Survey Design and Results", "content": "After collecting the LLM-generated reports for each file in our sample, we created a survey to evaluate how effectively these reports motivate developers to take action on resolving identified security issues, and how this compares to the reports or alerts generated by the security tools in their DevSecOps pipelines. To this end, the survey focuses on the clarity of consequences/perceived financial impact, and overall motivation for action compared to the status quo of alerting and reporting in the participants' workflows.\nAs such, we sent the generated reports (20 in total, 1 for reach LLM) and the source code samples to 23 software engineers who agreed to participate in the survey. The participants were selected on the basis that they already employ DevSecOps workflows and have experience with SAST and similar security tools for source code auditing. Moreover, we created a short questionnaire of 5 ques-"}, {"title": "5 Conclusions", "content": "The integration of LLM-generated reports into DevSecOps workflows shows significant potential in addressing the challenges of alert fatigue and motivating developers to take prompt action on security issues. The survey results indicate that both ChatGPT and Llama effectively communicate the potential consequences and financial impacts of identified security vulnerabilities, with a majority of participants finding the reports clear and actionable. Notably, ChatGPT excelled in providing clear and reasonable explanations of security issues, while Llama 3's more personal and motivational tone proved effective in driving action among developers. In both cases, developers found the LLM-generated reports more motivating to act on the identified issues compared to the alerts and reporting of security issues in their current workflows.\nDespite their advantages, the idea of LLMs generating reports in the DevSecOps context faces challenges in gaining trust for their recommendations. This could be attributed to the generic nature of the described solutions, which, although aligned with security best practices, might not always be relevant in tackling specific issues, such as credential leaks. Additionally, the developers' responses indicate a broader skepticism towards LLMs in critical security contexts, reflecting doubts about their reliability and accuracy. Nonetheless, the survey responses indicating a preference for Llama in producing motivating reports suggest that there is significant potential for integrating local, open-source LLMs into developers' workflows without the need for their code to leave their premises. Beyond coding assistance, such models could be used for alerting and report generation, mitigating alert fatigue with their capacity to produce compelling reports, and thus ensuring that critical security warnings do not fly under the radar of developers and security teams."}]}