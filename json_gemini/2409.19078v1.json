{"title": "Differential privacy for protecting patient data in speech disorder detection using deep learning", "authors": ["Soroosh Tayebi Arasteh", "Mahshad Lotfinia", "Paula Andrea Perez-Toro", "Tomas Arias-Vergara", "Juan Rafael Orozco-Arroyave", "Maria Schuster", "Andreas Maier", "Seung Hee Yang"], "abstract": "Speech pathology has impacts on communication abilities and quality of life. While deep learning-based models have shown potential in diagnosing these disorders, the use of sensitive data raises critical privacy concerns. Although differential privacy (DP) has been explored in the medical imaging domain, its application in pathological speech analysis remains largely unexplored despite the equally critical privacy concerns. This study is the first to investigate DP's impact on pathological speech data, focusing on the trade-offs between privacy, diagnostic accuracy, and fairness. Using a large, real-world dataset of 200 hours of recordings from 2,839 German-speaking participants, we observed a maximum accuracy reduction of 3.85% when training with DP with a privacy budget, denoted by \u025b, of 7.51. To generalize our findings, we validated our approach on a smaller dataset of Spanish-speaking Parkinson's disease patients, demonstrating that careful pretraining on large-scale task-specific datasets can maintain or even improve model accuracy under DP constraints. We also conducted a comprehensive fairness analysis, revealing that reasonable privacy levels (2<\u03b5<10) do not introduce significant gender bias, though age-related disparities may require further attention. Our results suggest that DP can effectively balance privacy and utility in speech disorder detection, but also highlight the unique challenges in the speech domain, particularly regarding the privacy-fairness trade-off. This provides a foundation for future work to refine DP methodologies and address fairness across diverse patient groups in real-world deployments.", "sections": [{"title": "1. Introduction", "content": "Speech pathology, which refers to speech impairments caused by various disorders, is a critical area of study due to its important impact on an individual's quality of life and communication abilities1,2. Early and accurate detection of speech disorders can lead to more effective interventions and improved outcomes for patients. Artificial intelligence (AI)-based models have shown remarkable potential in diagnosing and analyzing these speech disorders by leveraging vast amounts of data to identify patterns that may not be apparent to human clinicians3\u20135. Studies have highlighted the expanding role of pathological speech in evaluating neurological conditions such as Parkinson's3,6 and Alzheimer's, as well as speech disorders like dysarthria and dysglossia7,8. However, the integration of Al in this sensitive field raises substantial concerns about patient privacy9\u201312. Recent research has shown that pathological speech, as a biomarker, is more vulnerable to re-identification attacks compared to healthy speech, making the protection of patient data confidentiality crucial. Misuse or unauthorized access to such data can result in severe ethical and legal consequences. In response to these privacy concerns, various methods for privacy-preserving deep learning (DL) in pathological speech detection have been proposed, with federated learning (FL) being one of the more prominent approaches13\u201315. FL allows models to be trained across decentralized data sources without requiring direct data sharing, addressing data privacy during the training process. However, FL has limitations in model sharing, leaving potential vulnerabilities that could be exploited16\u201318. Therefore, additional safeguards are essential to ensure comprehensive protection of patient data in Al-driven speech disorder detection19,20\nAnother method that has been explored for protecting patient privacy in Al-based speech disorder detection is the automatic anonymization10\u201312 of pathological speech21. In this approach, the original speech data is anonymized before being used to train Al models, thereby reducing the risk of exposing sensitive patient information. Recent studies21 have demonstrated the effectiveness of this method in preserving pathological biomarkers, which are crucial for the accurate detection and diagnosis of speech disorders. However, the process of anonymization is not without its limitations. It has been observed that, depending on the underlying pathology, some crucial pathological information may be inadvertently removed during the anonymization process. This pathology-dependent nature of speech anonymization means that a one-size-fits-all approach is insufficient, and there is a need for curated anonymization methods tailored to each specific pathology. Additionally, anonymized speech data can still be vulnerable to re-identification techniques, posing a potential risk to patient privacy21. Therefore, while automatic anonymization offers a valuable tool for privacy preservation, it must be used with caution and in conjunction with other methods to ensure robust protection of patient data in Al-driven speech disorder detection.\nThese challenges underscore the need for more robust privacy-preserving techniques in Al-based speech disorder detection, leading to the motivation for adopting differential privacy (DP)22. Unlike traditional methods, DP provides a formal and quantifiable framework for protecting sensitive information, even in FL or other distributed training environments where privacy risks are elevated16,22. In such settings, adversaries can exploit vulnerabilities to extract detailed information during the training process or manipulate the model itself, posing major threats to patient privacy. Models trained on sensitive medical data, including pathological speech, are particularly vulnerable to attacks like membership inference and model inversion, where attackers can reconstruct aspects of the original"}, {"title": "2. Results", "content": "2.1. High diagnostic performance under privacy constraints\nWe first evaluated the diagnostic performance of models trained with DP compared to non-private models. Using a large-scale, multi-institutional pathological speech dataset of German speakers9,21,42, which includes n=1,979 training speakers and n=860 held-out test speakers , we addressed the multiclass detection of speech disorders and pathological conditions. Specifically, the tasks involved detecting speech disorders such as Dysarthria and Dysglossia, identifying the pathological condition CLP, and distinguishing healthy speech. Two distinct"}, {"title": "2.2. Under-represented groups are more affected by DP", "content": "The German dataset used in this study comprises n=2,983 participants. While this number may seem small compared to image-based datasets, it is important to note that it corresponds to up to 200 hours of recordings, which is considered a very large-scale dataset in the medical speech processing domain. To the best of our knowledge, it is among the largest pathological speech datasets utilized in related publications. Given the known impact of DP on under-represented groups, as reported in the literature, we sought to assess these effects while also generalizing our findings. To do so, we used the PC-GITA dataset44, which consists of speech recordings from a considerably smaller sample of participants (n=50 PD patients and n=50 age- and gender-matched healthy controls), all of whom are native Spanish speakers from Colombia. The task was PD detection.\nFor the non-private model, the results showed an AUROC of 83.27 \u00b1 1.10% and an accuracy of 81.75 \u00b1 1.35%. When trained with DP at \u025b = 7.42, the AUROC dropped to 73.33 \u00b1 3.87% and the accuracy to 69.47 \u00b1 3.46%, representing a substantial reduction of up to 12%. This reduction highlights the challenge of maintaining a favorable privacy-utility trade-off, especially for under-represented groups, where the trade-off becomes less effective. To address this issue, and following recent findings in the medical imaging domain 16,24, we applied a slightly more task-specific pretraining. Due to the lack of sufficiently large public datasets for pathological speech, we used a model pre-trained on the train-clean-360 subset (around 360 hours of clean speech) of the LibriSpeech45 dataset-a widely available healthy speech dataset of English speakers\u2014for weight initialization in the PD detection task. This approach led to a modest performance reduction under privacy constraints compared to the non-private model. For the DP model with \u025b = 4.39, the AUROC was 80.27 \u00b1 1.06% and the accuracy was 78.75 \u00b1 1.09%, representing a 3% reduction in both AUROC and accuracy compared to the non-private model. This demonstrates that task-specific pretraining can substantially mitigate the impact of under-representation."}, {"title": "2.3. Balancing sex-based fairness under privacy constraints", "content": "We evaluated our models based on patient sex and calculated the statistical parity difference (PtD)47 to measure fairness. PtD quantifies the difference in diagnostic accuracy between different groups, in this case, between male and female patients. A PtD value of 0 indicates perfect fairness, while positive values suggest a bias favoring one group (e.g., females), and negative values indicate a bias against that group.\nAs shown in Table 2, diagnostic performance of the non-private model for the female group was slightly higher than for males, with accuracy differences of up to 1.19% and PtD values up to 1.11 across different speech disorders, conditions, and controls. For the DP model at \u025b = 7.51, this trend remained consistent, with the model continuing to favor the female groups in all cases, showing PtD values up to 1.87 across the various categories. These results indicated that the privacy-fairness trade-off for sex groups was well-maintained at this privacy level with reasonable privacy-utility trade-off."}, {"title": "2.4. Age-based privacy-fairness trade-off is more complex under privacy constraints", "content": "Next, we evaluated our models based on patient age groups.\nTable 3 shows the results for age groups. The diagnostic performance of the non-private model was higher for the children (0 to 15 years old) group than for young patients (15 to 30 years old), with accuracy and PtD differences of 2.07% for CLP. However, for healthy controls, the children group only slightly outperformed the young patients, with a 0.87% difference in accuracy. For detecting healthy controls, a trend similar to that observed with sex groups was maintained. The DP model at \u025b = 7.51 showed a 4.23% reduction in accuracy and a slight fairness bias in favor of young patients (PtD = 1.24 \u00b1 0.13). At extremely high privacy levels \u025b < 1, the fairness bias increased, with a similar Pearson's correlation coefficient (r = 0.74).\nHowever, for CLP detection, the results differed . While the accuracy reduction for the DP model at \u025b = 7.51 for children was 3.69%, indicating a relatively good trade-off, the reduction for young patients was much larger, at 10.23%. Fairness analysis revealed a major bias towards children compared to young patients, with a PtD of 7.23 \u00b1 1.70. PtD strongly correlated with privacy levels, as demonstrated by Pearson's correlation coefficient (r = 1.00) for CLP patients between children and young patients.\nOn the other hand, the reductions in accuracy for Dysarthria and Dysglossia across early adults (30 to 50 years old), middle-aged patients (50 to 70 years old), and older patients (70 to 100 years old) ranged from 4.15% to 6.63% for the DP model at \u025b = 7.51, indicating a relatively good privacy-utility trade-off for these subgroups. The fairness analysis showed that the non-private model almost did not favor any of these groups over the others for Dysarthria or Dysglossia, with mean PtD values between -1.0 and 1.0 in all cases. Consistent with the results for sex groups and younger patients, PtD correlated with privacy levels (Pearson's r > 0.75 for all cases except for Dysglossia detection in older patients, where r = 0.55 indicated a moderate correlation). At extremely low privacy budgets, fairness biases were introduced. For a comprehensive overview of evaluation results for all age groups using all evaluation metrics, refer to Supplementary Tables 14-19."}, {"title": "3. Discussion", "content": "In this study, we investigated the impact of differential privacy (DP) on the diagnostic performance of deep learning (DL) models trained on pathological speech data. We focused on the trade-offs between privacy protection and diagnostic accuracy using a large, real-world dataset consisting of approximately 200 hours of recordings from 2,839 German-speaking participants9,21,42. This dataset"}, {"title": "4. Methods", "content": "4.1. Ethics statement\nThe German dataset received approval from the institutional review board of University Hospital Erlangen under application number 3473, in compliance with the Declaration of Helsinki. The protocol for the PC-GITA dataset44 was approved by the Ethical Committee of the Research Institute in the Faculty of Medicine at the University of Antioquia in Medell\u00edn, Colombia (approval number 19-63-673). All experiments were conducted in accordance with applicable national and international guidelines and regulations and informed consent was obtained from all adult participants, as well as from the parents or legal guardians of the children involved for both datasets.\n4.2. Datasets\n4.2.1. Speech disorders dataset\nThe speech disorders dataset9,21,42 used in this study encompasses a broad spectrum of speech samples collected from various locations throughout Germany. Participants had a mean age of 30 \u00b1 25 [standard deviation] years, with a balanced representation of both male and female participants and included individuals ranging from children to elderly adults. The dataset includes participants with Dysarthria and Dysglossia, as well as corresponding healthy controls. These participants were tasked with reading \"Der Nordwind und die Sonne,\"42 a phonetically diverse German adaptation of Aesop's fable \"The North Wind and the Sun,\" which consists of 108 words, 71 of which are unique. For participants with CLP and their healthy controls, the \u201cPsycholinguistische Analyse kindlicher Sprechst\u00f6rungen\u201d (PLAKSS)54 test was carried out, requiring them to name pictograms presented on slides, covering all German phonemes in various positions. To handle the tendency of some children to use multiple words or add extra words between target phrases, recordings were automatically segmented at pauses longer than one second21,42.\nData collection spanned from 2006 to 2019, primarily during routine outpatient examinations at the University Hospital Erlangen, as well as from over 20 other locations across Germany for control speakers, resulting in a diverse range of regional dialects among participants. All participants were informed about the study's objectives and procedures, and consent was obtained. A standardized recording protocol ensured consistent microphone setups and speech tasks across all sessions. The study excluded non-native speakers and individuals whose speech was significantly impacted by factors unrelated to the targeted disorders. The dataset was managed using the PEAKS42 software, a widely recognized open-source tool in the German-speaking research community and recordings were made at a 16 kHz sampling rate and 16-bit resolution9,21. For this study, we followed the exclusion criteria shown in ensuring speech quality and noise standards, and removing recordings with multiple speakers.\n4.2.2. Parkinson's Disease dataset\nThe PC-GITA dataset44 was used for Parkinson's disease (PD) detection, comprising speech recordings from 50 PD patients and 50 healthy controls, all of whom are native Colombian Spanish speakers. The male PD patients ranged in age from 33 to 77 years (mean 62.2 \u00b1 11.2 years), while female PD patients ranged from 44 to 75 years (mean 60.1 \u00b1 7.8 years). Among the healthy controls, the men were aged 31 to 86 years (mean 61.2 \u00b1 11.3 years), and the women were aged 43 to 76 years (mean 60.7 \u00b1 7.7 years)44. This ensures that the dataset is well balanced in terms of age and"}, {"title": "4.3. Theoretical background on differential privacy (DP)", "content": "DP is a formal framework designed to enable the extraction of useful information from sensitive datasets while safeguarding the privacy of individuals whose data is included22. DP provides a mathematical guarantee that the outcome of any computation on a dataset will be nearly identical whether a specific individual's data is included 20,52. This ensures that the presence or absence of any single data point has a minimal effect on the overall result, thereby preserving individual privacy.\nFormally, a randomized algorithm $\\mathcal{M}: \\mathcal{X} \\rightarrow \\mathcal{Y}$ satisfies $(\\epsilon, \\delta)$-DP if, for any two datasets $D_1$ and $D_2$ that differ in exactly one element, and for any subset of outputs $\\mathcal{S} \\subseteq Range(\\mathcal{M})$, the following inequality holds,\n$\\mathbb{P}(\\mathcal{M}(D_1) \\in \\mathcal{S}) \\leq e^{\\epsilon}.\\mathbb{P}(\\mathcal{M}(D_2) \\in \\mathcal{S}) + \\delta$ (1)\nIn simpler terms, DP ensures that the inclusion of an individual's data in a dataset does not significantly alter the likelihood of any particular outcome, thereby limiting the risk of privacy breaches16. The parameters \u025b and 8 define the privacy budget. Smaller values of \u025b correspond to stronger privacy guarantees, while 8 represents the probability that the privacy guarantee might not hold, typically in rare or extreme cases. In practice, 8 is set to a very small value, usually smaller than the inverse of the training set size16.\nOne of the key strengths of DP is its ability to provide quantifiable privacy guarantees over multiple computations, which is crucial for iterative processes like those in DL. This is managed through privacy accounting, which tracks the cumulative privacy loss over a series of operations. In DL, DP is commonly implemented using the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm43, which modifies the standard stochastic gradient descent (SGD) algorithm used for training neural networks to include mechanisms that ensure DP. The core concept is to introduce randomization into the training process, limiting the impact of any single data point on the model's output24.\nThe DP-SGD algorithm works as follows: (i) For each mini-batch of data, the gradients of the loss function are computed individually for each data point. These gradients are then clipped according"}, {"title": "4.4. Experimental design", "content": "Two distinct networks, specifically, models trained employing DP- or non-DP training, were trained on the same training dataset. Subsequently, testing was performed on a separate held-out test set, separately for both networks, resulting in strictly paired comparison scenario, removing the need for random effects models analysis and stuff. It should be noted that we used a multiclass classification approach, optimizing for average performance across all classes and did not perform a detailed comparison for individual disorders. In this study, we considered per-patient privacy."}, {"title": "4.5. Deep learning network architecture and training", "content": "4.5.1. Data preprocessing\nDuring the data preprocessing phase, any drifting noise present in the audio was removed using a forward-backward filter55. The final feature set consisted of 80-dimensional log-Mel-spectrograms, generated using a short-time Fourier transform with a size of 102421, a window length of 64 ms, 80 Mel filters, and a frequency range of 0 to 8 kHz.\n4.5.2. Network architecture\nGiven the two-dimensional structure of log-Mel-spectrograms and the compatibility of the original DP-SGD algorithm with CNNs rather than transformers for training, we selected the ResNet1853 model, originally designed for image classification, to enhance feature extraction56,57. Notably, previous research16,24,48, particularly in the medical imaging domain, has demonstrated the generalizability of the DP-SGD algorithm across various CNNs for medical diagnostics. To ensure compatibility with DP-SGD training, we used a modified ResNet1853 architecture incorporating adjustments proposed by Klause et al.58. Instead of batch normalization59, we employed group normalization60 with groups of 32, which is more appropriate for DP settings. The network's inputs were 3-channel Mel-filterbank energies, aligned with the pretrained weights from the large-scale ImageNet46 dataset, consisting of over 14 million images"}, {"title": "4.5.3. Non-DP training", "content": "For non-DP training, we used a batch size of 128, with 8 utterances per speaker randomly selected for each batch. For the experiments with the smaller PC-GITA dataset, we adjusted the batch size to 40, with utterances per speaker selected accordingly. To accommodate the varying lengths of log-Mel-filterbank energies, we randomly selected 180 frames for inclusion in the training process. The network inputs were structured as at 128 \u00d7 3 \u00d7 80 \u00d7 180 for the speech disorder dataset and 40 \u00d7 3 \u00d7 80 \u00d7 180 for the PC-GITA dataset, corresponding to batch size, channel size (adjusted to 3"}, {"title": "4.5.4. DP training", "content": "For DP training, all models were optimized using the NAdam optimizer62 with a learning rate of 5 \u00d7 10-4 to achieve optimal convergence, without applying weight decay. Binary cross-entropy was selected as the loss function. The maximum allowed gradient norm was set to 1.5, which was determined to be optimal for this context. Each data point in the DP training batches was sampled with a probability equal to the batch size (128 for the speech disorder dataset and 40 for the PC-GITA dataset) divided by the total number of training samples in the dataset. A DP accountant, based on R\u00e9nyi differential privacy63, was employed to manage the privacy budget (represented by \u025b and 6) and ensure it remained within predetermined limits. A 8 value of 0.001 was chosen for all networks. The value of \u025b depended on factors such as the introduced noise, the set d, the number of training steps, and the batch size. The reported \u025b was determined by the convergence step of each neural network, given the diversity of the datasets24."}, {"title": "4.6. Evaluation", "content": "4.6.1. Privacy-utility trade-off\nAccuracy and the area under the receiver operating characteristic curve (AUROC) were the primary evaluation metrics used to assess diagnostic performance, with outcomes for individual disorders averaged without applying weights. To analyze the privacy-utility trade-off, \u025b was used as the privacy measure, while both accuracy and AUROC served as utility measures. Sensitivity and specificity were calculated as secondary metrics for diagnostic performance. For sensitivity and specificity calculations, the threshold was determined using Youden's criterion64, which maximizes the difference between the true positive rate and the false positive rate.\nFor the speech disorder dataset, speakers were randomly allocated to training (70%) and test (30%) groups. This random allocation was consistent across experiments to ensure that the same training and test subsets were used when comparing anonymized data with original data, allowing for paired analyses that account for random variations. The division was designed to prevent overlap between training and test data. The final training set included n=1,979 speakers, and the final test set included n=860 speakers. A similar procedure was followed for the PC-GITA dataset, resulting in a final training set of n=80 speakers and a test set of n=20 speakers.\n4.6.2. Privacy-fairness trade-off\nTo evaluate the privacy-fairness trade-off, we assessed the performance of private and non-private networks across different demographic subgroups. Detailed demographic information is provided in"}, {"title": "4.7. Data availability", "content": "The German speech disorder dataset used in this study is internal data of patients of the University Hospital Erlangen and is not publicly available due to patient privacy regulations. A reasonable request to the corresponding author is required for accessing the data on-site at the University Hospital Erlangen in Erlangen, Germany. The PC-GITA44 dataset is a restricted-access resource. To gain access, users must agree to the dataset's data protection requirements by submitting a request to JROA (rafael.orozco@udea.edu.co). The LibriSpeech45 dataset is publicly available at https://www.openslr.org/12 under a CC BY 4.0 license."}, {"title": "4.8. Code availability", "content": "To encourage transparency and facilitate future research, we have publicly released our complete source code at https://github.com/tayebiarasteh/DPSpeech. The repository provides comprehensive documentation on the training procedures, evaluation protocols, data preprocessing steps, and anonymization methods used in our study. This will enable the research community to reproduce our results effectively. The code is implemented in Python 3.9 and leverages the PyTorch 1.13 framework for all deep learning operations."}, {"title": "5. Additional information", "content": "5.1. Acknowledgements\nWe acknowledge financial support by Deutsche Forschungsgemeinschaft (DFG) and Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg within the funding programme \u201cOpen Access Publication Funding.\" This study was funded by Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg, Medical Valley e.V., and Siemens Healthineers AG within the framework of d.hip campus.\n5.2. Author contributions\nThe formal analysis was conducted by STA, AM, and SHY. The original draft was written by STA and corrected by STA, PAPT, AM, and SHY. The software was developed by STA. The experiments were performed by STA and ML. Statistical analysis was performed by ML and STA. Datasets were provided by JROA, MS, AM, and SHY. STA cleaned, organized, and pre-processed the German data. JROA cleaned, organized, and pre-processed the Spanish data. STA and MS provided clinical expertise. STA, ML, PAPT, TAV, JROA, AM, and SHY provided technical expertise. STA designed the study. All authors read the manuscript, contributed to the editing, and agreed to the submission of this paper.\n5.3. Competing interests\nThe authors declare no competing interests."}]}