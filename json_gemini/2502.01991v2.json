{"title": "Can LLMs Assist Annotators in Identifying Morality Frames? - Case Study on Vaccination Debate on Social Media", "authors": ["Tunazzina Islam", "Dan Goldwasser"], "abstract": "Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks, such as identifying morality frames, make relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a \"think-aloud\" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.", "sections": [{"title": "1 Introduction", "content": "The COVID-19 pandemic marked an unprecedented moment in global history, not only as a public health crisis but also as a pivotal event in the use of digital platforms. Social media has significantly influenced public discourse, particularly in the context of highly polarized topics such as vaccinations [2, 9, 22, 27, 30, 38, 55, 61, 67, 68, 71] by playing a dual role in both disseminating vital information [28, 79-81] and spreading misinformation [20, 63, 74]. This infodemic [71] has complicated public health efforts, particularly around contentious issues like vaccination, where moral and ethical concerns often influence individual and collective behaviors.\nDebates about vaccination are often influenced by diverse moral perspectives [3, 6, 30, 64, 76]. Understanding how these moral perspectives shape public discourse is crucial for designing better tools and interventions in online environments. Morality frame analysis plays a key role in vaccine debates, as it helps to understand how moral beliefs influence individual and public opinions on vaccination. It provides a comprehensive framework for capturing the conveyed moral sentiments by identifying the relevant moral foundation (MF) [24-26] and at a fine-grained level, the moral sentiment expressed towards the entities involved [57, 66]. Moral Foundation Theory (MFT) [25, 26] suggests a theoretical framework for analyzing six moral values (i.e., foundations, each with a positive and a negative polarity) central to human moral sentiment.\nOur morality analysis is driven by social science research that highlights the connection between moral foundation preferences and COVID-19 vaccine-related health choices [13, 17, 60]. Pagliaro et al. [60] have demonstrated that the endorsement of moral principles such as fairness and care, as opposed to loyalty and authority, was found to correlate positively with trust in science. For fine-grained morality frame analysis, we employ the recently proposed morality-frame formalism [57, 66] that identifies moral roles associated with moral foundation expressions in text. These roles correspond to actor/target roles and positive or negative polarity, which can be interpreted in the context of a specific moral foundation. In Example 1, \"Pfizer vaccine\" is the negative actor in the context of degradation which causes disgust among \u201cChristian\u201d (negative target) and the reason stems from the \"use of fetal cell lines in vaccine development, framing this as a violation of sacred values, specifically within the Christian context\".\nGiven the complexity and the psycholinguistic nuances embedded in contentious topics, analyzing morality frames poses a substantial challenge, especially in the field of NLP. This challenge arises primarily from three factors. Firstly, the nuances can manifest in very different ways depending on context. For example, the concept of loyalty can be discussed in the context of the climate debate (e.g., \"support American energy\") or in the context of vaccination (e.g., \"take COVID-19 vaccine to protect your community and friends\"). Secondly, building models using supervised learning techniques requires extensive labeled datasets, which can be resource-intensive to create. Thirdly, interpreting psycholinguistic cues demands specialized knowledge and places a cognitive load on human annotators. Recent advancements in Artificial Intelligence (AI), particularly the development of Large Language Models (LLMs), present new opportunities to augment human capabilities in this domain. LLMs have demonstrated the ability to perform complex tasks with minimal input through few-shot learning [11], where the model is given a few examples to learn a new task. This capability can be leveraged to assist human annotators by generating initial labels and explanations for moral frames, thereby reducing their cognitive burden and improving the consistency and accuracy of annotations. In this paper, we investigate whether the newly emerged paradigm in NLP; few-shot prompting of LLMs [11] and the practice of providing explanations of answers when learning from examples in-context [43] -is better equipped to address those challenges.\nExplanations are fundamental to human learning [1], as they underscore task principles that facilitate broad generalizations [46, 47]. Consider the example text (\"Surprise: Fox News Hosts Are Following Strict COVID Protocols While Telling Viewers Masks and Vaccines Are Liberal Plots.\") from Fig. 1 for morality frame identification task. An explanation can elaborate on a brief answer (e.g., fairness/cheating) by connecting it to the broader reasoning process necessary to solve the problem (e.g., \"the text implies that Fox News hosts are being hypocritical by following strict COVID protocols while telling their viewers that masks and vaccines are liberal plots, which is unfair\"). Thus, explanations enhance understanding by demonstrating how task principles connect questions to their answers. Unlike previous methods of morality frame identification [57, 65, 66] that relied on annotated data [57, 66] or solely zero/few-shot prompting [65], our approach uses few-shot prompting with explanations through in-context learning with LLMs [11, 16, 40, 44]. We describe the methodology in section 3.\nWe have designed an evaluation task for LLMs-generated morality frames that utilizes a dedicated web application. This platform also provides explanations generated by the LLMs. If annotators believe the predictions are incorrect, they can submit the correct answers. The goal is to determine whether the LLMs' outputs accurately align with the expected morality frame expressed in the text, thereby facilitating a systematic assessment of the model's performance in handling complex psycholinguistic tasks. Additionally, we conduct a survey to gather feedback from participants about their experiences. We ask whether the explanations are helpful and how they improve the process, if the explanations reduce cognitive load and in what ways, the difficulty of the task rated on a scale of 1 to 5, and the time required to complete each batch of annotations. We detail the task in subsection 3.2. In this paper, we offer three primary contributions:\nInnovative Framework for Human-AI Collaborative Annotation: We introduce a novel framework that combines the strengths of LLMs with human expertise to enhance the process of morality frame identification in social media discourse. By leveraging few-shot learning with explanations, our approach not only improves the accuracy and efficiency of annotations but also significantly reduces the cognitive load on human annotators. This work demonstrates how AI can be integrated into complex psycholinguistic tasks.\nDevelopment of Web-based Annotation Tool: We have developed a specialized web-based tool that facilitates a collaborative annotation process, enabling annotators to interact with AI-generated labels and explanations. This tool is designed to support human decision-making by providing clear, context-sensitive guidance, thereby improving the overall quality of the annotation process. Our tool helps evaluate how effectively LLMs identify and interpret morality frames within complex texts.\nComprehensive Empirical Evaluation and Insights: Through"}, {"title": "2 Related Works", "content": "The rise of social media has drastically transformed public discourse, particularly on contentious topics [4, 30, 32, 34, 35, 53, 61, 69, 70], by providing a platform that can both inform [7, 12, 15] and mislead [52, 62, 72, 77] on a massive scale. This transformation has been vividly illustrated during the COVID-19 pandemic, which sparked a global infodemic [71]. Ferrara et al. [19] studied the narratives fueled with conspiracy theories and COVID-19 misinformation on the global news sentiment, on hate speech and social bot interference, and on multimodal Chinese propaganda. Wang et al. [74] developed a codebook to characterize the various types of COVID-19 misinformation images related to the virus, from false medical advice to conspiracy theories and analyzed how COVID-19 misinformation images are used on social media.\nSocial media platforms have become battlefields where moral perspectives clash, underscoring the necessity for a nuanced analysis of how moral beliefs shape public opinion and individual positions in vaccine debates [3, 6, 30, 64, 76]. MFT [25, 26] suggests a theoretical framework for analyzing the moral perspective, containing six basic moral foundations. Weinzierl and Harabagiu [76] identified vaccine hesitancy profiles in COVID-19 discussions by analyzing the underlying MFs. Islam and Goldwasser [30] analyzed COVID-19 vaccine campaigns on Facebook by predicting MF and themes. Beir\u00f3 et al. [6] employed the MFT to assess the moral narratives around vaccination debate on Facebook.\nRecently, Roy et al. [66] introduced a framework for analyzing moral sentiment called morality frames building on and extending MFT. Pacheco et al. [57] adapted this morality frame formalism that identifies moral roles associated with moral foundation expressions in the text. Pacheco et al. [57], Roy et al. [66] used annotated data to train a relational classifier using a declarative framework for specifying deep relational models called DRaiL [56]. One limitation of these works is their dependency on labeled datasets annotated by humans. In contrast, we rely on LLMs inference to generate labels and explanations for morality frames in this paper.\nRecent studies show that LLMs can perform tasks with just a few examples in-context [11, 16, 40, 44]. A variety of prior work has explored task instructions as a part of few-shot prompt [51] and can benefit from explicitly decomposing problems into multiple steps [50]. Other recent work has shown that LLMs can benefit from examples that decompose the reasoning process (can be seen as an explanation) leading to an answer [75]. Lampinen et al. [43] provided explanations after the answer in the prompt for few-shot settings.\nIn recent times, researchers have been exploring assistive possibilities of LLMs in several tasks [39, 41, 45, 78]. Kolla et al. [41] examined the feasibility of using LLMs for online moderation, i.e., identifying rule violations on Reddit. Lin et al. [45] proposed LLM-assisted macro revisions. However, researchers have evaluated the"}, {"title": "3 Methodology", "content": "In this section, we describe our collaborative annotation framework that leverages the strengths of LLMs to enhance the accuracy and efficiency of human annotations in identifying morality frames within social media discourse."}, {"title": "3.1 Identification and Explanation of Morality\nFrames with LLMs", "content": "We approach the task of identifying moral foundations as a text-generation problem, where the model is prompted in a few-shot manner to generate both the moral foundation label for a given text and provide the reasoning behind this choice. We engineer prompts in a few-shot style to generate the actor-target role with the polarity of the corresponding moral foundation and to provide an explanation. For each moral foundation of a given text, we prompt the LLMs to predict two general role types: actor and target, each with an associated polarity (positive, negative). An actor is a \"do-er\" whose actions or influence leads to either positive or negative outcomes for the target (the \"do-ee\").\nFor example, the statement \"Mark my words, we will fight Biden's authoritative COVID-19 vaccine mandate because it has no place in a free country....this is tyranny and cannot stand.\" expresses oppression as the moral foundation, where \"Biden\" is a negative actor, and \"we\" is a negative target. The classification arises because the text opposes the government's (Biden's) authoritarian action (vaccine mandate) and considers it tyranny, which restricts"}, {"title": "3.1.1 Prompt Templates", "content": "We provide general instruction, moral foundation and actor-target definitions at the beginning of the prompt as a guideline for the LLMs. Then, few-shot training examples and their associated labels, as well as explanations, are provided in the prompt. Finally, the test text is provided as the last example in the prompt, and the model is expected to generate the moral foundation label, MF explanation, actor-target role with polarity, and role explanation for the given text."}, {"title": "3.2 Tool Exploration and Human Judgments on\nMorality Frame Identification", "content": "We develop a specialized web application for collecting human judgments, allowing annotators to provide judgment about given moral foundations and actor-target polarity roles (generated by LLMs).\nWhen we contact the participants, we inform them of the task details, and they are invited to use the freely available task interface during the tool exploration phase. In this phase, participants can share their screens and explore our tools, i.e., reading instructions, participating in practice examples & demo tasks, and asking any questions. We try to make sure that participants can understand the instructions and complete the task without any issues.\nAfter ensuring that all participants reach a sufficient understanding of our task and tool, we provide each annotator batch-wise texts and LLMs-generated labels & explanations. If the given annotation is correct, the annotator will check 'yes'; otherwise, check 'no'. If the annotator chooses 'no', then the task is to find out the moral foundation of the given text, corresponding to one of six moral principles (e.g., \"I give to the poor\" expresses care), and then highlight the entities in the text according to (1) their roles - actor (a 'do-er') whose actions influence the target (the 'do-ee'), and (2) polarity, depending on the positive or negative influence of these actions. For example, \"I give to the poor\", \"I\" is a positive actor, and \"the poor\" is a positive target (benefiting from the actor's actions). On the other hand, \"We are suffering from pandemic\" expresses harm as moral principles where \"pandemic\" is a negative actor, and \"we\" is a negative target (suffering from the actor's actions). To maintain high-quality annotations, we provide the annotators with eight examples encompassing all six moral foundations and scenarios without moral (\"none\") implications. Annotators are required to read the guidelines, review the examples, and complete two practice examples before beginning the actual annotation task. Fig. 4 illustrates the provided examples, and annotators can see the explanations for choosing a moral foundation as well as an actor-target polarity by hovering mouse on"}, {"title": "3.3 Participants' Experiences Survey", "content": "We conduct a survey to collect feedback from participants who interacted with our system. The survey aimed to understand the annotators' experiences, focusing on several key areas:\nTask Difficulty: Annotators evaluate the difficulty of the task on a scale from 1 to 5 where 1 indicates \"very easy\", 2 indicates \"easy\", 3 indicates \"okay\", 4 indicates \"hard\", and 5 indicates \"very hard\"."}, {"title": "4 Experimental Details", "content": "In this paper, we conduct our study on a timely topic, the COVID-19 vaccine debate, centered around discussions on social media. We utilize the dataset introduced by Pacheco et al. [57], which consists of 750 tweets written in English about the COVID-19 vaccine from April to October 2021. Our task focuses solely on the tweet text and does not consider other attributes from that dataset."}, {"title": "4.1 Few-Shot Learning with Explanation", "content": "One of our goals is to leverage LLMs' few-shot prompting with explanation capabilities to identify morality frames discussed in a text. We use GPT-40\u00b9 [54] for our experiment. At the beginning of the prompt, we provide an overview of the task along with a description of the expected labels as part of the task instructions before introducing the few-shot examples. To alleviate the issue of being hallucinated, we force LLMs to answer the questions by selecting from the given moral foundation category only. For each task, we construct 7-shot prompts that include examples with explanations covering all six moral foundations (corresponding actor-target role with polarity) and non-moral (\"none\") cases. If the moral foundation of the text is \"none\", the corresponding actor-target-polarity will be \"none\". We place the explanation on a line after the answer (both for moral foundation and actor-target-polarity), preceded by \"Explanation:\" [43]."}, {"title": "4.2 Human Evaluation", "content": "Our specialized web tool allows annotators to assess moral foundations (with explanation) and actor-target polarity roles (with explanation) generated by LLMs. If they believe that the LLMs-generated answer is correct, they can select 'yes' and move on to the next . If they believe that the LLMs-generated answer is not correct, they can select a specific moral foundation and highlight the corresponding actor-target roles with polarity in the provided text. The graphical interface provides necessary instructions , eight examples with explanations, and two practice examples."}, {"title": "4.2.1 Participants", "content": "We compile a list of potential participants in preparation for the evaluation of our task. The prerequisites for inclusion on this list are that candidates must have at least an undergraduate-level academic degree. The initial selection is based on our academic networks. We have recruited nine (N = 9) participants-five males and four females, aged between 25 and 50. The group includes graduate students (both Ph.D. and Masters), a postdoctoral researcher, and a faculty member. Five of the nine participants are researchers in the NLP and computational social science (CSS) field. One of the participants is a Software Engineer. One participant works in Machine Learning (ML) in the Medical Surgery domain. Another is a Security researcher. Another participant has just finished their undergraduate degree and started their Masters, doing research in ML and Software Engineering. All participants reside in the United States.\nWe have developed internal guidelines that set ethical and legal standards for our research group. These guidelines include sending all potential participants a thorough information upon first contacting them, which allows them to determine whether or not they are willing to participate in the study. We inform them that there will be no compensation for their involvement. Upon agreeing to"}, {"title": "4.3 Results", "content": "If majority vote from annotators' choose 'yes' on LLMs-generated labels for a text, we consider that a 'win' situation. If the majority vote from the annotators' chooses 'no', we calculate the majority vote to get moral foundations and moral role labels provided by the annotators during the human evaluation phase for the corresponding text. We calculate the overall accuracy of the LLMs-generated morality frame prediction task as well as the accuracy and macro average F1 score for the moral foundation prediction task of LLMs. According to human evaluation, few-shot prompting with explanation using LLMs can predict morality frame in 90.79% accuracy. The results are provided in Table 2. In section 5, we provide a discussion about our key findings."}, {"title": "4.4 Survey Results", "content": "We conduct a survey to gather annotators' feedback regarding the effectiveness of explanations and their impact on reducing cognitive load, task difficulty, and the time required for each batch of annotations. Results are provided in Table 3. In subsection 5.2, we provide a discussion regarding participants' comments."}, {"title": "4.5 Ablation Studies", "content": "We provide an ablation study in which annotators provide their judgment regarding morality frames without looking at explanations. We find that the overall accuracy drops at 64.81% (1st row of Table 2) when we don't provide explanations to the annotators."}, {"title": "4.6 Analysis of Morality Framing", "content": "In this section, we dive deeper into the specific ways in which moral foundations and entity roles are framed within the vaccine debate on social media. By analyzing the correlation between different moral foundations, reasons, and stances, we aim to uncover how moral narratives are constructed and how these narratives shape public discourse."}, {"title": "4.6.1 Correlation Analysis of Moral Foundations, Reasons, and Stances", "content": "As there are previously annotated stances (i.e., pro-vax, anti-vax, neutral) for COVID-19 vaccine tweets [57] available, we use it for the analysis of 150 tweets. Pacheco et al. [57] provided the reasons (including the phrases under each reason) why people cite to support or oppose the vaccination debate. In addition, we use other themes named 'vaccine equity', 'vaccine rollout' borrowed from Islam and Goldwasser [30]. Finally, we utilize those reasons for our analysis. To evaluate the correlation between the different dimensions of the analysis, we calculate the Pearson's correlation matrices"}, {"title": "5 Discussion", "content": "This research addresses the question of whether LLMs can effectively assist human annotators in the complex task of identifying morality frames within social media content, particularly in discussions surrounding vaccination. By structuring our study around two key steps-first, generating necessary data and explanations using LLMs, and second, evaluating these outputs with human input-we aim to understand the potential and limitations of AI.\nThe task we focused on, moral frame identification, presents unique challenges due to its psycholinguistic nature. By leveraging LLMs' few-shot learning capabilities, augmented with detailed explanations, we explored how AI can reduce the cognitive load on human annotators and improve the efficiency and accuracy of their work. The integration of a \"think-aloud\" protocol allows us to gather insights into how annotators interact with Al-generated outputs, further enriching our understanding of human-AI collaboration in this context. In the following, we highlight our key findings."}, {"title": "5.1 Key Findings from LLMs' Few-shot\nPrompting with Explanation Capabilities", "content": "Our results in Table 2 suggest that LLMs can identify morality frames using the few-shot prompting with the explanation method with an overall accuracy of 90.79%. In some cases, annotators marked LLMs prediction incorrect though the moral foundation prediction is correct due to the incorrect actor-target role with polarity predictions by LLMs. That's the reason we observe higher accuracy (92.67%) and F1 score (93.51%) in moral foundation prediction than the overall morality frame prediction. This finding underscores the complexity of moral frame identification, where even advanced Al systems may struggle with nuanced aspects of psycholinguistic tasks.\nWe find that LLMs can identify moral cases better than non-moral ('none') cases. In non-moral cases, we notice that LLMs sometimes show bias in the explanation. For instance, consider the tweet: \"Pentagon to require COVID vaccine for all troops by Sept. 15\". LLMs classify this under the moral foundation of 'authority/subversion', providing the explanation: \"The text suggests the exercise of authority by the Pentagon (governmental institution) to ensure safety protocols (requiring vaccine) for the troops\". Here, LLMs depict the 'Pentagon' as a positive actor and 'troops' as a positive target, with the explanation: \"In this situation, the Pentagon (actor) is exercising their authority positively by requiring the COVID-19 vaccine for all troops (target) as a measure to protect their health\". However, the text itself does not explicitly mention any safety protocols for the troops. Actually, this text is a fact without explicit indication of moral judgment. This indicates that while LLMs can be powerful tools, careful oversight, and human judgment remain essential in tasks that involve moral and ethical considerations.\nMorality frame identification is a complex task, and occasionally, we encounter three-way disagreement among annotators. For example, in the following text: \"We were in a better situation this time last year with no vaccine. The gene jab is the equivalent of pouring petrol on the flames, leaky vaccines lead to more variants & the trialists have destroyed their immune systems. Bunch of Turkeys and xmas is coming!\", LLMs identify moral foundation as 'sanctity/degradation'. One annotator thinks LLMs generated answer is corret, and two annotators think it is incorrect. Of these dissenting annotators, one identifies the moral foundation of this text as 'none', whereas the other opts for 'care/harm'. We resolve those issues through discussion. These disagreements highlight the inherent subjectivity in moral frame identification and the need for discussion and consensus-building when using AI tools in qualitative research. Such instances underscore the value of a collaborative framework where Al augments rather than replaces human judgment."}, {"title": "5.2 Key Findings from Participants' Survey", "content": "Feedback from the participant survey reinforces the value of integrating LLM-generated labels and explanations into the annotation process. In the feedback survey, five (P1, P2, P3, P5, P6) out of nine participants mention the morality frame identification task without having any labels and explanations is very hard task. However, they noted that the addition of LLMs-generated labels and explanations reduced the task difficulty to easy (Table 3). Three participants"}, {"title": "6 Conclusion and Future Work", "content": "Our study has validated the significant role that LLMs can play in supporting annotators during complex psycholinguistic tasks such as morality frame identification within the context of vaccination debates on social media. We prompt LLMs with few-shot examples and explanations to identify moral foundation and actor-target-polarity with corresponding explanations. By integrating LLMs-generated answers with explanations into the annotation process, our study shows whether and to what extent it will aid annotators and streamline workflow. Our research offers insights into how LLMs can assist annotators and lays the groundwork for future research in generating data and explanations for complex psycholinguistic tasks.\nHowever, the study also underscores the importance of human oversight in AI-assisted tasks, particularly in areas requiring nuanced understanding and interpretation. The occasional misclassifications and biases introduced by the LLMs highlight the need for"}, {"title": "7 Limitations", "content": "LLMs are pre-trained on a huge amount of human-generated text. As a result, they may inherently contain many human biases . We did not consider any bias in our task.\nA previous study by Johnson and Goldwasser [37] has shown that a single tweet may contain multiple moral foundations. We did not consider multi-label moral foundations in this work.\nWe are aware of the limitations due to potential biases arising from our study participant selection.Our study relied on voluntary participation, which might induce bias as potential participants would agree to participate if they had at least some degree of interest and openness toward the psycholinguistic task. Additionally, in technology-based studies, \"novelty bias\" could influence the study outcome when participants are excited to try something new . Hence, we made sure that our sample included participants with different age groups, academic levels, diverse research areas, and gender (see subsection 4.2.1).\nThe main objective of this paper is to determine whether labels and explanations generated by LLMs are helpful and can assist annotators on hard psycholinguistic tasks. However, the approach is designed to be scalable without any modifications."}]}