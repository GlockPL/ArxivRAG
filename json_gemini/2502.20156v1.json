{"title": "Adaptive H&E-IHC information fusion staining\nframework based on feature extractor", "authors": ["Yifan Jia", "Xingda Yu", "Zhengyang Ji", "Songning Lai", "Yutao Yue"], "abstract": "Immunohistochemistry (IHC) staining plays a significant role\nin the evaluation of diseases such as breast cancer. The H&E-to-IHC\ntransformation based on generative models provides a simple and cost-\neffective method for obtaining IHC images. Although previous models\ncan perform digital coloring well, they still suffer from (i) coloring only\nthrough the pixel features that are not prominent in HE, which is easy\nto cause information loss in the coloring process; (ii) The lack of pixel-\nperfect H&E-IHC groundtruth pairs poses a challenge to the classical\nL1 loss. To address the above challenges, we propose an adaptive infor-\nmation enhanced coloring framework based on feature extractors. We\nfirst propose the VMFE module to effectively extract the color infor-\nmation features using multi-scale feature extraction and wavelet trans-\nform convolution, while combining the shared decoder for feature fu-\nsion. The high-performance dual feature extractor of H&E-IHC is trained\nby contrastive learning, which can effectively perform feature alignment\nof HE-IHC in high latitude space. At the same time, the trained fea-\nture encoder is used to enhance the features and adaptively adjust the\nloss in the HE section staining process to solve the problems related\nto unclear and asymmetric information. We have tested on different\ndatasets and achieved excellent performance. Our code is available at\nhttps://github.com/babyinsunshine/CEFF", "sections": [{"title": "1 Introduction", "content": "Immunohistochemistry (IHC) staining is a widely used technique in pathology\nfor visualizing common abnormal cells in tumors, which is crucial for developing\nprecise treatment plans. However, traditional detection methods are both time-\nconsuming and labor-intensive, with standard tissue pathology imaging involving"}, {"title": "2 Method", "content": "Figure 1 provides an overview of our proposed framework for adaptive IHC\nvirtual staining. As shown in Figure 1(a), the architecture is centered on the\nMulti-Scale Modulated Feature Fusion Generator, which utilizes the Virtual"}, {"title": "2.1 Multi-Scale Feature Extraction and Fusion", "content": "Considering the issues of error propagation during sampling from low reso-\nlution to high resolution in a U-Net-like generator, as well as the insufficient\ninformation processing in large-scale skip connections, we propose a generator\nbased on Virtual Multi-scale Feature Extraction (VMFE). The basic structure"}, {"title": "2.2 Contrastive Learning Strategy of Dual Encoders", "content": "In medical image processing, there must be an inherent connection between\nthe images before and after staining, that is, they contain a large amount of\nthe same semantic information. Based on this, we propose to use the method of\ncontrastive learning to train two encoders, which respectively encode the images\nbefore and after staining. Pathological images contain a vast amount of complex\ninformation, and it is difficult to comprehensively capture all features using a\nsingle encoder. Therefore, we use two independent encoders for separate train-\ning to ensure that various features in the images can be fully mined, and to\nimprove the comprehensiveness and accuracy of feature extraction. To measure\nthe similarity and dissimilarity between encoded features, guiding the encoder to\nlearn more discriminative and representative image features, We use the InfoNCE\nloss [18] function:\n\n$L_{NCE} = - \\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp(s(z_i, z_i^+)/\\tau)}{\\sum_{j=1}^{M} exp(s(z_i, z_j)/\\tau)}$   (1)"}, {"title": "2.3 Cross-Attention Feature Fusion between Encoder and\nGenerator", "content": "To leverage the information captured by the trained H&E encoder owing to\nthe use of contrastive loss, which encodes mutual information between H&E and\nIHC images we propose a cross-attention fusion module. This module integrates\na feature map from a specific layer of the encoder with the first feature map of\nthe generator block to guide the staining process.\nGiven the generator feature map $F_{gen} \\in R^{B\\times C\\times H \\times W}$ and the encoder feature\nmap $F_{enc} \\in R^{B\\times C\\times H \\times W}$, , we generate queries Q, keys K, and values V via 1\u00d71\nconvolutions, followed by reshaping into $R^{B\\times N \\times d}$, where N = H \u00d7 W and d is\nthe feature dimension. The fusion process is defined as follows:\nThe output feature map is computed as:\n\n$F_{out} = F_{gen} + \\alpha\\cdot BN (W_{out} * (softmax (\\frac{QK^T}{\\sqrt{d}}))V)$,   (4)"}, {"title": "2.4 Adaptive L1 Loss", "content": "Due to the non-strict symmetry between H&E images and IHC images, we\nadapt the L1 loss weight by leveraging the encoding information from the IHC\nencoder. The generated image and the ground truth are divided into multiple\npatches, and the cosine similarity of the corresponding patches' embedding vec-\ntors, after passing through the IHC encoder, is computed. The adaptive L1 loss\nis defined as:"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Datasets In this study, we selected two key datasets: the Breast Cancer Im-\nmunohistochemistry (BCI) [6] Challenge dataset and the MIST dataset [9]. The\nBCI dataset comprehensively covers different levels of HER2 expression, provid-\ning a rich data foundation for in - depth research on the characteristics related to\nHER2 expression. The MIST dataset, on the other hand, contains immunohis-\ntochemical staining data for HER2, PR, ER, and Ki67, presenting information\non breast cancer related indicators from multiple dimensions. Our division of\nthe test set and training set is consistent with that in the original paper."}, {"title": "Experimental Details", "content": "Our model was trained on an NVIDIA RTX 3090 GPU.\nFor both the encoder and the model, we employed the Adam optimizer. The\nencoder was trained for 300 epochs with a batch size of 64, while the model was\ntrained for 100 epochs with a batch size of 1. We randomly cropped the images\nto a size of 512\u00d7512 for training. The fusion strength was set to 0.2, while the\nparameters a and \u1e9e of the adaptive L1 loss were both set to 50."}, {"title": "Evaluation Methods", "content": "To comprehensively evaluate the model, we adopted\nmultiple metrics. PSNR measures the distortion between generated and real\nimages, with a higher value indicating better quality. SSIM assesses structural\nsimilarity, closer to 1 meaning more similar structures and better aligning with\nhuman vision. FID quantifies the difference between the distributions of gener-\nated and real images, with a lower value denoting better quality and diversity."}, {"title": "3.2 Comparative Experiments", "content": "The performance of the dual encoder. Dual Encoders aim to capture the\nconsistency of paired H&E and IHC images using contrastive learning. In this\nsection, we show the effectiveness of the dual encoder. We test the performance\nof the dual encoder by constructing paired pairs of positive samples and unpaired"}, {"title": "4 Conclusion", "content": "We propose an adaptive IHC virtual staining method framework using contrastive-"}]}