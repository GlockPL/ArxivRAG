{"title": "Adaptive H&E-IHC information fusion staining framework based on feature extractor", "authors": ["Yifan Jia", "Xingda Yu", "Zhengyang Ji", "Songning Lai", "Yutao Yue"], "abstract": "Immunohistochemistry (IHC) staining plays a significant role in the evaluation of diseases such as breast cancer. The H&E-to-IHC transformation based on generative models provides a simple and cost- effective method for obtaining IHC images. Although previous models can perform digital coloring well, they still suffer from (i) coloring only through the pixel features that are not prominent in HE, which is easy to cause information loss in the coloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs poses a challenge to the classical L1 loss. To address the above challenges, we propose an adaptive infor- mation enhanced coloring framework based on feature extractors. We first propose the VMFE module to effectively extract the color infor- mation features using multi-scale feature extraction and wavelet trans- form convolution, while combining the shared decoder for feature fu- sion. The high-performance dual feature extractor of H&E-IHC is trained by contrastive learning, which can effectively perform feature alignment of HE-IHC in high latitude space. At the same time, the trained fea- ture encoder is used to enhance the features and adaptively adjust the loss in the HE section staining process to solve the problems related to unclear and asymmetric information. We have tested on different datasets and achieved excellent performance.", "sections": [{"title": "1 Introduction", "content": "Immunohistochemistry (IHC) staining is a widely used technique in pathology for visualizing common abnormal cells in tumors, which is crucial for developing precise treatment plans. However, traditional detection methods are both time- consuming and labor-intensive, with standard tissue pathology imaging involving in vivo tissue sampling, tissue fixation, tissue processing, section staining, microscopic observation, image capture, and image analysis [1]. These factors hinder the widespread applicability of IHC staining in tissue pathology. With advance- ments in computer vision technology, researchers have applied computer vision techniques to the slide staining process (virtual staining), significantly improving detection efficiency and saving valuable time for patient treatment [2-5]. Existing virtual staining methods are mainly based on adversarial genera- tion techniques. Liu et al. proposed PyramidPix2Pix [6], which applies Gaussian convolutions to image pairs and processes them at multiple scales, reducing the requirement for pixel-level precise alignment. Li et al. introduced a novel loss function designed to mitigate the negative impact of these inconsistencies on model performance [6]. This loss function enables the model to better han- dle noise or low-quality data, thereby improving the robustness of the staining transformation. Li et al. also designed a multi-layer weak pathological consis- tency constraint, combined with an adaptive weight strategy and discriminator contrastive regularization loss, which significantly enhances the pathological con- sistency and realism of generated tissue slices [7]. Although the aforementioned studies have made significant advancements in the field of virtual staining, there are still several aspects that have not been fully addressed. i) Existing works mainly focus on j pixel information based stain generation task, overlooking the correspondence between potential staining grade labels of HE and IHC slides, which is often a key factor that doctors consider during diagnosis. ii) The feature extraction methods used in current generator networks are limited and tend to overlook critical details, leading to poor detail in the generated IHC images. The information features in HE slides are not immediately apparent, which places a significant demand on the feature extraction capabilities of the generator network. iii) The lack of pixel-perfect H&E-IHC groundtruth pairs poses a challenge to the classical L1 loss. To address the aforementioned issues, we make the following contributions: 1) We propose the VMFE module, which employs multi-scale feature extraction and utilizes wavelet transform convolutions [8,13,14] for efficient extraction of stain- ing information features, while incorporating a shared decoder for feature fusion. 2) Inspired by contrastive learning [15,16], we pre-train feature encoders for HE (Hematoxylin and Eosin) and IHC (Immunohistochemistry) images, aiming to unsupervisedly align staining labels for HE and IHC images in the latent space. 3) We leverage the trained feature encoders to enhance features and adaptively adjust the loss during the staining process for HE slides [17], addressing issues related to unclear and asymmetric information. Finally, we conduct extensive testing across multiple datasets to validate the effectiveness of our method."}, {"title": "2 Method", "content": "Figure 1 provides an overview of our proposed framework for adaptive IHC virtual staining. As shown in Figure 1(a), the architecture is centered on the Multi-Scale Modulated Feature Fusion Generator, which utilizes the Virtual Multi-scale Feature Extractor (VMFE) to process H&E images. It achieves this by processing the downsampled features through the VMFE module and fusing them with later-layer feature maps to fully leverage information. Additionally, we use the Cross-Attention module (CoA) to fuse the feature maps obtained from the encoded H&E images with those from the generator, providing more guidance for IHC image generation. Figure 1(b) highlights the pre-training pro- cess of the HE Encoder and IHC Encoder, where contrastive learning (using the InfoNCE loss function) trains the encoders to capture the semantic relation- ships between H&E and IHC images. Figure 1(c) illustrates the adaptive L1 loss mechanism, which dynamically adjusts the loss weights based on the cosine sim- ilarity between the patch embedding vectors of the generated IHC image and the ground truth image, obtaining an adaptive weighted L1 (AWL) loss to address the non-strict symmetry issue between H&E and IHC images, thereby improving staining accuracy."}, {"title": "2.1 Multi-Scale Feature Extraction and Fusion", "content": "Considering the issues of error propagation during sampling from low reso- lution to high resolution in a U-Net-like generator, as well as the insufficient information processing in large-scale skip connections, we propose a generator based on Virtual Multi-scale Feature Extraction (VMFE). The basic structure of this generator is resnet-6blocks, with VMFE replacing its downsampling com- ponent. VMFE primarily consists of wavelet convolution downsampling and a Multi-scale Sequential Feature Processing Module (MSFPM). For the input im- age X, wavelet convolution-based downsampling layers produce multi-scale fea- ture maps $X_1$, $X_2$, and $X_3$ with scales of 1, 1/2, and 1/4, respectively. These multi-scale feature maps have a larger receptive field. Then, mimicking the coarse-to-fine approach of traditional U-Net, we sequen- tially input the multi-scale feature maps $X_3$, $X_2$, and $X_1$ into the MSFPM. The MSFPM utilizes a convolution-based Gated Recurrent Unit (GRU) to modu- late the content between the previous activation $h_{t-1}$ and the current input $h_t$. This is because there exists an abstract temporal relationship among the fea- ture maps obtained through downsampling. By using this module, we aim to enable the network to comprehensively consider the sequential relationship of each feature map. The hidden state update of the module can be simplified as:\n$h_t = \\text{MSFPM}(X_t, h_{t-1}),$\nwhere $X_t$ ($t = 3,2,1$) denotes the input feature map, and $h_t$ represents the output hidden state. Since $X_3$, $X_2$, and $X_1$ have different scales, we downsample each feature map to a 1/4 scale, denoted as $\\tilde{X}_t = \\text{Downsample}(X_t,1/4)$. Then, the module's outputs $h_3$, $h_2$, and $h_1$ are respectively fused with the second, third, and fourth feature maps in the generator block via addition, i.e.,\n$F_k^\\prime = F_k + h_{4-k}, k = 2,3,4,$\nwhere k denotes the index of the feature maps in the generator block (correspond- ing to k = 2,3,4 for the second, third, and fourth feature maps, respectively), $F_k$ represents the original feature map, and $F_k^\\prime$ denotes the fused feature map, thereby enhancing the model's performance."}, {"title": "2.2 Contrastive Learning Strategy of Dual Encoders", "content": "In medical image processing", "18": "function:\n$\\mathcal{L}_{NCE} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(s(z_i, z_i^+)/\\tau)}{\\sum_{j=1}^M \\exp(s(z_i, z_j)/\\tau)}"}]}