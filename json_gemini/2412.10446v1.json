{"title": "Disentanglement and Compositionality of Letter Identity and Letter Position in Variational Auto-Encoder Vision Models", "authors": ["Bruno Bianchi", "Aakash Agrawal", "Stanislas Dehaene", "Emmanuel Chemla", "Yair Lakretz"], "abstract": "Human readers can accurately count how many letters are in a word (e.g., 7 in \"buffalo\"), remove a letter from a given position (e.g., \u201cbufflo\") or add a new one. The human brain of readers must have therefore learned to disentangle information related to the position of a letter and its identity. Such disentanglement is necessary for the compositional, unbounded, ability of humans to create and parse new strings, with any combination of letters appearing in any positions. Do modern deep neural models also possess this crucial compositional ability? Here, we tested whether neural models that achieve state-of-the-art on disentanglement of features in visual input can also disentangle letter position and letter identity when trained on images of written words. Specifically, we trained beta variational autoencoder (B-VAE) to reconstruct images of letter strings and evaluated their disentanglement performance using CompOrth - a new benchmark that we created for studying compositional learning and zero-shot generalization in visual models for orthography. The benchmark suggests a set of tests, of increasing complexity, to evaluate the degree of disentanglement between orthographic features of written words in deep neural models. Using CompOrth, we conducted a set of experiments to analyze the generalization ability of these models, in particular, to unseen word length and to unseen combinations of letter identities and letter positions. We found that while models effectively disentangle surface features, such as horizontal and vertical 'retinal' locations of words within an image, they dramatically fail to disentangle letter position and letter identity and lack any notion of word length. Together, this study demonstrates the shortcomings of state-of-the-art 3-VAE models compared to humans and proposes a new challenge and a corresponding benchmark to evaluate neural models.", "sections": [{"title": "Introduction", "content": "Reading is an invention of human modern culture. Unlike other domains of visual processing such as of faces, reading skills are not innate and require extensive practice (Dehaene et al., 2015). Reading acquisition therefore must rely on the development of a new neural mechanism in the human brain. Given that letters are the building blocks of words, to process new words, the neural mechanism needs to identify single letters in the input, their positions in a word, and compose this information to process entire words. While brain imaging has localized where in the brain visual processing of words occurs (Cohen et al., 2002), what is the precise neural mechanism that enables us to recognize words is largely unknown.\nRecent advances in deep neural network models have drastically improved the accuracy on Optical Character Recognition (OCR) tasks (Li et al., 2023). Deep neural models can now achieve similar-to-human performance on a variety of tasks, including vision and language. Although neural models are often considered black boxes, full access to their neural computations during processing is possible. Analyzing the properties of these networks provides new opportunities to study neural mechanisms underlying orthographic processing, and in particular, into how letter-identity and letter-position information are extracted from raw images and then composed together to encode whole words.\nTo study this question in neural models, we developed CompOrth - a battery of tests, which evaluates compositionality in models and their generalization performance. CompOrth provides several tests, which can be used to both evaluate the 'behavioral' performance of the models, as well as study neural mechanisms in the model. The tests are designed in a way that directly probes the question of whether a neural model extracts and disentan-"}, {"title": "Related Literature", "content": "Literature on cognitive sciences contains competing theories about how words are neurally encoded in the human brain. Based on experiments in humans, early studies theorized the presence of letter combination detectors (Dehaene et al., 2005), i.e., letters combine to form bigrams, which are further combined to form larger n-grams and finally words. Other theories suggested neural encoding based on open bigrams - neurons that are tuned for higher-order combinations of letters, which are not necessarily adjacent (Grainger and Whitney, 2004). Most recently, a compositional neural encoding scheme was suggested, whereby letter identities and letter positions directly combine to form words (Agrawal et al., 2020), in line with early suggestions in the literature (Davis and Bowers, 2004).\nModern neural models now provide new opportunities to study how words can be neurally encoded, possibly informing the above debate. One of the computational tasks in the field, most closely related to reading, is Optical Character Recognition (OCR). Various neural models were suggested for OCR, including Recurrent Neural Networks (Breuel et al., 2013), Convolutional Neural Networks (CNNs) (Zhang et al., 2017), Transformers (Li et al., 2023; Azadbakht et al., 2022), and hybrid architectures (Naseer and Zafar, 2019; Jain et al., 2017). Some studies have also tried to identify neural mechanisms in models trained on OCR, or similar tasks. For example, Hannagan et al. (2021)"}, {"title": "General Setup", "content": "The stimuli generated for CompOrth were designed to probe the encoding of single letters and their positions, and therefore they minimize the amount of other types of information the model needs to learn. For this, the strings (hereafter, 'words') in each test contain only two letters (e.g.,"}, {"title": "The CompOrth Benchmark", "content": "Stimuli: The stimuli generated for CompOrth were designed to probe the encoding of single letters and their positions, and therefore they minimize the amount of other types of information the model needs to learn. For this, the strings (hereafter, 'words') in each test contain only two letters (e.g., \u201cA\u201d and \u201cB\u201d), in the same case (upper case), the same font ('Arial') and the same letter size ('12'). For each combination, we generated all 62 possible words of 1 to 5 letters (e.g., \u201c\u0410\u201d, \u201cB\u201d, \u201cAA\u201d, \u201c\u0410\u0412\u201d, \u201c\u0412\u0412\u0412\u0412\u0410\u201d, \u201cBBBBB\u201d). Also, for each word, images were generated by varying their location in the image (hereafter, \u2018retinal' location), and the spacing between characters, resulting in a total of 11,904 images. All images were generated with white letters on a black background (Figure A.1).\nThe retinal location was varied by shifting the position of the string from the center of the image both vertically and horizontally. Specifically, words with zero displacement in both axes have their center at the center of the image. Meanwhile, words with a displacement of 1 in both axes will have their center shifted one pixel up and one pixel to the right. On the horizontal axis, the strings were moved from 4 pixels to the left to 4 pixels to the right. On the vertical axis, the strings were moved from 4 pixels up to 4 pixels down (Figure A.1).\nSpacing variation was introduced to decouple retinal (absolute) location of a letter within an image and its (relative) position within a word. This factor modifies the default font spacing and moves each character a certain number of pixels to the left or right. Ensuring that contiguous characters do not overlap, the spacing was varied from 2 pixels to the left to 2 pixels to the right, with 0 being the default spacing.\nThree Generalization Tests: CompOrth contains three groups of tests, of increasing complexity: (1) Spatial Generalization (Figure 1A-Left), which evaluates generalization across \u2018retinal' positions, (2) Length Generalization (middle), which evaluates generalization to unseen word lengths (shorter or longer), and (3) Compositional Generalization (right), which evaluates generalization to unseen combinations of letter identities and letter positions, thus evaluating whether the models develop abstract notion of letter position and letter identity. Each test consists of several splits into training and test sets, ensuring that in each split, the test set contains images generated from different combinations of factors than those in the training set. That is, each of the sets was generated by selecting one level of a given generative factor (e.g., bottom right-most shift of retinal position, as in Figure 1A-Left) and removing it from the training set. The corresponding test set then comprises all the images generated with the left-out level of the"}, {"title": "Generalization", "content": "factor (blue square). Specifically,\nSpatial Generalization: For each possible combination of x-y-shift, all words with that combination are used as the left-out while the rest of the words are used as the left-in set (Figure 1A, left).\nLength Generalization: For each length, all words with that length are used as the left-out set, while the rest of the words are used as the left-in set (Figure 1A, middle).\nCompositional Generalization: For each relative position of each letter (e.g., 'A' in 2nd position), all words with that combination (e.g., \u201cAA\u201d, \u201cBA", "AAA": "BAB\", etc.) are used as the left-out set, while the rest of the words are used as the left-in set (Figure 1A, right).\"\n    },\n    {\n      \"title\": \"Models\",\n      \"content\": null\n    },\n    {\n      \"title\": \"Model Architecture\",\n      \"content\": \"We benchmarked CompOrth with Variational Auto-Encoders (VAEs; Kingma and Welling, 2013), including a more recent variant of this model, known as B-VAE (Higgins et al., 2017). Following Higgins et al. (2017), the models comprised of a 4 convolutional and 2 fully connected layers encoder. The decoder has a mirrored architecture (Figure 1B). For CompOrth, an advantage of evaluating auto-encoders is that they can be tested on unseen words. In contrast, standard feed-forward classifiers have a finite set of output units (Hannagan et al., 2021), corresponding to different words, and therefore evaluating the model on unseen words is often not possible without retraining the model. Moreover, B-VAEs can be optimized to achieve neural disentanglement, which encourages the activity of single units in the latent layer to encode different generative factors of the training data, such as letter identity and letter position.\"\n    },\n    {\n      \"title\": \"Model Training\",\n      \"content\": \"For training, we used a batch size of 64 samples. Training and Evaluation were conducted with Nvidia Quadro RTX 8000 48 GB GPUs. The whole experiment, including grid search, took about 72 hours. For the grid search, we optimized for the following hyper-parameters: Beta ($2^i$, \u0456 \u2208 {0, ..., 7}), size of the latent layer ($2^i$, i \u2208 {3, ..., 7}) and initial learning rate ($10^i$, \u0456 \u2208 {\u22124, ..., \u22122}). The optimal learning rate across all combinations in the grid search was consistently $10^{\u20134}$. We set the maximal number of epochs to 1000, which we verified to be large enough to reach full convergence in all cases.\"\n    },\n    {\n      \"title\": \"Model Evaluation\",\n      \"content\": \"Reconstruction Loss For model selection, we used the standard reconstruction loss for visual AEs, which is calculated based on the pixel-by-pixel difference between the original and the reconstructed images.\nReconstruction Accuracy However, for testing compositionality with CompOrth, mere reconstruction loss might be little informative. This is since a large number of pixels in the images could be simply black, therefore the reconstruction loss might be low albeit poor recognizability of the reconstructed word. Moreover, the reconstruction of blurry unidentifiable letters can help to lower reconstruction loss without genuinely improving the word recognizability (see examples in Figure 2B). This is crucial when evaluating the model on unseen images when the model is faced with the compositionality challenge. To address this, we defined another evaluation metric \u2013 Reconstruction Accuracy, which quantifies word recognizability. For this, we trained another model - a standard feed-forward CNN-based classifier, using the entire set of words in CompOrth, and used it to evaluate the reconstruction quality of the B-VAE. We refer to this model as the Evaluator model. The Evaluator was trained with the original images only, until it reached perfect performance, and presented, at test time, with reconstructed images from the B-VAEs. The Evaluator contained 2 consecutive convolutional layers and 2 linear layers, and the last layer was a softmax layer across the 62 possible words. Reconstruction accuracy was then defined as the output probability of the CNN classifier for the desired word. For example, given an image with an unseen image containing \u201cABABA\", the reconstructed image (the output of the VAE) was presented as an input to the CNN classifier. The output probability of the CNN classifier, which corresponds to the target word \u201cABABA"}, {"title": "Results", "content": null}, {"title": "Model selection", "content": "We first optimized for the hyperparameters of the B-VAE models, using nested cross-validation. Figure 2A illustrates model selection, by showing both the reconstruction loss and the MIR for all models in the grid-search. Since both good reconstruction and disentanglement are desired properties of a model, the optimal models lie on the Pareto front (purple circles) of the problem. No other models outperform them in both criteria simultaneously. In what follows, we therefore report results based on average performance across all optimal models on the Pareto front. We later analyze particular cases from these models (section 4.3).\nTo illustrate reconstruction ability of the models,"}, {"title": "Behavioral Evaluation using CompOrth", "content": "Spatial Generalization \u2013 B-VAEs can generalize to unseen 'retinal' locations: Figure 3A-Left shows the mean generalization performance to unseen retinal positions, for all models from the Pareto front (mean performance in black). On average, the models show good ability to reconstruct words in positions where they were not seen during training. Except for three models on the Pareto front ($layer \u2013 size(ls) = 16$, orange and green lines, and beta = 64, brown line) all other individual models achieve mean accuracy above 90% for all spatial generalizations, also for vertical generalization (Figure A.2) . Figure 3A-Right further shows reconstruction examples: each of the plots shows 6 random samples (top) and their re-"}, {"title": "Spatial Generalization \u2013 B-VAEs fail to generalize to longer word lengths", "content": "construction (bottom) by a given model. Overall, the reconstruction is, qualitatively, similar to the original image, even for models with relatively low performance.\nLength Generalization \u2013 B-VAEs fail to generalize to longer word lengths: Figure 3B-Left shows the reconstruction accuracy for all left-out word lengths, as measured with the Evaluator Model (section 3.2.3). Overall, reconstruction accuracy for short left-out word length is high, in particular for the four models with also better performance on spatial generalization. However, a qualitative inspection of random samples from these models (3B-Right, for examples) show that letter parts are nonetheless present in the reconstruction of images with a single letter, remnants from the longer words in the training data. In general, for longer word lengths, generalization performance decreases. In particular, generalization performance for words with five letters is lowest."}, {"title": "Compositional Generalization \u2013 B-VAEs fail to generalize to unseen compositions of letter identity and letter position", "content": "A qualitative inspection shows that, indeed, the reconstruction of words with five letters contains in many cases only four letters (red rectangle).\nCompositional Generalization: B-VAEs fail to generalize to unseen compositions of letter identity and letter position: Finally, the compositional test assessed the ability of the models to develop an abstract understanding of letter position. Overall, results show that the compositional test was most challenging for the models compared to the other two tests (Figure 3C-Left), with one model's performance approaching chance level. Figure 3C-Right illustrates the type of errors the models make (e.g., in red rectangles). For example, when presented with strings where 'B' was never present in the 5th position in the training data, the models 'hallucinate' an 'A' in this position. Similar errors occur when 'B' or 'A' were omitted from other positions during training."}, {"title": "Neural Evaluation using Perturbation Experiments", "content": "We next studied to what extent beta-VAEs develop neural disentanglement of letter position and letter identity information. Figure 4A illustrates what such neural disentanglement could look like \u2013 it shows a possible encoding scheme, where different units of the latent layer encode for different positions in the word, and different levels of activity encode for different level identities. Therefore letter identity is independently encoded of letter position.\nTo test whether such neural disentanglement emerges in the model, even partly, we conducted a perturbation experiment. That is, given an input image from the training set, we computed the neural activations at the latent layer of the model, and then separately for each unit, we systematically perturbed its activity to different levels. After each perturbation, we then reconstructed the image. The difference between the input image and the reconstructed image (after perturbation) is revealing about the information that the perturbed unit encodes. For example, if a model developed neural disentanglement of letter position and letter identity (Figure 4A), then perturbing one of the latent units can cause a replacement of one letter with another one, in only the perturbed position.\nFigures 4B-D show examples from the perturbation experiments, from the model with the best reconstruction loss and strong performance on CompOrth (\u03b2 = 4, latent-size= 32; red lines in Figure 3). Each panel corresponds to a latent unit, rows of each panel correspond to different samples (input images) and columns to different levels of perturbation.\nUnit 22 in the latent layer of the model (Panel B) illustrates spatial encoding \u2013 positive perturbations caused increased translation of the reconstructed string along the vertical axis. However, spatial information is not fully disentangled from other factors, as perturbations also led to changes in the letters of the string. Unit 3 of the model (Panel C) illustrates word-length encoding \u2013 in some cases, increased perturbations added letters to the strings. However, here too, other types of information, such as letter identity, seem to be encoded in this unit. Finally, unit 23 (Panel D) illustrates the encoding of letter identity \u2013 increased perturbations led, in some cases, to change of letter identity. Figures A.3 & A.4 show perturbation effects for all 32 units"}, {"title": "The Relationship between Neural Disentanglement and Compositionality", "content": "However, analyzing all 32 latent units in the model, we did not identify an encoding scheme that fully disentangle letter identity and letter position (e.g., Figure 4A). The examples above provide only sporadic evidence in this direction, from example units, and none of the models on the Pareto front developed strong disentanglement. This is, in fact, consistent with the relatively poor performance of all models on the Compositional-Generalization test in CompOrth. Limited neural disentanglement is consistent with poor compositionality and thus low performance on CompOrth.\nWhile we haven't discovered strong neural disentanglement of identity and position in the previous section, a weak neural disentanglement might have nonetheless emerged in some of the models, which is hard to detect with mere perturbation experiments. Such weak disentanglement would possibly lead to a small, yet significant, improvement in performance on the Compositional-Generalization test in CompOrth.\nWe therefore next tested the hypothesis that neural disentanglement facilitates the separation of letter-positions and letter-identity information, and therefore, in turn, their composition. This predicts that models that achieve high neural disentanglement, as measured by MIR (section 3.2.3), will achieve better performance on compositional generalization, as measured by the Compositional-Generalization test in CompOrth. To test this, we computed the correlation between the MIR and reconstruction accuracy on CompOrth for all models on the Pareto front. We found a weak correlation p = 0.13 (Figure A.5), however, which was not statistically significant (p \u2013 val = 0.26)."}, {"title": "Summary and Discussion", "content": "We introduced CompOrth, a novel benchmark for evaluating orthographic processing in visual models. The primary goal of CompOrth is to assess compositionality \u2013 the ability of a model to generalize to new combinations of letter identities and positions beyond the training set. This task is considered trivial for humans, so passing the CompOrth test is essential for a model to be regarded as achieving human-like performance.\nWe tested a large number of variational auto-"}, {"title": "Encoding scheme", "content": "ciple in auto-encoders could encourage models to learn underlying patterns in the data by forming abstract representations and relying on rule-based encoding rather than memorization. However, there's a trade-off with reconstruction accuracy. An excessively constrained bottleneck can compromise reconstruction performance, as illustrated in the hyperparameter optimization process (Figure 2).\nWe further hypothesized that neural disentanglement, which tends to emerge in B-VAEs with high values of \u00df, would facilitate the separation of letter-positions and letter-identity information, and in turn, their composition. To test this, we conducted perturbation experiments with \u00df-VAEs, to see whether some of the units disentangle identity and position information. Exploring all units in the model, we have not identified any such units, which is consistent with the failure of the models on CompOrth. However, a weak neural disentanglement of identity and position may have emerged in some of the models, unobserved by our perturbation experiments. We therefore tested whether there exists a correlation between MIR and CompOrth performance, across all our VAE models. We found no significant correlation between MIR and reconstruction accuracy on CompOrth.\nThe observed failure of the models in this study is one more example of the shortcoming of artificial neural networks to dynamically and flexibly bind information, which might be distributedly encode in the network (Greff et al., 2020), even when neural disentanglement is explicitly optimized, as in B-VAEs. This binding problem affects the capacity of the models to achieve compositional ability by manipulating symbols (letters) and combine them in various, unbounded, ways, as humans (Fodor and Pylyshyn, 1988). Similar limitation, for similar reasons, was observed also in language models (Del\u00e9tang et al., 2022). However, unlike existing benchmarks for testing compositionality in language models (e.g., Lake and Baroni, 2018), for orthographic processing and OCR, there are no existing targeted benchmarks. CompOrth therefore aims to fill in this gap, by providing means to evaluate compositionality in vision models. Our study shows that B-VAE models do not achieve good compositionality and achieve only a limited neural disentanglement of letter position and identity, which suggests CompOrth as a simple yet challenging test for future models."}, {"title": "Ethical statement", "content": "This paper presents work whose goal is to bridge closer the fields of Machine Learning and Psycholinguistics; being theoretical in nature, we believe that no societal risks need to be specifically highlighted."}, {"title": "Limitations", "content": "This study investigates the ability of a neural architecture to disentangle relevant information in the input for compositional generalization. One possible limitation is that the models explored here were trained on the CompOrtho dataset only. However, we note that while the models were trained from scratch on CompOrth, the challenges posed by the benchmark are also applicable to pre-trained models. These models can be refined and evaluated using the same approach (Figure 1A) to assess their compositional generalization capabilities."}]}