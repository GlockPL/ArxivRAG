{"title": "DFM: Interpolant-free Dual Flow Matching", "authors": ["Denis Gudovskiy", "Tomoyuki Okuno", "Yohei Nakata"], "abstract": "Continuous normalizing flows (CNFs) can model data distributions with expressive\ninfinite-length architectures. But this modeling involves computationally expen-\nsive process of solving an ordinary differential equation (ODE) during maximum\nlikelihood training. Recently proposed flow matching (FM) framework allows\nto substantially simplify the training phase using a regression objective with the\ninterpolated forward vector field. In this paper, we propose an interpolant-free dual\nflow matching (DFM) approach without explicit assumptions about the modeled\nvector field. DFM optimizes the forward and, additionally, a reverse vector field\nmodel using a novel objective that facilitates bijectivity of the forward and reverse\ntransformations. Our experiments with the SMAP unsupervised anomaly detection\nshow advantages of DFM when compared to the CNF trained with either maximum\nlikelihood or FM objectives with the state-of-the-art performance metrics.", "sections": [{"title": "1 Introduction", "content": "Discrete-time (DNF) and continuous-time (CNF) normalizing flows have been previously extensively\nstudied and compared in details [Ruthotto and Haber, 2021]. With the pros and cons in each ap-\nproach, we are motivated to apply CNFs in real-world generative and density estimation applications.\nTheoretically, an infinite-length architecture with arbitrary parameterization can lead to a significant\nadvantages of CNFs when compared to DNF shortcomings. Meanwhile, CNFs with the ordinary\ndifferential equation (ODE) integration step endure higher computational complexity, numerical\ninstabilities and approximation errors [Chen et al., 2018]. In particular, the latter is crucial in density\nestimation which requires accurate estimates of the Jacobian matrix trace.\nRecent flow matching (FM) framework [Lipman et al., 2023] simplifies the training phase in CNFs\nby introducing a new regression objective. This objective minimizes mean square difference of a\nparameterized vector field model and an interpolated vector field between two data distributions.\nWhile the former is a conventional neural network with time-dependent conditioning, the latter\nrelies on certain assumptions about modeled data distributions. As a result, there is an extensive\nline of research that proposes various forms of the interpolated vector fields and the corresponding\nprobability paths. We summarize recent works in Table 1 with the formal introduction in Section 2.\nDiffusion models [Song and Ermon, 2019, Ho et al., 2020] that solve stochastic differential equations\n(SDEs) can also be generalized using the FM framework [Tong et al., 2024].\nIn this paper, we analyze current interpolation-based FM approach and its inherent limitations i.e. the\nGaussian probability path assumption between two data distributions [Lipman et al., 2023]. \u03a4\u03bf\naddress this limitation, we propose a novel interpolant-free dual flow matching (DFM) method.\nSpecifically, we accomplish interpolant-free FM using an additional parameterized reverse vector\nfield model. For simplicity, we model the reverse vector field using exactly the same architecture as\nfor the forward one. Then, we optimize our DFM using an objective that enforces transformation\nbijectivity of the modeled forward and the reverse vector fields."}, {"title": "2 CNF Preliminaries and Prior FM Methods", "content": "Continuous normalizing flows. We follow Lipman et al. [2023] and Tong et al. [2024] notation.\nWe consider a pair of data distributions q(x0) and q(x1) over RD with densities p(x0) and p(x1),\nrespectively. Often, the po = p(x0) density represents a known prior distribution while the data\ndensity p1 = p(x1) is not given with only access to an empirical q(x1) and p\u2081 needs to be estimated.\nThen, there are a probability density path p : [0,1] \u00d7 RD \u2192 R> 0, which is a time-dependent\nprobability density function pt(x) with t = [0, 1] such that $\\int p_t(x)dx = 1$, and a Lipschitz-smooth\ntime-dependent vector field u : [0,1] \u00d7 RD \u2192 RD. The vector field ut is used to construct a\ntime-dependent diffeomorphism i.e., the CNF $ : [0, 1] \u00d7 RD \u2192 RD that is defined via the ODE as\n$\\dot{\\phi_t(x)}/dt = u_t(\\phi_t(x))$ and $\\phi_0(x) = x_0$,\nwhere $\\phi_t(x)$ is the ODE solution with $\\phi_0(x)$ initial condition that transports \u00e6 from time 0 to time t.\nOn the other hand, $ induces a push-forward $p_t = [\\phi_t]#(p_0)$ that transports the density po from time\n0 to time t. The time-dependent density pt is characterized by the continuity equation written by\n$\\partial p_t(x)/\\partial t = -div(p_t(x)u_t(\\phi_t(x))) = -div(f_t(x))$,\nwhere the divergence operator, div, is defined as the sum of derivatives of ft(x) \u2208 RD w.r.t. all\nelements xa or, simply, the Jacobian matrix trace: $div(f(x)) = \\sum_{d=1}^D \\partial f(x)/\\partial x_d = Tr(J)$.\nThe vector field $u_t(\\phi_t(x))$ is often modeled without $\\phi_t(x)$ invertability requirement by an arbitrary\nneural network ve(t, xt) with the learnable weight vector \u03b8. Then, the continuity equation in (2) for\n(1) neural ODE can be written using the instantaneous change of variables [Chen et al., 2018] as\n$\\partial \\log p_t(x_t)/\\partial t + Tr(\\partial v_\\theta(t, x_t)/\\partial x) = 0$.\nThe (3) neural ODE can be solved both for a point 20 and the log-likelihood change as integration\n$\\left[\\log(p_1/p_0)\\right]_{x_0}^{x_1} = \\int_{t=0}^{t=1} \\partial [\\log(p_t/p_0)]/\\partial t dt$\n$\\left[\\log(p_1/p_t)\\right]_{x_t} = \\int_{t=0}^{t=1} -Tr(\\partial v_\\theta(t, x_t)/\\partial x) dt$, and initially $log(p_1/p_t)=0$\nThen, the CNF maximizes likelihood (MLE) during training and uses its estimate at the evaluation as\n$\\arg \\max_\\theta L_{MLE} := \\log p_1 = \\log p_0 - \\int_{t=0}^{t=1} Tr (\\partial v_\\theta(t, x_t)/\\partial x) dt$,\nwhere log po is the likelihood of a point \u00e6 from (4) when evaluated using a known prior q(x0).\nFlow matching training. Typically, solving the ODE (4) for the MLE objective (5) is computationally\nexpensive [Grathwohl et al., 2019, Zhuang et al., 2020]. The FM framework [Lipman et al., 2023]\nproposes an alternative objective that regresses the ve(t, xt) to ut by conditioning the latter by a\nvector z = x1. This has been extended by the conditional FM (CFM) framework [Tong et al., 2024]\nwhere the ut (x|z) and pt(x|z) are conditioned on a more general z ~ q(z) such that the marginal\nprobability density path and the corresponding marginal vector field are defined as\n$p_t(x) = \\int p_t(x|z)q(z) dz$, and $u_t(x) = E_{z\\sim q(z)} [U_t(x|z)p_t(x|z)/p_t(x)]$."}, {"title": "3 The Proposed Interpolant-free Dual Flow Matching", "content": "Dual CNF via a reverse vector field. Let's ex-\ntend the CNF framework that is defined in (1-4).\nWe introduce a dual CNF where its first part, im-\nplemented as the above non-invertible ve(t, xt),\napproximates the vector field ut($t(x)). In ad-\ndition, we employ an extension va(t, yt) with\nlearnable parameters A that models a reverse\nvector field model ut($\u00b9(y)). In other words,\nthere is the forward transformation xt = $t(x)\nand the inverse transformation yt = $t(y) =\n\u00b9(x) of the bijective map $t.\nThen, we can reformulate the equations (4) for\nva (t, yt) model with minor modifications. The\nneural ODE in (3) can be solved in reverse simul-\ntaneously for a point \u00e6\u2081 and the log-likelihood\nchange with the initial condition yt ~ q(x0)\nand log(po/Pt) = 0 as integration\n$\\left[\\log(p_0/p_1)\\right]_{x_1}^{x_0} = \\int_{t=0}^{t=1} -Tr \\left(\\frac{\\partial v_\\lambda (t, y_t)}{\\partial y} \\right) dt$.\nInterestingly, the (9) approach with the modified\nmaximum likelihood LMLE(A) is known in the\nDNF literature as a reverse divergence objective [Papamakarios et al., 2021]. When the target data p1\ncannot be analytically evaluated, the (9) is impractical for CNF training with the MLE objective.\nInterpolant-free DFM. On the other hand, the proposed dual CNF with the reverse model in (9) can\nbe used for the interpolant-free flow matching. Instead of the less expressive affine transformation\n($t(x|z) = \u03bct(z)+otx, x ~ N(0, I)) induced by the Gaussian interpolation (7), the proposed DFM\nonly requires bijectivity of the free-form transformations \u03c6t and \u00a2\u00b9 produced by, correspondingly,\nthe forward ve (t, xt) and the reverse vx (t, Yt) vector field models.\nThen, the proposed dual CNF with the bijective $t can be expressed as ODEs expressed by\n$\\begin{cases}\\dot{\\phi_t(x)}/dt = u_t(\\phi_t(x)) = v_\\theta(t, x_t) \\\\\\dot{\\phi_t(y)}/dt = u_t(\\phi_t(y)) = v_\\lambda(t, Y_t).\\end{cases}$\nAssuming the $t(y) = \u00b9(x) bijectivity in a neighborhood of t for \u00e6 and y, (10) can be rewritten\nusing the univariate inverse function theorem by substituting the top to bottom as\n$\\frac{d\\phi^{-1}(x)}{dt} = 1/(\\frac{d\\phi(x)}{dt}) \\Rightarrow diag \\left(v_\\theta (t, x_t) v_\\lambda(t, y_t)\\right) = I$."}, {"title": "4 Experiments", "content": "Benchmark. We employ real-world SMAP [Hundman et al., 2018] time series benchmark for\nunsupervised anomaly detection. The soil moisture active passive satellite (SMAP) dataset contains\nsoil samples and telemetry information from the Mars rover with 135K and 428K data points in the\ntraining (without anomalies) and test sets, respectively. SMAP data has 25 data dimensions collected\nfrom 55 entities. We follow Su et al. [2019] and transform the regression task into a classification\ntask using sliding windows (window size = 8) and replication padding [Tuli et al., 2022].\nFlow models. We report experimental results for the Glow-type DNF [Kingma and Dhariwal, 2018]\nfrom [Gudovskiy et al., 2024] with the state-of-the-art baselines. Second, we experiment with the\nvanilla CNF from Section 2 and the CNF trained using the CFM framework i.e., the FM from [Lipman\net al., 2023] and I-CFM from [Tong et al., 2024]. All CNF models have exactly the same U-Net\narchitecture [Ronneberger et al., 2015], learnable N(\u03bc, \u03c3\u00b2I) prior and identical evaluation using (5).\nEvaluation. We follow Su et al. [2019] and report precision (P), recall (R), AuC and F\u2081 score. We\nprovide results when we solve ODE using the fixed-step (F) Euler method with 4 steps and the\nvariable-step (V) Dopri5 method (atol=1e-1, rtol=1e-2) from the [Zhuang et al., 2021] library. We\nuse the Hutchinson stochastic estimator of the Jacobian matrix trace [Hutchinson, 1990].\nQuantitative results. We compare flow mod-\nels to other popular baselines: OmniAnomaly\n[Su et al., 2019], CAE-M [Zhang et al., 2021],\nTranAD [Tuli et al., 2022]. It is common in\nthese baselines to train and evaluate a separate\nmodel for each SMAP entity. In contrast, all our\nflow models use a single model for all entities\nin Table 2 i.e. they are entity-unconditional.\nWe can derive several important conclusions\nfrom Table 2 results. First, continuous-time\nnormalizing flows models, if properly trained\nand evaluated, are able to outperform discrete-\ntime normalizing flows as well as other non-\nflow models in this density estimation task. Sec-\nond, recent integration-free FM training meth-\nods using (8) perform similarly or better than\nthe CNF trained by computationally-expensive\nMLE from (5). Third, the proposed DFM significantly outperforms prior FM methods with only the\n2x complexity increase. In particular, DFM increases the non-saturated metrics such as precision and\nF\u2081 score by, correspondingly, 6.5 (88.2% \u219294.7%) and 3.1 (93.3% \u219296.4%) percentage points."}, {"title": "5 Conclusions", "content": "In this paper, we analyzed limitations of the interpolation-based flow matching framework that allows\nto efficiently train a CNF model. To address the limitations, we proposed the interpolant-free dual\nflow matching method. Our experiments with the SMAP benchmark showed that our DFM achieves\nstate-of-the-art results for the entity-unconditional unsupervised anomaly detection. In future, the\nDFM objective (11) and the practical loss (12) can further be extended to multivariate case."}]}