{"title": "JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase", "authors": ["Wanying Ding", "Vinay K. Chaudhri", "Naren Chittar", "Krishna Konakanchi"], "abstract": "Knowledge Graphs have emerged as a compelling abstraction for capturing key relationship among the entities of interest to enterprises and for integrating data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by leveraging knowledge graphs across the organization for multiple mission critical applications such as risk assessment, fraud detection, investment advice, etc. A core problem in leveraging a knowledge graph is to link mentions (e.g., company names) that are encountered in textual sources to entities in the knowledge graph. Although several techniques exist for entity linking, they are tuned for entities that exist in Wikipedia, and fail to generalize for the entities that are of interest to an enterprise. In this paper, we propose a novel end-to-end neural entity linking model (JEL) that uses minimal context information and a margin loss to generate entity embeddings, and a Wide & Deep Learning model to match character and semantic information respectively. We show that JEL achieves the state-of-the-art performance to link mentions of company names in financial news with entities in our knowledge graph. We report on our efforts to deploy this model in the company-wide system to generate alerts in response to financial news. The methodology used for JEL is directly applicable and usable by other enterprises who need entity linking solutions for data that are unique to their respective situations.", "sections": [{"title": "Introduction", "content": "Knowledge Graphs are being used for a wide range of applications from space, journalism, biomedicine to entertainment, network security, and pharmaceuticals. Within JP Morgan Chase (JPMC), we are leveraging knowledge graphs for financial applications such as risk management, supply chain analysis, strategy implementation, fraud detection, investment advice, etc. While leveraging a knowledge graph, Entity Linking (EL) is a central task for semantic text understanding and information extraction. As defined in many studies (Zhang et al. 2010; Eshel et al. 2017; Kolitsas, Ganea, and Hofmann 2018), in an EL task we link a potentially ambiguous Mention (such as a company name) with its corresponding Entity in a knowledge graph. EL can facilitate several knowledge graph applications, for example, the mentions of company names in the news are inherently ambiguous, and by relating such mentions with an internal knowledge graph, we can generate valuable alerts for financial analysts. In Figure 1, we show a concrete example in which the name \"Lumier\" has been mentioned in two different news items. \"Lumier\"s are two different companies in the real world, and their positive financial activities should be brought to the attention of different stakeholders. With a successful EL engine, these two mentions of \"Lumier's can be distinguished and linked to their corresponding entities in a knowledge graph.\nPrior work on EL has been driven by a number of standard datasets, such as CONLYAGO (Suchanek, Kasneci, and Weikum 2007), TAC KBP\u00b9, DBpedia\u00b2, and ACE\u00b3. These datasets are based on Wikipedia, and are therefore, naturally coherent, well-structured and rich in context (Eshel et al. 2017). We face the following problems when we use these methods for entity linking for our internal knowledge graph:\n1) Wikipedia does not cover all the entities of financial interest. For example, as of this writing, the startup \u201cLumier\u201d mentioned in Figure 1 is not present in Wikipedia, but it is of high financial interest as it has raised critical investment from famous investors.\n2) Lack of context information. Many pre-trained models achieve great performance by leveraging rich context data from Wikipedia (Ganea and Hofmann 2017). For JPMC internal data, we do not have information comparable to Wikipedia to support re-training or fine-tuning of existing models.\nTo address the problems identified above, we built a novel entity linking system, JEL, to link mentions of company names in text to entities in our own knowledge graph. Our model makes the following advancements on the current state-of-the-art:\n1) We do not rely on Wikipedia to generate entity embeddings. With minimum context information, we compute entity embeddings by training a Margin Loss function.\n2) We deploy the Wide & Deep Learning (Cheng et al. 2016) to match character and semantic information respectively."}, {"title": "Problem Definition and Related Work", "content": "Problem Definition\nWe assume a knowledge graph (KG) has a set of entities E. We further assume that W is the vocabulary of words in the input documents. An input document D is given as a sequence of words: D = {W1, W2, ..., wa} where wk \u2208 W,1 \u2264 k \u2264 d. The output of an EL model is a list of T mention-entity pairs {(mi, ei)}i\u2208{1,T}, where each mention is a word subsequence of D, m\u2081 = wi, ..., wr,1 \u2264 I \u2264 r < d, and each entity ei \u2208 E. The entity linking process involves the following two steps (Ceccarelli et al. 2013).\n1) Recognition. Recognize a list of mentions mi as a set of all contiguous sequential words occurring in D that might mention some entity e\u00bf \u2208 E. We adopted spaCy4 for mention recognition.\n2) Linking. Given a mention mi, and the set of candidate entities, C(mi) such that |C(mi)| > 1, from the KG, choose the correct entity, ei \u2208 C(mi), to which the mention should be linked. We focus on solving the linking problem in this paper.\nPopular Methods\nEntity Linking is a classical NLP problem for which the following techniques have been used: String Matching, Context Similarity, Machine Learning Classification, Learning to Rank, and Deep Learning. In the following several paragraphs, we will briefly discuss each of them.\nString Matching Methods. String matching measures the similarity between the mention string and entity name string. We experimented with different string matching methods for name matching, including Jaccard, Levenshtein, Ratcliff-Obershelp, Jaro Winkler, and N-Gram Cosine Simiarity, and found that n-gram cosine similarity achieves the best performance on our internal data. However, pure string-matching methods breakdown when two different entities share similar or the same name (as shown in Figure 1) which motivates the need for better matching techniques.\nContext Similarity Methods. Context Similarity methods compare similarities of respective context words for mentions and entities. The context words for a mention are the words surrounding it in the document. The context words for an entity are the words describing it in the KG. Similarity functions, such as Cosine Similarity or Jaccard Similarity, are widely used to compare the two sets of context words (Cucerzan 2007; Mihalcea and Csomai 2007), and then to decide whether a mention and an entity should be linked.\nMachine Learning Classification. Many studies adopt machine learning techniques for the EL task. Binary classifiers, such as Naive Bayes (Varma et al. 2009), C4.5 (Milne and Witten 2008), Binary Logistic classifier (Han, Sun, and Zhao 2011), and Support Vector Machines (SVM) (Zhang et al. 2010), can be trained on mention-entity pairs to decide whether they should be linked.\nLearn to Rank Methods. As a classification method will generate more than one mention-entity pairs, many systems use a ranking model (Zheng et al. 2010) to select the most likely match. Learning to Rank (LTR) is a class of techniques that supplies supervised machine learning to solve ranking problems.\nDeep Learning Methods. Deep learning has achieved success on numerous tasks including EL (Sun et al. 2015;"}, {"title": "Proposed Framework", "content": "Entity Embedding\nMost public entity embedding models (He et al. 2013; Yamada et al. 2016; Ganea and Hofmann 2017) are designed for Wikipedia pages and require rich entity description information. In our case, each entity has a short description that is insufficient to support a solid statistical estimation of entity embeddings (Mikolov, Yih, and Zweig 2013). To address this limitation, we use a Triplet Loss model to generate our own entity embeddings from pre-trained word embedding models with limited context information support.\nEntity Embedding Model. To prepare training data for this model, we select 10 words that can be used as positive examples and 10 words that can be used as negative example for each entity. To select the positive examples, we score each entity's description words with tf-idf, and select the words with 10 highest scores. To select the negative examples, we randomly select from words that do not appear in this entity's description. Thus, for each entity, we can construct 10 < entity, positive-word, negative-word > triplets to feed into triplet loss function formulated as Equation 1 below.\n$Loss = \\sum_{i=1}^{N}[|| f_{a} - f_{p}||_{2}^{2} - || f_{a} - f_{n} ||_{2}^{2} + \\alpha ]_{+}$                                                                                                                                 (1)\nwhere fa is the vector of an anchor that we learn, f is the vector from a positive sample, and for is the vector from a negative sample, a is the margin hyper-parameter to be manually defined. We train the entity embedding vectors (fa). We use off-the-shelf word embedding vectors(fp and fn) from the fastText language model. In our experiments, a = 2.0 led to the best performance.\nEntity Embedding Validation. To validate the entity embeddings, we choose five seed companies from different industries \u201cGoogle DeepMind\u201d, \u201cHulu\u201d, \u201cMagellan Health\", \"PayPal Holdings\u201d, \u201cSkybus Airlines\". We next select their ten nearest neighbors (as shown in Figure 2). We calculate a t-Distributed Stochastic Neighbor Embedding (t-SNE) to project the embeddings into a 2-dimension space. As clearly shown in Figure 2, five seed companies from different industries are clearly separated in space. For \"Google DeepMind\", we can find that all its neighbors 5 are, as expected, Artificial Intelligence and Machine Learning companies. This visualization gives us a sanity check for our entity embeddings.\nEntity Linking\nTwo factors affect an EL model's performance: Characters and Semantics.\nCharacters: \"Lumier\" will be easily distinguished from \"ParallelM\" because they have completely different character patterns. These patterns can be easily captured by a wide and shallow linear model.\nSemantics. In Figure 1, \"Lumier(Software)\" can be distinguished from \u201cLumier (LED)\" because they have different semantic meanings behind the same name. These semantic differences can be captured by a deep learning model.\nTo combine the two important factors listed above, we develop a Wide&Deep Learning model (Cheng et al. 2016) for our EL task (shown in Figure 4).\nWide Character Learning. Unlike many other approaches (Kolitsas, Ganea, and Hofmann 2018; Lample et al. 2016) that apply character embeddings to incorporate lexical information, we apply a wide but shallow linear layer for the following two reasons. First, embedding aims to capture an item's semantic meanings, but characters naturally have no such semantics. \"A\" in \"Amazon\" does not have any relationship with \"A\" in \"Apple\". Second, as embedding layer involves more parameters to optimize, it is much slower in training and inference than a simple linear layer.\nFeature Engineering. Many mentions of an entity exhibit a complex morphological structure that is hard to account for by simple word-to-word or character-to-character matching. Subwords can improve matching accuracy dramatically. Given a string, we undertake the following processing to maximize morphological information we can get from sub-words.\n1) Clean a string, convert it to lower case, remove punctuation, standardize suffix, etc. For example, \"PayPal Holdings, Inc. \" will change to \"paypalhlds\".\n2) Pad the start and end of the string; \"paypalhlds\" will be converted to \"*paypalhlds*\".\n3) Apply multiple levels of n-gram (n \u2208 [2,5]) segmentation; \"*paylpalhlds* \" will be { *p, ay,..., lhlds, hlds* }.\n4) Append original words, *paypal* and *hlds*, to the token list.\nWide Character Learning. We applied a Linear Siamese Network (Bromley et al. 1994) for wide character learning. We implemented two identical linear layers with shared weights (as shown in the left part of Figure 4). With this architecture, similar inputs, Tm and Te, will generate similar outputs, Ym and Ye. We applied the Euclidean distance to estimate output's similarity.\n$D_{syx}= d(Y_{m}, Y_{e}) = \\sqrt{\\sum_{i=1}^{n} (Y_{mi} - Y_{ei})^2 }$                                                                                                (2)\nDeep Semantic Embedding. We embed the mentions into vectors so that we can mathematically measure similarities"}, {"title": "Experiment and Analysis", "content": "Data Preparation\nWe first applied spaCy over financial news to detect all the named entity mentions. SpaCy features neural models for named entity recognition (NER). By considering text capitalization and context information, spaCy claims an accuracy above 85% for NER. Satisfied with spaCy's performance, we used it on financial news to recognize all the critical mentions that are tagged with \u201cORG\u201d. The data preparation process was as follows:\n1) We extracted mentions from the financial news with spaCy.\n2) We applied bi-gram cosine similarity between the extracted mentions and company names in our internal knowledge grpah.\n3) If the similarity score between a mention string and an entity name is smaller than 0.5, we treated that as a strong signal that the two are not linked, and marked them as 0.\n4) If the similarity score between a mention string and an entity name is equal to 1.0, we manually checked the list to avoid instances that two different entities share the same name (infrequent), and marked the pair as 1.\n5) If a mention and an entity name have cosine similarity larger than 0.75, but smaller than 1.0, we manually labeled:\n(a) Some cases are easy to tell, such as: \"Luminet\u201d vs \u201cLuminex\", we labeled those instances as 0 directly.\n(b) Some cases can be decided according to their description/context. We printed mention's context information and entity's description respectively, and made the decision based on those texts, such as \"Apple\" vs \"Apple Corps.\".\n(c) Some other cases need help from publicly information found through internet search to decide, such as \"Apollo Management\" vs \"Apollo Global Management\".\n6) If a mention and an entity name have cosine similarity between 0.5 and 0.75, we discarded it. These cases are too many for manual labeling, and too complicated for machine labeling.\n7) Negative examples from step 3, make the dataset very imbalanced containing many more negative pairs. We counted the number of examples obtained in steps 4. and 5., and randomly sampled a comparable number from the examples gathered in step 3.\nIn total, we have labeled 586, 975 ground truth mention-entity pairs, with 293,949 positive mention-entity pairs, and 293, 026 negative pairs. We split 80% of the data as training data, 10% as validation data, and 10% as testing data.\nBaselines\n1) String Matching We chose Bi-Gram and Tri-Gram Cosine Similarity as two of baselines. Before similarity calculation, all tokens were weighted with tf-idf scores. We set 0.8 as the threshold.\n2) Context Similarity We used Jaccard and Cosine similarity to measure the similarities between mention context and entity descriptions. A potential matched mention-entity pair should share at least one context word.\n3) Classification We chose Logistic Regression (LR) and SVM for experiments. We adopted the feature engineering method defined in (Zheng et al. 2010), but only kept the following features that we can generate from our data:\nStrSimSurface: edit-distance among mention strings and entity names.\nExactEqualSurface: number of overlapped lemmatized words in mention strings and entity names.\nTFSimContext: TF-IDF similarity between mention's context and entity's description"}, {"title": "Comparison on Accuracy", "content": "We first compare the methods with Precision and Recall.\nFor an easier comparison, we scaled each of True Positive, True Negative, False Positive, and False Negative into [0,0.5] showing as following.\n$True Positive= \\frac{Count(Predict=1 & Truth=1)}{2 \\times Count(Truth=1)}$\n$True Negative= \\frac{Count(Predict=0 & Truth=0)}{2 \\times Count(Truth=0)}$\n$False Positive= \\frac{Count(Predict=1 & Truth=0)}{2 \\times Count(Truth=0)}$\n$False Negative= \\frac{Count(Predict=0 & Truth=1)}{2 \\times Count(Truth=1)}$\nThe result is shown as Table 1, in which:\n$Precision=\\frac{True Positive}{True Positive + False Positive}$\n$Recall= \\frac{True Positive}{True Positive + False Negative}$\n$F1-Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n$Accuracy = True Positive+True Negative\nFrom Table 1, we find context based methods perform poorly as expected. Descriptions in our knowledge graph have very different wording styles from financial news. Simply comparing context words will definitely result in low accuracy. SVM-Rank surprisingly outperforms ENEL. The reason here is that ENEL does not model character features"}, {"title": "Comparison on Precision", "content": "Accuracy can only check a method's ability in distinguishing positive samples from negative samples. In a real EL task, we care more about a method's ability in finding the correct entity for a given mention. In this section, we utilize the \"Precision at top K (P@K)\" to compare the methods (shown in Figure 5 with K \u2208 {1,5,10}). LR, SVM, and SVM-Rank get 0s at all P@Ks. Both LR and SVM are classifiers, which are good at distinguishing positive pairs from negative pairs. However, they may label multiple mention-"}, {"title": "Deployment and Business Impact", "content": "JPMC has built a large-scale knowledge graph for internal use, that integrates data from third party providers with its internal data. The system contains several million entities (e.g. suppliers, investors, etc.) and several million links (e.g. supply chain, investment, etc.) among those entities 7. Entity linking is one of the core problems that needs to be solved when ingesting unstructured data. Currently, we are working closely with business units to process incoming news articles to extract news about companies, and connect them to the corresponding entities in the knowledge graph. Such news analytics will support users across JPMC in discovering relevant and curated news that matter to their business.\nHere we present a concrete example of the use of such news analytics. To protect the customer cofidentiality, the actual name of the company has been changed, but the illustrated computation is the same. \u201cAcma Retail Inc\" filed for bankruptcy due to the pandemic, and a lot of JPMC clients could feel stress as they are suppliers to Acma. Such stress can pass deep down into its supply chain and trigger financial difficulties for other clients. JPMC may face different levels of risks from suppliers with different orders in Acma's supply chain. With \"Acma\u201d mentioned in financial news linked with \"Acma Global Retail Inc\" in our knowledge graph (distinguished from \u201cAcma Furniture, LLC", "Acma Enterprise System\", etc.), we can accurately track down Acma supply chain, identify stressed suppliers with different revenue exposure, and measure our primary risk due to Acma's bankruptcy. Once stressed clients with significant exposure are detected, alerts will be sent out to corresponding credit officers. If \u201cAcma": "as linked with incorrect entities, it will result in too many false signals, resulting in wasted effort.\nA similar news analytics system, SNOAR, has been previously reported (Goldberg et al. 2003). SNOAR used a rule based and fuzzy match techniques for entity linking, which are not able to handle the example illustrated in Figure 1. In fact, before JEL was developed, Tri-Gram Cosine Similarity was tested for name string matching. As shown in Table 1 and Figure 5, tri-gram cosine similarity has practical limitations and could not distinguish entities sharing similar names (as shown in Figure 1). Compared to fuzzy match or tri-gram cosine similarity, JEL provides a better and more controllable solution.\nIn the deployed version of JEL, we will apply another blocking layer (overlap blocker for current configuration) ahead of JEL to reduce the candidate volume for each mention in an article. Entities sharing less than 2 bi-gram tokens will be filtered out in the blocking stage, and the rest of candidate entities will be sent to JEL for a more sophisticated linking proccess. This blocking process will dramtically reduce computational cost.\nDuring the first-round deployment, we are incorporating JEL into a streaming news platform, and its online performance will be tested on indicators including:\nScope: Ability in linking all available JPMC clients\nTimeliness: Ability in providing near real-time alerts\nAccuracy: Ability in sending accurate alerts\nFlexibility: Ability in integration of various components for end-to-end delivery of solutions.\nOur first step is to collect user feedbacks for parameter fine-tuning and algorithm re-configuration. Once the initial deployment is successful, we will release JEL as a standalone and reusable component with an API to provide a firm-wide service that can be used in various applications."}, {"title": "Conclusion", "content": "Knowledge Graphs are becoming a mission critical technology across many industries. Within JPMorgan Chase, we are using a knowledge graph as a company-wide resource for tasks such as risk analysis, supply chain analysis, etc. A core problem in utilizing this knowledge is EL. Existing EL models did not generalize to our internal company data, and therefore, we developed a novel model, JEL, that leverages margin loss function and deep and wide learning. Through an extensive experimentation, we have shown the superiority of this method. We are currently in the process of deploying this model on a financial news platform within our company. Even though our testing was done on our internal data, we believe, that our approach can be adapted by other companies for entity linking tasks on their internal data."}]}