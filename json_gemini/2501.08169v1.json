{"title": "Revolutionizing Communication with Deep Learning and XAI for Enhanced Arabic Sign Language Recognition", "authors": ["Mazen Balat", "Rewaa Awaad", "Ahmed B. Zaky", "Salah A. Aly"], "abstract": "This study introduces an integrated approach to recognizing Arabic Sign Language (ArSL) using state-of-the-art deep learning models such as MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with EfficientNet-B2 achieving peak accuracies of 99.48% and 98.99%, respectively. Key innovations include sophisticated data augmentation methods to mitigate class imbalance, implementation of stratified 5-fold cross-validation for better generalization, and the use of Grad-CAM for clear model decision transparency. The proposed system not only sets new benchmarks in recognition accuracy but also emphasizes interpretability, making it suitable for applications in healthcare, education, and inclusive communication technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "AI and machine learning are reshaping everyday life, sparking innovation across fields like healthcare, education, and social services [1]. These technologies are expanding human potential, tackling social issues, and promoting inclusivity by helping to bridge communication barriers among diverse groups [2].\nSign language plays a crucial role as a communication method for individuals who are deaf or hard of hearing, enabling effective interaction within their communities and broader society [3]. Arabic Sign Language (ArSL), widely used in Arabic-speaking regions, is marked by unique gestures and expressions that capture the cultural richness and diversity of the Arab world [4]. Developing precise and efficient ArSL recognition systems is vital for enhancing communication accessibility, thus promoting inclusivity and equal opportunities for people with hearing impairments.\nThe incorporation of automatic sign language recognition systems into daily life holds transformative potential across various sectors. In education, such systems facilitate real-time translation of educational materials, supporting deaf students in better engaging with their peers and educators [5]. Similarly, in healthcare, these systems improve communication between medical professionals and patients who use sign language, ensuring that critical information is effectively conveyed [6].\nThe deployment of these technologies in public spaces and consumer devices also raises awareness of sign language, fostering a more inclusive environment that encourages social interaction and reduces communication barriers [7].\nDespite these benefits, developing ArSL recognition systems involves several challenges. The complexity of hand gestures, diverse signing styles, and the influence of environmental factors like lighting and background conditions can hinder recognition accuracy [8]. Traditional approaches, such as using data gloves or relying on human interpreters, face limitations regarding practicality and scalability [9]. Thus, there is an urgent need for advanced, automated solutions capable of addressing these challenges to ensure reliable and real-time recognition.\nAs depicted in Fig. 1, the proposed pipeline comprises several stages, from data collection and preprocessing to model training, evaluation, and the integration of explainable AI techniques.\nThe main contributions of this research can be summarized as follows:\n1) The proposed system demonstrates superior recognition accuracy compared to existing state-of-the-art models, proving its efficacy in recognizing Arabic sign language gestures.\n2) Explainable AI techniques have been incorporated to ensure transparency in model decisions, which is critical in fields such as healthcare and education where understanding prediction rationale is essential.\n3) The recognition framework is adaptable to other sign languages, extending its applicability beyond ArSL. This adaptability enables usage across different languages and gestures.\n4) Through comprehensive preprocessing and model fine-tuning, the system maintains real-time recognition accuracy under diverse conditions, making it viable for real-world deployment.\nThe structure of this paper is as follows: Section 2 provides a comprehensive review of related literature on Arabic Sign Language (ArSL) recognition. Section 3 introduces the datasets used in this study, specifically the ArASL2018 and RGB Arabic Alphabets Sign Language datasets. Section 4 describes the methodology, including preprocessing steps and model architectures. Section 5 outlines the model training process and detailed descriptions. Section 6 covers the evaluation metrics applied to assess model performance. Section 7 delves into explainable AI techniques used to ensure transparent decision-making. Sections 8 and 11 presents experimental results, including performance comparisons with state-of-the-art methods. Lastly, Section 12 concludes the study, summarizing the findings and suggesting future research directions, building upon our previous work in sign language research [13]."}, {"title": "2. RELATED WORKS", "content": "Sign language communication tools, such as human interpreters, written communication methods, and Automatic Speech Recognition (ASR) systems, have provided essential support. However, these tools often lack comprehensive capabilities. The intricate and dynamic nature of sign languages-particularly Arabic Sign Language (ArSL)\u2014poses significant challenges for standard machine learning models, which often struggle with the nuanced gestures and varied signing styles that characterize ArSL [14]\u2013[16].\nIn recent developments, deep learning has shown considerable promise for ArSL recognition, notably through transfer learning approaches. For instance, Hu et al. [17] developed a model using the ArSL2018 dataset, focusing on Arabic alphabet signs. They resized the images to 32x32 pixels and applied data augmentation, achieving 95% accuracy using EfficientNetB4. Nonetheless, the model encountered issues related to class imbalances.\nAl Ahmadi et al. [18] utilized Convolutional Neural Networks (CNN) with transfer learning across three datasets (ASL-DS-I, ASL-DS-II, and ASL-DS-III), obtaining accuracy rates of 96.25%, 95.85%, and 97.02%, respectively. Their work underscored CNNs' robustness in processing Arabic sign language data.\nEl Baz et al. [19] focused on Arabic alphabet sign recognition using the RGB Arabic Alphabets Sign Language (AASL) dataset, collected from over 200 participants. After preprocessing, including background removal and data augmentation, they reported 99.4% training accuracy and 97.4% validation accuracy over 250 epochs.\nAbdelghfar et al. [20] explored Qur'anic Sign Language (QSL) recognition, using a subset of the ArSL2018 dataset. By employing Random Oversampling, SMOTE, and Random Undersampling to address class imbalances, their QSLRS-CNN model achieved a 97.31% accuracy after 200 epochs.\nAl Nabih et al. [21] introduced Vision Transformers (ViT) for ArSL recognition, fine-tuning a pre-trained ViT model on the ArSL2018 dataset to reach 99.3% accuracy. This demonstrated the potential of transformers to capture complex ArSL features, surpassing traditional CNNs.\nLahiani et al. [22] evaluated various pre-trained CNN models, including InceptionV3, VGG16, and MobileNetV2, on the ArSL2018 dataset. MobileNetV2, enhanced with transfer learning, achieved the highest accuracy of 96%.\nRenjith et al. [23] adopted a spatio-temporal approach that integrated both spatial and temporal features to capture sign language motions. Their method, tested on Chinese Sign Language (CSL) and ArSL, achieved accuracies of 90.87% and 89.46%, respectively, highlighting its effectiveness in handling dynamic sign language data.\nHassan et al. [24] used traditional machine learning techniques to recognize ArSL on the ArSL2018 dataset. They applied greyscale conversion and feature extraction using PCA and LDA, with the K-Nearest Neighbors (KNN) classifier achieving the best performance at 86.4% accuracy.\nOur research advances these existing methods by introducing cutting-edge models like ResNet50, MobileNetV3, and EfficientNet-B2 specifically for ArSL recognition. Additionally, explainable AI (XAI) techniques have been incorporated to enhance model transparency, which is vital in sensitive fields like healthcare and education. Our preprocessing pipeline includes oversampling and extensive data augmentation, addressing diverse data scenarios more effectively. This system not only surpasses previous models in accuracy but also ensures adaptability and scalability, making it applicable to other sign languages."}, {"title": "3. DATASETS", "content": "This study employs two datasets for Arabic alphabet sign language recognition: the ArSL2018 dataset [25] and the RGB Arabic Alphabets Sign Language Dataset (AASL) [26].\nA. Arabic Alphabets Sign Language Dataset (ArASL2018)\nThe ArSL2018 dataset, introduced by Latif et al. [25], comprises 54,049 grayscale images, each sized at 64x64 pixels, representing 32 Arabic sign language signs and alphabets. The dataset was collected from 40 participants of diverse age groups in Al Khobar, Saudi Arabia, using an iPhone 6S camera. To enhance robustness, the dataset includes variations in lighting, angles, and backgrounds. Figure 2 provides examples of these images.\nThe class distribution in ArSL2018, illustrated in Figure 3, shows an uneven number of samples across classes. This imbalance could introduce bias, potentially affecting the model's generalization performance.\nB. RGB Arabic Alphabets Sign Language Dataset (AASL)\nThe AASL dataset [26] contains 7,857 labeled RGB images representing 31 Arabic sign language alphabets. Collected from over 200 participants using various types of cameras, the dataset includes a range of conditions, such as diverse lighting, backgrounds, and orientations, enhancing its suitability for real-world applications. Examples of these images are presented in Figure 4.\nAs seen in Figure 5, the AASL dataset also exhibits uneven class distribution, which may skew model performance toward more represented classes."}, {"title": "4. METHODOLOGY", "content": "Ensuring a balanced and standardized dataset was essential for improving model performance and reducing bias. The data preparation phase focused on handling class imbalance and implementing preprocessing techniques to standardize the input data.\nA. Data Preparation\nGiven the differences in class distribution, separate strategies were employed for the ArSL2018 and AASL datasets:\nArSL2018 Dataset: The ArSL2018 dataset exhibited significant overrepresentation of certain signs, which could lead to model bias. To address this issue, undersampling was applied, capping the number of images at 1,250 per class. This adjustment balanced the dataset, reducing the likelihood of bias towards more frequent classes. Figure 6 displays the distribution after undersampling, confirming balanced representation across classes. This step was crucial for enhancing model generalization and ensuring equitable learning.\nAASL Dataset: In contrast, due to the smaller sample size in the AASL dataset, no undersampling was performed. All samples were preserved to maintain sufficient training data, ensuring that the model could capture the dataset's full diversity without compromising performance.\nB. Handling Class Imbalance: ArSL2018 and AASL Datasets\nC. Preprocessing\nThe preprocessing steps were designed to standardize the datasets while ensuring consistency for model training.\n\u2022 Image Resizing: Images from both datasets were resized to 224x224 pixels to match the input requirements of deep learning models like MobileNet, ResNet, and EfficientNet. This resizing ensured compatibility across models and uniform feature extraction. Bilinear interpolation was used for resizing, offering a balance in image quality by averaging pixel values [27].\n\u2022 Normalization: After resizing, pixel values were scaled to the range [0, 1] by dividing by 255. Additionally, standardization was applied by adjusting pixel values to have a mean of zero and a standard deviation of one, based on the training dataset. This step aligned the inputs with models pre-trained on ImageNet, which assume standardized inputs [28].\nThese preprocessing steps were crucial for improving model convergence, maintaining uniform scaling, and ensuring consistent data preparation across both datasets."}, {"title": "5. MODEL TRAINING AND DESCRIPTION", "content": "Model training was a multi-step process that involved selecting suitable models, defining a well-structured training procedure, and optimizing hyperparameters to ensure robust performance across evaluation metrics. Three models were chosen for this study: MobileNetV3 [31], ResNet50 [32], and EfficientNet-B2 [33], each selected based on its ability to handle the complexity of the dataset while balancing performance and computational efficiency.\nEach model used in this study has distinct advantages that make it well-suited for Arabic sign language recognition:\na) MobileNetV3: MobileNetV3 [31] is a lightweight convolutional neural network designed for efficiency and speed, making it suitable for real-time applications. It uses depthwise separable convolutions to reduce computational cost while maintaining high accuracy. The key equation in MobileNetV3's architecture is:\n$y_i = ReLU6 (\\sum_{j=1}^{k}DWConv(W_{ij} * X_j) + b_i)$.\nwhere DWConv represents depthwise convolution, $w_{ij}$ are the weights, $x_j$ are the input feature maps, and $b_i$ is the bias term. Here, k is the number of hidden layers. The ReLU6 activation function [34] is a variant of the ReLU function that caps the output at 6, which helps in preventing the activation from becoming too large.\nb) ResNet50: ResNet50 [32] is known for its deep residual learning approach [36], which solves the vanishing gradient problem by introducing residual connections. The core equation for a residual block in ResNet50 is:\n$y = x + F(x, \\{Wi\\})$.\nwhere x is the input, $F(x, \\{W_i\\})$ represents the residual mapping, and y is the output. Here, $\\{W_i\\}$ denotes the set of weights for the layers within the residual block. This architecture allows the network to learn identity mappings more easily, improving convergence and enabling the training of much deeper networks.\nThe residual block in ResNet50 typically consists of two or three convolutional layers with batch normalization and ReLU activation functions [37]. The key innovation is the addition of a shortcut connection that skips one or more layers, directly connecting the input to the output. This shortcut connection ensures that the gradient can flow directly through the network, mitigating the vanishing gradient problem.\nThe architecture of ResNet50 is designed to capture complex patterns and features in images, making it highly effective for image classification tasks. The use of residual connections not only improves the training of deep networks but also enhances the network's ability to generalize to new data.\nc) EfficientNet-B2: EfficientNet-B2 [33] is part of the EfficientNet family of models, designed to achieve high accuracy with fewer parameters and lower computational cost. The EfficientNet models use a compound scaling method that uniformly scales the depth, width, and resolution of the network. The scaling is governed by the following equations:\n$d = \u03b1^\\phi$, $\u03c9 = \u03b2^\\phi$, $r = \u03b3^\\phi$"}, {"title": "A. Model Selection", "content": "Each model used in this study has distinct advantages that make it well-suited for Arabic sign language recognition:"}, {"title": "B. Training Procedure", "content": "To ensure robustness and generalizability, 5-Fold Cross-Validation was employed. This method divides the data into five subsets (folds), with four used for training and one for validation in each iteration. The process is repeated five times, with each fold serving as the validation set once. The final performance metrics are obtained by averaging the results across all folds, minimizing bias and variance."}, {"title": "C. Hyperparameters", "content": "The models were trained using a set of optimized hyperparameters to ensure effective learning and prevent overfitting.\nEarly stopping was implemented to prevent overfitting by halting training when the validation loss ceased to improve over a set number of epochs (patience). This technique helps in avoiding unnecessary training and ensures that the model does not become too specialized to the training data, thereby improving its generalization to unseen data.\nAdditionally, a learning rate scheduler (ReduceLROnPlateau) [40] was used to reduce the learning rate when a plateau in validation performance was detected. This scheduler dynamically adjusts the learning rate during training, reducing it by a specified factor when the validation loss has not improved for a certain number of epochs."}, {"title": "6. PERFORMANCE EVALUATION METRICS", "content": "To provide a comprehensive assessment of the models' performance, multiple evaluation metrics were used, including accuracy, F1-score, precision, and recall. These metrics offer insights into different aspects of model performance, particularly in the context of class imbalance.\nAccuracy Accuracy measures the proportion of correctly classified instances among the total number of instances. It is a straightforward metric, but it may not always be reliable in the presence of class imbalance, as it could be biased towards the majority class. The formula for accuracy is:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$,\nwhere:\nTP = True Positives (correctly predicted positive samples)\nTN = True Negatives (correctly predicted negative samples)\nFP = False Positives (incorrectly predicted positive samples)\nFN = False Negatives (incorrectly predicted negative samples)\nWhile accuracy provides a general overview of how well the model performs, it may not fully capture performance in imbalanced datasets, as it does not differentiate between the types of errors made by the model.\nPrecision Precision focuses on the correctness of positive predictions, making it an important metric when false positives are costly. It indicates the proportion of true positive predictions among all positive predictions made by the model. The formula for precision is:\n$Precision = \\frac{TP}{TP+FP}$\nPrecision is particularly useful in applications where minimizing false positives is crucial, such as in medical diagnoses or accident detection, where predicting a positive class incorrectly could have serious consequences.\nRecall Recall, also known as sensitivity or true positive rate, measures the model's ability to identify all relevant instances. It represents the proportion of true positive samples that were correctly identified out of the total actual positive samples. The formula for recall is:\n$Recall = \\frac{TP}{TP+FN}$\nRecall is critical in scenarios where minimizing false negatives is more important, such as detecting accidents or medical conditions where missing a positive case could have severe implications.\nF1-Score The F1-score is the harmonic mean of precision and recall, providing a single measure that balances both metrics. It is particularly useful when dealing with imbalanced datasets, as it accounts for both false positives and false negatives. The formula for the F1-score is:\n$F1-Score = 2\\cdot\\frac{Precision \\cdot Recall}{Precision + Recall}$\nAn F1-score close to 1 indicates a good balance between precision and recall, making it effective in scenarios where both false positives and false negatives need to be minimized. It is a robust metric for evaluating models in the context of class imbalance, as it considers both over-prediction and under-prediction of classes.\nInterpretation and Use The choice of evaluation metrics is based on the specific requirements of the task and the dataset characteristics:"}, {"title": "7. EXPLAINABLE AI", "content": "To enhance the transparency and interpretability of the model's predictions, Explainable AI (XAI) techniques were integrated. Among these, Grad-CAM (Gradient-weighted Class Activation Mapping) was used to provide visual insights into the model's decision-making process. This approach helps users understand which areas of the input data influenced the model's predictions, improving trust and enabling more informed decision-making.\nA. Importance of Explainable AI\nExplainable AI is critical in real-world applications where understanding model predictions is as important as achieving high accuracy. The benefits of XAI include:\n\u2022 Transparency: It allows stakeholders to identify which parts of the input data had the greatest influence on model decisions.\n\u2022 Trust and Reliability: It builds user confidence by making the AI's decision-making process clearer, which is particularly crucial in sensitive fields like healthcare, accident detection, and autonomous systems.\n\u2022 Error Analysis: By analyzing why a model made incorrect predictions, XAI helps identify biases or errors, aiding in debugging and model refinement.\n\u2022 Compliance: It meets ethical and regulatory requirements, ensuring accountability in fields that demand responsible AI use, such as finance, healthcare, and law.\nB. Grad-CAM Overview\nGrad-CAM is a powerful tool for visualizing which regions of an input image contribute most to the model's predictions. It generates heatmaps that highlight the areas relevant to a specific decision, making the model's behavior easier to interpret. The core steps of Grad-CAM are outlined below.\na) Grad-CAM Methodology: Grad-CAM works by calculating the gradient of the predicted class score concerning feature maps from a convolutional layer. These gradients are then used to create a weighted combination of the feature maps, focusing on the most critical regions. The main steps are:\n1) Calculate the gradients: Compute the gradient of the target class score $y_c$ with respect to the feature maps $A^k$ from a selected convolutional layer:\n$\\frac{\\partial y_c}{\\partial A^k}$.\n2) Compute weights: The gradients are global average-pooled over the width and height of the feature maps to obtain weights $\u03b1_k$ for each feature map:\n$\u03b1_k = \\frac{1}{Z} \\sum_{i} \\sum_{j} \\frac{\\partial y_c}{\\partial A^k_{ij}}$\nwhere Z is the total number of pixels in the feature map.\n3) Generate the Grad-CAM heatmap: The final heatmap LGrad-CAM is obtained by taking a weighted sum of the feature maps, followed by a ReLU activation to focus on the most relevant regions:\n$L_{Grad-CAM} = ReLU(\\sum_k \u03b1_k \\cdot A^k)$\nb) Visual Insights from Grad-CAM: Grad-CAM produces a heatmap that can be overlaid on the original input image to highlight areas most influential in the model's decision-making. This visualization gives users an intuitive understanding of the image regions crucial for a particular prediction.\nC. Application of Grad-CAM in Model Evaluation\nIn this study, Grad-CAM was applied to models like MobileNet, ResNet, and EfficientNet to analyze their predictions. The resulting heatmaps provided the following insights:\n\u2022 Identifying Biases: Grad-CAM visualizations revealed potential biases, such as over-reliance on specific image features or irrelevant regions, guiding model refinement.\n\u2022 Assessing Model Reliability: Consistent focus on relevant image regions across samples indicated reliable performance, whereas inconsistent patterns suggested areas for further improvement.\n\u2022 Improving Trust in AI Decisions: The visual explanations provided by Grad-CAM helped users, especially non-experts, understand model behavior, fostering trust and acceptance of AI decisions."}, {"title": "8. MOBILENETV3 RESULTS", "content": "This section presents the experimental results of the MobileNetV3 model on the ArSL2018 and AASL datasets. The results are divided into subsections for each dataset. For each model, we provide validation and test results, followed by analysis using confusion matrices and Grad-CAM to interpret the models' decision-making.\nA. ArSL2018 Dataset\nIn this subsection, we present the results of our experiments on the ArSL2018 dataset using MobileNetV3 model. The results are reported for both validation and test sets, along with confusion matrices and Grad-CAM visualizations to interpret model performance. Validation Results: The validation results for MobileNetV3 on the ArSL2018 dataset are summarized in Table IV, showing high F1-Scores across all folds, with Fold 5 being the best.\nTest Results: The test results for MobileNetV3, shown in Table V, indicate consistent performance across all folds, with the highest F1-Score in Fold 5.\nConfusion Matrix: The confusion matrix for MobileNetV3 (Fold 5) on the ArSL2018 test set is shown in Figure 14. It indicates strong classification performance, with minimal misclassifications.\nGrad-CAM Analysis: Grad-CAM visualization for MobileNetV3 highlights the regions of the input images that contributed most to the model's predictions. As shown in Figure 15, the model focuses primarily on hand shapes, confirming its effectiveness in identifying relevant features for ArSL recognition.\nB. AASL Dataset\nIn this subsection, we present the results of our experiments on the AASL dataset using MobileNetV3 model. The results are reported for both validation and test sets, along with confusion matrices and Grad-CAM visualizations to interpret model performance. Validation Results: The validation results for MobileNetV3 on the AASL dataset are shown in Table VI, with Folds 2, 4, and 5 achieving perfect scores.\nTest Results: Table VII shows the test results for MobileNetV3, with consistently high performance across all folds.\nConfusion Matrix: The confusion matrix for MobileNetV3 (Fold 5) on the AASL test set, depicted in Figure 16, shows the model's classification performance, with the majority of misclassifications occurring in similar sign gestures.\nGrad-CAM Analysis: Grad-CAM visualization for MobileNetV3, shown in Figure 17, indicates that the model focuses on hand shapes and finger positions, demonstrating effective feature recognition for sign language gestures."}, {"title": "9. RESNET50 RESULTS", "content": "This section presents the experimental results of the EfficientNet-B2 model on the ArSL2018 and AASL datasets. The results are divided into subsections for each dataset. For each model, we provide validation and test results, followed by analysis using confusion matrices and Grad-CAM to interpret the models' decision-making.\nA. ArSL2018 Dataset\nIn this subsection, we present the results of our experiments on the ArSL2018 dataset using ResNet50 model. Validation Results: The validation results for ResNet50 are detailed in Table VIII, with Fold 5 achieving the highest F1-Score.\nTest Results: The test results for ResNet50 are summarized in Table IX, with the highest F1-Score observed in Fold 5.\nConfusion Matrix: Figure 18 displays the confusion matrix for ResNet50 (Fold 5), indicating accurate classification across most classes with only a few misclassifications.\nGrad-CAM Analysis: The Grad-CAM visualization for ResNet50 (shown in Figure 19) reveals that the model effectively focuses on key hand regions, providing transparent explanations for its predictions.\nB. AASL Dataset\nIn this subsection, we present the results of our experiments on the AASL dataset using ResNet50 model. Validation Results: The validation results for ResNet50 on the AASL dataset are shown in Table X, with perfect scores achieved in Folds 1, 4, and 5.\nTest Results: Table XI shows the test results for ResNet50 on the AASL dataset, with Fold 2 achieving the best F1-Score.\nConfusion Matrix: The confusion matrix for ResNet50 (Fold 2), shown in Figure 20, reveals a strong ability to accurately classify sign gestures, with fewer misclassifications compared to MobileNetV3.\nGrad-CAM Analysis: As shown in Figure 21, the Grad-CAM visualization for ResNet50 highlights critical areas of hand movements, demonstrating reliable model focus on relevant features."}, {"title": "10. EFFICIENTNET-B2 RESULTS", "content": "This section presents the experimental results of the EfficientNet-B2 model on the ArSL2018 and AASL datasets.\nA. ArSL2018 Dataset\nIn this subsection, we present the results of our experiments on the ArSL2018 dataset using EfficientNet-B2 model. Validation Results: The validation results for EfficientNet-B2 are shown in Table XII, with Fold 4 achieving the highest scores.\nTest Results: Table XIII provides the test results for EfficientNet-B2, with consistent performance across all folds.\nConfusion Matrix: Figure 22 shows the confusion matrix for EfficientNet-B2 (Fold 5), indicating accurate recognition with minimal errors.\nGrad-CAM Analysis: As seen in Figure 23, the Grad-CAM visualization for EfficientNet-B2 highlights the critical hand areas used for classification, demonstrating effective interpretability.\nB. AASL Dataset\nIn this subsection, we present the results of our experiments on the AASL dataset using EfficientNet-B2 model. Validation Results: The validation results for EfficientNet-B2 on the AASL dataset are presented in Table XIV, demonstrating perfect performance in Folds 3, 4, and 5.\nTest Results: The test results for EfficientNet-B2, shown in Table XV, reveal strong performance across all folds, with Fold 4 achieving the highest F1-Score.\nConfusion Matrix: The confusion matrix for EfficientNet-B2 (Fold 4) is depicted in Figure 24, showing minimal misclassifications and strong classification accuracy.\nGrad-CAM Analysis: The Grad-CAM visualization for EfficientNet-B2, presented in Figure 25, highlights the model's focus on the critical parts of hand gestures, confirming its interpretability."}, {"title": "11. COMPARISON WITH STATE-OF-THE-ART METHODS", "content": "Our approach outperforms several existing models in Arabic Sign Language (ArSL) recognition, as outlined in Table XVI. On the ArSL2018 dataset, our EfficientNet-B2 model achieves a test accuracy of 99.48%, surpassing previous models like those by Hu et al. [17], Abdelghfar et al. [20], and Al Nabih et al. [21]. Similarly, on the AASL dataset, EfficientNet-B2 reaches a test accuracy of 98.99%, improving upon results from El Baz et al. [19] and others.\nThese improvements in our models can be attributed to:\n\u2022 Advanced Data Augmentation: Our preprocessing pipeline includes extensive data augmentation, which improves model generalization and addresses class imbalances.\n\u2022 Explainable AI (XAI): The use of Grad-CAM visualizations enhances interpretability, making it easier to understand model decisions."}, {"title": "12. CONCLUSION AND FUTURE WORK", "content": "This study successfully developed an advanced system for Arabic Sign Language (ArSL) recognition using cutting-edge deep learning models, including MobileNetV3, ResNet50, and EfficientNet-B2, combined with Explainable AI (XAI) techniques like Grad-CAM to enhance transparency and interpretability. The proposed system demonstrated superior accuracy and F1-score compared to existing approaches on the ArSL2018 and AASL datasets, with EfficientNet-B2 achieving the highest performance. Robust data preprocessing, extensive data augmentation, and stratified 5-fold cross-validation contributed to balanced learning across diverse samples, making the model suitable for deployment in healthcare, education, and inclusive communication technologies.\nFuture work can explore the integration of the proposed model into real-time applications, such as video-based communication platforms, to enhance accessibility and inclusivity. Further research into transformer-based architectures, such as Vision Transformers (ViT) or hybrid models, could enhance performance by capturing long-range dependencies in sequential gestures. Expanding the system to a multi-modal framework that integrates audio, text, and visual signals could improve interpretability and versatility. Extending the model to recognize other sign languages-like American Sign Language (ASL) or Chinese Sign Language (CSL)\u2014can broaden its applicability, creating a global, multilingual sign language recognition framework. Improved data collection, especially for underrepresented classes, will enhance robustness and address class imbalances. Future implementations could also focus on creating explainable user interfaces (UIs) that provide clear model interpretations for users, fostering trust in Al decisions. Additionally, integrating the model with IoT devices, such as smart glasses or handheld devices, could offer immersive, real-time communication experiences."}]}