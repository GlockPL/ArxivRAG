{"title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "authors": ["Jiacong Wang", "Bohong Wu", "Haiyong Jiang", "Xun Zhou", "Xin Xiao", "Haoyuan Guo", "Jun Xiao"], "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation. In this paper, we present World to Code (W2C), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of W2C by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code.", "sections": [{"title": "1 Introduction", "content": "Fueled by the rapid development of Vision-Language Models (VLMs) (Zhu et al., 2023; Liu et al., 2024b; Team et al., 2023; Liu et al., 2024a; Dong et al., 2024b) and Diffusion Models (DMs) (Betker et al., 2023), collecting detailed and concrete high-quality captions for each image becomes more and more urging. However, expensive and tedious human labeling for high-quality image-text pairs further incurs the necessity of a cheap and reliable data construction pipeline without human intervention.\nRelated works on image-text data curation can be divided into two main streams. Distillation-based methods leverage closed-source commercial products (e.g., GPT-4V (Achiam et al., 2023)) with the state-of-the-art performance for image caption (Chen et al., 2023a; Li et al., 2024b; Chen et al., 2024a). Another line of work curates an image caption pipeline with existing VLMs to filter high-quality image-text for the training of better VLMs. These methods usually combine open-source LLMs (Touvron et al., 2023a,b; Chiang et al., 2023) and different visual specialists (Li et al., 2023a; Huang et al., 2023b; Zong et al., 2023; Zhang et al., 2024a; Fang et al., 2023; Minderer et al., 2022; Ren et al., 2024; Zhang et al., 2023b) to endow existing VLMs with new abilities, e.g., pixel grounding in GLaMM (Rasheed et al., 2023). However, the dependency on a mixture of specialists and human feedback in filtering noisy generations (Wang et al., 2023b) makes it difficult to scale the generated data and automate the process.\nRecent progress shows that generated results of LLMs (Wang et al., 2022; Li et al., 2023c) and VLMs (Zhang et al., 2024b) for prompts with similar meanings should be alike and we can help filter out noisy generated texts and captions by consistency checking among multiple prompt instructed results. In light of the above evidence, we present a self-instructed data construction pipeline, coined W2C. W2C autonomously extracts and articulates specific content from images, and enhances the reliability of the generated image captions by employing consistency filtering by assessing the outputs through multiple instructed prompt consistencies. The overall pipeline reduces requested specialists and frees off expensive human feedback as shown in Figure 1. In addition, we leverage the idea from human-machine interaction and organize the model-generated responses into a Python code format, following Eureka (Ma et al., 2023) and Text2Reward (Xie et al., 2023). Experiments have shown that our proposed W2C can improve VLMs on various visual question-answering benchmarks."}, {"title": "2 Related Work", "content": "Vision Language Models With the emergence of LLMs (OpenAI, 2023; Achiam et al., 2023; Touvron et al., 2023a; Team et al., 2023; Jiang et al., 2024), VLMs (Zhu et al., 2023; Zhang et al., 2023a; Team et al., 2023) have demonstrated exceptional capabilities in visual recognition and understanding, achieving remarkable results on various VLM benchmarks (Singh et al., 2019; Tito et al., 2021; Zhang et al., 2024b; Liu et al., 2023b; Ying et al., 2024; Fu et al., 2024). The seminal BLIP2 (Li et al., 2023a) firstly introduces Q-Former to adapt encoded image features as potential language tokens for LLM-based caption prediction. Following works (Liu et al., 2024a; Team et al., 2023; Dong et al., 2024c) improve the visual component by replacing VIT (Dosovitskiy et al., 2020) or scaling the input image resolution, while Zhu et al. (Zhu et al., 2023) extends BLIP2 by employing emergent open-source LLMs (Touvron et al., 2023a; Chiang et al., 2023), endowing current VLMs with significantly better instruction following and problem solving abilities. LLaVA/LLaVA-1.5 (Liu et al., 2024b, 2023a) further remove Q-Former and point out that simple MLP projection layers present impressive performance in aligning image representation with LLMs. Some works also highlight the importance of collecting high-quality cross-modal alignment data for improving the consistently scaling VLMs (Bai et al., 2023; Wang et al., 2023b; Li et al., 2023b).\nMulti-modal Dataset Construction The scarcity of high-quality human-labeled data inspires the synthesis of cross-modal data (Wang et al., 2024; Chen et al., 2023a; Rasheed et al., 2023; Wang et al., 2023a; Li et al., 2024b; Lu et al., 2023; Dong et al., 2024a; Chen et al., 2024c). Among them, Wang et al. (2023b) propose the AS-1B data generation pipeline and open-sourced high-quality dense captions on 1B images. GLaMM (Rasheed et al., 2023) further extends AS-1B by introducing about 10 specialists of different functionalities including grounding, tagging, and in-context learning. These specialists enable pixel-wise grounded dense captions for"}, {"title": "3 Method", "content": "Our data construction pipeline shares some similarities with GLaMM (Rasheed et al., 2023), where both methods focus on the region-level caption of the whole image. W2C further extend GLaMM to support generation-validation consistency filtering by exploring different organization formations of the labeled elements and present how VLMs boost themselves on basic multi-modal understanding tasks.\nTo make a comprehensive and systematic exposition of our W2C entire pipeline, the following will be divided into three parts for discussion:\n(1) Visual Concepts Extraction in Section 3.1, (2) Self-Instructed Information Extraction in Section 3.2, (3) Information Filtering via Self Consistency in Section 3.3, (4) Structured formatting in Section 3.4. The overview of our construction pipeline is shown in Figure 2 and all the used instruct prompts are shown in Appendix A.1."}, {"title": "3.1 Visual Concepts Extraction", "content": "To build a fully covered concept list for each image I in images dataset Draw, we prompt VLMs to generate both general captions (for a concise overview of the image) and detail captions (to bootstrap as many visual concepts as possible in the caption) using specific instruct prompts pg and pd. We use beam search to encourage the VLMs to provide as many visual concepts as possible to improve generation diversity. The general captions and detail captions are obtained as follows:\n$o_g, o_d = f_{VLM}(I, p_g), f_{VLM}(I, p_d).$ (1)\nSince visual concepts are mainly composed of noun phrases, we employ the NLTK toolkit (Bird, 2006) to extract all noun phrases denoted as N from og and od.\nWe use Grounding DINO to map extracted noun phrases to bounding boxes of the current image, where part of the false positive noun phrases are filtered as they fail to be mapped with corresponding areas in the image. Here we denote the filtered visual concepts as C, and their corresponding bounding boxes as B, which is formulated as follows:\n$B, C = DINO(I, N).$ (2)"}, {"title": "3.2 Self-Instructed Information Extraction", "content": "After obtaining visual concepts, we extract region-level captions and OCR information for cropped images of each concept bounding box, respectively.\nRegion-level Captions We crop image I for each visual concept $c_i \\in C$ with its corresponding bounding box $b_i \\in B$ to obtain a detailed caption and prompt the VLMs to provide a general caption centered on ci. Additionally, to encourage the VLMs to offer more concrete details about the properties of ci, we instruct the VLMs to include the color and material of ci in the caption. Denote the description prompt for the region-level caption as $P_{desc}(c_i)$ and the image cropped by bi as $I(b_i)."}, {"title": "3.3 Information Filtering via Self Consistency", "content": "Our consistency filtering strategy is inspired by the similar generator-validator consistency findings in ConBench (Zhang et al., 2024b), where different instruct prompts may lead to in-consistent captions of visual concepts, and the highly consistent generations are prone to be correct ones. In this paper, we propose to filter the visual concepts via generation-validation consistency, where we change the region-level captions into multiple visual question answering problems for both counting filtering and caption reranking.\nCounting Filtering via Consistency Different from AS-1B, we introduce Grounding DINO in our construction process, which can naturally filter part of the plausible visual concepts as these concepts usually fail to find corresponding bounding boxes in the image. However, Grounding DINO introduces new challenges for counting problems, as visual concepts ci might be mapped to multiple boxes that have a large overlap due to inappropriately designed hyper-parameters. To prevent the effect by plausibly mapped (bi, ci), we group all the ci that have the same name into C and calculate the existing times for each \u010di \u2208 C as ni. We then merge all the boxes for each \u010di (which might contain multiple visual concepts with the same name) into B, for a box bi \u2208 B we crop the image and prompt the VLMs to check whether the group element \u010di exist ni times in the image via instruct prompt pvalid-g:\n$\\check{c_i}$\n$O_{valid-g}(c_i) = f_{VLM}(P_{valid-g}( \\check{c_i}, n_i), I(b_i)).$ (5)\nCaption Re-ranking via Consistency To provide better region-level captions for a given visual concept, we use beam search to bootstrap multiple caption candidates. To select the best candidate, we again leverage the generator-validator consistency. Specifically, for each given visual concept ci, we get a list of caption candidate $[O_{desc}(c_i), O_{desc}(c_i), ..., O_{desc}(c_i)]$. We use NLTK to parse these captions and collect all the visual concepts that are contained in these captions. Taking n as the total number of extracted concepts in the captions of ci, we get a new visual concept list denoted"}, {"title": "3.4 Structured Formatting and Filtering", "content": "As shown in Figure 2, we organize the structured information into code format to fully represent the region-level information of an image. Inspired by Eureka (Ma et al., 2023) and Text2Reward (Xie et al., 2023), we organize the information as a structured representation into the Python format due to its generality and conciseness. The organization is achieved by the following three rules.\nOne general caption og of the whole image as the comments of each image Class.\nEach visual concept is an attribute for the image class. For each visual concept ci, we"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets For the data construction pipeline, we strictly use the images in the ShareGPT4V dataset for our self-instructed approach validation in a fair comparison. Since the original ShareGPT4V dataset contains duplicate images, We remove the repeated images in the original 102K data and get about 87K original images. We follow the practice of LLaVA-1.5 (Liu et al., 2023a) to adopt a two-stage training approach consisting of prompt tuning (PT) and instruct tuning (IT). For the experiments on low resolution setting, we follow the LLaVA-1.5 to use training dataset LLaVA558k for PT stage and LLaVA665k for IT stage on LLaVA-1.5 training stages. As the specific mixture ratio details of the LLaVA-NeXT data were omitted, we directly utilized the entire training set from each of the following datasets in the IT stage, forming a mixture of datasets including: LLaVA665k (Liu et al., 2023a), DocVQA (Tito et al., 2021), ChartQA (Masry et al., 2022) and ShareGPT4V (Chen et al., 2023a) on high resolution setting.\nTo comprehensively assess the effectiveness of our constructed dataset, we evaluate the model on widely adopted multi-modal benchmarks and grouding benchmarks, including TextVQA (Singh et al., 2019) (without providing OCR tokens), DocVQA (Tito et al., 2021), ChartQA (Masry et al., 2022), MME (Fu et al., 2024), MMT Bench (Ying et al., 2024), MMStar (Chen et al., 2024b), ScienceQA (Lu et al., 2022), POPE (Li et al., 2023d), GQA (Hudson and Manning,"}, {"title": "4.2 Main Results", "content": "Effectiveness of W2C data improve various VLMs in Visual Question Answering benchmarks We show a quantitative comparison results of the trained VLMs with and without the ShareGPT4V dataset, as well as W2C for replacement of the ShareGPT4V during the IT training stage in Table 1. W2C consistently improves the performance on different settings in both LLaVA-1.5 and LLaVA-NeXT. Especially, in the high resolution setting, our W2C presents impressive performance improvement on multi-modal visual understanding benchmarks such as MMT Bench, MM-"}, {"title": "4.3 Ablation Studies", "content": "Our results show advantageous performance in Table 1 and Table 2, but our analysis of these results shows the limitations of the base model's OCR ca-"}, {"title": "4.4 Code Parsing Ability Evaluation", "content": "We further present better cross-modality equivalence between image and text brought by the new code parsing ability. An ideal caption of the image should enable the ability to question without referring to the image. Therefore, we compare the quality of the code output and widely used detail caption output in the ability to handle downstream tasks via in-context learning on the same Large Language Model.\nExperimental Setting We conduct experiments on both LLaVA-1.5-7B/13B and LLaVA-NeXT-7B/13B on two widely used Visual Question Answering benchmarks, including GQA and the perception subset of MME. Due to the support of 32k long context and satisfying performance in the open-source community, we use Qwen-1.5-14B (Bai et al., 2023; Team, 2024) as the problem-solving LLM, and prompt it with few shot inputs."}, {"title": "5 Conclusion", "content": "This paper presents W2C , an enhanced data construction pipeline that only leverages existing VLMs themselves for detail and compositional captions for an image, which is further organized in Python code format. We present that existing VLMs can improve themselves on the understanding benchmarks in various scenarios, significantly reducing the need for a mix of visual specialists and heavy human annotations. Moreover, additional experiments show that the new code parsing ability of VLMs presents better capability in fully describing the image, with notable improvement in the few-shot evaluation on downstream tasks when the raw images are not provided. Our proposed W2C not only enhances the original capabilities on the widely used multi-modal understanding benchmarks but also endows existing VLMs with detailed and executable multi-modal parsing ability."}, {"title": "6 Limitation", "content": "Despite the advancements in improved multi-modal understanding benchmarks and new code parsing ability, W2C can be further improved in some aspects.\nIn this paper, we directly use the ShareGPT4V dataset images for a fair comparison with ShareGPT4V. However, it contains fewer OCR-centric images, limiting the final performance. Further investigation could be taken in studying the performance of W2C on more distribution of unlabeled datasets.\nThe experiments are mainly conducted on the SOTA open-source VLM structures, i.e., the LLaVA series which use MLP projectors for multi-modal alignment. The effectiveness of W2C can be further investigated on other VLM structures.\nGiven the promising performance of W2C on evaluation benchmarks, we would like to explore a more high-quality and diverse data generation pipeline in future investigations."}, {"title": "A Prompt Templates for W2C data construction pipeline", "content": "A.1 Prompt Templates\nW2C data construction pipeline calls the VLMs repeatedly by using different prompts. We guide the VLMs to accurately answer questions by designing universal prompt templates, thus ensuring better compliance with instruction. All the prompts are shown in Table 7."}, {"title": "B More experiments of W2C .", "content": "B.1 More Visual Question Answering Benchmarks\nWe show more Visual Question Answering benchmarks of W2C on LLaVA-NeXT-7B/13B under different combination of IT datasets in Table 8. The W2C method consistently demonstrates superior experimental results.\nB.2 Code Parsing Ability Evaluation\nWe have added an analysis of in-context learning for two representative datasets in Table 9: MM-Star and RefCOCOg. It's important to note that although we report the in-context learning results on RefCOCOg val set under the same settings, comparing these two types of outputs for grounding tasks is not practically meaningful. This is because when we instruct the W2C -trained model to output in detailed caption format, the captions do not usually contain specific box information like [x1,y1,x2,y2]. This leads to a low IoU score for in-context learning with 2/4 shot detailed captions. However, when outputting in code format, the model does predict box information, which accounts for the significant difference in results on RefCOCOg."}, {"title": "C Implementation Details for W2C experiments", "content": "C.1 Dataset Details\nAll the creators or original owners of assets used in the paper are credited properly, and the license and terms of use are explicitly mentioned and are respected properly. All datasets we use are from internet open-source datasets under CC-BY licenses and are cited properly.\nData Construction Pipeline Details We incorporate images from the open-source ShareGPT4V dataset, totaling approximately 87K images. For"}, {"title": "C.2 Implementation Details of our Pipeline", "content": "We employ beam search to fully leverage the powerful language generation capabilities and extensive knowledge base of VLM. This approach enables the generation of an increased number of captions, assisting us in acquiring a broader set of visual concept candidates. Due to the limitation of GPU memory, we set the generation beam to 8 on LLaVA-1.5 and 4 on LLaVA-Next. The learning rate for the"}, {"title": "C.3 Data Example", "content": "In Figure 3 and Figure 4, we present images from the ShareGPT4V dataset alongside the corresponding annotations we constructed by W2C. As shown in these images, the annotations generated entirely by the VLMs accurately describe both the global captions and the detailed captions of local entities within specific areas. Additionally, the OCR text is also encapsulated within the corresponding frames. For multiple entities present in the images, a display of group merging is also conducted."}]}