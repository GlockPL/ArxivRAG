{"title": "PUPPET-CNN : INPUT-ADAPTIVE CONVOLUTIONAL NEURAL\nNETWORKS WITH MODEL COMPRESSION USING ORDINARY\nDIFFERENTIAL EQUATION", "authors": ["Yucheng Xing", "Xin Wang"], "abstract": "Convolutional Neural Network (CNN) has been applied to more and more scenarios due to its\nexcellent performance in many machine learning tasks, especially with deep and complex structures.\nHowever, as the network goes deeper, more parameters need to be stored and optimized. Besides,\nalmost all common CNN models adopt \u201ctrain-and-use\" strategy where the structure is pre-defined\nand the kernel parameters are fixed after the training with the same structure and set of parameters\nused for all data without considering the content complexity. In this paper, we propose a new CNN\nframework, named as Puppet-CNN, which contains two modules: a puppet module and a puppeteer\nmodule. The puppet module is a CNN model used to actually process the input data just like other\nworks, but its depth and kernels are generated by the puppeteer module (realized with Ordinary\nDifferential Equation (ODE)) based on the input complexity each time. By recurrently generating\nkernel parameters in the puppet module, we can take advantage of the dependence among kernels\nof different convolutional layers to significantly reduce the size of CNN model by only storing and\ntraining the parameters of the much smaller puppeteer ODE module. Through experiments on several\ndatasets, our method has proven to be superior than the traditional CNNs on both performance and\nefficiency. The model size can be reduced more than 10 times.", "sections": [{"title": "1 Introduction", "content": "In recent years, Convolutional Neural Networks (CNNs) have become the most popular structure in deep learning\ncommunity. As one of the key building blocks in artificial intelligence, CNNs are playing a critical role in many\nmachine learning applications. Through unremitting exploration and improvement, CNNs have proven to be effective in\nmany research tasks, where the model structures are often complex and deep.\nHowever, traditional CNN suffers from several limitations, which compromises its actual efficiency. First, although a\nwider and deeper network usually has better performance [1, 2, 3], it comes at the cost of an explosion in the number of\nnetwork parameters and the consequent challenges in the model storage and optimization, which hinders its widespread\nadoption on many devices. Second, CNN was designed to emulate the information process of the human brain on the\nneuron level with a serial of processing and interactions across convolutional layers. However, existing models are\noften constructed with independent neurons (i.e. convolutional kernels in CNNs), ignoring the inter-layer dependencies\nduring the processing. Last, almost all classic CNNs adopt \"train-and-use\" strategy. The structure of the network\n(such as depth) is pre-defined, the kernel parameters are fixed after the training, and the same set of kernels and the\nsame network structure are applied to all data samples. However, different data samples contain contents of different\ncomplexities, and a network should ideally process every data sample with an appropriate structure and set of kernels to\nreach the best balance between the performance and efficiency. Obviously, current CNNs are not flexible enough to\ndeal with such heterogeneity in data, although it is very common to see in the real world."}, {"title": "2 Related Works", "content": "To tackle the performance-efficiency problem, some efforts have been made on model compression, using techniques\nsuch as playing kernel tricks, parameter quantization and knowledge distillation. However, the reduction of computa-\ntional complexity was usually at the cost of performance compromise. Also, all of them still tried to find a uniform\nnetwork structure with the same set of parameters for all data although there is a difference in application requirements\nand data complexity. A deep fixed infrastructure trying to handling all data may be redundant and introduce unnecessary\ncomplicatedness, which may also compromise the efficiency. Some researchers [4] found that not all data samples\nneeded deep structures and proposed using CNNs with input-adaptive depth. Nevertheless, no matter what the proposed\nstrategy was based upon, early-exiting [5, 6, 7, 8, 9, 10, 11] or layer-skipping [12, 13, 14, 15, 16, 17], previous depth-\nadaptive CNNs always used the method of \u201csubstraction\", i.e., training a very large model and only executing a part of\nit each time. Also, for different data samples, only the network structure was adaptive but the model parameters could\nnot be adjusted accordingly. On the other hand, parameter-adaptive networks [18, 19, 20, 21, 22, 23, 24, 25] applied an\nindependent small network to generate input-adaptive parameters for a pre-defined network structure. Although the\ntraining of a small network helps reduce the complexity faced by a deep network, the possible acceleration through the\nchange of network structure is neglected.\nIn this paper, we propose a new framework, named Puppet-CNN, to well balance the model performance and efficiency\nfor CNNs. More specifically, our framework is composed of two modules: a puppet module to process input data like\ndone by a common CNN and a puppeteer module. All kernels of the puppet module are generated by the puppeteer\nmodule, a continuous-time recurrent model constructed based on Ordinary Differential Equation (ODE). By doing so,\nthe optimization of a very deep puppet network is translated to the optimization of the puppeteer module, which is much\nsmaller with much fewer parameters to store. Besides, the kernels and depth of the puppet network are determined by the\npuppeteer ODE according to the complexity of the input data, which makes our framework more flexible and adaptive.\nFurther, by using ODE to generate kernel parameters in CNN, the convolutional layers in the puppet module can be\ninter-dependent for better extracting sequential features across layers. We would like to mention that our framework\nis \"puppet-independent\", that is, the puppet module can be any type of CNN model, no matter it is hand-crafted or\ngenerated through Network Architecture Search (NAS)."}, {"title": "2.1 Depth-Adaptive Neural Network", "content": "As a previous consensus, neural networks would be more effective with the increase of depth and width [1, 2, 3].\nHowever, deep structures are found to be unnecessary [4] in many cases, especially when dealing with simple inputs.\nRather than using a fixed structure for all data samples, input-adaptive networks were considered, where simple inputs\nonly go through a shallow structure and complex ones go through a deep model. In Early-Exiting [5, 6, 7, 8] methods,\nsimple samples were allowed to exit the model earlier. Generally, a model could use any existing CNN structures,\nsuch as in [7, 5], or stack a set of them with an increased depth like done in [9, 26, 8]. Criteria, such as the model\nperformance achieved based on the features from previous layers [5, 6] or the uncertainty of the intermediate classifier\noutput [7, 8], were applied to determine whether the Early-Exiting should be triggered. MSD-Net [10] integrated\ndense connection [27] and multi-resolution branches, and the same idea was also used in [28, 11]. In [29], all inputs\nwere forced to pass through shallow layers and changed were only made on the deep layers. To make the network\nmore adaptive, Layer Skipping were not only performed on deep layers, but also anywhere according to the input.\nGating functions were used in [12, 13], where the output from the previous layer was applied to determine whether the\ncurrent layer should be skipped. Built upon Adaptive Computation Time (ACT) [30], SACT-ResNet [14] used a halting"}, {"title": "2.2 Parameter-Adaptive Neural Network", "content": "In most traditional CNN works, parameters are optimized over all the training data and then fixed and applied to all the\ninputs during the inference, which limits the learning and expressive capabilities of the networks. Inspired by [18],\nsome attention has been drawn to use adaptive parameters, and methods are divided into two main branches: Parameter\nAdjustment and Parameter Generation. With the first group, a fixed set of kernel parameters are still learned over the\nwhole dataset, but the weights are adapted in response to the input during the testing phase using schemes such as an\nsample-wise attention mask in CondConv [19]. Dynamic Convolution [20] further added a normalized constraint to the\nattention mask. In [31, 32], spatial-specific soft attention was added to make the kernels dynamic. Rather than adjusting\nmodel parameters through attention on the fly, Parameter Generation or Parameter Prediction [33] directly generate\nparameters according to inputs using another model during testing, which is more straightforward but challenging. In\nthe early-stage work [33], the parameter matrix of the model was represented as a product of two smaller matrices, with\none fixed after the training and the other obtained according to the input at the test time. As two classic approaches\nof parameter generation, Dynamic Filter Network (DFN) [21] and HyperNetwork [22] generated parameters at run\ntime with a separate network for CNN and RNN respectively. As an extension from DFN, LS-DFN [23] generated\nspatial-specific parameters while the Decoupled Dynamic Filter (DDF) Network [25] decoupled dynamic kernels into\nspatial and channel ones. Meta Networks [34] generated parameters using a meta learner across tasks. Generalizing\nthe scheme in [19], in WeightNet [24], kernel parameters were generated through grouped FC layers. Besides, extra\ninformation, such as camera perspective, was integrated for parameter generation in [35], and edge attributes were\nutilized to generate convolutional parameters on graphs in [36]. Although parameters became input-adaptive in above\nworks, the network structure was not adaptive correspondingly. Still using a common DNN or CNN, parameters\ngenerated for each convolutional layer were independent. As the feature map was used as the input to obtain the new\nparameters for the next layer, the parameter-generation network needed to be relatively large to accommodate the large\nfeature maps of the first several layers. Generative methods [37, 38], such as diffusion models, are recently exploited to\ngenerate network parameters. Aiming to avoid training the network, however, these schemes only generate a fixed set of\nparameters for a fixed network structure without any input adaptation. Constrained by the relatively high computational\ncomplexity from the generative models, they can only generate parameters of a small network or partial parameters of a\nbig network."}, {"title": "2.3 Model Compression", "content": "Model compression has attracted more and more attention in recent years to deploy the model in devices with limited\nstorage space and processing speed. Current methods can be separated into several groups, including architecture\ncompression [39, 40, 41, 42], parameter quantization [42, 43, 44, 45, 46] and knowledge distillation [47, 48]. However,\nthese methods used the same structure for all input data, and reduced the computation or model size at the cost of\nperformance degradation since the tricks added into the kernels limited their expressiveness in the network. Like [21, 22],\nanother small network is used in our framework to generate parameters of a deep network. Differently, we propose to\nuse Neural Ordinary Differential Equation (Neural-ODE) [49] to recursively generate the parameters of each layer in\nthe target CNN, such that each time our model only needs to run over small kernels instead of the large feature maps for\nthe generation of a new layer's parameters. In addition, using input-adaptive depth will help reduce the unnecessary\ntime consumed by a fixed deep structure on simple inputs."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 CNN Overview", "content": "A typical convolutional neural network (CNN) block is composed of \u201ckernel multiplication + bias addition + non-linear\nactivation\", which can be expressed as\n$X_{l} = \\sigma(X_{l-1} \\times W_{l} + b_{l}),$                                                              (1)"}, {"title": "3.2 Model Framework", "content": "To reduce the model size while still keeping the diversity of kernels, we propose a new framework, called Puppet\nConvolutional Neural Networks (Puppet-CNNs). As shown in Fig. 1, our framework consists of two modules: a puppet\nmodule F and a puppeteer module G. Being a CNN model, the puppet F directly processes the input data, but its\nparameters are all recurrently generated by the puppeteer G. The puppeteer is built as a Neural-ODE model in our work.\nBesides its simplicity in implementation, with the parameters of a layer recurrently generated based on the ones from"}, {"title": "3.3 Recurrent Parameter Generation", "content": "To generate the parameters of convolutional blocks in the puppet-net F, we adopt a Neural-ODE module for our\npuppeteer G. More specifically, each $P_{l,F}$ in Eq. (2) within the puppet module F can be obtained by\n$P_{l,F} = \\int^{l}_{l-1} G(P_{l,F})dt,$\n$\\frac{dP_{l,F}}{dl} = G(P_{l,F}),$                                                              (6)\nwhere $G(\\cdot)$ is the derivative function in the ODE, parameterized by a very shallow model such as a 1-layer depthwise-\nseparable CNN [39] that has much fewer parameters. For simplicity, we realize the integration in Eq. (6) with the Euler\nMethod, so that we can rewrite the above formula as\n$P_{l,F} = P_{l-dl,F}+G(P_{l-dl,F})dl,$                                                              (7)\nwhere $dl$ is the update step size of the ODE model. As shown in Fig. 2, this allows us to use a small model with\nparameters $P_{G}$ to recurrently generate the parameters $P_{F}$ of a very deep CNN structure and makes the kernel parameters\nof adjacent layers inter-dependent. However, for a common convolutional layer, the dimension of the overall parameters\nis $(C_{out}, C_{in}, K, K)$, where $K$ is the kernel size while $C_{in}$ and $C_{out}$ are the input and output channels of the layer,\nand different layers need parameters of different dimensions. As the Neural-ODE can only have one group of input\nbut the kernel dimensions of different layers in the Puppet module are different, we configure the dimension of the\nvariable in ODE as $(C_{out,max}, C_{in,max}, K_{max}, K_{max})$, where each dimension is equal to the corresponding maximum\nin the puppet module, and we resize it as $(C_{out,max}, C_{in,max}, K_{max} \\times K_{max})$ during the generation. To achieve the\ndesirable dimensions at different layers, for each layer I of the puppet module F, we feed the output of the ODE module\ninto an average pooling layer and resize it to obtain the needed $P_{l,F}$ of dimension $(C_{out,l}, C_{in,l}, K_{l}, K_{l})$.\nIn Eq. (7), the weights $P_{l,F}$ are generated based on the ones for the previous layer, $P_{l-dl,F}$. We also need to give initial\nkernel parameters $P_{0, F}$ to the puppeteer ODE model to start the weight-generation process. In order for the proposed\nmodel to generate different parameters according to different inputs, we propose to calculate a sample-wise $P_{0, F}$ based\non data complexity each time before we process a data sample, which we will discuss in detail in the next subsection.\nIn summary, such a recurrent-parameter generation has the following advantages:\nBy converting independent parameters in CNNs into related ones between neighboring convolutional blocks,\nthe sequential feature extraction process in the puppet module F can better extract features of data;"}, {"title": "3.4 Information-based Depth & Parameter Adaptation", "content": "Our proposed puppeteer ODE module not only generates the kernels but also determines the depth of the network based\non the content complexity of the input sample. To measure the input complexity, we exploit the use of information\nentropy function $E(\\cdot)$ (widely used in image-related fields for complexity measurement [50, 51, 52]):\n$E(X_{o}) = \\sum_{X_{o,i}}p(x_{o,i}) log p(x_{o,i}),$                                                              (8)\nwhere $X_{o}$ is the input data, $x_{o,i}$ is the data value such as the pixel value in the images and $p(\\cdot)$ is the corresponding\nprobability. The pixel values in Eq. (8) alone cannot fully reflect the input complexity. We also include the gradient\nfeatures by calculating the information entropy on the corresponding frequency map $Y(X_{0})$ obtained by Fourier\nTransform, and the final complexity measurement is the combination as\n$H(X_{o}) = \\frac{1}{2}E(X_{o}) + \\frac{1}{2}E(Y(X_{o})).$                                                              (9)\nWe estimate the initial value $P_{0, F}$ and the propagation step $dl$ in the ODE module G with this measurement. We would\nlike to emphasize that due to the continuous-property of ODE, we can easily and quickly adjust the depth of the puppet\nnetwork F and the change rate of the kernel parameters $P_{F}$ through $dl$, as shown in Fig. 3, which is superior and more\nflexible than using RNN models. Specifically, we define the puppet module F as a consecutive stack of convolutional\nlayers with different output channels. Like in other common CNNs, a number of layers may use the same number of\noutput channels and the number of such layers in the puppet network F is determined by the ODE step $dl$. In other\nwords, we just need to give a list of distinct output channels $[C_{1}, C_{2}, ...]$, and the number of layers using each output\nchannel can be determined through the ODE propagation with the adaptation of the step $dl$. For simplicity, we adapt the"}, {"title": "3.5 Training Objective & Implementation Details", "content": "Training Objective: Our method is proposed as a generalized model to explore the best performance-efficiency balance\nof CNN. As it can be applied to any existing structures to complete different tasks, the loss function depends on a\nspecific problem. Take the image classification as an example, we use the cross-entropy loss to guide the training.\nImplementation Details: The puppet module F is designed as a purely convolutional structure, whose output channels\nare 64, 128, 256, 512 respectively, and the number of layers using each output channel is decided by the adaptive\npropagation step of the puppeteer ODE module G. Each convolutional layer in F is followed by a batch normalization\nlayer and a ReLU activation layer. Everytime the output channel is changed, a 2D max pooling layer is added. All the\nconvolutions use 3 \u00d7 3 kernels, which is generated by ODE propagation and processed by 2D average pooing. The\npuppeteer module G is realized by a 1-layer depth-separable convolution, followed by batch-norm and tanh activation."}, {"title": "4 Experiments", "content": "We use image classification as an example application to evaluate the effectiveness of our model. Our model, however,\nis not designed for a specific task. We propose a new way to build CNN models in a much more efficient and flexible\nmanner, that the model can be largely compressed and run robustly under different inputs with higher performance."}, {"title": "4.1 Datasets & Preprocessing", "content": "Cifar-10 [53]: Cifar-10 contains 60000 real images of 10 categories, and each image is colorful and of the\nsize 32 \u00d7 32. The dataset is pre-divided into the training and testing parts with the ratio as 5 : 1. Since our\nproposed method has fewer parameters to optimize, it is supposed to use less data for training, so we only\ninclude 10000 samples from the training set for study. Furthermore, during training, we select the 20% data\nfrom the set as validation set, and the remaining data are used for training.\nCifar-100 [54]: Cifar-100 also has 60000 images, but they are evenly distributed across 100 categories, and\nsplit by 4:1:1 for training, validation and testing. As there are only about 400 images for training in each\ncategory, we didn't reduce the data further in our studies.\nmini-ImageNet [55]: It is a subset of the ImageNet [56], containing 60000 images, evenly distributed across\n100 categories. We split the whole dataset into training, validation and testing parts by 8:1: 1. Similar to\nCifar-100, we didn't do the further data reduction, and all the images are resized and randomly cropped into\n64 x 64 ones."}, {"title": "4.2 Overall Performance", "content": "To illustrate the effectiveness of our Puppet-CNN, we perform experiments on Cifar-10. We select and rerun some other\nadaptive models in the baselines, including the methods using adaptive-parameters such as DFN [21], WeightNet [24]\nand DDFN [25] (modified to sample-wise, denoted as DDFN-SW), as well as the adaptive-depth methods such as\nBranchyNet [7], SkipNet [12] and DRNN [16]. Since our method focuses on the sample-wise adaptation, we only\nselect the most representative sample-wise adaptive methods in the experiments, but not those that also used pixel-wise\nor location-wise adaption. During implementation, since the size of data sample (i.e., image size) is relatively small,\nwe modify the baselines and restrict the maximum number of output channels of all models (including ours) to 512.\nDuring the training, since we mainly care about the pure performance of the model structure, we fairly re-train each\nmodel with 800 epochs using Adam() at the batch size of 64 on a single GeForce RTX 4090. The evaluation contains\ntwo aspects: performance (evaluated by Top-1 & Top-5 accuracy (\u2191)) and efficiency (evaluated by parameter size (\u2193)\nand inference speed (\u2193)).\nResults in Table. 1 show that our proposed model outperforms others on both performance and efficiency. Since\nonly a subset of the whole data is used for training, without using data augmentation such as flipping or cropping,\nbaseline models can hardly achieve the same good performance as reported in their papers. Benefited from much fewer\ntrainable parameters from the puppeteer ODE module, our model can better deal with data insufficiency, a problem\noften encountered in a practical application. Compared to baselines, our model is shown to obtain a relatively better\nperformance, with a significantly smaller number of parameters and a comparable speed."}, {"title": "4.3 Ablation Study", "content": "To better understand the effects of different components in our model, we divide the study into three categories: the\nPuppeteer-Puppet scheme, parameter adaption and depth adaption.\nPuppeteer-Puppet Scheme: The initial motivation of our Puppet-CNN is to reduce the model sizes of deep neural\nnetworks with the help of our Puppeteer-Puppet structure. To demonstrate this, we compare our model with some\nother methods that compress the model sizes by modifying the structures but at the cost of performance degradation.\nDifferently, as shown in Table. 2, our scheme can significantly reduce the model size while maintaining the good\nperformance achievable through deep networks. Also, within our Puppet-CNN, the puppeteer and puppet modules"}, {"title": "Information-based Parameter Adaptation", "content": "Since all parameters in the puppet module are generated by the puppeteer\nmodule, it provides the possibility of generating input-adaptive parameters to extract different features according to data\ncontent through the puppeteer ODE module. Based on Table. 3, when adding parameter adaption, further improvements\non Top-1 Accuracy are observed."}, {"title": "Information-based Depth Adaptation", "content": "Although the weight-generation process will cost extra time, our depth\nadaption scheme can help our model to save unnecessary processing time by considering the content complexity of\ninputs, thus reducing the average execution time on the dataset to make up for the extra generation cost. We compare\nour Puppet-CNN with original ResNet and \u201cPuppet-ResNet\", where ResNet's structure is fixed while the parameters are\ngenerated by the puppeteer ODE module according to the input. The results are listed in Table. 4. We can see that,\nwith the depth adaption, the total number of operations approaches that of the traditional model. The results in Table. 3\nand Table. 4 also demonstrate the effectiveness of the input complexity measurement function introduced in Sec. 3.4,\nthough we will put more effort onto the selection of it in our future work."}, {"title": "4.4 Robustness Study", "content": "To prove the robustness of our proposed model, we made extra performance studies on the network depth, the number\nof output channels of the puppet module, as well as the dataset complexity."}, {"title": "Network Depth", "content": "To evaluate the capability of our Puppet-CNN on model compression, we remove the depth-adaption\nfunction from our model and only keep the parameter-adaption. Our puppet module is set to follow the structure of\nVGG and ResNet respectively, with the depth fixed each time. In Fig. 4, as the network goes deeper, the model size is\nkept the same with our proposed method. i.e. the model compression rate gets higher. This also indicates that the depth\nof the network can be set high without increasing the model size."}, {"title": "Output Channels", "content": "As mentioned in Sec. 3.3, the number of variables in the puppeteer ODE module is related to the\nmaximum number of output channels in the puppet module. We compare the model sizes under different number of\nmaximum output channels, and the results are given in Table. 5."}, {"title": "Dataset Complexity", "content": "Compared to Cifar-10, Cifar-100 has more categories and is more complicated. In experiments\non Cifar-10, we only use 8000 images for training, i.e. 800 samples for each categories. However, in Cifar-100, each\ncategory only has 400 training samples, i.e. the training is more difficult. The case is the same for mini-ImageNet.\nWhen the task becomes more complicated, our model can achieve higher performance improvement compared to those\nfixed-CNN models, according to Table. 6."}, {"title": "5 Conclusion", "content": "We propose a new convolutional framework named as Puppet-CNN, whose depth and kernels are adaptive when\nprocessing different data samples to achieve the best performance-efficiency balance. The parameters of Puppet-CNN\nare generated by the puppeteer ODE module based on the sample information complexity. Our design has proved to be\nable to use much fewer parameters to generate a deep network, and the proposed framework can be plugged into any\nexisting CNN structure for efficient training and storage. The extensive experiments over several datasets demonstrate\nthat Puppet-CNN outperforms other common literature works with much smaller model size and the processing time\napproaching the average. In the future, we will test our method on other tasks. Reducing the parameter generation time\nis also a potential direction for our future study."}]}