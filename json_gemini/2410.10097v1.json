{"title": "REHRSeg: Unleashing the Power of Self-Supervised Super-Resolution for Resource-Efficient 3D MRI Segmentation", "authors": ["Zhiyun Song", "Yinjie Zhao", "Xiaomin Li", "Manman Fei", "Xiangyu Zhao", "Mengjun Liu", "Cunjian Chen", "Chung-Hsing Yeh", "Qian Wang", "Guoyan Zheng", "Songtao Ai", "Lichi Zhang"], "abstract": "High-resolution (HR) 3D magnetic resonance imaging (MRI) can provide detailed anatomical structural information, enabling precise segmentation of regions of interest for various medical image analysis tasks. Due to the high demands of acquisition device, collection of HR images with their annotations is always impractical in clinical scenarios. Consequently, segmentation results based on low-resolution (LR) images with large slice thickness are often unsatisfactory for subsequent tasks. In this paper, we propose a novel Resource-Efficient High-Resolution Segmentation framework (REHRSeg) to address the above-mentioned challenges in real-world applications, which can achieve HR segmentation while only employing the LR images as input. REHRSeg is designed to leverage self-supervised super-resolution (self-SR) to provide pseudo supervision, therefore the relatively easier-to-acquire LR annotated images generated by 2D scanning protocols can be directly used for model training. The main contribution to ensure the effectiveness in self-SR for enhancing segmentation is three-fold: (1) We mitigate the data scarcity problem in the medical field by using pseudo-data for training the segmentation model. (2) We design an uncertainty-aware super-resolution (UASR) head in self-SR to raise the awareness of segmentation uncertainty as commonly appeared on the ROI boundaries. (3) We align the spatial features for self-SR and segmentation through structural knowledge distillation to enable a better capture of region correlations. Experimental results demonstrate that REHRSeg achieves high-quality HR segmentation without intensive supervision, while also significantly improving the baseline performance for LR segmentation.", "sections": [{"title": "1. Introduction", "content": "Magnetic resonance imaging (MRI) is widely used for diagnosis and monitoring due to its high precision in distinguishing between different types of soft tissue, while avoiding the risks associated with ionizing radiation exposure [1]. A critical task in computer-assisted diagnosis and intervention in MRI is the high-resolution (HR) 3D segmentation and reconstruction of regions of interest (ROI) within complex anatomical structures, which can facilitate subsequent procedures such as 3D printing and surgical planning [2]. In recent years, deep learning techniques have emerged as the leading approach in MRI segmentation, with 3D-based neural networks becoming the predominant method to achieve voxel-wise segmentation, as opposed to treating slices independently [3]. Moreover, transformer-based techniques, which inherently capture global dependencies, have shown competitive performance, albeit at the cost of increased model complexity [4]. However, current methods for 3D MRI segmentation face challenges in clinical applications, where 2D scanning protocols are frequently employed to reduce acquisition time. Such protocols result in MR images with high in-plane resolution but lower inter-plane resolution, which cannot be directly processed by current segmentation models to achieve HR segmentation.\nRecently, several works have explored HR segmentation from low-resolution (LR) images [5, 6, 7, 8], presenting a potential solution to the aforementioned issues. These methods typically use downsampled images as the main input while producing HR segmentation directly, thereby avoiding the need for HR images during inference. Some approaches also leverage image super-resolution as a proxy task to assist in capturing fine-grained features and ensuring that high-frequency details are preserved in the segmentation results [5, 6, 9]. However, these methods still rely on the availability of HR images and corresponding annotations for training, which are costly to obtain. On the one hand, prolonged scanning times for HR acquisition may cause patient discomfort and increase the risk of motion artifacts [10]; on the other hand, annotating HR images is considerably more labor-intensive and time-consuming. A potential solution to obtain HR segmentation with only LR images for training is to upsample the acquired LR images and interpolate the labels using morphology-based operators [11]. This strategy has been adopted in some datasets [12], but it can lead to misaligned labels, particularly on the ROI boundaries.\nIn this paper, we rethink current methods for HR segmentation from LR images and propose a novel Resource-Efficient High-Resolution Segmentation framework (REHRSeg) for real-world clinical applications. As illustrated in Fig 1, conventional methods require HR annotated data, which are resource-intensive to collect. In contrast, REHRSeg replaces the supervision from real images with pseudo-HR data and addresses misalignment issues through a self-supervised super-resolution technique for annotated images. This strategy only requires LR data for training, making it more resource-efficient for practical use.\nFurthermore, REHRSeg takes a step further in exploring super-resolution-assisted segmentation by investigating the capability of self-supervised super-resolution (self-SR) to enhance segmentation. This exploration begins by addressing data scarcity using pseudo-HR data generated by self-SR to expand the dataset for the segmentation task. Additionally, REHRSeg incorporates uncertainty-aware learning to improve ROI boundary recognition by utilizing the uncertainty map extracted from uncertainty-aware super-resolution (UASR) head, which highlights regions that are difficult to reconstruct. Moreover, REHRSeg aligns deep features in self-SR to capture correlated regions for segmentation. The key contributions of this work is as follows:"}, {"title": "2. Related Work", "content": "MRI segmentation aims to identify and delineate ROI in acquired MR images, playing a crucial role in computer-aided diagnosis and disease progression monitoring [13]. Recent advancements in deep learning technologies have led to significant breakthroughs in automatic MRI segmentation. Among the most widely adopted architectures are U-shaped CNN-based networks [14, 15, 16, 17], with nnUNet [18] emerging as a leading model. nnUNet automatically configures various datasets and focuses on dataset preprocessing and inference strategies, currently dominating the field of medical image segmentation. Furthermore, vision transformer (ViT) techniques have been applied to MRI segmentation, leveraging the transformer's ability to capture global and long-range dependencies [19, 20, 21, 22, 23].\nDespite the high performance of these methods, they often require substantial computational resources for whole-volume segmentation of HR MRI images. Consequently, several approaches have aimed to develop efficient 3D MRI segmentation models with reduced resource demands by designing lightweight architectures. For example, ADHDC-Net [24] uses decoupled convolution, dilated convolution, and attention-based refinement to minimize the number of parameters and operations in brain tumor segmentation. Similarly, UNETR++ [21] employs efficient paired attention to reduce the complexity of self-attention computations and constrains the number of parameters by sharing weights between queries and keys. MISSU [25] incorporates a self-distillation mechanism to refine local 3D features with multiscale fusion blocks, which are removed during inference to ease computational burdens. However, these lightweight models often struggle to capture rich features, limiting segmentation performance. Our approach addresses this limitation by utilizing the priors embedded in super-resolution models to enhance segmentation performance without increasing computational costs during inference."}, {"title": "2.2. HR Segmentation from LR Image", "content": "Recently, several studies [26, 6, 27, 8, 9] have focused on achieving high-resolution (HR) segmentation from low-resolution (LR) images, primarily designed to efficiently capture global information and further reduce the computational cost, especially during inference. For example, AR-Seg [8] reduces computational costs for video semantic segmentation by lowering the resolution for non-keyframes, while employing a cross-resolution fusion module to prevent accuracy degradation. GDN [7] replaces conventional down-sampling with a learnable procedure to reduce image resolution for real-time segmentation. UHRSNet [28] fuses local features extracted from image patches with global features from downsampled images.\nSuper-resolution techniques have also been employed in this context to improve segmentation performance by capturing fine-grained details and providing richer semantic information. Lei et al. [29] propose a framework for remote sensing images that simultaneously performs quadruple super-resolution and segmentation. PFSeg [26] integrates super-resolution as an auxiliary task for patch-free 3D medical image segmentation, and incorporates a fusion module for joint optimization. ISDNet [27] introduces super-resolution tasks as guides in the coarse-grained branches of the network, and fuses this information into fine-grained branches. DS2F [9] redesigns the segmentation and super-resolution framework by proposing a shared feature extraction module and a proxy loss for ROI recognition.\nWhile these methods achieve HR segmentation from LR images, they still rely on resource-intensive HR annotations during training. In contrast, our proposed REHRSeg framework only requires LR annotations for training, significantly reducing the annotation burden. Additionally, REHRSeg does not depend on HR images for super-resolution assistance, making it more practical for clinical applications where 2D scanning protocols are commonly employed."}, {"title": "2.3. Self-supervised Super-resolution of MR Images", "content": "Due to limitations in medical resources and scanning time, the acquisition of MR images employs a 2D scanning protocol in many clinical settings, resulting in low inter-plane resolution with large slice thickness [30, 31]. Therefore, super-resolution technology has been employed to enhance the resolution of such images, particularly in the inter-plane direction. Deep learning methods, especially CNN-based architectures, have become the dominant approach for MRI super-resolution. Conventional deep learning methods typically rely on fully supervised training [32, 33, 34], requiring paired HR and simulated LR images. However, collecting HR images in real-world clinical scenarios is significantly more challenging and costly than acquiring LR images, making it difficult to obtain sufficient paired training samples.\nTo address this, self-supervised training methods have been increasingly applied to super-resolution tasks. These methods achieve super-resolution of MR images using only LR images for training. For example, Xuan et al. [35] propose a VAE-based generative model trained on LR images, and synthesizes HR images from interpolated latent space to train the super-resolution model. SMORE [36] uses HR and LR data inherent in the high-resolution plane for training, restoring image quality by improving resolution and reducing aliasing. Wang et al. [37] extend SMORE by introducing video frame interpolation as a preliminary task, and design an architecture based on implicit neural representation with a three-stage training protocol to refine the results. TSCTNet [38] incorporates a cycle-consistency constraint to enable self-supervised learning for reducing the slice gap for MR images.\nIn this work, we explore the capacity of self-supervised learning from annotated LR images in the field of MRI super-resolution to enhance and provide supervision for segmentation tasks, a direction not yet explored in prior super-resolution studies."}, {"title": "3. Method", "content": "The overall framework of REHRSeg is designed to produce high-resolution (HR) segmentation \\(\u00dd_{HR}\\), with an inter-plane resolution that is \\(r\\) times higher than the original low-resolution (LR) image \\(I_{LR}\\) and its annotation \\(Y_{LR}\\). As illustrated in Fig. 2, we begin by applying self-supervised super-resolution (self-SR) on the annotated MR images, generating coarse HR annotations \\(Y_{HR}\\) alongside the HR MR images \\(\u00ce_{HR}\\). Next, the self-SR model and the pseudo HR data are leveraged to enhance the segmentation model, enabling simultaneous generation of both refined LR segmentation \\(\\hat{Y}_{LR}\\) and HR segmentation \\(\\hat{\u00dd}_{HR}\\). This is achieved through three key components: 1) utilizing self-SR as a pseudo-data generator for segmentation, 2) incorporating an uncertainty-aware super-resolution (UASR) head in self-SR for additional boundary guidance, and 3) applying structural knowledge distillation from the self-SR model."}, {"title": "3.2. Self-SR for Annotated MR images", "content": "We introduce a self-supervised super-resolution (self-SR) method as a preliminary task in REHRSeg. This approach is built upon the principles of the most recent self-supervised MRI super-resolution method [37], with modifications to support the super-resolution of annotated MR images. Given a LR image \\(I_{LR}\\) and its corresponding annotation \\(Y_{LR}\\), both with a spatial resolution of \\(a \times a \times c\\), we assume that the frequency- and phase-encoding directions have identical spatial resolution. The super-resolution process is applied with a scaling factor of \\(r\\), generating HR images with a spatial resolution of \\(a \times a \times (c/r)\\) in two main steps: First, both the image and annotation are interpolated to achieve isotropic voxel spacing, so that the spatial consistency can be ensured between training and inference stages. Our experiments show that the utilization of SMORE [36] as an alternative to traditional interpolation techniques, such as B-spline interpolation for images and nearest-neighbor interpolation for annotations, can further improve the final results.\nIn the second step, we generate the LR-HR pairs required for self-SR training by simulating slice separation along the \\(x\\) axes. Specifically, the interpolated images are convoluted with a 1D Gaussian filter \\(h(x; r)\\) as the slice profile with FWHM equal to \\(r\\), followed by downsampling by a factor of \\(r\\) along with the annotations. Unlike SMORE, we avoid introducing aliasing artifacts, as our self-SR model directly operates on \\(I_{LR}\\) instead of the interpolated one to reduce computational cost. Additionally, the slice gap is not considered in our self-SR method, as many segmentation datasets lack this information. Notably, several SR methods such as SMORE[36], DeepResolve[32], and SAINR[34] also neglect the slice gap without experiencing significant performance degradation. In this scenario, the task can be considered as slice interpolation and deblurring for 3D data.\nTo accelerate training and ensure faster convergence, we leverage a pre-trained model from the domain of video frame interpolation. This strategy has been previously shown to be effective in self-supervised super-resolution [37]. Here we use the FLAVR [39] model pretrained on Vimeo-90K [40] to initialize the backbone of our self-SR model. By applying the trained model to \\(I_{LR}\\) and \\(Y_{LR}\\), we obtain the estimated pseudo HR image \\(\\hat{I}_{HR}\\) and its annotation \\(\\hat{Y}_{HR}\\). The pseudo data and the self-SR model are then used to enhance the segmentation model in the subsequent stage."}, {"title": "3.3. Self-SR as Pseudo-data Generator", "content": "We propose to utilize the self-SR model as a data generator to create pseudo-data for the MRI segmentation task, which is implemented in two distinct ways: data augmentation for LR segmentation and pseudo supervision for HR segmentation. Given the HR image \\(\\hat{I}_{HR}\\) and its annotation \\(\\hat{Y}_{HR}\\) produced by self-SR, we can simulate the LR data \\(\bar{I}_{LR}\\) and \\(\bar{Y}_{LR}\\) with a large slice thickness that matches the spatial resolution of the segmentation dataset, by blurring and downsampling the synthesized data as follows:\n\\(\bar{I}_{LR} = {\u00ce_{HR} *_z h(z;r)} \u2193_z,\n\bar{Y}_{LR} = {YHR}\\)"}, {"title": "3.4. Uncertainty-aware Learning with Self-SR Guidance", "content": "We further utilize the self-SR model to provide uncertainty guidance for the segmentation task, with the goal of enhancing its performance with uncertainty-aware segmentation. It is widely recognized that the tissues in medical images often overlap and blurriness occur on their boundaries, leading to high uncertainty in the delineation of these regions [41]. This issues not only lead to unreliable segmentation region, but also cause poorly reconstructed regions for the super-resolution tasks. Therefore, we propose to estimate uncertainty in the self-SR model by designing an uncertainty-aware super-resolution (UASR) head, which identifies the regions with high reconstruction error and provides guidance for the segmentation task.\nThe first step of uncertainty estimation with UASR involves the generation of intermediate results. We extract the 3D feature map \\(F_{sr} \\in R^{C\times D \times H \times W}\\) from the backbone of the self-SR model, where \\(D, H, W\\) represent the slice depth, height, and width of the input LR images, respectively. The features along the depth dimension are concatenated into a 2D feature map, which is then processed to generate intermediate features \\(F_{m}\\) via Leaky ReLU activation and convolution. These features are split along the channel dimension and merged back into the depth dimension, yielding N intermediate images \\(\\{P_{y}^i\\}_{i=1}^N\\) and annotations \\(\\{P_{y}^i\\}_{i=1}^N\\). The whole procedure can be expressed as:\n\\(F_{m} = LReLU(conv(merge(F_{sr}; 2; 1)),\nP_{i}^I = tanh(split(F_{m}; 1; 2)W_{i}^I + b_{i}^I),\nP_{i}^y = split(F_{m}; 1; 2)W_{i}^y + b_{i}^y\\)"}, {"title": "3.5. Structural Knowledge Distillation", "content": "We also propose to leverage knowledge distillation from the self-SR model to further enhance MRI segmentation. Given the disparity between image reconstruction and segmentation tasks, traditional distillation methods that constrain features with pixel-wise alignment only provide limited benefits.\nInstead, the structural correlation between regions shares similar patterns between the feature map produced by different tasks, which have also been confirmed in the previous work [42]. Therefore, the segmentation task has potential to benefit from the self-SR task by distilling the correlation patterns from it. Here the feature maps \\(F^{sr}\\) and \\(F^{seg}\\) are extracted from the trained self-SR model and current segmentation model. The shape of \\(F^{sr}\\) is aligned with \\(F^{seg}\\) with bilinear interpolation, resulting in feature maps of \\(C' \u00d7 D' \u00d7 H' \u00d7 W'\\). We model the spatial correlation between regions using a fully-connected affinity graph, where the nodes represent different spatial locations, and the edges denote their similarity. In this graph, we aggregate the \u03b2 voxels in a local patch to represent the feature of each node, setting the granularity of the graph to \u03b2. Consequently, the affinity graph comprises \\((D' \u00d7 H' \u00d7 W')/\u03b2\\) fully connected nodes. We compute the similarity \\(a_{ij}\\) between the \\(i\\)-th and \\(j\\)-th nodes by:\n\\(\bar{F_p} = avgPool(F),\na_{ij} = \bar{F_{p,i}}\bar{F_{p,j}}/(||\bar{F_{p,i}}||_2||\bar{F_{p,j}}||_2)\\)"}, {"title": "3.6. Optimization", "content": "The overall loss for training our segmentation model is composed of an uncertainty-aware segmentation loss, a pseudo HR segmentation loss, and a knowledge distillation loss:\n\\(L = L_{seg}^I(\u00dd_{LR}, Y_{LR}, U) + L_{seg}^{HR}(\u00dd_{HR}, Y_{HR}) + \u03bb(L_{corr}(F^{seg}, F^{sr}) + L_{spatial}(F^{seg}, F^{sr}))\\)"}, {"title": "4. Experiments", "content": "Our experiments are conducted on a public dataset with HR images and their annotations, as well as an in-house dataset with only LR labeled images. Due to the difference in data availability, we calculate quantitative metrics on the public dataset for HR results, while providing only qualitative HR results for the in-house dataset. Both datasets undergo 5-fold cross-validation to ensure a robust and unbiased evaluation."}, {"title": "4.1.1. Synthetic Public Dataset", "content": "We first generate a synthetic LR MRI dataset from the publicly available Meningioma-SEG-CLASS dataset [44], which contains pre-operative T1-CE (HR) and T2-FLAIR (LR) MR images from patients with pathologically confirmed Grade I or Grade II meningiomas. For our experiments, we extract 76 near-isotropic T1-CE images, each containing manually contoured tumors. Following standard practices in super-resolution techniques [32, 31, 45], all images are first Gaussian-filtered before resampling to simulate the acquisition of thicker slices produced by 2D scanning protocols. A downsampling factor of 4 is applied, and our method is completely blind to the original HR images and their annotations during training."}, {"title": "4.1.2. In-house Dataset", "content": "We also collect an internal dataset to evaluate the real-world application of our method. The dataset contains 91 patients that underwent pelvic MRI exams using contrast-enhanced, fat-suppressed T1WI. The MR images are reconstructed with in-plane spatial resolution ranging from 0.42 to 0.74mm\u00b2 and slice thickness between 3mm and 6mm. The MR images were manually labeled to generate the gold standard for training and validation, which were performed by two radiologists using an annotation tool in the Sense-Care research platform [46]. To standardize the data for segmentation model training, the in-plane resolution is resampled to 0.75mm\u00b2, while the original thickness is kept unchanged during training, similar to [47]."}, {"title": "4.2. Implementation Details", "content": "We utilize a downsampling factor (DSF) of 4 for both datasets, and initialize our self-SR backbone with the FLAVR model pretrained on 4\u00d7 video frame interpolation. To stabilize the training of the UASR head, we only include the uncertainty-guided loss during the final 20,000 iterations, following an initial fine-tuning phase of 130,000 iterations with the batch size of 64. For structural distillation, we aggregate only in-plane features with a granularity \u03b2 set to 1 \u00d7 2 \u00d7 2. The spatial distillation process employs a convolution layer following the previous work [50], since experiments show that more complex distillers, such as those in [43], do not improve segmentation performance in this context. The segmentation network is initialized using the default setting of nnUNet-3D [18], and its data augmentation strategy is applied throughout our experiments. To allow the segmenter to produce HR results, we interpolate the features before the last layer and use two convolution with a ReLU activation in between, which is light-weighted to produce the final results. All the experiments are performed on an NVIDIA RTX A6000 GPU."}, {"title": "4.3. Enhanced LR Segmentation", "content": "In this section, we evaluate whether the self-SR model can provide effective guidance for the segmentation model, by inspecting whether its performance is improved for LR segmentation. We also conduct experiments for HR segmentation on the Meningioma-SEG-CLAS dataset, as 2D-based segmentation methods can be directly applied to cross-sectional slices extracted from HR volumes. It is important to note that these 2D-based methods use HR images as inputs, whereas our method operates on LR images in the inference stage. Quantitative results presented in Table 1 indicate that 2D-based methods generally underperform 3D-based methods for MRI segmentation, particularly in terms of the HD95 metric. Notably, nnUNet remains a strong baseline and outperforms several transformer-based methods, especially for the in-house dataset where the complex data distribution poses challenges for these methods to achieve satisfactory results. This can be attributed to the limited amount of training data, which prevents transformer-based methods from fully leveraging their advantages.\nDespite these challenges, REHRSeg significantly improves performance over baseline (p < 0.05, HD95) and outperforms all the alternatives for both LR and HR segmentation. Visualization results in Fig. 5 also demonstrate that REHRSeg excels in both tumor types, and notably enhances the nnUNet baseline in boundary recognition. Moreover, REHRSeg shows strong performance in difficult cases where other methods fail to accurately capture the tumor shapes, as illustrated in the fourth row of Fig. 5."}, {"title": "4.4. Reconstructing HR Results Using LR Data", "content": "Although REHRSeg is trained on LR images and their corresponding annotations, it allows the acquisition of coarse HR results via the proposed self-SR model. Additionally, the segmentation model is enhanced using these self-SR results to produce HR segmentation outputs. The evaluation of these results are further illustrated in the following sections."}, {"title": "4.4.1. Super-resolution Results for Annotated Images", "content": "We first evaluate the quality of our method for self-supervised super-resolution. As there are no existing methods for the super-resolution of both images and labels, we implement three baseline methods, including: 1) B-spline interpolation for image and morphology-based interpolation for label [11] commonly used for producing spatially standardized data [12], 2) a modified version of SMORE (2D) [36] with an auxiliary segmentation head, and 3) our model trained from scratch (i.e., without pretraining on video frame interpolation).\nQuantitative results demonstrate that our method achieves the highest Peak Signal-to-Noise Ratio (PSNR) and Dice Score (DSC), although the Structural Similarity Index (SSIM) metric is slightly lower compared to SMORE. It is noteworthy that pretraining on a video frame-interpolation task significantly improves the performance of the self-SR model (from 29.15 to 29.86 in PSNR), which is also consistent with findings from previous work [37]. The qualitative results shown in Fig. 6 illustrate the super-resolution outcomes for labels overlaid on the super-resolved images. Simple B-spline interpolation results in blurred images and misaligned labels. While SMORE offers notable improvements over B-spline interpolation, it still exhibits issues such as discontinuities on the surface of the pallium (sagittal view) and blurriness on the ROI boundaries (axial and coronal views). In contrast, our proposed self-SR method can preserve image-label correspondence with clear boundaries. Although some serration remains along the borders, further refinement is possible using a video-pretrained model. These precise super-resolution results serve as the foundation that the knowledge gained from our model can guide and enhance the following segmentation process."}, {"title": "4.4.2. HR Segmentation from LR Image", "content": "We evaluate the performance of REHRSeg for HR segmentation from LR image in this section. In addition to the previous works with 3D-based [26, 9] or 2D-based [27] methods for HR segmentation, we also compare with the training protocol that uses the B-spline interpolated data defined in the previous section, which enables the trained model to produce HR segmentation results using upsampled LR images as inputs. This will consume considerably more computational resources during training and inference due to the enlarged input size. Quantitative results in Table 3 indicate that REHRSeg achieves top-ranked performance without any HR ground truth. It is also noteworthy to find that our HD95 metric is even better than fully supervised method, which can be explained by our effective super-resolution assisted segmentation design. The qualitative results for three different subjects in Fig. 7 also support the reported metrics. For instance, 2D-based methods such as ISDNet exhibit better results in the sagittal view compared to other views, but their overall performance is inferior to 3D-based methods like PFSeg. Furthermore, training with interpolated data using a powerful segmentation framework provides a strong baseline for this task. However, the boundaries in these interpolated results are less precise, likely due to misalignment between the interpolated images and labels. In contrast, REHRSeg provides the segmentation results whose boundaries are closest to the ground truth, even in challenging cases such as those from the coronal view, where other methods struggle to achieve satisfactory results."}, {"title": "4.5. Ablation Study", "content": "We conduct an ablation study of each component of REHRSeg to evaluate their effectiveness, and report the quantitative results in Table 4. The experiments start by the incorporation of pseudo data generated by our self-SR model to assist the segmentation model with extra data and an auxiliary task. We can observe an improvement of the LR segmentation performance, and more importantly the availability of HR segmentation. Next, the introduction of uncertainty guidance refines the segmentation model to be aware of the boundaries of ROI, indicated by more advanced segmentation results. Our knowledge distillation strategy achieves the best performance, which significantly surpasses the baseline. This trend is also reflected in HR segmentation, although the addition of uncertainty guidance results in a slight decrease on the HD95 metric."}, {"title": "4.5.2. Comparison of Different Self-SR Methods", "content": "We further assess the impact of replacing our self-SR model with alternative methods for reducing slice thickness. Specifically, we evaluate the B-spline interpolation and SMORE, using their super-resolution results solely as pseudo-data to assist the segmentation model, as these methods do not provide uncertainty maps or 3D feature maps. It is evident that B-spline interpolation negatively affects segmentation performance, as the morphology-based interpolation of labels does not accurately reflect the actual boundaries of the B-spline interpolated images, leading to sub-optimal segmentation results. Although SMORE yields a slight improvement on the DSC scores, they fail to enhance the HD95 metric. In contrast, our method improves both metrics and achieves even better performance when the self-SR model is pretrained on video frame interpolation."}, {"title": "4.5.3. Comparison of Different Knowledge Distillation Methods", "content": "We also evaluate the effect of incorporating different knowledge distillation methods in REHRSeg. For comparison, we include the established distillation techniques for dense prediction tasks: MIMIC [51] with pixel-wise distillation, IFVD with cosine similarity distillation, and REHRSeg using only correlation distillation (i.e., without cosine similarity). We further evaluate the effect of changing the distillation intensity, which is reflected by the weight of distillation loss during training. Quantitative results in Table 6 demonstrate that all methods effectively distill some useful knowledge from self-SR model, especially for the HR segmentation compared with the third row in Table 4. The cosine similarity and correlation-based distillation methods are generally better than pixel-wise distillation using MSE loss, while the proposed distillation strategy using the combination of these methods can further improve the performance. We also find that setting the distillation intensity \u03bb = 1.0 yields optimal overall results, with too large deviations from this value resulting in performance degradation."}, {"title": "5. Conclusion and Discussion", "content": "HR MRI segmentation has been extensively studied to provide accurate and detailed delineation of ROI, while encountering real-world application issues with only LR images in hand. Although existing methods have explored using LR images as inputs for HR segmentation in the inference stage, they still require HR images and their annotations for training, which are both expensive to acquire in clinical scenarios. In this paper, we propose a novel resource-efficient 3D HR segmentation framework, REHRSeg, that can be trained without the need of HR data. By further investigating the self-supervised super-resolution framework, we take advance of its capacities in enhancing the segmentation model from three different aspects. Experimental results for both the synthetic and real-world datasets are promising, as REHRSeg can obtain high-quality HR segmentation, while also achieving superior LR segmentation results in conventional settings.\nDespite the inspiring results as mentioned above, there are still potential extensions that can be explored. For example, more advanced uncertainty-guided strategy [41] can be used to refine the predicted ROI boundaries, and deeper integration of feature interaction between super-resolution and segmentation [9] may further improve the performance, albeit with an increase in computational cost. Moreover, the proposed REHRSeg method is anticipated to enhance other segmentation protocols such as few-shot learning and domain adaptation, which can further contribute to the developments of resource-efficient medical image segmentation."}]}