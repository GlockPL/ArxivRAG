{"title": "ESTIMATING CONTRIBUTION QUALITY IN ONLINE DELIBERATIONS USING A LARGE LANGUAGE MODEL", "authors": ["Lodewijk L. Gelauff", "Mohak Goyal", "Bhargav Dindukurthi", "Ashish Goel", "Alice Siu"], "abstract": "Deliberation involves participants exchanging knowledge, arguments, and perspectives and has been shown to be effective at addressing polarization. The Stanford Online Deliberation Platform facilitates large-scale deliberations. It enables video-based online discussions on a structured agenda for small groups without requiring human moderators. This paper's data comes from various deliberation events, including one conducted in collaboration with Meta in 32 countries, and another with 38 post-secondary institutions in the US.\nEstimating the quality of contributions in a conversation is crucial for assessing feature and intervention impacts. Traditionally, this is done by human annotators, which is time-consuming and costly. We use a large language model (LLM) alongside eight human annotators to rate contributions based on justification, novelty, expansion of the conversation, and potential for further expansion, with scores ranging from 1 to 5. Annotators also provide brief justifications for their ratings. Using the average rating from other human annotators as the ground truth, we find the model outperforms individual human annotators. While pairs of human annotators outperform the model in rating justification and groups of three outperform it on all four metrics, the model remains competitive.\nWe illustrate the usefulness of the automated quality rating by assessing the effect of nudges on the quality of deliberation. We first observe that individual nudges after prolonged inactivity are highly effective, increasing the likelihood of the individual requesting to speak in the next 30 seconds by 65%. Using our automated quality estimation, we show that the quality ratings for statements prompted by nudging are similar to those made without nudging, signifying that nudging leads to more ideas being generated in the conversation without losing overall quality.", "sections": [{"title": "1 Introduction", "content": "Deliberation is a practice that can help bring together stakeholders and ensure an exchange of opinions, facts and understanding. Deliberation is used in various contexts, including mini-publics Goodin and Dryzek (2006), citizen juries Smith and Wales (2000) and deliberative polls Fishkin (1997), to better understand a population's opinions on a complex topic or inform a decision. Some of these designs have reported promising spillover effects such as depolarization Van Der Does and Jacquet (2023); Fishkin et al. (2021).\nConducting deliberation online offers numerous advantages. In-person deliberation can be prohibitively expensive due to transportation and venue costs Luskin et al. (2014). Additionally, organizing unbiased and reproducible moderation is challenging and requires extensive training for human moderators. Online deliberation, however, can leverage automated processes that combine group intelligence and artificial intelligence to replace human moderators Gelauff et al. (2023). This not only reduces costs but also ensures consistent and scalable facilitation.\nFacilitating online deliberation presents both opportunities for data analysis and the responsibility to ensure the process aligns with its objectives. Key factors to consider include equitable participation, conversation quality, participant experience, and outcome quality. The online setting simplifies data collection and makes evaluation more affordable. As the scale of deliberation increases, rapid evaluation becomes crucial for organizers to adjust recruitment, materials, or process design promptly. In this paper, we design a scalable method for quantifying deliberative quality and show how it can assist platform designers in evaluating and improving moderation interventions and configurations.\nDifferent aspects of quality can be established by analyzing participant surveys, third-party observations, or the content of the deliberation. Especially the content (transcripts) of the deliberation provide a promising avenue to evaluate interventions and moderation decisions that affect the conversation locally. Traditionally, content analysis relies on human annotators to categorize or score contributions a resource-intensive method. The arrival of LLMs makes it feasible to use automated analysis to estimate the quality of a contribution, exchange or entire deliberation. We define criteria of interest, and design a query for the LLM to report scores for statements given the preceding contributions and other relevant context. We validate scores from these queries with human annotators.\nThis approach not only allows analysis at a much larger scale but also enables the use of such scores in real-time evaluation, information systems for human supervisors, and even moderation interventions. We illustrate one use of this approach by analysing the effect of sending targeted nudges to inactive participants on the quality of the conversation.\nRoadmap After discussing related work, we will introduce our data, platform, and methodology. We will explain how we use an LLM to estimate the quality of contributions, and how we evaluate and utilize these estimates. Next, we will compare the performance of our model to that of human evaluators. Finally, we will demonstrate how these quality estimations can be used to optimize the performance of the deliberation platform by comparing the quality of statements made after nudges to those made without nudges."}, {"title": "2 Related Work", "content": "Evaluating a deliberation or discourse has long been a question of interest, with the aspects of analysis evolving over time. Qualities often sought after include equality, rational reasoning and engagement Dutwin (2003). Black et al. (2014) identified various approaches to evaluating a deliberation. Besides indirect measurements (looking at antecedents and outcomes), they distinguish between micro-analytic and macro-analytic approaches. Micro-analytic approaches include content analysis (annotating units of conversation, usually based on a codebook by humans) and discourse analysis (qualitative analysis of the discourse). Macro-analytic approaches use a wider range of methods to gain insights into the overall deliberation event with commonly used self-assessments from participants, moderators and/or observers or case studies with an in-depth evaluation of some of the groups. For our purposes, content analysis is particularly relevant, and we will focus on that.\nThe criteria used in content analysis are not universally agreed upon and depend on what the evaluator is trying to understand. For example, Gerhards (1997) looked at the abortion debate in major German newspapers between 1970 and 1994, and evaluated representativeness, respect, level of justification and rationality. Steenbergen et al. (2003) looked at a parliamentary setting with (long, formal) spoken contributions. They evaluated them on a Discourse Quality Index with indicators of openness of participation, justification of demands, common good/empathy, respect (for the group, the topic and arguments) and constructiveness. In line with this work, Gold et al. (2015) defined four indicators and combined the content analysis with a visualization, defining participation (distribution of contributions), respect (frequency of interruptions), argumentation (looking at sentence structure) and persuasiveness (as an indirect metric)."}, {"title": "3 Data", "content": "The data we used for this study was collected from deliberations on the Stanford Online Platform for Deliberation. This platform is designed to support large-scale online deliberations, with multiple small-group rooms deliberating in parallel on the same agenda, all without the need for a human moderator. The platform mimics the setup of an in-person deliberative poll. Participants are assigned to rooms, each of which is part of a nested structure:\n\u2022 Room: a small group of typically 5\u201312 participants who discuss with each other.\n\u2022 Roomgroup: a set of rooms deliberating in parallel, using an identical configuration of the deliberation platform, with an identical agenda.\n\u2022 Session: Roomgroups that discuss the same agenda, typically with nearly identical configurations, but not necessarily at the same time. This could, for example, facilitate different availability, privacy preferences or languages.\n\u2022 Deliberation event: a series of sessions where the same group of participants discusses a set of agendas. A deliberation event typically consists of 1\u20135 sessions.\nWhile the platform supports various configurations and exceptions, we will describe a typical deliberation setup. The organizer of the deliberative event recruits a representative sample of their population of interest through polling firms and shares background materials. The agenda has 3-6 agenda items, each with a balanced set of arguments in favor and against the proposals. Participants often receive these materials before the event. Participants are then invited with details on the date and time of each session, along with a link to the landing page for their assigned room group. An admin team is available to assist participants with technical issues, supervise discussions, and intervene in case of technical or behavioral problems.\nThe participants are in advance, at the starting time or on arrival (randomly) distributed across rooms, where the software considers a minimum size and target size for each room. When the first small-group session begins, participants in each room are greeted with a video message and a reintroduction to the conversation topic. They watch an instructional video demonstrating the platform's buttons and functions. Participants then introduce themselves, appearing in small video screens with the speaker's video enlarged. Speaker turns are managed by an automated moderator using a visible queue that participants can join by requesting to speak. Each participant can speak for up to 45 seconds, after which the next person in the queue takes their turn.\nEach agenda item is introduced with an audio or video prompt, accompanied by a description and key arguments for and against the proposals, visible on the screen. Participants can propose to move to the next agenda item if they feel the topic has been sufficiently discussed. If a majority agrees, or if time runs out, the automated moderator introduces the next topic and presents the new arguments.\nOnce all agenda items have been discussed, the room enters the question development phase. Participants propose questions about the discussed topics and rank them by importance. The group then discusses the top-ranked questions, and the original proposer can edit their question. After discussing all questions or when time expires, the group votes on two questions to send to the panel of experts in the following plenary session. Participants are then redirected to the plenary session, usually after a short break, where a panel of experts answers their questions. After this, participants may be asked to join a second session, where they might be randomly reassigned to different groups."}, {"title": "4 Methodology", "content": "We design a prompting scheme for using LLMs to evaluate the quality of statements made in online video-conferencing-based deliberations. We then conduct experiments with human annotators to validate the usefulness of the LLM evaluations. Using these quality evaluations, we study the effect of nudges on our deliberation platform on the quality of the statements made. We use the current state-of-the-art GPT-4 model from OpenAI Achiam et al. (2023), which we will refer to as \"the model\"."}, {"title": "4.1 Estimating the Quality of a Statement", "content": "There are many possible definitions for the \"quality\" of a contribution in the context of a deliberation, as discussed through the various examples in \u00a72. In our study, we build on the goal of a deliberative poll, which is to provide participants with optimal conditions to form an informed opinion on the topic at hand Fishkin (2009).\nWe based three of our criteria on the questions and indicators proposed by Mason and Newman Mason (1992); Newman (1995): whether the contribution provides justification, is novel, expands the conversation. We added a fourth criterium whether the contribution allows further expansion because we're interested in how contributions in a deliberative conversation fit together. Specifically, we ask the annotators to score the statement on following criteria prompts:\nQ1 This statement includes examples or anecdotes to support the speaker's point.\nQ2 This statement introduces novel ideas, perspectives, or solutions.\nQ3 This statement builds on top of the previous statements and the proposal.\nQ4 This statement raises points which will likely improve the quality of the following discussion.\nIn the rest of the paper, we will refer to these prompts as Q1, Q2, Q3, and Q4 for brevity. Each time, we provided the annotator with the contribution, the topic of the current agenda item, and all preceding statements within the same agenda item. We then asked the annotator to rate their agreement with Q1\u20134 on a 5-point Likert scale (Strongly Disagree \u2013 Strongly Agree) and provide a justification for each criterion. The exact query given to the model is as follows:"}, {"title": "Query for Estimating the Quality of a Statement", "content": "System Instructions: Your expertise is required to help a research study to evaluate the quality of statements made in a deliberation on a topic of social interest.\nTask: You are observing a live deliberation on the following proposals <topic>.\nThe input is from a noisy speech-to-text system. Your task is to evaluate a statement in the context of the ongoing discussion. Specifically, you have to rate it on the standard Likert scale on 1 to 5 on whether: <criterium statement>.\nThis is what the rating on the standard Likert scale means. 1: Strongly Disagree. 2: Disagree. 3: Undecided. 4: Agree. 5: Strongly Agree.\nPlease give a succinct justification in one short sentence. Format your answer as follows:\nRating: x/5. Justification: [One short sentence].\nHere is the transcript of the deliberation from before the statement which is to be evaluated. This serves as the context for the evaluation. <previous contributions>.\nHere is the statement which you have to evaluate. <contribution>.\nThe system instructions aim to improve the quality of responses by assigning a role Wu et al. (2023). The query was refined by experimenting with the model's evaluations of ten randomly chosen contributions. This process helped us develop a query that enables the model to consistently distinguish between good and bad statements and provide meaningful justifications for its ratings."}, {"title": "4.2 Evaluating Automated Quality Estimations", "content": "In order to evaluate the quality estimates created by our quality model, we need to compare these estimates with a baseline. If we were presented with the challenge to estimate the quality of a contribution without access to language models, we would ask one or some research assistants (often familiar with the deliberation procedures to score them: interns, undergraduate and graduate students) to score the contributions, and if necessary take the average of those scores. For an evaluation, we want to compare the score of a human estimate with the model estimate."}, {"title": "4.3 Nudging to Encourage Participation", "content": "The platform implements nudges as pop-up overlays on the user's screen. The pop-up includes a phrase encouraging the user to contribute to the conversation or asking whether the group is ready to move on to the next agenda item. In this paper, we will focus on the nudges that encourage users to participate (a \"Speak\" nudge) when they have not participated for a period of three minutes. The textual content of the nudge is randomly drawn from a list of possible nudges. This list varies between events and can be broadly categorized in a general nudge (e.g. \"What would someone who disagrees with you say about this discussion\u201d), a personalized nudge that addresses the user by their screen name (e.g. \"XYZ, you haven't spoken in a while, is there something you would like to share with the group?\") or a procon nudge that explicitly refers to the listed argument in the agenda item that is estimated to have been discussed least. Since 2022, the platform has also included dummy nudges for research purposes: the platform would randomly \"skip\" a speak nudge altogether (typically 15%). The platform also emits Speak-Room nudges to all participants when no one has spoken for over 30 seconds. We do not study the effect of Speak-Room nudges in this work."}, {"title": "5 Evaluating Automated Quality Estimation", "content": "We will first establish the level of consensus between the human annotators (\u00a7 4.2), and then use this as a benchmark for the model's performance."}, {"title": "5.1 Inter-Rater Agreement of Human Annotators", "content": "Inter-rater reliability (IRR) or inter-rater agreement measures the consensus among different annotators of a dataset. A common statistic for studying IRR is within-group reliability ($r_{wg}$) James et al. (1993). While there are multiple variants of $r_{wg}$ O'Neill (2017), we report $r^{*}_{wg}$ as it is best suited for our setting where multiple annotators report Likert scale scores for many statements.\n$r^{*}_{wg} = 1 - \\frac{S^2}{\\sigma^2_{eu}}$\nHigher $r^{*}_{wg}$ indicates better IRR. Here $S^2$ is the mean of the observed variance in annotators' ratings on each item, and $\\sigma^2_{eu}$ is the variance of the rectangular, uniform null distribution, $(m^2 \u2013 1)/12$, where m is the number of discrete Likert-type response options. Here m = 5.\nThe $r_{wg}$ values for the human annotators are as follows: Q1: 0.519; Q2: 0.449; Q3: 0.605, and Q4: 0.496. Although interpreting these values can vary by field, we follow the standards recommended by LeBreton and Senter (2008): 0\u20130.30 (lack of agreement), 0.31\u20130.50 (weak agreement), 0.51\u20130.70 (moderate agreement), 0.71\u20130.90 (strong agreement), and 0.91-1.0 (very strong agreement). The human annotators show weak agreement on novelty (Q2) and their potential for further expansion (Q4). They have moderate agreement on whether contributions justify their claims (Q1) and expand the conversation (Q3)."}, {"title": "5.2 Ratings from Humans and the Model", "content": "We report the humans' and model's average scores across the 30 statements in Table 2. We obtained a 95% confidence interval for the difference of the means by taking 10,000 bootstrap resamples of the original data and calculating the mean difference for each resample. We observe that the model consistently gives lower scores than humans on Q1 and Q2 and slightly higher scores on Questions 3 and 4. Addressing this bias \u2013 whether through prompt adjustments, fine-tuning, or mean-correcting the model's scores \u2013 is an important direction for future research. In this study, we use the raw scores. We suggest a mean-correction approach in the full version of the paper, which shows that the model's performance can be enhanced via this simple correction.\nThe bias in the model's scores is more or less consistent across the genders of participants, as in Table 3. However, since our data is over only 15 statements for men and women each, further investigation is required to ensure that the model is not biased.\nA common approach to improve noisy ratings is to take the average of multiple individual ratings. We will compare the model's rating against ratings of individual annotators, as well as pair and triplet ratings (defined by the average of its members' ratings). The average rating of the remaining human annotators serves as the \u201cgolden\" rating. Both the model and the human groups are evaluated on how closely their ratings match this golden rating2.\nWe determine for each possible individual, pair and triplet whether the model or humans are closer to the golden score for each of the 30 contributions (there are 8-choose-g groups, where g is the group size). We report in Fig. 1 the fraction of data points where the model outperforms the groups of human annotators. Groups of three humans always outperform the model for each question, while single humans never do. In groups of two, humans perform better on Q1, while the model outperforms humans on all other questions. We also aggregate these scores over the 30 questions, and report in Fig. 2 for how many groups the model rates more statements closer to the golden rating than the humans. The same observations apply as in Fig. 1, but the margins of victory are larger for both humans and the model. The takeaway from this experiment is that the model is a reasonable alternative to human annotators because three annotators would be too expensive for most deliberation events. A second approach to evaluate the performance of the model against human annotators, is to ask other annotators to evaluate the scores and their justifications (See \u00a7 4.2). We report the scores of both human and model ratings and justifications in Fig. 3 and the averages in Fig. 4. The evaluators are not informed about the source of the ratings and justification. The model outperforms the humans by a significant margin, as is evident from the 95% confidence intervals (obtained via bootstrapping by drawing 10,000 samples)."}, {"title": "5.3 Efficiency", "content": "The cost of annotating a 45-minute deliberation event with GPT-4 is under $4. As of writing, OpenAI charges $30 per million tokens input and $60 per million tokens output. A typical 45-minute deliberation event contains 30-50 contributions, and we make four queries for each contribution, where a typical query has between 200-1000 tokens of input and 20 tokens of output. An additional advantage of automated quality estimation is that it can be implemented in real-time (under 2s delays for a query), allowing for platform interventions to be informed by it."}, {"title": "5.4 Analysis of Deliberation Events", "content": "Since we have established that the model is a reliable annotator, we can use it at scale to inform platform design choices and estimate the average quality of all rooms in deliberation events. For example, in Fig. 5, we plot the average quality of statements in rooms (average over all filtered contributions) for Q1 (justification) and Q2 (novelty). El has a higher overall quality on both questions than both E2 and E3 (both using the same agenda design). The figure also shows a large variation between rooms in the same event. This analysis can prove useful when evaluating the event to identify rooms that had a particularly meaningful conversation (for qualitative analysis) or when evaluating the event as a whole."}, {"title": "6 Evaluating Nudges Via Quality Estimates", "content": "We can use the quality estimates to evaluate the effect of interventions on the quality of conversation. We will apply this to our speak nudges, which are emitted to a participant after three minutes of not requesting to speak (\u00a74.3). We will first establish the effectiveness of the nudges towards increasing participation. We use dataset E4 here (recall \u00a73). We have data from 27,899 nudges sent and 5,222 nudges skipped."}, {"title": "6.1 Effectiveness at Prompting to Speak", "content": "With the skipped nudges, we have a randomized controlled trial. We can calculate the probability that the participant requests to speak in the 30-second interval following the emission of a nudge (or a skipped nudge) and establish the effect of a nudge by taking the difference in means. Out of the 27,899 nudges, 2,929 (10.4%) resulted in a positive participation outcome, (a request to speak). Out of these, 2625 resulted in an actual contribution. The 5,222 instances where nudges were skipped were followed by 332 requests to speak (6.4%) in 30 seconds, which then led to 301 statements.\nThis indicates that speak nudges result in a 65.1% increase in the probability of a request to speak and a 63.2% increase in the probability of a statement. In Fig. 6 we report these probabilities broken down in 5-second intervals, confirming the intuition that the effect of a nudge is most prominent a little after emission.\nWe report the effectiveness of repeated nudges in the full version of the paper. The first nudge is the most effective (around 15%), with effectiveness dropping quickly. By the fourth nudge, the effectiveness is comparable to a skipped nudge. This shows that sending many nudges is unproductive and that fewer, strategically timed nudges is better."}, {"title": "6.2 Effect of Nudges on Contribution Quality", "content": "Now that we have established that nudges increase participation, we are interested in whether those statements are higher or lower quality. We study the effect of nudging on contribution quality in three ways:\nAnalysis 1. Compare the average quality of contributions following emitted and skipped nudges in dataset E4.\nAnalysis 2. Compare the average quality of contributions after nudges to all other contributions in E1, E2, and E3.\nAnalysis 3. Compare the average quality of contributions after nudges to all other contributions for individual participants in datasets E1, E2, and E3.\nWe say a contribution follows a nudge (or a skipped nudge) if the speaking request comes in the following 30 seconds.\nAnalysis 1. The average quality score for contributions following a nudge is not less than that for contributions following skipped nudges. These are higher by 0.05, 0.07, 0.03, and 0.03 on Q1, Q2, Q3, and Q4, respectively. See Fig. 7 for 95% confidence intervals of the scores. Recall that nudges are sent (or randomly skipped) to participants with lower activity. This result is an important point in favour of targeted nudges. While nudges increase engagement, they do so without decreasing the quality of contributions, thereby generating more ideas in the conversation.\nAnalysis 2. When compared with all other contributions, we observe that the average quality of nudged contributions is lower on justification (Q1) and novelty (Q2) and higher on constructiveness (Q3), and allowing expansion (Q4) (Fig. 8). This difference in average quality is -0.09, -0.07, 0.03, and 0.02 for Q1, Q2, Q3, and Q4, respectively. This is small compared to the standard deviation of the scores, which are 1.36, 0.96, 0.79, and 0.70 in the same order. This result does not imply causation since nudges are more likely to be sent to less active participants. We give further breakdowns of these results across events and genders in the full version.\nAnalysis 3. For each participant, we calculated the difference in the average quality of their nudged and other contributions. We then averaged this difference across all participants with at least one nudged and one non-nudged contribution. This is 607 participants over E1, E2, and E3. The results are shown in (Table 4). The small negative effect seen in Analysis 2 is now reduced to near zero!"}, {"title": "7 Conclusions and Future Work", "content": "In this work, we demonstrate that LLMs are valuable tools for rating deliberative contributions, providing consistent and scalable quality assessments that significantly reduce the resource demands associated with human annotations. This approach aids in evaluating platform interventions, such as targeted nudges, and overall deliberation quality.\nOne significant limitation is the unpredictability of LLMs Perkovi\u0107 et al. (2024). In our tests on 30 randomly selected statements across 5 trials, we observed low average variances of 0.13, 0.08, 0.02, and 0.01 in the model's ratings on Q1-Q4, respectively, indicating good consistency in the model's ratings. Ensuring this consistency across different contexts remains crucial. Additionally, using AI in this context raises issues of transparency and accountability. LLM outputs are highly sensitive to prompt design Sclar et al. (2023); Errica et al. (2024), and the rapid development of LLMs presents the risk that current queries may not work as expected with future models. Analysts must remain vigilant about potential biases and may need to adjust queries over time. Evaluating AI-generated justifications can aid explainability, but ongoing human quality control may still be necessary. While developing a self-hosted LLM could mitigate some risks, it would also introduce additional costs and complexity.\nFurther research includes improving the model through fine-tuning and mean correction to enhance the accuracy of quality assessments. Another key direction is studying the relationship between discussion quality and opinion change, measured through pre- and post-event surveys, to understand how discourse quality influences participant perspectives. Designing platform interventions based on real-time quality assessments is an exciting avenue for future work, potentially allowing dynamic adjustments to enhance the effectiveness of deliberative processes. Integrating automated assessments with other evaluation metrics could provide a more comprehensive understanding of deliberative quality."}]}