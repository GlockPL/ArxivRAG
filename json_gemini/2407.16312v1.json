{"title": "MOMALAND: A SET OF BENCHMARKS FOR MULTI-OBJECTIVE MULTI-AGENT REINFORCEMENT LEARNING", "authors": ["Florian Felten", "Umut Ucak", "Hicham Azmani", "Gao Peng", "Willem R\u00f6pke", "Hendrik Baier", "Patrick Mannion", "Diederik M. Roikers", "Jordan K. Terry", "El-Ghazali Talbi", "Gr\u00e9goire Danoy", "Ann Now\u00e9", "Roxana R\u0103dulescu"], "abstract": "Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is un-derscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMALAND, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMALAND addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMALAND also includes algorithms capable of learning policies in such settings.", "sections": [{"title": "1 Introduction", "content": "Often, domains of critical social relevance such as smart electrical grids [42], traffic systems [32], taxation policy design [88], or infrastructure management planning [40] involve controlling multiple agents while making compromises among several conflicting objectives. The multi-agent aspect of above-mentioned domains has been well studied, and there is a wealth of literature on multi-agent approaches spanning several decades [25]. Separately, over the last 20 years, there has been an increasing interest in the multi-objective aspect of such domains [29]. However, the intersection of these two aspects, multi-objective multi-agent decision making (MOMADM), has not received much attention. Indeed, problems are often simplified by hard-coding a trade-off among objectives or centralising all decisions in a single agent. We argue that many, if not most, complex problems of social relevance have both a multi-objective and a multi-agent dimension. This is because such problems often affect multiple stakeholders, who may care about different aspects of the outcome, and may have different preferences for them. As such, it is crucial to advance the field of MOMADM to enable future progress in the application of artificial intelligence (AI).\nThe development of standardised benchmarks is a key factor that has driven progress in various areas of AI over the years. Without standardised, publicly available benchmarks, researchers spend a lot of unnecessary time re-implementing test environments from published papers, reproducibility is made much more difficult, and results published in different papers are potentially incomparable [49, 19]. Suites of standardised benchmarks have helped to address these issues already in some fields of AI such as reinforcement learning (RL). Such benchmarks are exemplified by the seminal Gymnasium library [79] for single-objective single-agent RL, the PettingZoo library [75] for multi-agent RL (MARL), and MO-Gymnasium [3] for multi-objective RL (MORL). Yet, there is no existing library dedicated to multi-objective multi-agent reinforcement learning (MOMARL).\nIn this article, we introduce MOMALAND, the first publicly available set of MOMARL benchmarks under standardised APIs. MOMALAND, incorporated within the Farama Foundation ecosystem (see Figure 1), draws inspiration from analogous projects and currently offers over 10 configurable environments encompassing diverse MOMARL research settings. By embracing open-source principles and inviting contributions, we anticipate that MOMALAND will evolve in tandem with research trends and host new environments in the future. As we discuss further in Section 2, MOMALAND aims to contribute to the unification of rather fractured evaluation practices, in order to provide researchers with clear and objective data on how well their algorithms perform with respect to other methods.\nAdditionally, MOMALAND includes utilities and learning algorithms intended to establish baselines for future research in MOMARL. Notably, it offers utilities enabling the utilisation of existing MORL and MARL solving methods through centralisation or scalarisation strategies. Importantly, while the provided baselines can find solutions for certain MOMARL settings, MOMALAND also features challenges with no known solution concept. Addressing these challenges requires tackling open research questions before deriving appropriate solving methods. Having set this framework, we strongly encourage contributing new work in MOMARL to the MOMALAND baselines.\nThe remainder of this paper is organised as follows: Section 2 describes previous work in this area, Section 3 clarifies background information on the field of MOMARL, Section 4 illustrates the APIs exposed and utilities provided in"}, {"title": "2 Related Work", "content": "Unlike traditional machine learning settings that often rely on fixed datasets, RL problems typically do not, making replication of experimental results challenging [19, 49]. Indeed, although the Markov Decision Process (MDP) definitions are typically well-specified in research papers, their actual instantiation can be influenced by implementation decisions. Notably, even minor discrepancies in environment specifications can have a substantial impact on RL algorithms' performance. Moreover, re-implementing some of these environments, such as those based on the MuJoCo engine [78], from scratch would require a significant amount of effort for researchers.\nTo mitigate these issues and accelerate research in standard RL settings, Gymnasium [79] (formerly known as OpenAI Gym [13]) introduced a standard API and collection of versioned environments. With millions of downloads, this library has become the standard for RL research. Gymnasium allows researchers to evaluate the performance of their contributions on a varied collection of environments with few code changes, and ensures that the environments used for comparison against state-of-the-art algorithms are the same.\nHowever, Gymnasium is tailored for single-agent, single-objective MDPs, and does not offer support for more complex domains involving multiple agents or objectives. Hence, it has been extended in various ways, such as PettingZoo [75] or OpenSpiel [39] for MARL and MO-Gymnasium [3] for MORL. The Farama Foundation, a recently created nonprofit, takes care of maintaining most of these libraries up to high standards.\nDemonstrating the rising interest in settings involving multiple agents and objectives, some initial MOMARL bench-marks were proposed by Ajridi et al. [2] and Geng et al. [22]. Additionally, R\u00f6pke [69] introduced Ramo, a framework offering a collection of algorithms and utilities for solving multi-objective normal-form games which are a particular model studied in MOMARL. However, there is currently no widely adopted library providing reliable and maintained implementations of general MOMARL environments [33], and this is precisely the gap targeted by MOMALAND.\nFinally, over time, numerous RL learning libraries containing algorithms that adhere to standardised APIs have been released. For example, Stable-Baselines3 [56] and cleanRL [34] offer a collection of high-quality implementations of state-of-the-art algorithms. Libraries for MARL like EpyMARL [48], and for MORL such as MORL-Baselines [19] are also available. Nonetheless, the recent development of MOMARL and the absence of standardised environments mean that only a limited number of methods (e.g., MO-MIX [33]) that can operate in these conditions have been developed, with no dedicated libraries for MOMARL yet. To address this, we also include utilities and baseline algorithms to provide initial solutions to some of the introduced environments."}, {"title": "3 Multi-Objective Multi-Agent Reinforcement Learning", "content": "In this section, a formal definition and notations of the MOMARL problem are first provided in Section 3.1. Then, solution concepts under different assumptions are discussed in Section 3.2. Following this, metrics for evaluating and contrasting solving methods in this area are presented in Section 3.3."}, {"title": "3.1 Formal Definition", "content": "The most general framework for modelling multi-objective multi-agent decision-making settings is the multi-objective partially observable stochastic game (MOPOSG). MOPOSGs extend Markov decision processes [52] to both multiple agents and multiple objectives, under the most general setting in which agents do not observe the full state of the environment [53].\nDefinition 1 (Multi-objective partially observable stochastic game). A multi-objective partially observable stochastic game is a tuple M = (S, A,T, R, \u03a9, \u039f), with n \u2265 2 agents and d \u2265 2 objectives, where:\n\u2022 S is the state space;\n\u2022 A = A\u2081 \u00d7 \u00b7\u00b7\u00b7 \u00d7 An is the set of joint actions, A\u00a1 is the action set of agent i;\n\u2022 T : S \u00d7 A \u2192 \u25b3(S) represents the probabilistic transition function;\n\u2022 R = R\u2081 \u00d7 \u2026 \u00d7 Rn are the reward functions, where R\u00a1 : S \u00d7 A \u00d7 S \u2192 \u211d\u1d48 is the vectorial reward function of agent i for each of the d objectives;"}, {"title": "3.2 Solution Concepts", "content": "The multi-objective decision-making literature [66, 29] discusses two distinct perspectives for defining solutions in multi-objective settings. We briefly discuss each perspective below, tailored to the multi-objective multi-agent setting."}, {"title": "Axiomatic approach", "content": "The axiomatic approach designates the Pareto set (PS) as the optimal solution set, under the minimal assumption that the utility function is a monotonically increasing function. Informally, Pareto dominance introduces a partial ordering over vectors, where one vector is preferred over another when it is at least equal on all objectives and strictly better on at least one. We define this formally in the following definition.\nDefinition 2 (Pareto dominance). Let v, v' \u2208 \u211d\u1d48. We say v Pareto dominates v', denoted v \u227b\u209a v', whenever\n\u2200j \u2208 {1, ...,d} : vj \u2265 v'; \u2227 \u2203j \u2208 {1, ..., d} : vj > vj.\nWhen only ensuring that\n\u2200j \u2208 {1, ...,d} : vj \u2265 vj,\nwe refer to this as weak Pareto dominance and denote this by v \u227f\u209a v'."}, {"title": "Team reward setting", "content": "When all agents must cooperate, they often share a team reward, i.e. v\u2081 = v\u2082 = . . . = v\u2099, denoted as v*. Given this shared reward, Pareto dominance can be straightforwardly applied. We define this below.\nDefinition 3 (Pareto dominance for team reward). In a team-reward setting, we say that a joint policy \u03c0 Pareto dominates another joint policy \u03c0' whenever v*\u03c0 \u227b\u209a v*\u03c0\u0384.\nWe subsequently define the set of all joint policies which are not Pareto dominated as the Pareto set.\nDefinition 4 (Pareto set for team reward). Let \u03a0 be a set of joint policies. The Pareto set P(\u03a0) in a team reward setting contains all joint policies that are pairwise undominated, i.e.\nP(\u03a0) = {\u03c0\u2208 \u03a0 | \u00ac\u2203\u03c0' \u2208 \u03a0 : \u03c5\u03c0\u0384 \u227b\u209a \u03c5\u03c0}."}, {"title": "Individual reward setting", "content": "While the Pareto set and Pareto front are natural solutions in cooperative settings, extending this to settings where each agent receives a different reward vector is non-trivial. Observe that the value function for joint policies, V\u03c0 = [v\u2081\u03c0, v\u2082\u03c0... v\u2099\u03c0], in multi-agent multi-objective agent settings, is a matrix where each row represents the payoff vector of a particular agent. A well-known solution concept extending Pareto dominance to this setting is the Pareto-Nash equilibrium [41] in which each player's value vector should be in the Pareto front induced by keeping the opponents' policies fixed.\nDefinition 5 (Pareto-Nash dominance). We say a joint policy \u03c0 Pareto-Nash dominates another joint policy \u03c0', denoted as V\u03c0 \u227b\u209a\u0274 V\u03c0', whenever\n\u2200i \u2208 {1, ..., n} : v\u1d62\u03c0 \u227f\u209a v\u1d62\u03c0' \u2227 \u2203i \u2208 {1, ..., n} : v\u1d62\u03c0 \u227b\u209a v\u1d62\u03c0'"}, {"title": "Utility-based approach", "content": "The utility-based approach advocates for exploiting any additional domain knowledge that might be available regarding the user's utility function. Such additional knowledge can lead to smaller optimal sets (e.g., if the utility function is known to be linear), or less time spent on exploring regions of the objective space that are not of interest to the user (e.g., when the user requires some minimum value for a certain objective). When no additional knowledge on the utility function is available, the utility-based approach falls back on the axiomatic approach.\nRoijers et al. [66] define two optimisation criteria in multi-objective decision-making when applying the utility function to the vector-valued outcomes. One can compute the expected value of the payoffs of a policy first and then apply the utility function, leading to the scalarised expected returns (SER) optimisation criterion:"}, {"title": "3.3 Evaluation of MOMARL algorithms", "content": "Because of the additional complexity in MOMARL compared to single-agent, single-objective RL, solution concepts vary, leading to different evaluation methods for MOMARL algorithms. In this section, we present commonly utilised evaluation methods in both MORL and MARL domains and examine their suitability for MOMARL settings. First, we outline performance indicators for settings where the agents' utilities are unknown, followed by a discussion on settings where the utilities are known."}, {"title": "3.3.1 Performance Indicators for Unknown Utility", "content": "First, it is important to acknowledge that, due to the scarcity of research in general settings, there are few, if any, established methods for evaluating the effectiveness of approaches that identify a Pareto-Nash set. Nevertheless, as discussed above, when the agents' utilities remain unknown and under the team reward setting, the Pareto set and Pareto front (Definition 4) are usually designated as optimal solution sets. These solution concepts have been well studied in multi-objective optimisation literature, as well as in MORL more recently.\nCompared to single-objective RL, the assessment and comparison of PFs obtained by different algorithms pose challenges due to the PFs being collections of points, i.e. there is no existing ordering of PFs. Defining such an order is not straightforward for two main reasons. Firstly, Pareto fronts discovered by various algorithms can be intertwined, meaning that one algorithm may outperform another in a portion of the objective space while the opposite may hold true in another portion. Secondly, PFs in high dimensions present difficulty in visualisation. In practice, performance indicators become useful to transform a PF into a scalar value. This establishes an order among PFs and enables comparisons. Various types of performance indicators have been introduced in the MO literature for this purpose. However, it is important to note that compressing a set of points into a single scalar value inevitably introduces bias. Hence, multiple performance indicators (assessing different criteria) are often used in practice when comparing PFs. These criteria include convergence, which assesses how close to optimality the discovered policies are, and diversity, which evaluates the variety of compromises the discovered policies offer to the user.\nSimilar to solution concepts, performance indicators can be categorised into two groups: axiomatic indicators, which do not make any assumptions about the decision maker's (DM's) utility, and utility-based indicators, which assume specific restrictions on the DM's utility function, e.g. linearity. Several of these indicators, employed throughout this work, are given below."}, {"title": "Cardinality.", "content": "This metric is computed by considering the number of points within the approximated PF found by the algorithm (F). It offers insights into the diversity of F by indicating the number of trade-offs identified.\nC(F) = |F|."}, {"title": "Hypervolume.", "content": "This is a hybrid metric quantifying both a PF's convergence and diversity. Given an approximate PF, F, and a pessimistic reference point, zref, the hypervolume indicator represents the volume of the objective space starting from zref that is weakly dominated by F. Formally, the hypervolume metric [90] is defined as:\nHV(F, zref) = \u03bb(\u222av\u2208\ud835\udc39 Box(v, zref))\nv\u227fzref\nwhere \u03bb(\u00b7) is the Lebesgue measure and Box(v, zref) = {p \u2208 \u211d\u1d48 | v \u227f p \u227f zref} denotes the box delimited above by v \u2208 F and below by zref. The reference point used in the hypervolume computation is typically an estimate of the worst-possible value per objective."}, {"title": "Expected utility.", "content": "In the case where the utility function of the DM, u, is linear, it becomes feasible to represent the expected utility over a distribution of reward weights, W, using the expected utility (EU) metric [89]. The EU metric is then defined as:\nEU(F) = Ew\u223c\ud835\udc4a maxxv\u2217 \u00b7 w"}, {"title": "3.3.2 Performance Indicators for Known Utility", "content": "Another setting for evaluating MOMARL algorithms is when the utility of the agents is known a priori. This allows falling back to single-objective MARL solution concepts and using the indicators provided in this field. A few examples of how one can evaluate the performance of agents in this case, depending on the nature of the task at hand include learning curves depicting achieved individual or joint utility over the learning process; analysing the cooperation or coordination capacity of the learned policies [24]; using game theoretic concepts such as convergence to social optimum (i.e., outcome maximising population welfare), Nash equilibria [67], correlated equilibria [54], or cyclic equilibria [70])."}, {"title": "4 APIs and Utilities", "content": "MOMALAND extends both PettingZoo APIs by returning a vectorial reward (i.e., a NumPy [27] array) instead of a scalar for each agent.\nThe first API, referred to as parallel, enables all agents to act simultaneously, as demonstrated in Listing 1. In this mode, signals such as observations, rewards, terminations, truncations, and additional information are consolidated into dictionaries, mapping agent IDs to their respective signals (line 9). Similarly, all actions are provided simultaneously to the step function as a dictionary, mapping each agent's ID to its corresponding action (line 6).\nThe second API, termed agent-environment cycle (AEC), is suitable for turn-based scenarios, such as board games [75]. A typical usage of this API is depicted in Listing 2. In this setup, each loop provides information solely for the agent currently taking its turn (line 7). For additional notes on the APIs, we refer to the documentation website: https://momaland.farama.org/api/aec/.\nThese APIs enable modelling all our benchmarking environments and offer the advantage of aligning closely with PettingZoo's conventions, thus facilitating comprehension for MARL practitioners and reuse of existing utilities such as SuperSuit's wrappers [76]. Additionally, MOMALAND provides utilities to expose most environments through both APIs (with the exception of some board games, where support for the parallel API is deemed unnecessary)."}, {"title": "4.1 Utilities", "content": "In addition to environments and standard APIs, MOMALAND provides several utilities that help algorithm designers in creating and evaluating algorithms in the proposed environments.\nThe library offers wrappers that allow modifying one aspect of the environment, such as normalising observations. Importantly, MOMALAND environments are compatible with PettingZoo and SuperSuit wrappers, as long as they do not alter the reward vectors. This allows relying on stable implementations and avoiding code duplication. Nevertheless, MOMALAND provides wrappers dedicated to handling the vectorial rewards, as this is the main difference with PettingZoo. For instance, the NormaliseReward(idx, agent) wrapper facilitates the normalisation of the idxth immediate reward component for a specified agent. Furthermore, the LineariseReward wrapper enables the transformation of agent reward vectors into scalar values through a weighted sum of reward components, thereby converting multi-objective environments into single-objective ones under the standard PettingZoo API, see Figure 1. This adaptation allows for the utilisation of existing multi-agent RL algorithms to learn for a designated trade-off. Moreover, the CentraliseAgent wrapper compresses the multi-agent dimension into a single centralised agent, providing direct conversion to the MO-Gymnasium API [3]. This adaptation enables learning using multi-objective single-agent algorithms, such as those featured in MORL-Baselines [19].\nAdditionally, MOMALAND includes a set of baseline algorithms showing example usage of the API and previously discussed utilities. These baselines are discussed in more detail in Section 6."}, {"title": "5 Environments", "content": "MOMALAND provides a variety of environments which offer a diverse range of challenges to benchmark MOMARL algorithms. Table 1 shows an overview of all environments, according to the criteria depicted in Figure 2, which describe all multi-objective multi-agent settings. Our environments cover a spectrum of features, including discrete and continuous state and action spaces, stateless and stateful environments, cooperative and competitive settings, as well as fully and partially observable states. Notably, the current set of environments provided within MOMALAND covers all configurations depicted in Figure 2, except MOBG and MOCBG. Some environments are multi-objective extensions of PettingZoo domains, others have been implemented from the current literature in MOMARL, and some are introduced in this work, e.g. the CrazyRL variants. In the following, we briefly outline each environment; see Appendix B for more details."}, {"title": "Multi-Objective Beach Problem Domain (MO-BPD)", "content": "The Multi-Objective Beach Problem Domain (MO-BPD) [44] is a setting with two objectives, reflecting the enjoyment of tourists (agents) on their respective beach sections in terms of crowdedness and diversity of attendees. Each beach section is characterised by a capacity and each agent is characterised by a type. These properties, together with the location selected by the agents on the beach sections, determine the vectorial reward received by agents. The number of agents is configurable.\nThe MO-BPD domain has two reward modes: (i) individual reward, where each agent receives the reward signal associated with its respective beach section; and (ii) team reward, where the reward signal for each agent is an objective-wise sum over all the beach sections. In terms of mathematical frameworks, under the individual reward setting, the MO-BDP is a MOPOSG, while the team reward setting casts the problem as a MODec-POMDP."}, {"title": "MO-ItemGathering", "content": "The Multi-Objective Item Gathering domain, adapted from K\u00e4llstr\u00f6m and Heintz [37], is a multi-agent grid world, containing items of different colours. Each colour represents a different objective and the goal of the agents is to collect as many objects as possible. The environment is fully configurable in terms of grid size, number of agents, and number of objectives.\nMO-ItemGathering is fully observable and has two reward modes: individual rewards (MOSG), where agents are rewarded only for their own collected items, or team rewards (MOMMDP), where agents receive a reward for any object collected by the group."}, {"title": "MO-GemMining", "content": "In Multi-Objective Gem Mining, extending Gem Mining / Mining Day [7, 61, 65] to multiple objectives, a number of villages (agents) send workers to extract gems from different mines. Each gem type represents a different objective. There are restrictions on which mines can be reached from each village. Furthermore, workers influence each other in their productivity. The number of different gem types, villages, and workers per village are configurable.\nMO-GemMining is stateless; each action corresponds to one independent mining day. It is fully cooperative and can be modelled as a multi-objective multi-agent multi-armed bandit (MOMAMAB)."}, {"title": "MO-RouteChoice", "content": "MO-RouteChoice is a multi-objective extension of the route choice problem [77], where a number of self-interested drivers (agents) must navigate a road network. Each driver chooses a route from a source to a destination while minimising two objectives: travel time and monetary cost. Both objectives are affected by the selected routes of the other agents, as the more agents travel on the same path, the higher the associated travel time and monetary cost. The number of agents is configurable. The environment contains various road networks from the original route choice problem [58, 77], including the Braess's paradox [12] and networks inspired by real-world cities.\nMO-RouteChoice is a stateless environment, thus a MONFG, where each agent chooses one of the possible routes from its source to its destination and receives an individual reward based on the joint strategy of all agents."}, {"title": "MO-PistonBall", "content": "MO-PistonBall is based on an environment published in PettingZoo [75] where the goal is to move a ball to the edge of the window by operating several pistons (agents). This environment supports continuous observations and both discrete and continuous actions. In the original environment, the reward function is individual per piston and computed as a linear combination of three components. Concretely, the total reward consists of a global reward proportional to the distance to the wall, a local reward for any piston that is under the ball and a per-timestep penalty. In the MOMALAND adaptation, the environment dynamics are kept unchanged, but now each reward component is returned as an individual objective. The number of agents is configurable.\nThis environment is a MOPOSG, where the only stochastic transition dynamics occur when determining the initial state of the ball."}, {"title": "MO-MW-Stability", "content": "Multi-Objective Multi Walker Stability is another adaptation of a PettingZoo environment, originally published in Gupta et al. [26], to multi-objective settings. In this environment, multiple walker agents aim to carry a package to the right side of the screen without falling. This environment also supports continuous observations and actions. The multi-objective version of this environment includes an additional objective to keep the package as steady as possible while moving it. Naturally, achieving higher speed entails greater shaking of the package, resulting in conflicting objectives. The number of agents is configurable.\nThis environment is cooperative and agents only have a partial view of the global state. Hence, it is a MODec-POMDP."}, {"title": "CrazyRL", "content": "CrazyRL consists of 3 novel continuous 3D environments in which drones (agents) aim to surround a potentially moving target. The two objectives of the drones are to minimise their distance to the target while maximising the distance between each other. The 3 environments differ in the behaviour of the target, which can be static, move linearly, or actively try to escape the agents.\nThese environments are cooperative and agents can perceive the location of everyone else. Hence, they are all MOMMDPs."}, {"title": "MO-Breakthrough", "content": "MO-Breakthrough is a multi-objective variant of the two-player, single-objective turn-based board game Breakthrough. In MO-Breakthrough there are still two agents, but up to three objectives in addition to winning: a second objective that incentivizes faster wins, a third one for capturing opponent pieces, and a fourth one for avoiding the capture of the agent's own pieces. The board size is configurable as well.\nAs the game is competitive and fully observable, MO-Breakthrough falls into the category of MOSGs."}, {"title": "MO-Connect4", "content": "MO-Connect4 is a multi-objective variant of the two-player, single-objective turn-based board game Connect 4. In addition to winning, MO-Connect4 extends this game with a second objective that incentivizes faster wins, and optionally one additional objective for each column of the board that incentivizes having more tokens than the opponent in that column. As the board size is configurable, so is the number of these objectives.\nMO-Connect4 is competitive and fully observable and therefore a MOSG."}, {"title": "MO-Ingenious", "content": "MO-Ingenious is a multi-objective adaptation of the zero-sum, turn-based board game Ingenious. The game's original rules support 2-4 players collecting scores in multiple colours (objectives), with the goal of winning by maximising the minimum score over all colours. In MO-Ingenious, we leave the utility wrapper up to the users and only return the vector of scores in each colour objective. The number of agents, objectives, and board size in MO-Ingenious are configurable.\nMO-Ingenious has two reward modes: (i) individual reward, where each agent receives scores only for their own actions; and (ii) team reward, where all collected scores are shared by all agents. Furthermore, it can be played with (i) partial observability as the original game, or in a (ii) fully observable mode. In terms of mathematical frameworks, this environment is therefore a MOPOSG, which can be configured to become a MODec-POMDP when playing in team reward mode, a MOSG when playing in fully observable mode, or a MOMMDP when using both."}, {"title": "MO-SameGame", "content": "MO-SameGame is a multi-objective, multi-agent variant of the single-player, single-objective turn-based puzzle game called SameGame. All legal moves in the game remove a group of tokens of the same colour from the board. The original game rewards the player for each action with a number of points that is quadratic in the size of the removed group. MO-SameGame extends this to a configurable number of agents, acting alternatingly, and a configurable number of different types of colours (objectives) to be collected.\nMO-SameGame has two reward modes: (i) individual reward, where each agent receives points only for their own actions; and (ii) team reward, where all collected points are shared by all agents. It is fully observable and can therefore be modelled as a MOSG in individual reward mode, or a MOMMDP when using team rewards."}, {"title": "6 Baselines", "content": "After introducing our collection of challenging environments and utilities, this section demonstrates typical learning results derived from the solution concepts and metrics presented earlier. We provide baselines that allow learning under different settings. These baselines are listed in Table 2. The second column describes whether the algorithm aims at learning one or multiple policies associated with different trade-offs within the multi-objective dimension. The third and fourth columns refer to the classification made in the work of R\u0103dulescu et al. [53] and discussed in Section 3.2. It is worth noting that these algorithms do not aim for maximum efficiency, but provide a solid foundation for future work. The rest of this section illustrates results obtained by using the algorithms on some of the proposed environments."}, {"title": "6.1 Team Reward with Unknown Team Utility", "content": "As explained earlier, this setting aims at finding the same solution concepts as single-agent multi-objective RL, i.e., a Pareto set of policies and its linked Pareto front."}, {"title": "6.1.1 Solving MOMARL Problems Using Decomposition", "content": "Algorithm 1 describes a simple extension of the MAPPO algorithm [87] to return a Pareto set of multi-agent policies in cooperative problems. Similar to the works of Felten et al. [18, 20], it employs the decomposition technique to divide the multi-objective problem into a collection of single-objective problems which can then be solved by a multi-agent RL algorithm. In this context, a scalarisation function, parameterised by weight vectors, allows performing the decomposition and targeting various areas of the objective space. The most common scalarisation function, weighted sum, is used in this algorithm for its simplicity (through our LineariseReward wrapper, line 6). Notice that the rewards of the environment are first normalised to mitigate the difference in scale of each objective (line 5). The weight vectors can be generated using various techniques from MORL and MO optimisation, e.g., optimistic linear support (OLS) [62], GPI-LS [4], uniformly [15, 9], or randomly (line 4). After training a multi-agent policy for a given trade-off using MAPPO [87], the policy is evaluated on the original environment, allowing to compute an estimate of v* (line 8) and add the policy to the Pareto set of policies if it is non-dominated (line 9). Finally, the algorithm returns all non-dominated multi-agent policies (line 11).\nFigure 5 illustrates the typical metrics results that can be obtained by running MOMAPPO (Algorithm 1) on a cooperative environment, mo-multiwalker-stability-v0 in this case. For these runs, the algorithm generated 20 weight vectors uniformly to explore the objective space, more details on experimental settings are available in Appendix A. The performance indicators plotted have been averaged and the 95% confidence interval is represented by the shaded area. These reflect the general performance of the algorithm over random seeds ranging between 0 and 9 included. Moreover, the PF plot gives an idea of the final result for a given run. The reference point used for hypervolume calculation is [-300, -300]."}, {"title": "6.1.2 Solving MOMARL Problems Using Centralisation", "content": "As mentioned in Section 4.1, MOMALAND also provides a CentraliseAgent wrapper that turns a multi-agent multi-objective environment into a single-agent multi-objective environment by providing a centralised observation as well as a single vectorial reward signal. The composition method of the vectorial reward is determined by a parameter and can be either a component-wise sum or average of the individual agent rewards. This allows the direct application of methods featured in MORL-Baselines [19].\nTo illustrate the compatibility between MOMALAND environments using the CentraliseAgent wrapper and MORL-Baselines, we select two approaches, that make different assumptions regarding the environment or utility characteristics. Pareto Conditioned Networks (PCN) [59] is a multi-policy approach designed for deterministic environments. PCN will return an approximate Pareto front as a solution. On the other hand, Generalised Policy Improvement Linear Support (GPI-LS) [4] assumes the utility function is linear and will thus return the convex hull as a solution [29].\nWe present in Figure 6 the results obtained by GPI-LS and PCN on the moitem_gathering_v0 environment. The experiments are run on the default map of the environment, namely an 8 \u00d7 8 grid, with 2 agents and 3 different object types (i.e., 3 objectives). The centralised vectorial reward signal is obtained using a component-wise addition over all agents' rewards. The number of timesteps is set to 50 and the results are averaged over 5 runs (random seeds ranging from 40 to 44), with the shaded area representing the 95% confidence interval. The reference point for the hypervolume calculation is [0, 0, 0]. More experimental details are available in Appendix A.\nWe observe that for this instance of the MO-ItemGathering environment, both PCN and GPI-LS show consistent learning behaviour over the runs, reaching similar performance in terms of hypervolume and expected utility. In terms of cardinality (i.e., number of solutions in the identified solution set), PCN manages to identify on average one additional solution, in comparison to GPI-LS."}, {"title": "6.2 Individual Reward and Known Utility Function", "content": "Here we consider the setting of independent learners, and known linear utility functions, reducing the problem to independent multi-agent RL. To demonstrate this setup, we run experiments on two MOMALAND environments, namelymobeach_v0 and moroute_choice_v0. The considered learning approach is scalarised independent Q-learning (IQL) [44], since we investigate congestion domains that are fairly simple in terms of state and actions spaces, but that involve a large number of agents (i.e., 50, 100 and 4200 agents)."}, {"title": "7 Open Challenges", "content": "In this section, we highlight some of the key challenges for future research on MOMARL."}, {"title": "7.1 Solution Concepts for MOMARL", "content": "In Section 3.2 we briefly outlined some possible solution concepts for MOMARL, focusing on the two main approaches in the literature: the axiomatic approach and the utility-based approach. To date, the utility-based approach has generally been the most common approach for MOMARL problems, as it allows for prior knowledge about the agents' preferences over objectives to be incorporated to simplify the problem.\nWhen following the utility-based approach, solution concepts from traditional single-objective game theory can be extended to multi-objective settings by measuring agent incentives with respect to individual utility (rather than with respect to individual rewards/payoffs in single-objective game theory). For example, R\u0103dulescu et al. [54] extended the well-known Nash equilibrium and correlated equilibrium solution concepts to MOMA settings using the utility-based perspective. Much of the analysis to date on solution concepts has focused on stateless single-shot settings (MONFGs), so further empirical studies are required in sequential settings. Extending existing solution concepts to MOMA settings is not trivial when following the utility-based approach, as one must consider the effect of the choice of optimisation criterion, either SER or ESR, as outlined in Section 3.2. The choice of the correct optimisation criterion is crucial when the utility functions are non-linear; selecting SER in place of ESR (or vice versa) can drastically alter the collective behaviour of the agents. For example, it has been demonstrated that it may not be possible for agents to reach a stable outcome, e.g., Nash equilibria may not exist under SER [53] or stable coalitions may not exist in coalition formation games [36]. It is also possible to have a mixture of optimisation criteria within the same system, where some agents follow SER and others follow ESR [67]. Work on such settings has been extremely limited to date and therefore further work is required to better understand the implications of mixed optimisation criteria.\nResearch on the axiomatic approach to MOMA problems is even less mature than the utility-based approach. The axiomatic approach may be a suitable fallback in settings where no information is available about the agents' utilities, although the space of joint policies that could be optimal is potentially much larger when no information is available about the utilities. As shown in Section 6.1, applying the axiomatic approach in team reward settings, where all agents receive the same reward vectors, is relatively straightforward and the problem is fully cooperative as all agent incentives are perfectly aligned. The Pareto optimal set in team reward settings simply includes all joint policies where the return vector is non-dominated. For individual reward settings (e.g., adversarial or mixed settings), Pareto optimal sets could be defined individually for each agent, as a joint policy that is Pareto optimal with respect to one agent's reward function may not necessarily be Pareto optimal for other agents. Such individual Pareto optimal sets would need to be conditioned on the behaviour of other agents in the system, so would in effect be a set of non-dominated responses to the other agents' policies [53]. When policies are deterministic with a finite number of discrete actions, the non-dominated response set for an agent would also have a finite number of policies. In settings with probabilistic policies, the non-dominated response set could potentially have an infinite number of policies.\nFinally, the relationship between the axiomatic and utility-based approaches in MOMA systems is currently not well understood and merits further study. Initial work by Mannion and R\u0103dulescu [43] in a team reward individual utility setting demonstrated that it is possible to have settings where none of the Nash equilibria are Pareto optimal, depending on the preferences of agents over objectives."}, {"title": "7.2 Utility Modelling and Preference Elicitation", "content": "In single-agent settings, it is possible to elicit and align preferences with respect to different trade-offs between objectives by directly interacting with the users [51, 64]. This is because it is beneficial for both the agent and the user to share such preferences openly. In multi-agent team utility settings, this would still be the case.\nHowever, once we find ourselves in the individual utility case, the process becomes significantly harder. One may look at the problem from multiple perspectives: agents can interact and model the preferences of their users, however agents can now also potentially model their opponents' utility function, in order to gain an advantage in the strategic interactions. To the best of our knowledge, interactive MOMARL, where agents have to concurrently learn their associated user's preferences, as well as how to optimally act in the environment, has not yet been explored. Overcoming the difficulties posed by misalignment of preferences, as well as the fact that it might no longer be in the agents' best interest to share their preferences openly (on the contrary, it might even be better to actively hide this information) are still very much open challenges. Potential directions for approaching these challenges include negotiation [21, 5], or social contracts [31]."}, {"title": "7.3 Algorithms and Environments for MOMARL", "content": "Because it is a relatively new area, limited research has been focused on MOMARL. Moreover, although there is a wealth of problems documented in the literature that involve both multiple agents and objectives [86, 50], they are often simplified and not treated as MOMA. This prevents easy identification of contributions and comparison to the current state of the art in the field.\nConsequently, few solving methods addressing both dimensions of the problem exist. Indeed, most works operate in the known utility setting, effectively relying on or adapting MARL methods, e.g. Mannion et al. [44], R\u0103dulescu et al. [55]. A notable exception to this is MO-MIX [33], which is able to learn a Pareto set of multi-agent policies in the team reward setting. As previously stated, additional research is required in general settings to establish solution concepts and develop algorithms that can identify these."}, {"title": "8 Conclusion", "content": "In this paper, we introduced MOMALAND, the first publicly available benchmark suite for MOMARL problems. Our library includes a collection of over 10 environments under two different APIs for turn-based or simultaneous actions. These environments offer a diverse set of challenges, varying in the number of agents, state and action spaces, reward structures, and utility considerations. Notably, some of these challenges have no known solution concept.\nWe showed how to leverage existing literature from both multi-objective RL and multi-agent RL to construct new MOMARL algorithms able to solve some of the presented challenges. These baselines, along with useful utilities, are also made available to help algorithm designers in their future research endeavours.\nWhile the release of MOMALAND addresses one of the key challenges required to progress the field of MOMARL, many open challenges remain, as highlighted in Section 7. We hope this benchmark suite will be a valuable asset to the research community and that our work will inspire and enable future progress in the field."}]}