{"title": "MOMALAND: A SET OF BENCHMARKS FOR MULTI-OBJECTIVE MULTI-AGENT REINFORCEMENT LEARNING", "authors": ["Florian Felten", "Umut Ucak", "Hicham Azmani", "Gao Peng", "Willem R\u00f6pke", "Patrick Mannion", "Diederik M. Roikers", "Jordan K. Terry", "Gr\u00e9goire Danoy", "Ann Now\u00e9", "Hendrik Baier", "El-Ghazali Talbi", "Roxana R\u0103dulescu"], "abstract": "Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is un-derscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMALAND, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMALAND addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMALAND also includes algorithms capable of learning policies in such settings.", "sections": [{"title": "1 Introduction", "content": "Often, domains of critical social relevance such as smart electrical grids [42], traffic systems [32], taxation policy design [88], or infrastructure management planning [40] involve controlling multiple agents while making compromises among several conflicting objectives. The multi-agent aspect of above-mentioned domains has been well studied, and there is a wealth of literature on multi-agent approaches spanning several decades [25]. Separately, over the last 20 years, there has been an increasing interest in the multi-objective aspect of such domains [29]. However, the intersection of these two aspects, multi-objective multi-agent decision making (MOMADM), has not received much attention. Indeed, problems are often simplified by hard-coding a trade-off among objectives or centralising all decisions in a single agent. We argue that many, if not most, complex problems of social relevance have both a multi-objective and a multi-agent dimension. This is because such problems often affect multiple stakeholders, who may care about different aspects of the outcome, and may have different preferences for them. As such, it is crucial to advance the field of MOMADM to enable future progress in the application of artificial intelligence (AI).\nThe development of standardised benchmarks is a key factor that has driven progress in various areas of AI over the years. Without standardised, publicly available benchmarks, researchers spend a lot of unnecessary time re-implementing test environments from published papers, reproducibility is made much more difficult, and results published in different papers are potentially incomparable [49, 19]. Suites of standardised benchmarks have helped to address these issues already in some fields of AI such as reinforcement learning (RL). Such benchmarks are exemplified by the seminal Gymnasium library [79] for single-objective single-agent RL, the PettingZoo library [75] for multi-agent RL (MARL), and MO-Gymnasium [3] for multi-objective RL (MORL). Yet, there is no existing library dedicated to multi-objective multi-agent reinforcement learning (MOMARL).\nIn this article, we introduce MOMALAND, the first publicly available set of MOMARL benchmarks under standardised APIs. MOMALAND, incorporated within the Farama Foundation ecosystem (see Figure 1), draws inspiration from analogous projects and currently offers over 10 configurable environments encompassing diverse MOMARL research settings. By embracing open-source principles and inviting contributions, we anticipate that MOMALAND will evolve in tandem with research trends and host new environments in the future. As we discuss further in Section 2, MOMALAND aims to contribute to the unification of rather fractured evaluation practices, in order to provide researchers with clear and objective data on how well their algorithms perform with respect to other methods.\nAdditionally, MOMALAND includes utilities and learning algorithms intended to establish baselines for future research in MOMARL. Notably, it offers utilities enabling the utilisation of existing MORL and MARL solving methods through centralisation or scalarisation strategies. Importantly, while the provided baselines can find solutions for certain MOMARL settings, MOMALAND also features challenges with no known solution concept. Addressing these challenges requires tackling open research questions before deriving appropriate solving methods. Having set this framework, we strongly encourage contributing new work in MOMARL to the MOMALAND baselines.\nThe remainder of this paper is organised as follows: Section 2 describes previous work in this area, Section 3 clarifies background information on the field of MOMARL, Section 4 illustrates the APIs exposed and utilities provided in"}, {"title": "2 Related Work", "content": "Unlike traditional machine learning settings that often rely on fixed datasets, RL problems typically do not, making replication of experimental results challenging [19, 49]. Indeed, although the Markov Decision Process (MDP) definitions are typically well-specified in research papers, their actual instantiation can be influenced by implementation decisions. Notably, even minor discrepancies in environment specifications can have a substantial impact on RL algorithms' performance. Moreover, re-implementing some of these environments, such as those based on the MuJoCo engine [78], from scratch would require a significant amount of effort for researchers.\nTo mitigate these issues and accelerate research in standard RL settings, Gymnasium [79] (formerly known as OpenAI Gym [13]) introduced a standard API and collection of versioned environments. With millions of downloads, this library has become the standard for RL research. Gymnasium allows researchers to evaluate the performance of their contributions on a varied collection of environments with few code changes, and ensures that the environments used for comparison against state-of-the-art algorithms are the same.\nHowever, Gymnasium is tailored for single-agent, single-objective MDPs, and does not offer support for more complex domains involving multiple agents or objectives. Hence, it has been extended in various ways, such as PettingZoo [75] or OpenSpiel [39] for MARL and MO-Gymnasium [3] for MORL. The Farama Foundation, a recently created nonprofit, takes care of maintaining most of these libraries up to high standards.\nDemonstrating the rising interest in settings involving multiple agents and objectives, some initial MOMARL bench-marks were proposed by Ajridi et al. [2] and Geng et al. [22]. Additionally, R\u00f6pke [69] introduced Ramo, a framework offering a collection of algorithms and utilities for solving multi-objective normal-form games which are a particular model studied in MOMARL. However, there is currently no widely adopted library providing reliable and maintained implementations of general MOMARL environments [33], and this is precisely the gap targeted by MOMALAND.\nFinally, over time, numerous RL learning libraries containing algorithms that adhere to standardised APIs have been released. For example, Stable-Baselines3 [56] and cleanRL [34] offer a collection of high-quality implementations of state-of-the-art algorithms. Libraries for MARL like EpyMARL [48], and for MORL such as MORL-Baselines [19] are also available. Nonetheless, the recent development of MOMARL and the absence of standardised environments mean that only a limited number of methods (e.g., MO-MIX [33]) that can operate in these conditions have been developed, with no dedicated libraries for MOMARL yet. To address this, we also include utilities and baseline algorithms to provide initial solutions to some of the introduced environments."}, {"title": "3 Multi-Objective Multi-Agent Reinforcement Learning", "content": "In this section, a formal definition and notations of the MOMARL problem are first provided in Section 3.1. Then, solution concepts under different assumptions are discussed in Section 3.2. Following this, metrics for evaluating and contrasting solving methods in this area are presented in Section 3.3."}, {"title": "3.1 Formal Definition", "content": "The most general framework for modelling multi-objective multi-agent decision-making settings is the multi-objective partially observable stochastic game (MOPOSG). MOPOSGs extend Markov decision processes [52] to both multiple agents and multiple objectives, under the most general setting in which agents do not observe the full state of the environment [53].\nDefinition 1 (Multi-objective partially observable stochastic game). A multi-objective partially observable stochastic game is a tuple M = (S, A,T, R, \u03a9, \u039f), with n \u2265 2 agents and d \u2265 2 objectives, where:\n\u2022 S is the state space;\n\u2022 A = A\u2081 \u00d7 \u00d7 An is the set of joint actions, A\u00a1 is the action set of agent i;\n\u2022 T : S \u00d7 A \u2192 \u25b3(S) represents the probabilistic transition function;\n\u2022 R = R\u2081 \u00d7 \u2026 \u00d7 Rn are the reward functions, where R\u00a1 : S \u00d7 A \u00d7 S \u2192 Rd is the vectorial reward function of agent i for each of the d objectives;\n\u2022 \u03a9 = \u03a9\u2081 \u00d7 \u00b7\u00b7\u00b7 \u00d7 \u03a9n is the set of joint observations, Ni is the observation set of agent i;\n\u2022 \u039f: S \u00d7 \u0391 \u2192 \u0394(\u03a9) is the observation function, which maps each state \u2013 joint action pair to a probability distribution over the joint observation space.\nAfter every timestep, each agent receives an observation according to the observation function O, instead of directly observing the state. In this case, memory is required for agents to successfully learn in the environment [73]. A particular form of this memory occurs when agents consider the complete history of the current trajectory denoted as h\u2208H [28] (i.e., the complete trace of executed actions and received observations).\nBy making additional assumptions on the MOPOSG model, regarding observability, the structure of the reward function, or whether the problem is sequential or not, we can derive a subset of models such as the multi-objective stochastic game (MOSG), multi-objective decentralised partially observable Markov decision process (MODec-POMDP), multi-objective Bayesian game (MOBG), multi-objective cooperative Bayesian game (MOCBG), multi-objective multi-agent Markov decision process (MOMMDP), multi-objective normal form game (MONFG), or multi-objective multi-agent multi-armed bandit (MOMAMAB), as illustrated in Figure 2 [53].\nIn such settings, an agent behaves according to a policy \u03c0\u2081 : H \u00d7 A\u2081 \u2192 [0, 1], that provides a probabilistic mapping between an agent's history and its action set. In MOMARL, agents usually aim to optimise their individual expected discounted return obtained from a joint policy \u03c0. Formally,\n$v^{\\pi} = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} R_i \\left(s_{t}, a_{t}, s_{t+1}\\right) | \\pi \\right]$\nwhere \u03c0 = (\u03c01,..., \u03c0\u03b7) is the joint policy of the agents acting in the environment, \u03b3 is the discount factor and Ri(St, at, St+1) is the vectorial reward obtained by agent i for the joint action at \u2208 A at state st \u2208 S.\nNote that since an agent only directly controls its own policy \u03c0i, this introduces subtleties not present in single-agent settings, such as non-stationarity (stemming from agents simultaneously learning in the environment) and additional credit assignment challenges (i.e., identifying the individual contribution of agents to the resulting reward signal). Moreover, as a consequence of the fact that the value function is a vector, v \u2208 Rd, they only offer a partial ordering over the policy space. Determining the optimal policy requires additional information on how agents prioritise the objectives or what their preferences over the objectives are. We can capture such a trade-off choice using a utility function, uz: Rd \u2192 R, that maps the vector to a scalar value.\nIn the context of multi-objective multi-agent decision-making, R\u0103dulescu et al. [53] propose a taxonomy along the reward and utility axes. Namely, they propose to characterise settings in terms of individual or team rewards and"}, {"title": "3.2 Solution Concepts", "content": "The multi-objective decision-making literature [66, 29] discusses two distinct perspectives for defining solutions in multi-objective settings. We briefly discuss each perspective below, tailored to the multi-objective multi-agent setting.\nAxiomatic approach\nThe axiomatic approach designates the Pareto set (PS) as the optimal solution set, under the minimal assumption that the utility function is a monotonically increasing function. Informally, Pareto dominance introduces a partial ordering over vectors, where one vector is preferred over another when it is at least equal on all objectives and strictly better on at least one. We define this formally in the following definition.\nDefinition 2 (Pareto dominance). Let v, v' \u2208 Rd. We say v Pareto dominates v', denoted v >p v', whenever\n$\u2200j \u2208 {1, ...,d} : vj \u2265 v'; \u2227 \u2203j \u2208 {1, ..., d} : vj > v'j.$\nWhen only ensuring that\n$\u2200j \u2208 {1, ...,d} : vj \u2265 v'j,$\nwe refer to this as weak Pareto dominance and denote this by v \u2265p v'.\nTeam reward setting When all agents must cooperate, they often share a team reward, i.e. v = v = . . . = \u03c5, denoted as v", "v": "p \u03c5\u03c0\u0384.\nWe subsequently define the set of all joint policies which are not Pareto dominated as the Pareto set.\nDefinition 4 (Pareto set for team reward). Let II be a set of joint policies. The Pareto set P(II) in a team reward setting contains all joint policies that are pairwise undominated, i.e.\n$P(\\Pi) = {\\pi \\in \\Pi | \\nexists \\pi' \\in \\Pi : v_{\\pi'} >_p v_\\pi },$\nThe Pareto front (PF), denoted as F(P), contains the value vectors corresponding to all Pareto optimal policies \u03c0\u03b5\u03a1(\u03a0). It is usually presented to the decision-maker after the learning process to let them choose the desired behaviour to deploy. For instance, Figure 3 illustrates the outcomes of running a MOMARL algorithm (Algorithm 1) in team reward settings. This has been performed in one of our new environments where agents learn to make a formation around a fixed target. In the depiction, the yellow sphere represents the target, while the agents are depicted by the red spheres. This environment offers a clear demonstration of the various behaviours achievable by making different compromises among two objectives involving being close to the target and far from other agents. In this environment, the agents converge to a final position determined by the desired trade-off specified by the decision-maker. Opting for a tight formation around the target enhances the surrounding objective, albeit at the expense of greater collision risks. See Appendix B.6 for further details on the environment.\nIndividual reward setting While the Pareto set and Pareto front are natural solutions in cooperative settings, extending this to settings where each agent receives a different reward vector is non-trivial. Observe that the value function for joint policies, V\u00ab = [vv... v, in multi-agent multi-objective agent settings, is a matrix where each row represents the payoff vector of a particular agent. A well-known solution concept extending Pareto dominance to this setting is the Pareto-Nash equilibrium [41] in which each player's value vector should be in the Pareto front induced by keeping the opponents' policies fixed.\nDefinition 5 (Pareto-Nash dominance). We say a joint policy \u3160 Pareto-Nash dominates another joint policy \u03c0', denoted as V\u3160 >PN V\u3160', whenever\n$\u2200i \u2208 {1, ..., n} : v_i^{\\pi} \\gtrsim_p v_{i}^{\\pi'}\\wedge \\exists i \\in {1, ..., n} : v_i^{\\pi} \\succ_p v_i^{\\pi'},$"}, {"title": "3.3 Evaluation of MOMARL algorithms", "content": "Because of the additional complexity in MOMARL compared to single-agent, single-objective RL, solution concepts vary, leading to different evaluation methods for MOMARL algorithms. In this section, we present commonly utilised evaluation methods in both MORL and MARL domains and examine their suitability for MOMARL settings. First, we outline performance indicators for settings where the agents' utilities are unknown, followed by a discussion on settings where the utilities are known."}, {"title": "3.3.1 Performance Indicators for Unknown Utility", "content": "First, it is important to acknowledge that, due to the scarcity of research in general settings, there are few, if any, established methods for evaluating the effectiveness of approaches that identify a Pareto-Nash set. Nevertheless, as discussed above, when the agents' utilities remain unknown and under the team reward setting, the Pareto set and Pareto front (Definition 4) are usually designated as optimal solution sets. These solution concepts have been well studied in multi-objective optimisation literature, as well as in MORL more recently.\nCompared to single-objective RL, the assessment and comparison of PFs obtained by different algorithms pose challenges due to the PFs being collections of points, i.e. there is no existing ordering of PFs. Defining such an order is not straightforward for two main reasons. Firstly, Pareto fronts discovered by various algorithms can be intertwined, meaning that one algorithm may outperform another in a portion of the objective space while the opposite may hold true in another portion. Secondly, PFs in high dimensions present difficulty in visualisation. In practice, performance indicators become useful to transform a PF into a scalar value. This establishes an order among PFs and enables comparisons. Various types of performance indicators have been introduced in the MO literature for this purpose. However, it is important to note that compressing a set of points into a single scalar value inevitably introduces bias. Hence, multiple performance indicators (assessing different criteria) are often used in practice when comparing PFs. These criteria include convergence, which assesses how close to optimality the discovered policies are, and diversity, which evaluates the variety of compromises the discovered policies offer to the user.\nSimilar to solution concepts, performance indicators can be categorised into two groups: axiomatic indicators, which do not make any assumptions about the decision maker's (DM's) utility, and utility-based indicators, which assume specific restrictions on the DM's utility function, e.g. linearity. Several of these indicators, employed throughout this work, are given below."}, {"title": "Cardinality", "content": "This metric is computed by considering the number of points within the approximated PF found by the algorithm (F). It offers insights into the diversity of F by indicating the number of trade-offs identified.\n$C(F) = |F|.$"}, {"title": "Hypervolume", "content": "This is a hybrid metric quantifying both a PF's convergence and diversity. Given an approximate PF, F, and a pessimistic reference point, Zref, the hypervolume indicator represents the volume of the objective space starting from zref that is weakly dominated by F. Formally, the hypervolume metric [90] is defined as:\n$HV(F, z_{ref}) = \\Lambda \\left[ \\bigcup_{v \\in F} Box(v, z_{ref}) \\right]$\nwhere \u039b(\u00b7) is the Lebesgue measure and Box(v"}, {"content": "This is a hybrid metric quantifying both a PF's convergence and diversity. Given an approximate PF, F, and a pessimistic reference point, Zref, the hypervolume indicator represents the volume of the objective space starting from zref that is weakly dominated by F. Formally, the hypervolume metric [90] is defined as:\n$HV(F, z_{ref}) = \\Lambda \\left[ \\bigcup_{v \\in F} Box(v, z_{ref}) \\right]$\nwhere \u039b(\u00b7) is the Lebesgue measure and Box(v\u03c0, Zref) = {p \u2208 Rd | v\u03c0 \u227f p \u227fP Zref} denotes the box delimited above by v \u2208 F and below by Zref. The reference point used in the hypervolume computation is typically an estimate of the worst-possible value per objective."}, {"title": "Expected utility", "content": "In the case where the utility function of the DM, u, is linear, it becomes feasible to represent the expected utility over a distribution of reward weights, W, using the expected utility (EU) metric [89]. The EU metric is then defined as:\n$EU(F) = E_{w\\sim W} max_{v\\pi \\in F} v^{\\pi} \\cdot w .$"}, {"title": "3.3.2 Performance Indicators for Known Utility", "content": "Another setting for evaluating MOMARL algorithms is when the utility of the agents is known a priori. This allows falling back to single-objective MARL solution concepts and using the indicators provided in this field. A few examples of how one can evaluate the performance of agents in this case, depending on the nature of the task at hand include learning curves depicting achieved individual or joint utility over the learning process; analysing the cooperation or coordination capacity of the learned policies [24]; using game theoretic concepts such as convergence to social optimum (i.e., outcome maximising population welfare), Nash equilibria [67], correlated equilibria [54], or cyclic equilibria [70])."}, {"title": "4 APIs and Utilities", "content": "MOMALAND extends both PettingZoo APIs by returning a vectorial reward (i.e., a NumPy [27] array) instead of a scalar for each agent.\nThe first API, referred to as parallel, enables all agents to act simultaneously, as demonstrated in Listing 1. In this mode, signals such as observations, rewards, terminations, truncations, and additional information are consolidated into dictionaries, mapping agent IDs to their respective signals (line 9). Similarly, all actions are provided simultaneously to the step function as a dictionary, mapping each agent's ID to its corresponding action (line 6).\nThe second API, termed agent-environment cycle (AEC), is suitable for turn-based scenarios, such as board games [75]. A typical usage of this API is depicted in Listing 2. In this setup, each loop provides information solely for the agent currently taking its turn (line 7). For additional notes on the APIs, we refer to the documentation website: https://momaland.farama.org/api/aec/.\nThese APIs enable modelling all our benchmarking environments and offer the advantage of aligning closely with PettingZoo's conventions, thus facilitating comprehension for MARL practitioners and reuse of existing utilities such as SuperSuit's wrappers [76]. Additionally, MOMALAND provides utilities to expose most environments through both APIs (with the exception of some board games, where support for the parallel API is deemed unnecessary)."}, {"title": "4.1 Utilities", "content": "In addition to environments and standard APIs, MOMALAND provides several utilities that help algorithm designers in creating and evaluating algorithms in the proposed environments.\nThe library offers wrappers that allow modifying one aspect of the environment, such as normalising observations. Importantly, MOMALAND environments are compatible with PettingZoo and SuperSuit wrappers, as long as they do not alter the reward vectors. This allows relying on stable implementations and avoiding code duplication. Nevertheless, MOMALAND provides wrappers dedicated to handling the vectorial rewards, as this is the main difference with PettingZoo. For instance, the NormaliseReward(idx, agent) wrapper facilitates the normalisation of the idxth immediate reward component for a specified agent. Furthermore, the LineariseReward wrapper enables the transformation of agent reward vectors into scalar values through a weighted sum of reward components, thereby converting multi-objective environments into single-objective ones under the standard PettingZoo API, see Figure 1. This adaptation allows for the utilisation of existing multi-agent RL algorithms to learn for a designated trade-off. Moreover, the CentraliseAgent wrapper compresses the multi-agent dimension into a single centralised agent, providing direct conversion to the MO-Gymnasium API [3]. This adaptation enables learning using multi-objective single-agent algorithms, such as those featured in MORL-Baselines [19].\nAdditionally, MOMALAND includes a set of baseline algorithms showing example usage of the API and previously discussed utilities. These baselines are discussed in more detail in Section 6."}, {"title": "5 Environments", "content": "MOMALAND provides a variety of environments which offer a diverse range of challenges to benchmark MOMARL algorithms. Table 1 shows an overview of all environments, according to the criteria depicted in Figure 2, which describe all multi-objective multi-agent settings. Our environments cover a spectrum of features, including discrete and continuous state and action spaces, stateless and stateful environments, cooperative and competitive settings, as well as fully and partially observable states. Notably, the current set of environments provided within MOMALAND covers all configurations depicted in Figure 2, except MOBG and MOCBG. Some environments are multi-objective extensions of PettingZoo domains, others have been implemented from the current literature in MOMARL, and some are introduced in this work, e.g. the CrazyRL variants. In the following, we briefly outline each environment; see Appendix B for more details."}, {"title": "Multi-Objective Beach Problem Domain (MO-BPD)", "content": "The Multi-Objective Beach Problem Domain (MO-BPD) [44] is a setting with two objectives, reflecting the enjoyment of tourists (agents) on their respective beach sections in terms of crowdedness and diversity of attendees. Each beach section is characterised by a capacity and each agent is characterised by a type. These properties, together with the location selected by the agents on the beach sections, determine the vectorial reward received by agents. The number of agents is configurable.\nThe MO-BPD domain has two reward modes: (i) individual reward, where each agent receives the reward signal associated with its respective beach section; and (ii) team reward, where the reward signal for each agent is an objective-wise sum over all the beach sections. In terms of mathematical frameworks, under the individual reward setting, the MO-BDP is a MOPOSG, while the team reward setting casts the problem as a MODec-POMDP."}, {"title": "MO-ItemGathering", "content": "The Multi-Objective Item Gathering domain (Figure 4, rightmost picture), adapted from K\u00e4llstr\u00f6m and Heintz [37], is a multi-agent grid world, containing items of different colours. Each colour represents a different objective and the goal of the agents is to collect as many objects as possible. The environment is fully configurable in terms of grid size, number of agents, and number of objectives."}, {"title": "MO-GemMining", "content": "In Multi-Objective Gem Mining, extending Gem Mining / Mining Day [7, 61, 65] to multiple objectives, a number of villages (agents) send workers to extract gems from different mines. Each gem type represents a different objective. There are restrictions on which mines can be reached from each village. Furthermore, workers influence each other in their productivity. The number of different gem types, villages, and workers per village are configurable.\nMO-GemMining is stateless; each action corresponds to one independent mining day. It is fully cooperative and can be modelled as a multi-objective multi-agent multi-armed bandit (MOMAMAB)."}, {"title": "MO-RouteChoice", "content": "MO-RouteChoice is a multi-objective extension of the route choice problem [77], where a number of self-interested drivers (agents) must navigate a road network. Each driver chooses a route from a source to a destination while minimising two objectives: travel time and monetary cost. Both objectives are affected by the selected routes of the other agents, as the more agents travel on the same path, the higher the associated travel time and monetary cost. The number of agents is configurable. The environment contains various road networks from the original route choice problem [58, 77], including the Braess's paradox [12] and networks inspired by real-world cities.\nMO-RouteChoice is a stateless environment, thus a MONFG, where each agent chooses one of the possible routes from its source to its destination and receives an individual reward based on the joint strategy of all agents."}, {"title": "MO-PistonBall", "content": "MO-PistonBall is based on an environment published in PettingZoo [75] where the goal is to move a ball to the edge of the window by operating several pistons (agents). This environment supports continuous observations and both discrete and continuous actions. In the original environment, the reward function is individual per piston and computed as a linear combination of three components. Concretely, the total reward consists of a global reward proportional to the distance to the wall, a local reward for any piston that is under the ball and a per-timestep penalty. In the MOMALAND adaptation, the environment dynamics are kept unchanged, but now each reward component is returned as an individual objective. The number of agents is configurable.\nThis environment is a MOPOSG, where the only stochastic transition dynamics occur when determining the initial state of the ball."}, {"title": "MO-MW-Stability", "content": "Multi-Objective Multi Walker Stability (Figure 4, third picture from the left) is another adaptation of a PettingZoo environment, originally published in Gupta et al. [26], to multi-objective settings. In this environment, multiple walker agents aim to carry a package to the right side of the screen without falling. This environment also supports continuous observations and actions. The multi-objective version of this environment includes an additional objective to keep the package as steady as possible while moving it. Naturally, achieving higher speed entails greater shaking of the package, resulting in conflicting objectives. The number of agents is configurable.\nThis environment is cooperative and agents only have a partial view of the global state. Hence, it is a MODec-POMDP."}, {"title": "CrazyRL", "content": "CrazyRL (Figure 4, second picture from the left) consists of 3 novel continuous 3D environments in which drones (agents) aim to surround a potentially moving target. The two objectives of the drones are to minimise their distance to the target while maximising the distance between each other. The 3 environments differ in the behaviour of the target, which can be static, move linearly, or actively try to escape the agents.\nThese environments are cooperative and agents can perceive the location of everyone else. Hence, they are all MOMMDPs."}, {"title": "MO-Breakthrough", "content": "MO-Breakthrough is a multi-objective variant of the two-player, single-objective turn-based board game Breakthrough. In MO-Breakthrough there are still two agents, but up to three objectives in addition to winning: a second objective that incentivizes faster wins, a third one for capturing opponent pieces, and a fourth one for avoiding the capture of the agent's own pieces. The board size is configurable as well.\nAs the game is competitive and fully observable, MO-Breakthrough falls into the category of MOSGs."}, {"title": "MO-Connect4", "content": "MO-Connect4 is a multi-objective variant of the two-player, single-objective turn-based board game Connect 4 (Figure 4, leftmost picture). In addition to winning, MO-Connect4 extends this game with a second objective that incentivizes faster wins, and optionally one additional objective for each column of the board that incentivizes"}, {"title": "6 Baselines", "content": "After introducing our collection of challenging environments and utilities, this section demonstrates typical learning results derived from the solution concepts and metrics presented earlier. We provide baselines that allow learning under different settings. These baselines are listed in Table 2. The second column describes whether the algorithm aims at learning one or multiple policies associated with different trade-offs within the multi-objective dimension. The third and fourth columns refer to the classification made in the work of R\u0103dulescu et al. [53] and discussed in Section 3.2. It is worth noting that these algorithms do not aim for maximum efficiency, but provide a solid foundation for future work. The rest of this section illustrates results obtained by using the algorithms on some of the proposed environments."}, {"title": "6.1 Team Reward with Unknown Team Utility", "content": "As explained earlier, this setting aims at finding the same solution concepts as single-agent multi-objective RL, i.e., a Pareto set of policies and its linked Pareto front."}, {"title": "6.1.1 Solving MOMARL Problems Using Decomposition", "content": "Algorithm 1 describes a simple extension of the MAPPO algorithm [87] to return a Pareto set of multi-agent policies in cooperative problems. Similar to the works of Felten et al. [18, 20], it employs the decomposition technique to divide the multi-objective problem into a collection of single-objective problems which can then be solved by a multi-agent RL algorithm. In this context, a scalarisation function, parameterised by weight vectors, allows performing the decomposition and targeting various areas of the objective space. The most common scalarisation function, weighted sum, is used in this algorithm for its simplicity (through our LineariseReward wrapper, line 6). Notice that the rewards of the environment are first normalised to mitigate the difference in scale of each objective (line 5). The weight vectors can be generated using various techniques from MORL and MO optimisation, e.g., optimistic linear support (OLS) [62], GPI-LS [4], uniformly [15, 9], or randomly (line 4). After training a multi-agent policy for a given trade-off using MAPPO [87], the policy is evaluated on the original environment, allowing to compute an estimate of v\" (line 8) and add the policy to the Pareto set of policies if it is non-dominated (line 9). Finally, the algorithm returns all non-dominated multi-agent policies (line 11).\nFigure 5 illustrates the typical metrics results that can be obtained by running MOMAPPO (Algorithm 1) on a cooperative environment, mo-multiwalker-stability-v0 in this case. For these runs, the algorithm generated 20 weight vectors uniformly to explore the objective space, more details on experimental settings are available in Appendix A. The performance indicators plotted have been averaged and the 95% confidence interval is represented by the shaded area. These reflect the general performance of the algorithm over random seeds ranging between 0 and 9 included. Moreover, the PF plot gives an idea of the final result for a given run. The reference point used for hypervolume calculation is [-300, -300]."}, {"title": "6.1.2 Solving MOMARL Problems Using Centralisation", "content": "As mentioned in Section 4.1, MOMALAND also provides a CentraliseAgent wrapper that turns a multi-agent multi-objective environment into a single-agent multi-objective environment by providing a centralised observation as well as a single vectorial reward signal. The composition method of the vectorial reward is determined by a parameter and can be either a component-wise sum or average of the individual agent rewards. This allows the direct application of methods featured in MORL-Baselines [19", "59": "is a multi-policy approach designed for deterministic environments. PCN will return an approximate Pareto front as a solution. On the other hand, Generalised Policy Improvement Linear Support (GPI-LS) [4", "29": "."}]}