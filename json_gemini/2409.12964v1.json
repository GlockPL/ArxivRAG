{"title": "OpenRANet: Neuralized Spectrum Access by Joint Subcarrier and Power Allocation with Optimization-based Deep Learning", "authors": ["Siya Chen", "Chee Wei Tan", "Xiangping Zhai", "H. Vincent Poor"], "abstract": "The next-generation radio access network (RAN), known as Open RAN, is poised to feature an AI-native interface for wireless cellular networks, including emerging satellite-terrestrial systems, making deep learning integral to its operation. In this paper, we address the nonconvex optimization challenge of joint subcarrier and power allocation in Open RAN, with the objective of minimizing the total power consumption while ensuring users meet their transmission data rate requirements. We propose OpenRANet, an optimization-based deep learning model that integrates machine-learning techniques with iterative optimization algorithms. We start by transforming the original nonconvex problem into convex subproblems through decoupling, variable transformation, and relaxation techniques. These subproblems are then efficiently solved using iterative methods within the standard interference function framework, enabling the derivation of primal-dual solutions. These solutions integrate seamlessly as a convex optimization layer within OpenRANet, enhancing constraint adherence, solution accuracy, and computational efficiency by combining machine learning with convex analysis, as shown in numerical experiments. OpenRANet also serves as a foundation for designing resource-constrained AI-native", "sections": [{"title": "I. INTRODUCTION", "content": "The Open Radio Access Network (Open RAN) architecture, characterized by its open interfaces, decentralized network elements, virtualized hardware and software, and intelligent control, represents a transformative approach to enhancing network deployment, fostering innovation and reducing costs for next-generation wireless networks [1]\u2013[3]. Open RAN can also integrate satellite-terrestrial networks (e.g., SpaceX Starlink), significantly extending its reach and improving connectivity in remote and underserved areas [4]-[6], thereby enhancing overall network performance and user experience. Furthermore, Open RAN can leverage deep learning technologies to embed intelligence into every unit of the architecture [2], [3]. By employing deep learning techniques, Open RAN can achieve intelligent decision-making at various levels, leading to improved spectrum efficiency, reduced power consumption, dynamic resource allocation, optimized network coverage, and decreased operational costs. This integration of Open RAN, satellite-terrestrial networks, and deep learning enhances adaptability, efficiency, and scalability, making these systems well-suited to the demands of 6G and beyond [2], [3]. However, it is essential to assess the technology's impact on energy consumption to ensure that the benefits of Open RAN are not negated by increased energy usage.\nThe impact of Open RAN on network energy consumption is critical, influencing operational costs, environmental sustainability, and overall network performance [2], [3]. With its decentralized and virtualized network elements, Open RAN offers opportunities to optimize energy usage compared to traditional RAN systems. Implementing intelligent power management techniques, which dynamically adjust power levels of network components based on real-time traffic demands, is one approach to reducing energy consumption. In this paper, we focus on optimizing power allocation in Open RAN systems to minimize total power consumption while ensuring that each user meets the required transmission data rates, thereby guaranteeing that the total transmission data rate from all subcarriers for each user meets or exceeds a predefined threshold.\nExtensive research has been conducted on optimizing power usage in wireless networks [7]\u2013[14],"}, {"title": "II. SYSTEM MODEL", "content": "We consider a downlink multi-carrier open RAN system, comprising a single base station and L active users who share M subcarriers. In this scenario, the base station operates as the transmitter, dispatching signals to the receivers, represented by the L active users across L unique communication links. We denote the index set of users by $l \\in \\{1, ..., L\\}$, and the set of subcarriers by $m \\in \\{1, ..., M\\}$. We assume the total bandwidth is B and each subcarrier has a bandwidth of B/M. In addition, we assume that the number of active users can exceed the number of subcarriers and that multiple receivers may share a single subcarrier, potentially leading to interference between receivers. Each receiver is capable of utilizing the necessary number of subcarriers to satisfy the bit rate requirements of its particular application. Let $p_l^m$ denote the transmit power from the BS to l-th user on the m-th subcarrier. User l is said to be active on subcarrier m if $p_l^m > 0$, and inactive otherwise. Let $G_{lj}^m$ stands for the channel gain between the j-th and the l-th user through the m-th subcarrier, and $\\sigma_l^m$ is the additive white Gaussian noise. We assume that the channel gains are perfectly known. The system model is illustrated in Fig. 2. Treating interference as noise, the SINR of the l-th receiver on the m-th subcarrier can be expressed in terms of $p^m = (p_1^m, p_2^m,...,p_L^m)^T$ as [32]\n$SINR_l^m(p^m) = \\frac{G_{ll}^m p_l^m}{\\sum_{j \\neq l} G_{lj}^m p_j^m + \\sigma_l^m}.$ \n\nThe problem of minimizing the total power consumption subject to given transmission rate requirements with multiple subcarriers is formulated as follows:\n$\\text{minimize} \\sum_{l=1}^L \\sum_{m=1}^M p_l^m$"}, {"title": "III. REWEIGHTED PRIMAL-DUAL ALGORITHM FOR TOTAL POWER MINIMIZATION", "content": "In this section, we reformulate and decouple the total power consumption minimization problem in (2). We analyze the constraints of the reformulated problem, which can be interpreted as log-convex standard interference functions. Then we propose a primal-dual algorithm to obtain the local optimal solution to the problem in (2) by leveraging the properties of the log-convex standard interference functions.", "subsections": [{"title": "A. Reformulation of the Total Power Minimization", "content": "We first introduce the auxiliary rate variables $r_l^m = (r_1^m, r_2^m,...,r_L^m)$ for all m, and rewrite (2) as\n$\\text{minimize} \\sum_{l=1}^L \\sum_{m=1}^M p_l^m$\n$\\text{subject to } f_l^m(SINR_l^m(p^m)) \\geq r_l^m \\quad \\forall l, \\forall m,$\n$\\sum_{m=1}^M r_l^m \\geq r_l,$\n\n$\\text{variables: } p^m =(p_1^m,...,p_L^m)^T \\geq 0 \\quad \\forall m, \\quad r^m =(r_1^m,...,r_L^m) \\geq 0 \\quad \\forall m.$\n\nThe problem in (3) allows us to decouple the transmission rate constraints across the multiple subcarriers. Now let us suppose the problem in (3) or (2) is feasible. Denote the optimal transmission rate for each subcarrier by $r^{m*}$ and the optimal power allocation by $p^{m*}$. Then we have the following theorem.\nTheorem 1: If the problem in (3) is feasible, then the transmission rate constraints in (3) are tight when (3) reaches to its optima, which means\n$\\sum_{m=1}^M r_l^{m*} = \\sum_{m=1}^M f_l^m(SINR_l^m(p^{m*})) = r_l \\quad \\forall l.$"}, {"title": "B. Reweighted Primal-dual Algorithm for Total Power Minimization", "content": "Next, we give another reformulation to (3). By letting $\\bar{p}_l^m = \\log p_l^m$ and $\\bar{r}_l^m = \\log r_l^m$ for all l and m, and taking the logarithm on both sides of the individual rate constraints in (3), we then rewrite (3) as the following equivalent optimization problem:\n$\\text{minimize} \\sum_{l=1}^L \\sum_{m=1}^M e^{\\bar{p}_l^m}$ \n\n$\\text{subject to } \\log f_l^m (SINR_l^m (e^{\\bar{p}^m})) \\geq \\bar{r}_l^m \\quad \\forall l, \\forall m,$\n$\\sum_{m=1}^M e^{\\bar{r}_l^m} \\geq r_l, \\forall l,$"}]}, {"title": "IV. OPENRANET: AN OPTIMIZATION-BASED DEEP LEARNING MODEL WITH PROJECTION AND CONVEX OPTIMIZATION LAYERS FOR TOTAL POWER MINIMIZATION", "content": "In this section, we design a deep learning model with projection and optimization layers, which we called OpenRANet, to obtain an approximate solution to the non-convex problem in (7), and then (2). Since (7) is convex only when a feasible minimum transmission rate requirement of each user $r_l^m$ at each subcarrier is given, the deep learning model seeks to find the appropriate $r_l^m$. We divide our model into four parts: feature extraction with a convolutional filter, a number of dense layers, a projection layer, and a convex optimization layer [29]-[31]. We then describe the forward and backward propagation mechanism to tune the learning parameters of our proposed model.", "subsections": [{"title": "A. The OpenRANet for Total Power Minimization", "content": "In traditional neural network-based machine learning, we can theoretically fit a complex function with a trained neural network model consisting of linear fully-connected layers and nonlinear activation functions. This is possible if the neural network is given enough capacity and training time, according to the universal approximation theorem [35]. However, for complex optimization problems, particularly the challenging large-scaled non-convex optimization problems, relying solely on machine learning techniques may make it difficult to capture the whole underlying structure of the problems [24], [25]. As domain-knowledge techniques have great power in many scenarios, combining modern machine-learning approaches with traditional optimization-based approaches holds significant potential for enhancing effectiveness and efficiency in addressing the complexity challenges of nonconvex optimization [24], [25]. One way is to identify and extract the convex structure of the complex optimization problem, such as the hidden convex optimization subproblems within it. These convex subproblems can then be integrated into the deep learning model as optimization layers [27]-[31], resulting in a model that exhibits characteristics of the complex optimization problem. This leads to the deep learning model becoming easier to train, enhancing its generalization ability, and reducing the overall complexity of the model. Moreover, when dealing with constraints of optimization problems, it is important to incorporate the constraint information into the deep learning model to ensure feasibility. The above considerations make us cautious about relying solely on machine learning methods to solve (7). Therefore, we propose OpenRANet, which extensively incorporates the constraint information and the convex subproblem of (7) into the deep learning model, resulting in significantly improved performance.\nRecall from the previous section that when the individual transmission rate requirements $\\bar{r}_l^m$ are known, the non-convex problem in (7) becomes equivalent to the problem in (10). The objective now is to learn the individual transmission rate $r_l^m$ in (7). Instead of solely relying on machine learning techniques, we aim to incorporate (10) as part of the deep learning model. This means treating (10), which is convex, as a layer within the model to find the solution to the non-convex (7). Furthermore, to ensure that the data rate constraints in (7) are not violated, we first project the output $r_l^m$ onto the feasible domain of problem in (7) before incorporating (10) as an optimization layer into the deep learning model. Consequently, our proposed deep learning model, OpenRANet, whose architecture is shown in Fig. 2, is designed with the following components:"}, {"title": "\u2022 Feature extraction:", "content": "To extract features for a large-scale open RAN system with a high number of users, directly using each system parameter as input features for a deep learning model can result in the curse of dimensionality. To address this issue, convolutional filters can be employed to effectively extract the crucial features of the system as follows:\n$G(W_0) = Conv(W_0, G),$\n$h_0(W_0) = [G(W_0), \\sigma, \\bar{r}],$\nwhere Conv(\u00b7) represents the convolution operation for the channel gain G with parameter set $W_0$."}, {"title": "\u2022 Dense layers:", "content": "Here we introduce k fully connected layers that incorporates learning parameters $W = \\{W_0, . . ., W_k\\}$ in the model that can be trained using knowledge of data:\n$h_k(W) = F_k(\\cdot\\cdot\\cdotF_2(W_2(F_1(W_1[h_0(W_0)])))),$\nwhere $F_k(\\cdot)$ represents a composite function composed of linear combinations and activation functions such as the RELU function."}, {"title": "\u2022 Projection layer:", "content": "The purpose of this layer is to incorporate the rate constraint information into the OpenRANet, thereby ensuring that the output of the OpenRANet meets the rate requirement. This, in effect, guarantees the practicality of the model. The Projection layer can be designed as:\n$\\bar{r}(W) = \\log \\frac{e^{h_k(W)}diag(r)}{diag(e^{h_k(W)}1)}$"}, {"title": "\u2022 Convex optimization layer:", "content": "This layer integrates the convex optimization subproblem of the nonconvex (7) into the model. Note that the log-convexity property in the standard interference function can be used to efficiently solve this convex subproblem, as shown in Thereom 2. This integration saves significant costs compared to the purely data-driven deep learning model, which would otherwise need to learn all the underlying structures of the (7). Consequently, it effectively enhances the performance and efficiency of the model. The optimization layer is designed as:\n$p^*(\\bar{r}(W)) = \\Phi(\\bar{r}(W), G, \\sigma),$\nwhere $\\Phi(\\bar{r}(W), G, \\sigma)$ is the solution to the optimization problem\n$\\text{argmin}_p \\sum_{l=1}^L \\sum_{m=1}^M e^{\\bar{p}_l^m}$\n\n$\\text{subject to } \\log f_l^m (SINR_l^m (e^{\\bar{p}^m})) \\geq \\log\\bar{r}_l^m (W), \\forall l, m,$\n\\text{variables: } $\\bar{p}^m =( \\bar{p}_1^m,..., \\bar{p}_L^m) \\quad \\forall m.$\nObserve that the output of the projection layer (22) is the approximate individual transmission rate $\\bar{r}(W)$, connecting (7) and its convex relaxation (24). Actually, (22) is designed to ensure the transmission rate meets requirement (8). Also, there is a trade-off coefficient to penalize the violation to (8) in the loss function (25), which is described in the next subsection. The problem in (24) can be viewed as a function that maps the system parameters $(G, \\sigma,\\bar{r})$ and the learning parameters W to the solution of (7). Given these parameters, the convex relaxation problem in (24) can be solved by a fast iterative algorithm, as presented in Algorithm 1. Note that (16) in Algorithm 1 is the iteration of the standard interference function of transmission rate function $f_l^m(SINR_l^m(p_l^m(k)))$ in the constraints (cf. Lemma 2), while (17) is due to the stationarity of the Lagrangian of (24). Also, the Algorithm 1 not only gives the primal solution $p^*$ but the dual solution $\\lambda^*$ to the problem in (24), as we can obtain $\\lambda^*$ by (18) using the output p* and x*. This is important as the dual can also be used in the backward propagation to tune the learning parameters, as we will show in the following section. Suppose the output of the convolution filter (one can choose an appropriate size for the kernel) is K. Then the dimension of $h_0(W_0)$ in (20) is $(2ML + K) \\times 1$. The dense layers are used for the approximation of the optimal data rate $r_l^{m*}$ per user per subcarrier, thus its dimension is $M \\times L$. The output (21) is then input to the projection layer in (22), whose output dimension is still $M \\times L$. The output of the projection layer in (22) is then used to update $\\bar{r}(W)$ in the convex optimization layer in (24), and the output of the (24) is the optimal solution of p, whose dimension is also $M \\times L$. Combining (20)-(23) and the Algorithm 1 for solving (23), the forward pass stage of our proposed OpenRANet is presented in \u201cForward Propagation\u201d in Algorithm 2. From the perspective of end-to-end learning, the learning parameters W are trained to approximate the optimal solution to (7). Compared to other traditional data-driven deep learning models (e.g., one can simply train a CNN only to output the $r_l^m$ in (10) and use this output to solve (24) and then (7)), tuning these learning parameters not only depend on a universally applicable black-box-like convolutional filter and fully-connected neural network, but also the projection layer (22) and the convex optimization layer (24), which capture the specific structure and information of (7). This significantly improves both the accuracy and interpretability of the model. Also, it reduces the number of learning parameters in our model compared to the pure data-driven learning models. We will present these benefits in the simulation section."}, {"title": "B. Training Process", "content": "To train the learning parameters W in the OpenRANet designed above for the non-convex problem in (7), we need forward and backward propagation for our model. Suppose we are given a dataset D with |D| training samples. The loss function between the output of our learning model and the ground truths is defined as\n$E(W) = \\frac{1}{|D|} \\sum_{(G, \\sigma,\\bar{r},p^*) \\in D} \\bigg( C||\\sum_{m=1}^M \\bar{r}_l^m (W) - \\bar{r}_l||^2 + ||\\Phi(\\bar{r}(W), G, \\sigma) - p^*||^2 \\bigg),$\nwhere C is a trade-off of the violation of the constraint $\\sum_{m=1}^M e^{\\bar{r}_l^m} \\geq \\bar{r}_l$, e.g., if we cannot tolerate any violation of this constraint, we can set C to a large positive number and vice versa.\nThe dataset D in (25) includes labeled training data with channel gain, noise power, and the minimal required transmission rate as inputs, while the optimal solution p* serves as the output of the OpenRANet model. In practical applications, this dataset can be obtained through various methods, such as simulations or historical data. To create the dataset, we can first conduct simulations of the O-RAN environment by modeling different scenarios that encompass various user distributions, channel gains, noise power levels, and required transmission rates. In these simulations, we compute the optimal power allocation for each scenario using established algorithms, like the branch and bound algorithm, which systematically explores the solution space to find optimal allocations while meeting constraints. Additionally, we can gather historical data from existing O-RAN deployments, which may include real-world measurements of channel conditions, noise levels, and actual power allocations used to meet data rate requirements. This historical data helps validate and refine the synthetic dataset, ensuring it reflects realistic operating conditions and variations."}]}, {"title": "V. NUMERICAL EXAMPLES", "content": "In this section, we first provide some simulation results on the performance of the reweighted primal-dual algorithm described in Algorithm 1 for local optimality of the problem in (7) with the transmission rate function for CDMA approximation. Then we demonstrate the performance of the proposed OpenRANet to approximate optimal solutions to (7) under different transmission rate constraints\u00b9.", "subsections": [{"title": "A. Performance of the Primal-dual Algorithm", "content": "Consider an open RAN system having 4 users communicating by way of 2 subcarriers. We simulate the wireless channel under Rayleigh, Rician, Nakagami, and Weibull fading assumptions. Let $G_{ij}$ stand for the non-negative path gain and $d_{ij}$ stand for the Euclidean distance between the i-th user and the j-th user. We adopt the well-known path loss model [37] $G_{ij}^m = 10^{-12.8} 10^{3.76} d_{ij}^{-3.76}$, where $10^{-12.8}$ is the attenuation"}]}, {"title": "VI. CONCLUSION", "content": "This paper addresses the issue of minimizing total power consumption while meeting transmission rate requirements in the open RAN system. This problem is challenging due to its nonconvex nature, making it difficult to achieve optimality. Additionally, the existence of coupling rate constraints complicates the solution distribution, leading to increased energy consumption in the networks. To tackle these obstacles, we initially introduce a primal-dual algorithm that utilizes the unique log-convexity property of standard interference functions to coordinate power allocation among cooperating subcarriers. Furthermore, we propose an optimization-based deep learning model called OpenRANet, which integrates constraint information and convex subproblems into the deep learning model for subcarrier and power allocation for minimizing total power consumption. Our numerical experiments indicate that OpenRANet outperforms many cutting-edge strategies. The OpenRANet framework can serve as a foundation, and we plan to extend it in the future to accommodate multi-cell systems and additional system power consumption requirements, such as the energy required for signal processing, cooling, and neural network processing. Another consideration is adapting the model to scenarios where the traffic patterns change over time. To address this issue, we can integrate strategies such as transfer learning and incremental learning into OpenRANet. We believe this is a significant issue that deserves further investigation in future studies."}]}