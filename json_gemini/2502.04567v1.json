{"title": "Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator", "authors": ["Zhuotong Chen", "Fang Liu", "Xuan Zhu", "Haozhu Wang", "Yanjun Qi", "Mohammad Ghavamzadeh"], "abstract": "Existing studies on preference optimization (PO) have centered on constructing pairwise preference data following simple heuristics, such as maximizing the margin between preferred and dispreferred completions based on human (or AI) ranked scores. However, none of these heuristics has a full theoretical justification. In this work, we develop a novel PO framework that provides theoretical guidance to effectively sample dispreferred completions. To achieve this, we formulate PO as minimizing the negative log-likelihood (NLL) of a probability model and propose to estimate its normalization constant via a sampling strategy. As we will demonstrate, these estimative samples can act as dispreferred completions in PO. We then select contrastive divergence (CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies the Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the parameterized reward model. Finally, we propose the OnMC-PO algorithm, an extension of MC-PO to the online setting. On popular alignment benchmarks, MC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further improvement.", "sections": [{"title": "1. Introduction", "content": "While large language models (LLMs) learn broad world knowledge, aligning their behavior precisely with human values is challenging due to the unsupervised nature of their training. Reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) has emerged as a class of effective algorithms to align LLMs (Schulman et al., 2017). Recent works on direct preference optimization (DPO) (Rafailov et al., 2024) and its variants (e.g., Identity preference optimization (Azar et al., 2024)) directly optimize an LLM to adhere to human values, without explicit reward modeling or RL.\nThe data for these algorithms are often collected in the form of preferences (Ziegler et al., 2019). This leads to more consistent labeling across human annotators as it reduces their cognitive load and avoids the need for absolute judgment, which can be more subjective. Existing studies on PO have predominately considered creating pairwise preference data via simple heuristics, such as choosing a dispreferred completion by maximizing the gap with the preferred response in terms of human (AI) ratings (Tunstall et al., 2023; Lambert et al., 2024). However, none of these heuristics has a full theoretical justification. Here, we ask the question: \u201cHow to choose dispreferred completion(s) for PO?\u201d\nTo answer this question, we develop a novel PO framework that provides theoretical guidance on developing effective sampling strategies to select dispreferred completions. To achieve this, we formulate PO as minimizing the negative log-likelihood (NLL) of a probability model. Unfortunately, NLL includes a normalization constant that is in the form of an integral, and thus, usually intractable. To address this issue, we propose to estimate the normalization constant using a sampling strategy (Naesseth et al., 2024). For instance, the ranking noise contrastive estimation applies conditional importance sampling to generate samples from a proposal distribution and uses them to approximate the integral within the normalization constant (Olmin et al., 2024). As we will show, these samples can act as dispreferred completions in PO, thus establishing a connection between the proposed NLL estimation and existing PO algorithms. Such a connection enables us to apply advanced sampling strategies, from the literature on estimating the normalization constant in NLL, for generating dispreferred completions in PO.\nAfter formulating PO as an NLL minimization problem, we propose to use an advanced sampling strategy, contrastive divergence (CD), to estimate the normalization constant in this NLL. CD applies Markov chain Monte Carlo (MCMC) sampling to approximate the gradient of the log-normalization constant (Hinton, 2002). The central component in CD is the MC kernel that generates the next sample conditioned on the current one. The MC kernel produces samples with high"}, {"title": "2. Preference Optimization as NLL Estimation", "content": "In this section, we revisit the PO objective function (Sec. 2.1) and formulate it as minimizing the NLL of a probability model (Sec. 2.2). Then we apply a sampling-based approach to solve this (Sec. 2.3)."}, {"title": "2.1. Background: Preference Optimization", "content": "RLHF aims to align a target policy \\(\\pi_\\theta\\) with human preference based on an reward model \\(r(x, y)\\). This optimization problem is formulated as\n\\[\\max_{\\pi_\\theta} E_{x \\sim p, y \\sim \\pi_{\\theta}(\\cdot|x)} [r(x, y)] \u2013 \\beta\\cdot KL[\\pi_\\theta(y|x)||\\pi_{ref}(y|x)],\\]\nwhere \\(p\\) represents the distribution over input prompts, \\(\\beta\\) is a hyper-parameter controlling the deviation from the reference policy \\(\\pi_{ref}\\). The reward model \\(r(x, y)\\) is typically estimated from empirically observed data and cannot accurately represent real-world human preference. The KL-divergence constraint prevents the model from over-fitting the estimated reward model (Skalse et al., 2022), as well as maintaining the generation diversity and preventing mode-collapse to single high-reward completions.\nFollowing prior works (Go et al., 2023), the closed-form solution to the KL-constrained reward maximization in Eq. (1) can be derived as,\n\\[\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\exp \\left(\\frac{1}{\\beta} r(x,y)\\right).\\]\nThe partition function \\(Z(x)\\) ensures that \\(\\pi^*\\) is a valid probability conditioned on any \\(x\\). \\(Z(x)\\) is typically intractable to compute since the output space is combinatorially large, which makes this representation hard to utilize in practice.\n\u03a4\u03bf estimate \\(\\pi^*\\), DPO reparameterizes the reward model in terms of the target policy, which enables directly optimizing the target policy from pairwise preference dataset. DPO is derived from rearranging the closed-form solution of \\(\\pi^*\\) in Eq. (2) to express the reward function in terms of its corresponding optimal policy, then substituting this expression into the Bradley-Terry model (Bradley & Terry, 1952). This yields the DPO objective function,\n\\[\\min E_{(x,y_0,y_1) \\sim D} \\left[- \\log \\sigma \\left(\\beta r_{\\theta}(x, y_0) \u2013 \\beta r_{\\theta}(x, y_1)\\right)\\right],\\]\nwhere \\(r_{\\theta}(x, y) := \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}\\) is the parameterized reward model, \\(y_0\\) and \\(y_1\\) represent preferred and dispreferred completions, respectively, \\(D\\) is the distribution of pairwise preference data. DPO optimizes the target policy to distinguish between preferred and dispreferred completions, conditioned on the same input prompt.\nExisting studies on PO have primarily focused on creating pairwise preference data using heuristic approaches. In the"}, {"title": "2.2. Preference Optimization as NLL Estimation", "content": "Background: NLL estimation. Unnormalized models can be used to approximate complex data distributions. However, estimating unnormalized models is not straightforward since the NLL estimation involves the typically intractable normalization constant. Given some observations from the target distribution, we seek to approximate it with a parametric probability model,\n\\[p_{\\theta}(y|x) := \\frac{p_{\\theta}(y|x)}{Z_{\\theta}(x)} = \\frac{p_{\\theta}(y|x)}{\\int p_{\\theta}(y'|x)dy'},\\]\nwhere \\(p_{\\theta}\\) is an unnormalized model and \\(Z_{\\theta}(x)\\) is its normalization constant. The NLL estimation minimizes the negative log-likelihood of \\(p_{\\theta}\\) of predicting these observations. Roughly speaking, as the number of observations approaches to infinity, the NLL estimation results in a parametric probability model that increasingly approximates the target distribution.\nProposed Formulation: PO as NLL estimation. In this work, we define the following probability model that is closely related to the expression of \\(\\pi^*\\) in Eq. (2).\n\\[p_{\\theta}(y|x) := \\frac{1}{Z_{\\theta}(x)} \\mu(y|x) \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y)\\right),\\]\n\\[Z_{\\theta}(x) = \\int \\mu(y|x) \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y)\\dy,\\]\n\\[r_{\\theta}(x, y) = \\log \\frac{\\pi_{\\theta} (y|x)}{\\pi_{ref}(y|x)}\\]\nwhere \\(\\mu\\) is a proposal distribution that we can sample from. For any set of parameters \\(\\theta\\), we assume that \\(p_{\\theta}\\) covers the support of \\(\\pi^*\\), such as \\(p_{\\theta}(y|x) > 0\\) whenever \\(\\pi^*(y|x) > 0\\), for all \\(x \\sim p\\). In this expression, the unnormalized model is defined as \\(p_{\\theta}(y|x) := \\mu(y|x) \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y)\\right)\\). To estimate \\(\\pi^*\\) with \\(p_{\\theta}\\), the NLL estimation minimizes the negative log-likelihood of \\(p_{\\theta}\\) to predict observations sampled from \\(\\pi^*\\),\n\\[\\theta^* = \\arg \\min_{\\theta} E_{x \\sim p, y \\sim \\pi^*(\\cdot|x)} \\left[\\mathcal{L}_{NLL} (\\theta,x,y)\\right],\\]\nwhere \\(\\mathcal{L}_{NLL} (\\theta, x, y) = \u2013\\beta r_{\\theta}(x, y) + \\log Z_{\\theta}(x)\\).\nRecall in Eq. (5) that the reward model can be represented in terms of the target policy, which allows for optimizing the target policy by solving the NLL estimation.\nIn practice, the first term of \\(\\mathcal{L}_{NLL}\\) in Eq. (6) is typically easy to optimize as the gradient of the target policy (e.g., the LLM) can be computed using existing automatic differentiation software. However, optimizing the normalization constant is non-trivial. In the next, we focus on samplingbased approaches to estimate the normalization constant."}, {"title": "2.3. Preference Optimization via Sampling-Based Solution for Its NLL Estimation Formulation", "content": "Proposed: PO via sampling-based solution of NLL estimation. Importance sampling applies samples from a proposal distribution to estimate the normalization constant (Naesseth et al., 2024). Ranking noise contrastive estimation (RNCE) (Olmin et al., 2024), as a more advanced sampling approach, utilizes both importance samples and true observations from the target distribution to estimate the intractable term. Given one observation \\(y_0\\) sampled from \\(\\pi^*\\) and M i.i.d. noisy samples from a proposal distribution, RNCE optimizes to classify as \\(y_0\\) coming from the true distribution.\nProposition 2.1. Suppose that we have \\(y_0 \\sim \\pi^*(y|x)\\), and M noisy samples \\(\\{y_i\\}_{i=1}^M\\), where each \\(y_i\\) is sampled from a proposal distribution, \\(y_i \\sim \\mu(y|x)\\). Then RNCE approximates the NLL estimation as follows,\n\\[\\mathcal{L}_{Sample} (\\theta, x, y_0)\\]\n\\[= - \\frac{\\beta}{\\beta} r_{\\theta}(x,y_0) + \\log \\sum_{i=0}^M \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_i)\\right).\\]\nThe detailed derivation is in Appendix A.2. Compared to the NLL estimation in Eq. (6), RNCE approximates the intractable normalization constant as \\(Z_{\\theta}(x) = \\frac{1}{M+1} + \\sum_{i=1}^M \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x, y_i)\\right)\\), with both \\(y_0\\) sampled from \\(\\pi^*\\) and noisy samples \\(\\{y_i\\}_{i=1}^M\\) from a proposal distribution (notice that \\(\\frac{1}{\\beta}\\) is cancelled by taking the gradient of \\(\\log \\mathcal{L}_{Sample}\\)). Consequently, \\(\\mathcal{L}_{Sample}\\) is equivalent to a cross-entropy loss that optimizes the model to classify \\(y_0\\) as the correct prediction among all (M + 1) candidates.\nProposed: Existing PO can be formulated as samplingbased solutions of NLL estimation. In the samplingbased solution from Eq. (7), we consider the true observation \\(y_0\\) as the preferred completion, and noise samples from the proposal distribution as dispreferred completions. This leads to an expression of PO as follows, with letting M = 1,\n\\[\\frac{\\beta}{\\beta} r_{\\theta}(x,y_0) + \\log \\sum_{i=0}^1 \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_i)\\right)\\]\n\\[= - \\log \\frac{\\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_0)\\right)}{\\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_0)\\right) + \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_1)\\right)}\\]\n\\[= - \\log \\sigma \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_0) \u2013 \\frac{\\beta}{\\beta} r_{\\theta}(x,y_1)\\right),\\]\nwhere \\(\\sigma\\) is the logistic function. This sampling-based solution with one noise sample is equivalent to DPO where the noise sample acts as a dispreferred completion (Eq. (3)). This provides a novel interpretation on dispreferred completions in existing PO: dispreferred completions can be"}, {"title": "3. Novel Preference Optimization via CD", "content": "Contrastive divergence (CD) uses MCMC methods to estimate the gradient of the log-normalization constant. CD starts the MCMC sampling from training data rather than a random state, which allows the sampling to converge faster. The sampling process involves a small number of MCMC steps (often just one), making it particularly effective for probability models where the normalization constant cannot be easily computed. CD represents a class of sampling strategies that can be implemented by developing different MC Kernels. The aforementioned RNCE is a special case of CD with random sampling. Based on the theoretical foundation that connects PO with sampling-based solutions for NLL estimation, we first derive a CD algorithm for PO, referred to as MC-PO (Sec. 3.1). We then extend MC-PO to an online setting, developing OnMC-PO (Sec. 3.2)."}, {"title": "3.1. Preference Optimization with MCMC Sampling", "content": "To begin with, we derive the gradient of the NLL estimation in Eq. (6).\n\\[\\nabla_{\\theta}\\mathcal{L}_{NLL}(\\theta, x, y)\\]\n\\[= -\\nabla_{\\theta}\\frac{\\beta}{\\beta} r_{\\theta}(x, y_0) + \\nabla_{\\theta}\\log Z_{\\theta}(x),\\]\n\\[= -\\nabla_{\\theta}\\frac{\\beta}{\\beta} r_{\\theta}(x,y_0) + E_{p_{\\theta}(y|x)} \\left[\\nabla_{\\theta}\\frac{\\beta}{\\beta} r_{\\theta}(x, y)\\right].\\]\nThis gradient term is intractable to compute since it involves an expected value over the probability model \\(p_{\\theta}\\) defined in Eq. (5). To address this, CD applies a MC Kernel \\(K_{\\theta}(y'|x, y)\\) to estimate the gradient of the lognormalization constant. the MC Kernel generates samples with high likelihood from \\(p_{\\theta}\\) via a proposal distribution.\nWe consider the MC Kernel defined in Algorithm 1. Given a proposal distribution \\(\\mu\\), this kernel is initialized at a pair of x and \\(y_0\\) sampled from \\(\\pi^*\\)1. At the initial step of the MCMC chain, it first generates L samples from the proposal distribution, then it samples the output y' from a softmax distribution computed from the unnormalized model over all L samples. At the next iteration, this kernel computation is repeated with an initialization of y' being the sampled output from the previous step. The MC Kernel aims to generate a sample with high estimated reward from a proposal distribution.\nProposed: CD samples hard negatives for PO. We first connect CD with RNCE and discuss the sampling strategy suggested by CD. Then we apply this sampling to PO.\nLemma 3.1. When CD runs the MCMC chain for only a single step, it shares the same objective function with RNCE in Eq. (7).\nThe detailed derivation is in Appendix A.3. Given a true observation \\(y_0\\) and noisy samples \\(\\{y_i\\}_{i=1}^M\\), the objective functions of CD and RNCE are equivalent under a special condition. However, the MC Kernel from CD, as outlined in Algorithm 1, suggests to sample in proportion to the reward model. This leads to more accurate NLL estimation. Specifically, the gradient of the log-normalization constant in Eq. (8) is represented as the expected value over the probability model. By sampling in proportion with the reward model, it effectively generates samples with higher probability mass from the probability model, thereby improving the coverage of the distribution in the expected value.\nRecall the connection between CD and RNCE established in Sec. 2.3, existing PO can be formulated as CD. CD leads to improved accuracy by sampling proportionally to the reward model, which suggests to choose hard negatives with high estimated rewards for PO. Here we show that PO benefits from hard negatives.\nLemma 3.2. Let M = 1, the gradient of the samplingbased objective in Eq. (7) can be derived as follows,\n\\[\\nabla_{\\theta}\\mathcal{L}_{Sample} (\\theta,x,y_0) = \u2013 \\frac{\\beta}{\\beta} \\sigma \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_1) \u2013 \\frac{\\beta}{\\beta} r_{\\theta}(x,y_0)\\right)\\]\n\\[\\nabla_{\\theta} \\left(r_{\\theta}(x,y_0) - r_{\\theta}(x,y_1)\\right),\\]\nwhere \\(y_0\\) and \\(y_1\\) are preferred and dispreferred completions, respectively.\nThe derivation is in Appendix A.4. When the estimated reward for a dispreferred completion exceeds that of a preferred one, this results in a larger gradient magnitude, lead-"}, {"title": "3.2. MCMC for Online Preference Optimization", "content": "Online MC-PO leads to an unbiased gradient estimator. Having an unbiased gradient estimation is a standard condition to guarantee general convergence of stochastic gradient descent (Bottou et al., 2018). We demonstrate that in an online setting where the true observation is sampled from the probability model, rather than from the target distribution, then the CD estimation of the gradient of log-normalization in Eq. (8) is an unbiased estimator.\nProposition 3.3. Let \\(\\tilde{Z}_{\\theta}(x)\\) be an estimation of the normalization constant,\n\\[\\tilde{Z}_{\\theta}(x) = \\frac{1}{M+1} \\sum_{i=0}^M \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_i)\\right).\\]\nWhen \\(y_0\\) is sampled from the probability model \\(p_{\\theta}\\), then\n\\[E_{p_{\\theta} (y_0|x), \\mu(\\left\\{y_i\\right\\}_{i=1}^M|x)} \\left[\\nabla_{\\theta}\\log\\tilde{Z}_{\\theta}(x)\\right] = \\nabla_{\\theta}\\log Z_{\\theta}(x),\\]\nwhere \\(\\mu(\\left\\{y_i\\right\\}_{i=1}^M|x)) = \\prod_{i=1}^M \\mu(y_i|x)\\).\nThe detailed derivation is in Appendix A.5. This explains the clear advantage of online methods over offline methods (Tang et al., 2024). Specifically, online PO algorithms generate preferred completions from the target policy that is proportional to the probability model \\(p_{\\theta}\\), intends to have an unbiased gradient estimation.\nPractical implementation for OnMC-PO. As suggested by Proposition 3.3, it is desirable to sample the preferred completion from the probability model \\(p_{\\theta}\\). We implement online MC-PO (OnMC-PO) as an extension of MC-PO. Given a input prompt, we sample multiple completions from the target policy \\(\\pi_{\\theta}\\) and identify the most preferred one as \\(y_0\\). Moreover, since the policy update at each step is relatively small, we consider a batched online algorithm (Rosset et al., 2024) where sampling from \\(\\pi_{\\theta}\\) is done after a number of gradient updates."}, {"title": "4. Related Works", "content": "Aligning LLMs with human preferences has predominately been considered as an RL problem (Ouyang et al., 2022). However, the on-policy nature of RLHF necessitates learning a reward model from preference data first, then maximize the estimated reward model with RL techniques, leading to a two-stage optimization process (Schulman et al., 2017). Recent developments in preference-based alignment techniques have streamlined this process (Rafailov et al., 2024; Azar et al., 2024). They enable direct model alignment through a singular loss. We categorize existing DPO variants as contrastive-based or classification-based approaches according to their objective functions. Contrastivebased approaches maximize the difference of the predicted likelihoods between preferred and dispreferred completions, while classification-based approaches conduct maximization on the preferred and minimization on dispreferred completions, respectively.\nSome notable contastive-based algorithms include DPO (Rafailov et al., 2024) that is derived from reparametrizing the reward function in RLHF to directly learn a policy from preference data. IPO (Azar et al., 2024) that replaces the logistic loss with a squared loss to address the shortcomings of Bradely-Terry preference modeling in cases where preference data are highly deterministic. SimPO (Meng et al., 2024) introduces length regularization on the logprobabilities of both preferred and dispreferred completions, eliminating the need for a reference model. RPO (Liu et al., 2024) that derives a superivsed next-word prediction regularization to prevent the decrease of the likelihood to predict preferred completions. The first classification-based algorithms is KTO (Ethayarajh et al., 2024) that formulate both maximization and minimization objectives w.r.t. a reference point. BCO (Jung et al., 2024) derives the reference point that minimizes the gap with DPO. NCA (Chen et al., 2024a) is derived from noise contrastive estimation for working with reward data (Gutmann & Hyv\u00e4rinen, 2010).\nIn this work, we formulate the alignment problem as sampling-based solutions to solve NLL estimation. We first propose the RNCE based sampling as a general PO solution that randomly selects dispreferred completions from a set of candidates. This solution is similar to InfoNCA (Chen et al., 2024a) that generalizes DPO to adopt multiple dispreferred completions. Different from InfoNCA, our proposed NLL estimation perspective of model alignment interprets dispreferred completions as the estimative samples to compute the normalization constant, which provides theoretical guidance on developing sampling strategies to choose dispreferred completions for PO. Based on the NLL estimation formulation, we further develop MC-PO that uses an MCMC kernel to select high-quality dispreferred completions, leading to improved model performance."}, {"title": "5. Numerical Experiments", "content": "In this section, we present main results of our experiments, highlighting the effectiveness of MC-PO and OnMC-PO on various benchmarks (Sec. 5.2) and providing an in-depth understanding on the effect of sampling strategies (Sec. 5.3). More extensive results can be found in Appendix B."}, {"title": "5.1. Experimental Setup", "content": "We summarize experimental setting here, more details can be found in Appendix B.1.\nModels and datasets. We perform PO under three different setups: (1) The base setup considers the Llama-3.1-8B-SFT model, which has been fine-tuned using supervised next-word prediction on the T\u00dcLU 3 SFT Mix dataset (Lambert et al., 2024), and Mistral-7B-SFT. We fine-tune these models on the Nectar dataset (Zhu et al., 2023). The Nectar dataset consists of 7 ranked completions per input prompt generated by different LLMs, which creates both high-quality and diverse candidate completions for sampling. For each input prompt, we consider the rank-1 completion as the preferred completion and subsequently eliminate the rank-2 completion to minimize noises in preference pairs. From the remaining 5 candidates, we then randomly select a dispreferred completion. (2) The instruct setup uses the off-the-shelf instruction-tuned Llama-3.1-8B-Instruct model (Dubey et al., 2024) to initialize the target policy \\(\\pi_{\\theta}\\). This model has undergone extensive instruction-tuning processes, making it more expressive compared to the initialization model in the base setup. We use prompts from the UltraFeedback dataset (Cui et al., 2024) to regenerate the preferred and dispreferred completions using the Llama-3.1-8B-Instruct model. This makes the instruct setup closer to an on-policy setting (Tang et al., 2024). Specifically, we generate 6 completions using temperatures of 0.6, 0.8, and 1 for each input prompt. Then, we apply the iterative pairwise ranking approach (Chen et al., 2024b) with the Llama-3.1-70B-Instruct to select the most preferred completion and randomly sample a dispreferred completion from remaining candidates. (3) The batched online setup is in the middle of the offline and purely online setups (Schulman et al., 2017; Lambert et al., 2024), striking a balance between efficiency and adaptability. We equally split the training steps into three batches and regenerate the preference data following the instruct setup using the current model checkpoint. This approach is more efficient than a purely online setting (Qi et al., 2024), as initializing the inference is often computationally expensive (Kwon et al., 2023).\nTraining. All training jobs are done using full-parameter tuning. We fix the effective batch size of 128 and the number of training epochs of 2. Hyperparameter optimization is"}, {"title": "5.2. Main Results: Comparing with SOTA PO", "content": "We first compare MC-PO with existing offline PO algorithms, then demonstrate that OnMC-PO further improves alignment performance. We categorize existing baselines as contrastive and classification-based approaches based on their objective functions. Specifically, contrastive-based algorithms include DPO (Rafailov et al., 2024), RPO (Liu et al., 2024), EXO (Ji et al., 2024), SimPO (Meng et al., 2024) and CPO (Xu et al., 2024). Classification-based algorithms include BCO (Jung et al., 2024), KTO (Ethayarajh et al., 2024), APO (D'Oosterlinck et al., 2024), SPPO (Wu et al., 2024) and NCA (Chen et al., 2024a). Details on baseline algorithms can be found in Appendix B.2.\nThe main results are summarized in Table 1. MC-PO outperforms existing baselines in five out of six studies. Notably, in the base setup, using the Mistral-7B-SFT model, MCPO outperforms DPO by 4.5% and 9% on Alpaca-Eval and Arena, respectively. Using the Llama-3.1-8B-SFT model, MC-PO leads to winrates of 35.84% and 63.77% on AlpacaEval and Arena, respectively. In the instruct setup, as all candidate completions are sampled from the Llama-3.1-8BInstruct model, sampling from these candidates proves less effective due to the low diversity in the candidate set. Consequently, MC-PO shows less improvement compared to existing baselines. When MC-PO is extended to online setting based on the batched online setup, OnMC-PO results in further improvement. With the Llama-3.1-8B-Instruct model, OnMC-PO leads to a winrate of 72.63% on Alpaca, outperforming existing baselines."}, {"title": "5.3. Analysis of Sampling Strategies in MC-PO", "content": "We also study how varying the sampling strategies in MCPO impact the PO performance as the quality of sampled preference dataset gets changed. We develop Max and Min samplings as variants of MC-PO based on the MCMC kernel defined in Algorithm 1. Here, Max (Min) sampling variant outputs the candidate with maximum (minimum) weight among all candidates, where the weight is calculated as\n\\[W_i = \\frac{\\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_i)\\right)}{\\sum_{j=0}^L \\exp \\left(\\frac{\\beta}{\\beta} r_{\\theta}(x,y_j)\\right)}.\\]\nMoreover, we construct preference datasets with varying candidate qualities. For instance, based on the Nectar dataset, we progressively remove highly ranked completions for each input prompt. The first dataset excludes the rank-2 completion, while the second dataset excludes both the rank-2 and rank-3 completions, resulting in diminished candidate quality. The results are summarized in Fig. 2, from which we observe the following insights:\n(1) Sampling from the MCMC kernel yields optimal performance. MC-PO achieves a balance between exploitation (sampling according to the categorical distribution), and exploration (retaining probabilities for choosing alternative candidates). As detailed in Sec. 3.1, this approach accurately estimates the gradient of the log-normalization constant, which in turn, leads to improved performance.\n(2) Min-based variant leads to low performance and high variance. From the NLL estimation viewpoint of PO, dispreferred samples are used for estimating the gradient of the log-normalization constant. CD proves that hard negatives yield a more accurate gradient estimation. The Min sampling variant, instead, is deemed as the worst sampling"}, {"title": "5.4. Data and Ablation Analysis of MC-PO", "content": "MC-PO is robust against noisy samples: We demonstrate that MC-PO maintains high model performance even when noisy samples are included in the candidate set. Differently, it has been proven that when the edit distance between pairs of completions is low, DPO leads to a reduction in the model's likelihood of preferring the desired completions (Pal et al., 2024).\nFor experimental setup, we consider the processed Nectar dataset and inject a noise sample into the candidate set by randomly switching two tokens of the preferred completion for each input prompt. As shown in Table 2, due to the small edit distance between all preference pairs, DPO(-), which applies the noise sample as dispreferred completion, leads to a degenerated model. DPO, which randomly selects a dispreferred completion, is impacted by the noise injec-"}, {"title": "6. Conclusion, Limitations and Future Work.", "content": "This paper formulates the alignment problem via NLL estimation and propose sampling-based solutions for better PO. Compared to DPO, our CD based MC-PO adds approximately 30% training time because of the computations required by the MCMC kernel. Also we run MC-PO's MCMC kernel for a single step and this is typically suboptimal because executing the MCMC chain for multiple steps is essential to acquire high-quality samples (Hinton, 2002), which will further adds more computational overhead to PO training. As future research, we aim to showcase the benefits of utilizing a multi-step MCMC based PO solutions and will develop more efficient training algorithms to speed up these algorithms."}, {"title": "A. Theoretical Derivations", "content": null}, {"title": "A.1. Background on Preference Optimization", "content": "RLHF formulation. Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. One of the key ingredients behind the success of language models is post-training alignment. Reinforcement learning from human feedback (RLHF) aims to align a target policy \\(\\pi_{\\theta}\\) with human preference based on an reward model \\(r(x, y)\\) that approximates human judgements. This optimization problem is formulated as\n\\[\\max_{\\pi_{\\theta}} E_{x \\sim p, y \\sim \\pi_{\\theta}(\\cdot|x)} [r(x, y)", "KL[\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)": "nwhere \\(\\beta\\) is a hyper-parameter controlling the deviation from the base reference policy \\(\\pi_{ref}\\). The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers.\nPrior works (Go et al., 2023; Peters & Schaal, 2007) prove that the optimal solution to the KL-constrained reward maximization objective takes the following form,\n\\[\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\exp \\left(\\frac{1}{\\beta} r(x,y)\\right),\\", "r(x,y)\\right).\\": "nSince the space of model completions is combinatorilly large, computing \\(Z(x)\\) exactly is often infeasible.\nDirect preference optimization (Rafailov et al., 2024). Direct preference optimization (DPO) enables to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. Specifically, DPO re"}]}