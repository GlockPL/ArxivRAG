{"title": "Why Vision Language Models Struggle with Visual Arithmetic?\nTowards Enhanced Chart and Geometry Understanding", "authors": ["Kung-Hsiang Huang", "Can Qin", "Haoyi Qiu", "Philippe Laban", "Shafiq Joty", "Caiming Xiong", "Chien-Sheng Wu"], "abstract": "Vision Language Models (VLMs) have\nachieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arith-\nmetic, seemingly simple capabilities like object\ncounting or length comparison, which are es-\nsential for relevant complex tasks like chart\nunderstanding and geometric reasoning. In this\nwork, we first investigate the root causes of\nthis deficiency through a suite of probing tasks\nfocusing on basic visual arithmetic. Our anal-\nsis reveals that while pre-trained vision en-\ncoders typically capture sufficient information,\nthe text decoder often fails to decode it cor-\nrectly for arithmetic reasoning. To address this,\nwe propose COGALIGN, a novel post-training\nstrategy inspired by Piaget's theory of cogni-\ntive development. COGALIGN trains VLMs\nto recognize invariant properties under visual\ntransformations. We demonstrate that this ap-\nproach significantly improves the performance\nof three diverse VLMs on our proposed prob-\ning tasks. Furthermore, COGALIGN enhances\nperformance by an average of 4.6% on CHOCO-\nLATE and 2.9% on MATH-VISION, outperform-\ning or matching supervised fine-tuning meth-\nods while requiring only 60% less training data.\nThese results highlight the effectiveness and\ngeneralizability of COGALIGN in improving\nfundamental visual arithmetic capabilities and\ntheir transfer to downstream tasks.", "sections": [{"title": "1 Introduction", "content": "In recent years, vision language models (VLMs)\nhave rapidly advanced, demonstrating remarkable\ncapabilities in integrating and processing multi-\nmodal information (Liu et al., 2023; Dai et al.,\n2023; Chen et al., 2024; Xue et al., 2024). These\nmodels have found extensive applications across\nvarious domains, ranging from visual common-\nsense reasoning to sophisticated tasks like web\nagents (Xu et al., 2024; Zhang et al., 2024a; Xie\net al., 2024; Lin et al., 2024). By leveraging both\nvisual and textual data, VLMs promise a nuanced\nunderstanding that surpasses what can be achieved\nby analyzing them individually.\nDespite these advancements, current VLMs\nexhibit noticeable deficiencies in performing\nfundamental visual arithmetic: these models\nstruggle with seemingly simple tasks like ac-\ncurately counting objects, comparing lengths,\nassessing angles, and evaluating relative sizes\nor areas (Rahmanzadehgervi et al., 2024; Wang\net al., 2024c; Huang et al., 2024a; Ullman,\n2024; Wei et al., 2024). These shortcomings are\nparticularly evident in complex tasks such as chart\nunderstanding (Huang et al., 2024c) and geometric\nproblem-solving (Gao et al., 2025).\nIn this study, we first delve into the root causes\nof VLMs' difficulties with visual arithmetic, ex-\nploring several hypotheses to elucidate why VLMs\noften fail when faced with such challenges (\u00a72).\nWe propose a suite of probing tasks, focusing on\nbasic visual arithmetic such as length comparison,\nto answer this question. Our analysis reveals that\npre-trained vision encoders coupled with a simple\nlinear classifier perform poorly on these probing\ntasks, indicating that a single linear layer is insuffi-\ncient to decode the complex visual representations\nfor arithmetic reasoning. However, when we fine-\ntune the text decoder of a VLM on these tasks, per-\nformance significantly improves. This suggests the\nbottleneck lies in the decoder's ability to effec-\ntively process and utilize the visual information,\nrather than in the visual representation itself.\nTo tackle these challenges, we propose a novel\npost-training strategy, COGALIGN, designed to im-\nprove the performance of VLMs in visual arith-\nmetic tasks (\u00a73). Drawing inspiration from Piaget's\ntheory of cognitive development (Piaget, 1952), our\nmethod focuses on enhancing VLMs' understand-\ning of conservation (recognizing that certain prop-"}, {"title": "2 Why Vision Language Models Struggle\nwith Visual Arithmetic?", "content": "As suggested in previous studies, VLMs struggle\nwith visual arithmetic (Rahmanzadehgervi et al.,\n2024; Wang et al., 2024c), leading to poor perfor-\nmance in tasks involving such capabilities such as\nchart understanding (Huang et al., 2024c) and geo-\nmetric problem-solving (Gao et al., 2025). In this\nsection, we aim to understand the root causes be-\nhind such phenomenon. We first propose a suite of\nprobing tasks we design to facilitate our analysis\n(\u00a72.1) and then illustrate the various analyses we\nconduct to validate our hypotheses (\u00a72.2).\n2.1 Probing Tasks\nWe propose four probing tasks for assessing visual\narithmetic capabilities, motivated by the fundamen-\ntal operations needed to interpret visual data quan-\ntitatively. For a VLM to successfully understand\na chart, for example, it must be able to compare\nlengths of bars or lines, discern relationships in-\ndicated by line slopes, and projecting points onto\naxes. An overview of the probing tasks are shown\nin Figure 1. All four tasks are discriminative and\ncan be considered binary classification tasks. Be-\nlow, we illustrate these tasks in details.\nAngle Comparison asks models to determine\nwhether the angle of two wedges are the same."}, {"title": "2.2 Probing Analysis", "content": "The research question we aim to answer is: Do\nvisual representations from pre-trained vision en-\ncoders contain enough information to perform vi-"}, {"title": "3 COGALIGN", "content": "To address the challenges VLMs face in performing\nvisual arithmetic, we propose a novel post-training\nmethod inspired by Piaget's theory of cognitive\ndevelopment (Piaget, 1952), which outlines four\nstages: Sensorimotor, Preoperational, Concrete Op-\nerational, and Formal Operational. Each stage rep-\nresents a different ability to process information\nand solve problems, culminating in abstract reason-\ning. The Concrete Operational Stage is particularly\nrelevant. At this stage, children develop (1) conser-\nvation, understanding that certain properties like\nlength remains constant despite changes in appear-\nance, and (2) decentration, the ability to consider\nmultiple aspects of a situation at once. These skills\nare essential for VLMs to perform visual arithmetic\naccurately, recognizing invariant properties such\nas length or angle across transformations. Current\nVLM training paradigms often neglect these cogni-\ntive processes, resulting in models that struggle to\nmaintain key properties during visual transforma-\ntions and to integrate multiple visual features effec-\ntively. While pre-trained visual encoders use losses\nthat encourage some invariance to transformations,\nthe integration of vision and language representa-\ntions in decoders often lacks explicit enforcement\nof conservation and decentration principles, lead-\ning to models that capture visual features but fail\nto reason about them effectively.\nTo address these issues, we present a\npost-training method, Cognitive Alignment\n(COGALIGN), aimed at enhancing VLMs' under-\nstanding of conservation and decentration. Our\napproach explicitly trains VLMs to recognize\ninvariant properties like length, angle, and count\nacross different visual transformations. We achieve\nthis by presenting the model with pairs of figures\nand associated queries designed to highlight these\nproperties. The queries prompt the model to\ncompare and contrast the figures, focusing on\nwhether a specific property is different or same\ndespite variations in appearance. This approach\nencourages the model to develop a stronger under-"}, {"title": "3.1 DPO Training Objective", "content": "Our goal is to train a model with parameters 0\nthat learns conservation and decentration from con-\ntrasting responses by maximizing the conditional\nprobability of positive responses over their nega-\ntive counterparts. Concretely, the DPO training\ndata consists of preference pairs, each containing\na user query Q, an input image I, a positive re-\nsponse Rp and a negative response Rn. The entire\nset of DPO training data can be represented as\n$\\mathcal{D} = \\{(Q, I, R_p, R_n)^{(i)}\\}_{i=1}^S$. The objective func-\ntion $\\mathcal{L}_{DPO}$ that DPO minimizes is:\n$\\mathcal{L}_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\mathbb{E}_{(Q, I, R_p, R_n) \\sim \\mathcal{D}} \\left[\\log \\sigma(r)\\right]$,\n$r = \\beta \\log \\frac{\\pi_{\\theta}(R_p | Q, I)}{\\pi_{\\theta}(R_n | Q, I)} - \\beta \\log \\frac{\\pi_{ref}(R_p | Q, I)}{\\pi_{ref}(R_n | Q, I)}$,\nwhere $\\sigma$ is the sigmoid function, $\\pi_{\\theta}$ is the param-\neterized policy under training, $\\pi_{ref}$ is the initial\nfrozen policy, and $\\beta$ is a hyper-parameter that con-\ntrols the deviation from $\\pi_{ref}$."}, {"title": "3.2 Training Data Synthesis", "content": "To effectively train VLMs on the principles of con-\nservation and decentration, we require a training\ndataset designed to highlight these concepts. This\nsection details our automated process for synthesiz-\ning training data, encompassing visual generation\nand tailored query-response construction. We draw\ninspiration from our probing tasks but adapt the for-\nmat to better suit the DPO training procedure. By\nplotting two shapes within a single image, we allow\nthe model to directly compare invariant properties\nlike length and angle across various transforma-\ntions. We devise eight fundamental tasks, each of\nwhich aim to enhance VLMs' different abilities to\nreason about visual arithmetic operations: under-\nstanding angle, length, distance, quantity, volume,\nposition, slope, and intersection."}, {"title": "3.3 Effectiveness on the Probing Tasks", "content": "To assess the effectiveness of COGALIGN on our\nproposed probing tasks, we trained three VLMs\nwith varying scales and architectures: LLaVA-\nOV-0.5B (Li et al., 2024), InternVL-2.5-MPO-1B\n(Wang et al., 2024b), and InternVL-2.5-MPO-4B\nusing COGALIGN, as described in \u00a73.1 and \u00a73.2,\nfor one epoch."}, {"title": "4 Generalizability of COGALIGN", "content": "Now that we have demonstrated the advantage of\nCOGALIGN on our probing tasks, we ask: does the\nimprovement on simple visual arithmetic tasks\ntransfer to more complex tasks? To answer\nthis question, we explore whether COGALIGN en-\nhances model performance in chart understanding\nand geometric problem-solving. In the following\nsubsections, we detail the experimental setup (\u00a74.1)\nand present our findings (\u00a74.2)."}, {"title": "4.1 Experimental Setups", "content": "Benchmarks We evaluate the effectiveness of\nour method on two tasks relevant to visual arith-\nmetic: chart understanding and geometry problem-\nsolving."}, {"title": "4.2 Results", "content": "The results for experiments on CHOCOLATE and\nMATH-VISION are shown in Table 4. We find\nthat COGALIGN is effective in enhancing chart\nunderstanding and geometric problem-solving\ncapabilities of VLMs even though COGALIGN\nwas not specifically optimized for these two tasks.\nOn average, COGALIGN boosts the performance\nby 4.6% and 2.9% on the CHOCOLATE and MATH-\nVISION datasets, respectively. This shows that\npatching fundamental capabilities such as visual\narithmetic of VLMs can enhance their capabilities\nin tasks involving such abilities.\nMore importantly, we find that COGALIGN\ndemonstrates better generalizability compared\nto supervised fine-tuning VLMs using task-\nspecific data."}, {"title": "4.3 Discussions", "content": "Impact of learning from contrasting examples\nWe investigate the impact of learning from contrast-\ning examples versus solely positive examples by\ncomparing DPO (the default COGALIGN setting)\nand SFT training method (using only the positive\nresponse).\nImpact on general VLM benchmarks To as-\nsess the impact of COGALIGN on general VLM\ncapabilities, we compare the performance on two\nadditional benchmarks: MME (Fu et al., 2023)\nand MMMU (Yue et al., 2024)."}, {"title": "5 Related Works", "content": "5.1 Vision Language Models\nVision language models (VLMs) are multimodal\nmodels that learns to generate text outputs based\non both visual and textual inputs. The development\nof large-scale VLMs has demonstrated impressive\nzero-shot capabilities, enabling them to perform\nwell with a variety of image types, such as docu-\nments and web pages (Liu et al., 2023; Dai et al.,\n2023; OpenAI, 2023; Google, 2023; Anthropic,\n2023). These VLMs generally consist of three\nmajor components: a vision encoder, such as CLIP\n(Radford et al., 2021) or SigLIP (Zhai et al., 2023),\nwhich processes visual inputs; a language model\nthat handles textual inputs and generates text to-\nkens; and a projector layer that connects the image\nand text modalities."}, {"title": "5.2 Shortcomings of Vision Language Models", "content": "While Vision-Language Models (VLMs) demon-\nstrate impressive performance across a range\nof tasks, several studies have highlighted their"}, {"title": "6 Conclusion", "content": "This study investigates the challenges faced by\nVLMs in performing visual arithmetic, revealing\nthat while visual encoders often capture necessary\ninformation, text decoders struggle to effectively\nutilize it. We introduce COGALIGN, a novel post-\ntraining strategy inspired by Piaget's theory of cog-\nnitive development, focusing on enhancing VLMs'\nunderstanding of conservation and decentration\nthrough DPO training."}, {"title": "7 Limitations", "content": "Probing Tasks While the probing tasks we have\nproposed provide valuable insights into the visual\narithmetic capabilities of VLMs, it is important to\nacknowledge that they may not encompass all pos-\nsible dimensions of visual reasoning. Our choice\nto limit the scope of these tasks was intentional,\nas they serve as initial, simple tests to determine\nwhether VLMs exhibit failure in fundamental as-\npects of visual arithmetic. However, there is potential\nto explore additional tasks that involve more com-\nplex interactions of basic geometric properties. For\nTraining Data Synthesis The training data syn-\nthesis method of COGALIGN is not only scalable\nbut also effectively enhances the visual arithmetic\ncapabilities of VLMs."}]}