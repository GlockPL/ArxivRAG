{"title": "TOWARDS NEURAL SCALING LAWS FOR TIME SERIES FOUNDATION MODELS", "authors": ["Qingren Yao", "Chao-Han Huck Yang", "Renhe Jiang", "Yuxuan Liang", "Ming Jin", "Shirui Pan"], "abstract": "Scaling laws offer valuable insights into the design of time series foundation mod-els (TSFMs). However, previous research has largely focused on the scaling laws of TSFMs for in-distribution (ID) data, leaving their out-of-distribution (OOD) scaling behavior and the influence of model architectures less explored. In this work, we examine two common TSFM architectures\u2014encoder-only and decoder-only Transformers\u2014and investigate their scaling behavior on both ID and OOD data. These models are trained and evaluated across varying parameter counts, compute budgets, and dataset sizes. Our experiments reveal that the log-likelihood loss of TSFMs exhibits similar scaling behavior in both OOD and ID settings. We further compare the scaling properties across different architectures, incorporating two state-of-the-art TSFMs as case studies, showing that model architecture plays a significant role in scaling. The encoder-only Transformers demonstrate better scalability than the decoder-only Transformers, while the architectural enhancements in the two advanced TSFMs primarily improve ID performance but reduce OOD scalability. While scaling up TSFMs is expected to drive performance break-throughs, the lack of a comprehensive understanding of TSFM scaling laws has hindered the development of a robust framework to guide model scaling. We fill this gap in this work by synthesizing our findings and providing practical guide-lines for designing and scaling larger TSFMs with enhanced model capabilities.", "sections": [{"title": "INTRODUCTION", "content": "Time series analysis is an important piece of data mining, facilitating decision-making and scientific inference across various domains (Zhang et al., 2023). As an important analysis task, time series forecasting has long been studied and drives a wide range of practical applications, from energy, climate and quantitative finance to urban computing and system management (Jin et al., 2023; Nie et al., 2024; Wen et al., 2024). Various methods have been proposed for this task, ranging from classical statistic models (Hyndman & Athanasopoulos, 2013), bespoke dynamical models (Prado, 2020), to the more recent deep-learning based approaches (Wen et al., 2022). Despite their compet-itive performance, the methods are typically designed for specific tasks, poor to generalize to other domains (Fan et al., 2023; Rasul et al., 2023). Concurrently, we are witnessing a paradigm shift in time series forecasting from task-specific models to universal models, with the emergence of time series foundation models (TSFMs). Timer (Liu et al., 2024), Moirai (Woo et al., 2024), and more recently proposed Time-MoE (Shi et al., 2024b) show trends of scaling in both data volume and model size, aiming to achieve performance breakthroughs through more resource investment.\nThe neural scaling law quantitatively describes how model performance grows with the scaling of three basic training factors: model parameters, computational resources and training dataset size. Establishing such scaling laws is crucial for developing TSFMs, as it provides a framework for predicting expected performance gains, enabling the community to rationally allocate efforts toward key designs. The exploration on scaling laws for TSFMs is still in an initial stage; recent research has primarily focused on studying ID scaling behavior (Edwards et al., 2024; Shi et al., 2024a). In practical applications, TSFMs primarily face challenges from unseen scenarios(Wang et al., 2024),"}, {"title": "PRELIMINARY", "content": "To investigate the scaling laws of TSFMs, we curated a large, diverse, and balanced dataset for pre-training. Leveraging this dataset, we trained both encoder-only and decoder-only transform-ers and two state-of-the-art TSFMs: Chronos and Moirai. For comparative analysis, we evaluated these models on (i) in-distribution and (ii) out-of-distribution test sets, focusing on key performance metrics to examine the scaling behavior across architectures."}, {"title": "DATASETS", "content": "A large scale, diverse, balanced and high quality pre-training dataset is the foundation to build FMs. To this end, we constructed our time series corpus for TSFM pre-training from the large-scale open time series archive, Lotsa (Woo et al., 2024). The corpus comprises approximately 15B time points from 39 datasets spanning seven distinct domains. To ensure that the model performs fairly across all domains, we maintained a balanced ratio of data from different domains. Furthermore, we performed quality filtering on the corpus by constraining the signal-to-noise ratio of a time series to be greater than 20 dB, ensuring that the pre-training corpus exhibits strong predictability."}, {"title": "MODELS", "content": "TSFMs are predominantly built upon the Transformer architecture (Wen et al., 2022). For our base-line models, we selected two widely adopted architectures: the encoder-only Transformers and the decoder-only Transformers. We also include the state-of-the-art models from both architectures, Moirai (Woo et al., 2024) (encoder-only) and Chronos (Ansari et al., 2024) (decoder-only), in our study for comparison. Although Chronos-T5 is technically an encoder-decoder Transformer, we categorize it as analogous to decoder-only Transformers because it primarily relies on the decoder's auto-regressive method for prediction. The primary distinction between encoder-only and decoder-only architectures lies in the attention mechanisms applied to the inputs, as illustrated in Figure 1. To better adapt them for time series forecasting, we introduce three key modifications in input layer, positional encoding and prediction head. More details are given in appendix B.\nPatch Embedding. There are several approaches for generating inputs for transformer-based TSFMs, including point embedding, patch embedding, and lagged feature embedding. Due to the high computational cost of point embedding for long sequences and the limited robustness of lagged feature embedding, we adopt patch embedding in our models. This method, initially introduced by Vision Transformers (Dosovitskiy et al., 2020) and later adapted by PatchTST (Nie et al., 2023) for time series forecasting, divides the time series into non-overlapping segments, which are then projected into a feature space.\nRotary Position Embedding. This technique (RoPE) has rapidly gained popularity as a positional encoding method in recent large language models (Su et al., 2024). Given its proven effectiveness in improving time series forecasting performance (Woo et al., 2023), we adopt RoPE as a replacement for the original Transformer's positional encoding. RoPE encodes absolute positions using a rotation matrix while embedding relative position dependencies directly into the self-attention mechanism.\nMixture of Distributions. Our models are designed to predict the probability distribution of future time series. However, real-world time series often exhibit complex distributions, including outliers, heavy tails, and extreme skew, which pose significant challenges for accurate modeling. To address"}, {"title": "TRAINING AND EVALUATION DETAILS", "content": "In this study, we focus exclusively on univariate time series forecasting to avoid the confounding effects introduced by multivariate time series, such as variable interactions, correlations, and the complexities of modeling multivariate relationships. Future research will address these factors, aim-ing to establish more comprehensive scaling laws for multivariate time series models.\nTraining Details. Our training objective is to optimize the mixture distribution log-likelihood. We utilize the AdamW optimizer with a batch size of 128 and a cosine learning rate scheduler with a linear warm-up of 104 training steps, training for a total of 105 steps. To facilitate learning data representations across diverse domains with varying series lengths and sample sizes, we visited each sample with probability $p_i = t_i/T$, where $t_i$ is the series' time points and $T$ is the corpus' total time points. We then randomly selected a segment from each chosen sample.\nEvaluation Details. We evaluate the model on a randomly selected 10% subset of the test data every 103 steps to reduce computational costs. For performance measurement, we observed that non-normalized metrics like MAE and MSE are highly sensitive to the amplitude of time series data, often causing the overall average to be disproportionately influenced by high-amplitude datasets. To mitigate this issue, we utilize the normalized metric, mean absolute percentage error (MAPE), along with the log-likelihood loss, to assess forecasting performance."}, {"title": "SCALING LAWS FOR TIME SERIES FOUNDATION MODELS", "content": "In this section, we first present experimental results using the encoder-only Transformer to explore scaling laws across different data distributions. Following this, we conduct a comparative study on the scaling behavior of encoder-only and decoder-only TSFMs, Moirai and Chronos, to investigate how model architectures influence the scalability of time series models."}, {"title": "SCALING LAWS ACROSS DATA DISTRIBUTIONS", "content": "Parameter Scaling. In Figure 2, we display the ID and OOD performance of a wide variety of encoder-only Transformers, ranging from small models with 3K parameters to large models with 300M parameters. We trained models on the full 15B pre-training corpus to convergence and report the minimum log-likelihood loss and MAPE. We can see that both ID and OOD performance roughly follow power-law behavior over five orders of magnitude in model sizes. Formally, the power law can be expressed as:\n$L(N) \\approx (\\frac{N_c}{N})^{\\alpha_N}$\nwhere $L$ is the performance metric function (i.e., MAPE, or log-likelihood), $N$ is a given parameter count, $N_c$ is the normalization coefficient, and $\\alpha_N$ is the exponent value that indicates the degree of performance improvement expected as we scale up $N$.\nCompute Scaling. Following the similar method in (Kaplan et al., 2020), we estimate the compute budget using the formula $C = 6NBS$, where $B$ is the batch size, $S$ is the number of parameter updates, i.e. the input sequence length, and 6 is the factor to account for the forward and backward passes. The ID and OOD test loss for compute budget varying over six orders of magnitude are\n$L(C) \\approx (\\frac{C_c}{C})^{\\alpha_C}$"}, {"title": "SCALING LAWS ACROSS MODEL ARCHITECTURES", "content": "The above results suggest that the power-law captures the scaling behavior of the encoder-only Transformer in both ID and OOD scenarios. Similarly, we analyze the scaling properties of the decoder-only Transformer, along with two other state-of-the-art TSFMs, Chronos and Moirai, to assess the impact of model architectures on scaling behavior."}, {"title": "DESIGN PRINCIPLES FOR TIME SERIES FOUNDATION MODELS", "content": "Building on our findings regarding the scaling laws in TSFMs, we elaborate design principles to guide the development of effective and scalable models. These principles are framed around three key dimensions: training data, model parameters and architecture, as well as computational budget.\nTraining Data. Our experiments show that increasing the size of the training dataset leads to a greater performance improvement on OOD data compared to ID data. Enlarging the pre-training dataset is crucial for achieving better generalization. However, maintaining diversity within the dataset is equally important while increasing the data volume. Additionally, we observed that while there is a performance bias between encoder-only and decoder-only Transformers, they exhibit a consistent scaling pattern as the data size increases. This indicates that the benefits of data expansion are not influenced by model architecture, meaning that collecting more data can proceed in parallel with model improvements.\nModel Parameters and Architecture. Our study highlights that model size is the most critical fac-tor for improving TSFM performance. Among the three factors we examined, increasing the model size yielded the greatest benefit for ID forecast. In terms of architecture, an encoder-only Trans-"}, {"title": "RELATED WORKS", "content": "Neural Scaling Laws. Neural scaling laws seek to provide a predictive framework for optimizing the allocation of computational resources to maximize model performance. In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2024). In vision domains, scaling laws have been explored in areas like dis-criminative modeling (Hestness et al., 2017) and visual auto-regressive modeling (Henighan et al., 2020). Recently, Edwards et al. (2024) introduced scaling laws for large time series models, show-ing that performance scales according to a power law with model size, compute, and dataset size. Shi et al. (2024a) examined the effect of time-series forecasting horizon on model scaling behav-ior, offering a theoretical framework to explain its influence. However, both studies have focused on in-distribution scenarios, leaving the investigation of scaling laws in out-of-distribution contexts largely unexplored.\nTime Series Foundation Models. Foundation models (Das et al., 2024; Goswami et al., 2024) represent a new paradigm aimed at generalizing across diverse domains and tasks by leveraging knowledge from large-scale data in the pre-training phase. They have significantly advanced time series forecasting, particularly in zero-shot scenarios, where predictions are made on data from pre-viously unseen domains. For instance, Woo et al. (2024) introduced Moirai, an encoder-only trans-former architecture that employs an \"any-variate\" attention mechanism to capture dependencies in multivariate time series. Ansari et al. (2024) proposed a method that tokenizes time series val-ues through scaling and quantization into a fixed vocabulary, training a series of transformer-based models known as Chronos. Liu et al. (2024) developed Timer, a simple decoder-only transformer architecture designed for univariate time series forecasting, while Rasul et al. (2023) introduced Lag-Llama, a decoder-only transformer that integrates lags as covariates to improve forecasting ac-curacy. These models incorporate various modifications to the standard Transformer architecture for time series data. However, the impact of these changes on model scaling properties has not been systematically studied. As model size increases, it remains an open question whether these modifications will continue to enhance performance."}, {"title": "DISCUSSION", "content": "We also conduct the following analysis to better understand the scaling behaviors of TSFMs. Due to the space limit, see their details in Appendix D.2 to D.4 .\nEmergent Behaviors. Fig. 8 shows some examples of zero-shot OOD time series forecasting, where the model's performance remains low until the model size reaches a critical threshold, after"}, {"title": "TIME SERIES MODELS", "content": "We define time series forecasting as the following problem: given a collection of multivariate time series samples with look back window L : (x1, ..., xL) where each xt at time step t is a vector of dimension of M, our goal is to forecast T future values (xL+1,..., xL+T).\nPatch Embedding. We split the input (x1,..., xL) into M univariate time series x(i) \u2208 R1\u00d7L, independently forecasting future time series for each variate. Each univariate time series x(i) is first divided into non-overlapped patches. Specifically, given the patch length as P, the patching process will generate the a sequence of patches x(i) \u2208 RP\u00d7N where $N = \\lfloor \\frac{L}{P} \\rfloor$ is the number of patches. Then the patches are mapped to the latent space of dm via a learnable linear projection Wp \u2208 Rdm\u00d7P. In our baseline models, the patch size P is set to 32.\nRotary Position Embedding. Rotary Positional Embedding (RoPE) is a type of position encod-ing that encodes absolute positional information with a rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. In detail, RoPE incorporates absolute position information to the embedding and transform them into queries, keys through func-tion:\nfq,k(xm, m) = Ro,mWq,kxm\nwhere\n$R_{R^{d}_{m}} = \\begin{bmatrix} M_1 & \\\\ M_2 & \\\\ & \\ddots & \\\\ & & M_{d/2} \\end{bmatrix}$, $M_j = \\begin{pmatrix} cos m\\theta_j & - sin m\\theta_j \\\\ sin m\\theta_j & cos m\\theta_j \\end{pmatrix}$"}]}