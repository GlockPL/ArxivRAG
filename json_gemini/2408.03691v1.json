{"title": "Generative Design of Periodic Orbits in the Restricted Three-Body Problem", "authors": ["Alvaro Francisco Gil", "Walther Litteri", "Victor Rodriguez-Fernandez", "David Camacho", "Massimiliano Vasile"], "abstract": "The Three-Body Problem has fascinated scientists for centuries and it has been crucial in the design of modern space missions. Recent developments in Generative Artificial Intelligence hold transformative promise for addressing this longstanding problem. This work investigates the use of Variational Autoencoder (VAE) and its internal representation to generate periodic orbits. We utilize a comprehensive dataset of periodic orbits in the Circular Restricted Three-Body Problem (CR3BP) to train deep-learning architectures that capture key orbital characteristics, and we set up physical evaluation metrics for the generated trajectories. Through this investigation, we seek to enhance the understanding of how Generative AI can improve space mission planning and astrodynamics research, leading to novel, data-driven approaches in the field.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Artificial Intelligence (AI) have significantly impacted the space sector [1, 2]. AI techniques have been applied in tasks such as space traffic management [3, 4], space object characterization [5, 6], satellite pose estimation [7], natural language processing for space mission design [8], and spacecraft operations using large language models [9]. Research on large pre-trained models has also emerged in astronomy, with applications in both generative and discriminative tasks. Notable models include AstroCLIP [10], which uses cross-modal contrastive learning for astronomical images and spectra, and ASTROMER [11], a transformer-based model for creating representations of light curves in a self-supervised manner.\nIn astrodynamics, machine learning, especially deep learning, has seen significant growth. Models are used to learn guidance or control laws [12], and deep learning has been applied to design complex trajectories in multi-body dynamics [13], the solution of two-point boundary value problems [14] and the classification of regular and chaotic motion [15]. Self-supervised learning techniques have shown effectiveness in tasks such as conjunction screening and maneuver detection, revealing potential for data-driven approaches in orbit analysis [16].\nIn the context of the Three-Body Problem (3BP), AI has been used to tackle complex challenges. Support vector machines classified trajectories in the circular restricted three-body problem [17], deep neural networks solved the chaotic three-body problem [18], and artificial neural networks predicted periodic orbits in three-body systems with arbitrary masses [19]. However, generative AI techniques have only recently been applied to astrodynamics [20, 21]. This paper presents some early results from the OrbitGPT project [22]."}, {"title": "1.1 The Restricted Three-Body Problem", "content": "The Three-Body Problem (3BP), introduced by Newton in 1687 [23], involves predicting the motion of three celestial bodies interacting gravitationally. Euler and Lagrange made significant contributions in the 18th century, describing equilibrium points where a small, massless particle influenced by two larger bodies (primaries) can remain stationary. Euler identified three collinear points, while Lagrange added two more points, assuming the primaries move in circular motion about their center of mass [24]. These models are known as the Restricted Three-Body Problem (R3BP) and the Circular-Restricted Three-Body Problem (CR3BP).\nThe equations of motion (EOM) for the CR3BP are:\n$\\begin{aligned}\n\\ddot{x} &= 2\\dot{y} + U_x  \\tag{1a} \\\\\n\\ddot{y} &= -2\\dot{x} + U_y  \\tag{1b} \\\\\n\\ddot{z} &= U_z \\tag{1c}\n\\end{aligned}$"}, {"title": "1.2 Periodic Orbits", "content": "Let \u0424(t, X\u2080) : R \u00d7 R\u2076 \u2192 R\u2076 be the evolution function for the dynamical system described by the equations of motion (EOM) in Eqs. 1. This function determines the state vector X(t) at any time t based on the initial state X\u2080: X(t) = \u0424(t, X\u2080). A periodic orbit with period T is a trajectory where the state vector returns to its initial state after time T: \u0424(T, X\u2080) = X\u2080.\nPeriodic and quasi-periodic orbits in the Circular Restricted Three-Body Problem (CR3BP) have fascinated scientists since the 19th century, especially with the advent of numerical computation. In 1892, Poincar\u00e9 proved that infinite periodic solutions exist for the Three-Body Problem and highlighted the potential for chaotic motion [26]. The discovery of unique families of periodic orbits in both two-dimensional and three-dimensional CR3BP began in the 1920s [27]. Within any family of periodic trajectories, characteristics such as period, energy, and stability indicators vary continuously.\nInterest in these orbits surged with the dawn of space missions, offering cost-effective and scientifically valuable opportunities. For instance, the International Sun/Earth Explorer 3 (ISEE-3) mission, launched in 1978, was the first to utilize a Halo orbit around the Lagrange point L\u2081 in the Sun-Earth system [28]. More recently, the James Webb Space Telescope, launched in December 2021, operates in a Halo orbit around the L\u2082 point in the same system [29]. NASA's CAPSTONE mission, launched in June 2022, uses a more stable Near Rectilinear Halo Orbit (NRHO) in the cislunar environment [30].\nThe analysis of periodic orbits in the CR3BP is a vibrant research area in aerospace science, contributing to our fundamental understanding of this dynamical system and enabling complex and efficient space missions."}, {"title": "1.3 Generative AI", "content": "Generative AI has become a transformative force across scientific and industrial domains, advancing data generation and pattern synthesis. Models like Generative Adversarial Networks (GANs) [31], Variational Autoencoders (VAEs) [32], and diffusion models [33] have shown remarkable capabilities in producing realistic data, including images, speech, text, and time series. These models learn underlying data distributions to generate new, realistic instances.\nLarge pre-trained models, such as GPT-4 [34] and Llama2 [35], have revolutionized natural language processing (NLP) with their ability to understand and generate human-like text, enabling applications like conversational agents and automated content creation. In computer vision, models like DALL-E [36] and Stable Diffusion [37] have demonstrated the power of generative AI in creating photorealistic images from textual descriptions, transforming our interaction with visual data."}, {"title": "1.4 Variational Autoencoder", "content": "A Variational Autoencoder (VAE) is a generative model that combines variational inference and neural networks to generate new data points. VAEs are useful for tasks like image synthesis, anomaly detection, and data compression, as introduced by Kingma and Welling [32].\nThe VAE consists of an encoder and a decoder, forming an autoencoder architecture with stochastic elements and probabilistic principles. The encoder transforms input data x into a latent representation z, producing parameters (mean \u03bc(x) and standard deviation \u03c3(x)) for the probability distribution $q_\\phi(z|x)$:\n$q_\\phi(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma(x))$"}, {"title": "1.5 Research Motivation", "content": "This paper is part of the OrbitGPT project [22], which aims to apply generative AI to astrodynamics. The primary goal is to develop a large orbit model (LOM) that generates orbital trajectories with desired features, reducing the need for conventional design or orbit determination algorithms. This approach could revolutionize space mission design by generating new types of orbits, minimizing mission analysis costs, capturing past orbital knowledge, and training specialized models through synthetic data generation."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Dataset", "content": "For this study, we use an extensive dataset precomputed by NASA, comprising 44,112 periodic initial conditions in the Earth-Moon CR3BP, classified into 40 families of orbits [38]. The dataset includes various types of orbits, such as those located at the libration points (e.g., planar Lyapunov, Axial, Halo, and Vertical Orbits) and those developing around the entire system (e.g., Butterfly, Dragonfly, planar Distant Retrograde and Prograde Orbits, and Long Period Orbits).\nThe initial dataset from NASA consists only of the initial conditions of the orbits, along with their periods, stabilities, and Jacobi constants. To obtain the full vector of positions for each orbit, the initial conditions are integrated over time for one period using 100 nodes (N=100) with Matlab's ODE113 solver [39], with both absolute and relative tolerances set to 1\u00d710\u207b\u00b9\u00b3.\nThe dataset is structured in a three-dimensional numpy array with a shape of data.shape = (num_orbits, 7, num_time_points), where num_orbits is 44,112, indexing each distinct orbit. The second dimension (7) contains seven scalar values for each orbit at every time point: position components (posX, posY, posZ), velocity components (velX, velY, velZ), and time. The third dimension, num_time_points, represents the number of time points at which data for each orbit is recorded, initially set to 100 nodes. The dataset is standardized using the min-max method for each scalar to facilitate efficient data handling and analysis."}, {"title": "2.2 Model", "content": "The model trained on the orbit data is a Variational Autoencoder (VAE), as depicted in Figure 2. This VAE utilizes a convolutional neural network (CNN) architecture specifically designed for time series data, implemented using the TSGM library [40].\nThe encoder consists of five Conv1D layers, each followed by dropout layers to enhance the robustness of the model and prevent overfitting. After passing through the convolutional layers, the data is flattened and fed into two fully connected (dense) layers. The first dense layer has 512 neurons with ReLU activation, reducing the dimensionality of the feature space while retaining essential information. The second dense layer further reduces the feature space to 64 dimensions. This step ensures that the high-dimensional input data is compressed into a more manageable form before reaching the latent space. The output from these dense layers is then used to compute the mean (\u03bc) and log-variance (log(\u03c3\u00b2)) of the latent variable distribution. These parameters define the latent space where the encoder maps the input orbit data.\nThe decoder follows an inverse architecture to the encoder. It first passes through dense layers with 64 and 512 neurons using ReLU activation, then a final dense layer reshapes the data for the five Conv1DTranspose layers. These layers, interspersed with dropout layers for robustness, reconstruct the input sequences, ending with a Conv1DTranspose layer with a sigmoid activation to produce the final output sequences matching the input dimensionality."}, {"title": "2.3 Convergence", "content": "The generative model described above may not produce fully physical solutions, but the generated trajectories can serve as initial guesses for trajectory optimization algorithms to generate actual physical periodic orbits.\nA Multiple-Shooting (MS) algorithm iteratively adjusts a discretised trajectory {$X_i$}$_{i=1}^N$, to satisfy a set of constraints:\n$F(\\vec{X}) = 0  \\tag{3}$\nHere, $\\vec{X} \\in \\mathbb{R}^{N\\times 6 + (N-1)}$ includes N states {$X_i$}$_1^N$ and N \u2013 1 time intervals dt\u1d62 = t\u1d62\u208a\u2081 \u2212 t\u1d62, i = 1,..., N \u2013 1. The constraint vector F \u2208 $\\mathbb{R}^{N\\times 6}$ enforces the respect of both the dynamical equations as in Eq. 4, and the periodicity condition reported in Eq. 5.\n$\\begin{aligned}\nF_i &= X_{i+1} - \\Phi (X_i, \\Delta t_i),  \\quad i = 1, ..., N-1 \\tag{4} \\\\\nF_N &= X_N - X_1 \\tag{5}\n\\end{aligned}$\nThe notation $\\Phi (X_i,\\Delta t_i)$ stands for the state vector obtained through the numerical integration of Eq. 1 from the initial condition $X_i$ over the time interval $\\Delta t_i$.\nThe algorithm then uses a Newton-Raphson method for iterative correction:\n$\\vec{X}^{j} = \\vec{X}^{j-1} - DF^{-1} F(\\vec{X}^{j-1}), \\quad j = 1, ..., N_{max}  \\tag{6}$\nwhere $\\vec{X}^j$ is the solution at iteration j, and DF is the Jacobian of the constraint vector, at iteration j \u2013 1.\nConvergence is achieved when the constraint vector meets an assigned tolerance. A good initial guess $\\vec{X}^0$ is crucial for convergence. To ensure significant guesses, we set a maximum of 20 iterations (Nmax = 20) and used 10% of the states equally spaced in the orbit as prompts. To visualize the effect of the refinement algorithm, we plotted an example of a generated orbit and its refinement in Figure 3."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Generation", "content": "For this experiment, the training dataset sequence length, and consequently the timesteps of the generation, were set to 100. A VAE with 2 latent dimensions was trained on the dataset over several epochs. Although we believe that the loss may not be entirely meaningful for performance due to the refinement algorithm used at the end of the pipeline, the final model loss was 8.6, with a reconstruction loss of 2.8 and a KL loss of 5.8. After the training, 100 new orbits were generated by sampling from the latent space (Figure 4).\nBy examining the results, it is evident that the model has learned and represented orbits that closely resemble actual orbits in essence and shape. However, the generated orbits exhibit an inherent fuzziness not present in real data, resulting in the orbits appearing as fuzzy lines rather than distinct ones. This fuzziness is further corroborated by the physical error check, which shows an average error of 34 for each node, whereas a physical orbit should exhibit an error in the order of magnitude of 1 \u00d7 10\u207b\u00b9\u00b3 (absolute tolerance of the ODE solver). Another important aspect is that the model consistently learned that periodic orbits need to close, meaning that the final value of the position needs to be close to the initial value."}, {"title": "3.2 Latent Space", "content": "We explored the latent space without the need for dimensionality reduction, as the latent dimensions were already set to two. This exploration is shown in Figure 5. To properly represent each data point in the latent space, we plotted the mean of each distribution, as it is the most representative point. As expected, orbits of the same class are observed grouping together in the latent space, forming filament-like structures. This clustering occurs despite the absence of labels during training, indicating that the model has learned an internal representation of the orbit families and groups orbits with similar characteristics. To further corroborate this insight, we applied Gaussian Mixture Models to cluster the latent space and then applied certain metrics to quantify the extent of the clustering. For a clustering of 40 classes, the performance yielded a Normalized Mutual Information (NMI) score of 0.78 and an accuracy rate of 0.56.\nFurthermore, we had a list of three features\u2014Jacobi constant, period, and stability\u2014for every orbit in the training dataset. We decided to plot the average of these features for each line in the latent space, both vertically and horizontally, effectively creating a representation of the features' distribution in the latent space (Figure 5). We found that the period increases incrementally from up to down along the vertical axis. This suggests that the model has encoded the period in an unsupervised manner along this axis, reminiscent of the insight from the seminal VAE paper by Kingma and Welling [32], where moving along one dimension of the latent space caused an incremental change in the generated faces' orientation.\nRegarding the other features, no clear correlation was found. We observed high stability in the middle of the distribution, which might indicate another type of encoding. The Jacobi constant appeared to be equally distributed throughout the entire latent space, showing no specific pattern."}, {"title": "3.3 Refinement", "content": "After the generation process, the 100 synthetic orbits were refined using the convergence algorithm described in Section 2.3. We found that 46 out of the 100 generated orbits were sufficiently accurate guesses for the refinement algorithm to successfully converge, achieving a ratio of convergence of 0.46 and an average of 10.1 iterations for convergence. The refined orbits, computed from the ones shown in Figure 4, are displayed in Figure 6. Interestingly, all of the final refined orbits were new, meaning none of them were present in the training data."}, {"title": "4 Discussion", "content": "The paper presented what can be considered a first example of Generative Astrodynamics. The results in the paper demonstrate how families of periodic orbits can be encoded into a two-dimensional latent space of features and decoded back into families of approximated periodic orbits. By sampling the latent space we demonstrated the generation of approximated periodic trajectories that converge to physical ones after a local refinement. The results in this paper are evidence of the transformative potential of generative AI in astrodynamics. One particularly promising area is the use of generative models for orbit discovery. We have demonstrated the feasibility of creating a pipeline that generates physical trajectories rather than merely optimizing existing ones. This achievement represents a significant advancement not documented in the literature at the inception of this project. By exploring other AI architectures and expanding our datasets to include other planetary systems, we aim to revolutionize space mission design, minimize mission analysis costs, capture and transfer past orbital knowledge, and enable the training of specialized models through synthetic data generation."}]}