{"title": "U-SHAPED AND INVERTED-U SCALING BEHIND\nEMERGENT ABILITIES OF LARGE LANGUAGE MODELS", "authors": ["Tung-Yu Wu", "Melody Pei-Yu Lo"], "abstract": "Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where performance seems to stagnate at first and then\nimprove sharply and unpredictably with scale beyond a threshold. By dividing\nquestions in the datasets according to difficulty level by average performance, we\nobserve U-shaped scaling for hard questions, and inverted-U scaling followed\nby steady improvement for easy questions. Moreover, the emergence threshold\nroughly coincides with the point at which performance on easy questions reverts\nfrom inverse scaling to standard scaling. Capitalizing on the observable though\nopposing scaling trend on easy and hard questions, we propose a simple yet effective\npipeline, called Slice-and-Sandwich, to predict both the emergence threshold and\nmodel performance beyond the threshold.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) (Team et al., 2023; Achiam et al., 2023; Brown, 2020; Touvron\net al., 2023a;b; Workshop et al., 2022; Li et al., 2023; Jiang et al., 2024) have shown strong potential\nin various downstream applications (Jumper et al., 2021; Fawzi et al., 2022; Naveed et al., 2023;\nKaddour et al., 2023). Though the training-loss scaling law has been well established (Kaplan\net al., 2020; Hoffmann et al., 2022), the literature is inconclusive regarding how performance on\ndownstream tasks scales. In particular, for certain downstream tasks (Srivastava et al., 2023; Lin et al.,\n2022a; Pilehvar & Camacho-Collados, 2019), LLMs seem to display emergent abilities: performance\nis stagnant even when model size scales up hundredfold, and then exhibits sharp improvement at a\nseemingly unpredictable critical threshold (Wei et al., 2022; Schaeffer et al., 2024a).\nPrior work (Schaeffer et al., 2024a;b; Lu et al., 2024) has identified crude performance metrics\nas a contributing factor to LLM's apparent emergent abilities because of its inability to capture\nimprovements of smaller models. Hu et al. (2023) proposes the PASSUNTIL metric, according to\nwhich models slowly improve with scale instead of stagnating. Schaeffer et al. (2024a) finds that\nLLMs display emergent abilities mainly on string-match and multiple-choice tasks (Schaeffer et al.,\n2024a), for which the traditional performance measure of accuracy exhibits strong discontinuity.\nThey propose using a continuous metric such as Brier Score (Brier, 1950) or linear metric such as\ntoken edit distance (TED) (Schaeffer et al., 2024a) to better predict LLM's scaling behavior for\ndownstream tasks. Schaeffer et al. (2024b) further ranks several performance metrics in correlation\nwith the model scale.\nAnother focus of prior literature is the predictability of model performance on traditional metrics\nlike accuracy. Wei et al. (2022) characterizes emergent abilities as unpredictable performance soar.\nSome studies (Ruan et al., 2024; Gadre et al., 2024; Hu et al., 2023; Owen, 2024; Ye et al., 2023)\nhave proposed pipelines to estimate task-specific scaling law, while they usually incorporate models\npast the emergence threshold into the training set to fit a Sigmoid function and do not provide an\nexplainable prediction of emergence abilities.\nThis paper contributes to both fronts of the literature's discussion on emergent abilities, especially for\nmultiple-choice tasks. First, we propose a novel procedure to describe LLM's performance separately\nfor each question's difficulty level."}, {"title": "2 SCALING TREND BY DIFFICULTY LEVEL: U-SHAPE VS. INVERTED-U", "content": "This section documents LLM's scaling trend by question difficulty level. Sec. 2.1 defines termi-\nnologies such as effective model size, emergence threshold, and our performance metrics. Sec. 2.2\ndescribes how we group questions by difficulty level. Sec. 2.3 presents and discusses the results of\nthe MMLU, arithmetic, and Persian-QA, which are all multiple-choice tasks."}, {"title": "2.1 TERMINOLOGY", "content": ""}, {"title": "2.1.1 EFFECTIVE MODEL SIZE AND EMERGENCE THRESHOLD", "content": "In this paper, we refer to the effective model size M as:\n$M = log_{10}( \\frac{C}{10^{21}} )$.\nwhere C \u2248 6ND (Kaplan et al., 2020) is the training compute, N is the number of training tokens\nand D is the number of model parameters. That is, we normalized training FLOPS C to M for clearer\nvisualization. In addition, we recognize the emergence threshold T as the effective model size M at\nwhich the model accuracy exhibits a sharp improvement, as illustrated in Fig. 1a."}, {"title": "2.1.2 CONTINUOUS PERFORMANCE METRICS", "content": "Prior work (Schaeffer et al., 2024a;b; Lu et al., 2024) has advocated for performance metrics that\ndistinguish finer differences. One candidate metric is the Brier Score (Brier, 1950):\n$Brier = \\frac{1}{K} \\frac{1}{C} \\sum_{t=1}^{K} \\sum_{i=1}^{C} (\\hat{p}_{t,i} - p_{t,i})^2$,\nwhere K is the number of samples and C is the number of classes. Pt,i is 1 if the t-th sample belongs\nto class i, otherwise 0. \u00cet,i is the model's predicted probability of the t-th sample being class i.\nHowever, the Brier Score depends not only on the model's predicted probability of the target choice\nbut also on the model's predicted probability distribution on all other choices. Since there is no a\npriori reason which type of distribution on non-target choices signify better ability\u00b9, we propose the\nbinary Brier Score that depends only on the predicted probability of the target choice and has an\nopposite sign as (Eq. 2) so that higher score means higher performance:\n$Binary\\_Brier = -\\frac{1}{K} \\sum_{t=1}^{K} (\\hat{p}_{t,c} - 1)^2$,\nwhere fpt,c is the model's output probability on t-th sample's correct choice c. For a two-class\ncategorization where the correct choice is one class, and all incorrect choices form the other, Eq. 3 is\njust the standard Brier Score with the sign flipped.\nWe further modify the definition by using the model's predicted probability conditional on available\nchoices, i.e., replacing fpt,c in Eq. 3 with\n$\\frac{p_{t,c}}{\\sum_{c'\\in available choices} p_{t,c'}}$\nIn fact, the relation of Brier Score with accuracy measure could be noisy due to insufficient confidence\ncalibrations (Hendrycks et al., 2021; Kapoor et al., 2024)."}, {"title": "2.2 GROUPING QUESTIONS BY DIFFICULTY LEVELS", "content": ""}, {"title": "2.2.1 MEASURING QUESTION DIFFICULTY LEVEL", "content": "For a sample question q of a downstream task, we define its difficulty level D\u2081 to be the average\nperformance on q based on the chosen performance metric (binary Brier Score in this paper) across\nall L LLM models smaller than the emergence threshold T for that downstream task:\n$D_{q} = \\frac{1}{L} \\sum_{i=1}^{L}Binary\\_Brier_{i,q}$,\nwhere Binary_Brier is the binary Brier Score of i-th LLM on sample question q, defined by Eq. 3."}, {"title": "2.2.2 QUESTION SORTING AND GROUPING", "content": "Because model performance on individual questions is quite noisy, we group questions by difficulty\nlevels. First, we sort questions by ascending difficulty level. Then, we evenly divide the sorted\nquestions into G groups. Thus, each group has a different difficulty level."}, {"title": "2.3 U-SHAPED AND INVERTED-U SCALING", "content": "Fig. 1a-3a show the scaling trend of accuracy on the MMLU, Persian-QA, and arithmetic datasets,\nwhere clear emergent phenomena are demonstrated. In contrast, Fig. 1b-3b show the scaling trend\nwhen performance is measured by binary Brier Score instead. Though the scaling trend on the\nPersian-QA dataset is smoother, scaling trend of the binary Brier Score still exhibits a sharp increase\npast the emergence threshold on the MMLU and arithmetic datasets.\nFig. 1c-3c show the scaling trend of the binary Brier Score with group number G = 10. More\nimplementation details are in App. A. Model performance on easier questions, such as 0_1404_brier\nin Fig. 1c and 0_105_brier in Fig. 2c, displays an inverted U-shape, followed by steady improvement,\ni.e. performance first increases with scale, then worsens with scale, followed by a second ascent. This"}, {"title": "3 POSSIBLE EXPLANATION FOR U-SHAPED AND INVERTED-U SCALING", "content": "We provide a possible explanation for the initially opposing scaling trends (inverted-U vs. U-shaped)\non easy vs. hard questions using the AI community's previous findings (Nakkiran et al., 2021; Wei\net al., 2023; McKenzie et al., 2023) in deep neural networks (DNNs)' and specific LLMs' behaviors."}, {"title": "3.1 SCALING TREND OF EASY QUESTION GROUPS", "content": "As discussed in Sec. 2 and shown in Fig. 1-3, for a downstream task with emergent abilities, model\nperformance on easy question groups first increases with scale, then decreases with scale, and finally\nreverts to increasing with scale. Fig. 4a illustrates the scaling trend if we flip the sign on the binary\nBrier Score so that a higher number means higher prediction loss. The pattern is then consistent\nwith the deep double descent phenomenon identified in Nakkiran et al. (2021). In the context of\ntesting error scaling law, Nakkiran et al. (2021) argues that initially, the bias-variance trade-off in the\nclassical statistical learning theory (Hastie et al., 2009) applies, which forms the \u201cclassical regime\u201d:\ncomplex models suffer from \u201coverfitting\u201d and thus, once complexity exceeds a certain threshold,\nmodels become over-sensitive to sample noises, and the effect from such bigger variance dominates\nthe effect of further reducing testing error. On the other hand, once the model is big enough (the\n\"modern regime\"), further increase in complexity allows the model to pick from more and more"}, {"title": "3.2 SCALING TREND ON HARD QUESTION GROUP", "content": "In contrast to the easy question groups, performance in hard question groups exhibits U-shaped scaling.\nMckenzie et al. (2023); Wei et al. (2023) have identified U-shaped scaling of LLM performance in\nsome downstream tasks, as illustrated in Fig. 4b. Wei et al. (2023) provides a potential explanation\nfor the initial inverse scaling: these tasks might contain a \u201cdistractor task\" that attracts models\nto learn to solve at first, and thus larger models perform worse. One such example is the NeQA\ntask (McKenzie et al., 2023), which negates each multiple-choice question in the OpenBookQA\ndataset (Mihaylov et al., 2018) to examine whether models would be misled by the negation. It turns\nout that model performance would first decline from random guesses because of the attempt to answer\nthe non-negation part of the question."}, {"title": "4 SLICE-AND-SANDWICH", "content": ""}, {"title": "4.1 PROBLEM FORMULATION", "content": "We aim to predict the performance soar of traditional metrics before it happens. Specifically, we want\nto use only data before the emergent threshold to forecast the incidence of emergent abilities and\nthe scaling trend past the emergent threshold. We compare the performance of our pipeline with the\ncurrent iconic baseline of Sigmoid-based task-specific scaling law (Ye et al., 2023), which uses the\nSigmoid function to regress accuracy on effective model size M."}, {"title": "4.2 PIPELINE OVERVIEW", "content": "Fig. 5 shows the overall pipeline of Slice-and-Sandwich. We use models smaller than the emergence\nthreshold T as the training set. As performance no longer stagnates with scale once we group\nquestions by difficulty level, we fit the scaling trend of a continuous metric (binary Brier score in this\npaper) on the easiest question group and hardest question group separately and use the fitted scaling\ntrend to forecast performance (measured in binary Brier Score) on easy and hard questions past T."}, {"title": "4.3 PREDICTING EMERGENT ABILITY", "content": ""}, {"title": "4.3.1 QUESTION GROUPING", "content": "To reduce data noise, we group questions into G = 3 difficulty levels for Slice-and-Sandwich. Fig. 6\nshows the binary Brier Score scaling trend for the easy, medium, and hard question groups. Notably,\nthe scaling trend of the medium group resembles that of aggregate performance: it stagnates for a\nwide range of model sizes and then starts to improve sharply. By looking at the scaling trend for each\ndifficulty level separately, the medium group's pattern may simply be aggregating the scaling trend\nbetween easier and harder question groups."}, {"title": "4.3.2 FITTING AND FORECASTING SCALING TREND OF EASY VS. HARD QUESTIONS", "content": "We use simple polynomial functions to fit the scaling trend of the binary Brier Score of the easy\nand hard question groups separately, using models before the emergence threshold T. We denote by\nF(x) and F(x) the fitted scaling trend of the easy and hard question groups, respectively, where\nx is the effective model size. We then use F(x) and F(x) to forecast performance (measured in\nbinary Brier Score) on the easy and the hard question groups of models with size x above T."}, {"title": "4.3.3 \u041e\u0412TAINING SCALING TREND IN TRADITIONAL METRIC", "content": "Since what people usually care about ultimately are those traditional metrics such as accuracy (Hu\net al., 2023), our last step is to project the forecast scaling trend in binary Brier Score, Fc(x), back\nto scaling trend in accuracy, denoted by Ft (x). One can replace binary Brier Score with other\ncontinuous metrics and accuracy with other traditional metrics. Specifically, we first estimate the\nrelation between the continuous metric (binary Brier Score) and the traditional metric (accuracy)\nusing models with sizes smaller than the emergent threshold T as the training set. We denote the\nestimated mapping from binary Brier Score to accuracy as G(.).\nOur forecast of scaling trend of accuracy is given by:\n$F_{t}(x) = G(F_{c}(x)) + C'$\nwhere C is a constant such that the average predicted accuracy of Ft (x) on the training set is the\nsame as the average true accuracy of all models in the training set."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 FITTING SCALING TREND OF EASY GROUP AND HARD GROUP", "content": "Fig. 7 shows the fitted scaling trend of the easy and hard question groups on the MMLU, arithmetic,\nand Persian-QA datasets. Though the fitted scaling trend might either be underestimated, e.g., hard\ngroup of MMLU (Fig. 7a) and Persian-QA (Fig. 7c), or overestimated, e.g., easy group of MMLU"}, {"title": "5.2 RELATION BETWEEN ACCURACY AND BINARY BRIER SCORE", "content": "Fig. 9 shows the close and almost linear relation between accuracy and binary Brier Score. As a result,\nsimple ordinary least squares (OLS) regression using only models before the emergence threshold\nyields a precise mapping G(\u00b7) from binary Brier Score to accuracy."}, {"title": "5.3 FORECASTING SCALING TREND IN ACCURACY", "content": "Finally, Fig. 10 shows the accuracy-based scaling trend, Ft(x), obtained through Eq. 7 with G(\u00b7),\ntogether with the baseline of fitting performance measured in accuracy on the Sigmoid function (Owen,\n2024; Ruan et al., 2024). Instead of directly assuming a monotone scaling trend with the Sigmoid\nfunctional form, our Slice-and-Sandwich approach allows data to speak for itself. For the MMLU\nbenchmark, our approach captures the forthcoming soaring trend, whereas the baseline approach\ndoes not. For the arithmetic dataset, though the baseline provides a seemingly decent forecast, it does\nnot capture the sharp increase in the improvement speed at all, whereas our approach does. In short,\nour Slice-and-Sandwich approach could be more explainable and capable of capturing the soaring\ntrends of emergent abilities. More experimental results on parameter robustness and the alternative\nmethod of Slice-and-Sandwich are in App. F."}, {"title": "6 CONCLUSIONS, LIMITATIONS, AND FUTURE WORK", "content": "Conclusion. We propose to separately analyze scaling trends of LLM's downstream performance for\nquestion groups with different difficulty levels. For six classical multiple-choice tasks with emergent"}, {"title": "Limitations and Future Work", "content": "This work focuses on multiple-choice tasks. To apply our framework\nto other types of downstream tasks with emergent abilities, it is important and maybe not straight-\nforward to identify a continuous metric that (1) differentiates easy questions from hard questions,\nand (2) is highly correlated with the traditional metric that people are ultimately interested in. We\ndemonstrate this point in our preliminary analysis for string match tasks (App. G). We believe this to\nbe a valuable avenue for future work."}]}