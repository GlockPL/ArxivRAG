{"title": "Explanations of Deep Language Models Explain Language Representations in the Brain", "authors": ["Maryam Rahimi", "Yadollah Yaghoobzadeh", "Mohammad Reza Daliri"], "abstract": "Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction those with higher attribution scores exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility.", "sections": [{"title": "1 Introduction", "content": "Recent progress in deep learning has led to the development of autoregressive language models that excel in capturing language structures and performing human-like capabilities in various linguistic tasks [1-4]. This progress poses a critical question at the intersection of artificial intelligence and neuroscience: Are the observed similarities between language models and human cognition merely superficial, or do they stem from shared underlying mechanisms?"}, {"title": "2 Results", "content": "Are LLM-derived explanations effective in explaining language processing in the brain? We explore this question through an encoding modeling approach that predicts brain activity based on expla-nations derived from attribution methods. Specifically, we focus on explanations that clarify why an LLM makes a particular prediction, providing a transparent window into the model's decision-making process.\nWe utilized brain data from a subset of the Narratives fMRI dataset [19], which includes 147 participants who listened to four distinct audio stories. We then processed the same stories using LLMS with a sliding window approach, where each model separately predicted the next word based on the preceding context. Attribution methods a class of XAI techniques were applied to quantitatively assess how specific words influence the model's decision-making (i.e., predicting the next word). These methods assign importance scores to each word, reflecting its impact on the LLM's predictions. The scores were subsequently organized into a matrix to create a feature space (Fig. 1b)."}, {"title": "2.1 Large Language Models' Explanation", "content": "In this study, we leverage feature attribution methods, a key class of XAI techniques, to gain insights into how LLMs generate predictions. These methods quantify the contribution of each token in a given context to the model's next-word prediction, providing a fine-grained view of the model's internal decision-making process. Feature attribution techniques have proven valuable across various applica-tions, including improving model interpretability [22], identifying biases [23], diagnosing model errors [24], and detecting shortcut learning patterns. In this study, we employ four distinct attribution methods, each of which is described in detail below.\nGradient Norm: This method computes the gradient of the model's output with respect to the input token embeddings and returns the L1 norm of the resulting gradient as the importance score"}, {"title": "2.2 Language Models' Explanation Predicts Brain Activity", "content": "Our results demonstrate that attribution methods effectively predict brain activity across a large bilateral cohort of voxels within the language network (Fig. 2a). Among the tested methods, Gradi-ent Norm and Gradient \u00d7 Input emerged as the most consistent and biologically plausible, providing explanations that closely align with neural activity. These methods significantly predicted brain responses in more than half of all language-related voxels across the evaluated LLMs, outperform-ing other approaches such as Erasure and Integrated Gradients (Fig. 2b, refer to Fig. Sla for results from the right hemisphere.). Given their superior alignment with brain activity and robustness across participants, we focus our subsequent analyses on Gradient Norm and Gradient \u00d7 Input.\nThe brain regions significantly predicted by these methods include the superior and middle tem-poral regions and lateral frontal areas, all of which are well-established components of the language network [34]. Across all models, peak brain scores were observed during the early stages of language processing, reaching approximately 60% of the noise ceiling in areas such as Heschl's gyrus, the supe-rior temporal gyrus (STG), and the Sylvian fissure. While higher-order language processing regions also exhibited significant predictions, their brain scores were lower compared to early processing areas (Fig. 2c; see similar results for the right hemisphere in Fig. S1b). Although no significant differences were observed between models in most ROIs, voxelwise comparisons revealed that Llama 2 significantly outperformed other LLMs in early language areas (Fig. 2d).\nThese findings underscore the effectiveness of Gradient Norm and Gradient \u00d7 Input in providing computational explanations for the neural mechanisms underlying language processing, both in the brain and in LLMs."}, {"title": "2.3 Large Language Models' Explanation Outperforms Internal Representations in Language-Related Regions", "content": "After evaluating the performance of attribution-based explanations in predicting brain activity, we compared their efficacy with internal representations (activations and attention), which are widely used in prior literature [5-7, 11, 35, 36]. Attribution methods quantify how a model's activations change in response to input when predicting the next word in a sequence. Based on this, we hypothesize that attribution explanations would show stronger performance in brain regions that are more sensitive to input changes. Additionally, since attribution highlights word-specific importance, it is expected to primarily capture lower-level linguistic features compared to activations.\nTo test this hypothesis, we independently input the encoder with activations, attention, and attri-bution from each LLM to predict fMRI brain activity. To summarize the overall performance of each feature space, we aggregated brain scores across all LLMs and stories and identified voxels where brain scores were significantly greater than zero."}, {"title": "2.4 Language Models' Explanation Reveals the Hierarchy of Language Processing in the Brain", "content": "Having established that attribution, as a representative XAI method, effectively explains brain activity during language processing, we now examine how such explanations can enhance our understanding of the relationship between LLMs and the brain. Both systems process language hierarchically, and prior research has demonstrated hierarchical alignment between LLM layer representations and language-related brain regions [5, 8, 11, 43]. Here, we extend this investigation to LLM's explanations, exploring whether their hierarchical structure can also predict brain activity.\nA key distinction of attribution-based explanations is their ability to capture the causal influence of specific layers on model predictions, whereas activations aggregate information across layers without explicitly attributing influence. Given this fundamental difference, we hypothesize that if explanations"}, {"title": "3 Discussion", "content": "In this study, we introduce explanations from LLMs as a novel tool to bridge the gap between language processing in the brain and artificial intelligence. Our analyses across multiple attribution methods and LLMs reveal that gradient-based attribution methods provide a robust foundation for modeling brain activity during natural language comprehension. Compared to traditional internal representations like activations and attention, attribution-based explanations demonstrate stronger alignment with early-stage language processing regions in the brain. Furthermore, we observe a hierarchical correspondence, where attributions from different model layers predict brain regions involved in parallel stages of language processing. These findings not only advance our understanding of the neural mechanisms underlying language but also establish a new framework for evaluating the plausibility of explanation methods, offering broad contributions to both neuroscience and artificial intelligence."}, {"title": "Attribution vs internal feature representation", "content": "To interpret these results from a neuroscientific perspective, it is crucial to consider the fundamental differences between attribution-based explanations and previously used internal representations.\nFirst, attribution methods reveal how a model's internal representations evolve in response to changes in the input. Unlike activations, which mainly reflect how the model encodes the current input state, gradient-based attribution methods calculate the gradient of the model's output with respect to its input. This gradient captures how small changes in the input influence the model's decisions, providing insight into the dynamics of the model's internal representations. While prior work has demonstrated a strong correspondence between model activations and brain activity, our findings suggest LLMs and the brain not only encode language in a similar way but also exhibit parallel dynamics in how their representations adapt to changing input. The stronger alignment of attribution scores with early auditory processing areas, such as Heschl's gyrus and the STG, indicates"}, {"title": "Predictive coding and LLMs", "content": "A well-established claim in prior literature is that language models that achieve better next-word prediction performance tend to show stronger alignment with brain activity, which has been interpreted as evidence for predictive coding in the brain. However, this interpretation has been challenged by a recent critique from Antonello et al. [17], which raises two primary concerns. One issue is that internal representations are too general to serve as definitive evidence of predictive processing, as they encode a broad range of linguistic features beyond just prediction. Additionally, when comparing the brain alignment of layers to their next-word information (measured through perplexity), a mismatch emerges: brain alignment peaks in the middle layers, while next-word information is strongest in the final layers.\nOur findings provide a compelling response to these concerns. Unlike internal representations, attribution-based explanations are inherently task-specific, as they quantify the contribution of each input word to predicting the target word. Moreover, Antonello et al. [17] used perplexity as a measure of predictive information, assuming that layers with greater predictive information should also exhibit stronger brain alignment. However, this assumption overlooks the fact that a layer's performance in brain prediction extends beyond the predictive information within its representations. A more precise approach is required one that captures the causal influence of each layer on next-word prediction. To address this, our study introduces layer conductance as a more robust alternative to perplex-ity, as it directly measures how much each layer influences the model's prediction. Applying this method, we find that the layers most critical for next-word prediction-identified through layer con-ductance are also the ones that best align with brain activity across a higher proportion of voxels. By adopting the same reasoning as Antonello et al. [17] but using a more reliable metric, we demonstrate that the layers most causally important for prediction are also the most brain-like. This directly coun-ters the second concern raised by the critique and reinforces the idea that predictive processing in the brain follows a hierarchical pattern similar to that observed in LLMs. Together, our findings establish attribution-based explanations as a more precise tool for investigating predictive coding in the brain."}, {"title": "Evaluating AI explainability by brain alignment", "content": "Beyond their neuroscientific implications, our findings introduce a novel evaluation framework for explainability methods in AI. Evaluating explanations remains a critical and unresolved challenge in XAI, particularly as its primary goal is to help end-users better understand AI systems and make informed decisions. Existing evaluation methods typically rely on two approaches: algorithmic-based evaluations, which assess how well explanations align with the model's internal reasoning [50\u201353], and human-centered evaluations, which measure how explanations affect user understanding and decision-making [54-57]. While algorithmic approaches benefit from established metrics like faithful-ness, human-centered evaluations often suffer from biases such as confirmation bias and label leakage"}, {"title": "Future work", "content": "Our study opens several avenues for future research. While substantial research has linked deep learn-ing models' internal representations to brain activity, the alignment of explanation methods with neural function remains underexplored. Future work should expand this investigation by examining a broader range of models, attribution techniques, and brain recording modalities to identify the factors shaping this alignment, such as context length and word positioning. Understanding these factors could provide deeper insights, such as identifying patterns of input context that exert a stronger influence on brain representations or pinpointing predictive brain modules specialized for different types of lin-guistic predictions, such as those related to distinct parts of speech. Moreover, this brain-explanation alignment approach is not limited to language processing and could be extended to other cognitive domains, including vision and auditory perception, as well as multimodal models that integrate mul-tiple sensory inputs. From an XAI perspective, future research should quantitatively compare the effectiveness of brain-based evaluation frameworks with traditional behavioral evaluation methods to assess their relative strengths and limitations."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Brain Representation", "content": "We utilized three preprocessed fMRI datasets from the Narratives collection [19] for our study: \u201cPie-man\", \"Shapes\", and \" Slumlord' and 'Reach for the Stars One Small Step at a Time' \". The \"Pieman\" dataset includes recordings from 86 participants (ages 18-45, mean age 22.5 \u00b1 4.3 years, 45 reported female) who listened to a 7:02-minute story (282 TRs, 957 words). The \"Shapes\" dataset consists of recordings from 58 participants (ages 18-35, mean age 23.0 \u00b1 4.5 years, 42 reported female) and features two distinct auditory descriptions of the animation \"When Heider Met Simmel\": \"shapesphys-ical\", which provides a purely physical account of the animation, and \u201cshapessocial\u201d, which conveys intentionality in the movements of the shapes. We used the \"shapessocial\" condition in our analysis, which lasts 6:45 minutes (270 TRs, 910 words). The \"Slumlord' and \"Reach for the Stars One Small Step at a Time'\u201ddataset comprises recordings from 18 participants (ages 18\u201327, mean age 21.0 \u00b1 2.3 years, 8 reported female) who listened to two consecutive stories within the same scanning run. The \"Slumlord\" story is 15:03 minutes long (602 TRs, 2,715 words), while \"Reach for the Stars One Small Step at a Time\" is 13:45 minutes long (550 TRs, 2,629 words).\nFollowing the data quality guidelines from the original paper, we excluded noisy recordings, result-ing in a total of 150 valid recordings from 147 unique individuals, as three participants contributed to more than one dataset in separate scanning sessions.\nWe used the preprocessed unsmoothed data without applying any additional preprocessing steps. Preprocessing in the Narratives collection was performed using the fMRIPrep pipeline, which included susceptibility distortion correction, slice-timing correction, spatial normalization to the \"fsaverage\" brain template, and projection onto the cortical surface. Both smoothed and unsmoothed versions"}, {"title": "4.1.1 Brain Parcellation", "content": "All brain mapping analyses were conducted at the voxel level. However, for certain comparisons, we computed the average brain score across voxels within ROIs (Fig. 3b and 4b) to facilitate clearer interpretation. These ROIs were defined by grouping brain voxels based on their anatomical location using the Destrieux Atlas [59], which provides a standardized parcellation of the cortex into 75 regions per hemisphere. The full names of ROIs abbreviated in our analyses are listed in Table 1."}, {"title": "4.2 Language Model Representations", "content": "The alignment of brain activity with both lexical and contextual embeddings (activations) from various pre-trained deep language models has been extensively explored in prior research [5, 6]. Additionally, studies have highlighted the potential of attention weights and transformations from models like GPT-2 and BERT for predicting brain activity in response to identical inputs [7, 8].\nIn this study, we employed attribution scores to explain the outputs of language models and compared these scores to fMRI recordings from participants exposed to the same input sentences. Specifically, we focused on three language models GPT-2 (124 million parameters), Phi-2 (2.7 bil-lion parameters), and Llama 2 (7 billion parameters)\u2014chosen based on their demonstrated brain alignment, language performance, and architectural diversity.\nWe used pre-trained models available on Hugging Face [60] and extracted representations for each word in the input stories based on their preceding context. This process involved constructing sequences where each sequence included a target word along with a specified number of preced-ing words. Tokenization was performed using the Hugging Face auto tokenizer, and the tokenized sequences were fed into the models to extract word-level representations."}, {"title": "4.2.1 Feature Attribution", "content": "Feature attribution is a method used to explain the predictions of machine learning models by assigning importance scores to each input feature. In this study, we utilized feature attribution to investigate the relationship between language model outputs and brain activity. This approach allowed us to evaluate the influence of preceding words on the model's next-word predictions and compare it to human predictive processing.\nWe evaluated four different attribution methods: Gradient Norm, Gradient \u00d7 Input, Integrated Gradients, and Erasure. These methods served as explanatory representations of the models, enabling an in-depth analysis of the alignment between model predictions and neural activity.\nGradient Norm: The Gradient Norm method [25, 26] measures the sensitivity of the model's output to changes in the input token embeddings x. The importance score for each token is computed as the"}, {"title": "4.2.2 Layer Conductance", "content": "Layer conductance is an extension of attribution methods that provides a deeper understanding of the internal functioning of neural networks [46]. While traditional attribution methods focus on iden-tifying which input features most influence predictions, layer conductance measures the contribution of individual hidden units within the network. This method quantifies how information flows through neurons and accumulates across layers. It is built upon Integrated Gradients [29] and assigns attri-butions of input features, computed by Integrated Gradients, to neurons in each layer via the chain rule. The conductance of neuron y for an input feature i is defined as:\n$\text{cond}(x) = \\int_{\\alpha=0}^{1}  \\frac{\\partial f(x' + \\alpha(x - x'))}{\\partial y} \\frac{\\partial y}{\\partial x_i} d\\alpha.$\nHere, x is the input, x' is the baseline, and f is the model. The term cond(x) quantifies how much of the input contribution to the prediction passes through neuron y, with the total conductance for a layer computed by summing the importance scores of all neurons within that layer.\nIn this study, we used layer conductance to construct a feature space, employing a moving window approach similar to the one used for feature attribution. To compute conductance scores, we used the Captum library, which provides a set of conductance values for each word in the input sequence, corresponding to the contributions of different model layers. For example, when a sentence is fed into GPT-2, Captum returns a vector of 12 conductance scores per word, where each value represents the contribution of a specific model layer for that word. The conductance values were computed for each input window, resulting in vectors per layer with the same dimensions as the window. Consequently, each word in the story was represented by a vector capturing its conductance scores across all the windows in which it appeared. By concatenating these vectors, we constructed the layer conductance feature space, representing the model's internal dynamics concerning the input data.\nConductance also serves as a metric for quantifying neuron importance. While Equation 7 char-acterizes the function of a neuron in terms of its effect on the input, Equation 8 evaluates the overall importance of a neuron by aggregating its influence across all input features.\n$\\text{cond}_y = \\sum_i  \\int_{\\alpha=0}^{1}  \\frac{\\partial f(x' + \\alpha(x - x'))}{\\partial y} \\frac{\\partial y}{\\partial x_i} d\\alpha.$"}, {"title": "4.2.3 Attention Weights", "content": "In Transformer-based language models, the attention mechanism allows the model to focus on the most relevant parts of an input sequence. Each attention head computes a distinct set of attention weights, represented as square matrices known as attention maps. These maps quantify how strongly each token attends to another. Given a sequence of n tokens, the attention map for each head is denoted as $M \\in R^{n \\times n}$, where each element $m_{i,j}$ represents the attention weight from token i to token j in that head.\nWhile previous studies have primarily used activations to encode brain activity, recent research has shown that attention weights from models like GPT-2 and BERT also correlate with brain activity during language comprehension tasks [7, 8]. We selected attention weights as one of the internal representations of LLMs to compare their brain alignment in conjunction with attribution methods and activations.\nTo derive the attention-based feature space, we applied the same sliding window procedure used for attribution measurements. Following [7], we fed the model input sequences of length 11 and extracted attention maps from all layers. In line with their approach, we computed the column-wise mean of M to obtain a feature vector representing the average attention received by each token in the input sequence. For tokens that were part of the same word (e.g., subword tokens created during tokenization), we summed their averaged attention weights to derive a single score per word.\nWe then combined these word-level feature vectors across all attention heads and layers. The size of this feature space varies by model, depending on the number of layers and attention heads. For instance, GPT-2 consists of 12 layers, each with 12 attention heads. Since each attention head generates a feature vector of length 11 (corresponding to the input sequence length), the resulting attention weight feature space has dimensions W \u00d7 1584, where W is the total number of words in the story."}, {"title": "4.2.4 Activations", "content": "Each layer in a language model computes a contextual hidden state, which represents each token as a vector incorporating information from its surrounding context. Consistent with prior research [5, 6, 11, 36, 47], we utilized these contextual hidden states as one of our feature spaces.\nTo construct this feature space, we applied a sliding window to the story, where each window consisted of the tokens from the story and their preceding I tokens. The window length I was set to 1024 for GPT-2 and Llama 2, and 2048 for Phi-2. The models were then fed with these input sequences, and the corresponding hidden states were extracted, resulting in a matrix of dimensions [embedding size \u00d7 number of input tokens]."}, {"title": "4.3 Aligning Brain and Language Model Representations", "content": "Our objective was to compare the brain alignment of explanatory and internal representations of LLMs in response to natural language stimuli. To accomplish this, we employed a regression-based approach to predict fMRI recordings at each voxel for each participant using the language model representations of the stimuli.\nFollowing the methodology in [21], we matched the sampling frequency of the fMRI recordings (TR = 1.5s) with the language model representations. Representations of words presented during the same TR were summed. To account for the hemodynamic delay of approximately 6 seconds in the fMRI signal, we applied a finite impulse response (FIR) model with six delays to capture the slow BOLD response. This approach concatenated the features of the six preceding TRs with the current features. We then standardized the features and applied principal component analysis (PCA) exclusively to the attention feature space due to its high dimensionality, selecting the top 20 components for further analysis. Ridge regression with cross-validation (RidgeCV) from the scikit-learn library [64] was employed to fit the regression model.\nTo evaluate the brain alignment of each feature space, we calculated the Pearson correlation between the actual and predicted values of the BOLD signal for each voxel. This correlation served as the \u201cbrain score\", a metric used in prior studies with similar methodologies [5, 20, 21, 47]. We implemented 5-fold cross-validation to ensure the reliability and generalizability of the results.\""}, {"title": "4.4 Significance Analysis", "content": "We performed a Wilcoxon signed-rank test for each voxel across individuals to assess whether the brain score was significantly greater than zero.\nTo compare brain scores across different feature spaces and models, we first identified voxels with significant predictions. Next, we applied a multiple comparison procedure using the Friedman test to compare brain scores within voxels that were significantly predicted by all models or feature spaces. This step allowed us to identify voxels where at least one method exhibited a significantly different brain score compared to the others. For voxels identified as having significant differences, we conducted pairwise comparisons using a one-sided Wilcoxon test to determine which method achieved the best brain score.\nTo correct for multiple comparisons, we adjusted all P values using the false discovery rate (FDR) method, following the Benjamini-Hochberg (BH) procedure [65]."}, {"title": "4.5 Noise Ceiling", "content": "The recorded fMRI signal consists of three main components [33]: (i) the stimulus-related signal, which reflects neural activity consistently evoked by the stimulus across participants, typically in stimulus-relevant brain regions; (ii) the idiosyncratic signal, capturing individual-specific responses influenced by factors such as emotional state, memory, or prior experiences; and (iii) noise, which includes any variability unrelated to the stimulus, such as physiological fluctuations and scanner artifacts."}]}