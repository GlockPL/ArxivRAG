{"title": "EEG-based Multimodal Representation Learning\nfor Emotion Recognition", "authors": ["Kang Yin", "Dan Li", "Hye-Bin Shin", "Seong-Whan Lee"], "abstract": "Multimodal learning has been a popular area of\nresearch, yet integrating electroencephalogram (EEG) data poses\nunique challenges due to its inherent variability and limited\navailability. In this paper, we introduce a novel multimodal\nframework that accommodates not only conventional modalities\nsuch as video, images, and audio, but also incorporates EEG\ndata. Our framework is designed to flexibly handle varying input\nsizes, while dynamically adjusting attention to account for feature\nimportance across modalities. We evaluate our approach on a\nrecently introduced emotion recognition dataset that combines\ndata from three modalities, making it an ideal testbed for multi-\nmodal learning. The experimental results provide a benchmark\nfor the dataset and demonstrate the effectiveness of the proposed\nframework. This work highlights the potential of integrating EEG\ninto multimodal systems, paving the way for more robust and\ncomprehensive applications in emotion recognition and beyond.", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal representation learning has gained significant\nattention in the field of artificial intelligence, particularly in\ntasks that involve complex human behaviors such as emotion\nrecognition [1]. By combining data from multiple modalities,\nsuch as video, audio, and physiological signals, multimodal\nsystems can capture diverse and complementary information\nto improve model performance [2]-[4]. However, integrating\nelectroencephalogram (EEG) data into such frameworks in-\ntroduces unique challenges due to the inherent variability,\nnoise, and limited availability of EEG datasets compared to\nother modalities. Despite its potential to provide direct insights\ninto brain activity, the effective utilization of EEG data in\nmultimodal settings remains an open research problem.\nEEG data, widely used in neuroscience and clinical re-\nsearch [5], offer a non-invasive window into the electrical\nactivity of the brain. This modality has the advantage of captur-\ning cognitive and emotional states in real-time, making it par-\nticularly valuable for emotion recognition tasks [6]. However,\nEEG signals are often noisy [7], highly variable across subjects\nand sessions, and recorded at a higher dimensionality than\ntraditional modalities like video and audio. These characteris-\ntics complicate the feature extraction process and hinder the\nstraightforward integration of EEG into multimodal learning\nframeworks [8]. Existing work on emotion recognition has\nlargely focused on conventional modalities, with many studies\nopting to exclude EEG due to these challenges [9]. As a\nresult, the full potential of EEG data in improving emotion\nrecognition systems remains underexplored [10], [11]. Recent\nadvances in multimodal learning have explored various fusion\nstrategies to integrate heterogeneous data sources, such as\nfeature concatenation [12], attention mechanisms [13], and\njoint embedding spaces [14]. These methods have shown\npromise in combining modalities like video and audio, where\ndata characteristics are more homogeneous. However, the\nintegration of EEG data presents a distinct set of limita-\ntions [15]. Current models often struggle to dynamically adjust\nto the variability in feature importance across different modal-\nities [16], especially when EEG is involved. Additionally,\nexisting multimodal systems may not be flexible enough to\nhandle the varying input sizes and feature distributions that\narise when combining EEG with other sensory data [17]. This\noften results in suboptimal performance or requires manual\ntuning to accommodate the unique nature of EEG signals [18],\n[19].\nIn this paper, we propose a novel multimodal framework\nfor emotion recognition that integrates EEG, video, and audio\ndata. Our approach dynamically adjusts attention weights to\nprioritize key features from each modality and adapts to\nvarying input sizes. Evaluated on a new multimodal emo-"}, {"title": "II. METHODOLOGY", "content": "The proposed multimodal framework, illustrated in Fig. 1,\nbegins by processing video, audio, and EEG data through\nspecifically tailored transformers to extract unique features\nfrom each modality. The video input is divided into frames,\nwhich are fed into a Vision Transformer (ViT) [20] to cap-\nture spatial and temporal visual patterns. Similarly, audio\nis transformed into a spectrogram and passed through an\nAudio Spectrogram Transformer (AST) [21], designed to\nextract meaningful frequency and temporal features. EEG\ndata, represented as temporal signals, are processed by the\nEEGformer [22], a transformer-based architecture equipped\nwith a 1-D channel-wise convolutional neural network (CNN)\nto handle the high-dimensional, channel-specific nature of\nEEG signals.\nOnce each modality's distinct features are\nextracted-denoted as \\(h_{vis}\\) for video, \\(h_{aud}\\) for audio,\nand \\(h_{eeg}\\) for EEG\u2014the features are concatenated and sent\ninto a shared multi-head attention module. This attention\nmechanism allows the model to learn the importance of\neach modality's features in a dynamic and context-dependent\nmanner. Through this process, the framework is able to\nweigh the contributions of visual, auditory, and brainwave\ndata appropriately for emotion recognition. Finally, the fused\nfeatures are passed through a multi-layer perceptron (MLP)\nto output the final emotion prediction, classifying emotions\nsuch as anger, sadness, neutrality, calmness, and happiness."}, {"title": "B. Network and Implementation", "content": "In the pre-training stage, each modality's transformer is\ntrained separately on its own input domain. This modality-\nspecific training phase allows the network to focus on learning\nthe unique representations inherent to each modality without\ninterference from other data sources. By pre-training the ViT\non video frames, the AST on audio spectrograms, and the\nEEGformer on brainwave signals, we ensure that each feature\nextractor captures the most salient features relevant to its input\ntype. This modular approach helps mitigate the complexity that\narises from combining heterogeneous data sources.\nThe fine-tuning stage involves fusing the extracted features\nfrom all three modalities and processing them together in a\njoint learning framework. At this stage, we freeze the weights\nof the pretrained feature extractors to retain their modality-\nspecific learned representations and introduce a shared multi-\nhead attention decoder for further fine-tuning. This attention-\nbased fusion mechanism dynamically adjusts the importance\nof each modality by attending to the most relevant features\nbased on the context of the emotion recognition task. The\nmodality-specific features \\(h_{vis}\\), \\(h_{aud}\\) and \\(h_{eeg}\\) are first flat-\ntened and then input into the multi-head attention decoder,\nwhich learns to combine and prioritize these features for\noptimal emotion classification."}, {"title": "III. EXPERIMENT", "content": "The EAV [23] dataset, recently released, includes 42 sub-\njects with 30-channel EEG, video, and audio recordings, each\ncontributing 200 interactions, with 20-second trials during\nboth listening and speaking tasks. While 200 EEG trials and\nvideo clips are available, only 100 audio files correspond\nto speaking-only interactions. This is the first public dataset\ncombining EEG, audio, and video for emotion recognition in\na conversational setting.\nFollowing the authors' preprocessing methods, we evaluate\nthe performance of classic transformer encoders and our\nproposed multimodal framework at the subject level."}, {"title": "B. Subject-wise Task Performance", "content": "Table I presents the performance of each unimodal model\nalongside our proposed multimodal framework, evaluated on\na subject-wise basis. We benchmark the EAV dataset using\nmultimodal inputs, achieving a performance of 70.86 % in\naccuracy. This represents an improvement of 3.64 % over\nthe vision-only modality, 12.69% over the audio modality,\nand 17.35 % over the EEG modality. These results highlight\nthe advantage of integrating multiple modalities, especially\ngiven the limitations of unimodal approaches. As previously\nreported in [23], using unimodal data for emotion recog-\nnition-particularly EEG and audio-does not yield highly\nsatisfying results. Our experiments show that EEG data alone\nachieves an average accuracy of 53.51 %, while audio data\nreaches 58.17 %. In contrast, video data performs significantly\nbetter, achieving an accuracy of 67.22 %, underscoring its\nprominent role in emotion recognition tasks.\nThe discrepancy in performance across modalities sug-\ngests that visual information carries more distinctive cues\nfor emotional state classification, as video captures a broad\nrange of non-verbal signals such as facial expressions and\ngestures. From our case studies, we observed that despite\nthe experimental design, which required subjects to remain\nfully engaged during both speaking and listening tasks, the\nemotional content in the audio recordings appeared subdued.\nIn contrast, video clips revealed more nuanced differences\nacross emotional classes, such as micro-expressions and body\nlanguage, which were more reliably captured by the vision-"}, {"title": "IV. CONCLUSION", "content": "We propose a two-stage, end-to-end multimodal framework\nthat integrates EEG, video, and audio inputs, enabling the joint\nlearning of different modalities for emotion recognition tasks.\nLeveraging transformers as the backbone for each modality,\nour framework effectively captures and fuses modality-specific\nfeatures, allowing for a more comprehensive understanding\nof emotional states. The effectiveness of this approach is\ndemonstrated through extensive benchmarking on the newly\nintroduced EAV dataset, which is specifically designed to\nsupport multimodal learning for emotion recognition.\nOur proposed framework is both simple and highly effec-\ntive, offering a strong baseline for future researchers to build\nupon. By presenting this framework, we hope to inspire further\nexploration into EEG-based multimodal learning, encouraging\nthe research community to not only benchmark on this dataset\nbut also delve deeper into the rich possibilities offered by\nintegrating diverse modalities. Through this work, we aim\nto contribute a foundational model that can guide future\nadvancements in multimodal emotion recognition and EEG-\nbased research."}]}