{"title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models", "authors": ["Fengli Xu", "Qianyue Hao", "Zefang Zong", "Jingwei Wang", "Yunke Zhang", "Jingyi Wang", "Xiaochong Lan", "Jiahui Gong", "Tianjian Ouyang", "Fanjin Meng", "Chenyang Shao", "Yuwei Yan", "Qinglong Yang", "Yiwen Song", "Sijian Ren", "Xinyuan Hu", "Yu Li", "Jie Feng", "Chen Gao", "Yong Li"], "abstract": "Language has long been conceived as an essential tool for human reasoning. The\nbreakthrough of Large Language Models (LLMs) has sparked significant research\ninterest in leveraging these models to tackle complex reasoning tasks. Researchers\nhave moved beyond simple autoregressive token generation by introducing the\nconcept of \"thought\" a sequence of tokens representing intermediate steps in\nthe reasoning process. This innovative paradigm enables LLMs' to mimic com-\nplex human reasoning processes, such as tree search and reflective thinking. Re-\ncently, an emerging trend of learning to reason has applied reinforcement learning\n(RL) to train LLMs to master reasoning processes. This approach enables the\nautomatic generation of high-quality reasoning trajectories through trial-and-error\nsearch algorithms, significantly expanding LLMs' reasoning capacity by provid-\ning substantially more training data. Furthermore, recent studies demonstrate that\nencouraging LLMs to \u201cthink\u201d with more tokens during test-time inference can fur-\nther significantly boost reasoning accuracy. Therefore, the train-time and test-time\nscaling combined to show a new research frontier\u2014a path toward Large Reason-\ning Model. The introduction of OpenAI's o1 series marks a significant milestone\nin this research direction. In this survey, we present a comprehensive review of\nrecent progress in LLM reasoning. We begin by introducing the foundational\nbackground of LLMs and then explore the key technical components driving the\ndevelopment of large reasoning models, with a focus on automated data construc-\ntion, learning-to-reason techniques, and test-time scaling. We also analyze pop-\nular open-source projects at building large reasoning models, and conclude with\nopen challenges and future research directions.", "sections": [{"title": "1 Introduction", "content": "\"If there is a severe deficit of language, there will be severe deficit of thought\" \u2014 Noam Chomsky\n\nFueled by advancements in deep learning and the availability of web-scale datasets, Large Language\nModels (LLMs) have emerged as a transformative paradigm on the path toward Artificial General\nIntelligence (AGI). These massive AI models typically adopt Transformer architecture and are pre-\ntrained on large-scale text corpus with the next-token prediction task [191]. The neural scaling law\ndemonstrates that their performance improves significantly as the model size and training data in-\ncrease [59]. More importantly, LLMs also unlock remarkable emergent abilities that are absent in\nsmaller models [159], such as in-context learning [33], role playing [124] and analogical reason-"}, {"title": "2 Background", "content": null}, {"title": "2.1 Pre-trianing", "content": "As the foundational stage of training LLMs, effective pretraining is crucial for developing reason-\ning abilities. Before discussing pretraining for LLMs' reasoning, we first outline the basic process\nof general LLM pretraining. Through pretraining, LLMs not only acquire core linguistic knowl-\nedge but also gain diverse world knowledge, establishing a robust groundwork for the emergence\nof advanced capabilities and effective value alignment [191]. Typically, LLM pretraining relies on\nhigh-quality text corpora [35, 168], including extensive collections of web content, books, codes\nand other types of data. Leveraging these rich textual corpora, LLMs are built on the transformer\narchitecture that are trained with next-token prediction task. After pretraining, LLMs generally\ndemonstrate exceptional in-context learning capabilities [14], enabling them to generate coherent\ntext and provide accurate answers to a wide range of questions by utilizing their vast knowledge\nbase. Notably, the pretraining stage plays a pivotal role in cultivating the reasoning abilities of\nLLMs. For example, research [160] has shown that datasets rich in code and mathematical content\nserve as a key foundation for developing robust reasoning skills. Following this observations, newly\ndeveloped LLMs [1] begin to introduce carefully designed synthetic data for enhancing the reason-\ning abilities of LLMs. During pretraining, a critical challenge lies in balancing the proportion of\ncode and mathematical data with general text corpora to maintain strong general linguistic abilities\nwhile unlocking the reasoning potential of LLMs."}, {"title": "2.2 Fine-tuning", "content": "While pretraining enables LLMs to exhibit reasoning abilities through in-context learning, fine-\ntuning techniques are widely employed to achieve zero-shot and improved reasoning capabilities for\nLLMs. Here, we first outline the basic fine-tuning process and then explore its potential for enhanc-\ning reasoning abilities. As described in [104], after the pretraining stage, LLMs enter a supervised\nfine-tuning phase (SFT), also referred to as the instruction tuning stage. The primary goal of this\nphase is to refine the model's output style, ensuring its responses are aligned with human needs and\nreal-life applications. This is achieved by training via diverse instruction datasets that reflect a wide\nrange of everyday human interactions, typically created through extensive and carefully curated\nmanual annotation and refinement [195]. With the advent of ChatGPT, new methods have emerged\nfor generating diverse instruction datasets. These include techniques that distill data directly from\npowerful LLMs [153, 167] and automated approaches for large scale dataset construction from exist-\ning corpora [158, 32]. Using these well-crafted instruction-tuning datasets, the fine-tuning process\ncontinually uses the next-token prediction objective, similar to pretraining. However, unlike pre-\ntraining, fine-tuning specifically calculates the loss for the answers while generally ignoring the loss\nfor the questions. Additionally, incorporating datasets that include chain-of-thought (CoT) [160]\nreasoning and mathematical problem-solving examples has been shown to significantly enhance the\nreasoning capabilities of LLMs, making this an area of active research. Following the practice in\ngeneral aspects, most current approaches leverage data distillation from advanced large reasoning\nmodels, followed by fine-tuning to enhance the reasoning capabilities of LLMs to obtain the final\nlarge reasoning models."}, {"title": "2.3 Alignment", "content": "Relying solely on direct data distillation from advanced large reasoning models limits the potential\nof new LLMs. A more promising approach is to use reinforcement learning for data construction and\nmodel training, which precisely corresponds to the final alignment stage in general LLM training. In\nthe general training of LLM, the alignment phase typically involves methods such as Reinforcement\nLearning from Human Feedback (RLHF) [104] to guide the model toward generating content that\nmeets the criteria of being helpful, harmless, and honest. The goal of this phase is to enhance the\nsafety and controllability of LLMs in the reality. Compared to the former SFT phase, this stage usu-\nally incorporates a large amount of carefully curated, manually labeled ranking data to accurately\nreflect human preferences [35, 168]. This data includes not only correct demonstrations but also un-\ndesirable cases that should be avoided. Standard RLHF typically involves an SFT model, a reward\nmodel, and a aligned model, which are iteratively optimized using methods like PPO [121]. Due\nto the high data requirements and training costs of standard RLHF, methods like Direct Preference"}, {"title": "2.4 Prompting LLMs for Advanced Reasoning", "content": "Human-like reasoning is one of the most important abilities that emerge in LLMs with sufficiently\nlarge model parameters [157]. While zero-shot reasoning may remain unreliable for some tasks,\nresearchers have discovered various prompting techniques to enhance these capabilities. These tech-\nniques can be broadly categorized into three main approaches: step-by-step reasoning, multi-path\nexploration, and decomposition-based methods.\n\nThe step-by-step reasoning approach, exemplified by Chain-of-Thought prompting [160], demon-\nstrates that explicitly showing intermediate reasoning steps significantly improves problem-solving\nabilities. Even simple prompts like \u201cLet's think step by step\u201d can effectively guide the reasoning\nprocess [62]. This approach has been further refined through Self-Consistency [153], which gen-\nerates multiple reasoning paths to arrive at more reliable conclusions, and Auto-CoT [189], which\nautomates the generation of effective reasoning chains.\n\nMulti-path exploration approaches extend beyond linear reasoning by considering multiple poten-\ntial solution paths simultaneously. Tree of Thoughts [172] organizes alternative reasoning path-\nways in a tree structure, enabling systematic exploration of different solution strategies. Graph-\nof-Thoughts [11] further generalizes this to a graph structure, allowing for more flexible reasoning\npatterns and backtracking capabilities. ReAct [173] enriches this paradigm by interleaving reasoning\nwith action steps, enabling more dynamic interaction with external environments.\n\nFor complex problems, decomposition-based methods have proven particularly effective. Least-\nto-Most Prompting [196] and Algorithm of Thoughts [122] systematically break down complex\nproblems into manageable components, while Plan-and-Solve [147] provides strategic guidance for\ntackling these subproblems. These methods are especially valuable when dealing with tasks that\nrequire multiple steps or different levels of analysis.\n\nThese extensive reasoning capabilities, enhanced through structured prompting strategies, have\nproven particularly effective for tasks requiring careful analysis and systematic thinking, enabling\nLLMs to accomplish a wide variety of complex social scientifically relevant tasks. The success of\nthese methods demonstrates that while LLMs possess inherent reasoning abilities, their full potential\ncan be unlocked through careful guidance and structure in the prompting process."}, {"title": "2.5 Agentic Workflow", "content": "On top of the instruction following and in-context learning capabilities of LLMs, researchers start\nto design agentic workflows that program the \u201cthinking patterns\u201d of LLMs [137]. Such agentic\nworkflows allow researchers to enhance LLM's reasoning ability without any additional training,\nbut it often requires more test-time compute. In-context learning [33, 25] is the ability to improve\nLLM's task specific performance by simply providing a few in-context demonstrations, enables\nLLMs to efficiently generalize to unseen problems without computationally expensive trainings [14].\nAlthough the origin of such capabilities remains a largely debatable topic, recent studies suggest in-\ncontext learning improves LLMs' performance by allowing them to capture the label space, the\ndistribution of input text, and the desired format of answers [97]. Such desirable features have\nenabled researchers to adapt general purpose LLMs to diverse task scenarios, such as simulating the\nperspective of certain demographic groups through in-context role play [22]. Recent studies suggest\neffective agentic workflow can largely improve LLMs abilities for simulating human behavior [105,\n127], human-LLM interaction [89], and collaborative task solving [107]. The ability to program\nLLMs with agentic workflow lays the foundation of improving LLM's reasoning capabilities with\ncomplex cognitive architecture."}, {"title": "3 Data Construction: from Human Annotation to LLM Automation", "content": null}, {"title": "3.1 Human Annotation", "content": "The role of human annotation in constructing datasets for LLMs is indispensable. Human annota-\ntors are characterized by their meticulousness, patience, and precision, as well as their adaptability to\nnovel scenarios and capability to handle ambiguous data effectively [98]. Zhou et al. [195] demon-\nstrate that even with minimal human-annotated data, models can achieve strong performance, high-\nlighting the critical role of carefully curated annotations in model effectiveness. Human-annotated\ndata plays a pivotal role in enhancing the reasoning capabilities of large language models. In the\ncontext of Reinforcement Learning with Human Feedback (RLHF)[104], preference data from hu-\nman annotators enables LLMs, initially trained on general text corpora, to align with intricate human\nvalues and complex ethical considerations. This generalizable approach of annotation helps in fine-\ntuning models for specific tasks. Building on this foundation, Lightman et al.[75] demonstrated the\nefficacy of using human annotators to evaluate the reasoning quality at each step of mathematical\nreasoning processes, significantly improving the accuracy of LLM reasoning. This highlights how\nhuman annotation can bridge the gap between general training data and domain-specific challenges,\nsuch as complex reasoning tasks.\n\nEnhancing reasoning capabilities in LLMs requires process supervision, where human annotators\nguide each step of the reasoning process [75]. However, such supervision demands extensive human-\nannotated data, making it resource-intensive and unsustainable. Given that LLM training typically\nrequires terabytes of data, the volume of which is critical for model performance, constructing\ndatasets purely through manual annotation becomes increasingly impractical. This highlights the\nneed for alternative approaches to improve reasoning without relying solely on human annotation.\nOne promising approach is the collaboration between humans and LLMs for annotation, where\nLLMs are leveraged to accelerate the process while preserving the high quality of human-generated\nannotations. Specifically, the annotation process can be divided into two stages: the pre-annotation"}, {"title": "3.2 LLM Automated Outcome Annotation", "content": "Data annotation is a challenging and resource-intensive task, particularly in scenarios requiring com-\nplex operations such as filtering, identifying, organizing, and reconstructing textual data. These tasks\nare often tedious, time-consuming, and demand significant human effort, making them a costly bot-\ntleneck in large-scale data construction efforts [142, 31]. To address these challenges, leveraging\nLLMs for data annotation provides a cost-effective and efficient alternative. With context window\nlengths exceeding 100k tokens, LLMs can effortlessly process lengthy texts and large volumes of\nstructured data [2], handling the intricate requirements of data annotation with remarkable efficiency.\nTheir strong instruction-following capabilities [187] enable them to flexibly accommodate diverse\nand complex annotation scenarios, while achieving a level of quality comparable to that of human\nannotators. By automating these demanding tasks, LLMs significantly reduce the reliance on human\nlabor, streamlining the annotation process and enhancing overall productivity [181].\n\nLLMs are capable of handling a wide variety of automated annotation tasks, ranging from simple\nquestion-answer extraction [106] to the inclusion of additional target information [161]. Without\nhuman demonstrations, LLMs rely on their powerful reasoning and in-context learning abilities to\nindependently address more complex annotation needs. For instance, Schick et al.[120] demon-"}, {"title": "3.3 LLM Automated Process Annotation", "content": "In complex reasoning tasks, each step of the model's output can significantly influence the final\nresult, making it essential to label intermediate decisions as \"correct,\" \"incorrect,\" or assign an in-\ntermediate reward, namely process annotation. However, manually labeling these steps is costly\nand time-consuming. For example, Lightman et al. [75] inputs massive manual efforts to produce\na large-scale process annotation dataset, i.e. PRM800K, which satisfies the requirement in training\nan effective process reward model (PRM) and greatly enhances the reasoning capability of LLMs.\nTherefore, automated methods are increasingly needed to efficient process annotation, ensuring scal-\nability and cost-effectiveness. Initial automated approaches hire external stronger LLMs to annotate\nthe intermediate process generated by smaller LLMs. Furthermore, Monte Carlo based method re-\nduces the reliance on external stronger LLMs, and can use weaker LLMs to complete data annotation\nand thereby train stronger LLMs via a self-reinforced manner.\n\nAnnotation with stronger LLM: As a straightforward automated labeling method, Luo et al. [84]\ndesign to utilize a more powerful external model to annotate the intermediate results of a generative\nmodel's inference process. Rather than relying on manual annotation, the method employs a pre-\ntrained, high-performance model, like GPT series, to evaluate each generated step. By leveraging the\ncapabilities of a stronger external model, this approach enhances both the accuracy and scalability\nof the labeling process, making it more feasible for large-scale tasks. However, the major limitation\nof this approach is its reliance on the highly capable external model, which means the performance\nof the labeling process is ultimately constrained by the capabilities of the external model used.\n\nAnnotation by Monte Carlo simulation: To reduce reliance on the powerful external models,\nWang et al. [148] and Wang et al. [156] propose an improved method that avoids directly scoring\nthe intermediate steps. Instead, their approaches use an external model to continue the reasoning\nfor several steps from the given intermediate output and randomly repeat this simulation process\nmultiple times. The quality of the intermediate step is then assessed based on the average outcome\nof these extended inferences. This Monte Carlo method has shown promising results in tasks such\nas mathematical problem solving and code generation."}, {"title": "4 Learning to Reason: from Supervised to Reinforcement Fine-tuning", "content": "While pre-trained models excel across various tasks, they often struggle with complex reasoning and\naligning outputs with human expectations. Fine-tuning is crucial to address these limitations, refin-\ning a model's performance on specific tasks and enhancing its reasoning capabilities. Initially, su-\npervised fine-tuning (SFT) is used, where models learn task-specific patterns from labeled datasets.\nHowever, as reasoning challenges grow, methods like reinforcement learning (RL) and Direct Pref-\nerence Optimization (DPO) offer a more effective approach, using reward models to more efficiently\nalign the model's output with human-like reasoning, fostering more coherent, responsible, and con-\ntextually aware outputs."}, {"title": "4.1 Optimizing Pre-trained LLM: Supervised Fine-tuning", "content": "Supervised Fine-Tuning is a learning technique that refines pre-trained models' capabilities for\nspecific tasks or domains using labeled data, while retains model's understanding on pre-trained\nknowledge. While pre-training allows models to learn broad, general-purpose features from mas-\nsive amounts of unstructured data, fine-tuning specializes the model by exposing it to smaller, task-\nspecific datasets with clear input-output mappings.\n\nSFT is a critical step in improving the reasoning ability of LLMs, enabling their application in down-\nstream tasks by adapting them from general-purpose systems to domain-specific tools. For example,\nLLMs like GPT [111], BERT [30], and T5 [113] are pre-trained on vast amounts of text data using\nself-supervised learning, equipping them with broad language understanding and generation capa-\nbilities. However, their outputs are not always aligned with task-specific requirements. Without\nfine-tuning, LLMs tend to perform poorly on certain reasoning tasks, such as object counting [182],\nsatellite understanding [91], and engineering questions answering [154]. Through SFT, we can\npartially address these challenges by refining the model's outputs based on labeled task-specific\ndatasets.\n\nHowever, the direct application of SFT may not fully explore the model's reasoning capabilities in\nthe desired domains, particularly in tasks that require more complex decision-making or multi-step\nproblem-solving. The introduction of CoT techniques [160] has revolutionized the SFT process, by\nexplicitly training the model to generate intermediate reasoning steps before arriving at an answer.\nWith CoT-based SFT, LLMs are encouraged to generate intermediate reasoning steps explicitly, thus\nenhancing their reasoning ability to tackle tasks that require more structured and organized thoughts.\nFor instance, ReasonBert [29] shows that fine-tuning models with reasoning chains significantly en-\nhances their performance on tasks such as math word problems and logical reasoning by incorporat-\ning step-by-step reasoning processes. Another key study [80] investigates how fine-tuning models"}, {"title": "4.2 Optimizing Pre-trained LLM: Reinforcement Learning", "content": "Due to the high reliance on expensive, high-quality labeled datasets, and high computational costs\nof SFT, reinforcement learning has emerged as a powerful alternative framework for training models\nto master reasoning processes. Unlike supervised learning, RL enables models to learn through trial\nand error reward signals, discovering optimal strategies for achieving specific objectives. As shown\nin Figure 2 (a), the model takes action based on its current state and receives feedback in the form\nof a reward signal. This feedback guides the model to update its parameters over time, optimizing\nfor cumulative rewards.\n\nClassic reinforcement learning. RL has become a critical step in the development of LLMs. In RL\nframework, the parameters of LLMs are updated based on the rewards for their actions. Specifically,\nthe value function or Q-function is updated based on the feedback of reward model, attributing\nthe credit for an action's outcome entirely to its immediate effect. This approach simplifies the\nframework, making it conceptually straightforward while enhancing the model's ability to respond\neffectively. Two key methods currently dominate RL training for LLMs: Reinforcement Learning\nfrom Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF).\n\nOuyang et al. [104] use RLHF to align LLMs with human intent. Besides, by fine-tuning GPT-3\non human-labeled demonstrations and rank comparisons, they develop a reward model predicting\nhuman annotator's preferences. It effectively aligns trained LLMs with human preferences, out-\nperforming GPT-3 in reasoning and instruction-following despite being smaller. Bai et al. [8] also\nleverage RLHF to create helpful and harmless language models. Following a Helpful, Honest, and\nHarmless framework, they fine-tune a base model, train a preference model with rejection sampling,\nand iteratively refine it with human feedback. This process produces AI assistants that excel in NLP\ntasks and demonstrate strong ethical reasoning.\n\nTo reduce the reliance on large human-labeled datasets, Bai et al. [9] propose Constitutional AI, a\nframework for training AI assistants to be helpful and harmless using principles instead of expensive\nhuman feedback. The process includes two phases: supervised learning and RLAIF. In the super-"}, {"title": "4.3 Enhancing Multi-step Reasoning with Outcome Reward Model", "content": "For complex reasoning tasks, such as mathematical problem-solving, LLMs need to perform multi-\nstep reasoning like Chain-of-Thought to ultimately reach an accurate solution. In these tasks, the\nreward feedback is typically only available after all the reasoning steps are completed and the final\nsolution is obtained. As shown in Figure 2 (b), this is known as the outcome reward model (ORM).\nIn such cases, the key to improving the LLM's reasoning ability lies in distinguishing the correctness\nand importance of intermediate reasoning steps based on the outcome rewards.\n\nClassic reinforcement learning. ReFT [143] applies the PPO [121] method from RLHF [104] to\nreasoning tasks. Based on the outcome reward model, the value function in PPO is able to infer the\ncontribution of intermediate reasoning steps. Compared to supervised fine-tuning, ReFT is capable\nof learning more diverse reasoning paths, exhibiting stronger generalization abilities in reasoning\ntasks. However, VinePPO [60] discovers that the value network trained with ORM in PPO exhibits\nsignificant bias when identifying the value of intermediate reasoning steps, a well-known challenge\nin RL called the credit assignment problem. To address this issue, VinePPO abandons the value net-\nwork in PPO and instead employs a Monte Carlo sampling method to compute unbiased estimates of\nthe value function. Experimental results demonstrate that VinePPO consistently outperforms typical\nPPO in mathematical reasoning tasks. Critical Plan Step Learning (CPL) is a method designed to en-\nhance LLM's generalization in reasoning tasks by searching within high-level abstract plans [150].\nCPL employs Monte Carlo Tree Search (MCTS) to explore different planning steps in multi-step\nreasoning tasks, and utilizes Step-APO to learn critical plan steps. This approach enables models"}, {"title": "4.4 Enhancing Multi-step Reasoning with Process Reward Model", "content": "Process Reward Model (PRM) based Reinforcement Learning represents a significant advancement\nin LLM reasoning, emphasizing the evaluation of intermediate steps rather than solely focusing on\nend-state outcomes. As shown in Figure 2 (c), the reward of PRM is distributed across each rea-\nsoning step, rather than being concentrated at the final outcomes. By providing nuanced feedback\nthroughout the reasoning trajectory, PRM enables models to optimize behavior with greater align-\nment to human preferences and complex task requirements. This approach is crucial for tasks that\ninvolve sequential decision-making, where intermediate steps or decisions are of significance for the\nfinal goal. We explore PRMs' evolution and highlight their role in improving reasoning by providing\nstep-level rewards during complex tasks.\n\nClassic reinforcement learning A series of recent works apply PRMs for mathematical or logistic\nreasoning, since a seminal work from OpenAI [75] has proven the importance of process reward.\nSELF-EXPLORE [55] uses PRMs to enhance mathematical reasoning by identifying and address-\ning \"first pits\", which are the initial incorrect steps in problem-solving. By rewarding steps that\ncorrect such errors, PRMs enable self-supervised fine-tuning without requiring extensive human an-\nnotations. This model achieves significant improvements in accuracy on mathematical benchmarks\nlike GSM8K and MATH by leveraging step-level fine-grained feedback. MATH-SHEPHERD [149]\nintroduces a PRM framework designed for step-by-step verification and reinforcement in math-\nematical reasoning tasks. By automating process supervision through MCTS-inspired methods,\nMATH-SHEPHERD eliminates the need for human annotations while ensuring high accuracy in\nmulti-step problem-solving. PRMs are employed to reinforce logical progression and correctness,\nresulting in improved performance on benchmarks like GSM8K and MATH. DeepSeekMath inte-\ngrates PRMs via Group Relative Policy Optimization (GRPO) [128], a RL algorithm that optimizes\nstep-level rewards. PRMs are used to enhance mathematical reasoning and reasoning consistency\nacross domains. By focusing on intermediate reasoning steps, DeepSeekMath achieves state-of-the-\nart performance on several benchmarks, showcasing the power of PRMs in mathematical domains.\nScaling Automated Process Verifiers introduces Process Advantage Verifiers (PAVs), a PRM variant,\nto evaluate step-level progress in problem-solving [123]. PAVs use step-level supervision to improve\nthe efficiency and accuracy of search algorithms and reinforcement learning. By focusing on steps\nthat make meaningful progress toward a correct solution, PAVs enable substantial gains in sample\nefficiency, compute efficiency, and reasoning accuracy compared to outcome reward models. This\ndemonstrates the importance of fine-grained process rewards in scaling LLM reasoning capabilities.\n\nInteractive process reward models. PRMs are also applied to interactive tasks, such as conversa-\ntion and multi-turn question answering. ArCHer employs a hierarchical RL approach using PRMs to"}, {"title": "4.5 Reinforcement Fine-tuning", "content": "Reinforcement fine-tuning (RFT) [101] is a technique recently proposed by OpenAI for customiz-\ning expert LLMs tailored to specific vertical domains. Currently, RFT remains part of a research\nprogram and the technical details have not been fully released. Available information suggests RFT\nutilizes a small amount of preference data provided by users along with a grader model to evaluate\nLLM's output. This technique enables iterative optimization of the LLM's multi-steps reasoning\ncapabilities. As a result, RFT technique can enhance LLM's strategy of reasoning through similar\nproblems in the optimized domains.\n\nThe grader model. RFT introduces the concept of a grader model to assess the outputs of LLMs.\nConsidering that reinforcement learning training typically requires a reward model to provide feed-\nback, the grader is likely analogous to a reward model, transforming textual inputs (e.g., questions\nand answers) into scalar values of reasoning quality. This suggests that the grader could act as a\nreward model trained on user-provided preference data, potentially operating as either an outcome\nreward model or a process reward model [76].\n\nData efficiency. In OpenAI's live sessions, it was mentioned that RFT can enable learning in new\ndomains with as few as dozens of user preference data. This suggests that RFT facilitates the explo-\nration of diverse reasoning paths to address tasks based on limited preference data. Such an approach\ndemonstrates remarkably high sample efficiency while mitigating the risk of overfitting [56].\n\nTraining stability. The stability of reinforcement learning training is notoriously difficult problem\nthat presents significant challenges to its broader application. Variations in random seeds or adjust-\nments to certain hyperparameters can greatly impact the training outcomes of RL. In the context of\nthe RFT project, OpenAI has announced plans to make this technology available to the public via\nAPIs, enabling users to fine-tune domain-specific expert models using their own data. This claim\npotentially indicates that RFT has achieved a level of stability sufficient for reliably fine-tuning\nlanguage models using RL techniques."}, {"title": "5 Test-time Scaling: from CoTs to PRM Guided Search", "content": null}, {"title": "5.1 Elicit Deliberate Thinking with Prompts", "content": "Beyond train-time optimization through techniques such as reinforcement learning, researchers have\ndiscovered that test-time prompting techniques like Chain-of-Thought and Tree-of-Thoughts can\nfurther enhance LLMs' capabilities [160, 153]. While simply asking models for direct answers\noften yields suboptimal results, guiding them through explicit reasoning processes at test-time sig-\nnificantly improves their performance [62]. These prompting strategies have shown remarkable\neffectiveness across various domains, from mathematical reasoning to complex decision-making\ntasks [173, 196]. The emergence of structured prompting methods like ReAct and Least-to-Most\nPrompting has demonstrated that LLMs can benefit from explicit guidance in organizing their\nthought processes, leading to more reliable and interpretable outputs [189]. Although these ap-\nproaches typically increase token consumption and computational overhead, they provide a com-\npelling complement to train-time methods by enhancing LLMs' reasoning capabilities and solution\naccuracy without requiring model parameter modifications [172, 11]. This suggests a promising\ndirection for improving LLM performance through sophisticated test-time interventions rather than\nsolely relying on model architecture or training modifications."}, {"title": "5.2 PRM Guided Search", "content": "As previously mentioned, PRM marks a significant shift from sparse outcome-based feedback to\ndetailed process-oriented supervision. Moreover importantly, PRM can also be utilized during the\ntest-time phase, where it can further boosts the model's reasoning capabilities. OpenAI ol series\nmodels stand as a prominent example of the advanced application of PRM. The new test-time scaling\nlaws suggest that inference capabilities can be effectively enhanced by increasing test-time compute,\nproviding a clear direction for the future development of LLMs. We introduce some methods applied\nduring the inference phase, as shown in Figure 3. Red hollow circles represent the reasoning paths\ndiscarded during the algorithm's exploration process in the inference phase, green hollow circles\nsignify the reasoning paths adopted during exploration, and green solid circles mark the endpoints\nof the reasoning paths once the correct answer is identified.\n\nMajority Vote: Majority vote is one of the most straight-forward strategy to generate one final\nanswer from dense test-time compute. During inference, each inference trace produces a prediction\nfor a given input. The basic idea is to select the answer which most inference traces accord with.\nThe predictions from all the models are then aggregated, and the class that appears the most (the\n\"majority vote\u201d) is selected as the final output: $f^* = \\argmax_f \\sum_y I_{\\text{final\\_ans}(y)=f}$, where I is the\nindicator function and y is each evaluation trace.\n\nTree Search [15]: Tree Search is a classic algorithm that systematically explores different choices\nby recursively constructing a search tree. It is commonly used in complex decision-making prob-\nlems, such as board games and planning tasks. Monte Carlo Tree Search (MCTS) is one of the most\nwidely used tree search methods. It consists of four main steps: Selection, Expansion, Simulation,"}, {"title": "6 Path toward Large Reasoning Model", "content": null}, {"title": "6.1 Development of OpenAI 01 Series", "content": "In September 2024, OpenAI released 01, a groundbreaking language model that represents a signif-\nicant advancement in AI reasoning capabilities, particularly excelling in complex tasks like mathe-\nmatics, coding, and scientific problem-solving. On December 20, 2024, OpenAI opened testing ap-\nplications for o3, an upgraded version of o1 [102], which is considered to have literal doctorate-level\nintelligence [7]. These model achieve remarkable results across various challenging benchmarks, in-\ncluding scoring at the gold medal level in International Mathematics Olympiad [73] and matching\nPhD-level performance in physics, chemistry, and biology questions [48]. Extensive evaluations\nshow distinct reasoning patterns of o1 series through systematic analysis of its basic reasoning ca-\npabilities. We list the key findings of existing research as follows:\n\nEffective knowledge integration. Initial comprehensive evaluations [194] demonstrate o1's struc-\ntured analytical approach and knowledge integration in fundamental problem solving tasks, achiev-\ning 83.3% success rate in competitive programming through step-by-step logical deduction, where\nthe model demonstrates clear ability to use their knowledge to decompose complex problems\nand follow formal derivation processes. The model's structured understanding and interconnected\nknowledge application is further evidenced in specialized fields like radiology and chip design,\nwhere accurate diagnosis and complex circuit analysis require integration of multiple domain con-\ncepts. Systematic assessments [68] quantitatively validate this pattern, showing 150% of human-\nlevel performance in structured analytical thinking and computational reasoning tasks. This advan-\ntage is particularly prominent in scenarios requiring knowledge integration across domains, such as\napplying physical principles to biological systems or combining statistical methods with domain-\nspecific constraints, indicating a fundamental capability in knowledge synthesis and application."}, {"title": "6.2 Open-source Attempts of Large Reasoning Models", "content": "Open-source frameworks have also made substantial strides in developing advanced reasoning capa-\nbilities for LLMs. These frameworks serve as invaluable references for researchers and developers\naiming to replicate or approximate the reasoning strengths of proprietary models like OpenAI's 01.\nIn this section, we introduce four significant open-source efforts, each of which employs distinct\nstrategies to enhance LLM reasoning (summarized in Table 2). By exploring their unique imple-\nmentations, we aim to provide insights into the diverse methodologies used to reinforce reasoning\nabilities in LLMs."}, {"title": "7 Other Test-time Enhancing Techniques", "content": "In addition to the PRM-guided search, there are numerous other techniques devised to enhance\nLLM reasoning abilities with more test-time compute"}]}