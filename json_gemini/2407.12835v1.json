{"title": "Regurgitative Training: The Value of Real Data in Training Large Language Models", "authors": ["Jinghui Zhang", "Dandan Qiao", "Mochen Yang", "Qiang Wei"], "abstract": "What happens if we train a new Large Language Model (LLM) using data that are at least partially generated by other LLMs? The explosive success of LLMs, such as ChatGPT and LLAMA, means that a substantial amount of content online will be generated by LLMs rather than humans, which will inevitably enter the training datasets of next-generation LLMs. In this paper, we evaluate the implications of such \"regurgitative training\" on LLM performance. Through fine-tuning GPT-3.5 with data generated either by itself or by other LLMs in a machine translation task, we find strong evidence that regurgitative training clearly handicaps the performance of LLMs. The ease of getting large quantities of LLM-generated data cannot compensate for performance loss \u2013 even training with a fraction of real data is enough to outperform regurgitative training. The same performance loss of regurgitative training is observed on transformer models that we train from scratch. We carry out textual analyses to compare LLM-generated data with real human-generated data, and find suggestive evidence that the performance disadvantage of regurgitative training can be attributed to at least two mechanisms: (1) higher error rates and (2) lower lexical diversity in LLM-generated data as compared to real data. Based on these mechanisms, we propose and evaluate three different strategies to mitigate the performance loss of regurgitative training. In the first strategy, we devise data-driven metrics to gauge the quality of each LLM-generated data instance, and then carry out an ordered regurgitative training process where high-quality data are added before low-quality ones. In the second strategy, we combine data generated by multiple different LLMs (as an attempt to increase lexical diversity). In the third strategy, we train an AI detection classifier to differentiate between LLM- and human-generated data, and include LLM-generated data in the order of resemblance to human-generated data. All three strategies can improve the performance of regurgitative training to some extent but are not always able to fully close the gap from training with real data. Our results highlight the value of real, human-generated data in training LLMs, which cannot be easily substituted by synthetic, LLM-generated data. Given the inevitability of having some LLM-generated data in the training sets of future LLMs, our work serves as both a cautionary tale of its performance implication as well as a call-to-action for developing effective mitigation strategies.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are trained on inexplicably large amounts of data. Although the exact training datasets are undisclosed, popular LLMs such as ChatGPT, Llama, Claude, and Mistral are believed to have been trained on a combination of content on public Internet (e.g., Common Crawl), proprietary datasets licensed from third-parties, as well as crowd-generated data (Brown et al., 2020; Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023; Anthropic, 2024). With their explosive successes come explosive adoption - people use LLMs in an ever increasing set of tasks, including writing (Noy and Zhang, 2023; Chen and Chan, 2023), coding (Chen et al., 2021), knowledge management (Lewis et al., 2020), scientific discovery (Bran et al., 2023; Vert, 2023), and many more.\nA natural consequence of such pervasive use is that, going forward, a substantial amount of content online will be created (at least partially) by LLMs. When building the next-generation LLMs, data generated by existing LLMs is likely to enter the training datasets. This produces a scenario which we refer to as Regurgitative Training, where a new LLM is trained using data that are at least partially generated by itself or other LLMs. The overarching question we seek to answer in this paper is: how does regurgitative training affect the performance of LLMs?\nRegurgitative training may be inevitable. Indeed, there is evidence suggesting that a large part of the open web is already generated by machine translation models (LLMs included, Thompson et al., 2024). Even data that are supposed to be human-generated (e.g., manual labels on crowdsourcing platforms) are often generated by LLMs (Veselovsky et al., 2023). As LLMs get better, it will be increasingly hard to distinguish between LLM-generated data from human-generated data post-hoc (Yang et al., 2023). Addition, some LLM developers have explicitly chosen to inject LLM-generated data into their training datasets. For example, Apple acknowledges that its multi-modal LLM named MM1 has been trained on instruction-response pairs generated from GPT-4 and Llama (McKinzie et al., 2024).\nA priori, the impact of regurgitative training on LLM performance is unclear. On one hand, it represents an appealing opportunity to obtain large quantities of synthetic training data at relatively low costs, thereby offering a data quantity advantage. On the other hand, however, LLM-generated data may have lower quality than real, human-generated data \u2013 they may contain more errors (Shumailov et al., 2023) or suffer more from the \"hallucination problem\" (Rawte et al., 2023). In fact, although major players in the LLM arena, including Microsoft, Google and Meta, are all reported to use synthetic data for LLM training, the"}, {"title": "Relevant Literature", "content": "Our work is closely related to self training in the semi-supervised learning literature and data augmentation in the deep learning literature, both of which are briefly reviewed in this section. As will be discussed later, although regurgitative training in LLMs is fundamentally different from the conventional schemes of self-training or data augmentation, both offer some valuable ideas that can inform our understanding of regurgitative training as well as potential approaches to manage its performance downsides."}, {"title": "2.1 Self Training", "content": "Self training is one of the classic approaches in semi-supervised learning to train a machine learning model using both labeled and unlabeled data (Scudder, 1965; Nigam and Ghani, 2000). Take classification tasks as an example, the idea is to first build a model on the labeled data via standard supervised learning procedures, obtain the model's predictions on the unlabeled data, then take the most confident predictions (e.g., data instances with most extreme predicted probabilities) and treat them as additional labeled data to re-train the model. As a way to convert some unlabeled data into labeled data, self training is useful especially when the original labeled data are scarce.\nThere is an extensive literature on self training, both in traditional machine learning (see Pise and Kulkarni, 2008; Triguero et al., 2015, for surveys of the topic) and in modern deep learning (e.g., Xie et al., 2020b).\nOne key insights from this body of work is that the effectiveness of self training depends heavily on the ability to accurately estimate \"prediction confidence\". Because predictions with high confidence are used as \"pseudo-labels\", having accurate confidence scores imply that the \"pseudo-labels\" are more likely to be correct (i.e., the same as ground-truth labels).\nRegurgitative training of LLMs resembles self training in that model-predicted pseudo labels are used to further train the model. However, it is unclear whether the conventional wisdom of self training would still apply for in the case of regurgitative training, because of the difficulty in assessing confidence of LLM outputs (Lin et al., 2023). In classification tasks, predicted class probabilities naturally serve as the measure to quantify the uncertainty in a classifier's predictions. However, LLMs are much more complex than classifiers - they generate multi-token answers in response to prompts. Unless in highly restricted scenarios (e.g., evaluating a single-digit response to the question \"what is 2+2\"), it is generally not straightforward to define or measure confidence in LLM outputs. As a result, current LLMs do not automatically produce confidence scores for their responses and uncertainty quantification in LLMs remain an open question with many ongoing research, including asking LLMs themselves for confirmation (\u201cself-reflection\u201d, Chen and Mueller, 2023), re-running the same prompt multiple times and measure internal consistency among responses (Kotelanski et al., 2023), and tapping into human expertise (Shankar et al., 2024)."}, {"title": "2.2 Data Augmentation", "content": "Data augmentation represents another approach to enrich potentially limited labeled data. It works by injecting noises into existing labeled data instances to artificially create new data instances that can be assigned the same labels. In language tasks, a common data augmentation strategy is back-translation (Yu et al., 2018), where a sentence is first translated into a different language then back to the original language to achieve paraphrasing. In vision tasks, data augmentation may involve image transformation techniques such as rotation, color / contrast modification, etc. (Cubuk et al., 2020; Xie et al., 2020a). These augmentation strategies can benefit model performance if the injected noises do not change the labels, thereby creating more training data with valid labels.\nAlthough data augmentation is procedurally quite different from regurgitative training, it does offer an insight that can help enhance the performance of regurgitative training. Xie et al. (2020a) found that data augmentation is more effective when the augmentation strategy can generate a diverse set of instances rather than only introducing small, local perturbations. Learning from a diverse set of augmented data can enable the model to achieve competitive performance with fewer examples. Conceptually, this finding is also consistent with observations made in other machine learning research outside of data augmentation (e.g., Gong et al., 2019), where the diversity of training data instances is positively associated with predictive performance. Later, we leverage this insight in one of the strategies designed to mitigate performance loss of regurgitative training, by mixing data generated by different LLMs as an attempt to introduce greater diversity to the training process."}, {"title": "2.3 Regurgitative Training", "content": "Regurgitative training of generative AI models represents a new problem that has only begun to receive schol-arly attention very recently. The earliest work we could identify is Shumailov et al. (2023), which document that using model-generated data to train next-generation models can create irreversible performance losses, a phenomenon they term \"model collapse\". They demonstrate this in common generative AI architectures such as variational autoencoders, Gaussian mixture models, and small-scale LLMs. Moreover, they provide theoretical intuitions that model collapse arises due to errors in model-generated data, which accumulates over more iterations of regurgitative training. Subsequently, the phenomenon of model collapse has also been observed in generative image models (Alemohammad et al., 2023; Bertrand et al., 2023).\nIn the meantime, efforts to mitigate model collapse are underway. Bertrand et al. (2023) show that model collapse can be avoided if (i) the proportion of real data is sufficiently high and (ii) model-generated data approximate the distributions of real data well enough. Furthermore, Gerstgrasser et al. (2024) propose to alleviate model collapse by \"accumulating data\"; that is, use the totality of real and model-generated data (rather than just the model-generated data) to train new models.\nWe build upon this nascent stream of research and aim to make several distinct contributions. First, we consider regurgitative training of a LLM not only by data generated by itself, but also by other LLMs with varying degrees of capabilities. This is already taking place in practice (e.g., McKinzie et al., 2024) but has not been systematically explored in the literature. Second, prior work such as Shumailov et al. (2023) focused on early versions of generative models (e.g., non-transformer-based models or small pre-trained models). Instead, we carry out comprehensive experiments with leading LLMs at the time of research (e.g., GPT-4 and Llama2) as well as transformer models trained from scratch, thereby providing a more up-to-date understanding of regurgitative training. Finally, we propose several new mitigation strategies beyond what has been tested so far, and empirically evaluate their effectiveness."}, {"title": "3 Performance Impact of Regurgitative Training", "content": "In this section, we aim to understand how regurgitative training affect the performance of an LLM through two sets of experiments, respectively constructed to reflect two representative practices in LLM training: (i) fine-tuning and (ii) training from scratch. Fine-tuning allows users to adapt an existing LLM to their own use cases and, as mentioned before, is a widely adopted practice in the industry (e.g., McKinzie et al., 2024). We expect a lot of LLM training will take the form of fine-tuning, because training a state-of-the-art LLM from scratch is highly complex and resource-intensive. Meanwhile, we also consider the case of training a smaller-scale LLM from scratch, which may be necessary for companies that cannot leverage third-party LLMs due to data security and privacy issues.\nFor both fine-tuning and training from scratch, we focus on machine translation as the generative task of interest. Translation represents a common application for LLMs, and the performance of a translation model can be evaluated with well-established standards and metrics in the literature. This enables us to robustly assess the performance variations resulting from regurgitative training. In the case of training from scratch, we also replicate the main findings with a different generative task, namely Q&A."}, {"title": "3.1 Experiments with Fine-Tuning", "content": "To carry out LLM fine-tuning for translation, we use the Europarl parallel corpus (Koehn, 2005). Sourced from the proceedings of the European Parliament, the corpus contains parallel sentences in multiple European languages. We specifically use pairs of German-English sentences. After basic pre-processing steps (e.g., removing special HTML tags, eliminating noisy characters, and handling null values), we end up with 1,908,849 sentence pairs for our analyses. We treat these sentence pairs as real data.\nA popular and widely used metric to evaluate the performance of a translation model is the BLEU (BiLingual Evaluation Understudy) score (Papineni et al., 2002). It evaluates the quality of a model-generated translation (also called a \"hypothesis translation\") in comparison to one or more reference translations.\nThe BLEU score is calculated as the n-gram overlap between the hypothesis translation and reference translations. It ranges from 0 to 1 and a higher BLEU score generally indicates better quality translations. Formally, the BLEU score is defined as\n$$BLEU = min \\{1, exp \\left(1-\\frac{r}{c}\\right)\\}\\ exp \\left(\\sum_{n=1}^{N} W_{n} log p_{n}\\right)$$\nIn the first term, c is the length of hypothesis translation and r is the \"effective\" length of reference translations (defined as the length of the one reference translation that best matches the hypothesis translation). This term serves as a \u201cbrevity penalty\" that assigns a higher score for a better match in lengths between hypothesis and reference translations. In the second term, $p_n$ denotes the n-gram precision and is defined as\n$$p_{n} = \\frac{\\sum_{n-\\text{gram }} Count_{matched}(n-\\text{gram })}{\\sum_{n-\\text{gram }} Count(n-\\text{gram })}$$\nwhere $Count_{matched}(n-\\text{gram })$ counts the number of n-gram matches between the hypothesis translation and reference translations. In Equation (1), the n-gram precision scores are then weighted by $w_n$ (e.g., uniform weighting $w_n = \\frac{1}{N}$) to compute the overall BLEU score.\nTo implement and evaluate regurgitative fine-tuning, several components need to be defined first, including a baseline LLM to be fine-tuned, a set of training data (either real or generated by other LLMs) used for fine-tuning, and a fine-tuned LLM for evaluation. In our context, we use the GPT-3.5 model as the baseline LLM, then fine-tune it with (i) real human-generated data, (ii) data generated by GPT-3.5 itself, and (iii) data generated by two other LLMs, namely GPT-4 and LLAMA2. This creates four fine-tuned LLMs, all of which are evaluated on the same testing data for performance comparison.\nMore specifically, we randomly select 5,000 sentence pairs from the original corpus for fine-tuning and 10,000 sentence pairs as the testing data. When fine-tuning with real data, the 5,000 German sentences are used as inputs and the corresponding 5,000 English sentences are used as target translations. When fine-tuning with LLM-generated data, the same 5,000 German sentences are used as inputs, but the target translations are generated by the corresponding LLM. For GPT-3.5, GPT-4, and LLAMA2, we obtain their translations with the same system prompt: \"You are a chatbot that can translate German to English.\", and the German sentences are given to the LLMs as user inputs. Using each set of data, we carry out progressive fine-tuning over five batches, adding 1,000 data instances per batch and recording translation performance on the testing data after each batch.\nWe show the results in Figure 1. Each line represents the translation perform of a particular model over five fine-tuned batches. The X-axis indicates batch index, marking the number of data instances utilized in the fine-tuning process. The Y-axis represents the BLEU score, where a higher value corresponds to better translation performance.\nFrom the figure, it is evident that the performance of fine-tuning with LLM-generated data (both from the baseline LLM itself and from other LLMs) clearly lags behind the performance of fine-tuning with real human-generated data. Moreover, regurgitative training with different LLMs have differential performance impact. Fine-tuning with data generated by GPT-3.5 itself does not significantly change performance, and fine-tuning with GPT-4 generated data only results in barely noticeable performance improvement over the baseline model (i.e., at point 0 on the X-axis). However, fine-tuning with LLAMA2 generated data significantly degrades performance compared to the baseline. This is likely because the three LLMs have different translation capabilities. Since we have the ground-truths translations for the 5,000 fine-tuning data, we can directly compute the BLEU scores of translations generated by the three LLMs, and indeed find GPT-4 to be the best (BLEU = 0.3454), followed by GPT-3.5 (BLEU = 0.3428) and LLAMA2 (BLEU = 0.2417).\nThese results underscore the overall negative, and potentially detrimental, effects of regurgitative training. Compared to training with real data, regurgitative training largely stalls learning. Regurgitative training with a better-performing LLM improves performance only marginally and is not sufficient to catch up with the performance on real data. Worse yet, regurgitative training with a less capable LLM can significantly hurt performance.\nNote that in the above experiments, we use utilize a small set of data for fine-tuning. This decision stems from the remarkable few-shot learning capabilities of modern LLMs (Brown et al., 2020). In addition, we also conduct a robustness check to understand whether the performance of regurgitative training may be different if more fine-tuning data are available. Specifically, we augment the fine-tuning data size by 20 times, to a total of 100,000, and incrementally add 10,000 per batch. For efficiency and cost considerations, we only run fine-tuning with real data and data generated by GPT-3.5 itself. We then evaluate each fine-tuned models on the same testing data as before. The results are presented in Figure 2. We again observe that regurgitative training is unable to improve translation performance and substantially underperforms training with real data."}, {"title": "3.2 Experiments with Models Trained from Scratch", "content": "We now turn to training models from scratch and understanding the performance impact of regurgitative training in this case. Specifically, we build transformer models using the translation data. The transformer architecture serves as a foundational component powering the majority of modern LLMs, and has found extensive applications in machine translation and a variety of other natural language tasks (Vaswani et al., 2017; Wolf et al., 2020). We therefore choose to train small-scale transformer models from scratch, as an attempt to approximate the practice of training transformer-based models without leveraging third-party LLMs.\nWe follow Vaswani et al. (2017) to build the baseline translation models. Transformer has an encoder-decoder architecture, which uses stacked layers of multi-head self-attention and point-wise, fully connected feed-forward networks for both encoder and decoder. It also employs a residual connection on each sub-layer, followed by layer normalization. For implementational details of these transformer elements, we refer to Vaswani et al. (2017).\nAs our previous fine-tuning results have shown, the performance impact of regurgitative training can vary with the capability of the model used to generate training data. Therefore, we train both a \"low-performance\" and a \"high-performance\" baseline models. This is done by gradually adding 50,000 sentence pairs (randomly sampled from the German-English corpus) per batch for training, and evaluate the model's translation performance on a fixed testing dataset of 50,000 sentence pairs. As shown in Figure 3, we observe that the model's performance improves quickly with the initial increase in training data size, and saturates after being trained with sufficient data. We choose the model trained with 50,000 data instances as our low-performance baseline and the one trained with 500,000 data instances as the high-performance baseline.\nThese two baseline models, corresponding to different performance levels, are then used to evaluate the effects of regurgitative training. We randomly sample a total of 300,000 data instances (outside of the training data of both the low- and high-performance baseline models) designated for regurgitative training. In batches of 10,000 data instances, we continue training both the low-performance baseline model and the high-performance baseline model with (i) real human-generated data, (ii) data generated by the low-performance model, and (iii) data generated by the high-performance model. After each batch of training, we evaluate the all models' performance on the same testing data of 50,000 instances. The results are presented in Figure 4.\nFor both low-performance and high-performance baseline models, regurgitative training with data gen-erated from the low-performance model clearly underperforms training with real data. The same is true for regurgitative training of high-performance model with data generated by itself, though the performance gap is fairly small. Curiously, regurgitative training of low-performance model with data generated by high-performance model actually outperforms training with real data for the first 19 batches (i.e., top two lines in the right plot). To understand whether this is a sustainable performance advantage, we sample more data to carry out another 20 batches of regurgitative training in this case. The results, shown in Figure 5, show that regurgitative training performance starts to plateau around 30 batches, and underperforms training with real data thereafter.\nThe above results further demonstrate the performance cost of regurgitative training, even when busi-nesses create and train their models from scratch. Consistent with our observations under regurgitative fine-tuning, regurgitative training with data from a more capable model is better than those from a less capable model - training with data generated from the low-performance model clearly harms performance. Regurgitative training with the more capable high-performance model can match or even surpass the perfor-mance of training with real data, but such advantages usually fade away as the size of regurgitative training data grows."}, {"title": "3.3 Replication: Question Answering Task", "content": "In addition to machine translation, we also conduct a replication study with another common generative language task - Question Answering (Q&A). We use the Stanford Question Answering Dataset (SQUAD) (Rajpurkar et al., 2016), which is a widely used benchmarking dataset for developing and testing Q&A methods. SQUAD is a reading comprehension dataset composed of questions created by crowd-workers based on a collection of Wikipedia articles, with answers being segments of texts from the corresponding passages in the articles. The dataset includes 87,599 entries in the training dataset (constructed from 442 articles) and 10,570 entries in the testing dataset (constructed from 48 other articles).\nInstead of end-to-end training (as was done in the previous section), here we use a pre-trained multilingual BERT model (bert-base-cased, Devlin et al., 2018) to extract word embeddings, which are fed into a feedforward neural network model for Q&A. Doing so allows us to evaluate regurgitative training under yet another widely adopted strategy for training generative language models (i.e., leveraging pre-trained representation models). We follow Rajpurkar et al. (2016) to evaluate Q&A performance with two metrics: Exact Match and F-1 score. Exact match measures the percentage of predicted answers that match the ground-truth answers exactly, and F-1 score measures the overlap between the predicted answers and the ground-truth answers by treating both predictions and ground-truths as bags of tokens. We calculate the average F-1 over all questions in the testing data.\nFollowing the same procedure as in the previous section, we use increasing amounts of real data to train baseline models in order to identify a low-performance model and a high-performance model (see Appendix A for detailed results). The low-performance model is trained on all entries from 40 articles and achieves 70.77% exact match rate and 80.34% average F-1 score, whereas the high-performance model is trained on all entries from 200 articles and achieves 78.63% exact match rate and 86.59% average F-1 score.\nNext we use these two baseline models for regurgitative training on entries from the remaining 242 articles (not used in training the two baseline models). In batches of 10 articles, we continue training both the low- and high-performance baselines with (i) real human-generated data, (ii) data generated by the low-performance model, and (iii) data generated by the high-performance model, for a total of 20 batches. After each batch of training, we evaluate the all models' performance on the same testing dataset provided by SQUAD. The results are included in Figure 7, where the first row shows performance of regurgitative training the high-performance baseline model and the second row shows performance of regurgitative training the low-performance baseline model.\nWe again observe that regurgitative training with model-generated data negatively affects Q&A perfor-mance, compared to training with real data. Different from the translation task, Q&A regurgitative training with data generated by the high-performance model does not improve performance (and certainly does not outperform training with real data), even though it still weakly outperforms regurgitative training with low-performance model generated data. In other words, the peril of regurgitative training is not limited to translation task and is even more severe in Q&A task."}, {"title": "4 Understanding Performance Loss from Regurgitative Training", "content": "Why does regurgitative training hurt performance compared to training with real data? In this section, we offer some preliminary evidence into the underlying mechanisms. Using the translation task as an example, we focus on characterizing the differences between LLM-generated training data and real data, and discuss how these differences may impact the performance.\nThe first mechanism is error \u2013 LLMs are not perfect and data generated by them can contain more errors than real data. New models trained on these error-prone data can therefore have inferior performance. This is also the mechanism identified and studied in prior work (Shumailov et al., 2023). We test this mechanism in the fine-tuning setting of Section 3.1 with the 5,000 data points used for regurgitative training. Recall that, with access to ground-truths for these data points, we have already calculated the BLEU scores of translations generated by GPT-3.5, GPT-4, and LLAMA2. We have confirmed that translations generated by GPT-4 have a slightly higher BLEU score than those generated by GPT-3.5, and both clearly have higher BLEU scores than Llama2-generated data. This aligns well with the testing performance of the corresponding fine-tuned LLMs (Figure 1).\nAlthough BLEU is widely used to measure translation quality, it also has an important limitation that it does not explicitly account for the semantic meaning of words. A translation that uses different words than those in the ground-truth will have a low BLEU score even if it is semantically correct. In other words, having a lower BLEU score does not necessarily mean that the translation is more erroneous. In light of this, we construct two new measures, both aiming at quantifying the semantic differences of LLM-generated data vs. real data. We take each set of training data (generated by one of the LLMs or human) and perform several pre-processing steps, including (i) lower-casing, (ii) removing punctuation, (iii) removing stopwords, and (iv) lemmatization (i.e., reducing a word to its stem form). These pre-processing steps allow us to focus only on the substantive content of each translation. The first metric is computed as the average cosine similarity between the embeddings of LLM and ground-truth translations, where the embeddings are obtained from the Sentence Transformer model (Reimers and Gurevych, 2019). After pre-processing, a smaller cosine similarity implies greater semantic discrepancies of LLM translations from the ground-truths, which is indicative of translation errors. The second metric counts the number of word tokens in a ground-truth translation that satisfy two conditions: (1) they do not show up in the corresponding LLM translation and (2) even their synonyms (retrieved based on WordNet, Miller, 1995) do not show up in the LLM translation. These non-synonymous deviations likely represent words mistranslated by LLM. Results of these two metrics are reported in the second and third rows of Table 1. We see that the two sets of GPT-generated data have higher semantic similarities with ground-truths and lower non-synonym deviations than LLAMA2, again supporting the mechanism that translation errors are partially responsible for the performance reduction of regurgitative training.\nBeyond errors, we also test a different mechanism related to lexical diversity. Several recent work suggest that LLM-generated content appears to be more homogeneous than human-generated content (Doshi and Hauser, 2023; Anderson et al., 2024; Zhou and Lee, 2024). We suspect that regurgitative training with less diverse LLM-generated data may hinder the model's ability to generalize and result in lower testing performance. We quantify lexical diversity with two metrics. The first is a straightforward count of the total number of unique word tokens in ground-truth or LLM translations. The second adopts the self-BLEU metric proposed by Zhu et al. (2018). Self-BLEU is the BLEU score of a given text against all other texts in a corpus. Because BLEU captures lexical similarity, self-BLEU accordingly reflects how similar a text is with the rest of the corpus (higher self-BLEU implies lower diversity). For LLM-generated translations, errors may artificially decrease self-BLEU without meaningfully increase lexical diversity. We therefore remove the previously mentioned non-synonymous deviations (as approximation of errors) from LLM translations. We then average self-BLEU over the 5,000 training data points. Results of both metrics are reported in Table 2. Ground-truth translations consistently use more unique tokens and have lower average self-BLEU than LLM translations. GPT translations use more unique tokens than LLAMA2 and the three LLMs have similar average self-BLEU.\nGiven the black-box nature of LLMs, we acknowledge that the exact process through which errors or lack of lexical diversity in training data affect model performance remains unclear. Nonetheless, these explo-rations provide plausible explanations for the negative performance impact of regurgitative training. More importantly, they naturally give rise to potential strategies to mitigate performance loss due to regurgitative training. We investigate a few different strategies in the next section."}, {"title": "5 Mitigating Performance Loss from Regurgitative Training", "content": "In this section, we propose and test a few strategies to mitigate the adverse performance impact of regurgitative training. Designing effective mitigation strategies requires first understanding the mechanisms of the adverse effects. Our explorations in the previous section provide suggestive evidence that errors and lack of lexical diversity may both be at play. Accordingly, we design three mitigation strategies to address one or both of these mechanisms:\n\u2022 Strategy 1 relies on quality quantification to gauge the likelihood of errors in synthetic data, and prioritize the use of data with high quality (i.e., low error likelihood) in regurgitative training;\n\u2022 Strategy 2 seeks to enhance lexical diversity by mixing together synthetic data generated by different LLMs in regurgitative training;\n\u2022 Strategy 3 builds an AI detection model to differentiate between synthetic vs. real data, and prioritize the use of synthetic data that most resemble real data for regurgitative training. As a competent AI detector may pick up on both errors and lexical diversity as predictive features, this strategy is designed to address both issues.\nDetails of each strategy and the corresponding evaluations on the translation task are discussed in the rest of this section. However, it is worth noting up front that the goal of mitigation is not to completely close the gap from the performance of training with real data - this may not be realistic in the short term. Instead, the goal is to use LLM-generated synthetic data in a more careful manner to reduce performance loss."}, {"title": "5.1 Mitigation Strategy based on Quality Quantification", "content": "The first strategy is to identify a method to assess the quality of synthetic data, and subsequently select higher-quality data for regurgitative training. This requires defining a metric that accurately measures, or at least correlates with, data quality specific to the task at hand. One such metric, commonly used in classification contexts, is prediction confidence score. Higher prediction confidence scores usually correlate with greater probability of correct predictions, and the semi-supervised learning literature routinely uses prediction confidence as a quality metric (e.g., Scudder, 1965). Because modern LLMs generate content by autoregressively predicting the next token, it is viable to also adopt prediction confidence, calculated based on predicted probabilities over the vocabulary, to quantify the quality of LLM-generated data. However, a practical obstacle is that when using third-party LLMs, prediction probabilities may not always be available. Therefore, we devise an alternative quality metric to guide the quality-based mitigation in the setting of LLM fine-tuning, assuming prediction probabilities are unavailable (Section 5.1.1). We also demonstrate the same mitigation strategy with transformers trained from scratch, assuming prediction probabilities are fully available (Section 5.1.2)."}, {"title": "5.1.1 Evaluation in Fine-Tuning Setting.", "content": "In translation task, in the absence of raw prediction probabilities, the BLEU score can be used as another metric to gauge data quality. We propose to train a supervised learning model to predict the BLEU score of a LLM-generated translation. To train such a BLEU prediction model, we randomly sample 150,000 German-English sentence pairs (not previously used in Section 3.1) and obtain the translations generated by GPT-3.5, GPT-4, and LLAMA2. For each pair of German sentence and LLM translation, we compute the BLEU score using the ground-truth translation as the reference. These LLM-generated translation pairs, along with their BLEU scores, form the labeled dataset for training the BLEU prediction model.\nThe labeled dataset is randomly split into 80% for training and 20% for testing. Each instance of the labeled dataset is structured as $input = (g_{1}, g_{2},..., g_{M}, [SEP", "SEP": "label = BLEU$, where $g"}]}