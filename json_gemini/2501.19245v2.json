{"title": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction Experiments", "authors": ["H\u00fcseyin Ayd\u0131n", "Kevin Godin-Dubois", "Libio Goncalvez Braz", "Floris den Hengst", "Kim Baraka", "Mustafa Mert \u00c7elikok", "Andreas Sauter", "Shihan Wang", "Frans A. Oliehoek"], "abstract": "Reinforcement learning (RL) offers a general approach for modeling and training AI agents, including human-AI interaction scenarios. In this paper, we propose SHARPIE (Shared Human-AI Reinforcement Learning Platform for Interactive Experiments) to address the need for a generic framework to support experiments with RL agents and humans. Its modular design consists of a versatile wrapper for RL environments and algorithm libraries, a participant-facing web interface, logging utilities, deployment on popular cloud and participant recruitment platforms. It empowers researchers to study a wide variety of research questions related to the interaction between humans and RL agents, including those related to interactive reward specification and learning, learning from human feedback, action delegation, preference elicitation, user-modeling, and human-AI teaming. The platform is based on a generic interface for human-RL interactions that aims to standardize the field of study on RL in human contexts.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) refers to a family of algorithms in which agents learn from the consequences of their own interactions with an environment to try to maximize the long-term reward obtained (Sutton and Barto 1998). The available compute has seen an increase in the use of RL methods in a wide variety of problems, many of which involve humans in some sense (Den Hengst et al. 2020). RL agents interact with humans in a wide variety of ways: in some problem settings, RL agents observe humans to achieve a common goal or act on their behalf (Natarajan et al. 2010); in other settings, RL agents communicate with humans to provide services or support their decision making (Zhao et al. 2021); human feedback can also be used to evaluate the value of the agents' actions in other settings (Knox and Stone 2009; Christiano et al. 2017).\nThese different types of interaction highlight the need to incorporate humans in the training and evaluation of RL agents. However, their interaction patterns remain limited to basic and unidirectional. In contrast, hybrid human-AI intelligence, as envisioned by Akata et al. (2020) emphasizes the need for rich, diverse, and dynamic interaction patterns between humans and AI agents in order to effectively address problems which humans nor agents cannot solve independently. Although there are software packages to study the interaction of RL agents and their environments, little attention has been paid to training and evaluation of RL agents in diverse and dynamic interaction patterns that include both humans and other agents.\nIn particular, popular RL libraries such as Gymnasium (Brockman 2016; Towers et al. 2024), PettingZoo (Terry et al. 2021), JaxMARL (Rutherford et al. 2024), MO-Gymnasium (Alegre et al. 2022) or HIPPO-Gym (Taylor et al. 2023), provide a framework to train RL agents in benchmarks of particular formulations of the RL problem. These libraries also provide an interface to include a custom domain in the training. However, none of these libraries by themselves specifically tackles rich, diverse and dynamic interactions among humans and RL agents such as multimodal communications for shared observations, delegated actions and common goals that we believe to be required for hybrid intelligence.\nTo address this research gap, we propose SHARPIE (Shared Human-AI Reinforcement-Learning Platform for Interactive Experiments) as a platform for studying how multiple humans and RL agents interact and collaborate effectively. This modular platform streamlines the design and execution of experiments with humans and interactive RL agents by:\n\u2022 providing a versatile wrapper for popular RL, multi-agent RL, and multi-objective RL environments and algorithms\n\u2022 supporting configurable communication channels between the human and the RL agents with various modalities\n\u2022 offering logging services, deployment utilities, and participant recruitment platforms integration.\nThe platform aims to empower researchers to address research questions on the interaction of RL agents and humans, including those related to e.g. interpretability, in-"}, {"title": "Motivating Use Cases", "content": "Table 1 provides practical examples of target use cases that illustrate the wide range of problems that SHARPIE is designed to deal with. It details for each use case the roles taken on by human(s) and agent(s), an example environment and an envisioned interaction interface. These example use cases are not comprehensive and we will only implement some of these initially. We provide them here for illustrative purposes.\nThe first use case, entitled 'reward annotation', corresponds to a setting in which a human annotator supplies the agent with a dense reward signal based on observations of the agent interacting with an environment. This reward signal complements a sparse reward signal in a maze navigation task, and models a setting in which a human expert guides the agent in completing its task successfully. It requires a visual representation of the environment and human reward feedback components in the User Interface (UI). The second, 'exploration', use case targets the same maze solving task but suggests richer platform capabilities, as it requires the participant to be able to define desired maze layouts and select promising policies interactively (Godin-Dubois, Miras, and Kononova 2025). This use-case highlights the use of a person to specify the task the agent should solve, which can be found in myriad examples of humans interacting with RL agents (Chang et al. 2024).\nThe third 'teaching' use case targets a multi-agent setting in which a number of agents need to learn from human demonstrators the task of covering a number of landmarks in the environment. This task invites researchers to study how goals (i.e. target locations) of the other agents can be estimated in a straightforward unidirectional way (Hou et al. 2024). The 'action delegation' use case deals with the problem of handing over control between human and learner, and suggests the need for humans and learners to effectively model the other agents' intended actions, goals and capabilities in a bidirectional setting.\nThe 'task specification' use case targets interactively guiding the agent towards a high-level goal by specifying (sub)tasks for sparse reward tasks or open-ended settings. This use case illustrates the need for communication channels between agents and humans to specify goals in e.g. free text or a formal representation of the task at hand (Icarte et al. 2022; Den Hengst et al. 2022). It allows studying the ability of humans to effectively express their (sub)tasks, as well as studying approaches to agree on the semantics of task specifications. The 'human-AI teaming' use case suggests the ability for human and RL agents to bidirectionally communicate their observations, intentions or goals (Zhu, Dastani, and Wang 2024). Through effective and efficient interactions among humans and agents, we can form AI-human teams to coordinate learning behaviors, for example to solve classical predator-prey tasks (Lowe et al. 2017).\nFinally, we consider two multiobjective decision-making problems. The first, 'utility elicitation' use case addresses the need for humans to select from a set of optimal policies in a multi-objective setting. The agent learns the attainable trade-offs between the gathering of treasures and the amount of time spent under water in a sea environment. It then presents their trade-offs as a Pareto front of polices. The human stakeholder can then select a solution policy that optimizes the particular trade-off that maximizes utility.\nThe multiobjective setting is generalized to multiple stakeholders in the final use case, in which the agent controls the amount of water released by an energy-generating dam. The different stake-holders, i.e., the inhabitants of an upstream village, a downstream village, and the dam operators, need to agree on a policy that balances the objectives of meeting energy and water demand while minimizes downstream and upstream costs of flooding. The individual utilities for these stakeholders are mined separately and then used to compute a set of policies that for some trade-off to inform the stakeholders as part of a shared decision-making process using example trajectories.\nWe close this section with a brief discussion of how the proposed framework supports the study of RL, cognitive sciences, and their interconnection. Firstly, the ability to collect human demonstrations, reward annotations, and task descriptions (use cases 1,2,3 and 5) allows the study of human decision making, task success, and task definitions in humans. Furthermore, collaboration between RL agents and humans (use cases 4 and 6) allows the study of social learning, trust and reciprocity dynamics, and theory of mind in hybrid human-AI settings (Jara-Ettinger 2019; Alexandre S. Pires 2024). The impact of RL agents on group decision-making processes can be studied through use cases 6, 7 and 8 (Seo and Lee 2017), while the presence of communication channels (5) allows the study of language, communication, and grounding (Zhang et al. 2024). We believe that the number of possible (inter)connections between RL and cognitive science that can be studied with SHARPIE will continue to grow as suitable environments, algorithms and UI components are contributed to the SHARPIE framework."}, {"title": "Related Work", "content": "Several successful software packages and platforms for the development of RL algorithms and evaluation environments have been made over the past decades, emphasizing a different number of objectives, different numbers of agents, different kinds of tasks and different programming languages, etc. We do not intend to review all software packages in RL and here we focus only on the related work that particularly deals with RL platforms that explicitly target the study of RL in a context that involves humans.\nHIPPO-GYM (Taylor et al. 2023), provides a framework that involves human interactions in a RL setting. However, this framework is limited to single-agent tasks and provides only specific interactions in those tasks (i.e. humans teaching RL agents). Knierim et al. (2024a) presents a framework in human teaching RL agents setting with audio communication. Their codebase also facilitates a Wizard-of-Oz approach (Green and Wei-Haas 1985), where a participant can play the role of a teacher while a researcher plays the role of an artificial agent. Although an alternative modality is proposed in this study, the interactions are designed to be one-way, with human teachers guiding RL-agents.\nAnother line of work presents Reinforcement Learning from Human Feedback (RLHF) platforms (Christiano et al. 2017; Yuan et al. 2024). However, the interfaces for human interaction provided by these platforms are only capable of handling elicited feedback for RLHF.\nThere are several software platforms for conducting behavioral experiments (Peirce 2007; De Leeuw 2015). None of these offers any infrastructure for any type of artificially intelligent entities, whether learning or not.\nVarious platforms and software packages have been developed for the deployment of RL (Gauci et al. 2019; Albers, Neerincx, and Brinkman 2022; Zhu et al. 2024). However, these are not focused on controlled experiments on the interaction between RL agents and humans.\nPrevious studies in multi-agent learning such as MAgent (Zheng et al. 2018) and MOMAland (Felten et al. 2024) are capable of handling multi-agent experiments. MAgent supports a large population of agents, ranging from hundreds to millions, whereas it is possible to train the agents for multi-"}, {"title": "SHARPIE Framework", "content": "The SHARPIE library is a Python-based web framework that is currently under active development and that aims to provide a versatile wrapper around popular Reinforcement Learning a) environments, b) algorithms, and c) methodologies (see Figure 1). For the first part, it can encapsulate any existing environment that follows the conventional Gymnasium API (reset, step, render, etc.) which encompasses most of the existing RL platforms (see Appendix A). Most importantly, the ambition of SHARPIE is not to be tightly integrated with any one particular environment or library, but rather to be compatible with many.\nFurthermore, one of its core components is a customizable and easily deployable front-end UI which is primarily web-based and multi-modal. This allows human users to interact with the environment and other (RL or human) agents. Additionally, RL agents may be able to request action delegation, explicitly or as prompted by their learning algorithm,"}, {"title": "Discussion & Future Work", "content": "We have presented and motivated the design and implementation of a framework to accelerate human-AI interaction studies, with a particular focus on studies for multi-agent tasks involving humans. As stated, the scope of this project is not to implement alternative environment and algorithms for such studies, but rather to integrate with existing solutions seamlessly and provide utilities to ease experimentation with participants. This is done by empowering researchers through a modular software interface relying, in part, on the de facto standard set by Gymnasium. With SHARPIE we aim to provide an easily integrable framework that researchers can use to painlessly set up experiments involving both human and artificial agents. Our hope is that in turn such an architecture lays the foundation for a standard for the interaction between human and artificial agents.\nThis modular approach also allows for numerous directions of improvement in terms of interoperability and features. In the former case, we aim to provide an increasing number of ready-made, supported plugins to handle a large part of the existing work on environments, libraries, and deployment options. In the latter case, we plan to widen the scope of possible human-agent interactions by incorporating additional modalities such as audio or video (Christofi and Baraka 2024; Knierim et al. 2024b). The resulting real-time and multimodal communication between users and agents would allow the study of rich and fine-grained communication protocols, and support experiments involving pitch and tone, nonverbal cues, etc. Finally, we envision a hosted version of SHARPIE that can be used for outreach, education and user literacy purposes."}]}