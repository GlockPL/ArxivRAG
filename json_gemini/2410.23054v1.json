{"title": "CONTROLLING LANGUAGE AND DIFFUSION MODELS BY TRANSPORTING ACTIVATIONS", "authors": ["Pau Rodr\u00edguez", "Arno Blaas", "Michal Klein", "Luca Zappella", "Nicholas Apostoloff", "Marco Cuturi", "Xavier Suau"], "abstract": "The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (ACT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. ACT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that ACT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how ACT enables fine-grained style control and concept negation.", "sections": [{"title": "1 INTRODUCTION", "content": "Pre-trained Generative Models (GMs) typically undergo an additional fine-tuning phase to better align them to a desired behavior. For example, Large Language Models (LLMs) are aligned via"}, {"title": "2 RELATED WORK", "content": "The growing capabilities and prevalence of GMs (Brown et al., 2020; Rombach et al., 2022), along with the rising costs of fine-tuning and alignment, have driven research into controllability of GMs.\nControlling LLMS ACTADD (Turner et al., 2023) uses a contrast prompt (one positive and one negative example) to construct a shift vector. CAA (Rimsky et al., 2023) builds on ACTADD by calculating the difference vectors for steering based on a dataset of contrast pairs (rather than a single pair), adding the mean difference during inference time for steering. ITI-C (Li et al., 2024) estimates the shift vector orthogonal to the hyperplane learnt by a binary linear classifier on two sets of sentences, showing an increase of truthfulness on the TruthfulQA benchmark (Lin et al., 2021). The same work proposes MassMean (ITI-M), with an additive vector computed as the difference in means for both sets of sentences. With a different approach, AURA by Suau et al. (2024) damp-ens activations proportionally to each neuron's ability to classify toxic and non-toxic sentences,"}, {"title": "3 TRANSPORTING NEURON ACTIVATIONS", "content": "We represent the activations of a GM given an input sentence $x \\in S$ as a tensor $R^{M \\times L \\times K}$, where $M$ is the number of activations per layer (assumed constant w.l.o.g. for simplicity), $L$ the number of layers, and $K$ the number of tokens decoded. We reduce each of the $K$ values to only one using an arbitrary pooling operator $\\oint$. From now on we write $Z : S \\rightarrow R^{M \\times L}$ for the map that turns a sentence into a matrix of activations statistics, noting that Z incorporates $\\oint$-pooling.\nWe consider two probability distributions on sentences $p$ and $q$. We will view these sentences ex-clusively through the lens of their aggregated activation matrices, i.e., we will examine probability distributions $\\mu := Z_{\\ddagger}p$ and $\\nu := Z_{\\ddagger}q$, where we have used the pushforward operator $\\ddagger$. In practice, we have access to samples $x^1, . . ., x^n \\sim p$ and $y^1, . . ., y^n \\sim q$. In the case of toxicity mitigation, $p$ covers toxic sentences and $q$ non-toxic ones. Input sentences ($x^i$) and ($y^i$) go through the model to yield activation matrices $a^i := Z(x^i)$ and $b^i = Z(y^i)$, each seen as i.i.d. samples from $\\mu$ and $\\nu$ respectively. This results in $n + n$ observations of $M \\times L$ matrices. In that context, our goal is to learn a transport map $T : R^{M \\times L} \\rightarrow R^{M \\times L}$ from $(a^i, b^i)$ that approximately pushes $\\mu$ to $\\nu$, i.e., $T\\mu\\approx \\nu$."}, {"title": "3.1 Low BUDGET ESTIMATORS FOR TRANSPORT MAPS", "content": "Since a modern GM can have millions of activations, we seek maps $T$ that are cheap to evaluate and store, with very few parameters. Following Suau et al. (2024), we focus on maps that factorize independently along each dimension (each activation). $T$ is therefore described as a collection of $M L$ independent univariate maps, where each map indexed by $m, l$ should ideally map the marginal distribution of $\\mu$ in that coordinate to that of $\\nu$. Recall that:\nProposition 3.1 (Univariate Transport Maps) (Santambrogio, 2015, Chap.2) Let $p, \\tau \\in P(R)$ be two univariate distributions. For any submodular cost $c : R\\times R \\rightarrow R$ (i.e., such that $\\frac{\\partial c}{\\partial x \\partial y} < 0$), the optimal transport map $T$ that can transport $p$ to $\\tau$ is $T^* = Q_{\\tau} \\circ F_p$, where $Q_{\\tau}$ and $F_p$ are respectively the quantile function of $\\tau$ and the cumulant density function (CDF) of $p$.\nEstimating and storing all $M L$ transport maps would therefore require dealing with as many quantile and CDF functions. Unfortunately, parameterizing each of these could quickly become intractable, which is why we scale down ambitions to simplify further our working hypothesis to only consider affine transport maps.\nEach of the $M L$ activations we consider results in two families of reals: source $(a_{ml}^e, ....., a_{ml}^e)$ and targets $(b_{ml}^e,..., b_{ml}^e)$. Simpifying notations, we drop mentions to m and l to focus on values"}, {"title": "3.2 SEQUENTIAL ITERATIVE MAPS", "content": "While it might be possible to follow the template approach outlined in Section 3.1 to apply univariate maps to each of the ML activations, this ignores the causal relationship across activations, where activations produced by a layer are processed by the next one, i.e., $a_{m,l+1} = f_\\theta(a_{m,l})$. Any intervention at the level of a layer must therefore be factored in accordingly before creating the intervention at the next one. To account for such causality, we estimate the transport maps for each layer incrementally: we first estimate the transport for the first layer (in the model graph), then we run inference again by applying the first layer map in order to estimate the map for the second layer, and so on until all maps are estimated. A similar approach is adopted in Zou et al. (2023), and detailed with our tools in Definition 3.2. In Appendix C we show that causal estimation achieves more effective conditioning than a simultaneous estimation. In this work, we use causal estimation for Mean-ACT and Linear-ACT.\nDefinition 3.2 (Affine Causal Transport Map) For $m < M$ and $l < L$, let $A_m := (a_{m,1},\u2026, a_{m, 1})$ and $B_m := (b_{m,1},..., b_{m, 1})$ denote n families of M activations for the first layer. Starting with $l = 1$, and setting\n$C_{m,1}: A_{m,1}, D_{m,1} := B_{m,1}$,\ncompute and store the 2M $(w_m, \\beta_m)$ parameters of all M transport maps associated with these activations using Definition 3.1:\n$\\forall m < M,\\forall l < L, T_{m,l} := T( \\cdot ; C_{m,l}, D_{m,\\ell}) : R \\rightarrow R$,\nwhere observations C and D are refreshed recursively for each of their entries m < M, as l is incremented,\n$C_{.,e+1} := f_\\theta([T_{m,l}(C_{m,l})]_m)$,\n$D_{.,e+1} := f_\\theta([T_{m,l}(D_{m,l})]_m)$.\nAt inference time, given a sentence x, we run the recursion starting from the first activation vector a = $(a_{m,1})_m$, looping for $1 < l < L$ as $a \\leftarrow f_\\theta([T_{m,l}(a_m)]_m$."}, {"title": "Interpolation Between Measures Using Transport", "content": "One can easily extend a transport map from measure $\\mu$ to $\\nu$ to one that is able to output an interpolating measure. The idea, outlined by McCann (1997), consists in defining the following \u03bb-parameterized map from any OT map T,\n$T(a, \\lambda) = (1 \u2013 \\lambda)a + \\lambda T(a),$(1)\nwhere $\\lambda \\in [0, 1]$ and $\\lambda = 1$ recovers the full transport. Conditioning GMs through OT allows the user to precisely control the presence of a concept with a continuous and interpretable knob (\u03bb) during generation, not requiring expensive parameter search (Li et al., 2024) or being limited by fixed, uncontrollable conditioning (Suau et al., 2024). In applications such as diffusion, where the utility of the model is harder to assess, our interpretable strength is of key importance, as shown in Section 5. Note that methods like ACTADD, CAA or ITI-C also have a conditioning strength parameter. However, this parameter is applied as a multiplier of a conditioning bias as $T(a, \\lambda) = a + \\lambda\\beta$ (see Section 3.3), thus making \u03bb unbounded, harder to interpret and not robust with respect to different models, layers, and tasks."}, {"title": "3.3 GENERALIZATION OF PRIOR INFERENCE-TIME INTERVENTIONS WORK", "content": "In this section, we show how many earlier works can be interpreted as special cases of Linear-ACT."}, {"title": "4 EXPERIMENTS ON LLMS", "content": "We empirically verify the performance of ACT on pre-trained LLMs on toxicity mitigation (Section 4.1), general concept induction (Section 4.2), and truthfulness induction in particular (Section 4.3), showing the efficacy and robustness of ACT in different scenarios related to LLMs."}, {"title": "4.1 TOXICITY MITIGATION IN LLMS", "content": "It is known that LLMs are prone to generate toxic language (Wen et al., 2023), especially when prompts are designed to elicit toxic behavior. In this section, we study how ACT is effective at toxic language mitigation compared to some recents methods such as AURA, ACTADD and ITI-C, on Gemma2-2B (Team et al., 2024) and Llama3-8B Dubey et al. (2024).\nWe prompt each LLM with 1000 randomly chosen prompts from RealToxicityPrompts (RTP) (Gehman et al., 2020), known to induce toxic language generation. Then, we collect the gener-ated continuation to each prompt and we evaluate toxicity with a ROBERTA-based classifier\u00b2, as in Suau et al. (2024). In addition, we also measure toxicity in a 0-shot manner by querying Llama3-8B-instruct as LLM-as-a-judge (Zheng et al., 2023) (more details on the way we instructed the LLM-as-a-judge can be found in Appendix E). As a measure of general LLM utility we report in Table 2: (i) perplexity (PPL) on a fixed set of 20k Wikipedia sentences measured with the intervened model, (ii) PPL of the generated sentences measured with Mistral-7B (Jiang et al., 2023) and (iii) MMLU (Hendrycks et al., 2021) 5-shot accuracy using the intervened model.\nLinear-ACT reduces toxicity up to 7.5\u00d7 and is robust to \u03bb, layer, and model choice We observe that Linear-ACT achieves up to 7.5\u00d7 reduction in toxicity on Gemma2-2B and 4.3\u00d7 on Llama3-8B, with minimal impact on PPL and MMLU. Most importantly, ACT obtains the best results at \u03bb = 1, which is in line with our OT formulation, since \u03bb = 1 means full transport. Linear-ACT and Mean-ACT obtain similar toxicity mitigation results. ITI-C achieves 5.6\u00d7 and 3.6\u00d7 toxicity reduction on Gemma2-2B and Llama3-8B respectively. In line with the ITI-C paper findings, ITI-C performs well on attention, but is very sensitive to models and layers, as well as to the choice of \u03bb (see a layer diagram in Appendix B and full tables and plots in Appendix G). AURA achieves 2.0\u00d7 and 3.1\u00d7 toxicity reduction per model and ACTADD induces the mildest mitigation."}, {"title": "4.2 INDUCING CONCEPTS IN LLMS WITH ACT", "content": "ACT allows transporting activations from distribution \u03bc to \u03bd (derived from sentence distributions p and q respectively). In an induction setting, p covers generic content, while q a specific concept that we want to induce. We mine the OneSec dataset (Scarlini et al., 2019), collecting 700 sentences that contain a specific concept (q) and 700 sentences randomly sampled from other concepts (p). We do so for seven different concepts (football, cloud, baby, church, book, flower, balloon) and we estimate an intervention for each of them. We assess the presence of a concept in the generated text in a LLM-as-a-judge manner by querying Llama3-8B-instruct (LLM-as-a-judge details in Appendix F).\nLinear-ACT can induce arbitrary concepts with consistent \u03bb = 1 Figure 4 shows the effect of increasing \u03bb both on the presence of the concept, p(yes), and the PPL measured with Mistral-7B on the generated text. We intervene upon the most effective layers for each method according to the toxicity results: Post-LN for ACT and attention for ITI-C. We do not include AURA because it is designed for mitigation, and ACTADD gives lower performance on this task. For Linear-ACT, we observe a peak of concept presence at \u03bb \u2248 1, with a median p(yes) = 0.87 (i.e., 87% of the generated sentences are classified as containing the induced concept) and an acceptable PPL = 8.5. For \u03bb > 1, the PPL quickly degrades and the presence of the concept diminishes. This is also consistent with the toxicity mitigation experiments in Section 4.1. Interestingly, the peak for Mean-ACT is at \u03bb \u2248 2.5, also highlighting that Mean-ACT is a poorer approximation of the OT transport. Notably, ITI-C achieves a similar p(yes) and PPL as Linear-ACT for \u03bb \u2248 5. However, note that ITI-c's best \u03bb is different than the ones for toxicity. Appendix H contains generation examples."}, {"title": "4.3 INDUCING TRUTHFULNESS IN LLMS WITH ACT", "content": "One particular concept that has gained attention in previous activation steering works is \"truthful-ness\" (Li et al., 2024). We study how ACT can increase truthfulness on Gemma2-2B and Llama3-8B, compared to the original model. Again, we compare to AURA, ACTADD and ITI-C. We evalu-ate all methods on the TruthfulQA multiple choice part that has been used in prior work (Lin et al., 2021; Li et al., 2024). We report both MC1 and MC2 of TruthfulQA, and control for overfitting on the TruthfulQA task by also evaluating MMLU 5-shot accuracy (Hendrycks et al., 2021).\nACT can induce truthfulness with consistent \u03bb = 1. The results of our experiments are summa-rized in Table 3. As we can see, ACT can successfully induce truthfulness in both models in its default setting \u03bb = 1 (corresponding to full transport). Both Linear-ACT and Mean-ACT achieve the best and second-best MC1 and MC2 accuracy improvements among all methods investigated. Linear-ACT increases MC1 by roughly 5% for Gemma2-2B and by almost 8% for Llama3-8B, which is about 1.5% and 3% more than the closest non-ACT baseline (ITI-C), while incurring even slightly less decrease in MMLU performance. Full results and experimental setup in Appendix I."}, {"title": "5 CONTROLLING IMAGE DIFFUSION MODELS", "content": "In this section, we show that ACT improves the controllability of text-to-image diffusion models (T2Is), a well-known challenge (Cao et al., 2024). We address two open problems in T2I generation: fine-grained style control (Section 5.1) and concept negation (Section 5.2). We show that off-the-shelf ACT succeeds at both tasks. In line with optimal transport theory and experimental results on LLMs (Section 4), ACT consistently achieves the strongest conditioning with full strength (i.e., \u03bb = 1). We also adapt ITI-C to the topology of images by training it on the spatial average pooling of activations (as we do by default for ACT), and applying it to each spatial position independently. Remarkably, ITI-C succeeds at the fine-grained control task with our adaptation, but requires tuning \u03bb, and it fails with concept negation.\nSetup. We apply ACT on the denoising convolutional UNet of Stable Diffusion XL (SDXL) (Podell et al.) and the denoising transformer of FLUX. Schnell\u00b3. For FLUX, we use the T5-XXL text encoding modality (Raffel et al., 2020) instead of CLIP (Radford et al., 2017) to account for the effects of language modelling. We use a distilled version of SDXL, which only re-quires 4 diffusion steps (Lin et al., 2024) like FLUX. We intervene upon all normalization layers in SDXL's UNET and the output of most residual layers in FLUX (details in Appendix J.8). We only show results for ACT and ITI-C since ACTADD is not applicable to images and AURA resulted in noisy images. To measure the presence of a style or a concept, we use a CLIP zero-shot classifier with the classes (+) \u201cA picture of a{style or concept}\u201d and (-) \u201cA picture of something\u201d."}, {"title": "5.1 STYLE CONTROL", "content": "A major challenge in T2I generation is fine-grained control. For example, while you can prompt SDXL to create a sketch of an object, it is hard to control the level of \u201csketchiness\u201d. Models like SDXL have a guidance parameter, but its use is limited since low guidance values tend to remove image semantics (see example in Appendix J.1).\nWe sample 2048 prompts from the COCO Captions (Chen et al., 2015) training set and append a series of tags generated with Llama-8B-instruct to induce the following styles: anime, art nouveau, cyberpunk, impressionism, sketch, watercolor (see Table 13 for details). Then we use the original prompt as the source distribution (p) and the style-modified prompt as the target distribution (q) to learn transport maps for style. To evaluate, we sample 512 prompts from the COCO Captions validation set and generate images with different intervention strengths.\nLinear-ACT is a robust method for fine-grained control in text-to-image generation. Figure 6a shows that Linear-ACT on SDXL and FLUX increases the presence of a desired style, e.g., on SDXL from ~ 12% to ~ 95% of the generated images while keeping ~ 80% of the similarity to the original prompt (\u03bb = 1). In accordance to the theory and experiments on LLMs, the maximum conditioning (i.e., highest 0-shot score) for ACT is achieved at \u03bb = 1 for both models. ITI-C can also accomplish fine-grained control, but its best performance is achieved at different \u03bbs, equal to 2 and 1 for SDXL and FLUX respectively, which is in turn not consistent with the best \u03bb found in LLM experiments. A closer look at images generated with ITI-C for best \u03bb in Figure 5 and appendix J.3 reveals that ITI tends to exaggerate style traits while distorting the semantics. This further highlights the reliability of ACT across different modalities, tasks, and models. While quantitatively ACT and ITI-C perform well, we invite the reader to compare the quality of the generated images and styles in Figures 1 and 5, and in more examples in Appendix J.3."}, {"title": "5.2 CONCEPT NEGATION", "content": "T2I diffusion models are known for struggling with concept negation (Li et al.; Hwang et al., 2024). For example, Hwang et al. (2024) showed that recent models such as Stable Diffusion (Rombach et al., 2022) and DALL-E 3 (Betker et al., 2023) are prone to generate a pink elephant when in-structed not to generate one. To improve controllability, some T2I generators like SDXL include a negative prompt mechanism to remove concepts from the generated images. However, we found that both SDXL (CLIP encoder + negative prompt) and FLUX (T5-XXL encoder) still tend to generate unwanted concepts (see some examples in Appendix J.2).\nWe use the COCO Captions (Chen et al., 2015) training set to sample 2048 prompts used to gen-erate the images. To create a source and target activation distribution to estimate ACT, we ask Llama3-8B-instruct to generate a diverse set of prompt modifiers requiring the model to include the"}, {"title": "6 LIMITATIONS AND DISCUSSION", "content": "In this work, we introduce Activation Transport (ACT), a general framework to achieve intuitive and fine-grained control of GMs. Our approach is based on optimal transport theory, effectively mapping activations from a source to a target distribution by preserving the latter, and unifies many previous activation steering works. We show experimentally that our Linear-ACT approach generalizes well across models and tasks, for both LLMs and T2I architectures. Moreover, ACT provides a robust parameter to control the amount of conditioning, bounded between 0 and 1, which makes it user-friendly and interpretable.\nWhile effective, Linear-ACT assumes a linear transport between i.i.d. activations, which are sim-plifications adopted for compute and memory reasons. Additionally, the map estimation purely depends on the samples used, thus being limited by their expressiveness. In future work, we plan on exploring non-linear maps and joint activations distributions."}, {"title": "ETHICS STATEMENT", "content": "Our method could theoretically be used to mitigate or induce the presence of any concept. Therefore, it could eventually lead to the development of censorship or misinformation tools.\nWhile our work can be used to align in pre-trained GMs, it should not be taken as a reason not to pursue the adoption of clean data and additional alignment strategies during the pre-training phase."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We will make our code and data publicly available on github. To aid reproducibility, all tables contain the best \u03bb found through grid-search and results are averaged over 5 runs. We include additional details on the intervened layers in Appendix B, ablations on the effect of transport support in Appendix D, the exact prompt templates of LLM as a judge in Appendices E and F, experimental details on TruthfulQA in Appendix I, as well as experimental details for T2I models in Appendix J."}, {"title": "A MEMORY AND COMPUTATIONAL ASPECTS", "content": "Linear-ACT requires storing 2 floats (\u03c9, \u03b2) per activation intervened. For example, Linear-ACT on post-LN layers of Gemma2-2B requires (2 \u00d7 52 layers \u00d7 2304 activations \u00d7 4 bytes) = 0.91 Mb. If we choose to use the support transport, 2 more floats per activation are stored Q\u221e = [min A, max A], which means an extra 0.91 Mb for the Gemma2-2B example. In terms of compute, Linear-ACT requires an extra element-wise product and sum per intervened layer. However, the inference cost of such operations is of second order compared to the overall LLM inference cost.\nOne has the option to fix \u03bb. If so, our Linear-ACT formulation in Definition 3.1 becomes Tlin(a) =\n((\u03bb\u03c9 \u2013 1) + 1)a + \u03bb\u03b2 = \u1ff6\u03b1 + \u03bb\u03b2. Assuming we intervene after a linear layer ya + d, we compose\nboth functions as (Tlin o f)(a) = \u1ff6\u03b3\u03b1 + (\u1ff6\u03b4 + \u03bb\u03b2), which is also a linear map whose parameters\ncan replace those of f in the computational graph, without any extra cost at inference time. The\nmemory cost is 0 if we fix \u03bb and compose Linear-ACT with the model linear layers."}, {"title": "B INTERVENED LAYERS", "content": ""}, {"title": "C CAUSAL VS. SIMULTANEOUS ESTIMATION OF ACT", "content": "In Table 4 and Table 5 we compare the estimation of ACT interventions in a causal and simultaneous way (see Section 3.1). We observe that causal estimations show better toxicity mitigation than its simultaneous counterparts."}, {"title": "D THE EFFECT OF THE TRANSPORT SUPPORT", "content": "In this section we validate the choice of transport support, as a way to make the pro-posed intervention more robust. In this experiment, we sweep different supports bynarrowing the quantiles (qt) of the input data set A, in the setting of toxicity mit-igation (as in Section 4.1), both for Mean-ACT and Linear-ACT. The supports testedare: [qt40, qt60], [qt30, qt70], [qt20, qt80], [qt10, qt90], [qt5, qt95], [qt3, qt97], [qt1, qt99], [qt0, qt100] and(-\u221e,\u221e).\nNote that [qt0, qt100] = Q\u221e, as defined in Section 3.1. We show the results of this sweep in Fig-ure 9, where we observe that [qt0, qt100] offers a good trade-off between conditioning strength andacceptable increase in PPL (below +1 points with respect to the original model)."}, {"title": "E 0-SHOT TOXICITY CLASSIFICATION", "content": "We query Llama3-8B-instruct with the system prompt below, where [SENTENCE] contains the sentence to be assessed in 0-shot manner.\n\"You are a highly accurate toxicity classification\nmodel. Your task is to determine if a given sentence\ncontains toxic content. You will output 'yes' only"}, {"title": "F 0-SHOT CONCEPT PRESENCE CLASSIFICATION", "content": "We query Llama3-8B-instruct with the system prompt template below.\n\"You are a chatbot who answers whether the provided\nsentence is referring to [CONCEPT] defined as\n[WORDNET_DEF]. Note that the sentence might not\ncontain the word [CONCEPT], but may just be\nreferencing concept as defined.\\n [SENTENCE]\"."}, {"title": "G EXTENDED RESULTS ON TOXICITY MITIGATION", "content": "We report here the full experimental results for toxicity mitigation, which have been summarized in Section 4.1. Note the variability in the optimal strength \u03bb for ITI-C and ACTADD, which compli-cates the applicability of these methods on different models and layers."}, {"title": "\u0397 EXTENDED RESULTS ON CONCEPT INDUCTION ON LLMS", "content": "Tables 8 to 10 contain examples of generated sentences by Gemma2-2B intervened for concept in-duction with Linear-ACT and ITI-C. These results complement those presented in Section 4.2. Note the more gradual increase of concept with Linear-ACT, reaching strong and consistent conditioning at \u03bb = 1 with great quality. ITI-C also performs well on concept induction, however the choice of \u03bb is less clear. For example, for \u03bb = 10 it fails for concept Cloud while it works well for Football."}, {"title": "I EXPERIMENTAL DETAILS AND EXTENDED RESULTS ON INDUCING TRUTHFULNESS", "content": ""}, {"title": "1.1 EXPERIMENTAL DETAILS", "content": "We follow the original experimental protocol for evaluations on the TruthfulQA multiple choice part, as described in Lin et al. (2021). This consists of preprompting the model with the same default prompt before each question as proposed by Lin et al. (2021) in Figure 21, which we replicate below for the reader. To then evaluate a model on a (preprompted) question, the likelihood of each multiple choice answer is computed independently (conditional on the default prompt and question). The answer option with the highest likelihood is counted as the model's answer to the question."}, {"title": "1.2 EXTENDED RESULTS", "content": ""}, {"title": "1.2.1 FULL RESULTS OVER 5 SEEDS FOR ALL LAYERS", "content": "Table 11: TruthfulQA results for Gemma2-2B, results over 5 runs. ITI-C, ACTADD and ACT have a strength parameter \u03bb which we sweep, reporting for each method the best result (best \u03bb) in MC1 Accuracy that incurs at least equal performance in MMLU accuracy compared to the best (in terms of MC1 accuracy) of the two ACT methods (see I.2.2, giving 0.1% slack)."}, {"title": "I.2.2 SWEEPING \u039b FOR ITI-C AND ACTADD", "content": "In Figures 12 - 15, we show the results of sweeping the value of \u03bb for ITI-C and ACTADD for both Gemma2-2B and Llama3-8B. For each model, we also indicate the MMLU accuracy of the best ACT method for that model with a horizontal grey dashed line, as this is our point of reference for choosing \u03bb for ITI-C and ACTADD: we choose the value of \u03bb that achieves the best MC1 accuracy, while achieving at least equal MMLU accuracy to this grey dotted line (up to a slack of 0.1%).\nFor ITI-C, where we see a clear relationship between MMLU and MC1 accuracy as \u03bb varies, we sweep \u03bb \u2208 [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0]. For ACTADD,"}, {"title": "J EXPERIMENTAL DETAILS AND EXTENDED RESULTS FOR T2I GENERATION", "content": "Appendix J.1 illustrates the effect of the guidance parameter in SDXL. Appendix J.2 illustrates the problem of concept negation when using negative prompts. Appendix J.3 contains additional qualitative examples of style control on SDXL and FLUX. Appendix J.4 contains additional qualitative examples for concept negation in SDXL and FLUX. Appendices J.6 and J.7 contain the list of tags used as prompt modifiers to generate the target/source distribution of activations for each style/con-cept respectively. Appendix J.8 contains details on FLUX's architecture conditioning."}, {"title": "J.1 GUIDANCE PARAMETER IN EXISTING DIFFUSION MODELS", "content": "We show in Figure 16 the effect of changing the guidance scale parameter in SDXL. While large val-ues lead to effective conditioning, lower values destroy content. This makes guidance non intuitive and harder to use by users."}, {"title": "J.2 NEGATIVE PROMPTING", "content": "Stable diffusion models allow using negative prompts to avoid unwanted elements in the generated images (Rombach et al., 2022; Podell et al.). Here, we show that this method is ineffective at removing pink elephant, white bear, and gorilla. Figures 17 and 18 contain some failure cases of SDXL and Stable Diffusion 3 (Esser et al., 2024) at removing unwanted concepts. Figure 25 and Figure 26 show results intervening SDXL with ACT, showing its effectiveness at removing these concepts with the same prompts. In Figure 27 we show some failure cases at concept negation."}, {"title": "J.7 CONCEPT PROMPTS", "content": "Table 14: List of tags generated with Llama-8B-instruct (right) to induce different concepts (upper left) or to prompt models not to generate them (lower left).\nPink elephant a pink elephant. containing a pink elephant. with a\npink elephant in plain view. and a pink elephant. it\ndisplays a pink elephant. featuring a pink elephant.\nin addition to a pink elephant. and also a pink\nelephant. and a pink elephant as well. the pink\nelephant can be clearly seen.\nGorilla a gorilla. containing a gorilla. with a gorilla in\nplain view. and a gorilla. it displays a gorilla.\nfeaturing a gorilla. in addition to a gorilla. and\nalso a gorilla. and a gorilla as well. the gorilla can\nbe clearly seen.\nWhite bear a white bear. containing a white bear. with a white\nbear in plain view. and a white bear. it displays a\nwhite bear. featuring a white bear. in addition to a\nwhite bear. and also a white bear. and a white bear as\nwell. the white bear can be clearly seen.\nNo pink elephant without a pink elephant. not containing a pink\nelephant. without a pink elephant in plain view.\nand a pink elephant that cannot be seen. it does not\ndisplay a pink elephant. not featuring a pink elephant.\nlacking a pink elephant. and not a pink elephant. and\na pink elephant is missing. the pink elephant cannot be\nseen.\nNo gorilla without a gorilla. not containing a gorilla. without\na gorilla in plain view. and a gorilla that cannot be\nseen. it does not display a gorilla. not featuring a\ngorilla. lacking a gorilla. and not a gorilla."}]}