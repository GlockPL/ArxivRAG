{"title": "Aligning Machine and Human Visual Representations across Abstraction Levels", "authors": ["Lukas Muttenthaler", "Klaus Greff", "Frieda Born", "Bernhard Spitzer", "Simon Kornblith", "Michael C. Mozer", "Klaus-Robert M\u00fcller", "Thomas Unterthiner", "Andrew K. Lampinen"], "abstract": "Deep neural networks have achieved success across a wide range of applications, including as models of human behavior in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do, raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-like behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-like structure from its representations into pretrained state-of-the-art vision foundation models. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognition and more practically useful, thus paving the way toward more robust, interpretable, and human-like artificial intelligence systems.", "sections": [{"title": "1. Introduction", "content": "While deep learning has recently driven rapid progress in areas of artificial intelligence such as natural language processing [LeCun et al., 2015; Brown et al., 2020] and computer vision [Radford et al., 2021; Dosovitskiy et al., 2020; Zhai et al., 2023; Kirillov et al., 2023], even the best of these systems often fail in ways that humans would not [Lapuschkin et al., 2016; Geirhos et al., 2018; Lapuschkin et al., 2019; Geirhos et al., 2020; Hermann et al., 2019; Thrush et al., 2022; Bowers et al., 2022; Muttenthaler et al., 2023b; Hermann et al., 2023]. These failures have led to renewals [Lake et al., 2017; Bowers et al., 2022] of older arguments that neural networks lack essential ingredients of human intelligence [Fodor and Pylyshyn, 1988; Marcus, 1998; McClelland et al., 2010; Holyoak and Hummel, 2014; Tenenbaum et al., 2011]. How can we build systems that have more human-like behavior?\nHuman perception is robust to changes in the environment and generalizes across different visual settings [Lake et al., 2015, 2017; Roads and Mozer, 2017; Geirhos et al., 2018; Lake and Baroni, 2023]. However, model performance declines\u2014often drastically\u2014if the data distribution between training and test sets is shifted [e.g., Sugiyama et al., 2007; Sugiyama and Kawanabe, 2012; Geirhos et al., 2020; Hendrycks et al., 2021a]. This lack of robustness in the representations of vision models poses a challenge for many downstream applications that require the ability to generalize [e.g., Lapuschkin et al., 2019; Pooch et al., 2019; Geirhos et al., 2020]. In addition, humans tend to be"}, {"title": "2. Results", "content": "What is missing in modern learning systems to exhibit more human-like behavior? We approach this question jointly from the perspectives of cognitive science and artificial intelligence. Specifically, our results below establish (1) a mathematical framework for aligning machine learning representations with human similarity judgments (see Sec. 2.1 and Sec. 4.1); and demonstrate that (2) our method-ological framework, soft-alignment, indeed gives rise to systems with more human-like behavior in object similarity tasks by teaching models about the hierarchical structure of human conceptual knowl-edge (see Sec. 2.2). We demonstrate this using (i) human object similarity experiments using triplet odd-one-out or multi-arrangement tasks (see Sec. 2.2.1) and (ii) a new, carefully curated dataset, Levels, that allows us to systematically study the differences between human and deep learning model similarity spaces across three levels of knowledge abstraction (see Sec. 2.2.2). Moreover, we find that (3) soft-alignment leads to models that better reflect the hierarchical structure of human conceptual knowledge (see Sec. 2.3) and show that (4) soft-alignment improves generalization and robustness of models for a wide range of computer vision tasks (see Sec. 2.4)."}, {"title": "2.1. Machine learning for alignment", "content": "To build models with more human-like behavior it is necessary to inject additional supervision about human behavior into the representations of foundation models. One way to achieve this is via an objective function that explicitly forces a model's representation to inherit a human concept space [Muttenthaler et al., 2023a,b; Fu et al., 2023]. Previous work has shown that the objective function in addition to the (pre-)training data is the most central variable in the pursuit of human-like vision models [Muttenthaler et al., 2023a; Conwell et al., 2023].\nWe used the THINGS dataset [Hebart et al., 2020] a behavioral dataset of human responses in a triplet odd-one-out task for object images to build a surrogate model (denoted as teacher) that approximates a human object similarity space. This similarity space is determined by the human odd-one-out responses from which abstract human concepts can be inferred [Hebart et al., 2020; Muttenthaler et al., 2022]. To better match the human responses, we learned an affine transformation of the pretrained teacher model's representation space of the form: $x' = Wx + b$, where $x \\in R^P$ is the model's latent representation for an object image in the data, and $W$ and $b$ are a learned transformation matrix and a bias vector respectively. The pairwise similarities of the teacher model between two object images i, j are modeled as the dot product between their corresponding transformed representations: $S'_{ij} := (Wx_i + b) \\cdot (Wx_j + b)$. We obtained the affine transformation by maximizing the probability to select a human choice $c := \\{a,b\\}$ given a triplet of object indices $t := \\{i, j, k\\}$, where $c \\subset t$, and the corresponding object representations of the pretrained teacher model over n images:\n$\\arg \\min_{W,b} \\frac{1}{n} \\sum_{s=1}^{n} \\log p (c_s|t_s, S') + \\Gamma(W)$,\nwhere $\\Gamma(W)$ performs $l_2$-regularization of the transformation matrix $W$ toward the scaled identity matrix to preserve the local similarity structure of the original representation space (see Eq. 3 for more details about the type of regularization). We used stochastic gradient descent (SGD) to find the transformation variables $W$ and $b$. See Sec. 4.1 and the Supplementary Material for more details about the optimization and how the affine transformation is learned.\nAfter we found a transformation that best matches the human responses, we clustered the transformed teacher representations into meaningful superordinate categories (see Sec. 4.1 and the Supplementary Material). We used these clusters to sample triplets\u2014two objects from one cluster and one from a different cluster\u2014for generating a synthetic dataset of human-like triplet odd-one-out"}, {"title": "2.2. Toward human-like systems via a surrogate approach", "content": ""}, {"title": "2.2.1. The Levels dataset of human similarity judgments", "content": "To systematically validate that soft-alignment yields model representations that reflect the hierarchical structure of human conceptual knowledge, we collected a new multi-resolution similarity judgment dataset called Levels. This dataset was based on the triplet odd-one-out task and included judgments on three different types of triplets: global coarse-grained semantic, which require deciding on the odd-one-out in broadly different categories; local fine-grained semantic, involving discerning subtle distinctions within the same category; and class-boundary, testing the capacity to identify category boundaries. For details about Levels see Sec. 4.3.\nDescriptive statistics for human response times and uncertainty measures. We examined the human response time (RT) differences across the three abstraction settings as an indicator of the cognitive processing demands required to select the odd-one-out (see Fig. 5A). As expected, responses were fastest in the class-boundary triplets (Mdn = 2.05 s, SE = 0.01), which probed decisions with relatively low ambiguity. RTs for fine-grained semantic triplets were significantly longer (Mdn = 3.01 s, SE = 0.01; t(447) = 48.784, p < 0.001), and the longest RTs were observed for the coarse-grained semantic triplets (Mdn = 3.25 s, SE = 0.01; t(447) = 15.036, p < 0.001 compared to fine-grained). An explanation for the longer RTs in the latter conditions can be that these triplets involved complex, multi-level decisions where participants may have weighed multiple factors (e.g., perceptual and/or semantic) to arrive at choice. This contrasts with the clear semantic boundaries that could be used to identify the odd-one-out in class-boundary triplets. Consistent with the RT results, we also found higher levels of cross-participant agreement (see Fig. 5C) in the class boundary condition compared to both the coarse-grained semantic and the fine-grained semantic conditions.\nWe used participants' RTs as a proxy of their decision uncertainty. The human RTs for the individual triplets correlated positively with the level of cross-participant disagreement on those triplets (see Supplementary Material Sec. A.3 and Fig. 5B), in each of the abstraction settings (coarse-grained semantic: r = 0.325, p < 0.001; fine-grained semantic: r = 0.364, p < 0.001; class-boundary: r = 0.443, p < 0.001), suggesting that human RTs provided a reasonable approximation of decision uncertainty for model-to-human comparison.\nHuman-to-human alignment. We computed the human noise ceiling for each abstraction setting in Levels using a Leave-One-Out (LOO) cross-validation approach. In LOO, the agreement level for a triplet is computed as the average match-rate between a held-out participant's response and the majority response of the remaining population. Thus, for a triplet with five responses, one response is held out and the remaining four comprise the population. The human-to-human reliability score is then calculated as the average agreement level across all triplets in the dataset."}, {"title": "2.2.2. Alignment at multiple levels of abstraction", "content": "The Levels dataset allows to systematically study discrepancies between human and deep learning model decisions across different levels of complexity. Here, we demonstrate that soft-alignment can reduce those discrepancies: it makes it possible to incorporate the hierarchical structure of human conceptual knowledge into foundation model representations.\nGlobal coarse-grained. We found the largest differences between model and human similarity judgments before AligNet fine-tuning were present for global coarse-grained semantics. The base models achieved low odd-one-out accuracies of 36.09% (ViT-B) \u2013 57.38% (DINOv2 ViT-B). Soft-alignment significantly improved the alignment between human and model responses to the extent that all models performed above the human-to-human reliability score of 61.92% (see Fig. 2D, leftmost column) with odd-one-out accuracies of 65.70% (DINOv1 ViT-B) \u2013 68.56% (DINOv2 ViT-B). The relative improvements in performance ranged from 19.48% (DINOv2 ViT-B) \u2013 93.51% (ViT-L).\nLocal fine-grained. The performance of most base models (except for DINOv1 ViT-B) did not strongly correspond to human responses for fine-grained semantics either and, thus, model performances were far from the human noise ceiling of 65.92%. Prior to fine-tuning, models achieved poor alignment scores of 46.04% (ViT-B) \u2013 57.72% (DINOv2 ViT-B), except for DINOv1 ViT-B which performed (62.92%) significantly better than any other model in that setting (see Tab. 3 in the Supplementary Material). AligNet fine-tuning improved this mismatch to some degree but not as substantially as it did for the coarse-grained abstraction level. AligNet models achieved odd-one-out accuracies of 58.93 (ViT-S) \u2013 62.92% (DINOv1 ViT-B). This is equal to a relative increase in performance of 7.84% (DINOv2 ViT-B) - 46.03% (ViT-L) (see Fig. 2D, middle column).\nClass-boundary. Supervised classifiers and image/text contrastive models performed close to the noise ceiling for class-boundary triplets prior to any fine-tuning. Their odd-one-out accuracies ranged from 81.96% (SigLIP ViT-B) to 93.67% (ViT-L) (see Tab. 3). However, the caption generator model (CapPa) often responded quite differently from humans in that setting and achieved a low odd-one-out accuracy score of 70.37%. AligNet fine-tuning changed the representation spaces of all models that we considered to be equally well aligned. There are small differences between the models but they are not statistically significant. Surprisingly, AligNet models' odd-one-out accuracies were higher than the human noise ceiling of 89.21% (see Fig. 2D, rightmost column) with the best model achieving an alignment score of 93.24% (ViT-L). This means that the responses of AligNet models were more similar to the average human responses\u2014since each triplet response is the majority response of the subject population\u2014than the level of agreement among the human subjects themselves. The variance in accuracy among AligNet models was much smaller compared to the other two abstraction settings, with odd-one-out accuracies of 93.09% \u2013 94.24%. The relative increases in performance for this setting were between 0.62% (ViT-L) - 32.29% (CapPa ViT-B). For more details about model performances on Levels see Tab. 3 in the Supplementary Material.\nHuman alignment depends on the abstraction level. We found that the best model before soft-alignment at a particular abstraction level was often not the best model after soft-alignment. While DINOv2 was the best aligned model before AligNet fine-tuning for the fine-grained (57.38%) and coarse-grained (57.72%) abstraction settings (see Tab. 3 in the Supplementary Material), it remained the best aligned model only for the fine-grained setting (62.24%) but did not improve as significantly as the other models for the coarse-grained (relative improvement of 18.78%) and class-boundary settings (relative improvement of 4.13%). On the other hand, ViT-L was the worst-aligned model before fine-tuning for the coarse-grained setting (35.45%), but after AligNet fine-tuning, it became the best-aligned model, achieving the highest odd-one-out accuracy (68.60%) across all models. The relative improvement in performance was 93.51%. In addition, ViT-L was both the best aligned model before (93.67%) and after (94.24%) AligNet fine-tuning for the class-boundary setting (both scores"}, {"title": "2.3. Aligned models reflect the conceptual hierarchy", "content": "How do the model representations change after soft-alignment? In Fig. 3 we show that while the model representations are dissimilar before alignment (especially those trained on classification tasks), they become much more similar to each other after soft-alignment. (See Sec. 4.4 for details.)\nOne reason for this convergence is that the models align better with the human semantic category hierarchy [cf. Rosch et al., 1976]. The organization during alignment is driven by two factors: first, as shown in Section 2.2, the labels generated by the model will tend to reflect human judgments and uncertainty, especially in the relationships at more abstract levels of the category hierarchy. However, the clustering process that we use to sample the triplets also reflects this hierarchical structure. In particular, it tends to produce triplets where the similar pair of images come from the same subordinate- or basic-level category, while the odd-one-out comes from either a different basic-level or superordinate category (Fig. 3C). Thus, our soft-alignment procedure embeds the global structure in multiple ways: both in clustering and labeling.\nThe impact of this hierarchical alignment is that the relationships between representations of different images change depending on the semantic relationship between them. The representations of images from the same basic category tend to move much closer together, those of images from the same superordinate category tend to move somewhat closer together, and those from different superordinate categories tend to move farther apart. Thus, the alignment procedure drives model representations to reorganize according to the global structure of human knowledge, as we intended. (Results in Fig. 3D-E are for ViT-B; for other models see SI B.3.2.)\nAs an illustrative example, in a standard ViT-B classifier the representations of lizards are initially closer to those of some plants and fruits due their similarity in texture, color, or background conditions; after alignment, they are naturally more similar to the representations of other animals and more distant from those of other, unrelated superordinate categories (see Figure 1D). This organization"}, {"title": "2.4. Alignment improves generalization and out-of-distribution robustness", "content": "What are the consequences of more human-like structured model representations induced by soft-alignment for machine learning tasks? To address this question, we investigated how alignment alters generalization and out-of-distribution robustness of foundation models across different machine learning downstream tasks.\nTo obtain a measure that systematically evaluates the quality of the models' representations, we froze the weights of the neural network model and trained a linear classifier on top of the model's image representations rather than training/fine-tuning the entire model. A key question for any real-world application is how well a model generalizes to new settings, tasks, and objects. Hence, we studied (a) one-shot classification tasks, (b) distribution shift and (c) out-of-distribution robustness.\nOne-shot classification. One-shot classification tasks are among the hardest scenarios for testing generalization: the objective is to classify images given only a single labelled example per class. In Fig. 4 we show one-shot performance of all models before and after AligNet finetuning on a ten image-classification datasets from a wide variety of domains, such as fine-grained birds classification [Birds; Welinder et al., 2010], land use classification from satellite imagery [UC Merced; Yang and Newsam, 2010], and colorectal cancer histology [Colon; Kather et al., 2016]. While for eight combinations of model and dataset, we find a small decrease in performance, the majority of cases (32) show an improvement, sometimes by a substantial margin (e.g. accuray of DINOv2 on Pets shows a 2.7 fold increase). Overall our alignment framework increased the generalization performance on these"}, {"title": "3. Discussion", "content": "The differences between natural intelligence and the capabilities of neural networks are the subject of long-standing debates [Fodor and Pylyshyn, 1988; Lake et al., 2017]. Despite the dramatic progress in Al of the past decade, these discussions persist, because even the most successful deep learning systems seem to fail in non-human-like ways [Lake et al., 2015; Lapuschkin et al., 2019; Geirhos et al., 2020; Thrush et al., 2022; Muttenthaler et al., 2023a; Lake and Baroni, 2023; Puatruaucean et al., 2023].\nIn this work, we have highlighted\u2014and addressed\u2014a key deficiency in a broad class of vision foundation models: their representations do not adequately represent the multi-level conceptual structure of semantic knowledge that is natural to humans (see Sec. 2.2). We demonstrate this deficiency by collecting a new dataset of stratified human similarity judgments across multiple levels of abstraction\u2014which we call Levels (see Sec. 2.2.2). To address this deficiency, we established a methodological framework for aligning deep learning models with human similarity judgments to build systems that exhibit more human-like behavior (see Sec. 2.1). By bootstrapping from a relatively small quantity of human data, we develop a surrogate neural network model of human object similarity judgments. Analogous to prior work on semi-supervised classification [e.g. Lee, 2013; Grandvalet and Bengio, 2004], we use this surrogate model to label a larger dataset for which we do not have human judgments. Specifically, we propose a cluster-based triplet-sampling technique for ImageNet images, which we then label with human-like odd-one-out responses from our surrogate model to create a large new synthetic dataset called AligNet.\nWe fine-tuned a wide range of vision foundation models on AligNet, using a novel KL-divergence-based objective function for injecting human-like similarity structure (i.e., hierarchical conceptual knowledge) into the models. We systematically demonstrate that using this framework yields signifi-cantly increased alignment to human judgments on many cognitive science tasks (see Sec. 2.2), as well as better performance on various representative machine learning tasks (see Sec. 2.4). In other words, soft-alignment helps to alleviate the brittleness of machine learning models under changing environments. Moreover, our results illustrate how the broader paradigm of studying representa-tional alignment [Kriegeskorte et al., 2008a; Sucholutsky et al., 2023] can not only yield insights by measuring how systems differ, but can be leveraged to actively align model representations with human conceptual hierarchies (see Sec. 2.3) to improve the models' generalization and robustness.\nThese results contribute to the long-standing debate over whether neural network models can capture the full range of human intelligence [Fodor and Pylyshyn, 1988; Marcus, 1998; McClelland et al., 2010; Holyoak and Hummel, 2014; Tenenbaum et al., 2011; Lake et al., 2017]. In particular, relational understanding is thought to be a distinguishing feature of human intelligence [Gentner, 2003; Waltz et al., 1999; Penn et al., 2008], and one relevant line of critique has argued that neural networks lack the capability to appropriately represent abstract relations like same and different [Marcus, 1998; Holyoak and Hummel, 2014], or to organize knowledge into hierarchies of concepts and their relations [Tenenbaum et al., 2011]. While some aspects of these critiques have been refuted in simpler synthetic settings [e.g. Geiger et al., 2023], similar criticisms persist for modern foundation models-including the specific claim that empirical weaknesses in vision models make them deeply flawed as cognitive models [Bowers et al., 2022]. Our results show that, while standard objectives do not adequately capture the hierarchy of relations within and across categories, these relations can be distilled into the models\u2014and this procedure substantially improves the models' resilience under the kinds of distribution shifts that have concerned some prior critiques.\nAlthough in this work we focused on the visual domain, similar global misalignments likely arise in other areas of machine learning research. For instance, in natural language processing, models are similarly trained with objectives that focus on distinguishing between close matches (e.g., prediction"}, {"title": "4. Methods summary", "content": "4.1. AligNet\nTHINGS dataset objects. Similar to Muttenthaler et al. [2023a,b], we use the THINGS triplet odd-one-out dataset [Hebart et al., 2020] for learning an affine transformation into a (global) human object similarity space. The THINGS dataset can formaly be defined as $D := (\\{a_s, b_s\\} | \\{i_s, j_s, k_s\\})^n_{s=1}$ which denotes a dataset of n object triplets and corresponding human odd-one-out responses, where $\\{a,b\\} \\subset \\{i, j, k\\}$ and $\\{a,b\\}$ is the object pair that was chosen by a human participant to have the highest similarity. Let $X \\in R^{m\\times p}$ be the teacher model representations for the m = 1854 objects in the THINGS dataset. Note that each category in the THINGS dataset is represented by one object image. From X we can construct a similarity matrix for all object pairs $S := XX^T \\in R^{m\\times m}$, where $S_{ij} = x_i^T x_j$ is the representational similarity for objects i, j.\nLinear transformation. Our goal is to learn an affine transformation into the THINGS human object similarity space of the form: $x' = Wx + b$. Here, $W \\in R^{P\\times P}$ is a learned transformation matrix, $b \\in R^{P}$ is a bias, and $x \\in R^{P}$ is the neural network representation for a single object image in the THINGS dataset. We learn the affine transformation for the representation of the image encoder space of the SigLIP-So400m teacher model (see Sec. A.1.4 for details about the teacher model). Using this affine transformation, an entry in the pairwise similarity matrix $S'$-which represents the similarity between two object images i, j-can now be written as $S'_{ij} := (Wx_i + b)^T (Wx_j + b)$.\nUncertainty distillation. We mainly follow the optimization process introduced in Muttenthaler et al. [2023b]. However, we modify their approach by injecting uncertainty measures about human odd-one-out responses into the representation space of the teacher, using a recent approximate Bayesian inference method for learning object concepts from human behavior [Muttenthaler et al., 2022]. Thus, we replace the negative log-likelihood of the discrete human odd-one-out choices\u2014which we refer to as hard-alignment\u2014with the negative log-likelihood of the probabilities for the pairwise triplet similarities obtained from the Bayesian inference model\u2014referred to as soft-alignment. Hard-alignment is defined as,\n$L_{hard-align}(S') := \\frac{1}{n}\\sum_{s=1}^n \\log q (\\{a, b\\}_s|\\{i_s, j_s, k_s\\}, S'),$\nwhere the affine transformation of the teacher's representation space is optimized to match the (discrete) human responses. In contrast, soft-alignment gives the following KL divergence between the human uncertainties $p^*$ and the teacher model probabilities $q$ obtained from applying a softmax function to the pairwise similarities of S,\n$L_{soft-align} (S') := \\frac{1}{n} \\sum_{s=1}^n \\log p^* (\\{i_s, j_s, k_s\\}) - p^* (\\{i_s, j_s, k_s\\}) \\log q_s (\\{i_s, j_s, k_s \\}, S'),$\nThe final objective for learning the uncertainty distillation (UD) transformation is defined as\n$\\arg \\min_{W,b} L_{soft-align} (X, W, b) + \\lambda \\Big|\\Big|W - (\\lambda \\sum_j |W_{ii}|/p)I \\Big|\\Big|^2,$\nwhere $I \\in R^{P\\times P}$ is the identity matrix. The right-hand side of the above objective is an $l_2$-regularization whose aim is to preserve the nearest neighbor information (or equivalently, the local similarity structure) of the pretrained representations while learning an affine transformation into the THINGS human object similarity space. The above equation is minimized using standard SGD."}, {"title": "Superordinate clusters.", "content": "We embed the ImageNet train set in the transformed representation space of the teacher and cluster the representations into superordinate categories using k-Means clustering. We find the optimal number of clusters k using the Elbow criterion. We use the clusters for generating triplets of distinct ImageNet images by always sampling two images from the same cluster and one image from a different cluster. For all triplets that we generate we identify their odd-one-out choice using the representations of the surrogate teacher model."}, {"title": "Human-like responses.", "content": "The responses of the surrogate model simulate a dataset of human-like triplet odd-one-out responses. In addition to the discrete odd-one-out choices, the dataset includes the exact relationships among all pairwise similarities in a triplet obtained from the probability space of the teacher model. Thus, we now have access to soft choices."}, {"title": "AligNet objective.", "content": "To distill the pairwise similarity structure of the teacher into a different student network, we introduce a novel Kullback-Leibler divergence based objective function similar to Eq. 2 that facilitates the distillation process. This loss function is defined as\n$L_{alignet} (S', S^{\\dagger}) := \\frac{1}{B} \\sum_{s=1}^B \\log \\sigma ( [S'_{i,j}, S'_{i,k}, S'_{j,k}] , \\tau' ) - \\sigma ( [S^{\\dagger}_{i,j}, S^{\\dagger}_{i,k}, S^{\\dagger}_{j,k}] , \\tau' ) \\log \\sigma ( [S^{\\dagger}_{i,j}, S^{\\dagger}_{i,k}, S^{\\dagger}_{j,k}] , \\tau^{\\dagger}),$\nwhere $S'$ and $s^{\\dagger}$ are the pairwise similarity matrices of the teacher and the student representation spaces respectively, B is the number of image triplets in a batch, $\\sigma$ is a softmax function that transforms the similarities into probabilities, and $\\tau'$ and $\\tau^{\\dagger}$ are temperature values for the teacher and student that we find via grid search. The final AligNet objective is defined as\n$\\arg \\min_{\\theta^T} L_{alignet} (f_{\\theta^T}) + \\lambda ||\\theta^* - \\theta^{\\dagger}||_2,$\nwhere $\\theta^*$ are the parameters of the pretrained student before fine-tuning and $\\theta^{\\dagger}$ are the parameters of the fine-tuned student. The right-hand side is similar to the $l_2$-regularization employed for learning the UD transformation. It tries to preserve the structure of the pretrained representation space as a function of $\\lambda$ which determines the strength of the regularization."}, {"title": "4.2. Representational Similarity Analysis", "content": "Representational Similarity Analysis (RSA) is a well-established method for comparing neural network representations\u2014extracted from an arbitrary layer of the model\u2014to representations obtained from human behavior [Kriegeskorte et al., 2008a]. In RSA, one first obtains representational similarity matrices (RSMs) for the human behavioral judgments and for the neural network representations (more specific details can be found in the Supplementary Material). These RSMs measure the similarity between pairs of examples according to each source. As in previous work [Cichy et al., 2019; King et al., 2019; Hebart et al., 2020; Muttenthaler et al., 2023a,b], we flatten the upper triangular of human and model RSMs respectively and quantify their similarities using use the Spearman rank correlation coefficient. In contrast to the Pearson correlation coefficient, the Spearman rank correlation is scale-invariant and thus better suited to measure similarities of judgments obtained from different sources."}, {"title": "4.3. Levels", "content": "We collected a new multi-level similarity judgment dataset from N = 473 human participants, which we named Levels. This dataset is based on the triplet odd-one-out task and included judgments on three different types of triplets: coarse-grained semantic, which require deciding on the odd-one-out in broadly different categories; fine-grained semantic, which involved discerning subtle distinctions within the same category; and class-boundary, which tested for the capacity to identify category"}, {"title": "4.4. Alignment with conceptual hierarchy", "content": "When analyzing alignment with the conceptual hierarchy, we use the original ImageNet category labels for the images [Deng et al., 2009]. ImageNet is structured according to the WordNet hierarchy, from which we extract basic- and superordinate-categories that align with the prior cognitive work."}, {"title": "A. Methods", "content": ""}, {"title": "A.1. AligNet framework", "content": "This section is organized as follows: We start by describing how we transform model representa-tions into a space that matches human similarity judgments about coarse-grained semantic object relations. We introduce a novel affine transformation that matches human similarity judgments and injects the uncertainties that humans assign to their triplet odd-one-out choices into a model's repre-sentation space. Using these human-aligned representations, we sample triplets of ImageNet [Deng et al., 2009] images differently than uniform random sampling by clustering the representations into superordinate categories and using those clusters for data partitioning. Finally, after having created AligNet triplets, we can fine-tune models with a novel triplet loss object function."}, {"title": "A.1.1. Representational alignment", "content": "Data. To increase the degree of alignment between human and neural network similarity spaces", "2023": "from the public THINGS object concept and image database [Hebart et al.", "2019": ".", "D_{things}": {}}]}