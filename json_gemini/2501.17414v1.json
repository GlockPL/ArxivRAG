{"title": "Reqo: A Robust and Explainable Query Optimization Cost Model", "authors": ["Baoming Chang", "Amin Kamali", "Verena Kantere"], "abstract": "In recent years, there has been a growing interest in using machine learning (ML) in query optimization to select more efficient plans. Existing learning-based query optimizers use certain model architectures to convert tree-structured query plans into representations suitable for downstream ML tasks. As the design of these architectures significantly impacts cost estimation, we propose a tree model architecture based on Bidirectional Graph Neural Networks (Bi-GNN) aggregated by Gated Recurrent Units (GRUs) to achieve more accurate cost estimates. The inherent uncertainty of data and model parameters also leads to inaccurate cost estimates, resulting in suboptimal plans and less robust query performance. To address this, we implement a novel learning-to-rank cost model that effectively quantifies the uncertainty in cost estimates using approximate probabilistic ML. This model adaptively integrates quantified uncertainty with estimated costs and learns from comparing pairwise plans, achieving more robust performance. In addition, we propose the first explainability technique specifically designed for learning-based cost models. This technique explains the contribution of any subgraphs in the query plan to the final predicted cost, which can be integrated and trained with any learning-based cost model to significantly boost the model's explainability. By incorporating these innovations, we propose a cost model for a Robust and Explainable Query Optimizer, Reqo, that improves the accuracy, robustness, and explainability of cost estimation, outperforming state-of-the-art approaches in all three dimensions.", "sections": [{"title": "1 INTRODUCTION", "content": "Query optimization is crucial for database management systems (DBMSs), directly impacting query execution performance. Accurately and efficiently estimating the cost of candidate query execution plans and selecting the optimal one remains a significant challenge. A query execution plan is typically represented as a tree,"}, {"title": "2 PROBLEM STATEMENT", "content": "This section highlights three core challenges in learning-based query optimization. First, we discuss query plan representation, which demands accurately encoding tree-structured plans into vectors while preserving node-level and structural information. Second, we present the explainability gap arising from ML versus traditional cost models. Finally, we highlight the need for robustness against execution uncertainties and discuss the limitations of current uncertainty quantification practices. In the following, we detail each challenge and define the problems to be solved."}, {"title": "2.1 Query Plan Representation", "content": "In learning-based query optimizers, query plan representation learning begins with taking the physical query plan as input, using a feature encoder and tree model to generate plan representations. These representations encapsulate critical information, including operators, parent-child relationships, and underlying data, serving as essential inputs for downstream tasks. Consequently, their quality sets a performance ceiling for the entire cost model, making the tree model's output pivotal to the optimization process.\nFormally, a physical query plan is represented as a rooted tree T = (V, E), where each node v \u2208 V represents an operator and edges (v', \u03c5) \u2208 E specify child-to-parent relationships, defining execution dependencies from leaves to the root. Each node v has a feature vector x \u2208 R^d, capturing node-level information such as operators, relations, and predicates. The objective is to design a tree model 96, parameterized by 6, that maps the tree T and the node features {x2}vey to a fixed-size representation hy \u2208 R^k:\nhy = 94 (T, {x}).\n A downstream model fo, parameterized by 6, utilizes this representation to predict the estimated execution cost \u0177 \u2208 R:\n\u0177 = fe(hr).\nThe optimal * and * are obtained by minimizing the expected cost estimation error over the distribution of query plans D:\n0*, * = arg min E(T,{x},y)~D [L (fo (94 (T, {x})), y)], (3)\n\u03b8,\u03c6\nwhere L is a loss function and y is the actual execution cost."}, {"title": "2.2 Explainability of Learning-based Cost Model", "content": "In classical query optimizers, cost models typically rely on statistical methods (e.g., histograms) and use a transparent, modular structure. The total cost of a query plan is the sum of its constituent operators' or subplans' costs. In such models, for a query plan tree T, the cost of a parent node up derives from its children's costs plus a local cost capturing the operator's own overhead:\nCclassical (up) = c(up) + \u03a3vc\u2208 children(up) Cclassical (uc).\nwhere c(up) is an function that estimates the local cost of operator Up. By recursively applying Eq.4 from the leaves to the root of the query plan, the total estimated cost of T is obtained as:\nCclassical(T) = Cclassical(root(T)).\nThis bottom-up aggregation makes final estimates clearly traceable to each node, enabling developers to tune performance at the operator or subplan level with clear insights into how local costs flow into the final total cost. In contrast, learning-based cost models predict the cost Clearned (T) using a parameterized (often black-box) function fe trained on historical query data, which do not explicitly decompose the plan into subgraphs in an interpretable way. Although these models often yield higher accuracy, they forgo the built-in transparency of classical approaches and obscure how individual predicates or local substructures affect the total estimate. This lack of transparency hinders the diagnosis of cost misestimations, complicates fine-grained tuning, and erodes trust when estimates deviate from observed execution times.\nTo restore the beneficial transparency of classical cost models, a learned cost model should have the ability to quantify the contribution of each subgraph Gs CT to the total cost Clearned (T). These subgraphs typically correspond to logical or physical subplans that can be individually optimized (e.g., specific joins or indexes). Identifying their contributions helps developers locate bottlenecks or highlight operators that may benefit from tuning or rewriting.\nDefinition 2.1 (Explainability of a Learning-based Cost Model). Let Gs = (Vs, Es) be any subgraph of T = (V, E), with Vs \u2286 V and Es E. A model is considered to have explainability with respect to subgraph contributions if there exists a function:\nF: {(Gs) | Gs \u2286 T} \u2192 R (6)\nthat quantifies how Gs influences Clearned (T). Since a black-box model fe does not directly expose how individual subgraphs contribute to the overall cost, F must be derived or learned to capture"}, {"title": "2.3 Robust Learning-based Cost Estimation", "content": "Most cost models prioritize the accuracy of cost estimates in their design, often overlooking inherent uncertainties. In reality, uncertainties in query plan execution are significantly influenced by factors such as structural characteristics, specific operations or predicates, and data properties. Moreover, estimation methods themselves may introduce further limitations. In this context, robustness in cost estimation refers to the ability of a cost model to maintain accurate plan selection despite these inherent uncertainties.\nThe classical problem of optimal plan selection is defined as: given a finite set of candidate execution plans P = {p1, p2, ..., Pn} and a cost function f(pi) that estimates the cost of executing plan pi for i = 1, ..., n, the goal is to find the optimal plan p* such that:\np* = arg min f(pi) (8)\nPiEP\nThis formulation overlooks the inherent inaccuracies in estimated costs, which may lead to selecting suboptimal plans at runtime. Robust query optimization aims to minimize such risks by incorporating uncertainties. Here, we introduce a function u(pi) that quantifies the uncertainty in the cost estimate for plan pi. The robust plan selection problem is then formulated as finding the plan p* that considers both estimated cost and uncertainty:\np* = arg min h (f(pi), u(pi)) (9)\nPiEP\nwhere h is a function representing the optimizer's strategy in balancing cost and uncertainty.\nA key challenge is quantifying these uncertainties and incorporating them into query optimization to achieve more robust performance. Current state-of-the-art research quantifies model uncertainty through methods such as Bayesian neural networks [34], Monte Carlo Dropout [11]; Gaussian negative log-likelihood [28] and spectral-normalized neural Gaussian processes [20] for query plan data uncertainty. After computing u(pi), they use predefined rules in plan selection to balance f(pi) and u(pi), thereby finding more robust and efficient plans. However, a significant limitation is that plan comparisons are excluded from the cost model's training, preventing adaptive refinement of its balancing strategies.\nIn summary, designing an adaptive balancer h that reconciles f(pi) and u(pi) while incorporating plan comparison results into the cost model's training remains a critical challenge for enhancing the robustness of learning-based query optimization."}, {"title": "3 MODEL OVERVIEW", "content": "Addressing the three main challenges from Section 2: limited query plan representation expressiveness, lack of explainability in cost estimations, and the need for robust plan selection under uncertainty, we propose an integrated solution consisting of three techniques. First, we introduce BiGG (Section 3.1), a novel representation learning method based on bidirectional GNNs and GRU, which preserves both node-level features and structural information, producing higher-quality plan representations. Second, to tackle the black-box nature of learned cost models, we develop a subtree-based explainability technique (Section 3.2) that quantifies the contribution of query plan subgraphs to cost predictions, restoring transparency comparable to classical methods while achieving higher accuracy. Finally, we present a robust learning-to-rank cost model (Section 3.3) that adaptively quantifies and integrates uncertainty into the plan selection process via pairwise plan comparisons, mitigating suboptimal choices arising from estimation errors or volatile execution environments. In the following, we detail each technique and illustrate how it addresses its corresponding challenge."}, {"title": "3.1 BiGG: A Novel Technique for Query Plan Representation Learning Based on Bi-GNNS", "content": "To improve query plan representation, we build on our previous work [2] by introducing BiGG, a novel representation learning method that leverages bidirectional GNNs and a GRU-based aggregator. This design preserves more during transformation, produces higher-quality plan embeddings, and yields more precise cost predictions, providing a robust foundation for downstream tasks."}, {"title": "3.1.1 Bidirectional GNNs", "content": "To improve the tree model's ability to represent query plans accurately, we employ GNNs due to their proficiency in capturing graph topology [14]. We innovatively treat the query plan trees as two single-directional graphs with opposite edge directions (parent-to-child and child-to-parent). In each layer , these two graphs are processed independently by TransformerConv [33] layers. We then integrate the corresponding node features from the two output learned graphs through a learnable parameter, which makes it possible to transmit information in both directions while still utilizing the direction information of the edges and retaining relevant structural information. This bidirectional GNN design facilitates information flow in both directions, enabling nodes to learn from both sides, unlike using single-directional edges. By treating the query plan tree as two graphs with opposite edge directions, the model preserves dependencies between parent and child nodes compared to using undirected edges. TransformerConv layers with multi-head attention [37] allow each node to adaptively aggregate neighbour information, enhancing the model's ability to capture the tree's local graph topology and global dependencies. This design benefits from GNNs but addresses the limitations of single-directional and undirected GNNs in learning query plans, markedly improving tree-structured plan representation learning."}, {"title": "3.1.2 Aggregation Operator based on GRUs", "content": "Conventional graph aggregation methods often yield suboptimal results with query plans because they typically use global pooling of node features, ignoring the tree's structural information. To address this, we apply"}, {"title": "3.1.3 Stronger Basis for Downstream Techniques", "content": "By capturing both node-level features and structural dependencies, BiGG serves as a powerful instantiation of gf, effectively addressing the representation problem outlined in Section 2. The richer representation not only enhances fe's cost estimation accuracy but also provides a stronger foundation for our subsequent techniques: Specifically, our subtree-based explainer (See Section 3.2) can more precisely assess each subplan's contribution to overall cost based on their embeddings, thus elucidating how particular subgraphs drive execution time; Additionally, our robust learning-to-rank cost model (See Section 3.3) benefits from these enhanced embeddings to better distinguish subtle plan differences under uncertainty, mitigating misestimation risks and leading to more reliable plan selection."}, {"title": "3.2 An Explainability Technique for Learning-based Cost Models", "content": "In Section 2.2, we discuss the lack of transparency in learning-based cost estimators. To address it, we propose a novel subtree-based explainability technique that extracts valid subtrees from a query plan, ensuring each subplan remains executable and retaining all essential operators and relations for accurate contribution estimation. We then employ a learning-based method to quantify the embedding similarity between each subtree and the full plan. This similarity measure allows the model to automatically learn and infer how each subgraph influences the final cost prediction, restoring transparency similar to traditional cost models. Below, we explain how to extract these subtrees, measure their embedding similarity, and integrate these insights into a learning-based explanation technique that trains simultaneously with the cost model."}, {"title": "3.2.1 Subgraph Extraction Based on Query Plan Subtrees", "content": "To explain the impact of subgraphs on the final cost prediction of query plans, a straightforward approach is to feed each subgraph Gs directly into the cost model. However, query plan trees impose strict parent-child dependencies, and omitting any child node creates"}, {"title": "3.2.2 Similarity Quantification Between Query Plan and Subtree Embeddings", "content": "We propose an approach to estimate the impact of subtrees on cost predictions by quantifying the embedding similarity between query plans and their subtrees. The literature [22] shows that if a subgraph significantly impacts the final prediction, there should be a notable similarity between the subgraph's embedding and the complete graph's embedding, enabling the model to make similar decisions. The similarity can be measured using functions such as cosine similarity, mutual information (MI) [15] and learning-based methods. By ranking these similarities, the subgraphs with the greatest impact on predictions can be identified."}, {"title": "3.2.3 Learning-based Explainability Technique", "content": "Building on subtree extraction and subtree-plan embedding similarity, we propose an explainability technique for learning-based cost models. We first extract subtrees of various sizes from the original query plan and encode them (along with the full plan) using the same tree model g\u00f8. During training, a learning-based explainer model Y automatically learns the contribution of each subtree to the overall cost prediction. In particular, Y estimates the contribution ECstk \u2208 [0, 1] of each subtree k by quantifying the similarity between its embedding Embstk and the embedding of the complete query plan Embot as below:\nECstk = \u03a8(CONCAT(Embstk, Embot)), ECstk \u2208 [0, 1] (10)\nThe actual contribution ratio ACst is defined as the ratio of the subtree's actual execution time to entire plan's total execution time:\nACstk = ETstk / ETot , ACstk \u20ac [0, 1]\nwhere the actual contribution ratio of the original query plan tree is always 1. An explanation loss function is designed to minimize the discrepancy between the estimated and actual contribution ratios: Given a query plan set P with N plans, a plan pi with K\u012f subtrees where pi \u2208 P, the explanation loss is shown as:\nExplanationLoss = 1/N \u03a3 Ni=1 1/Ki \u03a3 Ki l=1(ACstik - ECstik) 2\nThus, the explainer learns the relationship between subtree embeddings and the complete plan embedding, enabling it to estimate contribution ratios and explain the cost model's predictions. After training, for cost estimations that do not require explanations, Embot can directly represent the query plan without extracting subtrees or activating the explainer, reducing inference time. When explanations are required, the technique estimates each subtree's contribution, revealing how every subgraph or node operation affects the final prediction, as outlined in Algorithm 1.\nThis explainability technique addresses the challenge in Section 2.2. We employ the learning-based explainer as the function"}, {"title": "3.3 A Robust Learning-to-Rank Cost Model Based on Uncertainty Quantification", "content": "In Section 2.3, we discussed the challenges of robust plan selection under inherent uncertainties and limitations in existing uncertainty quantification methods. To address these issues, we propose a robust learning-to-rank cost model that quantifies and integrates uncertainty into cost estimation. Rather than relying on purely numeric estimates or fixed strategies, our model employs a ranking loss with pairwise plan comparisons to learn how to adaptively combine cost and uncertainty into a single metric for plan selection. Learning from these comparisons, it identifies cheaper plans more reliably while factoring in uncertainty, thereby enhancing robustness in plan selection. Below, we detail how the model quantifies uncertainty and leverages pairwise comparisons during training."}, {"title": "3.3.1 Uncertainty Quantification", "content": "Real-world query plans often exhibit variability due to data uncertainty [12], which in ML can stem from noise in inputs or labels, or from low-dimensional features that fail to adequately learn the sample. During cost estimation, uncertainty can arise from various complex factors, including fluctuations in execution time due to changes in the load or hardware conditions of DBMSs when sample query plans are executing and the variability in the plan representations. In this work, we focus on this uncertainty and develop a method to quantify and utilize it.\nA neural network can be designed to predict the parameters of the normal distribution [7], allowing it to predict not only the expected conditional value, but also the conditional variance of the target based on training data and the input sample. This functionality is achieved by integrating a secondary output branch into the original learning-based cost estimator that is tasked with variance prediction as shown in Figure 7. The optimization of this model involves maximizing the log-likelihood with a Gaussian prior [28], as demonstrated in the following uncertainty loss function:\nUncertaintyLoss = 1/Pi \u03a3 Pi i=1 (logopi + (Ypi - Hp\u2081)2/(2 \u03c3 i2)).\nwhere for the i-th plan embedding as input, up\u2081 is predicted by the first branch of the estimator and represents the expected value of the estimated cost, of, is from the second branch and reflects the expected variance as the data uncertainty, and yp\u2081 stands for the label (actual cost) of the input plan. Minimizing this loss function, we can obtain both the estimated cost and the expected conditional variance for a given plan embedding, enabling effective uncertainty quantification for the subsequent plan selection phase."}, {"title": "3.3.2 Learning-to-Rank Pairwise Plan Comparison", "content": "As discussed in Section 2, most existing uncertainty-aware cost estimation models usually apply the obtained uncertainty and estimated costs to the fixed plan selection strategy independently of the model training phase, preventing self-improvement based on selection outcomes. To address this, we propose a novel learning-based cost model architecture as shown in Figure 7 that uses a ranking loss function with plan pairs as inputs, allowing the model to adaptively integrate uncertainty and cost estimates. This approach improves the ability"}, {"title": "4 MODEL ARCHITECTURE", "content": "By integrating the three techniques in Section 3, we propose Reqo, a learning-based cost model that improves cost estimation accuracy, explainability, and plan selection robustness. Figure 9 shows its architecture, consisting of a plan feature encoder and three modules (representation learning, estimation, and explanation). Sections 4.1 to 4.5 detail each component and the training process."}, {"title": "4.1 Plan Feature Encoding", "content": "A query execution plan contains details about the operators used to access and join data, their physical implementations, sequences, and the tables and columns involved. To transform this complex information into fixed-length node features for the tree model, we propose a plan encoder inspired by RTOS [39]. As shown in Figure 8, each node feature comprises three parts:\nNode Type Embedding. We perform one-hot encoding of the node types (e.g., Hash Join, Index Scan) and pass them through a fully connected layer to obtain the node type embedding E(nodetype).\nTable Embedding. One-hot encoding is applied to all tables used in the node's operation to obtain E(table), ensuring no loss of information even if there are no related predicates.\nPredicate Embedding. For numerical columns, predicate operations are classified into eight cases (e.g., <, =, >, <, in). Each column c is represented by a feature vector F(c) of length eight, capturing these cases. The predicate values are normalized based on the column's value range in the database. For non-numerical columns like strings, we use word2vec [25] to convert characters into numerical values. Each column has a dedicated matrix M(c) to process F(c) and generate the column embedding E(c) = F(c) \u00d7 M(c).\nMax pooling is performed on all column embeddings of the same table to obtain the table embedding E(t). To avoid information loss, we concatenate the embeddings of all tables to form the predicate embedding E(predicate). Finally, we concatenate E(nodetype), E(table), and E(predicate) to get the node feature."}, {"title": "4.2 Representation Learning Module", "content": "The representation learning module generates plan-level embeddings from the encoded query plan tree. We employ our proposed tree model BiGG, which consists of four bidirectional GNN layers and a GRU aggregation layer (detailed in Section 3.1). This module"}, {"title": "4.3 Estimation Module", "content": "The estimation module (detailed in Section 3.3) processes the embedding from the representation learning module. This representation is input into a multi-layer perceptron (MLP) with 3 FC layers. The output then feeds into two separate branches, each consisting of an MLP with 3 layers. The first branch uses a Sigmoid activation function to produce a normalized expected execution time, while the second employs a SoftPlus function to generate a non-negative variance representing uncertainty. These outputs are integrated by an MLP with 2 layers and a Sigmoid activation, yielding an integrated value, which is used for plan comparison or selection."}, {"title": "4.4 Explanation Module", "content": "The explanation module extracts subtrees from the vectorized query plan tree using the mechanism detailed in Section 3.2.1. These subtrees are processed by the representation module to obtain subplan-level embeddings. Each subtree embedding is then concatenated with the complete plan embedding and inputs into the explainer, which consists of an MLP with 4 FC layers and a Sigmoid activation function. The explainer predicts the contribution ratio (ranging from 0 to 1) of each subtree toward the predicted execution time of the entire plan. These contribution values are used to explain the impact of specific subgraphs and nodes on the final prediction."}, {"title": "4.5 Model Training and Testing", "content": "The training of Reqo requires input in the form of query plan pairs, using the actual execution times of these plans and their subplans as labels. The model compares integrated values (execution time estimates and uncertainty) between different plans, leveraging ranking loss (Eq. 15) to learn from these comparison results. The estimation module is trained with both uncertainty loss (Eq. 13) and ranking loss, while the explanation module uses the explanation loss (Eq. 12). The explanation module is optional: If the model does not require explainability, training only uses the loss function (Eq. 16). When the explainability module is required, the overall loss function becomes the sum of the three losses, as shown in Eq. 17.\nLoss = UncertaintyLoss(Eq. 13) + RankingLoss(Eq. 15) + ExplanationLoss(Eq. 12) \nReqo does not require inputs to be paired during the testing phase. After training, plans can be directly ranked based on the integrated value produced by the estimation module, thereby achieving the prediction plan selection incorporating uncertainty."}, {"title": "5 EXPERIMENTAL STUDY", "content": "We experimentally evaluate and compare Reqo's performance with state-of-the-art cost models. The experimental setup is described"}, {"title": "5.1 Experimental Setup", "content": "All experiments were conducted on a Linux server (8-core Intel Silver 4216 CPU @ 2.1GHz, 128GB RAM, 32GB NVIDIA V100 GPU). PostgreSQL 15.1 was used to compile and execute queries for workload generation. The prototype was implemented in Python 3.10 using PyTorch [29], with hyperparameters tuned via Ray Tune [17]. ADAM [13] was used as the optimizer during training, with dropout and early stopping applied to prevent overfitting. All experimental results were averaged over 10-fold cross-validation.\nBenchmarks. We evaluate all the query optimizer cost models on four widely used benchmarks:\nThe STATS dataset and STATS-CEB workload [8] include 8 tables from the Stats Stack Exchange network with more complex data distributions than IMDB. STATS-CEB provides 146 query templates with varying join sizes and types. We generate a workload of 3,000 queries by adding random predicates to these templates."}, {"title": "5.2 Experimental Methodology", "content": "Comparison. We compare our proposed model, Reqo, against the classical RDBMS optimizer PostgreSQL and three recent works: Bao [23], Lero [45], and Roq [12]. Bao is selected for its advanced performance, Lero for its learning-to-rank mechanism, and Roq for its approach to quantifying uncertainty for robust plan selection. These comparisons allow us to evaluate improvements across different aspects based on state-of-the-art mechanisms.\nPostgreSQL serves as the baseline, representing the performance of commercial query optimizers. We use the plans selected by PostgreSQL's estimated cost for plan selection and explainability.\nBao is a learned query optimizer that enhances traditional optimizers by applying hints and reinforcement learning. We focus on its cost model, which predicts execution time by processing the vectorized plan tree through TCNN and MLP.\nLero is a learning-to-rank query optimizer. Similar to Bao in plan encoding, Lero trains its cost model to classify which of two plans is better rather than predicting numerical values. Thus, it is excluded from our cost estimation accuracy comparison.\nRoq is a robust risk-aware query optimization framework with a GNN-based query-level encoder and a plan encoding similar to Bao. Its cost model estimates execution time and uncertainty, and applies them in fixed robust plan selection strategies.\nReqo is our proposed model and is divided into segments for an ablation study. The base model (BiGG + single-branch MLP with MSE loss, no explanation) serves as a benchmark. We then add un-certainty quantification (base+unc.) to observe the impact of using UncertaintyLoss and dual-branch MLP without applying it to plan selection. Next, we integrate estimated execution time and uncertainty using a fixed value (base+unc.+ffixed) for plan selection to evaluate whether the ranking-based approach (base+unc.+flearned or Reqo w/o expl.) enhances the robustness. We also introduce the explanation module (Reqo w/ expl.) to assess its impact. Throughout, \"Reqo\" refers to the model that contains all modules.\nEvaluation Metrics. We employ six evaluation metrics:\n1. Prediction Error: We evaluate cost estimation performance using Q-Error [26], defined as Q-Error = max(yet, Yat)/min(Yet, Yat), where yet and yat are the estimated and actual execution times.\n2. Correlation: We use Spearman's rank correlation to measure the relationship between estimated and actual execution time, with values closer to 1 indicating a stronger correlation. Unlike Pearson's coefficient, it is less sensitive to outliers and scale differences, making it suitable for measurements that vary greatly in magnitude.\n3. Total Runtime Ratio: This ratio is computed by dividing the sum of actual execution times for optimizer-selected plans by the sum of actual execution times for optimal plans across all queries, offering an evaluation of overall plan selection performance.\n4. Plan Suboptimality: For a set of candidate execution plans p for the same query, we rank them by actual execution time ET(.) and identify the optimal plan po with the shortest execution time. The suboptimality of a plan pi e p is defined as:\nPlan Suboptimality = ET(pi) / ET(po)\nThis metric ranges from [1,\u221e) and reflects the model's ability to select optimal plans. Analyzing the distribution, especially the worst cases, helps assess the model's robustness in plan selection [6].\n5. Explanation Top-K Subgraph Accuracy: This metric assesses the model's accuracy in identifying the most influential subgraphs contributing to the final prediction. Each query plan is divided into minimal, non-overlapping subgraphs, each containing at most one parent node and its children if they are leaf nodes. If a parent has a non-leaf child, that child is excluded from the parent's subgraph and becomes the parent in a separate subgraph. Ranking these subgraphs by their contribution, the metric checks whether the top-K model selected most influential subgraphs Spred,k match the actual top-K subgraphs Sactual,k. Let I be an indicator function returning 1 if the subgraphs match and 0 otherwise. Specifically, Expl. Top-K Subgraph Acc = I{Si l({Spred,k = si Sactual,kk=1)\n6. Explanation Top-K Subgraph Influence Ratio: This metric evaluates the model's ability to identify most influential subgraphs by comparing the sum of actual contributions from the top-K model-selected subgraphs Spred,k against the top-K actual subgraphs Sactual,k. Formally,Expl. Top-K Subgraph Infl. Ratio = \u03a3K=1 AC(Spred,k) / \u03a3K=1 AC(Sactual,k)\nSubgraphs are partitioned the same as above. Unlike binary accuracy, this ratio captures cases where the model's chosen subgraphs contribute significantly, even if they are not the top-K subgraphs, providing a more comprehensive evaluation of explainability."}, {"title": "5.3 Experimental Results", "content": "5.3.1 Comparison for Cost Estimation. Figure 10 demonstrate that Reqo consistently outperforms Bao and Roq across all datasets in terms of Q-error and Spearman's correlation metrics. Reqo achieves lower Q-Error values at various percentiles and higher Spearman's correlation coefficients, indicating superior cost estimation performance. Notably, in complex workloads like TPC-DS, Reqo still excels, showcasing its effectiveness in handling challenging scenarios with more complex and deeper query plans. These results confirm Reqo's advancement over existing models and its effectiveness in both simple and complex query optimization tasks."}, {"title": "5.3.2 Comparison for Plan Selection", "content": "The runtime results (Figure 11) show the models' plan selection performance across the entire workload. Reqo consistently surpasses other models, demonstrating substantial performance enhancements. Notably, in the more complex TPC-DS workload, models like Bao, Lero, and Roq do not perform as well as PostgreSQL in terms of total runtime, despite exhibiting better performance in simpler workloads. In contrast, Reqo's advanced feature encoder, the powerful representation capabilities of BiGG, and the ranking-based uncertainty quantification mechanism ensure superior performance even with complex query plans. Specifically, in optimizing total runtime ratio, Reqo achieves performance enhancements of 16.6% over PostgreSQL, 24.6% over Bao, 20.4% over Lero, and 18.6% over Roq. These results underscore Reqo's efficiency in handling complex query scenarios, highlighting its superiority in runtime performance optimization."}, {"title": "5.3.3 Ablation Study", "content": "To analyze the impact of our proposed techniques on cost estimation and plan selection, we conducted an ablation study on the TPC-DS workload. Figure 12a shows variations in Spearman's correlation for different Reqo configurations versus other cost models. All learning-based models significantly"}, {"title": "5.3.4 Comparison for Robustness", "content": "We evaluate plan selection robustness via plan suboptimality, with results shown in Figure 13."}, {"title": "5.3.5 Comparison for Explainability", "content": "From Figure 15, we observe that the traditional PostgreSQL optimizer, despite underperforming in previous experiments compared to learning-based models, demonstrates relatively good explainability. Learning-based cost models, including our proposed Reqo (without the explainability technique), excel in cost estimation accuracy and robustness but fall short of PostgreSQL in explainability. The main reason is that these models prioritize to predict cost estimates, rather than identifying specific subgraphs that significantly influence the embedding and drive the model's predictions. This issue is a common limitation across nearly all current learning-based cost models: they provide accurate cost estimates but lack the ability to explain why certain decisions are made, hindering targeted query optimization.\nIn contrast, the classical optimizer used by PostgreSQL bases its cost predictions on detailed cost statistics for each node operation in the plan, enabling it to clearly show its decision-making process and therefore performs better in this experiment. The experimental results demonstrate that, without our proposed explainability technique, the learning-based cost models perform worse than PostgreSQL in almost all explanation evaluation metrics. In particular, there is a clear gap in accuracy when the learning-based cost model is required to simultaneously identify the two most influential subgraphs in order. This suggests that although these cost models can provide vague explanations, they struggle to precisely pinpoint"}, {"title": "6 RELATED WORK", "content": "This study primarily involves three aspects of query optimization cost models, focusing on learning-based technologies for tree models and robustness techniques for query optimizers.\nLearning-based Tree Model. RNN-based models like LSTM [9", "40": "due to their ability to capture long-term dependencies but require transforming trees into sequences, resulting in loss of structural information. Models such as Saturn [21", "43": "use self-attention mechanisms to enhance plan representation but still convert query plans into sequences. Tree-specific models like Tree-LSTM [36", "27": "process tree-structured data directly, preserving structural relationships and improving feature aggregation. However, they still do not work well with deep query plans [2", "2": ".", "24": "indicate a degree of robustness to estimation errors or limit the worst-case scenario, inherent predictive uncertainties persist without systematic methods for quantification. Recent research proposes solutions for quantifying uncertainty. Studies such as [12, 19, 38, 41, 42", "28": "and [5", "20": "in cost model training to estimate data uncertainty by predicting variances along with cost predictions. Others [4, 12, 19, 41", "45": "or Leon"}]}