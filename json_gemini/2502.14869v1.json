{"title": "Envisioning Stakeholder-Action Pairs to Mitigate Negative Impacts of AI: A Participatory Approach to Inform Policy Making", "authors": ["JULIA BARNETT", "KIMON KIESLICH", "NATALI HELBERGER", "NICHOLAS DIAKOPOULOS"], "abstract": "The potential for negative impacts of Al has rapidly become more pervasive around the world, and this has intensified a need for responsible Al governance. While many regulatory bodies endorse risk-based approaches and a multitude of risk mitigation practices are proposed by companies and academic scholars, these approaches are commonly expert-centered and thus lack the inclusion of a significant group of stakeholders. Ensuring that Al policies align with democratic expectations requires methods that prioritize the voices and needs of those impacted. In this work we develop a participative and forward-looking approach to inform policy-makers and academics that grounds the needs of lay stakeholders at the forefront and enriches the development of risk mitigation strategies. Our approach (1) maps potential mitigation and prevention strategies of negative Al impacts that assign responsibility to various stakeholders, (2) explores the importance and prioritization thereof in the eyes of laypeople, and (3) presents these insights in policy fact sheets, i.e., a digestible format for informing policy processes. We emphasize that this approach is not targeted towards replacing policy-makers; rather our aim is to present an informative method that enriches mitigation strategies and enables a more participatory approach to policy development.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid development of AI systems that have become commonplace in the daily lives of millions of people around the world has spurred a growing need for responsible AI governance. More broadly, this has resulted in a proliferation of frameworks, guides, and principles all detailing best practices for using, designing, and deploying these AI systems. In recent years, there have been various regulatory endeavors to temper and prevent harms of Al systems at federal or even multinational levels with measures like Biden's Executive Order on Artificial Intelligence in the United States [7], the EU AI Act [15], or the Chinese Generative AI Regulation [17]. Most of these approaches are risk-based, i.e., they aim to identify negative impacts that can materialize through Al systems and then propose mitigation measures [61]. One central aspect for the effectiveness of these approaches is to critically interrogate who is conducting the risk analysis and subsequent risk mitigation measures. Currently, the efforts in Al governance, whether official regulatory measures, academic works to create guidelines, or sets of standards established by technology companies, all tend to rely on a small set of expert knowledge [60].\nThus, a problem emerges-how to integrate and incorporate input from broader sets of stakeholders in order to develop policy consistent with democratic expectations. There is value in lay stakeholder (a subsample of the public with no formal or professional link to the technology at hand) knowledge, since they can report on lived experiences, describe how negative impacts materialize in a diversity of real-world settings that they inhabit, and formulate expectations based on their underlying values and norms for how these impacts might be mitigated [20, 34, 42]. Consequently, incorporating expertise and experience from laypeople offers regulators and policy makers a greater sense of what people actually expect. Such input is useful for policy makers so they have guidance and an empirical grounding to incorporate more diverse stakeholder opinions into their development process. Following this participatory process can also build trust and lead to stronger alignment of decision maker goals with the expectations of laypeople.\nOne methodological challenge for integrating knowledge from lay stakeholders is contextualizing the often complex set of AI impacts so that they can clearly understand the issues and effectively weigh in. An approach to this which we have explored in prior work [6, 34] is to narrativize these complex impacts through the use of written scenarios-short stories specifically designed to illustrate the negative impacts of AI systems. Here, we leverage this approach for presenting a broader set of laypeople with narrativized negative AI impacts to help identify mitigation measures and responsibilities of actors to implement those measures. Thus, our approach helps in streamlining the complex risk mitigation process in tapping into the perceptions of a broader set of stakeholders and gathering their insights for brainstorming and prioritization of possible actions and responsibility allocations.\nIn this paper we present an approach to systematically collect potential mitigation strategies for current and future negative impacts of an emerging technology, here focusing on the impacts of generative AI in the media environment [34]. This approach not only serves to help identify potential actions to mitigate impact, but also solicits responsibility allocations reflecting the priorities of lay respondents. Concretely, we present participants in a survey (Survey 1) with various scenarios for generative Al's future impact on the media environment and use these stimuli as an example to identify mitigation approaches, i.e., what should be done by whom to address negative impacts. We then engage a different set of participants (Survey 2) to prioritize those mitigation strategies. Finally, we utilize a large language model (LLM) to create policy fact sheets, i.e., short summaries of the highly prioritized action stakeholder pairs. Our aim is to (1) inform policy-makers and academics in the mapping of potential mitigation strategies and responsibilities of various stakeholders in the eyes of laypeople, (2) guide the prioritization of those measures, and (3) present short fact sheets based on lay input that can function as empirically-grounded fodder for policy makers."}, {"title": "2 RELATED WORK", "content": "To position our approach within the literature, we first outline the state of Al governance and critically assess the risk- based approaches typically used. We highlight the lack of input from lay stakeholders in risk assessment and mitigation approaches and elaborate how participatory methods help address this gap and can enhance policy formulation."}, {"title": "2.1 Governance of Al", "content": "Driven by massive investments in AI and the strategic race for power, Al systems are being rapidly (openly) released, with huge implications for individuals, organizations, and society. These impacts can be both positive and negative. As a result, governments around the world have adopted risk-based approaches that introduce various forms of AI policy that seek to balance the protection of societal values while enabling innovation [22, 61]. At the academic level (often co-authored by tech company researchers [64, 65]), many scholars have introduced their own impact assessment frameworks that aim to identify potential harms to their systems and outline actions to mitigate them [4, 8, 32, 56, 58- 60, 67]. In addition, standardization bodies publish safety guidelines on how to address the harms of AI systems [47]. In 2024, the EU brought the EU AI Act into force [15], which legally mandates risk assessments for Al systems, and the United States saw similar regulation with the Executive Order 14110: Executive Order on Artificial Intelligence [7].\nHowever, while acknowledging the variety of different policy and security frameworks, it is necessary to critically question the efficacy of methods of impact identification and subsequent mitigation strategies to actually protect a diverse citizenry. The emphasis here is on the diversity of the citizenry and the impact of technology on them. This is even more pronounced when considering that end-users are often directly involved with AI technologies and can actively shape the direction of technology development and implementation. Thus, a first critical factor in determining the effectiveness of AI governance proposals is to examine who is actively engaged in defining algorithmic impacts. Scholars have argued [61] that the objective nature of current assessments should be questioned, as identifying impacts and developing mitigation strategies are political and contextual decisions [48, 61]. Often these decisions are in the hands of experts or companies themselves, which can be biased [9, 30, 43, 54]. Even further, it is debatable how effective impact assessments are when they are conducted by companies whose primary goal is to profit from the technology [40, 60, 62]. Thus, while many scholars emphasize the importance of broader inclusion [5, 38, 39, 42], in practice this is critically lacking, especially with respect to ordinary citizens and civil society actors [27, 28, 35].\nA second critical factor to consider is the ability of current governance approaches to adapt to new AI applications that may have novel impacts. Currently, the regulatory framework in the EU (EU AI Act; Digital Service Act (DSA)) requires companies to address reasonably foreseeable risks or \u201cknown knowns"}, {"title": "2.2 Participatory Approaches for Envisioning Risk Mitigation Actions", "content": "In response to the top-down, expert-driven practice of current regulatory and operational risk-based approaches, scholars argue for a more inclusive and participatory approach to address the challenges posed by AI systems [26, 38, 42]. This also requires a change in approach to more qualitative forms of assessments in terms of thinking about plausible future developments [26, 49]. These anticipatory efforts have proven fruitful in the past: the anticipatory governance literature discusses approaches to identify and mitigate adverse impacts of technology at an early stage [11, 29]. The main goal of those studies is to identify negative impacts early in the development and implementation phase of emerging technologies and subsequently propose mitigation strategies before these impacts materialize [25, 29, 55]. By illuminating future pathways and proposing different policy options, anticipatory governance studies empirically provide a deliberative space to navigate the risk mitigation decision-making process [29, 41].\nOne such methodological approach to discovering potential impacts of technology is scenario writing or scenario planning [2, 3, 12, 51], which has also been identified by standards organizations as a viable method to support future thinking [24]. Scenarios allow decision makers to think through different plausible futures and discuss strategies for mitigating potential harm or even aiming for a desired future. In this method, scientists highlight the importance of different stakeholders (e.g., experts, stakeholder representatives, citizens, etc.) early on in the scenario planning process. It stimulates engagement with the issue, contributes to a learning process, and effectively contributes to ownership of technology development [3, 51]. Working with a diverse group of stakeholders can inform scenario-planning in various ways, such as identifying future trends and helping to frame the prioritization in order to \"reduce the number of trends and challenges and identify the most important driving forces\" [3]. Participatory foresight approaches are particularly useful in this regard, as they aim to engage a wide range of stakeholders, including lay stakeholders [10, 45]. Lay stakeholders possess situational knowledge that enriches expert-driven assessments and fills in blind spots in bottom-up impact exploration and mitigation [38, 45]. In addition, participants of scenario planning exercises can assist with informing policy making: \"Once alternative futures are mapped out, augmented with narratives, and vetted for quality, stakeholders can be engaged to assess the implications of the scenarios\" [3]. Prior empirical studies on the impacts of generative AI have shown that scenario approaches with diverse stakeholders have been fruitful in mapping and evaluating potential governance approaches [6, 19, 34, 39].\nThematically, we extend our work on the impact of generative AI on the news environment [6, 34]. In this field, generative Al has already had a profound impact, with various application potentials ranging from editing and summarizing to the creation of artificial content (for an overview, see [18, 46]). A recent survey of media leaders indicated that 87 percent of respondents think that newsrooms will be somewhat or completely transformed by generative AI [44]. While the implementation of generative AI brings many potential benefits, negative impacts also arise such as problems with accuracy (\"hallucinations\"), the spread of misinformation, and the loss of autonomy [14, 16].\nThis study extends previous approaches by using scenarios as a basis for identifying stakeholder-action pairs for negative impacts of generative Al in the news environment, i.e., enumerating possible strategies for mitigating the negative impacts outlined in the scenarios and assigning responsibility for them to specific actors that have the potential to mitigate the impact. This approach allows us to gauge lay stakeholders' perceptions and use their expertise to uncover not only what actions they envision, but also who they want to be accountable. This enriches a mostly expert-driven definition of mitigation strategies and enables a more democratic approach to policy development."}, {"title": "3 DATA AND METHODOLOGY", "content": "The goal of this work is to develop a participatory approach to support policy making by using inputs from a broader participative base to uncover the most important needs and prioritizations of lay stakeholders. We did so by first running a survey utilizing written scenarios in order to contextually narrativize the negative impacts for the lay stakeholders to make sure they are comprehensible, then asked them to brainstorm potential responses or interventions to these negative impacts in the form of actions specific actors can take, which we term \u201cstakeholder-action pairs (SAPs)\" for this work. We then ran a second survey asking lay stakeholders to rank the importance of these SAPs relative to specific impacts in terms of both priority that should be given to this SAP as well as agreement with the proposed SAP. We then used LLMs to convert these results into \"fact-sheets\" synthesizing the most important aspects of these results into one-page guides to inform policy makers. Our full process is illustrated in Figure 1."}, {"title": "3.1 Selection of Impact Types for Evaluation", "content": "In order to select a set of negative impact types within the broader domain of generative Al in the media, we build on prior work [6] which developed a scenario-based approach to evaluate impacts from a typology categorized by impact theme (e.g., media quality) and impact type (e.g., sensationalism). Based on human evaluations of the impact types detailed in the scenarios, that work scored the impact type on a scale from 1 (low) to 5 (high) across four dimensions (severity, plausibility, magnitude, and specificity to vulnerable populations) both pre- and post-introduction of a transparency based policy aimed at mitigating negative impacts from generative AI. We used the pre-policy mitigated rankings of the impacts to identify a smaller set of impacts examined in this work.\nMore specifically, to identify the set of ten impact types for use in this work, we took a weighted average of severity, plausibility, magnitude, and specificity to vulnerable populations scores with a higher weight (.4) given to severity and an equal weight (.2) given to the other three values. These weights were assigned in order to give greater emphasis to the metric (severity) perceived to have a crucial relevance to risk mitigation prioritization for impact assessment [26, 39], while also still taking into account the other dimensions. From this ranked list, one of the authors who is an expert in policy and regulation research evaluated the impact types in regard to the following question: \"Which impact types in this list are likely receptive to policy intervention?\u201d. This expert evaluation resulted in exchanging three of the top ten impact types from the weighted ranking (Labor: Changing Job Roles, Media Quality: Clickbait, and Political: Opinion Monopoly) with three others from the top 20: Media Quality: Lack of Diversity/Bias, Lack of Fact Checking and Trustworthiness: Overreliance on AI. Categorized according to the original taxonomy [34]. Our final list of important impact types likely to be receptive to policy intervention is alphabetically as follows: Autonomy: (1) Loss of Control, Labor: (2) Unemployment, Media Quality: (3) Credibility/Authenticity, (4) Lack of Diversity/Bias, (5) Lack of Fact Checking, (6) Sensationalism, Political: (7) Fake News/Misinformation, (8) Manipulation, Trustworthiness: (9) Overreliance on AI, and (10) Well-Being: (10) Addiction. A short description for each impact type can be found in Appendix A.4."}, {"title": "3.2 Lay Stakeholder Brainstorming of Approaches to Mitigate Negative Impacts of Al", "content": "In order to be certain lay stakeholders contextually understand the impact types we wish to evaluate in this study, we first provided them with a scenario narrativizing the negative impact in the context of generative Al in the media environment. These are the same scenarios used by [6], which were written using GPT-4 and created for evaluation of the severity, plausibility, magnitude, and specificity to vulnerable populations that informed the selection of impact types used in this paper. These scenarios are set in the United States five years in the future\u00b9. We decided to use these generated scenarios as opposed to human-written scenarios because LLMs help us create salient narratives to elicit targeted responses whereas human written scenarios tend to be more complex, often intertwining several impact types rather than isolating a specific impact of interest. As such, the LLM-written scenarios tend to be more easily understood and therefore more useful as a tool to get participatory feedback.\nWe presented the scenarios to the evaluators, directing them only to pay attention to the specific impact type for which we are eliciting brainstormed SAPs. After they read the scenario narrativizing the impact type, we then asked them to brainstorm four SAPs in the form of stakeholder-action pairs (SAPs). We ask them: \"Specific to harms from generative Al concerning  in regards to  (e.g., credibility/authenticity in regards to media quality): What people, organizations, or entities (in real life, not characters in this scenario) do you think could take action in order to prevent the harms illustrated in this scenario from occurring?\". We then provided them four side-by-side free text response boxes where they could fill in the \"People/Organizations/Entities\" and then the corresponding \"Actions they should take.\u201d They each completed this process three times, seeing one of three scenarios for three randomly assigned impact types, resulting in 12 SAPs brainstormed per participant.\nAfter aggregating scenarios based on impact type, each impact type on average received 48 brainstormed SAPs. We then qualitatively evaluated the proposed stakeholder actions, setting more standardized values for the stakeholders (e.g., \"Congress\" and \"Federal Government\" both became \"Government\") and combining duplicates or similar SAPs for analysis in the second study. Evaluators were often more granular in their stipulation of stakeholders-they often described specific branches of government (e.g., legislators or judicial), named specific roles in news publishers (e.g., journalists or editors), and sometimes even mentioned technology companies by name (e.g., Meta or Twitter) or called them \"AI companies,\" but for the purposes of this evaluation the authors qualitatively rolled those into aggregate categories. We also removed SAPs that would not take the form of a policy intervention, such as \"Families and friends need to hold interventions for those addicted to consuming content generated by AI,\" though we do examine these further in our analysis below. We aggregated the proposed SAPs by impact type; after cleaning the SAPs, each impact type had on average 23 SAPs (Median = 22), with a minimum of 16 and maximum of 31, and an overall total of 228."}, {"title": "3.3 Establishing Value-Ranking: Agreement, Prioritization, and Consensus", "content": "With a long unordered list of lay stakeholder generated potential SAPs for these complex impacts, we needed to develop a way to evaluate their relative importance in terms of both agreement the approach should be pursued and the priority with which to do so. For each of the ten impact types, we converted all of the SAPs to \u201cshould\u201d statements, e.g., \"Schools should promote healthy behavior of generative Al consumption.\u201d We asked them to evaluate a set of SAP \u201cshould\u201d statements based on agreement using a Likert scale from 1 (Strongly Disagree) to 7 (Strongly Agree) and priority using a ternary scale of Low, Medium, and High.\nWe utilized the same set of LLM-generated scenarios as detailed in Survey 1 to be consistent and confident the proposed SAPs will have relevance to the scenarios narrativizing the negative impacts presented to participants. We first surfaced the scenarios to evaluators, again directing them to only pay attention to the specific impact for which we ask them to evaluate. Then we asked them specific to that impact to \"rank how much you agree with the following"}, {"title": "3.4 Study Recruitment and Participants", "content": "For both of these studies we utilized the research platform Prolific. As the scenarios were located within a US-context, we recruited participants based in the US, fluent in English, with an approval rating of at least 95%, and at least 100 previous submissions on Prolific. For both studies, each evaluator read three scenarios and answered a short set of questions following each. We ensured the participants received a fair wage for their participation, paying $4.50 for completion of the first survey (median completion time of 17 minutes) and $3.25 for completion of the second study (median completion time of 12 minutes), which resulted in an estimated payment of $15.88/hour and $16.25/hour respectively. We excluded 1 participant in each study for not passing attention checks phrased as nonsensical questions.\nFor the first study we had a final pool of 40 participants, with self reported demographics of 50% Male, 40% Female, and 10% not consenting to report; 67% White, 12% Mixed, 7% Black, 7% Asian, and 7% not consenting to report; median age of 36, with the youngest being 20 and the oldest 61. For the second study, we had a final pool of 86 participants, with self reported demographics of 62% Female, 36% Male, 2% not consenting to report; 59% White, 12% Mixed, 9% Black, 6% Asian, 10% Other, and 3% not consenting to report; median age of 42, with the youngest being 18 and the oldest being 73. We emphasize that this was not a representative sample, and we did not take any steps to ensure equal stratification across any demographic lines."}, {"title": "3.5 Converting to \"Policy Fact Sheets\"", "content": "In order to allow this method to be useful to inform policy-making, it would be helpful to deliver this content in a digestible way to policy-makers. We do this translation step to a concise summary of the survey findings in creating short \"fact sheets\" for each impact type that highlight the high-level trends and aspects lay stakeholders find most useful, while also highlighting aspects that have a lower consensus and might therefore be more contentious. These fact sheets are a proof of concept to illustrate how we could use these findings to craft input for policy makers, however actual documents to inform their decision making would need to incorporate results from a much larger and representative sample of their own communities and jurisdictions. To create a streamlined process to create these, we use an LLM (specifically GPT-40 which pointed to \"gpt-40-2024-08-06\") and prompt engineering to convert this data to a synthesized document of high level trends. We elucidate findings from designing this process with guidance from a member of our team who is an expert in policy and regulation research. Full prompting details can be found in the Appendix A.6."}, {"title": "4 RESULTS", "content": "We first examine the various SAPs proposed by lay stakeholders from our first survey. After consolidating these as discussed in Section 3.3, we are left with 228 policy-actionable SAPs across the 10 impact types. After cleaning and consolidating these 228 SAPs, we identified 12 different stakeholders to which responsibility was allocated (see Appendix A.1 for more detailed definitions of these stakeholders) and 41 different types of actionable approaches encapsulating the various actions specified in the survey. We now examine these by responsibility allocation, actions, the various interactions in which lay stakeholders assigned responsibility to actions, and finally non-policy actionable SAPs."}, {"title": "4.1 Brainstormed Stakeholder-Action Pairs", "content": "Responsibility Allocation. Across the 10 impact types, we identified 12 different types of stakeholders to whom evaluators assigned responsibility for impact mitigation or prevention, displayed in Figure 2. Of these, government was the most common actor (27%), followed closely by technology companies (26%) and news publishers (21%). These three categories appeared in every single impact type, though notably technology companies were allocated responsibility in 50% of the potential SAPs for credibility/authenticity in regards to media quality, and over a third of the SAPs in both overreliance on AI and loss of control. Government was allocated responsibility in over a third of the SAPs for negative impacts related to unemployment, lack of fact checking, sensationalism, and fake news/misinformation.\nFor some impact types, respondents allocated responsibility to these three actors the vast majority of the time, though some were much more dispersed. Schools (9%) were allocated responsibility at least once in each impact type, and social media companies (6%) were mentioned more often in negative impacts relating to overreliance on AI, fake news/misinformation and manipulation. Proposed responses to negative impacts related to addiction tended to allocate responsibility to a more diverse set of stakeholders than the big three actors (i.e., not just government, tech companies and news publishers); for this impact type responsibility was frequently allocated to schools (18%), public health officials (11%), and independent third parties/researchers (7%) in addition to the big three. Approaches proposed for negative impacts relating to unemployment were allocated to stakeholders such as employers and unions, actors that did not come up in any other category. Local communities (3%) came up in half of the impact types, but was most commonly discussed in responses to negative impacts relating to sensationalism.\nActionable Approaches. After consolidating the SAPs, the various actions proposed in the 228 responses were distilled into 41 distinct actions, which can be found displayed in Figure 4 in Appendix A.2. The most commonly proposed approach was to fact check content or ensure the accuracy of content generated by AI (12%). The second most popular was educating people by strengthening their digital literacy and critical engagement (9%), followed by transparency requirements (7%). Both banning (7%) and limiting (6%) the use of Al in writing news articles came up frequently, as well as emphasizing humans over Al or not replacing humans with AI (6%) and educating people about the general harms resulting from AI (5%). There were no other actions that came up in more than 5% of responses.\nWith the exception of negative impacts related to unemployment, which had a third dedicated to not replacing humans with Al in various ways, every impact type had at least 12 distinct action responses with the average having 16 different action responses. Addiction impacts had the most diverse set of actions (as well as responsibility allocation), with 20 different actions spanning various actors, the most common of which was limiting who has access to Al models (11%). Both fact checking and educating people about digital literacy came up in every single impact type, though the latter was most common in impacts relating to sensationalism. 11 of the 41 specific actions only came up once each; four of these occurred in impacts related to overreliance on Al with approaches such as \"ban Al companies from using public figures' likeness in training data\" and \"ensure that all newsrooms who use AI have a license to do so.\u201d\nStakeholder-Action Pairs. As a next step, we looked at the interaction of the two aforementioned components: what people want done and by whom. Some actions were only assigned responsibility to certain actors, such as any bans of Al in specific industries or certain penalties like punishing the creation of potentially harmful AI only being allocated to the government. The responsibility of others however, such as educating the public about the harms of AI or strengthening digital literacy and critical engagement, were assigned to multiple actors: government, independent third parties, local communities, news publishers, NGOs, public health officials, schools, social media companies, and technology companies. Some actions were highly concentrated in specific actors but had some others crop up, such as news publishers predominantly receiving responsibility allocation for both fact checking/ensuring accuracy and limiting the use of AI in writing news articles; however, the government, technology companies, independent researchers, and social media companies all were proposed to undertake those actions as well.\nSome actions saw an interesting story emerge. For fake news, for example, people proposed multiple actions for both prevention and mitigation across specified actors. Technology companies and news publishers were seen as responsible for preventing the generation of fake news, but social media companies were predominantly seen as the responsible actor in charge of preventing the spread of fake news, and the government was seen as the primary actor in charge of punishing those individuals and entities who (a) create fake news, (b) create technology capable of generating fake news, and (c) spread fake news.\nFrequency of suggested SAPs is not necessarily the best metric for evaluating the perceived importance of various approaches-sometimes outliers can be more important than the commonly thought ones. This is why it is essential to not solely evaluate the most frequently brainstormed SAPs, but also to evaluate how lay stakeholders value them. In the next section (Section 4.2), we analyze the SAPs ranked by a set of lay stakeholders to understand which approaches and interventions people actually want to see manifested.\nNon-Policy Implementable Stakeholder-Action Pairs. A small subset of SAPs proposed were not implementable by policy, but that does not mean they are not highly relevant to the conversation. Those measures tended to take the"}, {"title": "4.2 Value Ranking the Proposed SAPs", "content": "We next asked evaluators to rank the proposed SAPs (framed as should statements, e.g., \"News publishers should ban Al to write news stories\" by both agreement on a Likert scale of 1 (Strongly Disagree) to 7 (Strongly Agree) and priority (Low, Medium, High). For each impact type, we aggregated and ranked responses by mean priority and then examined the agreement scores by mean and standard deviation. A full ranked list for every impact type can be found in the Appendix A.3 in Tables 1-10.\nOn average, our participants thought that most of the brainstormed SAPs should occur. The average agreement score of all SAPs was 5.65/7 (median: 6), and 66% of evaluations were a 6 or a 7. There was also a consensus that the majority of these items were high priority; when assigned ordinally (Low = 1, Medium = 2, High = 3), the average priority score was 2.28. Overall, 48% of all items were ranked high priority, 33% medium priority, and 19% low priority, indicating predominant support amongst our participants for prioritizing the stakeholder actions we studied.\nTwo of the three major stakeholders from the previous section-news publishers and technology companies-still dominate the value ranking. News publishers were assigned responsibility in over a third (36%) of all SAPs ranked in the top five of the 10 impact types by priority, and technology companies made up one-fifth (22%). Schools were the next most frequent (18%), followed by government (10%). Despite getting allocated responsibility the most out of all SAPs in Survey 1, government proportionally rated unfavorably in terms of the priority ranking falling to fourth place. Almost all of the other stakeholders appeared at least once in the top rankings of SAPs with the exception of NGOs and local communities, both of which consistently rated quite low.\nFact checking persisted as being the highest valued action in terms of priority, with 28% of the highest rated SAPs having to do with fact checking in some regard, assigned to news publishers (64%), technology companies (29%), and social media companies (7%) in the highest priority rankings. Next came transparency-mostly in terms of labeling something as AI-generated or providing more insight into why models provide given outputs. In these high priority cases this was mostly thought to be the responsibility of news publishers (50%), tech companies (33%), and social media companies (17%). Tied for second with transparency in terms of high priority was education. People consistently valued educating both the youth and the greater public about both general harms of AI and how to strengthen digital literacy and critical engagement. Though in the larger list there were many possible stakeholders that people considered allocating responsibility to (schools, government, tech companies, etc.), there was a consensus that lay stakeholders highly valued schools incorporating this into their curriculums rather than other entities taking charge of this.\nAs we noted earlier, just because recommendations were outliers in terms of how many people came up with the idea in the brainstorming phase did not necessarily make it less important when evaluating all SAPs together. The SAP \"Tech companies should require user consent for training of personalization algorithms/user facing AI,\" only came up once during the brainstorm phase, but it was one of the highest priority actions evaluated within impacts relating to loss of control in autonomy, with a high consensus of people agreeing this should occur (mean = 6.24; sd = 1.09). Similarly, three highly rated unemployment SAPs never came up anywhere else, such as \"Unions should force employers to have fair practice for lay-offs when Al replacements are involved.\"\nTechnology companies sometimes received responsibility over consulting another stakeholder, such as \"tech companies should hire independent third parties to monitor and fact check the outputs of their models,\" which indicates that though lay stakeholders believe they should not be the fact checkers themselves, the financial burden of seeing it manifest should still fall on them.\nJust as interesting as the most commonly valued SAPs are those that consistently rank among the bottom. The most common two words by far in the bottom ranking of these by both priority and agreement were \u201cban\" and \u201climit.\u201d Lay stakeholders consistently evaluated outright bans and major limitations on AI or other actors as low priority and low agreement scores. Actions that had responsibility allocated to local communities also tended to rank low; however, this was mostly due to being ranked as low priority rather than lack of agreement. There was not one instance in which local communities taking any action (e.g., \"organize and promote annual get togethers to promote responsible use of Al sources\") ranked outside of the bottom five SAPs. Some SAPs had high priority but low consensus in terms of agreement. This may indicate some highly contentious SAPs. One example of this is within employment: \"News publishers should limit the use of AI in journalism\"-this SAP had a mean priority score of 2.62/3, indicating a relatively high priority, but a mean agreement score of 5.54/7 and standard deviation of 2.22 indicating a lower consensus in ratings. Another SAP that had similar scores was \"Employers should not replace human jobs with AI\" (mean priority:"}, {"title": "4.3 Policy Fact Sheets", "content": "As discussed in the Methods Section 3.5, as a proof of concept we converted the findings of our surveys into short \"policy fact sheets\" to show how these insights could be delivered to policy makers. A member of our team who is an expert in policy and regulation research helped guide the content we wanted to surface on these documents. We learned through the process that the most useful manner to structure these documents was by presenting policy makers with a whole network of agents that lay stakeholders identify as having a share in ensuring responsible use. As such, we organized the results by the actors (stakeholders) and the actions identified to be taken by them. We also highlight actions by priority and consensus to help elucidate areas in which policy makers may want to tread lightly due to diverging agreement and items that tend to have more widespread support.\nThrough presenting the findings in this manner, it became clear that some stakeholder actions were implausible, such as \"use more fact checkers\" or \"automate fact checking,\" either for pragmatic resources reasons or technical limitations. Some were also politically fraught (and potentially dangerous), such as suggesting governments penalize the production and sharing of false information. This helped guide the structure of these sheets as presentations of what lay stakeholders find useful/not-useful and desirable/undesirable to then be taken into account in a larger discussion of policy development, rather than as direct policy implementations. We also emphasize that when providing this information to"}]}