{"title": "Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization", "authors": ["Haoran Li", "Zhennan Jiang", "Yuhui Chen", "Dongbin Zhao"], "abstract": "With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL.", "sections": [{"title": "Introduction", "content": "RL has achieved remarkable results in many fields, such as video games [1], Go [2], Chess [3, 4] and robotics [5-8]. Since it is hard to parameterize the complex policy distribution over high-dimensional state and continuous action spaces, the performance and stability of visual RL are still unsatisfactory. As the most common policy distribution, Gaussian distribution is easy to sample, but its unimodal nature limits the expressiveness to represent complex behaviors [9]. While complex distributions have the rich, expressive power to improve the exploration ability [10], the difficulty of sampling makes it hard to apply to online RL directly. Parameterizing the complex policy distribution to balance ease of sampling and expressiveness is a bottleneck to improving the efficiency of visual RL.\nAs an emerging generation model, the diffusion model [11] stands out in fields such as image generation [12-14] and video generation [15, 16] with its ability to model complex distributions and ease of sampling characteristics. These properties have also been explored for learning a complex policy [17]. For example, diffusion models are used to imitation learning to learn the diverse expert policies [18, 19] or trajectories [20\u201323] in datasets. In addition, due to their excellent expressive and"}, {"title": "Related Work", "content": "Diffusion Model in Reinforcement Learning\nDue to its high-quality sample generation ability and training stability, diffusion models [11] have been widely applied in fields such as image generation, video generation, and natural language processing and have also been promoted in RL. Since the diffusion model can represent complex distribution in datasets, it is commonly used in offline RL to model behavior policies [18, 25, 35] or expected policies [34, 36\u201338] to meet the requirements of diversity policy constraints and achieve a balance between constraint and exploitation. The diffusion model can also model trajectory distribution [20, 39], achieving specified trajectory generation under different guidance. In addition,\ndiffusion models are also employed to generate data to augment limited training data [27, 28].\nAlthough diffusion models have been widely applied in offline learning, using diffusion models in online learning remains a challenging problem. [40] proposes the concept of action gradient, which uses a value function to estimate the gradient of actions and updates the actions in the replay buffer. The diffusion model-based policy is trained based on the updated actions. [41] employs a diffusion model as the world model to generate complete rollouts at once instead of auto-regressive generation.\n[30] introduces the Q-score matching (QSM), which iteratively matches the parameterized score of a policy with the action gradient of its Q-function. Considering the low inference efficiency and long"}, {"title": "Preliminary", "content": "Reinforcement Learning\nOnline RL solves sequential decision problems, typically modeled through Markov Decision Pro-cesses (MDP). MDP is represented by 6 tuples (S, A, R, T, \\rho_0, \\gamma). Here, S is the state space, A is the action space, R and T represent the reward function and state transition function of the environment, respectively. $\\rho_0$ is the initial distribution of the state, and \\gamma is the discount factor. In visual RL, it is difficult for agents to directly obtain the state st from the image ot \u2208 O, where O is observation space. Therefore, an image encoder f(\u00b7) is usually required, and the state is estimated from the image through this encoder. The goal of the agent is to learn an optimal policy \\pi^* and the corresponding encoder f* to maximize the expected cumulative reward $E_{\\pi(f(\\cdot))}[\\sum_{t=0}^\\infty r_t]$ under that policy.\nConsistency Policy\nThe consistency model [32] is a new diffusion model proposed to address the time inefficiency caused by hundreds of reverse diffusion steps in diffusion models. It replaces the iterative denoising process with learned score functions in traditional diffusion models by constructing a mapping between noise and denoised samples, and directly maps any point on the probability flow ordinary differential equation (ODE) trajectory to the original data in the reverse diffusion process. Thus, it only requires a small number of steps or even one step to achieve the generation from noise to denoised data. Consistency policy [33, 34] is a new policy representation under the actor-critic framework, which replaces traditional Gaussian models with the consistency model and updates the policy by maximizing the state-action value. Consistency policy is defined as\n$\\pi_{\\theta}(a_t|s_t) = C_{skip}(T_k)a_{t_k} + C_{out}(T_k)F_{\\theta}(a_{t_k}, T_k|s_t)$\nwhere {$T_k: k \u2208 [N]$} a sub-sequence of time points on the time period [$\\epsilon$, K] with $T_1 = \\epsilon$ and $T_N = K$. $a_{t_k}$ is the noised action and $a_{t_k} = a_t + T_k z$ where $z \u223c N(0, I)$ is Gaussian noise. $F_{\\theta}$ is a trainable network that takes the state st as a condition and outputs an action of the same dimension as the input at. $C_{skip}(\\cdot)$ and $C_{out}(\\cdot)$ are differentiable functions such that $C_{skip}(\\epsilon) = 1$ and $C_{out}(\\epsilon) = 0$ to ensure consistency policy is differentiable at $T_k = \\epsilon$. \\epsilon is a real number close to 0. To train this policy, [33] directly applies the above policy to the actor-critic framework and updates the policy using the following the Q-loss, which is named Consistency-AC.\n$L_q(\\theta) = -E_{s_t\u223cB,a_t\u223c\\pi_{\\theta}} [Q_{\\phi}(s_t, a_t)]$\nwhere B is the replay buffer and $Q_\\phi$ is the state-action value function. Compared to diffusion-based policies, consistency policy have significant advantages in inference speed and performance in online learning tasks [33]."}, {"title": "Dormant Ratio of Neural Networks", "content": "The expressive ability is crucial for training the policy with RL. [29] proposes the concept of dormant rate Br, which quantifies the expression ability of a neural network by calculating the proportion of dormant neurons in the neural networks.\n$\\beta_r = \\frac{\\sum_{l=1}^{L} H^l}{\\sum_{l=1}^{L} N^l}$\nwhere $N^l$ represents the number of neurons in the l-th layer. $H^l$ is the number of neurons in the l-th layer whose score $s^i$ is less than \\tau. The score of each neuron is calculated as follows:\n$s^i = \\frac{E_{x\u2208D} |h_i(x)|}{\\sum_{k=1}^{N} E_{x\u2208D} |h_k(x)|}$\nHere $h_i(x)$ is the activation function of the i-th neuron in the l-th layer. D is the distribution of the input x. In the following sections of this paper, we use the dormant rate to evaluate the expression ability of consistency policy during the training."}, {"title": "Is Consistency-AC Applicable to Visual RL?", "content": "Does the non-stationary distribution in online RL affect the training of consistency models? Unlike offline RL, online RL does not have pre-collected datasets. The data distribution for training the policy is constantly changing with policy improvement. So, whether this non-stationarity distribution affects the training of consistency models is a question that needs to be explored. In order to investigate the impact of non-stationarity of data for consistency model training, we record the dormant rate of the policy network during consistency model training under two different settings: online training and offline training. We employ two tasks (MuJoCo Halfcheetah and MuJoCo Walker2d) and conduct 4 random seeds for each setting. In order to eliminate the impact of Q-loss, we follow the behavior clone setting and train the consistency model with consistency loss [32] using data from offline datasets or online replay buffers. The distribution of the data in the replay buffer varies with policy improvement. Therefore, we can infer that the non-stationary distribution of online RL does not significantly affect the consistency model training.\nIs the actor-critic framework suitable for training consistency policy? The actor-critic frame-work is a highly effective policy training framework for online RL, in which the policy network achieves policy improvement by maximizing the value function. Some works [33, 34] directly apply consistency models to this framework. Although they achieve good results in RL tasks with low dimensional state spaces, whether the actor-critic training framework is compatible with consistency model training remains a question that needs further investigation. To evaluate the impact of the actor-critic framework on the training of consistency models, we compare the dormant rates of policy networks under the consistency loss and Q-loss settings under the actor-critic framework. Therefore, we can determine that the Q-loss under the actor-critic framework will destabilize the consistency policy training.\nWill high-dimensional state space exacerbate the degradation phenomenon of consistency policy? Compared to RL with low dimensional state space, training stability in visual RL is still a challenge. In order to investigate whether the degradation phenomenon of consistency policy will become more significant under visual RL tasks, we compare the dormant rates of the policy networks with the state as input and image as input on 2 tasks (Walker-walk and Cheetah-run in DeepMind control suite) under the setting of online learning. Therefore, we can infer that visual RL will exacerbate the instability of consistency policy training caused by the Q-loss under the actor-critic framework."}, {"title": "Consistency Policy with Prioritized Proximal Experience Regularization", "content": "Consistency Policy with Entropy Regularization To solve the problem of consistency policy quickly falling into local optima under the influence of the Q-loss, we introduce policy regularization to stabilize policy improvement. Here, we employ entropy regularization to constrain policy behavior. The objective of RL is:\n$J(\\theta) = E_{s_t\u223cB,a_t\u223c\\pi_{\\theta}}[\\sum_{t=0}^\\infty \\gamma^t r_t(s_t, a_t)] - \\eta E_{s_t\u223cB, a_t\u223c\\pi_{\\theta}} [log \\pi_{\\theta}(a_t|s_t)]$\nwhere \\pi\u03b2 is the proxy distribution required for policy regularization. Entropy regularization is a commonly method for stabilizing policy training in RL. When \\pi\u03b2 is a uniform distribution, the above objective is equal to maximum entropy RL, which maximizes the entropy of the policy while optimizing the return. The prerequisite for this method is to obtain the closed form of the policy distribution to calculate its entropy. However, for diffusion models or consistency models, obtaining the closed form of the policy distribution is very difficult. Thanks to the development of generative models, we can use score matching instead of solving analytic entropy in entropy regularization RL, thus achieving sample-based policy regularization. Therefore, the training loss of consistency policy with the entropy regularization is:\n$L(\\theta) = -E_{s_t\u223cB,a_t\u223c\\pi_{\\theta}}[Q_{\\phi}(s_t, a_t)] + \\eta L_c(\\theta)$\nwhere Lc is consistency loss defined by following:\n$L_c(\\theta) = E_{k\u223cU(1,N-1),s_t\u223cB,a_t\u223c\\pi_{\\beta},z\u223cN(0,1)} [\\lambda(T_k)d(\\pi_{\\theta}(s_t, a_{t_k+1}, T_{k+1}), \\pi_{\\beta}(s_t, a_{t_k}, T_k))]$\nHere \u03bb(\u00b7) is a step-dependent weight function, d(\u00b7,\u00b7) is a distance metric. Since there is no need to obtain the closed form of the proxy distribution, only the data under that distribution needs to be obtained, making the selection of proxy distribution flexible. The remaining question is how to construct a suitable proxy distribution \\pi\u03b2.\nPrioritized Proximal Experience Regularization When the proxy distribution is uniform, this method approximates the maximum entropy consistency policy (MaxEnt CP). It should be noted that"}, {"title": "Conclusion", "content": "In this paper, we analyze the problems faced by extending consistency policy to visual RL under the actor-critic framework and discover the phenomenon of the collapse of consistency policy during training under the actor-critic framework by analyzing the dormant rate of the neural networks. To address this issue, we propose a consistency policy with prioritized proximal experience regularization (CP3ER) that effectively alleviates the training collapse problem of consistency policy. The method is evaluated on 21 visual control tasks and shows significantly better sample efficiency and performance than the current SOTA methods. It is worth mentioning that, to our knowledge, our proposed CP3ER is the first method to apply diffusion/consistency models to visual RL tasks.\nOur experimental results show that the consistency policy benefits from its expressive ability and ease of sampling, effectively balancing exploration and exploitation in RL with high-dimensional state space and continuous action space. It achieves significant performance advantages without any auxiliary loss and additional exploration strategies. We believe that consistency policy will play"}, {"title": "Visual Continuous Control Tasks", "content": "Environment Setup We evaluate the methods on 21 visual control tasks from DeepMind control suite [55] and Meta-world [56]. We split these tasks into three domains, including 8 medium-level tasks in the DeepMind control suite, 7 hard-level tasks in the DeepMind control suite, and 6 tasks in the Meta-world. The details of each domain are included in the appendix B.\nBaselines We compare current advanced model-free visual RL methods, including DrQ-v2 [54], ALIX [57] and TACO [58]. The more detailed results are shown in the appendix B.\nDoes CP3ER have performance advantages compared to current SOTA methods?\nMedium-level tasks in DeepMind control suite. We evaluated CP3ER on 8 medium-level tasks [54] in DeepMind control suite. It should be noted that TACO uses auxiliary losses of action and state representation during training to improve sample efficiency. Moreover, our proposed CP3ER uses no additional losses or exploration strategies. This means that CP3ER has better training stability.\nHard-level tasks in DeepMind control suite. We also evaluate CP3ER on seven challenging tasks [52, 54] in the DeepMind control suite. Our proposed CP3ER surpasses the performance of all methods without relying on any additional loss or exploration strategies. Moreover, it has significant advantages on all metrics including mean, IQM, median, and optimal gap.\nMeta-world. We also evaluated the methods on 6 complex tasks in the Meta-world."}, {"title": "Ablation Study", "content": "Can policy regularization improve the behavior of the policy during training?\nAction distribution analysis with toy example. In order to further explore the impact of policy regularization on the training, we borrow the 1D continuous bandit problem [9] to analyze the policy network behavior. Policy regularization ensures the diversity of action distribution during consistency policy training, preventing the policy from falling into local optima too early. Moreover, consistency policy has robust exploration compared to the Gaussian policy and achieves higher returns.\nDormant rate analysis. We have shown that the Q-loss can rapidly increase the dormant rate of the consistency policy network, leading to a loss of policy diversity. In order to analyze whether entropy regularization can alleviate the phenomenon, we record the dormant rates of the policy networks during the training in 3 tasks.\nWhat is the impact of different modules on the performance?\nWe conduct ablation studies in 2 tasks to evaluate the contribution of each module in the proposed method. Using uniform distribution to regularize policies can make the policy (CP3ER w. MaxEnt) improvement difficult, resulting in low sample efficiency. Compared to using behavior distribution in the replay buffer (CP3ER w. URB), the policy (CP3ER) obtained through prioritized proximal experience sampling has a closer distribution to the current policy, making policy optimization easier and resulting in higher sample efficiency."}, {"title": "Limitations", "content": "Although CP3ER enhances the training stability of consistency policy in the actor-critic framework and achieves excellent performance in visual control tasks, the policy diversity of CP3ER has not been thoroughly explored. In CP3ER, this diversity only depends on the multimodal actions in the replay buffer, which gradually disappears as the training steps increases. Therefore, CP3ER may face the risk of losing diversity in consistent policy, thereby weakening its exploration ability to a certain extent. In addition, there is a lack of theoretical analysis on CP3ER. Although it is based on the actor-critic framework, its policy improvement and convergence property require more rigorous theoretical analysis."}, {"title": "Broader Impacts", "content": "This work mainly focuses on the field of visual RL and proposes a new method that may significantly improve the efficiency of visual RL. This method may improve the efficiency of robot skill learning and have a wide impact on visual control fields such as robots, but it will not involve ethical and safety issues. Therefore, this work should not have negative social impacts."}, {"title": "Implementation details", "content": "In this paper, we propose Consistency Policy with Prioritized Proximal Experience Regularization (CP3ER), which is built on the basis of DrQ-v2.  Compared to DrQ-v2, its difference lies in the use of Prioritized Proximal Experience (PPE) when sampling data from the replay buffer, and the use of consistency policy instead of Gaussian policy in the Actor. In addition, it employs a mixture of Gaussians to model the value distribution rather than the deterministic double Q-networks in DrQ-v2."}, {"title": "Procedure of the proposed algorithm", "content": "We have demonstrated the complete procedure of CP3ER in Algorithm 1, 2 and 3."}, {"title": "Hyperparameters", "content": "We present a summary of all the hyperparameters for CP3ER in Table 1, where DMC is the abbreviation of DeepMind control suite. It is worth noting that for tasks in different domains, only the learning rate and feature dimension are different, while other parameters are the same for all tasks. The parameters of our proposed method are not task-sensitive, which helps it be applied to a wider range of visual control tasks without the need for fine-tuning of parameters."}, {"title": "More Results", "content": "In this section, we present more detailed experimental results on tasks in DeepMind control suite and Meta-world, including performance curves during the training, performance profiles, and probability of performance improvement. We compare CP3ER to 4 baselines including DrQ-v2 [54], ALIX [57], TACO [58] and DrM [52]. All evaluations are based on a single NVIDIA GeForce RTX 2080 Ti. For CP3ER, training a run with 2M frames on this device will take about 13 hours.\nResults on Medium-level Tasks in DeepMind Control Suite\nIn this subsection, we show the detail results on the 8 medium-level tasks [54] in DeepMind control suite. These tasks include: acrobot-swingup, cheetah-run, finger-turn-hard, hopper-hop, quadruped-run, quadruped-walk, reacher-hard and walker-walk.\nResults on Hard-level Tasks in DeepMind Control Suite\nIn this subsection, we show the detail results on the 7 hard-level tasks [52, 54] in DeepMind control suite. These tasks include: dog-run, dog-stand, dog-trot, dog-walk, humanoid-run, humanoid-stand and humanoid-walk.\nResults on Tasks in Meta-world\nIn this subsection, we show the detail results on the 6 hard tasks [52] in Meta-world. These tasks include: assembly, disassemble, hammer, hand insert, pick place wall and stick pull. In these tasks, we compared 4 baselines, including DrM."}]}