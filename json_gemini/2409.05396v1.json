{"title": "FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model", "authors": ["Jianzhi Lu", "Ruian He", "Shili Zhou", "Weimin Tan", "Bo Yan"], "abstract": "Facial movements play a crucial role in conveying altitude and intentions, and facial optical flow provides a dynamic and detailed representation of it. However, the scarcity of datasets and a modern baseline hinders the progress in facial optical flow research. This paper proposes FacialFlowNet (FFN), a novel large-scale facial optical flow dataset, and the Decomposed Facial Flow Model (DecFlow), the first method capable of decomposing facial flow. FFN comprises 9,635 identities and 105,970 image pairs, offering unprecedented diversity for detailed facial and head motion analysis. DecFlow features a facial semantic-aware encoder and a decomposed flow decoder, excelling in accurately estimating and decomposing facial flow into head and expression components. Comprehensive experiments demonstrate that FFN significantly enhances the accuracy of facial flow estimation across various optical flow methods, achieving up to an 11% reduction in Endpoint Error (EPE) (from 3.91 to 3.48). Moreover, DecFlow, when coupled with FFN, outperforms existing methods in both synthetic and real-world scenarios, enhancing facial expression analysis. The decomposed expression flow achieves a substantial accuracy improvement of 18% (from 69.1% to 82.1%) in micro-expressions recognition. These contributions represent a significant advancement in facial motion analysis and optical flow estimation. Codes and datasets can be found here.", "sections": [{"title": "1 Introduction", "content": "The human face could be the most encountered object in a person's life, which emphasizes the vital role of analyzing it [9, 22, 45, 48, 50]. Facial optical flow, representing facial movement, is crucial in applications like micro and macro expression recognition [26], facial motion capture [54], facial video generation [25, 52], and more. Despite its importance, the absence of a dedicated dataset and baseline has hindered its advancement.\nThe primary challenges in facial optical flow estimation involve: 1) Facial expressions arise from facial muscle movements, presenting a non-rigid motion [24] distinct from the rigid motion observed in general datasets like Sintel [7] and KITTI [34]; 2) Delicate expression movements are often overshadowed by overall head motion, as illustrated by the mouth region in Fig. 1(c). While state-of-the-art optical flow methods [18, 21, 44] can estimate the entangled facial flow Fig. 1(b), they are not able to separately isolate these local movements, which are crucial for facial expression analysis.\nIn this paper, we aim to precisely estimate facial optical flow and decompose it into two components: Head Flow, which signifies the isolated rotation and movement of the human head, and Expression Flow, representing the transformation of local facial expressions."}, {"title": "2 Related Work", "content": "General Optical Flow Estimation. Optical flow estimation has been a fundamental vision task ever since this concept was brought by Berthold et al. [17]. Initially approached as an energy minimization problem, it utilized human-designed data and prior terms as optimization objectives [3-6, 51]. With the evolution of convolutional neural networks and deep learning, contemporary approaches adopt an end-to-end learning paradigm [13, 19, 42]. Notably, RAFT [44] represents a significant advancement in optical flow estimation, it constructs a 4D multiscale correlation volume and utilizes a GRU block to operate flows. Addressing the occlusion problem, GMA [21] and later works [18, 43] propose to use global feature and motion aggregation that could perceive long-range connections. These approaches primarily address general challenges such as fast-moving objects and occlusions. However, they are not designed to meet specific challenges like non-rigid motion and entangled representation in facial optical flow estimation, as mentioned earlier.\nGeneral Optical flow datasets. Datasets for optical flow estima-tion can be broadly categorized into real-world [15, 23, 34, 39, 40] data and synthetic data [7, 13, 14, 32, 33, 38, 41]. Among real-world data, KITTI [15, 34] is renowned in the domain of autonomous driving. It offers sophisticated training data derived from intricate device setups. On the synthetic side, Flyingchairs [13] and Flyingthings [32] generate optical flow labels by orchestrating random movements of foreground object models against a background. MPI Sintel [7], Monkaa [32] and Spring [33] gain the flow-image pairs from rendered animation movie scenes. AutoFlow [41] proposes an approach to search the hyperparameter for rendering training data, while RealFlow [16] synthesizes images using predicted optical"}, {"title": "3 FacialFlowNet Dataset", "content": "We generate a realistic facial flow dataset with 9,635 unique faces, each displaying diverse shapes, expressions, and head poses. It comprises 105,970 image pairs at 512x512 pixels resolution, with corresponding flow labels. Similar to previous works [7, 13, 32, 33, 38], we utilize Blender and its Cycles rendering engine for generating synthetic frames and flow labels. Our pipeline, illustrated in Fig. 3, takes a UV texture, a set of FLAME parameters, a background image, and camera/light parameters as input. It outputs video sequences with lengths of 5, 10, 15, or 20 frames, along with corresponding optical flow labels. The following sections provide details on the modules used for dataset generation."}, {"title": "3.1 3D Face Reconstruction", "content": "Face model: We use FLAME as our parameterized face model to reconstruct the meshes. FLAME [27] is a statistical model trained from around 33,000 3D face scans. It uses linear transformations to describe identity and expression-dependent shape variations, and standard linear blend skinning (LBS) to model neck, jaw, and eyeball rotations. It has parameters for identity shape $\\beta \\in \\mathbb{R}^{IBI}$, facial expression $\\gamma \\in \\mathbb{R}^{I41}$, and pose parameters $\\theta \\in \\mathbb{R}^{3k+3}$ for rotations around k = 4 joints (neck, jaw, and eyeballs) and the global rotation. With all these parameters, FLAME can output a mesh with $n_0 = 5023$ vertices. Formulated as:\n$\\Mu (\\beta, \\theta, \\psi) \\rightarrow (V, F)$", "latex": ["\\beta \\in \\mathbb{R}^{IBI}", "\\gamma \\in \\mathbb{R}^{I41}", "\\theta \\in \\mathbb{R}^{3k+3}", "\\Mu (\\beta, \\theta, \\psi) \\rightarrow (V, F)"]}, {"title": "3.2 UV-Texture Extraction", "content": "FLAME comes with an appearance model, which is converted from Basel Face Model's albedo space [37] to FLAME's UV layout [29]. To enhance the realism of our dataset, high-fidelity, and quality texture maps are essential. However, as depicted in Fig. 4(c), the existing method [28] yields low-quality, low-resolution textures, potentially resulting in unrecognizable and detail-lacking reconstructed results. AffectNet's in-the-wild variations further challenge texture quality.\nFFHQ-UV [2] offers a solution to this issue.\nDataset: FFHQ-UV [2] proposes a StyleGAN-Based Facial Image Editing module, creating FFHQ-Norm with consistent lighting, neutral expressions, and no occlusions from in-the-wild images. This dataset serves as the ideal input for our UV-texture Extraction module.\nMethod: FFHQ-UV's [2] textures are incompatible with FLAME, for they use the facial parametric model HiFi3D++ [8], which differs from FLAME in terms of both topology and vertex count. So we adapt their pipeline using a FLAME-based reconstruction method"}, {"title": "3.3 Dataset Rendering", "content": "Image Generation: For image generation, we use the open-source 3D creation suite Blender. Inspired by [20, 32, 38], we aim to improve the network's robustness by integrating real photos as backgrounds in our dataset. We randomly select 400 background images from the internet, crop them to the same size, and ensure they encompass various indoor and outdoor scenes. During rendering, we position the image as a stationary plane behind the head model, manually adjusting the camera and lighting parameters to align the rendered results with the source images.\nGround Truth Generation: Modifying Blender's internal render engine pipeline enables the passage of vectors between different frames. The render pass is typically used for producing motion blur [7, 13, 32, 33, 38], and it produces the motion in the image space of each pixel; i.e., the ground truth of optical flow. Additionally, we also employ it to generate ground truth depth information, examples are available in the supplementary material."}, {"title": "3.4 Data Diversity", "content": "Facial Expression Diversity: We expect our dataset to inherit AffectNet's expression diversity. [35]. To confirm this, we employ DAN [47], a facial expression recognition network, to classify the 9000 faces selected from AffectNet [35] and our dataset. Fig. 5 visually presents the classification results. It's important to note that the classification model used is pre-trained on AffectNet, explaining the balanced distribution of the eight classification results in Fig. 5(a). Fig. 5(b) demonstrates that, despite a domain gap between synthetic and original images, our dataset effectively preserves a considerable degree of expression diversity.\nUV-Texture Diversity: To assess identity diversity in our UV-Textures dataset, we calculate the identity vector using Arcface [11] for each image. The standard deviation and coefficient of variation of these vectors measure the identity variations. Tab. 1 presents evaluation results of the original dataset (FFHQ-Norm), and three rendered datasets with different UV-textures (FFHQ-UV [2], PO. [28] and ours). It shows that our textures preserve the most identity variations in FFHQ-Norm, whereas PO. hardly preserves identity differences. Fig. 5 also illustrates the same conclusion. To analyze"}, {"title": "4 DecFlow Architecture", "content": "Our overall network diagram is shown in Fig. 6. We base our ap-proach design on the successful recurrent architecture in RAFT [44] and GMA [21]. However, the previous architecture lacks the perception of facial semantic information and only provides an entangled optical flow for face movements. Therefore, we propose a facial semantic-aware encoder and a decomposed decoder to ad-dress the problem. Our model takes in a pair of consecutive frames of face movement and extracts the correlation volume and context feature. The decoder takes the context features and motion features as input, producing aggregated motion features that share information across the image. Finally, the two decoders predict the facial flow and head flow separately and obtain the expression flow by subtracting the head flow from the facial flow."}, {"title": "4.2 Facial Semantic-aware Encoder", "content": "Inspired by the extra encoders in SAMFlow [53] and MatchFlow [12], we posit that incorporating features with facial semantics can enhance the accuracy of facial optical flow prediction, given the relatively fixed structure of the human face (nose, eyes, and mouth). To extract the 2D context features, we start by individually using the encoder of DAD-3DNet [31] and the context network of GMA [21] to encode the first image. The results from these encoders are concatenated and passed through a residual convolutional block to reduce channels and fuse features. Mathematically, this process can be expressed as:\n$\\Phi_c = Res (E_G (I_1) \\oplus E_D (I_1))$", "latex": ["\\Phi_c = Res (E_G (I_1) \\oplus E_D (I_1))"]}, {"title": "4.3 Facial Decomposed Flow Decoder", "content": "We further propose a facial decomposed flow decoder that can de-compose the facial motion into the head flow and expression flow. We adopted the same decoder structure as GMA, but the key differ-ence is that we employed two parallel decoders to independently predict facial and head flow. The head flow $F_h$ is subtracted from the facial flow $F_f$ to obtain the expression flow $F_e$: $F_e = F_f - F_h$.\nIn summary, our network takes two frames as input and outputs three types of flows: facial, head, and expression flow.\nWe first train the facial flow decoder and fix the parameters to train the head flow decoder. It is because the two decoders have separate optimization goals and can not simply co-train, which will cause a performance drop in facial flow (Tab. 5). When training the head flow decoder, we constrain head flow and expression flow as optimization targets. This way, the decoder can perceive the expression difference.\nTo supervise the estimating results, we define separate loss func-tions for facial, head, and expression flow. When training for the facial flow decoder, we use the optical flow loss in RAFT [44]. Then, we train the head flow decoder with the decomposed optical flow"}, {"title": "5 Experiments", "content": "We follow the standard optical flow training procedure [19, 42] of first training GMA [21] on FlyingChairs [13] for 120k iterations (batch size 8) and Flying Things [32] for 120k iterations (batch size 6). Further training on MPI Sintel [7] for another 120k iterations (batch size 6). We then train our DecFlow on FacialFlowNet for 10k iterations (batch size 6) separately for the facial flow decoder and the head flow decoder, with the pre-trained weights from GMA. The training is performed on two 3090 GPUs with PyTorch [36], following GMA's hyperparameters and strategy [21]."}, {"title": "5.2 Evaluation Datasets", "content": "FacialFlowNet (FFN) is divided into training, testing, and valida-tion sets with a ratio of 97:2:1, comprising 90,193 pairs, 10,395 pairs, and 5,382 pairs of images respectively.\nMEAD [46] contains high-resolution video frames with rich facial movements. From this dataset, we select 26 videos filmed from a frontal perspective, comprising a total of 3,402 frames. This subset is used to evaluate our method's efficacy in analyzing real facial dynamics. Additionally, we apply the method by Alkaddour et al. [1] to generate flow labels for 20,445 image pairs extracted from MEAD, denoted as FMEAD, for comparison with dedicated facial optical flow datasets.\nCK+ [30] has 593 acted facial expression sequences from 123 par-ticipants. We extracted the first and last frames of these sequences, resulting in 1,186 images for evaluating our method's performance in real facial optical flow estimation.\nCASME II [49] contains 256 micro-expression videos from 26 sub-jects, featuring five prototypical expressions: happiness, disgust, repression, surprise, and others."}, {"title": "5.3 Quantitative Results on Synthetic Dataset", "content": "The primary evaluation metric is the average end-point error (EPE). To address the static background in FFN, we calculate the average"}, {"title": "5.1 Implementation Details", "content": "5.4 Quantitative Results on Real World Datasets\nWe conduct experiments to validate our method's effectiveness in real facial flow estimation, addressing the domain gap between synthetic and real data. Due to the absence of ground-truth flow labels for real images, we compare the estimated flows with 3DMM's vertices and facial landmarks.\nEvaluation with 3DMM's vertices: MICA [54] provide a metri-cal monocular tracker that can track facial motion by performing precise 3D face reconstruction on every frame of the video. Each reconstructed face mesh consists of 1787 facial vertices. We use it to perform facial motion tracking on images from MEAD [46] and CK+ [30], obtaining facial vertex coordinates for each image. As shown in Fig. 7, vertices were categorized into lips, forehead, cheeks, nose, and eye regions. For precision, we only used the 1435 vertices from the lips, cheeks, and eye regions, as the vertices in the nose and forehead regions remained relatively stationary.\nTo compare optical flow with the selected vertices, we use EPE as our metric. For each pair of images, such as $I_1$ and $I_2$, we obtain the facial vertex coordinates $C_1$, $C_2$, and the optical flow Flow. The average EPE was then calculated using the following formula:\n$E (I_1, I_2) = \\frac{\\sum_{i=1}^{n} \\| F((C_i-C_i'), Flow (C_i)) \\|}{n}$", "latex": ["I_1", "I_2", "C_1", "C_2", "E (I_1, I_2) = \\frac{\\sum_{i=1}^{n} \\| F((C_i-C_i'), Flow (C_i)) \\|}{n}"]}, {"title": "5.5 Micro Expressions Recognition", "content": "We choose micro-expressions recognition as the downstream task and employ MMNet [26] to assess the performance of various opti-cal flow networks. Then, we estimate the optical flow of the onset frames and apex frames in the original, untrimmed, and unaligned videos. These flows serve as inputs for the main branch of MMNet to learn motion-pattern features."}, {"title": "5.6 Qualitative Results on Real-World Images", "content": "Qualitative evaluation results of various methods are presented in Fig. 9. The samples in rows 1-4 have small emotional movements, entirely obscured by head motion. However, by using our method, these masked expression flows can be clearly decomposed. The examples in rows 5-8 depict expressions with intense emotional features and noticeable movements. Consequently, in the facial flow, the motion regions of these expressions are more pronounced. Compared to other methods, our approach predicts more accurate facial flow, visualized as clearer facial features, rather than focusing solely on large motion regions. Furthermore, by observing the last"}, {"title": "5.7 Ablation Study", "content": "Tab. 3 demonstrate that finetuning on FacialFlowNet enhances the accuracy of multiple baselines [18, 21, 43, 44] in both synthetic and real-world datasets, confirming the effectiveness of our dataset. Tab. 4 also highlights the significant advantage of expression flow over facial flow in micro-expressions recognition, confirming the effectiveness of our decomposed flow decoder. Subsequently, an ablation study validates our facial semantic-aware decoder, as pre-sented in Tab. 5. Compared with GMA, adding another decoder for head flow estimation alone may affect the accuracy of facial flow. However, with the inclusion of the facial semantic-aware encoder, the network achieves superior accuracy compared to GMA while demonstrating the ability to decompose the facial flow."}, {"title": "6 Conclusion", "content": "This paper focuses on the challenges in non-rigid motion and en-tangled representation in facial flow estimation. We contribute FacialFlowNet, a large-scale facial optical flow dataset with 9,635 identities and 105,970 image pairs. Our dataset significantly im-proves the accuracy of facial flow estimation across various optical flow methods. Additionally, we propose DecFlow, the first network capable of decomposing facial optical flow. Extensive experiments demonstrate the superior performance of our approach in facial flow estimation and expression analysis."}]}