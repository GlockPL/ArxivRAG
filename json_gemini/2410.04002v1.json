{"title": "Take It Easy: Label-Adaptive Self-Rationalization for Fact Verification and Explanation Generation", "authors": ["Jing Yang", "Anderson Rocha"], "abstract": "Fact verification is a crucial process in journalism for combating disinformation. Computational methods to aid journalists in the task often require adapting a model to specific domains and generating explanations. However, most automated fact-checking methods rely on three-class datasets, which do not accurately reflect real-world misinformation. Moreover, fact-checking explanations are often generated based on text summarization of evidence, failing to address the relationship between the claim and the evidence. To address these issues, we extend the self-rationalization method-typically used in natural language inference (NLI) tasks-to fact verification. Self-rationalization refers to a model's ability to generate explanations or justifications for its responses, which is essential for reliable fact-checking. We propose a label-adaptive learning approach: first, we fine-tune a model to learn veracity prediction with annotated labels (step-1 model). Then, we fine-tune the step-1 model again to learn self-rationalization, using the same data and additional annotated explanations. This approach allows the model to adapt to a new domain more effectively than fine-tuning end-to-end self-rationalization directly. Our results show that our label-adaptive approach improves veracity prediction by more than ten percentage points (Macro F1) on both the PubHealth and AVeriTec datasets, outperforming the GPT-4 model. Furthermore, to address the high cost of explanation annotation, we generated 64 synthetic explanations from three large language models: GPT-4-turbo, GPT-3.5-turbo, and Llama-3-8B and few-shot fine-tune our step-1 model. The few-shot synthetic explanation fine-tuned model performed comparably to the fully fine-tuned self-rationalization model, demonstrating the potential of low-budget learning with synthetic data. Our label-adaptive self-rationalization approach presents a promising direction for future research on real-world explainable fact-checking with different labeling schemes.", "sections": [{"title": "I. INTRODUCTION", "content": "Explainable fact verification is key for modern automated fact-checking. Recent fact-checking datasets usually contain annotated explanations [1], [2] to address its importance. However, research on explainable fact-checking methods mainly focuses on text summarization [1], [3], [4] and, in such cases, explanations as summaries are not representative of real-world fact-checking explanations as they are not comparing the differences between claim and evidence to make conclusions. Self-rationalization, in turn, whereby models are trained to produce predictions and natural language explana-tions jointly, is a mainstream explainable approach for Natural Language Inference (NLI) tasks and could be used to further improve fact verification explanations [5]. Notwithstanding, self-rationalization in its typical formulation is conditional to the target dataset labels being part of the language model pre/training [5], [6].\nAs an example, consider Figure 1. It depicts different methods performances on a recently released fact-checking dataset AVeriTec [2]. This dataset comprises four labels, besides the typical 3-class label (Support, Not Enougn Info (NEI), Refute), it includes a new one for \"Conflict (Conflicting Evidence)\". When performing zero-shot on the T5-3B (green bars), a model pre-trained with NLI datasets (fact verification is often considered similar to NLI) shows reasonable results on the \u201cSupport\u201d and \u201cRefute\u201d classes but performs poorly on \u201cNEI\u201d, and completely fails on the new \u201cConflict\u201d class. Self-rationalization fine-tuned on T5-3B, depicted by the orange bars in Figure 1, fails to learn the new class, resulting in low veracity prediction performance.\nThis problem is significant because most fact-checking datasets (e.g., FEVER [7]) usually label claim veracity with three classes: SUPPORT, REFUTE, and NEI (not enough information), which is comparable to NLI labels (entailment, contradiction, and neutral). However, many real-world fact-checking datasets usually have different labeling schemes with the number of classes varying from 2-27 classes [8] in some cases. As the labeling scheme shifts from NLI tasks, directly applying self-rationalization with models pre-trained on NLI datasets performs poorly for fact checking.\nIn this context, we propose a label-adaptive self-rationalization approach to tackle the challenge of the labeling shift for fact verification/checking. We first fine-tune a pre-trained model to learn the classification task with different labels; then, we fine-tune it again with labels and explanations to learn the self-rationalization task (explanations). Our results show that the 2-step formulation significantly outperforms direct self-rationalization learning by more than 20 percentage points (on the AVeriTec dataset) (Figure 1). This approach also achieves the best results compared to state-of-the-art methods. In summary, our contributions herein are twofold:\n\u2022 We propose a 2-step self-rationalization approach custom-tailored to the fact-checking domain;\n\u2022 We propose to generate few-shot synthetic explanations by LLMs for step-2 self-rationalization, in case of lacking annotated explanations. In this case, the model's perfor-mance is comparable with the entire dataset."}, {"title": "II. RELATED WORK", "content": "This section presents available explainable fact-checking datasets in the literature and the most principled methods proposed to deal with this problem thus far.\n\nA. Explainable Fact-checking Datasets\nExplainability has been an important research front in fact-checking; however, only a few datasets are constructed for this task. LIAR-PLUS [9] was the first dataset by extending the LIAR [10] dataset with extracted justifications from PolitiFact fact-checking articles. Kotonya et al. [1] constructed a large dataset called PubHealth with claims about health topics collected from various fact-checking websites. e-FEVER [11] was a dataset based on FEVER [7], with synthetic explanations generated by GPT-3. A more recent dataset, AVeriTec, was released by Schlichtkrull et al. [2], in which the claims were also extracted from real-world fact-checking websites. Unlike previous explanation datasets, in which the explanations are summarized versions of the evidence, AVeriTec justifications are human-written explanations that reason over the retrieved evidence in the form of questions and answers.\nB. Explainable Fact-checking Methods\nSummarization for explanation generation has been a popular approach, as most explanation datasets have their annotated explanations in the form of summarized evidence. Atanasova et al. [3] first proposed an extractive summarization ap-proach based on the LIAR-PLUS datasets. They generate fact-checking explanations by selecting important sentences from the original fact-checking ruling comments. Kotonya et al. [1] used a joint extractive-abstractive summarization approach to generate human-understandable explanations based on their PubHealth dataset. Russo et al. [4] benchmarked extractive and abstractive approaches and showed that performing an extractive approach before abstractive yielded the best result. The problem with the summarization approach is that the summaries cannot build clear connections between the claim and evidence to draw a conclusion.\nAnother approach to generating explanations is through prompting of large language models (LLMs). Zhang et al. [12] proposed a prompting method (HiSS) to generate intermedi-ate reasoning steps and a final prediction using GPT-3.5. The reasoning steps are composed of decomposed sub-claims followed by questions and answers related to each sub-claim. Zarharan et a. [13] tested zero-/few shot abilities of LLMs on the PubHealth dataset, and they showed that parameter-efficient fine-tuning on the Mixtral-7B outper-formed GPT-4 model. The main issue with using LLMs is that their pre-training data are not transparent, which can cause data contamination, i.e., the test dataset might have been seen during their pre-training, causing unreliable performances."}, {"title": "III. METHODOLOGY", "content": "Provided with labels and explanations, directly fine-tuning for self-rationalization fails on newly added labels (as shown in Figure 1), thus we take a step-by-step approach to slowly adapt the model for the new domain and class. Our method is based on T5-3B model, as its size is comparable to many open large language models, and self-rationalization has been shown to perform well on T5 models [5], [6], [14].\n\nA. Label-Adaptive Self-rationalization Learning\nOur proposed approach is illustrated in Figure 2. It com-prises two steps: in Step-1, the model learns to adapt to the new class with only provided labels; in Step-2, the model learns the self-rationalization task with labels and added explanations. We describe the details as follows:\nGiven a dataset $D = (C, E, L, Expl)$, with each sample $s_i = \\{C_i,e_i,l_i, expl_i\\}$, $C_i, e_i, l_i, expl_i$ represent a claim, evidence, label, and explanation, respectively, we perform two steps.\nStep-1: Label Adaptation. We first adapt and fine-tune the T5 model to generate the veracity label $l_i$. Given the input $X_i = \\{C_i, e_i\\}$, we follow the same standard prompt template that was used to pre-train T5 for the NLI task (\u201cclaims\u201d and \u201cevidence\u201d are mapped to \"hypothesis\" and \"premise\u201d as T5 is more familiar with these words), as shown in first row of Figure 2.\nStep-2: Self-Rationalization. After fine-tuning the model with the veracity prediction task, we now add gold explanation $expl_i$ to fine-tune the resulting T5 model again after Step-1. Shown in second row of Figure 2, we change the encoder prompt to add the word \u201cexplain\", and for the decoder prompt, a separation word \u201cexplanation\", inspired by [5].\nTo simulate a realistic scenario with limited annotated ex-planations, we employ large language models (LLMs) to gen-erate few-shot synthetic explanations. Specifically, we evaluate this task using GPT-3.5-turbo-0125, GPT-4-turbo, and Llama-3-8B-Instruct. We use the same prompt for generating the explanations with the three models, as shown below:"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Implementation Details\nIn each fine-tuning experiment, we select the best model from the last epoch (50) without using a validation set. For AVeriTec, we use a batch size of 4 and a max input length of 512. For PubHealth, due to the length of the evidence, we use a batch size of 2 and a max input length of 1024. All experiments are based on NVIDIA A100 GPUs. For GPT-4 zero-shot baseline, we set the temperature to be 0.7, with a max output length of 200.\nB. Evaluation Metrics\nTo evaluate the veracity prediction and explanation quality, we first extract the label and explanation from the generated text using the separator \"explanation: \". For veracity prediction, we assess performance based on accuracy and macro F1 score. For explanations, we use both reference-based metrics (ROUGE scores and METEOR) and reference-free metrics. The latter is crucial in realistic scenarios where the test dataset lacks reference explanations for comparison. Specifically, we use the following reference-free metrics:\n\u2022 Auto-J [15]: The metric is a model based on LLAMA-2-13B-chat by fine-tuning on judgments of LLM-generated responses with diverse user queries. It supports both single and pair-wise evaluations. We use it for single reference-free evaluations. The evaluation output comprises textual analysis and an overall quality rating between 1-10.\n\u2022 TigerScore [16]: Another trained model-based metric that provides explainable evaluations for text generation tasks by following instructions. It outputs an overall error score ranging from 0 to -infinity, along with a textual analysis detailing the location and type of each detected error. We use the TIGERScore-13B model in our evaluation.\nFor the reference-free metrics, the input must be formatted using instruction-based prompts. Our instructions are similar to those used for generating synthetic explanations with LLMs. We evaluate the explanations based on ground truth labels.\nC. Baselines\nWe compare our two-step approach (denoted as 2-R, with R denoting Rationalization) with the following baselines:\n1) 0-L: zero-shot T5-3B baseline. As NLI datasets were used for T5 pre-training, we formatted veracity prediction as an NLI task and prompted T5-3B to generate predictions. L denotes Label prediction.\n2) 1-R: Compared to 2-R, this baseline model is directly fine-tuned with labels and explanations without first fine-tuning for the veracity prediction task.\n3) 1-L: veracity prediction model fine-tuned with labels only (Step-1 model). The model cannot generate expla-nations, thus is not included for explanation comparison.\n4) Baseline approach by Schlichtkrull [2]. They have sep-arate models for predicting veracity and generating ex-planations on the AVeriTec dataset, with the best results obtained with BERT-Large and BART-Large.\n5) Baseline approach by Kotonya [1]. They also have separate models for the two tasks; on the PubHealth dataset, the best results are based on SCIBERT and BERT models.\n6) Zarharan et al. [13]: They studied different LLMs' per-formance on the PubHealth dataset. All their models are based on summarized evidence to reduce the evidence length, using GPT-3.5-turbo for the summarization. The best results were achieved with parameter-efficient-fine-tuning (PEFT) on the Mixtral-7B model.\n7) GPT-4. We conduct zero-shot prompting on GPT-4-turbo for the AVeriTec dataset. As reported in [13], GPT-4's performance on the PubHealth dataset is directly reported in our work. We prompt the model to generate the output in JSON format to obtain the predicted veracity label and explanation"}, {"title": "V. RESULTS AND DISCUSSIONS", "content": "We present results on veracity prediction and explanation generation in comparison with baselines; and the results of fine-tuning on few-shot synthetic LLM-explanations.\n\nA. Veracity Prediction Performance\nTable III shows the veracity prediction results on different baseline models and our 2-R model. As expected, 0-L (zero-shot on T5-3B) cannot predict the class \u201cmixture\u201d for either dataset. For AVeriTeC, our 2-R model is comparable with GPT-4, with the best accuracy of 85.2%, while being a much smaller model. For PubHealth, the 1-L model achieved the best performance, while 2-R model slightly dropped (2%) on Macro F1 after learning to generate explanations. Both outperform the larger baseline models (Mixtral-7B and GPT-4). For both datasets, the 2-R model improved performance (Macro F1) by more than 10 percentage points compared with the 1-R model, showing that letting models learn the veracity task first greatly helps the model to adapt to the new domain with new classes. Specifically, the 1-R model struggled with predicting classes \u201cneutral\u201d and \u201cmixture\u201d, but with our label-adaptive approach (2-R), the model was able to improve predictions on these classes significantly.\nB. Generated Explanation Quality\nWe show the evaluation of generated explanation quality in Table IV. For both datasets, GPT-4 generated explanations have the best scores on the reference-free metrics, indicating the reasoning abilities of GPT-4, although it has a tendency to be verbose (having the longest explanations on average). Our 2-R approach has the highest ROUGE scores, outper-forming the baselines. For the AVeriTec dataset, the 2-R model generates better explanations than the 1-R model, as agreed by all metrics. For the PubHealth dataset, the scores for the two models are very similar, and both have the highest ROUGES and METEOR scores. In general, the results show that fine-tuned models generate explanations that are better aligned with reference explanations, as the training data follow a similar pattern.\nC. Results from Synthetic Few-shot Explanations\nTo demonstrate the potential of our two-step approach in data-scarce scenarios, we test Step-2 with few-shot fine-tuning. We select 16 samples per class (64 samples total) to prompt an LLM to generate synthetic explanations. These samples and their generated explanations are then used to fine-tune the 1-L model. For robust results, we select few-shot samples with three different random seeds and report the results in average and standard deviation. The results for veracity prediction and explanation generation are shown in Table V and VI.\nThe veracity prediction results show that Step-2 with very few amount of data still achieve much better performance than end-to-end self-rationalization model (1-R), and perform comparably to the 2-R with full dataset fine-tuning. In terms of explanation quality, the reference-free metrics indicate that the best explanations are from the 2-R (GPT-3.5), with a similar Auto-J score compared to the best, and the lowest TigerScore among few-shot models.\nSurprisingly, the 2-R (GPT-4) model performs worse than both 2-R (GPT-3.5) and 2-R(Llama-3-8B), in contrast to Table IV, where GPT-4 model generated explanations are much better. We hypothesize that when generated text is long (2-R(GPT-4) model explanations are almost twice as long compared with the rest), it is more detailed but also more likely to contain errors.\nWe show an example of explanations generated by different models from the PubHealth dataset in Figure VII). We see that as the explanation becomes longer, models tend to hal-lucinate and makes more errors. In this sense, GPT-3.5 and Llama-3-8B generated explanations are better for having shorter explanations and thus less likely to make errors. This gap is particularly captured by TigerScore (Table IV), which measures the number of errors in the explanations."}, {"title": "VI. CONCLUSIONS, LIMITATIONS AND FUTURE WORK", "content": "We proposed an effective two-step approach for joint fact-verification and explanation generation with self-rationalization. Our results show that having a label prediction step significantly helped the model to adapt to new classes and perform better. Our method with T5-3B outperformed larger models, including Mixtral-7B and GPT-4. We further utilized LLMs to generate few-shot synthetic explanations to fine-tune our T5-3B model, and it outperformed end-to-end self-rationalization models fine-tuned on the entire dataset. We also show that T5-3B models struggle with generating longer explanations when learning from GPT-4 explanations.\na) Limitations: Our work nonetheless has its limitations. 1) When using a generative model for classification, the naming of labels is factor that affects performance, as different models may have their own way of formatting the labels during pretraining. 2) We use the same instructions for different LLM models, but there may be other instructions that help them generate more accurate explanations. 3) Our method is based on the encoder-decoder architecture, so it may not be generalized to decoder-only architecture models. 4) Our explanation evaluation is based on automatic metrics, although they have been shown to correlate well with humans, they were not specifically designed to evaluate generated explanations for fact verification.\nb) Future work: Future work may focus on studying what models/instructions can generate better synthetic expla-nations for smaller models to learn from. Moreover, testing the approach to multilingual models and datasets is also a promising endeavor."}]}