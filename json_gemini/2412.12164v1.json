{"title": "GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake News Detection", "authors": ["Lingzhi Shen", "Yunfei Long", "Xiaohao Cai", "Imran Razzak", "Guanming Chen", "Shoaib Jameel", "Kang Liu"], "abstract": "Multimodal fake news detection often involves modelling heteroge- nous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal con- sistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these meth- ods are primarily based on static feature modelling, making it dif- ficult to adapt to the dynamic changes and relationships between different data modalities. This paper develops a significantly novel approach, GAMED, for multimodal modelling, which focuses on generating distinctive and discriminative features through modal decoupling to enhance cross-modal synergies, thereby optimizing overall performance in the detection process. GAMED leverages multiple parallel expert networks to refine features and pre-embed semantic knowledge to improve the experts' ability in informa- tion selection and viewpoint sharing. Subsequently, the feature distribution of each modality is adaptively adjusted based on the respective experts' opinions. GAMED also introduces a novel clas- sification technique to dynamically manage contributions from different modalities, while improving the explainability of deci- sions. Experimental results on the Fakeddit and Yang datasets demonstrate that GAMED performs better than recently devel- oped state-of-the-art models.", "sections": [{"title": "1 Introduction", "content": "Imagine encountering a social media post with a seemingly in- nocuous image and a captivating new headline. Unfortunately, a sinister truth lurks beneath this alluring facade - it is well-crafted fake news [16, 22]. Nowadays everyone is an editor and everyone can publish news especially on social media [50]. As a result, there is an escalating threat of multimodal fake news [29, 64], a potent weapon that weaponizes the synergy of text and visuals to manipulate public discourse and erode trust in information [60, 62]. According to [74], fake news is defined as \"purposefully crafted, sen- sational, emotionally charged, misleading or fabricated information that mimics the form of mainstream news\". Traditional multimodal detection [4], typically rely on basic fusion techniques [43, 67], striving to decipher the complex inter- actions within multimodal narratives [40]. The result is that they fail to capture the nuances that distinguish genuine news from its fabricated counterparts [10, 59]. This critical gap in detection capabilities stems from several inherent limitations, for instance, many current fusion techniques suffer from feature suppression issues [70], hindering the model's ability to grasp the intricate dance between textual and visual elements within a single content. Another reason is the over-reliance on the inherent capabilities of pre-trained models without any additional refinement step for feature representations in the pipeline [3, 47], which is prone to lose information that is crucial for classification. Similarly, exist- ing methods often focus solely on identifying consistency between modalities [55], and they fail to consider utilizing discriminative fea- tures as a complement. However, in the real world, many fabricators"}, {"title": "2 Related Work", "content": "Unimodal fake news methods: Unimodal fake news detection [8, 15, 51] has made significant strides in tackling fake news [53]. Traditional machine learning algorithms, such as SVM, Decision Trees, and Na\u00efve Bayes [6, 21, 27], along with modern deep learning approaches, including CNN, RNN, and LSTM [23, 37], have been extensively compared and studied. For text analysis, transformer- based models such as BERT [12] have paved the way for advance- ments such as RoBERTa and GPT-3 [5], enhancing the detection of subtle linguistic cues. Similarly, sophisticated architectures such as ResNet and ViTs [28] have revolutionized image analysis, en- abling the identification of manipulated visuals. However, unimodal approaches have an inherent weakness: they struggle against mul- timodal fake news that blends text, images, and other media for a more convincing narrative [73]. Multimodal fake news methods: Multimodal fake news detec- tion has emerged as a critical research area, which analyzes data from multiple sources \u2013 text, images, videos, and social context - to form a more holistic view of the information [25]. By leveraging the complementary strengths, these approaches aim to uncover discrep- ancies that might not be evident when analyzing text alone. This field has gravitated towards leveraging the synergistic potential of advanced fusion techniques and models such as VisualBERT [32], ViLBERT [17], and LXMERT [58], which facilitate dynamic, context- aware integration of text and images through self-attention mecha- nisms. These models have significantly advanced the capacity to understand and analyze the complex interplay between modalities, often focusing on exploiting cross-modal dynamics and consisten- cies as potent indicators of misinformation. Moreover, the integra- tion of external knowledge sources [14, 39], through methods like knowledge graph embeddings [16], has provided additional con- text for verifying claims, enhancing the models' ability to discern truth from deception. Despite these advancements, multi-modal detection still faces notable challenges, particularly in processing effectiveness and the adaptive generalization to new forms of fake content, such as deepfakes. The quest for explainability [71] in these complex models remains an ongoing challenge. Moreover, existing multimodal research predominantly focuses on innovative fusion techniques, while how to leverage the distinctive potential of each modality remains an unresolved issue [38, 41]."}, {"title": "3 Our Novel GAMED Model", "content": "As depicted in Figure 1, GAMED is a novel modality-decoupling design for detecting fake news across textual and visual modalities. The process starts with extracting features from text and images, followed by a stage that simulates expert review and opinions using the MMoE-Pro network to refine feature representations. Subse- quently, the distribution adjustment stage, guided by the AdaIN adaptive mechanism, dynamically fine-tunes the impact of each modality, giving precedence to the most pertinent and trustworthy data. Finally, a novel voting system with veto power is introduced in the decision-making stage by combining consensus-based and confidence-based evaluation methods. The entire GAMED work- flow also benefits from the semantic information encoded (KE) by pre-trained language models that encode structured (e.g., knowl- edge graphs) and unstructured text information. In Algorithm 1, we meanwhile present the detailed pseudo-code of GAMED. Feature Extraction: We represent our multimodal news data as a collection N = [I, T] \u2208 D, where I, T, D are the image, the text, and the dataset, respectively. Each data point within D allows us to analyze the interplay between visual content and textual narrative. We exploit the Inception-ResNet-v2 (IRNv2) [57] as a feature extrac- tor to extract image patterns (IP), denoted by $f_{ip}$. We add a special filter BayarConv [7] as an early layer. Our intuition is that when images are tampered with, they often leave subtle traces of forgery that are not easily detected by traditional convolutions, such as artefacts, lighting, and texture. We capture image semantic (IS) features on the global and local details of the image by combining ViT with a masked auto-encoder model (MAE) [19, 36], denoted as $f_{is}$. The use of data augmentation (DA), such as rotation, flipping, and scaling, enables the model to better generalize inconsistencies often encountered in fake images to enhance the robustness and diversity of image semantic data. We exploit the ERNIE2.0 [56] model to extract the text (T) rep- resentations, denoted as $f_t$. ERNIE2.0 includes several advantages such as modelling sentence-level relations (in addition to word- level), and large-scale semantic knowledge stored in knowledge graphs. With these improvements, ERNIE2.0 can better evaluate the consistency of text content with visual information in images, and analyze facts and entity relationships in text. The structured knowl- edge encoded in ERNIE2.0 helps enhance the reasoning ability of the entire architecture in global synergies across modalities. Expert Review and Opinions: The workflow of the novel expert network is depicted in the model in Figure 2 (left). In this stage, we simulate the scenario of experts with rich expertise to review. The expert networks of different modalities accept features provided by the extractors of their respective modules, and then the mixture of experts will jointly review and select these features, and provide preliminary predictions. Our MMoE-Pro upgrades the traditional MMoE [45] mainly by introducing token attention and relaxing the softmax constraint in its gating mechanism. In particular, suppose the input f, which"}, {"title": "Distribution Adjustment:", "content": "We depict this stage in Figure 2 (right). The coarse prediction results from the previous stage are calculated as mean and standard deviation as an acceptable input form for AdaIN. AdaIN then adaptively adjusts the feature distribution ac- cording to the contribution of each modality. This step ensures that the most relevant and reliable information is prioritized. We first calculate the parameters, mean \u03bc and standard deviation \u03c3, required by AdaIN. Unlike the standard AdaIN approach, our \u03bc and \u03c3 are generated through MLP networks rather than being directly extracted from the style features. Specifically, for each output O from the coarse prediction, we use MLPs to generate the mean and standard deviation, denoted as \u03bc = MLP\u03bc (sigmoid(0)) and \u03c3 = MLP\u03c3 (sigmoid(0)), where $MLP_\u03bc$ and $MLP_\u03c3$, represent the MLPs for calculating the mean and standard deviation, respectively. The adjustment process of AdaIN is then formalized as $e = \u03c3(r \u2013 \u03bc_r)/\u03c3 + \u03bc$. (2)"}, {"title": "Veto Voting:", "content": "The novel veto classifier is depicted in Figure 3. Before the voting mechanism is triggered, the enhanced representations of all modalities produced by AdaIN are concatenated and denoted as $e_{mix}$ (Figure 1), and then the MMoE-Pro performs the same re- finement process to obtain $r_{mix}$ (Figure 1) and $O_{mix}$ successively. Finally, this concatenated prediction output and the previous coarse prediction output of each modality are used as input into the voting stage. Our novel veto voting combines the inspiration of thresh- olds and confidence to dynamically manage the contributions and conflicts of each modality prediction to ensure the reliability and transparency of the decision-making. We define two thresholds to distinguish between high confi- dence and low confidence, where $\u03b8_{high}$ and $\u03b8_{low}$ define the high confidence and low confidence thresholds, respectively, used to determine if a module's prediction can be used as a decision ba- sis. For the prediction output Oi of each module i, we apply the sigmoid function to convert it into a confidence probability value as $P_i = sigmoid(O_i)$ where $P_i$ is the confidence of the prediction from module i. Let the confidence of the concatenated output be $P_{mix}$, initially set as $P_{mix} = sigmoid(O_{mix})$. Suppose $P_{mix}$ is used as the basis for comparing other module confidences and for the final decision. Let Oi and $O_{mix}$ be the raw outputs of the module prediction and the concatenated output, respectively. Let the major- ity class be the class decided by the majority of module decisions; whether module i belongs to the majority class can be determined"}, {"title": "4 Experiments and Results", "content": "To rigorously evaluate the efficacy of GAMED in detecting fake news, we conducted extensive experiments. This section details the experimental framework, evaluation criteria, and the notable results obtained. The objective is to determine whether GAMED outperforms recent robust models and to assess the contribution of each component through ablation studies. A qualitative analysis also demonstrates the transparency of the decision-making process."}, {"title": "4.1 Experimental Setup", "content": "Datasets: We conducted training, validation, and test of GAMED and other models on two publicly accessible datasets: Fakeddit and Yang. Fakeddit is a vast collection with over one million labelled samples, classified as real or fake news. It offers a balanced divi- sion, consisting of 628,501 fake news instances and 527,049 real news instances. Derived from a wide range of 22 subreddits, Faked- dit provides a rich diversity of domains and topics, mirroring the real-world scenario. Fakeddit is a dataset that offers fine-grained categories. For our model, which focuses on binary classification tasks, we utilize only the 2-way labels. The Yang dataset includes 20,015 news articles, with 11,941 marked as fake and 8,074 as real. The fake news is sourced from more than 240 websites, while the genuine news is obtained from reputable, authoritative outlets like the New York Times and Washington Post. The dataset used in Ying et al. [69] is not publicly accessible due to strict API restrictions on obtaining image data from Twitter and Weibo. Our attempts to contact the authors were unsuccessful. Settings: The Fakeddit dataset comprises 563,612 training sam- ples, 58,798 validation samples, and 59,271 test samples. In con- trast, Yang's dataset contains 4,655 training samples, 582 valida- tion samples, and 583 test samples. Each text has a correspond- ing image. In processing image data, we utilize two pre-trained models: mae-pretrain-vit-base for semantic analysis and pytorch- InceptionResNetV2 for pattern recognition. Text data is processed with ernie-2.0-base-en. MAE-ViT and ERNIE have a hidden dimen- sion of 768, with their parameters kept frozen. Our preprocessing steps aim to optimize the handling of input data. This process in- cludes resizing all images to a consistent size of 224 x 224 pixels. We also set a maximum tokenization length of 197 for both text and image data. All MLPs in GAMED include one hidden layer and SiLU activation function. We use AdamW optimizer with 1 \u00d7 10-4 learning rate. The model typically reaches peak accuracy within 9- 10 epochs on Fakeddit and 5-7 epochs on Yang. The default setting is 14 epochs. Evaluation Metrics: To align with comparative models, we use the widely accepted metrics: Accuracy (Acc), Recall (R), Precision (P), and F1 Score for this task."}, {"title": "4.2 Overall Results", "content": "The results in Table 1 demonstrate that our GAMED is quanti- tatively superior in performance when compared with different competitive models including those that are recently developed. On Fakeddit, GAMED achieves 93.93% accuracy, surpassing the state- of-the-art open-source detection scheme MTTV by 2.05% and LoRA- fine-tuned CLIP and LLaVA combination by 1.39%. On the Yang dataset, GAMED achieved a remarkable accuracy of 98.46%, surpass- ing the state-of-the-art MCNN by 2.16%. Meanwhile, GAMED also ranks first in Precision, Recall, and F1 on both Fakeddit and Yang. We tested recently developed BMR architecture on both Fakeddit and Yang datasets, and the results demonstrate that our GAMED outperforms BMR in almost all evaluation metrics, except for the Precision of Fakeddit. Given the popularity of large language models (LLMs), in Ta- ble 2, we compare GAMED with LLMs-based fake news detection schemes, and the experiments are conducted on Fakeddit and de- picted in Table 2. We obtained similar conclusions on the Yang dataset too. The Direct approach employs the model for fake news detection without any preprocessing of the input data, relying solely on the model's internal knowledge to directly generate predictions and reasoning. In contrast, the Chain of Thought (CoT) [63] ap- proach enhances the model's ability to handle complex tasks by prompting it to \"think step by step\", guiding the model to first produce a reasoning process before delivering a final prediction. Both LLAVA [35] and the GPT-4 family [1] underperformed on this task, falling short of most traditional language models listed in Table 1. Furthermore, fine-tuned LLMs like InstructBLIP [11] or tool-enhanced LLMs like FacTool [9] did not improve the capabili- ties of LLMs. The state-of-the-art architecture LEMMA [65] has an accuracy of only 82.4%, i.e., significantly lower than our GAMED by 11.5%. In Precision, Recall and F1 score, GAMED shows improve- ment better than LLMs, except that it ranks second in Recall for the real news category, slightly lower than GPT-4V using CoT. Our model performs better than the strong comparative models with several reasons. As mentioned before, we address some of the key shortcomings in the existing models. By employing modal de- coupling and cross-modal synergy, GAMED preserves and enhances the discriminative features of each modality. The feature refinement components dynamically emphasize the most relevant information, in contrast to the static methods used before. Moreover, GAMED exploits semantic knowledge from pre-trained models, deepening its understanding of facts and relationships, which is a capability that many comparative models lack. Finally, our novel veto voting mechanism, which combines consensus and confidence, ensures that the most reliable predictions drive the final decision, offering greater flexibility than traditional voting or fusion methods. As depicted in Figure 4, we randomly select ten fake and ten real news samples to visualize the heat map. The colours of the heat map"}, {"title": "4.3 Ablation Study", "content": "Removing Individual Modules: In Table 3, we present the abla- tion results. By removing modules, we find that the text module performs the best among all individual modules, achieving an ac- curacy of 90.1%, which even exceeds many excellent fake news detection models. Next are IS and IP, but their combined perfor- mance only reached 88.5%, still trailing the text module by 1.6%. The worst-performing individual module is MM, with a peak accuracy of only 61.4%, demonstrating the effectiveness of our weakened fusion module design. However, the combination of individual IS and T using the same data achieved an accuracy of 90.9%, signifi- cantly outperforming the MM module by 29.5%. More importantly, GAMED maintained strong detection capabilities despite the poor performance of the MM module. This further supports our view that the contributions of unimodal distinctiveness and their cross- modal synergies to model performance outweigh standalone modal fusion. Finally, compared with the high accuracy of GAMED, these removals prove that no single modality or any combination can reach the overall performance of GAMED. Knowledge Enhancement: We used BERT to replace ERNIE to process the text data, but the result dropped by 1.2%. Although both two models are based on similar transformer architecture, the advantage of ERNIE is that it further incorporates a structured knowledge graph to enhance the understanding of facts and re- lations. In addition, we used ERNIE1.0 instead of ERNIE2.0, and the result dropped by 0.3%. This is because ERNIE2.0 introduced a more complex knowledge increment strategy and larger knowledge parameters than ERNIE1.0 during pre-training. In the previous indi- vidual module removal, we found that the performance of a single text module was stronger than the combined performance of images modules, and the initial performance gap between IP and IS was large; but after the enhancement of the text module, not only the performance was greatly improved, both exceeded 90%; however, the gap between IP and IS became very small. ERNIE's victory high- lights that external knowledge integration is crucial to enhancing the overall performance in complex tasks, i.e., multimodal fake news detection. In addition, to measure the impact of different feature extractors, we used InceptionV3 instead of Inception-ResNet-v2 and ViT instead of MAE-VIT, which resulted in a 1.5% and 0.8% drop in accuracy for GAMED, respectively. Expert Network: We replaced MMOE-Pro with standard MMOE, resulting in a 1.1% drop in GAMED's accuracy. This decline is due to MMoE-Pro's enhancements in feature sharing and task relation- ship modelling, which allow the model to effectively process and fuse multimodal data. We replaced the MMoE-Pro networks with the ViT blocks for feature refinement, but the accuracy dropped"}, {"title": "4.4 Qualitative Analysis", "content": "As shown in Figure 6, we demonstrate the interpretability of the decisions made by GAMED using real examples. We randomly se- lected three prediction results on the Fakeddit test set and traced the corresponding samples. From left to right, following the veto voting rule in Section 3, the prediction result of the first sample is \"Real\", this is because the confidence of several modalities is between the pre-set low threshold $\u03b8_{low}$ and the high threshold $\u03b8_{high}$, so the initial concatenated prediction $P_{mix}$ is directly used as the final pre- diction. $P_{mix}$ represents the result after the multimodal features are concatenated, and it makes full use of the complementarity of each modality to make the prediction result more comprehensive and reliable. The second sample shows that since the output confidence of the text module $P_t$ is higher than the high threshold $\u03b8_{high}$ and higher than $P_{mix}$, the prediction of the initial $P_{mix}$ is replaced with the prediction $P_t$ as the final decision. The text modality provides extremely reliable information in this context. We can maximize the use of this reliable information and improve the accuracy of the final decision. The third sample reflects the decision made by GAMED when the output confidence of the image pattern module $P_{ip}$ is lower than the low threshold $\u03b8_{low}$. This is because too low confidence means that the information of this modality may be unreliable or misleading, even if it belongs to the majority class. By ignoring this unreliable prediction and comprehensively reconsid- ering the combination of the highest output in all modalities and the concatenated output, the robustness of the final decision is en- sured. The fourth sample simulates the black-box decision-making process of many current models, in which the model cannot clearly explain the specific reasons for its decision. This may not only lead to a decrease in user trust in the model's prediction results but also make it difficult to debug and improve effectively when errors occur. In addition, GAMED's modal-decoupling design is also used for interpretability. For example, when AdaIN adaptively adjusts the feature distribution of different modules, we can judge the contribution of each modality and its discriminative features to the prediction. In contrast, those black-box models do not have a clear explanation path when processing input data. This means that the internal working mechanism of the model is invisible to users and developers, resulting in people being unable to understand or verify the reasoning process behind it even if the model makes a correct classification."}, {"title": "5 Conclusions", "content": "This paper developed GAMED - a novel architecture that signifi- cantly improves fake news detection. GAMED overcomes the short- comings of current multimodal approaches through a dynamic mechanism of modal decoupling and cross-modal synergy. It em- beds the benefits of semantic information encoded in knowledge graphs into the whole workflow from pre-trained language mod- els. Feature selection is performed jointly by a mixture of experts, accompanied by subsequent adaptive distribution adjustment, pro- gressively refining the feature representation of each modality in the pipeline and enhancing its discriminability and distinctiveness. Finally, a flexible and transparent decision process is introduced. Our experiments on benchmark datasets, Fakeddit and Yang, show that GAMED improves upon recent top-performing models in detec- tion accuracy. Future work would explore adding more modalities such as audio or video for a more holistic analysis of fake news."}, {"title": "6 Acknowledgment", "content": "This work was supported by the Alan Turing Institute/DSO grant on improving multimodality misinformation detection through affective analysis. We gratefully acknowledge NVIDIA for provid- ing computational resources through its NVIDIA Academic Hard- ware Grant Program 2021. Additional support was provided by the Interdisciplinary Research Pump-Priming Fund, University of Southampton."}, {"title": "7 Ethical Considerations", "content": "Some of the key ethical considerations include addressing the issues when models can perpetuate existing societal biases if the training data is biased. This can lead to discriminatory outcomes, such as un- fairly targeting certain groups or individuals. Besides that, different cultures have varying norms and understandings of truth and fake news. Models trained on data from one culture may not perform well or ethically in another. Another fundamental challenge lies in overly aggressive detection models that could lead to the suppres- sion of legitimate speech, particularly for marginalized voices or those critical of authority. Incorrectly flagging accurate information as fake news can damage reputations and stifle public discourse. Our goal in this work is to develop a model that could understand how it reached its conclusions. Black-box models make it difficult to identify and address biases. Overreliance on automated detection systems could erode trust in traditional media and journalism."}]}