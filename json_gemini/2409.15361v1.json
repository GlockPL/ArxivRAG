{"title": "Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning", "authors": ["Essa Jan", "Nouar AlDahoul", "Moiz Ali", "Faizan Ahmad", "Fareed Zaffar", "Yasir Zaki"], "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to their adoption across a wide range of tasks, ranging from code generation to machine translation and sentiment analysis, etc. Red teaming/Safety alignment efforts show that fine-tuning models on benign (non-harmful) data could compromise safety. However, it remains unclear to what extent this phenomenon is influenced by different variables, including fine-tuning task, model calibrations, etc. This paper explores the task-wise safety degradation due to fine-tuning on downstream tasks such as summarization, code generation, translation, and classification across various calibration. Our results reveal that: 1) Fine-tuning LLMs for code generation and translation leads to the highest degradation in safety guardrails. 2) LLMs generally have weaker guardrails for translation and classification, with 73-92% of harmful prompts answered, across baseline and other calibrations, falling into one of two concern categories. 3) Current solutions, including guards and safety tuning datasets, lack cross-task robustness. To address these issues, we developed a new multitask safety dataset effectively reducing attack success rates across a range of tasks without compromising the model's overall helpfulness. Our work underscores the need for generalized alignment measures to ensure safer and more robust models.", "sections": [{"title": "1 Introduction", "content": "The popularity of ChatGPT (OpenAI et al., 2024) has led to an explosion of Large Language Models (LLMs), with new models released every month. Consequently, many new LLMs have come to light, such as Llama3 (Llama Team, 2024), Mistral MOE8x7B (Jiang et al., 2024), Gemini1.5 (GeminiTeam et al., 2024), and Claude3.5 (Anthropic, 2024).\nSince the data used during pre-training and instruction-tuning has noise, these models undergo safety alignment before being released to ensure that they produce safe outputs (Dong et al., 2024). Different datasets like SafetyBench (Zhang et al., 2024b), CyberSecEval (Bhatt et al., 2023), HolisticBias (Smith et al., 2022) and ControversialInstruction (Sun et al., 2023) have been introduced to reduce bias and enhance LLMs' safety and fairness.\nCompanies fine-tune LLMs to apply them to various downstream tasks, as the larger world models enhance performance across a range of tasks (Zhang et al., 2024a). Various red teaming efforts (Chao et al., 2024; Anil et al., 2024) have shown that LLMs can be jail-broken through different adversarial attacks. However, it was also shown that fine-tuning LLMs even on benign data\u00b9 can lead to weaker safety guardrails (Qi et al., 2023).\nDespite this, there is limited understanding of the effect of various fine-tuning settings and tasks on the guardrails of these LLMs, and how the outcomes vary across different domains. This paper primarily focuses on four Research Questions (RQs) examining the degree to which fine-tuning compromises the LLMs' safety guardrails:\n\u2022 RQ1: Do LLM safety guardrails differ across multiple tasks?\n\u2022 RQ2: How does fine-tuning on various tasks using benign data impact model safety across different tasks?\n\u2022 RQ3: Are existing solutions, such as guard models or safety tuning datasets, generalizable across multiple harmful tasks?\n\u2022 RQ4: How does the robustness of proprietary Language models differ from open-source ones when it comes to safety guardrails?\nTo answer these RQs, in our research, we expand on the idea of fine-tuning on benign data and evaluate the effect of fine-tuning with benign data across"}, {"title": "2 Related Work", "content": "2.1 Instruction Tuned LLMs and fine-tuning\nFine-tuning adapts pre-trained LLMs for specific tasks by updating model parameters to improve performance (Howard and Ruder, 2018).Instruction-tuned models are optimized to follow user instructions, addressing the tendency of base models to generate irrelevant or incorrect responses (Bender et al., 2021; Bommasani et al., 2021; Ouyang et al., 2022). Safety alignment through Reinforcement Learning from Human Feedback (RLHF) further ensures that models can reject harmful inputs while being truthful and helpful (Ouyang et al., 2022), evaluated using benchmarks like TruthfulQA and RealToxicityPrompts (Lin et al., 2022; Gehman et al., 2020).\n2.2 Jailbreaking Attempts\nAs instruction-tuned LLMs gain popularity, several studies have explored methods to bypass their safety guardrails. (Wei et al., 2023a) demonstrates how techniques like prefix injection and prompt-based attacks can compromise these models. Prompt injection has also been used to breach black-box LLMs (Liu et al., 2024a,b).Persuasive prompts have been shown to jailbreak LLMs, though models like Claude are more resistant due to Reinforcement Learning from AI Feedback (RLAIF) (Zeng et al., 2024; Anthropic, 2022; Bai et al., 2022). Cross-lingual vulnerabilities have been identified, where GPT-4's safety filters are bypassed low-resource language translations (Yong et al., 2024). Studies such as (Xu et al., 2023) highlight tools like PromptAttack, while (Zhao et al., 2024) shows that in-context learning with trigger words can effectively jailbreak models.\n2.3 Safety-Tuning and Guard Models\nRecent efforts have focused on improving LLM safety alignment during fine-tuning. (Bianchi et al., 2024) shows that incorporating 3% safety examples during Llama's fine-tuning improves safety without significantly reducing helpfulness, though the focus remains on text generation tasks. Similarly, (Ji et al., 2023; Wang et al., 2024) explore multilingual safety datasets but also focus on text generation. (Pan et al., 2024) introduces a method to limit state drift during fine-tuning, improving alignment but increasing computation. Most recent safety benchmarks, such as SorryBench and SaladBench (Xie et al., 2024; Zhang et al., 2024b; Li et al., 2024; Yuan et al., 2024), also concentrate on text generation, leaving gaps in specialized tasks.\nGuard models for filtering LLM outputs have been introduced (Inan et al., 2023), but performance is limited on certain tasks. OpenAI's moderation API (Markov et al., 2023) is available but limited to English inputs."}, {"title": "3 Methodology", "content": "Here, we outline the methodology, including the training datasets used for fine-tuning and the testing data for assessing the impact on security and unsafe outputs. We also present baseline results from pre-fine-tuned models as benchmarks for comparison\u00b3.\n3.1 Datasets curation\nTraining datasets: To fine-tune our models, we selected a range of benign datasets, aligned with"}, {"title": "4 Evaluations", "content": "4.1 GPT-4o judge vs. human annotators\nTo validate the accuracy of our proposed GPT-40 judge, we evaluated the outcome of the three LLM models (i.e., GPT-40-mini, Gemini 1.5 Flash, and Llama3.1-8B) using three different human annotators. Specifically, those annotators were asked to label the outcome of the LLM responses as either 1 (i.e., answered), or -1 (i.e., rejected). We then computed the majority vote of the human annotators and measured the agreement with our GPT-40 judge. Table 3 specifies the agreement (Cohen Kappa score (Cohen, 1960)), accuracy, and macro F1-score of GPT-40 with the human annotations.\nAs can be seen, our proposed GPT-40 judge provides almost perfect agreement for the different models (i.e., >80%, indicating that it is highly consistent in its classifications with the majority vote of the human annotators). In addition, the results also show that the proposed GPT-40 judge provides high accuracy and macro F1-score with >90% for both metrics across the different models.\n4.2 Base Models Results\nThe base model results indicate that GPT-40-mini, Gemini 1.5 Flash, and Llama3.1-8B exhibit strong guardrails for text generation, with an Attack Success Rate (ASR) of 0% for generated text, as shown in Table 4 (see the blue-colored cells of the first row). However, most models demonstrate weaker guardrails for the translation and classification tasks (see the red- and yellow-colored cells, respectively). Our experiments show that the highest ASR for translation is 91.8% for GPT-40-mini's responses to harmful prompts, while the highest ASR for classification is 80.4% for Llama3.1-8B's responses. These findings demonstrate that the safety guardrails of LLMs vary significantly across different tasks, thus answering RQ1. This insight highlights the importance of evaluating such guardrail mechanisms across various types of harmful prompts, which is overlooked in related work\n4.3 Fine-tuning Analysis\n4.3.1 Fine-tuning Category Analysis\nOur fine-tuning results, summarized in the bottom four rows of Table 4, indicate that, for open-source models, fine-tuning on any of the four investigated tasks yields the highest ASR against translation tasks and, to a certain extent, the classification tasks. This pattern is consistent not only with Llama3.1-8B but also with Gemma, Llama2 and Phi as detailed in Appendix Tables 11 to 13. On the other hand, both proprietary models (GPT-40-mini and Gemini 1.5 Flash) tend to have weaker guardrails for classification tasks when fine-tuned, which results in a higher ASR.\nThe observed increase in ASR across different models and tasks demonstrates that fine-tuning generally increases susceptibility to adversarial attacks for both proprietary and open-source models, with the exception of text generation, where attacks remain effectively blocked. Specifically, as shown in the base model results in Table 4, LLMs tend to have highly effective safeguards in place for text generation. Consequently, even after fine-tuning, the ASR for text generation remains close to 0%. These findings indicate that fine-tuning makes LLMs more vulnerable to safety risks compared to their base models, particularly in tasks like translation and classification, thus addressing RQ2. In conclusion, while task-specific fine-tuning can enhance performance, it also introduces significant safety risks.\n4.3.2 Model Guard Performance\nAs demonstrated earlier, while Task1 (i.e., text generation) harmful prompts are effectively blocked by both the base and fine-tuned models, the ASR for the remaining three tasks increases after fine-tuning. One potential solution to the above problem is to use what is known as a guard mechanism or moderator. Such a mechanism is designed to oversee and filter either, or both, the input and output of LLMs to ensure that they comply with ethical and safety guidelines. As such, we set out to evaluate each model's recommended guard mechanism. That is OpenAI moderator (Markov et al., 2023) for GPT-40-mini, low & above safety setting for Gemini 1.5 Flash, and Llama Guard (Inan et al., 2023) for Llama3.1-8B.\nAs shown in Table 5, OpenAI moderator successfully blocks a substantial portion (69%) of translation prompts that were initially answered by the base GPT-40-mini. Despite the reduction, we still see a very high ASR for classification and translation tasks. For Gemini 1.5 Flash, we apply the highest blocking settings (low & above) present in the model safety settings, resulting in the lowest ASR across all tasks in comparison to the guards of our other two models. However, for Llama3.1-8B, even with the application of Llama3-Guard (see Appendix Table 10), the ASR for classification prompts remains high (51-67.3%) across both the base and fine-tuned versions. Translation prompts also exhibit a relatively high ASR (23.5-35.3%) across the models. These findings indicate that while guard models are effective at blocking text and code generation prompts, both Llama3-Guard and OpenAI moderator face challenges in blocking translation and classification prompts. These results answer our RQ3, highlighting that using such guards can be ineffective for certain tasks.\n4.3.3 Existing Safety Datasets\nIn this subsection, we shift our attention to evaluating the effect of adding safety data to the fine-tuning process in addition to the benign data to improve the safety guardrails of LLMs. Inspired by the work of Bianchi et al. 2024 and by using their safety data, we fine-tune our LLMs on the various tasks, and measure the ASRs across harmful prompts belonging to different tasks. In addition, we also apply the models' specific guards on top of the safety-tuned LLMs to evaluate their combined effect in combating the guardrails degradations.\nTable 6 reveals that despite safety tuning and the addition of guard models, the ASR remains high for both classification and translation tasks across most models. The only exception is Gemini 1.5 Flash, which shows a high ASR only for the classification tasks. In contrast, text and code generation attacks generally see a significant drop in success rates, approaching near zero. Additionally, open-source models still exhibit more safety violations compared to proprietary models. These findings provide valuable insights into RQ3, indicating that current safety solutions-both safety tuning and guards-lack robustness, as they focus primarily on text and code generation while overlooking classification and translation tasks.\nWe conducted a sensitivity analysis on one of the fine-tuned models to ensure the reproducibility of the ASR values across all our models. The results of this analysis are provided in Appendix Table 15.\n4.3.4 MultiTaskBench Safety Tuning\nAs shown in Table 6, existing safety-tuned datasets (Bianchi et al., 2024), which primarily focus on text generation, effectively lower the ASR for text generation tasks and perform reasonably well for code generation. However, the results also reveal significant challenges in translation and classification tasks, where the ASR remains notably high for both open-source and proprietary models. For proprietary models, the classification ASR ranges from 10% to 32%, while for Llama3.1-8B models, three out of four fine-tuned models exhibit a classification ASR above 40%. Similarly, the ASR for translation tasks remains elevated, particularly for Llama3.1-8B and GPT-40-mini models.\nTo address these issues, we propose a MultiTaskBench safety-tuning dataset, as detailed in Section 3.1. The results of fine-tuning with this dataset on all four tasks, shown in Table 7, demonstrate a substantial reduction in ASR across all tasks. For translation tasks, most of the models achieve an ASR of 0%. Even though Llama3.1-8B models still show a relatively higher ASR for translation after fine-tuning on the translation task (from 98% to 35%), this represents a significant improvement. Similarly, for classification tasks, all three models exhibit considerable reductions in ASR.\nDespite the significant reduction in model harmfulness, the MultiTaskBench over-refusal evaluation shows that helpfulness is only minimally impacted (as presented in Table 8). Including 20% of our safety data in fine-tuning leads to a per model average increase of just 1-4% in false positives or overrefusals compared to the fine-tuned models without the safety data (except for two tasks in Gemini 1.5 Flash, where the increase is around 12%). This suggests that these safety measures can enhance the model's alignment without significantly compromising its overall helpfulness. These results also allow us to compare proprietary and open-source LLMs, answering RQ4, where we observe considerable differences in guardrail degradation and susceptibility across model types.\nTo further assess the generalizability of our findings, we evaluated both the usefulness and ASR of a model fine-tuned with a 5% sample of our safety data (see Appendix Table 14). Additionally, we tested the usefulness of the same fine-tuned model"}, {"title": "5 Conclusion", "content": "In this paper, we investigated the safety vulnerabilities that arise when fine-tuning LLMs on downstream tasks. Expanding on the idea of fine-tuning on benign data, we demonstrate that fine-tuning on specific tasks, like translation and classification, leads to more guardrail degradation than others. Similarly, guardrails for some tasks, like translation, show degradation irrespective of the fine-tuning category. Our evaluation also reveals that current safety measures, such as guard models and safety-tuning datasets, often show limited efficacy across tasks, frequently failing to generalize.\nTo address these challenges, we introduced MultiTaskBench, a safety-tuning dataset designed to enhance LLM safety across a variety of downstream tasks. Our evaluation demonstrates that MultiTaskBench effectively mitigates safety risks without compromising the LLM's overall performance. Given the growing adoption of LLMs across industries, ensuring consistent and robust safety across diverse tasks is crucial. Hence, we highlight the need for more generalized alignment techniques"}, {"title": "6 Ethical considerations", "content": "We aim to advance LLMs' safety efforts by giving restricted access to our datasets and models for public use. While acknowledging potential misuse, our goal is to promote research that leads to safer, more refined models. We hope these resources will help the community develop LLMs that uphold the highest standards of safety and ethics."}, {"title": "7 Limitations and future direction", "content": "Due to resource limitations, our experiments are primarily focused on four tasks. However, to enhance the safety alignment of LLMs across a wider range of downstream tasks, more diverse and rigorous testing is needed. Additionally, there is a need for more generalized helpfulness testing across various tasks and out-of-distribution prompts.\nWe also hypothesize that models lose their effectiveness against advanced adversarial and jailbreaking attacks after being fine-tuned on benign data. Therefore, future research should explore how the success rates of different prompt injection and jailbreak attacks, for which the models were previously aligned, change after fine-tuning."}]}