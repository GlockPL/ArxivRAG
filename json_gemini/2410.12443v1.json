{"title": "Reconstruction of Differentially Private Text Sanitization via Large Language Models", "authors": ["Shuchao Pang", "Zhigang Lu", "Haichen Wang", "Peng Fu", "Yongbin Zhou", "Minhui Xue", "Bo Li"], "abstract": "Differential privacy (DP) is the de facto privacy standard against privacy leakage attacks, including many recently discovered ones against large language models (LLMs). However, we discovered that LLMs could reconstruct the altered/removed privacy from given DP-sanitized prompts. We propose two attacks (black-box and white-box) based on the accessibility to LLMs and show that LLMs could connect the pair of DP-sanitized text and the corresponding private training data of LLMs by giving sample text pairs as instructions (in the black-box attacks) or fine-tuning data (in the white-box attacks). To illustrate our findings, we conduct comprehensive experiments on modern LLMs (e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-40, Claude-3, Claude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used datasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and sentence-level DP. The experimental results show promising recovery rates, e.g., the black-box attacks against the word-level DP over WikiMIA dataset gave 72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on ChatGPT-40, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study indicates that these well-known LLMS have emerged as a new security risk for existing DP text sanitization approaches in the current environment.", "sections": [{"title": "1 Introduction", "content": "State-of-the-art large language models (LLMs), such as ChatGPT [7] and LLaMA [54], have been applied in numerous real-world applications due to their unprecedented capabilities, achieved through training billions of parameters on vast amounts of text from the Internet [26]. However, several studies [8, 19, 30, 31, 38, 40, 44, 45, 51, 52, 64] have reported privacy leaks due to interactive communication between external users or adversaries and LLMs. For instance, [30, 38, 45] exploit LLMs to extract sensitive information from their training"}, {"title": "2 Background", "content": "This section briefly introduces the existing DP text sanitization approaches and scope of privacy. We also give the summary of notations used in this paper in Table 14 in Appendix A and introduction to language models in Appendix B."}, {"title": "2.1 Differentially Private Text Sanitization", "content": "Differential Privacy [22] (DP) provides privacy guarantees for every single record in a dataset by adding extra noise into query results to limit the influence of each record. In practice, DP has become the de facto standard of privacy definition for machine learning algorithms. Deferentially private text sanitization is commonly applied in text anonymization tasks. There are two types of well-established DP text sanitization approaches: word-level DP [24,25] and document/sentence-level DP [32, 36, 41, 57]. We give the detailed description and algorithms of the word-level DP, the sentence-level DP and their comparisons in Appendix C. A summary of these DP methods is in Table 1."}, {"title": "2.2 Personally Identifiable Information", "content": "Personally Identifiable Information (PII) refers to any data that can be used to identify a specific individual. PII is crucial for privacy and security, as its misuse can lead to identity theft, financial loss, and privacy violations. Pil\u00e1n et al. [50] category PII to direct identifier when one data can re-identify an individual and quasi-identifier when identifiers combined with others can re-identify an individual. This includes direct identifiers like names, phone numbers, and IP addresses, and quasi-identifiers like birth date, gender, and postal code. Named Entity Recognition (NER) is a technology that locates and classifies named entities in text into predefined categories (e.g., names, locations, times). NER is crucial in various natural language processing applications, including search engines, question-answering systems, and content categorization. State-of-the-art NER (e.g., Flair [10], NLTK [14], and SpaCY [28]) classify sequences using a Transformer neural networks. In our case, we leverage NER to tag PII for the given text because it is challenging for a large dataset. In particular, Table 15 in Appendix D provides the semantics and examples of classes for grouping using NER in our work. Finally, it is noteworthy that in our experiments we did not store any PII as auxiliary knowledge for our attacks."}, {"title": "3 Problem Formulation", "content": "This section introduces the research question and the threat model, including the attacker's capabilities and the attack target."}, {"title": "3.1 Problem Statement", "content": "Recall our motivating example of the \"future\" privacy issues in Section 1. Since the scope of privacy might not be fixed, portions of the training data used for existing LLMs could potentially become private in the future, even if they were not initially considered as such.\nAmong all privacy enhance techniques, differential privacy (DP) [22] is an ideal solution to address such \"future\" privacy issues by using DP-sanitized text as prompts to update the knowledge base of a trained LLM. This allows the updated LLM to use the privacy-preserved knowledge instead of the original private information when answering external requests. Thanks to the rigorous privacy guarantee provided by DP, the existing prompts leakage attacks [8,51,64] can only reveal the DP-sanitized prompts but not the original ones.\nNevertheless, take a reasonable assumption where the DP-sanitized prompts should have similar semantics to the original text, a question remains open: In our future privacy circumstance, given DP-sanitized prompts, could LLMs recover the original privacy of the sanitized prompts?"}, {"title": "3.2 Threat Model", "content": "Adversary's Capabilities. We consider that adversaries either have black-box or white-box access to trained LLMs. For black-box access, an adversary can prompt the LLM with a prefix and obtain the probability distribution of the next token via paid APIs, e.g., ChatGPT and Claude; whereas the white-box adversaries have an auxiliary dataset following the same distribution as the private training dataset and the access to modify the model parameter \u03b8 of the target LLMs, such as OPT, Pythia, GPT-Neo and GPT-J. We further assume that the same as other DP approaches, the DP-sanitized prompts, the privacy budget and the DP approach (word-level DP or sentence-level DP) are publicly available [12]. Note that the difference between the DP-sanitized text and the original text is determined by the privacy budget and the DP approach. Adversary's Target. The adversaries aim to generate a reconstructed version of the original text corresponding to the DP-sanitized text. Prior work [18] shows that only 1% of the pre-training dataset is memorized by some LLMs, indi-"}, {"title": "4 Methodology", "content": "In this section, we give the technical details of our reconstruction attacks against the DP sanitization, given black-box access or white-box access to the LLMs, respectively."}, {"title": "4.1 Overview", "content": "In a nutshell, the two attacks, black-box instruction-based attacks and white-box fine-tuning-based attacks, contains the following four constructing blocks, which covers the functionalities of LLMs (GENERATE) and text processing operations (GENPROMPT, CONCATENATE, and SANITIZE). Specifically, the key idea behind our attacks is to use pairs of example text and its DP-sanitized copy to trigger the memorization of the target LLM, so that LLM could return the private text for given DP-sanitized prompts. Algorithm 1 depicts our two attacks from Line 1 and Line 5, respectively.\n\u2022 GENERATE(p, 0): Given prompt p and a trained LLM's model parameter 0, the model returns the answer. With black-box access to the model, we can utilize APIs to generate the response.\n\u2022 GENPROMPT(PT, x): For a given sequence x and prompt template PT, generate a new prompt.\n\u2022 CONCATENATE(x,y): For two sequences x and y, generate a new sequence by concatenating them.\n\u2022 SANITIZE(x, P, \u03b5): For a given text x, return sanitized text by applying DP text sanitization approach P under the privacy budget \u03b5."}, {"title": "4.2 Black-box Instruction-based Attacks", "content": "Instruction-tuned LLMs can be applied to various tasks by following (human) instructions. This is because they have been fine-tuned on extensive data of instructional questions and corresponding answers for dialogue scenarios. Consequently, we can leverage the instruction-following capabilities of these models to perform reconstruction attacks. Specifically, we propose an attack based on instructions for instruction-tuned LLMs, which reconstructs sanitized text by querying the model with a specific prompt. Line 1 to Line 4 in Algorithm 1 presents the formal workflow of the black-box instruction-based attack. As shown in Figure 2(a), for the target sanitize text x and a instruction-tuned LLM with model parameter 0, we assume that an adversary has the black-box access to the model. First, the adversary prepares a prompt template PT as blow where the {original text} is the private text and the {edited text} is the DP-sanitized text.\nThen, the adversary constructs instructions using the prompt template PT and sanitized text x. Finally, the adversary inputs the instruction into the model and uses the output as reconstructed text corresponding to the sanitized text. The prompt template aims to enable LLMs to respond with content related to sanitized text based on the previous training. This attack method is simple yet efficient, and we discover it can recover a significant amount of original text under commonly"}, {"title": "4.3 White-box Fine-tuning-based Attacks", "content": "When given white-box access to an LLM, adversaries could do much more than using instructions to impact the behaviors of LLMs. Intuitively, adversaries fine-tune the target LLMs using structured pairs of original text and DP-sanitized text, then guide the LLMs to return the private text based on given DP ones. Line 5 to Line 14 in Algorithm 1 and Figure 2(b) provide the workflow of the white-box fine-tuning-based attack, where adversaries know a target DP-sanitized text x, an auxiliary dataset Xaux, the DP text sanitization approach P, the privacy budget \u03b5, and a pre-trained LLM with parameter \u03b8. The adversary samples the data point y from the auxiliary dataset, following the same distribution as the target training dataset, and constructs a new sequence z by concatenating the DP-sanitized text \u1ef9 = SANITIZE(y, P, \u03b5) using privacy budget \u03b5 and original text y as z = CONCATENATE(\u1ef9, y). Then, the adversary fine-tunes the model using the following objective:\nL(\u03b8) = - \u03a3 log Pr(zi|Z1, Z2, ..., Zi\u22121;\u03b8),  (1)\ni\nwhere zi is each token in z. After the fine-tuning process, the adversary inputs the sanitized prompts into the model and uses the output as reconstructed text corresponding to the sanitized text. The goal of fine-tuning is to make the model learn the pattern from the sanitized text to the original text. In practice, following previous works on fine-tuning LLMs for paraphrasing [60], we separate the sanitized text \u1ef9 and original text y when constructing z."}, {"title": "5 Experimental Evaluation", "content": "In this section, we evaluate our attacks, the black-box instruction-based attacks and the white-box fine-tuning-based attacks, on the real world datasets and the latest applications of LLMs against two famous DP implementations, the word-level DP [24] and the sentence-level DP [57]."}, {"title": "5.1 Large Language Models", "content": "In this work, based on how the adversaries could interact with the LLMs, we categorize the LLMs into two types - the LLMs only accepted external instructions (targets of the black-box attacks) and the LLMs were further fine-tuned by given datasets (targets of the white-box attacks). Table 3 gives the summary of LLMs used in our experiments, including the model size, model version, and the accessibilities to the model parameters and the training datasets, where \u201cY\u201d and \u201cN\u201d indicate accessible or inaccessible, respectively. Appendix E show the details of the LLMs in our experiments."}, {"title": "5.2 Datasets", "content": "To evaluate the performance of our attacks, we should have the training data of the LLMs as the ground truth. However, limited by the intellectual property, such information is not publicly available for those LLMs being the targets of our black-box attacks (e.g., LLaMA and ChatGPT). Hence, following existing studies [52], we also assume that WikiMIA is part of the training data used by those LLMs. For the LLMs being attacked by the white-box attacks, since they are open-sourced, we use their training dataset directly in the experiments. Table 4 provides the statistics for the datasets we used. Note that we follow the existing works [38,52] to truncate the original data into specific lengths. The details of the datasets are as follows."}, {"title": "5.3 Differentially Private Text Sanitization Approaches", "content": "In the experiments, we choose MadLib [25] (Algorithm 2) as the word-level DP in our experiments, since this is the commonly used DP word-level implementation [20, 41]. In MadLib, the distance function is defined in Euclidean space and Laplace noise is added to the word embedding vector. We choose DP-Prompt [57] (Algorithm 3) as the sentence-level DP in our experiments, since Utpala et al. [57] demonstrate the DP-Prompt achieves better utility compared to other approaches when using ChatGPT. In DP-Prompt, we consider using ChatGPT-3.5 as base model to generate the paraphrased text."}, {"title": "5.4 Metrics", "content": "To measure the effectiveness of the reconstruction attacks, we consider the following evaluation metrics for privacy extraction.\n\u2022 RECALL and PRECISION. In our study, we focus on the privacy leakage of sensitive content in the text rather than the shared vocabulary (e.g., time, addresses, and names), which are more prone to causing privacy concerns; whereas common words like \"the\" and \"this\" do not lead to privacy issues. That is, we aim to recover the privacy-sensitive content from the original text. Hence, we extract and mark PII sequences (see Section 2 for"}, {"title": "5.5 Implementation", "content": "To tag PII sequences from text, we leverage Flair [10] as the PII extraction model released in HuggingFace [2]. Specifically, Flair is a NER framework based on Transformer networks to classify tokens as PII sequences. It is noteworthy that in our experiments we did not store any PII as auxiliary knowl-"}, {"title": "5.6 Results of Black-box Instruction-based Attacks", "content": "Setup. To evaluate the effectiveness of the black-box instruction-based attack, we conduct experiments on WikiMIA using both the word-level DP and the sentence-level DP under various privacy budgets (\u03b5 = {4,8,12} for the word-level DP [24] or T = {2.0, 1.5, 1.0} for the sentence level DP [57], where the relationship between T and the privacy budget can be found in Section 2) with instruction-tuned LLMs, including LLaMA, Gemma, ChatGPT, and Claude. We adjust the prompt per Figure 2 for different models and query models with the sanitized text.\nResults. Table 6 and Table 7 provides an example of the black-box instruction-based attack on WikiMIA under \u03b5 = 8 using ChatGPT-4, Claude-3-Opus, LLaMA-3-70B and Gemma-2-27B for the word-level DP and the sentence-level DP, respectively. The original text x, sanitized text x and reconstructed text \u00ee are listed, where the highlighted text is PII sequences extracted by the NER model, and the text in the red box contains the PII sequences that remain unchanged. For word-level DP, we observe that the reconstructed text can correct the errors in the sanitized text (e.g., reconstructed text using ChatGPT-4 correct \"31st\" to \"51st\"). Compared to word-level DP, sentence-level DP retains most of the PII sequences from the original text, thereby fewer PII sequences are successfully reconstructed.\nThe numeric results on WikiMIA for the word-level DP are detailed in Table 8 and Table 9, respectively. For the word-level DP, in a practical privacy budgets (e.g., \u00a3 = 8 and x = 12), the black-box instruction-based attacks successfully reconstruct sensitive information at the word level with high probability. Nevertheless, the black-box instruction-based attacks have no significant effect under a smaller privacy budget due to substantial discrepancies between the original and sanitized texts. For the sentence-level DP, we observe that SUCC, RECALL, and PRECISION are lower than the word-level DP, but SCORE is higher. The reason behind it is in two folds. First, the sentence-level DP fails to remove the sensitive information (see examples as the text enclosed in the red boxes in Table 9). Second, the metrics of RECALL, PRECISION, and SUCC measures the difference between the DP-sanitized text and the reconstructed text. Hence, these three metrics do not show a promising results.\nFor models (e.g., LLaMA and Gemma) giving weight access, which, however, were not used in the black-box attacks, we observe that the black-box instruction-based attack per-"}, {"title": "5.7 Results of White-box Fine-tuning-based Attacks", "content": "Setup. To evaluate the effectiveness of the white-box fine-tuning attacks on the pre-trained LLMs, we conduct experiments on Pile-CC, Pile-Wiki, and Pile-Enron with pre-trained LLMs, including OPT, Pythia, GPT-Neo and GPT-J. Specifically, for each dataset, we randomly select a subset containing 10,000 samples from the original datasets and split the subset into a training set having 8,000 samples, a validation set having 1,000 samples, and a test set having 1,000 samples. We fine-tune the pre-trained LLMs using the training set and the validation set and evaluate on the test set. Due to the high costs incurred by the sentence-level DP when querying the black-box API, we consider performing the fine-tuning attack for the word-level DP only.\nResults. Table 13 provides an example of the white-box fine-tuning-based attacks on Pile-CC under \u00a3 = 8 with Pythia-6.9B,"}, {"title": "6 Related Work", "content": "This section mainly introduces related works with ours in LLMs's security and privacy. Here, we category them into the following several perspectives, i.e., Training data leakage in LLMs, Training data memorization in LLMs, Prompt leakage in LLMs and Membership inference attacks for LLMs."}, {"title": "7 Mitigation & Discussion", "content": "Mitigation via machine unlearning in LLMs. Machine unlearning [46] is a technology of removing specific data or learned knowledge from a model. In particular, the pre-training data of LLMs includes a large amount of copyrighted content [21]. Several works [23, 62] investigate leveraging machine unlearning technology to forget the copyrighted content in LLMs. Similarly, our reconstruction attacks can be mitigated if we can leverage machine unlearning to eliminate the memory of sensitive information in pre-training data. Nev-"}, {"title": "8 Conclusion", "content": "This paper identifies a potential privacy risk associated with applying DP in LLMs. We demonstrate that it is possible to reconstruct the text sanitized by the DP through querying LLMs. Based on the access to trained LLMs, we propose black-box instruction-based attacks and white-box fine-tuning-based attacks. Extensive experimental results show that our attacks effectively reconstruct the DP-sanitized text under a practical privacy budget (e.g., \u03b5 \u2265 8) against both word-level and sentence-level DP."}, {"title": "Ethical Considerations", "content": "Our research is intended to enhance the security of deployed systems by helping stakeholders better understand the causes of privacy issues in AI models. Our findings could inform users and designers of such a potential vulnerability, so that more comprehensive privacy solutions can be taken in the future.\nWe only attempt to infer training data generated by LLMs; no attempt was made to further infer private and sensitive data exposure from LLMs. Furthermore, we make no attempt to de-anonymize any inferred training data. Our work aligns with the ethical guidelines of the Menlo Report, as we not only explain the reasons behind model about privacy leakage and propose basic defense methods through detection but also offer new insights for future enhancements in privacy protection.\nSpecifically, this paper discovered the capability of the modern large language models (LLMs) on recovering private content from differentially private prompts where the private information was sanitized. Such findings were based on experiments over the publicly available data and the open APIs to publicly available LLMs (see details in Section 5). We faithfully followed the Terms of Service of LLMs when interacting with the LLMs. Since it was believed that differentially private outputs were not invertible, our work might have the following negative outcomes for public interests.\n\u2022 Disclosure. We discovered a vulnerability in the existing privacy-preserving technique of differential privacy. According to the paper, adversaries could use the latest implementations of LLMs to recover historical data that was intended to be protected by differential privacy. To mitigate this vulnerability, we discussed potential solutions in Section 7.\nWe did not engage in any malicious activities, such as exposing sensitive information, disrupting legitimate services, or causing financial or reputational harm to the LLM vendors providing these services."}, {"title": "Compliance with the Open Science Policy", "content": "This paper adheres to the principles of open science by ensuring that all research data and complete source code used in the study will be publicly available upon the acceptance of the paper to foster further research."}, {"title": "4.3 White-box Fine-tuning-based Attacks", "content": "When given white-box access to an LLM, adversaries could do much more than using instructions to impact the behaviors of LLMs. Intuitively, adversaries fine-tune the target LLMs using structured pairs of original text and DP-sanitized text, then guide the LLMs to return the private text based on given DP ones. Line 5 to Line 14 in Algorithm 1 and Figure 2(b) provide the workflow of the white-box fine-tuning-based attack, where adversaries know a target DP-sanitized text x, an auxiliary dataset Xaux, the DP text sanitization approach P, the privacy budget \u03b5, and a pre-trained LLM with parameter \u03b8. The adversary samples the data point y from the auxiliary dataset, following the same distribution as the target training dataset, and constructs a new sequence z by concatenating the DP-sanitized text \u1ef9 = SANITIZE(y, P, \u03b5) using privacy budget \u03b5 and original text y as z = CONCATENATE(\u1ef9, y). Then, the adversary fine-tunes the model using the following objective:\n\\(\\mathcal{L}(\\theta) = - \\sum_i \\log \\Pr(z_i | z_1, z_2, ..., z_{i-1}; \\theta),\\)  (1)\nwhere zi is each token in z. After the fine-tuning process, the adversary inputs the sanitized prompts into the model and uses the output as reconstructed text corresponding to the sanitized text. The goal of fine-tuning is to make the model learn the pattern from the sanitized text to the original text. In practice, following previous works on fine-tuning LLMs for paraphrasing [60], we separate the sanitized text \u1ef9 and original text y when constructing z."}, {"title": "Word- and Sentence-level DP Approaches", "content": "Word-level DP approaches. The goal of the word-level DP is to perturb each word in a sentence under the metric DP. Formally, consider a sequence x = {x1,x2,...,xn} with n tokens, each token xi is convert to a m-dimensional word embedding \\(\\phi_i = \\phi(x_i)\\) by a pre-trained word embedding model \\(\\phi: V \\rightarrow \\mathbb{R}^m\\), where V represents the vocabulary. To obtain the noisy embedding \\(\\phi_i\\), a noise z sampled from a multivariate probability distribution \\(p_{\\varepsilon}(z)\\) is injected into \\(\\phi_i\\). Subsequently, the original word xi is replaced with a word \\(\\hat{x}_i\\) whose embedding \\(\\phi(\\hat{x}_i)\\) is closest to the \\(\\hat{\\phi}_i\\) within the embedding space. The previous framework of the word-level DP satisfies \\(e\\delta\\)-MetricDP, where \\(d: X \\times X \\rightarrow \\mathbb{R}^+\\) is a distance defined within sentence space (e.g., in Euclidean metric space, \\(d(x, x') = \\sum_{i=1}^n ||\\Phi(x_i) - \\Phi(x'_i)|||\\) for x and x' with the same length). The outline of the word-level DP is provided in Algorithm 2.\nSentence-level DP approaches. Compared to the word-level DP, the sentence-level DP aims to achieve DP at the full document rather than a single word. A common approach is to leverage LMs for paraphrasing tasks to achieve DP [41,57]. Given a private text x, the prompt for paraphrasing is constructed by prompt template pT and x. In an autoregressive language model, the output logit in the last layer of the decoder is denoted as u \u2208 R|V| when modeling the next token. To limit the sensitivity, the logit is clipped as \\(\\bar{u} = \\text{CLIP}(u, C)\\),"}, {"title": "Algorithm 3 Sentence-level DP (based on paraphrasing) [32,", "content": "Algorithm 3 Sentence-level DP (based on paraphrasing) [32,\n41].\nInput: private text x, temperature T, LM with parameters \u03b8, output length m, prompt template pT, clipping constant C, vocabulary V\nOutput: sanitized text x\n1: Construct prompt y \u2190 GENPROMPT(pT,x)\n2: x \u2190 []\n3: for i \u2190 1,2..., m do\n4: Compute logit u with LM using prompt y\n5: Clipping logit \\(\\bar{u} \\leftarrow \\text{CLIP}(u, C)\\)\n6: Obtain distribution \\(P \\leftarrow \\text{SOFTMAX}(\\bar{u}, T)\\)\n7: Sample the next token v from V using P\n8: x \u2190 x \u222a {v}, y \u2190 y \u222a {v}\nwhere the clipping function is \\(\\text{CLIP}(u, C) = u \\cdot \\min(1, C / ||u||)\\). The probability for each word in the vocabulary is computed as \\(P_j = \\text{SOFTMAX}(\\bar{u}) = \\exp(\\bar{u}_j/T) / \\sum_j \\exp(\\bar{u}_j/T)\\), where T represents the temperature for controlling randomness level of output. The next token is sampled from the conditional probability distribution P over the vocabulary V. Note that the sampling procedure can be viewed as the exponential mechanism [42], and the temperature is related to the privacy budget. Given the output length m, the clipping constant C, the paraphrasing procedure is 2mC\u03b5/T-LDP following the exponential mechanism. The outline of the sentence-level method is presented in Algorithm 3."}]}