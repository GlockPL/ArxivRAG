{"title": "Search Engines, Large Language Models or Both? Evaluating Information Seeking Strategies for Answering Health Questions", "authors": ["Marcos Fern\u00e1ndez-Pichel", "Juan C. Pichela", "David E. Losadaa"], "abstract": "Search engines have traditionally served as primary tools for information seeking. However, the new Large Language Models (LLMs) have recently demonstrated remarkable capabilities in multiple tasks and, specifically, their adoption as question answering systems is becoming increasingly prevalent. It is expected that LLM-based conversational systems and traditional web engines will continue to coexist in the future, supporting end users in various ways. But there is a need for more scientific research on the effectiveness of both types of systems in facilitating accurate information seeking. In this study, we focus on their merits in answering health questions. We conducted an extensive study comparing different web search engines, LLMs and retrieval-augmented (RAG) approaches. Our research reveals intriguing conclusions. For example, we observed that the quality of webpages potentially responding to a health question does not decline as we navigate further down the ranked lists. However, according to our evaluation, web engines are less accurate than LLMs in finding correct answers to health questions. On the other hand, LLMs are quite sensitive to the input prompts, and we also found out that RAG leads to highly effective information seeking methods.", "sections": [{"title": "1. Introduction", "content": "With the recent advancements in Natural Language Processing (NLP), Large Language Models have become major players in numerous Information Access tasks (Longpre et al., 2023; Bubeck et al., 2023; Touvron et al., 2023). The release of ChatGPT in November 2022 has been a game-changer globally, marking a significant milestone and revolutionising many sectors. One of the outstanding features of current conversational AIs stands on their ability to generate coherent and human-like text, which has garnered attention and excitement among practitioners, researchers and the general public. This breakthrough has precipitated a transformative shift in the orientation of information access research towards LLMs, their potential applications and the interconnection between LLMs and other computer-based tools.\nBut the emergence and global adoption of advanced LLMs has sparked the urgent need to explore and understand their capacities and knowledge acquisition attributes. Some research studies have focused on the capabilities of these models under specific language understanding and reasoning benchmarks (Jiang et al., 2020; Liang et al., 2022; Chang et al., 2024). However, with the rapid embrace of generative language models, another significant shift in information access has occurred. The conversational paradigm has gained traction, enabling more interactive and user-friendly search experiences (Mao et al., 2023; Friedman et al., 2023; Polak and Morgan, 2023; O'Leary, 2022); and many citizens currently turn to conversational AIs for consulting multiple types of information needs.\nHowever, the role of traditional web search engines (SEs) in answering user-submitted queries is far from being relegated. For example, SEs are still the main reference for numerous information seeking tasks. As stated in the annual Digital News Report conducted by the University of Oxford, \u201csearch engines continue to grow as a direct access to news websites\" (Reuters Insitute, University of Oxford, 2023). Thus, it is expected that both conversational language models and traditional search engines will continue to coexist in the foreseeable future. Furthermore, there is extensive research on how to improve SEs with LLM-based elements and also on how to enhance LLMs' generations with search-based results.\nIn line with these developments, it is essential to acknowledge the significance of accurate medical information. Fox and colleagues demonstrated that 80% of Internet users seek medical advice online (Fox, 2011). However, the presence of misinformation in search results can be severely harmful, especially when it concerns health-related content (Sharma et al., 2019). Medical hoaxes, miracle diets or advice given by unqualified sources are prevalent in online media, and the potential consequences of health misinformation can result in personal damage (Pogacar et al., 2017; Vigdor, 2020). We are not aware of specific statistics or research studies on the number of users using conversational AIs for health information seeking. Still, popular models such as ChatGPT have millions of users worldwide and, thus, it is crucial to put their answers under scrutiny. This is of paramount importance in a critical domain such as health.\nSumming up, there is a pressing need to evaluate the abilities of classic and new information access tools in answering medical questions and, as a matter of fact, there is a lack of comprehensive studies in the literature that compare the effectiveness of LLMs with that of traditional SEs in the context of health information seeking. Moreover, since the performance of LLMs is highly dependent on the input prompt (Jiang et al., 2020; Brown et al., 2020; Liu et al., 2023), understanding their effectiveness with different types of prompts is of utmost importance. It is also crucial to explore the effect of combining both classes of tools and, for example, explore the behaviour of LLMs when prompted with medical questions together with related search results. This paper aims to contribute towards filling these gaps by conducting a thorough study aimed to answer the following research questions:\n\u2022 RQ1. To what extent do search engines retrieve results that help to answer medical questions? Does the correctness of the information provided drop as we go down in the search engine result page?\n\u2022 RQ2. Are LLMs reliable in providing accurate medical answers? How do different models compare in terms of their effectiveness in providing responses to medical questions?\n\u2022 RQ3. Does the given context influence the capabilities of LLMs in providing the right answers? Do these models exhibit improvements when presented with a few in-context examples?\n\u2022 RQ4. Do LLMs improve their performance when fed with web retrieval results?\nOur comparison of SEs included experiments with Google, Bing, Yahoo! and DuckDuckGo. We found that Bing seems to be the most solid choice. Our evaluation also suggests that extracting the answers from the top ranked webpage often produces good results. This is good news, as web users are known to be reluctant to inspect many items from the search engine result pages (SERPs). Moreover, LLMs show an overall good performance but they are still very sensitive to the input prompt and, in some instances, they provide highly inaccurate responses. Finally, augmenting LLMs with retrieval results from SEs looks very promising and, according to our experiments, even the smallest LLMs can achieve state-of-the-art performance if fed with appropriate retrieval evidence. These findings contribute to a better understanding of health information seeking with the new conversational Als and with the traditional search engines."}, {"title": "2. Related work", "content": "This paper is related to several scientific areas. The advances in information credibility and correctness, particularly in the area of health information, are relevant to our research and are discussed in Section 2.1. Section 2.2 reviews the most significant literature in health information access with traditional search engines and with the new LLMs. Finally, in Section 2.3 we review the main trends in retrieval augmented LLMs."}, {"title": "2.1. Health Information Credibility and Correctness", "content": "Credibility represents a subjective perception of the extent to which information from a webpage or other source can be trusted (Fogg, 1999). Previous research has extensively studied how online information credibility is established (McKnight and Kacmar, 2007; Ginsca et al., 2015; Kakol et al., 2017; Bodaghi et al., 2023). For instance, Viviani and Pasi (2017) presented a thorough survey on existing credibility determination methods, showing that health misinformation poses a socially relevant problem. The work by Matthews et al. (2003) showed that a corpus about alternative cancer treatments contained 90% of documents with at least one false claim. On the other hand, Sondhi and his peers presented an automatic approach, using classic supervised learning technology, for medical reliability classification of webpages (Sondhi et al., 2012). Fern\u00e1ndez-Pichel et al. (2021b) successfully reproduced Sondhi's study and further applied it to new collections, thereby demonstrating its potential for generalisation. The same team also performed a comparison between traditional learning methods and neural-based approaches solutions for health-related misinformation detection (Fern\u00e1ndez-Pichel et al., 2021a), concluding that traditional learning approaches still constitute a robust baseline for some prediction tasks.\nUnder similar supervised learning settings, other studies focused their efforts on how different characteristics or features influence credibility. Zhao et al. (2021) proposed a novel health misinformation detection model using both \"central-level\" features (e.g., the topics discussed) and the so-called \u201cperipheral-level\u201d features (including linguistic and sentiment features, and user behavioural features). Their approach was validated on a real-world dataset, obtaining 85% of accuracy in health misinformation detection. This study showed that behavioural features are more discriminative than linguistic features in detecting untrustworthy contents. In (Griffiths et al., 2005), the authors demonstrated that network-based features alone, such as those based on PageRank, do not suffice to determine the reliability of online content.\nAnother line of work specifically focused on end-users and their perceptions of credibility. Seminal work by Fogg (2003) proposed the prominence-interpretation theory, which helps to determine which website cues influence the perception of credibility. Other studies showed that perceived credibility depends on reading skills (Hahnel et al., 2018). Similarly, Liao and Fu (2014) studied age differences in credibility judgements. On the other hand, Schwarz and Morris (2011) focused on how to augment a search engine result page to improve medical credibility judgements. In (Fern\u00e1ndez-Pichel et al., 2024), a user study with 1,000 participants was performed to analyse people's perceptions to the credibility of online health information. Among their main findings, these authors showed that individuals tend to overestimate the credibility of low quality sites. In the literature, related aspects, such as credibility, trustworthiness, and correctness have been explored. Our research particularly focuses on the correctness of responses for health questions provided by web search systems and conversational AIs.\nSome teams focused their efforts on designing solutions that estimate the correctness of medical information. For instance, Pradeep et al. (2021) presented Vera, a transfomer-based ranker that achieved state-of-the-art results in health misinformation detection tasks. This model was fine-tuned with assessments from the TREC 2019 Decision Track and it achieved the best results in the TREC 2022 Health Misinformation Track. This shared-data task fosters the development of systems that promote credible and correct documents over misinformation. However, systems like Vera take a health question and its correct response as an input and then search for harmful or helpful documents. These technological tools could support, for example, moderation services for online platforms. However, the need of pairs of questions and correct responses represents a limitation and, thus, the creators of Vera also conducted research on how to automatically infer the correct response for a health question (Pradeep and Lin, 2024). Specifically, the authors evaluated two different approaches: i) using LLMs under different settings (zero-, few-shot and chain-of-thought prompting), and ii) using the ranking produced by Vera and averaging the decisions extracted from the top 50 retrieved results. Their results suggest that the LLM-based approach (powered by GPT-4) outperformed the rest of strategies. Our comparison of SEs and LLMs is related to this study, but we compare multiple commercial search engines (while Vera is not a tool that is widely available to the public) and, furthermore, we do not only evaluate GPT models but consider six LLMs of different families. Our study, therefore, is reflective of the type of answers that the general public might get when interacting with popular information access tools. Additionally, we compare the answering capabilities of the LLMs with and without search results provided by the web search engines. This is an aspect that has not received enough attention in health information seeking. We also evaluate the effectiveness of the answers provided as we go down in the ranked lists of the SERPs."}, {"title": "2.2. Search Engines and Large Language Models in Health Information Access", "content": "Web search is widely used to obtain health advice and effective medical information retrieval has attracted the attention of the scientific community over the years (Fox, 2011). For instance, Baujard et al. (1998) conducted a seminal study focusing on the main trends in medical information retrieval and proposed an agent to perform effective medical information discovery. Similarly, Bin and Lun (2001) addressed the difficulty of finding relevant medical information in the web. These authors suggested that general-purpose search engines constitute a strong baseline for health information retrieval but also designed an agent-based system that was able to outperform SE baselines. Soldaini et al. (2016) studied the influence of injecting expert medical vocabulary to the query. Their results showed an increase around 7-12% in correct results for the modified queries. However, effective information retrieval (i.e., topicality) is not enough. As Pogacar et al. (2017) have shown, on-topic incorrect results can severely bias people's decisions. The need for new algorithmic solutions able to provide accurate results for health queries has motivated the development of shared-tasks that promote the identification of correct and credible information over misinformation, such as the TREC Health Misinformation (HM) Track (Clarke et al., 2020, 2021) or the CLEF eHealth initiative (Goeuriot et al., 2020; Kelly et al., 2019; Suominen et al., 2018). In this study, we are specifically interested in analysing how search results and LLMs' responses can help to answer binary health questions. Thus, we assess here the extent to which search results or LLM completions provide misleading responses with respect to the established medical consensus for each health topic.\nWith the impressive recent development of LLMs, interest in assessing the correctness of the health-related AI-based completions has escalated. For example, Chervenak et al. (2023) demonstrated ChatGPT's abilities to answer fertility questions. Duong and Solomon (2023) compared the performance of Large Language Models against humans for answering multiple-choice questions of human genetics. Similarly, Holmes et al. (2023) conducted a comparative study of LLMs knowledge on a highly specialised topic, radiation oncology physics. They concluded that OpenAI's models outperformed all others. In (Jahan et al., 2023; Hamidi and Roberts, 2023; Samaan et al., 2023), the authors conducted studies on the role of LLMs for biomedical tasks, patient-specific EHR questions, and bariatric surgery topics, respectively. With a broader perspective, Johnson et al. (2023) involved physicians in a thorough evaluation of ChatGPT's accuracy in answering medical queries, and other researchers (Thirunavukarasu et al., 2023) evaluated ChatGPT's ability with the Applied Knowledge Test (AKT) of the Membership of the Royal College of General Practitioners, demonstrating performance close to human experts. All of the aforementioned studies are restricted to a single model, usually ChatGPT, and/or to a specific medical area. Fern\u00e1ndez-Pichel et al. (2024) evaluated several LLMs of different nature with general health questions. This team examined the influence that different prompts and in-context examples have on the effectiveness of the LLMs' output. In our study, we go one step further and present a thorough comparison between LLMs and traditional web search engines and, additionally, test the combination of LLMs and SEs through retrieval-augmented generation (RAG) strategies."}, {"title": "2.3. Retrieval-Augmented Generation (RAG)", "content": "Several previous studies focused on how to exploit retrieval evidence to enhance the generative responses supplied by Als (Li et al., 2022). Asai et al. (2023) provided a detailed overview of the advances in retrieval augmented LMs and explored some of their applications. Guu et al. (2020) proposed REALM, an augmented language model with a module that retrieves evidence from a large textual corpus such as Wikipedia. In (Borgeaud et al., 2022), the authors proposed RETRO, an architecture consisting of a language model which benefits from results obtained from a large database of trillions of tokens.\nLazaridou et al. (2022) conducted an evaluation on different few shot prompting strategies. They compared a method that provided examples of questions and correct answers (closed book strategy) against a method that fed the model with supporting evidence for the answer (open book strategy). The injected evidence came from an offline labelled collection or from relevant passages obtained from Google's search API. Their experiments showed that both types of open book strategies improved the closed book method. Izacard and Grave (2021) augmented a T5 model for an open domain question answering task. For retrieval of relevant passages, these authors used an offline collection and BM25 or DPR as search methods. The main conclusion was that augmentation is beneficial and that improvements are noticeable up to a maximum of 100 recovered passages. Asai et al. (2021) conducted a similar study, but they argued that injecting off-topic evidence might be counterproductive. Thus, their solution included an evidentiality estimation layer that predicts whether the passage provides actual evidence to answer the question.\nOther studies focused on helping language models to interact with search engines. For instance, Nakano et al. (2021) developed a text-based web-browsing environment that can be exploited by a GPT-3 fine-tuned model. Thoppilan et al. (2022) showed that using external APIs (such as those supported by information retrieval systems) significantly improves groundedness, which is defined as the extent to which a generated response contains claims that can be validated against a known source. Shuster et al. (2022) proposed an architecture that first searches for evidence against a search engine, then selects the most relevant sentences from the retrieved documents, and finally, generates a response.\nIn the health domain, Li et al. (2023) fine-tuned a Llama model with medical conversations and also injected medical evidence extracted from Wikipedia and other medical sources. We contribute to this recent line of work by assessing the effect of including search engine's evidence for the generation of correct health answers. In Section 6.2.4, we detail these experiments, oriented to prompt several LLMs with evidence retrieved from Google's top results for a given health question."}, {"title": "3. Binary Question Answering with Search Engines and Large Language Models", "content": "In this research, we focus on estimating the ability of web search engines and conversational AIs to provide correct health responses. To that end, we selected a binary health question answering (QA) task as our reference for evaluation. This type of information needs are prevalent, as many users usually search for specific advice, e.g. \u201cCan X (treatment) cure Y (disease)?\u201d. This binary setting facilitates the quantitative assessment of the systems' responses. Automatic systems must provide the correct answer to these health questions and, as shown below, the target or reference response comes from the established medical consensus for each search topic.\nFor evaluating the responses provided by the SE, each health question was submitted to the engine. Next, we automatically extracted from each retrieved webpage the most relevant excerpt that answers the question, more details are provided in Section 5. After the passages were chosen, it was necessary to ascertain whether they provided an affirmative, negative, or non-responsive answer to the health question. To that end, we exploited GPT-3 model's capabilities to perform reading comprehension. Brown et al. (2020) evaluated the ability of this model on different reading comprehension settings. The model achieved remarkable results, for example, F1 of 85% in the CoQA dataset. Dijkstra et al. (2022) also demonstrated this model's ability to perform reading comprehension tasks in the educational domain. In our case, we defined a prompt that included the passage and health question, and specifically asked the model to answer the question based on the provided passage (without resorting to its internal knowledge)\u00b9. This estimation process arguably produces the sequence of answers that a search engine user would obtain from inspecting the SE results. Figure 1 illustrates the whole process, while specific experimental details are reported in Section 5.\nOur interest here is not only to analyse the overall effectiveness of web search systems, but also to study the quality of the responses as we go deeper into the ranked lists. To that end, we propose two models of user behaviour that simulate alternative forms of inspection of the retrieved results. The lazy user model represents a user who stops inspecting results when presented with the first entry that gives a yes/no response to the user's question\u00b2. This user, therefore, sticks to the first answer found and does not spend time searching for contrasting evidence. The second model, referred to as diligent user model, represents a user who traverses the ranking from the top position and stops after finding three responses. We assume that this user takes a decision about his health question based on majority voting from the three provided responses. Web users are known to be reluctant to explore many search results and these two models represent two rational forms of exploration in the quest of the response for the health question. We leave the study of additional user models, including click models for web search (Chuklin et al., 2015), for future work. In Appendix A, we provide the pseudo-code for both user models.\nFor assessing conversational Als, we submitted the health question to several generative models and obtained their completions. We worked under different settings, including zero- and few-shot strategies (providing examples of correct responses to other health questions), and different types of prompts. We forced the LLMs to provide a 'yes' or 'no' output. Specific details about the models and the configuration of the experiments can be found in Section 5\nIn this study, we also performed an \u201conline\u201d retrieval augmented generation (RAG) experiment (Asai et al., 2023). There are several approaches for augmenting generative language models with retrieved evidence. Following (Guu et al., 2020), we injected textual chunks in the input layer of the LLM. In essence, we prompted different large language models with passages extracted from the top entries of the SERPs. In this way, our experimental setup allows the"}, {"title": "4. Health Questions", "content": "To obtain a solid set of health-related search topics we leveraged the data created under the TREC Health Misinformation (HM) Track. This is a three-year shared-task that aimed to foster the development of systems capable of detecting false health information, thereby empowering individuals to make health-related choices grounded on reliable and factual information (Clarke et al., 2020, 2021). These datasets contain health-related queries posed as questions (for instance, \u201cDoes wearing masks prevent COVID-19?\u201d) and their correct responses (yes/no). Each topic represents a searcher who is looking for information that is useful for making a \u201cyes\u201d or \u201cno\u201d decision regarding a health-related information need. The binary ground truth field represents the best understanding of current medical practice (gathered by the task organisers when building the collection). Figure 2 shows an example of a topic (the question is stored into the description field while the binary response is stored into the stance field\u00b3).\nThe TREC HM 2020 dataset was restricted to questions related to COVID-19, while topics from TREC HM 2021 and 2022 collections covered a wide range of health topics. The 2020 questions were disclosed in the middle of 2020, which raises the possibility that they could have been included in the pre-training phase of some Large Language Models. Such inclusion could potentially give them an advantage over generative models that were trained earlier. The 2021 dataset was made available in mid-July 2021 and, thus, it could have been used for training newer models such as ChatGPT and GPT-4, but not for GPT-3 (its training ended earlier). The 2022 dataset, on the other hand, was released after all the LLMs had been created. This configuration of health topics therefore shapes an assorted evaluation with questions created and released at different dates. In our analysis we pay special attention to the topic set creation dates, the knowledge cutoff dates of each LLM, and make additional experiments to estimate memorisation (see Section 6.2.5). In the case of search engines, their constant crawling and indexing activities allow them to have access to continuously updated information and, of course, this distinctive feature of SEs needs to be taken into account when analysing results.\nSumming up, the selection of health topics integrates a diverse set of binary health-related questions, which pose different levels of difficulty to the models based on their exposure to such data and the level of specificity of the information needs."}, {"title": "5. Experimental Setup", "content": "We evaluated four well-known search engines: Google, Bing, Yahoo, and Duckduckgo. We used a scraping tool\u2074 on the organic search results to collect the top retrieved webpages. We gathered the top 20 ranked entries since users rarely go beyond the second page of results and, thus, the extraction focuses on webpages that have some chance of being inspected by a standard user. Then, we obtained the raw content of the webpage, converted it to workable plain text, and split it into passages. To obtain the most relevant passage, the health question and each passage were embedded into a vectorial contextual representation using MonoT5 (Pradeep et al., 2023), which is a highly effective model fine-tuned for passage retrieval. This constitutes a well-established approach for the automatic extraction of excerpts answering user queries (Nogueira and Cho, 2019). Rosa et al. (2022) demonstrated the effectiveness of this model for several retrieval tasks.\nIn some cases, the retrieval results do not provide an answer to the question. In our accounting of correct answers across ranking positions these cases are recorded as failures, as they do not respond with the correct answer. It is important, though, to bear in mind that there is an important distinction between a non-answer and an incorrect answer. In the future, we will further delve into this issue but, in the current study, we focus on analysing the relative trends of correct responses. Furthermore, the user models discussed above assume that users skip non-answers and, thus, our user model evaluation constitutes a natural complement to the report on the proportion of correct answers across ranking positions.\nWe also evaluated six LLMs of different nature and architectures (closed and open source). Fine-tuned models specifically trained for health or clinical topics, such as ChatDoctor (Yunxiang et al., 2023) or BioBERT (Lee et al., 2020), were not included in our comparative study because we focus here on systems that are widely available to non-specialised users. We are specifically interested in evaluating LLMs that are currently used by millions of individuals worldwide. The models included in our comparative study are:\n\u2022 GPT-3, also known as text-davinci-002 (d-002). It is a model with a decoder-only structure with 175 billion parameters. Its training corpus is extensive, including diverse sources and the entirety of Wikipedia, with information current up to June 2021.\n\u2022 text-davinci-003 (d-003), the subsequent iteration of GPT models, built upon its predecessor by incorporating InstructGPT methodologies (Ouyang et al., 2022). This model has been refined through reinforcement learning with human feedback (RLHF) and has the same training data timeline as d-002.\n\u2022 ChatGPT represents an evolution of OpenAI models towards a more dialogue-oriented and user-friendly behaviour. Its knowledge cutoff goes up to September 2021. For our experiments we used gpt-3.5-turbo version (its snapshot from June 2023).\n\u2022 GPT-4, another conversational agent by OpenAI that represents a significant leap forward, outperforming ChatGPT in various complex tasks that require human-like reasoning, such as passing academic examinations (OpenAI, 2023). Its training data is up-to-date as of September 2021.\n\u2022 Flan T5 (FT5), a sequence-to-sequence model from Google. It underwent fine-tuning with a diverse array of instruction-based datasets, as described in (Longpre et al., 2023). The data for this model was sourced from the \"Flan 2022\" open-source repository, which includes a wide range of contents collected up until 2022. For our experimentation, we used the flan-t5-xl version.\n\u2022 Llama2, a language model developed by Meta AI. It was trained with more than 1 million human annotations of conversational data. Its training data goes up to September 2022, but its fine-tuning also includes data up to July 2023. For these experiments, we used the llama-13b-chat version.\nThe first four models were tested through OpenAI's official Python API5, while Flan T56 and Llama7 were tested through their Hugging Face implementations. We set the models' temperature to 0, with the intention of minimising the randomness or creativity of the completions.\nA pivotal part of this research consists of determining the effectiveness of these models for answering health questions under different input conditions or contexts. Having in mind that online users are known to be reluctant to have complex interactions with automated systems, we first tested the LLMs' effectiveness when responding to non-expert individuals who send the question and give little or no context at all:\n\u2022 no-context prompt: a context formed only with the medical question, i.e. \u201cCan Vitamin D cure COVID-19?\u201d.\n\u2022 non-expert prompt: The text \u201cI am a non-expert user searching for medical advice online\" is added before the health question. This prompt intends to be representative of a regular end user searching for medical advice.\nAs a next step, we also tested more elaborated prompts and, additionally, assessed the influence of including in-context examples. It is unlikely that a regular user opts for these complex strategies but, still, they can help to further understand and exploit the models' internal knowledge:\n\u2022 expert prompt: The text \u201cWe are a committee of leading scientific experts and medical doctors reviewing the latest and highest quality of research from PubMED. For each question, we have chosen an answer, either 'yes' or \u2018no', based on our best understanding of current medical practice and literature.\u201d followed the corresponding medical question. This contextual instructions were produced by Waterloo's team in their participation in the TREC 2022 HM track (Pradeep and Lin, 2024). The core idea is to guide the LLM towards reputed sources.\nTo automatically record responses, we forced the models to answer only with \u201cyes\u201d or \u201cno\u201d tokens. In contrast to the scenario with search engines, there can therefore be no unanswered questions here. More complex prompt engineering techniques, like Chain-of-Thought (CoT) (Wei et al., 2022), are left for future work.\nFor the RAG experiments, we fed some LLMs with relevant passages obtained from the search results produced by Google. We injected passages from Google's top 5 results and we clearly instructed the LLM to compare the provided evidence with its internal knowledge prior to providing a definitive answer. The extraction of relevant passages from Google's top 5 results was also supported by MonoT5."}, {"title": "6. Results", "content": "In Section 6.1 we report the effectiveness of different search engines in providing correct responses and we also analyse user experience following the two user models explained in Section 3. In Section 6.2 we evaluate the performance of LLMs under zero- and few-shot settings. Here, we also propose a taxonomy of errors made by LLMs, make an error analysis, present results for retrieval-augmented strategies and, finally, estimate the effects of memorisation or data leakage during LLMs' training process."}, {"title": "6.1. Search Engines", "content": "Using the methodology explained in Section 3, we evaluated the effectiveness of the different search engines in providing the correct answer to the health questions. From the results, we observe no significant drop in performance as we move down the rankings, which speaks well of the SEs' retrieval capabilities. Regarding different engines, Bing seems a solid choice across all datasets. The 2022 topics produced the best results. A possible explanation for this outcome is that the 2022 collection contains less specialised health questions, and it might be easier to retrieve correct answers from the web.\nWe also wanted to analyse the extent to which retrieval results provide an actual answer to the health questions. Ranked lists contain off-topic results and, additionally, many on-topic pages do not provide a clear response to the user's request. For each of the 50 health questions available in each collection, we processed its top retrieved results to identify whether they answered the health question10. For example, for Google's rankings and top 1 position we computed the number of questions that had an actual response. The same count was obtained for each position (up to rank #20) and, finally, a global SE answering score was obtained by averaging the number of questions answered at each position. These scores, plotted in Figure 4, are much lower than 50, reflecting that many retrieval results do not"}, {"title": "6.1.1. User Behaviour Models", "content": "Let us now evaluate the search results using the two user behaviour models (lazy and diligent) described in Section 3. In Figure 5, we report the percentage of correct responses, incorrect responses and no answers for the three collections 11. We also report in Table 1 the effort required to make a decision, measured as the mean number of results a user needs to inspect before reaching the stopping criterion.\nObserve that the lazy behaviour produces better results with less effort compared with the diligent behaviour. Indeed, the additional effort spent by the diligent user does not translate into a lower percentage of incorrect responses. This might sound surprising, but it somehow reinforces the confidence in the search engine top result (the top answer suffices). In fact, our results suggest that the diligent user, who goes deeper in the ranking to find additional responses, would make poorer health-related decisions."}, {"title": "6.2. Large Language Models", "content": "Next, we evaluate the performance of LLMs for our binary question answering task under zero- and few-shot settings. We also propose a taxonomy for the errors that these models tend to produce and perform an error analysis based on this taxonomy. Finally, the results for retrieval-augmented strategies are presented, along with an estimation of the effects of memorisation during the LLMs' training process."}, {"title": "6.2.1. Zero-shot evaluation", "content": "Figure 6 plots the proportion of correct answers for the three prompting strategies (\u201cno context\u201d, \u201cnon-expert\" and \"expert\u201d). Llama2 and text-davinci-003 emerge as the top performers for the TREC HM 2020 and 2021 datasets. When it comes to the TREC HM 2022 data, GPT-4 and ChatGPT stand out. Additionally, there are variances in performance linked to the use of different prompts. In general, the \u201cexpert\u201d context is the most effective. We believe this effectiveness is due to the incorporation of key phrases like \u201cresearch from PubMed\u201d or \u201cmedical practice and literature\u201d, which guide the model towards more credible knowledge sources.\nAlthough the models generally show relative stability, their performance still varies based on the inputs provided. For example, FlanT5 and text-davinci-002 are highly sensitive to the type of prompt. This raises concerns, as a model's proportion of correct answers can drop from 90% to 75% or even lower numbers. While the overall performance levels are high, these inconsistencies are troubling. Even when using the most reliable prompt (\"expert"}]}