{"title": "SFPrompt: Communication-Efficient Split Federated Fine-Tuning for Large\nPre-Trained Models over Resource-Limited Devices", "authors": ["Linxiao Cao", "Yifei Zhu", "Wei Gong"], "abstract": "Large pre-trained models have exhibited remark-able achievements across various domains. Thesubstantial training costs associated with thesemodels have led to wide studies of fine-tuning foreffectively harnessing their capabilities in solvingdownstream tasks. Yet, conventional fine-tuningapproaches become infeasible when the modellacks access to downstream data due to privacy con-cerns. Naively integrating fine-tuning approacheswith the emerging federated learning frameworksincurs substantial communication overhead and ex-erts high demand on local computing resources,making it impractical for common resource-limiteddevices. In this paper, we introduce SFPrompt, aninnovative privacy-preserving fine-tuning methodtailored for the federated setting where direct up-loading of raw data is prohibited and local devicesare resource-constrained to run a complete pre-trained model. In essence, SFPrompt judiciouslycombines split learning with federated learning tohandle these challenges. Specifically, the pre-trained model is first partitioned into client andserver components, thereby streamlining the client-side model and substantially alleviating computa-tional demands on local resources. SFPrompt thenintroduces soft prompts into the federated modelto enhance the fine-tuning performance. To furtherreduce communication costs, a novel dataset prun-ing algorithm and a local-loss update strategy aredevised during the fine-tuning process. Extensiveexperiments demonstrate that SFPrompt deliverscompetitive performance as the federated full fine-tuning approach while consuming a mere 0.46% oflocal computing resources and incurring 53% lesscommunication cost.", "sections": [{"title": "Introduction", "content": "Large pre-trained models have achieved unprecedented suc-cess across various domains, including natural language pro-cessing (NLP) and computer vision (CV) [Brown et al., 2020;Wang et al., 2023]. To support more diverse and complextasks, the size of the pre-trained models has increased sub-stantially, e.g., the model sizes of the GPT series have in-creased from 117M to 175B [Desislavov et al., 2021]. Nev-ertheless, as the model size increases, so too does the train-ing cost (e.g., training GPT-3 would cost over $4.6M [Dale,2021]). To lower training costs and enable wide adoption, aprevailing paradigm is to fine-tune the pre-trained models toadapt downstream tasks. However, existing fine-tuning meth-ods often necessitate access to downstream task data, whichis usually not possible in practice. With the enactment ofregulations like the EU's General Data Protection Regulation(GDPR) [Voigt and Von dem Bussche, 2017], along with in-creasing attention to privacy and security, obtaining down-stream data becomes more and more challenging. How toconduct fine-tuning without accessing raw downstream datais a fundamental problem that needs to be solved.\nTo work in these privacy-preserving environments, inte-grating fine-tuning with the emerging federated learning (FL)[Zhao et al., 2023; Xiao et al., 2023; Chen et al., 2022] hasgained traction. However, existing efforts still have the fol-lowing limitations:\nOverwhelming Communication Cost. As the model sizeincreases, collaboratively fine-tuning a pre-trained model in-troduces significant communication costs due to model ex-change. For instance, GPT-3 [Brown et al., 2020], encom-passing a staggering 175B parameters that total approxi-"}, {"title": "Background and Related Work", "content": "To adapt large pre-trained models to specific downstreamtasks efficiently, researchers have proposed a variety of"}, {"title": "Federated Fine-tuning Pre-trained Models", "content": "Federated Fine-tuning Pre-trained Models\nparameter-efficient fine-tuning methods [Houlsby et al.,2019; Hu et al., 2021; Jia et al., 2022]. However, thesemethods all assume complete access to the raw data, which isnot always possible with the growing awareness of data pri-vacy and the enforcement of data regulations. Consequently,recent works applied the FL framework to fine-tuning pre-trained models in privacy-preserving environments. The ex-isting methods can be broadly categorized into two mainstreams. The first involves a direct combination of FL andfine-tuning methods [Zhao et al., 2023; Guo et al., 2023;Zhang et al., 2022]. For example, Zhao et al. [Zhaoet al., 2023] freezes the pre-trained model and aggregatesprompts to fine-tune the pre-trained models. The second ap-proach focuses on combining parameter-efficient fine-tuningand model emulator derived from a lossy compressed versionof a large pre-trained model. Xiao et al. [Xiao et al., 2023;Niu et al., 2022] fine-tunes the adapter on the downstreamdata with the emulator's assistance. Although these earlystudies confirm the potential of FL in fine-tuning pre-trainedmodels, they still demand high computational local devicesand incur significant communication costs."}, {"title": "Split Federated Learning", "content": "Split Federated Learning\nSplit federated learning (SFL) [Thapa et al., 2022] is a novelframework that combines Split Learning (SL) [Gupta andRaskar, 2018; Vepakomma et al., 2018] and FL [McMahanet al., 2017; Kone\u010dn\u1ef3 et al., 2016; Kairouz et al., 2021].SFL reduces local computing costs by using SL to split themodel into two modules: the client model and the servermodel. The client model has lower computational complex-ity, minimizing computational overhead at clients. The servermodel with heavy complex computations is shouldered by theserver. SFL then achieves faster collaboratively training byperforming parallel processing across clients using FL. Intu-itively, SFL can be a good framework to address the trainingchallenges in distributed scenarios with constrained local re-sources.\nNevertheless, naively applying SFL to fine-tune large pre-trained models still introduces significant communicationburdens because of the frequent transmission of forward andbackward signals. To further validate the point, we use a pre-trained vision transformer [Dosovitskiy et al., 2020] as an ex-ample. Suppose there is a client with 1,000 locally availableimages. During each global round, which represents the cy-cle from the current parameter aggregation to the next, theclient performs 10 local epochs, representing the number of"}, {"title": "Methodology", "content": "Methodology\nWe propose SFPrompt for efficiently adapting large pre-trained models to distributed downstream tasks with privatedata. SFPrompt employs a methodical three-phase strategy:client self-update, split training, and parameters aggregation,executed sequentially to ensure optimal performance. We il-lustrate the overall framework of SFPrompt in Figure 3.\nSFPrompt partitions the model, denoted as \\(W\\), into threecomponents to fully leverage the local computational re-source. The head is the first few layers represented as \\(W_h\\) forfeature extracting, the body is represented as \\(W_b\\) for modelingthe dependency between features, and the tail is the classifierrepresented as \\(W_t\\) for mapping the feature to the task-specificoutput. In practical terms, the splitting strategy in SFPromptis dynamic, not rigid. It is determined by the client's hard-ware resources, ensuring optimal performance without over-whelming the client's hardware.\nThe server distributes the client-side model, formed by thehead and tail models and denoted as \\(W_c = [W_h, W_t]\\), to theselected clients \\(K\\), where \\(k \\in K\\) denotes the \\(k\\)-th client par-ticipating in the training process. Conversely, the body \\(W_b\\) ishoused on the server, forming \\(W_s\\). \\(W_c\\) exhibits a lower com-putational complexity and model size compared to the foun-dation model \\(W\\), making it more suitable for implementationat clients, particularly in resource-limited scenarios."}, {"title": "Phase 1: Client Self-Update", "content": "Phase 1: Client Self-Update\nUpon careful observation in the related work, which is shownin Fig 2, it becomes apparent that merely employing SFL,coupled with the existing fine-tuning methods, engenders newchallenges. To tackle this issue, we approach the problemfrom two angles in SFPrompt.\nLocal-loss Update. To reduce the number of interactions be-tween the server and the client, SFPrompt introduces thelocal-loss to achieve client self updating. We establish a con-nection between the final layer of \\(W_{h,k}\\) and the local tail\\(W_{t,k}\\), such that the output of \\(W_{h,k}\\) becomes the input of \\(W_{t,k}\\).We define the local loss function as \\(L_c\\):\n\\(L_c = \\frac{1}{|D_k|} \\sum_{x \\in D_k} l(x; (W_{h,k}, W_{t,k}); p)\n(1)\nwhere \\(l(x; (W_h, W_t); p)\\) is the loss computed based on theinput \\(x\\), the head model \\(W_{h,k}\\), the local tail model \\(W_{t,k}\\) and"}, {"title": "Phase 2: Split Training", "content": "Phase 2: Split Training\nAs training progresses, clients completing phase 1 introduce aset of p continuous, learnable parameters, known as prompts,into the input space. The input space contains the embed-dings of significant training samples following data pruning.Subsequently, the clients perform forward propagation usingthe model \\(W_h\\) to generate 'smashed' data, an intermediateoutput at the cut layer. The client then sends this smasheddata to the server. On the sever-side, model \\(W_b\\) performs for-ward propagation and transmits smashed data, the output of\\(W_b\\), back to the client. Upon receiving the data, the clientconducts both forward and backward propagation to updatethe tail model \\(W_t\\) and compute the gradient. The computedgradient is transmitted back to the server. After the servercompletes backward propagation, it transmits the gradient tothe client, enabling the client to update the prompt p based onthe received gradient.\nThroughout the entire process, the prompt p and tail \\(W_t\\) aretuned, while the backbone \\(W_b\\) and head \\(W_h\\) remain frozen.This signifies that only a small subset of all parameters is"}, {"title": "Phase 3: Parameters Aggregation", "content": "Phase 3: Parameters Aggregation\nAfter completing the previous two phases, the parameters ofthe tail model \\(W_t\\) and pare updated. During this phase,clients transmit their updated local tail model and prompt pa-rameters to the server, fostering a collaborative training pro-cess. The server subsequently conducts a global aggregation,then distributes the aggregated tail model and prompts the se-lected clients for the next round of training. This process ismathematically represented as follows:\n\n(W_{t,r+1},P_{r+1}) = \\frac{1}{K} \\sum_{k \\in K} (W_{t,k,r}, P_{k,r})\n(3)\nwhere r\u2208 R is the number of global rounds, the final globalmodel \\(W_R\\) is derived by integrating the models \\(W_h\\), \\(W_b\\),"}, {"title": "Analysis of SFPrompt", "content": "Analysis of SFPrompt\nWe delve into an in-depth analysis of SFPrompt. For eas-ier understanding, we introduce the network split fraction,denoted as \u03b1 and \u03c4. Then, we define that \\(|W_h| = \u03b1|W|\\),\\(\\|W_b = \u03c4|W|\\) and \\(|W_t| = (1 \u2212 \u03b1 \u2212 \u03c4)|W|\\), where \\(|W|\\) sig-nifies the total number of parameters in model W. We set thesize of prompt parameters to p, the dataset pruning fractionto \u03b3, and assume that the cut layer's size is q. Further, we as-sume the computational power of the client and server as \\(P_c\\)and \\(P_s\\) respectively, with the condition that \\(P_c < P_s\\). Thetime needed to update model W on the dataset D is expressedas \\(|D||W|\\), where forward propagation requires time \\(\\beta|D||W|\\),and backward propagation demands time \\((1 \u2013 \u03b2) |D||W|\\). We"}, {"title": "Experiments", "content": "Experiments"}, {"title": "Experimental Setup", "content": "Experimental Setup\nPre-trained model and Downstream tasks. In our experi-ments, we focus on vision fine-tuning tasks, employing ViT[Dosovitskiy et al., 2020], which is pre-trained on ImageNet-21k [Deng et al., 2009]. Specifically, we evaluate the perfor-mance of these models on four image classification datasets,namely, CIFAR-10 and CIFAR-100 [Krizhevsky et al., 2009],SVHN [Netzer et al., 2011], and Flower-102 [Nilsback andZisserman, 2008].\nBaselines. In our evaluation, we conduct a comprehensivecomparison of SFPrompt with other commonly used fine-tuning methods to demonstrate its efficiency.\n\u2022 FL: This includes all traditional FL methods, such asFedSGD [McMahan et al., 2017], which directly ex-change the model to conduct the fine-tuning tasks.\n\u2022 SFL [Thapa et al., 2022]: SFL splits the model and com-bines it with FL to train the model parallelly.\n\u2022 SFL+FF: This method combines SFL and full fine tun-ing (FF) that tunes all the model parameters.\n\u2022 SFL+Linear: The method integrates SFL and Linear,which only fine-tune the linear layer while keeping therest of the parameters frozen.\nDistributed Scenario Setting. We construct a distributedlearning scenario characterized by a central server endowedwith substantial computational resources, coupled with 50 in-terconnected clients, each of which has limited computationalcapacity. At each round of the training process, only 5 clientsare randomly selected to participate in the training and per-form 10 local epochs in one global round. Data distributionacross different clients falls into two types: IID (Independentand Identically Distributed) and Non-IID. The Non-IID datais split using a Dirichlet distribution [Hsu et al., 2019] param-eterized by a = 0.1."}, {"title": "Evaluation of SFPrompt", "content": "Evaluation of SFPrompt\nAccuracy. We present the evaluation of SFPrompt's perfor-mance in various fine-tuning tasks across the chosen datasets.The comparison results among FF and linear are demon-strated in Fig 4. On the CIFAR-10 dataset, SFPromptachieves performance comparable to baseline methods. Theadvancement of SFPrompt becomes more pronounced on themore complex CIFAR-100 dataset, where SFPrompt excelledentirely over the other two methods. Particularly in non-iidsettings, the advantage of SFPrompt becomes even greater.In this context, SFPrompt achieved a substantial 10.61%"}, {"title": "Ablation Study", "content": "Ablation Study\nLocal-loss Update. To further understand the effectivenessand robustness of SFPrompt, we conduct an ablation study,exploring the influence of different components and configu-rations on the model's performance. Firstly, we compare SF-Prompt with SFPrompt w/o local-loss update to explore theinfluence of local-loss update, as shown in Figure 6. The re-sults indicate that the local-loss update step is instrumental incontributing to SFPrompt's performance.\nDataset Pruning. We evaluate the effect of different localdataset pruning fractions on the performance of the model,as depicted in Figure 7. Interestingly, even with deep prun-ing of the local dataset under the IID condition, the impacton model performance was minimal. When only 20% of thelargest EL2N values in the local dataset were retained, theperformance was only reduced by 3.39% compared to the fulldataset. Under non-IID conditions, even after pruning 80%of the data, the performance decline is quite limited, amount-ing to only 4.32%. This occurs because, although SFPromptdoesn't utilize the complete dataset during global training, it"}, {"title": "Conclusions", "content": "Conclusions\nIn this paper, SFPrompt is introduced as a privacy-preservingand efficient distributed fine-tuning framework. SFPromptsplits the large pre-trained model into server-side and client-side to lower the computational burden on the client andfurther introduce prompt parameters to enable efficient fine-tuning. SFPrompt prunes the redundant local data and up-dates the model based on the local loss to further reduce theadditional communication cost. Our extensive experimentsreveal that SFPrompt achieves state-of-the-art performance,outperforming existing baselines."}]}