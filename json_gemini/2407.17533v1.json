{"title": "SFPrompt: Communication-Efficient Split Federated Fine-Tuning for Large Pre-Trained Models over Resource-Limited Devices", "authors": ["Linxiao Cao", "Yifei Zhu", "Wei Gong"], "abstract": "Large pre-trained models have exhibited remarkable achievements across various domains. The substantial training costs associated with these models have led to wide studies of fine-tuning for effectively harnessing their capabilities in solving downstream tasks. Yet, conventional fine-tuning approaches become infeasible when the model lacks access to downstream data due to privacy concerns. Naively integrating fine-tuning approaches with the emerging federated learning frameworks incurs substantial communication overhead and exerts high demand on local computing resources, making it impractical for common resource-limited devices. In this paper, we introduce SFPrompt, an innovative privacy-preserving fine-tuning method tailored for the federated setting where direct uploading of raw data is prohibited and local devices are resource-constrained to run a complete pre-trained model. In essence, SFPrompt judiciously combines split learning with federated learning to handle these challenges. Specifically, the pre-trained model is first partitioned into client and server components, thereby streamlining the client-side model and substantially alleviating computational demands on local resources. SFPrompt then introduces soft prompts into the federated model to enhance the fine-tuning performance. To further reduce communication costs, a novel dataset pruning algorithm and a local-loss update strategy are devised during the fine-tuning process. Extensive experiments demonstrate that SFPrompt delivers competitive performance as the federated full fine-tuning approach while consuming a mere 0.46% of local computing resources and incurring 53% less communication cost.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained models have achieved unprecedented success across various domains, including natural language processing (NLP) and computer vision (CV) [Brown et al., 2020; Wang et al., 2023]. To support more diverse and complex tasks, the size of the pre-trained models has increased substantially, e.g., the model sizes of the GPT series have increased from 117M to 175B [Desislavov et al., 2021]. Nevertheless, as the model size increases, so too does the training cost (e.g., training GPT-3 would cost over $4.6M [Dale, 2021]). To lower training costs and enable wide adoption, a prevailing paradigm is to fine-tune the pre-trained models to adapt downstream tasks. However, existing fine-tuning methods often necessitate access to downstream task data, which is usually not possible in practice. With the enactment of regulations like the EU's General Data Protection Regulation (GDPR) [Voigt and Von dem Bussche, 2017], along with increasing attention to privacy and security, obtaining downstream data becomes more and more challenging. How to conduct fine-tuning without accessing raw downstream data is a fundamental problem that needs to be solved.\nTo work in these privacy-preserving environments, integrating fine-tuning with the emerging federated learning (FL) [Zhao et al., 2023; Xiao et al., 2023; Chen et al., 2022] has gained traction. However, existing efforts still have the following limitations:\nOverwhelming Communication Cost. As the model size increases, collaboratively fine-tuning a pre-trained model introduces significant communication costs due to model exchange. For instance, GPT-3 [Brown et al., 2020], encompassing a staggering 175B parameters that total approxi-"}, {"title": "2 Background and Related Work", "content": "To adapt large pre-trained models to specific downstream tasks efficiently, researchers have proposed a variety of parameter-efficient fine-tuning methods [Houlsby et al., 2019; Hu et al., 2021; Jia et al., 2022]. However, these methods all assume complete access to the raw data, which is not always possible with the growing awareness of data privacy and the enforcement of data regulations. Consequently, recent works applied the FL framework to fine-tuning pre-trained models in privacy-preserving environments. The existing methods can be broadly categorized into two main streams. The first involves a direct combination of FL and fine-tuning methods [Zhao et al., 2023; Guo et al., 2023; Zhang et al., 2022]. For example, Zhao et al. [Zhao et al., 2023] freezes the pre-trained model and aggregates prompts to fine-tune the pre-trained models. The second approach focuses on combining parameter-efficient fine-tuning and model emulator derived from a lossy compressed version of a large pre-trained model. Xiao et al. [Xiao et al., 2023; Niu et al., 2022] fine-tunes the adapter on the downstream data with the emulator's assistance. Although these early studies confirm the potential of FL in fine-tuning pre-trained models, they still demand high computational local devices and incur significant communication costs."}, {"title": "2.2 Split Federated Learning", "content": "Split federated learning (SFL) [Thapa et al., 2022] is a novel framework that combines Split Learning (SL) [Gupta and Raskar, 2018; Vepakomma et al., 2018] and FL [McMahan et al., 2017; Kone\u010dn\u1ef3 et al., 2016; Kairouz et al., 2021]. SFL reduces local computing costs by using SL to split the model into two modules: the client model and the server model. The client model has lower computational complexity, minimizing computational overhead at clients. The server model with heavy complex computations is shouldered by the server. SFL then achieves faster collaboratively training by performing parallel processing across clients using FL. Intuitively, SFL can be a good framework to address the training challenges in distributed scenarios with constrained local resources.\nNevertheless, naively applying SFL to fine-tune large pre-trained models still introduces significant communication burdens because of the frequent transmission of forward and backward signals. To further validate the point, we use a pre-trained vision transformer [Dosovitskiy et al., 2020] as an example. Suppose there is a client with 1,000 locally available images. During each global round, which represents the cycle from the current parameter aggregation to the next, the client performs 10 local epochs, representing the number of"}, {"title": "3 Methodology", "content": "We propose SFPrompt for efficiently adapting large pre-trained models to distributed downstream tasks with private data. SFPrompt employs a methodical three-phase strategy: client self-update, split training, and parameters aggregation, executed sequentially to ensure optimal performance. We illustrate the overall framework of SFPrompt in Figure 3.\nSFPrompt partitions the model, denoted as W, into three components to fully leverage the local computational resource. The head is the first few layers represented as Wh for feature extracting, the body is represented as Wb for modeling the dependency between features, and the tail is the classifier represented as Wt for mapping the feature to the task-specific output. In practical terms, the splitting strategy in SFPrompt is dynamic, not rigid. It is determined by the client's hardware resources, ensuring optimal performance without overwhelming the client's hardware.\nThe server distributes the client-side model, formed by the head and tail models and denoted as Wc = [Wh, Wt], to the selected clients K, where k \u2208 K denotes the kth client participating in the training process. Conversely, the body Wo is housed on the server, forming Ws. Wc exhibits a lower computational complexity and model size compared to the foundation model W, making it more suitable for implementation at clients, particularly in resource-limited scenarios."}, {"title": "3.2 Phase 1: Client Self-Update", "content": "Upon careful observation in the related work, which is shown in Fig 2, it becomes apparent that merely employing SFL, coupled with the existing fine-tuning methods, engenders new challenges. To tackle this issue, we approach the problem from two angles in SFPrompt.\nLocal-loss Update. To reduce the number of interactions between the server and the client, SFPrompt introduces the local-loss to achieve client self updating. We establish a connection between the final layer of Wh,k and the local tail Wt,k, such that the output of Wh,k becomes the input of Wt,k. We define the local loss function as Lc:\n$$L_c = \\frac{1}{|D_k|} \\sum_{x \\in D_k} l(x; (W_{h,k}, W_{t,k}); p)$$\nwhere l(x; (Wh, Wt); p) is the loss computed based on the input x, the head model Wh,k, the local tail model Wt,k and the prompt p. By inputting private data into the constructed model to perform local-loss updates, we eliminate the need for frequent interaction with the server compared with SFL, thereby reducing communication costs.\nDuring this process, the local tail model Wt and the prompt p updates, while the head model remains frozen. Importantly, this process incurs no additional communication costs as it does not establish a connection to the server-side model.\nLocal Dataset Pruning. Although we've made efforts to minimize the overall number of interactions, the cost of each individual interaction remains high. Before the split training, the selected clients participating in the training perform dataset pruning. We initially identify a collection of important training samples from the local dataset. We employ the norm of the error vector (EL2N) [Paul et al., 2021], which is defined as follows:\n$$E||p(W_t, x) - y||_2$$\nwhere p(Wt, x) represents the output of the neural network in the form of a probability vector when the input is x, and y denotes the one-hot encoded labels. EL2N scores are remarkably effective at identifying significant examples, which enables us to gauge the impact of a training point on the loss of an arbitrary example. By focusing on training samples with greater effect, we can reduce the need for unnecessary data transfer.\nSpecifically, we link the head Wh and the tail Wt, procuring the EL2N scores through the predicted class probability minus the one-hot label encoding. We prune the dataset using a preset pruning fraction \u03b3, ensuring that we retain the examples with higher EL2N scores. Therefore, the communication process only passes these processed important samples, not all samples, thereby reducing the communication cost.\nOverall, SFPrompt minimizes single-round data transmission by filtering local redundant data and leveraging local-loss updates to reduce frequent interactions, effectively cutting communication costs. The more details of SFPrompt on the client can be referred to Algorithm 1."}, {"title": "3.3 Phase 2: Split Training", "content": "As training progresses, clients completing phase 1 introduce a set of p continuous, learnable parameters, known as prompts, into the input space. The input space contains the embeddings of significant training samples following data pruning. Subsequently, the clients perform forward propagation using the model Wh to generate 'smashed' data, an intermediate output at the cut layer. The client then sends this smashed data to the server. On the sever-side, model Wb performs forward propagation and transmits smashed data, the output of Wb, back to the client. Upon receiving the data, the client conducts both forward and backward propagation to update the tail model Wt and compute the gradient. The computed gradient is transmitted back to the server. After the server completes backward propagation, it transmits the gradient to the client, enabling the client to update the prompt p based on the received gradient.\nThroughout the entire process, the prompt p and tail Wt are tuned, while the backbone W\u2081 and head Wh remain frozen. This signifies that only a small subset of all parameters is being fine-tuned. Additionally, in this process, the raw data is always located on the local side, and there is no direct data sharing. The details of SFPrompt employed on the server side can be referred to Algorithm 2."}, {"title": "3.4 Phase 3: Parameters Aggregation", "content": "After completing the previous two phases, the parameters of the tail model Wt and p are updated. During this phase, clients transmit their updated local tail model and prompt parameters to the server, fostering a collaborative training process. The server subsequently conducts a global aggregation, then distributes the aggregated tail model and prompts the selected clients for the next round of training. This process is mathematically represented as follows:\n$$(W_{t,r+1}, P_{r+1}) = \\frac{1}{K} \\sum_{k \\in K} (W_{t,k,r}, P_{k,r})$$\nwhere r\u2208 R is the number of global rounds, the final global model WR is derived by integrating the models Wh, Wb, Wt.R, and PR for inference. This integration results in the fine-tuned model, primed for downstream tasks.\nIt's noteworthy to mention that SFPrompt achieves efficient tuning by only fine-tuning the tail model and prompt parameters. Importantly, this entire process occurs without sharing the raw data, thereby preserving data privacy."}, {"title": "3.5 Analysis of SFPrompt", "content": "We delve into an in-depth analysis of SFPrompt. For easier understanding, we introduce the network split fraction, denoted as \u03b1 and \u03c4. Then, we define that |Wh| = \u03b1|W|, |Wb| = \u03c4|W| and |Wt| = (1 \u2212 \u03b1 \u2212 \u03c4)|W|, where |W| signifies the total number of parameters in model W. We set the size of prompt parameters to p, the dataset pruning fraction to \u03b3, and assume that the cut layer's size is q. Further, we assume the computational power of the client and server as Pc and Ps respectively, with the condition that Pc < Ps. The time needed to update model W on the dataset D is expressed as |D||WI, where forward propagation requires time \u03b2|D||WI, and backward propagation demands time (1 \u2013 \u03b2)|D||WI. We have standardized uplink and downlink rates as R, which simplifies to when K clients collaboratively work in order to streamline our analysis. We now proceed to analyze SF-Prompt as following several perspectives.\nComputational Cost. At the start of each global round, SF-Prompt selects a set of K clients to transmit the model Wc = [Wh, Wt], causing a latency of Wc K (1-T)|W|K. The selected clients then perform the forward propagation step, which requires a computation latency of YB(|wn|+p)|D| \u03b3\u03b2(\u03b1|W|+p)\\D\\, and send the output of the cut layer to the server at the cost of YDIK. Next, the server performs forward propagation on the received data, which equates to a computation of BT7|D||W|. Then send the output of the body to clients at the cost of DK A. The clients perform the forward and backward propagation on Wt, which requires a computation latency of ((1\u2212\u22127)|W|+p)|D| and send the gradients back server costing D. Then the server performs the backward propagation costing (1-\u03b2) \u03c4\u03b3|D||W|K. Subsequently, the clients update their local model We using the backward signal from the server, leading to additional computation costs of \u03b1(1\u2212\u03b2)|D|(|W|+r). Finally, each client uploads the prompt p and Wt to the server for global aggregation, causing a latency of (W++p)K\nWe compare SFPrompt with FL and SFL to analyze the efficiency and effectiveness, considering factors such as per-client computational burden, total communication cost, and overall latency, as detailed in Table 1. Primarily, both SFL and SFPrompt exhibit a reduced computational load due to their model-splitting strategy. Secondly, SFPrompt consistently outperforms SFL in terms of communication cost, as it capitalizes on local training to minimize frequent transmission. Lastly, our analysis shows that SFPrompt holds a significant advantage over over FL when the model scale W > 297 D, making it an ideal choice for fine-tuning large models.\nPrivacy. SFPrompt maintains a privacy level consistent with other SFL schemes [Thapa et al., 2022] by keeping raw data localized. Nevertheless, akin to previously employed distributed learning methods, our algorithm might be susceptible to privacy concerns through model inversion attacks against the server, given that the server retains the parameters of the entire network. It's important to note that our work is orthogonal to existing privacy-preserving methods in SFL, such as protecting the intermediate activations [Vepakomma et al., 2020; Titcombe et al., 2021], and the incorporation of these methods can further fortify model privacy."}, {"title": "4 Experiments", "content": "We present the evaluation of SFPrompt's performance in various fine-tuning tasks across the chosen datasets. The comparison results among FF and linear are demonstrated in Fig 4. On the CIFAR-10 dataset, SFPrompt achieves performance comparable to baseline methods. The advancement of SFPrompt becomes more pronounced on the more complex CIFAR-100 dataset, where SFPrompt excelled entirely over the other two methods. Particularly in non-iid settings, the advantage of SFPrompt becomes even greater. In this context, SFPrompt achieved a substantial 10.61%"}, {"title": "4.2 Evaluation of SFPrompt", "content": "improvement over FF and a 5.01% improvement over Linear. We further extended our verification to the SVHN and Flower-102 datasets, where SFPrompt continued to demonstrate good performance. The detailed terminal results, showcasing the effectiveness of SFPrompt across different scenarios, are presented in Table 3."}, {"title": "Communication Cost", "content": "We evaluate the communication cost of SFPrompt, comparing it with FL and SFL. The results, presented in Table 2, clearly illustrate that SFPrompt incurs lower communication costs compared to the other methods. This is due in part to SFPrompt performing local-loss updates, thereby eliminating the need for frequent data transmission and also pruning redundant examples from the local dataset to further reduce communication costs. SFPrompt's communication cost is 47% that of FL and 6% that of SFL. Intriguingly, the gap between these methods grows as the model size increases, making the efficiency improvements more pronounced. Using the ViT-large with more parameters as the base model, the communication cost for SFPrompt is reduced to 19% that of FL. This disparity stems from FL's communication cost being tied to the model size, while SF-Prompt's cost is associated with the number of interactions and the size of the transmitted data, having little correlation with the model size."}, {"title": "Computational Burden", "content": "We assess the computational cost in terms of FLOPs, a widely accepted measure of model complexity. The results reveal that both SFPrompt and SFL simplify local model computation. This simplification is achieved by splitting the model to position the simpler model on clients, which is with limited computational resources. The computational complexity of SFPrompt is a mere 0.46% that of FL, and this advantage magnifies as the model size increases. With the model size increasing, SFPrompt manages to maintain a simple model structure on the client side. Although the client model of SFPrompt and SFL is identical, SFPrompt utilizes fewer samples, thereby reducing FLOPs."}, {"title": "4.3 Ablation Study", "content": "To further understand the effectiveness and robustness of SFPrompt, we conduct an ablation study, exploring the influence of different components and configurations on the model's performance. Firstly, we compare SF-Prompt with SFPrompt w/o local-loss update to explore the influence of local-loss update, as shown in Figure 6. The results indicate that the local-loss update step is instrumental in contributing to SFPrompt's performance.\nDataset Pruning. We evaluate the effect of different local dataset pruning fractions on the performance of the model, as depicted in Figure 7. Interestingly, even with deep pruning of the local dataset under the IID condition, the impact on model performance was minimal. When only 20% of the largest EL2N values in the local dataset were retained, the performance was only reduced by 3.39% compared to the full dataset. Under non-IID conditions, even after pruning 80% of the data, the performance decline is quite limited, amounting to only 4.32%. This occurs because, although SFPrompt doesn't utilize the complete dataset during global training, it leverages the complete data in subsequent local-loss updates. Additionally, in a distributed scenario, data from other clients can still provide rich information and compensate for losses to support the training process."}, {"title": "5 Conclusions", "content": "In this paper, SFPrompt is introduced as a privacy-preserving and efficient distributed fine-tuning framework. SFPrompt splits the large pre-trained model into server-side and client-side to lower the computational burden on the client and further introduce prompt parameters to enable efficient fine-tuning. SFPrompt prunes the redundant local data and updates the model based on the local loss to further reduce the additional communication cost. Our extensive experiments reveal that SFPrompt achieves state-of-the-art performance, outperforming existing baselines."}]}