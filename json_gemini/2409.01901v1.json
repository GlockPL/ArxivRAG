{"title": "3D Lexicons for American Sign Language and Sign Language of the Netherlands", "authors": ["O. Ranum", "G. Otterspeer", "J.I. Andersen", "R.G. Belleman", "F. Roelofsen"], "abstract": "In this work, we present an efficient approach for capturing sign language in 3D, introduce the 3D-LEX v1.0 dataset, and detail a method for semi-automatic annotation of phonetic properties. Our procedure integrates three motion capture techniques encompassing high-resolution 3D poses, 3D handshapes, and depth-aware facial features, and attains an average sampling rate of one sign every 10 seconds. This includes the time for presenting a sign example, performing and recording the sign, and archiving the capture. The 3D-LEX dataset includes 1,000 signs from American Sign Language and an additional 1,000 signs from the Sign Language of the Netherlands. We showcase the dataset utility by presenting a simple method for generating handshape annotations directly from 3D-LEX. We produce handshape labels for 1,000 signs from American Sign Language and evaluate the labels in a sign recognition task. The labels enhance gloss recognition accuracy by 5% over using no handshape annotations, and by 1% over expert annotations. Our motion capture data supports in-depth analysis of sign features and facilitates the generation of 2D projections from any viewpoint. The 3D-LEX collection has been aligned with existing sign language benchmarks and linguistic resources, to support studies in 3D-aware sign language processing.", "sections": [{"title": "1. Introduction", "content": "Sign language processing (SLP) is a dynamic research area concerned with advancing computational methods for sign languages (SL). This multidisciplinary field encompasses tasks such as the automatic understanding, recognition, translation and production of sign language, contributing to a more inclusive future in language technology.\nDespite receiving increased attention across computer sciences (Koller, 2020; Rastgoo et al., 2021), SLP remains less developed compared to other areas within Natural Language Processing (Yin et al., 2021). A significant factor contributing to this disparity is the lack of large-scale, high-quality, and publicly accessible sign language corpora (Bragg et al., 2019). Notably, the majority of these datasets are recorded with cameras that view signers from a single, (near-)frontal perspective (Ali et al., 2022). This scarcity of data impedes modern machine-learning algorithms from learning robust sign representations grounded in the three-dimensional nature of sign languages.\nLiterature supports that dept-awareness and viewing angle matters in both human (Watkins et al., 2024) and machine (Gao et al., 2023; Rastgoo et al., 2020) SL understanding. This implies that representations should reflect a degree of 3D awareness, or risk reduced accuracy under normal real-world conditions, such as non-frontal viewpoints.\nWhile systems such as OpenPose (Cao et al., 2021) enable the estimation of 3D poses from video footage, the precision of such reconstructions is in principle lower than the accuracy achieved through direct 3D motion capture techniques (Jedli\u010dka et al., 2020). Navigating imperfectly reconstructed 3D representations can pose significant challenges for downstream SLP tasks.\nProviding a 3D ground truth to existing datasets could significantly improve the feasibility of many SLP tasks. Against this backdrop, we introduce 3D Lexicons (3D-LEX) for American Sign Language (ASL) and the Sign Language of the Netherlands (Nederlandse Gebarentaal, NGT). The 3D-LEX datasets include 1,000 isolated signs from each language recorded with three distinct motion capture techniques, as illustrated in Figure 1. The vocabularies have been aligned with existing SL resources, including the WLASL (Li et al., 2020) and SEMLEX (Kezar et al., 2023) benchmarks for isolated sign recognition, the ASL-LEX 2.0 (Sehyr et al., 2021) lexicon and the SignBank NGT (SB NGT) lexicon (Crasborn et al., 2020). The 3D-LEX dataset facilitates the generation of 2D projections from any viewpoint and supports in-depth analysis of sign language features, offering several key advantages:\nAutomatic recognition of phonetic properties: High-resolution 3D data allows for detailed studies of sign language features, including handshapes, place of articulation, and orientations.\nMulti-view SL recognition: Ground truth 3D representations facilitate the rendering of synthetic multi-view 2D data from any angle and translation. This data can be used to train models that are capable of multi-view SL recognition, a task that has received little attention in the SLP literature so far.\nSL production for XR applications: Current work on SL production focusing on 2D outputs, such as synthetic photorealistic videos or 2D skeleton animations, are not directly suitable for Extended Reality (XR) applications. While reconstructing 3D motion from multiple 2D views is an area of active research, leveraging 3D data to produce 3D animations currently still offers a more effective and accurate approach.\nThe 3D-LEX v1.0 dataset was developed during our initial exploration of motion capture equipment for capturing three-dimensional sign representations. We acknowledge that the methodology outlined in Section 3 presents significant opportunities for improvement. Specifically, ensuring consistency in data quality will be a primary objective in our future efforts. Nevertheless, even in this nascent stage of development, we could demonstrate the utility of the 3D-LEX data. In Section 4 we showcase how the dataset can be leveraged to produce semi-automatic annotations of handshapes. Evaluating the annotations in a downstream isolated sign recognition (ISR) task demonstrates that the labels achieved parallel benefits to leveraging annotations provided by linguists. We discuss several observed limitations and prospects for improvement in Section 5, and Section 6 highlights some ethical considerations."}, {"title": "2. Background", "content": "Sign languages are visual, complete, and natural languages, each with a distinct structure, grammar, and lexicon. They employ a combination of manual markers (e.g. handshapes, hand location, palm orientation and movements) and non-manual markers (e.g. mouthings, facial expressions, gaze) to convey meaning (Stokoe). Sign languages serve as the primary language in Deaf communities."}, {"title": "2.1. Sign Language", "content": "Sign languages are visual, complete, and natural languages, each with a distinct structure, grammar, and lexicon. They employ a combination of manual markers (e.g. handshapes, hand location, palm orientation and movements) and non-manual markers (e.g. mouthings, facial expressions, gaze) to convey meaning (Stokoe). Sign languages serve as the primary language in Deaf communities."}, {"title": "2.2. Sign Language Datasets", "content": "The majority of publicly available resources demonstrating sign language are captured in video. These datasets consists of either isolated signs (e.g. Sehyr et al., 2021; Athitsos et al., 2008; Kezar et al., 2023; Joze and Koller, 2019; Li et al., 2020) or continuous sign sentences (e.g. von Agris and Kraiss, 2010; Schembri et al., 2013). Key distinguishing features between the collections include the source language, signer variability, data scope, linguistic domain, and the availability and quality of annotations.\nMost datasets comprise RGB video formats, but they may also include depth estimations or skeletal poses generated from joint approximations. While these datasets usually feature a single, (near-)frontal viewpoint, there is a growing trend in lab-curated datasets to provide multiple viewing angles (e.g. Duarte et al., 2020; Mopidevi et al., 2023; Rastgoo et al., 2020; Gao et al., 2023). Depth cameras have been used to capture 3D positioning, for example using the Kinect depth sensor (e.g. Oszust and Wysocki, 2013; Cooper et al., 2012; Huang et al., 2018). For an extensive summary of sign language datasets, refer to Kopf et al. (2022).\nDatasets facilitating 3D awareness in sign representations either leverage depth estimations or 3D reconstruction techniques. For the creation of more precise 3D representations, numerous motion capture datasets have been curated (e.g. Lu and Huenerfauth, 2010; Heloir et al., 2006; Benchiheub et al., 2016), typically to generate signing avatars (Bragg et al., 2019) or for exploring automatic synthesis of sign language utterances (e.g. Jedli\u010dka et al., 2020; Gibet, 2018)."}, {"title": "3. The 3D-LEX Dataset", "content": null}, {"title": "3.1. Data Scope", "content": "The 3D-LEX v1.0 dataset includes lexical datasets sampled from ASL and NGT, where the scope was defined to ensure integration with existing benchmarks. A total of 1,000 signs are selected from each language, and recorded with two data collection techniques to capture manual markers and one technique to capture non-manual markers\u00b9.\nWe release three distinct data formats corresponding to the different capturing techniques, and one component integrating handshapes and body pose data."}, {"title": "Handshape Data", "content": "The handshape(s) of each sign is captured with the StretchSense Pro Fidelity Motion Capture Gloves\u00b2. The gloves measure the splay and bend of the fingers, alongside the relative rotation of each joint within the hand. The available data include the stretch sensor readings and exported FBX\u00b3 files. Detailed guidance on interpreting and assessing StretchSense data can be found in the project's Git repository for data evaluation\u2074."}, {"title": "Body Pose Data", "content": "The place of articulation, movement, and body pose of each sign is captured using a Vicon (V) Motion Capture setup with optical markers. The raw marker location data is published, alongside processed FBX data, which has been exported via Shogun Post."}, {"title": "Face Blendshape Data", "content": "Facial features are captured as blendshapes with the Live Link Face5 (LLF) application and ARKit on iPhone."}, {"title": "Retargeted Animation Data", "content": "For sign language production and animation purposes, we release FBX files containing the body pose data and the handshapes."}, {"title": "3.2. Production Method", "content": "To efficiently capture the lexicons, we have developed a recording pipeline that achieves an average capture time of 10 seconds per sign. This includes the time for sign demonstration, performance, recording, and storage of the captured sign, though it varies with the sign's length. Setup preparations, which involve fitting the suit, positioning markers, and calibrations, require approximately 1 hour with our current method."}, {"title": "3.2.1. Recording Setup and Procedure", "content": "Our studio setup includes a designated detection zone for the Vicon cameras, an iPhone equipped with Live Link Face mounted on a tripod, one screen to display glosses and reference videos, and a second screen to showcase the recordings for immediate evaluation.\nA triple-foot pedal system facilitates the remote operation of the motion capture control system. Each pedal is configured for a distinct function: The left pedal triggers the start and stop of recordings across all three motion capture systems simultaneously; the middle pedal stores the latest recording and issues a request to the SignCollect platform to display the next gloss in the vocabulary; and the right pedal is used to proceed to the next sign without saving any data. Signcollect is a platform developed to enable the efficient processing of glosses, providing a studio interface managed by gesture or pedal control. For details on the SignCollect platform consult Otterspeer et al. (2024).\nThe capture process for a single sign involves the following steps: First, the signer assumes an upright posture, with arms relaxed at their sides in a neutral position. By pressing the right pedal, a sign is prompted from the SignCollect platform, and the sign's gloss and a reference video are displayed on one of the screens. A recording is started by pressing the left pedal, and the signer performs the sign and returns to the neutral stance before the recording is ended with another press of the left pedal. The recorded data is automatically exported to SignCollect and visualized on an avatar rendered with Unreal Engine v5.3, allowing the signer to immediately review the quality of the data. If the data's quality is satisfactory, the signer can advance to the next gloss by pressing the right pedal, which saves the preceding recording. Should the sign's execution be deemed inadequate, the signer can repeat the recording by pressing the left pedal again or proceed by pressing the right pedal. For visualizing the sign we created an avatar in Ready Player Me Studio, a cross-platform avatar generator that allows users to build avatars for general purposes.\nA total of five signers contributed to capturing the ASL and NGT vocabularies. The signers were given two options to operate the pedal. Either they could control the pedal and capture process themselves, or they could delegate the pedal control to a team member. Preferences varied, with three signers opting for controlling the pedal themselves and two preferring assistance to concentrate on signing. Details regarding the number of words"}, {"title": "3.2.2. Vicon Motion Capture System", "content": "Setup: A Vicon rig is affixed to the ceiling, equipped with ten Vicon Vero v2.2 optical motion capture cameras7, as detailed in Figure 2. To mitigate occlusions, particularly those caused by the lower hands of the signer, an additional two Vicon Vero cameras are placed on the floor in front of the signer.\nThe markers are placed on the signer following the standard Vicon FrontWaist 53-marker set template, as displayed in Figure 3. Shogun Post is used to make a retarget for the motion capture data, which is used during recording to stream the data to Unreal Engine from Shogun Live.\nCalibration: For calibrating the Vicon camera system, we adhere to the built-in calibration protocol provided by Vicon. To ensure consistency in the calibration and that the origin remains approximately in the same position across multiple recording sessions, we place masking tape on the floor. This tape serves a dual purpose: one set of markings indicates the precise location for positioning the calibration wand during each calibration process. Another set of tape strips marks the designated spot where the signer is to stand during recordings.\nSoftware Specifications: To manage the Vicon camera system, we utilize Shogun Live 1.11, and to perform the retarget of the motion capture data we use Shogun Post 1.11."}, {"title": "3.2.3. StretchSense Gloves", "content": "Setup: The StretchSense Pro Fidelity gloves interface with Hand Engine Pro through two USB"}, {"title": "4. Evaluation", "content": "To demonstrate the utility of 3D sign data we turn to one of the envisioned benefits mentioned initially: the facilitation of automatic phonetic labeling. In particular, we present a baseline method for semi-automatic handshape annotation. The efficacy of the annotations is evaluated in an ISR task, through comparison with labels provided by linguists and against scenarios devoid of any labels.\nWhile we expect that the data can be used to label other phonetic properties (e.g. hand location, movement, orientations, eyebrow position) we here zoom in on the handshapes. This is an intentional choice, as we consider the use of StretchSense gloves to be the most experimental data acquisition technique for sign language capture. The development of semi-automatic annotation methods benefits both linguistic research and various SLP tasks, including recognition and production."}, {"title": "4.1. Semi-Automatic Handshape Annotations", "content": "In this section, we demonstrate one simple approach for generating phonetic annotations derived from the 3D-LEX handshape data. Due to the absence of an NGT benchmark for isolated sign recognition, we only generate and assess labels derived from the 3D-LEX ASL vocabulary.\nOur approach is designed to produce labels that resemble the handshape annotations typically found in ISR benchmarks, facilitating a meaningful comparison between automated and expert annotations. The glosses in ISR benchmarks are commonly assigned a single handshape label, based on the dominant handshape observed in a single reference video. We ensure that the number of possible label classes in our estimations corresponds approximately to the set of classes identified in the video-data benchmark WLASL. For the implementation and instructions on how to replicate our findings, please refer to the GitHub repository.\nTemporal segmentation To differentiate characteristic handshape signals from any resting or transitional poses, we construct a temporal segmentation method by calculating the Euclidean distance to each frame relative to the calibration poses. This method enables us to perform a first-order discrimination of signals within a recording.\nWe estimate and segment the poses of both hands to take into consideration that the signer may not strictly enforce the use of their dominant hand. Subsequently, we calculate the frequency of each observed handshape and select the handshape with the highest frame count. As the typically most frequent signal is the resting pose '5', we only select the '5' handshape if it is detected in more than 90% of the frames, otherwise, we select the second most frequently occurring class. The frames where the dominant handshape was detected are then selected as candidate frames for downstream analysis. Figure 5 showcases the output of a Euclidean distance handshape classification approach on frames from the captured sign 'zero'. Here, the handshape 'o' was identified as the characteristic handshape of the sign.\nSemi-automatic labeling The Euclidean distance labeling technique limits the identification of handshapes to those poses used during the glove calibration phase. This is suboptimal, as the calibration methodology of stretch sensors for capturing sign language is still in a nascent stage. Specifically, the calibration poses may not cover the full range of handshapes present in the lexicons.\nTo enable a more flexible identification of handshapes, we applied k-means clustering on the average poses of the frames selected during the temporal segmentation. We selected k=50, which is approximately the number of handshapes identified in ASL-LEX for the 3D-LEX vocabulary. We assign a new handshape label to each sign in 3D-LEX ASL, corresponding to the arbitrary cluster IDs assigned while clustering the high-dimensional features."}, {"title": "5. Limitations and Prospects", "content": "In the process of capturing our data, we have observed many potential areas for improvement. In this section, we highlight some of the current limitations in our methodology, and our intent for addressing them in future work.\nLike numerous datasets in sign language research, a significant limitation of 3D-LEX is signer diversity. A dataset comprising a single example for each sign, and which contains only five signers, is insufficient for representing the diversity and rich prosody inherent to sign languages. It is as such not possible to use 3D-LEX in isolation to learn representations useful in sign applications. Consequently, 3D-LEX can primarily serve for limited feature studies or to support video datasets by either providing a 3D ground truth or synthesizing multi-view 2D data from one signer. Future work should consider exploring 3D data which includes both multiple examples per signer and multiple signers per gloss.\nWhile all participants were native signers, it is critical to highlight that only one had ASL as their primary language. As a result, a significant segment of the 3D-LEX ASL dataset was produced by signers whose primary language is NGT but who were proficient in ASL. The impact of employing signers whose primary sign language differs from the captured target language, on the quality and authenticity of lexical sign data remains an area for future research. This concern is recognized as a limitation in v1.0 of 3D-LEX.\nThe dataset has a limited scope, which comprises a non-exhaustive set of phonological features and vocabularies from the complete languages. However, our method facilitates the production of larger vocabularies and data for additional sign languages.\nWe observed several limitations in our current pipeline. While experimenting with the data acquisition control we noticed varying preferences among signers for operating the pedal. The choice of operator resulted in the emergence of several distinct patterns within the data. When signers themselves operate the pedal, it's generally more efficient but introduces a signal from foot movement at the start and end of each sign. Conversely, using an external operator can result in greater variability in the timing of recordings, affecting the consistency of the recorded time window around each sign. Efforts to streamline these production elements are anticipated in future work.\nWhile our system has been designed with a focus on efficiency, we have identified several limitations concerning the hardware. To the best of our knowledge, 3D-LEX is the first publicly available dataset using the StretchSense gloves to conduct statistical analysis on handshapes in sign language. These gloves were initially developed to generate animation data, which typically does not require the same degree of accuracy in capturing detailed, varied and intricate movements of fingers and hands. Therefore, employing these gloves to provide detailed studies of handshapes in sign language represents a novel and experimental approach. Although the gloves have shown promising capabilities, their performance has presented several challenges.\nNotably, the precision of the gloves' measurements is closely tied to how well they fit the signers' hands and the length of time they are worn. A snugger fit typically leads to higher accuracy. However, prolonged usage has been observed to decrease accuracy, likely due to the glove's position shifting on the hand, thereby deviating from its calibrated stance. Shifts can occur for example when hands swell from accumulated heat and from natural movements during wear. Larger gloves relative to the hand size are more prone to positional shifts, exacerbating this issue.\nThe Hand Engine software is prone to overfitting the sensor data to the calibration poses, a tendency that amplifies when training involves an extensive calibration pose set. Currently, the calibration process utilizes either 20 or 25 poses. We observed that such a detailed pose repertoire complicates Hand Engine's ability to accurately replicate more complex poses and distinguish between poses where the shift in stretching values are relatively small. Figure 7 illustrates a series of poses that exhibit substantial differentiation challenges for the gloves under our calibration framework. With the current version of Hand Engine, future research may gain advantages from employing a smaller set of calibration poses. Ideally, these selected handshapes should not only be representative of those within the dataset but also exhibit maximum distinction from each other within the calibration set.\nAn in-depth assessment of calibration methods to address overfitting issues warrants further exploration."}, {"title": "6. Privacy and Ethical Considerations", "content": "The success of machine learning methods has led to large increases in requests for data. While this implies heightened concerns for privacy across computational sciences, it is important to recognize that data collection from minority language communities is at particular risk: Both because a status as deaf classifies as sensitive information, but also because data collection from small populations limits anonymity (Bragg et al., 2020). Additionally, certain sign language datasets that are publicly accessible were compiled without obtaining informed consent from the individuals featured, particularly those datasets that gather information from platforms such as YouTube. All signers contributing to the production of 3D-LEX gave informed consent and received compensation. Moreover, the anonymity of contributors is enhanced compared to typical video datasets, since the motion capture recordings do not visually reveal the signers. To further protect signer anonymity, each participant has been assigned a unique signer ID."}, {"title": "7. Conclusion", "content": "In this paper, we introduce a new and efficient method for collecting 3D sign language data, resulting in the 3D-LEX dataset, and describe a semi-automatic approach for producing phonetic annotations. The 3D-LEX dataset was produced leveraging three distinct motion capture systems, with two collection techniques to capture manual markers and one technique to capture non-manual markers. Although our approach shows considerable room for improvement, we highlight its potential by automatically generating handshape labels for 1,000 ASL signs. Our initial evaluations of the labels on a downstream ISR task reveal that the semi-automatic annotations offer benefits parallel to those of expert annotations. In conclusion, the 3D-LEX v1.0 demonstrates considerable potential even in its early stages of development. We anticipate that future research using 3D-LEX will investigate synthesizing multi-view data from the 3D ground truths to support tasks such as multi-view SLR, and develop approaches annotating additional phonetic classes."}]}