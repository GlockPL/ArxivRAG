{"title": "Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning", "authors": ["Ronglong Fang", "Yuesheng Xu"], "abstract": "Deep neural networks (DNNs) have showcased their remarkable precision in approximating smooth functions. However, they suffer from the spectral bias, wherein DNNs typically exhibit a tendency to prioritize the learning of lower-frequency components of a function, struggling to effectively capture its high-frequency features. This paper is to address this issue. Notice that a function having only low frequency components may be well-represented by a shallow neural network (SNN), a network having only a few layers. By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data. We implement the proposed idea by exploiting the multi-grade deep learning (MGDL) model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features. We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features. Our study reveals that MGDL excels at representing functions containing high-frequency information. Specifically, the neural networks learned in each grade adeptly capture some low-frequency information, allowing their compositions with SNNs learned in the previous grades effectively representing the high-frequency features. Our experimental results underscore the efficacy of MGDL in addressing the spectral bias inherent in DNNs. By leveraging MGDL, we offer insights into overcoming spectral bias limitation of DNNs, thereby enhancing the performance and applicability of deep learning models in tasks requiring the representation of high-frequency information. This study confirms that the proposed method offers a promising solution to address the spectral bias of DNNs. The code is available on GitHub: Addressing Spectral Bias via MGDL.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have achieved tremendous success in various applications, including computer vision [15], natural language processing [38], speech recognition [26], and finance [27]. From a mathematical perspective, the success is mainly due to their high expressiveness, as evidenced by theoretical demonstrations showing their capability to approximate smooth functions with arbitrary precision [3, 10, 14, 17, 48]. Various mathematical aspects of DNNs as an approximation tool were recently investigated in [11, 16, 22, 25, 35, 45\u201347, 49]. However, it was noted in [30, 42] that the standard deep learning model, which will be called single grade deep learning (SGDL), trains a DNN"}, {"title": "2 Proposed Approach and Multi-Grade Learning Model", "content": "We introduce a novel approach to tackle the spectral bias issue and review the MGDL model.\nWe begin with a quick review of the definition of DNNs. A DNN is a successive composition of an activation function composed with a linear transformation. Let R denote the set of all real numbers, and d, s be two positive integers. A DNN with depth D consists an input layer, D 1 hidden layers, and an output layer. Let ND := {1,2,..., D}. For j \u2208 {0} \u222a ND, let dj denote the number of neurons in the j-th hidden layer with d\u2080 := d and dD := s. We use W\u2c7c \u2208 \u211d^(dj\u00d7dj\u22121) and b\u2c7c \u2208 \u211d^dj to represent the weight matrix and bias vector, respectively, for the j-th layer. By \u03c3 : \u211d \u2192 \u211d we denote an activation function. When \u03c3 is applied to a vector, it means that \u03c3 is applied to the vector componentwise. For an input vector x := [x\u2081, x\u2082,...,x\u2090]\u1d40 \u2208 \u211d^d, the output of the first layer is defined by H\u2081(x) := \u03c3 (W\u2081x + b\u2081) . For a DNN with depth D > 3, the output of the (j + 1)-th hidden layer can be identified as a recursive function of the output of the j-th hidden layer, defined as H\u2c7c\u208a\u2081(x) := \u03c3 (W\u2c7c\u208a\u2081H\u2c7c (x) + b\u2c7c\u208a\u2081), for j \u2208 ND\u2212\u2082. Finally, the output of the DNN with depth D is an s-dimensional vector-valued function defined by\n\nND ({Wj, bj}\u2c7c=\u2081;x) = ND(x) := WDHD\u2212\u2081 (x) + bD.        (1)\n\nSuppose that data samples D := {xe, ye}e=\u2081 are chosen. The loss on D is defined as\n\nL({Wj, bj}\u2c7c=\u2081; \u0398) :=  \u2211\u2081  ye \u2212 ND (Wj, bj}\u2c7c=\u2081;) (xe).  (2)\n\nThe traditional SGDL model is to minimize the loss function L defined by (2) with respect to \u0398 := {Wj, bj}\u2c7c=\u2081, which yields the optimal parameters \u0398* := {W*, b*}\u2c7c=\u2081 and the corresponding DNN N\u266d (\u0398*; \u00b7). When D is relatively small, for example, D < 5, we call ND an SNN. It is well-recognized that training an SNN is notably easier than training a DNN.\nWe motivate the proposed idea by a simple example. We consider the function f(x), x \u2208 [0, 1], whose Fourier transform is shown in Figure 1 (Left), where the Fourier transform is defined by f(t) := \u222b\u2080\u00b9 f(x)e^(\u2212i2\u03c0t x)dx. To compute the Fourier transform of f defined on [0, 1], we extend f to the entire real line by assigning its value to be zero for x \u2209 [0, 1]. Observing from Figure 1 (Left), the function f has significant high-frequency components, with frequencies varying from 0 to 200. The function f can be represented as\n\nf(x) = f\u2081(x) + (f\u2082 \u2218 f\u2081)(x) + (f\u2083 \u2218 f\u2082 \u2218 f\u2081)(x) + (f\u2084 \u2218 f\u2083 \u2218 f\u2082 \u2218 f\u2081)(x), x \u2208 [0, 1], (3)\n\nwhere \u2218 denotes the composition of two functions. Note that the Fourier transforms fj, j = 1, 2, 3, 4, are displayed in Figure 1 (Right). Clearly, the functions fj, j = 1, 2, 3, 4, are of low-frequency, with frequencies mainly concentrating on the interval [0,50]. This example surely demonstrates that a function of high-frequency can be expressed as a sum of compositions of lower-frequency functions.\nThis observation leads to the proposed approach of addressing the spectral bias of DNNs to be studied in this paper. The legitimacy of the proposed idea may be reinforced by the Jacobi-Anger identity [2], which expresses a complex exponential of a trigonometric function as a linear combination of its harmonics. Even though both the complex exponential function and the trigonometric function are of low-frequency, their composition contains many high-frequency components. We now review the Jacobi-Anger identity, the identity named after the 19th-century mathematicians Carl Jacobi and Carl Theodor Anger. It has the form\n\ne^(ia sin(bx)) = \u2211_(n=-\u221e)^\u221e J_n(a)e^(inbx), (4)\n\nwhere i denotes the imaginary unit and Jn(a) denotes the n-th Bessel function of the first kind, see details in [2]. Taking the real part of the both sides of the Jacobi-Anger identity (4), we obtain that\n\ncos(a sin(bx)) = \u2211_(n=-\u221e)^\u221e J_n(a) cos(nbx).  (5)\n\nThe left-hand side of (5) is a composition of two low-frequency functions cos(ax) and sin(bx), having frequencies a/(2\u03c0) and b/(2\u03c0), respectively, while the right-hand side is a linear combination of cos(nbx) with n taking all integers. The high-frequency of the composition can be estimated by a rule of thumb. Specifically, the left-hand side of (5) is a frequency-modulated sinusoidal signal [32, 33], with its frequencies spreading on an interval centered at zero. It follows from the well-known Carson bandwidth rule [8, 29, 32], regarded as a rule of thumb, that more than 98% frequencies are located within the interval [\u2212(ab + b)/(2\u03c0), (ab + b)/(2\u03c0)]. Therefore, the highest frequency of cos(a sin(bx)) can be well-estimated by (ab + b)/(2\u03c0), which is greater than the product of the frequencies of cos(ax) and sin(bx). These suggest that a composition of two low-frequency functions may lead to a high-frequency function.\nThe example presented earlier, together with the Jacobi-Anger identity, inspires us to decompose a given function into a sum of different frequency components, each of which is a composition of lower-frequency functions, a decomposition similar to equation (3) for the function f represented in Figure 1 (Left). In other words, for a function g of high-frequency, we decompose it in a \u201csum-composition\" form as\n\ng= \u2211_(k=1)^K \u220f_(j=1)^k gj, (6)\nwhere \u220f_(j=1)^k gj := gk \u2218 ... \u2218 g\u2082 \u2218 g\u2081, and gj, j\u2208 Nk, are all of low-frequency. The function f represented in (3) is a special example of (6). In the context of approximation by neural networks, we prefer expressing gj by SNNs, as a function having only low-frequency components can be well-represented by an SNN. The MGDL model originated in [43, 44] furnishes exactly the decomposition (6), with each gj being an SNN. We propose to employ MGDL to learn the decomposition (6), where the low-frequency function gj is represented by an SNN.\nIt is worth explaining the motivation behind the MGDL model. MGDL was inspired by the human education system which is arranged in grades. In such a system, students learn a complex subject in grades, by decomposing it into sequential, simpler topics. Foundational knowledge learned in previous grades remains relatively stable and serves as a basis for learning in a present and future grades. This learning process can be modeled mathematically by representing a function that contains higher-frequency components by a \u201csum-composition\u201d form of low-frequency functions. MGDL draws upon this concept by decomposing the learning process into multiple grades, where each grade captures different levels of complexity.\nWe now review the MGDL model that learns given data D := {xe, ye}e=\u2081^N. Following [43], we split a DNN with depth D into L grades, with L < D, each of which learns an SNN ND\u2081, defined as (1), with depth D\u2081, from the residue {e_\u03b9}e=\u2081^N of the previous grade, where 1 < D\u2081 < D and \u2211_(l=1)^L D\u2081 = D + L \u2212 1. Let \u0398\u03b9 := {W_, b_\u03b9}\u2c7c^(D\u2081) denote the parameters to be learned in grade l. We define recursively g\u2081(\u0398\u2081; x) := ND\u2081 (\u0398\u2081;x), g\u03b9\u208a\u2081(\u0398\u03b9\u208a\u2081;x) := ND\u03b9\u208a\u2081 (\u0398\u03b9\u208a\u2081;\u00b7) \u2218 HD\u2081\u2212\u2081(\u04e8_\u03b9;\u00b7) \u2218 ... \u2218 HD\u2081\u2212\u2081(\u04e8\u2081; \u00b7)(x), for l \u2208 NL\u2212\u2081, and the loss function of grade l by\n\nL\u03b9 (\u0398\u03b9; D) :=  \u2211_(l=1)^N  ye \u2212 g\u03b9 (\u0398\u03b9; xe). (7)\n\nwhere O\u03b9* := {W*\u03b9, b*\u03b9}\u2c7c^(D\u2081) are the optimal parameters learned by minimizing the loss function L\u03b9 with respect to \u0398\u03b9. The residues are defined by e\u2080 := ye and e\u03b9\u208a\u2081 := e\u03b9 \u2212 g\u03b9(O\u03b9*; xe), for l \u2208 NL\u2212\u2081, l \u2208 NN. When minimizing the loss function L\u2081(\u0398\u2081; D) of grade 1, parameters Oj, j \u2208 N\u2081\u2212\u2081, learned from the previous l \u2013 1 grades are all fixed and HD\u2081\u2212\u2081\u2212\u2081(\u04e8\u2212\u2081;\u2025) \u2218 ... \u2218 HD\u2081\u2212\u2081(\u04e8\u2081; \u00b7) serves as a feature or \"basis\". After L grades are learned, the function \u011fL learned from MGDL is the summation of the function learned in each grade, that is,\n\ngL ({O*\u03b9}\u03b9=\u2081^L; *) := \u2211_(\u03b9=1)^(L\u22121)g\u03b9(O*\u03b9; x), (8)\nwhere g\u03b9(\u0398; x) := N\u2081(\u2081; \u2022) \u2218 HD\u2081\u2212\u2081\u2212\u2081(\u2081\u2212\u2081; \u2025) \u2218 ... \u2218 HD\u2081\u2212\u2081(\u2081;\u00b7)(x), and HDk\u2212\u2081\u2212\u2081 for 1 \u2264 k \u2264 L and ND\u2081 are SNNs learned in different grades. Thus, MGDL enables us to construct the desired \"sum-composition\" form (6). When L = 1, MGDL reduces to the traditional SGDL model.\nIn MGDL, we use the mean squared error (MSE) loss function. It was established in [43] that when the loss function is defined by MSE, MGDL either learns the zero function or results in a strictly decreasing residual error sequence (see, Theorem 1 in Appendix A). Since the regression problems conducted in this paper naturally align with MSE losses, it is a suitable choice. In practice, MGDL can also be applied with other loss functions, such as cross-entropy loss, when solving classification problems. In MGDL, the computation cost remains relatively consistent across all grades. For xl := HD\u2081\u2212\u2081\u2212\u2081(\u04e8\u2212\u2081;\u00b7) \u2218 HD\u2081\u2212\u2082\u2212\u2081(\u04e8\u2212\u2082; \u2025) \u2218 ... \u2218 HD\u2081\u2212\u2081(\u04e8\u2081; \u00b7)(xe), we recursively let x\u2080 := xe, x_k^l := HD_k^(l\u22121)\u2212\u2081 (\u04e8\u2212\u2081; \u00b7) ox_k\u2212\u2081^l, k = 2, 3, ..., n. When training grade l, we use the"}, {"title": "3 Numerical Experiments", "content": "In this section, we study MGDL empirically in addressing the spectral bias issue of SGDL. We consider four examples: Subsections 3.1, 3.2, and 3.4 investigate regression on synthetic, manifold, and MNIST data, respectively, for which the spectral bias phenomena of SGDL are identified in [30]. Section 3.3 deals with regression on colored images, which were studied in [36] by using the Fourier features to mitigate the spectral bias. Our goal is to compare the learning performance of MGDL with that of SGDL on these datasets and understand to what extent MGDL can overcome the spectral bias exhibited in SGDL.\nThe loss functions defined in (2) for SGDL and (7) for MGDL are used to compute the training and validation loss when D is chosen to be the training and validation data, respectively. We use the relative squared error (RSE) to measure the accuracy of predictions obtained from both SGDL and MGDL. Assume that N is a trained neural network. For a prediction value \u0177e := N(xe) at xe, we define RSE :=  \u2211_(e=1)^N ||\u0177e - yel|\u00b2 / \u2211_(l=1)^N ||ye||\u00b2. When D represents the training, validation, and testing data, RSE is specialized as TrRSE, VaRSE, and TeRSE, respectively.\nDetails of the numerical experiments conducted in this section, including computational resources, the network structure of SGDL and MGDL for each example, the choice of activation function, the optimizer, parameters used in the optimization process, and supporting figures are provided in Appendix B."}, {"title": "3.1 Regression on the synthetic data", "content": "In this experiment, we compare the efficacy of SGDL and MGDL in learning functions of four different types of high-frequencies.\nThe experiment setup is as follows. Given frequencies \u03ba := (\u03ba\u2081, \u03ba\u2082,..., \u03ba_M) with corresponding amplitudes a := (a\u2081, a\u2082,..., \u03b1_M), and phases \u03c6 := (\u03c6\u2081, \u03c6\u2082,..., \u03c6_M), we consider approximating the function \u03bb : [0, 1] \u2192 \u211d defined by\n\n\u03bb(x) := \u2211_(j=1)^M aj sin (2\u03c0 \u03ba_j x + \u03c6_j), x \u2208 [0, 1] (9)\n\nby neural networks learned with SGDL and MGDL. We consider four settings, in all of which we choose M := 20, \u03baj := 10j and \u03c6_j ~ U(0, 2\u03c0) for j \u2208 N\u2082\u2080, where U denotes the uniform distribution and the random seed is set to be 0. In settings 1, 2, 3, and 4, we choose respectively the amplitudes aj := 1, aj := 1.05-0.05j, aj(x) := e^(\u2212*) cos(jx), and aj := 0.05j, for j \u2208 N\u2082\u2080. Note that in setting 3, we explore the case where the amplitude varies as a function of x for each component. The amplitudes versus one-side frequencies for A across the four settings are depicted in Figure 7 in Appendix B.1. For all the four settings, the training data consist of pairs {xe, \u03bb(xe)}l\u2208N\u2086\u2080\u2080\u2080, where xe's are equally spaced between 0 and 1. The validation and testing data consist of pairs {xe, \u03bb(xe)}l\u2208N\u2082\u2080\u2080\u2080, where xe's are generated from a random uniform distribution on [0, 1], with the random seed set to be 0 and 1, respectively.\nNumerical results for this example are reported in Figures 2 and 3, as well as 8 and 9 (in Appendix B.1), and Table 1. Figure 2 displays the amplitude versus the one-side frequency of the functions learned across four grades of MGDL in settings 1-3, and five grades in setting 4. In all the four settings, MGDL exhibits a pattern of learning low-frequency components in grade 1, middle-frequency components in grade 2, and high-frequency components in grades 3, 4 (and 5 for setting 4). We let N_* := ND\u2081 (\u0398*\u03b9;) and H := HD\u2082\u2212\u2081(\u0398;\u00b7). The SNN N\u2081* learned in grade 1 represents a low-frequency component of \u03bb, the SNNs N\u2082 and N learned in grades 2 and 3, composed with H\u2081 and HH\u2081, respectively, represents higher-frequency components of \u03bb. Likewise, the SNN N\u2084*, learned in grade 4, composed with HH\u2082H\u2081 represents the highest-frequency component of \u03bb. This fact is particularly evident in setting 4 (see, the fourth subfigure in Figure 2), where the amplitude within the data increases with the frequency. For settings 1, 2, and 3, grade 4 does not learn much. This is because for the functions of these three settings, the amplitudes of higher-frequencies are proportionally small. However, grade 4 is still important for these settings. As shown in Figure 3 (right), grade 4 reduces the loss from 10\u207b\u00b2 to 10\u207b\u2075 for setting 1 and from 10\u207b\u2074 to 10\u207b\u2076 for settings 2 and 3. This indicates that if we want a high precision, we need to include grade 4. For setting 4, we need grade 5 to learn its highest frequency component. These findings suggest that MGDL is particularly well-suited for learning the high-frequency components of the function.\nFigure 3 compares the progress of the training and validation losses against the number of training epochs for SGDL and MGDL across the four settings. We observe that when learning a task involving high-frequency components by SGDL, the training and validation losses decrease slowly due to the spectral bias of DNN. While the same task is learned by MGDL, the learning process progresses through distinct grades. In grade 1, MGDL primarily learns low-frequency, resulting in a slow decrease in loss. In grade 2, the training loss and validation loss both decrease more rapidly due to the use of the composition of SNN N\u00bd with the feature H\u2081, facilitating in learning high-frequency features. This accelerated learning aspect of MGDL is further evidenced in grades 3 and 4 (as well as grade 5 for setting 4). Table 1 compares the accuracy achieved by SGDL and MGDL. Within a comparable or even less training time, MGDL increases accuracy, measured by TeRSE from 10\u207b\u00b9 to 10\u207b\u2075, 10\u207b\u00b3 to 10\u207b\u2076, 10\u207b\u00b9 to 10\u207b\u2075, and 10\u207b\u00b9 to 10\u207b\u00b3 in settings 1, 2, 3 and 4, respectively. Across the four settings, TeRSE values are reduced by a factor of 592 ~ 7,058. These comparisons highlight MGDL's advantage in effectively learning high-frequency oscillatory functions.\nFigure 8 in Appendix B.1 depicts the functions, in the Fourier domain, learned by SGDL (row 1) and MGDL (row 2) across the four settings, demonstrating that MGDL has a substantial reduction in the 'spectral bias' exhibited in SGDL. This is because high-frequency components are learned in a higher grade, where they are represented as the composition of a low-frequency component with the low-frequency components learned in the previous grades, and each grade focuses solely on learning a low-frequency component by an SNN. We also include Figure 9 in Appendix B.1 to compare the spectrum evolution between SGDL (1st row) and MGDL (2nd row) across settings 1-4. Notably, although in iterations SGDL and MGDL both learn low-frequency components first and then followed by middle and high-frequency components, MGDL learns in grade by grade, exhibiting significant outperformance."}, {"title": "3.2 Regression on the manifold data", "content": "The second experiment compares regression by SGDL and MGDL on two-dimensional manifold data, studied in [30] but with twice higher frequencies.\nThe goal of this experiment is to explore scenarios where data lies on a lower-dimensional manifold embedded within a higher-dimensional space. Such data is commonly referred to as manifold data [4]. Let y be an injective mapping from [0, 1]\u1d50 \u2192 \u211d\u1d48 with m \u2264 d and M := y([0, 1]\u1d50) denote the manifold data. A target function \u03c4: M \u2192 \u211d defined on the manifold can be identified with function \u03bb := \u03c4\u2218\u03b3 defined on [0, 1]\u1d50. Regressing the target function \u03c4 is therefore equivalent to finding f : \u211d\u1d48 \u2192 \u211d such that f\u2218\u03b3 matches \u03bb. Following [30], we set m := 1, d := 2, and choose the mapping y as\n\nyq(x) := [1 + sin(2\u03c0qx)/2] (cos(2\u03c0x), sin(2\u03c0x)), x \u2208 [0, 1],   (10)\n\nfor a nonnegative integer q. Clearly, \u03b3q : [0, 1] \u2192 \u211d\u00b2 and M := yq([0, 1]) defines the manifold corresponding to a flower-shaped curve with q petals when q > 0, and a unit circle when q = 0. Suppose that \u03bb: [0, 1] \u2192 \u211d is the function defined by (9). Our task is to learn a DNN f : \u211d\u00b2 \u2192 \u211d such that f\u2218\u03b3 matches \u03bb. We consider two settings for \u03bb. In settings 1 and 2, we choose aj := 0.025j and aj(x) := e^(\u2212x) cos(jx) for j\u2208 N\u2084\u2080, respectively. For both the settings, we choose \u03baj := 10j and \u03c6j ~ U(0, 2\u03c0) for j \u2208 N\u2084\u2080 with the random seed set to be 0, and consider the cases where q := 4 and q := 0. Note that the smaller q is, the more difficult the learning task is. The training data consists of pairs {\u03b3q(xe), \u03bb(xe)}l\u2208N\u2081\u2082\u2080\u2080\u2080, where xe's are equally spaced between 0 and 1. The validation and testing data consist of pairs {\u03b3q(xe), \u03bb(xe)}l\u2208N\u2084\u2080\u2080\u2080, where xe's are generated from a random uniform distribution on [0, 1], with random seed set to be 0 and 1, respectively.\nNumerical results for this example are reported in Figures 4-5, and 10 (in Appendix B.2), and Table 2. Figure 4 illustrates the frequency of functions learned across four grades of MGDL for settings 1 and 2, where q := 0. In both of the settings, MGDL exhibits a pattern of learning low-frequency components in grade 1, middle-frequency components in grade 2, and high-frequency components in grades 3 and 4. Therefore, the high-frequency components within the function mainly learned in higher grades, in which the learned function is a composition of the SNNs learned from several grades. That is, MGDL decomposes a high-frequency component as the composition of several lower-frequency components, facilitating effectively learning high frequency features within the data.\nTable 2 compares the approximation accuracy achieved by SGDL and MGDL for settings 1 and 2. For SGDL, reducing the value of q makes the learning task for both settings more challenging, due to the spectral bias of DNNs. When q := 4,0 in setting 1 and q := 0 in setting 2, learning becomes especially challenging for SGDL. In such cases, MGDL significantly outperforms SGDL by achieving higher accuracy in approximately half to one-third of the training time for both settings. Figure 5 displays the training and validation loss for SGDL and MGDL. Figure 10 illustrates the"}, {"title": "3.3 Regression on two-dimensional colored Images", "content": "In the third experiment, we compare performance of MGDL and SGDL for regression of two-dimensional color images.\nWe test the models with the 'cat' image from website Cat, and the 'sea' and 'building' images from the Div2K dataset [1]. The input to the models is the pixel coordinates, and the output is the corresponding RGB values. The training dataset consists of a regularly spaced grid containing 1/4 of the image pixels, while the test dataset contains the full image pixels. We use peak signal-to-noise ratio (PSNR) defined as in (12) to evaluate the accuracy of images obtained from MGDL and SGDL. PSNR values computed over the train and test dataset are denoted by TrPSNR and TePSNR, respectively.\nNumerical results for this experiment are presented in Table 3, Figure 6, and Figures 11-13 (in Appendix B.3). Table 3 compares the PSNR values of images obtained from MGDL and SGDL. For MGDL, it is evident that adding more grades consistently improves the image quality, as both training and testing PSNR values increase substantially with the addition of each grade across all images Cat, Sea, and Building. MGDL outperforms SGDL by 2.35 ~ 3.93 dB for the testing PSNR values. Specifically, for images Cat, Sea and Building, MGDL surpasses SGDL by 2.35, 3.93 and 2.88 dB, respectively. This demonstrates the superiority of MGDL in representing images compared to SGDL. Figure 6 illustrates the training and testing PSNR values during the training process for the three images. It is evident that MGDL facilitates a smoother learning process as more grades are added, in comparison to SGDL. This observation aligns with the results presented in Table 3.\nPredictions of each grade of MGDL and of SGDL are illustrated in Figures 11, 12, and 13 for images Cat, Sea, and Building, respectively. In all cases, grade 1 captures only the rough outlines of the images, representing the lower-frequency components. As we progress to grades 2, 3, and 4, more details corresponding to the higher-frequency components are added. This demonstrates the effectiveness of MGDL in progressively learning high-frequency features within the images. Moreover, the image quality achieved with MGDL is notably superior to that with SGDL."}, {"title": "3.4 Regression on MNIST data", "content": "In our fourth experiment, we apply MGDL to regress on high-dimension data, the MNIST data. We compare the training and validation loss, and TeRSE for SGDL and MGDL, when learning high-frequency features from the data.\nWe set up this experiment following [30], with a focus on comparing performance of SGDL and MGDL in learning high-frequency features. We choose the handwriting digits from MNIST dataset [21], composed of 60,000 training samples and 10,000 testing samples of the digits \"0\" through \u201c9\u201d. We represent each digit j = 0,1,..., 9 by a one-hot vector ej+1 \u2208 \u211d\u00b9\u2070, whose (j + 1)-th component is one and all others are zero, and denote by \u03c4\u03bf : \u211d\u2077\u2078\u2074 \u2192 \u211d\u00b9\u2070 the classifier, which is a piecewise constant function defined by \u03c4o(x) := ej+1 if x represents the digit j. We split the available training samples to form the training and validation data, denoted as D\u2080 := {xe, \u03c4o(xe)}leNntrain and D'\u2080 := {x'e, \u03c4o(x'e)}l\u2208Nnval, respectively, with ntrain := 45,000 and nval := 15,000, and use the testing samples as the testing data, denoted as D\"o := {x\"e, \u03c4o (xe) } l\u2208Nntest with ntest := 10,000.\nClearly, Do, Do' and D\" are subsets of {\u211d[0,1]\u2077\u2078\u2074, {ej+1}j=0^9}. Letting \u03c8_\u03ba(x) := sin(2\u03c0 \u03ba||x||\u00b2), corresponding to a radial wave defined on the input space \u211d\u2077\u2078\u2074, we define the target function by \u03a4\u03b2,\u03ba(X) := To(X) (1 + \u03b2\u03c8\u03ba(x)), where \u03ba is the frequency of the wave and \u03b2 is the amplitude. Note that \u03c4\u03bf and \u03b2\u03c8\u03ba contribute respectively the lower-frequency and high-frequency components (regarded as noise) of the target function \u03c4\u03b2,\u03ba, as discussed in [30]. The modified training and validation data denoted by DB,\u043a := {Xe, TB,K(xe)}leNntrain and D's := {x'e, TB,K(x'e)}l\u2208Nnval respectively, are used to train DNNs. Our goal is to use SGDL and MDGL to regress the modified data DB, through minimizing their respective training loss, to compare their robustness to noise. The training loss is evaluated on D\u03b2,\u03ba and validation loss is on D'\u03b2,\u03ba\u00b7 TrRSE, VaRSE, and TeRSE are evaluated on D\u03b2,\u03ba D'\u00df,\u03ba, and D\"\", respectively, noting that D' are test data without noise.\""}, {"title": "4 Conclusion", "content": "By observing that a high-frequency function may be well-represented by composition of several lower-frequency functions, we have proposed a novel approach to learn such a function. The proposed approach decomposes the function into a sum of multiple frequency components, each of which is compositions of lower-frequency functions. By leveraging the MGDL model, we can express high-frequency components by compositions of multiple SNNs of low-frequency. We have conducted numerical studies of the proposed approach by applying it to one-dimensional synthetic data, two-dimensional manifold data, colored images, and higher-dimensional MNIST data. Our studies have concluded that MGDL can effectively learn high-frequency components within data. Moreover, the proposed approach is easy to implement and not limited by dimension of the input variable. Consequently, it offers a promising approach to learn high-frequency features within (high-dimensional) dataset.\nLimitation: Mathematical understanding of the spectral bias of DNNs is absent. Theoretical foundation for MGDL to address the spectral bias issue needs to be established. Numerical studies are preliminary, limited to four examples. More extensive numerical experiments will be conducted in our future work."}, {"title": "A Analysis of the Multi-Grade Deep Learning Model", "content": "It was established in [43] that when the loss function is defined in terms of the mean square error, MGDL either learns the zero function or results in a strictly decreasing residual error sequence in each grade.\nTheorem 1. Let D be a compact subset of \u211d\u02e2 and L\u2082(D, \u211d\u1d57) denote the space of t-dimensional vector-valued square integral functions on D. If f \u2208 L\u2082(D, \u211d\u1d57), then for all i = 1, 2, . . .,\n\nf = \u2211_(i=1)^I fi + ei, fi := N*_i \u2218 \u00d1_(i-1)\u2218\u00a8\u00a8\u00a8\u2218\u00d1*_1.\n\nwhere N*_i is the SNN learned in grade l of MGDL, and for i = 1, 2, . . ., either fi+1 = 0 or\n\n||e_(i+1)|| < ||ei||.\n\nAll numerical examples presented in this paper validate Theorem 1."}, {"title": "B Supporting material for Section 3", "content": "We provide in this appendix details for the numerical experiments in Section 3", "30": "for all the four experiments. For each experiment", "20": "with \u2018Xavier' initialization [13", "19": "calculated as tk := tmaxe^(\u2212\u03b3k)", "\u03b3": 1}]}