{"title": "Venire: A Machine Learning-Guided Panel Review System for Community Content Moderation", "authors": ["Vinay Koshy", "Frederick Choi", "Yi-Shyuan Chiang", "Hari Sundaram", "Eshwar Chandrasekharan", "Karrie Karahalios"], "abstract": "Research into community content moderation often assumes that moderation teams govern with a single, unified voice. However, recent work has found that moderators disagree with one another at modest, but concerning rates. The problem is not the root disagreements themselves. Subjectivity in moderation is unavoidable, and there are clear benefits to including diverse perspectives within a moderation team. Instead, the crux of the issue is that, due to resource constraints, moderation decisions end up being made by individual decision-makers. The result is decision-making that is inconsistent, which is frustrating for community members. To address this, we develop Venire, an ML-backed system for panel review on Reddit. Venire uses a machine learning model trained on log data to identify the cases where moderators are most likely to disagree. Venire fast-tracks these cases for multi-person review. Ideally, Venire allows moderators to surface and resolve disagreements that would have otherwise gone unnoticed. We conduct three studies through which we design and evaluate Venire: a set of formative interviews with moderators, technical evaluations on two datasets, and a think-aloud study in which moderators used Venire to make decisions on real moderation cases. Quantitatively, we demonstrate that Venire is able to improve decision consistency and surface latent disagreements. Qualitatively, we find that Venire helps moderators resolve difficult moderation cases more confidently. Venire represents a novel paradigm for human-Al content moderation, and shifts the conversation from replacing human decision-making to supporting it.", "sections": [{"title": "1 Introduction", "content": "In many online communities, a team of volunteer moderators is responsible for creating and enforcing rules to govern acceptable speech. Although rules are often collectively set by the moderation team, moderators act more independently when enforcing them within the community. This implies a tension inherent to moderation efforts. While community policy reflects a moderation team's shared values, the day-to-day practice of this policy relies on individual decision makers. Prior work has found that moderators themselves disagree over how to apply community rules at concerning rates audits of moderator decision-making have found disagreement rates as high as 16% [21] and 23% [25]. This raises concerns that moderators on the same team may be enforcing rules inconsistently. Such inconsistency can be frustrating for community members. If users see that their post has been removed while similar posts remain unmoderated, they may feel unfairly singled out or targeted [18, 26]. Further, inconsistent rule application makes it harder for community members to learn a group's norms. Naively, one might try to increase decision consistency by requiring every moderation case be reviewed by a panel of moderators [29]. In practice, this is infeasible. Universal panel review creates far too much extra work for moderation teams, which are already stretched thin [4, 24]. In this paper, we explore an alternative approach. Instead of mandating universal panel review, we build Venire\u00b9, a machine learning-guided panel review system.\nVenire acts as an overlay for Reddit's existing moderation queue, and gives moderators the ability to manually flag cases for panel review. When a case undergoes panel review, its final outcome is determined by a vote amongst multiple moderators rather than by a single decision-maker. Venire is backed by an ML model which uses moderation log data to predict how each team member would respond to an incoming case. Venire recommends a case for panel review when it predicts the moderation team is likely to disagree over how to handle it. Ideally, Venire helps moderation teams surface latent disagreements, while keeping the increase in moderator workload to a minimum. We present a series of three studies through which we design, build, and evaluate Venire. First, we conducted preliminary interviews to assess whether our intended goals for Venire were aligned with moderator needs.Second, we performed two technical evaluations to ensure that Venire could deliver on the promising of increasing decision consistency and surfacing latent disagreements. Finally, we investigated how moderators use the system in practice by performing think-aloud study. In the think aloud study, moderators used Venire to make decisions on real moderation cases pulled from the r/ChangeMyView subreddit.\nOur initial interviews revealed that moderators were open to the idea of using a system like Venire. Moderators appreciated having a potential built-in channel as an alternative to informal deliberation processes already occurring within their communities. Moderators anticipated that the ML panel recommendations could help surface disagreements that were being missed. We also surfaced potential benefits we had not anticipated. Most critically, moderators felt Venire had value as an onboarding tool for new moderators. Because Venire provides a preliminary assessment of how contentious a case will be, it can avert controversial rulings from a new moderator, and increase a newbie moderator's confidence when handling straightforward cases.\nIn our technical evaluations, we conducted simulation-based analysis on two dataset to assess the quality of our ML model's panel recommendations. Using a large, publicly available toxicity dataset [22], we demonstrate that our predictive model can effectively triage moderation cases for panel review: we are able to approximate the decision-consistency benefits of universal panel review while assigning only a third cases of moderation cases to a panel. To make our analysis more ecologically valid, we constructed a smaller dataset containing labels from Prolific crowdworkers. To improve ecological validity, crowdworkers were asked to enforce an actual moderation rule from r/ChangeMyView, a popular Reddit community. Even on this smaller dataset, our model was able to anticipate controversial moderation cases about as accurately as a human rater. However, panel allocation was less efficient compared to in the toxicity dataset-it took assigning 60% of all cases to a panel to approximate the decision consistency of universal panel review. Still, our results with both datasets indicate that model-assigned panels significantly outperformed random panel assignment at improving decision-consistency and surfacing disagreements.\nFinally, we tested Venire's capabilities as an onboarding tool in our think-aloud study with moderators. We had moderators from other communities enforce a rule from r/ChangeMyView,"}, {"title": "2 Background", "content": "The goal of this work is to develop a human-AI workflow for surfacing disagreements over subjective content moderation cases. As such, we present a review of prior work on: existing practices amongst community moderators for establishing a consistent moderation policy, prior attempts to build human-AI moderation tools, and machine learning approaches for modeling subjective decision-making."}, {"title": "2.1 Community Practices for Improving Consistency", "content": "Most commonly, moderators of online communities create sets of shared guidelines for rule en-forcement to ensure the team acts in a consistent matter [4, 6, 8, 21, 32\u201334]. In interviews, both Seering et al. [33] and Cullen and Kairam [6] find that these guidelines are developed iteratively over a community's lifespan, often in response to specific incidents where moderators felt a user crossed the line. Policy tends to be set collective by mod teams [6, 8, 32, 33], though moderators sometimes take community input, or defer to a \u201chead\u201d moderator [6, 33]. Occasionally, disputes over such policy can cause communities to fracture, splitting off into separate groups [8]. Even with an informal policy in place, moderators still consult with another when they are unsure how to handle a particular case [6, 33]. Cullen and Kairam [6] notes that such incidents can reveal places where moderators' mental models of how a rule should work, or even core values, are misaligned, creating an opportunity for reflection.\nStill, not every community's moderation team revolves around a comprehensive, iteratively developed policy. For example, Seering et al. [33] found that at least one moderator was told to \u201cdo whatever you feel makes the [community] better\u201d when they were made a moderator. Similarly, across studies, many interviewees report experiencing limited onboarding when they started as moderators, instead relying on more implicit processes to develop a feel for the community norms [6, 33, 34].\nTo our knowledge, only two studies have tried to directly assess how often moderators disagree with one another [21, 25]. The studies had moderators review sets of comments previously posted to two different large, discussion-based subreddits [21, 25]. For comments that received two in-study annotations, Lucas et al. [25] found a disagreement rate of 23% (Fleiss' \u03ba = 0.46, N = 222), while Koshy et al. [21] found a disagreement rate of 13% (N = 134). When comparing in-study labels to real life outcomes, the rates of disagreement were 28% (N = 246 annotations across 134 comments) and 26% (N = 1020 annotations across 798 comments) respectively. The present work is primarily motivated by the prevalence of content moderation disagreements found in these two studies, in spite of the existing measures that online communities take to ensure consistency."}, {"title": "2.2 Human-Al Content Moderation Tools", "content": "Researchers in the CHI and CSCW communities have built a number of human-in-the-loop AI systems for content moderation [15]. These tools can be categorized as facilitating either top-down [4, 5, 13, 14, 23, 31] or personalized content moderation [17, 19]. Although tools within each category tend to follow similar design patterns, they differ substantially in terms of the task the underlying machine learning model is trained on, the way model predictions are presented in an interface, and the place in the moderation process human decision-making is utilized. Because our work falls under the umbrella of top-down content moderation tools, we provide a more detailed review of prior top-down approaches."}, {"title": "2.2.1 Al tools for top-down moderation", "content": "Typically in top-down tools, a machine learning model is used to flag content for a human moderator to review-moderator decisions in turn affect all users within a given community or platform [4, 5, 11, 13, 14, 23, 31]. Amongst top-down tools, one of the key differentiating factors is the source of the training data. One approach is to use a generalized model. For example, in building a tool for Discord moderation, Choi et al. [5] use Perspective API.\nThe Perspective model is trained using crowdworker toxicity labels on comments across multiple social media platforms. In contrast, to build a tool for Reddit moderation, Chandrasekharan et al. [4] use historical data scraped from Reddit to train an ensemble of models that predict whether comments in specific communities will get removed or not. Halfaker and Geiger [13] adopt the most bespoke approach, creating a \u201cWiki Labels\u201d system through which Wikipedians can contribute training data labels to specific model development requests.\nNotably, almost all existing tools have been built with the goal of either reducing moderator labor, or helping moderators identify norm violations they would have otherwise missed [4, 5, 13, 14]. Our approach, using machine learning models to improve the consistency of human decision-making, is relatively unique in this regard.\nStill, a few other researchers have also built tools that center disagreement in the content moderation process [11, 23]. In developing the jury learning framework, Gordon et al. [11] argue that rather than predicting a majority vote or average label across annotators, machine learning models should be trained to predict a label for each annotator in the dataset, using the annotators ID and demographic information as features. They create an interface on top of a model trained in this that allows the end user to choose which voices in the training dataset to amplify within their communities' automated tool. Kuo et al. [23], whose work is perhaps most similar to our own, develop a tool that allows communities to curate evaluation datasets for AI tools they might want to adopt. As community members annotate data points, cases with disagreements are prioritized to receive additional ratings. Although our work shares a similar focus on contentious moderation cases, the goal of our work is to use a model to allocate panels efficiently, rather than to use panels to more accurately evaluate a model."}, {"title": "2.3 Modeling Subjective Decision-Making", "content": "Gordon et al's jury learning framework [11] is part of a broader trend amongst HCI and machine learning researchers, recognizing that modeling individual annotator beliefs can be beneficial for subjective tasks [3, 9, 10, 30]. This viewpoint, sometimes referred to as \u201cperspectivism,\u201d is accompanied by varying motivations. Drawing on feminist theory, Blackwell et al. [2] argue that traditional majority vote aggregations of annotator labels can reinforce the viewpoints of dominant social groups. Other researchers appeal to more technical benefits of perspectivist approaches: that they more accurately represent the data generating process [3, 10], that they provide the ability to capture uncertainty in training labels that arise from human variation [3, 10, 11], and that they afford end users more flexibility of models [3, 11]. We provide a brief, non-exhaustive, review of prior perspectivist modeling approaches and applications."}, {"title": "2.3.1 Direct Disagreement Prediction", "content": "One approach adopted in prior work is to directly model the variance in annotator labels as a function of features of the training instance (i.e., without using annotator features) [12, 28]. Gurari and Grauman [12] apply this approach to visual question"}, {"title": "2.3.2 Annotator-Aware Approaches", "content": "In contrast to direct disagreement prediction, annotator aware approaches utilize annotator-level features when making predictions [9, 11, 22, 27, 36]. These features almost always include an annotator ID, for which corresponding embeddings are learned [9, 11, 27, 36]. Prior work differs on exactly how embeddings are learned, and where embeddings are incorporated in a neural architecture. Demographic information for each rater is also sometimes utilized [9, 11], though ablation analysis from [11] found limited additional value to using these features in a toxicity detection task.\nPractically speaking, an important distinction between direct disagreement prediction and annotator-aware approaches is the type of training data needed. Direct disagreement prediction requires multiple labels per training instance, but does not require any annotator identifiers features. In contrast, annotator aware approaches generally require annotator identifiers, but do not require multiple labels per training instance."}, {"title": "2.4 Towards Venire", "content": "The existing literature demonstrates that even well-intentioned moderation teams may suffer from undetected consistency issues [21, 25], and that such consistency issues negatively affect user experience [18, 26]. Further, training a machine learning model to predict disagreements appears to be technically feasible [11, 12], making it possible to build the predictive model underlying Venire. However, we believe there are a few major open questions that need to be answered before building and testing Venire. First, how do moderators view the labor-consistency tradeoff inherent to panel review? Would they deem it worthwhile to increase the amount of decisions that need to be made in order to catch potential disagreements? And how could a panel review system complement existing moderation practices? Second, even if disagreement prediction is possible in some cases, is it possible to implement for a realistic content moderation task and with the kinds of data available in a subreddit's moderation log? These questions motivated our decision to conduct two preliminary studies leading up to building and evaluating the Venire interface."}, {"title": "3 Preliminary Interviews: Does Venire Support Moderator Needs?", "content": "We conducted a round of preliminary interviews with moderators to better understand how they would view the labor demands of panel review. More broadly, we wanted to surface moderators' general attitudes towards the idea of an ML-assisted panel review system, and better understand moderators' existing processes for improving decision consistency. Simply put, our goal was to evaluate whether our vision for Venire aligned with moderator needs. We summarize these interview objectives in the following research questions:\nRQ1a: How important do moderators think decision consistency is? What factors do they weigh it against?\nRQ1b: What processes do moderators currently employ to improve decision consistency?"}, {"title": "3.1 Initial Prototypes", "content": "To ground our interviews, we created a workflow diagram to communicate the idea behind our system (Figure 1). We also created two interactive interface mock-ups in Figma (Figure 2), representing more and less rigid versions of the panel voting process. In order to minimize disruptions to existing moderator workflows, we based the interface mock-ups heavily on the existing Reddit moderation queue interface."}, {"title": "3.1.1 System Mock-up #1: Strict Voting", "content": "Figure 2 contains the first version of the system we presented to moderators. In this version of the interface, moderators make two decisions for each comment: whether it should be removed or not, and whether it should undergo panel review or not. Moderators also see the model-generated recommendation for panel review based where appropriate. Clicking the \"More info\" button allows the user to see a full breakdown of how the model predicts the moderation team will react to the comment (Figure 3). If the moderator chooses to flag a case for panel review, it will remain in the queue, until k moderators cast a vote on it, where k is a configurable number. After the kth vote is input, a decision will be determined based on the majority vote amongst moderators. Note that any moderators on the subreddit's mod team are allowed to weigh in on a panel decision-panel votes are not solicited from specific moderators. To minimize bias, moderators are unable to see the direction of existing votes until after they vote themselves."}, {"title": "3.1.2 System Mock-up #2: Suggested Actions", "content": "Rather than enforcing a majority vote across k raters, the second interface treats panel review more loosely. All moderators are given the ability to signal their preference for removal or approva on the interface .There is no built-in aggregation mechanism for these suggestions. Instead, aat any time, a moderator can choose to make the final removal/approval decision, taking into account the other suggested actions as they see fit. When cases are predicted to be controversial, moderators are advised to make a suggestion rather than an immediate decision. This version of the system focuses on making the opinions of different moderators visible to one another, and aims to disrupt the existing moderation workflow as little as possible."}, {"title": "3.2 Interview Protocol", "content": "Interviews were conducted over Zoom and lasted around 60 minutes each. Each interview consisted of three parts. First we asked moderators to describe the rules of the subreddit they moderate. We then asked moderators to recall any experiences they have had with disagreements over how to enforce the rules in their community, steps they have taken to ensure decision consistency (RQ1b), and general attitudes towards decision consistency as a goal for moderation (RQ1a). Next, we presented the workflow diagram to moderators to give them a high-level idea of how the system would work. Moderators were asked to speculate about the strengths and weaknesses of the system (RQ1c) and whether they thought it would be helpful in their community. If moderators did not explicitly mention concerns about increased workload here, we prompted them about it.\nFinally, participants were given a guided tour of the two interactive mockups. Afterwards, we asked moderators to contrast the two interfaces, and state which one they preferred. We also solicited lower-level design feedback and asked moderators whether there were any additional benefits or harms from using the system they could imagine. Moderators were compensated the equivalent of $30 USD in their local currency via Amazon giftcard or PayPal."}, {"title": "3.3 Recruiting", "content": "Recruitment messages were sent to Reddit moderation teams via the platform's modmail feature. Messages were sent to subreddits with over 10,000 subscribers and at least one subjective rule listed in the subreddit's sidebar. The lead author manually curated a list of subreddits based on these criteria. The r/ChangeMyView subreddit was also directly contacted since the researchers had access to a dataset of moderation decisions from this community that could later be used to train Venire's machine learning model. Several measures were taken to avoid spamming moderators with recruitment messages, which we detail in the Appendix A."}, {"title": "3.4 Results", "content": "3.4.1 Attitudes Towards Decision Consistency (RQ1a). Unsurprisingly, almost all the moderators we interviewed believed moderation decision consistency to be desirable (N=6). P3, speaking directly to the motivations of the project, stated \u201cin an ideal world [...] every post would go through panel review.\u201d This was especially the case for subreddits that dealt with highly sensitive or political topics (N=3). P1, who moderates r/ChangeMyView, argued that decision consistency was essential to user participation in their community:\nStill, most moderators contextualized the importance of decision consistency alongside other factors (N=5). Minimizing workload and stress came up most frequently (N=4). With regards to stress, P5 said that moderation could sometimes feel like \u201cfactory work\u201d, and that it was important to \"get through the queue\u201d with the \u201cleast amount of damage to yourself.\u201d Other moderators highlighted the voluntary nature of moderation work when thinking about the consistency-workload tradeoff. P2 argued that \u201cwhoever's the person shouldering most of the workload gets to make the calls in part because those moderators would be \u201cmore in tune with how the subreddit currently is.\u201d Similarly, P4 said \u201cwe really try to back up our moderators as much possible in their decisions, even if it's something we might disagree with.\u201d\nOutside of workload concerns, P3 mentioned that incorporating diverse perspectives into the moderation team could be worth sacrificing some decision consistency for. Additionally, P2 noted that more senior moderators were sometimes able to take actions within the community that other moderators were not able to. This is a form of decision inconsistency that was viewed positively. They described these moderators as \u201chaving a bit more goodwill\u201d amongst community members, which allowed them to \u201cshut something down\u201d where another moderator might not have been able to."}, {"title": "3.4.2 Existing Practices (RQ1b)", "content": "Every moderator we interviewed described taking steps in the past to either preempt a potential disagreement (N=8) or resolve a disagreement that surfaced after a moderation action had already been taken (N=5). Most moderators recalled soliciting a second opinion from another moderator through a side channel like Discord (N=5). This practice was sometimes specifically encouraged for new moderators (N=2). Moderators also described holding discussions prior to implementing a new rule to try to iron out potential inconsistencies (N=3). In a few cases, moderators outlined specific rules or policies where taking a vote amongst multiple mods was required (N=2). In P1's community, certain kinds of post removals \u201crequired two moderators to sign off on.\u201d\nStill, most moderators felt disagreements were relatively rare to begin with (N=5). At first glance, this was surprising given the disagreement rates found in prior work. However, the moderators we interviewed attributed the rarity of disagreements to the fact that their subreddits only had a few moderators (N=2) or to the fact that their community's rules were straightforward (N=2). In contrast, the subreddits studied in prior work had large moderation teams, and tight moderation standards. Still, P1 mentioned that disagreements could be going unnoticed in their community, stating: \u201cif a moderator does make a decision, very rarely are the other moderators going to even be aware of it.\""}, {"title": "3.4.3 Potential Benefits and Harms of ML-Guided Panel Review (RQ1c)", "content": "Moderators outlined a number of potential benefits to a panel review system. Even without the predictive model, moderators appreciated having a built-in channel for disagreement-resolution that might have otherwise happened in a side channel (N=5). And as we anticipated, a number of moderators felt that a machine learning model could help surface disagreements that would have gone unnoticed (N=6). P1, for example, said \u201cI would say this tool would be great for helping to figure out [...] if there are controversial cases that are being decided too quickly.\u201d A few moderators explicitly stated that catching these disagreements could lead to policy updates (N=3). P7 was one such moderator:\nWe also surfaced a few unanticipated benefits. Most notably, moderators highlighted the benefits of ML-guided panel review as an onboarding tool (N=4). In the context of recruitment, P2 argued that this would be beneficial to both senior and new moderators. For the senior moderators, P2 felt that they \"won't feel like they have to be double checking or [...] correcting them all the time.\u201d and for the moderators being onboarded \u201cthey can be a little bit more confident that if they take an action and it turns out to be a mistake, that it will be caught. Like a safety net.\u201d P5 echoed this sentiment saying \u201cI do think that you'll have people more willing to moderate in general. So right now when you recruit moderators, the stickiness of a moderator is not high. [...] It could be that when they run across these difficult to moderate content, they don't know what to do.\u201d P3 contrasted using the panel review system to their current practices for onboarding new moderators, saying:\nHowever, moderators mentioned a number of drawbacks that are important to consider. Workload concerns came up multiple times (N=5), especially with respect to false positive flags from the ML model (N=3). At the same time, moderators felt like the workload could be manageable if disagreements were relatively rare (N=4). One participant, P5, questioned the fundamental value of highly deliberative moderation at all, saying:\nModerators also felt that panel review could increase the time it takes for a final decision to be reached on a post (N=3). P6, for example, said \u201cif it takes three days to get an answer [...] the post is gone [...] it doesn't really even matter anymore.\u201d However, P2 offered a potential solution saying the system could instead be \u201cmore of an appeals process that would let you reverse a decision [...] rather than something that might block you from taking action.\u201d\nFinally, a few moderators speculated that the panel review feature might simply go unused (N=4). P3 and P6 both argued that moderators may not have the self-awareness to flag a case for panel review. P6 stated \u201cit takes a certain level of person to say 'I think it's possible that I didn't make the best decision here.' \u201d P3 argued one solution might be to have the ML model force panel review rather than merely suggesting it. Other moderators simply felt that moderators within the same"}, {"title": "3.4.4 Mockup Preference", "content": "Moderators largely preferred the first strict voting mock-up to the suggested action mockup (N=5 vs N=1). Moderators who preferred the strict voting mock-up liked the rigidity of the voting process. These moderators described the strict voting mock-up as \u201cmore formal\" and the suggestion action mock-up as \u201cmore passive\u201d and \u201csofter.\u201d P2, the sole moderator who preferred the suggested action mockup argued that they liked the \u201cnon-binding\u201d nature of the suggested action button. They felt the voting system \u201cputs a bit more pressure on you\u201d because \u201cif you're the second person voting, you might or might not be making a moderator action, you don't know.\""}, {"title": "3.4.5 Summary of Findings", "content": "Overall, we find that Venire's intended goals, surfacing disagreements and improving decision consistency, are aligned with moderators' values. Still, moderators reported a number of additional factors, like stress management and decision speed that these goals should be weighed against. Although moderators reported employing practices to improve decision consistency already, they saw potential for Venire to further these efforts. Thus, our findings gave us the confidence to proceed with building Venire."}, {"title": "4 Technical Evaluation: Can Venire Improve Decision Consistency?", "content": "In this section, we present the results of two experiments that test the technical feasibility of predicting moderation disagreements. Our primary goal is to demonstrate that ML-guided panel review can deliver on the promise of improving decision consistency and surfacing disagreements. In our first experiment, we demonstrate this using a large, publicly available toxicity dataset. In our second experiment, we construct a new, more ecologically valid dataset using crowdworkers on Prolific, and repeat our analysis. Crucially, the training dataset used in the second experiment is much smaller, and based on a more realistic content moderation task. We find that our model performs worse on this new dataset, but still allocates panels more effectively than a random panel assignment baseline. More concretely, the goal of our technical evaluations is to answer the following research questions:\nRQ2a: To what extent can an ML-guided panel review improve the consistency of moderation decisions?\nRQ2b: To what extent can an ML-guided panel review surface disagreements between moderators?\nRQ2c: How does the performance of an ML-guided panel review system change when we move from an ideal dataset to one that more closely matches moderation log data?"}, {"title": "4.1 Modeling Approach", "content": "Our modeling approach is inspired by recent perspectivist NLP papers [9, 11, 27, 36]. We define the prediction task as follows. As input, the model is shown a text x\u2081 and the identity of a rater j. Our model then outputs a prediction for how rater j would label that x\u012f. Prior work varies on the specific neural architecture used. In preliminary experiments, we found adapting Yin et al's approach to be most effective [36]. We treat each rater ID, j, as a special token that is prepended to xi. We feed this new string into a BERT model, and pass the contextual embedding for the rater ID token through a feedforward layer to produce the final prediction. Section 4.1 demonstrates our proposed architecture. Like other perspectivist approaches [9, 11] we learn rater-specific embeddings to capture attributes of decision-making style. This embedding is passed through multiple transformer layers alongside the text, to produce a final contextual embedding which captures attributes of the text and the rater.\nDuring deployment, our model makes a prediction about how each possible rater would label a xi. These predictions can be aggregated to produce: 1) a \u201cmajority vote\u201d prediction, M(x\u012b), of the fraction of raters that would label x\u012f as positive, and 2) a \u201cdisagreement score\u201d D(xi), that captures how controversial x\u2081 is. To produce D(x\u012b), we calculate the empirical probability that two randomly selected rater predictions disagree with one another [28]. It is natural to ask why we go through the process of producing a rater-specific prediction in the first place\u2014an alternative approach might be to simply train a model that learns D(x\u012b) directly. In the context of our study, this alternative approach is infeasible, since it requires us to have multiple rater labels for each comment in the training dataset; real moderation log data is largely singly-labeled."}, {"title": "4.1.1 Hyperparameters", "content": "We use a base BERT model for the toxicity prediction task and a BERT-large model for the Prolific study. Because the comments for the toxicity prediction task are short, we use only the first 126 tokens of the text for prediction. For the Prolific study, we use a 256-token window. When comments exceed the token window, we create an embedding for each 256-token slice of the comment and combine them via max-pooling to produce a final token embedding. Additionally, for the Prolific task, we also feed the model the first 256 tokens of text of the immediate parent comment for reply comments, and the first 256 tokens of the post body for top-level comments. Unfortunately, this additional thread context was not available in the toxicity dataset. We separate the target comment's text from the thread context using BERT's SEP token. Our model is implemented in PyTorch and trained with the AdamW optimizer-we use a learning rate of 2e-5, batch size of 32, and weight decay of 0.0075. We train for 3 epochs on the toxicity dataset and 5 epochs on the Prolific dataset. The hyperparameters were determined after conducting a modest grid search using a validation set. All model training was conducted on a single Nvidia T4 GPU provided by Google Colab."}, {"title": "4.2 Experiment 1: Toxicity Dataset", "content": "4.2.1 Dataset Description. For our initial evaluations, we leverage a large, multiply-labeled toxicity dataset provided by Kumar et al. [22]. This dataset contains 107620 comments sampled from Twitter, Reddit and 4chan. Each comment received five, 5-point Likert ratings of toxicity. A total of 17280 raters contributed to the dataset, and each rater labeled at least 20 comments. For our evaluation, we treat toxicity prediction as a binary classification problem, treating Likert ratings of 3 (\"moderately toxic\") or higher as \"Toxic\", and 2 (\"slightly toxic\") or lower as \"Not toxic\" [22]. The labels in this dataset are mildly imbalanced-only 29% of supplied ratings were \"Toxic\", and the majority vote was \"Toxic\" for only 21% of comments. We use 75% of the comments in the dataset for training, 10% for validation, and reserve 15% for reporting results."}, {"title": "4.2.2 Toxicity Predictions", "content": "The first two rows of Table 2 demonstrate our model's ability to predict toxicity labels. Our model achieves an accuracy of 82% (AUROC 0.88) when predicting individual annotator ratings (using a threshold of 0.5), and 86% (AUROC 0.92) when predicting the majority vote amongst all five annotators. Majority vote predictions are made by producing a binary prediction for each of the five annotators and taking a majority vote amongst predictions."}, {"title": "4.2.3 Disagreement Predictions", "content": "To test our model's ability to predict disagreements amongst raters, we divide comments in the test set into two groups-high consensus comments (decided unanimously or with a single dissenting rater) and low consensus comments (decided by a 3/2 split amongst the raters). Under this definition, 30% of comments were considered low consensus. The goal of the disagreement prediction task is to discriminate between these two classes. We produce two kinds of predictions for comment consensus-level: \"annotator blind\" predictions and \"annotator aware\" predictions. As the name suggests, annotator-blind predictions are produced without looking at the identities of the five raters who were assigned to the comment. Instead, we randomly sample 100 annotators from the training set, and produce a prediction for each one. We then aggregate these 100 predictions into a single disagreement score (as described in Section 4.1), and threshold the disagreement score to produce a final binary consensus-level prediction. This threshold is chosen by calculating the disagreement score at which a low consensus outcome is more likely than a high consensus outcome, assuming the binary annotator-level predictions are correct. To produce annotator-aware predictions, we perform the same procedure, but use the 5 raters actually assigned to the comment rather than a random sample. The second two rows of Table 2 contain the results-annotator-blind predictions are 70% accurate (0.69 AUROC), while annotator-aware predictions are 73% accurate (0.74 AUROC)."}, {"title": "4.2.4 Simulation Analysis", "content": "While the previous results give us insight into our model's raw predictive power", "input": "xi, hinitial (xi), and a list of model-generated predictions fj(x) for how each human rater j will respond to xi. For each case, a panel allocation strategy"}]}