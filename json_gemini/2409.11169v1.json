{"title": "MAISI: Medical AI for Synthetic Imaging", "authors": ["Pengfei Guo", "Can Zhao", "Dong Yang", "Ziyue Xu", "Vishwesh Nath", "Yucheng Tang", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "abstract": "Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 \u00d7 512 \u00d7 768) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.", "sections": [{"title": "1. Introduction", "content": "Medical imaging analysis has been integral to modern healthcare, providing critical insights into patient diagnosis, treatment planning, and monitoring. The rapid advancement of machine learning (ML) approaches has revolutionized diagnostic and therapeutic practices in modern healthcare. However, the development of effective ML models in this domain continues to face the following significant challenges [34, 40, 77]: (1) data scarcity: the rarity of certain medical conditions (e.g., certain types of cancer, and rare diseases) complicates the data acquisition process, which leads to the limited acquired data that might not adequately represent the diversity of real-world cases. (2) high human-annotation costs: annotating medical images, such as MRI and CT scans, is inherently more expertise-demanding than annotating objects in general images. Medical images often contain subtle features that are critical for accurate diagnosis and treatment. Expert knowledge is usually required to accurately identify and annotate these conditions. (3) privacy concerns: conventional data acquisition and processing of medical images often require access to large volumes of patient data, which raises ethical concerns and poses significant logistical challenges due to the sensitive nature of patient information.\nTo address these limitations, generating synthetic data has emerged as a promising direction. By creating artificial yet realistic medical images, synthetic data can augment existing datasets, reduce the dependency on real patient data, and provide a cost-effective alternative to manual data annotation. With the recent advancement of the generative model, many novel approaches, such as generative adversarial networks (GAN) [21] and Diffusion Models (DM) [29], have been extensively studied for their capacity to generate photo-realistic images in various tasks in general computer vision society. In the context of medical image generation, several generative models have been successfully applied for medical image synthesis, such as multi-contrast MR/CT image synthesis [22, 33, 61], cross-modality image translation [9, 15, 56, 65, 75], and image reconstruction [14, 46, 64, 76].\nHowever, several key challenges are not fully explored in previous studies. First, realistic high-resolution (larger than the volume dimension of 5123) 3D volume generation is still a challenging task due to the huge memory consumption imposed by unified 3D frameworks, which must handle the vast amount of data involved in such high-dimensional representations [58]. Overcoming this memory bottleneck is essential for advancing the realism and applicability of 3D volume generation in clinical contexts. Second, the constraint of fixed output volume dimensions and voxel spacing poses substantial limitations in real-world applications [11]. These parameter presets are often incompatible with the diverse requirements of different tasks, such as the analysis of varying anatomical structures. The ability to dynamically adjust both the volume dimensions and the voxel spacing is crucial for enhancing the flexibility and utility of 3D generative models. Third, another common limitation of current generative models for medical image generation is their specialization to dedicated datasets or particular types of organs. These models, once trained, are typically not generalizable beyond the specific data and target organ they are developed on, which restricts their broader application in diverse settings. Developing more versatile models that can adapt to multiple datasets and organ types and mitigate the need for extensive retraining is a key objective for advancing the field [10].\nIn this paper, we propose a method, namely Medical AI for Synthetic Imaging (MAISI), a new framework for high-resolution 3D CT volume generation, which consists of three 3D networks including two foundation models (i.e., a volume compression network, a latent diffusion model [49]) and a ControlNet [70] for versatile generation tasks. Volume Compression Network is trained on a large amount of data (i.e., 39,206 3D CT volumes) and is responsible for compressing the 3D medical images into latent space and mapping the generated latent features back to image space by a visual encoder and a visual decoder, respectively. To reduce the memory footprint, we introduce the tensor splitting parallelism (TSP) inspiring from the tensor parallelism technique [57], originally proposed for linear layers, to the 3D convolutional layers allowing for the encoding and decoding of high-resolution CT volumes in a unified 3D network. The latent diffusion model in MAISI facilitates the creation of realistic latent features of 3D medical images. Benefiting from a compressed latent space with flexible dimensions and taking body region and voxel spacing as conditions, it enables the generation of complex anatomical structures with a high degree of fidelity while maintaining relatively low memory consumption. The latent diffusion model is trained on 10,277 CT volumes from diverse datasets, encompassing various body regions and disease conditions to enhance its generalizability and robustness, which enables the model to capture the knowledge represented in a wide range of clinical scenarios. Further, the integration of ControlNet [70] into the MAISI framework introduces a mechanism for dynamic control over the generated outputs. This component enhances MAISI's versatility and applicability across a wider range of tasks (e.g., conditional generation based on segmentation masks, as illustrated in Fig. 1, image inpainting, etc.). Additionally, this capability minimizes the need for extensive retraining of the two underlying foundation models when transitioning between different tasks or clinical objectives, thereby conserving both time and computational resources.\nTo summarize, this paper makes the following contributions:\n\u2022 A novel framework, MAISI, for high-resolution 3D CT volume generation is proposed, which enables the versatile generation of synthetic CT images.\n\u2022 Tensor splitting parallelism (TSP) is introduced to 3D convolutional networks. To the best of our knowledge, MAISI is the first attempt to generate realistic 3D CT images larger than 5123 voxels.\n\u2022 MAISI provides dynamic control over outputs, enabling annotated synthetic images to improve downstream task performance."}, {"title": "2. Related Work", "content": "Medical image synthesis has become an increasingly prominent research area, particularly in response to the challenges discussed in Sec. 1. Early approaches [7, 43, 51] to medical image synthesis were predominantly based on traditional image processing techniques, such as the example-based approach [52] and geometry-regularized dictionary learning [31], which, while effective to some extent, are limited in their ability to generate realistic and diverse medical images. The advent of machine learning, particularly deep learning, has significantly advanced the field, enabling more sophisticated and accurate models for image synthesis [59].\nGAN in medical image synthesis. GAN [21], one of generative models [12, 17, 37, 45], has been widely adopted for various tasks, such as MRI/CT image synthesis [22, 33, 61], cross-modality image translation [9, 56, 65, 69], image reconstruction [46, 64] and super-resolution [1, 47, 67], in medical imaging synthesis due to its promising ability to generate high-quality images. One of the most critical applications of GAN in medical imaging is data augmentation by generating annotated images. Several studies [8, 22, 32, 74], have employed GAN to generate lesion images to augment training data for improving downstream tasks to overcome data scarcity issues. However, those methods focus on 2D medical imaging or small volumetric patch synthesis, which is fundamentally limited due to neglecting the inherent complexity and the 3D nature of medical data. In this work, we focus on generating full CT volume in realistic dimensions (up to 512 \u00d7 512 \u00d7 768) to model complex volumetric features in a unified framework.\nDM in medical image synthesis. Diffusion models [29,49] have recently emerged as a powerful generative model that has shown great potential in medical imaging synthesis due to its capabilities in high-quality image synthesis, stable training process, and flexibility in conditioning [5, 13,66]. [35,38,55] demonstrate the effectiveness of DM-based methods in generating high-quality 2D medical images that capture intricate details with minimal artifacts, making them suitable for clinical use. GenerateCT [23] is designed to synthesize 3D CT volumes from free-form medical text prompts and accomplishes arbitrary-size CT volume generation by decomposing the process into a sequential generation of individual slices using DM. However, due to the nature of 2D approaches, the issue of 3D structural inconsistencies across slices is noticeable and problematic in the generated images. Application-wise, many recent studies [30, 39, 42, 63, 72, 73] are focusing on tumor synthesizing and improving models' performance in downstream tasks. DiffTumor [10] seeks to enhance the robustness and generalizability of tumor segmentation models across various organs, such as the liver, pancreas, and kidney, by leveraging high-quality synthetic tumors generated through specialized diffusion models. In this work, we focus on achieving conditional generation tailored to versatile tasks by leveraging robust foundation models, which significantly minimizes the need for extensive retraining across different applications, thereby conserving both time and computational resources."}, {"title": "3. Methodology", "content": "As shown in Fig. 2, the development of MAISI involves three stages. In the first stage, the volume compression network (i.e., VAE-GAN [49]) is trained on a substantial dataset comprising 39,206 3D CT volumes and 18,827 3D MRI volumes. This network effectively compresses high-resolution 3D medical images into a latent space that is perceptually equivalent to the image space, reducing memory usage and computational complexity for later stages. In the second stage, a latent diffusion model is trained on 10,277 CT volumes sourced from diverse datasets. This model operates within the compressed latent space, conditioned on specific body regions and voxel spacing, to generate features of realistic and complex 3D anatomical structures in flexible dimensions. Training on a broad range of data enhances the model's generalizability and adaptability in different tasks. The final stage involves the integration of ControlNet [70] into the MAISI framework. This component allows for dynamic control over the generated outputs by injecting additional conditions into the trained latent DM in the second stage, potentially supporting a wide range of tasks. The integration reduces the need for extensive retraining when the model is adapted to different tasks. In what follows, we provide detailed descriptions of each key component of the MAISI framework."}, {"title": "3.1. Volume Compression Network", "content": "The volume compression model builds upon previous studies [19, 49] and employs a Variational Autoencoder (VAE) trained on combined objectives, which integrates perceptual loss $L_{lpips}$ [71], adversarial loss $L_{adv}$, and L1 reconstruction loss $L_{recon}$ on voxel-space. These combined objectives ensure that the volume reconstructions adhere closely to the image manifold and enforce local realism [49]. In addition, we follow [36, 48, 49] adding Kullback-Leibler (KL) regularization $L_{reg}$ toward a standard normal on the learned latent features for avoiding high-variance latent spaces.\nGiven a CT volume $x \\in \\mathbb{R}^{H \\times W \\times D}$ in grayscale voxel space, where H denotes the height, W the width, and D the depth, the encoder $\\mathcal{E}$ of AE downsamples x and generates the latent representation $z = \\mathcal{E}(x) \\in \\mathbb{R}^{h \\times w \\times d}$ with much smaller spatial dimensions. The decoder $\\mathcal{D}$ of AE approximates the reconstructed volume $\\hat{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$ from the latent features. A 3D discriminator, denoted as $\\mathcal{C}$, is utilized to identify and penalize any unrealistic artifacts in the reconstructed volume $\\hat{x}$. As shown in Fig. 2 step 1, the overall objective $\\mathcal{L}_{AE}$ to train the volume compression network ($\\mathcal{E}, \\mathcal{D}$) in MAISI can be defined as follows:\n$\\min_{\\mathcal{E},\\mathcal{D}} \\max_{\\mathcal{C}} (\\mathcal{L}_{recon} (x, \\mathcal{D}(\\mathcal{E}(x))) + \\mathcal{L}_{lpips}(x, \\mathcal{D}(\\mathcal{E}(x)))+ \\mathcal{L}_{reg}(\\mathcal{E}(x)) + \\mathcal{L}_{adv}),$ (1)\nwhere\n$\\mathcal{L}_{adv} = log\\mathcal{C}(x) + log(1 - \\mathcal{C}(\\mathcal{D}(\\mathcal{E}(x))))$. (2)\nGenerating high-resolution 3D volumes, particularly those exceeding dimensions of 5123 voxels, poses a significant challenge due to the substantial memory demands imposed by the 3D convolution networks. In order to address memory bottleneck, previous methods [23, 53] achieve 2D high-resolution image synthesis via an additional super-resolution model. However, in the context of 3D whole-volume generation, the memory consumption can still quickly reach the hardware limitation of modern GPUs (e.g., NVIDIA A100 80G). To overcome GPU memory constraints, sliding window inference [6] is a common technique. It divides the large network input into smaller 3D patches in a sliding-window fashion and then stitches the network's output of each patch together to form the final results. When used in the 3D medical image segmentation model inference, it can often lead to artifacts/discontinuities along the window boundaries. While overlapping windows can help in segmentation tasks by smoothing over the boundary artifacts of probability maps, we empirically found this issue in transition areas between windows is more pronounced for the synthesis task due to the direct generation of image intensities, and thus the direct adaptation of sliding window inference is not self-sufficient. To minimize the use of the sliding-window approach for image synthesis, we propose a simple yet effective solution by introducing tensor splitting parallelism into convolutional networks. The tensor parallelism [57] is initially developed to distribute the inputs or model weights of matrix multiplication operations in fully connected layers across multiple GPUs. Unlike language models [18] built upon linear layers, the memory bottleneck usually attributes to the large feature maps. As demonstrated in Fig. 3, the proposed TSP is utilized to divide feature maps into smaller segments while preserving necessary overlaps across both convolution and normalization layers of AE. Each segment is assigned to a designated device, and these segments are subsequently merged to generate the layer's output. This flexible implementation enables segments to be distributed across multiple devices to accelerate the inference and also allows each segment to be processed sequentially within a single device in a loop to reduce peak memory usage."}, {"title": "3.2. Diffusion Model", "content": "The Diffusion Model in MAISI operates on a compressed latent space with flexible dimensions and incorporates body region and voxel spacing as conditional inputs, facilitating the high-fidelity generation of anatomical structures. Diffusion models are probabilistic models that aim to learn a data distribution p(x) by gradually denoising a normally distributed variable. This process is equivalent to learning the reverse dynamics of a fixed Markov Chain over a sequence of T steps. The denoising score-matching [60] is widely adopted in image synthesis tasks [16,54]. In the context of latent diffusion model [49], the learning model $\\epsilon_{\\theta}$ functions as a uniformly weighted sequence of denoising autoencoders $\\epsilon_{\\theta}(z_t, t); t = 1...T$, which are designed to predict a denoised version of the input latent features $z_t$ and $z_t$ represents a noisy variant of the original input at time step t. The neural backbone $\\epsilon_{\\theta}$ is defined as a time-conditional U-Net [50]\nAs shown in Fig. 2 step 2, the diffusion model in MAISI additionally conditions on both the body region and voxel spacing. The body region is defined by a top-region index $i_{top}$ and a bottom-region index $i_{bottom}$, indicating the extent of the CT scan coverage. $i_{top}$ and $i_{bottom}$ are defined by 4-dimensional one-hot vectors for head-neck, chest, abdomen, and lower-body regions). We ascertain the body region either through segmentation ground truth or predated segmentation masks from whole-body CT segmentation models, such as TotalSegmentator [62] or VISTA3D [26]. The condition of voxel spacing s is defined by a vector containing three float numbers representing the physical size of each voxel along each of the three dimensions in millimeters. We denote the primary conditions as $c_p := \\{i_{top}, i_{bottom}, s\\}$. Formally, the training objective of MAISI diffusion model is as follows:\n$\\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t, c_p} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c_p)||_1]$, (3)\nwhere the neural backbone $\\epsilon_{\\theta}$ is configured to condition on time step t and the primary conditions as $c_p$. Moreover, $\\epsilon_{\\theta}$ undergoes training on the latent variable $z_t$, which varies in dimensions throughout the training process. This training regimen is designed to facilitate the generation of outputs with flexible volumetric dimensions."}, {"title": "3.3. Additional Conditioning Mechanisms", "content": "In addition to the primary conditioning on body region and voxel spacing described in the Sec. 3.2, MAISI incorporates an additional mechanism for enhancing the control and flexibility of the generated outputs through the integration of ControlNet [70]. It is seamlessly embedded into the MAISI architecture with the latent diffusion model, to provide additional conditioning paths that allow for task-specific adaptations. ControlNet [70] is designed to inject auxiliary conditions into the diffusion process, enabling more precise control over the generated anatomical structures. It operates by creating two copies of the neural network blocks: a locked copy that preserves the original model's knowledge, and a trainable copy that learns to respond to specific conditions. These copies are connected using zero convolution layers, which gradually evolve from zero weights to optimal settings during training. These additional conditions can include a variety of inputs such as segmentation masks for conditional generation based on masks, or masked images and tumor masks for the tumor inpainting [10]. Similar to [38, 44, 70], we employ a compact encoder network to transform the additional condition from its original resolution into latent features, which are denoted by the task-specific condition $c_f$. This transformation process effectively aligns the additional condition with the spatial dimensions of the latent space. The integration of ControlNet [70] occurs during the third stage (Fig. 2 step 3) of MAISI's development, where it is trained with the frozen latent diffusion model. The overall learning objective of the entire diffusion algorithm, which incorporates the Control-Net [70], is formulated as follows:\n$\\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0,1), t, c_p, c_f} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c_p, c_f)||_1]$. (4)\nThis integration adds a flexible mechanism to MAISI for controlling the generation of 3D anatomical structures. By injecting task-specific conditions, MAISI can be fine-tuned to meet the specific needs of various medical imaging tasks without retraining the two foundation models, making it a versatile tool for various medical image synthesis tasks."}, {"title": "4. Experiments", "content": "To develop and evaluate the proposed MAISI framework, we curate a large-scale medical imaging dataset from publicly available datasets to capture a diverse range of anatomical structures, imaging conditions, and disease states. These datasets are integral to training the three networks within the MAISI framework. The Volume Compression Network (MAISI VAE) is trained on a dataset"}, {"title": "5. Discussion and Limitations", "content": "While the proposed MAISI demonstrates great potentials in generating high-quality CT images, it is essential to recognize its limitations and potential societal impacts. While MAISI shows robust performance across various datasets, its ability to accurately represent demographic variations (such as age, ethnicity, and gender differences) in generated anatomy has not been extensively validated. Future studies can focus on ensuring that synthetic data adequately captures this diversity to avoid bias in downstream applications.\nThe capabilities of generating high-resolution images of MAISI, while innovative, still demand substantial computation resources. This could limit accessibility for researchers and institutions with less computational power, potentially widening the gap between high-resource and low-resource entities. Future efforts can focus on improving the accessibility of MAISI, particularly in resource-constrained environments."}, {"title": "6. Conclusion", "content": "In this paper, we propose MAISI, a novel framework for generating high-resolution 3D CT volumes using a combination of foundation models and ControlNet [70]. MAISI aims to provide an adaptable and versatile solution for generating anatomically accurate images. Our experiments demonstrate that MAISI can produce realistic CT images with flexible volume dimensions and voxel spacing, offering promising potential to augment medical datasets and improve the performance of downstream tasks."}, {"title": "A. Dataset Details", "content": "A.1. MAISI VAE\nFor the foundational 3D VAE in MAISI, we include a diverse dataset comprising 37,243 CT volumes for training and 1,963 CT volumes for validation, covering the chest, abdomen, and head and neck regions. Additionally, we include 17,887 MRI volumes for training and 940 MRI volumes for validation, spanning the brain, skull-stripped brain, chest, and below-abdomen regions. The training data were sourced from various repositories, including TCIA COVID-19 Chest CT, TCIA Colon Abdomen CT, MSD03 Liver Abdomen CT, LIDC Chest CT, TCIA Stony Brook COVID Chest CT, NLST Chest CT, TCIA Upenn GBM Brain MR, AOMIC Brain MR, QTIM Brain MR, TCIA Acrin Chest MR, and TCIA Prostate MR. This extensive and varied dataset not only ensures that our model is exposed to a broad range of anatomical regions but also supports its application to both MRI and CT images.\nThe details of MAISI VAE training data are shown in Table S1.\nA.2. MAISI Diffusion\nThe datasets for developing the Diffusion model used in MAISI comprise 10,277 CT volumes from 24 distinct datasets, encompassing various body regions and disease patterns. Table S2 provides a summary of the number of volumes for each dataset. For compatibility with the shape requirement of the U-shape network, we resample the dimensions of volumes to multiples of 128. Fig. S1 visualizes the characteristics and spatial complexity of the data involved in training the diffusion model.\nA.3. MAISI ControlNet\nThe ControlNet training dataset for MAISI CT Generation discussed in Sec. 4.4 contains 6,330 CT volumes (5,058 and 1,272 volumes are used for training and validation, respectively) across 20 datasets and covers different body regions and diseases. Table S3 summarizes the number of volumes for each dataset."}, {"title": "B. Additional Implementation Details", "content": "MAISI VAE. To establish the VAE as a foundational model, we employ an extensive range of data augmentation techniques. For CT images, intensities are clipped to a Hounsfield Unit (HU) range of -1000 to 1000 and normalized to a range of [0,1]. For MR images, intensities were normalized such that the Oth to 99.5th percentile values were scaled to the range [0,1]. For MR images, we applied intensity augmentations including random bias field, random Gibbs noise, random contrast adjustment, and random histogram shifts. Both CT and MR images underwent spatial augmentations, such as random flipping, random rotation, random intensity scaling, random intensity shifting, and random upsampling or downsampling.\nThe MAISI VAE model is trained with 8 32G V100 GPU. It is initially trained for 100 epochs using small, randomly cropped patches of size [64,64,64]. This approach is adopted to improve the model's ability to generalize to images with partial volume effects. After this initial phase, training is continued for an additional 200 epochs using larger patches of size [128,128,128], which allows the model to capture more contextual information and improve overall accuracy.\nThe MAISI VAE is used to compress the latent features that will be employed in latent diffusion models, where having a well-structured and meaningful latent space is crucial for effective diffusion dynamics. Therefore, during MAISI VAE training, we adjust the weight of the KL loss to ensure the standard deviation remains between 0.9 to 1.1. This calibration balances the model's focus between accurate data reconstruction and adherence to the prior distribution. As the MAISI VAE is intended to serve as a foundational model, maintaining this balance also helps to prevent over-fitting [28].\nMAISI Diffusion. Data preprocessing for diffusion model training involves applying a series of precise transformations to the image data, including loading the images, ensuring the correct channel structure, adjusting the orientation according to the \"RAS\" axcode, and scaling intensity values from -1000 to 1000 to normalize the data between 0 and 1. The process further refines the images by adjusting dimensions to the nearest multiple of 128, recording the new spatial details, using trilinear interpolation. Then each image is passed through a pre-trained autoencoder, generating a compressed latent representation that is saved for subsequent model training. The diffusion model requires additional input attributes, including output dimensions, output spacing, and top/bottom body region indicators. These dimensions and spacing are extracted from the header information of the training images. The top and bottom body regions can be identified either through manual inspection or by using segmentation tools such as TotalSegmentator [62] and VISTA3D [26]. These regions are encoded as 4-dimensional one-hot vectors: the head and neck region is represented by [1,0,0,0], the chest by [0, 1, 0, 0], the abdomen by [0, 0, 1, 0], and the lower body (below the abdomen) by [0, 0, 0, 1]. These additional input attributes are stored in a separate configuration file. In this example, it is assumed that the images encompass the chest and abdomen regions.\nNext, the diffusion model training process begins with an initial learning rate of $1e^{-4}$, a batch size of 1, and spans 200 epochs. To ensure the data is optimally prepared for training, various transformations are applied to the image inputs. The U-Net architecture is employed for noise prediction, with distributed computing utilized to enhance efficiency when multiple GPUs are available. The Adam optimizer is responsible for adjusting the model's parameters, while a polynomial learning rate scheduler controls the update rate over training steps. Noise is systematically introduced to the input data by the noise scheduler, and the model iteratively refines its predictions using an L1 loss function to minimize this noise. Mixed precision training and gradient scaling are implemented to optimize memory usage and computational performance.\nMAISI ControlNet. We train a versatile ControlNet Model (MAISI CT Generation task in Sec. 4.4) to support all five types of tumors using the datasets summarized in Table S3. The data preprocessing protocol is the same in the training of the MAISI Diffusion Model. The Adam optimizer is employed for training purposes, with hyperparameters \u03b2\u2081 = 0.9 and \u03b22 = 0.999. The learning rate is set at 0.0001, with the polynomial learning rate decay. The batch size is set to 1 per GPU. Training is performed on a server with 8 A100 GPUs with about 10k optimization steps. For the MAISI Inpainting task, we employ the same hyperparameters for training but only use datasets with supported tumor types, including MSD Task03 [3] (liver tumor), Task06 [3] (lung tumor), Task07 [3] (pancreas tumor).\nDownstream tumor segmentation. The implementation of all tumor segmentation models is based on the Auto3DSeg4 pipeline. Auto3DSeg is an auto-configuration pipeline designed for 3D medical image segmentation, utilizing MONAI [6]. The pipeline begins with data analysis to extract global information from the dataset, followed by algorithm generation based on data statistics and predefined templates. It then proceeds to model training to obtain optimal checkpoints. All used tumor dataset is split into 80% for training and 20% for testing. The training set is further divided into five folds for 5-fold cross-validation. We report the segmentation performance on the holdout testing set. For the MAISI CT Generation task, we generate synthetic data from augmented real masks containing tumors. Fig. S2 shows an example of mask augmentation for a case with the lung tumor. For the MAISI Inpainting task, we follow the same setting in DiffTumor [10] and use the"}]}