{"title": "A Survey on Large Language Model Acceleration based on KV Cache Management", "authors": ["Haoyang Li", "Yiming Li", "Anxin Tian", "Tianhao Tang", "Zhanchao Xu", "Xuejia Chen", "Nicole Hu", "Wei Dong", "Qing Li", "Lei Chen"], "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) [1], [2], trained on massive corpora, have revolutionized various domains such as natural language processing [3], [4], [5], computer vision [6], [7], [8], and multi-modal [9], [10], [11] tasks. Their ability to understand context and perform logical reasoning has enabled remarkable success in various fields, such as time series analysis [12], [13], recommendation [14], [15], autonomous driving [16], [17], [18], and healthcare [19], [20]. These breakthroughs are powered by state-of-the-art architectures and training paradigms, enabling models to achieve unparalleled performance across diverse tasks. Prominent LLMs, such as GPT [21], [22], [23], LLaMA [24], [25], DeepSeek [26], [27], [28], Mistral [29], [30], and GLM [31], [32], are built on the foundational transformer architecture [33], which excels at capturing long-range dependencies in sequential data. However, despite their powerful capabilities, the computational and memory demands of LLMs, particularly during inference, present significant challenges when scaling them to real-world, long-context, and real-time applications.\nA critical bottleneck in LLM inference lies in the efficient management of Key-Value (KV) pairs. Recently, caching techniques [34], [35] have been extensively employed to store previously computed intermediate results, allowing their reuse in subsequent inference steps to accelerate the model, such as graph neural networks [36], [37], [38]. Fortunately, the auto-regressive generation mechanism inherent to LLMs presents an opportunity to leverage KV caching for efficient text generation. Specifically, auto-regressive generation enables LLMs to produce text token by token, with each token conditioned on all previously generated ones. While this approach is highly effective for generating coherent and contextually relevant outputs, it suffers from poor scalability with long input sequences, as the computational and memory requirements grow quadratically with sequence length. The KV cache addresses this issue by storing key and value matrices from previous decoding steps, enabling their reuse and significantly reducing redundant computations.\nSeveral recent surveys [2], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50] have explored the domain of efficient large language models (LLMs). These surveys primarily examine various aspects of LLM efficiency, presenting valuable insights while leaving room for further refinement and innovation. In particular, many of these works primarily focus on holistic approaches to improving LLM efficiency, examining a wide range of techniques across multiple dimensions, such as span data-level optimizations (e.g., prompt engineering), model architecture-level optimizations (e.g., efficient transformer designs), and system-level optimizations (e.g., task scheduling). For instance, Ding et al. [42] explore efficiency techniques that integrate"}, {"title": "2 PRELIMINARY", "content": "Large language models (LLMs), pretrained on vast corpora, have demonstrated superior capabilities in context understanding and logical reasoning. These models have achieved remarkable success across a wide range of tasks in various domains, including natural language processing [3], [4], [5] and computer vision [6], [7], [8]. Mainstream LLMs, such as GPT [58], LLaMA [24], and DeepSeek [26], are primarily built on the transformer architecture [33]. To explore the role of Key-Value (KV) cache management in accelerating LLM computations, we first outline the core components of the transformer model and then introduce the mechanisms for managing the KV cache to accelerate the LLMs. Important notations in this survey are summarized in Tab. 1."}, {"title": "2.1 Transformer Architecture", "content": "Transformers [33] have become the backbone of LLMs due to their ability to efficiently capture long-range dependencies sequential data, such as text. This capability makes them particularly well-suited for tasks like machine translation, text generation, and image captioning. The transformer architecture follows an encoder-decoder structure, where most LLMs utilize only the decoder component. We first introduce the core components of the Transformer decoder and then describe the critical auto-regressive generation mechanism. Particularly, we do not describe certain components in transformer, such as normalization, as they do not impact the understanding of KV cache management."}, {"title": "2.1.1 Transformer Decoder", "content": "As shown in Figure 1, a decoder-based transformer architecture is composed of multiple stacked Transformer blocks, each designed to process sequential data effectively. Typically, a Transformer block consists of two core components, i.e., a Multi-Head Self-Attention (MHSA) mechanism and a Feed Forward Network (FFN). These blocks are arranged sequentially, where the output of one block is passed as input to the next. This iterative design allows the model to refine its understanding of the input sequence progressively, making it highly effective for tasks such as text generation and language modeling.\nPositional Encoding. Before the input sequence is processed by the Transformer blocks, it undergoes a preprocessing phase. First, a tokenizer processes the input sentence $X$ by splitting it into discrete units, such as words or subwords. The resulting sequence can be represented as $X = [x_1,x_2,\u00b7\u00b7\u00b7,x_{|X|}]$. These tokens are then mapped to dense vector representations using an embedding layer, i.e., $\\hat{X} = I_X E \\text{,where } I_X \\in {0,1}^{n \\times d_{vocab}} \\text{ represents the one-hot vector of tokenized input } X, E \\in \\mathbb{R}^{d_{vocab} \\times d_{x}} \\text{ is the embedding matrix, and } \\hat{X} = [\\hat{x_1},\\hat{x_2},\u00b7\u00b7\u00b7,\\hat{x_{|X|}}] \\in \\mathbb{R}^{n \\times d_{x}}$ is the resulting matrix of embedded token representations. Since the Transformer architecture does not inherently account for"}, {"title": "Transformer Block", "content": "the order of tokens in a sequence, positional encodings are added to the token embeddings $\\hat{X}$ to incorporate positional information. This can be expressed as $X = \\hat{X} + PE(\\hat{X})$, where $PE(\\hat{X}) \\in \\mathbb{R}^{n \\times d_{x}}$ represents a function [59], [60], [61] (e.g., sine and cosine-based positional encoding) that generates positional embeddings for the input $\\hat{X}$.\nTransformer Block. Once the input features are prepared, they are passed through a series of stacked Transformer blocks. Each block begins with the Multi-Head Self-Attention (MHSA) mechanism, which captures both local and global dependencies. For each token, the self-attention mechanism computes a weighted sum over all other tokens in the sequence, where the weights are derived from the similarity between the tokens. Particularly, since the operations within each transformer block are identical, we use a single transformer block as an example. Specifically, given the input to a block, denoted as $X \\in \\mathbb{R}^{|X| \\times d_x}$, the MHSA mechanism computes the query vectors $Q_i \\in \\mathbb{R}^{|X| \\times d_k}$, key vectors $K_i \\in \\mathbb{R}^{|X| \\times d_k}$, and value vectors $V_i \\in \\mathbb{R}^{|X| \\times d_v}$. These vectors are obtained through learned linear transformations as follows:\n$Q_i = XW_{Q_i}, K_i = XW_{K_i}, V_i = XW_{V_i},$ (1)\nwhere $W_{Q_i} \\in \\mathbb{R}^{d_x \\times d_k}, W_{K_i} \\in \\mathbb{R}^{d_x \\times d_k} \\text{ and } W_{V_i} \\in \\mathbb{R}^{d_x \\times d_v}$ are the learned weight parameters. Then, the self-attention operation is applied to each triple $(Q_i, K_i, V_i)$, and obtain the output of the i-th attention head $Z_i$ as follows:\n$Z_i = Attention(Q_i, K_i, V_i) = Softmax(\\frac{Q_iK_i^\\top}{\\sqrt{d_k}})V_i,$ (2)\nwhere $\\sqrt{d_k}$ is a scaling factor to ensure the numerical stability. To capture diverse relationships, multiple attention with h heads are applied to $X$ in parallel, and their outputs are concatenated with one transformation as follows:\n$Z = Concat(Z_1, Z_2,..., Z_h)W_O,$ (3)\nwhere Concat is concatenation operation and $W_O \\in \\mathbb{R}^{h d_v \\times d_o}$ is the trainable parameters.\nFollowing the self-attention mechanism, the output is passed through a Feed Forward Network (FFN). The FFN is a fully connected neural network that applies two linear transformations separated by a nonlinear activation function $\\sigma(\\cdot)$ (e.g, ReLU [62]) :\n$FFN(Z) = \\sigma(ZW_1 + b_1)W_2 + b_2$ (4)\nwhere $W_1 \\in \\mathbb{R}^{d_o \\times d_1}$ and $W_2 \\in \\mathbb{R}^{d_1 \\times d_2}$ are two parameters. Also, $b_1 \\in \\mathbb{R}^{d_1}$ and $b_2 \\in \\mathbb{R}^{d_2}$ are two bias vectors."}, {"title": "2.1.2 Auto-regressive Generation Mechanism", "content": "LLMs employ an autoregressive mechanism to generate text token by token, with each token conditioned on the previously generated ones. This iterative process ensures that the output sequence remains coherent and contextually appropriate. Formally, given an input sequence of tokens $X = [x_1,x_2,...,x_t]$, the model predicts the next token $x_{t+1}$ at each decoding step t by modeling the conditional probability distribution as follows:\n$P(x_{t+1}|x_1,x_2,..., x_t) = Softmax(h_tW_{out} + b_{out}),$ (5)\nwhere $h_t \\in \\mathbb{R}^{d_h}$ represents the hidden state of the LLM regarding $X$ at step t, $W_{out} \\in \\mathbb{R}^{d_h \\times d_{vocab}}$ is the output projection matrix, and $b_{out}$ is the bias vector. The softmax function converts the logits into a probability distribution over the vocabulary. Then, at each decoding step, the model generates the next token $x_{t+1}$ by sampling from the predicted probability distribution:\n$x_{t+1} \\sim P(x_{t+1}|x_1,x_2,\u00b7\u00b7\u00b7, x_t).$ (6)\nThe generated token $x_{t+1}$ is then appended to the sequence $X = [x_1,..., x_t, x_{t+1}]$, and the process continues until a special end-of-sequence (EOS) token is generated or a predefined maximum length is reached."}, {"title": "2.2 Key-Value Cache in Transformer Models", "content": "Auto-regressive generation is a powerful mechanism that enables LLMs to produce high-quality, contextually coherent text. However, it presents computational challenges for long sequences, as the Keys and Values need to be recomputed for each token during the generation process. The KV cache optimization addresses this issue by storing the previously computed Keys and Values and reusing them for subsequent token generation, thereby reducing redundant computations and improving inference efficiency."}, {"title": "2.2.1 Auto-regressive Generation with KV Cache", "content": "Here, we describe how caching KV pairs of tokens accelerates LLM inference. Specifically, at each decoding step t, the model performs self-attention over the entire sequence $X = [x_1,..., x_{t-1}, x_t]$ to generate the next token $x_{t+1}$. This process requires the computation of Keys and Values matrices for all previously processed tokens in $X = [x_1,..., x_t]$. Notably, when generating the token $x_t$, the LLM has already computed the Keys and Values for the tokens in $X[1 : t - 1] = [x_1,..., x_{t-1}]$. The KV cache optimizes this process by storing the previously computed Keys and"}, {"title": "Values matrices for $X[1 : t - 1]$", "content": "Values matrices for $X[1 : t - 1]$ and reusing them, thereby only requiring the computation of Keys and Values for the new token $x_t$. This significantly improves efficiency by eliminating redundant computations.\nFormally, at decoding step t, the new token embedding $x_t$ is used to compute the query vector $q$, key vector $k$, and value vector $v$ as follows:\n$q = \\hat{x_t}W_Q, k = \\hat{x_t}W_K, v = \\hat{x_t}W_{V},$ (7)\nThe newly computed $k$ and $v$ are then appended to the cached key and value matrices from previous steps:\n$K^t = Concat(K^{t-1}, k), V^t = Concat(V^{t-1}, v),$ (8)\nwhere $K^{t-1} \\in \\mathbb{R}^{t-1 \\times d_k}$ and $V^{t-1} \\in \\mathbb{R}^{t-1 \\times d_v}$ represent the cached key and value matrices of tokens in $X[1: t-1]$. These cached matrices are then used in the scaled dot-product attention computation for token $x_t$. The attention output $z_t^i$ for the token $x_t$ at step t is calculated as:\n$\\hat{z_t}^i = Softmax(\\frac{q K^{\\top}}{\\sqrt{d_k}})V,$(9)\nThen, a similar KV reuse process can be applied to different attention heads in each layer of the LLM."}, {"title": "2.2.2 Time and Space Complexity Analysis", "content": "Given a transformer-based L-layer LLM with h attention heads per layer and an input sequence of length $X = [x_1,...,x_t]$, we analyze the time saved and the space required to store cached KV pairs. For simplicity, we assume the Keys and Values of $t_c$ tokens are stored for all heads across all LLM layers.\nSaved Time. For each token, the saved computation time comes from avoiding the repeated computation of Keys and Values in Equation (1), self-attention result in Equation (2), and linear transformation in Equation (3). We omit the time analyze on operations in transformer that do not affect the understanding of KV cache acceleration, such as layer norm and position encoding.\n*   QKV Computation. The time of computing Queries, Keys and Values for each token in Equation (1) is $\\triangle_1 = O(2d_x d_k + d_x d_v)$.\n*   Self-attention Result. Additionally, computing each attention result $z_i$ in Equation (2) takes $O(t (d_k + d_v))$.\n*   Linear Transformation. To merge the h attention results in Equation (3) the time is $\\triangle_2 = O(h d_v + d_v d_o)$.\nTherefore, for $t_c$ cached tokens across h attention heads and L layers, the total saved computation time is:\n$O (L \\cdot h t_c (d_k + d_v) + L \\cdot h \\cdot t_c (\\triangle_1 + \\triangle_2))$ (10)\nThus, the saved time is directly proportional to the number of cached tokens $t_c$, significantly accelerating model computation, especially for longer sequences (when t is large).\nExtra Space. Compared to computation without caching, additional space is required to store the cached KV pairs for $t_c$ tokens across h attention heads and L layers. Assuming each Key and Value is stored in Float16 precision, the total extra space needed can be expressed as:\n$O(L \\cdot h t_c 2 \\cdot sizeof(Float16))$ (11)"}, {"title": "2.3 Challenges in KV Cache Management", "content": "As analyzed in Sec. 2.2.2, reusing cached KV pairs enables the LLM to avoid recomputing past tokens, resulting in significant speedups during inference. However, as sequence lengths grow, the size of the KV cache increases proportionally, placing significant pressure on memory. Consequently, it becomes challenging to manage this cache effectively to accelerate LLM computation without excessive space usage.\n*   Cache Eviction Policies: Determining which items to evict when the cache reaches its capacity is a complex problem. Popular policies [35] like Least Recently Used (LRU) or Least Frequently Used (LFU) do not align with LLMs patterns, leading to suboptimal performance.\n*   Memory Management: The memory required for the KV cache grows linearly with both the sequence length and the number of layers, which can quickly exceed the hardware memory limits, especially for long sequences. Consequently, managing the collaboration between different types of storage hardware (e.g., GPU, CPU, or external memory) becomes a significant challenge.\n*   Latency Bottlenecks: Accessing and updating the cache at each decoding step can introduce latency, particularly for hardware with limited memory bandwidth.\n*   Compression Trade-offs: Compressing the KV cache can reduce memory usage but may degrade model performance if key information is lost.\n*   Dynamic Workloads: Handling dynamic and unpredictable workloads, where access patterns and data requirements frequently change, requires adaptive caching strategies that can respond in real time.\n*   Distributed Coordination: In distributed KV caches, maintaining coordination across multiple nodes to ensure consistency, fault tolerance, and efficient resource usage adds significant complexity."}, {"title": "3 TAXONOMY", "content": "In the above sections, we analyzed how the number of cached Key-Value (KV) pairs significantly impacts both the computation time and the additional memory required during inference. Efficient KV cache management is critical to balancing performance improvements and resource utilization, especially as sequence lengths and model sizes continue to grow. After carefully reviewing existing approaches, we categorize KV cache optimization strategies into three levels: token-level optimization, model-level optimization, and system-level optimizations. Each level addresses specific aspects of the challenges associated with KV cache management and offers distinct techniques to enhance efficiency."}, {"title": "4 TOKEN-LEVEL OPTIMIZATION", "content": "In the token level, optimization focuses exclusively on improving KV cache based on the characteristics and patterns of KV pairs of tokens, without considering enhancements from model architecture improvements or system parallelization techniques. In general, token-level optimization methods are primarily guided by observations from LLMs and sequential inputs. Existing approaches can be categorized into five main types: KV cache selection, KV cache budget allocation, KV cache merging, KV cache quantization, and KV cache low-rank decomposition."}, {"title": "4.1 KV Cache Selection", "content": "KV cache selection mechanisms have emerged as a critical optimization strategy, aimed at reducing memory utilization of KV caches, minimizing inference latency, and enhancing overall throughput in large language models. These optimization objectives have driven the development of various selection methodologies, which can be classified into two distinct categories: (1) static KV cache selection, which performs token filtering exclusively during the prefilling phase, with selected tokens remaining fixed throughout subsequent decoding steps; and (2) dynamic KV cache selection, which continuously updates KV cache during the decoding phase, enabling adaptive cache management. In dynamic KV cache selection approaches, KV cache tokens that are not selected may be permanently evicted or offloaded to hierarchical caching devices such as CPU memory, implementing a multi-tier storage strategy. Given that real-time KV cache selection during decoding may incur substantial computational overhead, several studies have focused on developing optimized retrieval algorithms to enhance the efficiency of this process. These optimizations include block-level retrieval instead of token-level granularity to reduce search complexity, asynchronous query mechanisms to hide latency, and parallel retrieval pipelines to accelerate the selection process. These optimization efforts aim to mitigate the computational burden while maintaining the effectiveness of token selection."}, {"title": "4.1.1 Static KV Cache Selection", "content": "Static KV cache selection methods perform a one-time compression on the KV Cache immediately after the prefilling phase is completed. The model then uses this compressed KV cache for subsequent decoding inference. FastGen [132] introduces a pattern-aware approach by identifying five fundamental attention structures and implementing targeted selection strategies. These include proximity-based retention for local attention patterns, selective preservation of critical tokens for punctuation-focused attention, frequency-based filtering for sparse attention distributions, and complete token retention for broad attention patterns. SnapKV [133] simplifies FastGen's approach by focusing solely on retrieving tokens based on their importance scores. It demonstrates that among all prompt tokens, only a portion carries crucial information for response generation, with these tokens maintaining their significance during the generation phase. The approach employs an end-positioned observation window to detect these important contextual tokens. Their corresponding key-value pairs are then concatenated with the tokens from the observation window. Attention-Gate [134] introduces a learnable KV-Cache eviction mechanism that processes the entire context sequence and generates token-wise eviction decisions through a parameterized policy network, enabling dynamic in-context memory management."}, {"title": "4.1.2 Dynamic Selection with Permanent Eviction", "content": "This category of methods performs frequent KV cache selection during the decoding phase, permanently removing unselected KV cache tokens from memory. Early works employ a sliding-window mechanism to address long-text inference challenges, where tokens falling outside the window are permanently evicted and become inaccessible. StreamingLLM [135] uncovers a crucial phenomenon in transformer attention where preserved key-value pairs from initial sequence tokens maintain crucial model performance. This attention sink effect manifests through asymmetric attention weight accumulation at early positions, regardless of semantic significance. The approach leverages this characteristic by incorporating attention sink positions with recent context for efficient processing. LM-Infinite [136] demonstrates that conventional techniques, including sliding-window patterns and relative positional"}, {"title": "4.1.3 Dynamic Selection without Permanent Eviction", "content": "The aforementioned permanent eviction-based approaches face two significant limitations. First, the irreversible eviction of tokens potentially impairs the model's performance on long-sequence tasks, particularly in needle-in-a-haystack scenarios, and these methods prove challenging to adapt to multi-turn dialogue contexts. Second, KV cache selection during the decoding phase introduces computational overhead, adversely affecting decoding latency and compromising end-to-end acceleration. To address these challenges, several studies have focused on developing decoding-phase KV cache selection strategies without permanent eviction. These approaches typically employ multi-tier cache systems (e.g., CPU-GPU hierarchical caching) and leverage advanced data structures and system-level enhancements to optimize retrieval efficiency, enabling efficient inference with reduced GPU KV cache footprint.\nTo accelerate the retrieval of critical tokens, several research efforts have proposed index-based approaches that organize and access KV cache at block or cluster granularity, enabling efficient query and extraction operations. InfLLM [121] maintains full KV cache in blocks while facilitating long sequence processing through a hierarchical storage strategy. The framework employs CPU-GPU memory orchestration, preserving essential tokens and current computational units in GPU memory while offloading less frequently accessed units to CPU memory. To further enhance top-k block retrieval precision, the Quest [122] framework presents a refined block representation approach based on minimal and maximal key values in KV cache blocks. PQ-Cache [98] also implements block-based KV cache management and identifies salient tokens through Maximum Inner-Product Search (MIPS), leveraging Product Quantization (PQ) codes and centroids. SqueezedAttention [123] employs K-means clustering in an offline stage to group semantically similar keys, with each group represented by a centroid. During inference, it compares input queries against these centroids to identify and load only the semantically relevant keys from the context. Similarly, RetrievalAttention [124] index KV cache tokens using approximate nearest neighbor search (ANNS) techniques. Additionally, EM-LLM [125] dynamically segments incoming tokens into episodic events. Besides, it implements a hybrid retrieval mechanism that combines semantic similarity matching with temporal context to efficiently access relevant KV cache segments.\nTo accelerate top-k token identification, SparQ [137] identifies the r most significant elements in the incoming query vector and selectively retrieves the corresponding components along the hidden dimension of the cached key matrix K for approximate attention computation. To overlap prefetching latency, InfiniGen [138] employs asynchronous prefetching, utilizing indices of salient KV entries selected by queries from the previous layer to retrieve KV cache entries in the current layer. To ensure maximum model performance, RecycledAttention [139] sustains the entire KV cache during inference computations, yielding no"}, {"title": "4.2 KV Cache Budget Allocation", "content": "The hierarchical architecture of LLMs leads to diverse information extraction patterns across layers, with each layer's KV-cache contributing differently to model performance. This inherent heterogeneity indicates that uniform KV-cache compression across layers may be suboptimal. KV cache budget allocation addresses this challenge by intelligently distributing memory resources based on each component's importance to prediction accuracy, thereby optimizing memory utilization while minimizing accuracy degradation. Current budget allocation strategies can be categorized into two levels of granularity: layer-wise budget allocation, which assigns different compression ratios across model layers, and the more fine-grained head-wise budget allocation, which enables precise memory distribution across individual attention heads within each layer, offering more flexible and targeted optimization opportunities."}, {"title": "4.2.1 Layer-wise Budget Allocation", "content": "In contrast to conventional approaches with uniform KV cache sizes, PyramidKV [116] employs a pyramid-shaped memory allocation strategy, assigning larger cache capacities to lower layers that progressively decrease in upper layers. This design is supported by the observation that lower layers exhibit uniform attention distributions across input sequences, while upper layers show concentrated attention on specific tokens. PyramidInfer [117] also adopts a pyramid-shaped budget allocation strategy while selecting tokens with high attention values at each layer. Additionally, during the decoding phase, PyramidInfer dynamically maintains a set of significant tokens through frequent updates driven by attention values. Unlike previous methods, DynamicKV [118] implements an input-adaptive budget allocation strategy by analyzing attention patterns. Specifically, it computes the average attention scores between recent and historical tokens, identifies the top-k tokens with highest attention values across layers, and proportionally distributes the budget based on the density of significant tokens in each layer. Similarly, PrefixKV [119] identifies the most important tokens for each layer by computing the average attention score of tokens within that layer. PrefixKV [119] then uses a unified threshold to determine the number of retained tokens, adaptively adjusting the retention for each layer based on its importance distribution. CAKE [141] examines attention scores through two lenses: the spatial distribution of inter-token attention and the temporal evolution of attention focus. These measurements are combined to compute layer-specific importance scores, which further guide the allocation of memory resources. Additionally, SimLayerKV [120] identifies lazy layers - those exhibiting limited effectiveness in capturing long-range dependencies. The framework then selectively preserves cache entries, maintaining initial and recent tokens for lazy layers while retaining complete KV cache for non-lazy layers."}, {"title": "4.2.2 Head-wise Budget Allocation", "content": "AdaKV [110] leverages the observation that attention patterns exhibit distinct concentrations across different heads. It implements head-specific memory allocation by optimizing an L1 loss bound between the original and pruned multi-head attention outputs. Within the constraints of a layer-wise budget, the method distributes cache capacity among heads to maximize the preserved attention information collectively. Building upon AdaKV, CriticalKV [111] introduces significant enhancements by recognizing that the importance of KV cache entries extends beyond attention weights to encompass value states and pretrained parameter matrices. Leveraging this insight, the framework implements a novel selection algorithm that identifies essential cache entries by minimizing the maximum potential output perturbation. LeanKV [112] implements a fine-grained memory optimization strategy that operates independently for each attention head and input request. The method identifies the smallest subset of tokens necessary to preserve the majority of information flow, allocating cache space based on a predefined attention score threshold - typically maintaining 95% of the total attention mass.\nRetrieval head-based methods represent a specialized category of head-wise allocation strategies that focuses on identifying and prioritizing attention heads crucial for extracting key information from long sequences. This approach allocates larger cache budgets to these specialized heads, known as retrieval heads [142], due to their significant role in information extraction. RazorAttention [113] characterizes two distinct categories of retrieval heads: echo heads, which focus on previously occurring identical tokens, and induction heads, which attend to antecedent tokens that precede current token repetitions. This framework implements differential caching strategies, maintaining complete cache entries for retrieval heads while condensing remote tokens into consolidated compensation tokens for non-retrieval heads. HeadKV [114] further enhances RazorAttention by introducing a novel head assessment framework that simultaneously evaluates both retrieval and reasoning capabilities to optimize KV cache allocation strategies. DuoAttention [115] further introduces a parameterized approach to distinguish between two categories of attention mechanisms: retrieval heads, essential for comprehensive long-context processing, and Streaming heads, which primarily engage with recent tokens and attention sinks. This classification is achieved through learned parameters that automatically identify retrieval heads requiring full attention spans."}, {"title": "4.2.3 Summary and Future Directions", "content": "Despite recent advances and growing attention in KV cache budget allocation research, several critical challenges remain unaddressed. First, the relationship between allocation strategies and model performance requires further investigation. For instance, a notable discrepancy exists between pyramid-shaped allocation strategies [116], [117] advocating larger budgets for lower layers, and retrieval head-based studies [113], [114] which demonstrate that lower layers rarely exhibit retrieval head characteristics and thus require minimal cache resources. Additionally, the field lacks comprehensive experimental comparisons, particularly regarding the compatibility and performance benefits of head-wise budget allocation strategies with state-of-the-art frameworks like vLLM [143] and FlashAttention [144]. Also, existing methods, such as PyramidInfer [117], demonstrate some adaptability to input attention patterns. However, future research could target real-time, task-specific allocation strategies that dynamically adjust memory budgets during inference based on input characteristics, task complexity, or downstream requirements."}, {"title": "4.3 KV Cache Merging", "content": "KV cache merging offers a promising solution by compressing or consolidating KV caches without significantly degrading model accuracy. Rather than a uniform compression strategy, KV cache merging techniques leverage the inherent redundancy within and across layers to dynamically optimize memory utilization. These methods aim to reduce the size of KV caches while preserving critical information necessary for accurate attention computations, enabling efficient inference in resource-constrained settings."}, {"title": "4.3.1 Intra-layer Merging", "content": "As the input sequence length increases, the number of Keys and Values grows, leading to higher computational costs for the attention process. To address this, CCM [101], LoMA [102], DMC [103] propose to learn a compression module to compress KV of tokens.\nSpecifically, CCM [101] inserts a special indicator token, [COMP], into the input sequence and compresses the accumulating past attention key/value (KV) pairs in each layer between these indicators into a compact memory space. This compression leverages techniques inspired by the Compressive Transformer [145] and Gisting [146]. Instead of computing attention across all tokens, CCM [101] computes attention scores for each new token by referencing the merged token. Similarly, LoMA [102] inserts a special token into the input sequence to determine which consecutive tokens should be compressed. LoMA [102] performs compression using bidirectional attention, repetition zone supervision, and carefully designed attention masks and loss functions. DMC [103] learns a variable to decide whether to append new KV pairs to the cache when necessary or to merge them into existing KV representations using a weighted average. Note that CCM [101], LoMA [102], and DMC [103] require supervised learning to learn a compression module.\nInstead, CaM [104], KVMerger [108], and D2O [105] are training-free, which rely on observations and directly propose rule-based or heuristic-based merging strategies. Specifically, they separate the Keys and Values of tokens in each layer into important (retained) and unimportant (evicted) tokens. They then keep potentially useful unimportant tokens by merging their Keys and Values with retained important tokens, ensuring that no valuable information is lost. Particularly, D2O [105] merges merges the Key (or Value) of a evicted token with one retained token based on cosine similarity. Similar to D2O based on cosine similarity, AIM [106] and Look-M [107] merges Keys (resp. Values) of multiple tokens into one. CaM [104] merges the Keys (or Values) of multiple evicted tokens with retained tokens based on attention scores to get the final merged results. Also, KVMerger [108] first identifies the merge token sets by clustering consecutive tokens with high cosine similarity, ensuring that only adjacent tokens with strong contextual relevance are grouped together. Then, KVMerger merges the tokens in each merge set into the pivotal token (chosen based on the highest attention score) using Gaussian kernel weights, where closer tokens contribute more to the merged state.\nInstead of merging the KV of multiple tokens into one, CHAI [109] observes that heads in multi-head attention often produce highly correlated attention scores for tokens, particularly in the later layers of LLMs. To exploit this"}, {"title": "4.3.2 Cross-layer Merging", "content": "redundancy, CHAI [109", "109": "selects one representative head to perform the attention computation, and the computed attention scores are shared across all heads in the cluster.\n4.3.2 Cross-layer Merging\nMiniCache [99", "99": "merges the Key (and Value) pairs of each token from adjacent similar layers into a single shared representation. Specifically, MiniCache [99"}]}