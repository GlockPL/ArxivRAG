{"title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\nVision-Language AI", "authors": ["Robert Wolfe", "Aayushi Dangol", "Alexis Hiniker", "Bill Howe"], "abstract": "Multimodal AI models capable of associating images and text\nhold promise for numerous domains, ranging from automated\nimage captioning to accessibility applications for blind and\nlow-vision users. However, uncertainty about bias has in some\ncases limited their adoption and availability. In the present\nwork, we study 43 CLIP vision-language models to determine\nwhether they learn human-like facial impression biases, and\nwe find evidence that such biases are reflected across three dis-\ntinct CLIP model families. We show for the first time that the\nthe degree to which a bias is shared across a society predicts\nthe degree to which it is reflected in a CLIP model. Human-like\nimpressions of visually unobservable attributes, like trustwor-\nthiness and sexuality, emerge only in models trained on the\nlargest dataset, indicating that a better fit to uncurated cultural\ndata results in the reproduction of increasingly subtle social\nbiases. Moreover, we use a hierarchical clustering approach to\nshow that dataset size predicts the extent to which the underly-\ning structure of facial impression bias resembles that of facial\nimpression bias in humans. Finally, we show that Stable Dif-\nfusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial\nbiases in Stable Diffusion XL-Turbo. While pretrained CLIP\nmodels may prove useful for scientific studies of bias, they\nwill also require significant dataset curation when intended for\nuse as general-purpose models in a zero-shot setting.", "sections": [{"title": "Introduction", "content": "OpenAI's multimodal GPT-4 powers the beta version of Be\nMy AI, an extension of the Be My Eyes app (Be My Eyes\n2023a,b) that provides \"instantaneous identification, inter-\npretation, and conversational visual assistance\" to blind and\nlow-vision users. Until recently, the app allowed users to\nask questions about images of people and receive live ex-\nplanations. The temporary discontinuation of this feature\nwas motivated by concern that GPT-4 \"would say things it\nshouldn't about people's faces, such as assessing their gender\nor emotional state\" (Hill 2023).\nThe decision belies a broader concern: that by learning to\nassociate language and images, multimodal AI may make\ninsufficiently informed judgments about human attributes\nbased solely on a person's face. When studied in human\nsubjects, this kind of inference is known as a \"first impres-\nsion\" or \"facial impression\u201d bias (Todorov 2017), and it is\nknown to affect consequential spheres of human social life\nsuch as criminal sentencing (Wilson and Rule 2015), employ-\nment decisions (Stoker, Garretsen, and Spreeuwers 2016),\nand political elections (Antonakis and Dalgas 2009). Such\nimpressions can include traits like trustworthiness, which are\nunobservable from a person's face and societally mediated to\nextent that they are consistent in a population (Todorov 2017).\nWhile psychologists have used computational geometry and\nsupervised machine learning approaches to modeling facial\nimpression biases (Blanz and Vetter 2023), it is not known\nwhether semi-supervised vision-language AI models could\ninadvertently learn such biases in pretraining and propagate\nthem to the many domains in which such models are used.\nWhile features permitting facial image analysis are dis-\nabled in GPT-4, the opportunity to study facial impression\nbias is afforded by CLIP (\"Contrastive Language Image Pre-\ntraining\"), a state-of-the-art vision-language model that al-\nlows users to define text classes at inference using natural\nlanguage (Radford et al. 2021). Rather than fine-tuning CLIP\nto model facial impressions similar to prior work using su-\npervised learning, we study this bias in three families of\npretrained CLIP models used in a wide range of multimodal\ncomputer vision tasks: the nine models trained by OpenAI\n(Radford et al. 2021); five \u201cFaceCLIP\u201d models post-trained\nfor facial analysis (Zheng et al. 2022); and 29 \u201cScaling\" mod-\nels trained by Cherti et al. (2022) on systematically differing\namounts of data, allowing for statistical analysis of the effects\nof model and dataset parameters on facial impression bias.\nAnalyzing whether CLIP models learn human-like facial\nimpression biases requires a reliable source of human data.\nThis research uses the authoritative One Million Impressions\n(OMI) dataset of Peterson et al. (2022), which includes 1,004\nimages of faces rated by human participants across 34 at-\ntributes, with which Peterson et al. (2022) learned a super-\nvised model of facial impression biases. In the present work,\nwe used CLIP to compute the similarity of each OMI image\nto text prompts for the 34 attributes, mimicking the task given\nto human subjects, and we compared the CLIP similarities to\nhuman subject ratings. We offer four primary findings:\n1. CLIP models learn societal facial impression biases, in-\ncluding for unobservable traits such as trustworthiness\nand sexuality. Moreover, the extent to which an attribute\""}, {"title": "Related Work", "content": "We review the related work on facial impression bias, vision-\nlanguage AI, and the impact of scale in deep learning."}, {"title": "Facial Impression Bias", "content": "A wealth of psychological research indicates that humans\nmake immediate judgments about the attributes of people they\ndo not know based solely on facial appearance (Willis and\nTodorov 2006; Oh, Buck, and Todorov 2019; Charlesworth\net al. 2019). Information inferred from faces includes char-\nacter traits (like trustworthiness and outgoingness) and so-\ncially constructed group memberships (like gender and eth-\nnicity) as well as relatively objective traits (like hair color\nand weight) (Todorov 2017; Peterson et al. 2022). Research\non first-impression biases in humans has found that the in-\nference of attributes from facial appearance plays a role in\nnumerous consequential domains, including employment de-\ncisions (Stoker, Garretsen, and Spreeuwers 2016; Graham,\nHarvey, and Puri 2017; Swider, Harris, and Gong 2021),\ncriminal sentencing (Wilson and Rule 2015; Johnson and\nKing 2017), and the election of political candidates (Anton-\nakis and Dalgas 2009; Olivola and Todorov 2010; Lenz and\nLawson 2011; J\u00e4ckle et al. 2020). While facial impression\nbiases may be consistent among a population, inferences of\nunobservable attributes such as character traits are inaccurate\nand often reflect societal stereotypes (Sutherland and Young\n2022; Todorov 2017). AI systems are increasingly employed\nto automate or mediate access to information in domains such\nas hiring (Li et al. 2021), political analysis and advertising\n(Papakyriakopoulos et al. 2022), and law (Choi et al. 2023),\nand to the extent that such systems reflect facial impression\nbiases, they may have socially undesirable impacts."}, {"title": "Relationship to Social Group Biases", "content": "Some studies suggest a connection between first-impression\nbiases and demographic traits such as gender and ethnicity.\nOh, Buck, and Todorov (2019) find that gender biases as-\nsociating men with competence are reflected in participant\nimpressions of the competence of faces. Xie et al. (2021) find\nthat the structure of impressions of novel faces is predicted\nby learned social stereotypes about gender and race. Peter-\nson et al. (2022) find that facial impression biases are corre-\nlated with demographics, such that judgments of traits like\n\"cuteness\" are related to age. The relationship between first-\nimpression bias and social stereotypes can have real-world\nconsequences. For example, White phenotypic prototypical-\nity (looking like the average White person) can moderate use\nof force by police (Kahn et al. 2016)."}, {"title": "Computational Models of Facial Impression Bias", "content": "Peterson et al. (2022) use the OMI dataset to model facial im-\npressions using the StyleGAN-2 network (Karras, Laine, and\nAila 2019), and demonstrate its capacity to manipulate faces\nsuch that the average U.S. perceiver would consider them\nsimilar to an attribute (such as trustworthiness). They build\non research on the scientific modeling of facial impression\nbiases, which commonly utilizes techniques including land-\nmark annotations of faces (Turk and Pentland 1991), para-\nmetric three-dimensional mesh modeling (Blanz and Vetter\n2023), geometric morphological analysis (Sano and Kawa-\nbata 2023), and supervised deep learning models (Yu and\nSuchow 2022). As noted by Peterson et al. (2022), creating a\ncomputational model of a bias differs from modeling the at-\ntribute itself (i.e., trying to predict if an individual is trustwor-\nthy from their face, rather than whether the average person\nwould perceive an individual as trustworthy), which would\namount to physiognomy (y Arcas, Mitchell, and Todorov\n2023) for an unobservable attribute like trustworthiness."}, {"title": "CLIP and Vision-Language AI", "content": "The present work studies CLIP, a multimodal vision-\nlanguage model pretrained using a symmetric cross-entropy\nloss (Oord, Li, and Vinyals 2018; Zhang et al. 2020) to\npair images with associated text captions (Radford et al.\n2021). After pretraining, CLIP can rank, retrieve, or classify\nimages based on association with text classes specified at\ninference rather than pre-selected at the time of training,\nmaking it a \"zero-shot\u201d vision-language model (Radford\net al. 2021), as well as a good source for semantically rich\nembeddings (Wolfe and Caliskan 2022b). CLIP is composed\nof a language model (usually GPT2 (Radford et al. 2019)),\nand an image encoder, such as a Vision Transformer (\u201cViT\u201d) \n(Dosovitskiy et al. 2020) or a ResNet (He et al. 2016). The\nlanguage and image models are jointly pretrained, and\nrepresentations are projected into a multimodal embedding\nspace, in which cosine similarity quantifies the similarity\nbetween image and text (Radford et al. 2021). In addition to\nstandard CLIP models, we study \"FaceCLIP\" models trained\nby Zheng et al. (2022), who introduce Facial Representation\nLearning (FaRL), which combines CLIP training with a\nmasked image modeling objective (Xie et al. 2022) and\ntrains on a faces-only subset of LAION-400M. Models\ntrained using FaRL set state-of-the-art on downstream facial\nanalysis tasks such as face parsing (Zheng et al. 2022)."}, {"title": "Text-to-Image Generators", "content": "CLIP is an essential component for many generative text-to-\nimage models. One of the first uses of a CLIP model was to\nprovide training supervision to OpenAI's first DALL-E image\ngenerator model (Ramesh et al. 2021). Other text-to-image\ngenerators like VQGAN-CLIP similarly use CLIP embedding\nspace measurements in their objective function (Crowson\net al. 2022). More recent image generators such as Stable\nDiffusion 2 employ CLIP models as text encoders (Rombach\net al. 2022a), passing CLIP text embeddings to a U-Net or\nsimilar latent diffusion architecture capable of generating an\nimage conditioned on those text embeddings. More recently,\nDALL-E 3 (\"unCLIP\u201d) decodes images directly from a CLIP\nembedding space, translating CLIP text embeddings into\nimage embeddings, and inverting them (Ramesh et al. 2022).\nAs discussed in the Data section, we study open-weight text-to-image generators utilizing a CLIP text encoder.\""}, {"title": "Impact of Scale in Deep Learning and in CLIP", "content": "Research shows that the impact of data scale on deep learning\nmodels is empirically predictable (Hestness et al. 2017) and\nthat task performance scales with training dataset size (Sun"}, {"title": "Bias in Vision-Language AI", "content": "Prior research identifies societal biases in CLIP models,\nwhich have been found to over-represent images of men\nin retrieval contexts (Wang, Liu, and Wang 2021), to exhibit\nsocial stereotypes when used in robotics applications (Hundt\net al. 2022), and to reflect biased cultural defaults related to\nrace and nationality (Wolfe and Caliskan 2022a,c). Genera-\ntive text to image models typically reflect biases of the CLIP\nmodel from which they learn or decode (Luccioni et al. 2023;\nBianchi et al. 2023), as do vision-language chatbots that use\nCLIP as an image encoder (Fraser and Kiritchenko 2024). As\nwith other machine learning models (Hall et al. 2022), bias\nin CLIP can be traced back to its pretraining data, and audits\nof LAION-400M have found examples of racial and gender\nbiases (Birhane, Prabhu, and Kahembwe 2021)."}, {"title": "Synthetic Media for Vision-Language Research", "content": "Our work builds on prior work employing synthetic images\nto study societal biases. Similar to the OMI dataset studied\nin this research, Wolfe, Banaji, and Caliskan (2022) assessed\nclassification biases in CLIP related to images of multiracial\nindividuals by generating images of thousands of faces using\nStyleGAN-2. More recently, Fraser and Kiritchenko (2024)\nuse Midjourney (Midjourney 2024) to generate pairs of im-\nagess of Black and White or Male and Female individuals in\nidentical attire and situations, which they use to assess bias in\nconversational vision-language models such as Instruct-BLIP\n(Dai et al. 2023). The use of synthetic data is well-motivated\nfor bias research, where it may be undesirable to assign values\nrelated to traits like trustworthiness or smugness to the faces\nof human subjects in a dataset intended for public release."}, {"title": "Data", "content": "This research uses the One Million Impressions (OMI)\ndataset and 43 English-language CLIP models trained on\nweb-scraped text-and-image datasets, as well as three text-to-image generators employing CLIP models as text encoders."}, {"title": "The One Million Impressions Dataset", "content": "The OMI dataset is a collection of 1,004 images of human\nfaces produced by Peterson et al. (2022) using StyleGAN-2\n(Karras et al. 2020). Each face is rated by 30 or more human\nparticipants on Amazon Mechanical Turk for 34 attributes.\nFor each attribute, participants rate the face on a sliding scale,\nwhere one end represents one pole of an attribute binary\n(such as \"trustworthy\") and the other end represents the op-\nposing pole of the binary (such as \u201cuntrustworthy\"). The\nOMI dataset records the mean participant rating for each of\nthe 34 attributes. Consistent with Peterson et al. (2022), we\nuse these ratings as measurements of human bias at a societal\nscale, against which CLIP associations can be compared.\""}, {"title": "CLIP Training Data", "content": "We study CLIP models pretrained on one of five datasets,\nordered from smallest to largest:\n\u2022 LAION-Face: A 20-million sample subset of human faces\nand captions filtered from LAION-400M (see below) us-\ning RetinaFace (Deng et al. 2019) and intended for train-\ning facial analysis models (Zheng et al. 2022).\n\u2022 LAION-80M: An 80-million sample subset of LAION-\n2B (see below) created by Cherti et al. (2022) to study\nscaling behavior in CLIP.\n\u2022 LAION-Aesthetics: A 120-million sample subset of aes-\nthetically pleasing images from LAION-5B as determined\nusing a CLIP model (Schuhmann et al. 2022).\n\u2022 WebImageText (WIT): A web-scraped corpus of 400\nmillion images and captions, constructed by Radford et al.\n(2021) from a query list using Wikipedia and WordNet.\n\u2022 LAION-400M An open source collection of 407 million\nimage-text pairs intended to replicate the WIT dataset\n(Schuhmann et al. 2021).\n\u2022 LAION-2B: An open source English-language dataset of\n2.32 billion image-text pairs (Schuhmann et al. 2022)."}, {"title": "Pretrained CLIP Models", "content": "This research studies the following CLIP models:\n\u2022 OpenAI CLIP: 9 models pretrained by Radford et al.\n(2021) on the WIT dataset.\n\u2022 Scaling CLIP: 29 models pretrained by Cherti et al.\n(2022) on LAION-80M, LAION-400M, and LAION-2B\nto study CLIP scaling behavior.\n\u2022 FaceCLIP: 5 models trained on the LAION-Face dataset\nof Zheng et al. (2022). FaceCLIP models post-train from\npretrained OpenAI CLIP-ViT models."}, {"title": "Pretrained Stable Diffusion Models", "content": "This research studies three Stable Diffusion (SD) models:\n\u2022 Stable Diffusion XL-Turbo: A high-resolution text-to-image generator employing adversarial distillation diffu-\nsion to speed up the rate of image generation (Sauer et al.\n2023). Uses a CLIP-ViT-L and CLIP-ViT-bigG for its text\nencoder, and pretrains on an internal dataset.\n\u2022 Runway Stable Diffusion 1.5: A high-resolution text-to-image generator finetuned on LAION-Aesthetics (Rom-\nbach et al. 2022a). Uses a CLIP-ViT-L-14 as the text\nencoder, and pretrains on LAION-5B.\n\u2022 Stable Diffusion 2: A text-to-image generator using a\nCLIP-ViT-H as the text encoder, and pretraining on a\nfiltered subset of LAION-5B (Rombach et al. 2022a)."}, {"title": "Approach", "content": "We used embeddings from 43 CLIP models to compare facial\nimpression biases measured in CLIP to biases measured in\nhumans by Peterson et al. (Peterson et al. 2022), and extended\nsubspace projection methods from prior work to study bias\nin generative text-to-image models."}, {"title": "Obtaining Image and Text Embeddings", "content": "We obtained image embeddings for the 1,004 images in the\nOMI dataset after projection to each CLIP model's text-image\nlaternt space. Text embeddings use the \u201ca photo of image class\u201d\nprompt recommended by Radford et al. (2021). Because OMI\nconsists of images of faces, we modify this prompt to \"a\nphoto of someone who is attribute.\" In keeping with the bi-\nnary sliding scale of Peterson et al. (2022), we computed an\nimage's association with each attribute by subtracting its co-\nsine similarity with one pole of the attribute binary (\u201ca photo\nof someone who has dark hair\u201d) from its similarity with\nopposing pole (\"a photo of someone who has light hair\u201d).\nFormally, given a model m from which embeddings are ob-\ntained, the association $m_a^i$ of an image vector $i_j$ at index $j$\nof the OMI dataset with an attribute a is the difference of the\nvector's cosine similarity with a positive pole text vector $t_a+$\nand its cosine similarity with a negative pole text vector $t_a-$ :\n$m_a^i = cos(i_j, t_a+) \u2013 cos(i_j,t_a-)$\n(1)"}, {"title": "Adjusting Prompts for Negation", "content": "CLIP may fail to adjust for negation in text prompts (Parcal-\nabescu et al. 2021) and can behave like a visual bag-of-words\nmodel (Yuksekgonul et al. 2022). For example, CLIP might\nmatch the text \u201ca photo with no apples", "outgoing": "ttribute uses", "shy": "s the negative text class, rather than", "outgoing.": "his strategy is not viable for\nsome attributes, like those related to ethnicity, which instead\nuse", "someone": "s the negative prompt. The full\nset of text prompts is provided in the appendix."}, {"title": "Computing CLIP Model-Human Similarity", "content": "We denote the ordered set of n=1,004 OMI images as I. The\nvector of associations $m^a$ for a model m with an attribute a\nfor all images $i \u2208 I$ is given by:\n$m^a = (m_0^a, m_1^a,..., m_{n-1}^a, m_n^a)$ (2)\nSimilarly, the vector of human-rated associations $h^a$ for at-\ntribute a for all images $i \u2208 I$ is given by:\n$h^a = (h_0^a,h_1^a,...,h_{n-1}^a, h_n^a)$ (3)\nwhere $h^a_j$ denotes the OMI mean for image $i_j$ at index j. The\nsimilarity $s^m_a$ of bias in a model m for attribute a to human\nbias is given by Spearman's p:\n$s^m_a = p(m^a, h^a)$ (4)"}, {"title": "CAT: Correlated Attribute Test", "content": "We compute the correlation between two attributes in a CLIP\nmodel using a simple test we call the CAT. As above, the\nvector of associations $m^a$ for a model m is given by:\n$m^a = (m_0^a, m_1^a,..., m_{n-1}^a, m_n^a)$ (5)\nThe measurement $CAT_m(a, b)$ between attributes a and b in\na model m is given by Spearman's p:\n$CAT_m(a, b) = p(m^a, m^b)$ (6)"}, {"title": "Subspace Projection for Text-to-Image Models", "content": "We draw on subspace projection methods used by Boluk-\nbasi et al. (2016) and Omrani Sabbaghi, Wolfe, and Caliskan\n(2023) to measure facial impression biases in generative text-\nto-image models. First, we first obtain image embeddings\nfor the 1,004 OMI images from the top layer of a ViT-Large-\nPatch32-384 model pretrained on ImageNet. For each at-\ntribute a, we learn a weights vector $w^a$ predicting the OMI\nattribute ratings $h^a$, corresponding to a semantic subspace in\nthe embeddings for the attribute. We then use a generative\nmodel g to generate N images via a prompt corresponding to\neither the positive (a+) or negative (a\u00af) pole of an attribute.\nWe embed each generated image $g_j$ at position j with the\nViT-Large-Patch32-384 to obtain the vector $g_j$, and compute\nan attribute association $g^a$ as its projection product with $w^a$:\n$g_j^a = \\frac{g_j w^a}{||w^a||}$ (7)\nThe vector of associations $g^a$ for a generative model g with\nan attribute a is given by:\n$g^a = (g_0^a, g_1^a,..., g_{n-1}^a, g_n^a)$ (8)"}, {"title": "Experiments", "content": "Four experiments test the existence of human-like facial im-\npression bias in vision-language AI, with consideration given\nto Human IRR, model and dataset scale, and downstream\nimpact in image generation."}, {"title": "Model vs. Human Biases", "content": "We tested whether OpenAI, Scaling, and FaceCLIP CLIP\nmodels reflect human-like facial impression biases. We ob-\ntained the human-model similarity $s^m_a$ for each model m with\neach attribute a for the 34 OMI attributes. We then compared\nthe mean human-model similarity for each group of mod-\nels to the Human IRR for the attribute reported by Peterson\net al. (2022), calculating Pearson's p between Human IRR\nand model-human similarity for each of the 34 attributes. A\nlarge coefficient indicates that the more societally consistent\na facial impression bias (i.e., as Human IRR increases), the\nmore likely the bias is to be learned during semi-supervised\nCLIP training. We also computed Pearson's p pairwise be-\ntween OpenAI, Scaling, and FaRL models to assess whether\nmodels trained on different datasets learn similar biases."}, {"title": "Effects of Dataset Scale", "content": "We calculated human-model similarity $s^m_a$ for each model\nm with each attribute a for the 34 attributes studied, and\nwe constructed a multiple linear regression to predict the\nmodel-human similarity $s^m_a$ for a given CLIP model m and\nan attribute a. We examined the 27 CLIP models trained by\nCherti et al. (2022) and produced via the combination of\nthree CLIP architectures (ViT-B32, ViT-B16, and ViT-L14),\nthree dataset sizes (80M, 400M, 2B), and three total train-\ning example counts (3B, 13B, 34B). Independent variables\ninclude Human IRR (from Peterson et al. (Peterson et al.\n2022)), as well as Dataset Size, Model Parameter Count, and\nTotal Training Examples (from Cherti et al. (2022)). We nor-\nmalized variables with range outside of (0, 1) by dividing by\ntheir max. We conducted post hoc comparisons between each\nlevel of scale (80m, 400m, 2b), using paired-samples t-tests."}, {"title": "Structure of Facial Impressions", "content": "We computed the correlation matrix $C^m_{a,b}$ for the 27 CLIP\nmodels studied in the Dataset Scale analysis by obtaining\n$CAT_m(a, b)$ for every attribute pair $a \u2208 A$ and $b \u2208 A$. We\ncomputed a corresponding matrix $C_h$ by obtaining corre-\nlations for the same attributes using the human ratings of\nthe OMI dataset. We then measured the similarity of $C^m_{a,b}$\nand $C_h$ based on the normalized Frobenius inner product\n$F_{m,h}$. We used a one-way ANOVA to test for differences in\n$F_{m,h}$ between the 80M, 400M, and 2B models. We used a\npaired-samples t-test to conduct post hoc comparisons.\nPeterson et al. (2022) study the structure of facial impres-\nsions by computing the correlation matrix of OMI ratings and\nqualitatively examining its hierarchical structure. The present\nresearch also qualitatively compares the structure of the OMI\ncorrelation matrix to the hierarchically clustered correlation\nmatrix of a CLIP-ViT-L-14 trained on LAION-2B for 13-billion total samples. If the structure of OMI biases is similar\nto CLIP, we expect to observe similar attribute clusters at\nhigher levels, with differences emerging in leaf nodes."}, {"title": "Generative Text-to-Image Models", "content": "Finally, we extended the analysis to SDXL-Turbo, SD2, and\nRunway SD1.5. We first studied the similarity of these mod-\nels' representations to human attribute ratings. To do so, we\ngenerated N=25 images for each attribute's positive pole and\n25 for its negative pole. We adjusted prompts for the models\nto \"a realistic portrait photo of someone who is attribute,\u201d\nbecause image generators may create cartoonish images that\ndo not center the face. We extracted embeddings for these\nimages and computed their projection products $g_{a+}$ and $g_{a-}$.\nIf generative text-to-image models reflect human-like facial\nimpression biases, we expect images generated from a posi-\ntive prompt to have positive projections, and images from a\nnegative prompt to have negative projections. We thus frame\nmodel-human similarity as a classification problem, wherein\nimages generated from the positive pole prompt receives a\nlabel of 1, and from the negative pole prompt receive a label\nof 0. The OMI subspace is positioned as a classifier, which\npredicts 1 where an image vector's projection product is pos-\nitive, and 0 where it is negative. We report Recall, Precision,\nand F1 Score. We validated this approach using the Outdoors\nattribute, a control group for measuring validity in the OMI\ndataset, obtaining F1=.94 for SDXL-Turbo\nWe then study social bias in Stable Diffusion XL-Turbo\nby projecting the positive prompt images generated for the\nWhite and Black attributes onto all 34 of the OMI attribute\nsubspaces. For each subspace, we compute the White-Black\ndifferential bias by obtaining an effect size (Cohen's d) be-\ntween the projection products $g_{White+}$ and $g_{Black+}$, and we\nthen measure statistical significance using a paired samples\nt-test."}, {"title": "Results", "content": "Our results indicate that 1) CLIP models exhibit facial im-\npression biases; 2) Human IRR is a significant predictor of\nwhich biases are learned; 3) models trained on larger datasets\nexhibit emergence of subjective facial impression biases, and\nexhibit high model-human similarity, as do many socially\nconstructed attributes such as gender and cuteness. Notable\nexceptions include Black, White, and skin color attributes,\nwhich fall short of expectations based on Human IRR, as\nvisualized in Figure 3. Model-human similarities of traits\nlike Trustworthiness, Electability, and Intelligence are statis-\ntically significant but lower, consistent with the lower IRR\nof these attributes. That CLIP learns these biases at all is\nnoteworthy: to our knowledge, this is the first research to\ndocument unobservable facial impression biases learned by a\nsemi-supervised vision-language model (rather than a super-\nvised model of facial impression biases) that are consistent\nwith human societal biases.\nAs described in Figure 4, all three families of models\nexhibit strong correlations ranging from .72 to .76 between\nthe mean model-human similarity of a trait and its Human\nIRR. Coefficients are larger between OpenAI and FaceCLIP\nmodels than with Scaling CLIP models, likely a result of\nFaceCLIP post-training from OpenAI base models."}, {"title": "Dataset Scale", "content": "A multiple linear regression finds that only Human IRR and\nDataset Size are statistically significant predictors of model-human similarity. As described in Table 2, Human IRR plays\nthe larger role of the two independent variables, as evidenced\nby a much larger coefficient and t-value. Total Training Sam-\nples, Image Parameters, and Text Parameters are not sta-tistically significant predictors of facial impression bias in"}, {"title": "Structure of Facial Impressions", "content": "Results indicate that dataset size impacts the extent to which\nthe structure of facial impression bias in CLIP reflects the\nstructure of the facial impression bias in humans. Figure 5\nvisualizes the hierarchical similarities between the attribute\ncross-correlation matrix for CLIP-ViT-L-14 (the most com-\nmonly used CLIP model as of this writing) and the OMI\nattribute cross-correlation matrix. The most salient similari-ties between the two include a cluster of correlated racial andethnic identities, such as Hispanic, Middle-Eastern, Native\nAmerican, and Pacific Islander, as well as a cluster grouping"}, {"title": "Generative Text-to-Image Models", "content": "As shown in Table 4, we observe more variance in text-to-image generator F1 scores than in CLIP model-human sim-ilarities. SDXL has the most human-like associations, withSpearman's p = .60, p < .001 between Human IRR andSDXL F1 scores. Runway-SD1.5 is also correlated with IRR,with p = .50, p < .01, while SD2 is not significantly corre-lated with IRR, with p = .32, p = .06. Notably, the Attrac-tive attribute has the highest F1 score for SDXL, whereas it isthe 16th most human-similar attribute in CLIP models, sug-gesting the importance of representing beauty for user-facingimage generators, which"}]}