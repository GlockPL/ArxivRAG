{"title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI", "authors": ["Robert Wolfe", "Aayushi Dangol", "Alexis Hiniker", "Bill Howe"], "abstract": "Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we use a hierarchical clustering approach to show that dataset size predicts the extent to which the underlying structure of facial impression bias resembles that of facial impression bias in humans. Finally, we show that Stable Diffusion models employing CLIP as a text encoder learn facial impression biases, and that these biases intersect with racial biases in Stable Diffusion XL-Turbo. While pretrained CLIP models may prove useful for scientific studies of bias, they will also require significant dataset curation when intended for use as general-purpose models in a zero-shot setting.", "sections": [{"title": "Introduction", "content": "OpenAI's multimodal GPT-4 powers the beta version of Be My AI, an extension of the Be My Eyes app (Be My Eyes 2023a,b) that provides \"instantaneous identification, interpretation, and conversational visual assistance\" to blind and low-vision users. Until recently, the app allowed users to ask questions about images of people and receive live explanations. The temporary discontinuation of this feature was motivated by concern that GPT-4 \"would say things it shouldn't about people's faces, such as assessing their gender or emotional state\" (Hill 2023).\nThe decision belies a broader concern: that by learning to associate language and images, multimodal AI may make insufficiently informed judgments about human attributes based solely on a person's face. When studied in human subjects, this kind of inference is known as a \"first impression\" or \"facial impression\u201d bias (Todorov 2017), and it is known to affect consequential spheres of human social life such as criminal sentencing (Wilson and Rule 2015), employment decisions (Stoker, Garretsen, and Spreeuwers 2016), and political elections (Antonakis and Dalgas 2009). Such impressions can include traits like trustworthiness, which are unobservable from a person's face and societally mediated to extent that they are consistent in a population (Todorov 2017). While psychologists have used computational geometry and supervised machine learning approaches to modeling facial impression biases (Blanz and Vetter 2023), it is not known whether semi-supervised vision-language AI models could inadvertently learn such biases in pretraining and propagate them to the many domains in which such models are used.\nWhile features permitting facial image analysis are disabled in GPT-4, the opportunity to study facial impression bias is afforded by CLIP (\"Contrastive Language Image Pre-training\"), a state-of-the-art vision-language model that allows users to define text classes at inference using natural language (Radford et al. 2021). Rather than fine-tuning CLIP to model facial impressions similar to prior work using supervised learning, we study this bias in three families of pretrained CLIP models used in a wide range of multimodal computer vision tasks: the nine models trained by OpenAI (Radford et al. 2021); five \u201cFaceCLIP\u201d models post-trained for facial analysis (Zheng et al. 2022); and 29 \u201cScaling\" models trained by Cherti et al. (2022) on systematically differing amounts of data, allowing for statistical analysis of the effects of model and dataset parameters on facial impression bias.\nAnalyzing whether CLIP models learn human-like facial impression biases requires a reliable source of human data. This research uses the authoritative One Million Impressions (OMI) dataset of Peterson et al. (2022), which includes 1,004 images of faces rated by human participants across 34 attributes, with which Peterson et al. (2022) learned a supervised model of facial impression biases. In the present work, we used CLIP to compute the similarity of each OMI image to text prompts for the 34 attributes, mimicking the task given to human subjects, and we compared the CLIP similarities to human subject ratings. We offer four primary findings:\n1. CLIP models learn societal facial impression biases, including for unobservable traits such as trustworthiness and sexuality. Moreover, the extent to which an attribute"}, {"title": "Related Work", "content": "We review the related work on facial impression bias, vision-language AI, and the impact of scale in deep learning."}, {"title": "Facial Impression Bias", "content": "A wealth of psychological research indicates that humans make immediate judgments about the attributes of people they do not know based solely on facial appearance (Willis and Todorov 2006; Oh, Buck, and Todorov 2019; Charlesworth et al. 2019). Information inferred from faces includes character traits (like trustworthiness and outgoingness) and socially constructed group memberships (like gender and ethnicity) as well as relatively objective traits (like hair color and weight) (Todorov 2017; Peterson et al. 2022). Research on first-impression biases in humans has found that the inference of attributes from facial appearance plays a role in numerous consequential domains, including employment decisions (Stoker, Garretsen, and Spreeuwers 2016; Graham, Harvey, and Puri 2017; Swider, Harris, and Gong 2021), criminal sentencing (Wilson and Rule 2015; Johnson and King 2017), and the election of political candidates (Antonakis and Dalgas 2009; Olivola and Todorov 2010; Lenz and Lawson 2011; J\u00e4ckle et al. 2020). While facial impression biases may be consistent among a population, inferences of unobservable attributes such as character traits are inaccurate and often reflect societal stereotypes (Sutherland and Young 2022; Todorov 2017). AI systems are increasingly employed to automate or mediate access to information in domains such as hiring (Li et al. 2021), political analysis and advertising (Papakyriakopoulos et al. 2022), and law (Choi et al. 2023), and to the extent that such systems reflect facial impression biases, they may have socially undesirable impacts."}, {"title": "Relationship to Social Group Biases", "content": "Some studies suggest a connection between first-impression biases and demographic traits such as gender and ethnicity. Oh, Buck, and Todorov (2019) find that gender biases associating men with competence are reflected in participant impressions of the competence of faces. Xie et al. (2021) find that the structure of impressions of novel faces is predicted by learned social stereotypes about gender and race. Peterson et al. (2022) find that facial impression biases are correlated with demographics, such that judgments of traits like \"cuteness\" are related to age. The relationship between first-impression bias and social stereotypes can have real-world consequences. For example, White phenotypic prototypicality (looking like the average White person) can moderate use of force by police (Kahn et al. 2016)."}, {"title": "Computational Models of Facial Impression Bias", "content": "Peterson et al. (2022) use the OMI dataset to model facial impressions using the StyleGAN-2 network (Karras, Laine, and Aila 2019), and demonstrate its capacity to manipulate faces such that the average U.S. perceiver would consider them similar to an attribute (such as trustworthiness). They build on research on the scientific modeling of facial impression biases, which commonly utilizes techniques including landmark annotations of faces (Turk and Pentland 1991), parametric three-dimensional mesh modeling (Blanz and Vetter 2023), geometric morphological analysis (Sano and Kawabata 2023), and supervised deep learning models (Yu and Suchow 2022). As noted by Peterson et al. (2022), creating a computational model of a bias differs from modeling the attribute itself (i.e., trying to predict if an individual is trustworthy from their face, rather than whether the average person would perceive an individual as trustworthy), which would amount to physiognomy (y Arcas, Mitchell, and Todorov 2023) for an unobservable attribute like trustworthiness."}, {"title": "CLIP and Vision-Language AI", "content": "The present work studies CLIP, a multimodal vision-language model pretrained using a symmetric cross-entropy loss (Oord, Li, and Vinyals 2018; Zhang et al. 2020) to pair images with associated text captions (Radford et al. 2021). After pretraining, CLIP can rank, retrieve, or classify images based on association with text classes specified at inference rather than pre-selected at the time of training, making it a \"zero-shot\u201d vision-language model (Radford et al. 2021), as well as a good source for semantically rich embeddings (Wolfe and Caliskan 2022b). CLIP is composed of a language model (usually GPT2 (Radford et al. 2019)), and an image encoder, such as a Vision Transformer (\u201cViT\u201d) (Dosovitskiy et al. 2020) or a ResNet (He et al. 2016). The language and image models are jointly pretrained, and representations are projected into a multimodal embedding space, in which cosine similarity quantifies the similarity between image and text (Radford et al. 2021). In addition to standard CLIP models, we study \"FaceCLIP\" models trained by Zheng et al. (2022), who introduce Facial Representation Learning (FaRL), which combines CLIP training with a masked image modeling objective (Xie et al. 2022) and trains on a faces-only subset of LAION-400M. Models trained using FaRL set state-of-the-art on downstream facial analysis tasks such as face parsing (Zheng et al. 2022)."}, {"title": "Text-to-Image Generators", "content": "CLIP is an essential component for many generative text-to-image models. One of the first uses of a CLIP model was to provide training supervision to OpenAI's first DALL-E image generator model (Ramesh et al. 2021). Other text-to-image generators like VQGAN-CLIP similarly use CLIP embedding space measurements in their objective function (Crowson et al. 2022). More recent image generators such as Stable Diffusion 2 employ CLIP models as text encoders (Rombach et al. 2022a), passing CLIP text embeddings to a U-Net or similar latent diffusion architecture capable of generating an image conditioned on those text embeddings. More recently, DALL-E 3 (\"unCLIP\u201d) decodes images directly from a CLIP embedding space, translating CLIP text embeddings into image embeddings, and inverting them (Ramesh et al. 2022). As discussed in the Data section, we study open-weight text-to-image generators utilizing a CLIP text encoder."}, {"title": "Impact of Scale in Deep Learning and in CLIP", "content": "Research shows that the impact of data scale on deep learning models is empirically predictable (Hestness et al. 2017) and that task performance scales with training dataset size (Sun et al. 2017; Brown et al. 2020). Zhai et al. (2022) empirically demonstrate that both model and data scale impact visual task performance, and set new state of the art on Imagenet (Deng et al. 2009) by efficiently scaling a ViT. In CLIP models, Cherti et al. (2022) demonstrate a relationship between pretraining data scale and task performance. Prior work also demonstrates increases in hate speech in CLIP models trained on larger uncurated datasets (Birhane et al. 2024)."}, {"title": "Bias in Vision-Language AI", "content": "Prior research identifies societal biases in CLIP models, which have been found to over-represent images of men in retrieval contexts (Wang, Liu, and Wang 2021), to exhibit social stereotypes when used in robotics applications (Hundt et al. 2022), and to reflect biased cultural defaults related to race and nationality (Wolfe and Caliskan 2022a,c). Generative text to image models typically reflect biases of the CLIP model from which they learn or decode (Luccioni et al. 2023; Bianchi et al. 2023), as do vision-language chatbots that use CLIP as an image encoder (Fraser and Kiritchenko 2024). As with other machine learning models (Hall et al. 2022), bias in CLIP can be traced back to its pretraining data, and audits of LAION-400M have found examples of racial and gender biases (Birhane, Prabhu, and Kahembwe 2021)."}, {"title": "Synthetic Media for Vision-Language Research", "content": "Our work builds on prior work employing synthetic images to study societal biases. Similar to the OMI dataset studied in this research, Wolfe, Banaji, and Caliskan (2022) assessed classification biases in CLIP related to images of multiracial individuals by generating images of thousands of faces using StyleGAN-2. More recently, Fraser and Kiritchenko (2024) use Midjourney (Midjourney 2024) to generate pairs of images of Black and White or Male and Female individuals in identical attire and situations, which they use to assess bias in conversational vision-language models such as Instruct-BLIP (Dai et al. 2023). The use of synthetic data is well-motivated for bias research, where it may be undesirable to assign values related to traits like trustworthiness or smugness to the faces of human subjects in a dataset intended for public release."}, {"title": "Data", "content": "This research uses the One Million Impressions (OMI) dataset and 43 English-language CLIP models trained on web-scraped text-and-image datasets, as well as three text-to-image generators employing CLIP models as text encoders."}, {"title": "The One Million Impressions Dataset", "content": "The OMI dataset is a collection of 1,004 images of human faces produced by Peterson et al. (2022) using StyleGAN-2 (Karras et al. 2020). Each face is rated by 30 or more human participants on Amazon Mechanical Turk for 34 attributes. For each attribute, participants rate the face on a sliding scale, where one end represents one pole of an attribute binary (such as \"trustworthy\") and the other end represents the opposing pole of the binary (such as \u201cuntrustworthy\"). The OMI dataset records the mean participant rating for each of the 34 attributes. Consistent with Peterson et al. (2022), we use these ratings as measurements of human bias at a societal scale, against which CLIP associations can be compared."}, {"title": "CLIP Training Data", "content": "We study CLIP models pretrained on one of five datasets, ordered from smallest to largest:\n\u2022 LAION-Face: A 20-million sample subset of human faces and captions filtered from LAION-400M (see below) using RetinaFace (Deng et al. 2019) and intended for training facial analysis models (Zheng et al. 2022).\n\u2022 LAION-80M: An 80-million sample subset of LAION-2B (see below) created by Cherti et al. (2022) to study scaling behavior in CLIP.\n\u2022 LAION-Aesthetics: A 120-million sample subset of aesthetically pleasing images from LAION-5B as determined using a CLIP model (Schuhmann et al. 2022).\n\u2022 WebImageText (WIT): A web-scraped corpus of 400 million images and captions, constructed by Radford et al. (2021) from a query list using Wikipedia and WordNet.\n\u2022 LAION-400M An open source collection of 407 million image-text pairs intended to replicate the WIT dataset (Schuhmann et al. 2021).\n\u2022 LAION-2B: An open source English-language dataset of 2.32 billion image-text pairs (Schuhmann et al. 2022)."}, {"title": "Pretrained CLIP Models", "content": "This research studies the following CLIP models:\n\u2022 OpenAI CLIP: 9 models pretrained by Radford et al. (2021) on the WIT dataset.\n\u2022 Scaling CLIP: 29 models pretrained by Cherti et al. (2022) on LAION-80M, LAION-400M, and LAION-2B to study CLIP scaling behavior.\n\u2022 FaceCLIP: 5 models trained on the LAION-Face dataset of Zheng et al. (2022). FaceCLIP models post-train from pretrained OpenAI CLIP-ViT models."}, {"title": "Pretrained Stable Diffusion Models", "content": "This research studies three Stable Diffusion (SD) models:\n\u2022 Stable Diffusion XL-Turbo: A high-resolution text-to-image generator employing adversarial distillation diffusion to speed up the rate of image generation (Sauer et al. 2023). Uses a CLIP-ViT-L and CLIP-ViT-bigG for its text encoder, and pretrains on an internal dataset.\n\u2022 Runway Stable Diffusion 1.5: A high-resolution text-to-image generator finetuned on LAION-Aesthetics (Rombach et al. 2022a). Uses a CLIP-ViT-L-14 as the text encoder, and pretrains on LAION-5B.\n\u2022 Stable Diffusion 2: A text-to-image generator using a CLIP-ViT-H as the text encoder, and pretraining on a filtered subset of LAION-5B (Rombach et al. 2022a)."}, {"title": "Approach", "content": "We used embeddings from 43 CLIP models to compare facial impression biases measured in CLIP to biases measured in humans by Peterson et al. (Peterson et al. 2022), and extended subspace projection methods from prior work to study bias in generative text-to-image models."}, {"title": "Obtaining Image and Text Embeddings", "content": "We obtained image embeddings for the 1,004 images in the OMI dataset after projection to each CLIP model's text-image latent space. Text embeddings use the \u201ca photo of image class\" prompt recommended by Radford et al. (2021). Because OMI consists of images of faces, we modify this prompt to \"a photo of someone who is attribute.\" In keeping with the binary sliding scale of Peterson et al. (2022), we computed an image's association with each attribute by subtracting its cosine similarity with one pole of the attribute binary (\u201ca photo of someone who has dark hair\u201d) from its similarity with opposing pole (\"a photo of someone who has light hair\"). Formally, given a model m from which embeddings are obtained, the association $m_a$ of an image vector $i_j$ at index j of the OMI dataset with an attribute a is the difference of the vector's cosine similarity with a positive pole text vector $t_a^+$ and its cosine similarity with a negative pole text vector $t_a^-$ :"}, {"title": "Adjusting Prompts for Negation", "content": "CLIP may fail to adjust for negation in text prompts (Parcal-abescu et al. 2021) and can behave like a visual bag-of-words model (Yuksekgonul et al. 2022). For example, CLIP might match the text \u201ca photo with no apples\" to a photo of apples, due to how unlikely it is for a text caption (i.e., CLIP's training supervision) to describe something not present in the photo. To adjust for this, negative pole prompts were chosen such that they did not simply negate the positive class. For example, the \"outgoing\" attribute uses \"a photo of someone who is shy\" as the negative text class, rather than \"a photo of someone who is not outgoing.\" This strategy is not viable for some attributes, like those related to ethnicity, which instead use \"a photo of someone\" as the negative prompt. The full set of text prompts is provided in the appendix."}, {"title": "Computing CLIP Model-Human Similarity", "content": "We denote the ordered set of n=1,004 OMI images as I. The vector of associations $m^a$ for a model m with an attribute a for all images $i \\in I$ is given by:"}, {"title": "CAT: Correlated Attribute Test", "content": "We compute the correlation between two attributes in a CLIP model using a simple test we call the CAT. As above, the vector of associations $m^a$ for a model m is given by:"}, {"title": "Subspace Projection for Text-to-Image Models", "content": "We draw on subspace projection methods used by Bolukbasi et al. (2016) and Omrani Sabbaghi, Wolfe, and Caliskan (2023) to measure facial impression biases in generative text-to-image models. First, we first obtain image embeddings for the 1,004 OMI images from the top layer of a ViT-Large-Patch32-384 model pretrained on ImageNet. For each attribute a, we learn a weights vector $w_a$ predicting the OMI attribute ratings $h^a$, corresponding to a semantic subspace in the embeddings for the attribute. We then use a generative model g to generate N images via a prompt corresponding to either the positive ($a^+$) or negative ($a^\u2212$) pole of an attribute. We embed each generated image $g_j$ at position j with the ViT-Large-Patch32-384 to obtain the vector $g_j$, and compute an attribute association $g^a_j$ as its projection product with $w_a$:\nThe vector of associations $g^a$ for a generative model g with an attribute a is given by:"}, {"title": "Experiments", "content": "Four experiments test the existence of human-like facial impression bias in vision-language AI, with consideration given to Human IRR, model and dataset scale, and downstream impact in image generation."}, {"title": "Model vs. Human Biases", "content": "We tested whether OpenAI, Scaling, and FaceCLIP CLIP models reflect human-like facial impression biases. We obtained the human-model similarity $s_m^a$ for each model m with each attribute a for the 34 OMI attributes. We then compared the mean human-model similarity for each group of models to the Human IRR for the attribute reported by Peterson et al. (2022), calculating Pearson's p between Human IRR and model-human similarity for each of the 34 attributes. A large coefficient indicates that the more societally consistent a facial impression bias (i.e., as Human IRR increases), the more likely the bias is to be learned during semi-supervised CLIP training. We also computed Pearson's p pairwise between OpenAI, Scaling, and FaRL models to assess whether models trained on different datasets learn similar biases."}, {"title": "Effects of Dataset Scale", "content": "We calculated human-model similarity $s_m^a$ for each model m with each attribute a for the 34 attributes studied, and we constructed a multiple linear regression to predict the model-human similarity $s_m^a$ for a given CLIP model m and an attribute a. We examined the 27 CLIP models trained by Cherti et al. (2022) and produced via the combination of three CLIP architectures (ViT-B32, ViT-B16, and ViT-L14), three dataset sizes (80M, 400M, 2B), and three total training example counts (3B, 13B, 34B). Independent variables include Human IRR (from Peterson et al. (Peterson et al. 2022)), as well as Dataset Size, Model Parameter Count, and Total Training Examples (from Cherti et al. (2022)). We normalized variables with range outside of (0, 1) by dividing by their max. We conducted post hoc comparisons between each level of scale (80m, 400m, 2b), using paired-samples t-tests."}, {"title": "Structure of Facial Impressions", "content": "We computed the correlation matrix $C^m_{ab}$ for the 27 CLIP models studied in the Dataset Scale analysis by obtaining $CAT_m(a, b)$ for every attribute pair a \u2208 A and b \u2208 A. We computed a corresponding matrix $C_h$ by obtaining correlations for the same attributes using the human ratings of the OMI dataset. We then measured the similarity of $C_m$ and $C_h$ based on the normalized Frobenius inner product $F_{m,h}$. We used a one-way ANOVA to test for differences in $F_{m,h}$ between the 80M, 400M, and 2B models. We used a paired-samples t-test to conduct post hoc comparisons.\nPeterson et al. (2022) study the structure of facial impressions by computing the correlation matrix of OMI ratings and qualitatively examining its hierarchical structure. The present research also qualitatively compares the structure of the OMI correlation matrix to the hierarchically clustered correlation matrix of a CLIP-ViT-L-14 trained on LAION-2B for 13-billion total samples. If the structure of OMI biases is similar to CLIP, we expect to observe similar attribute clusters at higher levels, with differences emerging in leaf nodes."}, {"title": "Generative Text-to-Image Models", "content": "Finally, we extended the analysis to SDXL-Turbo, SD2, and Runway SD1.5. We first studied the similarity of these models' representations to human attribute ratings. To do so, we generated N=25 images for each attribute's positive pole and 25 for its negative pole. We adjusted prompts for the models to \u201ca realistic portrait photo of someone who is attribute,\u201d because image generators may create cartoonish images that do not center the face. We extracted embeddings for these images and computed their projection products $g^{a+}$ and $g^{a-}$. If generative text-to-image models reflect human-like facial impression biases, we expect images generated from a positive prompt to have positive projections, and images from a negative prompt to have negative projections. We thus frame model-human similarity as a classification problem, wherein images generated from the positive pole prompt receives a label of 1, and from the negative pole prompt receive a label of 0. The OMI subspace is positioned as a classifier, which predicts 1 where an image vector's projection product is positive, and 0 where it is negative. We report Recall, Precision, and F1 Score. We validated this approach using the Outdoors attribute, a control group for measuring validity in the OMI dataset, obtaining F1=.94 for SDXL-Turbo\nWe then study social bias in Stable Diffusion XL-Turbo by projecting the positive prompt images generated for the White and Black attributes onto all 34 of the OMI attribute subspaces. For each subspace, we compute the White-Black differential bias by obtaining an effect size (Cohen's d) between the projection products $g^{White+}$ and $g^{Black+}$, and we then measure statistical significance using a paired samples t-test."}, {"title": "Results", "content": "Our results indicate that 1) CLIP models exhibit facial impression biases; 2) Human IRR is a significant predictor of which biases are learned; 3) models trained on larger datasets exhibit emergence of subjective facial impression biases, and exhibit high model-human similarity, as do many socially constructed attributes such as gender and cuteness. Notable exceptions include Black, White, and skin color attributes, which fall short of expectations based on Human IRR, as visualized in Figure 3. Model-human similarities of traits like Trustworthiness, Electability, and Intelligence are statistically significant but lower, consistent with the lower IRR of these attributes. That CLIP learns these biases at all is noteworthy: to our knowledge, this is the first research to document unobservable facial impression biases learned by a semi-supervised vision-language model (rather than a supervised model of facial impression biases) that are consistent with human societal biases.\nAs described in Figure 4, all three families of models exhibit strong correlations ranging from .72 to .76 between the mean model-human similarity of a trait and its Human IRR. Coefficients are larger between OpenAI and FaceCLIP models than with Scaling CLIP models, likely a result of FaceCLIP post-training from OpenAI base models."}, {"title": "Dataset Scale", "content": "A multiple linear regression finds that only Human IRR and Dataset Size are statistically significant predictors of model-human similarity. As described in Table 2, Human IRR plays the larger role of the two independent variables, as evidenced by a much larger coefficient and t-value. Total Training Samples, Image Parameters, and Text Parameters are not statistically significant predictors of facial impression bias in vision-language models.\nTable 1 describes the mean (with standard deviation) and maximum model-human similarity for each dataset size, and it reports Cohen's d between the groups of models trained on the three dataset sizes. Effect sizes obtained between the 400M and the 80M level are large and statistically significant for 17 out of 34 attributes, with large absolute differences between attribute means, such as .41 for Cute at the 80M level vs. .67 at the 400M level. For most attributes, comparisons are not statistically significant between the 2B and 400M levels, though they may return small or medium effect sizes. A notable exception emerges for several unobservable attributes, including Trustworthiness and Sexuality, which exhibit large effect sizes and statistically significant differences between the 2B level and the 400M level. While observable attributes such as Happpiness reflect human ratings well at the 80M level, and more subjective but still visually observable attributes like Cute are reflected consistently at the 400M level, it is not until the 2B level that CLIP models reflect subjective and visually unobservable attributes such as Trustworthiness. The results indicate that increases in the scale of the pretraining data have more significant effects for learning subtle societal biases reflecting attributes with lower IRR. Models trained on additional data approximate a distribution that more closely reflects the perceptions of society as a whole, learning to use the biased visual heuristics present in the human-authored captions in the pretraining data, even for unobservable attributes."}, {"title": "Structure of Facial Impressions", "content": "Results indicate that dataset size impacts the extent to which the structure of facial impression bias in CLIP reflects the structure of the facial impression bias in humans. Figure 5 visualizes the hierarchical similarities between the attribute cross-correlation matrix for CLIP-ViT-L-14 (the most commonly used CLIP model as of this writing) and the OMI attribute cross-correlation matrix. The most salient similarities between the two include a cluster of correlated racial and ethnic identities, such as Hispanic, Middle-Eastern, Native American, and Pacific Islander, as well as a cluster grouping together the Smugness, Gender, Sexuality, and Age attributes. There are also differences between the model and human ratings: while Trustworthy is correlated with Cute in OMI, it is correlated with Smart and Happy in CLIP. Similarities between CLIP attribute correlations and OMI attribute correlations are more evident at the higher levels of the hierarchy, with differences appearing toward the leaf nodes.\nA one-way ANOVA provides evidence that similarities in the structure of facial impression biases increase for models trained on larger datasets, with F(2) = 15.71, p < .001 after Bonferonni correction, demonstrating statistically significant differences in $F_{m,h}$ between the Scaling-2B, Scaling-400M, and Scaling-80M models. As described in Table 3, we observe statistically significant differences and large effect sizes both between the 80M and 400M levels and between the 400M and 2B levels. Figure 6 further illustrates the differences among the three model groups, showing that the magnitude of difference is greater between the 80M level and the 400M level than between the 400M level and the 2B level."}, {"title": "Generative Text-to-Image Models", "content": "As shown in Table 4, we observe more variance in text-to-image generator F1 scores than in CLIP model-human similarities. SDXL has the most human-like associations, with Spearman's p = .60, p < .001 between Human IRR and SDXL F1 scores. Runway-SD1.5 is also correlated with IRR, with p = .50, p < .01, while SD2 is not significantly correlated with IRR, with p = .32, p = .06. Notably, the Attractive attribute has the highest F1 score for SDXL, whereas it is the 16th most human-similar attribute in CLIP models, suggesting the importance of representing beauty for user-facing image generators, which often undergo additional training to better reflect user aesthetic preferences (Rombach et al. 2022b). With the exception of Liberal, unobservable traits rank in the bottom half of F1 scores for SDXL, and Trustworthy is lowest of any trait. Though SD2 and Runway-SD1.5 are more human-like than SDXL for trustworthiness, the results suggest that exploiting biased heuristics may be more straightforward for a classifier like CLIP.\nImages generated by Stable Diffusion XL-Turbo also bear signs of racial bias: as shown in Figure 7, we observe statistically significant differences indicating that generated images of White individuals are more likely to be perceived as dominant, privileged, memorable, attractive, electable, and happy than images of Black individuals, which are more likely to be perceived as more liberal and heavy (vs. thin). Note that many of these relationships are not observed in the OMI correlation clusters seen in Figure 5, indicating that they originate not with the OMI dataset but with the text-to-image model."}, {"title": "Discussion", "content": "Our results make clear the inter-connection of visual perception in AI with the human social world: where a facial impression bias is more consistently shared among humans, CLIP is also more likely to learn it. Facial impression biases have notable consequences in professional and civic life (Stoker, Garretsen, and Spreeuwers 2016), and prove difficult to dislodge even after intervention (Jaeger et al. 2020). While CLIP may serve as a tool for studying such biases, it may also reinforce or amplify these biases in society, especially given their presence in user-facing image generators."}, {"title": "Scale and Bias", "content": "Training on larger datasets results in emergent and amplified facial impression biases. CLIP models exhibit more human-like biases related to trustworthiness and sexuality when trained on LAION-2B, and nearly every OMI attribute increases in model-human similarity between the 80M level and the 400M level. That model parameterization plays no detectable role in facial impression bias underlines that what CLIP models have learned is a biased visual heuristic reflected in the training data, not a more precise"}]}