{"title": "The Disparate Benefits of Deep Ensembles", "authors": ["Kajetan Schweighofer", "Adrian Arnaiz-Rodriguez", "Sepp Hochreiter", "Nuria Oliver"], "abstract": "Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness investigates how a model's performance varies across different groups, typically defined by protected attributes such as age, gender, or race. In this work, we investigate the interplay between the performance gains from Deep Ensembles and fairness. Our analysis reveals that they unevenly favor different groups in what we refer to as a disparate benefits effect. We empirically investigate this effect with Deep Ensembles applied to popular facial analysis and medical imaging datasets, where protected group attributes are given and find that it occurs for multiple established group fairness metrics, including statistical parity and equal opportunity. Furthermore, we identify the per-group difference in predictive diversity of ensemble members as the potential cause of the disparate benefits effect. Finally, we evaluate different approaches to reduce unfairness due to the disparate benefits effect. Our findings show that post-processing is an effective method to mitigate this unfairness while preserving the improved performance of Deep Ensembles.", "sections": [{"title": "Introduction", "content": "Deep Ensembles (Lakshminarayanan et al., 2017) have demonstrated their efficacy as a simple and robust method to improve the performance of individual Deep Neural Networks (DNNs). Their superior performance has made them a popular choice for real-world applications (Bhusal et al., 2021; Dolezal et al., 2022), including high-stakes scenarios where the impact on people's lives of machine learning supported decisions can be profound, such as in healthcare, education, finance or the law. In such applications, it is crucial to examine how these models perform across different groups that are defined by a protected attribute (e.g., gender, age, race, etc.) which is the focus of the field of Algorithmic Fairness (Barocas et al., 2023). Ensuring equitable operation of these models across protected groups is imperative, as they can significantly impact individuals and communities, potentially widening existing disparities if not adequately addressed. Although the differences in performance across protected groups (group fairness violations) of individual DNNs has been extensively studied (Zhang et al., 2018; Sagawa et al., 2020; Zhang et al., 2022; Arnaiz-Rodriguez & Oliver, 2024), the impact on fairness of ensembling these networks remains underexplored.\nIn this paper, our aim is to fill this gap by conducting an extensive empirical study of the fairness implications of Deep Ensembles, analyzing their underlying causes, and exploring mitigation strategies. Our empirical study is based on two popular facial analysis datasets and a widely used medical imaging dataset, each with multiple targets and protected group attributes. We evaluate a total of fifteen tasks across five different DNN model architectures and using three standard group fairness measures. Our analyses reveal that Deep Ensembles unevenly benefit different protected groups in what we refer to as the disparate benefits effect (c.f. Fig.1). We further investigate the causes of this"}, {"title": "Related Work", "content": "Algorithmic Fairness. Algorithmic fairness is defined using various ethical and legal concepts (Barocas & Selbst, 2016; Corbett-Davies et al., 2017; Binns, 2018), resulting in diverse statistical and causal notions of equality across tasks and contexts (Kusner et al., 2017; Mehrabi et al., 2021). We focus on group fairness metrics -statistical discrimination metrics for classification (Carey & Wu, 2023)- that measure error rate differences between groups defined by protected attributes (Hardt"}, {"title": "Background", "content": "We consider the canonical setting of binary classification with inputs $x \\in \\mathbb{R}^D$, targets $y \\in \\{0,1\\}$, and group attributes $a \\in \\{0,1\\}$ defined according to protected or sensitive variables, such as gender, age, or race. Furthermore, we consider DNNs as the models to map an input x to the 1-dimensional probability simplex $\\Delta^1 = \\{(s_0, s_1) \\in \\mathbb{R}^2 | s_0 \\geq 0, s_1 \\geq 0, s_0 + s_1 = 1\\}$. We define this mapping as $f_w: \\mathbb{R}^D \\rightarrow \\Delta^1$ for a model with parameters w. The output of this mapping defines the distribution parameters of the predictive distribution of the model, denoted by $p(y | x, w)$. A training dataset $D = \\{(x_j, y_j)\\}_{j=1}^J$ is used to determine the model parameters by minimizing the cross-entropy loss. The final prediction $\\hat{y}$ is given by the argmax over the predictive distribution.\nDeep Ensembles. Deep Ensembles (Lakshminarayanan et al., 2017) are an ensemble method that uses DNNs as the base learners. While shallow learners aggregate ensemble predictions via majority voting, Deep Ensembles typically average the output distributions of individual members. Furthermore, individual models are generally trained independently on the same data using different random seeds for initialisation and training. Deep Ensembles are widely recognized as a way to perform approximate sampling from the posterior distribution $p(w | D) = p(D | w)p(w)/p(D)$ (Wilson & Izmailov, 2020; Ashukha et al., 2020), often providing the most faithful posterior approximations (Izmailov et al., 2021). The predictive distribution of an ensemble with N members is given by\n$p(y | x, D) = \\int_{w} p(y | x, w) p(w | D) dw \\approx \\frac{1}{N} \\sum_{n=1}^{N} p(y | x, w_n),$ (1)\nwhere $w_n \\sim p(w | D)$. Thus, it is an approximation of the posterior predictive distribution. The prediction of the Deep Ensemble, equivalent to a single model, is given by $\\hat{y} = \\text{argmax } p(y | x, D)$."}, {"title": "Experimental Setup", "content": "Datasets. In our experiments, we evaluated Deep Ensembles on three different vision datasets. First, two facial analysis datasets, namely FairFace (Karkkainen & Joo, 2021) and UTKFace (Zhang et al., 2017). For those datasets, all models were trained on the training split of FairFace and evaluated on the official test split of FairFace and the full UTKFace dataset. Protected group attributes were binarized, except for gender which was already binary. For the attribute age, we defined young and old, where a person is considered old from 40 onwards to obtain a roughly balanced age distribution. For the attribute race, we binarized it into white vs non-white. We trained the models using one of the attributes as target variable and evaluating it with the remaining two attributes as protected group variables for all possible combinations of target and protected group attributes. Second, the CheXpert medical imaging dataset (Irvin et al., 2019) using the recommended targets provided by Jain et al. (2021) and protected group attributes provided by Gichoya et al. (2022). The no finding target was used to train and evaluate the models. Samples without all protected group attributes have been removed. A random subset of 1/8 was split as test dataset. Protected group attributes were binarized as for the facial analysis datasets. Additional details are given in Sec. D.1 in the appendix.\nModels and training. We used five different DNN architectures, namely ResNet18/34/50 (He et al., 2016), RegNet-Y 800MF (Radosavovic et al., 2020) and EfficientNetV2-S (Tan & Le, 2021) for our evaluation, due to their widespread adoption and competitive performance in vision tasks. The models that were trained on the FairFace training dataset were trained for 100 epochs using SGD with momentum of 0.9 with a batch size of 256 and learning rate of 1e-2. Furthermore, a standard combination of linear (from factor 1 to 0.1) and cosine annealing schedulers was used. The models that were trained on the CheXpert training dataset were trained for 30 epochs given that the training dataset is roughly thrice the size of FairFace, resulting in a similar number of gradient steps and similar learning rate schedule. We independently trained 10 models for 5 architectures on 4 target variables with 5 seeds. Thus, a total of 1,000 individual models were obtained for our evaluation. The results discussed in the main paper correspond to using ResNet50 as the model architecture. Additional results for other model architectures are provided in Sec. F.1 and Sec. F.2 in the appendix."}, {"title": "The Disparate Benefits Effect of Deep Ensembles", "content": "In this section, we study the disparate benefits effect for Deep Ensembles using the experimental setup described in Sec. 4. First, we investigate the disparate benefits effect on the FairFace (FF) test dataset. Second, we apply the same models trained on FF to the UTKFace (UTK) dataset. UTK contains similar facial images as FF but from a different source, representing a realistic setting for facial analysis under slight distribution shifts. Third, we investigate the disparate benefits effect on the CheXpert (CX) medical imaging dataset to assess whether the impact on fairness of Deep Ensembles also occurs in other domains than facial analysis. Our analysis examines two primary facets of the disparate benefits effect: (i) the relationship between the number of ensemble members and the changes in performance and fairness violations (Fig. 1); and (ii) the targets and protected group attributes where a statistically significant disparate benefits effect is observed (Tab. 1).\nFacial analysis (FF). The top row of Fig. 1 shows results for FF, where models were trained on target age and evaluated under the protected group attribute gender. We find that performance increases while fairness decreases when adding ensemble members. In particular, the largest decrease in fairness occurs when the first member is added to the Deep Ensemble. Tab. 1 lists the change ($\\Delta$) in performance and fairness violations between the individual models and a Deep Ensemble of 10 members for all tasks. The performance always increases for the Deep Ensemble (positive $\\Delta$). However, fairness does not necessarily increase after ensembling. We observe a disparate benefits effect with significant changes in the fairness metrics for four out of six target / protected group combinations. It occurs primarily when individual members already exhibit substantial levels of fairness violations (gray cells in Tab. 1). The strongest disparate benefits effect (largest absolute $\\Delta$) has negative impact, thus increasing the fairness violations. However, there are also cases where the Deep Ensemble is a more fair classifier than individual models (negative $\\Delta$). Overall, our results show that Deep Ensembles can decrease fairness, necessitating mitigation strategies.\nFacial analysis under a distribution shift (UTK). The middle row of Fig. 1 depicts the results on the UTK dataset, with the same target and protected group as for FF (top row). Individual ensemble members exhibit higher fairness violations than for FF, which can be explained by the distribution shift between FF and UTK. However, the magnitude and behavior of the disparate benefits effect when adding ensemble members are similar to those observed with the FF dataset. The results for all target / group combinations are listed in Tab. 1. Findings for UTK are overall similar to those reported on the FF dataset. An notable exception is that the difference in SPD with target variable race and protected group attribute age is of opposite sign and larger for UTK than for FF."}, {"title": "What is the Reason for Disparate Benefits?", "content": "In this section, we investigate the potential causes behind the disparate benefits effect. We first investigate how the per-group PR, TPR and FPR metrics change when adding ensemble members, as the considered fairness metrics (Eq. (5) - Eq. (7)) are derived from them. Although this provides insight about why the disparate benefits effect occurs, it lacks an explanation for the underlying cause. We hypothesize that the disparate benefits effect results from the predictive diversity among ensemble members. Our empirical results agree with this hypothesis, suggesting that a gap in average predictive diversity between groups is causing the disparate benefits effect. We conclude with a synthetic experiment to demonstrate the soundness of our hypothesis in a controlled setting.\nChanges to predictions for increasing ensemble size. We begin by examining how the metrics PR, TPR, and FPR for each group change when ensemble members are added, since the considered fairness metrics (Eq. (5) - Eq. (7)) are based on these. Fig. 2 shows these changes for the model trained on FF with age as target variable and gender as protected group, evaluated on the FF test dataset. The results show that the increase in SPD comes from a decrease in the PR of the disadvantaged"}, {"title": "Mitigating the Negative Impact of Disparate Benefits", "content": "In this section, we investigate strategies to mitigate the negative consequences of the disparate benefits effect in the cases when fairness decreases due to ensembling. We focus on interventions that can be applied to trained ensemble members and thus operate in a post-processing manner. This allows to leverage the existing architecture and training procedure of the ensemble members as opposed to pre-and in-processing methods that would require expensive re-training of individual members.\nFirst, we analyze whether it would be possible to non-uniformly weight ensemble members to attain a better trade-off between performance and fairness violations in the Deep Ensemble. Second, we examine the characteristics of the predictive distribution of the Deep Ensemble. We find that Deep Ensembles are more calibrated than individual members on our considered tasks and consequently more sensitive to the selected prediction threshold. Inspired by this finding, we investigate a group-dependent threshold optimization approach (Hardt et al., 2016), often simply referred to as Hardt post-processing (PP) in the algorithmic fairness literature, to mitigate the negative impact of the disparate benefits effect of Deep Ensembles. The results show that PP is highly effective in ensuring fairer predictions while maintaining the enhanced performance of Deep Ensembles.\nWeighting of ensemble members. We analyze whether it is possible to improve the performance / fairness violations trade-off of Deep Ensembles by assigning different weights to each ensemble member, as opposed to the standard uniform weights reflected in Eq. (1). Although the results, shown in Fig. 24 in the appendix, suggest the possibility of better trade-offs, developing a method that systematically identifies the optimal weights to achieve significantly improved outcomes remains a non-trivial challenge. Specifically, we tried two approaches: selecting the best weighting on the validation set and weighting the individual ensemble members proportional to their fairness violations. Both methods lead to ensembles that are on average in between the performance and fairness violations of the Deep Ensemble with standard uniform weighting and individual models, with high variance. A detailed discussion is provided in Sec. F.4 in the appendix.\nBetter calibration leads to more sensitivity to the prediction threshold. Next, we analyze the predictive distribution of Deep Ensembles to identify mechanisms to mitigate the negative fairness"}, {"title": "Conclusion", "content": "In this work, we have reported on the existence of a disparate benefits effect of Deep Ensembles in experiments on three vision datasets, investigating 15 different tasks and considering five different model architectures. We have investigated potential causes for this effect, with our findings suggesting that differences in the predictive diversity of the ensemble members are a potential cause. Finally, we have evaluated different approaches to mitigate the disparate benefits effect. We find that Deep Ensembles are better calibrated than the individual members and thus more sensitive to the prediction threshold. As a result, Hardt post-processing is found to be an effective solution to ensure fairer decisions while maintaining the improved performance of Deep Ensembles.\nWhile our experiments have focused on socially salient protected groups, we anticipate that the findings will generalize to robust classification settings where inputs can be clustered according to some group attribute. The controlled experiment provides strong evidence for this generalization.\nThe main limitations of our study are that we focus on vision tasks and hence on ensembles of Convolutional Neural Networks, and that we assess fairness with three group fairness metrics that, while widely used, are not sufficient to guarantee fair outcomes. The fairness of predictions of a model in the real-world can't be reduced to any single metric and must be carefully assessed depending on the application. In future work, we thus plan to explore other notions of fairness, such as individual fairness, and extend our analysis to other types of models and datasets, including text. Furthermore, we intend to investigate the disparate benefits effect for Deep Ensembles where pre- or in-processing fairness methods have been applied to individual ensemble members."}, {"title": "Details on Computing Group Fairness Metrics", "content": "Group fairness metrics, as previously discussed, are based on assumptions related to the independence of the prediction with respect to the protected attribute and the target. For completeness, we present below how to estimate the metrics given in Eq. (5) - Eq. (7) with samples. We start by defining the number of correct (TP, TN) and wrong decissions (FP, FN) of a model:\n$TP := \\sum_{k=1}^K 1[f(x_k) > t] 1[y_k = 1]$,\n$TN := \\sum_{k=1}^K 1[f(x_k) < t] 1[y_k = 0]$\n$FP := \\sum_{k=1}^K 1[f(x_k) > t] 1[y_k = 0]$,\n$FN := \\sum_{k=1}^K 1[f(x_k) < t] 1[y_k = 1]$.\nHere, $D' = \\{(x_k, y_k, a_k)\\}_{k=1}^K$ is the test dataset; a datapoint $(x_k, y_k, a_k)$ consists of input features, observed outcome and protected group attribute; $f(x_k)$ is the model's predicted value for $x_k$; and t is the classification threshold. To compute these metrics for a specific value a of protected group attribute A (e.g., male for gender), we add the term $1[a_k = a]$ to each computation, resulting in group-specific true positives $TP_{A=a}$, true negatives $TN_{A=a}$, false positives $FP_{A=a}$, and false negatives $FN_{A=a}$.\nOnce all these buliding blocks are computed, the group-specific Positive Rate ($PR_{A=a}$) is given by\n$PR_{A=a} = P(\\hat{Y} = 1 | A = a) \\approx \\frac{TP_{A=a} + FP_{A=a}}{TP_{A=a} + FP_{A=a} + TN_{A=a} + FN_{A=a}}$\nFinally, equal opportunity and equalized odds depend on the conditional true/false negative/positive rates, depending on the values of the protected group attribute A and are calculated as:\n$TPR_{A=a} = P(\\hat{Y} = 1 | Y = 1, A = a) \\approx \\frac{TP_{A=a}}{TP_{A=a} + FN_{A=a}}$,\n$TNR_{A=a} = P(\\hat{Y} = 0 | Y = 0, A = a) \\approx \\frac{TN_{A=a}}{FP_{A=a} + TN_{A=a}}$,\n$FPR_{A=a} = P(Y = 1 | Y = 0, A = a) \\approx \\frac{FP_{A=a}}{FP_{A=a} + TN_{A=a}}$,\n$FNR_{A=a} = P(Y = 0 | Y = 1, A = a) \\approx \\frac{FN_{A=a}}{TP_{A=a} + FN_{A=a}}$"}, {"title": "Group fairness metrics as a factorization of P(Y,\u0176 | A).", "content": "In order to analyze the trade-offs and connections between different statistical group fairness metrics, a common approach is to use the factorization of $P(Y, \\hat{Y} | A)$, which offers a clear intuition of the incompatibilities between some of them. Then, all the introduced metrics are related as per:\n$P(\\hat{Y} | Y, A = 1) \\times P(Y | A = 1) = P(Y | Y, A = 1) \\times P(\\hat{Y} | A = 1)$\n$P(\\hat{Y} | Y, A = 0) \\times P(Y | A = 0) = P(Y | Y, A = 0) \\times P(\\hat{Y} | A = 0)$ (9)\nFor instance, it suggests that, if the target prevalence is different across groups and the model is perfectly calibrated (sufficiency), then separation and independence conditions cannot be satisfied simultaneously."}, {"title": "Biases and Group Unfairness", "content": "Biases induced by datasets have been studied in Pombal et al. (2022). They consider the joint distribution P(X, Y, A). Generally there is a bias under a distribution shift with P*(X, Y, A) \u2260 P(X, Y, A), where the distribution after the shift P* the model is applied on is different to the"}, {"title": "Bayesian Perspective on the Average Predictive Diversity", "content": "In this section, we motivate the average predictive diversity DIV (c.f. Eq. (8)) from a Bayesian perspec-tive. Given are a training dataset $D = \\{(x_j, y_j)\\}_{j=1}^J$ as well as a test dataset $D' = \\{(x_k, Y_k)\\}_{k=1}^K$; the protected attribute is omitted for brevity in this section. Furthermore, we are given a prior distribution p(w) on the model parameters.\nMarginal Likelihood. Through Bayes' rule, we obtain a posterior distribution over the model parameters given the training dataset $p(w | D) = \\frac{p(D | w)p(w)}{p(D)}$. Recall that the marginal likelihood is given by $p(D) = \\int_{w} p(D | w)p(w)dw$, i.e., the expected likelihood on the dataset over all models according to their prior distribution. Intuitively, the marginal likelihood thus measures how well possible models represent the given dataset.\nThe disparate benefits effect occurs on a test dataset D'. Consequently, we are interested in the marginal likelihood under the test dataset $p(D')$. For the test dataset D', the posterior distribution given the training dataset p(w | D) is the new prior distribution p(w). The marginal likelihood under the test dataset is thus given by\n$p(D') = \\int_{W} (\\prod_{k=1}^{K} p(y = Y_k | x_k, w)) p(w) dw \\approx \\frac{1}{N} \\sum_{n=1}^{N} (\\prod_{k=1}^{K} P(y = Y_k | x_k, w_n)),$ (12)\nwith $w_n$ drawn according to p(w) = p(w | D). In practice, the set of model parameters $\\{w_n\\}_{n=1}^N$ obtained from the training of the Deep Ensemble is used to approximate the integral.\nLikelihood Ratio. If the likelihood under the posterior predictive distribution\n$\\hat{p}(D') = \\prod_{k=1}^{K} (\\int_{W} p(y = Y_k | x_k, w) p(w | D) dw \\approx \\prod_{k=1}^{K} (\\frac{1}{N} \\sum_{n=1}^{N} p(y = Y_k | x_k, w_n)),$ (13)\nagain with $w_n$ drawn according to p(w) = p(w | D), does not differ from the marginal likelihood, there is no difference between predicting with a single model sampled according to the posterior and predicting with the ensemble of all sampled models. Thus, we investigate the likelihood ratio $\\frac{\\hat{p}(D')}{p(D')}$ as a natural measure of diversity in the predictions of the models that make up the ensemble."}, {"title": "Details of the Experimental Setup", "content": "The code to reproduce our experiments is available at https://github.com/ml-jku/disparate-benefits.\nDatasets\nWe conducted all our experiments on facial analysis and medical imaging datasets. In the following, we provide details about the datasets.\nFacial Analysis. We used two widely used facial analysis datasets, FairFace\\u2071 (Karkkainen &\nJoo, 2021) (License: CC BY 4.0) and UTKFace\\u2072 (Zhang et al., 2017) (License: research only, not\ncommercial). FairFace was created for advancing research in fairness, accountability and transparency\nin computer vision as it addresses the lack of diversity in existing face datasets used for research\npurposes. The FairFace dataset comprises 108,501 facial images collected from publicly available\nsources, such as Flickr and Google Images, and covers a diverse range of demographics, including\nvarious ethnicities, ages, genders, and skin tones. The dataset includes annotations for gender, age,\nand ethnicity. UTKFace contains over 20,000 facial images of individuals collected from the publicly\navailable datasets UTKinect (Xia et al., 2012) and FGNET (Lanitis et al., 2002), as well as images\nscraped from the internet. It includes annotations for three demographic attributes: age, gender, and\nethnicity.\nMedical Imaging. We used the medical imaing dataset CheXpert\\u2073 (Irvin et al., 2019) (License:\nStanford University Dataset Research Use Agreement). It consists of a large publicly available dataset\nof 224,316 chest X-rays along with associated radiologist-labeled annotations for the presence or\nabsence of 14 different thoracic pathologies. It is designed to address the challenges of class imbalance\nand target noise commonly encountered in medical image classification tasks. CheXpert has become a\nwidely used benchmark dataset in the field of medical imaging and has been instrumental in advancing\nresearch on automated chest radiograph interpretation, particularly in the context of deep learning\napproaches. We use the recommended targets provided by Jain et al. (2021) (visualCheXbert targets)\nand group attributes provided by Gichoya et al. (2022)\\u2074."}, {"title": "Models and Training", "content": "We used the ResNet18/24/50, RegNet-Y 800MF and EfficientNetV2-S implementations of Pytorch (Paszke et al., 2019). Hyperparameters as reported in the main paper were the result of an initial manual tuning on the respective validation sets, but mostly align with commonly utilized hyperpa-rameters for classical image datasets such as CIFAR10. The raw performance on the task was not of extreme importance, but is comparable to previous studies on the same datasets with similar network architectures (Karkkainen & Joo, 2021; Zhang et al., 2022; Zong et al., 2023)."}, {"title": "Computational Cost", "content": "For training the models, we utilized a mixture of P100, RTX 3090, A40 and A100 GPUs, depending on availablility in our cluster. Training a single model took around 3 hours on average over all considered model architectures and datasets, resulting in 3,000 GPU-hours. Evaluating these models on the test datasets accounted for approximately 150 additional GPU-hours."}, {"title": "Complete Experimental Results", "content": "The experimental results included in the main paper describe a subset of all the considered tasks. In this section, we provide the results of the complete set, along with additional supporting tables and figures.\nPerformance and fairness violation of Deep Ensemble and individual members. Tab. 3 and Tab. 2 contain the performance and fairness violations of individual ensemble members and the resulting Deep Ensemble, respectively.\nThe disparate benefits effect for Deep Ensembles. Fig. 8 - 10 depict the change in performance and fairness violations when adding individual ensemble members for all considered tasks.\nChanges in PR, TPR and FPR. Fig. 11 - 13 display the change in PR, TPR and FPR per group when adding individual ensemble members for all considered tasks.\nDifference in average predictive diversity. Fig. 14 - 16 depict the differences in average predictive diversity per target and protected group.\nHardt post-processing (PP). Tab. 4 - 18 contain the results of mitigating unfairness by means of PP (Hardt et al., 2016) on all considered tasks. PP was either applied with the threshold set to the average fairness violation of the individual ensemble members on the validation set (val) or to 0.05. Note that for some tasks, the original fairness violation of both the Deep Ensemble and its members was already lower than 0.05, where PP leads to an increase in unfairness up to the desired threshold. Experiments on FairFace and CheXpert use the respective validation sets to learn the group dependent thresholds in PP. For experiments on UTKFace, the FairFace validation set was used to learn the thresholds, as it was designed to emulate a real-world distribution shift scenario. Also for UTKFace, the same conclusions as for the FairFace experiments described in the main paper hold, i.e., while PP is very effective to mitigate unfairness in the Deep Ensembles, the desired fairness violation (0.05) is not reached due to the distribution shift. Note that the balanced accuracy was used as the performance metric for CheXpert, because the metric investigated in the main paper, the AUROC, does not consider selecting a threshold."}, {"title": "Additional Investigations", "content": "This section presents additional investigations that are complementary to those presented in the main section of the manuscript. First, we investigate how the disparate benefits effect behaves for different model sizes of the individual ensemble members. Second, we conduct the same investigation on different model architectures. Third, we study whether the disparate benefits effect also occurs for heterogeneous Deep Ensembles composed of members with different model architectures. Fourth, we report an alternative approach to mitigate the negative impact on fairness due to Deep Ensembling by means of weighting individual members differently in the ensemble. Finally, we study the sensitivity of the Deep Ensemble and its individual members to the threshold used to make the prediction."}, {"title": "Model Size", "content": "The experiments in the main paper were conducted using ResNet50 models. In this section we investigate whether the size of the models plays a major role in determining the existence and strength of the disparate benefits effect. The results are shown in Fig. 17 - 19. As seen in the Figures, in the majority of cases the performance gains due to ensembling slightly increase for larger model classes. The fairness violations however increase to a larger degree, see e.g. Fig. 17 (a) and (b), Fig. 18 (a), (b) and (c) as well as Fig. 19 (a). Generally, we observe an increase in the magnitude of the change in fairness violations with larger model classes for all tasks that exhibit significant disparate benefits (c.f. Tab. 1)."}, {"title": "Model Architecture", "content": "In this section we investigate the role of the specific model architecture on the existence and strength of the disparate benefits effect. The results are shown in Fig. 20 - 22. In the majority of cases, disparate benefits occur throughout all considered model architectures. Especially for EfficientNetV2-S we observe significant disparate benefits for some cases where we do not observe them in the main investigation based on ResNet50. For example for UTK, target race, group age under AOD (Fig. 21e) or CX, group race under EOD and AOD. Overall, we do not find a systematic difference of the results for different model architectures."}, {"title": "Heterogenous Ensembles", "content": "The results presented in Fig. 1 in the main paper are obtained from a homogeneous Deep Ensemble composed of ResNet50 models. The results presented in Fig. 23 consider the same target / protected group combinations for the same datasets using a heterogeneous Deep Ensemble of ResNet18/34/50 models. We observe the disparate benefits effect for heterogeneous ensembling to a similar extent than for homogeneous ensembling."}, {"title": "Deep Ensemble Weighting", "content": "In this section, we study whether there exist weightings to combine the individual models in the Deep Ensemble that perform better than a standard uniform averaging as in Eq. (1). The approximation in Eq. (1) thus changes to\n$P_X(y|x, D) \u2248 \\sum_{n=1}^{N} \\lambda_n p(y|x, w_n).$ (16)\n$\\Lambda$ satisfies $\\sum_{n=1}^{N} \\lambda_n = 1$ and $\\lambda_n \u2265 0 \u2200n$. Note that Eq. (16) results in Eq. (1) if $n = \\frac{1}{N} \u2200n$. We consider $X \\sim Dir(\u03b1_1, ..., \u03b1_N)$ with $\u03b1_n = 1 \u2200n$. Weightings are thus drawn uniformly at random from a N - 1 dimensional probability simplex. In our empirical investigation, we sampled 2,000 weightings $\\Lambda$ and evaluated the resulting ensembles on the three tasks. The results are given in Fig. 24, showing individual members and the different resulting ensembles, as well as their convex hull. In the case of the FF and UTK datasets, there apprears to be a strong correlation between fairness violations and performance, and the weights hardly provide more Pareto optimal models. However, regarding the CX dataset, we observe that there are many weightings that would yield a more favorable outcome than uniform averaging as generally done by Deep Ensembles. In the following, we outline two methods to choose such a weighting. However, both methods did no lead"}, {"title": "Thresholds Selection", "content": "Finally, we report the results of analyzing the dependency of the Deep Ensemble and individual ensemble members on selecting the threshold for prediction. When using the usual argmax, implicitly a threshold of 0.5 is used. In the post-processing experiments, we found that applying the method even under an additional fairness constraint can improve the performance. We evaluated all trained models on their respective validation datasets. The results are depicted in Fig. 25. The results show that the Deep Ensemble is more sensitive to the threshold on the FF dataset, especially for target variable age. Regarding the CX dataset, the balanced accuracy exhibits roughly the same behavior under varying thresholds for the Deep Ensemble than for individual members. However, the spread of the optimal threshold is much smaller throughout all experiments."}]}