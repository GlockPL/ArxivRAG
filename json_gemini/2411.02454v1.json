{"title": "GRAPH-BASED CONFIDENCE CALIBRATION FOR LARGE LANGUAGE MODELS", "authors": ["Yukun Li", "Sijia Wang", "Lifu Huang", "Li-Ping Liu"], "abstract": "One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks such as question answering (Wei et al., 2022; Shen et al., 2023; Zheng et al., 2023; Qin et al., 2023; Singhal et al., 2023), text summarization (Tang et al., 2023; Deroy et al., 2023; Tam et al., 2023; Roit et al., 2023), and even creative writing (G\u00f3mez-Rodr\u00edguez & Williams, 2023; Wang et al., 2024; Deng et al., 2024). Despite their impressive performance, LLMs often give wrong answers in question-answering tasks. There is an urgent need to check the correctness of LLMs' responses. One particularly interesting question is to calibrate the confidence levels of the correctness of responses from LLMs (Kuhn et al., 2022; Ulmer et al., 2022; He & Jiang, 2023; Van Landeghem et al., 2022; Vazhentsev et al., 2023; Ulmer et al., 2024). Accurate confidence estimation is vital for deploying LLMs in the real world, as it allows users to gauge the reliability of the model's predictions and make informed decisions based on these outputs. On the contrary, miscalibrated confidence may lead to over-trust in incorrect responses or doubts about the correct ones. For example, a misleading response may steer a patient in the wrong direction when making health decisions; it may also lead an investor to impulsive investment decisions.\nIn this work, we consider calibrating the confidence with the correctness of LLMs' responses. This task is challenging in several aspects. First, due to LLMs' superior ability to generate text, mistakes in an LLM's response are usually at the semantic level, making it hard to detect even for humans. There are methods using an auxiliary Language Model (e.g., DeBERTa (He et al., 2020)) to check whether the LLM's response answers the question Ulmer et al. (2024). Since the LLM is supposed to be much stronger than the LM, the LLM should be able to avoid most mistakes that can be detected by an LM; this type of method may omit a significant fraction of wrong answers. Second, it is hard to detect mistakes from the LLM's internal working mechanism. Because the LLM uses many hidden layers to process the information, it is hard to discern the signal from a small number of hidden units. Even if this is possible, it is not easy to apply this type of method to black-box LLMs."}, {"title": "2 RELATED WORK", "content": "Due to the urgent need to improve the reliability of LLMs, confidence estimation and calibration for these models have become active areas of research. Existing research in LLM uncertainty quantification can be summarized into two main categories: uncertainty quantification and confidence calibration (Geng et al., 2023). Confidence estimation for short responses (e.g., for multi-choice or yes-no questions) is generally less complicated than for long responses (Ye et al., 2024). For a brief response, the LLM's output logits are informative about its confidence; the easy comparisons of responses to the true answer facilitate both calibration and evaluation. Confidence estimation for long responses cannot simply depend on LLM's output logits (Duan et al., 2023; Bakman et al., 2024) because the logits indicate more about the probability of text and less about the semantics behind it. There are also methods using the internal state of an LLM (Ren et al., 2022), but it is not always available to have such information about the LLM interface.\nAnother approach is to check the LLM's consistency in its responses. Kotelanski et al. (2023) demonstrate that repeated sampling and consistency checks across multiple outputs can serve as reliable proxies for model confidence. Manakul et al. (2023) generates multiple responses from the LLM and checks the consistency between responses using various methods, including querying the LLM. Chen & Mueller (2023) combines the consistency between responses and the LLM's self-reflection certainty to quantify the uncertainty. Kuhn et al. (2022) considers confidence from semantic equivalence and proposes a method based on clustering of responses. Lin et al. (2024) organize responses in a graph with their pairwise semantic similarity and then extract graph statistics for confidence estimation. Zhang et al. (2024) examines methods of comparing responses via entailment and contradiction relationships. These studies highlight the importance of semantic consistency in confidence estimation. These methods are evaluated by comparing their estimated confidence values against the correctness of responses. The correct labels are often obtained by reference matching"}, {"title": "3 METHOD", "content": "Our ultimate goal is to quantify the probability of the correctness of a response from an LLM. Since the LLM can give a correct answer with different phrases, we need to consider the probability that the response is semantically correct.\nBackground: The formulation of semantic equivalence (Kuhn et al., 2022) provides a framework for our analysis. Let R be the space of all possible responses. Given a question q, the space R is divided into a set set $\\mathcal{C}_q$ of semantic classes: $R = \\bigcup_{C \\in \\mathcal{C}_q} C$ and $C' \\cap C = \\emptyset$ for any two different semantic classes C', C' \u2208 $\\mathcal{C}_q$. For two responses r1,r2 \u2208 C in the same equivalent class, they are considered as the same semantic response: if one is the correct answer, the other is correct as well, and vice versa. Then, we can consider the quality of the LLM's responses at the semantic level. In particular, a semantic response C has probability\n$\\begin{equation}\np(C|q) = \\sum_{r \\in C}p(r|q).\n\\end{equation}\nHere p(r|q) is the probability of a single response from the LLM.\nHowever, it is non-trivial to define the equivalent class, and we will discuss the approximation later. To estimate p(C|q), one approach is through semantic similarities between response samples of an LLM for the same question q. Let (r1,...,rn) be n responses from the same question q, and they form k clusters $\\mathcal{C}_q = \\{\\check{C}_1,...,\\check{C}_k\\}$ by their semantic similarity. We can use Natural language inference (NLI) systems to predict the relationships (e.g., entailment and contradiction) between responses and derive their similarity.\nWe assume that each cluster cluster $\\check{C}$ is from a different semantic class C, then p(C|q) can be approximated by\n$\\begin{equation}\np(C|q) \\approx \\frac{|\\check{C}|}{n}\n\\end{equation}\nFrom the cluster probabilities, the uncertainty of the LLM on the question q is estimated as the entropy of the empirical distribution over clusters (Kuhn et al., 2022), and the confidence of a response ri \u2208 C is estimated as |C|/n (assuming similarity values are binary) (Lin et al., 2024).\nNow, we depart from the setup of semantic classes and consider the correctness of responses. Let C* be the correct semantic answer to question q. Without knowing which responses are correct answers, a common assumption is that the model's confidence reflects the correctness, that is, the model's confidence in a semantic response is approximately the probability of correctness, then\n$\\begin{equation}\np(\\check{C}_k \\subseteq C^*) \\approx \\frac{|\\check{C}_k|}{n}\n\\end{equation}\nIt says that the more certain the model is about a semantic response, the more likely the response is correct. Conversely, a wide variation in the LLM's responses indicates low confidence in all"}, {"title": "3.1 CONFIDENCE CALIBRATION AS GRAPH LEARNING PROBLEM", "content": "Now, we set a supervised learning problem and train a model to calibrate the confidence of the correctness of responses. We first consider the correctness labels of the LLM's responses. In the supervised setting, we have a correct answer r* to the question q. Then r* to assign correctness labels to sampled responses {r1,r2, ..., rn} for the same question q. In our work, we use the ROUGE similarity. Specifically, we compute the ROUGE similarity $sim_r(r_i, r^*)$ between a sampled response and the correct answer to decide the correctness label.\n$\\begin{equation}\nY_i = \\mathbb{1}[sim_r(a, r_i) \\geq \\tau], i = 1, ..., n.\n\\end{equation}\nHere $\\mathbb{1}[\\cdot]$ is one if the condition is true or 0 otherwise. The ROUGE metric is reasonably accurate in measuring semantic similarity between short sentences (Lin & Och, 2004).\nWe then consider the input to the calibration model. We form a similar graph G over responses to encode information about their consistency. The graph contains the clustering structure of responses and likely further useful information to predict the correctness of responses. The graph G = (V, E, w) is a fully connected graph, with the node set V consisting of n responses and the edge weight Wij being the similarity between the pair of responses (ri, rj). We compute the similarity from the two responses' embeddings. In particular, we first use the Sentence-BERT model (Reimers & Gurevych, 2019) to compute the two responses' vector representations and then compute the cosine similarity\n$\\begin{equation}\nW_{ij} = sim_{cos}(emb(r_i), emb(r_j)), i, j = 1,...,n.\n\\end{equation}\nHere, emb(\u00b7) represents the embedding function.\nThen, we treat the problem as a node classification problem (Xiao et al., 2022). In particular, we run a Graph Neural Network (GNN) gnn(.) to predict the probability of each response being correct\n$\\begin{equation}\n\\hat{p} = gnn(G).\n\\end{equation}\nHere $\\hat{p} \\in [0, 1]^n$ contains the probabilities for n responses being correct.\nTo provide clustering information to the GNN, we first run the K-means clustering algorithm on the responses' embeddings and assign cluster IDs from 0 to K \u2212 1 based on the order from largest to smallest (ties are randomly broken). Then, we feed each response's cluster membership as a one-hot"}, {"title": "3.2 IMPROVE THE ESTIMATION THROUGH MULTIPLE PROMPTS", "content": "It is well known that the syntactic form of a question influences responses and introduces additional variance. To reduce this variance and evaluate the LLM's semantic consistency, we analyze the LLM's responses to multiple prompts derived from the same question. These responses are treated as answers to the same semantic question. We then apply the same method as before to predict the correctness of each response.\nIn particular, we rephrase the original question q into k different forms {q1, ..., qk } while maintaining the original sentence's semantic meaning. We employ a multiple rephrased questions strategy for answer sampling. Specifically, we prompt the GPT-4 to give k different but with the same meaning rephrased questions for the given question q. Then, we sample n/k responses from the LLM for each rephrased question and still get a total of n responses, from which the confidence calibration is the same as we have described above. The model is more likely to produce diverse responses for questions about which the LLM is less certain. Confidence calibration is more accurate in this scenario because the model's uncertainty becomes more apparent."}, {"title": "4 EXPERIMENTS", "content": "The goal of this section is to compare our proposed framework with baseline methods in terms of confidence calibration. All experiments are conducted on NVIDIA A100 GPUs with 80GB of memory. The supplementary materials and Appendix provide the code for our model, more experiment details in Appendix A.1, and prompting strategy and Appendix A.6."}, {"title": "4.1 DATASET AND EXPERIMENT SETUP", "content": "Dataset: We conduct experiments on two public benchmark datasets: (1) CoQA (Reddy et al., 2019), an open-book conversational question answering (QA) task; (2) TriviaQA (Joshi et al., 2017), a commonsense QA task. We use the first 4k as training data and 1582 for validation and testing, both of equal size. We provide more details about training in the Appendix A.1.\nBaselines: We compare our methods with the following baselines.\nLength-normalized sequence likelihoods (Seq. likelihood) (Malinin & Gales, 2021; Kuhn et al., 2022) is a standard measure for confidence. This method calculates the likelihood of each sequence and normalizes it by the length of the sequence to provide a fair comparison between different lengths of sequences. Platt scaling (Platt, 1999), a variant of the sequence likelihood baseline, applies Platt scaling to the raw likelihoods. GraphSpectral (Chen et al., 2024) uses the graph theory to estimate the confidence. Then we also include post-hoc uncertainty calibration,GraphSpectral+Iso and GraphSpectral+Platt into the baseline methods. Self-check GPT(Manakul et al., 2023) checked the consistency between responses querying the LLM. Verbalized Uncertainty (Lin et al., 2022; Tian et al., 2023; Xiong et al., 2024) generates verbal statements about the model's confidence in its predictions. Verbalized Qual maps the confidence percent (Verbalized %) into numerical values. APRICOT (Ulmer et al., 2024), a supervised method, fine-tunes the Deberta language model"}, {"title": "4.3 OUT OF DOMAIN EVALUATION", "content": "Domain shift poses significant challenges for deploying machine learning models in real-world scenarios where data variability is expected. To comprehensively assess the robustness and generalization capabilities of our proposed model compared to baseline methods, we conducted a series of out-of-domain (OOD) evaluations.\nExperiment setup: We evaluate the confidence calibration of different approaches under out-of-domain settings. We have two experiment configurations: out-of-domain dataset OODD, and out-"}, {"title": "4.4 SENSITIVITY ANALYSIS", "content": "In this subsection, we conducted several sensitivity analyses of our model.\nNumber of training samples We conducted experiments to examine the relationship between performance and the amount of training data. Specifically, we tested our model performance on the Llama3 TriviaQA dataset and varied the training size from 100 to 4000. The results are displayed in Table 4. We observed that the model's performance does not drop significantly with the reduced training data. These experimental results indicate that the model performs well with limited data availability, demonstrating its applicability in real-world scenarios where only smaller datasets are available. We also tested the baseline performance, the results are shown in Appendix A.5.\nHyperparameter sensitivity We conduct the sensitivity analysis of our model's calibration error performance concerning two key configurations: the number of sampled answers used to construct the graph and the number of Graph Convolutional Network (GCN) layers in the GNN model. The results are displayed in Fig. 3. The experiments are conducted using the Llama3 model on the TriviaQA dataset. For Fig. 3 (a) experiments, we varied the number of sampled answers from 10 to 50 while keeping other configurations and hyperparameters fixed, as described in the experimental setup. We observe that increasing the number of sampled answers slightly improves performance,"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In summary, in this work, we proposed one effective strategy of confidence calibration by combining the LLM's self-consistency with labeled data and training an auxiliary GNN model to estimate the correctness of its responses to questions. Experiments demonstrate that the proposed approach improves confidence calibration significantly across several datasets compared to baseline methods. We acknowledge that one limitation of our approach is the computation time of sampling multiple answers from LLMs. However, this cost can be effectively alleviated by parallel sampling. In the Appendix A.2, we provide the details of the sampling time. In future work, we aim to extend the framework to incorporate the data uncertainty coming from ambiguous questions and also explore the multi-step confidence calibration in the chain-of-thought framework. Our approach relies on a consistency graph to calibrate confidence.\nLarge Language Models (LLMs) are powerful tools that deliver valuable information across a wide range of applications. However, the misinformation they may generate can lead to significant societal issues. Our calibration model evaluates the accuracy of an LLM's responses, thereby enhancing their reliability. When integrated into real-world systems, our model would enable LLMs to abstain from answering uncertain queries or allow users to decide the level of trust they place in the responses. We anticipate that our research will positively influence the responsible deployment and use of LLMs in society."}, {"title": "A APPENDIX", "content": "A.1 HYPERPARAMETERS AND MODEL CONFIGURATIONS\nModel hyper-parameters:\nOur model used three GCN layers; typically, the embedding dimension was 256, 512, and1024 for three GCN layers. For the training process, we used the binary cross-entropy loss with a decaying learning rate that reduced the learning rate by 0.9 if the validation loss did not improve 10 epochs (with an initial learning rate of 10-4 and a minimum learning rate of 10-7). The optimizer was Adam with B\u2081 = 0.9 and \u03b22 = 0.98. The batch size was 32. For the rephrased prompts, we set k = 3, n = 30, so for each rephrased question, we sampled ten answers. While calculating the ECE, we divide the confidence into B = 10 bins.\nEvaluation Setup:\nFor each question, we evaluate the confidence prediction corresponding to the most likely answers from the LLM response. The setup is consistent with the baseline methods.\nGraph construction:\nFor each question, we prompt the LLM to give 30 answers, and the temperature for LLM is set to be 0.6. For each answer, the SentenceBert model Reimers & Gurevych (2019) is used to get each answer's embedding. The cosine similarity between each answer's embedding is taken as the edge weight of the graph. We apply the K-Means clustering method to cluster similar semantic responses. The maximum cluster number is set as 3.\nA.2 COMPUTATIONAL COST\nWe performed all experiments on NVIDIA A100 GPUs with 80GB of memory. Generating 30 responses using the Llama3 and Vicuna models for 6000 questions from CoQA and TriviaQA data required up to 4 hours, with an average of approximately 2 seconds per question. The CoQA dataset demanded more processing time due to the longer contextual information in the input. The time can be shortened by parallel sampling.\nA.3 ADDITIONAL CASES\nTo better understand our method intuitively, we have collected a few examples to show the difference between our algorithm and APRICOT."}, {"title": "A.6 PROMPTING STRATEGY", "content": "Here, we showed the prompts we used to generate the rephrasing questions."}]}