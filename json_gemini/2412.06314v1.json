{"title": "CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images", "authors": ["Yijie Dang", "Weijun Ma", "Xiaohu Luo"], "abstract": "Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model.", "sections": [{"title": "1. Introduction", "content": "Since the outbreak of the novel coronavirus disease (COVID-19) in 2019, healthcare systems worldwide have faced tremendous pressure. In such a critical situation, timely and accurate diagnosis is crucial for effective patient management. Currently, reverse transcriptase-polymerase chain reaction (RT-PCR) is considered as the gold standard of diagnosing COVID-19 for its high specificity (Wang et al., 2020b). While PCR testing serves as the primary clinical diagnostic test, it is not without certain limitations. Firstly, the implementation of PCR testing poses challenges in resource-limited settings or remote areas due to its requirement for specialized laboratory facilities and skilled personnel, which are often scarce in such environments. Secondly, the turnaround time for PCR results can be relatively long, ranging from a few hours to days, which may hinder timely decision-making and patient management. Finally, a limitation is the potential for false negatives. Factors such as improper sample collection, variations in viral load during different stages of infection, or technical errors in the PCR process, which can contribute to false-negative results, leading missed diagnoses and potential virus transmission (Ai et al., 2020). To address these challenges, medical imaging techniques have been extensively utilized as a primary or adjunctive tool (Vantaggiato et al., 2021).\nAmong many medical imaging techniques, computed tomography (CT) imaging has emerged as a valuable tool in the detection and assessment of COVID-19 (Kucirka et al., 2020), offering detailed insights into the extent and severity of lung involvement. CT scans of COVID-19 patients commonly exhibit distinctive radiological patterns, such as ground-glass opacities (GGOs), consolidations (CONs), and crazy-paving patterns, as well as various other features. GGO, characterized by an increased density in lung regions with a hazy shadow reminiscent of glass transparency, presents as high-opacity areas with blurred margins on CT scans. CON, characterized by increased density due to inflammatory exudates or hemorrhage filling alveoli, appears as denser, solid white areas, with potential for clear or blurred edges. Crazy-paving patterns, displaying thickened alveolar walls and increased interstitial texture due to edema and inflammation, exhibit an intricate, brick-like interlocking pattern on CT images. This variability poses difficulties for radiologists in accurately segmenting and distinguishing infected areas from normal lung tissue. As a result, the pursuit of automated segmentation techniques for COVID-19 lung CT images has garnered significant attention in recent years. Automated segmentation of lung regions from CT scans plays a crucial role in quantifying the extent of lung involvement, assisting in disease diagnosis, monitoring disease progression, and evaluating treatment response. It enables the extraction of quantitative features, such as lesion volume and density, which can aid in patient risk stratification and treatment planning (Lei et al., 2020; Ng et al., 2020; Pan et al., 2020).\nOver the years, various computerized methods have been proposed for lung segmentation in CT images, ranging from traditional image processing algorithms to advanced machine learning techniques. Deep learning, particularly convolutional neural networks (CNNs), has emerged as a dominant approach in medical image segmentation, demonstrating promising results in various applications.\nHowever, the complex and variable nature of COVID-19 lesions within the lungs presents significant challenges for automatic segmentation using deep learning techniques. COVID-19, caused by the SARS-CoV-2 virus, manifests in the lungs through diverse and highly variable pathological findings. Firstly, the size of COVID-19-associated lesions can vary considerably, with some patients exhibiting small, focal areas of infection, while others display widespread, diffuse involvement of multiple lung lobes. This variability necessitates deep learning models to possess a high degree of flexibility and adaptability to accurately segment these patterns. Secondly, the shapes and patterns of the lesions also exhibit significant diversity. Ground-glass opacities (GGO) typically manifest as round or oval-shaped opacities, whereas consolidation (CON) may appear as irregular or patchy areas. In some cases, lesions may present with a mixed appearance, combining GGO with areas of consolidation or fibrosis. This heterogeneity in lesion morphology poses considerable challenges for deep learning models to consistently and accurately segment the infected regions. Finally, the location of lesions within the lungs can vary significantly, with some patients having lesions primarily in peripheral regions and others in more central locations. This spatial variability requires deep learning models to have a comprehensive understanding of lung anatomy and the potential locations of COVID-19 lesions.\nIn order to tackle the aforementioned challenges, we introduce an innovative deep network architecture called CAD-Unet designed specifically for CT slices. In this architecture, we couple capsule networks into the encoder stage of the Unet architecture to enhance the segmentation performance of the proposed network. Capsule networks can better preserve the relative spatial positional information between features, thereby enhancing their recognition capabilities for the varying sizes, complex shapes, and differentiated lesion locations exhibited by COVID-19 pulmonary lesions. This capability aligns with the fundamental principle of capsule networks, where information is stored and transmitted at the neuron level as vectors rather than scalars (LaLonde et al., 2021). These vectors encapsulate details about the spatial attitude of objects, including their locations, scale locations, and orientations between different parts (Tran et al., 2022). Furthermore, the implementation of a dynamic routing algorithm in capsule networks facilitates the transfer of information between capsule layers, allowing the model to effectively capture the positional relationships between objects. We conduct comprehensive comparative experiments using four publicly accessible datasets, illustrating the superior performance of the proposed architectural framework in the context of pulmonary infection segmentation.\nIn summary, the main contributions of this paper are:\n\u2022 We introduce a dedicated network architecture designed to address the segmentation task of COVID-19 pulmonary CT lesions, with a particular focus on handling complex lesion shapes that are difficult to identify. This architecture integrates both capsule networks and U-Net, enhancing the model's capability to capture and encode critical information in lung CT images.\n\u2022 We innovatively design the integration of capsule networks pathway and U-Net encoder pathway in parallel, coupled through a pathway coupling mechanism. This design not only maximizes the complementary strengths of the two network structures but also allows for efficient information fusion. The model learns and expresses features more comprehensively, excelling particularly in scenarios involving complex lesion shapes and ambiguous boundaries.\n\u2022 We conduct extensive experiments on four publicly available datasets, including both binary segmentation tasks and multi-class segmentation tasks, and compare the proposed architecture against three baseline architectures"}, {"title": "2. Related works", "content": "The COVID-19 pandemic has sparked extensive research in the field of medical imaging, particularly in the domain of lung CT lesion segmentation. There are various methods for lung CT lesion segmentation. In this section, we will only discuss segmentation architectures based on Unet and the utilization of capsule networks for target segmentation."}, {"title": "2.1. Segmentation architecture based on Unet", "content": "The segmentation architecture based on Unet (Ronneberger et al., 2015) has found extensive application in the field of medical imaging, particularly for lung CT lesion segmentation tasks. Unet constitutes a classical CNNs architecture characterized by the concurrent presence of an encoder and a decoder. The encoder gradually extracts both low-level and high-level features from the image, while the decoder maps these features back to the image space, yielding intricate segmentation outcomes.\nIn the context of the COVID-19 pandemic, numerous researchers have adopted the Unet architecture for segmenting lesion regions in lung CT images, aiming to provide precise support for clinical diagnosis and disease monitoring. For instance, Chen et al. (2020) initially applied Unet++ (Zhou et al., 2018) to COVID-19 lung CT lesion segmentation. Unet++, an extension of the Unet framework, employs a series of nested and densely connected skip pathways between encoder and decoder subnetworks, further enhancing semantic relationships between them and achieving improved performance in lung CT lesion segmentation tasks. Zhao et al. (2021) reengineered the connection structure of Unet++ to propose SCOAT-Net, introducing a biologically motivated attention learning mechanism. This approach introduces specially designed spatial-wise and channel-wise attention modules, which collaborate to enhance the network's attention learning and consequently improve the segmentation accuracy. Building upon Unet, Bougourzi et al. (2023b) developed PDAtt-Unet, a pyramid dual-decoder Att-Unet (Lian et al., 2018) architecture that utilizes pyramid structures and attention gates to maintain global spatial awareness across all encoding layers. During the decoding phase, PDAtt-Unet incorporates two separate decoders employing attention gates to simultaneously segment infections and the lung region."}, {"title": "2.2. Capsule networks for object segmentation", "content": "Capsule networks, introduced by Sabour et al. (2017), have garnered significant attention within the field of object segmentation due to their unique architecture and potential to address certain limitations of traditional CNNs. In contrast to CNNs, that rely on pooling and hierarchical feature extraction, capsule networks aim to model the hierarchical relationships between features by representing them in the form of capsules.\nThe fundamental innovation of capsule networks lies in their ability to preserve spatial hierarchies and capture viewpoint information of objects. Traditional CNNs often struggle with handling variations in object poses and tend to rely heavily on the spatial pooling of features. Conversely, capsules within a capsule network encode multiple attributes of an object, such as pose, deformation, and texture, while also considering the relationships between these attributes. This makes capsule networks inherently suited for object segmentation tasks, where understanding spatial hierarchies and viewpoints is critical.\nHowever, the original architecture of capsule networks and dynamic routing algorithm incurs significant computational expenses in terms of memory and runtime (LaLonde et al., 2021). In order to address this concern, LaLonde et al. (2021) extended the concept of convolutional capsules and redefined the dynamic routing algorithm in two crucial ways. Firstly, children capsules are selectively routed to parents capsules within a localized spatial kernel. Secondly, transformation matrices are shared among all members of the grid within a specific capsule type, yet not shared across different capsule types. Subsequently, a multitude of researchers have delved deeper into the exploration of capsule network models for the purpose of object segmentation. For instance, Survarachakan et al. (2020) developed the Multi-SegCaps model, EM-routing SegCaps model, and U-Net model capable of segmenting an arbitrary number of classes. These models were applied to the segmentation of individual 2D slices, utilizing two neighboring slices on each side (five in total). Tran et al. (2022) proposed the 3DConvCaps model, which employs capsule networks for 3D image segmentation. 3DConvCaps constitutes a 3D encoder-decoder network with a convolutional capsule encoder, facilitating the learning of lower-level features using convolutional layers (short-range attention), while simultaneously modeling higher-level features (long-range dependencies) with capsule layers.\nIn this paper, the integration of the two techniques has been undertaken with the aim of COVID-19 lung CT lesion segmentation. However, distinct from these previous investigations, the CAD-Unet establishes a path coupling between Unet-encoder and convolutional capsules, thereby fusing the strengths of convolution and capsules to extract target features. Moreover, the incorporation of attention gates and a dual decoder has been implemented. These modules collectively interact to significantly bolster the overall performance of the CAD-Unet architecture. Subsequently, a comprehensive elucidation of the proposed network architecture is furnished."}, {"title": "3. The proposed approach", "content": "Within this section, an extensive elucidation of the CAD-Unet network architecture, fundamental constituents of the net-"}, {"title": "3.1. Network architecture", "content": "The overall architecture of CAD-Unet is illustrated in Fig. 1. Based on the foundational Unet framework, the capsule encoder path of capsule networks and the encoder path of Unet operate in parallel. Each layer of Unet incorporates ResBlocks to consolidate features. The decoder phase consists of two pathways: one for predicting the infected region, which serves as the final prediction, and the other for predicting the lung region, aiming to focus the network attention on the lung area since the infected region is exclusively present within the lung region. In the following sections, we will provide a detailed description of the core modules and loss functions employed in this network."}, {"title": "3.2. Convolutional capsule layer", "content": "Capsule networks, originally proposed by Sabour et al. (2017), consist of multiple capsule layers and innovatively represent the information transfer between capsule layers as vectors rather than scalars. These vectors not only encode specific entity types but also describe how entities are instantiated, including their postures, textures, deformations, and the presence of these features themselves Choi et al. (2019). Among them, the norm of the vector indicates the probability of the entity's existence, while the orientation of the vector indicates the configuration of the entity. Information between capsule layers is transmitted through a dynamic routing algorithm, which iteratively updates the weights between capsules, enabling higher-level capsules to effectively route the outputs of lower-level capsules. As a result, more accurate and robust feature representations are obtained. This dynamic routing mechanism is a key characteristics of capsule networks, allowing the network to learn hierarchical representations and spatial relationships of objects.\nUsing capsule-based networks for object segmentation poses several issues. The inherent complexity of the original capsule network architecture and dynamic routing algorithm results in significant computational demands, affecting memory usage and runtime performance (LaLonde et al., 2021). To address these issues, LaLonde et al. (2021) proposed SegCaps, where they introduced capsule sharing for each category, effectively reducing the computational overhead. Specifically, as illustrated in 2, the input feature map size of the capsule layer is $H \\times W \\times C \\times A$, where $H \\times W$ represents the feature map dimensions, $C$ is the number of capsule types, and $A$ is the size of each capsule. Firstly, the capsule $u_i$ for class i is linearly mapped to a higher-level feature $\\hat{u}_{i}$ using the same transformation matrix $W_i$. Then, all $\\hat{u}_{i}$ are weighted and summed to obtain the input $s$ of the higher-level capsule. Finally, the squash function is applied to $s$ to ensure that the vector direction remains unchanged while compressing the vector length between 0 and 1, representing the probability of entity presence. The dynamic routing process can be represented (1) as follows:\n$\\begin{aligned} &\\hat{u}_{i}=W_{i} u_{i}, \\\\ &s=\\sum c_{i} \\hat{u}_{i}, \\\\ &v=\\operatorname{squash}(s), \\\\ &\\operatorname{squash}(s)=\\frac{\\|s\\|^{2}}{1+\\|s\\|^{2}} \\frac{s}{\\|s\\|},\\end{aligned}$\nwhere $i$ is the capsule category, $u$ represents the input to the higher-level capsule, $W$ is the transformation matrix, $\\hat{u}_{i}$ corresponds to the input of the higher-level capsule, $c_{i}$ represents the coupling coefficients determined through the iterative dynamic routing process, and $v$ denotes the output of the capsule at this layer. $\\| \\|$ denotes $L_{2}$ norm.\nLinear mapping is achieved by the convolutional layer, where $W$ actually represents the parameters of the convolutional kernels. The convolutional layer employs a kernel size of $5 \\times 5$ with a stride of 2 , which enables the completion of one downsampling operation on the original feature map.\nThe coupling coefficients $c_i$ between capsule categories are determined by the dynamic routing algorithm. This algorithm is an iterative process that updates the values of $c_i$ through multiple rounds of iterations. The number of iterations r is a hyperparameter and is set to 3 in our architecture."}, {"title": "3.3. Two parallel encoder pathways", "content": "COVID-19 lung lesions exhibit diverse morphologies in CT images, including infiltrations, nodules, and patchy patterns, among others. Solely utilizing convolutional networks for learning target region features has limitations, while capsule networks utilize vectors to convey information, enabling a better capture of target morphology. Additionally, the dynamic routing algorithm connects different layers of capsules, allowing for a better capture of contextual relationships between objects. This advantage empowers capsule networks to handle issues such as object relationships and hierarchical structures in images. Therefore, we innovatively couple capsule networks with convolution in parallel, leveraging the strengths of capsule networks to compensate for the limitations of convolution.\nSpecifically, in the encoder stage of the network, we design two parallel paths: the capsule encoder path and the Unet encoder path, and the feature information from these two paths interacts with each other. The input image $x \\in R^{H \\times W \\times C}$ is fed into both paths, where $H$ and $W$ represent the height and width of the input image, respectively, and $C$ denotes the number of channels in the input image. Now, we will provide a detailed explanation of each path.\n(a) Capsules encoder pathway: In the capsule encoding pathway, we initially generate primary capsules through a convolutional layer with a kernel size of $5 \\times 5$, resulting in capsules of a single category $C_{0} \\in R^{H \\times W \\times 1 \\times 16}$. The primary capsule $C_{0}$ is then propagated to the primary capsule layer, which is a conventional convolutional capsule layer. It undergoes one routing iteration and returns 16-dimensional capsules for two categories, denoted as $C_{1} \\in R^{\\frac{H}{2} \\times \\frac{W}{2} \\times 2 \\times 16}$.\nFollowing this, at each stage, the convolutional capsule layer"}, {"title": "3.4. Attention gate", "content": "The conventional Unet framework incorporates skip connections that directly link low-level features from the encoder to high-level features in the decoder. However, this strategy introduces redundant and irrelevant information to the high-level features, adversely impacting the precision of target segmentation. Motivated by the Attention U-Net (Oktay et al., 2018), we integrate attention gates into the proposed architecture to address this limitation. The attention gates selectively emphasize relevant features while suppressing redundant and less informative ones, which greatly improves segmentation accuracy. By adaptively weighting the contributions of encoder and decoder features, the attention mechanism enhances the model's capability to focus on crucial regions, thereby refining the segmentation outcomes.\nThe structure of the attention gate is elucidated in Fig.4. This module takes an input feature map $x \\in R^{H \\times W \\times C_{x}}$ and a gate signal $g \\in R^{H \\times W \\times C_{g}}$, where $x$ signifies the output feature map of the encoder $l$-th layer, and $g$ represents the output feature map of the decoder $(l+1)$-th layer. Subsequently, the feature map $x$ and the gate signal $g$ undergo linear transformations through $1 \\times 1$ convolutional layers, followed by batch normalization and element-wise summation. Following this, the ReLU activation function is applied, and another 1 \u00d7 1 convolutional layer is utilized to derive spatial attention coefficients, denoted as Att for each pixel. Finally, these obtained attention coefficients are applied to the input feature map $x$, as expressed in equation (5):\n$\\begin{aligned}\\text { Att }&=\\sigma(\\operatorname{ReLU}(\\operatorname{BN}(W x)+\\operatorname{BN}(W g))) \\oplus F, \\\\text { out }_{A G} &=X i \\text { Att },\\end{aligned}$\nwhere $W_{x}$ and $W_{g}$ are $1 \\times 1$ convolutional operations used for linear transformations, and $\\psi_{1}$ is a $1 \\times 1$ convolutional operation employed to generate spatial attention coefficients. $\\text { out }_{A G}$ represents the output of the attention gate module."}, {"title": "3.5. Dual decoder", "content": "To focus the network on the infected regions, we employ the dual decoder used in DAtt-Unet (Bougourzi et al., 2023b). The dual decoder is designed to achieve simultaneous segmentation of both infections regions and lung regions. The objective is to guide the training process to explore the interior of the lung regions and differentiate between infected tissue and non-pulmonary tissue.\nAs shown in Fig.1, the decoder consists of two pathways: pathway 1 predicts the lung region, and pathway 2 predicts the infection region. The prediction results from pathway B serve as the final prediction target, while the existence of pathway A aims to guide the model focus on the lung region, as infection areas only occur within the lungs.\nSpecifically, except for the last layer, each layer in the decoder receives two inputs: the output from the previous layer of the decoder and the concatenated result of the attention gate output at the same layer. Similar to the encoder, these inputs undergo a ResBlock and an upsampling operation to become the current layer output. This output serves two purposes: 1) it is forwarded to the attention gate unit as the gating signal $g$, and 2) it is passed to the next layer of the decoder. The dual decoder process can be represented as equation (6):\n$\\mathrm{d}_{i, j}=R B_{j}(\\operatorname{Cat}_{j}(A G_{j}(U S_{j}(\\mathrm{d}_{i-1, j}), X_{i, j}), U S_{j}(\\mathrm{d}_{i-1, j}))), i \\in{0,1,2,3,4}, j \\in{1,2},$\nwhere i represents the layer index, and j represents the decoder pathway number. $d_i$ denotes the output of the i-th layer of the decoder, and $x_i$ represents the output of the i-th layer of the encoder. The upsampling operation is denoted by US, and AG represents the attention gate unit. It is essential to note that the two pathways of the decoder do not share any modules, including the attention gate unit."}, {"title": "3.6. Hybrid loss function", "content": "For CT lung lesion segmentation tasks, the complex morphology of the target region edges poses issue for accurate segmentation. Existing research has shown that edge information can provide valuable constraints to guide feature extraction for segmentation (Zhao et al., 2019; Wu et al., 2019; Zhang et al., 2019). Therefore, we employ the hybrid loss function proposed in PDEAttUnet (Bougourzi et al., 2023b) to guide the network training, which consists of three components: 1) infection segmentation loss ($L_{Inf}$) for the final task prediction, 2) lung segmentation loss ($L_{Lung}$) to assist in the final prediction of lung regions, and 3) edge loss ($L_{Edge}$) to enhance the segmentation of target edges. The hybrid loss function is formulated as equation (7):\n$\\begin{aligned}L_{\\text {all }}&=\\alpha L_{\\text {Inf }}+\\beta L_{\\text {Lung }}+\\gamma L_{\\text {Edge }}, \\\\L_{\\text {Inf }}&=-\\frac{1}{B \\cdot W \\cdot H} \\sum_{m=1}^{B} \\sum_{i=1}^{W \\cdot H} G_{R_{i}} \\cdot \\log (p_{i})+\\left(1-G_{R_{i}}\\right) \\log (1-p_{i}), \\\\L_{\\text {Lung }}&=-\\frac{1}{B \\cdot W \\cdot H} \\sum_{m=1}^{B} \\sum_{i=1}^{W \\cdot H} G_{L_{i}} \\cdot \\log (s_{i})+\\left(1-G_{L_{i}}\\right) \\log (1-s_{i}), \\\\L_{\\text {Edge }}&=-\\frac{1}{B \\cdot W \\cdot H} \\sum_{m=1}^{B} \\sum_{i=1}^{W \\cdot H} G_{E_{i}} \\cdot \\log (P_{E_{i}})+\\left(1-G_{E_{i}}\\right) \\log (1-P_{E_{i}}),\\end{aligned}$\nwhere $\\alpha, \\beta$, and $\\gamma$ are the weights for the infection segmentation loss ($L_{Inf}$), lung segmentation loss ($L_{Lung}$), and edge loss ($L_{Edge}$), respectively. $W$ and $H$ represent the width and height of the predicted mask, and $B$ is the batch size. The ground truth labels for infection and lung of pixel i are denoted as $G R_{i} \\in{0,1}$ and $G L_{i} \\in{0,1}$, respectively. Additionally, $p_{i}$ and $s_{i}$ represent the prediction probabilities of infection and lung for pixel i, obtained from the decoders used for segmenting infection and lung, respectively. On the other hand, for edge loss function, $G E_{i} \\in{0,1}$ and $P E_{i}$ are the ground truth label and the predicted probability of edge infection for pixel i, respectively. The ground-truth edge pixels are derived by applying morphological gradient to the ground-truth infection regions. Once the ground-truth edge pixels are obtained, the binary cross entropy (BCE) loss function is employed to compute the loss between the ground-truth edge pixels and their corresponding pixels in the mask map. The weight $\\gamma$ of the edge loss is determined through experimental evaluation for each dataset."}, {"title": "4. Experiments and results", "content": "The proposed method is evaluated on four publicly available datasets, namely COVID-19 CT segmentation (Radiologists, 2019), segmentation dataset nr.2 (Radiologists, 2019), COVID-19-CT-Seg dataset (Ma et al., 2021), and CC-CCII segmentation dataset (Zhang et al., 2020). Table 1 provides a summary of these datasets. These datasets will be used to evaluate the segmentation performance of the proposed architecture on binary and multi-class segmentation tasks. In the binary segmentation, the labels are divided into two classes: background and infection region. In the multi-class segmentation, the labels are divided into three classes: background, GGO, and CON.\nThe COVID-19 CT segmentation (Radiologists, 2019) dataset consists of 100 axial CT images (slices) from over 40 COVID-19 infected patients. It includes multi-class segmentation labels and will be used for the multi-class segmentation task.\nSegmentation dataset nr.2 (Radiologists, 2019) contains 9 3D CT scans with a total of 829 slices. Among these slices, 373 have been annotated by radiologists with binary and multi-class segmentation labels for COVID-19 infection. This dataset will be used for both binary and multi-class segmentation tasks.\nThe COVID-19-CT-Seg dataset (Ma et al., 2021) comprises 20 COVID-19 CT scans, where all cases are confirmed COVID-19 infections. This dataset has been annotated by experienced radiologists and labeled for binary classification. Among the total of 3520 slices, 1844 slices are infected. It will be used for binary segmentation tasks.\nThe CC-CCII segmentation dataset (Zhang et al., 2020) consists of 750 CT slices from 150 COVID-19 patients, manually annotated for background, lung field, GGO, and CON segmentation. It will be utilized for both binary and multi-class segmentation tasks."}, {"title": "4.2. Evaluation metrics", "content": "The model is evaluated by using the following metrics:\n1. F1 score (F1-S): The F1-S is the harmonic mean of precision and recall, indicating the balance between the true positive rate and the false positive rate,\n$\\text {F1score }=100 \\cdot \\frac{2 \\cdot T P}{2 \\cdot T P+F P+F N}$\n2. Intersection over union (IoU): The IoU, also referred to as the Jaccard index, computes the ratio of the intersection to the union of predicted and ground truth regions,\n$\\text {IoU }=100 \\cdot \\frac{T P}{(T P+F P+F N)}$\n3. Recall(Rec): The Rec, also referred to as sensitivity or the true positive rate, quantitatively measures the proportion of true positive predictions among all actual positive instances,\n$\\operatorname{Rec}=100 \\cdot \\frac{T P}{T P+F N}$\n4. Specificity (Spec): The Spec measures the proportion of true negative predictions among all actual negative instances,\n$\\operatorname{Spec}=100 \\cdot \\frac{T N}{F P+T N}$\n5. Precision (Prec): The Prec represents the proportion of true positive predictions among all positive predictions made by the model,\n$\\operatorname{Prec}=100 \\cdot \\frac{T P}{T P+F P},$\nwhere TP, TN, FP, FN are True Positives, True Negatives, False Positives, False Negatives, respectively."}, {"title": "4.3. Experimental setup", "content": "We conduct model training using the PyTorch framework. The experimental platform consists of an Intel Core i5 13600kf CPU, a single NVIDIA GeForce RTX 3090ti 24GB GPU, and the Windows 11 operating system. We utilize the Adam optimizer and set the batch size to 6, performing 120 epochs of training on the network. The initial learning rate is set to le-4 and is decayed by a factor of 0.1 after the 50th epoch and then again after the 90th epoch. Additionally, we apply data augmentation to the dataset, including random angle rotations ranging from -35 degrees to 35 degrees, as well as random horizontal and vertical flips. In terms of the hybrid loss function, the weights are set as 0.7, 0.3, and 1, respectively."}, {"title": "4.4. Experimental results", "content": "For the binary segmentation task, we compared our model with Unet, UNet++, Att-Unet, InfNet, SCOAT-Net, nCoVSeg-Net, and PDEAtt-Unet. Tables 2, 3, and 4 present the experimental results of the proposed model on datasets 2, 3, and 4, respectively. It can be observed that the proposed model achieved superior performance in most metrics across these datasets.\nOn dataset 2, our model outperformed the best-performing segmentation model, PDEAtt-Unet, with improvements of 4.23 in F1 score, 5.85 in IoU, and 4.90 in Recall. These significant improvements can be attributed to the parallel integration of the capsule network pathway and the U-Net encoder pathway in our model design. Capsule networks excel at capturing and preserving hierarchical relationships between objects, while U-Net is adept at extracting local features from images. Through the pathway coupling mechanism, our model effectively combines the advantages of both networks, resulting in more accurate segmentation of complex COVID-19 lung lesions.\nOn dataset 3, our model achieved improvements of 1.02 in F1 score, 1.33 in IoU, and 2.40 in Recall compared to PDEAtt-Unet. These enhancements further demonstrate the effectiveness of our model design, particularly in handling the diversity and complexity of different datasets. By integrating the capsule network and U-Net encoder in parallel, our model can more effectively identify and segment various lung lesions caused by COVID-19.\nOn dataset 4, our model also showed improvements in F1 score, IoU, and Recall compared to PDEAtt-Unet, with increases of 0.1, 0.15, and 1.94, respectively. Although these improvements are relatively smaller, they still indicate the consistency and stability of our model across different datasets. This consistency is due to the pathway coupling mechanism in our model, which allows efficient information flow between the two network pathways, enhancing segmentation accuracy and robustness.\nIn the multi-class segmentation task, we merge dataset 1 and dataset 2 into one multi-class segmentation dataset. On this combined dataset, we compare the proposed model with Unet, Att-Unet, Unet++, CopleNet, AnamNet, SCOAT-Net, and D-TrAttUnet among other models. Table 5 presents the experimental results of the proposed model for multi-class segmentation. It can be observed that the model achieved the best segmentation performance, outperforming the previous state-of-the-art segmentation model D-TrAttUnet by 2.79 in F1 score and 3.25 in IoU for the GGO classification. For the CON classification, the model surpasses the previous best segmentation model D-TrAttUnet by 2.00 in F1 score and 1.94 in IoU. Additionally, unlike D-TrAttUnet, the model does not use multiple layers of transformers, resulting in significantly reducing the number of model parameters by almost an order of magnitude compared to D-TrAttUnet."}, {"title": "4.5. Ablation study", "content": "To further validate the efficacy of CAD-Unet and its individual components, ablation experiments are conducted on four datasets for two tasks. Binary segmentation results on dataset 2, dataset 3, and dataset 4 are summarized in Table 6, while multi-class segmentation results on dataset 1 and dataset 2 are presented in Table 7. These experiments evaluate the effects of the capsule layer, dual decoders, and attention gate.\nIn the binary segmentation task, we initially examine the enhancement of the CAD-Unet module on the baseline model (Unet). As shown in No.1 and No.2 of Table 6, incorporating the capsule module leads to improvements in F1 score by 2.80, 1.40, and 0.65 on the three datasets, with varying degrees of enhancement in other metrics. Subsequently, the impact of the capsule, dual decoders, and attention gate modules on the binary segmentation capability of the proposed CAD-Unet is investigated, from No.3 to No.6 as depicted in Table 6. Disabling the capsule results in a decrease in F1 score by 11.44, 1.51, and 1.01 on the three datasets, which indicates the critical role of the capsule layer in capturing hierarchical spatial relationships and enhancing feature representation, significantly contributing to the accuracy of the model. Following the deactivation of the attention gate, F1 scores decrease by 6.74, 0.39, and 0.24 on the three datasets, while deactivating the dual decoders lead to even larger decreases of 15.51, 2.51, and 0.34. These findings suggest that the attention gate and dual decoders in CAD-Unet also bring significant improvements to segmentation performance in binary segmentation tasks. The introduction of dual decoders helps the network focus on the lung region, while the attention gate design enables the model to concentrate more on crucial spatial information, thereby enhancing the recognition of subtle features.\nIn the multi-class segmentation task, similar ablation experiments are conducted. As shown in Table 7, akin to the results in the binary segmentation task, the CAD-Unet module demonstrates a noticeable performance improvement over the baseline model (Unet). Incorporating the capsule into the baseline model results in F1 score improvements of 2."}]}