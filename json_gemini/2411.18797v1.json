{"title": "UOE: UNLEARNING ONE EXPERT IS ENOUGH FOR\nMIXTURE-OF-EXPERTS LLMS", "authors": ["Haomin Zhuang", "Yihua Zhang", "Kehan Guo", "Jinghan Jia", "Gaowen Liu", "Sijia Liu", "Xiangliang Zhang"], "abstract": "Recent advancements in large language model (LLM) unlearning have shown\nremarkable success in removing unwanted data-model influences while preserv-.\ning the model's utility for legitimate knowledge. However, despite these strides,\nsparse Mixture-of-Experts (MoE) LLMs\u2013a key subset of the LLM family-have\nreceived little attention and remain largely unexplored in the context of unlearn-.\ning. As MoE LLMs are celebrated for their exceptional performance and highly\nefficient inference processes, we ask: How can unlearning be performed effec-\ntively and efficiently on MoE LLMs? And will traditional unlearning methods be\napplicable to MoE architectures? Our pilot study shows that the dynamic routing\nnature of MoE LLMs introduces unique challenges, leading to substantial utility\ndrops when existing unlearning methods are applied. Specifically, unlearning dis-\nrupts the router's expert selection, causing significant selection shift from the most\nunlearning target-related experts to irrelevant ones. As a result, more experts than\nnecessary are affected, leading to excessive forgetting and loss of control over\nwhich knowledge is erased. To address this, we propose a novel single-expert un-\nlearning framework, referred to as UOE, for MoE LLMs. Through expert attribu-\ntion, unlearning is concentrated on the most actively engaged expert for the spec-\nified knowledge. Concurrently, an anchor loss is applied to the router to stabilize\nthe active state of this targeted expert, ensuring focused and controlled unlearn-\ning that preserves model utility. The proposed UOE framework is also compatible\nwith various unlearning algorithms. Extensive experiments demonstrate that UOE\nenhances both forget quality up to 5% and model utility by 35% on MoE LLMs\nacross various benchmarks, LLM architectures, while only unlearning 0.06% of\nthe model parameters.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the extraordinary ability in generating human-like content (Touvron et al., 2023), the rapid\ndevelopment of large language models (LLMs) have raised a series of ethical and security concerns,\nsuch as pretraining on copyrighted data (Sun et al., 2024), bias perpetuation (Motoki et al., 2023),\nthe generation of toxic, biased, or illegal contents (Wen et al., 2023), and facilitating making cyber-\nattacks and bio-weapons (Li et al., 2024). As a solution, the problem of Machine Unlearning (MU)\narises (also referred to LLM unlearning) (Liu et al., 2024c), aiming to scrub the influence of the\nundesired training data and removing their corresponding generation abilities, while preserving the\ninfluence of other remaining valid data (Jia et al., 2024a; Shi et al., 2024; Jia et al., 2024b).\nWhile LLM unlearning has recently become a major research thrust, past efforts have only focused\non effective unlearning methods for conventional LLMs. In contrast, sparse Mixture-of-Experts\nLLM (MOE LLM) (Jiang et al., 2024; xAI, 2024; Databricks, 2024; Abdin et al., 2024; Liu et al.,\n2024a), designed to reduce computational burdens during inference, remained unexplored in this\ncontext. As a key member of the LLM family, MoE LLMs offer substantial scalability without a\ncorresponding increase in computational costs (Jiang et al., 2024; Team, 2024; Dai et al., 2024).\nThanks to their dynamic routing mechanism, MoE LLMs direct inference through different model"}, {"title": "2 RELATED WORKS", "content": "Machine Unlearning for LLMs. A growing body of research has investigated the problem of un-\nlearning in large language models (LLMs) (Yao et al., 2024; Lu et al., 2022; Jang et al., 2022; Kumar\net al., 2022; Zhang et al., 2023a; Pawelczyk et al., 2023; Eldan & Russinovich, 2023; Ishibashi &\nShimodaira, 2023; Yao et al., 2023; Maini et al., 2024; Zhang et al., 2024; Li et al., 2024; Wang\net al., 2024a; Jia et al., 2024b; Liu et al., 2024c;b; Thaker et al., 2024). These studies have practical\napplications, such as removing sensitive information (Jang et al., 2022; Eldan & Russinovich, 2023;\nWu et al., 2023) and preventing the generation of harmful or biased content (Jang et al., 2022; Eldan\n& Russinovich, 2023; Wu et al., 2023; Lu et al., 2022; Yu et al., 2023; Yao et al., 2023; Liu et al.,\n2024d), memorized sequences (Jang et al., 2022; Barbulescu & Triantafillou, 2024), and copyrighted\nmaterial (Eldan & Russinovich, 2023; Jang et al., 2022). To facilitate unlearning, recent methods\naim to bypass the need for retraining models from scratch by excluding the forget set containing\nthe targeted data to be removed (Ilharco et al., 2022; Liu et al., 2022; Yao et al., 2023; Eldan &\nRussinovich, 2023; Jia et al., 2024b; Zhang et al., 2024; Li et al., 2024; Thaker et al., 2024; Liu\net al., 2024b). Techniques like task arithmetic also enable efficient model editing through parameter\nmerging (Hu et al., 2024; Ilharco et al., 2022). Although these methods do not provide exact unlearn-\ning akin to full retraining, they remain efficient and effective under empirical unlearning evaluation\nmetrics. Approaches often include model fine-tuning and optimization (Liu et al., 2022; Yao et al.,\n2023; Eldan & Russinovich, 2023; Jia et al., 2024b; Zhang et al., 2024; Li et al., 2024), or input\nprompting and in-context learning (Thaker et al., 2024; Pawelczyk et al., 2023; Liu et al., 2024b).\nOther approaches, such as localization-informed unlearning, identify and locally edit model units\n(e.g., layers or neurons) closely related to the data or tasks being unlearned (Meng et al., 2022; Wu\net al., 2023; Wei et al., 2024b). Despite these efforts, studies have shown that forgotten knowledge\ncan often still be extracted from models post-unlearning (Patil et al., 2024; Liu et al., 2024e; Lynch\net al., 2024; Shostack, 2024). However, most existing research has focused on dense LLMs, leav-\ning unlearning in MoE LLMs largely unexplored. For example, the unlearning of Mixtral-8 \u00d7 7B\nis discussed in Li et al. (2024), but only a single method with ad-hoc adjustments was examined.\nThis work aims to fill this gap by conducting a comprehensive study of various unlearning methods,\nbenchmarks, and MoE models, addressing the specific challenges posed by the MoE architecture.\nMoE-based LLMs. Sparse Mixture-of-Experts (MoE) models are designed to activate only a subset\nof expert networks for each input, enabling substantial model scaling with minimal computational\noverhead (Shazeer et al., 2017). Current approaches to MoE model development can be categorized\ninto two types: training from scratch (Fedus et al., 2022; Zoph et al., 2022a; Shen et al., 2023) and\nbuilding from dense checkpoints (Zhang et al., 2021; Komatsuzaki et al., 2022; Zhu et al., 2024).\nOver recent years, MoE models have seen key advancements, including improvements in scala-\nbility (Riquelme et al., 2021; Kim et al., 2021; Zhou et al., 2022; Zoph et al., 2022a), efficiency\noptimization (Fedus et al., 2022; Lepikhin et al., 2020; Chowdhery et al., 2023), and expert bal-\nancing techniques (Cong et al., 2024; Zoph et al., 2022b; Dai et al., 2022). The implementation of\ntransformer-based MoE models has been successfully integrated into LLMs, significantly enhancing\ninference efficiency (Jiang et al., 2024; Dai et al., 2024; Team, 2024; xAI, 2024; Hong et al., 2024;\nAbdin et al., 2024; Lieber et al., 2024; Yang et al., 2024; Zhu et al., 2024; Databricks, 2024; Xue\net al., 2024). For example, DeepSeekMoE (Dai et al., 2024) improves expert specialization by seg-\nmenting experts into smaller subsets for flexible activation, while isolating shared experts to reduce\nredundancy and capture common knowledge. Similarly, Qwen1.5-MoE (Team, 2024) partitions\na standard FFN layer into smaller segments to create multiple experts, introducing a fine-grained\nrouting mechanism that enables Qwen1.5-MoE to match the performance of 7B models while us-\ning only one-third of the activation parameters. Despite the efficiency gains provided by MoE's\ndynamic routing system, existing research highlights additional challenges compared to traditional\ndense models, including unstable training (Zoph et al., 2022a; Dai et al., 2022), robustness issues\n(Zhang et al., 2023b; Puigcerver et al., 2022), and complications in parallel deployment (Hwang\net al., 2023; Gale et al., 2023). In this work, we show that the root cause of the ineffectiveness of\nexisting unlearning methods for MoE LLMs also stems from the dynamic routing system."}, {"title": "3 PRELIMINARIES", "content": "In this section, we start by presenting the mathematical formulation of LLM unlearning. The lack of\nexploration on MoE LLM unlearning inspires us to investigate whether existing unlearning methods"}, {"title": "Preliminaries on MoE LLM unlearning.", "content": "Based on the generic formulation of LLM unlearning\noutlined in Liu et al. (2024c), the task of LLM unlearning can be formulated as eliminating the\ninfluence of a specific \u2018unlearning target\u2019\u2013whether it is related to data, knowledge, or model capa-\nbilities-from a pretrained LLM (denoted by $\\theta_0$). The unlearning target is typically defined by a\nforget set $D_f$, which contains the information or knowledge to be removed. To ensure the model\nretains its generation ability (i.e., utility) after unlearning, a retain set $D_r$ is introduced, consisting\nof data unrelated to the unlearning target. With this setup, the LLM unlearning problem is usually\nformed as a regularized optimization problem, finetuned from $\\theta_0$ using both the forget set $D_f$ and\nthe retain set $D_r$:\n$\\min l_f(\\theta; D_f) + \\lambda l_r(\\theta; D_r).$ \nHere, $\\theta$ represents the model parameters to be updated during unlearning, $l_f$ and $l_r$ denote the\nforget loss and retain loss, respectively, with $\\lambda \\geq 0$ serving as a regularization parameter to balance\nbetween unlearning and preserving utility.\nNext, we provide a brief introduction to how the routing system operates in the MoE LLM architec-\nture. In MoE LLMs, e.g., DeepSeek-v2-Lite (Liu et al., 2024a), the feed-forward networks (FFNs)\nof Transformers are split into multiple experts and activated by the output of the router in front of\ncorresponding\nthe expert layers, see Fig. 1(b) for illustration. In the l-th layer, given the input $u_t^{(l)}$\nto the t-th token, router layers calculate the score of each token and assign them to top-K experts:\n$s_{i,t}^{(l)} = \\text{Softmax}(\\text{Router}(u_t^{(l)}))$\n$g_{i,t}^{(l)} = \\begin{cases} s_{i,t}^{(l)} & \\text{if } s_{i,t}^{(l)} \\in \\text{TopK}(\\{s_{k,t}^{(l)}\\mid 1 \\leq k \\leq N\\}) \\\\ 0 & \\text{otherwise} \\end{cases}$\nHere, Router(\u00b7) denotes the router layer, $s_{i,t}$ is the token-to-expert affinity, TopK(\u00b7) selects the\nhighest K value in the set, N is the number of experts, and $g_{i,t}$ is the score assigned by router\nfor the i-th expert. Then, the hidden state $h_t'^{(l)}$ of FFNs can be calculated as: $h_t'^{(l)} = u_t^{(l)} +\\\\\n\\Sigma_i g_{i,t}^{(l)} \\text{FFN}_i^{(l)}(u_t^{(l)})$, where $\\text{FFN}_i^{(l)}(\u00b7)$ denotes the i-th expert. Then, $h_t'^{(l)}$ is sent to the next layer\nof Transformer blocks for further processing."}, {"title": "Unlearning for MoE LLM is not trivial: a pi-lot study.", "content": "The goal of unlearning is twofold: (1)\nto ensure the model forgets the targeted informa-\ntion and knowledge stored in $D_f$, and (2) to pre-\nserve the model utility without significant degrada-\ntion. Our pilot study reveals that the special rout-\ning system in MoE LLMs introduces additional chal-\nlenges to unlearning, rendering existing methods in-\neffective. We applied four widely used LLM un-\nlearning methods: GA (Gradient Ascent) (Eldan &\nRussinovich, 2023), GDIFF (Gradient Difference)\n(Maini et al., 2024), NPO (Negative Preference Op-"}, {"title": "4 OUR PROPOSAL: UNLEARNING ONE EXPERT (UOE)", "content": "In this section, we delve into the failure cases highlighted in Sec. 3 by analyzing the behavior of\nrouters and their expert selection patterns. We then identify two primary root causes underlying the\npoor unlearning performance in MoE LLMs. Based on these insights, we introduce UOE, a new\nunlearning paradigm designed to achieve controllable and effective unlearning for MoE LLMs.\nUncovering the root cause: 'short-cut' in MoE LLM unlearning and expert selection shift. In\norder to fully understand the failure cases of MoE LLM unlearning, we begin by inspecting and\nmonitoring the expert selection pattern of the unlearned model. In Fig. 2, we show the proportion\nof tokens assigned to each selected expert on the data samples from WMDP dataset (Li et al., 2024).\nFor the input of a specific topic, a small portion of experts (around 6 to 9 out of 64 experts) were\nassigned with the majority of the tokens in each layer, which was also confirmed in Wang et al.\n(2024b). Thus, we have the following insight:"}, {"title": "UOE for effective MoE LLM unlearning.", "content": "As\ndiscussed earlier, a new paradigm tailored for\nMoE LLM unlearning is urgently needed to ad-\ndress the challenges of unintentional expert se-\nlection shifts in routers and excessive unlearn-\ning of non-target experts. Therefore, we pro-\npose a framework that (1) identifies the most\nrelevant target experts, (2) ensures that these\ntarget experts remain highly activated through-\nout the unlearning process to avoid selection\nshifts, and (3) limits the impact of unlearning\non non-target experts. Spurred by these, we in-\ntroduce UOE, where unlearning is confined to\na single expert. We refer the readers to Alg. 1\nfor an illustration of UOE. This approach starts with an expert attribution process to accurately\nidentify the most relevant experts for the unlearning task.\nWhile the token assignment ratio for each expert, as shown in Fig. 2, can\nserve as a basic attribution metric, it overlooks finer details that are important for precise compar-\nisons, due to the hidden states in each layer summed by weighted average. To address this, we adopt\na gating score-based task affinity calculation method from (Wang et al., 2024b). Specifically, the\naffinity score for the i-th expert $e_i^{(l)}$ in the l-th layer of an MoE LLM is defined as:\n$\\gamma_i^{(l)} = \\frac{1}{Z} \\sum_{j=1}^Z \\frac{1}{L_j} \\sum_{t=1}^{L_j} g_{i,t}^{(l)},$\nwhere Z is size of the calibration dataset used for expert attribution, $L_j$ represents the length of the\nj-th input sequence $x_j$, and $g_{i,t}^{(l)}$ is the probability score assigned to expert $e_i^{(l)}$ for the t-th token.\nFollowing Wang et al. (2024b), the attribution data can be a subset universally sampled from the\noriginal forget set. We find that a subset containing over 100,000 tokens is robust enough to select\nthe most relevant experts for an unlearning task. For each layer, we rank the experts based on their\naffinity score and select the top expert as the target expert for unlearning.\n A key challenge in unlearning is the expert selection shift, where the true\ntarget experts are hidden by the routers, while less relevant experts are activated during inference and\ninadvertently involved in the unlearning process. To mitigate this, we propose the router anchor loss,\nwhich encourages the previously identified target expert to remain consistently activated throughout\nunlearning. The loss is formulated as:\n$L_{anchor}^{(l)} = ||g^{(l)} - [\\alpha_1, \\alpha_2,...,\\alpha_E]||^2,$"}, {"title": "5 EXPERIMENT", "content": "5.1 EXPERIMENT SETUPS\nTo demonstrate the effectiveness of our proposed method, we eval-\nuate and compare it against different baselines on two widely accepted LLM unlearning benchmarks:\nWMDP (Li et al., 2024) and RWKU (Jin et al., 2024). WMDP assesses the model's ability to unlearn\nand prevent the generation of hazardous knowledge in biosecurity, cybersecurity, and chemical se-\ncurity contexts. RWKU, on the other hand, evaluates the model's capability to eliminate knowledge\nabout 200 real-world celebrities, simulating a private information protection task. We note that other\ncommonly used benchmarks, such as TOFU (Maini et al., 2024) and MUSE (Shi et al., 2024), are\nless appropriate in this work. These benchmarks require models to be fine-tuned before unlearning,\nwhich introduces additional biases to the results for MoE LLMs due to the known instability in\ntraining and the tricky hyper-parameter tuning involved (Jiang et al., 2024), often leading to training\ncollapse (Zoph et al., 2022a).\nModels. We evaluate different unlearning methods on two MoE LLMs: Qwen1.5-MoE-A2.7B-\nChat (Qwen), mistralai/Mixtral-8x7B-Instruct-v0.1 (Mixtral), and DeepSeek-V2-Lite (DeepSeek),\nrepresenting the two mainstream MoE LLM training schemes: upcycle-from-dense and train-from-\nscratch, respectively. Qwen has a total of 14.3 billion parameters, with 2.7 billion activated during\ninference, while DeepSeek has 16 billion parameters, of which 2.4 billion are activated during infer-\nence. Mixtral has 45 billion parameters, of which 12.9 billion are activated. Due to the computation\nlimitation, Mixtral is only applied on parameter-efficient fine-tunin settings.\nWe evaluate the performance of the unlearned LLMs based on two key metrics:\nunlearning efficacy (UE) and preserved model utility (UT). For the WMDP task, UE is measured"}, {"title": "5.2 EXPERIMENT RESULTS", "content": "Effectiveness of UOE in preserving model utility and unlearning efficacy. In Tab. 2, we present\nthe UE (unlearning efficacy) and UT (utility) performance of our proposed UOE when integrated\ninto different unlearning methods GA, GDIFF, NPO, and RMU. First, one of the most notable find-\nings is that UOE significantly improves model utility (UT) across all tested methods. For instance,\nwhen applied to baseline methods like GA, GDIFF, and RMU, UOE consistently mitigates the se-\nvere utility drops (greater than 10%) that occur with the unmodified methods. This is particularly\nevident in scenarios where baseline methods without UOE exhibit drastic performance degradation\nin model utility (highlighted in red), while the same methods paired with UOE show substantial\nrecovery. For example, the utility of GA on Qwen for the WMDP task drops from 0.5979 to 0.3393,\nbut with UOE, the utility improves to 0.5012, restoring much of the lost performance. Similarly,\nGDIFF on RWKU suffers a significant utility loss from 0.5979 to 0.3495, but when UOE is ap-\nplied, utility rises back to 0.5253, nearly matching the original pretrained performance. Second,\nbeyond utility preservation, the unlearning efficacy (UE)\u2014measured through FA-remains either\nunaffected or slightly improved when UOE is employed. This balance between utility preservation\nand effective unlearning highlights the advantage of UOE. For instance, GDIFF+UOE reduces the\nforget accuracy (FA) on Qwen (WMDP) from 0.2964 to 0.2445, demonstrating better unlearning\nwhile still achieving a higher utility score. Similarly, RMU+UOE on DeepSeek (WMDP) lowers\nthe FA from 0.2530 to 0.2859, with a corresponding utility improvement from 0.4540 to 0.5424.\nNotably, methods such as GDIFF and RMU, which experience significant utility loss when used"}, {"title": "Comparison of different design choices in UOE framework.", "content": "In designing our UOE method, we opted to\nunlearn only a single expert in one specific layer, guided\nby the affinity score. However, to justify this design deci-\nsion, we conducted a series of empirical studies compar-\ning alternative approaches. These experiments were car-\nried out using the RMU unlearning method on the WMDP\ntask, where we tuned each approach until the model fully\nunlearned (i.e., FA reached 25%), and then compared (1)\nif the problem of the expert selection shift has been prop-\nerly addressed and (2) the model utility score (UT) across\nthe different strategies."}, {"title": "6 CONCLUSION", "content": "In this paper, we for the first time examine the challenges of applying existing MU techniques\nto MoE LLMs and carefully investigate the synergy between the dynamic routing system of MoE\nLLM and the unlearning effects. To address these issues, we proposed UOE, a novel framework that\nunlearns a single expert in a targeted layer while stabilizing expert selection through a router anchor\nloss. This approach mitigates expert selection shifts and achieves efficient unlearning with minimal\nparameter updates. Extensive experiments show that UOE significantly outperforms traditional\nunlearning methods and other parameter-efficient fine-tuning techniques, providing a robust solution\nfor MoE LLM unlearning tasks."}, {"title": "A EXPERIMENT SETTINGS", "content": "Hyperparameter selection. We set the learning rate to 5e-5 for GA, NPO, and GD while setting it\nto 1e-4 for UOE. The batch size is 4 for GA, NPO, and GD, while it is set to 16 for UOE. In NPO,\nthe beta value is set to 0.001. The alpha for the retain loss is set to 1 in both GD and NPO. For RMU,\nwe follow the hyperparameters specified in the original work. We configure the steering coefficients\nas 8000 for Qwen and 32000 for Deepseek, as UOE targets deeper layers in these models. For ESFT,\nwe set the threshold p = 0.15.\nDataset Settings. For the WMDP dataset, we use the cyber-forget-corpus as the forget set and\nwmdp-cyber as the evaluation set, in line with WMDP (Li et al., 2024). The Wikitext (Merity\net al., 2016) dataset serves as the retain set for both GD and RMU tasks, also following WMDP (Li\net al., 2024). In the RWKU (Jin et al., 2024) dataset, we follow the original study by selecting\n100 individuals as unlearning targets. The train_original_passage set, which includes Wikipedia\ndescriptions of these 100 individuals as provided in the paper, is used as the forget set.\nEvaluation Settings. We utilize the LM Evaluation Harness (Gao et al., 2024) to measure zero-shot\naccuracy on the MMLU and WMDP cyber datasets. The mean accuracy across all tasks in MMLU\nserves as a measure of model utility. For the RWKU dataset, we adhere to the original settings,\nusing the prompt \"Please complete the blank in the following question. Question:\" for fill-in-the-\nblank tasks and \"Please briefly answer the following question. Question:\" for generation tasks."}, {"title": "B JAILBREAK ATTACK ON UOE", "content": "Recent works introduce prompt-level attacks (Zou et al., 2023; Liu\net al., 2023; Zhuang et al., 2023; Wei et al., 2024a) to recover hid-\nden knowledge from generative models. To investigate the soft\nprompt attack, we employ the most representative and powerful at-\ntack GCG (Zou et al., 2023) in a white-box setting to optimize the\nprompt, aiming to force responses to begin with \u201cSure, here is the\nanswer:\u201d. The number of optimization steps is increased to 5000, while other hyperparameters re-\nmain at the default settings. Given the computational demand (approximately 1 GPU hour on an\nA100 for generating a single soft prompt), we optimize 400 prompts across 400 samples in RWKU.\nSince not all responses begin with \"Sure, here is the answer:\", we filter for those containing the word\n\"answer\" and then assess the forget quality both with and without GCG-generated prompts. Experi-"}, {"title": "C TOKEN PROPORTION SHIFT VISUALIZATION", "content": "We visualize the expert selection distribution in one layer across the unlearning process in Fig.6 in\nrevision. The figures sort the experts in layer 21 by the selection proportion in the original model\nand keeps this order to plot the figure in unlearned models to show the changes in all experts. The\nresults show that GA algorithm even decreases the uncertainty in WMDP after unlearning."}, {"title": "D SENSITIVITY ANALYSIS OF HYPERPARAMETER a", "content": "We conduct experiments on Deepseek unlearned by GA with RWKU dataset to explore the perfor-\nmance of different \u03b1. As shown in Tab. 10, the results indicate that UOE is robust to a wide range\nof \u03b1 and achieves the best performance when \u03b1 = 1."}]}