{"title": "Is Peer-Reviewing Worth the Effort?", "authors": ["Kenneth Church", "Raman Chandrasekar", "John E. Ortega", "Ibrahim Said Ahmad"], "abstract": "How effective is peer-reviewing in identifying\nimportant papers? We treat this question as\na forecasting task. Can we predict which pa-\npers will be highly cited in the future based\non venue and \"early returns\" (citations soon\nafter publication)? We show early returns are\nmore predictive than venue. Finally, we end\nwith constructive suggestions to address scal-\ning challenges: (a) too many submissions and\n(b) too few qualified reviewers.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Prioritization as a Forecasting Task", "content": "How effective is peer-reviewing in identifying\nimportant papers? Since readers cannot afford to\nread everything, should they prioritize papers in\ntop venues, or something else? Following Davletov\net al. (2014); Ma et al. (2021), we treat this ques-\ntion as a forecasting task. Can we predict which\npapers will be highly cited in the future? Both\nvenue and \"early returns\" (citations soon after pub-\nlication) are statistically significant, but early cita-\ntions have larger correlations with future citations\nas shown in Figure 1. This figure will be explained\nin more detail in section 3. Data for figures and\ntables is posted on GitHub.\nAbramo et al. (2019) also found early citations to\nbe more predictive than venue (impact): \"the role\nof the impact factor in the combination becomes\nnegligible after only two years from publication.\""}, {"title": "1.2 H-Index and Impact", "content": "In some organizations, authors are encouraged to\npublish in top tier venues, using statistics such as\nh-index (Hirsch, 2005) and impact (Garfield, 2006)\nto rank authors, venues, countries (Hyland, 2023)\nand more. We use similar summary statistics to\nshow that conditioning on early citations is more\neffective than conditioning on venue. That is, we\ngroup papers by venue and by early citations (one\nyear after publication), and summarize citations for\nthe fourth year after publication with h (h-index)\nand \u03bc (impact). Results do not depend too much on\nthe details of these definitions of early and future\ncitations because citations are highly correlated\nover time\nWhen we discuss Tables 4-5, h and u are better\nfor papers conditioned on early citations than for\npapers conditioned on venue. In particular, papers\nin less selective venues (Workshops/ArXiv) with a\nfew early citations tend to have more citations in\nthe future than papers in more selective venues.\nIn addition to h and \u00b5, Tables 4-5 report N (num-\nber of papers in each group) and \u03c3 (standard devi-\nation). N will be used in discussions of inclusive-\nness and o will be used in discussions of robustness.\nWe will suggest prioritizing papers with early cita-\ntions is more effective than prioritizing by venue:"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Metrics: H-index and Impact Factor", "content": "There is considerable work on metrics of success\nsuch as impact factor (Garfield, 2006) and h-index\n(Hirsch, 2005). Both of these summary statistics\nare computed over a group of papers, where papers\nare typically grouped by author or by venue, de-\npending on whether one is interested in measuring\nsuccess by author or by venue. We will group pa-\npers in additional ways such as papers with T or\nmore citations in the first year after publication in\norder to compare scores of success by early returns\nwith scores by other factors such as venue.\nImpact factor, \u03bc, is simply the average of citation\ncounts for papers in the group, and h-index, h, is the\nnumber of papers in the group with h or more cita-\ntions. Many journals report impact factors. Google\nScholar ranks venues by h5, a variant of h-index,\ncomputed over the last five years. In addition to\ntop venues, Google also provides details for many\nfields such as Computational Linguistics."}, {"title": "2.2 Numerous Challenges to Reviewing", "content": "The peer-review process, despite being an integral\npart of academic scholarship, has been a subject of\ncriticism on multiple fronts (Jefferson et al., 2002):\nthe practice of peer review is based on\nfaith in its effects, rather than on facts.\nIn this work, we assume reviews and other as-\nsessments of value should be leading indicators\nof future citations, following suggestions we have\nmade elsewhere (Church, 2005, 2020). While this\nassumption may be controversial, it provides an\nobjective path forward. There are, of course, nu-\nmerous challenges in reviewing processes; the first\nthree challenges below are discussed in Sections\n2.2.1-2.2.3; scale/growth is discussed in 2.3.2."}, {"title": "2.2.1 Purpose of peer-reviewing?", "content": "What is the purpose of peer-reviewing? The task\nis not very well-defined. According to Rogers and\nAugenstein (2020), \u201creviewers and area chairs face\na poorly defined task forcing apples-to-oranges\ncomparisons.\" An evaluation of biomedical re-\nsearch publications (Chauvin et al., 2015) con-\ncluded: \"The most important tasks for peer review-\ners were not congruent with the tasks most often\nrequested by journal editors in their guidelines to\nreviewers.\""}, {"title": "2.2.2 Validity and Reliability", "content": "There is considerable discussion of validity and\nreliability in Experimental Psychology (Krippen-\ndorff, 2018). Evaluations of the reliability of peer-\nreviewing are worrisome. Cortes and Lawrence\n(2021) revisited an experiment based on NIPS-\n2014 (now known as NeurIPS): \u201cFrom the con-\nference 10% of the papers were randomly chosen\nto be reviewed by two independent program com-\nmittees... results showed that the decisions between\nthe two committees was better than random, but\nstill surprised the community by how low it was.\u201d\nThe follow up study looked at review scores and\nfuture citations. They failed to find a significant\ncorrelation for accepted papers (their figure 6). For\nrejected papers that appeared elsewhere, the corre-\nlation was not large (their figure 8).\nA recent evaluation of reviews (Goldberg et al.,\n2023) found \"many problems that exist in peer\nreviews of papers\u2014inconsistencies, biases, miscal-\nibration, subjectivity-also exist in peer reviews of\npeer reviews.\""}, {"title": "2.2.3 Vulnerabilities, Cheating and Ethics", "content": "There are opportunities for authors, reviewers and\nother parties to use/abuse chatbots. A number of\nfunding agencies (NIH\u2074 and ARC) and journals"}, {"title": "2.3 Related Work on Predicting Citations", "content": "This paper questions whether peer-reviewing is\nworth the effort. Prior work is more about improv-\ning predictions (subsubsection 2.3.1), or helping au-\nthors increase their citations (subsubsection 2.3.2)."}, {"title": "2.3.1 Improving Predictions", "content": "There is a considerable body of work on predict-\ning citations. Predicting citations can be viewed\nas a special case of time series prediction. There\nare many use cases, especially in finance: (Salinas\net al., 2020). Prior work often focuses on methods:\nlinear regression (Pobiedina and Ichise, 2016), neg-\native binomials (Onodera and Yoshikane, 2015),\nclustering (Davletov et al., 2014), nearest neigh-\nbors (Yan et al., 2011) and deep networks (Abr-\nishami and Aliakbary, 2019; Ruan et al., 2020).\nThere is considerable work on link prediction in\nthe literature on GNNs (graph neural networks) us-\ning the ogbl-citation2 task in OGB (Open Graph\nBenchmark) (Hu et al., 2020). In more recent work,\nde Winter (2024) aims to \"pave the way for AI-"}, {"title": "2.3.2 Advice to Authors", "content": "There is considerable advice to authors on how\nto increase citations. We have argued elsewhere\n(Church, 2017) that secondary sources are cited\nmore than primary sources; the most cited papers\noften help others make progress, e.g., datasets,\nGitHubs, models on HuggingFace, benchmarks,\ntools, surveys, textbooks. By construction, the last\nword on a topic is not cited. The most cited pa-\nper is rarely the first, last or best; simplicity and\naccessibility are preferred over timing and quality.\nTahamtan et al. (2016) survey the literature on\nadvice to authors, assigning prior work to 28 fac-\ntors, which we have aggregated/condensed down\nto 8. Their 28 factors seem plausible, though it is\nnot possible to discuss all 28 factors in this paper.\n1. Intrinsic properties of paper: quality, length,\nnumber of references. Figures, charts and ap-\npendices can increase citations, but challeng-\ning equations can decrease citations.\n2. Venue: metrics (\u03bc, h), prestige, language.\n3. Discipline/subject/topic/methodology\n4. Accessibility and visibility of papers: Avoid\npay walls (Lawrence, 2001; Eysenbach, 2006),\nand promote papers on social media/ArXiv.\n5. Primary Source vs. Secondary Source: Text-\nbooks and survey papers are highly cited, as\nare tools, benchmarks and datasets.\n6. Demographics of author(s): Number of au-\nthors, self-citations, country, gender, age, rep-\nutation, productivity, affiliation, funding."}, {"title": "3 Predictions Based on Citations", "content": "As suggested above, we will use a prediction task\nto show that early returns are more effective than\nvenue. Figure 1 is based on citation counts from\nSemantic Scholar (S2) (Wade, 2022). For papers\nin ACL Anthology, PubMed and ArXiv, published\nbetween 2016 and 2019, we extracted citations by\nyear, as illustrated in Table 1. There are slightly\nmore than a million papers per year in PubMed,\n100k/year in ArXiv and 3k/year in ACL. The next\n3 subsections use these citations to:\n1. Compute correlations (\u03c1) over time and venue\n2. Compute h-index (h) and impact (\u03bc) for pa-\npers grouped by early citations and venue\n3. Forecast citations with regression\nAll 3 subsections demonstrate that early citations\nare more predictive of future citations than venue."}, {"title": "3.1 Correlations", "content": "The top of Table 2 focuses on 3710 ACL papers\npublished in 2016. The correlation (\u03c1) of 0.80 be-\ntween 2016 and 2017 compares the citation counts\nfor these 3710 papers in 2016 and 2017. The bot-\ntom of Table 2 is similar except for the source of\npapers is now 1,026,798 PubMed papers. Both\nthe top and bottom of Table 2 start with papers\npublished in 2016. The correlation of 0.80 above\nbetween 2016 and 2017 drops slightly to 0.77 for\nPubMed papers.\nTable 3 is like Table 2, but for venues. Venue is\na binary indicator variable containing 1 if the paper\nappears in that venue and 0 otherwise. Figure 1 is\nbased on correlations for ACL papers published in\n2016. Figure 1 (left) is based on Table 2 (top), and\nFigure 1 (right) is based on Table 3 (top).\nIn addition to the main point, there are a number\nof interesting (though smaller) effects:\n1. Main point: Correlations for early returns are\nmuch larger than correlations for venue.\n2. Prestige: Top venues (the ACL main confer-\nence, EMNLP and TACL) have larger correla-\ntions with future citations than workshops.\n3. Forecasting horizon: Because short-term\nforecasting is easier than long-term forecast-\ning, correlations closer to the main diagonal\nof Table 2 are relatively large.\n4. Quantization: Correlations for 2016 are rel-\native\nrelatively small because dates are quantized to\nyears. There are two dates: year of publica-\ntion and year of citation. Citation counts for\nthe year of publication are relatively small\nbecause that is a partial year.\n5. Latency: It takes time for papers to accu-\nmulate citations, and therefore, correlations\nimprove for several years after publication.\nThese observations are robust. Tables 2-3 repli-\ncate the correlations for two types of sources of\npapers. The tables below replicate similar obser-\nvations over two publication dates, using h and \u03bc\ninstead of \u03c1."}, {"title": "3.2 H-index and Impact", "content": "How can we identify papers that will be highly\ncited in the future? The previous section used cor-"}, {"title": "3.3 Forecasting with Regression", "content": "We will use the regression model in Equation 1 to\ncompare early returns and venue.\n```latex\npercentile_{year+4} ~ venue+\\newline factor(pmin(T, citations_{year+1}))\n```\nThis model predicts the percentile of the paper in\nthe fourth year based on the venue and early cita-\ntions. Early citations are treated as a factor variable;\nthus, the model produces a coefficient for each\ncount between 1 and T, as illustrated in Table 6.\nThis model performs a few simple transforms on\nboth the input and output variables:\n1. Percentile transform (Bornmann et al., 2012,\n2014): Predict percentiles instead of raw\ncounts. Percentiles are based on citations in\nfourth year after publication.\n2. Thresholding transform: Since input citation\ncounts have long tails, we use pmin to limit\nthe number of factors in the regression to T."}, {"title": "4 Conclusions", "content": ""}, {"title": "4.1 Early Citations vs. Venue", "content": "We showed that \"early returns\" (citations soon after\npublication) are more predictive of future citations\nthan venue. This conclusion is based on:\n1. subsection 3.1: Correlations (\u03c1)\n2. subsection 3.2: h-index (h) and Impact (\u03bc)\n3. subsection 3.3: Regression\nThese observations suggest a simple actionable\nrule-of-thumb (use early returns) that has advan-\ntages over current practice (reviewing) in terms of\nexclusivity, inclusivity and robustness:\n1. Exclusivity: Simple rule of thumb: for most\nvenues, 1+ early citations are as good as re-\nviews in terms of \u03bc; 20+ early citations are\nbetter than reviews for most (all) venues.\n2. Inclusivity: There are more papers (N) with\n1+ early citations than in most (all) venues.\n3. Robustness: Results were replicated over sev-\neral sources of papers and publication dates."}, {"title": "4.2 DDI Alternative to Reviewing", "content": "A number of challenges for reviewing were men-\ntioned: poorly defined tasks/incentives, validity,\nreliability, subjectivity, biases, time, cost, scale and\ncheating. Given these realities, is peer-reviewing\nworth the effort? Are there faster, cheaper and more\neffective alternatives?\n1. Open Peer-Review (OPR) (List, 2017)\n2. Don't Do It (DDI): Use early citations to re-\nduce the load on peer-reviewing.\nSince OPR \"has neither a standardized definition\nnor an agreed schema of its features and implemen-\ntations,\" Ross-Hellauer (2017), \u201cproposes a prag-\nmatic definition of [OPR] as an umbrella term for...\npeer review models... including making reviewer\nand author identities open, publishing review re-\nports and enabling greater participation....\"\nThe DDI alternative is even more pragmatic and\nconstructive. Instead of reviewing papers, we sug-\ngest the community post papers on ArXiv, and use\nearly returns to help readers, authors and commit-\ntees address questions such as:\n1. Readers: Who should read what?\n2. Authors: Who should cite what for what?\n3. Promotion and Award Committees:\nWho should be recognized for what?"}, {"title": "4.3 New Role for Venues", "content": "What should be the role for venues under this sug-\ngestion? We suggest venues continue to publish\nhigh impact papers in their area that conform to\ntheir methods and practices, but to do so in a way\nthat copes more effectively with scale. As men-\ntioned above, the current system suffers from two\nconcerns: (a) too many submissions and (b) too\nfew qualified reviewers. We suggest introducing\na process upstream of program committees to ad-\ndress both concerns. To reduce the load, program\ncommittees should focus on papers with impressive\nearly citations, as well as papers nominated by a\nprocess described below in section 4.4.\nIn addition to the first concern, reducing the load,\nthese suggestions also help with the second con-\ncern, identifying qualified/motivated reviewers. It\nshould be easier for those who have cited the arti-\ncle to write a review since they have already read\nthe article and most of the background material.\nThey are not only better informed than a random\nreviewer, but they are also probably more sympa-\nthetic to the basic approach.\nThis proposal also simplifies the definition of the\nreviewing task. By the time reviewers see the paper,\nthere is already considerable evidence of impact.\nThe question for reviewers becomes more about\njudging fit than predicting impact."}, {"title": "4.4 Nomination Process", "content": "In addition to early citations, program committees\nshould accept nominations of papers to review from\nthesis advisors and established researchers in in-\ndustrial research laboratories, following precedents\nestablished by nomination processes for awards\nsuch as ACM Doctoral Dissertation. To offset the\nreviewing load on society imposed by the nomi-\nnation process, nominators should agree to review\nfour papers for each paper they nominate. In this\nway, the proposed process addresses both concerns\nraised above: (a) too many submissions and (b) too\nfew qualified/motivated reviewers."}, {"title": "5 Ethics", "content": "The proposed DDI method will not work with\ndouble-blind review, but people who have already\ncited the submission are unlikely to be biased\nagainst the submissions they have cited.\nMutual admiration societies have always existed\nin academia. There is a danger that the proposed\nDDI method will encourage those practices. How-\never, citations leave an audit trail that makes it very\neasy for everyone to see what is happening. As the\ncliche goes, sunlight is the best disinfectant.\nReviewing is a controversial topic. From the\nperspective of a conference organizer, we should\nencourage controversial papers that engage the au-\ndience, and contribute significantly to the field."}, {"title": "6 Limitations", "content": "Citation counts can be gamed. See discussion of\ncheating in subsubsection 2.2.3.\nThis work is largely limited to English since the\nvenues we consider emphasize English.\nThere is a risk that the proposed DDI/nomination\nmethod will help the rich get richer; to compensate\nfor this, there could be a process to encourage nom-\ninations from more diverse places."}]}