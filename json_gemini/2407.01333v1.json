{"title": "Deep Reinforcement Learning for Adverse Garage Scenario Generation", "authors": ["Kai Li"], "abstract": "Autonomous vehicles need to travel over 11 billion miles to ensure their safety. Therefore, the importance of simulation testing before real-world testing is self-evident. In recent years, the release of 3D simulators for autonomous driving, represented by Carla and CarSim, marks the transition of autonomous driving simulation testing environments from simple 2D overhead views to complex 3D models. During simulation testing, experimenters need to build static scenes and dynamic traffic flows, pedestrian flows, and other experimental elements to construct experimental scenarios. When building static scenes in 3D simulators, experimenters often need to manually construct 3D models, set parameters and attributes, which is time-consuming and labor-intensive. This thesis proposes an automated program generation framework. Based on deep reinforcement learning, this framework can generate different 2D ground script codes, on which 3D model files and map model files are built. The generated 3D ground scenes are displayed in the Carla simulator, where experimenters can use this scene for navigation algorithm simulation testing.", "sections": [{"title": "I. INTRODUCTION", "content": "The Self-Driving System, also known as the Autonomous Driving System (ADS), is a comprehensive integration of hardware and software designed to autonomously manage motion control based on its perception and understanding of surrounding environmental conditions, aiming to achieve a level of competence comparable to that of a human driver. Perception, decision-making, and control constitute the three fundamental components of an autonomous driving system.\nAs illustrated in Figure 1, the architecture of an autonomous driving system encompasses the environmental perception system, the positioning and navigation system, the path planning system, the motion control system, and the driver assistance system [1]. The environmental perception system processes data collected from various sensors and transmits it to the central processing unit (CPU). A navigation system, such as GPS, is used to determine the vehicle's location within its environment. The path planning system is responsible for determining the vehicle's travel path, typically utilizing electronic maps. After integrating all the data, the CPU sends control commands to the motion control system, which manages specific vehicle movements, such as acceleration, braking, and steering. Driver assistance system comprises a range of subsystems that aid in completing driving tasks, including automatic parking systems and emergency braking systems.\nIn ADS, the performance of autonomous driving relies on data collected from various types of sensors. This data is processed by algorithms, which ultimately determine the behavior of the autonomous driving system. Consequently, a diverse set of sensor data can enhance the performance of the autonomous driving system.\nHowever, experiments have shown that autonomous vehicles need to travel over 11 billion miles to ensure their safety [2]. In practical use and testing, traffic accidents caused by autonomous vehicles often place the ADS under intense scrutiny. As one of the most critical quality assurance technologies, ADS testing has garnered attention from both academia and industry [3]. Nonetheless, due to the numerous components and high complexity of ADS, testing faces many challenges.\nCurrently, ADS testing methods are gradually shifting from Naturalistic Field Operational Testing (N-FOT) to simulation-based testing [4], also known as simulation testing. Simulation testing involves simulating various vehicle and environmental information within a computer system and verifying whether the autonomous driving algorithms operate as intended. During simulation testing, the autonomous driving system makes decisions based on sensor data collected in the simulated environment, and these decisions are reflected in the behavior of the vehicle controlled within the simulation, thereby achieving an effect similar to real vehicle testing. Compared to real vehicle testing, simulation testing consumes fewer resources and is more efficient in training, making it particularly useful for unit testing of algorithms, which targets specific functions.\nAutonomous driving simulation software provides the nec-essary testing environment for simulation-based tests. In recent years, the release of three-dimensional simulation software for autonomous driving, such as Carla [5] and CarSim [6], marks a shift from two-dimensional bird's eye view (BEV) simulations to more realistic three-dimensional scenarios. During simulation testing, experimenters need to create static scenes and incorporate dynamic elements such as traffic and pedestrians to build a comprehensive experimental scenario. Some of these scenarios are designed to challenge the performance and effectiveness of autonomous driving algorithms and are referred to as adverse scenarios.\nConstructing static scenes in a three-dimensional model often requires experimenters to manually build three-dimensional model prefabs, place them in the scene, and configure their parameters and attributes to test the algorithm's performance under various conditions, which is both time-consuming and labor-intensive [7]. Unfortunately, there is a scarcity of open-source adverse scenario sets for underground parking garages, which poses a challenge for testing autonomous vehicle parking tasks.\nFor Level 5 (L5) autonomous vehicles [8], Automated Valet Parking (AVP) represents the final task in one driving journey. AVP allows passengers to leave the vehicle in a drop-off zone, such as at the entrance of a parking lot, and enables the vehicle to autonomously complete the subsequent parking maneuvers [9]. For most ADS parking tasks, the real-world scenarios are typically underground parking garages (hereinafter referred to as \"garages\"). We aim to develop an automated construction framework to address the time and effort issues associated with manually constructing these scenarios. This framework will generate a corresponding set of three-dimensional adverse underground garage scenarios based on different input parameters and will be compatible with mainstream 3D simulation software. This approach seeks to resolve the current simulation testing issues of having few static scenes and manual scene construction's time-consuming, labor-intensive nature.\nIn this context, this paper proposes a reinforcement learning (RL) [10]-based procedural generation framework, using underground garage scenarios for AVP tasks as a representative example. The framework will 1) automate the construction and generation of simulated adverse underground garage sce-narios in the Carla environment, 2) utilize deep reinforcement learning to create diverse underground garage scenarios, and 3) support customizable parameters and initial maps to alter the generated outcomes."}, {"title": "B. Contributions", "content": "The main contributions of this paper are as follows:\n1. New Automated Method for Generating Autonomous Driving Underground Garage Scenarios: Under the proposed procedural generation framework, we can create diverse static underground garage scenarios. It significantly reduces the time experimenters spend on constructing static scenes. Additionally, these static scenes can be combined with dynamic elements to produce more challenging underground garage scenarios.\n2. High-Quality Encoding Using Reinforcement Learn-ing: The paper employs advanced reinforcement learning methods, designing environments, and reward functions for training to obtain higher-quality encodings. Since the reward functions are based on real-world rules [11], we can extract relatively optimal scenarios from the generated data. These scenarios make efficient use of space and closely resemble the distribution of roads and parking spaces in real underground garages, making them more applicable to real-world automated parking situations encountered by autonomous vehicles.\n3. Generation of Diverse Adverse Scenarios: The encodings ultimately yield a variety of map models, forming a comprehensive map scenario library. We have designed rule-based validation metrics to differentiate the difficulty levels of various underground garages. These adverse scenarios of varying difficulty levels reduce the workload of experimenters in designing 3D scenes and provide more randomized test data, thereby enhancing the test coverage of algorithms."}, {"title": "C. Outline", "content": "This paper outlines the related work referenced or cited in the design of the procedural generation framework. Chapter 2 introduces the current state of research on static scene generation in simulation testing, the basic concepts of proce-dural generation, relevant methods in reinforcement learning, and the 3D autonomous driving simulation software Carla, including the file formats related to map generation. Chapter 3 elaborates on the system composition and core content of the framework: static underground garage scene genera-tion based on deep reinforcement learning, divided into four parts: mathematical model construction, environment design, reward function design, and 3D map generation. Chapter 4 describes the experimental design for the deep reinforce-ment learning component of the framework, introduces two evaluation metrics-coverage and difficulty-and tests the planning algorithm, one of the core AVP algorithms, in Carla to verify the reliability of the difficulty metric for underground garages. Chapter 5 presents the experimental results designed in Chapter 4 and discusses these results. Chapter 6 summarizes the entire work and proposes future research directions."}, {"title": "II. RELATED WORK", "content": "The generation of simulation testing scenes for autonomous vehicles is an area still under exploration. After an extensive review of related literature, this paper focuses more on pro-cedural model generation, which is commonly found in the construction of game elements within certain video games. Procedural Content Generation (PCG) refers to the creation of game content using algorithms with limited or indirect user input [12]. This paper primarily discusses a major research direction of PCG-generating different levels and scenes. PCG is frequently used in games, such as in the game \"Minecraft,\" where PCG algorithms generate the entire block world and its contents. In the context of this paper, the game content is the 3D model of the underground garage, and the user inputs are the dimensions, shape, and other elements of the garage. Therefore, the problem of underground garage generation in this paper can be seen as a PCG problem within the transportation domain.\nThe research problem addressed in this paper is the au-tomated creation of simulation testing scenes. Autonomous driving simulation testing scenes include static and dynamic scenes. Static scenes comprise a series of objects, such as road networks and static obstacles, that do not change positions during the simulation. On the other hand, dynamic scenes include a series of movable vehicles and pedestrians forming traffic flows. This paper primarily focuses on the generation of static scenes for testing. The core content is the generation of the road network for the generation of static underground garage scenes.\nRule-Based Road Network Generation: Parish and Muller [13] extended the L-system [14]\u2014a system used to simulate the morphology of various organisms\u2014to generate the road network in the CityEngine 1 urban generation system. Chen et al. [15] proposed a procedural road network content generation method guided by underlying tensor fields, enabling intuitive user interaction.\nSearch and Evolutionary Algorithm-Based Road Net-work Generation: Ishhan Paranjape et al. developed the system CruzWay [16] to support the creation of test scenarios. CruzWay uses TownSim [17], an agent-based urban evolution algorithm, to automate road network generation. The afore-mentioned works based on rules, search, and evolutionary algorithms [18], [19], [20] use urban semantic elements, such as elevation maps, water resource maps, and population density maps, as input to the program [21]. These methods can ultimately generate large-scale urban transportation networks. However, the road network in this paper is smaller in scale compared to urban networks and has more restrictive rules, such as the absence of overly wide lanes and the presence of numerous orthogonal intersections. Alessio Gambi et al. used a combination of genetic algorithms and procedural con-tent generation techniques to automatically create challenging simulation road networks for testing lane-keeping algorithms, resulting in more instances of lane departure compared to random testing [22]. This work shares the same goal as this paper: providing test scenarios for autonomous driving system algorithms.\nDeep Learning-Based Road Network Generation: Chu et al. proposed Neural Turtle Graphics (NTG), a method using neural networks to generate road networks represented by graphics. Owaki et al. introduced a new network model, RoadNetGAN [23], an extension of NetGAN [24], which generates urban road networks by predicting road connections. These supervised learning methods use datasets for training and can generate road network structures similar to those in the datasets. The quality of these methods is determined by comparing the generated networks with those in the datasets. However, research focusing on underground garages as test scenarios is lacking, and there are insufficient datasets for evaluating the generated garbage results. This paper employs deep reinforcement learning for road network generation. This method does not rely on any dataset, which we believe provides higher coverage and randomness. Additionally, it can adapt to complex constraint rules and performs best in small to medium-sized scenarios."}, {"title": "B. Reinforcement Learning", "content": "Reinforcement Learning (RL) is a branch of machine learn-ing that focuses on how agents take different actions in an environment to maximize rewards. RL comprises agents, en-vironments, states, actions, and rewards. The agent is respon-sible for training and decision-making, while the environment includes state sets, action sets, and a reward function. States represent the data set in the environment, and the reward function specifies the numerical rewards the environment provides to the agent during the interaction. In an episode, the environment and the agent interact and progress together. The interaction between the agent and the environment can be viewed as a finite Markov Decision Process (finite MDP), an efficient model for solving linear decision problems.\nIn this paper, finite MDPs for the reinforcement learning process are defined as follows: finite MDPs can be represented as a quadruple $M = \\{S, A, P, R\\}$, where S is the finite state set, A is the finite action set, $P:S\\times R\\times S \\times A \\rightarrow [0,1]$ is the probability transition function, and $R : S\\timesA\\rightarrowR$ is the reward function. Here, we define the probability of transitioning from state s to the next state s' given action a as $P(s', r\\mid s, a)$, and the immediate reward obtained from this transition as $r_a$ [25].\nWe define a policy $\\pi$, with the mapping represented as $\\pi : S \\times A \\rightarrow [0, 1]$. We use $\\pi(s)$ to denote the action a in state s under policy $\\pi$. To evaluate the quality of a policy, we use the action-value function $q_{\\pi}(s, a)$ to estimate the expected long-term cumulative reward of taking action a in state s under policy $\\pi$. It is formally defined as:\n$q_{\\pi}(s, a) = E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}|S_t = s, A_t = a]$\t(1)\nwhere $\\gamma$ is the discount factor, $R_t$ is the reward at time step t, and $E_{\\pi}$ denotes the expectation with respect to policy $\\pi$.\nThe goal of reinforcement learning is to find an optimal policy $\\pi^*$ that maximizes the expected long-term discounted cumulative reward starting from any initial state $s \\in S$:\n$\\pi^* = argmax_{\\pi} E_{\\pi}[\\sum_{t=0}^{\\infty} R_t |S_0 = s]$\t(2)\nThe interaction process between the agent and the envi-ronment is illustrated in Figure 3. Initially, the environment provides the agent with the current state and reward. Based on the state information, the agent decides on an action, and upon receiving the action, the environment provides a new state and reward for the next cycle. This process continues until reaching a final state.\nIn each episode, the agent and the environment generate a finite Markov sequence:\n$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ..., S_{T-1}, A_{T-1}, R_T$\t(3)"}, {"title": "C. 3D Autonomous Driving Simulation Software", "content": "The Carla Simulator [5] (hereinafter referred to as Carla) is an open-source simulation simulator developed by Intel Labs, Toyota Research Institute, and the Computer Vision Center, based on the Unreal Engine (UE). Carla is written in C++ and supports C++ and Python APIs. Compared to other commercial simulation software, Carla supports convenient custom map importing. Carla can be compiled and built directly from the source code, and supports map generation using files in Film Box and OpenDrive formats. Therefore, this paper selects Carla as the 3D autonomous driving simulation software for map integration."}, {"title": "D. Map Formats", "content": "FilmBox (FBX) 2 is a universal 3D model file format developed by Kaydara. Acquired by Autodesk in 2006, it has since become a primary exchange format for many 3D modeling tools. In Carla, FBX files are used to construct static scenes and provide volumes for collision detection. This paper uses the FBX Software Development Kit to generate FBX files.\nASAM OpenDrive (OpenDrive) 3 uses Extensible Markup Language (XML) to describe road networks. OpenDrive can describe the geometry of roads, lanes, and related objects such as road signs and traffic lights. In the OpenDrive format, a complete road consists of many segments. Each road segment is represented by a <road>element, and the road reference line determines the centerline of the road in space, typically represented as a straight line, curve, or spiral. Intersections are represented by <junction>elements and can describe areas where three or more roads intersect. Intersections are defined as connecting three or more roads. In Carla, OpenDrive pro-vides a list of waypoints, which can be used for path planning for autonomous vehicles. During point-to-point cruising tasks, vehicles can navigate by finding the closest path containing a series of waypoints. This paper uses the scenario generation Python library to generate OpenDrive files."}, {"title": "III. METHODOLOGY", "content": "To better address the issue of generating underground park-ing garage scenes, we outline the following requirements for the generated static scenes:\n1) The parking garage should be a simple single-level structure without interior walls.\n2) Only double-lane road segments in horizontal and vertical directions should exist within the parking garage.\n3) There should be only one entrance and one exit for the parking garage, and they should be connected by the road network.\n4) All parking spaces should be vertical parking spaces.\n5) Following the \"Garage Architectural Design Code\", verti-cal parking spaces should appear in groups of three, four, and six in the parking garage, as illustrated in Figure 4.\nThe generation of adverse scenarios in underground parking lots is divided into several steps as follows:\n\u2022 Firstly, a two-dimensional grid map is generated to rep-resent the planar structure of the parking lot. Each grid represents a minimum separable block within the parking lot, such as roadblocks and parking space blocks. The grid map is encoded using a matrix.\n\u2022 Subsequently, each block of the encoding matrix is mapped to a three-dimensional scene and additional static obstacles, such as pillars and barriers, are incor-porated. This step generates files representing the three-dimensional models.\n\u2022 Finally, the roadblocks of the encoding matrix are trans-formed into road network format information. This step produces files representing the road network."}, {"title": "A. Mathematical Model Construction", "content": "We classify the minimum separable blocks in the construc-tion of the underground garage. The categories are presented in Table I. Each number represents a block type. Thus, we can utilize any of these encoding matrices to represent the planar structure of an underground garage. Among these, adjacent roadblocks are interconnected, and we aim for accessibility from any roadblock to any other. Entrances and exits are linked to roadblocks, ensuring travel from an entrance to an exit. There are five types of parking spaces: three-stall, four-stall, six-stall, obstructed three-stall (obst-three-stall), and obstructed four-stall (obst-four-stall). The obstructed three-stall and obstructed four-stall blocks are necessarily connected to obstructed blocks. This handling ensures that parking spaces are located along the sides of roads to accommodate vehicles.\nThis study employs the deep reinforcement learning ap-proach to generate the encoding matrix, necessitating the definition of various metrics within the reinforcement learning framework for this problem. To this end, we transform the encoding matrix generation problem into a dynamic problem solvable by reinforcement learning. We assume the presence of a car, which starts from the entrance block of the garage and can move to adjacent blocks in each step. During its progression, the car \"colors\" the passed blocks as roadblocks and colors the adjacent blocks of the colored roadblocks. The coloring rules for individual blocks b are governed by a state machine, as illustrated in Figure 7. Blocks are initially transitioned based on the input map into free blocks, obstacle blocks, entrance blocks, or exit blocks. While entrance and exit blocks remain unchanged, the other two types of blocks undergo different transitions. These transitions are primarily classified into two categories: coloring and changes in the states of adjacent blocks. When the car is at a particular block, a coloring action occurs. We define the function $Adj(b)$ to represent the number of roadblocks among the four adjacent blocks of block b, with the formula defined as:\n$Adj(b) = |\\{b'|type_{b'} = ROAD\\ BLOCK,b' \\in \\{b_{left}, b_{top}, b_{right}, b_{down}\\}\\}|$\t(6)\nwhere $b_{left}, b_{top}, b_{right}$, and $b_{down}$ represent the blocks to the left, top, right, and bottom of block b, respectively. Additionally, we define $Sym(b)$ as an indicator of whether the adjacent block set of block b contains symmetric roadblocks. Specifically, when both $b_{left}$ and $b_{right}$ are roadblocks, or when both $b_{top}$ and $b_{down}$ are roadblocks, $Sym(b)$ is expressed as 1; otherwise, it is 0.\nIt is noteworthy that this study introduces specific rules for the blocks with six parking spaces. All six parking spaces in a single block must face the same direction, either north-south or east-west. This direction is determined by the environment at the beginning of each episode. Eventually, we obtain an encoding matrix colored by the car's movements. This problem can be addressed using reinforcement learning techniques. The transformed problem is defined as follows: the car is situated in a parking garage environment, initially containing only one entrance and one exit (non-overlapping). The agent can control the car to move to adjacent blocks and color them, aiming to generate a parking garage that meets the constraints as much as possible while remaining usable. We need to design the state space, action space, and reward function for the environment, which will be detailed in the next subsection. After each episode, reinforcement learning will produce a complete encoding matrix for the parking garage. The collection of encoding matrices generated across all episodes constitutes the output of this reinforcement learning iteration."}, {"title": "B. Environment Design", "content": "The core of environment design lies in defining the state space S and action space A. The environment in this paper primarily consists of an encoding matrix and data representing the car. The encoding matrix indicates the range of movements available to the car in the environment, while the car data stores the position and orientation of the car within the encoding matrix. The actions of the car are defined as moving left, right, up, or down, and the action set A is defined as follows:\n$A = \\{Left, Right, Up, Down\\}$\t(7)\nAt each step, the car can move to an adjacent block. The environment in this paper is designed as a \"partially observable environment.\" In this environment, observations are used instead of the states mentioned in the MDP framework. This means that the agent receives observations from the environment rather than states. In a partially observable envi-ronment, observations primarily consist of a visibility matrix B, error index e, coverage rate $\\varphi$, connectivity index $\\theta$, and car orientation d. In this environment, we assume that the agent cannot access the entire structure of the encoding matrix or the position of the controlled car to make decisions. Instead, the agent processes features extracted from the encoding matrix to make decisions. These features include the visibility matrix, error index, coverage rate, and connectivity index. The visibility matrix refers to a sub-matrix of the encoding matrix with the same width and height, centered around the car. It simulates the observable range of a human driver in a parking garage. The error index indicates the number of times the agent violates constraints. If the error index exceeds the maximum allowable value, the current episode ends directly. The coverage rate represents the percentage of colored blocks in the garage relative to the initial number of uncolored blocks. It evaluates the extent to which the agent has colored the garage. The connectivity index is a Boolean value indicating whether the entrance and exit are connected. The garage is considered usable only when the car has reached the exit, indicating that the starting point and the exit are connected.\nThe observation space in the constrained environment is smaller than the state space, which speeds up training but may lead to the loss of some structural information about the garage."}, {"title": "C. Reward Function Design", "content": "The design of the reward function is a crucial aspect of reinforcement learning, as it often determines the quality of training outcomes. In this study, the reward function is designed with reference to national parking garage design standards to closely approximate real-world garage design rules. Positive rewards indicate gains, while negative rewards imply losses. Larger rewards encourage actions, while smaller rewards discourage actions. The reward function is based on the rules of the environment, which are categorized into constraint rules and utility rules.\nConstraint rules primarily restrict the agent's movement while ensuring connectivity between the entrance and exit roads of the garage. Utility rules focus on the user experience and efficiency of garage usage, aiming to maximize the number of parking spaces, increase the percentage of parking area in the garage, reduce the number of intersections, and improve traffic efficiency. Additionally, utility rules contribute to evaluating the quality of a garage, playing a crucial role in subsequent garage validation.\nThe reward function R(t) is defined as follows:\n$R(t) = k_cR_c(t) + k_uR_u(t)$\t(8)\nwhere $R_c$ represents the reward function for constraint rules, $R_u$ represents the reward function for utility rules, and $k_c$ and $k_u$ are parameters ranging from 0 to 1. The constraint rules mainly consist of road network constraints, obstacle collision constraints, entrance-exit connectivity constraints, and backward movement constraints."}, {"title": "D. 3D Maps Generation", "content": "This section will explain how to convert the encoding matrix into a road network matrix and generate FBX and OpenDrive files. These files constitute the 3D map, which will be imported into Carla to ultimately generate the 3D garage.\nWe stipulate that each element in the encoding matrix has a length and width of 9 meters.\nWe convert the encoding matrix into a road network matrix. Each element in the road network matrix contains the current block type, coordinates, and orientation angle. The block type corresponds to our pre-built 3D models, the coordinates indicate the position of the pre-built 3D model in 3D space, and the orientation angle indicates how much the pre-built 3D model should be rotated. Here, we divide the roadblocks into four types: straight road, turning road, T-intersection, and crossroad, as shown in the figure below.\nOnce we obtain the road network matrix, we can corre-spondingly use pre-built block model files to combine and gen-erate the entire three-dimensional floor model of the garage.\nWe extract all road information from the road net-work matrix to generate the OpenDrive file. Here, we cat-egorize straight roads and turning roads into one cate-gory, defined as <road>in the OpenDrive format, while T-junction road and crossroad are grouped into another cat-egory, defined as <junction>in the OpenDrive format. For two adjacent <road>segments, we directly connect them. For <junction>segments, we connect them to the adja-cent <road>segments. There might be cases where two T-intersections are adjacent. In such cases, we add an implicit road segment between the two intersections to connect them."}, {"title": "IV. EXPERIMENTS", "content": "We set up the environment for our experiment as blow is custom-built using stable-baselines [32], and the algorithm employs the DQN algorithm encoded in stable-baselines. FBX files are generated using the FBX SDK provided by Autodesk, while OpenDrive files are generated using the Python third-party library scenario generation. All initial data for the parking lots are manually authored."}, {"title": "B. Assessment Metrics", "content": "Carla provides the shortest feasible topological path P from any starting point to the destination. Path P consists of a series of interconnected waypoints. We define the drivable path in the parking lot as the shortest path from the entrance to any parking space along the roads. We establish two metrics to evaluate the diversity of generated parking lots: coverage $\\delta$ and difficulty $\\lambda$. Coverage assesses the richness of drivable paths produced by the parking lot generation. Difficulty evaluates the complexity of drivable paths generated by the parking lot. Both coverage and difficulty are directly computed from the encod-ing matrix. For computational convenience, we approximate the coverage of a parking lot as a function of the remaining vacant blocks within the parking lot. The difficulty of a parking lot is approximated as a function of the expected road length and intersection count. This approximation corresponds to the utility rules defined in Section 3 regarding the reward function design.\nWe define the coverage $\\delta$ of a parking lot G as:\n$\\delta = 1 - \\frac{|\\{p|type_p = FREE\\ BLOCK,p \\in S_G\\}|}{|\\{p|type_p = FREE\\ BLOCK, p \\in S_{G'}\\}|}$\t(12)\nwhere $S_G$ represents the set of blocks in parking lot G, and $G'$ denotes the initial structure of parking lot G.\nLet X be the random variable representing road length, with minimum value $l_{min} = 2$ and maximum value $l_{max} = 6$. Similarly, let Y be the random variable representing the number of road intersections, with minimum value $c_{min} = 2$ and maximum value $c_{max} = 4$. Normalizing the expectations, we obtain:\n$\\eta_1 = \\frac{E(X) - l_{min}}{l_{max} - l_{min}}$\t(13)\n$\\eta_2 = \\frac{E(Y) - c_{min}}{c_{max} - c_{min}}$\t(14)\nBased on experience and utility rules, we consider longer road lengths and a higher number of intersections as simpler. Therefore, we define the difficulty $\\lambda$ of a parking lot G as:\n$\\lambda = w_1(1 - \\eta_1) + w_2(1 - \\eta_2)$\t(15)\nwhere $w_1$ and $w_2$ are parameters, with $w_1 = 0.33$ and $w_2 = 0.67$."}, {"title": "C. RL Experiments", "content": "RL Experiments focus on generating encoding scripts using the DQN algorithm with five maps of different sizes. As shown in Figure 11, the first three maps are relatively regular, with rectangular vacant blocks in the center and obstacles at the edges, except for the entrance and exit. Map from Figure 11a is smaller, while the others are larger. Maps from Figure 11b and Figure 11c have different exit positions. Since we encourage the creation of parking lots with multiple straight roads in reinforcement learning, the map from Figure 11c is expected to be easier to generate usable parking lots. The relatively challenging maps are from Figure 11d and Figure 11e, where obstacles obstruct the path to the destination, making it more difficult to find a viable route. We trained each map five times, with a total of 2,000,000 timestamps per training run (timestamps represent the interactions between the agent and the environment), and plotted the convergence curves of their rewards."}, {"title": "D. Garage Difficulty Verification Experiments", "content": "We designed an experiment to test one of the core algo-rithms of the AVP, the cruise algorithm. For this purpose, we used the garages generated in the previous experiment in Carla for automated driving cruise testing. We set the starting point and the finishing point in the garage, and the autonomous driving vehicle needs to depart from the starting point and finally reach the endpoint. The vehicle may encounter the following situations during the journey: collision with parked vehicles, collision with walls, and algorithmic exceptions leading to deadlock. These situations will result in task failure.\nTo simulate the parking task of the AVP in a real parking lot, we set the starting point as the entrance of the garage and the endpoint as the road edge closest to any parking space. This setting is consistent with the feasible driving path mentioned earlier. The vehicle has three failure cases: collision, timeout, and deadlock. Collision mainly occurs when the vehicle collides with the boundary walls and pillars, as shown in Figure 12a; Timeout occurs when the vehicle fails to reach the endpoint within the predetermined time. Deadlock refers to algorithmic problems, such as deviation from the predetermined trajectory, and the vehicle cannot reach the specified position, as shown in Figure 12b.\nWe selected the cruise algorithm based on the deep rein-forcement learning model 4 to test. The algorithm can obtain the nearest undriven point in the drivable path and control the vehicle to drive to that point. In addition, the algorithm can perceive the image data captured by the front camera of the vehicle and process it into a depth map and image segmentation map [33]. After processing by the model, it can output corresponding driving behaviors: accelerate, turn left, turn right, and brake. We plan to use the pre-trained model to test on 50 different difficulty garages generated from garage Figure 11c. We will statistically measure the success rate of testing on each garage and verify and analyze the correlation between the success rate of testing and the difficulty of the garage."}, {"title": "V. RESULTS", "content": "The convergence curves of reward for the reinforcement learning experiment on generating encoding are shown in Figure 13. The x-axis represents the number of episodes, and the y-axis represents the return. Due to the different sizes and initial states of the garages, the number of episodes for each map is different under the same number of time steps.\nIt can be observed that the training effects of the 11 \u00d7 7 garage from Figure 13a and the 13 \u00d7 13 garage with corre-sponding entrance and exit from Figure 13c are the best, with the average return values exceeding 0, and the average return manages to arrive on time in most cases. It is noteworthy that more than half of the data have a success rate of over 80%, indicating that the model can handle most of the sampled garages effectively.\nThe linear regression fitting results for Table III are as fol-lows. The calculated correlation coefficient between difficulty and success rate is -0.638, suggesting a significant negative linear relationship between the two."}, {"title": "VI. DISCUSSION", "content": "We observed that the curves in Figure 13b", "result": "first, insufficient data; as the number of data sam-ples increases, there are fewer deviating data samples, which better reflect the correlation between the two coefficients. Second, the formula and definition of difficulty are inadequate; while difficulty is defined to correspond to the utility rules of"}]}