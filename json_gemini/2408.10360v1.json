{"title": "HASPER: An Image Repository for Hand Shadow Puppet Recognition", "authors": ["Syed Rifat Raiyan", "Zibran Zarif Amio", "Sabbir Ahmed"], "abstract": "Abstract-Hand shadow puppetry, also known as shadowgra- phy or ombromanie, is a form of theatrical art and storytelling where hand shadows are projected onto flat surfaces to create illusions of living creatures. The skilled performers create these silhouettes by hand positioning, finger movements, and dexterous gestures to resemble shadows of animals and objects. Due to the lack of practitioners and a seismic shift in people's entertainment standards, this art form is on the verge of extinction. To facilitate its preservation and proliferate it to a wider audience, we introduce HASPER, a novel dataset consisting of 8,340 images of hand shadow puppets across 11 classes extracted from both professional and amateur hand shadow puppeteer clips. We provide a detailed statistical analysis of the dataset and employ a range of pretrained image classification models to establish baselines. Our findings show a substantial performance superiority of traditional convolutional models over attention- based transformer architectures. We also find that lightweight models, such as MOBILENETV2, suited for mobile applications and embedded devices, perform comparatively well. We surmise that such low-latency architectures can be useful in developing ombromanie teaching tools, and we create a prototype application to explore this surmission. Keeping the best-performing model INCEPTIONV3 under the limelight, we conduct comprehensive feature-spatial, explainability, and error analyses to gain insights into its decision-making process. To the best of our knowledge, this is the first documented dataset and research endeavor to preserve this dying art for future generations, with computer vision approaches. Our code and data are publicly available.\nImpact Statement-This research is an impetus towards utiliz- ing AI tools to revitalize the hitherto underexplored cinematic art form of hand shadow puppetry. Such tools may help un- derstand the creativity frontier in generative models, facilitate the development of applications to teach shadowgraphy, and unveil several prospects for entertainment. The existing works, though distally relevant to shadowgraphy, explore the digitization of such precursory art forms via approaches that have since been rendered primitive and obsolete. The novel dataset that we introduce in this paper, namely HASPER, consists of 8,340 diverse samples garnered from performance clips of variably skilled puppeteers. Our extensive benchmarking reveals that the task of classifying the puppet silhouettes is reasonably solvable", "sections": [{"title": "I. INTRODUCTION", "content": "Ombromanie, the ancient art of hand shadow puppetry, is a form of art that involves the mesmerizing interplay of light and shadow through the construction and manipulation of shadow figures or silhouettes on a surface, typically a screen or a wall, using one's hands, body, or props [1]. The alias \"cinema in silhouette\"\u00b9 is sometimes used to refer to this proto-cinematic medium of entertainment. Its working principle is very simple-the puppeteer adeptly positions their hands between a radiant light source and a translucent screen, consequently conjuring shadows and silhouettes that emulate different creatures [2] as shown in Figure 1. Despite its rich history and captivating allure across many cultures\u00b3, there"}, {"title": "II. LITERATURE REVIEW", "content": "The recognition and classification of hand shadow puppet images are intriguing problemspaces in the context of deep learning, albeit relatively underexplored. After rigorously an- alyzing the existing pool of research on the topic, we could identify several quasi-related works."}, {"title": "A. Image Classification and Recognition", "content": "Among the pioneering endeavors in hand shadow image classification was that of Huang et al. [7], who created SHADOW VISION-a system to emulate an immersive virtual shadow puppet theater experience, employing a user's hand gestures over an overhead projector to control the creation and manipulation of objects within a 3D Open Inventor\u2074 envi- ronment. The chain of stages underlying the implementation of SHADOW VISION were acquisition, segmentation, feature extraction, and recognition of the infrared shadow puppet images. They also adopted a 3-layer neural network and the centralized contour moments modeling technique, using 13 features (7 moments of the object, length, angle, and the 4 endpoints of the axis of inertia). The data used for this study isn't publicly available, and the methodology can be deemed somewhat obsolete in the modern purview, due to being supplanted by the emergence of deep learning models. Some recent works explore different convolutional models to assess their efficacy in Indonesian shadow puppet recogni- tion. Sudiatmika et al. [8], Sudiatmika and Dewi [9] used the deep CNN models, ALEXNET [10] and VGG-16 [11], and constructed a dataset of 2,530 images spanning 6 classes of puppets from museums in Bali. They also experimented with other convolutional models, such as MASK R-CNN [12] and MOBILENET [13], in two separate studies [14, 15].\nIn a similar spirit, our work is an endeavor towards estab- lishing a performance benchmark of the recent SOTA feature extractor models for hand shadow puppet contour images, in a more large-scale and comprehensive manner."}, {"title": "B. 3D Modeling and Human Motion Capture", "content": "One of the earliest works involving silhouettes is a study by Brand [16] that explored the mapping of monocular monochro- matic 2D shadow image sequences of humans to animated 3D body poses, using a configural and dynamical manifold created from data with a topologically special hidden Markov model (HMM), acquired via the process of entropy minimization without resorting to any articulatory body model. Several advances in vision-based human motion capture and analysis since then have leveraged human silhouette templates [17, 18], more specifically, hand and finger silhouettes [19, 20, 21]."}, {"title": "C. Robotics", "content": "Huang et al. [22] introduced computer vision-aided shadow puppetry with robotics by matching shape correspondences of input images. They claimed that due to the physical limitations of human arms, it is often not feasible to construct complex shadow forms. Instead, they developed a framework that enabled them to produce shadow images with the mechanical arms of a robot. The authors built a library of shadow images and used them to orient the robotic arms into a formation resembling the intended shadow puppet. The data used for this study isn't publicly available."}, {"title": "D. Human-Computer Interaction", "content": "The authors of [23] proposed a framework for controlling two Chinese shadow puppets a human model and an animal model, with the use of body gestures via a Microsoft Kinect sensor. Carr and Brown [24] conducted a similar work by building a real-time Indonesian shadow puppet storytelling application that is capable of mimicking the full-body actions of the user, using the Microsoft Kinect sensor. In order to"}, {"title": "III. DATASET CONSTRUCTION", "content": "The series of steps involved in our data acquisition process is broadly divided into three tasks (a) procuring the perfor- mance clips, (b) extraction of the frames, and (c) categorization of each sample frame with a proper label. Figure 2 portrays this workflow behind our dataset preparation. We incorporate manual oversight at each step of the dataset creation in order to reconcile any exigencies pertaining to the quality of HASPER."}, {"title": "A. Collating Shadowgraphy Clips", "content": "At the outset of the process, we procure 45 different clips of 9 different professional shadowgraphists from YouTube\u2075. The video sources are licensed under fair use and a list consisting of the links to all of them is available in our GitHub"}, {"title": "B. Extracting Samples", "content": "To mitigate the presence of excessively similar and redun- dant image samples, we extract frames from these clips at reasonable intervals of $k$ after downsampling the clips to a resolution of 1430 \u00d7 1080. The values of $k$ are judiciously chosen for the clips of each class, and every kth frame is selected as a candidate image sample (e.g., with $k$ \u2248 180, 200, 220 for a 60 FPS clip). Table I encapsulates some essential"}, {"title": "C. Labeling", "content": "After the extraction of the frames, the samples undergo manual scrutiny by 3 annotators who are pursuing undergrad- uate studies in Computer Science and Engineering (CSE). If a series of contiguous samples prima facie exhibit substantial similarity, we only keep a single image from that set of samples. The rest are discarded to avoid redundancy and to instill diversity. Another criterion that dictates the legitimacy of an image sample is its intelligibility. If the majority of the annotators agree on the unintelligibility of a sample, they discard it in unison. After performing this omission of unsuitable samples for each class, we end up with 11 different directories of images, each containing the curated samples of a particular class. The images in these folders are then further partitioned into training and validation sets, maintaining a 55:45 split approximately. We also pragmatically incorporate a proper distribution of the samples sourced from amateur clips over both the training and validation sets, to avoid making the latter unfairly difficult for the classification models."}, {"title": "IV. DATASET DESCRIPTION", "content": "To provide a tangible exposition of the diverse samples in the dataset, Figure 3 presents a collection of representative images across all the 11 classes. With minimally astute per- spicacity, we can observe that the samples vary in terms of the nature of the backgrounds, the anatomical structure of the puppeteers' hands, the photometric opacity and sharpness of the projected silhouettes, and a panoply of other aspects."}, {"title": "A. Background Variance", "content": "The hand shadow puppetry setup that a puppeteer's crew arranges before the performance greatly dictates the nature of the background on which the shadow puppets are displayed. If the location of the light source is very near to the wall or the translucent screen, then we can observe an elliptical shadow contour on the background as evident in Figures 3a and 3k. The angular directionality of the light also manifests a gradient effect on the background as can be seen in Figures 3d and 3h. The temperature and color of the light emanated by the light sources onto the screens also add to the diversity."}, {"title": "B. Nature of the Silhouettes", "content": "The positioning of the light source with respect to the puppeteer's hands plays a role in shaping the shadows' quality. Proximity to the light source yields crisp, well-defined shad- ows (e.g., Figure 4a), while increasing the distance fosters softer, more diffuse shadows (e.g., Figure 4b) with a central umbra and peripheral penumbra. The higher the contrast between the silhouettes and their respective backdrops, the more visible and well-contoured the shadow puppets are. The direction of the light source influences the orientation and shape of the shadows. Shadows cast by overhead lighting sources may appear elongated, while shadows cast by low- angle lighting sources may exhibit softer edges and less pronounced contrast and sharpness. The shadows also differ in terms of the magnitude of their opacity, i.e., the degree to which the hands prevent the transmission of light being projected onto the screen."}, {"title": "C. Hand Anatomy and Stylistic Flair of the Puppeteers", "content": "The physiological properties of the puppeteers' hands can vary significantly due to a combination of genetic factors, environmental influences, and lifestyle choices. These nuanced"}, {"title": "D. Comparative Analysis", "content": ""}, {"title": "1) Inter-class Similarity", "content": "Due to the conspicuous resemblance in the anatomical structures of certain animal species, the samples belonging to the classes corresponding to those animals exhibit a notable degree of similarity as well. Figures 3e and 3h are prime examples of such structural similitude that can be observed between the 'Deer' and 'Moose' classes. These similarities make the image classification task on HASPER quite chal- lenging and culminate to being the reason behind a lot of misclassifications, as discussed in Section V-C."}, {"title": "2) Intra-class Dissimilarity", "content": "Some classes include samples of multiple species of the same animal, and these samples are starkly different in appearance from one another. Given the presence of such quasi-disparate samples, along with the individualistic flair that manifests through the puppeteers' stylistic choices, a particular class may show a lot of intra-class dissimilarity. As aforementioned, Figure 5 portrays the heterogeneity of this nature among the samples from the 'Deer' class."}, {"title": "E. Statistical Analysis", "content": "Table I presents the statistical properties of the HASPER dataset. It tabulates the proportion of samples belonging to each of the 11 classes and their corresponding training- validation splits. The 'Dog' class has the highest number of images (831 samples \u2248 9.96%), while on the contrary,"}, {"title": "V. DEVELOPING BENCHMARK FOR HASPER", "content": "A series of pretrained models are used as feature extractors to develop a benchmark for the dataset. The models are pretrained on the IMAGENET [51] dataset and fine-tuned on HASPER. We implement the training pipeline using the Pytorch framework. This section presents an overview of the models, evaluation metrics, and experimental results."}, {"title": "A. Experimental Setup", "content": ""}, {"title": "1) Baseline Models", "content": "For this classification task, we use 31 feature extractor models as baselines, which are listed in Table II. Some of these models have a track record of good performance across various other image classification tasks [52]. We examine both con- ventional Convolutional Neural Networks (CNNs) and CNNs augmented with attention mechanisms. Some models have"}, {"title": "2) Performance Metrics", "content": "We use top-k validation accuracy values (with k = 1, 2, 3), Precision, Recall, and F1-score as evaluation metrics to per- form comparative analyses among the aforementioned models. The latter three judgment criteria are used due to the slightly imbalanced nature of HASPER, as evident in Table I."}, {"title": "3) Classifier Network", "content": "We adopt two approaches to arrive at the final 11- dimensional layer since there are a total of 11 classes to predict from. The first approach is to directly append an 11- dimensional fully connected layer at the tail-end of the vanilla models. The second approach incorporates the classifier block portrayed in Figure 6."}, {"title": "4) Hyperparameters and Optimizer", "content": "We use Stochastic Gradient Descent (SGD) [53], with a learning rate a = 0.001 and momentum y = 0.9, as the optimizing method, and Cross Entropy Loss as the loss metric for all the models. To decay the learning rate, we use Step Scheduler, which decays a by 0.1 every 5 epochs. Each model undergoes training for 50 epochs to ensure equitable compari-"}, {"title": "5) Data Augmentation and Preprocessing", "content": "In order to generate a more diverse pool of training samples, we also incorporate data transformation techniques-Random Resize, Random Perspective, Color Jitter, Random Horizontal Flip, Random Crop, Random Rotation, Gaussian Blur, and Random Affine with translation and shearing-while training the models. We choose these data augmentation techniques since the classes in HASPER are mostly rotationally asymmet- ric and incongruent. Consequently, the augmented samples aid in eliciting better generalization abilities and robustness for all the models. The input images that are fed to the models are appropriately resized a priori using Bicubic Interpolation."}, {"title": "B. Results and Findings", "content": ""}, {"title": "1) Performance Analysis", "content": "The INCEPTIONV3 model yielded the best performance with a top-1 accuracy of 88.97%. The vanilla version of the model also yields reasonably high Precision, Recall,"}, {"title": "2) Feature Space Visualization and Analysis", "content": "In order to visualize the learned feature space of INCEP- TIONV3, we resort to the dimensionality reduction technique"}, {"title": "3) Qualitative Analysis and Explainability", "content": "As depicted in Figure 9, we adopt a plethora of Explainable AI (XAI) techniques for the best-performing IncePTIONV3 model to understand its decision-making. While viewing the GradCAM (Gradient-weighted Class Activation Mapping) [55] attention heatmaps, it becomes apparent that the model puts more gravitas on the common-sense distinguishing traits. For example, in Figure 9b, we observe the regions of the image samples predominantly influencing their respective classifica- tion scores-the wingspan and beak of a bird, the gallinaceous comb of a chicken, the horns and concave head of a cow, the appendages of a crab, the horns of a deer, the long-slanted head of a dog, the tusks of an elephant, the upright horns of a moose, the big eyes and small ears of a panther, the petite hands and head of a rabbit, as well as the shell and antennae of a snail. As human beings, we evoke these same distinguishing characteristics while classifying the images using our own visual reasoning faculties. As exemplified in Figure 9c, for lo- cal interpretation, we use the model-agnostic technique called LIME (Local Interpretable Model-agnostic Explanations) [56]. We also demonstrate the spatial support of the top-1 predicted"}, {"title": "C. Error Analysis", "content": "The confusion matrix for the INCEPTION V3 model on our dataset, presented in Figure 11, reveals that the \u2018Panther' class exhibits the highest count of misclassifications. One obvious reason for this is the somewhat significant inter- class similarity among the 'Dog', 'Elephant', and 'Panther' classes. Most of the misclassified samples are from visually similar classes. We can posit that navigating the intricacies of visually similar classes poses a significant challenge in this image classification task, as evident from the other pale-red entries of the confusion matrix in Figure 11. Even to the keen human eye, distinguishing between these classes may be perplexing, as they share common visual features, shapes, or color patterns that result in a high degree of resemblance. We examine various aspects, such as the distinctive features or characteristics that might have led to confusion and the degree of similarity between the misclassified classes. Figure 10a and 10b show the confusion between a 'Crab' sample and a 'Rabbit' sample which look visually quite similar. The same holds for the sole 'Moose' sample that is misclassified as a 'Bird' sample by the INCEPTIONV3 model, as depicted in Figure 10c and 10d. We observe that misclassifications of this type occur when images belonging to different, but visually akin categories, are erroneously assigned to the wrong class. The green entries along the diagonal of the confusion matrix in Figure 11 indicate the reasonably good classwise prediction performance of the INCEPTIONV3 model, even though there exists a slight class imbalance in HASPER."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we explore an intriguing subject matter, hand shadow puppetry, in the realm of computer vision. We introduce HASPER, a novel dataset with a sizable collection of 8,340 hand shadow puppet images distributed across 11 classes, taken from both expert and amateur puppetry perfor- mance clips. We fine-tune 31 pretrained image classification models on HASPER to establish a benchmark for the dataset. To explain and analyze the performance of the most erudite model, INCEPTIONV3, in comparison with other baseline models, we visually manifest its feature space using the t-SNE dimensionality reduction technique. We also perform thorough qualitative and error analyses for the INCEPTIONV3 model. We envisage the possibility of developing applications for imparting the art of shadowgraphy, via mobile and embedded devices. We claim that this work is novel and significant since it is the first publicly available dataset and study on image classification benchmarking that focuses only on ombromanie. There are many avenues in our work that warrant further investigation. We hope to reconcile those desiderata by enrich- ing our dataset with numerous permutations of arm positions and finger movements, preferably by employing more skilled individuals with varying palm and wrist structures, thereby creating more diverse silhouettes. We stipulate that the dataset, in a more supplemented state, will be better suited to fine- tune models with a very large number of parameters. We also plan to experiment with a gesture detection technology such as MediaPipe\u00b9\u2070 or Microsoft Kinect\u00b9\u00b9 for leveraging depth coordinates of hand landmarks [58], and assess their efficacy in classifying hand shadow puppets."}]}