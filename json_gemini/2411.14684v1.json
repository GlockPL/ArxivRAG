{"title": "CROSS GROUP ATTENTION AND GROUP-WISE ROLLING FOR MULTIMODAL Medical Image SYNTHESIS", "authors": ["Tao Song", "Yicheng Wu", "Minhao Hu", "Xiangde Luo", "Linda Wei", "Guotai Wang", "Yi Guo", "Feng Xu", "Shaoting Zhang"], "abstract": "Multimodal MR image synthesis aims to generate missing modality image by fusing and mapping a few available MRI data. Most existing approaches typically adopt an image-to-image translation scheme. However, these methods often suffer from sub-optimal performance due to the spatial misalignment between different modalities while they are typically treated as input channels. Therefore, in this paper, we propose an Adaptive Group-wise Interaction Network (AGI-Net) that explores both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, groups are first pre-defined along the channel dimension and then we perform an adaptive rolling for the standard convolutional kernel to capture inter-modality spatial correspondences. At the same time, a cross-group attention module is introduced to fuse information across different channel groups, leading to better feature representation. We evaluated the effectiveness of our model on the publicly available IXI and BraTS2023 datasets, where the AGI-Net achieved state-of-the-art performance for multimodal MR image synthesis. Code will be released.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal medical data plays an important role in modern clinical diagnosis and treatment by providing diverse, complementary information about organs and tissues, aiming at enhancing both accuracy and confidence in clinical decision-making. For example, the MRI T1 modality is usually used to indicate human anatomies and the T2 modality can highlight soft tissues. However, factors such as patient non-compliance during scanning, extended scanning times, and the degradation of individual modality hinder the broader adoption of multimodal imaging Thukral (2015); Krupa & Bekiesi\u0144ska-Figatowska (2015). As a result, it is highly desirable to synthesize missing modalities from a limited number of available multimodal data Iglesias et al. (2013); Huo et al. (2018).\nSimilar to the image translation task, multimodal medical image synthesis usually requires capturing the interrelationships between modalities, integrating fine-grained features across modalities, and establishing mappings from multiple inputs to a single output. Since multimodal image synthesis leverages information from multiple input modalities, it reduces the complexity of network mapping and enhances the reliability of the synthesis compared to the natural image-to-image translation. However, in clinical scenarios, multimodal images are often misaligned due to factors such as motion artifacts, necessitating alignment through iterative optimization or learnable registration methods, leading to accumulated registration errors.\nIn recent years, significant progress has been made in the field of multimodal MR image synthesis. Approaches such as learning modality-specific representations and latent representations of multi-modal images Zhou et al. (2020); Joyce et al. (2017); Chartsias et al. (2017); Meng et al. (2024)"}, {"title": "2 RELATED WORK", "content": "Multimodal Image Synthesis. Multimodal image synthesis improves upon traditional single-modality image synthesis by extracting intra-modality features and capturing inter-modality correlations, thereby enhancing synthesis accuracy and reliability. Numerous studies have focused"}, {"title": "3 METHOD", "content": "An overview of our core CAGR module is shown in Fig. 4. The CAGR primarily consists of the Cross Group Attention module and Group-wise Rolling module. The function of the Cross Group Attention module is to selectively suppress aliasing noise caused by irrelevant modality features and enhance the expression of relevant modality features by leveraging both intra-group and inter-group information. The Group-wise Rolling module is to dynamically perform group-wise rolling of convolutional kernels based on the predicted offsets. In this section, we begin by introducing the intra-group and inter-group attention mechanisms used in the Cross Group Attention module. Next, we explain the group-wise rolling mechanism for convolutional kernels with specified offsets within the Group-wise Rolling module. Finally, we provide details on the network implementation based on the CAGR module."}, {"title": "3.1 CROSS GROUP ATTENTION", "content": "Before performing the rolling convolution operation, for multimodal image synthesis tasks, the input feature $x \\in R^{C_{in}\\times H_{in}\\times W_{in}}$ can be easily divided into n groups, where each group $x_i, i \\in$"}, {"title": "3.2 GROUP-WISE ROLLING", "content": "In this section, we provide a detailed explanation of the Group-wise Rolling module within CAGR. This module operates in two main steps: predicting the parameters for rolling convolution based on the enhanced input features by Cross Group Attention, and generating group-wise rolled convolution kernels.\nTo predict the rolling offsets for each group in a data-dependent manner, we designed a lightweight network called the routing function. The routing function takes the enhanced image feature x as input and predicts n groups rolling offsets $[(o_{x_1},o_{y_1}),..., (o_{x_n}, o_{y_n})]$ for kernels. Each group independently predicts offsets ${o_{x_i}}_{i\\in{1,2,...,n}}$ and ${o_{y_i}}_{i\\in{1,2,...,n}}$ along the x-axis and y-axis, respectively. The overall architecture of the routing function is illustrated in Fig. 4. Initially, the enhanced input image feature $x \\in R^{C_{in}\\times H_{in}\\times W_{in}}$ is fed into a lightweight Group convolution Krizhevsky et al. (2012) with a kernel size of 3 \u00d7 3, followed by Layer Normalization ValizadehAslani & Liang (2024); Vaswani (2017) and GELU Hendrycks & Gimpel (2016) activation. The activated features are then average pooled to form a feature vector of dimension $C_{in}$. This pooled feature vector is passed into two separate branches. The first branch is the rolling offset prediction branch, which predicts offsets along the x-axis and y-axis for each group. No activation function is applied to the predicted offsets, enhancing the expressive power of the rolling convolution. The second branch termed the group scale factor prediction branch, is responsible for predicting the scale factor $\u03bb$ for each group. It consists of a linear layer with bias and Sigmoid activation. The weights of both the rolling offset prediction and group scale factor prediction branches in the routing function are initialized to zero, and the bias in the group scale factor prediction branch is initialized to one, ensuring stability at the beginning of the training process. Notably, the number of groups n is significantly smaller than the number of input channels of the convolution $C_{in}$. Consequently, it is straightforward to partition the convolution kernels into n distinct groups across the $C_{in}$ channels, with each group having a unique set of offsets.\nThe standard convolution takes enhanced input features $x \\in R^{C_{in}\\times H_{in}\\times W_{in}}$ and kernel weights $W \\in R^{ C_{out}\\times C_{in} \\times k \\times k }$, producing the output feature $y \\in R^{ C_{out} \\times H_{out} \\times W_{out}}$. For the multi-channel enhanced input feature in multimodal imaging, convolution is applied uniformly across spatial positions with the same kernel weights $w_m \\in R^{C_{in}\\times k \\times k}$, $m \\in {1,2,\u2026, C_{out}}$ performing $C_{out}$ operations to obtain the output feature y with $C_{out}$ channels. In our approach, we divide the kernel weights W into n groups $w_i \\in R^{ C_{out}\\times C_{in}/n \\times k \\times k }$, $i \\in {1,2,\u2026\u2026\u2026, n}$ in $C_{in}$ dimension, where kernels from different groups can independently capture features specific to different modalities through the grouping strategy.\n$W = \\{w_m \\in R^{ C_{in} \\times k \\times k }\\}, m \\in \\{1,2,\u2026, C_{out}\\}$\n$= \\{W_i \\in R^{C_{out}\\times C_{in}/n \\times k \\times k}\\}, i \\in \\{1,2,..., n\\}.$", "tex": ["W = \\{w_m \\in R^{ C_{in} \\times k \\times k }\\}, m \\in \\{1,2,\u2026, C_{out}\\}$\n$= \\{W_i \\in R^{C_{out}\\times C_{in}/n \\times k \\times k}\\}, i \\in \\{1,2,..., n\\}."]}, {"title": "3.3 NETWORK ARCHITECTURE", "content": "In terms of network implementation, since our proposed CAGR module can be easily integrated as a plug-and-play component into any network structure with convolutional layers, we built the proposed network architecture, AGI-Net, based on the commonly used ResUnet Zhang et al. (2021). ResUnet consists of three down-sampling stages, three up-sampling stages, and a central body stage. Each stage includes two ResBlocks (z + F(relu(F(z)))). We replaced the first convolution in each ResBlock within the three down-sampling stages, the body stage, and the first up-sampling stage with the CAGR module to form the new network architecture, referred to as AGI-Net. Ablation studies on replacing different parts of the network and their impact on performance are discussed in the experimental section."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 EXPERIMENT SETTINGS", "content": "Datasets. We evaluated the proposed method on two publicly available MRI multimodal benchmark datasets: IXI and BraTS2023 LaBella et al. (2024). From the IXI dataset, we selected 577 patients who had T1, T2, and PD-weighted images. The dataset was randomly split into training, validation, and test sets. The training set comprised 500 patients with a total of 44,935 2D images, the validation set contained 37 patients with 3,330 2D images, and the test set had 40 patients with 3,600 2D images. All images were resized to 256x256. Similarly, from the BraTS2023 dataset, we randomly selected T1, T2, and FLAIR images from 580 patients. This dataset was split into 500 patients for the training set, 40 patients for the validation set, and 40 patients for the test set. In terms of 2D images, the training set contained 40,000 images, the validation set 3,200 images, and the test set 3,200 images, with an image size of 240x240. All MRI modalities were normalized to the [0, 1] range using min-max normalization based on the 99.5th percentile maximum value and a minimum value of 0.\nImplementation Details. For training, we set the total number of iterations to 120k using the Adam optimizer with a learning rate of 1e-4 and a batch size of 16. All experiments were conducted in a uniform environment using 4 NVIDIA Tesla V100 GPUs. We utilized the widely recognized Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Absolute Error (MAE) metrics to evaluate the image synthesis quality."}, {"title": "4.2 COMPARISON WITH STATE-OF-THE-ART METHODS", "content": "We conducted multimodal image synthesis experiments under three scenarios on both the IXI and BraTS2023 datasets, comparing our method against existing approaches using three metrics: PSNR, SSIM, and MAE. The existing methods are categorized into two types based on their generative framework: 1) Diffusion-based methods, which employ multi-step iterative diffusion and sampling, such as DDPM Ho et al. (2020) and IDDPM Nichol & Dhariwal (2021); and 2) Adversarial-based methods, which utilize a single-step approach grounded in the adversarial game between a generator and a discriminator, such as pGAN Dar et al. (2019), mmGAN Sharma & Hamarneh (2019), MedSynth Nie et al. (2018), and pixel2pixel Isola et al. (2017). The Ours method integrates AGI-Net with the highly competitive pixel2pixel Isola et al. (2017). As shown in Tab. 1 and Tab. 2, our approach consistently outperforms the existing methods across all multimodal image synthesis scenarios. As shown in Tab. 6, replacing the network from ResUnet to AGI-Net across different methods leads to a significant performance improvement."}, {"title": "4.3 ABLATION STUDY", "content": "We first conducted ablation studies on different components of the CAGR module and compared it with existing dynamic convolution modules by replacing CAGR with these alternatives. The results demonstrate that CAGR not only significantly outperforms standard convolution methods but also surpasses existing dynamic convolution modules. We further analyzed the impact of the number of groups n and the effects of replacing CAGR at different stages of the network. Lastly, we introduced random translations to the input multimodal images to increase misalignment between modalities, verifying the effectiveness of our method."}, {"title": "4.4 LIMITATION AND FUTURE WORK", "content": "Although the proposed AGI-Net demonstrates superior performance in multimodal MR image synthesis, addressing potential spatial misalignments between the input multimodal images and the target modality remains challenging. Future work will involve this and focus on developing a unified synthesis framework to further reduce costs in clinical deployments."}, {"title": "5 CONCLUSIONS", "content": "We present an adaptive group-wise interaction model for multimodal MR image synthesis, featuring two key components: the Cross-Group Attention and Group-wise Rolling modules. The Cross-Group Attention module is designed to fuse both intra-group and inter-group information, effectively mitigating spatial noise between different modality groups. Following this, the convolutional kernels are adaptively rolled in a data-driven manner based on the specific modality groups. This module is flexible and can be integrated into any convolutional backbone for multimodal MR image synthesis. Experimental results show that our AGI-Net significantly enhances image synthesis performance on public multimodal benchmarks while maintaining computational efficiency."}]}