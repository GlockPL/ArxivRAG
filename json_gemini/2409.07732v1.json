{"title": "Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT", "authors": ["Irene Weber"], "abstract": "Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood. This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort. Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results. Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts. ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents. This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks. Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT. This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are extensive artificial neural networks trained on vast amounts of textual data to generate coherent continuations of given prompts. The initial training, which is time-consuming and computationally intensive, is typically followed by additional training phases. Fine-tuning with specific tasks and example responses enables LLMs to solve particular types of problems, while Reinforcement Learning with Human Feedback focuses them on delivering high-quality and socially preferred responses. Research has shown that LLMs can not only produce correct natural and formal language texts conveying plausible contents, but are also capable of reasoning, planning, and simulating other forms of intelligent behaviors. Thus, LLMs offer a wide range of potential applications, the extent of which is still not fully explored.\nFrequently, LLMs are applied for creating and processing texts, for communicating, planning, and computer programming. LLMs require that all tasks and inputs are provided in a textual format. For many applications, LLMs are prompted with freely phrased, natural language text or program code. Yet, they are also capable of processing texts that are structured such that they represent data or formatted documents."}, {"title": "Irene Weber", "content": "The term unstructured document refers to textually encoded information lacking explicit organization, such as natural-language text without a defined context or fixed format. Structured data refers to information with an explicit and strict regular structure, like data originating from database management systems. In structured data, the meaning of a data element is defined by the structure in which it is registered, and the order of data elements, in general, is not meaningful. Semi-structured documents fall between unstructured documents and structured data. They have a flexible structure, often combining heterogeneous textual contents, such as short, potentially ungrammatical text fragments, longer free-text, and markup tags.\nThere are various methods for indicating structure in texts, including markup languages like Markdown or HTML, data exchange formats like XML, JSON, and YAML, formalized languages, and tabular formats as e.g., comma-separated value (CSV) data. Specialized formalisms often build upon generic formats like XML or JSON, such as formalisms for representing process models or other types of graphs. Documents containing formatting markup, such as HTML or LaTeX, are generally considered semi-structured. XML, JSON, and similar formalisms can represent semi-structured documents as well as structured data, depending on the presence and flexibility of an underlying schema. For the remainder of the paper, we will not separately name semi-structured texts where it is not relevant, but will instead understand structured texts to subsume semi-structured texts.\nIt is known that LLMs can handle structured inputs, having encountered the common formalisms during their basic training. Many studies explore how effectively LLMs can create structured documents from natural language text. In contrast, this paper focuses on the ability of LLMs to process already structured texts. We do not aim to convert natural language descriptions of, e.g., graphs or processes, into representations structured according to some formalism. Rather, we investigate how well LLMs can process or restructure inputs that are already structured.\nRestructuring structured documents has practical applications, particularly in writing documents that include formatting and layout information, such as Markdown, HTML, or LaTeX. By inserting or adjusting such formatting, LLMs can support authoring activities beyond merely generating new content. Further applications include converting between different document formats, which is essential when data needs to be reformatted for automatic processing. In software development, the capabilities of an LLM can replace traditionally programmed conversion routines, which are often expensive to develop and test. Integrating an LLM can reduce software development costs and enable more flexible and powerful solutions than would be achievable with classical programming. However, this approach incurs ongoing operational costs if a paid LLM-as-a-Service is utilized.\nAlthough the tasks performed by the LLM in editing structured documents may seem less demanding than other currently researched tasks, they can still bring significant labor savings and efficiency gains. The prerequisite for this to be useful is that the application"}, {"title": "Structured Documents by ChatGPT", "content": "of the LLM for these tasks incurs little effort. This paper addresses the following research question:\n(RQ) Can LLMs be applied for editing structured or semi-structured documents with little effort?\nBy 'little effort,' we mean that simple, quickly designed prompts should suffice, and the outputs of the LLM should be of high quality, requiring minimal manual post-processing. 'Editing semi-structured documents' refers to modifying their structure rather than their semantic content. To our knowledge, this question has not yet been investigated in research."}, {"title": "Related work", "content": "This study offers a qualitative exploration of an LLM's ability to transform structured inputs or convert structured inputs from one format to another. No previous work explicitly investigating this topic was identified.\nThe most closely related work focuses on LLM table understanding. For example, Singha et al.  and Sui et al.  conduct benchmark tests to evaluate LLM performance in interpreting structural tables. These studies present tables in various formats, including HTML, JSON, or Markdown to a range of LLMs, which then answer questions about the table data or table structure in natural language. These tests are conducted on a large scale, with performance assessed automatically. We also reviewed several applications of LLMs that operate on or produce structured outputs similar to those investigated here, as summarized in Tab. 1. However, an extensive literature review of such applications is beyond the scope of this paper.\nWu et al.  present an application for co-reference resolution, a common task in Natural Language Processing (NLP). Their application queries an LLM twice. The first query tags a natural language input with XML tags, while the second query consumes this semi-structured result as input and yields a structured output. Two further applications use LLMs for extracting data into a queryable, highly structured tabular format. One processes various types of semi-structured documents (e.g., HTML, TXT, XML) , while the second scans scientific articles, i.e., natural language texts, to retrieve cooling rates of metallic glasses . An extensive overview of applications of LLMs for tasks encountered in NLP reports works where LLMs produce structured outputs from unstructured inputs . Several papers focus on processing graphs with LLMs. One study describes the geometric structure of graphs in natural language and then utilizes the LLM to perform graph tasks, specifically node classification [Ye24]. In [Ch24], an LLM is employed for generating structured training data to train a Graph Neural Network for node classification, thus avoiding the high costs of using the LLM for node classification directly. Jiang et al.  present StructGPT, a system that interfaces with various structured data pools, specifically, databases and knowledge graphs. StructGPT retrieves data from the"}, {"title": "Irene Weber", "content": "pools and passes it to an LLM, which is tasked to answer questions based on this structured data. The LLM either provides the answer directly or generates a database query that can retrieve the answer.\nThe capability of ChatGPT-4 to generate entity-relationship diagrams, business process models in BPMN, and UML class diagrams from descriptions phrased in natural language is evaluated in [FFK23]. The models and diagrams are generated using representations based on JSON. In [He23], LaTeX is proposed as a means to communicate mathematical concepts and create drawings with an LLM. The LLM is tasked to generate mathematical exercises and corresponding solutions in LaTeX. It is also applied to translate LaTeX formulas into natural language, which can be read aloud to visually impaired persons. Furthermore, the LLM is tasked with creating drawings using TikZ commands, a language for producing vector graphics in LaTeX documents. Xia et al.  contribute a benchmark dataset designed to evaluate the capabilities of LLMs in producing structured outputs across a range of application domains and document formats. Their benchmark dataset comprises prompts which instruct an LLM to create a document in a specific format with the format specified by an example. A further LLM is applied to assess whether the evaluated LLMs successfully generated documents in the required format. Laban et al.  employ LLMs for editing (not generating) unstructured natural language texts. Their system aims to assist authors in writing. While these studies involve LLMs processing structured inputs or producing structured outputs, none of them investigates the capability of LLMs for reformatting or restructuring structured documents."}, {"title": "Method", "content": "To address the research question, we conduct experiments using various document formats. The research adopts a qualitative rather than a quantitative approach. The number of experiments is deliberately kept low, and the results are reviewed and evaluated \"by hand\". This approach allows for identifying details and making unexpected observations that automated tests with large datasets might overlook, as they typically provide only percentages of correctness as, e.g., in [Xi24]. The research aims to investigate tasks that closely resemble real-world scenarios. We conducted two series of experiments. The first series involves documents formatted with LaTeX, a widely-used typesetting language familiar to ChatGPT. The second series uses less common document formats. Here, ChatGPT is tasked with converting RIS records into an XML format used by OPUS. RIS is a standardized markup format for exchanging bibliographic information between literature management programs, while OPUS is a software used by institutions to set up publication databases [Ko26]. To ensure meaningful insights and avoid introducing unintentional biases, we use realistic sample documents. In the first series of experiments, a LaTeX-formatted table taken from a research paper [We24a] was processed. The highly specific technical terms originally presented in this table were replaced with more neutral terms using ChatGPT, without altering the structure of the table. Example documents for the second series are obtained from real university servers.\nExperiments were conducted using ChatGPT (then based on GPT-3.5) through OpenAI's chat interface on April 29 and May 1, 2024. Each experiment's prompt was input into the interface, and the model's response was then analyzed externally. Chat history was cleared after each experiment to ensure independent processing. The input documents, prompts, and outputs are available online in an electronic appendix [We24b]."}, {"title": "Experiment Series 1: Restructuring and reformatting LaTeX", "content": "This experiment series comprises four steps in which the LaTeX table is progressively edited. The prompt for each step consists of an instruction and a table in LaTeX format, with the chat history cleared after each LLM query. Tab. 2 lists the prompts. Fig. 1 depicts the table and a piece of its LaTeX definition before the first edit.\nTo show the generated LaTeX tables and test the generated LaTeX commands, we manually inserted the LLM-generated tables into LaTeX documents such that a PDF could be created. The resulting tables are depicted in Fig. 2 to 5. Protocols of the experiments, along with prompts and complete versions of the input and output LaTeX tables, can be found in the electronic appendix [We24b]."}, {"title": "Results", "content": "In all experiments, ChatGPT generated tables in correct LaTeX syntax that the LaTeX compiler processed without issues. It was able to make all desired changes, although in some experiments, this was achieved only after modifying the prompts, as reported below. The results were not consistently reproducible, meaning that identical queries with cleared chat history sometimes, but not always, produced different outputs. This variability might stem from ChatGPT's temperature settings.\nPrompt 1 produced the desired result, see Fig. 2. Prompt 2a returned the input table nearly unchanged with only a subtle modification in one cell: the content of the last column of the third row (\"[8, 7]\") were replaced by the content of the cell above it (\"[4], Code\"). Prompt 2b produced the desired result, as shown in the Fig. 3. Prompt 3a successfully restructured the table as requested. It merged rows 3 to 5, despite differences in the spelling of the \"Topic\" column, and adopted the spelling \"Data Science Basics\", as illustrated in Fig. 4. Additionally, it added a dividing line before the last table row. A second query with an unchanged prompt 3b also correctly restructured the table, but this time it adopted the spelling \"DataScienceBasics\" when merging rows 3 to 5 and did not generate an additional dividing line.\nIn step 4, ChatGPT was instructed to format specific table contents using various prompt variants, as shown Tab. 2. Specifically, certain table cells' texts were to be printed in italics, excluding commas. In all queries, the specified table contents were reformatted in Italics. However, ChatGPT only succeeded in skipping the commas as requested in some queries. Repeated queries with identical prompts sometimes succeeded and sometimes failed. The result of a successful query using Prompt 4c is depicted in Fig. 5. In some step 4 queries,"}, {"title": "Irene Weber", "content": "ChatGPT added extra LaTeX commands. Specifically, it embedded the provided LaTeX text fragment in a LaTeX table environment (a structure that allows controlling the placement of the table and the inclusion of a caption and label) or even provided a complete LaTeX document (excluding the bibliography)."}, {"title": "Experiment Series 2: Converting structured documents", "content": "The second series of experiments investigates the capability of the LLM in converting structured documents between different formats. We use RIS and OPUS XML data originating from the OPUS servers of Landshut University of Applied Sciences\u00b2 (HAWL) and Technical University Rosenheim\u00b3 (THR) for the case study. Both servers offer the option to export stored publications in RIS and in XML format. Tab. 3 gives an overview over the data used for the experiments. The differing numbers of fields show that the RIS and XML exports are not as uniform as might be expected."}, {"title": "Additional prompts", "content": "ChatGPT was also prompted to convert a RIS into OPUS XML with a zero-shot prompt, i.e., a prompt lacking an example. The zero-shot prompting yielded a syntactically correct XML with a plausible structure and plausibly named fields, but differing from an actual OPUS XML export. This indicates that ChatGPT did not learn these formats or their interconnections during its training. ChatGPT was also asked about details of the publication by Seehuber et al. [Se22] and by Zugschwert et al.  and stated not to know them as follows: I don't have access to specific publications or writings"}, {"title": "Irene Weber", "content": "by SeeHuber, Cr\u00e4mer, and Kippelsberger in 2022 regarding Luftqualit\u00e4t (air quality). [...] my last update in January 2022."}, {"title": "Results", "content": "ChatGPT generated the XML format for all prompts without any syntactic errors. Fig. 8 shows excerpts of the output generated for Seliger.ris. The complete outputs of all experiments can be found in the electronic appendix [We24b]. ChatGPT correctly created all XML fields present in the example XML. For author fields occurring in varying numbers, it created the correct number of fields in the XML and filled them correctly with the authors' names as values. The names occurring in the format \"Lastname, Firstname\" in RIS documents were transferred to XML as \"Firstname Lastname\" matching the provided example. The RIS files do not contain language information. ChatGPT added this information to match the actual language of the publication, replacing \"deu\" with \"eng\", for example, <title language=\"deu\"> \u2192 <title language=\"eng\">, according to the provided XML example. Fields that were present in the example SEEHUBER.XML but not in the example SEEHUBER.RIS were correctly filled in the generated XML documents; e.g.,\nPU VDE VERLAG GMBH \u2192 <publisherName>VDE VERLAG GMBH</publisherName>\nCY D\u00fcsseldorf \u2192 <publisherPlace>D\u00fcsseldorf</publisherPlace>\nRIS fields of type KW (keywords) that were present in the new RIS documents, e.g., in SELIGER.RIS, but not in the example SEEHUBER.RIS and SeeHUBER.XML, were not added"}, {"title": "Irene Weber", "content": "to the generated XML, i.e., SELIGER.XML, meaning that no new field identifiers for keywords were hallucinated. For RIS fields of type A2 (editors of conference proceedings) that were not provided in the ZUGSCHWERT.RIS or the SELIGER.RIS, ChatGPT did not generate entries in the XML, hence editor names were neither copied nor hallucinated.\nFor fields not present in the example SEEHUBER.RIS but having more or less matching fields in the example SEEHUBER.XML, ChatGPT constructed appropriate entries in the XML documents. In constructing these values, ChatGPT worked in a very detailed manner. We analyze three occurrences more closely: First, ChatGPT correctly derived document IDs, presumably by extracting them from the URLs provided in the RIS tag UR. Notably, RIS documents do not comprise a tag for the document ID, while the XML format comprises a dedicated <id> field (compare Fig. 6 and Fig. 7). Second, ChatGPT constructed the string value for the <collection role=\"collections\"> XML field, presumably from the values provided in the A1, T2, Y1 and SN RIS tags, where the last part of this string and the SN value do not completely match in the SEEHUBER example provided to ChatGPT. It should be noted that the RIS documents contain multiple A1 fields (the authors), and ChatGPT consistently selected the value of the first A1 in all experiments. Third, it constructed the download links for the <file> XML fields from multiple parts taken from several RIS fields and the document ID, which it had to extract from the UR. It also adapted the year 2021 in"}, {"title": "Structured Documents by ChatGPT", "content": "the download link in ZUGSCHWERT.XML. Tab. 4 juxtaposes the RIS fields containing the provided information and the constructed strings for each publication.\nIt should be emphasized that ChatGPT constructed the strings in the f and c fields from scratch, and that these strings were embedded within the complete XML by a single query to the LLM, rather than being constructed and positioned separately. Not all the constructed values are \"correct\" in the real world. While the derived document IDs are valid, the constructed links are not. The link in MUENCH.XML is a near miss, as the actual link only differs in the spelling \"Muench\" versus \"M\u00fcnch\" from the generated one."}, {"title": "Discussion and Conclusion", "content": "This paper contributes a qualitative investigation on the original research question whether an LLM can successfully edit semi-structured documents and transform structured documents when prompted with basic and straightforward instructions. We conducted two case studies comprising multiple experiments, one restructuring a LaTeX table, and one converting RIS documents to OPUS XML format. Our results indicate that the research question has a positive answer. In all experiments, the LLM produced syntactically correct documents which could be further processed without issues. The research followed a qualitative approach, conducting a limited number of experiments. Additional and broader experiments are needed to determine if this finding generalizes to other restructuring tasks and LLMs.\nWhile related studies on LLM capabilities [Zh23, Xi24, Si23, Su24] conduct massive tests and evaluate results automatically (e.g., through LLMs [Xi24]), our qualitative approach includes a comprehensive and in-depth manual analysis of the results. This allows us to contribute the following detailed observations. In the LaTeX experiments described in Sect. 4, the LLM was tasked with restructuring a LaTeX table. We found that the LLM understood concepts related to tables such as \"row\", \"column\" and \"cell\" very well. Referring to table columns by their titles worked better than referencing them by their position (i.e., \"last\"). While the LLM reliably recognized and handled the structure explicated by LaTeX annotations, it struggled with recognizing commas as structure indicators. These observations lead to the hypothesis that explicit structural annotations (such as LaTeX commands) may enhance an LLM's understanding of tasks and data provided in prompts, thereby yielding better outputs. Specifically, they might improve the LLM's instruction-following and format-following capabilities [Zh23, Xi24], which are crucial when developing LLM-integrated applications [We24a]. Further experiments exploring this hypothesis will be valuable.\nThe RIS XML experiments in Sect. 5 reveal that the LLM has impressive pattern matching skills, which become evident in the strings it constructed, compare Tab. 4. It seems plausible that its working principle involves identifying relationships (i.e., patterns) between RIS and XML elements in the example documents and replicate these in the documents it was tasked with generating. Some data elements generated are correct with respect to the real world, while other data elements are near misses or completely deviating, as elaborated in Sect. 5.3."}, {"title": "Structured Documents by ChatGPT", "content": "However, it does not seem appropriate to label the latter data elements as \"hallucinated\", as the process that generated them is comprehensible and reasonable, albeit overgeneralizing to some extent. This pattern matching behavior deserves further investigation, as it may constitute a novel approach to understanding the processes leading to hallucinations in LLMs [Ji23a]."}]}