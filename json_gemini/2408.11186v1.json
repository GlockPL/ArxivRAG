{"title": "Autonomous Negotiation Using Comparison-Based Gradient Estimation", "authors": ["Surya Murthy", "Mustafa O. Karabag", "Ufuk Topcu"], "abstract": "Negotiation is useful for resolving conflicts in multi-agent systems. We explore autonomous negotiation in a setting where two self-interested rational agents sequentially trade items from a finite set of categories. Each agent has a utility function that depends on the amount of items it possesses in each category. The offering agent makes trade offers to improve its utility without knowing the responding agent's utility function, and the responding agent accepts offers that improve its utility. We present a comparison-based algorithm for the offering agent that generates offers through previous acceptance or rejection responses without extensive information sharing. The algorithm estimates the responding agent's gradient by leveraging the rationality assumption and rejected offers to prune the space of potential gradients. After the algorithm makes a finite number of consecutively rejected offers, the responding agent is at a near-optimal state, or the agents' preferences are closely aligned. Additionally, we facilitate negotiations with humans by representing natural language feedback as comparisons that can be integrated into the proposed algorithm. We compare the proposed algorithm against random search baselines in integer and fractional trading scenarios and show that it improves the societal benefit with fewer offers.", "sections": [{"title": "Introduction", "content": "Negotiation can resolve conflicts in multi-agent environments and enable agents to reach agreements without a centralized controller by exchanging offers. We develop an autonomous negotiation algorithm, sequential trading with cone refinements (ST-CR), for multi-agent environments.\nWe consider a variant of the multi-issue bargaining problem (Inderst, 2000) where two self-interested rational agents sequentially trade items from a finite set of categories. Each agent has a state defined by the current amount of items it possesses in each category. Each agent has a utility function that depends on its current state and the function is not necessarily separable with respect to the item categories. The offering agent presents a trade offer to improve its utility function, and the responding agent accepts offers that improve its utility function."}, {"title": "Related Works", "content": "We review works on negotiation algorithms and comparison-based optimization as we use comparisons to find trades.\nAutonomous Negotiation Algorithms. Negotiation algorithms often require information sharing between agents to facilitate trades. The shared information can include utility values (De Vries and Vohra, 2003; Conen and Sandholm, 2001; Maruo and Kashima, 2024) or counteroffers from the responding agent (Faratin, Sierra, and Jennings, 1998, 2002), (Coehoorn and Jennings, 2004), (Zhang, Ren, and Zhang, 2015), (Montazeri, Kebriaei, and Araabi, 2020). In scenarios with more than two negotiating agents, mediator-based methods agents must disclose information to a central authority (Hattori, Klein, and Ito, 2007; Klein et al., 2003). However, these information-sharing paradigms may not be feasible due to privacy concerns or computational limitations. For example, in settings such as Dutch auctions, agents respond to the offers only with acceptance or rejection (Thomas, 2012). The proposed sequential trading with cone refinements (ST-CR) algorithm treats acceptance or rejection responses as comparisons to find mutually beneficial trades, eliminating the need for extensive information sharing.\nNegotiation algorithms employ different techniques to utilize obtained information. For instance, numerical valuation methods typically involve creating a utility model for each agent based on the valuations of item bundles (De Vries and Vohra, 2003; Conen and Sandholm, 2001; Maruo and Kashima, 2024). Counteroffer-based approaches, e.g. Faratin, Sierra, and Jennings (1998), use similarity criteria to present offers that closely match counteroffers from the opposing party. Other counteroffer approaches use reinforcement learning (Montazeri, Kebriaei, and Araabi, 2020) or Bayesian predictors (Zhang, Ren, and Zhang, 2015) to learn the responding agent's negotiation strategy or preferences. Mediator-based methods, e.g. Klein et al. (2003), aggregate preferences through a central mediator, which employs optimization algorithms to propose trades. In contrast, ST-CR estimates the responding agent's utility gradient from rejected offers By determining the responding agent's ascent direction, ST-CR identifies mutually beneficial trades without exhaustively searching the space of offers.\nComparison-Based Optimization. Instead of directly accessing an objective function or its gradients, comparison-based optimization algorithms rely on comparisons between points (Nelder and Mead, 1965; Jamieson, Nowak, and Recht, 2012). These techniques are particularly useful in scenarios where the objective function is unknown or difficult to measure. An example is negotiation settings where the utility function of the responding agent is not accessible. In such cases, responses to offers can be represented as comparisons between two points defined by the trade amounts, facilitating optimization without direct function access.\nComparison-based optimization techniques use strategic comparisons to estimate gradients or find optimal solutions without direct access to the objective function. The Nelder-Mead method compares values at the vertices of a simplex to converge toward an optimal point (Nelder and Mead, 1965). Jamieson, Nowak, and Recht (2012) use a comparison-based line search with coordinate descent to systematically search the space of potential solutions. Cheng et al.'s (2020) Sign-OPT aggregates randomly sampled comparison directions for gradient estimation. Zhang and Li's (2024) Comparison-GDE uses a cutting-plane approach for gradient estimation. Karabag, Neary, and Topcu (2021) refine a cone of possible gradient directions using halfspace cuts informed by comparisons. While these methods are effective for single-objective optimization, we consider a multi-objective setting where arbitrary comparisons may not benefit the offering agent.\nSimilar to Karabag, Neary, and Topcu (2021), we use comparisons to cut the gradient cones and enclose the remaining directions using a cone with a smaller semi-vertical angle. Karabag, Neary, and Topcu (2021) use three-point comparisons to obtain the halfspace cuts. However, in the considered setting, the rationality assumption for the offering agent does not allow three point comparisons. Instead, we use two-point comparisons, which may lead to erroneous halfspace cuts. We derive a new cone angle update rule to provably enclose all possible gradient directions, even with erroneous cuts."}, {"title": "Preliminaries and Notation", "content": "The L2-norm of a vector v is ||v||. A function $f : R^n \\rightarrow R$ is $\\beta$-smooth if $| f (y)-f(x)-(\u2207f(x), y-x)| \u2264 \\beta||x-y||/2$ for all $x, y \\in R^n$. The directional derivative of f along vector v is $\u2207_v f(x) = (\u2207f(x), v)/||v||$. The angle between $v_1$ and $v_2$ is\n$\\angle(v_1, v_2) = cos^{-1}(\\frac{\\langle v_1, v_2 \\rangle}{||v_1|| ||v_2||})$. A cone with direction $\\tau$ and semi-vertical angle $\\theta$ is $C(\\tau, \\theta) = \\{x \\in R^n | \\angle(x, \\tau) \\leq \\theta\\}$."}, {"content": "The L2-norm of a vector v is ||v||. A function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is $\\beta$-smooth if $| f (y)-f(x)-(\u2207f(x), y-x)| \u2264 \\beta||x-y||^2/2$ for all $x, y \\in \\mathbb{R}^n$. The directional derivative of f along vector v is $\u2207_v f(x) = (\u2207f(x), v)/||v||$. The angle between $v_1$ and $v_2$ is\n$\\angle(v_1, v_2) = \\cos^{-1}(\\frac{\\langle v_1, v_2 \\rangle}{||v_1|| ||v_2||})$. A cone with direction $\\tau$ and semi-vertical angle $\\theta$ is $C(\\tau, \\theta) = \\{x \\in \\mathbb{R}^n | \\angle(x, \\tau) \\leq \\theta\\}$."}, {"title": "Sequential Multi-Issue Bargaining Problem", "content": "We consider a negotiation setting that is a variant of the multi-issue bargaining problem (Inderst, 2000) where an offering A and responding agent B make sequential trades with items belonging to n categories. Each agent's state is defined by the (non-negative) number of items they possess in each category. The offering and responding agents' states are $S_A$ and $S_B$, respectively. For example, consider a bargaining problem with apples and oranges as categories. An example agent state is $(x_1,x_2) = (1,2)$ where $x_1$ and $x_2$ are the number of apples and oranges, respectively, in the agent's possession. The offering and responding agents have smooth utility functions $f^A(S_A)$ and $f^B (S_B)$ respectively, which reflect their valuations of different states. We note that each individual utility function is not necessarily separable with respect to the item categories. In the fruit example, a person who wants a single serving of apple-orange juice and likes apples more than oranges, may have the utility function $(x_1-2x_2)^2 + (x_1-2)^2 + (x_2 \u2013 1)^2$ whose optimal point is (2, 1). The offering agent A presents trade offers to improve its utility function, while the responding agent B responds to the offers according to its utility function. The offering agent knows the states of both agents and its own utility function, but does not know the responding agent's utility function.\nThe agents aim to maximize their utility functions by trading with each other. A trade offer $T = (t_1, t_2,..., t_n)$ is a set of changes in the number of items in each category where $t_i$ is the increase in item i for the offering agent. An offer is feasible if it results in no negative quantities of any item for either agent in the post-trade state. The offering agent has an offer budget m, which is the maximum number of (accepted or rejected) offers it is can to make to the responding agent. When determining whether to propose or accept an offer, the agents consider the benefit associated with the offer. At states $S_A$ and $S_B$, the benefits of an offer T are $f^A(S_A + T) \u2013 f^A(S_A)$ and $f^B(S_B \u2013 T) \u2013 f^B (S_B)$ for the offering and responding agents, respectively. The societal benefit is the sum of the benefits for both agents. An offer is mutually beneficial if $f^A(S_A + T) - f^A(S_A) \\geq 0$ and $f^B(S_B - T) - f^B(S_B) \\geq 0$. We assume that the agents are rational. Consequently, the offering agent will only make an offer T if $f^A(S_A + T) - f^A(S_A) \\geq 0$, and the responding agent will only accept an offer if $f^B (S_B \u2013 T) \u2013 f^B (S_B) \\geq 0$. Formally, the responding agent's response function is\n$R_B(T) = \\begin{cases} accept & \\text{if } f^B (S_B \u2013 T) \u2013 f^B (S_B) \\geq 0, \\\\ reject & \\text{if } f^B (S_B \u2013 T) \u2013 f^B (S_B) < 0.  \\end{cases}$\nDepending on the responding agent's response to the offer T, the agents transition to next states $S'_A$ and $S'_B$ where\n$S'_A, S'_B = \\begin{cases} S_A+T, S_B-T & \\text{if } R_B(T) = \\text{accept}, \\\\ S_A, S_B & \\text{if } R_B(T) = \\text{reject}. \\end{cases}$\nA joint state $(S^*_A, S^*_B)$is Pareto-optimal if there is no feasible and mutually beneficial offer, i.e., for all feasible offers T, $f^A(S^*_A +T)-f^A(S^*_A) < 0$ or $f^B(S^*_B-T) \u2013 f^B(S^*_B) < 0$.\nProblem definition. Consider a multi-issue bargaining problem with two rational agents. The offering agent knows the initial states $S_A$ and $S_B$, as well as its own utility function $f^A (S_A)$, but does not know the responding agent's utility function $f^B(S_B)$ or its offer budget m. The goal is to maximize societal benefit after m trade offers."}, {"title": "Sequential Trading with Cone Refinement", "content": "We propose Sequential Trading with Cone Refinements (ST-CR), an autonomous negotiation algorithm to find a Pareto-optimal point through sequential trades. Mutually beneficial trades are offers lying in the intersection of the ascent directions of agents' utility functions. To find these directions, we infer the potential gradients of the responding agent with a cone based on rejected offers. By the rationality assumption, each rejected offer provides information on the responding agent's gradient, resulting in a more accurate estimate.\nPreliminaries on Gradient Direction Estimation. Given the sign of the directional derivatives in curated directions, one can approximate the direction of the responding agent's gradient to find an ascent direction. Building on this observation, Karabag, Neary, and Topcu (2021) model the set of potential gradients as a cone. The signs of directional derivatives inferred from comparisons in orthogonal directions are then used as halfspace constraints to refine the cone. The cone model allows the remaining potential gradients after orthogonal halfspace refinements to be easily enclosed by a new cone. Similarly, ST-CR maintains a cone $C(\\tau, \\theta)$ of gradients with direction $\\tau\\in \\mathbb{R}^n$ and semi-vertical angle $\\theta$.\nTo refine the cone of potential gradients, ST-CR utilizes the signs of the directional derivatives. By making a linear approximation of each agent's utility function at the states $S_A$ and $S_B$, we use a first-order Taylor approximation to describe beneficial trades in terms of each agent's utility gradient:\n$f^A(S_A + T) \u2013 f^A(S_A) \\approx \\langle T,\\nabla f^A(S_A) \\rangle \\geq 0,$\n$f^B (S_B \u2013 T) \u2013 f^B(S_B) \\approx \\langle -T,\\nabla f^B (S_B) \\rangle \\geq 0.$\nBy the rationality assumption, the responding agent will reject an offer if $\\langle -T,\\nabla f^B(S_B) \\rangle < 0$, implying that $\u2207_{-T}f^B (S_B) > 0$. Therefore, the agent's responses provide information on the sign of directional derivatives, which ST-CR uses to refine the potential gradient cone. Fig. 1 shows cone refinement after a rejected offer T in two dimensions."}, {"title": "Procedure 2: Cone Refinement (CR)", "content": "Karabag, Neary, and Topcu (2021) accurately determine the signs of directional derivatives or identify inconclusive directions by making comparisons at three points on a line. In our setting, making three-point comparisons corresponds to making two opposite offers. This guarantees that at least one of the offers will not benefit the offering agent, violating the rationality assumption for the offering agent. Hence, we use two-point comparisons to estimate signs of directional derivatives. Two-point comparisons can lead to sign errors, consequently affecting the cone refinement. As we later show in Theorem 1, we account for such errors by increasing the cone's semi-vertical angle to enclose all potential gradients.\nWe now overview ST-CR's two-stage approach. The first stage (Algorithm 1 Lines 3\u20136), uses heuristic offers to quickly improve societal benefit. The second stage (Algorithm 1, Line 8 Procedure 2) uses rejected offers to refine the potential gradient cone. Once an offer is accepted, ST-CR transitions to the next state (Algorithm 1 Lines 5 and 11).\nStage 1: Heuristic Offers (Algorithm 1, Lines 3 - 6). When making consecutive trades, information from previous negotiations can inform new offers. For example, ST-CR begins each new negotiation by proposing the previously accepted offer. If the responding agent has a smooth utility function, its gradient does not change significantly after accepting an offer, making previously accepted offers likely to be accepted again. We later show in numerical experiments that this heuristic improves ST-CR's performance. As per the rationality assumption, this offer is only presented if it is beneficial to the offering agent. If the responding agent rejects the offers presented in this stage, we move to stage 2.\nStage 2: Gradient Refinement (Procedure 2). We use rejected offers to refine the space of potential gradients and identify a mutually beneficial offer. If no such offer is found after refinement, the gradients of both agents are closely aligned, indicating a near Pareto-optimal point. To refine the space of potential gradients, we represent the potential gradients as an n-dimensional cone, $C(\\tau, \\theta)$.\nStage 2.1: Initializing the Cone of Potential Gradients (Procedure 2 Lines 15 - 25). When negotiation begins, the offering agent does not know the responding agent's utility function $f^B$ and, consequently, has no information on $\\nabla f^B (S_B)$. We first determine the n-dimensional quadrant of $\\nabla f^B (S_B)$ by checking if the responding agent wants more or less of each item category: ST-CR makes n offers $T_1 = (\\pm d, 0, . . ., 0), . . ., T_n = (0,0,...,\\pm d)$ where d is a constant. ST-CR adjusts the signs of the offers such that $\\langle T,\\nabla f^A(S_A) \\rangle \\geq 0$ to improve the offering agent's utility. To accurately refine the cone using Taylor approximation and avoid overshooting along mutually beneficial trades, we limit the magnitude of offers by d. If the responding agent accepts any of the offers, ST-CR proceeds to the next state and repeats from stage 1 (Line 20). If the responding agent rejects all of the offers, the direction of the quadrant is inferred as Q[i] = Ti[i] (Line 22). The quadrant is enclosed by a cone with direction $\\tau$ = Q and semi-vertical angle $\\theta$ = $\\pi$/2 (Line 25). Instead of performing this stage, one can also expand the cone from the previous trade to account for gradient changes and, as a heuristic, use the expanded cone for initialization.\nStage 2.2: Determining Mutually Beneficial Offers (Procedure 2 Lines 32 - 34). Given n categories and the cone direction $\\tau$, we generate n - 1 offers $T_1,..., T_{n-1}$ that are orthogonal to each other and $\\tau$ (Line 34). By the rationality assumption, each offer satisfies $\\langle T, \\nabla f^A(S_A) \\rangle \\geq 0$. If the responding agent accepts an offer, a state transition occurs (Algorithm 1 Line 12), and ST-CR repeats from stage 1. After n - 1 consecutively rejected offers in stage 2.2, the space of potential gradients reduces to the intersection of the gradient cone and hafspaces generated by the rejected offers. In stage 2.3, we enclose this space with a smaller cone.\nStage 2.3: Updating the Cone of Potential Gradients (Procedure 2 Lines 28 - 31). If the responding agent rejects n-1 offers, we refine the cone using the rejected offers. Using the first-order approximation and the rationality assumption, each rejected offer T \u2208 {$T_1, \\ldots, T_{n-1}$} = (V \\ r) indicates that $\\langle -T, \\nabla f^B (S_B) \\rangle < 0$. Each of the n - 1 offers is treated as a halfspace constraint $\\langle T, x \\rangle > 0$. ST-CR then calculates a new cone $C(\\tau', \\theta')$ enclosing the remaining gradient directions ${x|\\langle T, x \\rangle \\geq 0} \\cap C(\\tau, \\theta)$. Fig. 1 shows a cone refinement in two dimensions after a rejected offer T. The new cone direction is determined using the prior cone direction and the corners of the space (Lines 29 - 30) such that"}, {"title": "Remark on Rationality Assumption", "content": "Motivated by the first-order Taylor approximation, ST-CR selects offer directions that align with the offering agent's gradient direction ensuring that $\\langle T,\\nabla f^A(S_A) \\rangle > 0$. For non-linear utility functions, this method can lead to non-beneficial trades for the offering agent if $|\\|\\nabla_T f^A(S_A)\\|\\|$ is small, overshooting the optimal trade in the direction of T. However, for a $\\beta$-smooth $f^A$, the potential loss of benefit is bounded such that $f^A(S_A) - f^A(S_A+T) < \\beta d^2/2$. To prevent these losses, the offering agent can avoid overshooting by decreasing the offer magnitude. Alternatively, one can eliminate such trade directions. If trade T is not beneficial despite $\\langle T, \\nabla f^A(S_A) \\rangle \\geq 0$, the offering agent is near-optimal along the line of T. Consequently, we can exclude trades with a component on this line and continue trading within the null space of the line."}, {"title": "Theoretical Guarantees of ST-CR", "content": "We show that, after a finite number of consecutively rejected offers by ST-CR, either the gradients (i.e., preferences) of the agents are aligned, or the responding agent is at a near-optimal state.\nST-CR uses two-point comparisons to infer the signs of the directional derivatives and subsequently refine the cone of potential gradients. As discussed previously, this approach can result in sign errors if the current state is near-optimal along the offer direction. When such an error happens, the cone is cut using the wrong side of the hyperplane. The wrong cuts are the main challenge to derive theoretical guarantees. To analyze the effects of wrong cuts, we use the smoothness property of the utility function. If a sign error occurs for an offer T of magnitude d, i.e., $\u2207_{-T}f^B(S_B) \\geq 0$, inferred as $\u2207_{-T}f^B(S_B) < 0$, the magnitude of the directional derivative satisfies $|\\|\u2207_{-T}f^B(S_B)\\|\\| < \\beta d/2$. We consider two cases.\nCase 1: There is no offer direction with a large directional derivative. In this case, all halfspace cuts may happen in the wrong direction. However, the responding agent's current state is near-optimal since its gradient has a bounded magnitude. For example, the purple gradient $\\nabla f^B (S_B)(\\blacklozenge)$ in Fig. 2 is not enclosed by the halfspace cuts but is bounded\u00b9.\nCase 2: There is an offer direction with a large directional derivative. In this case, we consider a \"hypothetical\" gradient that matches the inferred signs. It shares the same directional derivatives as the true gradient in the directions where the inferred signs are correct and has the opposite directional derivatives in the other directions. The hypothetical gradient is enclosed in the cone constructed with the angle update rule $\\theta \\leftarrow sin^{-1}(sin(\\theta) \\sqrt{1 - 1/n})$ (Karabag, Neary, and Topcu, 2021). We show that, due to the existence of a trade direction with a large directional derivative, the maximum angle"}, {"title": "Numerical Experiments", "content": "We test ST-CR using randomized scenarios. Implementation details are given in the appendix, and the code can be found at Scenario. We consider quadratic utility functions such that\n$f^A(S_A) = S_A^T \\frac{(pQA + QB)}{\\rho+1} S_A+2S_A^T \\frac{(\\rho u_A + u_B)}{\\rho+1}$\n$f^B (S_B) = S_B^T \\frac{(QA + \\rho QB)}{\\rho+1}S_B + 2S_B^T \\frac{(u_A + \\rho u_B)}{\\rho+1}$\nwhere $Q_A$ and $Q_B$ are random negative semi-definite matrices, $u_A$ and $u_B$ are random vectors, and $\\rho\\in [1,\\infty)$ is a mixing constant. Each random matrix is generated by creating an n \u00d7 n matrix with entries uniformly distributed between 0 and 1, and then multiplying this matrix by its negative transpose. $u_A$ and $u_B$ are generated by sampling integers uniformly between 1 and 200. Values of $\\rho$ closer to 1 correspond to high alignment between the utility gradients, while values farther from 1 indicate low alignment. Each agent's state is initialized by uniformly sampling integers between 1 and 200 for each item category. We limit the offers to trade at most 5 items from each category ($d = 5\\sqrt{n}$). The testing includes integer scenarios and fractional scenarios. If any of the trading algorithms reach a state where one agent possesses all items in a category, we remove the category from consideration and continue with the remaining categories.\nBaselines. We implement two baseline algorithms to compare with ST-CR. The first baseline is a random search that uniformly samples offer directions, then scales offers so that $||T|| \u2264 d$. To enhance the random search, we include a heuristic that starts by offering the most recently accepted trade. The second baseline is a random search with momentum. For the first trade, the offering agent uses the random search baseline. In subsequent trades, the agent makes the previously accepted offer with uniformly sampled deviations, with the magnitude of deviations increasing as more offers are rejected. For all baselines, the offering agent only presents offers that improve its utility. Psudocodes for both baselines are presented in the appendix.\nResults. In Fig. 5, we present the cumulative societal benefit (averaged across 1000 randomly sampled scenarios) achieved by each algorithm as a function of the number of offers. The benefit is normalized by the maximum cumulative benefit across all algorithms. In Figs. 5a - 5d, we present plots for integer and fractional scenarios with 3 and 5 item categories. We observe that ST-CR consistently achieves a higher societal benefit with fewer offers compared to the baselines. As the number of item categories increase, the performance gap between ST-CR and the baselines increases for a fixed number of offers, indicating the suitability of ST-CR for complex trading scenarios with limited offer budgets.\nFigs. 5a-5d show that algorithms using the previously accepted trade heuristic achieve the highest benefit with the fewest offers. Since the agents' utility functions are smooth and offers have bounded magnitudes, previously accepted trades are likely to be accepted again. As a result, the heuristic can increase benefit with fewer offers. As trading progresses and agents approach optimal points, the chances of prior trades being accepted decrease due to overshooting, reducing the efficiency of algorithms with prior trade heuristics.\nFigs. 5e-5f show the effects of gradient alignment in fractional trading scenarios. To ensure alignment between the agents' utility gradients and conduct a consistent comparison, the each agent's state is initialized with 100 items in each category. For highly aligned gradients ($\\rho$ = 1.1), all of the algorithms require more offers to increase societal benefit compared to less aligned gradients ($\\rho$ = 5). In scenarios with highly aligned gradients, the space for mutually beneficial offers is smaller, limiting all algorithms' performance."}, {"title": "Conclusion", "content": "We consider a two-agent sequential negotiation setting with minimal information sharing. We introduced ST-CR, an autonomous negotiation algorithm that leverages comparisons to identify mutually beneficial trades. We demonstrated ST-CR's effectiveness in two-agent negotiations and provided theoretical guarantees on finding beneficial trades, or certifying closeness to optimality. ST-CR relies on the rationality of agents, which may not hold in some scenarios. Future work could explore modifications to handle non-rational or deceptive agents, further expanding ST-CR's applicability."}, {"title": "Integrating ST-CR and Language Models", "content": "In addition to negotiation between ST-CR and autonomous agents, we consider integer negotiation scenarios between ST-CR and a human negotiator. In these example scenarios, ST-CR and the human trade apples, bananas, and oranges. ST-CR acts as the offering agent A and the human is the responding agent B. The state of each agent is [x1, x2, x3] where x1 is the number of apples, x2 is the number of bananas, and x3 is the number of oranges. Instead of having an explicit utility function, the human negotiator begins each scenario by inputting a target state uB it would like to reach through trading. In order to show the non-trivial sections of ST-CR, we use the target state to initialize the the responding agent's gradient quadrant. Therefore, we assume that stage 2.1 of ST-CR is already performed and begin negotiation from stage 2.2. ST-CR is also given a target state uA that is generated generated by sampling integers uniformly between 1 and 100. Using these two states, the utility functions for both agents are $f_A (S_A) = -S_A^T I S_A + 2S_A^T I u_A$ and $f_B (S_B) = -S_B^T I S_B + 2S_B^T I u_B$ where I is the n \u00d7 n identity matrix. The human negotiator is not required to follow this utility function. However, approximating it enables the testing program to estimate the human's benefit following a trade. When making offers ST-CR uses the format\nUsing a standardized format for offers allows the language model to easily interpret the human's response.\nWe utilize language models to perform sentiment analysis on human responses to determine if a human is accepting or rejecting an offer. To do this, we first replace the identifiers in the previous offer with two named individuals to allow the language model to easily interpret who is receiving what items. In our code, we replace \"User\" with \"Alice\" and \"ST-CR\" agent with \"Bob\". We then provide the language model with the previous offer and the human's response, then give it the prompt\nUsing the language model's response to this prompt, we determine if the human is accepting or rejecting the offer.\nBeyond acceptance or rejection, humans can provide additional information to ST-CR through counteroffers, adjustments to prior offers, or general preferences. If sentiment analysis determines that the human user is not accepting the trade offer, we prompt the language model to determine if the user is providing any additional information\nAdjustments to prior offers are readily represented as counteroffers, so we use the same logic to handle both cases. If the language model believes that the human user is providing a counteroffer, we give GPT the following prompt\nUsing this approach, we obtain a counteroffer in a standardized form, which is readily integrated into ST-CR.\nIf the language model believes that the human is providing a general preference, we utilize the GPT-4.0 function calling feature to get a linear relationship of the form (w1, v) \u2265 (w2, v) where w1 and w2 represent states and v is the vector of values for each category. For example, a response \"I prefer 2 apples over 3 oranges and a banana\" can be represented as <[2,0,0], [3, 1, 1]) > ([0, 1, 3], [3, 1, 1]). A response \"I prefer apples over oranges\" can be represented as <[1,0,0], [2, 1,0]) > ([0, 1, 0], [2, 1, 0]). A response \"I don't want oranges\" can be represented as ([0, -1,0], [1, \u22121, 1]) > ([0,0,0], [1, \u22121, 1]). We use w1 and w2 as comparison states in ST-CR and do not use the values v generated by GPT. This linear relationship can be represented as a counteroffer that increases the items in w1 and decreases the items in w2. For example, ([0, -1, 0], [1, -1, 1]) > ([0, 0, 0], [1, -1, 1]) can be represented as:\nOne challenge when working with language models is ensuring accuracy in its responses. As we will later show, without an underlying algorithm, language models can make mistakes when interpreting human feedback. A simple way of resolving this issue is by having the language model verify its own responses. After the language model parses the human's response, we feed the human's response and the generated counteroffer back into the language model using the following prompt\nIf the language model claims that the counteroffer is inaccurate, the language model generates a new one, repeating this process until the counteroffer is deemed accurate. Once verified, ST-CR assesses the counteroffer's benefit to the offering agent and presents it to the human as a new offer. Allowing the human to reject the parsed counteroffer adds an extra layer of verification."}, {"title": "Language Model Performance without ST-CR", "content": "In order to obtain a baseline for human negotiation through natural language, we implemented an autonomous negotiation method that uses the language model GPT-4.0 to generate trade offers without any underlying model. When a trading scenario begins, we give GPT the prompt in Fig. 8 to motivate negotiation and structure its responses. The prompt provides the language model with its utility function which has a target state of 25 apples, 50 bananas, and 75 oranges. The model presents a trade to the human user, who then provides a natural language response. Just as with ST-CR, if the human accepts the trade, the model transitions to a new state. If the human rejects the offer, the model must generate a new offer. This process repeats until the human decides to end the negotiation process.\nIn Fig. 9a, we observe that GPT without an underlying model can successfully generate mutually beneficial trades. In this instance, we also observe that the GPT agent can effectively parse human feedback and adjust its offers accordingly. However, we note that language models can exhibit inconsistency and make mistakes in some instances. For example, in Fig. 9b, we observe that the GPT model can propose trades that are not beneficial to itself. This suggests that the GPT model may not obey the self-interested and rational constraint or may wrongly interpret its utility function Another issue arises when the GPT model persistently offers similar trades despite prior rejection. In Fig. 9c, we highlight a specific example where the GPT model makes several consecutive offers of apples and bananas in exchange for oranges. These offers point in similar directions, which are not beneficial to the human negotiator. Our algorithm, which uses orthogonal offers to refine the search space, would consider offers in different directions, leading to a more efficient use of the offer budget. These observations collectively suggest that, without an underlying algorithm, language models may lack the structure to effectively negotiate with humans and may require extensive prompt tuning or chain-of-thought reasoning to accomplish such tasks. To address these limitations, we integrate ST-CR, leveraging language models primarily for sentiment analysis and feedback parsing."}]}