{"title": "Better Think with Tables: Leveraging Tables to Enhance Large Language Model Comprehension", "authors": ["Jio Oh", "Geon Heo", "Seungjun Oh", "Jindong Wang", "Xing Xie", "Steven Euijong Whang"], "abstract": "Despite the recent advancement of Large Language Models (LLMs), they struggle with complex queries often involving multiple conditions, common in real-world scenarios. We propose Thinking with Tables, a technique that assists LLMs to leverage tables for intermediate thinking aligning with human cognitive behavior. By introducing a pre-instruction that triggers an LLM to organize information in tables, our approach achieves a 40.29% average relative performance increase, higher robustness, and show generalizability to different requests, conditions, or scenarios. We additionally show the influence of data structuredness for the model by comparing results from four distinct structuring levels that we introduce.", "sections": [{"title": "1 Introduction", "content": "The recent advancements in Large Language Models (LLMs) have significantly transformed the field of natural language processing, leading to substantial improvements in various tasks. For instance, models like GPT and Gemini (Brown, 2020; OpenAI, 2024; Chowdhery et al., 2023; Team, 2024) have shown remarkable abilities to produce human-like text, generating coherent and contextually appropriate responses across a variety of domains (Zhang et al., 2021; Liu and Lapata, 2019; Rajpurkar et al., 2018; Karpukhin et al., 2020; Yu et al., 2022; Wei et al., 2022).\nIn real-world applications, users expect LLMs to handle sophisticated requests with additional conditions and details (Sen et al., 2022). For example, factual retrieval requests in practical scenarios contains several conditions and details like \"Give me the soccer players with nationality Argentina and preferred foot is left foot. \" Many prior works simplify task objectives to minimize randomness in model outputs, often ignoring specific settings. However, LLMs are relatively less effective for their requests when adding multiple conditions or details (He et al., 2024d; Xiong et al., 2020; Zhang et al., 2024a). Such detailed requests are seldom found in text used to train LLMs, making it even more challenging for the model to generate accurate responses (He et al., 2024c; Zhou et al., 2023). Thus, these requests effectively assess the model's ability to generalize to unseen tasks (Aksu et al., 2023; Yin et al., 2023).\nIn real-world scenarios, information organized in structured formats such as tables often facilitates humans in dealing with complicated or detailed tasks, also known as functional thinking (Blanton and Kaput, 2011; Cloutier and Ravasi, 2021). This thought process highlights relationships and conditions, hence simplifying key concepts, prioritizing action, and making problems more manageable, which lead to better performance.\nParticularly for the task with additional conditions, drawing a table with rows and columns makes it easier to identify elements that satisfy multiple criteria. Building on this cognitive strategy, we propose that LLMs can similarly benefit from Thinking with Tables, which provides a pre-instruction to request a table of related information (Figure 1). This method leverages the advantages of simple, yet rich, tabular representations. Tables consist of comprehensive information comparable to that found in texts, while offering a potentially more accessible format for LLMs to interpret.\nRecent research show prompts significantly influence LLM performance (Sclar et al., 2023; He et al., 2024a,d; Tam et al., 2024). While these studies provide valuable insights into the impact of prompt engineering, they often focus on single-dimensional or ad-hoc prompt designs. The primary challenge lies in structuring and presenting information in a way that enhances model understanding while maintaining generalization across diverse tasks with multiple conditions. In this paper, we address and overcome these limitations by nudging the model to organize information into a two-dimensional structured format, which improves both the performance and robustness of LLMs. Unlike prior work, our approach demonstrates that structurizing knowledge can mitigate common failures in LLM responses on the detailed requests. A natural language format often involves diverse expressions and phrasing, which can lead to inconsistencies and inaccuracies in LLM responses. By introducing a structured pre-processing step before delivering the main instructions, we can help LLMs produce more consistent and accurate answers. This approach not only aligns with the cognitive strategy, but also provides a reliable method for handling diverse inputs, ensuring that detailed instructions are interpreted and executed correctly.\nIn this paper, we demonstrate that internally structuring information in a tabular format can improve LLM performance when addressing complex requests with multiple conditions. Specifically, we focus on request types derived from structurable data that are prevalent across LLM benchmarks. Thinking with Tables improves the baseline for all request types, on average by 40.29%. We show that this behavior remains consistent across varying levels of difficulty by differing numbers of conditions. We also empirically show that our method remains effective even when the model fails to structure all the information completely.\nIn addition, our model analysis reveals that LLMs exhibit higher attention scores when Thinking with Tables compared to the baseline. Higher attention implies that the model better focus on relevant features, enhancing contextual understanding with tables. We also demonstrate how widely-used requests on structurable data have fundamental connections to database querying languages like SQL and are quite expressive.\nOur main contributions are as follows:\n\u2022 We demonstrate that Thinking with Tables improves LLM performance and robustness for widely used requests with multiple conditions.\n\u2022 We generate four distinct structuring levels of information between table and natural text, and show that the structuring level impacts performance through experimental results.\n\u2022 We provide extensive analyses across various top-performing models along with ablation studies showing generalizability."}, {"title": "2 Related Work", "content": "LLM on Complex Instructions Recently, many works have been conducted on how difficult LLMS understand and generate accurate outputs for complex instructions, including the development of benchmarks (He et al., 2024d; Xiong et al., 2020; Zhang et al., 2024a). The most relevant work is presented by He et al. (2024c), which also proposed the method to enhance LLM performance on complex instructions. However, their focus is primarily on how to obtain and utilize effective training data, while we demonstrate the impact of data structuredness on requests with multiple constraints.\nLLM with Tables Tabular structure is an organized format that contains large amounts of information. This systematic characteristic makes tabular data essential for many applications (Deng et al., 2022a,b; Chen et al., 2020a,b). Researchers have explored different methods to represent and encode tabular data. Wang et al. (2024) suggest a multi-step tabular reasoning approach with table evolution to improve table understanding. Deng et al. (2024) systemically evaluate how different text-based or image-based prompt methods affect LLMs' performances on table-related tasks. The most relevant work is Tab-CoT (Jin and Lu, 2023), a variation of the Chain-of-Thoughts (CoT) (Kojima et al., 2022) approach that adopts a tabular reasoning scheme to generate answers. However, they mainly focus on arithmetic tasks, and the questions they address lack constraints or detailed conditions.\nLLM-assisted Table Management Recently, approaches to utilize LLM for effectively managing data with tables are also suggested. Sun et al. (2023); Hong et al. (2024a,b) propose techniques for generating SQL queries using LLMs and provide insights into improving the interaction between user queries and database schemas. Arora et al. (2023); Wu et al. (2021); Zhang et al. (2024b) investigate to extract tables from diverse data sources including semi-structured tables, texts, and images. Our goal is to improve model performance on instructions with detailed conditions, which LLMs often fail to answer correctly, rather than utilizing LLMs as table management tools."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Thinking with Tables", "content": "When interacting with LLMs, we typically give commands and requests in text. LLMs usually provide high-quality answers, but often output incorrect results when the requests include detailed conditions or constraints (He et al., 2024c,d).\nTo resolve this problem, we propose Thinking with Tables that internally structures information in a tabular format. In this paper, we specifically focus on the request types derived from structurable data, where tables are most likely to be effective (see Sec. 5.7 for details). These request types, related to factual information retrieval or manipulation, are widely used by users in interaction with LLMs and present in various benchmarks (see Sec. 4.2 for detailed explanations on request types) (Joshi et al., 2017; Sen et al., 2022; et al., 2023; Li et al., 2023; Oh et al., 2024; Chen et al., 2024; Sun et al., 2024).\nTables in LLMs We utilize the delimeter \"l\" for table formatting introduced by Jin and Lu (2023). They demonstrated that proper formatting is crucial for LLMs to accurately interpret tabular structures, while alternative delimeters, such as \",\", lead to failure in capturing the inherent structure of tables."}, {"title": "Method Formalization", "content": "As mentioned in previous sections, users often provide complex instructions that involve multiple conditions in real-world applications. In this part, we specify these requests and introduce how to trigger the model to think with tables. To begin with, we break down a complex instruction into two components: a main request and conditions. If the instruction is \"Give me the soccer players with nationality Argentina and preferred foot is left foot, \" the main request is \"Give me the soccer players \" and the two conditions are \"nationality is Argentina\" and \"preferred foot is left foot. \"Generally, when a complex instruction is directly given as input to the LLM, we express the model output, $A_{direct}$, as follows:\n$A_{direct} = LLM(R_M \\oplus C_1 \\oplus ... \\oplus C_n)$ (1)\nwhere $R_M$ denotes the main request and $C_{i \\in [1...n]}$ denotes ith condition among the total n additional conditions. $\\oplus$ represents the concatenation of the main request and the conditions (He et al., 2024c). A logical operator, either \u201cand\u201d or \u201cor\u201d, can be placed between two conditions.\nTo enhance LLM performance, we suggest adding a pre-instruction that prompts the LLM to internally organize information before providing the main instruction. For example, a pre-instruction could be: \"Create a table of soccer players.\" With the pre-instruction, the model output, $A_{TwT}$, can be expressed as:\n$A_{TwT} = LLM(R_M \\oplus C_1 \\oplus ... \\oplus C_n | T)$, (2)\nwhere table $T$ is the intermediate output of the LLM on the pre-instruction $I_{pre}$ (The details about $I_{pre}$ are in Sec. 3.3). Before the model executes the main instruction directly, this process allows the model to load and structure the information that will be used for that instruction.\nHowever, the formulation (Eq. (2)) is implausible for extensive evaluation due to different internal knowledge each model possesses. Even when the pre-instruction $I_{pre}$ is fixed to minimize randomness, the intermediate output $T$ will vary across different models. This variability makes it inefficient to consistently calculate an accurate gold answer for the table each time a request is made. Therefore, we predefine the surrogate table $T$ and prepare the requests along with their corresponding gold answers for evaluation (the details of the requests are in Sec. 4.2). Thus, the model output $\\tilde{A}_{TwT}$ for evaluation is as follows:\n$\\tilde{A}_{TwT} = LLM(R_M \\oplus c_1 \\oplus ... \\oplus c_n, T)$ (3)"}, {"title": "3.2 Comparison between Text and Table", "content": "To fairly evaluate the effectiveness of the tabular format, we must compare results when the data type is either text or table, ensuring both contain the same amount of information. In practice, natural and long texts often include extra expressions and content beyond the essential information needed to answer the user's request.\nIntuitively, it seems more challenging to extract information and provide appropriate analyses from natural texts than from tables. Therefore, we adopt two approaches to facilitate the construction of text-table pairs for a fair comparison. The first method is called a text-based construction, which involves removing extraneous information from the natural text. After extracting the information and constructing tables, irrelevant sentences are eliminated. This method is straightforward and intuitive, but has limitations when it comes to accurately analyzing experimental results. (1) The first limitation is that the difficulty of extracting and analyzing information can vary depending on the complexity of each sentence. (2) The second limitation is that irrelevant information may remain if it appears within the same sentence as essential information.\nTo address these limitations, we also employ a reverse approach called a table-based construction, which constructs natural sentences based on the attributes in the table. By using this method, we can generate multiple levels of text that include exactly the same amount of information as the table, as described in Tbl. 1. We construct three levels of text and denote them as Template-based, Order-fixed, and Natural text.\nWe fix the positions of attribute values and expressions within each sentence for Template-based text, but without using a table. For example, in Tbl. 1, each sentence follows the structure: \"{Name} is a soccer player from Nationality} playing for {Club} with uniform number {Number}.\" For Order-fixed text, we paraphrase the expressions of attribute values, such as changing Portugal to Portuguese and uniform number to jersey number in Tbl. 1, while keeping the order fixed. A key difference from Template-based text is that paraphrased expressions vary for each attribute. Finally, we reorder and paraphrase entire sentences for Natural text, mimicking realistic settings. In this paper \"text\" generally refers to Natural text in Tbl. 1.\nIn Sec. 5, we generate text-table pairs using the table-based method for Soccer and Movie datasets and the text-based method for PII dataset."}, {"title": "3.3 Pre-instruction $I_{pre}$", "content": "In addition to analyzing the effect of tabular format, we conduct an experiment for the pre-instruction $I_{pre}$ in Eq. 2. As mentioned in Sec. 3.1, the intermediate output of the model can vary depending on the pre-instruction, which is particularly relevant in real-world applications where such variations might impact usability and consistency. Given this variability, we separately analyze the impact of the pre-instruction to better understand its influence on model behavior and output structure.\nThe primary role of the pre-instruction is to guide the model in structuring its internal knowledge. By setting a clear framework or initial guidance, the pre-instruction directs the model to format information in a way that aligns with the intended tabular structure. Therefore, we evaluate an LLM's ability to convert information into a tabular format according to the given pre-instruction as follows:\n$Pr[LLM(I_{pre} | D_{internal}) = T] \\approx Pr[LLM(I_{pre}, \\tilde{D}_{input}) = D_{table}]$\nwhere $D_{internal}$ denotes true internal knowledge that cannot be observed, and $D_{input}$ and $D_{table}$ represent the surrogate data of $D_{internal}$ in natural text and table formats, respectively. This setup allows us to systematically observe how well the model can structure the surrogate input data into the desired tabular output, reflecting its adaptability and effectiveness in structured formatting tasks.\nIn Sec. 5, we demonstrate that LLMs can robustly generate tables based on information initially presented in a natural text format. Notably, the conversion is achieved even without explicit column specifications or predefined schemas, highlighting the model's capacity for flexibly organizing information internally. LLMs can autonomously infer columnar structures, which has important implications for applications where predefined formats may not be practical or possible."}, {"title": "4 Experimental Setup", "content": "In this section, we provide the models and datasets utilized in our experiments. We describe the specific types of requests formulated to assess the model's performance and outline the evaluation criteria applied to each request."}, {"title": "4.1 Large Language Models and Datasets", "content": "LLMs Compared To evaluate the general applicability of our approach, we assessed the top-performing models, five proprietary (GPT-3.5, GPT-4 (OpenAI, 2024), GPT-4o, Gemini-1.5-Flash (Team, 2024), and Claude-3.5-Sonnet) and three open-source models (LLaMA-3.1-70B, Mixtral-8x22B, Gemma-2-27B).\nDatasets In experiments, we use three datasets presenting different domains: Soccer (Leone, 2019), Movie (Paul, 2018), and PII (Paullier, 2024) (Details are in Sec. A.2)."}, {"title": "4.2 Main Instruction and Evaluations", "content": "As noted in Sec. 3.1, a main instruction comprises a main request $R_M$ and additional conditions $c_i$. In this section, we show how we select main requests and generate conditions in the experiments.\nCondition Generation Among all attributes in the dataset, we randomly select an attribute and corresponding value for generating each condition. For example, if nationality attribute is chosen, one value is randomly selected among the unique values of nationality in the dataset, such as Spain, England, or Portugal. This strategy ensures the generation of diverse and unbiased set of questions. Additionally, we verify that the prompted table and text have mainly entities that satisfy the generated conditions. This approach allows us to evaluate the model's capability to respond to valid data scenarios, rather than frequently encountering edge cases where no entities satisfy the conditions.\nFor the main experiment, we generate two conditions and concatenate these conditions with one of two logical operators, \u201cand\u201d or \u201cor\u201d. We vary the number of conditions as an ablation study, described in Sec. 5.4. Since condition types are diverse in the real world, we vary the condition types as well. For the Soccer dataset, conditions are based on exact equality, whereas for the Movie and PII datasets, we use inequality and partial equality. For example, \"Nationality is Argentina\" is based on equality, while \"Rating is higher than 3.0\" and \"E-mail domain is gmail\" are based on inequality and partial equality, respectively.\nRequest Types With the generated conditions, we formulate instructions for six request types inspired by complex QA and complex instruction benchmarks (Sen et al., 2022; He et al., 2024d; Zhang et al., 2024a). Following Sec. 3.1, we cover widely used instructions with completeness that can be derived from structurable data. Instructions can be expanded with database querying languages as explained in more detail in Sec. 5.7.\nTo ensure a comprehensive evaluation of the model's performance, we generate three different prompt templates for each request type like Figure 2. This approach mitigates the risk of performance bias on a certain template, enhancing the robustness of our evaluation scheme. We generate 100 condition pairs each concatenated with both \"and\" and \"or\" for all templates, resulting in a total of 600 instructions for each request type.\n\u2022 Retrieval: Requests to retrieve entities satisfying given conditions. For example, \"Give me soccer players of {Conditions}.\"\n\u2022 Deletion: Requests to delete all information about entities satisfying given conditions. For example, \"Forget soccer players of {Conditions}.\"\n\u2022 Update: Requests to redact specific information of entities satisfying given conditions. For example, \"Replace the jersey numbers of soccer players to N/A if {Conditions}.\"\n\u2022 Superlative: Requests to retrieve information based on an entity's position in an ordered list. For example, \"Give me one soccer player with the highest jersey number among soccer players {Conditions}\"\n\u2022 Aggregation (Sum): Requests to calculate the sum of the specific information (numerical attributes) for entities satisfying given conditions. For example, \"Sum the jersey number of soccer players {Conditions}.\"\n\u2022 Aggregation (Count): Requests to count the number of entities satisfying given conditions. For example, \"Count the number of soccer players {Conditions}.\"\nEvaluation We employ different metrics tailored to each request type to fairly evaluate model performance. For tasks where true/false positives and negatives are applicable (Retrieval, Deletion, and Update), we calculate F1 scores. We compute accuracy measures for requests that require a single answer (Superlative and Sum), whereas for Count requests, we compute absolute difference values to account for cases where the model's output is close to the gold answer. This approach allows better performance evaluation over the traditional answer-based accuracy metrics."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Main Results", "content": "We evaluate the effectiveness of Thinking with Tables by comparing performance when surrogate information is presented in either text or table format, as described in Sec. 3.1. Tbl. 2 summarizes the performance of eight models across six request types. Thinking with Tables outperforms the baseline for all request types, with an average relative increase of 40.29% (5.34 pp, percentage points). Notably, Thinking with Tables enhances performance by 11.96 pp for Update, which is the most challenging among the request types. In particular, the GPT-family models which are known to be pretrained with tables (Jin and Lu, 2023) benefit significantly.\nBeyond performance improvement, tables help models to produce stable results. Even for the same request, a model's result may vary depending on the wording of the request. Thinking with Tables reduces the variability in results by structurizing the relationship between the entities or attributes with schemas, increasing the robustness of the model. The results are shown in Fig. 3. For most models the variance is reduced significantly when the model thinks with tables compared to text."}, {"title": "5.2 How does structuring level impact LLMS?", "content": "Continuing from Sec. 3.2, we compare average performances between different structuring levels for Soccer dataset in Tbl. 3 (see full results in Tbl. 6). As described in Tbl. 1, Table represents the most structured level, while Natural text represents the least structured level, where all expressions are paraphrased, and the order of attributes is completely randomized. On average, Table achieves the best performance, and model performance improves as the information becomes more structured."}, {"title": "5.3 Why does Thinking with Tables work?", "content": "We take a closer look within the model investigating why tables contribute to better performance within Llama3.1:70B model through attention analysis. For requests in Retrieval task for the Soccer dataset, we compute the attention values of (1) table components (schema and row) and the request and (2) corresponding textual sentences and the request. We aggregate the attention values across all heads and layers and average them for comparison. Thinking with Tables shows higher attention values compared to text, as demonstrated in Tbl. 4. Moreover, it also shows lower standard deviation, indicating higher stability. These results suggest that tables assist models to better focus on its knowledge leading to better results."}, {"title": "5.4 Can Thinking with Tables generalize across varying condition numbers?", "content": "To demonstrate the generalizability of our approach, we apply it to Retrieval requests with different number of conditions for the Soccer dataset. Note that we concatenate the conditions with an \"or\" operator as the intersection will mostly result in null when the number of conditions increases. We conduct the experiment on all eight models and illustrate the results of three models in Fig. 4 (Others are shown in Tbl. 5). For all models, model performance decreases as the number of conditions increases. However, our approach consistently outperforms the baseline even with five conditions."}, {"title": "5.5 Is it still beneficial if the knowledge is incompletely structured?", "content": "In real-world scenarios, the model's internal text-based knowledge is vast and diverse. It might be impractical to represent them as a single table. To simulate this, we design experiments where only a limited subset of tables are appended with text for Retrieval requests on the Soccer dataset. We conduct the experiment for eight LLMs, with the results for three models shown in Fig. 4 (others are in Tbl. 7). As illustrated in the figure, even structuring a quarter of the information improves performance for most models (3.4pp improvement in average). Moreover, converting all information into a Table format enhances performance by about 10pp for GPT-40 (6.4pp in average for all models)."}, {"title": "5.6 Can LLMs structure knowledge?", "content": "We test the models to check whether they can successfully structure knowledge. We prompt the models to convert the text into a table for all three datasets as mentioned in Sec. 3.3. Based on the text given, LLMs are able to correctly fill in 98.6% of the rows and columns (entities and attributes) on average, with and without explicit columns given. This implies the intrinsic ability of LLMs to structure knowledge."}, {"title": "5.7 Extensions", "content": "Natural language prompts involving structurable data or retrieving information based on conditions often inherently rely on the principles of relational algebra or tuple relational calculus along with SQL functions (see Sec. A.6.1 for details). Additional to the requests in the main experiment that are known to be widely used in real-world scenarios, we provide three additional query types to show completeness and generizability of our work. For all extensions, Thinking with Tables boosts model performance and robustness across all models.\nExistence Oh et al. (2024) generates requests from based on database theories, where the binary requests can be expressed with an existential quantification with and without negation in tuple relational calculus. Aligning with the prior work, that emphasizes the importance of model rationales for better evaluation, we report rationale accuracy (R) (see Tbl. 10 for an exemplar response) to assess model performance. The results are shown in Tbl. 8.\nProjection Beyond Retrieval, one might want to extract an entity's attributes or properties, where these requests can be generated with a projection operation in relational algebra. The results are shown in Fig. 5.\nDifference Condition Going beyond the \u201cand ($\\land$)\u201d and \"or ($\\lor$)\" logical operators for condition generation, \u201cset difference ($\\setminus$)\u201d can be used to merge two elements in relational algebra operations from set theory (Franchitti, 2015). A \\ B can be expressed as A$\\land$\\B, hence an example of a condition will be preferred foot is left and nationality is not Argentina. The results are shown in Tbl. 9."}, {"title": "6 Discussion", "content": "Beyond Requests Based on Structurable Data\nThe request types that we currently test on are focused on requests that can be derived from data that are structurable. However, tables have the potential to benefit models across a wide range of prompts extending beyond request types that can be converted to database languages that we use in our work. For example, Jin and Lu (2023) demonstrates that even Zero-Shot CoT can benefit from table structures. Moreover, He et al. (2024b) shows that structuring prompt formats help increase model performance and stability. Hence we believe that tables can be used to improve the performances of unexpected tasks as well.\nBeyond Evaluation Our approach leverages an in-context method to help LLMs internally organize unstructured data. We observe that GPT models that are pretrained with tables (Jin and Lu, 2023), benefit the most from Thinking with Tables. Thus, we believe that utilizing tables in the pre-training phase, for instance text to table conversion or next cell prediction, will boost model performance."}, {"title": "7 Conclusion", "content": "We propose Thinking with Tables, which enhances LLM comprehension for complex requests with multiple conditions on structurable data. The key intuition is to align with human cognitive preferences, where humans benefit from organizing information as an intermediate step when dealing with complex tasks. We show through extensive experiments that Thinking with Tables significantly improves LLM performance and robustness (less variability). Moreover, through ample ablation studies, we show the generalizability and completeness of our approach and possible future extensions, highlighting the potential of structured representations to address intrinsic limitations of text. Hence, Thinking with Tables can lead to more reliable LLMs in complex real-world scenarios."}, {"title": "Limitations", "content": "We state a few limitations of our work. First, as explained in Eq. (3), we evaluate the model performance using fixed surrogate tables for better consistency, but the surrogate tables themselves may not be what exactly an LLM thinks internally. Second, Thinking with Tables mainly focuses on request types on structurable data, although there are opportunities to expand to other common and complex request types as we discussed in Sec. 6."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Models and Hyperparameter Details", "content": "GPT-3.5, GPT-4, and GPT-4o were run through Azure OpenAI API. Gemini-1.5-Flash and Claude-3.5-Sonnet were run through Google and Anthropic APIs respectively. Open sourced models, Mixtral-8x22B, Llama-3.1:70B, and Gemma2-27B were run on 16 A100 GPUs without parallelism for better speed. All the models' temperature parameters were set to zero to control randomness."}, {"title": "A.2 Dataset Details", "content": "Continuing from Sec. 4.1, we use three datasets presenting different domains. Soccer and Movie datasets are tabular datasets; and the PII dataset consists of AI-generated text and the table extracted from the text. We selected 100 entities for each dataset in the experiments.\n\u2022 Soccer (Leone, 2019): We use a relation with the attributes player name, club, jersey number, nationality, league, and preferred foot.\n\u2022 Movie (Paul, 2018): We use a relation with the attributes movie title, director name, movie length, actor name, released year, movie, and rating.\n\u2022 PII (Paullier, 2024): This dataset contains AI-generated texts and several information extracted from those texts. The extracted relation has the attributes name, email, phone number, job, address, hobby, and job experience years.\nAll datasets are publicly available allowing any usages on Kaggle with CC0: Public Domain license for the Soccer dataset, Public Domain license for the Movie dataset, and Apache 2.0 license for the PII dataset. All datasets are mostly in English with no privacy or ethical concerns."}, {"title": "A.3 More Results from Sec. 5.2", "content": "We present the results between different structuring levels in Table 6. Bold and underlined fonts indicate the best and second-best performances, respectively. In general, models show the best or second-best performance when thinking with tables. Template-based text also demonstrates competitive performance compared to Table. Regarding model types, structuring information proves more effective for high-end models."}, {"title": "A.4 More Results from Sec. 5.4", "content": "We present extensive results when the number of conditions varies in Table 5. In most cases, Thinking with Tables outperforms the baseline with natural text. Gemini and Claude perform well with natural text when the number of conditions is small, but their performances improve with tables as the number of conditions increases."}, {"title": "A.5 More Results from Sec. 5.5", "content": "Table 7 shows extensive results adding five models not included in Figure 4. In average, most models shows gradually improvements across the portion of structuredness. Especially, the advanced models like GPT-40 and Gemini are dramatically improved by structuring only a quarter of the entire natural text (up to 14pp). Moreover, open-source models like Mixtral, Llama, and Gemma also show improvements ranging from 0.6pp to 4.9pp."}, {"title": "A.6 More Results from Sec. 5.7", "content": ""}, {"title": "A.6.1 Expanding Requests to DB Theory", "content": "The requests that commonly arise from structurable data, which we test on, often inherently rely on principles of database querying languages such as relational algebra, relational calculus, and SQL. As shown in Tbl. 11, all requests tested in this paper can be converted into a database querying language. Note that this conversion can also be done in an opposite direction (i.e. database querying language to request) diversifying request types. In our paper we test Thinking with Tables for the most commonly and widely used request types in our main experiment (Sec. 5.1) and show in Sec. 5.7 that our work consistently outperforms the baseline even for extended requests."}, {"title": "A.6.2 Case Study of Existence Requests", "content": "We show an exemplar model response of the Existence requests in Tbl. 10 that demonstrates the importance of rationale evaluation."}, {"title": "A.6.3 Results of Existence Requests", "content": "As shown in Tbl. 11, questions proposed in Oh et al. (2024) can be also expressed with an existential operator in relational calculus. We test whether Thinking with Tables improves the model performance on the Soccer dataset shown in Tbl. 8. The red numbers in the parenthesis in the table indicate the performance difference between the original and negated request template (BN(Y) and BN(N) in Oh et al. (2024), respectively), where lower values indicate higher robustness. Thinking with Tables outperforms the baseline for most models, 6.5pp and 6.9pp in average for original and negated requests respectively and 4.1pp reduction in variance between the original and negated requests."}, {"title": "A.6.4 Results of Projection Requests", "content": "Consistent to the results for existence request types, LLMs show better performance and robustness when Thinking with Tables as shown in Figure 5. GPT-4 and Gemini shows large amounts of performance improvements compared to using natural text. On the other hand, for GPT-3.5, Claude, and Mixtral, the enhancements are relatively smaller, but they shows much better stability across the different instruction templates."}, {"title": "A.6.5 Results of Difference Conditions", "content": "We show that the requests can be extended also in terms of conditions introducing an additional logical operator, \"$\\setminus$\" (set difference). Tbl. 9 shows the experimental results for three request types for the Soccer dataset. Thinking with Tables outperforms the baseline with an average relative increase of 26.45% (4.83pp)."}]}