{"title": "Semantic Scene Completion Based 3D Traversability Estimation for Off-Road Terrains", "authors": ["Zitong Chen", "Chao Sun", "Shida Nie", "Chen Min", "Changjiu Ning", "Haoyu Li", "Bo Wang"], "abstract": "Off-road environments present significant challenges for autonomous ground vehicles due to the absence of structured roads and the presence of complex obstacles, such as uneven terrain, vegetation, and occlusions. Traditional perception algorithms, designed primarily for structured environments, often fail under these conditions, leading to inaccurate traversability estimations. In this paper, ORDformer, a novel multimodal method that combines LiDAR point clouds with monocular images, is proposed to generate dense traversable occupancy predictions from a forward-facing perspective. By integrating multimodal data, environmental feature extraction is enhanced, which is crucial for accurate occupancy estimation in complex terrains. Furthermore, RELLIS-OCC, a dataset with 3D traversable occupancy annotations, is introduced, incorporating geometric features such as step height, slope, and unevenness. Through a comprehensive analysis of vehicle obstacle-crossing conditions and the incorporation of vehicle body structure constraints, four traversability cost labels are generated: lethal, medium-cost, low-cost, and free. Experimental results demonstrate that ORDformer outperforms existing approaches in 3D traversable area recognition, particularly in off-road environments with irregular geometries and partial occlusions. Specifically, ORDformer achieves over a 20% improvement in scene completion IoU compared to other models. The proposed framework is scalable and adaptable to various vehicle platforms, allowing for adjustments to occupancy grid parameters and the integration of advanced dynamic models for traversability cost estimation.\nNote to Practitioners\u2014The motivation of this paper is to help identify areas where vehicles can safely drive in off-road terrains, improving both safety and efficiency in navigation. Current methods typically process semantic and geometric pass-ability features separately, resulting in coarse drivable area recognition. This paper presents ORDformer that combines semantic and geometric features and introduces the RELLIS-OCC dataset specifically for training the model. In real vehicle tests, ORDformer demonstrates effectiveness in challenging off-road scenarios. Future work will integrate ORDformer with 3D path planning algorithms and release datasets with more complex", "sections": [{"title": "I. INTRODUCTION", "content": "AUTONOMOUS vehicles are increasingly deployed across various environments, including urban, suburban, and off-road settings. While significant advancements have been made in structured environments with well-defined roads and traffic regulations, navigating unstructured off-road terrains remains a substantial challenge. Off-road environments lack clear pathways and present irregular obstacles, as well as diverse terrain features such as uneven ground, vegetation, and occlusions caused by rocky piles or dense foliage. These complexities require advanced perception capabilities to ensure safe and efficient navigation.\nIn urban environments, datasets such as KITTI [1], nuScenes [2], Waymo Open [3], and ApolloScape [4] provide valuable real-world data for obstacle detection and localization. Traditional perception methods [5]-[8], designed for 3D object detection tasks, detect and localize obstacles, representing them with bounding boxes. However, these methods struggle to identify irregular and continuously distributed objects in off-road conditions. Recent approaches [9]-[12] for 3D occupancy prediction focus on capturing fine-grained spatial structures and semantic information through voxel-based representations. While these methods provide more accurate modeling of obstacles, off-road scenarios also require evaluating the traversability of the environment.\nTo address these challenges, ORDformer is proposed as a novel multimodal method for off-road traversability esti-"}, {"title": "II. RELATED WORKS", "content": "A. Off-road Environment Datasets\nPerception technology in autonomous driving largely relies on large-scale training datasets; however, existing datasets often do not clearly distinguish between off-road and un-structured road environments. Due to this situation, datasets designed for unstructured roads have been compiled and summarized."}, {"title": "III. METHODS", "content": "The objective is to predict a dense semantic scene within the camera view in front of the platform by leveraging monocular images and LiDAR point clouds as input, thereby enabling accurate and reliable traversability estimation in off-road environments. Specifically, the image $I_t$ and LiDAR point cloud $L_t$ are taken as inputs, and the output is a voxel grid $Y_t\\in \\{C_o, C_1, C_2, \\ldots, C_M \\}^{H \\times W \\times Z}$, where each voxel is classified as either empty $\\{c_o\\}$ or occupied by a specific traversability class within $\\{C_1, C_2 \\cdots, C_M \\}$. In this context, M denotes the total number of traversability classes, while H, W, and Z represent the length, width, and height of the voxel grid, respectively. To facilitate this process, RELLIS-OCC, the first occupancy dataset with traversability cost annotations specifically designed for off-road scenarios, is introduced.\nA. Architecture of ORDformer\nOff-road environments lack structured roads and traffic regulation information, making the traversability of such ter-rains closely linked to to the geometric constraints of the platform and necessitating detailed geometric ground analysis. Consequently, there is an increased demand for advanced 3D occupancy prediction capabilities in perception algorithms. To address these challenges, a multimodal semantic scene completion method specifically designed for off-road condi-tions is proposed. This method leverages LiDAR point clouds and monocular images to generate dense semantic occupancy predictions from a forward-facing perspective. The overall pipeline of ORDformer is illustrated in Fig. 2.\nThe model begins by extracting features from both modali-ties: the LiDAR data, which provides precise geometric infor-mation, and images, which contribute rich semantic details. These features are then fused using deformable attention mechanisms to effectively integrate spatial and semantic infor-mation. Subsequently, the fused features are processed through deformable self-attention layers to enhance the 3D occupancy"}, {"title": "1) Feature Extraction", "content": "To achieve more accurate occu-pancy estimations, multimodal information is utilized to ex-tract environmental features. The model employs a ResNet-50 backbone to extract image features $F^{2D}\\in \\mathbb{R}^{l \\times b \\times d}$ from the monocular camera image $I_t$, where $l \\times b$ represents the spatial resolution and d denotes the feature dimension. Concurrently, the sparse point clouds $L_t$ from the LiDAR input are voxelized to generate sparse voxel occupancy supervision $S_{spa} \\in \\{0,1\\}^{H\\times W\\times Z}$. Subsequently, a query upscale network is utilized to produce lower spatial resolution occupancy supervision $S_{den} \\in \\{0,1\\}^{H/2 \\times W/2 \\times Z/2}$, thereby enhancing the robustness of the queries."}, {"title": "Mask Token", "content": "The mask token module proposed by Y. L.[56] is incorporated to represent empty voxel spaces within the environment. When combined with voxel occupancy su-pervision, this approach forms a more comprehensive 3D voxel feature, facilitating improved occupancy predictions."}, {"title": "2) Feature Fusion", "content": "In the model, deformable attention (DA) mechanisms [57] are adopted to facilitate the inter-action between local regions of interest in the 3D and 2D feature spaces. Deformable attention dynamically samples $N_s$ points around a reference point, and the attention weights are adaptively learned based on the query vector, while the spatial offsets for sampled points are predicted relative to the reference point, enabling flexible and efficient feature aggregation. The attention results are computed based on these samples using the following equation:\n$DA (q, p, F) = \\sum_{s=1}^{N_s} A_s W_s & F(p+ \\delta p_s),$ (1)\nwhere, q is the query vector, p denotes the reference point associated with q, F represents the input feature map, $W_s \\in \\mathbb{R}^{d \\times d}$ are learnable weight matrices for value generation at each sampled point, $A_s \\in [0, 1]$ are attention weights for each sampled point, dynamically learned based on the query, $\\delta p_s \\in \\mathbb{R}^2$ are the predicted offsets for the sampled points relative to the reference point p, and $F(p+\\delta p_s)$ denotes the feature vector at the location $p+\\delta p_s$, extracted via bilinear interpolation from the input features."}, {"title": "Deformable Cross-Attention (DCA)", "content": "Multiple DCA layers are employed to enhance the interaction between the occu-pancy supervision queries $S_{den}$ and the image features $F^{2D}$, thereby facilitating the integration of multimodal information. The 3D grids are projected onto the 2D image feature space $F^{2D}$ using projection matrices. The resulting 2D points serve as reference points for the queries $S_{den}$, from which surround-ing features are sampled. A weighted sum of the sampled features is computed to produce the output of the DCA layers. Consequently, refined queries $\\hat{S}_{den}$ are obtained, fusing environmental semantic features with geometric features.\n$DCA (s_{den}, F^{2D}) = DA(s_{den}, P(p, t), F^{2D}),$ (2)\nfor each query $S_{den}$ at position $p = (x, y, z)$, the camera projection function $P(p, t)$ is applied to map the query to a corresponding reference point on the image $I_t$."}, {"title": "Deformable Self-Attention (DSA)", "content": "The refined queries $\\hat{S}_{den}$ are concatenated with mask tokens to form the initial voxel features $F^{3D} \\in \\mathbb{R}^{H/2 \\times W/2 \\times Z/2 \\times d}$. Subsequently, these features are processed through multiple DSA layers to obtain the enhanced voxel features $F^{3D} \\in \\mathbb{R}^{H/2 \\times W/2 \\times Z/2 \\times d}$. The DSA layers enable the model to learn spatial 3D occupancy features more efficiently and accurately. In the equations, the query $q_p$ can be either a mask token or a refined query located at position p = (x, y, z).\n$DSA (F^{3D}, F^{3D}) = DA(q_p, p, F^{3D}).$ (3)"}, {"title": "3) Output", "content": "The enhanced voxel features $F^{3D}$ are input to fully connected (FC) layers and passed through a soft-max function to produce dense semantic occupancy predic-tions $Y_t\\in \\{c_o, C_1, C_2, \\ldots, C_M \\}^{H \\times W \\times Z}$, where each voxel is classified as either empty $\\{c_o\\}$ or occupied by a specific traversability class within $\\{C_1, C_2 \\cdots, C_M \\}$. Mapping these predictions back to the 3D grid yields a detailed occupancy map that highlights traversable areas and provides semantic labels essential for autonomous off-road navigation."}, {"title": "4) Training Loss", "content": "ORDformer is trained end-to-end from scratch by minimizing three primary loss components: the semantic scale loss $L_{sem}^{scal}$, the geometric scale loss $L_{geo}^{scal}$, and the weighted cross-entropy loss $L_{ce}$. The total training loss $L_{total}$ is defined as the sum of these individual losses:\n$L_{total} = L_{ce} + L_{sem}^{scal} + L_{geo}^{scal}.$ (4)\nThe weighted cross-entropy loss is computed as follows:\n$L_{ce} = - \\frac{1}{|\\Omega|} \\sum_{i\\in \\Omega} \\omega_{tar_i} \\cdot \\log\\left(\\frac{e^{Y_{i, tar_i}}}{\\sum_{c=1}^M e^{Y_{i, c}}}\\right),$ (5)\nwhere \u03a9 represents the set of valid voxels, M is the number of classes, $tar_i$ denotes the true class of voxel i, $\\omega_{tar_i}$ is the class weight for the true class tar of voxel i, $Y_{i,tar_i}$ is the logit output of the model for the true class $tar_i$, and $Y_{i,c}$ is the logit output of the model predicting voxel i as class c.\nIn the context of semantic scene completion, model perfor-mance is evaluated using several metrics, including Precision $P_c$, Recall $R_c$, and Specificity $S_c$. These metrics assess the ability of the model to correctly classify occupied and non-occupied voxels. The metrics are formulated as follows:\n$P_c (tar, p) = \\log\\left(\\frac{\\sum_{i \\in \\Omega} \\delta_{tar_i, c}}{\\sum_{i \\in \\Omega} P_{i,c}}\\right),$ (6)\n$R_c (tar, p) = \\log\\left(\\frac{\\sum_{i \\in \\Omega} \\delta_{tar_i, c}}{\\sum_{i \\in \\Omega} \\delta_{[tar_i = c]}}\\right),$ (7)\n$S_c (tar, p) = \\log\\left(\\frac{\\sum_{i \\in \\Omega} (1 - P_{i,c})\\delta_{[tar_i \\neq c]}}{\\sum_{i \\in \\Omega} \\delta_{[tar_i \\neq c]}}\\right),$ (8)\nwhere, $P_{i,c}$ represents the predicted probability that voxel i belongs to class c, and [] denotes the Iverson bracket.\nFor greater generality, the scale loss $L^{scal}$ maximizes the above class-wise metrics:\n$L^{scal} (tar, p) = - \\frac{1}{M} \\sum_{c=1}^M (P_c(tar, p) + R_c(tar, p) + S_c(tar, p)).$ (9)"}, {"title": "B. 3D Traversable Occupancy Annotations", "content": "The RELLIS-OCC dataset with 3D traversable occupancy annotations is generated based on the RELLIS-3D dataset [18], comprising five sequences and a total of 11,201 frames. In this context, sequences numbered 00000 to 00003 are designated as the training set, while sequence 00004 is utilized as the validation set.\n1) Dense Semantic Occupancy Annotations: In the experiments, networks trained with sparse LiDAR points fail to predict sufficiently dense occupancy, highlighting the need for dense occupancy labels. Dense forward-facing labels are generated from the semantically annotated point clouds of the RELLIS-3D dataset, as shown in Fig. 3. The method transforms multi-frame LiDAR point sequences into a unified coordinate system and voxelizes the dense points into grids, including dynamic objects. This process also captures surface occupancy information since LiDAR scans surface points.\nDue to the complex and occluded nature of off-road obstacles, occupancy ground truth within a range of [38.4 m, 51.2 m, 8m] is selected. The prediction range is set to [0 m, 38.4 m] along the X-axis, [-25.6 m, 25.6 m] along the Y-axis, and [-2m, 6m] along the Z-axis. The ground truth is voxelized into a 192 \u00d7 256 \u00d7 40 grid with a voxel size of 0.2 m and data is filtered to retain voxels within the camera's field of view. A remapping of the original categories is then performed to assess their impact on traversability. Voxel data statistics for the RELLIS-OCC dataset, comprising 10 semantic category labels (including void), are presented in Fig. 4."}, {"title": "2) Geometric Feature Parameter Calculation", "content": "A 3D oc-cupancy voxel is constructed by integrating registered dense point clouds. Each voxel $G_j = \\{x_j, y_j,z_j\\}$ is centered at coordinates $(x_j, y_j)$ in the ground map system and has an elevation $z_j$. The neighborhood of each voxel is defined as a circular region $\\Omega$ arround $G_j$, where the point distribution approximates a circle. Each point from the registered dense point cloud is assigned to a voxel, and all points within a voxel form a matrix g.\n$g = \\begin{bmatrix}\nX_1 & X_2 & X_3 & \\cdots & X_n\\\\\nY_1 & Y_2 & Y_3 & \\cdots & Y_n\\\\\nZ_1 & Z_2 & Z_3 & \\cdots & Z_n\n\\end{bmatrix}$ (10)\nStep Height h: The maximum elevation difference between the central voxel $G$ and other voxels $G_j$ within its neighbor-hood $\\Omega$ is calculated to assess the variation in terrain height around the central voxel.\n$h = max (G^z - G_j^z), G_j \\in \\Omega,$ (11)\nwhere, $G^z$ and $G_j^z$ represent the elevations of the central voxel G and the neighboring voxel $G_j$, respectively.\nSlope s: The slope is calculated from the elevation values within the neighborhood. For each voxel G, the centers of all voxels in its neighborhood \u03a9 are fitted to a plane. The angle between the normal vector n of this plane and the unit vector Z = (0,0,1) along the Z-axis of the map coordinate system is defined as the slope.\n$s = arccos \\frac{n \\cdot Z}{||n|| \\cdot ||Z||},$ (12)\nwhere, the value of n is obtained by calculating the covariance matrix using Principal Component Analysis (PCA).\nUnevenness u: The unevenness is estimated by calculating the logarithmic mean squared error (MSE) between the actual elevation of each voxel within the neighborhood \u03a9 and the fitted plane. The unevenness is expressed as:\n$u = log \\left(\\frac{1}{m} \\sum_{i=1}^m [Z_{actual} - (a_0 \\cdot x + a_1 \\cdot y + c)]^2\\right) \\forall i \\in \\Omega,$ (13)\nwhere, $a_0$ and $a_1$ are the coefficients representing the slope in the x and y directions, respectively, and c is the intercept, $Z_{actual}$ represents the actual elevation of the current voxel."}, {"title": "3) Vehicle Obstacle Crossing Condition Analysis", "content": "In off-road conditions, the traversable area of a vehicle is strongly coupled with its geometric passability, as the difficulty of traversing the ground relates to various road geometric fea-tures. An analysis of the failure conditions for vehicle obstacle crossing, based on the principles of vehicle ground mechanics, results in the creation of the Step Mask.\nStep Mask: The failure conditions for vehicle obstacle crossing include body suspension, front-end collision, vertical obstacles, ditches, and lateral tipping. Since the dataset does"}, {"title": "a) Vehicle Vertical Obstacle Overcoming Conditions", "content": "The dimensionless expression for the maximum obstacle height that the front wheels of a 4x4 wheeled vehicle can overcome is:\n$\\frac{h}{r} = 1 - \\frac{\\mu(\\xi + \\sqrt{\\eta^2 - \\eta\\sqrt{1 - 2\\mu + \\eta^2}})}{(1 + \\mu^2) + \\eta^2},$ (14)\n$\\eta = \\frac{1 - \\mu\\xi - \\sqrt{(1 + \\mu^2)}}{\\mu},$ (15)\nwhere, h represents the height of the step obstacle, r represents the wheel radius, \u03bc represents the coefficient of friction, l represents the wheelbase of the vehicle, a represents the horizontal distance from the front axle to the center of gravity of the vehicle."}, {"title": "b) Vehicle Traversing Trenches Conditions", "content": "Given a specific obstacle height h, the corresponding trench width ratio $\\frac{l_d}{D}$ can be determined.\n$\\frac{l_d}{D} = 2 \\sqrt{\\frac{h}{D} (1 - \\frac{h}{D})},$ (16)"}, {"title": "c) Vehicle Crossing Overhang Obstacles Conditions", "content": "For a vehicle to successfully traverse overhang obstacles, the height $h_{obj}$ of the lowest point of the obstacle above the ground must exceed the sum of $h_{pc}$, which is the height of the obstacle point cloud relative to the LiDAR sensor, and $h_{Lid}$, the height of the LiDAR sensor above the ground. When the vehicle encounters an overhang obstacle, the following conditions must be met:\n$h_{obj} > h_{pc} + h_{Lid}.$ (17)"}, {"title": "d) Vehicle Traversing Longitudinal Slopes Conditions", "content": "For traversing longitudinal slopes, the ground platform must satisfy the following condition:\n$\\Theta_{max} > \\alpha,$ (18)\nwhere $\\Theta_{max}$ represents the maximum climbing angle that the vehicle can handle, and \u03b1 represents the current slope angle of the terrain."}, {"title": "4) Traversability Cost Occupancy Annotations", "content": "Rules are established to create an initial mapping from voxel semantic categories to traversability cost, with adjustments made based on the previously computed Step Mask. The Step Mask is determined by evaluating the geometric passability of the vehicle. Voxels not meeting the geometric passability criteria"}, {"title": "C. Architecture of ORD-BKI", "content": "The research presented in [59] introduces the S-BKI model, which infers grid attributes for discrete semantic grid maps to achieve continuous and dense semantic mapping. To enable a fair comparison, the S-BKI model is built upon, utilizing LiDAR point clouds as input. Based on the vehicle's geometric passability, a feasible domain recognition model, ORD-BKI, is developed, which employs Bayesian Kernel Inference for semantic scene completion"}, {"title": "IV. EXPERIMENTS", "content": "A. Quantitative Analysis\nExperiments are conducted on the RELLIS-OCC dataset for the tasks of 3D semantic occupancy prediction and 3D traversability cost estimation, with the results compared to those obtained by the algorithms presented in [59], [62]\u2013[64]. Due to GPU memory constraints, the Monoscene model is trained on 4\u00d7 V100 GPUs, while the other models are trained on 4\u00d73090 GPUs. A consistent batch size of 4 is maintained, and each model is trained for 20 epochs.\nEvaluation Metrics: The RELLIS-OCC dataset comprises both semantic labels and traversability cost annotations. The traversability cost estimation task is framed as a semantic oc-cupancy prediction task, encompassing four categories: lethal, medium-cost, low-cost, and free. Consequently, traditional metrics employed in 3D semantic occupancy prediction tasks are utilized for evaluation. For the scene completion (SC) task, the Intersection over Union (IoU) of occupied voxels, disregarding their semantic classes, is used as the evaluation metric. For the semantic scene completion (SSC) task, the IoU"}, {"title": "1) 3D Semantic Occupancy Prediction", "content": "The results of the 3D semantic occupancy prediction task are presented in Table II. To ensure a fair comparison, all methods are evaluated using consistent perception ranges and voxel sizes. Fig. 4 illustrates the distribution statistics for voxel labels within the dataset. The limited availability of ground truth training data for categories such as person and object poses significant challenges for many models in accurately recognizing these classes. Additionally, the RELLIS-3D dataset, which serves as the foundation for RELLIS-OCC dataset, was collected in a campus environment with very limited overlap between se-quences. This, combined with the rarity of hard-surface scenes in the validation set, leads to generally lower recognition metrics for the hard-surface category.\nDespite these challenges, ORDformer demonstrates superior performance across a wide range of categories. Notably, it achieves state-of-the-art results in identifying tree and bush, with IoU scores of 37.09 and 23.65, respectively. Furthermore, while many models struggle with challenging classes such as mud, ORDformer achieves an IoU of 10.34, significantly higher than competing approaches. Most impressively, ORD-former outperforms other models by over 20% in the scene completion (SC) IoU metric, achieving a score of 50.20 compared to the second-best result of 38.64 from SSCNet-full. These results underscore the effectiveness of ORDformer in integrating multimodal data and performing robust occupancy predictions in complex environments."}, {"title": "2) 3D Traversability Cost Estimation", "content": "The results of the 3D Traversability Cost Estimation task are presented in Ta-ble III. Across all methods, those based on semantic scene completion outperform the ORD-BKI method, which relies on Bayesian Kernel Inference. This underscores the effectiveness of semantic scene completion approaches in enhancing off-road traversable area recognition. Notably, methods using only LiDAR point clouds as input, such as LMSCNet and SSCNet, exhibit inferior performance in the traversability cost estimation task compared to models that incorporate camera input. This suggests that image input is indispensable for accurate traversability cost estimation.\nOur investigation reveals that while methods employing image-to-point cloud semantic mapping are widely used for traversable area recognition tasks [45], [51], [59], there is a lack of quantitative evaluation of their environmental per-ception results. Taking the ORD-BKI method as an exam-ple, a quantitative analysis of its perception performance"}, {"title": "B. Qualitative Analysis", "content": "Visualization of traversability prediction results from dif-ferent models is presented in Fig. 8. By comparing these results with the ground truth, it is evident that ORDformer effectively identifies road traversability challenges in off-road environments. Additionally, the model accurately infers and completes regions containing vegetation and other obstacles.\nAs illustrated in Fig. 9, a pile of rocks appears in front of the platform. Remarkably, the model effectively recognizes the rocky pile as impassable and accurately assesses its spatial distribution. Although the ground truth data designates the edges of the rocky obstacles as lethal regions, it incorrectly assigns labels to the flat areas within the rocky pile. Neverthe-less, the model correctly infers the entire rocky pile as lethal, demonstrating its generalization capabilities.\nIn off-road scenarios, trees are among the most common obstacles that ground unmanned platforms must avoid during trajectory planning. Therefore, accurate detection of trees is essential. Fig. 10 illustrates the detection results of ORDformer for multiple consecutively distributed trees. Different colored bounding boxes indicate the correspondence between various trees in images captured at different times, the ground truth, and traversability estimation results. By accurately identifying the spatial distribution of trees within the effective detection range, ORDformer ensures that ground unmanned platforms can effectively avoid these common obstacles in off-road environments, thereby facilitating safe trajectory planning.\nThe inference results of the ORD-BKI model are visualized, as shown in Fig. 11. The results demonstrate that although the ORD-BKI model effectively identifies traversable ground areas in the two-dimensional plane, its performance in representing the 3D environment is suboptimal. This observation suggests that semantic mapping approaches are significantly influenced by the density of LiDAR point clouds. Such approaches are more suitable for platforms equipped with multiple supple-mentary blind-spot LiDARs. In contrast, unmanned platforms equipped with only a single LiDAR sensor experience limita-tions in representational performance for 3D traversable area recognition tasks due to the low point cloud density within the field of view of the camera.\nGround surfaces become partially exposed due to vehicle passage, resulting in the formation of ruts. These rutted areas typically exhibit sparse weed distribution and lack clear bound-aries with the surrounding densely weed-covered regions. Their characteristics lie between dirt and grass, which poses"}, {"title": "C. Real Vehicle Experiments", "content": "ORDformer is implemented on a real vehicle platform and validated through tests conducted at the Xiaotianshan Professional Outdoor Off-Road Site in Zhangjiakou City, Hebei Province, China. The test vehicle used is a Jeep Grand Cherokee, equipped with a Suteng Innovation 80-line LiDAR and a Senyun Intelligent monocular camera"}, {"title": "V. CONCLUSION", "content": "In this paper, We propose ORDformer, a multimodal method for off-road traversability estimation that utilizes LiDAR point clouds and monocular images to generate dense semantic occupancy predictions. By integrating geometric and seman-tic information, ORDformer enhances environmental feature extraction, enabling more accurate estimation of traversable regions in complex, occluded terrains. Furthermore, we intro-duce the RELLIS-OCC dataset with 3D traversable occupancy annotations, enabling detailed terrain assessment in off-road environments. Extensive experiments validate the superior performance of ORDformer compared to existing approaches, particularly in handling obstacles with diverse geometries and partial occlusion. ORDformer holds significant potential for real-world deployment, as it can be easily adapted to differ-ent vehicle platforms and a range of off-road environments, providing a robust solution for autonomous navigation in challenging conditions.\nFor future work, the proposed traversability estimation method will be integrated with 3D planning algorithms to enable comprehensive 3D trajectory planning in off-road con-ditions. Additionally, recognizing the limitations of existing public datasets, we plan to collect an expanded off-road dataset containing complex obstacle information and evaluate the performance of ORDformer in detecting negative and overhanging obstacles."}]}