{"title": "Towards a Generative Approach for Emotion\nDetection and Reasoning", "authors": ["Ankita Bhaumik", "Tomek Strzalkowski"], "abstract": "Large language models (LLMs) have demonstrated impressive performance in mathematical and commonsense\nreasoning tasks using chain-of-thought (CoT) prompting techniques. But can they perform emotional reasoning by\nconcatenating 'Let's think step-by-step' to the input prompt? In this paper we investigate this question along with\nintroducing a novel approach to zero-shot emotion detection and emotional reasoning using LLMs. Existing state of\nthe art zero-shot approaches rely on textual entailment models to choose the most appropriate emotion label for\nan input text. We argue that this strongly restricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead, we propose framing the problem of\nemotion analysis as a generative question-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion detection question step-by-step. Our\npaper is the first work on using a generative approach to jointly address the tasks of emotion detection and emotional\nreasoning for texts. We evaluate our approach on two popular emotion detection datasets and also release the\nfine-grained emotion labels and explanations for further training and fine-tuning of emotional reasoning systems.", "sections": [{"title": "1. Introduction", "content": "Emotion detection from text has been a long stand-\ning research problem in NLP, and has constantly\nadvanced with the development of newer tools and\ntechnologies. Traditional emotion detection sys-\ntems used lexicons and machine learning algo-\nrithms to predict the emotion label for an input text.\nWith the availability of large annotated datasets,\ntraining deep learning systems and pre-trained\nmodels became popular to address this task. All\nsuch existing methods approach this problem as a\ntext classification task, aiming to select one or more\nemotion labels from a predefined set of labels. How-\never most of these emotion label sets are very basic\nand restrictive. For example, popular Ekman emo-\ntions like happiness and sadness are generalized\nand do not convey much context specific informa-\ntion in any situation. (Coppini et al., 2023) perform\nseveral real-life experiments to demonstrate the\nlimitations of widely used categorical emotion mod-\nels in every day situations. Current state of the art\nzero-shot emotion detection systems prompt textual\nentailment models using the the fixed set of emo-\ntion labels for a dataset (Wenpeng Yin and Roth,\n2019). Thus, the model needs to be prompted for\nevery label to determine the one with the highest\nentailment score. Further, suitable prompts need\nto be designed for every dataset to achieve optimal\nperformance (Basile et al., 2021a; Plaza-del Arco\net al., 2022). This further demonstrates the need\nof a more generalized approach that works across\ndifferent corpora and domains.\nAs humans we naturally follow a reasoning pro-\ncess to analyze the emotions in a situation. This\nprocess is influenced by diverse elements like back-\nground information, external stimuli, commonsense\nknowledge, and personal experiences. Similarly,\nwhile analyzing the emotions in a piece of text we\ninstantaneously perform this reasoning before at-\ntaching a final emotion label to it. Our work is in-\nspired by this idea of generating a suitable emotion\nlabel following by a step-by-step reasoning process\n(Fig. 1). We introduce the novel idea of framing\nthe emotion detection problem as a generative QA\ntask. We leverage the power of LLMs that have\nbeen used popularly to solve arithmetic and com-"}, {"title": "2. Emotional Reasoning", "content": "We introduce the task of generating an emotional\nreasoning along with the predicted emotion label\nfor an input text. This reasoning can be described\nas a step-by-step thought process that would lead\nus to finally predict one emotion label for the input.\nFor example, when someone says \"The weather\nis so gloomy today\", we immediately reason that\ngloomy weather makes people sad or unhappy.\nHowever, for inputs which do not have an explicit\ncause mentioned like \"The new president will stop\nthe construction of the wind turbines.\", we would\nrequire further context and the author's perspective\nto understand the emotions that they are trying to\nexpress. It could either be happiness or disappoint-\nment depending on the context of the situation (Lee\net al., 2023). This further highlights how emotional\nreasoning with context incorporation is significantly\ndifferent from previous work on emotion cause ex-\ntraction (Chen et al., 2010, 2018b; Xia and Ding,\n2019).\nInstead of creating new datasets for the task\nof emotional reasoning, we utilize the ISEAR and\n#Emotional Tweets datasets that are already anno-\ntated for emotion labels. As these datasets have\nshown promising results for fine-tuning on emotion\nclassification, we believe that introducing explana-\ntions alongside these labels would enhance the\npotential for building systems that are more inter-\npretable and explainable. We present updated ver-\nsions of these datasets that include fine-grained\nemotion labels that express the emotions of the\nauthor more closely than the gold labels, and top-3\nexplanations for these labels."}, {"title": "3. Methodology", "content": "We present a generative approach to perform zero-\nshot emotion detection and reasoning over textual\ninput. The overall architecture is illustrated in Fig.\n2. First, we perform context generation to retrieve\nnecessary background context for the specific do-\nmain and dataset. Next, we use this context along\nwith a QA prompt to either detect the most proba-\nble emotion from a fixed set of labels or generate\na list of suitable emotion words and a step-by-step\nreasoning for each label. For this paper, we focus\nonly on a single step of reasoning for each input."}, {"title": "3.1. Context Generation", "content": "The context generation step uses few-shot prompt-"}, {"title": "3.2. Emotion Generation", "content": "Due to the huge success of LLMs in various QA\ntasks with CoT, we frame the emotion detection\ntask in a similar format. We use the generated\ncontext as a part of this prompt to provide the LLM\nadditional background knowledge about the input\ntext and the dataset. The prompt Q is constructed\nfor emotion detection over a fixed set of labels E\nand for open-ended generations.\nPrompt(Q) =\nQ: Given the context, what emotions\ndoes the author of the input text\nfeel and why?\nGive me the reason followed by the\nfinal emotion label.\nContext: {context}\nInput: {input}\nA: Let's think step-by-step.\nTo predict the emotion of an input text t over a\nfinite set of labels E, we prompt the LLM m to obtain\nthe probability of every $e \\in E$. We complete the\nprompt Q using each context $c_i \\in C$ to generate\nthe most probable emotion label $\\hat{e_i}$:\n$\\hat{e_i} = \\arg \\max_{e\\in E} p_m(e | t, c_i)$\n(1)\nWe obtain a set of n emotion predictions for every\ncontext as $\\hat{E} = \\{(C_1, \\hat{e_1}), (C_2, \\hat{e_2})... (C_n, \\hat{e_n})\\}$. Follow-\ning earlier work on self-consistency (Wang et al.,\n2023), we apply a marginalization over $\\hat{E}$ to select\nthe most consistent emotion label e using majority\nvoting:\n$e = \\arg \\max_e \\sum_{i=1}^n \\mathbb{1}(\\hat{e_i} = e)$\n(2)\nTo generate other suitable emotion words and\ncorresponding explanations, we simply use nucleus\nsampling to complete the emotion QA prompt. Con-\nditioning the generated output with the addition of\ncontext helps us to generate a suitable reasoning"}, {"title": "3.3. Answer Selection", "content": "In the final step we design an answer selection al-\ngorithm to retrieve all emotion words and the top-k\nemotional reasoning answers from the q generated\noutputs. Due to the open ended nature of LLM\ngenerations, this step is essential in eliminating\nany arbitrary or incomplete outputs that may have\nbeen generated. We only select outputs that have\nbeen generated with a very high confidence. To\nimplement this idea we use a soft majority voting\ntechnique that finds the most common outputs us-\ning semantic similarity rather than relying solely\non exact matches. We use BERTScore, a popular\nevaluation metric for text generation to measure se-\nmantic similarity between the output texts (Zhang\net al., 2019b). This uses contextual embeddings\nto represent the tokens and compute agreement\nusing cosine similarity. Further, we use an external\ncorpus of emotion/feeling words\u00b9 to extract only\nemotion words from the top-k generations."}, {"title": "4. Experiments", "content": "To evaluate our approach we choose datasets from\ntwo different domains that have been popularly\nused for existing works on zero-shot emotion de-\ntection.\nISEAR (International Survey on Emotion An-\ntecedents and Reactions) contains 7,665 texts\nwhere participants reported situations when they\nfelt the emotions of anger, disgust, fear, joy,\nsadness, shame and guilt (Scherer and Wallbott,\nhttps://www.berkeleywellbeing.com/\nlist-of-emotions.html\n1994).\n#Emotional Tweets consists of 21,051 tweets that\nwere collected using hashtags corresponding to the\nemotion labels anger, disgust, fear, happy, sadness,\nand surprise (Mohammad, 2012)."}, {"title": "4.2. Implementation", "content": "We use Flan-T5 base (250M) and Flan-T5 xxl (11B)\nas our text generation models. The few-shot prompt\nP is composed of k = 5 hand-written samples. For\ncontext generation, we use nucleus sampling with p\nset to 0.9 to generate n = 10 contexts. To generate\nan open-ended emotion label/reasoning we again\nuse nucleus sampling with p as to 0.9 to generate\n10 explanations for every context. The maximum\nnumber of tokens generated at every step can be\n60. Our experiments have been performed on 2\nNVIDIA A100 GPUs."}, {"title": "4.3. Baselines", "content": "Standard Zero-shot Prompting: We use a\nstraightforward prompt that is used for most text\nclassification tasks, and greedy decoding generate\nthe predicted emotion label:\nThis is an emotion classification\ntask.\nText: {input}\nEmotion:\nPrompting using CoT: We append Let's think\nstep-by-step to the standard QA prompt and use\ngreedy decoding to predict the emotion label:\nQ: What emotion is expressed by the\nauthor in the input text? Let's\nthink step-by-step\nText: {input}\nEmotion:\nState-of-the-art Zero-shot Emotion Detection:\nThe SOTA zero-shot classifiers use Natural lan-"}, {"title": "5. Results", "content": "We report the evaluation metrics for zero-shot emo-\ntion detection over the fixed set of emotion labels\nin both the datasets. Though the novel contribu-\ntion of our method is to generate an open ended\nset of emotion labels and explanations, we report\nthese scores to highlight that our approach would\nbe effective on all three emotion analysis tasks. We\nobserve that using simple prompting techniques or\nCoT results in very poor performance in the task\nof emotion classification. However, using a QA\nprompt like ours (EmoGen) boosts the performance\nand thus can be used in place of existing textual\nentailment models for emotion detection."}, {"title": "5.2. Emotional Reasoning Dataset", "content": "We generate updated versions of both the datasets\nto include the top-3 emotion labels beyond the pre-\ndefined list of labels in the gold datasets. We also\ninclude an emotional reasoning for each label that\nreplicates the chain-of-thought that the model used\nto generate the final label. We plot the frequen-\ncies of emotion labels in 500 samples of the ISEAR\ndataset to compare the distributions of gold emo-\ntion labels and generated labels. The distributions\nin Fig. 4 illustrate the breakdown of certain emotion\nlabels into more detailed sub-labels. For example,\nsadness has been further categorized into upset,\nsadness, disappointment and regret, shame has\nbeen categorized into shame and embarrassed.\nThis breakdown into more fine-grained emotion la-\nbels helps us to move away from a fixed set of emo-"}, {"title": "5.3. Significance of Context Generation", "content": "We analyze the complete set of generated outputs\nfor the ISEAR dataset before extracting the top-3\nemotion labels from them. We carry out this experi-\nment to demonstrate the significance of the context\ngeneration step in our approach. We observed\nseveral instances where different contexts result in\ndistinct CoT reasonings and, consequently, result in\ndifferent emotion labels (Fig. 5). This observation\naligns with our initial claim that context plays an im-\nportant part in generating the appropriate emotion\nlabel for a particular dataset or domain.\nWe repeat the experiment of emotion reasoning\ngeneration without the incorporation of context. We\nobserve a drastic decrease in the quality of the CoT\nreasoning process. The model mostly replicates\ntext to produce a reasoning from the input in the\nabsence of an intermediate context."}, {"title": "5.4. Error Analysis", "content": "We analyze the 20 examples that were rated to have\nincorrect emotional reasoning. Errors in reasoning\noccur due to three major reasons: (1) the LLM is\nunable to capture a multiple clauses in the input text,\n(2) the LLM outputs the emotion label only with no\nexplanation, (3) the input text contains some other\nemotion word which confuses the LLM. Examples\nof errors have been listed in Table 3."}, {"title": "6. Related Work", "content": ""}, {"title": "6.1. Emotion Detection from Text", "content": "Current state-of-the-art emotion detection systems\ncan be largely categorized into two categories: do-\nmain specific fine-tuned models (Ma et al., 2019;\nChiorrini et al., 2021) and zero-shot systems that\nrely on sentence embeddings and textual entail-\nment models (Zhang et al., 2019a; Olah et al., 2021;\nChen et al., 2022). In spite of impressive perfor-\nmance across benchmarks these systems have\nlimitations. They are extremely rigid when it comes\nto adapting to a new set of emotion labels or do-\nmains. Moreover, they are black box systems that\nonly produce a set of emotion scores for an input"}, {"title": "6.2. Reasoning using LLMs", "content": "Recently, pre-trained LLMs like GPT-3 (Brown et al.,\n2020), OPT (Zhang et al., 2022), T5 (Raffel et al.,\n2020), ChatGPT (Ouyang et al., 2022) have be-\ncome extremely popular for various natural lan-\nguage processing tasks. With the introduction of\nChain-of-Thought prompting techniques (Wei et al.,\n2022; Wang et al., 2023), the reasoning capabilities\nof these LLMs have been explored for common-\nsense, arithmetic and science question answering\ntasks (Rajani et al., 2019; Lin et al., 2020; Lu et al.,\n2022; Liu et al., 2022; Fei et al., 2023). We rely\non these works to explore the task of emotional\nreasoning using LLMs."}, {"title": "6.3. Context Generation for QA Tasks", "content": "Several recent works have also shown the impor-\ntance of incorporating LLM generated text into\nprompts for QA tasks. Augmenting the input ques-\ntion with knowledge generated by the LLM has im-\nproved the performance of commonsense reason-\ning tasks (Liu et al., 2022). Generation of context\nto aid in closed-book QA has outperformed exist-\ning fine-tuned models (Su et al., 2023). We follow\nthese works to show that the incorporation of con-text is essential for the task of emotional reasoning\nand adapt it to multiple domains."}, {"title": "7. Conclusion", "content": "In this paper, we present a two-step generative\napproach to the task of emotion analysis. We in-\ntroduce the task of step-by-step emotional reason-\ning to explain the chain-of-thought leading to the\nemotion label. We use existing emotion detection\ndatasets to evaluate our approach and will release\nthe datasets updated with additional emotion labels\nand their explanations. Future work will include the\nuse of LLMs in multi-step emotional CoT reasoning\nfor longer texts and dialogues. This would be a step\ntowards building more human-like empathetic con-\nversational assistants with potential applications\nacross various domains."}, {"title": "8. Acknowledgements", "content": "Details of research grants funding this work have\nbeen withheld to maintain anonymity. It will be\nincluded in the camera ready version."}, {"title": "9. Limitations and Ethical\nConsiderations", "content": "In this work, we introduce a novel generative ap-\nproach to emotion detection from text and compare\nagainst popular zero-shot methods and prompting\ntechniques. We do not compare against existing\nfine-tuned emotion detection models owing to the\nfact that they work only over fixed sets of emotion\nlabels. Our work focuses more on a generalized\napproach to the task that can be adapted to specific\ndomains using the context generation step. It is cru-\ncial to manually write the few-shot prompt for every\ndataset after careful consideration of the dataset\ncreation process. It is to be noted that throughout\nour work we experiment with multiple prompts and"}]}