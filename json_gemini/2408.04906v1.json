{"title": "Towards a Generative Approach for Emotion\nDetection and Reasoning", "authors": ["Ankita Bhaumik", "Tomek Strzalkowski"], "abstract": "Large language models (LLMs) have demonstrated impressive performance in mathematical and commonsense\nreasoning tasks using chain-of-thought (CoT) prompting techniques. But can they perform emotional reasoning by\nconcatenating 'Let's think step-by-step' to the input prompt? In this paper we investigate this question along with\nintroducing a novel approach to zero-shot emotion detection and emotional reasoning using LLMs. Existing state of\nthe art zero-shot approaches rely on textual entailment models to choose the most appropriate emotion label for\nan input text. We argue that this strongly restricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead, we propose framing the problem of\nemotion analysis as a generative question-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion detection question step-by-step. Our\npaper is the first work on using a generative approach to jointly address the tasks of emotion detection and emotional\nreasoning for texts. We evaluate our approach on two popular emotion detection datasets and also release the\nfine-grained emotion labels and explanations for further training and fine-tuning of emotional reasoning systems.", "sections": [{"title": "1. Introduction", "content": "Emotion detection from text has been a long stand-\ning research problem in NLP, and has constantly\nadvanced with the development of newer tools and\ntechnologies. Traditional emotion detection sys-\ntems used lexicons and machine learning algo-\nrithms to predict the emotion label for an input text.\nWith the availability of large annotated datasets,\ntraining deep learning systems and pre-trained\nmodels became popular to address this task. All\nsuch existing methods approach this problem as a\ntext classification task, aiming to select one or more\nemotion labels from a predefined set of labels. How-\never most of these emotion label sets are very basic\nand restrictive. For example, popular Ekman emo-\ntions like happiness and sadness are generalized\nand do not convey much context specific informa-\ntion in any situation. (Coppini et al., 2023) perform\nseveral real-life experiments to demonstrate the\nlimitations of widely used categorical emotion mod-\nels in every day situations. Current state of the art\nzero-shot emotion detection systems prompt textual\nentailment models using the the fixed set of emo-\ntion labels for a dataset (Wenpeng Yin and Roth,\n2019). Thus, the model needs to be prompted for\nevery label to determine the one with the highest\nentailment score. Further, suitable prompts need\nto be designed for every dataset to achieve optimal\nperformance (Basile et al., 2021a; Plaza-del Arco\net al., 2022). This further demonstrates the need\nof a more generalized approach that works across\ndifferent corpora and domains.\nAs humans we naturally follow a reasoning pro-\ncess to analyze the emotions in a situation. This\nprocess is influenced by diverse elements like back-\nground information, external stimuli, commonsense\nknowledge, and personal experiences. Similarly,\nwhile analyzing the emotions in a piece of text we\ninstantaneously perform this reasoning before at-\ntaching a final emotion label to it. Our work is in-\nspired by this idea of generating a suitable emotion\nlabel following by a step-by-step reasoning process\n(Fig. 1). We introduce the novel idea of framing\nthe emotion detection problem as a generative QA\ntask. We leverage the power of LLMs that have\nbeen used popularly to solve arithmetic and com-"}, {"title": "2. Emotional Reasoning", "content": "We introduce the task of generating an emotional\nreasoning along with the predicted emotion label\nfor an input text. This reasoning can be described\nas a step-by-step thought process that would lead\nus to finally predict one emotion label for the input.\nFor example, when someone says \"The weather\nis so gloomy today\", we immediately reason that\ngloomy weather makes people sad or unhappy.\nHowever, for inputs which do not have an explicit\ncause mentioned like \"The new president will stop\nthe construction of the wind turbines.\", we would\nrequire further context and the author's perspective\nto understand the emotions that they are trying to\nexpress. It could either be happiness or disappoint-\nment depending on the context of the situation (Lee\net al., 2023). This further highlights how emotional\nreasoning with context incorporation is significantly\ndifferent from previous work on emotion cause ex-\ntraction (Chen et al., 2010, 2018b; Xia and Ding,\n2019).\nInstead of creating new datasets for the task\nof emotional reasoning, we utilize the ISEAR and\n#Emotional Tweets datasets that are already anno-\ntated for emotion labels. As these datasets have\nshown promising results for fine-tuning on emotion\nclassification, we believe that introducing explana-\ntions alongside these labels would enhance the\npotential for building systems that are more inter-\npretable and explainable. We present updated ver-\nsions of these datasets that include fine-grained\nemotion labels that express the emotions of the\nauthor more closely than the gold labels, and top-3\nexplanations for these labels."}, {"title": "3. Methodology", "content": "We present a generative approach to perform zero-\nshot emotion detection and reasoning over textual\ninput. The overall architecture is illustrated in Fig.\n2. First, we perform context generation to retrieve\nnecessary background context for the specific do-\nmain and dataset. Next, we use this context along\nwith a QA prompt to either detect the most proba-\nble emotion from a fixed set of labels or generate\na list of suitable emotion words and a step-by-step\nreasoning for each label. For this paper, we focus\nonly on a single step of reasoning for each input."}, {"title": "3.1. Context Generation", "content": "The context generation step uses few-shot prompt-\ning to extract the relevant context from a LLM. We"}, {"title": "3.2. Emotion Generation", "content": "Due to the huge success of LLMs in various QA\ntasks with CoT, we frame the emotion detection\ntask in a similar format. We use the generated\ncontext as a part of this prompt to provide the LLM\nadditional background knowledge about the input\ntext and the dataset. The prompt Q is constructed\nfor emotion detection over a fixed set of labels E\nand for open-ended generations.\nTo predict the emotion of an input text t over a\nfini"}]}