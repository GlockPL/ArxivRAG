{"title": "Uncertainty Quantification in Continual Open-World Learning", "authors": ["Amanda Rios", "Ibrahima Ndiour", "Parual Datta", "Jerry Sydir", "Omesh Tickoo", "Nilesh Ahuja"], "abstract": "Al deployed in the real-world should be capable of au-tonomously adapting to novelties encountered after deployment. Yet, in the field of continual learning, the reliance on novelty and labeling oracles is commonplace albeit unrealistic. This paper addresses a challenging and under-explored problem: a deployed AI agent that continuously encounters unlabeled data - which may include both unseen samples of known classes and samples from novel (unknown) classes - and must adapt to it continuously. To tackle this challenge, we propose our method COUQ \"Continual Open-world Uncertainty Quantification\", an iterative uncertainty estimation algorithm tailored for learning in generalized continual open-world multi-class settings. We rigorously apply and evaluate COUQ on key sub-tasks in the Continual Open-World: continual novelty detection, uncertainty guided active learning, and uncertainty guided pseudo-labeling for semi-supervised CL. We demonstrate the effectiveness of our method across multiple datasets, ablations, backbones and performance superior to state-of-the-art. We will release our code upon acceptance.", "sections": [{"title": "1. Introduction", "content": "Real-world AI systems frequently face evolving data distributions due to changes in operating conditions and the emergence of new classes after deployment. To ensure a robust response, AI systems should ideally be able to detect these novelties and continuously learn from them, while minimizing computing and labeling costs. Early research on continual learning (CL) focused on the important problem of catastrophic forgetting [17, 34] but relied on a so-called 'Oracle' for two critical functions: (i) identifying novel test samples and (ii) providing labels for these samples. While beneficial for advancing initial CL research, the assumption of an omniscient oracle is unrealistic for real-world applications. In practice, adaptive AI systems should be capable of automatically identifying novelties, a challenging task known as novelty detection or out-of-distribution (OOD) detection. Further, learning from novelties usually requires labels for the novel samples (e.g. open-world classification [35]); yet, annotation of all such samples is expensive and impractical. Therefore, for data-efficient model updates, it is advantageous to label only a a small subset of informative data samples judiciously chosen from the larger pool of novel samples; a task known as sample selection in active learning [11, 29, 38, 45, 47].\nAlthough both these problems of novelty-detection and active sample-selection have been widely explored, most research has been in non-continual settings. There is extensive research on cost-effective active labeling [19, 44, 52] for conventional non-continual training, but surprisingly this has found little adoption in CL models [1, 5, 49]. Moreover, solutions for unsupervised and semi-supervised CL are scarce [7, 10, 25] and often operate with a very strong oracle assumption: that in unlabeled input, past classes do not appear in conjunction with newly introduced classes - bypassing the need for novelty detection. For such methods, removing the oracle causes significant performance degradation. Methods under Generalized Category Discovery (GCD) [31, 48] do address the scenario in which both old and new classes co-occur in the test data. However, they are designed for single-task setups where the entire unlabeled dataset is presented to the model at once for categorization, rather than in a continual, class-incremental manner. These solutions are ill-suited for (post-deployment) continual learning as they assume full data availability (labeled and unlabeled) and perform expensive full re-trainings poorly scalable to CL. Similarly, most solutions for novelty or out-of-distribution (OOD) detection [23, 28, 30, 37, 50] were developed for and evaluated against a single fixed binary partition of known (old) versus novel classes and not on continual splits. Such conventional OOD models are not designed to continually integrate and learn from the detected novel data.\nOverall, both novelty detection and active learning depend on the availability of reliable, high-quality uncertainty estimates, see figure 1. A variety of uncertainty quantification techniques have been studied: softmax probability and its temperature-scaled variants [23, 30]; predictive uncertainty from Bayesian Neural Networks [18]; feature-based Mahalanobis distances [28] and its variants [2]. These methods work well in fixed settings where the model is trained once. However, in CL settings where the model parameters are continually updated, the quality of uncertainty estimates deteriorates as knowledge from novel tasks and classes is incrementally integrated. Such estimates, if used either for novelty detection or for active sample selection, will lead to poor prediction of novelties and/or a poor choice of samples selected for CL. Furthermore, errors tend to accumulate as new tasks are encountered, resulting in progressively poorer performance over time. A recent model, incDFM [42], attempted to address this problem by proposing a solution to continual novelty detection and integrated it into the broader pipeline of unsupervised class incremental learning. However, the number of novel classes introduced at each task was limited to one, so that any detected novelty could then be trivially labeled as belonging to a single novel class. The more general scenario of class increment learning with multiple novel classes was not addressed, which introduces significant new challenges: the number of new classes is not known a priori, and error-propagation is exacerbated since a novel class sample may not only be misidentified as an old-class sample (the only type of wrong prediction possible in incDFM), but could also be labeled incorrectly.\nContribution: We present an iterative uncertainty estimation technique suited for learning in an open-world, continual multi-class incremental environment wherein, at each continual task, a model is exposed to a mix of data from both known and an arbitrary number of unknown classes."}, {"title": "2. Problem Statement - Background", "content": "2.1. Problem Setting: Consider a deep neural network model \\(y = \\Phi(x)\\) expected to learn from a sequence of continual tasks. \\(\\Phi\\) can be partitioned into a backbone network that produces features \\(u = g(x)\\) followed by a classifier \\(y = f(u)\\) operating on those features to produce the final prediction y, i.e. \\(\\Phi = f \\circ g(x)\\). At each continual task, t, the model is presented with an initially unlabeled set of samples \\(U^{(t)}\\) comprising a mixture of unseen samples of \u201cold\u201d/learned classes \\(U_{old}^{(t)}\\) and unseen samples from new/unlearned classes \\(U_{new}^{(t)}\\),\n\\(U^{(t)} = U_{old}^{(t)} \\cup U_{new}^{(t)}\\), where\n\\(U_{old}^{(t)} = \\{X | X \\sim \\bigcup_{k=1}^{t-1} D_k\\}, U_{new}^{(t)} = \\{X | X \\sim D_t\\},\\)\nwhere \\(D_t\\) comprises data from the set of new classes \\(C_{new}^t = \\{c_i\\}, i = 1, ..., N_t\\), introduced at task t. For notational convenience, we denote \\(C_{old} = \\bigcup_{k=0}^{t-1} C_{new}^k\\) which is the collection of classes observed up to and including task t - 1. Note that samples in \\(U_{old}^{(t)}\\) belong to previous classes but are \"unseen\", i.e., were never used in training during prior tasks. The goal is to accurately differentiate between \\(U_{old}^{(t)}\\) and \\(U_{new}^{(t)}\\) and simultaneously learn to process/classify the novel classes present in \\(U_{new}^{(t)}\\). This implies gaining the ability to classify/distinguish among different novel classes in \\(U_{new}^{(t)}\\). To accomplish this, we propose a multi-class continual uncertainty quantification algorithm that is able to generate uncertainty scores per detected novel class. The uncertainty scores can also be used for (i) selecting informative novel samples for active labeling, and (ii) selecting confidently novel samples for unsupervised pseudo-labeling."}, {"title": "2.2. Other Relevant Solutions", "content": "The described problem setting is complex and can be broken down into a series of sub-problems (and sub-solutions) that will be outlined below. Some of the (sub-)solutions will be used as comparison methods to benchmark our approach in \u00a73.3."}, {"title": "2.2.1. Learning from Novelties", "content": "Early works on \"Open World Classification\" [16, 46] were essentially the same as OOD detection and did not focus on learning from the detected novel data. More recent works on \"Category Discovery\" attempt to do this [20] by estimating the number of novel classes and assign each novel sample to the appropriate novel class. They assume, however, that the unlabeled set contains only novel-class data (\\(U = U_{new}\\)), thus not requiring novelty detection. Work on \u201cGeneralized Category Discovery\u201d (GCD) [48] removes this restriction and permit U to include both known and novel classes. While this represents a significant advancement, it remains a challenging problem that, to the best of our knowledge, has only been addressed in non-continual settings [31, 48], without the added complexities inherent to continual learning."}, {"title": "2.2.2. Continual Learning", "content": "Most CL approaches that focus on mitigating catastrophic forgetting are fully supervised and assume access to fully labeled data streams [15, 26, 34, 36, 40, 41, 51]. More recent approaches, though, do not make this assumption and explore unsupervised, semi-supervised, and few-shot continual learning methods [7, 10, 25]. However, these too assume that the incoming data contains only novel classes (\\(U^{(t)} = U_{new}^{(t)}\\)), thus bypassing the need for novelty detection and avoiding error propagation. In this category, we compare to CCIC [9], a semi-supervised CL method that leverages the MixMatch technique [8] to learn more efficiently from both labeled and unlabeled samples. As shown in \u00a73.3, approaches like CCIC scale poorly to our generalized setting where the incoming data can include both old and novel classes. Lastly, continual novelty detection (CND) remains under-explored [4], with a notable exception of incDFM [42], which constrained \\(U_{new}^{(t)}\\) to have only one new class at a time. We compare our approach to incDFM in the results \u00a73.3 and show that it does not generalize well when \\(U_{new}^{(t)}\\) may contain an arbitrary number of novel classes."}, {"title": "2.2.3. Active Learning", "content": "Active learning aims to learn from a small set of informative data samples judiciously chosen from a larger unlabeled dataset. Diverse strategies have been used for selecting the samples based on uncertainty [29, 47] or diversity [11]. We refer to [38, 45] for exhaustive surveys of the methods, which overwhelmingly operate in an offline fashion. Defining an effective AL heuristic in the generalized setting of \\(U^{(t)}\\) is challenging [49] as will be shown in 3.3. An early attempt was made by the authors of GBCL [6]. Their method integrates AL with few-shot continual learning by selecting samples that are most distant from a continually updated Gaussian mixture model of all old classes. We compare to GBCL in \u00a74.3."}, {"title": "3. Our Approach: COUQ", "content": "Choice of elemental uncertainty metric: We refer to our method as COntinual Uncertainty Quantification (COUQ). Our method is agnostic to the choice of the underlying uncertainty measure used within our algorithm, as long as it can reliably estimate uncertainty per novel class or per old class. However, this is not an easy feat since many existing static novelty detection approaches make for very poor per-class uncertainty estimators. In our current formulation, we leverage the feature reconstruction error (FRE) metric introduced in [32], which has been shown to effectively estimate per-class uncertainty in the non-continual setting. For each in-distribution class, FRE learns a PCA (principal component analysis) transform \\(\\mathcal{T}_m\\) that maps high-dimensional features u from a pre-trained deep-neural-network backbone g(x) onto lower-dimensional subspaces. During inference, a test-feature \\(u = g(x)\\) is first transformed into a lower-dimensional subspace by applying \\(\\mathcal{T}_m\\) and then re-projected back into the original higher dimensional space via the inverse \\(\\mathcal{T}_m^\\dagger\\). The FRE measure is calculated as the \\(l_2\\) norm of the difference between the original and reconstructed vectors:\n\\(FRE_m(u) = ||f(x) - (\\mathcal{T}_m^\\dagger \\circ \\mathcal{T}_m)u||_2.\\)\nIntuitively, \\(FRE_m\\) measures the distance of a test-feature to the distribution of features from class m. If a sample does not belong to the same distribution as that mth class, it will usually result in a large reconstruction score \\(FRE_m\\). FRE is particularly well suited for continual settings since for each new class an additional PCA transform can be trained without disturbing the ones learnt for previous classes."}, {"title": "3.1. Algorithmic Steps", "content": "Initial training and deployment: At the outset (i.e. task t = 0), we assume that the main model \\(\\Phi(x)\\) has been trained to classify among an initial fixed set of classes \\(C_{new}^0\\) (following notation from Sec.2). An initial set of PCA transforms \\(\\{\\mathcal{T}_{0}\\}_{m \\in C_{new}^0}\\) have also been learnt.\nContinual Learning and Adaption: At any task t, t > 0, as unlabeled data arrives, COUQ will follow an iterative procedure to derive uncertainty scores that can be used to detect novelties and classify them if present. Classification of novelties can be performed in an unsupervised manner using clustering techniques such as K-means, or in a semi-supervised manner via a combination of active and Pseudo labeling. We do not prescribe a particular approach; rather, we show that our uncertainty estimation algorithm can work well with both semi-supervised and unsupervised approaches. In the former case, the selection of samples for active labeling itself can be guided by the uncertainty score from COUQ. Finally, note that this iterative procedure is an inner-loop iteration (indexed by i) employed at each task, which is different from the outer-loop iteration over tasks (indexed by t). Each iteration i utilizes the scores and novelty class predictions from the previous iteration to progressively obtain better uncertainty scores \\(S^{t,i}(u)\\). To simplify the notation, we index only w.r.t iteration i, with the understanding that uncertainties are recalculated at every task.\ni = 0: Initializing COUQ. At the first inner-loop iteration, uncertainty scores \\(S^0(u)\\) are simply\n\\(S^0(u) = \\min_{j \\in C_{old}^t} FRE_j^0(u),\\)\n\\(FRE^i\\) indicates scores at the \\(i^{th}\\) iteration of the tth task. These reflect the distance to the detected past classes encountered till t - 1. This score can be used for novelty detection by noting that samples with high \\(S^0(u)\\) values have a significant probability of being novel. Once confidently novel samples have been identified, these require a 'novelty mapper' \\(M^i(u)\\) to assign labels or IDs for the subsequent iterations. This mapper can be obtained from an unsupervised approach such as K-means clustering by selecting the id of the closest cluster centroid. Alternately, we can choose a small number \\(b_0\\) samples with high \\(S^0(u)\\) values for active querying and train a pseudo-labeler (such as a small MLP) on these \\(b_0\\) samples to predict the class-ids for the remaining confidently novel samples. Either way, the novel classes that are identified are appended to \\(C_{new}^t\\) and are used for computing initial estimates of per-class PCA transforms for the new classes \\(\\{\\mathcal{T}_{i=0}^{t}\\}_{m \\in C_{new}^t}\\).\ni > 0: Iterative Training. For all subsequent iterations of the same task, COUQ computes a per-novel-class uncertainty score, relying on the previous iteration's mapper novel class-id predictions and the corresponding previous iterations per-novel-class PCA transforms \\(\\{\\mathcal{T}_m^{t,i-1}\\}\\). The overall multiclass uncertainty score for a given unlabeled sample u is defined in eq 3. A novel class-id pseudo-label m is predicted by the novelty mapper from the previous iteration \\(M^{i-1}(u)\\), and we select the corresponding PCA transform \\(\\mathcal{T}_m^{t,i-1}\\) to calculate the score.\n\\(S^i(u) = \\min \\{\\min_{j \\in C_{old}^{t-1}}FRE_j^i(u), FRE_m^{t,i-1}(u) \\} ; m = M^{i-1,t}(u) \\in C_{new}^t \\)\nThis score can be used to broadly categorize samples in U(t) as follows:\n1.  Identify as novel with high-confidence: These are samples with the highest score values, which will occur for a high numerator relative to the denominator. A high value of numerator implies large distance from previously seen classes \\(C_{old}^{t-1}\\), while a low value of the denominator implies low distance from novel class m. Such a sample likely belongs to \\(U_{new}^{(t)}\\) and is a strong candidate to be pseudo-labeled as class m.\n2.  Identify as old-class with high-confidence: This is the opposite of the previous case: low score values corresponding to low numerator (low distance w.r.t \\(C_{old}^{t-1}\\)) and high numerator value (high-distance from the novel class m). Such a sample likely belongs to \\(U_{old}^{(t)}\\) and is not needed any further for novelty detection (since we are assuming no distribution-shift for old classes).\n3.  Ambiguous: Samples for which the score is neither definitively high or low. These could be old-class samples having relatively high scores, or novel-class samples having relatively low scores. Owing to this ambiguity, a clear determination cannot be made, and hence these samples would most benefit from active labeling.\nWe iteratively improve the quality of our multiclass uncertainty measure, \\(S^i(u)\\), via a pseudo-labeling and/or active-labeling at each iteration. Using the novel class-id predictions from iteration i - 1 (un-supervised or semi-supervised), we separate and sort the scores per predicted novel class m \u2208 \\(C_{new}^t\\). We select the topmost a percent per predicted-class. These are the samples predicted as novel class m with highest-confidence. If active querying is also used, then we additionally select \\(b_i\\) most ambiguous samples per predicted novel class to actively label, so long as the tiny active label budget has not been exhausted. Together, the accumulated selected active or pseudo-labeled samples are used for (i) computing all novel PCA transformations \\(\\{\\mathcal{T}_m^{t,i}\\}\\), (ii) re-training the novelty mapper - either the unsupervised K-means or the small MLP pseudo-labeler - in preparation for the next iteration, and (iii) updating the main model \\(\\Phi\\) classifier (or only the classifier f if using a frozen backbone g). Further details on COUQ iterations, such as stopping criteria, as well as a detailed Algorithm flow box are included in supplementary."}, {"title": "3.2. Application to Continual Novelty Detection", "content": "As described, uncertainty estimation in COUQ can be used directly for novelty detection. Importantly, because the addition of new PCA transforms (per detected novel class) does not impact those already stored in memory, novelty detection performance does not significantly degrade, e.g. does not \"catastrophically forget\". Furthermore, the use of an iterative approach results in higher-quality and more consistent uncertainty estimates than other one-shot approaches, thus also minimizing continual error propagation."}, {"title": "3.3. Applications to Active Sample Selection", "content": "We describe next how the uncertainty score from Eq. (2) can be used for efficient active annotation. The samples thus labeled can be used not only for improving the quality of the multiclass uncertainty measure as desribed above, but also for updating the weights of a downstream continual classifier f. One possible sample selection strategy would be to prioritize novel samples for annotation. This could be done by selecting the most confident novel samples for active annotation. However, we find that including samples of ambiguous novelty, scores \\(S^i(u)\\) which are neither too high or too low, is more informative. We set our default AL strategy to pick 1:1 between ambiguous and confident novel samples per detected novel class. Details of how ambiguous samples are determined can be found in the supplementary. In \u00a74.3.1, we compare various sample selection strategies using different uncertainty scores and show that our proposed AL strategy using \\(S^2(u)\\) uncertainty scores significantly outperform others."}, {"title": "4. Experiments", "content": "We apply our uncertainty estimation method COUQ to a general open-world continual learning setting defined in \u00a72. We demonstrate its utility and effectiveness on the following sub-tasks therein: (1) Continual Novelty Detection, results in \u00a74.2; (2) Continual Active Learning, results in \u00a74.3."}, {"title": "4.1. Experimental Setup", "content": "Model: The backbone g used in our method and all baselines is a frozen, pre-trained deep model. This is a common practice in transfer learning in CL [41, 42], and is theoretically based on the principle that low-level visual features obtained from a frozen model are thought to be task non-specific and do not need to be constantly re-learned during CL tasks. We tested over 3 different pretrained foundation backbones: ResNet50 [21] pre-trained on ImageNet1K via unsupervised SwAV [13] and ViTs16 or ViTb16 [3] pre-trained on Imagenet1K via unsupervised DINO [14]. The features u are used for computation of the uncertainty scores in Eqs. (2) and (3) and also inputted to the classifier f for eventual continual class-prediction. f is implemented as a one hidden-layer perceptron (of size 4096).\nAs described in \u00a73.1, we test two variants of the novelty mapper \\(M^i(u)\\) to assign class-ids to confidently novel samples: (1) a K-means clustering in a fully unsupervised scenario that is trained to cluster the confident novel samples, and (2) a fully-connected layer (different from the main classifier f) on top of the frozen backbone in the semi-supervised scenario. Here, the mapper is trained with cross-entropy loss on the few actively labeled samples in addition to the confident novel samples pseudolabeled samples from the previous inner-loop iteration.\nFinally, while COUQ is agnostic to which CL method is used to prevent catastrophic forgetting of the continual classifier, we showcase results employing a conventional CL technique termed \u201cExperience Replay\" (ER) [12, 43]. In ER methods, a limited number of exemplars from the old-classes must be stored in a buffer of fixed size B. These are used in conjunction with the novel samples to train the classifier without catastrophic forgetting. We set the B to 5000 for Im21K-OOD and Places, and 2500 for Cifar100, Eurosat, and Plants. Further details about COUQ being used with ER are shared in the supplementary.\nExperiments: Overall, we test Continual Novelty Detection and Continual Open-World Classification on 5 diverse datasets: Imagenet21K-OOD (Im21K-OOD) [39], Places365-OOD (Places) [53], Eurosat [22], iNaturalist-Plants-20 (Plants) [24] and Cifar100-superclasses [27]. All of the aforementioned datasets were constructed to have class orthogonality (be out-of-distribution) with respect to Imagenet1K, which was used to pretrain the backbone (w/ exception of Cifar100). Further details on datasets are in the supplementary. At each incoming unlabeled pool (task), we fix a mixing ratio of 2:1 of old to new classes per task, with old classes drawn from a holdout set (0.35% of each dataset prepared at experiment onset). We set pseudolabeling selection to \u03b1 = 20% of samples predicted as novel, as described in \u00a73.1. After iterative updates of COUQ or baselines, for fair evaluation, we measure AUROC on an independent test set with the same ratio of old to new class samples as in the training pool (see \u00a74.1). For experiments not purposely varying the continual class increments, we set default values as follows: 5 for Im21K-OODD, 5 for Places, 3 for both cifar100-superclasses and Plants, 2 for eurosat. More details can be found in the supplementary.\nBaselines: Since we test our approach in various settings, we describe the relevant baselines chosen for each setting.\n(A) Baselines for only Continual Novelty Detection (CND): (1) incDFM [42], a method that includes an updatable continual novelty detector, but assumes single class novelties (see \u00a72.2); (2) DFM [33], a precursor of incDFM originally proposed for non-continual novelty detection;\n(B) Baselines for only Continual Open-World Novelty Learning (CONL): (3) CCIC [9] a semi-supervised CL model that adapts MixMatch [8] to the CL setting; (4) GBCL, a few-shot continual active learning approach [5];\n(C) Baselines used for both CND and CONL: (5) (ER-variants) Semi-supervised baselines built upon the CL technique of Experience Replay [12, 43] based on [49] for active continual learning. For these, uncertainty metrics such as (5.1) Entropy, (5.2) Margin, (5.3) Softmax are used to actively select the most uncertain samples for labeling which will be used in continual training updates. The same uncertainty metrics are then used to output scores for evaluation; (6) For further comparison, we introduce additional baselines built upon the previous item ER-variants but using the corresponding uncertainty quantification (e.g. Entropy, Margin or Softmax) to iteratively pseudolabel the most confident samples during the inner loop akin to our approach.\""}, {"title": "4.2. Continual Novelty Detection", "content": "We first demonstrate the effectiveness of COUQ within the general problem of continual novelty detection (CND). Recall that at each task the novelty detector should be able to detect novelties reliably from an unknown and multiple number of novel classes. We evaluate CND performance using the common threshold-agnostic Area-under-receiver-operating-curve (AUROC) score. We show results for our method and all the relevant baselines over all 5 datasets and 3 different foundation model backbones in table 1. We test CND performance in both semi-supervised and unsupervised setups as described earlier. For the semi-supervised case, we fix AL budget at a default value of 1.25% for all baselines (including our own method) and across all datasets, as detailed in Section 4.1. Default class-increment the number of novel classes introduced at each task is indicated in parenthesis by the dataset name in the table. Entries in Table 1 are averaged AUROC scores over all continual tasks. We plot CND performance (AUROC) over continual tasks (time) in Fig. 2 (left, A.1, B.1). Key takeways from Table 1 are as follows: (1) Both our actively-supervised method version, COUQ (AL+P), and our unsupervised version, COUQ-Unsup (P), outperform competing methods by large margins over all experimental variations. In fact, our unsupervised variant overperforms even other methods that rely on semi-supervision. To note, semi-supervised methods are included in the first rows of the table, separated by a line. Naturally, COUQ-AL outperforms COUQ-Unsup. (2) Traditional uncertainty metrics \u2013 Entropy, Margin, Softmax \u2013 which are computed from the continual classification decision boundary perform poorly overall. One likely reason is the compounded negative effect of error-propagation (miss-identified samples) together with the pressures of catastrophic forgetting on the classification decision boundary. Note that in both ER- and PseudoER-variants, its corresponding uncertainty metric is used to select active samples with highest uncertainty and pseudolabels of high-confidence (in case of PseudoER). (3) Finally, note that PseudoER variants fail to consistently outperform ER. This is because, unlike our method, they are unable to produce high-quality, high-confidence pseudolabels. This highlights the importance of our COUQ uncertainty metric 3 in measuring pseudolabel confidence.\nVarying experimental parameters: Next, we explore the impact of varying experimental parameters. First, we test with different AL budgets (from 0.625% to 5%) for which the results are shown in Fig. 2 (Center, A.2-B.2). Our method - both supervised and unsupervised \u2013 continues to outperform baselines over a wide range of AL-budgets. Finally, we vary the class-increments from their default values with results plotted in Fig 2 (right, A.3 and B.3). We see that the compared SOTA novelty detector incDFM [42] performs quite well for the increment of one novel class per task, for which it was originally proposed. However, when the class increment increases, this method degrades in performance because it groups multiple novel classes together without distinction, which severely hurts detection capacity. We note a similar pattern in DFM. In comparison, due the clustering-based novelty mapper, our unsupervised COUQ-Unsup variant is able to significantly better model the novelty distribution along time and class increments.\nAblations: Next, we perform ablations to highlight the impact of various components of our proposed method. The results are presented in Table 2. In the first row (GT-Sup or ground-truth supervision), all the confident novel samples identified in a task are sent to a labeler with ground-truth label instead of a limited number of judiciously chosen ones. This is unrealistic in a real-world setting given high costs of labeling. Hence, it represents a conceptual upper-bound of performance, and there is no error-propagation between task transitions. For the remaining variants described next, an active budget of 1.25% is assumed as before. These include: AL-Amb - Querying samples ambiguous uncertainty scores as described in \u00a73.3 for active labeling. This is the default strategy; (2) AL-Top - Querying samples with highest uncertainty scores (i.e. most-confidently novel samples) for active labeling rather than ambiguous samples as in COUQ; (3) AL-Random - Using a random selection of samples for active labeling; (4) No-Iters - performing COUQ (with default stretgy) in oneshot rather than over multiple inner-loop iterations. In this case, we use all supervision budget upfront at i = 0 and then also pseudolabel in one-shot prior to updating S(i) 3. We additionally use 'P' to denote when pseudolabeling is used in addition to active sample selection. Hence, the last row in Table 2 contains results with AL only without pseudolabeling. First, we observe a small drop (2.2%) in performance of our method relative to the fully-labeled GT-Sup case. This indicates that for CND, COUQ manages to minimize the impact of error propagation. Next, we observe that other active labeling strategies AL-Top or AL-Rand decrease performance by 4.9% and 2.9% respectively, underscoring the informativeness of querying ambiguous samples for AL with the goal of continual novelty detection. We observe the importance of minimizing error propagation via our method's iterativeness since No-Iters results in an significant 8.3% decrease in performance. Finally, we show that pseudolabeling among the multiple novel classes detected is fundamental to performance given the AL budget's tiny size. Excluding pseudolabeling results in 10.7% average decrease."}, {"title": "4.3. Continual Open-World Novelty Learning", "content": "Here we apply COUQ to Continual Open-World classification/learning (which we term CONL). The setting is identical to that described in \u00a74.1. However, in addition to novelty detection, the goal is also to learn to continuously classify and thus incorporate novel class samples into knowledge continuously. Within this general framework, we demonstrate two ways in which COUQ can be applied to help solve CONL. The first is using COUQ to decide which samples to actively label at each task. The second is to use it to reliably rank samples by their confidence of being of a given novel class m, from which it can be derived a reliable pseudolabeling algorithm. We finally combine the two aforementioned uses (Active and Pseudo) to offer a comprehensive and robust response to the CONL problem."}, {"title": "4.3.1. Active Labeling", "content": "We first isolate the effects of using COUQ for Active labeling only, with results in Table 3. We do so by removing pseudolabeling in Eq. 3, and using only the actively labeled samples for (i) updating the iterative metric, and (ii) updating the downstream continual classifier. The first three rows of Table 3 show variants of AL using COUQ, same as those used in \u00a74.2. Note however that 'P' is omitted from their names since pseudolabeling is not used. Similar to CND ablations, here too we see an improvement of AL-Amb over AL-Top and No-iters. Overall, COUQ with ambiguous selection strategy consistently outperforms other SOTA continual active learning baselines. The last row contains the lower-bound of random sample selection."}, {"title": "4.3.2. Pseudo Labeling", "content": "Next, we isolate the effect of COUQ when used only for pseudolabeling by adopting a random selection strategy for labeling instead. Table 5 shows that for pseudolabeling, COUQ overperforms conventional uncertainty metrics such as Margin. Note that for COUQ, the pseudolabels are used to update both Eq. (3) and the downstream continual classifier. In the case of baselines, it is only the latter. Similar to previous results, removing the iterativeness (COUQ(P;oneshot) leads to an 8% decrease in performance. Most importantly, only COUQ pseudolabeling is consistently superior to abstaining from using pseudolabeling (lowerbound Rand) and updating via only the few randomly labeled samples. Lastly, we want to emphasize that there are several semi-supervised learning techniques, which exploit the use of unlabeled data akin to pseudolabeling, and which are orthogonal to our method. Some examples are Consistency propagation, semi-supervised contrastive losses [9, 31], etc. These approaches can be used in tandem with COUQ and we leave that for future work."}, {"title": "4.3.3. Combining Active and pseudolabeling", "content": "Finally, we assess the performance of COUQ on a complete open-world CL pipeline comprising active labeling, pseudolabeling, and novelty detection. Table 4 shows the cumulative average accuracy of the continual-learning classifier at the end of all tasks for all 5 datasets and 3 architecture variations. The \u201cOracle\u201d method constitutes an upper-bound. It has perfect knowledge of old and new class labels (100% supervision) and is trained using the same architecture and experience replay hyper-parameters as COUQ and all baselines. Overall, our method COUQ(AL+P), which includes both uncertainty-aware active and pseudolabeling via 3, outperforms all baselines by a large margin, even with stringent labeling budgets of 2"}]}