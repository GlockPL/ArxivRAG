{"title": "Reinfier and Reintrainer: Verification and Interpretation-Driven Safe Deep Reinforcement Learning Frameworks", "authors": ["Zixuan Yang", "Jiaqi Zheng", "Guihai Chen"], "abstract": "Ensuring verifiable and interpretable safety of deep reinforcement learning (DRL) is crucial for its deployment in real-world applications. Existing approaches like verification-in-the-loop training, however, face challenges such as difficulty in deployment, inefficient training, lack of interpretability, and suboptimal performance in property satisfaction and reward performance. In this work, we propose a novel verification-driven interpretation-in-the-loop framework Reintrainer to develop trustworthy DRL models, which are guaranteed to meet the expected constraint properties. Specifically, in each iteration, this framework measures the gap between the on-training model and predefined properties using formal verification, interprets the contribution of each input feature to the model's output, and then generates the training strategy derived from the on-the-fly measure results, until all predefined properties are proven. Additionally, the low reusability of existing verifiers and interpreters motivates us to develop Reinfier, a general and fundamental tool within Reintrainer for DRL verification and interpretation. Reinfier features breakpoints searching and verification-driven interpretation, associated with a concise constraint-encoding language DRLP. Evaluations demonstrate that Reintrainer outperforms the state-of-the-art on work six public benchmarks in both performance and property guarantees. Our framework can be accessed at https://github.com/Kurayuri/Reinfier.", "sections": [{"title": "Introduction", "content": "Recently, deep reinforcement learning (DRL) has demonstrated exceptional advancements in the development of intelligent systems across diverse domains, including game (Silver et al. 2016), networking (Luong et al. 2019), and autonomous driving (Ohn-Bar and Trivedi 2016). Nevertheless, the inherent opacity of the deep neural network (DNN) decision-making process necessitates verifiable and interpretable safety and robustness before the practical deployment (Hasanbeig, Kroening, and Abate 2020; Srinivasan et al. 2020). Therefore, significant efforts have been invested in learning approaches integrated with formal verification to develop property-guaranteed DRL systems.\nDRL properties are typically defined in terms of safety (informally, the agent never takes undesired actions), liveness (informally, the agent eventually takes desired actions), and robustness (informally, the agent takes similar actions with perturbation in the environment) (Baier and Katoen 2008). An important approach to achieving interpretability is by addressing interpretability problems, which can evaluate the behaviors of the trained models and provide useful insight into their underlying decision-making mechanisms. Wildly concerned problems include decision boundary (identifying the boundary between different inputs that leads to different actions) (Zhang et al. 2021; Wang, Fredrikson, and Datta 2021), counterfactual explanation (identifying the counterfactual inputs that make a property unsatisfied, which highlight the system's weaknesses) (Molnar 2020; Wachter, Mittelstadt, and Russell 2017), sensitivity and importance analysis (identifying which input feature plays a central role in the decision-making process) (Dethise, Canini, and Kandula 2019; Guidotti et al. 2018).\nHowever, developing verifiable and interpretable DRL systems remains a technical challenge. Firstly, verification results often serve only as adversarial counterexamples (Madry et al. 2017; Jin et al. 2022; Clarke et al. 2003; Xie et al. 2019) or just indicate whether the training process should be prolonged, leading to inefficient interaction and lengthy training time. For instance, the verification-in-the-loop approaches, where the model is alternately trained and verified against properties in each iteration until all predefined properties are proven (Jin et al. 2022; Gross et al. 2022; Amir et al. 2023; Zhang et al. 2023), rely on a single counterexample derived from time-consuming verification for property learning, making the process inefficient. Secondly, our evaluation results reveal that existing approaches fall short in terms of both property satisfaction and reward performance. Lastly, interpretability, a widely-concerned factor in developing trustworthy DRL systems, is overlooked.\nIn this paper, we propose a novel verification-driven interpretation-in-the-loop framework Reintrainer (Reinforcement Trainer) that can develop reliable DRL systems with verifiable and interpretable guarantees. Specifically, we design the Magnitude and Gap of Property Metric algorithm integrated with reward shaping (Ng, Harada, and Russell 1999) strategies during training. This algorithm leverages the gap (the difference between the current training model and the predefined properties), and the density (each input feature's contribution metric to the model's output based on interpretability), to measure the magnitudes of violations, i.e. the distances between the boundary of the property-"}, {"title": "Preliminaries", "content": "Markov Decision Process (MDP). An MDP is defined as M = (S, A,T, \u03b3,r). S is the state space; A is the action space; T = {P(\u00b7|s, a) : s \u2208 S, a \u2208 A} is the transition dynamics, and P(s's, a) is the probability of transition from state s to state s' when the action a is taken; r: S\u00d7A \u2192 R is the reward function; \u03b3 \u2208 [0, 1) is the discount factor. I \u2286 S is the initial state space. A policy \u03c0 : S \u2192 A defines that the decision rule the agent follows. The goal in DRL is to learn an optimal policy maximizing the expected discounted sum of rewards, i.e., arg max \u0395\u03c4~\u03c0 [\u2211to \u03b3tr (st, at)], where t demotes the timestep, 7 denotes a trajectory (so, ao, 81, ...), and \u03c4 ~ \u03c0 indicates the distribution over trajectories depends on the policy \u03c0.\nFormal Verification of DRL Systems. Given a DRL system whose policy is modeled by a DNN N, a property \u03c6 defines a set of constraints over the input x and the output N(x) (or denoted by y). Verifying such a system aims to prove or falsify: \u2200x \u2208 R\", P(x) \u21d2 Q(N(x)) (for all x, if pre-condition P(x) holds, post-condition Q(N(x)) is also satisfied) (Landers and Doryab 2023). For simplicity, the agent's policy \u03c0 is consistent with its DNN N, and the state s input to the policy is consistent with the input x of the DNN. In this paper, k denotes the verification depth (the maximum timestep limitation considered in verification), and n denotes the number of the state features.\nIn the properties of formal verification (e.g., \u03c6 = s \u2208 [(1,2), (3, 4)] \u21d2 a \u2208 [0, +\u221e)), the property-constrained spaces consist of the constraint intervals (e.g., $[s] = [(1, 2), (3, 4)], where [s] denotes the constraint interval of the state s in property 6, indicating the Oth feature of the state s\u00ba \u2208 [1,3] and the 1st feature s\u00b9 \u2208 [2,4]), rather than concrete system states and actions (e.g., s = (1, 2)). Further, $[s] donates the lower bounds of the constraint interval of the state, while d[s] donates the upper bounds. The properties of verification can be formulated using satisfiability modulo theories (SMT) expressions, and the sets and functions defined in M can be treated as predicates; for example, when s \u2208 S, predicate S(s) is True, or when T(s's, a) > 0, predicate T(s, s') is True.\nFramework Overview\nFig. 1 shows the overview of Reintrainer consisting of three sequential stages in every iteration:\n(i) Training stage: This stage consists of several training epochs. After several epochs, the model is updated and we move to the verification stage. Specifically, in every epoch, the model is rolled out in the environment, and we obtain set of trajectories {Ti} stored in the buffer. Subsequently, within the experiences of each trajectory, we measure the magnitude of violation or satisfaction of the predefined properties in DRLP and trace back the sequence leading to viola-"}, {"title": "Design of Verifier and Interpreter", "content": "We design and implement Reinfier, which achieves functions far beyond Reintrainer's requirements and also addresses the challenges of reusability and completeness in terms of the type of properties as well as interpretability questions. Therefore, it can also be used as an independent and user-friendly tool for DRL verification and interpretation. We introduce novel concepts in the main text, while the detailed functional design is provided in Appendix C.\nFig. 2 shows the working procedure of Reinfier. To avoid reinventing the wheel, we opt to use existing DNN verifiers DNNV (Shriver, Elbaum, and Dwyer 2022), Marabou (Katz et al. 2019) and Verisig (Ivanov et al. 2021) as the DNN verification backend. A parser for given properties in DRLP and an adapter for given DNN models designed and implemented by us function as intermediaries, bridging Reinfier with the DNN verifiers. We implement the single verifier by realizing model-checking algorithms or using reachability analysis, obtaining conventional Boolean DRL verification results (whether properties hold or not) derived from previous DRL verification works (Eliyahu et al. 2021; Ivanov et al. 2021). Building upon this foundation, we introduce the novel batch verifier, featuring breakpoints searching on the DRLP templates. The numeric DRL verification results from our batch verifier provide a more accurate measure of the DRL model's safety and robustness. Additionally, our interpreter for answering interpretability questions features a cohesive methodology and corresponding solutions based on the identified breakpoints."}, {"title": "Descriptive Language: DRLP", "content": "The detailed syntax of DRLP is defined in Appendix B. A DRLP verification script can be structurally divided into three segments. The first segment initializes a series of pre-assigned variables used for the verification. The second segment, starting with a delimiter @Pre, is a set of the pre-conditions P (the prerequisite of properties). The third segment, starting with a delimiter @Exp, is a set of the post-conditions Q (the expected results). In the script, x denotes the input of DNN, y denotes the output of DNN, and k denotes the verification depth; variables prefixed with \"-\" are categorized as iterable variables. Fig. 3 shows four verification scripts encoded with the properties and a DRL system with two state features and one continuous action.\nSafety and Liveness Property. A verification script on the safety property is shown in Fig. 3(a). We first detail pre-conditions (Line 3-7). Each state feature x belongs to the interval [-1,1] (Line 3). Considering two cases that a equals 0 or 1 (Line 1), the value of each initial state feature is re-"}, {"title": "Magnitude and Gap of Property Metric Algorithm", "content": "The hypothesis we advocate posits that penalties (reward reductions applied to discourage undesired actions) are di-rectly proportional to the magnitude of violations. Furthermore, this proportionality extends to the deviation from current states to those states where the property holds. This deviation can be equivalently described as the distance between the boundary of the property-constrained spaces and the states where violations occur. The theoretical proof that our hypothesis and algorithm for property learning do not affect the optimality of the policy is provided in Appendix A.2. Additional details are provided in Appendix D."}, {"title": "Distance of States", "content": "Distance density. The concept of sensitivity analysis in Sensitivity Analysis inspires us that the fluctuations in the output could be different for the same input feature at different values, thus we introduce the distance density considering that perturbations only need evaluating in one direction at the boundary: given the DNN N and the property \u03c6, when the input feature s is subject to perturbation \u025b, the density of the lower bound \u03c1(\u03c6, s) can be formulated as:\n$\\rho(\\phi, s) = \\max d(N([s]_i), N(s))$\n(2)\nwhere s \u2208 [(\u03c6[s0], ..., \u03c6[si] + \u025b, ..., \u03c6[sn\u22121]), (\u03c6[s0], ..., \u03c6[si], ..., \u03c6[sn\u22121])]. The upper bound density function \u03c1 can be defined similarly.\nExact middle point. The exact middle point, denoted as [s], represents the position with the maximum distance to the boundary, and also the point where the density of the lower and upper boundaries switches. Consequently, the exact middle point [s] is the midpoint of the density-weighted upper and lower bounds, formulated as:\n$[s] = \\frac{\\rho(\\phi, s^l) \\cdot \\phi[s]_i + \\rho(\\phi, s^u) \\cdot \\phi[s]_i}{\\rho(\\phi, s^l) + P(\\phi, s^u)}$\n(3)\nDistance of States in 1-Dimension Given the state feature si whose current observation is v, its normalized distance dist to its property-constrained space \u03c6 is formulated as:\n$dist(\\phi[s]_i, v) = \\begin{cases} \\frac{v-\\phi[s]_i}{\\bar{\\phi}[s]_i-\\phi[s]_i} \\ \\ \\text{if } v \\in [\\phi[s]_i, \\bar{\\phi}[s]_i] \\\\ 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{else} \\end{cases}$\n(4)\nwhere p\u2081 represents the penalty intensity coefficient, and we typically set p\u2081 to 1.\nDistance of States in n-Dimension Considering the differences in input feature to affect the outputs, we employ a density-weighted (P2-norm. Given the state s whose current observation is s, its distance Dist to the property-constrained space \u03c6 is formulated as:\n$Dist(\\phi, s) = \\frac{\\sqrt{\\sum_{i=0}^{n-1} [\\rho(\\phi, s) (dist(\\phi[s]_i, s_i))^{p_2}}]}{\\sum \\rho(\\phi, s)}$\n(5)"}, {"title": "Metric of Violation's or Satisfaction's Magnitude.", "content": "For the vast majority of cases, setting the distance of actions to a fixed value is sufficient. However, similarly, it can also be calculated as the distance of states. Given the observed environmental state s and the action taken a, we measure the magnitude of violation(-) or satisfaction(+) when the property \u03c6 is violated or satisfied, which is formulated as:\n$Dif f(\\phi, s, a) = \\pm Dist(\\phi, s) \\cdot Dist([a], a)$\n(6)"}, {"title": "Metric of the Gap", "content": "We introduce the concept of gap to quantitatively measure the divergence between the anticipated parameter specified in the predefined property and the identified breakpoints from the on-training model. The high computational complexity of finding breakpoints confines the gap measurement to just one parameter z from the state features or the action.\nDefinition 1. Property \u00f2 is a relaxation of property \u03c6, if and only if \u03c6 \u21d2 \u00f2.\nTheorem 1. Property $ = P \u21d2 Q is a relaxation of property \u03c6 = P \u21d2 Q, if P \u2286 P, Q \u2287 Q.\nIn this context, we find breakpoints by stepping through z within the \u222e's relaxation domain \u00d2\u03c6=P\u21d2Q = {P \u21d2 Q : \u2200\u00cc, \u00d2, \u00cc \u2286 P, \u00d2 \u2287 Q} on DNN N. Subsequently, {bp} denotes the identified breakpoints, and the gap function is formulated as:\n$g(\\phi, N) = \\min_{\\Phi \\in \\{bp\\}} |\\bar{z}| - |z||$\n(7)\nThe gap metric can serve as an indicator for property learning and can also be combined with a learning rate schedule function Lr as: F\u209c = Lr(g(\u03c6, N)) \u00d7 Diff(\u03c6, s, a)."}, {"title": "Traceback", "content": "For properties that span multiple steps, including multi-step safety and liveness, violation occurring at a particular step implies increasing unsafety in the preceding trajectory. Therefore, it is imperative to trace back to adjust the reward trajectories in the buffer before policy optimization. For action avoidance property, we calculate the traced intermediate reward Ft backward from the last trajectory in the buffer with a discount factor \u03bc\u2208 [0,1) as: Ft = Ft - X\u207b\u00b9Ft\u2212\u2081 + \u03bcFt+\u2081; for destination reach property, Ft = XFt+\u2081 \u2212 Ft + \u00b5Ft+\u2081. The final shaped reward given to the agent rt is formulated as: rt = rt + Ft."}, {"title": "Evaluation", "content": "The evaluation of Reinfier, the supplementary and the ablation studies of Reintrainer are provided in Appendix E.\nBenchmarks and Experimental Settings. We evaluate Reintrainer on the same six public benchmarks used in Trainify (Jin et al. 2022). Mountain Car (MC) (Moore 1990) task consists of a car placed at the bottom of a valley, and its goal is to accelerate the car to reach the top of the right hill. Cartpole (CP) (Barto, Sutton, and Anderson 1983) task consists of a pole attached by an unactuated joint to a cart, which moves along a frictionless track, and its goal is to balance the pole. Pendulum (PD) (Brockman et al. 2016) task consists of a pendulum attached at one end to a fixed point while the other end is free and starts in a random position, and its goal is to apply torque on the free end to swing it into an upright position. B1 and B2 (Ivanov et al. 2021; Gallestey and Hokayem 2019) are two classic nonlinear systems, where agents in both systems aim to arrive at the destination region from the preset initial state space. Tora (Jankovic, Fontaine, and KokotoviC 1996) task consists of a cart attached to a wall with a spring, and inside which there is an arm free to rotate about an axis; its goal is to stabilize the system at the stable state where all the system variables are equal to 0.\nAll experiments are conducted on a workstation with two 8-core CPUs at 2.80GHz and 256 GB RAM. We adopt the same training configurations for each task respectively.\nFor each task, we predefine certain properties related to their safety or liveness. All these properties, along with their types and meanings, are presented in Table 1, and their definitions are provided in Appendix E.2.4. Reintrainer can automatically translate DRLP scripts, encoded from predefined properties, into violation measure strategies at runtime.\nWe employ the DQN algorithm (Mnih et al. 2013) for tasks MC and CP with discrete actions, while we employ the DDPG algorithm (Lillicrap et al. 2015) for the other four tasks with continuous actions. For tasks MC, CP, and PD, all training hyperparameters for vanilla algorithms are finely tuned to achieve their optimal performance, and Reintrainer directly employs these same hyperparameters. We train two specific types of Trainify: Trainify#1 employs the hyperparameters from its original work, and Trainify#2 employs the same hyperparameters as the vanilla algorithms. For tasks B1, B2 and Tora, we employ the total same hyperparameters from the work of Trainify for Reintrainer, vanilla algorithms and Trainify#1.\nFinal Model Comparison. We evaluate the final models in their environments for 100 episodes, and calculate the average and standard deviation of the cumulative rewards, as shown in Table 2. The results demonstrate that our approach significantly outperforms both types of Trainify and even fine-tuned vanilla algorithms in all cases. We also formally verify the predefined properties on the final models. The results in Table 2 show that the vanilla algorithms do not guarantee these properties to be satisfied, while in some cases the property is naturally satisfied. The final models trained by Trainify fail to satisfy certain properties, a shortfall we attribute to the inherent limitations of its method, which primarily focuses on subdividing the abstract state space based on counterexamples but lacks mechanisms to prevent the agent from taking unexpected actions, a critical aspect for ensuring safety. In contrast, Trainify performs well in terms of reachability properties, such as $4 and $5, enabling the agent to reach specific states. The evaluation results of the final models reveal that our approach guarantees property satisfaction without incurring performance penalties com-"}, {"title": "Related Work", "content": "Numerous safe DRL methods have been proposed to ensure DRL safety. The most popular methods are constrained optimization-based safe DRL, which focus on optimizing the agent's behavior while adhering to safety constraints through Lagrangian relaxation (Ray, Achiam, and Amodei 2019; Jiaming Ji 2023) and constrained policy optimization (Yang et al. 2022, 2020; Achiam et al. 2017). However, they only minimize the cost of unsafe actions during training and cannot guarantee absolute safety in deployment.\nFormal verification-based methods can rigorously ensure learning safety compared to the aforementioned methods. Among these, verification-in-the-loop training, rather than train-then-verify, can more efficiently train models. The pioneering work (Nilsson et al. 2015) proposes a correct-by-construction approach for developing Adaptive Cruise Control by defining safety properties and computing the safe domain. Trainify (Jin et al. 2022) develops DRL models driven by counterexamples, refining abstract state space, which limits model structure and is highly inefficient. COOL-MC (Gross et al. 2022) combines a verifier and training environments without any safe learning method. The work (Amir et al. 2023) leverages verifiers to select the best available policy and continues training on the chosen policy.\nMoreover, interpretability is another method to indirectly enhance model safety. DRL interpretability focuses on explaining the decision-making rules of models (Meng et al. 2020; Zhang et al. 2021; Wachter, Mittelstadt, and Russell 2017; Lee and Landgrebe 1997) and analyzing model characteristics (Dethise, Canini, and Kandula 2019; Molnar 2020; Guidotti et al. 2018). UINT (Huang et al. 2023) first proposes that several interpretability questions can be for-"}, {"title": "Limitations and Conclusion", "content": "For multi-step properties, Reinfier itself cannot address stochasticity or unknown environments as related works (Eliyahu et al. 2021; Amir, Schapira, and Katz 2021; Ivanov et al. 2021; Jin et al. 2022), which might limit its application scenarios; the former can be alleviated through over-approximation(Clarke et al. 2000), while the latter can be addressed by environment modeling; and the lengthy verification time necessitates the improvement of verification techniques, like incremental verification. Besides, the current integration with reward shaping proves effective for simpler properties but might fall short for more intricate ones, thus integrating our framework with constrained optimization-based algorithms could enhance performance.\nWe present a verification-driven interpretation-in-the-loop framework Reintrainer, which can develop trustworthy DRL systems through property learning with verification guarantees and interpretability, without performance penalties. Additionally, we propose a general DRL verifier and interpreter Reinfier featuring breakpoints searching and breakpoints-based interpretation, and the language DRLP. Evaluations indicate that Reintrainer surpasses the SOTA across six public benchmarks, achieving superior performance and property guarantees, while Reinfier can verify properties and provide insightful interpretability answers."}, {"title": "A Theoretical Proofs", "content": "A.1 Theorem 1 with Proof\nDefinition 1. Property \u00f2 is a relaxation of property &, if and only if \u00a2 \u21d2 .\nTheorem 1. Property $ = P \u21d2 Q is a relaxation of property \u00a2 = P \u21d2 Q, if \u00cc \u2286 P, Q \u2287 Q.\nProof. \u21d2 means that if P \u21d2 Q, then \u00cc \u21d2 Q must also hold.\nLet's subtract (s) from both sides:\nQM(s, a) \u2013 \u03c8(s)\n=Es'~P(\u00b7\\s,a) [R(s, a, s') + \u03b3\u03c8(s')+ \\gamma \\max_{\u03b1' \\in A} (QM(s', a') - y(s')) - \\psi(s)]\n=Es'~P(\u00b7\\s,a) R(s, a, s') + \u03b3\u03c8(s')-\n\u03c8(s) + \u03b3 \\max_{\u03b1' \\in A}(QM(s', a') \u2013 (s'))]\n=Es'~P(\u00b7\\s,a) R(s, a, s') + \\gamma \\max_{\u03b1' \\in A}(QM(s', a') \u2013 (s')) \u2013 \u03c8(s)\n=Es'~P(\u00b7\\s,a) R(s, a, s') + \u03b3 \\max_{\u03b1' \\in A}(QM(s', a') \u2013 (s')) \u2013 \u03c8(s)\nLet\nQM'(s,a) := QM(s, a) \u2013 y(s).\nand recall that\nF(s, a, s') = \u03b3\u03c8(s') \u2013 \u03c8(s).\nTherefore,\nQM' (s, a)\n=Es'~P(\u00b7\\s,a) r(s, a, s') + F(s, a, s')+ \\gamma \\max (QM(s', a')]\n=Es/~P(\u00b7.\\s,a) [r'(s,a,s') + \\gamma \\max (Q_M^(s', a'))]\nThis is the Bellman equation for M', so\nQ_M' = Q_M.\nCorollary 1. Fr does not affect the optimality of the policy.\nProof. Recall that\nDiff ($, s, a) = \u00b1Dist(\u00a2, s) \u00d7 Dist([a], a)\nFt = Lr(g($, N)) \u00d7 Dif f($, s, a)\nConsidering the violation condition, therefore,\nFt = -Lr(g($, N)) \u00d7 Dist($, s) \u00d7 Dist(p[a], a)\nFor the vast majority of cases, Dist([a], a) is set to a fixed value, and within one Reintrainer iteration, Lr(g($, N)) keeps unchanged. Therefore,\nFt = \u043aDist(s)\nSo, Ft can be considered as a potential value (s).\nRecall that, for destination reach property,\nFt = Ft+1 - Ft + \u00b5Ft+1\n= [\u03bb\u03c8(st+1) \u2013 \u03c8(st)] + \u03bc\u0112t+1\nGenerally, \u03bc enhances property learning based on the discount factor y and typically takes on a smaller value. Therefore,\nFt \u2248 \u03bb\u03c8(St+1) \u2013 V(St)"}, {"title": "B Supplementary Details of DRLP", "content": "B.1 Motivation\nThe novel DRLP appears because existing methods to encode properties, interpretability questions, and constraints aren't user-friendly. While SMT and ACTL formulas are common, they resemble pseudocodes more than programming languages. Inspired by Python's popularity in DRL, we use its syntax to encode SMT formulas more intuitively with syntactic sugars, enhancing ease of writing. In previous works, Trainify(Jin et al. 2022) claims ACTL use, but uses hardcoded conditional statements without parsing ACTL actually; model-checking approaches like whiRL(Eliyahu et al. 2021; Amir, Schapira, and Katz 2021) use DNN verifiers' API, unsuitable for DRL verification. Verisig(Kazak et al. 2019; Ivanov et al. 2021) uses hybrid system models which lack readability.\nB.2 DRLP Syntax\nWe design a Python-embedded Domain-Specific Language named DRLP (Deep Reinforcement Learning Property) and implement its parser. Its currently supported syntax is defined in Backus-Naur Form as:\nB.3 DRLP Design\nDRLP Structure. A DRLP script is structured into three segments: drlp_v encompassing pre-assigned variables and Python codes to execute, drlp_p comprising the pre-condition P initiated by the delimiter @Pre, and drlp_q comprising the post-condition Q initiated by the delimiter @Exp.\nIn drlp_v, the statements are not directly associated with a specific property, but they facilitate to batch verification. Their execution outcomes, encompassing all variable values within the local scope, are transferred to the subsequent two segments for the assignment of variables without concrete values. Variables prefixed with \"-\" are iterable vari-"}, {"title": "C Supplementary Design of Reinfier", "content": "C.1 DNN Adapter\nDue to the multiple interactions between the agent and the environment, all constraints from the property, the DRL system, and the DNN need encoding to be consistent with verification depth k for model checking algorithms. Therefore, the agent DNN should be unrolled and encoded k times larger than the original when the verification depth is k. This unrolled DNN is used as the input DNN for DNN verification. x denotes the j-th value of the input of the i-th original DNN; the input of the i-th original DNN x\u1d62 is (x, ..., xn\u2212\u2081); the input x (xo, ..., Xk\u2212\u2081). y denotes the j-th value of the output of the i-th original DNN; the output of the i-th original DNN yi is (y, ..., ym\u2212\u2081); the output y is (Yo, \u2026\u2026\u2026, Yk\u2212\u2081).\nC.2 Type of Properties\nAll constraints from the property and the DRL system need encoding. Thus, pre-condition P can be divided into the following four parts:\n(i) State boundary condition S: describes the boundary of the environment in DRL. and all input features should be bounded. (ii) Initial state condition I: describes the initial state of the system, because a DRL system could start from a defined state or within a particular state boundary. (iii) State transition condition T: describes how the environment transits from the current state xi to the new state xi+\u2081. Generally, the action N(xi) taken by the agent plays a pivotal role in the state transition, but Xi+\u2081 is not only determined by N(xi) but also affected by random factors. (iv) Other condition C: describes extra constraints related to specified properties. It stands apart from the S, I, and T parts, which usually describe the unchanging characteristics of a particular DRL system. For example, liveness properties (Baier and Katoen 2008) require no state cycle existing, such as xo! = x\u2082; and this constraint should be considered as an extra constraint due to its exclusive association with this particular property, rather than being a characteristic of the entire system. We believe that distinct division in this way is beneficial for users to formulate and articulate properties.\nPost-condition S: describes the expected result of a given DRL system under previous constraints.\nSafety properties (Baier and Katoen 2008) represent one of the paramount property types, where the agent is expected to persistently avoid undesirable actions under given pre-conditions. Safety properties guarantee that the DRL system will never transition into bad states. The predicate Good(N(x)) evaluates as True when in good states and vice versa. Such a property is formulated as:\n$\\forall x_0, x_1..., \\prod_{i=0}^{a+k} (S(x_i) \\land I(x_i) \\land T(x_i, x_{i+1}) \\land C(x_i))\n\u21d2(Good(N(xi)))$\n(8)\nAnother type of property is liveness property (Baier and Katoen 2008), where the agent might take undesirable ac-"}, {"title": "C.3 Single Verifier", "content": "C.3.1 Model Checking Algorithm. Inspired by related works (Eliyahu et al. 2021; Amir, Schapira, and Katz 2021), we implement the following algorithms in Single Verifier.\nBounded model checking (BMC). As the verification depth k increases from 1, both the original DNN and the DRLP script are unrolled or translated with depth k. Then, DNN verifier is queried. The query is formulated as:\n$\u221ax_0, ..., x_{k-1},((\\prod_{i=0}^{k-1}(S(x_i) \u2227 C(x_i))) \u2227 I(x_0) \\land T(x_{i}, x_{i+1}))\u2192\\prod_{i=0}^{k-1} Q((N(x_i))$\n(11)\nk-induction. The first phase of k-induction, the basic situation verification, is identical to the BMC process; while the second phase diverges. In the inductive hypothesis verification, the original DNN is unrolled with depth k + 1, and the DRLP script is translated for indication verification with depth k + 1 and an assumption that the property holds with depth k. Then, the DNN verifier is queried. The query is for-"}, {"title": "D Supplementary Details of Reintrainer", "content": "D.1 Distance of States in 1-Dimension.\nTo elucidate our algorithm in Reintrainerand our motivation", "actions": "accelerate to the left", "features": "the car's position along the x-axis p", "by": "\u03c1", "0.40,0.07)": "Action \u2260 0) defined in Evaluation", "function": "given the state feature s whose current observation is v", "s": "in 1-dimension is formulated as:\ndist' ([x", "min([\u03c6[x": "i - v", "x": "i - v) (15)\nDistance density. However", "as": "nSensitivity(N, 20, x) = max d(N(xo), N(xo)) (16)\nX0\nwhere xo \u2208"}]}