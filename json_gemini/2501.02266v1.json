{"title": "LLMzSz\u0141: a comprehensive LLM benchmark for Polish", "authors": ["Krzysztof Jassem", "Micha\u0142 Ciesi\u00f3\u0142ka", "Filip Grali\u0144ski", "Piotr Jab\u0142o\u0144ski", "Jakub Pokrywka", "Marek Kubis", "Monika Jab\u0142o\u0144ska", "Ryszard Staruch"], "abstract": "This article introduces the first comprehensive benchmark for the Polish language at this scale: LLMzSz\u0141 (LLMs Behind the School Desk). It is based on a coherent collection of Polish national exams, including both academic and professional tests extracted from the archives of the Polish Central Examination Board. It covers 4 types of exams, coming from 154 domains. Altogether, it consists of almost 19k closed-ended questions. We investigate the performance of open-source multilingual, English, and Polish LLMs to verify LLMs' abilities to transfer knowledge between languages. Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined. We show that multilingual LLMs can obtain superior results over monolingual ones; however, monolingual models may be beneficial when model size matters. Our analysis highlight the potential of LLMs in assisting with exam validation, particularly in identifying anomalies or errors in examination tasks.", "sections": [{"title": "1 Introduction", "content": "LLMs, as a form of Artificial Intelligence, have been invented to perform useful work in a specific environment defined by humans. Obviously, the question of how to evaluate LLMs has been raised since their inception (Hendrycks et al., 2020). Similarly, humans are expected to possess general and specialized knowledge to perform their work and, again, since the beginning of civilization, human capabilities have been assessed with general and professional exams.\nHumans, however, do not operate in an abstract vacuum. Their performance is always evaluated within a specific culture, its traditions, and legal system. Language leaves its stamp as well, not only because exams are always expressed in a particular language, but also because it is the lens through which reality is understood. One does not have to accept the Sapir-Whorf Hypothesis in its entirety to argue that expected answers are not always translatable between cultures and traditions.\nIn this paper, we present a new Polish LLM benchmark called LLMzSz\u0141 (LLM za Szkoln\u0105 \u0141aw\u0105, LLMs Behind the School Desk) representing a collection of Polish national exams. The characteristic feature of the dataset is the inclusion, in one coherent environment, of school and professional examinations. The incorporation of vocational exams makes it possible to advance hypotheses on LLMs' specific competences for various professions that include:\n\u2022\n\u2022 practical application of knowledge in real professional situations,\n\u2022 real-life task solving,\n\u2022 acquaintance of industry regulations, safety standards and applicable laws,\n\u2022 competence in cutting-edge technology,\n\u2022 communication skills,\n\u2022 work organization.\nIn a way, the benchmark represents what is expected from an \u2018educated' person to be able to navigate efficiently in the realities of modern Poland. The design choices we made while developing LLMzSz\u0141 distinguish it from other benchmarking efforts that rely on the collection of examination materials such as the MMLU proposed by Hendrycks et al. (2020). First, instead of randomly searching the web for exams and collecting the results, we decided to rely on a single, reliable, authoritative source of tasks developed by educational experts. In our case, it is the collection of exams published by the Polish Central Examination Board which is an institution responsible for the development of testing materials for nationwide exams"}, {"title": "2 Related Work", "content": "The capabilities of LLMs are verified by their performance against human-prepared benchmarks. In the pre-LLM era, linguistic benchmarks consisted of \"low-level tasks\u201d, such as: grammatical correctness, sentiment analysis, paraphrase detection, semantic similarity, sentence entailment, natural language inference or coreference resolution. With the appearance of LLMs, emerging benchmarks have tended to focus on higher-level skills, such as reasoning, problem-solving or human-like conversation. A cost-effective way to design benchmarks to evaluate LLMs in combined aspects of knowledge, reasoning, and problem-solving is to build them on existing exams prepared for humans. Hendrycks et al. (2020) prepared the MMLU benchmark: 57 tests comprising 15,908 questions from humanities, social sciences, STEM and other subjects, collected from freely accessible sources. Later, MMLU was automatically translated (using, among others, DeepL and GPT-3.5) to produce the m_mmlu dataset for 35 languages. A similar activity was conducted solely for the Spanish language by Plaza et al. (2024) with the use of Azure Translator. In the paper, a detailed analysis was carried out to discover the discrepancies between the performances of multilingual models for English and Spanish. Gema et al. (2024) identify numerous errors in the MMLU benchmark, which shows that there is still room for the development of new MMLU-like benchmarks, with a focus on their ground-truthfulness.\nThe most recent trend in LLM benchmarking is to develop regional MMLU equivalents from national resources. Son et al. (2024) organized a dataset derived from Korean exams that covered 45 subjects from humanities and STEM. Y\u00fcksel et al. (2024) developed a smaller benchmark for Turkish (9 subjects), for which questions were prepared by high school experts. Li et al. (2024) created a dataset that covers four main categories: medicine, law, psychology, and education. Most of the questions were taken from university or college exams, but the legal tests were based on the professional qualification examination.\nMedicine and law are the most frequent disciplines in the profession-directed benchmark tests."}, {"title": "3 Dataset", "content": "The preparation of our dataset began with examination of the structure of each subject in the selected exams. The main characteristic that we focused on was the number of multiple-choice questions with one correct answer. Subjects with a limited number of such questions were excluded as the additional effort required to process these exams would not be justified by the small number of usable questions they would provide. This selection process resulted in the following categories for the dataset: math, natural sciences, biology, physics, and the Polish language for pre-high school questions, and arts, mechanics (including mining and metallurgy), and agriculture (including forestry) for professional exams.\nThe tests are published annually by the Polish Central Examination Board, which simplifies the task of finding and downloading PDF files. All exam questions and their corresponding answer keys are stored in separate files on the CKE website, making it crucial to track which answer document matches which question document.\nThe textual data were extracted using the PyPDF library without the use of LLMs. Extracting all the necessary data from PDF files proved to be straightforward, but due to extreme inconsistencies in the format of the answer keys, matching questions and answers had to be done manually for every type of exam except the professional exams, which were processed using simple scripts. Many PDF files lacked a text layer, making it impossible to extract the questions without using OCR tools. A universal method for extracting both questions and answers from PDF files could be a valuable step toward scalable data extraction, allowing the dataset to expand annually.\nBoth the questions and answers had to be cleaned to remove any anomalies in the data, as well as questions requiring additional resources (such as pictures or charts) that could not be processed. We also chose to exclude task numbers and answer labels from our dataset, representing the correct answers by their index in the answer table. Several data-cleaning techniques were applied (e.g., identifying answers that contained processed text beyond a single question). These steps were crucial in preserving as many questions as possible without discarding them due to detected irregularities."}, {"title": "3.2 Evaluation harness", "content": "As an evaluation environment, the open source LM Evaluation Harness framework was used. The task configuration was based on the MMLU configuration; in particular, for each answer, a language model in question was run to return the probability (likelihood), and the answer with the highest probability was compared with the gold answer to calculate the accuracy. The following prompt template was used:\nPrzyk\u0142adowe pytanie egzaminacyjne, test jednokrotnego wyboru\nEng. Sample exam question, single-choice test\n{{question.strip()}}\nA. {{answers[0]}}\nB. {{answers[1]}}\nC. {{answers[2]}}\nD. {{answers[3]}}\nPrawid\u0142owa odpowied\u017a:\nEng. The correct answer:"}, {"title": "3.3 Dataset availability", "content": "The LLMzSz\u0141 dataset is available at"}, {"title": "4 Evaluation Results", "content": "Table 6 in Appendix A presents the evaluation results for all variants of the open-weight models tested:"}, {"title": "4.1 Model size", "content": "Figure 1 illustrates the performance scores plotted against model sizes. The overall best-performing model is Mistral-Large-Instruct-2407, achieving the accuracy of 67.17. However, the size of the model (123B parameters) may make it difficult to deploy it in certain production environments. The second-best-performing model, Meta-Llama-3.1-70B-Instruct, has a slightly lower accuracy of 66.59 its parameter size is significantly lower (70B). The model still remains impractical for deployment on a single 80GB VRAM GPU in FP16.\nAmong smaller models (<15B parameters), the best-performing is Bielik-11B-v2.1-Instruct, with an accuracy of 57.52 and a parameter size of 11B. Although its performance is approximately 10 percentage points lower than that of the leading models, it may still be suitable for cost-sensitive production environments. In the category of models with fewer than 8B parameters, notable performers include Meta-Llama-3-8B-Instruct (accuracy 44.83), gemma-7B (accuracy 46.84) and Qwen2-7B (accuracy 45.59). Models smaller than 3B parameters generally perform at the level of random guessing, with an accuracy of approximately 25%."}, {"title": "4.2 Model language", "content": "When it comes down to models with fewer than 8B parameters, the best performers are either multilingual or English-focused. For slightly larger models, the best performing model is the one fine-tuned for Polish. Overall, the best models are multilingual, though this may be influenced by the lack of models available in Polish and English in this class (except the old Llama-2, which is an older model than Llama-3.1). Note that not all multilingual models may include Polish in their training datasets. For example, the Yi model family is described as containing English and Chinese, which may explain its low quality of around 40%, although still above the random guess threshold. We hypothesize that multilingual models, especially large ones, are able to transfer knowledge from other languages to Polish."}, {"title": "4.3 Release date", "content": "Figure 2 presents the models' scores against their release date. The first effective models are Llama-2 from July 2023. Since then, the only models that show increasing accuracy across the board have been the Mistral and Llama models."}, {"title": "4.4 Instruct vs non-instruct models", "content": "Table 2 describes the difference between a fine-tuned model ('Instruct' or 'Chat' model) and the same model without fine-tuning. For eight out of nine models, instruction tuning improves performance, except for Mistral-7B-v0.1. This leads to the conclusion that fine-tuning generally helps, although it is not always guaranteed."}, {"title": "5 Detailed evaluation analysis", "content": "This analysis aims to find which features of input, expected output, and actual output are responsible for low results of the models. The Mann-Whitney U test, which examines the correlation between specific features and the models' scores, can be used for this purpose. Clearly, answers containing numbers (e.g. in<answer1>:SHAPE:9 meaning a one-digit answer) and, in general, questions related to calculations, pose the greatest challenge for models. This is mirrored by word features, such as wynosi (is equal), koszt (cost), oblicz (calculate) or units of measurements (mm, ha). The most difficult, for the LLM, were professional exams, in particular, \u2018Protection and management of forest resources' (code R.13). Interestingly, the model exhibits some bias, tending to give the wrong answer if the answer code is B (exp: 1).\nAn even simpler idea is to analyze the questions for which a model assigned a very low probability to the expected answer. It might indicate a mistake on the exam sheet. For example, in this way we found an error in the 2018 edition of the Organization and management of the maintenance process of motor vehicles exam (M.42-X-18.06). The expected answer for Question 24 was erroneously specified as insolvent instead of strategic."}, {"title": "6 Evaluating the human-prepared exams by LLMs", "content": "In this section, our objective is to study the correlation between the performances of LLMs and those of human examinees over the years. We acknowledge various limitations of this study. The model scores are restricted to closed questions, whereas the public scores of the examinees include both closed and open questions. In case of professional exams, the public data contain only information on the overall number of passes.\nThe results obtained by the models were used to verify which language models received the highest scores on certain categories of exams, and what score they received in subsequent years. This made it possible not only to infer which models performed best in a given discipline but, when juxtaposed with the percentage of pass rates by examinees (Table 4), to determine whether the exams may have been more difficult or easier in subsequent years."}, {"title": "6.1 Middle school exams", "content": "In the category of middle school exams (which consist of Junior High school exam and 8-grade exam), the Mistral-Large language model performed the best, achieving an average score of 43.62.\nIn the years 2015-2019 significant fluctuations can be observed in both human and model performance. The average real results indicate that the exams were relatively stable in 2015 (48.00) and 2016 (49.00). In 2017, there was a decline to 47, and in the following years the results showed more volatility, rising to 52.00 in 2018, and declining to 43.00 in 2019.\nThe language models showed a similar trend on average (avg. score). They achieved their highest scores in 2017 (41.64), while in 2019 there was a significant drop to 30.29, which is confirmed in the correlation analysis of individual exams in Table 5."}, {"title": "6.2 High school exams", "content": "For high school exams, we examined how language models performed in answering questions from three disciplines: biology, mathematics, and physics based on exam sheets from 2015-2023.\nOf the disciplines analyzed, the highest score of all the models tested was achieved by Mistral-Large, with an average score of 77.16 in biology, 43.81 in math, and 67.27 in physics, respectively."}, {"title": "6.2.1 Biology", "content": "The high school biology test scores of the students have steadily decreased, from 43 in 2015 to 26 in 2023, with a marked deterioration in 2018 (32) and 2023. Meanwhile, language model scores rose, reaching 76 in 2018 and 77 in 2023. The lack of correlation may be due to the increase in difficulty of open questions."}, {"title": "6.2.2 Mathematics", "content": "Students' scores in mathematics steadily declined, from 41 in 2015 to 17 in 2023. In contrast, the results of the language models were stable, oscillating between 30 and 38."}, {"title": "6.2.3 Physics", "content": "The results of the high school physics exams show clear declines in student performance over the years, from 44 in 2015 to 34 in 2020. There was also a downward trend in the language models"}, {"title": "6.3 Professionals exams", "content": "For professional exams, the statistics for human performance considered the pass rate.\nThe Mistral-Large turned out to be the highest performer in the arts and mechanical-mining and metallurgical professional exams, with scores of 55.70 in the two disciplines, respectively. In agricultural and forest examinations, Llama-3.1-70B performed best, achieving 58.84."}, {"title": "6.3.1 Arts", "content": "Over the years, professional art exams have become more difficult for both models and those taking the exam, especially since 2020. However, the"}, {"title": "6.3.2 Mechanical, Mining and Metallurgical", "content": "Professional exams in Mechanical and Mining and Metallurgy (MMM) became easier for passers over the years, reaching a peak pass rate in 2022. For language models, the results were more stable, with a slight increase between 2019 and 2023."}, {"title": "6.3.3 Agriculture and Forestry", "content": "Professional exams in agriculture and forestry were generally easy for students, especially since 2017, when the pass rate exceeded 90. Language models coped steadily with the worksheets in successive years, with results in the 48-55 range of correct answers."}, {"title": "6.4 Final remarks on correlation between model human performance", "content": "Our research has shown that some models show a high correlation with human performance for school tests (e.g. Mistral for Junior High tests or Bielik for 8-grade schools). If this phenomenon is confirmed with more data (possibly including open questions), it will advocate for a possible use of LLMs as a primary tool for the verification of exam questions prior to their publication. If a model performance differs significantly from previous years in either direction, it may indicate one of the following issues:\n\u2022 The test is of different difficulty than in previous years.\n\u2022 The questions are not properly randomized.\n\u2022 Some answers may be erroneous (see Section 5 for an example).\nIn the case of vocational exams, the stability of language model scores over the years may suggest that the difficulty of closed questions has remained similar. Thus, it can be assumed that the decrease in student scores might have been affected by other factors, e.g. by the increase of difficulty of open questions."}, {"title": "7 Conclusion", "content": "In this paper, we present a new benchmark for large language models built on the basis of Polish school and professional exams. The proposed benchmark relies on the data collected from a credible, nationwide source. It is stratified by difficulty into middle school, high school, and vocational tiers and contains precise time stamp information for all the collected data. It is the largest and most comprehensive LLM benchmark developed for the Polish language that has been published to date. We assessed the performance of a wide range of LLMs with regard to the proposed benchmark and confronted them with the results achieved by humans. The results show that multilingual LLMs outperform monolingual ones and that the use of monolingual models can be justified when size limitations are a concern. Furthermore, the outcomes suggest that LLMs can streamline the preparation of future exams by identifying errors in exam questions."}, {"title": "8 Limitations", "content": "As any competence test that is not designed to test the notion of intelligence in isolation, the presented benchmark intermixes to some extent the measurement of the reasoning capabilities of the model with the assessment of its ability to memorize factual knowledge and answer the questions through a (fuzzy) search with regard to the textual data aggregated in the process of model training.\nTaking into consideration that the exams are published online, there is also a risk of data contamination. Although the exam questions are stored separately from the answers, and they are released in the form of PDF files, this issue cannot be neglected. To mitigate the risk of data contamination we recommend tracking the performance of the model under study with regard to our benchmark separately for the exam questions that where published before and after the release date of the model.\nBeing an exam-based dataset the presented benchmark does not cover informal language or spontaneous speech phenomena that arise in the conversational setting. The extent to which an examinee's proficiency in solving tests generalizes to performance in real-life situations is an open question. Measuring real-world competence of an LLM in skills covered by vocational exams would require the construction of an embodied agent which was not feasible at the time of writing. Thus, we deliberately refrained from drawing conclusions in this area."}]}