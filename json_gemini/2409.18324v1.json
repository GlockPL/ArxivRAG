{"title": "Input-Dependent Power Usage in GPUs", "authors": ["Theo Gregersen", "Pratyush Patel", "Esha Choukse"], "abstract": "Abstract-GPUs are known to be power-hungry, and due to\nthe boom in artificial intelligence, they are currently the major\ncontributors to the high power demands of upcoming datacenters.\nMost GPU usage in these popular workloads consist of large\ngeneral matrix-matrix multiplications (GEMMs), which have\ntherefore been optimized to achieve high utilization of hardware\nresources.\nIn this work, we show that modifying the input data to\nGEMMs, while maintaining the matrix shapes and sizes can\nnotably change the power consumption of these kernels. We\nexperiment with four kinds of input variations: value distribution,\nbit similarity, placement, and sparsity, across different data types.\nOur findings indicate that these variations can change the GPU\npower usage during GEMM by almost 40%.\nWe hypothesize that input-dependent power usage variations\noccur due to changes in the number of bit flips in the GPUs. We\npropose leveraging this property through compiler and scheduler\noptimizations to manage power and reduce energy consumption.", "sections": [{"title": "I. INTRODUCTION", "content": "Power demand for datacenters, supercomputers, and ma-\nchine learning is exploding, mainly driven by the growth\nin large language models (LLMs) [1]\u2013[4]. Recent estimates\nforecast substantial annual increases in datacenter energy\nconsumption and raise concerns about demand exceeding grid\ncapacity in next few years [3], [5], [6]. With the industry\npushing for more compute, addressing power consumption is\nkey to sustainability [7]\u2013[9]. Previous work has explored and\nutilized various techniques for managing power, such as power\ncapping [10], [11], frequency scaling [9], approximation [12],\n13], and batching [14], [15]. Our work shows that input data\ncan also be utilized to manage power.\nWe focus on compute kernels and datatypes applicable to a\nvariety of accelerator workloads. Accelerators such as GPUs\nare central to modern machine learning. For instance, Meta\nplans to amass around 600,000 H100 GPUs by the end of 2024\n[16], Microsoft has enabled OpenAI training and inference\nwith large cluster deployments of A100 and H100s [17], and\nthe top supercomputers leverage GPUs [18]. GPUs are also\nvery power hungry. For instance, recent NVIDIA H100 SXM5\nGPUs have a total power draw of 700 watts [19], with a DGX-\nH100 consisting 8 GPUs needs provisioning of 10,000W [19].\nResearch on reducing accelerator peak power draw or\nenergy often focuses on hardware design and the surrounding\ninfrastructure: system design and scheduling [14], [15], [20],\n21], power control [9], [22], or efficient chips [23]\u2013[25].\nInstead, we target input data for general matrix multiplica-\ntion (GEMM) kernels [26]\u2013[28]. GEMM kernels comprise a\nlarge portion of machine learning cycles and are important\noperations for GPUs due to natural compatibility with parallel\nexecution [24], [29], [30]. Prior work explores efficient imple-\nmentations of GEMM [31]\u2013[33] and the performance impacts\nof quantization [34]\u2013[36], sparsity, and data ordering [37] on\nGEMM in LLMs. We demonstrate that GEMM input values\nand placement can have significant impact on GPU power as\nwell.\nWhile research has broadly characterized accelerator power\nduring machine learning and high-performance computing\n(HPC) workloads [4], [8], minimal prior work measures GPU\npower consumption due to varying inputs. Previous work has\nshown that input data values can significantly impact GPU\npower, but only considered single instructions such as FMUL\nor IMUL [38]. Bhalachandra et al. investigated the effects\nof GEMM input patterning on power, but only looked at\ninput value entropy for a single datatype and placement of\nzero versus non-zero values [39]. In addition to input value\nentropy, we explore the impact of other patterns such as\nsorting, bit-level sparsity, hamming weight, and similarity.\nWe also compare across several datatypes and assess the\neffect of NVIDIA tensor cores. We find that GEMM input\npatterns can change GPU power consumption by up to 38%.\nThis observation lends itself to a variety of potential future\napplications for power and energy efficiency: power-aware\nsparsity, data pruning for power capping, and efficient data\nplacement algorithms."}, {"title": "II. BACKGROUND", "content": "GEMM operations are fundamental to machine learning\n[30], [40] and many common computational tasks [41]. As\nsuch, GEMM is an important target for power efficiency.\nGEMM is a fundamental linear algebra operation. For\nmatrix A with dimensions (N, K), matrix B with dimensions\n(K,M), matrix C with dimensions (N, M), and scalars \u03b1\nand \u03b2, a standard GEMM execution typically computes the\nfollowing matrix output [27]:\n$D = \\alpha A \\cdot B + \\beta C$\nTo reduce memory use, the D output matrix is often set to C\nand updated in-place. GPU makers such as NVIDIA typically\nprovide proprietary kernels (i.e., compute routines) to execute\noperations like GEMM on their hardware [29]. Kernel libraries\nsuch as cuBLAS [26] and cuSPARSE [28] are available\nthrough public APIs, however the underlying implementations\nare black boxes. A more transparent alternative is CUTLASS\n[27], an open-source kernel library maintained by NVIDIA.\nTo improve performance, NVIDIA GPUs can utilize tensor\ncores. Tensor cores are specialized for matrix math operations\nsuch as matrix multipy and accumulate (MMA) and provide\nacceleration for specific datatypes. For instance, the NVIDIA\nAmpere architecture provides 20\u00d7 FP32 MMA throughput\ncompared to the previous generation [42]."}, {"title": "III. EXPERIMENT SETUP", "content": "To explore the impact of GEMM inputs on GPU power, we\nrun a series of GEMM operations on an NVIDIA A100 PCIe\nvirtual machine (VM) hosted on Azure. The NVIDIA A100\nPCIe GPU has a maximum thermal design power (TDP) of 300\nwatts [42]. We use standard NVIDIA CUTLASS kernels [27]\noptimized for GEMM execution, measure power every 100ms\nwith NVIDIA dcgm command-line tools [43], and measure\nelapsed time with C++ standard library high resolution clocks.\nAll experiments use 2048 by 2048 matrices. We selected\n2048 as the largest power of two that did not consistently\nthrottle the A100 GPU. During our experiments, the A100\nGPU averaged 98.5% utilization. The C matrix is zeroed, and\nboth A and B matrices use the same pattern with B transposed\nunless otherwise noted. Reported results are averaged over 10\nseeds with 20k iterations each for FP16-T and 10k iterations\neach for the other datatypes. The A and B matrices use\ndifferent seeds.\nWe explore four datatype setups: 32-bit floating point\n(FP32), 16-bit floating point (FP16), 16-bit floating point with\ntensor cores enabled (FP16-T), and 8-bit integer (INT8). For\neach datatype, we experiment with value similarity, physical\n(bit) similarity, data placement, and sparsity. All of the floating\npoint experiments use the same generated FP32 values, with\nnumeric conversion to their respective datatypes (round to\nnearest value). When generating input values, we use appropri-\nate parameters to ensure that all values practically fall within\neach datatype's representation range.\nDuring testing, we observed slight variations in power mea-\nsurements depending on when experiments were run. Power\nmeasurements occasionally shifted by up to 10W when the\nVM instance changed, even when using the same configura-\ntion. We attribute this to process variation across GPUs. To\nminimize this effect, we executed all experiments on the same\nVM instance. We also trim the first 500ms of power measure-\nments to account for warmup. Across all experiments for a\ngiven datatype, the average iteration runtime (Figure 1) was\nconsistent to a microsecond-level; this is expected since each\nexperiment uses the standard cutlass kernel. Figure 2 shows\naverage iteration energy for a GEMM operation with Gaussian\nrandom variables. Note the identical patterns between the\niteration runtimes and energy observations, showing that the\npower used with random variables is similar across the input\ntypes. In the rest of paper, we report power measurements\nrather than total energy, as power is the key bottleneck for\nlarge-scale machine learning [1], [2], [8]."}, {"title": "IV. INPUT-DEPENDENT POWER ANALYSIS", "content": "We profile different kinds of inputs to GEMM kernels and\nprovide our takeaways (Tn).\nA. Value Distribution\nFirst, we explore the effects of input distribution on GPU\npower during GEMM.\nDistribution standard deviation: Figure 3a shows the aver-\nage power draw during GEMM when the A and B matrices are\nfilled with Gaussian random variables with a fixed mean of 0\nand varied standard deviation. TI: Input distribution standard\ndeviation does not significantly impact power.\nDistribution mean: For the results in Figure 3b, the A and\nB matrices are filled with Gaussian random variables with a\nfixed standard deviation of 1 and varied mean. T2: Larger\ninput value means can reduce power for FP datatypes.\nInputs from a set: The third experiment (Figure 3c) fills A\nand B with values selected uniformly, with replacement, from\na set of Gaussian random variables with a mean of 0 and\nstandard deviation of 210 for FP and 25 for INT8. T3: Inputs\nfrom a small set of unique values decrease power consumption."}, {"title": "B. Bit Similarity", "content": "Next, we consider input data bit similarity. In these experi-\nments, the A matrix is initially filled with one random value\nand the B matrix is filled with another random value.\nRandom bit flips: Figure 4a illustrates how power changes\nwhen random bits are flipped in each element. T4: Input data\nwith highly similar bits uses less power.\nLeast significant bits: Figure 4b shows how power changes\nas the least significant bits are randomized. T5: As more least\nsignificant bits are randomized, power increases.\nMost significant bits: Figure 4c illustrates how power shifts\nwhen the most significant bits are randomized. T6: As more\nof the most significant bits are randomized, power increases.\nInput data types: Figure 4 also shows that FP16-T on tensor\ncores has the highest power usage compared to the other data\ntypes. This is important to note, since the default data type\nin AI applications is FP16-T. Approximation related research\ntries to reduce the time and memory to run AI inference, but\ncan also have a positive impact on power efficiency. T7: FP16-\nT is the most power hungry data type."}, {"title": "C. Placement Patterns", "content": "Next, we explore the impact of input data placement on\nGEMM power. Across each of the following experiments, the\ninitial values for matrices A and B are constant. Both matrices\nare filled with random variables from a Gaussian distribution\nwith a mean of 0 and standard deviation of 210 for FP and 25\nfor INT8.\nSorted into rows: For this experiment (Figure 5a), we\npartially sort both matrices into rows. Sorting n percent means\nthat the lowest n percent of values are sorted into the first n\npercent of indices (row-wise). The B matrix is not transposed.\nT8: Sorting input values can decrease power consumption.\nSorted and aligned: For Figure 5b, the matrices are partially\nsorted into rows again. However, this time the B matrix is\ntransposed, so the lowest values in A are multiplied with the\nlowest values in B during GEMM. T9: Aligning sorted values\ndecreases power even more than just sorting.\nSorted into columns: Figure 5c is a similar experiment\nto that of figure 5a, but the input values are sorted into\ncolumns rather than rows. T10: Sorting values into columns\ncan decrease power consumption.\nSorted within rows: We also experiment with sorting within\nmatrix rows and aligning across matrices. Figure 5d shows\nhow power changes when the A and B matrix rows are\npartially sorted. T11: Intra-row sorting can decrease power,\nbut to a lesser extent than sorting fully."}, {"title": "D. Sparsity", "content": "Next, we explore the impact of sparsity on GEMM power.\nThese experiments use standard GEMM, not sparse GEMM.\nGeneral sparsity: Figure 6a shows power as the matrix is\nmade sparser. T12: Matrix sparsity decreases GEMM power.\nSparsity after sorting: In Figure 6b, the initial A and B\nmatrices are fully sorted before sparsity is added. With this\npatterning, power has a curve that peaks around 30 - 40%\nsparsity for floating point datatypes. Although both sorting\nand sparsity decrease power in isolation, this trend indicates\nthat they do not compound when paired. T13: Sparsity applied\nto sorted matrices can actually increase power consumption.\nSparsity in least significant bits: Finally, we consider spar-\nsity in physical structure. Figure 6c is the result of setting each\nmatrix item's least significant bits to zero. T14: Zeroing least\nsignificant bits can reduce power.\nSparsity in most significant bits: Figure 6d illustrates the\neffect of setting each matrix item's most significant bits to\nzero. T15: Zeroing most significant bits can reduce power."}, {"title": "E. Generalization", "content": "Our results hold across different GPU generations. We show\nthis by replicating several of the experiments on an NVIDIA\nH100 80GB HBM3 GPU (TDP 700W, local cluster), NVIDIA\nQuadro RTX 6000 24GB GPU (TDP 260W, Chameleon cloud\n[44]), and NVIDIA Tesla V100-SXM2-32GB GPU (TDP\n300W, Chameleon cloud [44]). We present results for FP16\nruns of the Distribution mean experiment, Most significant bits\nexperiment, Sorted into rows experiment, and General sparsity\nexperiment. Figure 7 illustrates the results across GPUs. The\nmatrix size was 512 by 512 for the RTX 6000 (it throttled at\n2048 by 2048) For the V100, A100, and H100 GPUs, power\nconsumption trends are consistent. The RTX 6000 has less\nprominent changes in power, likely because it is the oldest\nof the tested GPUs (e.g., uses GDDR6 memory rather than\nHBM) and has a lower TDP."}, {"title": "F. Bit Alignment and Hamming Weight", "content": "To investigate broader trends across experiments, we look\nat bit alignment between values multiplied during GEMM, as\nwell as Hamming weights of the matrix values. Bit alignment\nbetween two values is 0 if all of the bits are opposite, and\nalignment is 1 if all of the bits are the same. Figure 8\nillustrates average GPU power during GEMM in relation to\nthe average bit alignment between the A and B matrices and\naverage Hamming weight in the A matrix (B has similar\nweight because of shared patterns). Each dot represents one\nexperiment configuration from the prior subsections. Across\nall floating point datatypes, there seems to be correlation with\nhigher bit alignment or lower Hamming weight and decreasing\naverage GPU power during GEMM. However, this is not an\nentirely consistent trend."}, {"title": "V. DISCUSSION AND FUTURE WORK", "content": "Going forward, we plan to identify what causes input-\ndependent power usage variation in GPUs and develop practi-\ncal techniques to improve the power and energy efficiency of\nGPU applications at scale. We list below future directions.\nIdentifying Causes. Based on prior work, we hypothesize that\nGPU power draw depends on inputs due to changes in the\nnumber of bitflips during computation [45], or how many bits\nare set [38]. For example, running a GEMM with zero matrices\nwould incur no bitflips, and thus, it likely has lower power\ndraw. Value similarity likely helps by reducing bit flipping at\nthe hardware level. We plan to do extensive experiments to\nvalidate this hypothesis and investigate other hardware-level\nfactors that might contribute to reduced power draw.\nInput-dependent GPU Power Models. We are building input-\ndependent GPU power models to more precisely capture how\ninput variations impact the GPU peak power draw. Such a\npower model would take in different data patterns as inputs\n(e.g., specified via a domain-specific language), and estimate\nthe power usage as output. Using such power models, future\nwork could build power-aware compilers and optimizers to\nreduce the power draw of GPU applications that can tolerate\ninput variations.\nPower- and Energy-efficient Machine Learning. Since machine\nlearning applications, like large language models, are very\npower intensive [8], a key future direction is to leverage\ninput changes to drive down their power and energy usage.\nSpecifically, we are exploring three different directions. First,\nwe are trying to modify model weights into value ranges that\nuse less power; for example, shifting the weight values towards\nlarger averages could reduce the power draw as shown in\nSection IV. Second, we plan to investigate whether we can\npartially or fully sort neural network model weights so as to\nreduce power draw. Since weights within a layer correspond\nto independent neurons, rearrangement is computationally\nequivalent as long as each neuron processes its own input.\nRecent work leverages permutation invariant transformations\nto manipulate GPU tiles without changing the computation\nresults [46]. Similar transformations could potentially be used\nto set patterns that reduce power. Finally, we would like\nto develop sparsity designs that reduce power usage while\nalso optimizing performance, accuracy, and/or memory trade-\noffs [37]. While it is challenging to estimate/limit the impact\nof input variations on model accuracy, we are hopeful that the\nbenefits will outweigh the pitfalls."}]}