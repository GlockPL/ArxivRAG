{"title": "Sequencing the Neurome: Towards Scalable Exact Parameter Reconstruction of Black-Box Neural Networks", "authors": ["Judah Goldfeder", "Quinten Roets", "Gabe Guo", "John Wright", "Hod Lipson"], "abstract": "Inferring the exact parameters of a neural network with only query access is an NP-Hard problem, with few practical existing algorithms. Solutions would have major implications for security, verification, interpretability, and understanding biological networks. The key challenges are the massive parameter space, and complex non-linear relationships between neurons. We resolve these challenges using two insights. First, we observe that almost all networks used in practice are produced by random initialization and first order optimization, an inductive bias that drastically reduces the practical parameter space. Second, we present a novel query generation algorithm that produces maximally informative samples, letting us untangle the non-linear relationships efficiently. We demonstrate reconstruction of a hidden network containing over 1.5 million parameters, and of one 7 layers deep, the largest and deepest reconstructions to date, with max parameter difference less than 0.0001, and illustrate robustness and scalability across a variety of architectures, datasets, and training procedures.", "sections": [{"title": "1", "content": "The rapid rise of Deep Learning and Artificial Intelligence demands a deeper understanding of the inner workings of Artificial Neural Networks, with stakes higher than ever. Neural Networks are now used ubiquitously in every day life: from personalized movie recommendations to automated research assistance and portfolio management. The ability to precisely reconstruct a neural network, discerning the firing patterns of individual neurons solely through query access, is of central importance, with massive implications in safety, security, privacy, and interpretability. Such methods may even hold the key to eventually unlocking the inner workings biological neural networks.\nUp until this point, due to the difficulty of the problem, practical results have been very limited. This is not fully surprising: In the general case, recovering the weights of a neural network is a hard problem[1-4]."}, {"title": "2", "content": "One approach is to relax the constraints, and instead of producing an exact weight reconstruction, these methods are satisfied with a generating high quality approximation of the model behaviour [4, 5], often called a substitute network [6]. This is accomplished using similar techniques to knowledge distillation [7], where the black-box network takes on the role of teacher, and the substitute model the student. This type of approach is attractive due to its simplicity: the teacher provides information in the form of input-output pairs, and the substitute learns directly from this data. While this mode of investigation has proven fruitful in a wide range of settings, [8, 9] and architectures [10], it cannot provide an exact specification of a neural network, and is thus limited in its usefulness and the guarantees that it provides.\nA second approach limits the problem in a different way: by focusing specifically on exact weight recovery of a specific type of Neural Network: Feed forward networks with ReLU activations [11]. The ReLU function has a distinct piece-wise nature, and identifying when this transition occurs in each neuron can allow for the parameters to be identified, up to an isomorphism. This idea has produced lots of theoretical work [12-18], and recently has also led to some very recent strong empirical results [4, 19, 20]. While these algorithms currently represent the state of the art in exact weight recovery, in practice these studies have only been applied to small networks, and reconstruction is a slow process that is fully limited to ReLU activations. While others have explored different analytic methods for inferring the weights, [21-24], some of which can be applied to broader settings such as TanH networks, actual empirical results from all of these works have been very limited.\nOf course, the most appealing approach would be to use the relatively simple and versatile methods of knowledge distillation, but to thereby precisely reconstruct the parameters of the network. However, directly training a student network to not just mimic the teacher, but converge on its exact parameters, is a very difficult proposition. Martinelli et. al. [25] proposes learning a larger substitute network, and then pruning it down to the proper size, although the resultant network usually will have more neurons than the blackbox and thus not be exactly identical. They went as far as to claim that, both from a theoretical and empirical perspective, directly learning the exact weights on a network of the exact same size as the black box is infeasible, and will inevitable get stuck in a high loss minima [25].\nThis work directly refutes this claim, and provides the first ever exact recovery of a neural network's full parameter set using the student teacher paradigm without extra neurons. Moreover, we show that our approach is actually well motivated by theory, and can solve larger, deeper, and more varied networks than previously seen in the literature. The best methods in the state of the art demonstrate exact reconstructions of up to 100k parameters on shallow ReLU and TanH networks, and up to 5 layers deep on a small ReLU network of roughly 1000 parameters [19]. We reconstruct networks with over 1.5 million parameters, and up to 7 layers deep, and demonstrate results on a variety of activation functions, network architectures, and training datasets.\nWe identify two main challenges in exact network reconstruction: navigating the massive parameter search space, and selecting informative queries that allow for sample efficient recovery.\nOur approach to solving the first challenge is motivated by an important observation that has been almost entirely overlooked by previous work. While for the general case, reconstructing the parameters of a neural network is an NP-hard problem, we are primary not interested in solving for neural networks with arbitrary weight patterns, but in neural networks that are likely to exist in the real world. Because almost all networks are randomly initialized with a known distribution, and trained via backpropogation, the possible values that the parameters will practically take on is a minute subset of the full parameter space. To give an analogy from the field of image recognition: if previous algorithms have attempted to be valid for any possible configuration"}, {"title": "3", "content": "of pixels, we are proposing only considering pixel combinations likely to occur in real photographs.\nTo address the second challenge, we propose a novel sampling method called Committee Disagreement Sampling. From an information theory standpoint, the most useful sample is the one that evenly splits up the remaining parts of the hypothesis space that are consistent with the current samples (the version space) [26, 27]. While generating maximally informative samples is NP-hard, it can be approximated using query by committee. This approach generates proxies for the most informative samples by selecting the samples that maximize disagreement among a population of hypotheses [28]. Our sampling method generates new samples by generating random values and iteratively refining them using backpropagation to directly learn samples that maximize the disagreement of a population of potential solutions."}, {"title": "1 Results", "content": "Given a blackbox neural network, the goal is to reconstruct all of its internal parameters. We can query the network with any possible input and observe the corresponding output at the final layer. However, we have no access to any internal activations or weight values. A successful reconstruction extracts the parameters of the target network with a minimum number of input queries.\nLike prior work[4, 19, 20], we assume exact knowledge of the target network architecture, including the number of neurons, their connectivity, and the activation functions. This is a reasonable assumption in practice because many companies and researchers publicly release the architectures of their trained models while keeping the exact trained weight values confidential [29, 30]. Further, even when the architecture is not publicly released, side-channel attacks have been demonstrated that can infer this information[31-34].\nUnlike other approaches [35], we will assume no direct or surrogate knowledge of the training dataset. However, we will make some assumptions about the training pipeline, namely that it uses standard procedures common in the training of modern neural networks. We will assume that all data was scaled to have a mean of 0 and standard deviation of 0.5, and that the network parameters were initialized with a mean of 0 and standard deviation of\n$\\sigma = \\sqrt{\\frac{2}{N_{in} + N_{out}}}$\nwhere $n_{in}$ and $n_{out}$ are the number of incoming and outgoing neurons per layer [36]. We further assume that the network was trained on the data using some form of gradient based first order optimization, although the exact optimizer, number of epochs, or learning rate, is not assumed, and can be anything."}, {"title": "1.2 Accounting for Isomorphisms", "content": "Neural networks with different internal parameters can still exhibit the exact same input-output behavior. The input-output behavior of a network only defines its internal parameters up to an isomorphism, and depending on the architecture and type of activation functions used, different isomorphisms can be observed [12, 25, 37, 38]. Since two neural networks that are isomorphisms of each other are functionally identical, it is impossible to differentiate them using only query access, and thus when evaluating our solutions, we need to take these isomorphisms into consideration. There are three primary types of isomorphisms that are relevant for our network reconstructions:\nPermutations Every neuron in a neural network computes an activation function over a linear combination of its input values. This linear combination implies that the order of the input values does not affect the computed result. Consequently, the input-output functionality of a neural network does not change when the internal order of the neurons changes. In other words, the order in which internal neurons are enumerated is arbitrary, and any two internal neurons can be swapped, as long as their connections to the preceding and next layer are preserved.\nScaling Networks with piece-wise linear activation functions, like ReLU and LeakyRelU, exhibit one more isomorphism: scaling. This is directly caused by the piece-wise linearity. Thus, for any positive scaling factor a, the following holds:\n$f(\\sum w_i.(x_i * a) + b. a) = a \u2022 f (\\sum w_i. x_i + b)$\nThis means that the output weights of a neuron can be scaled up as long as the input weights are scaled down with the same value.\nPolarity Similarly, networks with an activation function symmetrical around zero, like TanH, exhibit another isomorphism of their own: polarity. For any input value, the activation function satisfies:\n$f(-x) = -f(x)$\nTherefore, the sign of the input weights of any neuron can be inverted when the sign of the output weights is inverted as well. When evaluating a solution, we search for the isomorphism of the solution that is most similar to the blackbox, and then compute parameter distance."}, {"title": "1.3 Reconstruction Experiments", "content": "To asses our algorithm, we began with a 3 layer 784x128x10 ReLU network with just over 100k parameters. This corresponds with the biggest network reported in prior state of the art methods [4, 19], although it is the smallest network that we will consider in our work.\nWhen comparing to these prior SOTA methods, a few important things must be noted. First, while the work of Carlini et. al., the strongest existing method, is open source, we were unable to reproduce the results they reported. While the code worked for smaller networks, when we tried running it on our hardware to reconstruct the MNIST network with input dimension 784, or anything larger, it simply ran for several hours until crashing. Thus, we only provide comparison for the 784x128x10"}, {"title": "1.4 Convergence Analysis", "content": "To better understand how our algorithm converges, we performed an in depth analyis, using the most complex network we dealt with, the 7 layer 784x128x80x40x32x16x10 network trained on MNIST. We looked at convergence per layer, as well as convergence per parameter. It is important to note that at iteration 25 we began relaxing the learning rate, which is why we see a discontinuity at that point.\nThere are several key takeaways. We can see that layers closer to the input converge first, and that, while the mean error gets low very rapidly, the max error in each layer takes far longer to converge. In the per parameter analysis, we plot every single network parameter, and can see the same phenomenon, where although the majority of errors are decreasing, a few pesky parameters stay with much higher error than the rest, as shown in figure 3.\nWe also measure how the reconstruction network converges to the functionality of the blackbox network. Input convergence was plotted as a series of heatmaps of size 28x28, the input space from MNIST. Each pixel represents the sum of all parameter errors that that pixel leads to, in every layer. Red represents larger error, black lower error. Ouput convergence was plotted per output neuron. Since MNIST has 10 classes (0-9), there are ten output neurons. We ran both the blackbox network and reconstruction through MNIST, and calculated the output difference average for each output Neuron, to represent in-distribution performance similarity. We should note that the reconstruction algorithm had no knowledge of MNIST.\nThe input behaviour shows that initially the error is highest on pixels towards the center. This makes sense, since in MNIST most semantic information is located in the center, and thus this is where the most complex weight behaviour is found. This error gradually decreases over iterations. The output behaviour shows convergence to near-identical behavior for all 10 classes. Further, all of our reconstructions made the same classification as the blackbox network in 100% of cases. Input and output convergence are shown in figure 4."}, {"title": "2 Discussion", "content": "We believe this work is important for several reasons.\nSecurity Knowledge of a network's structure is of central importance in adversarial machine learning [43]. If we know the network parameters, we can attack it using gradient-based attacks [44]. While some attacks do not rely on such knowledge [45], and there exists work that aims to make models robust to these types of attacks [46], this still represents perhaps the most significant attack vector for neural networks.\nPrivacy If we know the weights of the network, we can infer the training data [47, 48] which can be a severe privacy violation [49], especially in medical domains [50]. Further, it may be undesirable for the weights of a network to be known. For example, large language models are very expensive to train, sometimes costing upwards of millions of dollars [29], and their owners may not want them being replicated.\nInterpretability Another area where this analysis is useful is interpretability. As Deep Learning has become ubiquitous, the need for greater model interpretability has been stressed by many, for reasons ranging from ethics and legality to safety and security [51]. The ability to reproduce a network's weights can give us insight into how it trains, what sorts of minima are common in networks trained via SGD, how subcomponents are related, and other aspects as well that can help reduce the black box affect.\nSafety An additional important concern, related to the above, is the safety of a network for its users. An end user may commonly use a network provided by a third party for some important task, and relies solely on the guarantees of the third party that the network does what it purports to [52]. The ability to reproduce the weights of the network can give users security and assure them that the network is safe to use, and opens up the possiblity of formal analysis of the parameters[16].\nBiological considerations One of the greatest mysteries of the biological world is the human brain. Despite decades of research, much of its functionality is still not well understood. Reverse engineering biological neural networks is of foundational interest in neuroscience, and as has been noted by earlier work in this area [12], the ability to reverse artificial networks may give some insight into biological ones. Although there are many differences between artificial and biological neurons, neuroscientists have identified significant similarities, especially when zooming in to small regions [53], and many biological neurons appear to be well modeled by a ReLU artificial neuron [54]. In fact, as early as 1981, similar experiments to the ones in this paper had already been conducted on biological neurons [55]. While this is still a very far away thought, much like how sequencing the genome was a massive breakthrough brought about by steady incremental improvement [56], we believe work in this area will eventually contribute to our understanding of biological neurons.\nLimitations and Future Directions Due to the stochastic nature of our method, there are times when it fails to work. For all experiments presented in this paper, the method was successful at least two thirds of the time, but there was not a 100 percent success rate for all networks. Furthermore, our networks used the variant of ReLU called LeakyReLU, and our algorithm struggled more on standard ReLU networks due to the phenomenon of dying neurons."}, {"title": "3 Methods", "content": "In addition, further study is required to understand when and why this method fails. In particular, we note that narrow deep networks, while having a small fraction of the number of parameters of wide deep networks, were significantly harder to reconstruct and in a few cases failed.\nLooking towards the future, we believe this study will be a powerful step towards exactly reconstructing full-sized real world networks. A large body of recent work demonstrates that for over-parameterized networks, the weights barely move during training[57]. Chizat and Bak [58] differentiate between the 'lazy regime', where the weights barely move, and the rich regime where the weights move a lot, and give conditions where lazy learning can occur even in small models. Li et. al. [59] further demonstrated that even in the rich regime, the majority of parameters still exhibit lazy behaviour and barely move from their initial values, and as training goes on for a long time, predictable features tend to emerge [60]. All of this evidence indicates that for larger networks, our prior assumption of random initialization and gradient based training provides an even stronger prior on the weight values, which is why we believe our approach is the best way to scale to larger networks."}, {"title": "3.1 Reconstruction Algorithm", "content": "Instead of trying to reconstruct any arbitrary network, as has been the focus of previous work, we focus on networks that have been produced via random initialization, and trained with gradient descent and backpropogation. There are several recent results that suggest this may be easier to solve than the general case. These ideas come from what has been called 'The Modern Mathematics of Deep Learning', an area of analysis that emerged from trying to understand why neural networks seem to generalise so well and resist overfitting, even when heavily over-parameterized [61]. This area of inquiry introduces several models that aim to describe how the weights of a neural network evolve during training.\nWhile some alternatives have been proposed [62-64], the Neural Tangent Kernel (NTK) [65] is the most widely successful and adopted model, and it suggests that for over-parameterized networks, the weights barely move during training [57, 66, 67]. Chizat and Bak[58] differentiate between the 'lazy regime', where the weights barely move, and the non-lay regime (later dubbed the 'rich regime' [68]) where the weights move a lot, and give conditions where lazy learning can occur even in small models. Li et. al. [59] further demonstrated that even in the rich regime, the majority of parameters still exhibit lazy behaviour and barely move from their initial values.\nWhile the NTK was first proposed for shallow feed forward networks, it has since been extended to deep networks[69], CNNs[70], RNNs [71], GANs [72], Resnets [73], Auto-encoders [74], Transformers [75], and even decision trees[76]. Further, despite these studies being relegated to the realm of theory, often considering hypothetical network structures that cannot exist in practice, they do seem to model networks well in many real world cases [77]. In addition, while the usefulness of the NTK to describe the training dynamics breaks down as we train for longer, the Neural Collapse"}, {"title": "3.2 Query Generation Algorithm", "content": "phenomena gives indication that even as training goes on for a long time, predictable features will emerge [60].\nIt is also the case that networks trained using SGD, even with different random ini- tializations, will tend to learn similar features[70], even across a variety of architectures [78] possibly a result of the so-called simplicity bias [79, 80], redundancy phenomenon [81], symmetries [82, 83], and tendency of SGD to ignore certain minima [84]\nThe above results imply that, due to the inherent inductive biases of SGD, even after the training period, we still have strong priors of what the majority of the network weights will look like. In addition, a new model trained using SGD, is likely, at least under some circumstances, to find similar features to the original. This motivates that simply initializing a surrogate model of the same architecture as the blackbox, and trying to reconstruct the blackbox by sampling from it, and training the surrogate with a gradient based optimizer, is a strong candidate for exact weight recovery, assuming the black box itself was produced via gradient descent. Accordingly, our reconstruction algorithm is as follows:\nAlgorithm 1 Reconstruction Algorithm\nRequire:\npopulation size p, query number q,\nouter iterations o, epochs e,\nlearning rate \u03b1, schedule S\nEmpty dataset D\nProcedure:\nRandomly initialize a population of p surrogate network with the same architecture as the target network\nfor o iterations do\nProduce q samples and append them to dataset D\nfor e epochs do\nTrain population on D using learning rate \u03b1\nend for\nif o\u2208 S then\n\u03b1 \u2013 \u03b1/10\nend if\nend for\nReturn network in population with lowest loss"}, {"content": "Unlike in most modern ML settings, we know that the data we are training on was produced by a network of the same architecture as the substitute network, and thus a zero error hypothesis is guaranteed to be in our hypothesis space. Thus, it is logical to apply the result from the halving algorithm, that the most informative sample is the one that evenly splits the version space, and to approximate this using query by committee [28], as discussed above. This general setup is common in the active learning"}, {"title": "Query Generation Algorithm", "content": "paradigm, where, just like in our case, we can arbitrarily query an oracle, but wish to minimize such queries [85], and is related to adversarial sampling in student-teacher distillation [86], except we do not have access to the internals of the teacher network. Query by committee requires three ingredients:\n1. The ability to construct a diverse committee\n2. A disagreement criterion\n3. A method of optimizing the queries over the disagreement criterion\nWhile 1 is relatively straightforward via different random initializations, 2 and 3 are less obvious. Common methods suggested for 3 include hill climbing [87], or simply just trying many samples and keeping the best ones. Inspired by the 'hard sampling' method of Fang et. al. [88], we propose a novel sample generation algorithm that directly uses gradient descent to optimize the samples for maximal committee disagreement, along with a novel disagreement criterion that is generalizable to arbi- trary length output vectors, and is continuous, so it can be optimized using gradient descent.\nThe following is our query generation algorithm\nAlgorithm 2 Query Generation Algorithm\nRequire:\npopulation P, query number q,\nepochs e,\nlearning rate \u03b1, schedule S\nProcedure:\nRandomly initialize a learnable tensor I of shape q x input_dim\nfreeze the weights of P\nfor e epochs do\nForward-propogate I through P, obtaining disagreement loss DLP(I)\nBack-propogate loss and obtain gradient with respect to I\nUpdate I using learning rate \u03b1\nif e\u2208 S then\n\u03b1 \u2013 \u03b1/10\nend if\nend for\nReturn I"}, {"content": "This algorithms can be seen visually in figure 1.\nOur disagreement criterion is defined as follows:\nLet I be a single input vector, of dimension input_dim.\nLet our population of networks P that form the committee consistent of networks N1...Np.\nWe want to calculate pairwise disagreement among network outputs. We initially defined disagreement as Ll-norm distance between outputs (Manhattan distance), but this led to a scaling issue, where our algorithm learned to cheat by realizing"}, {"title": "Alignment Algorithm", "content": "that simply having larger output magnitudes will produce a larger disagreement, even though nothing else has changed. This is especially a problem in ReLU networks, and led our algorithm to not learn anything useful. To rectify this, we first apply a normalization to each output vector by dividing each element by the vector's L1 norm. After normalization, we calculate the L1 distance between the vectors as the disagreement metric, solving the scaling issue.\nMore formally, we define a normalization function f, as $f(x) = \\frac{x}{|x|}$.\nThe disagreement between two vectors, u and v, is defined as\n$d(u, v) = \\sum_{i=1}|f(u_i) - f(v_i)|$\nTo get disagreement loss, we calculate the pairwise distance matrix between every network output with every network output, for each network in the population.\n$\\begin{bmatrix}\nd(N_1(I), N_1(I)) & d(N_1(I), N_2(I)) & d(N_1(I), N_3(I)) & ... & d(N_1(I), N_p(I)) \\\\\nd(N_2(I), N_1(I)) & d(N_2(I), N_2(I)) &  & ... &  \\\\\n\\vdots & & & & \\\\\nd(N_p(I), N_1(I)) & d(N_p(I), N_2(I)) & & & d(N_p(I), N_p(I)) \\\\\n\\end{bmatrix}$\nNote, the diagonal here is 0, and the matrix is symmetrical around the diagonal, but this does not affect our calculation.\nWe then define the loss of input I with respect to population P as the negated mean of this matrix:\n$DLP(I) = -mean(D) = - \\frac{1}{p^2} \\sum_{i=1}^{p} \\sum_{j=1}^{p} D_{ij}$"}, {"title": "3.3 Alignment Algorithm", "content": "We can mathematically describe the internal parameters of a neural network by enumerating the weight matrices of every layer in the network.\nFor the top left network in Figure 2, that would be:\n$\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22} & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\nW_{41} & W_{42} & W_{43}\n\\end{bmatrix} \\begin{bmatrix}\nW_{11} & W_{12} \\\\\nW_{21} & W_{22} \\\\\nW_{31} & W_{32} \\\\\n\\end{bmatrix}$ (1)\nAgain, excluding the bias parameters for brevity. Similarly, for the bottom left network in Figure 2, we have:\n$\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22} & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\nW_{41} & W_{42} & W_{43}\n\\end{bmatrix} \\begin{bmatrix}\nW_{11} & W_{12} \\\\\nW_{21} & W_{22} \\\\\nW_{31} & W_{32} \\\\\n\\end{bmatrix}$ (2)\nIn this representation, isomorphisms can be expressed as matrix operations. For example, swapping two neurons corresponds with swapping two column in a weight matrix and swapping the corresponding two rows in the subsequent weight matrix."}, {"title": "Similarity of neural networks", "content": "A similar observation can be made for the scaling isomorphism. The top middle network in Figure 2 can be represented with:\n$\\begin{bmatrix}\n\\alpha W_{11} & W_{12} & W_{13} \\\\\n\\alpha W_{21} & W_{22} & W_{23} \\\\\n\\alpha W_{31} & W_{32} & W_{33} \\\\\n\\alpha W_{41} & W_{42} & W_{43}\n\\end{bmatrix} \\begin{bmatrix}\nW_{11} & \\alpha W_{12} \\\\\nW_{21} & \\alpha W_{22} \\\\\nW_{31} & \\alpha W_{32}\n\\end{bmatrix}$ (3)\nAnd the bottom middle network in Figure 2 can be represented with:\n$\\begin{bmatrix}\nW_{11} & \\alpha W_{12} & W_{13} \\\\\nW_{21} & \\alpha W_{22} & W_{23} \\\\\nW_{31} & \\alpha W_{32} & W_{33} \\\\\nW_{41} & \\alpha W_{42} & W_{43}\n\\end{bmatrix} \\begin{bmatrix}\n\\alpha W_{11} & W_{11} \\\\\nW_{21} & W_{22} \\\\\nW_{31} & W_{32}\n\\end{bmatrix}$ (4)\nThis example illustrates that the scaling isomorphism can be applied by:\n1. scaling a weight matrix column with factor a\n2. scaling the corresponding row in the subsequent weight matrix with factor $\\frac{1}{\\alpha}$\nAnalogously, the polarity isomorphism can be applied by:\n1. inverting the sign of a weight matrix column\n2. inverting the sign of the corresponding row in the subsequent weight matrix"}, {"title": "3.3.1 Similarity of neural networks", "content": "When we calculate the similarity between two neural networks, we need to take these isomorphisms into account. We can do this by defining a canonical representation for each isomorphism group that is unique in every group. For every network, we define its canonical form as follows:\n1. All weight matrix columns have unit norm, except for the last weight matrix.\n2. All weight matrix columns have a positive sum, except for the last weight matrix.\n3. All weight matrix columns are sorted according to their L1-norm, except for the last weight matrix.\n(1) is only valid when the activation function is piece-wise linear and (2) is only valid when the activation function is symmetric around 0.\nNow, we can calculate the similarity between two networks by converting both of them to their canonical form and calculating the sum of the L2-distances between their weight matrices.\nGiven a neural network we design the following procedure to convert it to its canonical form.\n1. For i from 1 through N-1:\n(a) calculate the L2-norm of all columns in weight matrix i.\n(b) divide all columns by their L2-norm.\n(c) multiply the corresponding rows in weight matrix i+1 by the same L2-norm."}, {"title": "3.4 Recognizing Convergence", "content": "An important consideration in our algorithm is recognizing when we have converged, or if we are not converging. We have two reliable methods of doing this, and empirically, both have consistently worked, in the sense that every experiment that converged exhibited both properties, and every experiment that did not converge exhibited neither property. The two conditions are:\n1. Population Agreement\n2. Vanishing Loss\nAs we converge on a solution, several networks in our population will start to converge on the same weights. Once we have several members of the population reach identical weights, within a small epsilon, we can be sure our soluton has converged."}, {"title": "A.1 Non Adaptive Sampling", "content": "Dataset Sampling While our attack model does not assume knowledge of the original training data, it is logical to think that such knowledge may be useful for reconstructing the network, especially since Martinelli et. al. [25] demonstrated that for oversized substitute networks, this is sufficient. Thus, one method of non adaptive sampling is simply using the blackbox training dataset.\nExpanded Dataset Sampling Similar to above, we make use of an extended real dataset larger than the original one used to train the black box, but still in a similar distribution. For our MNIST experiments, we do this by appending QMNIST, FashionMNIST, and KMNIST.\nRandom Gaussian Sampling Random sampling is the easiest form of sam- pling, and requires the least compute and domain knowledge. We considered random Gaussian sampling, with the mean 0 standard deviation 1.\nRandom Uniform Sampling We also considered random uniform sampling, with range [-1,1]."}, {"title": "A.2 Adaptive Sampling", "content": "In addition to the above methods, we also considered adaptive sampling methods, where the samples we draw change based on what stage of the reconstruction process we are up to, and how well our hypothesis networks are fitting to the samples.\nResampling Easy Regions Borrowing easy and hard terminology from earlier work on sampling generators [88], we generate samples that are near the region where our network is approaching the target network functionality well\nResampling Hard Regions Here, we generate samples that are near the region where our network is predicting badly."}, {"title": "B TanH Activation", "content": "We mentioned above that our algorithm works for other activations. Here we demonstrate this, and give results on networks using the TanH activation function."}, {"title": "C Analysis of Priors", "content": "Our algorithm makes use of two priors:"}, {"title": "C.1 Untrained Network", "content": "1. The assumption that weights do not move much during training\n2. The assumption that we know the original weight distribution\nHere, we explore what happens when we apply stronger versions of these priors. We devised two experiments.\nUntrained Network We do not train the blackbox network at all. This represents a stronger version of the assumption that the weights did not move during training: here the weights did not move at all.\nKnowledge of Initial Weights Instead of assuming we know the original weight distribution, we assume we know the original weights exactly. We experimented with two different ways of incorporating this knowledge. In one version, we initialized the entire committee population with the blackbox initial weights, and then added some small noise to give them variance. In the second method, we initialized a single network in the population with the original blackbox weights, and the rest of the population randomly.\nObviously, making both these assumptions at the same time renders the problem trivial, but independently they isolate our assumptions so that we can explore their significance."}, {"content": "It turns out that a fully untrained network is actually harder to solve than a trained one. This is because the outputs vary very little, and it is thus very difficult to tease out the weights via querying. However, we were able to validate our hypothesis somewhat, by demonstrating that a network trained for only a single epoch, where the weights barely moved, is indeed easier to reconstruct, as evident by the quicker convergence of the max errors in each layer. (We also note that, upon examining the code of Jagielski et. al. [4]), the network they reconstructed was trained on only a few dozen input samples)"}, {"title": "C.2 Knowledge of Initial Weights", "content": "When incorporating knowledge of initial blackbox weights, when we initialized the entire committee population with the blackbox initial weights, and then added some small noise to give them variance, we failed to solve at all, since the committee had too little diversity.\nAs a second attempt, we initialized only a single member of the population to the blackbox initial weights. This network did not converge faster than the randomly initialized networks."}, {"title": "D Visual of Committee Generated Samples", "content": "Here, we show heatmaps of our committee generated samples, at different iterations of the algorithm. Somewhat surprisingly, the samples still look like random noise, even after the networks have begun ton converge. This is somewhat logical, since our networks are likely to agree on simpler inputs."}]}