{"title": "TOWARD ROBUST EARLY DETECTION OF ALZHEIMER'S DISEASE VIA AN INTEGRATED MULTIMODAL LEARNING APPROACH", "authors": ["Yifei Chen", "Shenghao Zhu", "Zhaojie Fang", "Chang Liu", "Binfeng Zou", "Yuhe Wang", "Shuo Chang", "Fan Jia", "Feiwei Qin", "Jin Fan", "Yong Peng", "Changmiao Wang"], "abstract": "Alzheimer's Disease (AD) is a complex neurodegenerative\ndisorder marked by memory loss, executive dysfunction, and\npersonality changes. Early diagnosis is challenging due to\nsubtle symptoms and varied presentations, often leading to\nmisdiagnosis with traditional unimodal diagnostic methods\ndue to their limited scope. This study introduces an advanced\nmultimodal classification model that integrates clinical, cog-\nnitive, neuroimaging, and EEG data to enhance diagnostic ac-\ncuracy. The model incorporates a feature tagger with a tabu-\nlar data coding architecture and utilizes the TimesBlock mod-\nule to capture intricate temporal patterns in Electroencephalo-\ngrams (EEG) data. By employing Cross-modal Attention Ag-\ngregation module, the model effectively fuses Magnetic Res-\nsonance Imaging (MRI) spatial information with EEG tem-\nporal data, significantly improving the distinction between\nAD, Mild Cognitive Impairment, and Normal Cognition. Si-\nmultaneously, we have constructed the first AD classification\ndataset that includes three modalities: EEG, MRI, and tabu-\nlar data. Our innovative approach aims to facilitate early di-\nagnosis and intervention, potentially slowing the progression\nof AD. The source code and our private ADMC dataset are\navailable at https://github.com/JustlfC03/MSTNet.", "sections": [{"title": "1. INTRODUCTION", "content": "Alzheimer's Disease (AD) is an irreversible, progressive neu-\nrodegenerative disorder characterized by comprehensive de-\nmentia, including memory impairment, executive dysfunc-\ntion, and changes in personality and behavior [15]. This de-\nbilitating condition significantly impairs a patient's thinking,\nmemory, and independence, severely impacting daily life. As\nthe global population ages, the prevalence of AD is increas-\ning, posing substantial challenges to the economy, healthcare\nsystem, patients, and their families. Diagnosing AD in clini-\ncal settings is complicated by the subtlety of early symptoms,\nthe diversity of symptom presentations, lengthy detection pro-\ncesses, and inconsistent diagnostic criteria, which are often\nhighly dependent on a physician's expertise. Mild Cognitive\nImpairment (MCI) represents a critical window period before\nthe onset of dementia [16]. During this stage, the brain under-\ngoes minor and often unnoticed changes, leading to frequent\nmisdiagnosis. Therefore, timely and accurate diagnosis for\npatients in this period has great significance.\nRecently, deep neural network techniques have signif-\nicantly advanced the classification of AD, overcoming the\nlimitations of traditional manual diagnosis methods. Re-\nsearch in this field can be broadly divided into two categories:\nunimodal and multimodal classification tasks. Unimodal di-\nagnosis often falls short in capturing the full complexity\nof AD, resulting in diagnostic challenges and high rates of\nmisdiagnosis. Conversely, multimodal approaches integrate\nmultiple types of data, including clinical information, cog-\nnitive scales, neuroimaging, and EEG signals, to provide a\nmore comprehensive understanding of the disease. Cognitive\nscales, for example, offer a rapid and precise assessment of\nintellectual status and cognitive deficits [17]. Neuroimaging,\nparticularly Magnetic Resonance Imaging (MRI), provides\ndetailed images of brain structures, allowing for the observa-\ntion of changes in brain volume and cortical atrophy. Elec-\ntroencephalograms (EEG) objectively reflect brain neural\nactivity, containing rich physiological and pathological infor-\nmation [18]. The integration of these multimodal data types\nenhances the accuracy and robustness of models, offering\nsignificant advantages over unimodal approaches."}, {"title": "2. RELATED WORK", "content": "Several studies have demonstrated the effectiveness of mul-\ntimodal classification approaches in diagnosing AD, but nei-\nther make full use of multimodal information. For instance,\nLiu et al. [1] introduced a Monte Carlo integrated neural net-\nwork using ResNet50, achieving 90% accuracy. Wang et al.\n[2] utilized a multimodal learning framework with graph dif-\nfusion and hypergraph regularization, reaching 96.48% ac-"}, {"title": "3. METHOD", "content": "As illustrated in Figure 1, we propose an enhanced multi-\nmodal classification model. This model integrates a feature\ntagger with a coding architecture designed specifically for\ntabular data. Furthermore, we employ the TimesBlock mod-\nule to process EEG data, which effectively models complex\ntemporal patterns by performing 2D transformations on the\nmulti-periodic features of the temporal data. To further en-\nhance performance, we utilize a Cross-modal Attention Ag-\ngregation module (CMAA). This module fuses the spatial in-\nformation from MRI data with the temporal information from\nEEG data, capturing the intricate correlations."}, {"title": "3.1. Tabular Feature Encoder", "content": "In the diagnostic process of AD, tabular data such as patient\nclinical data and cognitive test results contain valuable in-\nformation. However, the diverse nature of these data types\nposes challenges in processing them uniformly, often result-\ning in suboptimal diagnostic accuracy. Traditional methods\nhave struggled to fully leverage the richness of this informa-\ntion. Our multimodal classification model addresses these\nchallenges by efficiently processing tabular data using a fea-\nture tagger and Transformer architecture, inspired by FT-\nTransformer [7]. By integrating the feature tagger with an\nencoding architecture tailored for tabular data, our model\nenhances the representation of specific modalities, thereby\nimproving diagnostic accuracy."}, {"title": "3.1.1. Feature Tokenizer", "content": "As illustrated in Figure 2 (a), the primary function of the fea-\nture tokenizer is to convert numerical features (such as a pa-\ntient's age and MMSE score) and categorical features (such\nas gender and education) into embedding vectors. These em-\nbeddings are then input into the coding architecture designed\nfor tabular data for feature extraction. The Feature Tokenizer\ntransforms the input features x into embeddings \\(T\\in R^{k\\times d}\\).\nThe embedding for a given feature \\(x_j\\) is computed as follows:\n\\[T_j = b_j + f_j(x_j) \\in R^d; :\\rightarrow R^d,\\]\nwhere \\(b_j\\) is the bias for the j-th feature. For numerical fea-\ntures, \\(f^{(num)}\\) is implemented as an element-wise multiplica-\ntion with the vector \\(W^{(num)} \\in R^d\\). For categorical features,\n\\(f^{(cat)}\\) is implemented as a lookup table \\(W^{(cat)} \\in R^{S_j\\times d}\\). The\nspecific formulas are as follows:\n\\[T^{(num)} = b^{(num)} + x^{(num)}\\cdot W^{(num)} \\in R^d,\\]\n\\[T^{(cat)} = b^{(cat)} + eW^{(cat)} \\in R^d,\\]\n\\[T=stack[T^{(num)}_{1},..., T^{(num)}_{k(num)}, T^{(cat)}_{1}, ..., T^{(cat)}_{k(cat)}] \\in R^{k\\times d},\\]\nwhere \\(e_j\\) is a one-hot vector for the categorical feature."}, {"title": "3.1.2. Tabular Encoder Architecture", "content": "The Tabular Encoder Architecture processes the embedding\nvectors generated by the feature tokenizer. It captures com-\nplex relationships between features using a multi-head self-\nattention mechanism and a feed-forward network. The final\noutput of the Tabular Encoder is derived from the processed\noutput of the Transformer layer. In the Transformer's output,\nthe [CLS] token summarizes the entire input sequence. This\ntoken is passed to the prediction layer, where it is combined\nwith aggregated features from the image and EEG modali-\nties via cross-modal attention. We also employ the PreNorm\nvariant for easier optimization. In this setting, it is necessary\nto remove the first normalization from the first Transformer\nlayer to achieve optimal performance."}, {"title": "3.2. Temporal Feature Encoder", "content": "EEG data contain rich temporal information but are often\nplagued by noise and irrelevant details. Inspired by Wu et al.\n[8], we incorporate the TimesBlock module to handle EEG\ndata. As shown in Fig. 2 (b), the TimesBlock module ef-\nfectively models complex temporal patterns by performing a\n2D transformation on the multi-periodic features of the time-\nseries data, decomposing complex time-varying patterns into\nintra-periodic and inter-periodic variations.\nThe process begins by transforming a one-dimensional\ntime series \\(X_{1D} \\in R^{T\\times C}\\), where T is the time length and\nC is the channel dimension, using a fast Fourier transform\n(FFT) to calculate periodicity:\n\\[A = Avg(Amp(FFT(X_{1D}))),\\]\n\\[f_1,..., f_k = arg Topk(A) _{f^*\\in\\{1,...,[]\\}},\\]\n\\[p_1,..., p_k = \\frac{T}{f_1},\u2026\u2026, \\frac{T}{f_k},\\]\nwhere \\(A \\in R^T\\) represents the intensity of each resolved fre-\nquency component in \\(X_{1D}\\). The k frequencies \\(\\{f_1,\u2026, f_k\\}\\)\nwith the largest intensity correspond to the most significant\ncycle lengths \\(\\{p_1,\u2026\u2026,p_k\\}\\). This process extracts key cycle\nfeatures and filters out noise. Next, the original time series\n\\(X_{1D}\\) is folded based on the selected cycles:\n\\[X_{D} = Reshape_{p_i, f_i}(Padding(X_{1D})), i\\in \\{1,\u2026,k\\},\\]"}, {"title": "3.3. Cross-modal Attention Aggregation Module", "content": "MRI data provide spatial information, while EEG signals of-\nfer temporal insights. Combining them allows for a more\ncomprehensive analysis. We use a CMAA module to fuse\nMRI and EEG features effectively, capturing complex corre-\nlations in challenging multimodal tasks.\nInitially, 3D MR images are processed using the Dense-\nBlock module to capture detailed brain structural information,\nsuch as cortical atrophy and volume changes. Concurrently,\nEEG features extracted by the TimesNet Block are scaled into\n3D features, enhancing temporal information representation.\nCross-modal attention is then applied to the processed MRI\nand EEG features. The advanced features obtained through\ncross-modal attention are then combined with the simple con-\ncatenated features in the aggregation module. Finally, in the\naggregation module, integrated features further capture deep\nrelationships through residual dense connectivity."}, {"title": "4. EXPERIMENT", "content": ""}, {"title": "4.1. Dataset and Experimental Details", "content": "Our experiments were conducted on our private ADMC\ndataset, which includes EEG, MRI, and scale data from 100\nsubjects. The subjects' demographic details are as follows:\nmean age of 72.4 years, age range from 56 to 93 years, with\n56 females and 22 married individuals. The dataset is divided\ninto 80 samples for training and 20 samples for evaluation.\nWe plan to make this private dataset publicly available to\nfacilitate further research. We implemented the MSTNet net-\nwork model using the PyTorch framework and trained it on\nan NVIDIA Tesla V100 GPU. During training, we set the\ndropout rate to 0.1, the batch size to 100, and the learning rate\nto 0.0001. The training process lasted for 24 hours."}, {"title": "4.2. Comparison Experiment", "content": "As presented in Table 1, we compare the proposed MSTNet\nmodel with several advanced unimodal and multimodal mod-\nels. The unimodal models include HTCF [13], JD-CNN [11],"}, {"title": "4.3. Ablation Study", "content": "We conducted a series of ablation experiments to assess\nthe impact of the DenseBlock module, TimesBlock module,\nCMAA module, and Feature Biases on the performance of\nMSTNet. The results are detailed in Table 1. The experiments\nreveal that the removal of each module results in varying de-\ngrees of performance decline. Specifically, the absence of\nthe DenseBlock module severely hampers the model's ca-\npability to process and extract intricate visual features from\nimages, resulting in a 15% reduction in Accuracy. Similarly,\nremoving the TimesBlock module significantly diminishes\nthe model's ability to capture temporal features from the\nEEG signal, leading to a 10% reduction in Accuracy. The\nexclusion of the CMAA module leads to a reduction in mul-\ntimodal integration capabilities, resulting in a 15% reduction\nin Accuracy. Additionally, the model's stability is compro-\nmised without the Feature Biases, causing a 5% reduction\nin Accuracy. These findings underscore the essential role of\neach module in maintaining the overall effectiveness of our\nproposed advanced MSTNet model."}, {"title": "5. CONCLUSION", "content": "In this study, we introduce an advanced multimodal classifi-\ncation model designed for the diagnosis of AD. This model\novercomes the limitations of traditional single-modality ap-\nproaches by effectively integrating diverse data types. The\nTimesBlock module is utilized to process EEG data, enabling\nthe capture of complex temporal patterns. Additionally, a fea-\nture tagger and table encoding architecture are employed to\nenhance feature extraction from tabular data. Cross-modal\nattention aggregation is applied to integrate spatial MRI data\nwith temporal EEG information. Our model has demonstrated\nsuperior performance in classifying AD, MCI, and NC, as\nconfirmed by experiments conducted on our private ADMC\ndataset. By leveraging the strengths of multiple data modali-\nties, our approach represents a significant advancement in the\nfield of neurodegenerative disease classification."}]}