{"title": "TOWARD ROBUST EARLY DETECTION OF ALZHEIMER'S DISEASE VIA AN INTEGRATED MULTIMODAL LEARNING APPROACH", "authors": ["Yifei Chen", "Shenghao Zhu", "Zhaojie Fang", "Chang Liu", "Binfeng Zou", "Yuhe Wang", "Shuo Chang", "Fan Jia", "Feiwei Qin", "Jin Fan", "Yong Peng", "Changmiao Wang"], "abstract": "Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by memory loss, executive dysfunction, and personality changes. Early diagnosis is challenging due to subtle symptoms and varied presentations, often leading to misdiagnosis with traditional unimodal diagnostic methods due to their limited scope. This study introduces an advanced multimodal classification model that integrates clinical, cognitive, neuroimaging, and EEG data to enhance diagnostic accuracy. The model incorporates a feature tagger with a tabular data coding architecture and utilizes the TimesBlock module to capture intricate temporal patterns in Electroencephalograms (EEG) data. By employing Cross-modal Attention Aggregation module, the model effectively fuses Magnetic Resonance Imaging (MRI) spatial information with EEG temporal data, significantly improving the distinction between AD, Mild Cognitive Impairment, and Normal Cognition. Simultaneously, we have constructed the first AD classification dataset that includes three modalities: EEG, MRI, and tabular data. Our innovative approach aims to facilitate early diagnosis and intervention, potentially slowing the progression of AD. The source code and our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.", "sections": [{"title": "1. INTRODUCTION", "content": "Alzheimer's Disease (AD) is an irreversible, progressive neurodegenerative disorder characterized by comprehensive dementia, including memory impairment, executive dysfunction, and changes in personality and behavior [15]. This debilitating condition significantly impairs a patient's thinking, memory, and independence, severely impacting daily life. As the global population ages, the prevalence of AD is increasing, posing substantial challenges to the economy, healthcare system, patients, and their families. Diagnosing AD in clinical settings is complicated by the subtlety of early symptoms, the diversity of symptom presentations, lengthy detection processes, and inconsistent diagnostic criteria, which are often highly dependent on a physician's expertise. Mild Cognitive Impairment (MCI) represents a critical window period before the onset of dementia [16]. During this stage, the brain undergoes minor and often unnoticed changes, leading to frequent misdiagnosis. Therefore, timely and accurate diagnosis for patients in this period has great significance.\nRecently, deep neural network techniques have significantly advanced the classification of AD, overcoming the limitations of traditional manual diagnosis methods. Research in this field can be broadly divided into two categories: unimodal and multimodal classification tasks. Unimodal diagnosis often falls short in capturing the full complexity of AD, resulting in diagnostic challenges and high rates of misdiagnosis. Conversely, multimodal approaches integrate multiple types of data, including clinical information, cognitive scales, neuroimaging, and EEG signals, to provide a more comprehensive understanding of the disease. Cognitive scales, for example, offer a rapid and precise assessment of intellectual status and cognitive deficits [17]. Neuroimaging, particularly Magnetic Resonance Imaging (MRI), provides detailed images of brain structures, allowing for the observation of changes in brain volume and cortical atrophy. Electroencephalograms (EEG) objectively reflect brain neural activity, containing rich physiological and pathological information [18]. The integration of these multimodal data types enhances the accuracy and robustness of models, offering significant advantages over unimodal approaches."}, {"title": "2. RELATED WORK", "content": "Several studies have demonstrated the effectiveness of multimodal classification approaches in diagnosing AD, but neither make full use of multimodal information. For instance, Liu et al. [1] introduced a Monte Carlo integrated neural network using ResNet50, achieving 90% accuracy. Wang et al. [2] utilized a multimodal learning framework with graph diffusion and hypergraph regularization, reaching 96.48% accuracy by integrating phenotypic features with genetic data. Elazab et al. [19] reviewed AD diagnosis using machine and deep learning models, highlighting multimodal data fusion techniques and recent advancements. Subsequently, Liu et al. [4] developed a cascaded multimodal hybrid Transformer architecture, achieving an AUC of 0.994.Additionally, Lei et al. [5] created the FIL-DMGNN model, which effectively mitigated modal competition, resulting in excellent diagnostic outcomes. Dwivedi et al. [6] proposed a novel method for fusing MR and PET images, achieving high accuracy rates using a dual support vector machine. Lastly, Fang et al. [14] introduced the GFE-Mamba model, which integrates MRI and PET data through a 3D GAN-ViT architecture, demonstrating high accuracy in predicting AD."}, {"title": "3. METHOD", "content": "As illustrated in Figure 1, we propose an enhanced multimodal classification model. This model integrates a feature tagger with a coding architecture designed specifically for tabular data. Furthermore, we employ the TimesBlock module to process EEG data, which effectively models complex temporal patterns by performing 2D transformations on the multi-periodic features of the temporal data. To further enhance performance, we utilize a Cross-modal Attention Aggregation module (CMAA). This module fuses the spatial information from MRI data with the temporal information from EEG data, capturing the intricate correlations."}, {"title": "3.1. Tabular Feature Encoder", "content": "In the diagnostic process of AD, tabular data such as patient clinical data and cognitive test results contain valuable information. However, the diverse nature of these data types poses challenges in processing them uniformly, often resulting in suboptimal diagnostic accuracy. Traditional methods have struggled to fully leverage the richness of this information. Our multimodal classification model addresses these challenges by efficiently processing tabular data using a feature tagger and Transformer architecture, inspired by FT-Transformer [7]. By integrating the feature tagger with an encoding architecture tailored for tabular data, our model enhances the representation of specific modalities, thereby improving diagnostic accuracy."}, {"title": "3.1.1. Feature Tokenizer", "content": "As illustrated in Figure 2 (a), the primary function of the feature tokenizer is to convert numerical features (such as a patient's age and MMSE score) and categorical features (such as gender and education) into embedding vectors. These embeddings are then input into the coding architecture designed for tabular data for feature extraction. The Feature Tokenizer transforms the input features x into embeddings $T\\in R^{k\\times d}$. The embedding for a given feature xj is computed as follows:\n$T_j = b_j + f_j(x_j) \\in R^d_f; :\\rightarrow R^d$,\n(1)"}, {"title": "3.1.2. Tabular Encoder Architecture", "content": "The Tabular Encoder Architecture processes the embedding vectors generated by the feature tokenizer. It captures complex relationships between features using a multi-head self-attention mechanism and a feed-forward network. The final output of the Tabular Encoder is derived from the processed output of the Transformer layer. In the Transformer's output, the [CLS] token summarizes the entire input sequence. This token is passed to the prediction layer, where it is combined with aggregated features from the image and EEG modalities via cross-modal attention. We also employ the PreNorm variant for easier optimization. In this setting, it is necessary to remove the first normalization from the first Transformer layer to achieve optimal performance."}, {"title": "3.2. Temporal Feature Encoder", "content": "EEG data contain rich temporal information but are often plagued by noise and irrelevant details. Inspired by Wu et al. [8], we incorporate the TimesBlock module to handle EEG data. As shown in Fig. 2 (b), the TimesBlock module effectively models complex temporal patterns by performing a 2D transformation on the multi-periodic features of the time-series data, decomposing complex time-varying patterns into intra-periodic and inter-periodic variations.\nThe process begins by transforming a one-dimensional time series $X_{1D} \\in R^{T\\times C}$, where T is the time length and C is the channel dimension, using a fast Fourier transform (FFT) to calculate periodicity:\n$A = Avg(Amp(FFT(X_{1D})))$,\n(3)\n$f_1,\\ldots, f_k = arg\\ Topk(A) \\ f*\\in{1,\\ldots,[]}$,\n(4)\n$p_1,..., p_k = \\frac{T}{f_1},\\ldots,\\frac{T}{f_k}$,\n(5)\nwhere A \u2208 RT represents the intensity of each resolved frequency component in X1D. The k frequencies ${f_1,\\ldots, f_k}$ with the largest intensity correspond to the most significant cycle lengths ${p_1,\\ldots,Pk}$. This process extracts key cycle features and filters out noise. Next, the original time series X1D is folded based on the selected cycles:\n$XD = Reshape_{p_i, f_i}(Padding(X_{1D})), i\\in {1,\\ldots,k}$,\n(6)"}, {"title": "3.3. Cross-modal Attention Aggregation Module", "content": "MRI data provide spatial information, while EEG signals offer temporal insights. Combining them allows for a more comprehensive analysis. We use a CMAA module to fuse MRI and EEG features effectively, capturing complex correlations in challenging multimodal tasks.\nInitially, 3D MR images are processed using the Dense-Block module to capture detailed brain structural information, such as cortical atrophy and volume changes. Concurrently, EEG features extracted by the TimesNet Block are scaled into 3D features, enhancing temporal information representation. Cross-modal attention is then applied to the processed MRI and EEG features. The advanced features obtained through cross-modal attention are then combined with the simple concatenated features in the aggregation module. Finally, in the aggregation module, integrated features further capture deep relationships through residual dense connectivity."}, {"title": "4. EXPERIMENT", "content": "Our experiments were conducted on our private ADMC dataset, which includes EEG, MRI, and scale data from 100 subjects. The subjects' demographic details are as follows: mean age of 72.4 years, age range from 56 to 93 years, with 56 females and 22 married individuals. The dataset is divided into 80 samples for training and 20 samples for evaluation. We plan to make this private dataset publicly available to facilitate further research. We implemented the MSTNet network model using the PyTorch framework and trained it on an NVIDIA Tesla V100 GPU. During training, we set the dropout rate to 0.1, the batch size to 100, and the learning rate to 0.0001. The training process lasted for 24 hours."}, {"title": "4.1. Dataset and Experimental Details", "content": "As presented in Table 1, we compare the proposed MSTNet model with several advanced unimodal and multimodal models. The unimodal models include HTCF [13], JD-CNN [11], and TableTransformer [10]. Among the multimodal models, we consider those developed by Zhang et al. [9], Qiu et al. [3], and Rad et al. [12]. The MSTNet model, equipped with the CMAA module, demonstrates superior performance over the unimodal models across all evaluated metrics. For instance, MSTNet effectively leverages spatial information from MRI and temporal information from EEG, outperforming TableTransformer in this regard. Furthermore, MSTNet solves existing multimodal classification models, which often face challenges in multimodal feature extraction and fusion. Compared to the best unimodal model, MSTNet improved its F1-score by 0.79%, and against the best multimodal model, MSTNet improved its F1-score by 4.17%."}, {"title": "4.2. Comparison Experiment", "content": "We conducted a series of ablation experiments to assess the impact of the DenseBlock module, TimesBlock module, CMAA module, and Feature Biases on the performance of MSTNet. The results are detailed in Table 1. The experiments reveal that the removal of each module results in varying degrees of performance decline. Specifically, the absence of the DenseBlock module severely hampers the model's capability to process and extract intricate visual features from images, resulting in a 15% reduction in Accuracy. Similarly, removing the TimesBlock module significantly diminishes the model's ability to capture temporal features from the EEG signal, leading to a 10% reduction in Accuracy. The exclusion of the CMAA module leads to a reduction in multimodal integration capabilities, resulting in a 15% reduction in Accuracy. Additionally, the model's stability is compromised without the Feature Biases, causing a 5% reduction in Accuracy. These findings underscore the essential role of each module in maintaining the overall effectiveness of our proposed advanced MSTNet model."}, {"title": "4.3. Ablation Study", "content": "In this study, we introduce an advanced multimodal classification model designed for the diagnosis of AD. This model overcomes the limitations of traditional single-modality approaches by effectively integrating diverse data types. The TimesBlock module is utilized to process EEG data, enabling the capture of complex temporal patterns. Additionally, a feature tagger and table encoding architecture are employed to enhance feature extraction from tabular data. Cross-modal attention aggregation is applied to integrate spatial MRI data with temporal EEG information. Our model has demonstrated superior performance in classifying AD, MCI, and NC, as confirmed by experiments conducted on our private ADMC dataset. By leveraging the strengths of multiple data modalities, our approach represents a significant advancement in the field of neurodegenerative disease classification."}, {"title": "5. CONCLUSION", "content": "where by is the bias for the j-th feature. For numerical features, $f^{(num)}$ is implemented as an element-wise multiplication with the vector $W^{(num)} \\in R^d$. For categorical features, $f^{(cat)}$ is implemented as a lookup table $W^{(cat)} \\in R^{S_j \\times d}$. The specific formulas are as follows:\n$T^{(num)} = b^{(num)} + x^{(num)} \\cdot W^{(num)} \\in R^d$,\n$T^{(cat)} = b^{(cat)} + eW^{(cat)} \\in R^d$,\n$T = stack[T^{(num)}_1,..., T^{(num)}_{k(num)}, T^{(cat)}_1, ..., T^{(cat)}_{k(cat)}] \\in R^{k\\times d}$,\n(2)\nwhere eff is a one-hot vector for the categorical feature."}]}