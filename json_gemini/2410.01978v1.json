{"title": "LLM+KG@VLDB'24 Workshop Summary\nData Management Opportunities in Unifying Large Language\nModels+Knowledge Graphs", "authors": ["Arijit Khan", "Tianxing Wu", "Xi Chen"], "abstract": "The unification of large language models (LLMs) and\nknowledge graphs (KGs) has emerged as a hot topic. At\nthe LLM+KG'24 workshop, held in conjunction with\nVLDB 2024 in Guangzhou, China, one of the key themes\nexplored was important data management challenges and\nopportunities due to the effective interaction between\nLLMs and KGs. This report outlines the major direc-\ntions and approaches presented by various speakers dur-\ning the LLM+KG'24 workshop.", "sections": [{"title": "1. INTRODUCTION", "content": "Large language models a relatively newer form of\ngenerative AI (genAI), have become ubiquitous. They\nare revolutionizing natural language processing with ap-\nplications ranging from solving problems, streamlining\nworkflows, augmenting analytics, code synthesis, to ac-\ncessing information via conversational functionality, e.g.,\nCopilots and digital assistants. LLMs are skilled at learn-\ning stochastic language patterns in the form of paramet-\nric knowledge. Based on it, they predict the next tokens\nfor the given contexts. However, LLMs may lack con-\nsistent knowledge representations. Hence, they often\nexperience hallucinations and generate unreliable or fac-\ntually incorrect outputs. Knowledge graphs can offer ex-\nternal, factual, and up-to-date knowledge to LLMs via,\ne.g., retrieval augmented methods, thereby improving\nthe LLMs' accuracy, consistency, and transparency. On\nthe other hand, LLMs can also facilitate data curation,\nknowledge extraction, KG creation, completion, embed-\nding, and various downstream tasks over KGs such as\nrecommendation and question answering (QA). Further-\nmore, the unification of LLMs and KGs creates new\ndata management opportunities and challenges includ-\ning consistency, scalability, knowledge editing, privacy,\nfairness, explainability, data regulations, human-in-the-\nloop, software-hardware collaboration, cloud-based so-\nlutions, and AI-native databases.\nThe LLM+KG'24 ambition was to provide a unique\nplatform to researchers and practitioners for presenta-"}, {"title": "2. \u039a\u0395\u03a5\u039dOTES", "content": "After the welcome by the LLM+KG'24 chairs, the\nprogram started with three keynotes by Guilin Qi (South-\neast University, China), Haofen Wang (Tongji Univer-\nsity, China), and Wei Hu (Nanjing University, China),\nrespectively."}, {"title": "2.1 Integrating KGs with LLMs: From the\nPerspective of Knowledge Engineering", "content": "The first keynote talk on integrating KGs with LLMs\nfrom the knowledge engineering point-of-view was given\nby Guilin Qi from Southeast University, China. In his\nkeynote, Prof. Qi started with the enlightening question,\n\"What is knowledge?\" and shared a number of interest-\ning perspectives. First, according to the Oxford Dic-\ntiory, knowledge is the information, understanding, and\nskills that one gains through education or experience.\nSecond, informally speaking, knowledge can be fact-\nbased, description of information (e.g., text, image), or\nskills obtained by practice. Third, one way to decide\nwhether artificial intelligence (AI) has human intelli-\ngence or not could possibly be by the Al's ability to\nlearn and apply knowledge. Fourth, a Knowledge Base\n(KB) is a collection of knowledge, including documents,\nimages, triples, rules, parameters of neural networks,\netc. Fifth, a KG is a data structure for representing\nknowledge using a graph. Prof. Qi further emphasized\n'KGs as knowledge bases' as follows: \u201cKnowledge graphs\noriginated from how machines represent knowledge, use\ngraph structures to describe relationships between things,\ndeveloped in the rise of Web technologies, and landed in\napplications such as search engine, intelligent QA, and\nrecommender systems\".\nNext, Prof. Qi introduced the fundamentals of lan-\nguage models and whether they can be used as 'paramet-\nric knowledge bases' [28]. He compared the reasoning\ncapabilities of LLMs and KGs, their advantages and dis-\nadvantages, and elaborated significant research scopes\nand practical values due to the complementary nature\nand mutual enhancements between symbolic knowledge\nof KGs and parametric knowledge of LLMs.\nIn the direction of 'KGs for LLMs', Prof. Qi dis-\ncussed how KGs enhance pre-training [48], fine-tuning\n[43], inference [44], prompting [3, 16], retrieval/ knowl-\nedge augmented generation [46], knowledge editing [52,\n47], knowledge fusion [31], and knowledge validation\n[10] of LLMs. In the other direction of 'LLMs for KGs',\nhe mentioned knowledge engineering by LLMs, where\nLLMs can act as both resources (e.g., data augmenta-\ntion) and enablers (e.g., encoding, reading comprehen-\nsion, and QA). He also stated several opportunities such\nas LLMs for entity and relation extraction, triple gen-\neration, ontology matching [9], entity alignment [13],\nknowledge base QA [35], ontology reasoning [39], and\nKG reasoning [25, 50, 38], among others.\nProf. Qi concluded his keynote by underlining inter-\nesting opportunities due to LLMs+KGs integration and\nthe engineering efforts required to work properly in ar-\neas, e.g., OpenKG(https://github.com/OpenKG-ORG)\nand new knowledge platforms to support generalizable,\ntrustable, and stable knowledge services. His conclud-\ning remark was to look at \"Language as the \"form\",\nknowledge as the \"heart\", and graph as the \"skeleton\":\""}, {"title": "2.2 Industry-level KG Platforms for Large-\nscale, Diverse, and Dynamic Scenarios", "content": "In the second keynote talk on industry-level knowl-\nedge graph platforms, Haofen Wang from Tongji Uni-\nversity, China stated that traditional knowledge seman-\ntic frameworks such as RDF/OWL and labeled prop-\nerty graph (LPG) have major limitations in knowledge\nmodeling and management and are often inadequate in"}, {"title": "2.3 KG-based LLM Fine-tuning and\nIts Applications", "content": "The third keynote talk on knowledge graph-enhanced\nlarge language model fine-tuning was given by Wei Hu\nfrom Nanjing University, China. In his keynote, Prof.\nHu emphasized the knowledge gap problem of general-\npurpose LLMs, that is, they often lack accurate domain\nknowledge, resulting in inaccurate and unreliable out-\nputs, and even difficulty in real-world applications.\nAmong various knowledge enhancement techniques\nfor LLMs, Prof. Hu focused on an LLM fine-tuning\nframework with adaptive integration of multi-source KGs,\nconsisting of knowledge extraction, knowledge fusion,\nand KG-enhanced LLMs. In the field of knowledge ex-\ntraction, he introduced problems such as domain named\nentity recognition, document-level relation extraction [41],\ncontinual event extraction [42], document-level event\ncausality identification [19], and continual relation ex-\ntraction [40]. In the area of knowledge fusion, Prof. Hu\ndiscussed embedding-based entity alignment [36, 33, 7,\n32], knowledge transfer [14, 45], adding human-in-the-\nloop [12, 11], benchmarking, and the OpenEA toolkit\n[34]. In the field of KG-enhanced LLMs fine-tuning, he\nintroduced KnowLA [20], a knowledgeable adaptation\nmethod for PEFT (parameter-efficient fine-tuning), par-\nticularly for LoRA (Low-Rank Adaptation). (i) KnowLA\nwith LoRA can align the space of the LLM with the\nspace of KG embeddings, and (ii) KnowLA can acti-\nvate the parameterized potential knowledge that origi-\nnally exists in the LLM, even though the used KG does\nnot contain such knowledge.\nIn the concluding remarks, Prof. Hu underscored in-\nteresting applications of KG-enhanced LLMs in trans-\nlating configuration files during device replacements in\ncommunication networks, unified PEFT+RAG (retrieval\naugmented generation), as well as the research activi-"}, {"title": "3. INDUSTRIAL INVITED TALK", "content": "Siwei Gu and Yihang Yu from NebulaGraph deliv-\nered an inspiring invited industrial talk and a hands-on-\ndemonstration on GraphRAG [23]. NebulaGraph (https://\ngithub.com/vesoft-inc/nebula) is a cloud-native,\nopen-source, distributed graph database, capable of han-\ndling large-scale graphs having trillions of nodes and\nedges, with milliseconds latency. NebulaGraph has over\n3000 user adaptations globally, and is used by Meituan,\nWeChat, and Snapchat, among others.\nIntegrating GenAI with Graph: Innovations and In-\nsights from NebulaGraph. Retrieval-Augmented Gen-\neration (RAG) is a technique to optimize the output of\nan LLM so that it references an authoritative, up-to-\ndate knowledge base outside of its training corpus be-\nfore generating a response. Given a user's query, the\nclassic RAG approach uses vector similarity to retrieve\nsemantically similar matches. It also builds offline in-\ndexes over embedding vectors to speed-up online re-\ntrieval. However, the conventional RAG suffers from\nthe following limitations."}, {"title": "4. RESEARCH PAPERS", "content": "The 9 peer-reviewed research papers presented in this\nworkshop can be broadly classified into three categories:\nLLMs for KGs (4 papers), KGs for LLMs (2 papers),\nand unifying LLMs+KGs (3 papers)."}, {"title": "4.1 LLMs for KGs", "content": "Jixuan Nie et al. leverage domain-specific knowledge\nfrom ontology and Chain-of-Thought (CoT) prompts to\nextract higher-quality triples from unstructured text [24].\nEmily Groves et al. empirically compare three natu-\nral language processing paradigms \u2013 in-context learn-\ning, fine-tuning, and supervised learning in automated\nknowledge curation for biomedical ontologies [6]. Yongli\nMou et al. explore in-context learning capabilities of\nGPT-4 for instruction-driven adaptive knowledge graph\nconstruction, while also proposing a self-reflection mech-\nanism to enable LLMs to critically evaluate their outputs\nand learn from errors using examples [21]. Daham M.\nMustafa et al. use the W3C Open Digital Rights Lan-\nguage (ODRL) ontology and its documentation to for-\nmulate prompts in large language models and generate\nusage policies in ORDL from natural language instruc-\ntions [22]."}, {"title": "4.2 KGs for LLMs", "content": "Xinfu Liu et al. propose an enhanced collaborative\nLLMs method for open-set object recognition, incorpo-\nrating KGs to alleviate the factual hallucination prob-\nlem in LLMs [18]. Fali Wang et al. study a novel\ninfuser-guided knowledge integration framework to in-\ntegrate unknown knowledge into LLMs efficiently with-\nout unnecessary overlap of known knowledge [37]."}, {"title": "4.3 Unifying LLMs+KGs", "content": "Ningyu Zhang et al. introduce OneEdit a neural-\nsymbolic prototype system for collaborative knowledge\nediting using natural language and facilitating easy-to-\nuse knowledge management with KGs and LLMs [49].\nHanieh Khorashadizadeh et al. present a survey on the\nsynergistic relationship between LLMs and KGs [15].\nEmanuele Cavalleri et al. present the SPIREX system to\nextract triples from scientific literature involving RNA\nmolecules [4]. They exploit schema constraints in the\nformulation of LLM prompts and also utilize graph ma-\nchine learning on an RNA-based knowledge graph (RNA-\nKG) to assess the plausibility of extracted triples."}, {"title": "5. PANEL", "content": "The workshop was concluded with a panel discussion\non the unification of LLMs, KGs, and Vector databases\n(Vector DBs). The panelists were Wei Hu (Nanjing Uni-\nversity, China), Shreya Shankar (UC Berkeley, USA),\nHaofen Wang (Tongji University, China), and Jianguo\nWang (Purdue University, USA).\nLLMs, KGs, and Vector DBs: Synergy and Opportu-\nnities for Data Management. The LLM+KG'24 chairs\nfirst asked some questions.\nQ1. What are the synergies among LLMs, Vector DBs,\nand graph data management including KGs?"}, {"title": "6. CONCLUSIONS", "content": "The keynotes and presentations at LLM+KG'24 pro-\nvided an overview of current developments and open\nproblems in the domain of KGs for LLMs, LLMs for\nKGs, and unifying LLMs+KGs. In summary, we con-\nclude that there are several interesting works going on\nin the area of LLMs+KGs, with a lot of research still\nneeds to be done. Hopefully, this report and the accepted\npapers from LLM+KG'24 will spur others to work on\nmany emerging data management issues in this domain."}]}