{"title": "Efficient Transformer for High Resolution Image Motion Deblurring", "authors": ["Amanturdieva Akmaral", "Muhammad Hamza Zafar"], "abstract": "This paper presents a comprehensive study and improvement of the Restormer architecture for high-resolution image motion deblurring. We introduce architectural modifications that reduce model complexity by 18.4% while maintaining or improving performance through optimized attention mechanisms. Our enhanced training pipeline incorporates additional transformations including color jitter, Gaussian blur, and perspective transforms to improve model robustness as well as new Frequency loss term. Extensive experiments on the RealBlur-R, RealBlur-J [10], and Ultra-High-Definition Motion blurred (UHDM) [23] datasets demonstrate the effectiveness of our approach. The improved architecture shows better convergence behavior and reduced training time while maintaining competitive performance across challenging scenarios. We also provide detailed ablation studies analyzing the impact of our modifications on model behavior and performance. Our results suggest that thoughtful architectural simplification combined with enhanced training strategies can yield more efficient yet equally capable models for motion deblurring tasks. Code and Data Available at https://github.com/hamzafer/image-deblurring.", "sections": [{"title": "I. INTRODUCTION", "content": "Image deblurring is a fundamental problem in computer vision and image processing, crucial for applications such as photography, surveillance, medical imaging, and autonomous systems [23]. The goal of image deblurring is to restore a clear, sharp image from a blurred input, where the blur may arise due to factors such as focus issues, camera shake, or rapid movement of the target [21], [22]. This blur severely reduces the utility of the images by obscuring crucial details, thereby degrading the performance of downstream computer vision tasks such as object detection, recognition, and segmentation.\nTraditional convolutional neural networks (CNNs) have been widely used for computer vision tasks due to their strong feature extraction capabilities [2]. However, CNNS are inherently limited by their fixed receptive fields, making it challenging for them to capture long-range dependencies effectively, particularly in high-resolution images [20]. \u03a4\u03bf overcome these limitations, recent advancements in deep learn-ing have introduced Transformer-based architectures, which excel in capturing global context through self-attention mechanisms [4], [16]. Transformers, originally developed for natural language processing, have shown remarkable success in vision tasks by modeling relationships over entire images, providing significant improvements in image quality restoration tasks [4].\nIn this study, we explore the use of Restormer [20], an efficient Transformer model specifically designed for high-resolution image restoration. Restormer introduces a multi-Dconv head transposed attention mechanism and a gated-Dconv feed-forward network, which together allow it to model long-range dependencies while maintaining computational efficiency [20]. This Transformer model is well-suited for high-resolution images, as it captures both local and global inter-actions without the need for computationally prohibitive self-attention over large spatial resolutions [20].\nOur focus is the specific type of deblurring task: motion blur. Motion blur is typically caused by the movement of the camera or objects within the scene, resulting in smeared or streaked regions across the image [6].\nTo evaluate the performance of Restormer in this con-texts, we conduct experiments on a variety of benchmark datasets. For motion deblurring, we reproduced results us-ing the RealBlur-R and RealBlur-J datasets, which provide real-world blurred images with ground truth references [10]. Additionally, we curated hard positive and negative examples from these datasets to fine-tune the pre-trained model on the RealBlur [10]. This fine-tuning aims to further improve the model's ability to generalize across different blur types and intensities, especially in real world conditions. Furthermore, we retrained the baseline model with provided parameters to validate reproducibility of the results and conduct ablation studies for further improvements. To enhance the baseline model, we conducted comprehensive research to include modi-fications to the architecture and the training process. Finally, to evaluate our additions we benchmark the described models on a novel dataset provided in [23] called Ultra-High-Definition Motion blurred set (UHDM) for motion blur tasks. To the best of our knowledge, this dataset has not yet been evaluated using the Restormer model.\nIn this report, we describe our methodology for fine-tuning Restormer on deblurring datasets and assess the effectiveness of our approach in enhancing the model for restoring image clarity across different blurring conditions."}, {"title": "II. RELATED WORK", "content": "This study conducts a comprehensive comparative analy-sis of the primary methodology against existing approaches in the field. Through critical evaluation, both strengths and limitations of various methodologies are examined, leading to the identification of significant research opportunities andknowledge gaps. The following publications were selected for comparative analysis:\n\u2022\nUformer [17]: The paper presents a Transformer-based architecture for image restoration tasks, featuring a U-shaped hierarchical design enhanced with two key components. The first is the Locally-enhanced Win-dow (LeWin) Transformer block, which employs non-overlapping window-based self-attention to efficiently capture long-range dependencies while reducing compu-tational complexity. The second is a learnable multi-scale restoration modulator that adaptively adjusts features across multiple decoder layers to enhance restoration quality.\n\u2022\nStripformer [15]: Authors introduce a novel transformer-based architecture for image deblurring that efficiently handles region-specific blur patterns through strip-based attention mechanisms. The advancement includes two key components: intra-strip attention for pixel-wise feature dependencies within horizontal and vertical strips, and inter-strip attention for capturing global region-wise cor-relations.\n\u2022\nMulti-scale Cubic-Mixer [26]: This study proposes Multi-scale Cubic-Mixer, a deep network architecture for image deblurring that operates without self-attention mechanisms to achieve computational efficiency. The model's novelty lies in its frequency-domain approach, where it processes blurred images through Fast Fourier Transform to work with both real and imaginary com-ponents of the Fourier coefficients. By operating in the frequency domain and utilizing a three-dimensional (Channel, Width, Height) MLP structure, the network is able to captures both long-range dependencies and local features from blurred images.\n\u2022\nAdversarial Promoting Learning (APL) [25]: A novel framework that jointly handles defocus detection and deblurring without requiring pixel-level annotations or paired deblurring ground truth is presented. The approach is based on the complementary nature of these tasks defocus detection guides deblurring by segmenting focused areas, while effective deblurring necessitates ac-curate defocus detection. The framework consists of three key components: a defocus detection generator (Gws) that produces detection maps to segment focused and defocused regions, a self-referenced deblurring genera-tor (Gsr) that utilizes the focused areas as references to restore defocused regions, and a discriminator that enables adversarial optimization. Both generators are trained alternately through adversarial learning against unpaired fully-clear images, where Gsr aims to produce convincing deblurred results while Gws is driven to generate accurate detection maps to guide the deblurring process.\n\u2022\nTest-time Local Converter (TLC) [3]: Authors propose a TLC approach to address the train-test inconsistency in image restoration networks that use global operations(like global average pooling and normalization). The key in-sight is that these operations behave differently during training with patches versus inference with full images, leading to shifts in feature distribution that degrade per-formance. TLC converts these global operations to local ones during inference by aggregating features within local spatial windows rather than across the entire image."}, {"title": "A. Positive Aspects", "content": "\u2022\nRestormer is a versatile model applicable to various image restoration tasks (e.g., denoising, and deblurring). Its design allows it to handle different restoration tasks within the same framework, while many other methods are more specialized.\n\u2022\nRestormer's architecture is specifically tailored for high-resolution images, where traditional CNNs or other Transformers would struggle with the larger spatial di-mensions."}, {"title": "B. Negative Aspects", "content": "\u2022\nRestormer is optimized as a general-purpose restoration tool and lacks explicit mechanisms to address defocus-specific artifacts. Models like APL [25], which focus on defocus detection and targeted deblurring, could outper-form it in scenarios with significant defocus blur.\n\u2022\nUnlike frameworks that integrate defocus blur detection, Restormer lacks built-in mechanisms to identify and focus on blurred regions, potentially leading to over- or under-processing of different image areas."}, {"title": "C. Research Gap", "content": "Restormer model's complexity leads to substantial memory and computational demands, especially during inference on devices with limited resources. Models like Stripformer, which employ simpler, strip-based attention, demonstrate that reduc-ing model complexity without sacrificing performance is fea-sible. A more streamlined architecture could make Restormer more accessible for real-time applications and edge devices. It's substantial training time, is a significant limitation in practical settings. In addition, incorporating region-specific blur detection could be an another area of research."}, {"title": "III. RESTORMER OVERVIEW", "content": "This section covers the details of the proposed method in [20]."}, {"title": "A. Detailed Summary of Restormer Model [20]", "content": "The Restormer model introduces an efficient Transformer architecture specifically designed for high-resolution image restoration tasks by addressing computational bottlenecks. This is achieved through innovative modifications to the multi-head self-attention (SA) layer and the adoption of a multiscale hierarchical module, which reduces computational demands compared to traditional single-scale networks."}, {"title": "B. Pipeline Overview [20]", "content": "Restormer follows a 4-level symmetric encoder-decoder architecture (see Figure 1). Initially, a degraded input image $I \\in R^{H \\times W \\times 3}$ is processed using a convolution layer to extract low-level feature embeddings $F_o \\in R^{H \\times W \\times C}$. These features are passed through the encoder-decoder pipeline, which progressively reduces spatial dimensions while increas-ing the channel capacity. Each level of the encoder-decoder architecture is composed of multiple Transformer blocks, with the number of blocks increasing progressively from the upper levels to the lower levels, ensuring computational efficiency.\n\u2022\nEncoder: Downsampling is performed hierarchically to extract latent features, while the number of Transformer blocks increases progressively across levels to enhance efficiency.\n\u2022\nDecoder: The decoder reconstructs high-resolution rep-resentations from the low-resolution latent features $F_l \\in $][R^{H/8 \\times W/8 \\times 8C} using upsampling techniques.\nTo manage downsampling and upsampling, the architecture employs pixel-unshuffle and pixel-shuffle operations [14], re-spectively. Encoder features are concatenated with decoder features through skip connections [11] to enhance the recovery of fine details. Post-concatenation, a 1 \u00d7 1 convolution is applied at all levels (except the top) to halve the channel count. The refined feature maps $F_d$ are enriched at a high spatial resolution in the final refinement stage, improving structural and textural detail preservation. The output residual image $R \\in E R^{H \\times W \\times 3}$ is combined with the input to produce the restored image: $\\hat{I} = I + R$.\nThe Restormer architecture introduces two primary innova-tions in its Transformer block:\n1) Multi-Dconv Head Transposed Attention (MDTA):\nMDTA redefines self-attention (SA) by computing cross-covariance across channels instead of spatial dimensions, resulting in a global attention map with linear complex-ity. It involves depth-wise convolutions to enhance local context before generating the attention map. More pre-"}, {"title": "1) Multi-Dconv Head Transposed Attention (MDTA):", "content": "cisely, from a layer-normalized tensor $Y \\in R^{\\hat{H} \\times \\hat{W} \\times \\hat{C}}$,\nthe MDTA module generates query (Q), key (K), and\nvalue (V) projections, enriched with local context. This\nis achieved using 1 \u00d7 1 point-wise convolutions ($W^{p}$) \nfor cross-channel aggregation and 3 \u00d7 3 depth-wise\nconvolutions ($W^{d}$) for encoding spatial context:\n$Q = W_d W_p Y$, $K = W_d W_p Y$, $V = W_d W_p Y$.\nThe query and key are reshaped such that their dot-product interaction produces a transposed-attention map\n$A \\in R^{C \\times C}$, avoiding the large regular attention map of\nsize $R^{\\hat{H}\\hat{W} \\times \\hat{H}\\hat{W}}$. The MDTA process is defined as:\n$\\bar{X} = W_p \\cdot Attention(Q), K, V) + X$,\n$Attention(Q, K, V) = V \\cdot Softmax(\\frac{KQ}{\\alpha})$\nwhere $\\bar{X}$ and X are the input and output feature\nmaps. Projections $Q\\in R^{\\hat{H}\\hat{W} \\times \\hat{C}}$, $K \\in R^{C \\times \\hat{H}\\hat{W}}$, and\n$V \\in R^{\\hat{H}\\hat{W} \\times C}$ are obtained by reshaping tensors from\nthe original $R^{\\hat{H} \\times \\hat{W} \\times \\hat{C}}$. Here, $\\alpha$ is a learnable scaling\nparameter for controlling the dot-product magnitude,\nand channels are divided into multiple heads to learn\nattention maps in parallel, similar to conventional multi-head self-attention (SA) [16]."}, {"title": "2) Gated-Dconv Feed-Forward Network (GDFN):", "content": "The GDFN module uses the element-wise product of two parallel transformations\n\u2014 one linear and the other activated with the GELU [5] non-linearity to control the flow of information through the network. Depth-wise convolutions are applied to encode spatially neighboring pixel information, further enriching the local context. Given an input tensor $X \\in R^{H \\times W \\times \\hat{C}}$, the GDFN is defined as:\n$\\bar{X} = W \\cdot Gating(X) + X$,\n$Gating(X) = \\phi (W_l^2(LN(X))) \\odot W_l^2(LN(X))$,"}, {"title": "C. Implementation Details [20]", "content": "The model is trained progressively, starting with smaller image patches and bigger batch size in earlier epochs and gradually transitioning to larger patches and smaller batch size a process called progressive training. The Restormer model is trained with the objective of restoring sharpness and fine details, minimizing perceptual and structural differences between deblurred images and ground truth.\n\u2022\nKey hyperparameters and configurations used in training Restormer [20] include the following:\n\u2022\nTransformer Architecture: The model employs a hierar-chical structure with the following configurations across levels:\nNumber of Transformer Blocks: [4, 6, 6, 8] for levels 1 to 4.\nAttention Heads in MDTA: [1, 2, 4, 8], progres-sively increasing to capture richer contextual infor-mation.\nNumber of Channels: [48, 96, 192, 384], allowing for greater feature representation at deeper levels.\nThe refinement stage contains 4 additional Transformer blocks, and the channel expansion factor in GDFN is set to \u03b3 = 2.66.\n\u2022 Learning Rate: The initial learning rate is set to 3\u00d710\u22124 and is gradually reduced to 10\u22126 using a cosine annealing schedule [8]. This helps prevent overfitting and stabilizes convergence during training.\n\u2022\nOptimizer: The AdamW optimizer [7] is used with parameters \u03b21 = 0.9, \u03b22 = 0.999, and a weight decay of 1 x 10\u22124.\n\u2022\nLoss Function: The L\u2081 loss is employed for training, ensuring pixel-wise precision and minimizing absolute differences between predictions and ground truth. It is also known as mean absolute error (MAE), which is defined as:\n$L_1 = \\frac{1}{N}\\sum_{i=1}^{N} \\mid y_i - \\hat{y_i} \\mid,$\nwhere N is the number of pixels, \u0177r represents the predicted pixel value, and yi is the corresponding ground truth value. This loss penalizes deviations linearly, en-couraging the model to minimize the absolute differences between predictions and targets.\n\u2022 Training Schedule: Models are trained for 300K itera-tions, starting with a patch size of 128 \u00d7 128 and a batch"}, {"title": "\u2022 Training Schedule:", "content": "size of 64. Progressive learning is applied by updating patch size and batch size pairs as follows:\n{(1602, 40), (1922, 32), (2562, 16), (3202, 8), (3842, 8)},\nat iterations {92K, 156K, 204K, 240K, 276K}, respec-tively.\nData Augmentation: Horizontal and vertical flips are applied to enhance the diversity of the training dataset."}, {"title": "D. Evaluation Metrics [20]", "content": "Authors in [20] compute evaluation metrics using standard practices to ensure comparability with existing methods. Be-low are the details of the metrics:\na) PSNR/SSIM in Y Channel:: PSNR and SSIM scores are computed using the Y channel of the YCbCr color space [20]. This focuses the evaluation on luminance infor-mation, which is critical for perceptual quality, while ignoring chrominance channels (Cb, Cr) that may introduce biases.\nThe PSNR measures the ratio between the maximum pos-sible intensity of an image and the mean squared error (MSE) between the predicted image \u00ce and the ground truth I. It is defined as:\n$PSNR = 10 log_{10} (\\frac{MAX^2}{MSE})$ [12],\nwhere:\n\u2022\nMAX is the maximum possible pixel value (e.g., 255 for 8-bit images),\n\u2022\nMSE is the mean squared error, given by:\n$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (I_i - \\hat{I_i})^2,$\nwith N as the total number of pixels.\nHigher PSNR values (> 30dB) indicate better image restora-tion quality.\nSSIM evaluates the similarity between two images by considering luminance, contrast, and structural information. It is computed as:\n$SSIM(I, \\hat{I}) = \\frac{(2\\mu_I\\mu_{\\hat{I}} + C_1)(2\\sigma_{I\\hat{I}} + C_2)}{(\\mu_I^2 + \\mu_{\\hat{I}}^2 + C_1)(\\sigma_I^2 + \\sigma_{\\hat{I}}^2 + C_2)}$ [18],\nwhere:\n\u2022\n\u00b5I and \u00b5\u00ce are the mean pixel intensities of I and \u00ce,\n\u2022\n\u03c3I and \u03c3\u00ce are the variances of I and \u00ce,\n\u2022\n\u03c3I\u00ce is the covariance between I and \u00ce,\n\u2022\nC\u2081 = (k\u2081.MAX)2 and C2 = (k2. MAX)2 are constants to stabilize the equation, where k\u2081 \u226a 1 and k2 < 1.\nSSIM ranges from 0 to 1, with values closer to 1 indicating higher structural similarity.\nb) LPIPS:: The Learned Perceptual Image Patch Sim-ilarity (LPIPS) metric is calculated to evaluate perceptual similarity between restored and ground truth images. Authors use the implementation from the LPIPS library, with a pre-trained AlexNet backbone. The metric is computed as:\n$LPIPS(x, y) = \\frac{1}{H_{l}W_{l}} \\sum_{h,w} ||\\phi_{l}(x)_{h,w} - \\phi_{l}(y)_{h,w}||$ [24],\nwhere:"}, {"title": "b) LPIPS::", "content": "\u2022\nx and y are the restored and ground truth image patches,\n\u2022\n\u03a6l represents the feature map from layer l of AlexNet,\n\u2022\nHl and Wl are the dimensions of the feature map at layer l."}, {"title": "IV. CUSTOMIZATIONS", "content": "This section outlines the customizations and enhancements implemented to improve the Restormer model's effectiveness in motion blur. These modifications were designed to over-come challenges identified during the evaluation of the paper.\nUpon evaluating the Restormer models on real-world dataset [10], we observed that the baseline configurations struggled to handle anomalies involving color differences. To address this challenge, we introduced additional trans-formations during training to improve model robustness and performance.\nThe baseline Restormer training pipeline includes only horizontal and vertical flips, and no transformations related to color changes are applied. To bridge this gap, we introduced color jitter, Gaussian blur, brightness and contrast adjustment transformations, enabling the model to better handle changes in illumination, sharpness, and color intensity. These transfor-mations simulate real-world conditions, such as varying illu-mination, sharpness, and color intensity, allowing the model to better handle diverse visual scenarios [19]. Perspective transforms were also applied to mimic geometric distortions, further increasing the variability and representativeness of the training data.\nIn parallel, we focused on optimizing the architecture to reduce model complexity while maintaining, and in some cases improving, performance. We decreased the number of layers and Transformer blocks in both the encoder and decoder, which led to a reduction in the number of parameters (by 18.4%) and computational cost (see Figure 3). To compensate for the reduction in layers, we doubled the number of attention heads per stage, allowing the model to better capture global and local features in a computationally efficient manner. Over-all changes made to the model can observed from Figures 2 and 3. These changes were inspired by the work done in [16]. Authors claim that attention mechanisms are scalable and adaptable to vision tasks, even with reduced computational overhead [16].\nFor evaluation, we incorporated color difference metric deltaE2000 as expressed in [13] to complement standard pixel-level metrics like PSNR and SSIM. It is defined as:\n$\\Delta E_{00} = \\sqrt{(\\frac{\\Delta L'}{K_L S_L})^2 + (\\frac{\\Delta C'}{k_c S_c})^2 + (\\frac{\\Delta H'}{K_H S_H})^2 + R_T \\frac{\\Delta C'}{\\kappa_c S_C} \\frac{\\Delta H'}{\\kappa_H S_H}},$\nwhere:\n$\\Delta L' = L_2 - L_1$, (lightness difference)\n$\\Delta C' = C_2 - C_1$, (chroma difference)\n$\\Delta H' = 2\\sqrt{C_1C_2} sin(\\frac{\\Delta h}{2})$, (hue difference)\n$R_T = -2 \\frac{\\sqrt{C_{avg}^7}}{\\frac{C_{avg}^7}{C_{avg}^7 +25^7}} sin(\\frac{60\u00b0e^{-(\\frac{h_{avg} -275}{50}})^2}}{S{avg}}) $, (rotation term)\nScaling factors SL, SC, and SH are functions of Lavg, Cavg, and havg, and are adjusted to account for perceptual uniformity differences.\nThese metrics provide a quantitative measure of the model's ability to restore accurate color fidelity by comparing per-ceptual color differences between restored images and ground truth. This addition offers a more comprehensive assessment of the model's performance, particularly in scenarios where precise color reproduction is critical.\nAdditionally, the loss function was modified to improve the model's ability to restore both spatial and frequency-domain details. Alongside the baseline pixel-wise L\u2081 loss, we introduced a frequency-domain loss inspired by Fourier transform analysis as discussed in [1]. The combined loss is defined as:\n$L_{total} = L_{pixel} + \\lambda L_{freq}$,"}, {"title": "V. DATASETS", "content": "where Lpixel represents the pixel-level L\u2081 loss, and Lfreq ensures the preservation of high-frequency details critical for visual quality. The frequency loss is defined as:\n$L_{freq} = \\frac{1}{N} \\sum_{i=1}^{N} |||F(\\hat{I_i})| - |F(I_i)|||_1,$\nwhere F denotes the Fourier transform, |F(I)| is the mag-nitude of the Fourier transform, \u00ce\u00bf and I\u2081 are the restored and ground truth images respectively, and N is the total number of images in the batch.. This term emphasizes minimizing the discrepancy between the frequency components of the restored and ground truth images, enhancing the model's ability to recover sharp edges and fine textures. The weighting factor A was set to 0.1 in our experiments."}, {"title": "A. GoPro [9]", "content": "The GoPro dataset comprises 3,214 pairs of blurry and sharp images with a resolution of 1280\u00d7720 [9]. Typically, these are divided into 2,103 images for training and 1,111 images for testing [9]. However, for training Restormer [20] authors utilized the whole dataset. The dataset is created using videos captured with a GoPro camera, where multiple consecutive frames are averaged to produce various blurred images. The mid-frame of each sequence serves as the ground-truth image corresponding to the associated synthetic blurred image. Figure 10 in Appendix A illustrates several examples of blurred images from the GoPro dataset."}, {"title": "B. RealBlur [10]", "content": "The RealBlur dataset consists of 4,738 pairs of sharp and blurred images captured using a dual-camera system with synchronized Sony A7RM3 cameras and wide-angle lenses. It includes images of resolution 680\u00d7773, processed to reduce noise and ensure alignment. The dataset is divided into 3,758 image pairs for training and 980 pairs for testing. Unlike synthetic datasets, which blend sharp frames to generate blur, RealBlur captures authentic motion blur under diverse indoor and outdoor scenes, including low-light conditions. Two vari-ants of the dataset are provided: RealBlur-R (raw format) and RealBlur-J (JPEG format). Postprocessing involves denoising, geometric alignment, and photometric alignment to create high-quality training and evaluation data. Examples from the datasets are shown in Figures 11 in Appendix A."}, {"title": "C. UHDM [23]", "content": "The UHDM subset within the MC-Blur dataset comprises 8,000 training images and 2,000 testing images, all at 4K-6K resolution. The dataset is designed to address challenges in deblurring Ultra-High-Definition (UHD) images by incorpo-rating large blur kernels with sizes ranging from 111\u00d7111 to 191\u00d7191 pixels. These kernels are generated using 3D camera trajectories and convolved with sharp UHD images, providing realistic and varied motion blur. This subset emphasizes restor-ing fine details required for UHD image deblurring, presenting a significant challenge for current deblurring algorithms. It is the first large-scale UHD motion-blurred dataset, catering to the increasing prevalence of high-definition cameras in real-world applications. Figure 12 in Appendix A presents example images from the set."}, {"title": "VI. EXPERIMENTS AND RESULTS", "content": "Our experimental evaluation was conducted using PyTorch framework on NVIDIA RTX 4090 GPU. To ensure compre-hensive analysis, we implemented four distinct experimental configurations. First, we reproduced the baseline results using the authors' provided model checkpoints. Second, we fine-tuned provided checkpoint on the benchmark dataset, authors used in their paper to test their models. Second, we trained the model from scratch following the original specifications to validate reproducibility. Finally, we evaluated our improved model incorporating the architectural modifications and train-ing enhancements described in Figures 2 and 3.\nWe conducted evaluations on multiple benchmark datasets to assess model performance across different scenarios. The RealBlur-R and RealBlur-J datasets from [10], each containing 980 image pairs, provided real-world blurred images with corresponding ground truth references. Additionally, we uti-lized the UHDM dataset [23] comprising 2000 high-resolution image pairs to evaluate performance on higher resolution imagery. To analyze model behavior on challenging cases, we identified hard positives and negatives using a PSNR threshold between 20dB and 30dB provided in Table I."}, {"title": "A. Experimental Setup and Implementation", "content": "TABLE I: Performance comparison based on hard positives and hard negatives across different datasets with the threshold of PSNR between 20dB and 3dB."}, {"title": "B. Motion Blur Baseline Model Results", "content": "To evaluate the performance of the Restormer model on motion blur tasks, we utilized the pre-trained model check-points provided by the authors to reproduce their results. These checkpoints were tested on the RealBlur-J and RealBlur-R datasets [10] separately, aligning with the evaluation protocol described in the original paper. Additionally, we trained the model from scratch using the GoPro dataset [9], adhering to the hyperparameters and training settings detailed in the paper, to assess the reproducibility of the reported results.\nAfter having the reproduced model, we observed slight de-viations in performance compared to the checkpoint provided by the authors. These deviations (see Tables II, III, IV and V)"}, {"title": "C. Fine-tuned Model Results", "content": "Fine-tuning the baseline model on specific datasets yielded substantial improvements in performance metrics. From the Table III we can see significant 3.509 dB gain in PSNR. Notably, the DeltaE metric improved from 0.866 to 0.685, demonstrating enhanced color fidelity. RealBlur-J dataset fol-lows the same trend, according to the Table V.\nThe analysis of hard examples provides particularly in-teresting insights into models behavior. As shown in Fig-ures 13 and 15 in the Appendix A, we identified challenging cases from the RealBlur-R and -J datasets. These examples demonstrate severe motion blur combined with low lighting conditions especially in Figure 13 (Appendix A). All of the models struggle with these images, producing artifacts in regions of high frequency detail and failing to properly restore clarity. However, fine-tuned model shows great improvement even visually. The same can be observed in hard positive examples (Figures 14 and 16, see Appendix A), where all models perform reasonably well, but subtle differences emerge and fine-tuned models present superior results both quantita-tively and visually. Notably, the models fine-tuned on RealBlur datasets performed relatively poorly on UHDM, suggesting potential overfitting to their respective domains (see Table VI)."}, {"title": "D. Improved Model Results", "content": "To evaluate the results, we benchmarked the models on a new dataset UHDM [23] containing 2000 high resolu-tion images with different levels of blur. When comparing performance across different model variants (see Table VI), the improved model achieved the highest PSNR, showcasing marginal improvements over the baseline checkpoint and the reproduced model. The improved model's better performance can be attributed to its architectural modifications and en-hanced training regime. This is particularly evident in the hard example analysis in Table I, where the improved model reduced the number of hard negatives to 598 compared to the baseline's 604, checkpoint's 608 and fine-tuned models' higher counts (up to 1000 for RealBlur-J fine-tuned model). Analyzing hard negative and positive examples in Figures 17 (see Appendix A) and 18 (see Appendix A), we can observe similar trend, that the improved model achieves better results in restoring details and colors both empirically and perceptu-ally. The challenging nature of the UHDM dataset is reflected in the relatively low PSNR scores across all models, with even the best performing model achieving only 21.359 dB. This can be attributed to UHDM's high-resolution images and complex blur patterns, which pose significant challenges for deblurring models."}, {"title": "D. Improved Model Results", "content": "TABLE VI: Performance comparison of models on UHDM [23]."}, {"title": "VII. ABLATION STUDIES", "content": "In this section, we present a detailed analysis of the mod-ifications made to the Restormer model and their impact on performance by comparing on the baseline model trained from scratch.\nThe comparison of loss, PSNR, and SSIM metrics between the two versions of the Restormer model reveals deeper insights into their training dynamics and performance. The loss curve of the baseline model (Fig. 4) shows persistent oscillations throughout the training process, indicative of training instability, which can be attributed to the limited data augmentations and reliance solely on L1 loss. On the other hand, the loss curve of the modified model (Fig. 5) is notably smoother and converges more rapidly, indicating better stability."}, {"title": "VII. ABLATION STUDIES", "content": "However, new model doesn't show better color fidelity in the reconstruction process, compared to the reproduced model which has the best result. This suggests that color transformation added to the data augmentation process might be redundant, although when benchmarking on RealBlur [10] dataset, improved model shows greater performance.\nThe architectural changes shown in Figure 2 and 3 are another crucial factor contributing to the improved train-ing dynamics and performance. The reduction in parameters (18.4%) and total layers (30%) (see Fig. 3) made the model more efficient while maintaining its representational power, according to Table VII. Overall training of the baseline model took 28 hours, while the model with reduced complexity took 23 hours, giving a difference of 5 hours on NVIDIA RTX4090 GPU. At the same time, the doubling of attention heads per stage enhanced the model's ability to focus on multiple regions of the image simultaneously, which likely contributed to the observed performance gains in both PSNR and SSIM. Moreover, as it was discussed before due to GPU memory constraints, we retrained the baseline model using progressive training on 1 GPU, starting with a patch size of 128 x 128 and a batch size of 8, and following:\n{(160, 4), (192, 4), (256, 2), (320, 1), (320, 1)},\nat iterations {92K, 156K, 204K, 240K, 276K}. In contrast, the new setup was more memory efficient allowing us to use a"}, {"title": "VII. ABLATION STUDIES", "content": "{(160,6), (192, 4), (256, 2), (320, 2), (384, 1)}.\nIn addition to this, the changes led to the significant reduction in model size, with our model's weights totaling 81.5 MB compared to 99.9 MB for the original models and 99.8 MB for the fine-tuned versions highlighting the efficiency of our architecture. Overall, these adjustments balance computational efficiency and feature representation."}, {"title": "VII. ABLATION STUDIES", "content": "TABLE VII: Inference times comparison of models on the datasets."}, {"title": "VII. ABLATION STUDIES", "content": "Overall, the modified"}]}