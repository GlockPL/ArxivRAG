{"title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "authors": ["Ali Anaissi", "Ali Braytee", "Junaid Akram"], "abstract": "We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of artificial intelligence into healthcare has shown immense potential to improve both the quality and efficiency of medical services [1]\u2013[3]. Among the many AI-driven advancements, medical question-answering (QA) systems, powered by Large Language Models (LLMs), have emerged as a key tool for providing accurate and timely answers to complex medical questions [4]\u2013[6]. This study seeks to push the boundaries of medical QA by utilizing advanced LLMs such as LLaMA-2 and Mistral, which have demonstrated remarkable abilities to understand and generate human-like text. By improving how healthcare professionals access vital information, our work aims to enhance decision-making, promote better patient care, and ensure that accurate medical knowledge is available to all, contributing to the overall well-being of society.\nSeveral fine-tuning techniques have been developed to improve the performance of LLMs. Notable among these are LORA (Low-Rank Adaptation) [7], rsLoRA (Rank-Stabilized LoRA) [8], and DoRA (Weight-Decomposed Low-Rank Adaptation) [9]. LORA facilitates efficient parameter fine-tuning by adding trainable low-rank adapters to specific model layers. RsLORA enhances this method by introducing a rank stabilization scaling factor, thereby preventing gradient collapse and ensuring stable learning at higher ranks [10]. DORA further refines the approach by decomposing the weights of pre-trained models into magnitude and direction matrices, allowing for more precise model adaptation.\nDespite these advancements, significant challenges persist in the domain of medical QA [11]. Existing systems frequently encounter difficulties in comprehending complex medical terminology and delivering contextually accurate responses [12], [13]. Additionally, the continuously evolving nature of medical knowledge necessitates frequent updates, which are often computationally intensive and time-consuming. Moreover, current models are susceptible to inaccuracies and may propagate outdated or incorrect information, thereby compromising their reliability and trustworthiness in critical healthcare scenarios [14], [15].\nTo address these challenges, we propose a novel approach that synergizes advanced fine-tuning techniques with an innovative retrieval-augmented generation method. We introduce rsDoRA+, which combines the benefits of rsLoRA and DORA with different learning rates for low-rank matrices. Additionally, we propose a new method, ReRAG (Retrieval on Demand and Question Rewrite), which integrates retrieval and question rewrite components to further enhance model performance. RsDoRA+ improves the model's understanding of medical terminology, reasoning, and contextual accuracy by leveraging decomposed model weights with rank stabilization. ReRAG optimizes the model's performance by providing relevant information on demand and ensuring the retrieval of the most pertinent data.\nOur contributions in this study are as follows:\n\u2022 Introduction of rsDoRA+, a fine-tuning technique that combines decomposed model weights with rank stabilization and differential learning rates for low-rank matrices, thereby significantly enhancing the performance of LLMs in medical QA services.\n\u2022 Development of ReRAG, a retrieval-augmented generation method that integrates on-demand retrieval and question rewrite components to improve the accuracy and relevance of medical QA responses.\n\u2022 Creation of a specialized LLM tailored for medical information retrieval, enabling healthcare providers to deliver fast and reliable responses, thus enhancing decision-making efficiency and patient trust.\n\u2022 Demonstration of the potential for advanced information technology to significantly improve the quality of medical"}, {"title": "II. RELATED WORK", "content": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17].\nThe RAG approach improves LLM performance through pre-training, combining various memory types to generate fact-based, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches.\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates self-reflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DORA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23]. DORA further refines this approach by decomposing weights into magnitude and direction matrices, thereby improving robustness and latency [9]. Rank-Stabilized LoRA (rsLoRA) addresses limitations in higher-rank learning by scaling adapters by the square root of the rank, thus enabling fine-tuning efficiency and performance without increasing computational costs [8]. The success of rsLoRA paves the way for future research into more efficient fine-tuning techniques, such as AdaLoRA [24]. Overall, these methodologies collectively enable LLMs to handle dynamic, context-sensitive medical information, significantly improving the accuracy and depth of medical responses. The integration of these advanced techniques into LLMs represents a considerable advancement in addressing the challenges of modern healthcare information systems, transforming the decision-making processes among healthcare practitioners."}, {"title": "III. PROPOSED METHOD/FRAMEWORK", "content": "To enable large language models (LLMs) to adapt efficiently to new data distributions under limited resources, parameter-efficient fine-tuning techniques are essential. Low-Rank Adaptation (LoRA) has emerged as a mainstream method by adding trainable low-rank adapters to selected layers, which are products of low-rank matrices scaled by a factor related to the rank. However, traditional LoRA's aggressive scaling factor limits its performance at higher ranks, leading to slower learning. To address this, Rank-Stable LoRA (rsLoRA) introduces a more conservative scaling factor, enhancing learning stability and performance at higher ranks. Experimental results indicate that rsLoRA significantly improves stability and learning efficiency when using higher ranks.\nMoreover, LoRA and rsLoRA show a positive correlation between the magnitude and direction of updates, often proportional, which might limit precise adjustments to model weights. Decomposed Low-Rank Adaptation (DoRA) addresses this by decomposing pre-trained weights into magnitude and direction matrices for fine-tuning, specifically using LORA for direction updates. This minimizes the number of trainable parameters while enhancing learning capability and stability without additional inference overhead. Additionally, the standard LoRA method uses the same learning rate for adapter matrices A and B, leading to inefficiencies in models with high embedding dimensions. LoRA Plus (LoRA+) sets different learning rates for matrices A and B, significantly increasing the learning rate of matrix B compared to A, improving training efficiency and feature learning ability.\nIn Natural Language Processing (NLP), adversarial training enhances model robustness by introducing adversarial examples, slight perturbations added to the dataset that are almost imperceptible to humans but can cause the model to make incorrect predictions. This makes the model more resistant to such examples and improves robustness. Adding noise to embeddings during training reduces overfitting to specific details like format, exact wording, and text length of the fine-tuning dataset. Therefore, we added a certain proportion of noise to the embeddings at the LLM base layer to mitigate overfitting during the instruction fine-tuning phase and better utilize the knowledge from the pre-training phase.\nOur approach to refining the training process for LLMs, particularly in the embedding layer, involves a strategy called outlining, which enhances the model's ability to adapt and learn from new data efficiently. This process adjusts the learning rates for adapter matrices A and B strategically. Specifically, the learning rate for matrix B is set as a multiple, denoted as A, of the learning rate for matrix A. This ensures intensive fine-tuning of certain model aspects without overwhelming computational demands, maintaining a balance that allows for efficient and effective learning. This adjustment"}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The question-answering dataset employs data from Medical Meadow, integrating Anki Flashcards and MediQA to showcase our model's generalizability across medical datasets. Medical Meadow, a comprehensive collection sourced from the medAlpaca project and established medical databases, provides diverse medical data. Anki Flashcards, created by medical students, cover various medical disciplines with concise summaries and mnemonics. We used OpenAI's GPT-3.5-turbo to rephrase these flashcards into coherent question-answer pairs. MediQA comprises manually generated summaries of answers to consumer health questions, ensuring rich, relevant data for training.\nFor rsDoRA+ deployment, we set up the environment by installing necessary libraries, dependencies, and downloading base models for fine-tuning. We automated training and evaluation processes with shell commands, and analyzed metrics such as accuracy, loss, Rouge, and BLEU scores to evaluate model performance. For ReRAG, we installed libraries, set up retrieval and vector databases, and used rsDoRA+-fine-tuned models as generators. Detailed steps are in ReadmeReRAG.md.\nTo validate rsDoRA+ and ReRAG, we designed experiments using medical dialogue datasets like MediQA and Anki Flashcards. We used 7B models like LLama2 and Mistral for their balance of performance and speed, comparing Full-Parameter Fine-Tuning and LoRA Fine-Tuning baselines with various configurations of rsDoRA+, rsLoRA combined with DORA, and LoRA+. In ReRAG experiments, using langchain, the base model (LLama2 chat model fine-tuned with rsDoRA+) was tested. Unlike SelfRAG's extensive data generation with GPT-4, we used a resource-efficient GPT-3.5 API approach. The Retrieve Token Node answered with a binary score to decide if detailed information was needed. The Retrieve Node embedded 100-word text pieces using the 'tao-8k' model, with the retriever finding the best-suited texts via Euclidean distance. The IsRel Node assessed relevance, discarding irrelevant texts, while the Question Rewrite Node performed new retrievals if needed. Predict Nodes used either the original question or a new query with retrieved texts.\nFor evaluation, we used an independent test set and standard metrics like BLEU and ROUGE to assess performance, ensuring the accuracy and relevance of responses in medical QA services. Our results demonstrate that rsDoRA+ significantly enhances LLaMA2 model performance, while ReRAG improves LLaMA2 performance as shown in Figure 4. The integration of rsDoRA+ into ReRAG has markedly boosted QA performance in the medical field, with improvements across four metrics being 123%, 181%, 153%, and 201%, respectively, indicating exceptional performance at high ranks with enhanced QA and answer richness.\nrsDoRA+ is a novel fine-tuning method optimized from models like DoRA, combining the advantages of DoRA, LoRA+, and rsLoRA, and incorporating NEFtune to prevent overfitting. Before fine-tuning, rsDoRA+ introduces a certain proportion of noise into the embeddings, sampled from a uniform distribution [-1,1] and scaled by \u03b1/Ld, as shown in Equation 5.\n$Noise = \\frac{\\alpha}{L \\cdot d} U(-1,1)$  (5)\nWhere \u03b1 is an adjustable parameter, L is the input length, and d is the embedding dimension. This noise helps mitigate overfitting and better utilize pre-training knowledge. Experiments comparing different values of \u03b1 using NEFtune were conducted on the MediQA and Anki Flashcards datasets, using LLaMA2-7b-chat as the baseline model with DoRA and rsLoRA fine-tuning. We tested \u03b1 values of 0.1, 0.5, 1, 2, and 5, as illustrated in Figures 5 and 6. NEFtune significantly improved the model's question-answering performance.\nIn the MediQA dataset, the ROUGE-L score was only 0.3% lower than the baseline when \u03b1 = 2. The most significant improvement was at \u03b1 = 5, showing a consistent performance increase of approximately 2%. NEFtune's impact varied; it had a smaller effect on individual word overlap but a more substantial impact on the overlap of four-word sequences and longer texts. However, performance did not consistently improve with increasing \u03b1-at \u03b1 = 2, performance decreased slightly compared to \u03b1 = 0.5 and \u03b1 = 1. This variation may be due to differing hyperparameters, such as the number of fine-tuning epochs.\nAfter introducing noise with \u03b1 = 5, we tested the fine-tuning approach and evaluated performance using the BLEU-4 metric. Given that LoRA enables large language models to adapt efficiently to new data distributions, our model was optimized based on the LoRA framework. However, we found that traditional LoRA's aggressive scaling factor slowed the learning speed of high-rank adapters, limiting performance.\nWe observed that DoRA and LoRA+ did not perform well in high-rank scenarios and could not achieve effective performance improvements by increasing the rank. By adjusting the adapter scaling factor Yr, we made the model's handling of rank more conservative, allowing it to maintain stability at high ranks and achieve performance improvements. Figures 8 and 7 show that adopting the $Y_{rs-L} = \\alpha/\\sqrt{r}$  strategy allowed rsDoRA+ to inherit the high-rank stability of rsLoRA and amplify the impact of the scaling factor on model performance.\nFor the MediQA dataset, the trend of rsDoRA+ and rsLoRA remained consistent. As the rank increased from 8 to 128, the BLEU-4 score significantly improved. Compared to the optimal rank (rank=8) in traditional LoRA methods, the model's performance increased from 0.3929 to 0.4817, an improvement of approximately 10%. Traditional LoRA and LoRA+ methods exhibited fluctuating performance with increasing rank and even slight declines. When the rank reached 512, the performance of rsDoRA+ and rsLoRA was not as good as at rank=128, with the BLEU-4 score dropping to slightly higher than at rank=32 (0.424). This might be due to the small size of the MediQA dataset (2.2k rows), where a higher rank results in too many parameters without sufficient data support, leading to incomplete training. This was verified in subsequent tests on the Flashcards dataset.\nFor the Anki Flashcards dataset, the BLEU-4 score of rsDoRA+ and rsLoRA gradually increased as the rank rose from 8 to 512. rsDoRA+ improved from 0.4648 at rank=8 to 0.5137 at rank=512, a 5% performance increase. Given the larger dataset size (34k rows) compared to MediQA, there was enough data to support training at high ranks. Traditional LORA and LoRA+ methods showed no significant changes at high ranks, with LoRA's performance even deteriorating, as observed with MediQA. These experiments validated the effectiveness of replacing the scaling factor, resulting in significant performance improvements in both datasets. rsDoRA+ inherited the trend observed in rsLoRA. However, the size of the dataset must be considered during high-rank experiments. Insufficient data may lead to negative effects due to the excessive number of parameters.\nIn practical applications, professional databases update rapidly, and the various costs associated with fine-tuning often prevent models from learning the latest knowledge. Additionally, in many fields, including the medical field, information security needs to be considered. The Retrieval-Augmented Generation (RAG) method effectively addresses this issue by enabling the model to obtain real-time accurate information during the generation process. In medical problems, specialized terminology frequently appears. The significance of these keywords is critical in the generation process. However, most retrieval methods often fail to capture these keywords accurately and obtain useful information. Our question rewrite function effectively resolves this issue, ensuring that the retrieved information is more relevant and the generated answers are more accurate.\n$Cosine(v, w) = \\frac{v \\cdot w}{||v|| \\cdot ||w||} = \\frac{\\sum_{i=1}^{N} v_i w_i}{\\sqrt{\\sum_{i=1}^{N} v_i^2 \\sum_{i=1}^{N} w_i^2}}$ (1)\n$x' = a + \\frac{(x - min(x))(b - a)}{max(x) - min(x)}$ (2)\n$dist(x_1, x_2) = \\sqrt{\\sum_{i=1}^{n} (x_{1i} - x_{2i})^2}$ (3)\n$Sum(X_i) = \\sum_{j=1}^{n} (dist(X_i, X_j))$ (4)\nThe ReRAG method's results are shown in Table VI. Mauve (Measuring Automated Unintended Vulnerability and Efficacy) evaluates model generation quality, comparing prediction and real answer distributions. A Mauve score of 1 indicates a perfect match between the prediction's and real answer's distributions, showing fluency, informativeness, and diversity. The baseline for our comparative experiment, rsDoRA+, does not involve retrieval. Traditional RAG, with a vector search component, provides retrieved text for every input. ReRAG WQE has a similar structure to ReRAG but lacks the question rewrite component.\nFigure 10 shows that ReRAG outperforms all other methods and the baseline. ReRAG's performance, represented by the red bars, is higher than ReRAG without the question rewrite component, demonstrating the component's effectiveness. Approximately 70% of questions in the final output included supplementary information, indicating that around 30% of questions did not require retrieval, highlighting the method's utility.\nOur experimental results show that when \u03b1 = 5, NEFTune performs best. At \u03b1 = 5, our model performs better on long and difficult problems (i.e., the MediQA dataset) compared to normal text length problems (the WikiFlashcard dataset). Medical datasets often contain specialized terms and concepts not commonly seen in general language training. Introducing noise through NEFTune helps the model handle unfamiliar medical terms, reducing overfitting to common language patterns. Adding noise to embedding vectors acts as a random walk in the embedding space, increasing the model's tolerance to input data uncertainty.\nNext, we moved to rsDoRA+ experiments. Our results show continuous performance improvement as the rank increases, similar to rsLoRA. Methods without adjusted scaling factors, such as traditional LoRA, perform poorly at high ranks. Traditional LORA often faces gradient collapse at high ranks due to its scaling factor, leading to slower learning and limited performance improvement. rsDoRA+ solves this problem by adjusting the scaling factor to  $\\sqrt{r}$ preventing gradient collapse at high ranks, ensuring stable learning and better performance with the same computational resources.\nIn practical operations, incorrect retrieved information or added retrieval information for simple questions can lead to inaccurate model generation, a serious issue in the medical field. The ReRAG method successfully solves this problem, ensuring the model obtains more relevant information and simple questions do not receive redundant information. We first fine-tuned the model using rsDoRA+ to optimize performance, then integrated the ReRAG method into the QA system. ReRAG combines the advantages of information retrieval and generation models, retrieving relevant information from a large text database and incorporating it into the answer generation process, enhancing the quality and accuracy of answers.\nMedical questions often require extensive background knowledge and precise information. The QA system, integrating rsDoRA+ and ReRAG, provides more accurate answers and delivers results faster when handling complex medical questions. This approach could be a robust solution for medical QA systems, optimizing fine-tuning methods and integrat-"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced a comprehensive framework designed to improve large language models (LLMs) for medical question-answering services, with the goal of making healthcare more accessible and reliable. We developed rsDoRA+, an advanced fine-tuning technique that integrates Rank-Stable LORA, DORA, and LoRA+ with NEFTune, which helps reduce overfitting and makes learning more stable. This is especially important when processing large amounts of medical data, ensuring the model's performance remains accurate and consistent. Our experiments showed that this approach significantly enhances LLMs, particularly by carefully managing noise introduction to boost stability. We also introduced ReRAG, a method that combines retrieval-augmented generation with a question rewrite component. This ensures that the information retrieved is relevant and accurate, improving the model's ability to answer medical questions reliably. When compared to traditional approaches, ReRAG outperformed existing models, offering more precise and contextually appropriate answers.\nBy integrating these techniques, our framework not only tackles technical challenges like overfitting but also enhances the relevance and quality of the information provided. This work underscores the potential of AI-driven healthcare services to offer more accurate and dependable support for healthcare professionals, ultimately contributing to better patient care and more equitable access to medical knowledge for all."}]}