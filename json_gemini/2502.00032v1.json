{"title": "QUERYING DATABASES WITH FUNCTION CALLING", "authors": ["Connor Shorten", "Charles Pierse", "Thomas Benjamin Smith", "Karel D'Oosterlinck", "Tuana Celik", "Erika Cardenas", "Leonie Monigatti", "Mohd Shukri Hasan", "Edward Schmuhl", "Daniel Williams", "Aravind Kesiraju", "Bob van Luijt"], "abstract": "The capabilities of Large Language Models (LLMs) are rapidly accelerating largely thanks to their integration with external tools. Querying databases is among the most effective of these integrations, enabling LLMs to access private or continually updating data. While Function Calling is the most common method for interfacing external tools to LLMs, its application to database querying as a tool has been underexplored. In this report, we propose and extensively test a tool definition for database querying that unifies accessing data with search queries, filters, or a combination both, as well as transforming results with aggregation and groupby operators. Our proposed tool definition additionally enables the LLM to route queries across multiple collections of data. To evaluate its effectiveness, we conduct a study with 8 LLMs spanning 5 model families. We present a novel pipeline adapting the Gorilla LLM framework to create synthetic search database schemas and queries. We present an analysis comparing our proposed DBGorilla dataset to popular text-to-SQL benchmarks such as BIRD, Spider, and WikiSQL. Using the DBGorilla benchmark, we show that Claude 3.5 Sonnet, GPT-40, GPT-40 mini, and Gemini 1.5 Pro are all highly effective at utilizing our proposed tool definition for querying databases. We primarily evaluate these models with the Exact Match of predicted and ground truth query APIs. To gain a more holistic understanding of model performance, we also report Abstract Syntax Tree (AST) alignment scores and LLM-as-Judge preference rankings of predicted queries. Among the eight models tested, Claude 3.5 Sonnet achieves the highest performance with an Exact Match score of 74.3%, followed by GPT-40 mini at 73.7%, GPT-40 at 71.8%, and Gemini 1.5 Pro at 70.2%. We further breakdown these results by API component, finding that LLMs are highly effective at utilizing operators on boolean-valued properties, but struggle to understand text property filters and differentiate them from search queries. We further visualize the performance across the synthetic use cases, showing robust results with the higher performing models such as GPT-40, but significant performance variance across use cases from lower performing models. To further understand the impact of tool definitions on connecting LLMs with querying databases, we conduct ablation studies exploring the impact of parallel tool calling, adding a rationale as an argument of the tool call, using a separate tool per database collection, and tool calling with structured outputs. We find minimal performance variance across these ablation experiments with GPT-40. Our findings demonstrate the effectiveness of enabling LLMs to query databases with Function Calling. We have open-sourced our experimental code and results at github.com/weaviate/gorilla.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable successes in natural language understanding and reasoning. The applications of LLMs are rapidly advancing as they are connected with other software tools in architectures broadly described as Compound AI Systems. From Zaharia et al., a Compound AI System \"tackles AI tasks using multiple interacting components, including multiple calls to models, retrievers, or external tools\" [1]. Connecting AI models to external software tools complements their weaknesses, such as accessing private or continually updating data, as well as"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Compound AI Systems", "content": "Tool use is one of the most promising opportunities to improve the capabilities of LLMs. There are two common design patterns for interfacing tool use in Compound AI Systems: Function Calling and Flow Engineering [12]. Visualized in Figure 4, Function Calling entails equipping the LLM with a set of functions described in the prompt. The LLM inference is then orchestrated in a function calling loop. At each step, the LLM either chooses to complete the response, or call one or multiple functions and wait for their respective responses to continue the next iteration of the loop. Contrastively, Flow Engineering describes a pre-determined flow of inferences and external tools calls. This abstraction helps clarify how tools are interfaced to LLMs. However, there is a significant overlap and this is a constantly evolving area of AI research. For example, an engineered LLM and tool calling flow could be itself abstracted and interfaced as a function for the agent to call. In a similar analog, a flow could implement the open-ended looping core to the definition of Function Calling. Understanding these distinctions is important for the evolution of prior works on interfacing search and database querying as an LLM tool. In either case, we need methods to evaluate how well LLMs can select the correct tool for the task and format the tool's respective arguments [3].\nSearch has been one of the most commonly used tools for LLMs. Most commonly, this has taken the shape of RAG [13], a pre-determined flow of retrieval with the user input as query, followed by response generation. RAG flows were further pioneered with architectures such as Baleen RAG, in which the user input is first translated into search queries with an LLM inference, sent to a retrieval engine, and passed into a final response generation. One of the early efforts to expand search to the Function Calling interface was WebGPT [14], in which the LLM can format search queries to send to the web, as well as paginate through the results. Zhang et al. debuted the term \u201cAgentic Information Retrieval\" [15] to capture the intersection of learning to search with the Function Calling interface."}, {"title": "2.2 Text-to-SQL", "content": "Developing mostly in parallel to search as a tool, AI researchers and practitioners have been exploring the use of database APIs as a tool. Even before breakthrough capabilities in LLMs, Text-to-SQL research has been a heavily studied discipline [16]. Text-to-SQL research has mostly targeted the application of making it easier for humans to learn how to query databases. We primarily studied three popular Text-to-SQL benchmarks in this work: WikiSQL [17], Spider [18, 5], and BIRD [19]. WikiSQL consists of 80,654 hand-annotated examples of questions and SQL queries distributed across 24,241 tables from Wikipedia. The original Spider dataset contains 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. BIRD contains 12,751 Text-to-SQL pairs and 95 databases spanning 37 professional domains. We visualize samples from the BIRD dataset in Figure 3 to help readers further understand the current state-of-the-art in Text-to-SQL benchmarking.\nIn Spider 2.0, the authors diverge from Text-to-SQL prediction to Text-to-SQL workflows, adopting a more holistic view of data querying and transformation with SQL.\nNow that most databases are evolving to support search indexes and integration with LLMs, additional query languages are emerging to expand SQL, such as LOTUS [8] and SUQL [10]. Our work is further related to managing multiple database collections in architectures such as Data Warehouses, Lakehouses [20], or Ontologies [21]. In order to study machine learning for databases, we need new benchmarks and datasets reflective of the challenges of database systems."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Details of Function Calling Setup", "content": "As described in the context of Compound AI Systems, Function Calling entails equipping the LLM with a set of functions described in the prompt. The LLM inference is then orchestrated in a Function Calling loop. At each step, the LLM either chooses to complete the response, or call one or multiple functions and wait for their respective responses to continue the next iteration of the loop. We limit the Function Calling loop to a single step and evaluate the accuracy of the predicted Function Calling arguments. We use the tool definition shown in Appendix A.\nMore concretely, we store each record in our DBGorilla dataset with the properties, nl_command, ground_truth_query, and schema. We send the schema to the database to create the collections. We then retrieve metadata about available collections and their properties from the database, although you could also achieve this from the schema stored in the dataset. We then parse this meta information into a description string detailing the collections and their properties, as well as a list of collection names that are used for routing queries as an enum-valued API argument. The description of collections and database querying is carefully constructed to fit within 1024 tokens due to token limits from the LLMs tested when using their respective Function Calling SDKs. The tool schema then exposes a query_database function with parameters tailored to the database's querying capabilities. The collection_name argument is the only required argument, which is restricted to the enumerated list of available collections. Optional parameters enable search queries, filters, aggregations, and groupby. We then pass the natural language command stored in the dataset to the LLM with the tool definition and record the arguments used in the function call. If the LLM chooses not to call a function, it achieves a score of 0 for this instance. This is motivated by the highly targeted nature of these natural language commands, which we will discuss further later on. We test the GPT-40 and GPT-40 mini LLMs [23], Gemini 1.5 Pro and Gemini 2.0 Flash experimental [24], Claude 3.5 Sonnet [25], Command R+ and Command R7B [26], and Llama 3.1 8B Instruct [27]."}, {"title": "3.2 DBGorilla Dataset Construction", "content": "We present a novel dataset for measuring the effectiveness of LLMs to query databases with Function Calling, DBGorilla. DBGorilla consists of two phases: synthetic schema and query generation. The construction of this dataset heavily relies on structured generation methods [28, 29, 30]. Structured outputs lets us easily control the validitiy of generated schema and query structure. We can easily use this framework to generate more synthetic schemas, or schemas with different property distributions. Further, we can easily switch out the query operators available to the LLM and construct"}, {"title": "3.2.1 Synthetic Database Schemas", "content": "The schema generation process utilizes GPT-40 to create synthetic database schemas through a structured prompt. The database generation prompt contains a reference example schema that demonstrates the desired structure and detail level, along with specific requirements for each collection including two text properties (one with rich searchable content), one numeric property, and one boolean property. We note that these requirements can be varied to create diverse schema types, such as eight boolean properties and one searchable text property if desired. In order to test routing queries to multiple collections, the generator is further instructed to ensure collections are meaningfully related. However, we do not explicitly link these collections together with foreign key relationships. Using these inputs, we produce five schema sets, each containing three interconnected collections. An example is shown in Table 1, a use case modeling a restaurant system with Restaurants, Menus, and Reservations collections. Each generated schema follows consistent conventions with collection names in camel case format, comprehensive property definitions including types and detailed descriptions. We further generate a use_case_overview to facilitate interpretability of generated schemas."}, {"title": "3.2.2 Synthetic Queries", "content": "The query generation process follows the algorithms introduced in Self-Instruct [31] to create comprehensive test cases of API use cases. We extend Self-Instruct to add Reflexion [32] to assess, and potentially correct, generated queries with another LLM inference. The addition of Reflexion to synthetic query generation helps us get a quantitative sense of dataset quality and qualitatively when manually inspecting individual queries with the user interface shown in Figure 6. We generate all valid combinations of query operators, including options for semantic search, filters (integer, text, boolean), aggregations, and grouping operations. For each operator combination, we create a Pydantic model that specifies required fields and includes descriptions of how each operator should be used, ensuring that the natural language query necessitates all selected operators. Using GPT-4o, we then generate natural language queries by providing the database schema and operator requirements, validating that each query requires all specified operators to be answered correctly by creating structured output models on the fly for each query combination. We run this"}, {"title": "3.3 Evaluating Predicted Queries", "content": "We present three strategies for evaluating the quality of predicted database queries with Function Calling. We primarily use Exact Match Scoring evaluation. Exact Match scoring returns a boolean assessment if the predicted and ground truth queries are exactly identical. We additionally utilize Abstract Syntax Tree (AST) evaluation. AST scores are a highly effective method to measure how aligned predicted queries are with the ground truth API components the natural language command is crafted to target. We further introduce a preference ranking evaluation using LLM-as-judge. This is largely inspired by the challenge of evaluating real-world queries that do not come with a ground truth API path. Preference ranking evaluation further offers additionally flexibility in query quality judgement and leniency in cases where the the LLM chooses not to call a function. Although we note that due to the highly targeted nature of natural language commands in our dataset, it is never effective to choose not to call a function. Additionally, in our controlled environment with ground truth API queries, we can gain insight into the alignment of these metrics.\nThe AST evaluation methodology employs a weighted scoring system to assess query similarity. The largest weight (40%) is assigned to correctly matching the target collection, mismatching collections results in a score of 0. The remaining 60% is evenly distributed (15% each) across four components: search queries, filters, aggregations, and group by operations. We do not evaluate if the search queries are similar, only if the predicted and ground truth queries both use the search query or not. Contrastively, filters, aggregations, and groupby values must be identical to the ground truth query to achieve the 0.15 points for the match. The final score ranges from 0.0 to 1.0, with 1.0 indicating perfect structural alignment across all elements.\nIn order to better understand the performance of Large Language Models to format database queries, we conduct an Ilm-as-judge preference ranking test. The test utilized structured outputs to rank the 8 LLM responses on a scale of 1 to 8. The model additionally presents an explanation of why it decided on the particular ranking for the user query. We report the number of 1st place rankings each model receive, as well as a weighted score across ranks. We report the number of first place ranks each model achieves, as well as a weighted rank scoring. The weighted scoring system grants 100 points for first place, 70 for second, 50 for third, 35 for fourth, 25 for fifth, 20 for sixth, 15 for seventh, 10 for eighth, 5 for ninth, and 0 for tenth place and beyond. This approach heavily rewards finishing near the top but still provides partial credit for mid-range positions, aiming to capture strong overall performance rather than sporadic high placements."}, {"title": "3.4 DBGorilla Expansion and Maintenance Cost", "content": "The computational costs associated with running the DBGorilla benchmark reveal significant economic implications for model selection, deployment, and benchmark maintenance. As shown in Table 2, there is a stark contrast in costs across model families, with Claude 3.5 Sonnet being the most expensive at $2.84 total cost, while Command R7B"}, {"title": "4 Experimental Results", "content": "The DBGorilla leaderboard shown in Figure 1 illustrates a clear hierarchy in model performance across different evaluation metrics, with interesting patterns in performance. Claude 3.5 Sonnet leads overall with an Exact Match score of 74.3%, followed closely by GPT-40 mini at 73.7%, GPT-40 at 71.8% and Gemini 1.5 Pro at 70.2%. There is then a somewhat steep dropoff to 59.4% from Command R+, followed by a significant performance gap to Gemini 2.0 Flash (exp) at 37.1% and Llama 3.1 8B Instruct at 32.1%. Performance varies across different query complexities. For simple queries (requiring a single argument), the top models perform remarkably well, with GPT-40 achieving a score of 87.5% and Claude Sonnet 3.5 reaching 77.5%. Looking across query complexities, measured as requiring more than 1 operator, we see an encouraging robustness in performance. Claude 3.5 Sonnet's performance on Simple Queries at 77.5% is not too far off its' effectiveness with Complex Queries at 72.1%. The collection routing metric reveals another interesting pattern, while most top models hover around 96 to 98%, Command R+ stands out with 94.3% accuracy despite a relatively lower 59.4% total Exact Match score, suggesting it has a particular strength in understanding and correctly selecting the appropriate database collection. The Abstract Syntax Tree (AST) scoring analysis reveals further nuance into model performance beyond Exact Match metrics. Claude 3.5 Sonnet achieves a nearly perfect 0.973 AST score. This indicates near perfect structural understanding of query components in cases missing the strict criterion of Exact Match scoring. The top four performing models all maintain AST scores above 0.95, additionally illustrating strong comprehension of query operator structure."}, {"title": "4.1 Component and Schema Variance Analysis", "content": "To better understand where the LLMs went wrong in their predicted queries, we break performance down by API component involved in the ground truth queries. We present a radar plot visualization of this in Figure 5 and a detailed view of results in Table 6 and Table 7. Boolean filters stand out as the most successfully handled component across all models, with GPT-40 and Claud 3.5 Sonnet both achieving 87.5% Exact Match accuracy. However, their performance drops on boolean aggregations with scores of 62.5% and 66.25%, respectively. Most interestingly, the models show a significant performance decline on text filters, failing to distinguish them from search queries. The evaluation across different database schemas, shown in Table 2, reveals varying levels of domain adaptability among the models. GPT-40 demonstrated the most consistent cross-domain performance, with results ranging from 73.44% on the Restaurants use case to 67.8% on the Visual Arts use case, a range of 5.64%. This stability stands in marked contrast to smaller models. Gemini 2.0 Flash exhibited dramatic performance variance, ranging from 57.81% to 23.44%. These findings indicate a strong correlation between model size and the ability to maintain consistent performance across varied domains, with larger models demonstrating superior schema adaptability."}, {"title": "4.2 No Tool Selected", "content": "Shown in Figure 4, the Function Calling Loop begins with an initial design to call a function or respond to the user. We find that all LLMs tested occasionally skip function calling and immediately return the response without querying the database. This is particularly emphasized in the Gemini 2.0 Flash model, which skips function calling more often than not at a rate of 53.97%. This also explains Gemini 2.0 Flash's poor performance on the broader set of evaluation metrics such as Exact Match, Abstract Syntax Scoring, and Preference Ranking."}, {"title": "4.3 Preference Rankings", "content": "Across the 315 tested queries, only 5 result in identical predictions for the 8 LLMs tested. On average, each query has 5.8 unique predictions from the 8 LLMs. From the preference ranking results shown in Table 3, Gemini 1.5 Pro, GPT-40 mini and GPT-40 emerge as the most favored models, consistently occupying the highest portion of first-place votes. Meanwhile, Command R7B, Llama 3.1 8B Instruct, and Claude 3.5 Sonnet often fall toward the bottom in aggregated rankings. These results vary significantly from the Abstract Syntax Tree (AST) evaluations, which highlighted the same top three as highly skilled in generating structurally correct database queries. However, the slight discrepancies, such as Gemini 1.5-pro coming in first on llm-as-judge preference rankings, but fifth on AST, point to a key difference between technical correctness (how accurately the query matches a reference structure) and user preference (readability, clarity, or \"perceived helpfulness\")."}, {"title": "5 Ablation Studies", "content": "Our main experiments highlight the performance of different LLMs at querying databases with Function Calling using our proposed tool definition and query operators. We additionally present a series of ablation studies to assess how various experimental factors and emerging schools of thought on Compound AI System design influence performance. We explore the impact of requiring a rationale for each tool call, enabling parallel tool calls, using structured output formats rather than Function Calling, and distributing queries across multiple per-collection tools instead of a single unified tool. Shown in Table 6, we find minimal performance variance across these ablations."}, {"title": "5.1 Tool Rationale", "content": "We begin by introducing a required rationale argument to our database querying tool. This achieves an Exact Match score of 73.2% with 96.8% collection routing accuracy. While the addition of rationales enhances human interpretability for system debugging, its potential benefit for LLM response parsing remains untested. An illustrative example demonstrates how rationales can reveal model misconceptions. When processing the query \"How many different types of exhibit highlights are featured in each museum, grouped by museum name?\", we observed:\nGround truth: Museums, TextAggregation(exhibitHighlights:COUNT), GroupBy(museumName)\nPredicted: Museums, TextAggregation(exhibitHighlights:TYPE), GroupBy(museumName)"}, {"title": "5.2 Parallel Function Calls", "content": "Another key aspect of Function Calling, illustrated in Figure 4, is the use of parallel tool calls. At each step, the LLM can be allowed to make simultaneous tool calls. In this ablation, we set parallel_tool_calls to be true and score each query based on the highest scoring tool called. This achieves a slightly lower Exact Match score of 71.2% with 95.9% collection routing accuracy. With parallel tool calls enabled, GPT-40 averages 1.21 calls per query. Upon inspecting the parallel tool calls, we find that this typically results in calls to complementary collections such as a query for the Restaurants collection and Reservations collection in the use case shown in Table 1. In our discussion section, we present a further analysis of how parallel function calls may impact Compound AI System design."}, {"title": "5.3 One Tool per Collection", "content": "We begin by decomposing our tool definition into a tool per collection. This has important implications for scaling given the token limit for tool descriptions imposed by the LLM providers tested in this study. Splitting collection across multiple tools provides a practical solution for systems with a large number of collections. In our experiments, the one tool per collection approach achieved an Exact Match score of 72.3% with 96.8% collection routing accuracy, demonstrating performance comparable to other implementations. We hypothesize that future Compound AI Systems may use the natural language command as input to only create tools for potentially useful collections, or transform multiple collections into a materialized view for Function Calling."}, {"title": "5.4 Structured Generation", "content": "Our final ablation study challenges the potential bias in LLMs towards their specific Function Calling SDK. We replace Function Calling with Structured Generation using the ResponseOrFunctionCall model shown in Appendix A. Structured generation of function calls achieved a similar Exact Match score of 72.8% with a collection routing accuracy of 97.1%. The result suggest that models can effectively work with alternative interfaces for calling external functions. However, this experiment does not eliminate the potential bias towards the Function Calling SDK when processing longer sequences of function calls and their responses."}, {"title": "6 Discussion and Future Work", "content": ""}, {"title": "6.1 Database Gyms", "content": "A key advantage of synthetic database environments, or database gyms [22], is the control offered over schema complexity, such as the number of collections and their property type distributions, as well as query patterns. Real-world databases often contain numerous collections with complex relationships, but publicly available datasets are limited or subject to confidentiality concerns. Synthetic data generation allows us to vary schema sizes, property types, naming conventions, and data relationships. This approach also supports benchmarking edge cases in data management that are challenging to obtain from real data, such as inconsistent naming schemes, partial null fields, or schema evolutions.\nOur current setup uses three collections per use case and four properties per collection. Future versions of DBGorilla can introduce more collections, variance in property distribution per collection, and explicit relationships between collections, such as foreign keys. This would enable more sophisticated queries and enable testing for deeper reasoning about interrelated data. Furthermore, generating multiple commands per query-operator combination and introducing more abstract or multi-hop queries would better mimic real-world information needs. This expansion could also include iterative querying scenarios where the result of one query informs subsequent ones. Another opportunity to make the evaluation more robust is to generate multiple queries for each combination of query components, rather than a single"}, {"title": "6.2 Querying Databases with Compound AI Systems", "content": "Recent efforts such as Reflexion prompting [33], DSPy Assertions [34], SPADE [35], and Network of Networks [39] show how LLMs can automatically refine problematic outputs through self-correction steps. Applying similar ideas to database querying could enable an iterative process wherein queries are revised based on validation or user feedback. Recent work has explored more systematic approaches to developing and optimizing LLM pipelines [40], such as DSPy which introduces a programming model that abstracts LLM pipelines as text transformation graphs with declarative modules that can be automatically optimized [41, 42]. Approaches such as MIPRO [43] or AvaTaR [44] could further optimize prompt design and function definitions by contrasting successful and unsuccessful samples, ensuring models learn to manage tokens effectively for large-scale schemas."}, {"title": "7 Conclusion", "content": "This work demonstrates that Function Calling provides an effective and generalizable interface for enabling natural language database access. Through comprehensive evaluation of 8 LLMs across 5 model families, we show that leading models can achieve high accuracy in translating natural language to structured database operations, with Claude 3.5 Sonnet reaching an Exact Match score of 74.3% and GPT-40 achieving 71.8%. Our analysis reveals particular strengths in boolean operations across all models, suggesting a promising direction for optimizing database schemas around boolean properties. The DBGorilla benchmark, with its synthetic schema generation and comprehensive query evaluation framework, provides a foundation for future research in this area. As database systems continue to evolve toward natural language interfaces, Function Calling provides a promising foundation for bridging the gap between human intent and database operations."}]}