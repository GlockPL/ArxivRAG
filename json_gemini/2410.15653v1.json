{"title": "Opportunities and Challenges of Generative-AI in Finance", "authors": ["Akshar Prabhu Desai", "Ganesh Satish Mallya", "Mohammad Luqman", "Tejasvi Ravi", "Nithya Kota", "Pranjul Yadav"], "abstract": "Machine Learning and data mining have created widespread impact across various domains. However, these techniques are limited in their ability to reason, understand and generalize w.r.t language specific tasks. The aforementioned challenges were overcome, with the advancement of LLM's/Gen-AI. Gen-AI techniques are able to improve understanding of context and nuances in language modeling, translation between languages, handle large volumes of data, provide fast, low-latency responses and can be fine-tuned for various tasks and domains. In this manuscript, we present a comprehensive overview of the applications of Gen-AI techniques in the finance domain. In particular, we present the opportunities and challenges associated with the usage of Gen-AI techniques in finance. We also illustrate the various methodologies which can be used to train Gen-AI and present the various application areas of Gen-AI techniques in the finance ecosystem.\nTo the best of our knowledge, this work represents the most comprehensive summarization of Gen-AI techniques within the financial domain. The analysis is designed for a deep overview of areas marked for substantial advancement while simultaneously pin-point those warranting future prioritization. We also hope that this work would serve as a conduit between finance and other domains, thus fostering the cross-pollination of innovative concepts and practices.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Learning and data mining have created widespread impact across several domains such as healthcare, payments, Internet of Things (IoT), automation and e-commerce [96]. The success of these techniques can be attributed to their ability to identify and generalize patterns across large amounts of data. In particular, these techniques have led to automation of repetitive tasks, improved precision and accuracy, enhanced decision making, personalized and tailored customer experi-ence and operational scalability across various methodologies. Clustering [52], future prediction [56], anomaly detection [57], time-series monitoring [58] and graph-based learning are some examples of data mining and machine methodologies widely used across domains. However, machine learning and data mining techniques such as Support Vector Machine's (SVMs) [53], gradient boosted trees [55] and logistic-regression [54] based techniques are limited in their ability to reason, under-stand and generalize w.r.t. language specific tasks.\nThe aforementioned challenges were overcome, with the advancement of Large Language Models (LLM's). LLM's are able to improve understanding of context and nuances in language modeling [98], translation between languages, handle large volumes of data, provide fast and low-latency responses and can be fine-tuned for various tasks and domains. The advancement was sparked by the development of transformer architecture, which is based solely on attention mechanisms, dispensing with recurrence and convolutions [47]. Experiments have been performed to demonstrate how LLM's have superior quality while requiring significantly less time to train [99]\u2013[102]. Please note that we will use the words Gen-AI and LLM's interchangeably from here on.\nThere are several opportunities associated with the usage of Generative Artificial Intelligence (Gen-AI) based technologies in finance [12]. Broadly speaking, they can be used across multiple avenues such as interactive (e.g. chatbots), assistive (e.g. easing payment related activities, right card to use), educative (e.g. enabling users to understand finance concepts) and advisory (e.g. trading assistant).\nAlong with the opportunities, there lies several challenges associated with the widespread adoption of Gen-AI in finance. Some of these challenges are originating from the scarcity of private and sensitive quality data available for training, issues stemming from pre-training and fine-tuning of financial LLM's, inference latency, computational costs associated with deploying these models in production, exorbitant cost associ-ated with the pricing of API's available for commercial usage, inherent biases and privacy embedded within these models [88], [103].\nThere are several avenues via which Gen-AI models can be trained and developed for financial use-case. Users can lever-age either open source models or external service providers"}, {"title": "II. OPPORTUNITIES OF GEN-AI IN FINANCE", "content": "In this section we will outline the opportunities associated with the usage of Gen-AI in the finance domain. Broadly, the opportunities could be classified into four major categories i.e. interactive, assistive, educative and advisory."}, {"title": "A. Interactive", "content": "Gen-AI is the state-of-the-art technology for both Task-Oriented Dialog (TOD) [123] and Open-Domain Dialog (ODD), thereby making it the go-to technology for building conversational and interactive applications. McKinsey [68] estimates that Gen-AI could further reduce the volume of human-serviced contacts by up to 50 percent, depending on a company's existing level of automation. Applying generative Al to customer care functions could increase productivity at a value ranging from 30 to 45% of current function costs. Efficient customer service is one of the competitive advantages finance companies have and often build closed source propri-etary Gen-AI systems to gain competitive advantage. Klarna is one of the Buy Now Pay Later (BNPL) companies that has publicly stated that its new customer service tool can do the work equivalent of 700 full time human agents and handles more than 66% of its customer conversations. It has saved the company over $40M and reduced the ticket resolution times from 11 minutes to 2 minutes [119].\nGen-AI is enabling the paradigm shift towards end-to-end training of conversational agents by eliminating the need for separate components for natural language understanding, dialogue state tracking, and natural language generation. Their inherent ability to ask follow up questions and generate concise explanations of complex financial concepts for any user's level of understanding is a big value add. These con-versational agents can effectively track references to previous utterances, entities, and user preferences, enabling them to generate more coherent, consistent, and personalized responses that are available 24/7 [37]. These are a vast improvement over rule based chat-bots which could respond to a fixed set of queries with a pre-configured set of responses. Fine-tuning and Retrieval Augmented Generation (RAG) [38] can further help LLM's become more domain specific and/or access more up to date/ relevant information to support customers ranging from routine to complex inquiries while creating more personalized and engaging experiences [39].\nCustomer agent avatars created dynamically for digital communications can be tailored to match the customer's preferences, creating a sense of familiarity and trust. Gen-AI could also be used to create appealing backgrounds for debit, credit cards, coupons and gift cards. Many models including Parti [40] can generate high-fidelity photo-realistic images and supports content-rich synthesis involving complex compositions and world knowledge. One could go one step further and create dynamic visually pleasing themes for apps that are more engaging and context specific."}, {"title": "B. Assistive", "content": "GenAI's ability to assist through digital automation, co-piloting, autofilling, shopping would greatly enable individuals by being a sophisticated assistant. Further, this may greatly assist individuals with mobility and dexterity challenges.\nGen-AI can be trained to call API's [60] enabling them to execute tasks, automate routines assist people improving the quality of life of individuals.This can help facilitate payments and shopping [42]. A payments co-pilot can help identify opportunities for better payment options between cards with better benefits and suggest relevant offers for the user's context. This co-pilot can also help choose between different instrument modes like cards, bank accounts and buy now pay later payment modes that the user could qualify for and will help users make better payment choices based on the metadata of the transaction.\nAuto-filling forms is another use case that Gen-AI can potentially excel at. Auto-fill is a hard problem due to the dynamic and changing nature of forms across the internet. Writing automation logic to auto-fill forms doesn't scale and fails soon. Ability of Gen-AI to understand any page content and context via parsing document object model and the user intent provides new opportunities to fill out forms more accurately.\nAgents can be built that help to shop by being the digital shopping assistant. LLaSA is one such LLM designed specifi-cally for this purpose [107]. Users could simply describe what they are looking for and the LLM's could parse the inventory and provide recommendations that closely match the user's"}, {"title": "C. Educative", "content": "Gen-AI forms the foundation around which tools that help improve financial literacy can be built. This would lead to creation of applications that can guide, educate and empower users.\nWhile the use of Machine Learning (ML) and Artificial Intelligence (AI) for estimating and recommending optimal investment strategies are not new [4], [13], Gen-AI's ability to ingest vast amounts of financial data to analyze market sentiment, news and other data sources can help inform better trading strategies [2], [3].\nGen-AI can be used to perform deep dives into existing financial data about a customer or company, which can have variety of applications for a company or individual, ranging from uncovering financial trends that could lead to cost efficiency measures, investing opportunities and at a minimum, provide a quick access knowledge base for financial advisors. Chatbots [43] extensively trained on the finance data helps users understand complex topics like mortgage planning, in-vestments like stock options, mutual funds and retirement planning. Their ability to have multi turn conversations enable them to answer questions to all kinds of users i.e. from beginners to advanced."}, {"title": "D. Advisory", "content": "Financial institutions leverage the capabilities of Gen-AI to build powerful tools for financial advisory applications (e.g. risk assessment, pro-active detection, summarization and recommendation).\nGen-AI can help to build sophisticated risk models [44] by processing vast amounts of financial data to identify anoma-lous patterns of fraud. Such models can be used for better credit scoring, risk assessment for loans, to identify fraudulent activities in payment systems and to forecast financial risks using chain of thought prompting [46] to elicit better reasoning and accuracy.\nFurther, It can be used to create customized security alerts based on transaction history and behavior which could help reduce the risk of fraud. SMS based scams [smishing] are on the rise, In 2021 alone, SMS phishing has caused a huge $44 billion in losses just within the United States [48]. Unfortunately, generative AI chat-bots are often abused to create such smishing campaigns [49]. Ironically, AI agents themselves can provide a path forward to tackle these issues by parsing, analyzing and flagging such messages. Such features could be built into the operating system of the phone helping to proactively detect and alert the users of a potential scam.\nLLM's showcase a remarkable capacity for document sum-marization. By leveraging extensive pre-training on diverse text corpora, they can generate concise yet informative sum-maries by extracting salient and relevant information about the topic that a user is interested in. They can identify key themes, track discourse structure and distill the core message all while capturing the essence of the original text. Further, this capability can be used for text summarization tasks and explanation of signup forms, legal agreements, financial terms and conditions."}, {"title": "III. CHALLENGES OF USING GEN-AI IN FINANCE", "content": "Gen-AI has the potential to be a transformational technology in finance and payments. However, this area is not without its unique challenges. We have categorized these challenges into the following categories [92]."}, {"title": "A. Data Availability", "content": "There are two challenges when it comes to data used to train Gen-AI models.\nThe first challenge stems from an abundance of publicly available data along with simultaneous scarcity of private and sensitive quality data [12], [14], [16]. Finance and payments industry has very stringent reporting regulations and as a result a lot of data is available publicly from various financial institutions. However, when it comes to the internal and private data of financial institutions, there is a scarcity of quality data. When Gen-AI models are trained on this abundant public and scarce private data, it is not clear which data gets used for specific decision making. For example, auto insurance risk assessment for an individual could be made based on publicly available data about the individual, whereabouts of their vehicle while the individual might not be even aware that this data is available to their insurance company [10].\nSecondly, even when the data quality bar is met, many attributes of the financial data are considered privacy sensitive and are subject to stringent regulations on its use [14].\nThe above two challenges can be partially overcome by the use of synthetic data. Synthetic data [109], [110] is one of the solutions financial institutions are using to solve the problem of privacy and quality of data. Synthetic data refers to data that is artificially generated for the training purposes and very representative of the real world data and yet is not really from a real world entity [16]. While such data might help institutions mitigate the data privacy issue it also may deprive them of the use of emergent properties of Gen-AI that can uncover hidden patterns and trends in the data [111]."}, {"title": "B. Challenges in fine tuning", "content": "The importance of fine tuning Gen-AI models is well understood [18]. Gen-AI models require extensive fine tuning when they are applied to specific domains such as finance. Fine tuning is often performed using raw data where input and expected output is both fed to the model, another approach is Instruct fine-tuning [19] where the expected output is not provided directly but rather more explicit instructions are provided to the model.\nFinBERT [17], a LLM for finance domain is a good data point that highlights the importance of pre-training and fine tuning. FinBERT's insight was that financial vocabulary is an important factor in accuracy of financial models. Such vocabulary needs to be carefully embedded into the model of the pre-training and not just fine-tuning (Example, associating \"bank\" with \"lending\" more than \u201criver\u201d).\nFinGPT which uses LoRA (Low Rank Adaptation) tech-nique [6] to finetune available general purpose open source LLM's [97] (e.g. LLaMA) and apply them to finance domains shows the limitations and strengths of fine tuning. Research [20] has reported that fine tuned models based on open source models like LLaMA are able to outperform BloombergGPT on finance related tasks even with a modest amount of training data of 50K samples [20]. However such models were fine tuned for very specific types of financial tasks.\nCurrent research suggests [20] that availability of high quality human labeled data is critical for fine tuning. In the finance domain producing human labeled data is harder due to challenges around regulations, privacy and availability of humans with such specialized knowledge. Additionally research [20] predicts that the cost of fine tuning an existing open source model could be more than $30,000."}, {"title": "C. Latency and time validity of results", "content": "Latency refers to the amount of time a system takes to produce output once you provide it an input. In the financial domain, latency is of high importance because the more transactions happen per second, more revenue is generated. In some of the cutting edge high frequency trading platforms, lowering latencies by 1 millisecond can lead to additional revenue around $100M [21].\nLatency is also a concern around adoption of Gen-AI technologies. Gen-AI models could take a significant amount of time to produce their output. Benchmarking of various Gen-Al models shows that the inference latency varies a lot based on the context window [21]. In finance applications context windows are expected to be much longer especially when the context might include past data specific to the query, time series data and various signals required for inference. Hence any Gen-AI based synchronous operation will end up impacting the latency significantly [114].\nAnother aspect that must be considered in conjunction with latency is the time validity of the data. In financial applications where Gen-AI is expected to work off the real time data, the real time data might have very limited time validity. For example stock prices change very rapidly and might be invalid by the time the Gen-AI model is inferred."}, {"title": "D. Computation costs", "content": "Computational costs associated with training/inferring Gen-Al models are a source of another challenge III-B towards widespread adoption. Estimating aforementioned costs is an evolving field with rapidly changing baselines [112].\nCredit card payments hover around 2B transactions per day. Stock market transactions and other banking transactions are even more [22]. Such transactions are enabled by different types of institutions, which incur a cost every time a transac-tion executes. Introducing Gen-AI based computations would further increase cost.\nThe pricing models of Gen-AI APIs available for commer-cial use vary from $5 to $15 per million input tokens. If we assume that each credit card transaction might involve 500 tokens, this then comes to around the cost of $0.0025 per transaction or $5M for 2B transactions per day in additional expenses [87].\nIf we apply similar math to stock market predictions then the token window is going to be a lot longer and costs even more. Hence, managing computational costs is going to be one of the most critical challenges to use Gen-AI in finance in future. As of today, no major player has published any real world data around economics of using Gen-AI in finance."}, {"title": "E. Risk considerations", "content": "Gen-AI systems have some well known data privacy and bias issues. In finance, where regulations and laws govern nearly all activities, these problems become extremely critical to manage [88].\nGen-AI models are trained on massive amounts of data, some of which could be the user's private data. Even when anonymized some of this data could be linked to specific users by clever prompt engineering and inference. This data leakage is one of the biggest risks of using Gen-AI in finance [115].\nCertain data used in training might later be discarded by the entity that owns it or the entity might request it to be deleted [95]. However, when such data is used in training; even though it is not explicitly stored in the models, it is still remembered by the model. This raises privacy and regulatory concerns [88]. In the era of right to be forgotten [116] regulations, this is a major risk.\nBesides data leakage, embedded bias in finance is another major problem. Different regions have different laws govern-ing what kind of information can or can not be used for making decisions about say creditworthiness of an individual [89]. Past research [90] has shown that pricing of say housing market is affected by human bias around race. Gen-AI models trained on such data might perpetuate such biases without even realizing about their existence.\nThe challenges around data privacy and ethics go hand in hand with regulatory compliance of FIs. As of today, we do not have comprehensive regulations [91] on use of Gen-ai in finance but we can expect such regulations to emerge soon. It is important that companies design their Gen-AI strategies keeping these challenges in mind.\nGen-AI's domain independent problems such as hallucina-tion and inconsistent reasoning [94] also is a problematic as-pect in finance where wrong information can lead to significant financial losses. Various LLM's for example have been found to be highly inconsistent when it comes to banking related information [93]."}, {"title": "IV. GEN-AI METHODOLOGIES IN FINANCE ECOSYSTEM", "content": "In this section, we present various ways in which Gen-AI techniques can be trained or fine-tuned for potential use-cases."}, {"title": "A. Out-of-Box LLM", "content": "In this technique, the users leverage either open source models or Gen-AI/LLM service providers like OpenAI [24], Gemini [28], Microsoft [30] and Perplexity [29] for their tasks. This method requires no training data, and either minimal to no compute resources. If users leverage Gen-AI techniques from service providers, then they do not need to go through the process of deploying Gen-AI models either in house or on cloud.\nFurther, users can make use of prompt engineering [25] techniques to complete their task. Broadly, there are 2 main varieties of prompt engineering techniques i.e. zero-shot [26] and few-shot [27], [64]. In the case of zero-shot learning, the LLM's are able to perform the task at hand without seeing any prior examples, but just by leveraging the knowledge present in them. On the other hand in case of few-shot learning, users provide a few examples as part of the prompt for the LLM's to learn and perform the task at the end of the prompt. In particular, in this technique, the LLM's leverage their induction heads to perform in-context learning and arrive at the result [117], [118]. In summary, the goal is to nudge the state of LLM into the right direction which provides the best response for the given task at hand.\nPrompt engineering techniques for task completion is great for users with minimal data and/or compute resources. How-ever, there are a couple of drawbacks involved in using this method.\n1) For use cases where the LLM's are unable to learn from in-context examples (usually happens in the case of using smaller LLM's), then it is better to fine-tune the model for the specific task.\n2) Including examples for the LLM's to learn from takes up the space context length to provide task relevant prompts.\nBoth the drawbacks are now becoming irrelevant because of quantized models (see below) and almost infinite context length (1M+ context length in Gemini [28])."}, {"title": "B. Fine-tuning", "content": "In scenarios, where utilizing LLM's out of the box is not working for the task at hand, then users could leverage fine-tuning to train the LLMs to perform better at the specific tasks the user has in mind. At a high level, fine-tuning techniques can be divided into the following categories.\n1) Instruction Fine-tuning: In instruction fine-tuning [31], the pre-trained LLMs are further trained on a labeled set of prompts and answer pairs. This newly trained LLM is tuned to answer specifically to specific kinds of instruction/prompt. As the name suggests, the training data need to be in the form of instructions. So users need to either collect the training data in the form of instructions or could leverage prompt template libraries like prompt-engine-py or dynamic prompts to take normal datasets and convert them into instruction datasets for fine-tuning.\n2) Task specific Fine-tuning: This technique [32] involves the users fine-tuning a pre-trained Gen-AI model to perform a specific kind of task in mind. For example, the users might fine-tune the pre-trained LLM to perform sentiment detection given an input prompt. It involves very few examples for the LLM to train on, but still requires a decent amount of compute as the entire model needs to be loaded into memory for the training part.\nTask specific fine-tuning [32] is prone to exhibit a phe-nomenon called catastrophic forgetting. In catastrophic forget-ting, the underlying LLM has totally forgotten the knowledge of the world it had obtained as part of its pre-training and its performance on the other tasks after task specific fine-tuning is much worse than its performance on the same tasks before fine-tuning. In order to circumvent the issue of catastrophic forgetting, users can employ multi-task instruction fine-tuning or employ parameter efficient fine-tuning described in section IV-B3.\n3) Parameter Efficient Fine-tuning: Instruction Fine-Tuning IV-B1 or Task specific fine-tuning IV-B2 are resource intensive and are plagued with catastrophic forgetting. In order to circumvent both these issues users can leverage Parameter Efficient Fine-tuning (PEFT) [33]. Two most com-monly used techniques in PEFT are LoRA [6] which rely on re-parameterization technique and soft prompts which add additional trainable layers to the LLM. We will now describe both of them in detail.\n1) LORA introduces a new way to train the LLM's by retaining their pre-training knowledge. The weights of the LLM are frozen and new low rank decomposition matrices are added to every layer in the transformer. The authors of LORA have achieved good results with having the low-rank of the decomposition matrices as low as 1 or 2. Since these additional matrices are very small in size (order of MBs), multiple LoRA models can be trained and stored. Since these low rank decomposition matrices are quite small in comparison to the original LLM weights, and as part of training no optimizer states are stored for the LLM weights, very minimal compute resources are required during training of LoRA models.\n2) Soft Prompts [65] fall under the additive method paradigm where no weights of the model are changed. Instead of modifying the weights of the pre-trained LLM's, soft prompts rely on prepending trainable tokens or soft prompts to the input tokens during training for a given task. The loss is propagated all the way to these trainable tokens and an efficient representation is learnt for the task at hand. The idea of these soft tokens is to choose the right vectors for a given space on the N-dimensional hypersphere of the embedding vector space. Since these soft tokens are very light-weight, multiple such soft tokens can be learnt similar to LoRA."}, {"title": "C. Agentic Systems", "content": "Even though LLM's have made great strides in their ability to understand natural languages (via zero-shot or few shot examples), LLM's still struggle in providing accurate up to date information, hallucinate answers [34], [35] or are unable to perform precise mathematical calculations [59]. A very simple solution is to provide the ability for LLM's to utilize external tools such as search engine, calculators etc which help the LLM's in overcoming their major drawbacks.\nToolformer [60] is one of the examples for LLM's that utilize tools to overcome the above mentioned drawbacks. In the case of Toolformer, the LLM training involves three steps\n1) Given a dataset C = {x1,x2,...,xN} of plain text, utilizing the in-context learning ability of the already trained LLM, each of the plain text is augmented with potential API(tool) calls.\n2) API/tool calls are executed and their responses are col-lected. Responses which do not reduce the loss (wrong API/tool call responses) are discarded and and the rest of the responses are stored to be used as dataset for fine-tuning the LLM for tool usage.\n3) Utilizing this newly augmented dataset the LLM is then fine-tuned."}, {"title": "D. Quantization", "content": "Traditionally models were trained and deployed using 32-bit floating point numbers to represent the parameters of the model. With the exponential increase in the number of parameters in LLM's, one major concern that arises is the amount of time taken for inference. In order to speed up the inference time researchers leverage quantization to represent the parameters of the model using less bits (float16, bfloat16 or int8) without much loss in accuracy. Users utilize one of the two quantization schemes\n1) Post Training Quantization: In this method a pre-trained 32-bit floating point number based model is converted into low bit numbers. The quantization maybe data free or a very small amount of data called calibration data can be used. A crucial step in this quantization scheme is to find a good quantization ranges for the quantizer as noted in [61]. In this quantization scheme no retraining of the LLM is involved. This method of quantization (specifically the case with data free quantization) is particularly helpful when security or data privacy may limit data access [62].\n2) Quantization aware training (QAT) involves in retraining the pre-trained LLM using training data. In this quanti-zation scheme since the quantization operation is non-differentiable, during back-propagation the gradient is approximated using an identity function also known as Straight Through Estimator [63]. This method is partic-ularly useful if the quantized model will be in use for a quite some time. Since this method involves retraining of the LLM on lower precision, the training must be performed for an extended period so that quantized LLM converges to better loss in the loss manifold [62]."}, {"title": "V. GEN-AI APPLICATIONS IN FINANCE ECOSYSTEM", "content": "Gen-AI, is still in its nascent stages and implementation within finance sector comes with it's own set of risks and chal-lenges. However, the trans-formative nature of this technology is driving many applications in this domain. In this section, we will highlight such applications both in the industry and the research community."}, {"title": "A. Numerical Reasoning", "content": "In recent times, datasets with complex numerical reasoning tasks have been proposed. In particular, FinQA [75] is based on the earnings reports of S&P 500 companies. The questions in FinQA, such as \u201cConsidering the weighted average fair value of options, what was the change of shares vested from 2005 to 2006?\u201d and \u201cWhat was the net change in tax positions in 2014?\", require information from both tables and unstructured texts to answer. Similarly, ConvFinQA [76], an extension of FinQA, is a multi-turn conversational question-answering dataset over financial reports, consisting of 3,892 conversations with 14,115 questions.\nGPT-4 achieved an accuracy of 78% on FinQA and 76% on ConvFinQA dataset which is much higher than the average human [86]. This ability has enabled real world used cases like Fintool [79] an AI equity research tool that is engineered to discover financial insights about companies.\nCredit Karma by Intuit [85], is making use of the Intuit Assist to launch Gen-AI experiences which include asking questions about a user's personal finance, understanding spend and personal financial roadmap, save money by using offers and much more. Mastercard reports that their new transaction decision technology [82] based on LLM's could help financial institutions improve their fraud detection rates by 20%, on average.\nIn RiskLabs, Cao et al. [45] explored a new framework that leverages Gen-AI to analyze and predict financial risks. It uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data and showcases the critical role of LLM's in financial risk assessment and opens new avenues for their application in this field.\nCapturing multi-modal input via Gen-AI is gaining traction in the finance domain. Chang et al [80] proposed usage of key technologies of Gen-AI into financial market prediction which can effectively capture the complexity of financial market data, and with a high degree of accuracy. Lezhi et al [81] showcase how Multimodal Gen-AI could be used for fun-damental investment research. Through fine-tuning methods applied to a base model (Llama2), they develop an AI agent that can assist investors in tasks such as understanding market conditions, generating investment ideas, and formatting results with stock recommendations. JP Morgan share their LLM called DocLLM, which is a layout-aware generative language model for multimodal document understanding which can include forms, invoices, receipts, reports, contracts, and other\""}, {"title": "B. Trading", "content": "Gen-AI has also seen applications in trading yielding im-pressive results in research settings.\nIn particular, Wu et al. (2023) introduced BloombergGPT [3], a 50-billion parameter large language model designed specifically for the financial domain. This was trained on a combination of an internal dataset built from Bloomberg's extensive archives called FinPile along with publicly available datasets. They demonstrated the effectiveness of training a model that excels in tasks like sentiment analysis and question answering in the financial context, while maintaining profi-ciency in general language tasks.\nFurther, Mai (2024) introduced StockGPT [2], which was trained on numeric data containing around 70 million daily U.S. stock returns over nearly 100 years. The stock return series was used as token sequences helping StockGPT learn predictive patterns through the attention mechanism. The model demonstrates remarkable performance, achieving high annual returns and a Sharpe ratio of 6.5 on a hold-out test sample from 2001 to 2023.\nFurthermore, FinBERT [17] is a pre-trained language model specifically for the financial domain. It outperforms generic language models on financial sentiment classification tasks. Financial sentiment classification for trading using various GPT's has been explored yielding results with sharpe ratio over 3. [120]\nAdditionally, Lopez et al. [121] showcased that incorporat-ing advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies. Fatourosa et al. [122] propose that a portfolio rebalanced monthly using the buy/sell signals generated from an LLM can outperform a passive index by 10-30%."}, {"title": "C. Summarization and Assistive Applications", "content": "Gen-AI techniques have also been applied in text summa-rization and assistive applications. In particular, Xiao et al. [74] discussed how there was a clear preference among human evaluators for LLM-generated summaries over human-written summaries or summaries generated by fine-tuned models.\nFurther, JP Morgan uses a generative AI tool that is designed to serve as a 'research analyst' for over 50,000 employees, aiding in various tasks that enhance productivity and decision-making within the firm [83]. Models like SaulLM-7B [72] based on mistral 7B architecture, are designed and distributed that help in legal text comprehension and generation.\nCompanies like Kudos and Max-Rewards understand credit card offerings and combine them with user's spends to suggest the best credit cards for maximising rewards. Morgan Stan-ley's COIN (Contract Intelligence) uses AI to review legal documents and extract relevant information."}, {"title": "D. Other Applications", "content": "Positive customer interactions aided by Gen-AI leads to increased brand satisfaction, trust and engagement [124] which would in-turn help institutions acquire customers and market and sell products better.\nLeading CRM companies like Salesforce [69], Hubspot [70], Zendesk [71] and others offer AI based chatbots for better customer experience and reducing support volumes.\nFerrero et al. [125] showcase how GenAI is increasingly applied to customer service operations and provide some brand response strategies to mitigate the challenges with this appli-cation. Timothy et al. [77] proposes a conceptual framework that analyzed the unique demands of suggestion contexts and built a prototype called Omnifill that uses LLM's to generate suggestions for various form-filling tasks.\nTurboTax, a tax preparation software, uses Intuit Assist [78], which is a generative AI-powered financial assistant. It uses a combination of the company's prior tax prep experience, data and documents from the filer, and current tax code to do the hard work for tax filing."}, {"title": "VI. CONCLUSION", "content": "Machine Learning and data mining techniques have led to massive impact via automation of repetitive tasks, im-proved precision and accuracy, enhanced decision making, personalized and tailored customer experience and operational scalability across various domains (e.g. healthcare, finance and automation). However, aforementioned techniques are limited in their ability to understand and reason languages.\nThe aforementioned challenges were overcome by the ar-rival of Gen-AI techniques. Gen-AI techniques are able to improve understanding of context and nuances in language modeling, translation between languages, handle large vol-umes of data, provide fast and low-latency responses and can be fine-tuned for various tasks and domains. In this manuscript we have provided a brief overview of the application of Gen-AI techniques within the finance ecosystem.\nIn particular, we discussed the various opportunities and challenges associated with the widespread adoption of Gen-AI techniques in the finance domain. We also present on how such large models can be trained and be made available by overcoming challenges stemming from critical issues such as hallucination, data-quality and computational costs. Lastly, we provide application areas where these models are or can be used.\nTo the best of our knowledge, this is the first work which comprehensively summarizes the usage of Gen-AI techniques in the finance domain. This summarization would not only provide a clear overview of"}]}