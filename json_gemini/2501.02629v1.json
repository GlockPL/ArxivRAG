{"title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense", "authors": ["Yang Ouyang", "Hengrui Gu", "Shuhang Lin", "Wenyue Hua", "Jie Peng", "Bhavya Kailkhura", "Tianlong Chen", "Kaixiong Zhou"], "abstract": "As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers and adversarially exposing them to generate more harmful data, one can understand their inherent and diverse vulnerabilities to attacks. With these exposures, we then \"unlearn\" these issues, reducing the impact of affirmative tokens and hence minimizing jailbreak risks while keeping the model's responses to safe queries intact. We conduct extensive experiments on two models, four benchmark datasets, and multiple state-of-the-art jailbreak benchmarks to demonstrate the efficacy of our approach. Results indicate that our framework reduces the harmfulness and attack success rate of jailbreak attacks without compromising utility for benign queries compared to recent defense methods.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have showcased impressive capabilities across a wide range of natural language tasks. Despite these advancements, ensuring their safety and alignment with human values remains a critical challenge. Numerous reports highlight that LLMs can generate unauthentic (Ji et al., 2023; Yao et al., 2024a), privacy-leaking (Huang et al., 2024), and even harmful outputs (Yao et al., 2024b), hindering their deployment in real-world applications such as education that demand precise and ethical responses.\nAmong these potential risks, one prominent challenge is that LLMs remain particularly vulnerable to \"jailbreak attack\", (Perez et al., 2022; Deng et al., 2023; Wei et al., 2023; Zou et al., 2023; Shen et al., 2024; Yi et al., 2024; Zhao et al., 2024b; Huang et al., 2023; Liu et al., 2024; Li et al., 2024), a type of adversarial prompt that provokes the model to produce harmful responses that violate usage policies and societal norms. Current defense techniques tailored for jailbreak attacks generally fall into three categories (Xu et al., 2024b): 1) self-processing defenses (Li et al., 2023b; Wu et al.,"}, {"title": "Preliminary Work", "content": "Jailbreak Attacks. Jailbreak attacks are adversarial prompts designed to bypass the safety mechanisms of LLMs, causing them to generate disallowed or harmful content (Zou et al., 2023; Liu et al., 2024). Formally, given a well-aligned language model M with parameters \\(\\theta\\), the attacker seeks an adversarial prompt \\(X_{harm}\\) such that the model produces a harmful response \\(Y_{harm}\\):\n\n\n\\(Y_{harm} = M(X_{harm}; \\theta).\\)\n\n\n\\(Y_{harm}\\) contains harmful or inappropriate content.\nJailbreak Defense. The objective of jailbreak defense is to modify model M or use extra safety prompts to prevent the generation of harmful responses, even when presented with adversarial prompts. In this work, we focus on altering model parameters \\(\\theta\\). The defense aims to ensure that for any input X, including adversarial prompts, the model's output Y adheres to safety guidelines:\n\n\n\\(Y = M_{def}(X; \\theta_{def}),\\)\n\n\nwhere Y is safe and compliant generated content, and \\(\\theta_{def}\\) are the updated model parameters after applying defense mechanisms.\nLLM Unlearning. LLM unlearning refers to techniques that selectively remove undesirable behaviors or knowledge from a trained language model without retraining it from scratch (Yao et al., 2024c)\nIn the context of jailbreak defense, unlearning aims to reduce the model's propensity to generate harmful content in response to adversarial prompts by updating the model parameters \\(\\theta\\) to decrease the likelihood of producing such content."}, {"title": "Layer-AdvPatcher", "content": "As illustrated in Figure 2, our framework consists of three interacted steps, each of which is experimentally shown effective to the precise and effective defense against jailbreak attacks."}, {"title": "Toxic Layer Identification", "content": "Our motivation stems from two key observations: (1) The first affirmative tokens generated by LLMs in response to jailbreak prompts are more likely to lead to harmful outputs (Zou et al., 2023), and (2) certain layers within LLMs tend to amplify toxic or affirmative tokens when exposed to harmful prompts (Wang et al., 2024a).\nTo analyze the harmful tendencies of different layers, we conduct experiments using several AdvBench (Zou et al., 2023) prompts to explore the model's token generation process. At each layer l, we use decoding head to project hidden states into vocabulary space and track probability \\(P_i(t_i|X_{harm})\\) assigned to each token \\(t_i\\), where \\(X_{harm}\\) denotes the harmful prompt and \\(t_i\\) is the target affirmative token. We manually construct a set of popular affirmative tokens (e.g., \"sure,\" \"absolutely,\" \"yes\") and denote it as \\(T_{affirm}\\), which can clearly distinct the toxic and safe layers. The details of \\(T_{affirm}\\) are listed in Appendix A.2. The toxic score at layer l"}, {"title": "Adversarial Augmentation", "content": "This step is to expose the jailbreak vulnerability of toxic layers by fine-tuning them to generate adversarial outputs. There are extensive jailbreak prompts to induce the harmful knowledge stored at the toxic layers. Thus the traditional defense strategies designed on limited benchmark datasets might cannot generalize to sophisticated attackers. We propose to augment the diversity of harmful data via randomly perturbing the input prompts and supervising the adversarial response generation from toxic layers, identifying and mitigating their inherent vulnerabilities.\nStep 1: Training Data Preparation. We construct an adversarial training dataset, where each sample comprises a pair of harmful prompt input and corresponding malignant output. Particularly, harmful prompt \\(X_{harm}\\) is provided by the existing jailbreak datasets such as AdvBench. It will be used to infer LLMs to elicit target responses composed of three key components: affirmative token representing positive confirmations of harmful instructions, transition responses that acknowledge the harmful request without providing explicit harmful content, and harmful content that contains specific instructions or explicit harmful information. The harmful output \\(Y_{harm}\\) is a concatenation of these elements."}, {"title": "Toxic Layer Editing", "content": "We propose to erase the model's undesired ability of generating harmful responses by adopting"}, {"title": "Experiments", "content": "This section assesses the effectiveness, helpfulness, efficiency, and compatibility of Layer-AdvPatcher."}, {"title": "Experimental Setup", "content": "Models and Dataset. Following (Zhao et al., 2024a), we deploy Layer-AdvPatcher on two open-source LLMs, namely Llama2-7b-chat (Touvron et al., 2023) and Mistral-7b (Jiang et al., 2023) to comprehensively evaluate its performance. To assess the effectiveness of our defense against jailbreak attacks, we employ AdvBench to generate adversarial prompts using various attack techniques, with GPT-Judge (Qi et al., 2024) and attack success rate (ASR) as the primary evaluation metric. In our locating process, we analyze 100 harmful prompts to identify the toxic layers. To measure the helpfulness of the edited LLMs, we use 800 diverse instructions from the commonly referenced benchmark Just-Eval (Lin et al., 2023).\nAttack Setup. We evaluate three state-of-the-art jailbreak attacks: GCG (Zou et al., 2023), PAIR (Chao et al., 2023), DeepInception (Li et al., 2023a). For GCG, we use EasyJailbreak (Zhou et al., 2024) for agile implementation. Then we follow the default parameter setting in EasyJailbreak and apply gpt-40-mini as the attack model that generates jailbreak suffixes. To assess the defense performance when a naive attacker directly inputs harmful queries to the language model, we utilize two harmful query benchmark datasets: Advbench (Zou et al., 2023) and HEx-PHI (Qi et al., 2024). Detailed setup of these attack methods and harmful query datasets can be found in Appendix A.1.\nBaselines Setup. We consider four recent defense strategies: Self-Examination (Helbling et al., 2023), Paraphrase (Jain et al., 2023), Unlearning (Yao et al., 2024c), and SafeDecoding (Xu et al., 2024a) as our comparing baselines. We adopt the hyper-parameters suggested in their original papers for each method. For our proposed Layer-AdvPatchermethod, we identify specific layers and parameters for unlearning. For Mistral-7B-Instruct-v0.3, we select the 29-30 layers QV and Input LayerNorm for optimization."}, {"title": "Main Results", "content": "Benchmark Comparison to SOTA Defense Approaches. Table 1 presents a benchmark comparison, displaying harmfulness scores and ASR (attack success rates, shown in brackets) for various"}, {"title": "Ablation Studies", "content": "Impact of Dataset Used to use for Unlearning We used three kinds of datasets to do our layer-specific unlearning (Yao et al., 2024c) in this ablation study section:\n1.  AdvBench-Train: The standard AdvBench training set, containing 80% of the original dataset. We refer to this dataset as AdvBench-Train.\n2.  Augmented-Normal: This dataset was generated by a model fine-tuned on AdvBench-Train and is an augmented version of the original dataset.\n3.  Augmented-Diversified: This dataset is based on a diversified version of AdvBench-Train, where affirmative tokens were replaced with other toxic tokens, making the dataset 10x larger.\nAs shown in Figure 3, the Attack Success Rate (ASR) differs across layers and datasets. In layer 27, the augmented dataset (Augmented-Diversified) performs better with lower ASR. However, in layer 28, the opposite occurs, which is interesting. The reason may be that the diversity in Augmented-Diversified may help unlearning in layer 27 but not in layer 28, possibly introducing complexity or noise that affects different layers in different ways. The larger dataset size in Augmented-Diversified may lead to overfitting in layer 28, making the model less generalizable and more vulnerable to attacks, while Augmented-Normal performs better in this case.\nImpact of Layer and Parameters on Unlearning We evaluated the effect of different parameter choices for unlearning layers in the model, focusing on the query, key, value attention matrices, and"}, {"title": "Related Work", "content": "Jailbreak Attack. Recent studies have extensively explored the vulnerabilities of LLMs to jailbreak attacks, which use adversarial prompts to bypass safety mechanisms and provoke harmful or policy-violating responses. One of the mainstream attacks is red teaming and automated jailbreaking (Perez et al., 2022; Deng et al., 2023), which adopts automated techniques to uncover vulnerabilities in LLMs, accelerating the discovery of adversarial behaviors across multiple models. Another line of work develops more advanced techniques for generating stealthy jailbreak prompts that are difficult to detect, using subtle manipulations to bypass model safeguards (Liu et al., 2023; Li et al., 2023a). Besides the attack modeling, some of existing works delve into understanding the limitations of current safety mechanisms (Wei et al., 2023; Zou et al., 2023), showing how adversarial prompts can transfer across different language models.\nJailbreak Defense. Current defense techniques against jailbreak attacks are generally categorized into self-processing defenses, additional helper defenses, and input permutation defenses. First, the self-processing defenses aim to make LLMs self-regulate without extensive fine-tuning (Li et al., 2023b; Wu et al., 2024; Zhang et al., 2024). These approaches help the model align its outputs by prioritizing safe goals or using adversarial techniques to defend itself. Second, the additional helper defenses involve external frameworks or mechanisms to enhance model safety (Pisano et al., 2024; Wang et al., 2024b). They use external alignments or adversarial carriers to mitigate jailbreak attacks. Third, the input permutation defenses focus on ensuring safety by altering or certifying the robustness of inputs to prevent adversarial exploitation (Kumar et al., 2024; Cao et al., 2024). These methods work by transforming or certifying input prompts to maintain alignment while resisting adversarial attacks. Despite their strengths, all three defense categories face challenges in balancing effective defense with maintaining model performance.\nLLM Unlearning. Machine unlearning in the context of LLMs can be categorized into two primary directions: parameter-based unlearning and data-based unlearning. First, the parameter-based unlearning focuses on selectively updating or adjusting the model's parameters to mitigate undesirable behaviors without retraining the entire model (Liu et al., 2018; Tarun et al., 2023). Second, data-based unlearning involves the selective removal or alteration of specific data points that contributed to undesirable behaviors during the training phase (Cao and Yang, 2015; Sekhari et al., 2021)."}, {"title": "Conclusion", "content": "In this work, we propose Layer-AdvPatcher, a novel jailbreak defense framework that precisely targets and mitigates toxic behaviors in LLMs by adversarially exposing and editing the identified toxic layers. By following a three-step pipeline-toxic layer locating, adversarial augmentation, and toxic layer editing our approach successfully identifies the model layers responsible for generating harmful outputs and addresses their vulnerabilities through adversarial exposure and localized unlearning on the augmented harmful dataset. The targeted nature of our framework ensures both effectiveness in reducing jailbreak susceptibility and maintaining model performance. Extensive evaluations across multiple advanced attack methods and utility benchmarks demonstrate the superiority of Layer-AdvPatcher in achieving robust defense compared to recent defense strategies."}, {"title": "Limitations", "content": "A key limitation of this work is while the framework demonstrates efficacy on models like Llama2-7B and Mistral-7B, it has not been tested in larger models (e.g., Llama3-13B), both in terms of computational resources and time. However, it does not significantly weaken the novelty and contribution of the proposed concept of self-exposure and then localized editing. The proposed framework is modular in nature, which can be adapted and scaled to larger models with proper engineering. The proposed techniques of toxic layer identification, adversarial augmentation, and layer editing are applicable across different scales if the computational resource is larger enough.\nA possible ethical consideration is the open-sourcing dataset derived from the identified toxic layers. There is a risk that malicious actors could misuse this information to create more sophisticated jailbreak attacks or find new vulnerabilities."}]}