{"title": "Layered Diffusion Model for One-Shot High Resolution Text-to-Image Synthesis", "authors": ["Emaad Khwaja", "Abdullah Rashwan", "Ting Chen", "Oliver Wang", "Suraj Kothawade", "Yeqing Li"], "abstract": "We present a one-shot text-to-image diffusion model that can generate high-resolution images from natural language descriptions. Our model employs a layered U-Net architecture that simultaneously synthesizes images at multiple resolution scales. We show that this method outperforms the baseline of synthesizing images only at the target resolution, while reducing the computational cost per step. We demonstrate that higher resolution synthesis can be achieved by layering convolutions at additional resolution scales, in contrast to other methods which require additional models for super-resolution synthesis.", "sections": [{"title": "1. Introduction", "content": "High-resolution synthesis of images in pixel space poses a challenge for diffusion models. Previous methods have addressed this by either learning in low-dimensional latent space or generating images via cascaded models focused on increasing resolutions. However, these methods introduce additional complexity to the diffusion framework. Hoogeboom et al. proposed a simpler diffusion model that synthesizes high-resolution images by using shifted noise scheduling and model scaling, among other optimizations.\nIn this work, we extend the simple diffusion model framework to perform hierarchical image synthesis at multiple resolutions. Our goal is to design a light-weight model that can capture spatial image features at specific resolution scales within a singular model, rather than training with respect to the highest resolution. We argue that this approach is preferable to cascaded models, as it allows the upscaling process to be guided by a multitude of learned features within the network, rather than from singular output image pixels for each successive super-resolution model.\nRecently, Gu et al. demonstrated a similar idea, termed \"Matroyshka Diffusion.\" They report similar improvements when compared to multiple baseline approaches. Our model differs primarily in the use of scaled noise between resolutions (Section 2.2) and training optimizations employed for faster convergence (Section 3)."}, {"title": "2. Architecture", "content": "We introduce a new image generation architecture based on a U-Net structure that produces images conditioned on text prompts at multiple resolution scales (Fig. 1). Our architecture differs from a typical U-Net in a number of key ways:\n\u2022 Multiple Inputs: At each resolution scale, we apply an input convolution to expand the input image to the hidden dimension size of the layer of the respective resolution scale.\n\u2022 Isolated Downsampling: Within a standard U-Net architecture, downsampling convolutions are used to decrease tensor resolution. In our model, rather than downsample higher resolutions, the current tensor in the first part of the network, we utilize an input convolution of downscaled image using the technique described in Section 2.2. The information within these tensors is only retained only as skip connections concatenated to the upsampling layers. However, at the lowest resolution (128 x 128), the downsampling is allowed and the model operations proceed as normal (see Fig. 1). We believe this encourages robust learning at every level rather than optimizing for high resolution features, which is important at the lowest resolution where image content is decided . Additionally, eliminating high resolution convolutions decreases the total number of floating point operations (FLOPs) within the model.\n\u2022 Multiple Outputs: We apply out convolutions at relevant resolution scales in the upsampling part of the network which are used to evaluate loss (mean squared error) during training. While higher resolution information is isolated during downsampling, it is concatenated with the output of the lower model layers via skip connection once upsampled. We scale loss with respect to resolution scale. During inference, only the output convolution of the highest resolution in necessary.\nOur model aims to learn spatial frequencies at different levels of the network. We adopt a hierarchical design philosophy for this model. We begin with a base model initially trained at a resolution of 128 \u00d7 128 with an embedding dimension of 256. To include a higher resolution, we include largely parallel convolutional layers atop this structure with (e.g. 256x256 with hidden dimension of 128). We show that our multi-scale model can generate higher quality images than a single resolution-scale model that directly generates images of the same resolution conditioned on text (where hidden dimension sizes are identical).\nWe compare models using Fr\u00e9chet inception distance (FID) and Inception Score (IS) on the MS-COCO validation set."}, {"title": "2.2. Noise Scaling", "content": "Rather than employing independently sampled Gaussian noise for each resolution scale, denoted as \\( \\epsilon \\), we explore a strategy which uses the sinc (also known as Whittaker-Shannon) interpolation formula whereby we randomly sample noise at the highest resolution, and downsample to the lower resolutions. Sinc interpolation acts as a lowpass filter to ensures the preservation of the noise signal across layers, allowing effective sharing of pixel information across different resolutions while maintaining the Gaussian nature of the noise . We formulate our downsampling operation, \\( S \\), as:\n\\[\nS(\\epsilon_0(m, n), T) = \\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1} \\epsilon_0(m, n) \\cdot \\left( sinc(\\frac{m-\\tilde{m}}{T}) \\cdot sinc(\\frac{n-\\tilde{n}}{T}) \\right)\n\\]\nwhere \\( \\epsilon_0 \\) represents noise randomly sampled at the highest resolution via \\( \\epsilon \\sim \\mathcal{N}(0, I) \\), \\( M \\) and \\( N \\) are the dimensions of the target downscaled resolution, \\( \\epsilon_0[m, n] \\) are the pixel values of the high resolution noise at location \\( (m, n) \\), \\( (\\tilde{m}, \\tilde{n}) \\) denotes the corresponding pixel positions in the scaled noise, and \\( T \\) is the downsampling scale. This formula represents a 2D version of the sinc interpolation formula applied to each noise term.\nWe observed superior image synthesis when using sinc interpolation (Fig. A.2), with markedly lower FID and higher IS (Table A.1). We believe this approach is beneficial when compared to randomly sampled noise because pixel-level spatial frequencies for input noise are preserved at each resolution. While we use bilinear scaling for downsampling images (Equation 3), applying the same operation to \\( \\epsilon_0 \\) is problematic because bilinear interpolation nonlinearly modifies the signal profile and does not ensure that the output noise maintains a Gaussian distribution ."}, {"title": "2.3. Cosine Schedule Shifting", "content": "Hoogeboom et al. demonstrated that shifting the cosine schedules used to noise images helped overcome the obstacle of global image structure being defined for a long duration during image synthesis (see Fig. A.3). We similarly implement shifted cosine schedules for the higher resolution layers of the model (Fig. A.4). We first found the optimal delay for a layered model with highest resolution synthesis at 256 \u00d7 256 to be \\( log(\\frac{1}{2}) \\). We then maintain this offset at the 256 \u00d7 256 resolution and swept to find the optimal delay for a model trained to synthesize at 512 \u00d7 512, which was \\( log(\\frac{1}{8}) \\) (Fig. A.5). By doing so, our model is encouraged to synthesize fundamental image features earlier and incorporate textured information at higher levels later on in the diffusion process."}, {"title": "2.4. Mathematical Formulation", "content": "A text-to-image diffusion model \\( \\Sigma_{\\theta} \\) is trained on a denoising objective of given by:\n\\[\nE_{x,c,\\epsilon,t} [w_t || \\Sigma_{\\theta}(a_t x + \\sigma_t \\epsilon, c) - x ||^2 ]\n\\]\nwhere \\( (x, c) \\) are data-conditioning pairs, \\( t \\sim U([0, 1]) \\), and \\( a_t, \\sigma_t \\), and \\( w_t \\) are functions of \\( t \\) that influence sample quality.\nTo extend this objective function to multiple resolutions as discussed in Section 2.1, we introduce a summation over different resolutions as:\n\\[\nE_{x_0,c,\\epsilon_0,t} [w_t \\sum_{i=0}^{K} ||\\Sigma_{\\theta_i} (a_{t,i} D(x_0, i) + \\sigma_{t,i} S(\\epsilon_0, i), c) - D(x_{0, i}) ||^2 ]\n\\]\nwhere \\( K \\) is the number of resolutions, \\( x_{0,i} \\) is the predicted image at resolution \\( i \\), \\( a_{t,i} \\) and \\( \\sigma_{t,i} \\) are obtained via shifted cosine schedules described in Section 2.3, \\( D(x_{0, i}) \\) represents the bilinear downsampling operation applied directly to the highest resolution image \\( x_0 \\), and \\( S(\\epsilon_0, i) \\) represents the sinc interpolation from Equation 1."}, {"title": "3. Training Optimizations", "content": "Saharia et al. demonstrated accelerated training convergence with respect to TPU time by training the 1024 \u00d7 1024 super-resolution model on 64 \u00d7 64 \u2192 256 \u00d7 256 crops. We similarly found that we could utilize cropping to increase training speed by cropping all input images to 128 \u00d7 128. However, to accommodate our layered model architecture, a crop must also be applied within the a tensor immediately before upscaling (Fig. A.7). This was done with the following procedure:\n1. Randomly select a 64 \u00d7 64 square within the base 128 \u00d7 128 image.\n2. Identify the corresponding 128 x 128 positions within the 256 \u00d7 256 image via simple multiplication.\n3. Crop 256 \u00d7 256 image to 128 \u00d7 128.\n4. Repeat for higher resolutions by utilizing the previous resolution.\n5. Apply the corresponding 64 \u00d7 64 crops in the upsample stack of the U-Net prior to the upsampling convolution to bring the output image to 128 \u00d7 128."}, {"title": "3.2. Model Stacking", "content": "A limitation of this implementation is that it requires images of at least the largest resolution of the model, which fundamentally limits the training set size based on depending on the highest resolution present. For example, a model trained on images of [512\u00d7512, 256\u00d7256, 128\u00d7128] would require a minimum base image resolution of 512 \u00d7 512\nTo address this issue, we leveraged the hierarchical nature of our model by iterative rounds of training and checkpointing. Each resolution scale is constructed upon its predecessor, allowing us to initialize a higher resolution model with weights from a model trained with a smaller maximum resolution."}, {"title": "4. Results", "content": "In Table 1, we evaluate our layered diffusion model against baseline models trained at singular target resolution from the perspective of image generation quality (FID and IS) as well as total floating point operations (FLOPs). We observe that our multi-scale synthesis outperforms single-scale models when the maximum output resolution was both 256 \u00d7 256 and 512x 512 Note that the 256 \u00d7 256 comparison is evaluated on a model trained with on two input images [128 \u00d7 128, 256 \u00d7 256], and not the individual outputs of the 512 \u00d7 512 model.)\nAt 512 x 512 resolution, our model reduces the FID score by 25% and increases the IS score by 24%, while also decreasing the FLOPs by 14%. This indicates that our model can generate more realistic and diverse images with less computation. Our multi-scale model also improves the FID and IS scores at 256 \u00d7 256 resolution by 8% and 4%, respectively, compared to the single-scale model. We observe image features in accordance with these findings (Fig. A.1), where higher resolution features can be seen in outputs from the layered model."}, {"title": "5. Conclusion", "content": "In this study, we propose a novel one-shot text-to-image diffusion model that can generate high-resolution images from natural language descriptions in a single forward pass. Our model employs a layered U-Net architecture that predicts images at multiple resolution scales simultaneously, using a sinc interpolation formula to scale noise noise and a shifted cosine schedule for the diffusion steps. We show that this method outperforms the baseline of synthesizing images only at the target resolution, while reducing the computational cost per step. Moreover, our model does not need any additional super-resolution models to enhance the image quality. We also explore various techniques to improve the training efficiency and the image quality, such as cropping and stacking. We believe that our model is an interesting path to explore further which streamlines text-to-image synthesis to a one-shot method."}]}