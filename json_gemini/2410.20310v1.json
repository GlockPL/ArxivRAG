{"title": "ANOMIX: A Simple yet Effective Hard Negative Generation via Mixing for Graph Anomaly Detection", "authors": ["Hwan Kim", "Junghoon Kim", "Sungsu Lim"], "abstract": "Graph contrastive learning (GCL) generally requires a large number of samples. The one of the effective ways to reduce the number of samples is using hard negatives (e.g., Mixup). Designing mixing-based approach for GAD can be difficult due to imbalanced data or limited number of anomalies. We propose ANOMIX, a framework that consists of a novel graph mixing approach, ANOMIX-M, and multi-level contrasts for GAD. ANOMIX-M can effectively mix abnormality and normality from input graph to generate hard negatives, which are important for efficient GCL. ANOMIX is (a) A first mixing approach: firstly attempting graph mixing to generate hard negatives for GAD task and node- and subgraph-level contrasts to distinguish underlying anomalies. (b) Accurate: winning the highest AUC, up to 5.49% higher and 1.76\u00d7 faster. (c) Effective: reducing the number of samples nearly 80% in GCL. Code is available at https://github.com/missinghwan/\u0391\u039d\u039f\u039c\u0399\u03a7.", "sections": [{"title": "Introduction", "content": "Graph anomaly detection (GAD) aims to identify nodes that significantly deviate from the majority, and has drawn much attention from various domains (Kim et al. 2022), such as fraud detection (Zhang et al. 2022a) and IoT network intrusion detection (Zhou et al. 2022). Early works (Perozzi and Akoglu 2016; Li et al. 2017; Peng et al. 2018) on GAD have been largely dependent on domain knowledge and statistical methods. Recently, deep learning approaches have proved that they can effectively handle large-scale high-dimensional data and extract patterns from the data, thereby achieving satisfactory performance without the burden of handcrafting features (Akoglu, Tong, and Koutra 2015). These approaches are known to ineffectively handle complex interactions and attribute information on attribute graph (Ma et al. 2023). More recently, Graph Neural Networks (GNNs) have been adopted to efficiently and intuitively detect anomalies from graphs due to highly expressive capability via the message passing mechanism in learning graph representations (e.g., (Ding et al. 2019; Duan et al. 2023)).\nThe approaches for GAD are predominantly advanced in an unsupervised manner (Ding et al. 2019; Jin et al. 2021) because labeling is costly and requires domain knowledge.\nAmong these methods, graph contrastive learning, which estimates the dissimilarities via contrasting positive and negative samples, has demonstrated its effectiveness in GAD (Liu et al. 2021; Duan et al. 2023). For these graph contrastive learning, generally, a substantial number of samples are required to prevent the degeneration issue that a model considers all samples similar representations (Zhang et al. 2022b), while enhancing the performance. Apparently, managing such a large number of negative samples can be computationally expensive. An effective way to diminish the number of samples and improve detection accuracy is generating or mining an expressive set of hard negatives, which are difficult to distinguish (Lee et al. 2020). Motivated by the hard negative approach, we aim to design mixing-based methods to generate hard negatives for GAD. The mixing-based methods are inspired by classical data augmentation methods. For example, Mixup (Zhang et al. 2018) generates synthetic images (see Fig 1(Up)), which can help identify samples near the decision boundary. In addition, Mixup acts as an effective regularization strategy for training image classifiers, which smoothens decision boundaries and improves the arrangements of hidden representations.\nAlthough the mixing-based methods are effective in augmenting the image data, designing mixing-based approach for graph learning is challenging (Wang et al. 2021). This challenge is rooted in the irregularity and connectivity of graph data (Zhang et al. 2022b). For example, in GNNs, node representations are made via the message-passing mechanism, which aggregates the representations between"}, {"title": "Related Work", "content": "each node and its neighbors. Hence, this node representation depends on the nodes and edges inside its receptive field. As a result, we need to mix receptive field in the subgraphs to mix nodes. However, unlike pixels in an image, nodes are not arranged in a regular grid but are unordered, making it challenging to align nodes in different subgraphs for graph mixing. In addition, the mixing each node can interfere with one another due to complex connectivity between nodes, and it can cause conflicts and perturb the mixed features.\nThe existing graph mixing methods attempt to augment input graph via randomly mixing structure and attribute for either graph or node classification tasks (Wang et al. 2021). These approaches may lead to unexpected changes to both structural and contextual features of graph (Park et al. 2021). Moreover, since these methods are originally designed for the classification tasks, it could not be appropriate to directly adopt these mixing techniques for GAD task due to inherent limitation (e.g., severe class imbalance). Designing graph mixing approach for GAD task is non-trivial, since the class imbalance issue naturally leads to very limited number of anomalies in graph. These very few anomalies could cause some difficulties, e.g., model degeneration (Akoglu, Tong, and Koutra 2015). Therefore, an effective approach for mixing graphs can be necessary to address the hardness above and to apply the mixing to GAD task, while improving detection accuracy.\nIn this paper, we propose ANOMIX, a novel framework that contrasts multi-level, node- and subgraph-level, representations of normality and abnormality with the generated hard negatives. In addition, we devise ANOMIX-M, a simple yet effective mixing approach to generate hard negatives by using a few labeled information as prior knowledge for the label-guided generation as depicted in Fig. 1 (Down). Since the abnormality features can become similar to the normality representations through a message passing mechanism in GNNs (Kipf and Welling 2016), it can be harder for a model to detect anomalies than the features embedded with only anomalies. These hard negative samples can help identify the underlying anomalies, frequently misclassified as normal nodes. To further effectively learn these designed hard samples, we consider node- and subgraph-level contrasts for normality/abnormality with the hard negative samples.\nIn summary, our main contributions are as follows:\n\u2022 Mixing graph: We introduce ANOMIX-M, a first graph mixing approach designed for GAD. To our best knowledge, this is the first attempt to mix graphs for GAD in graph contrastive learning.\n\u2022 Multi-level contrast: We design ANOMIX, a framework that effectively captures node- and subgraph-level features from the mixed graphs.\n\u2022 Detection accuracy: Experiments on public benchmarks show that ANOMIX outperforms the state-of-the-art competitors, reporting up to 5.49% higher AUC value. Moreover, extended studies demonstrate the effectiveness of \u0391\u039d\u039f\u039c\u0399\u03a7.\n\u2022 Effectiveness of hard negatives: We theoretically and empirically demonstrate how our mixing approach effectively works as hard negatives and analyze its robustness through extended experiments."}, {"title": "Graph Anomaly Detection", "content": "The early works (Akoglu, Tong, and Koutra 2015; Peng et al. 2018; Li et al. 2017) usually adopt the statistic-based approaches, which detect the anomalous information from either node features and graph structure. These shallow approaches are limited to extract high-dimensional features and complex structures. With the advent of GNNs, various GNN-based approaches for GAD have been presented with skyrocketing growth of GNN techniques. DOMINANT (Ding et al. 2019) firstly adopts GNNs in reconstruction manner, which encodes structure and attribute information and then decodes the structure and attribute features respectively. CoLA (Liu et al. 2021) uses self-supervised learning with sampling local subgraphs and comparing the subgraphs with target nodes. ComGA (Luo et al. 2022) captures global, local, and structure anomalies by utilizaing structure information of community structure, attribute and structure features of the whole graph. GRADATE (Duan et al. 2023) augments subgraphs and contrasts the subgraphs in several matches."}, {"title": "Graph Contrastive Learning", "content": "Graph contrastive learning (GCL) excavates supervised information for downstream tasks without expensive labels and has made great achievements (Liu et al. 2022). Early works including DGI and InfoGraph adopt the idea of local-global contrastive objective[16] to node/graph representation learning respectively by contrasting node-graph pairs. A significant portion of research in GCL focus on graph augmentations. For instance, GRACE (Zhu et al. 2020) employ random graph augmentation techniques. GraphCL (You et al. 2020) and JOAO (You et al. 2021) focus on empirically combining different types of augmentations. ProGCL (Xia et al. 2022) reweights the negatives and empowers GCL with hard negative mining. RES-GCL (Lin et al. 2024) incorporates randomized edgedrop noises for robust GCL."}, {"title": "Hard Negative Mining via Mixing Graph", "content": "Hard negative mining refers to generating negative pairs, which are difficult to discriminate. Mixing-based mining inspired by classical augmentation method Mixup (Zhang et al. 2018). Early work (Wang et al. 2021) introduces the two-branch graph convolution to mix the receptive field subgraphs for the paired nodes. M-Mix (Zhang et al. 2022b) mines hard negatives via mixing multiple samples and assigning different mixing values dynamically. G-mixup (Han et al. 2022) mixes the graphs in different classes to generate synthetic graphs for graph classification. S-mixup (Ling et al. 2023) considers node-level correspondence (i.e., soft alignment matrix) between two graphs when performing mixup.\nDepart the success of hard negative mining via mixing in vision and node/graph classification tasks, the mixing-based"}, {"title": "PROPOSED FRAMEWORK: ANOMIX", "content": "ANOMIX consists of two modules: (1) Graph Mixup module; and (2) Multi-Level Contrast modules. Fig. 2 presents overall procedures."}, {"title": "Preliminaries", "content": "Attributed graph. An attributed graph can be denoted as G = (V, E, X) where V = {V1, V2, \u2026\u2026\u2026, Un} is a set of nodes with |V| = n, E is a set of edges with |E| = m, and X \u2208 \u211dn\u00d7d denotes node attributes. The i-th row of X, xi \u2208 \u211dd(i = 1, . . ., n), is the attribute information of node vi. Additionally, A \u2208 {0,1}n\u00d7n is the adjacency matrix corresponding to the structure of G.\nGraph anomaly detection. Graph anomaly detection is commonly formulated as a ranking problem (Kim et al. 2022). Given an attributed graph G, the task aims to score each node vi with a scoring function ki = f(vi) by preserving the context information. Hence, the anomaly score ki is the degree of abnormality of vi. Based on the scores, anomalous nodes can be detected by the ranking."}, {"title": "Graph Mixup (ANOMIX-M)", "content": "To generate hard negatives via mixing graphs, we first compose ego-nets (or subgraphs), which take a target node and its neighboring nodes, for both abnormality and normality, as shown in Fig. 2. We generate each normality- and abnormality-centered ego-net based on labeled and unlabeled information. The motivation behind ego-net generation is to model underlying abnormal nodes surrounded by normal nodes and to provide sufficient diversity of noises for anomalies as hard negatives. The representations of the abnormality-centered ego-net can contain neighboring normality features as noises in the abnormality perspective. Since we do not have the labeled normal nodes, we consider the unlabeled data as normal. Our empirical results show that ANOMIX performs very effectively by using this simple strategy, and it is robust to different contamination levels (i.e., the proportion of anomalies in the unlabeled data).\nFor our graph sampling strategy, we employ a random walk, which is one of the most fundamental sampling algorithms and widely used in sampling subgraphs for GAD (Liu et al. 2021; Jin et al. 2021). Specifically, taking a target node vi as a center node, we sample both abnormal and normal ego-nets with a fixed size K. These ego-nets are expressed as G^{(i)}_{nd} = (A^{(i)}_{nd},X^{(i)}_{nd}) (node-level) and G^{(i)}_{sb} = (A^{(i)}_{sb},X^{(i)}_{sb}) (subgraph-level). Note that we generate node- and subgraph-level ego-nets G^{(i)}_{nd}, G^{(i)}_{sb} for normality and abnormality, respectively. In addition, we mask each target node to prohibit information leakage during the following contrastive learning stages (Jin et al. 2021; Liu et al. 2021). In detail, the attribute vector of the target node is replaced with a zero vector as follows: X^{(i)}_{nd} [1,:] \u2190 0, X^{(i)}_{sb} [1,:] \u2190 0."}, {"title": "Multi-Level Contrast", "content": "Node-level contrast. In node-level contrastive network, a model is to be trained with representations between the masked target node in the sampled ego-net G^{(i)}_{nd} and original target node vi. First, the node representations H^{(i)}_{nd} in ego-net are achieved by GCN:\nH^{(l)}_{nd} = GCN(D^{(i)}_{nd} (A^{(i)}_{nd} + I)(H^{(l-1)}_{nd}W^{(l-1)}),\nwhere A^{(i)}_{nd} = A^{(i)}_{nd} + I is the subgraph adjacency with self-loop, D^{(i)}_{nd} is the degree matrix of ego-net, W^{(l-1)} is the weight matrix of the (l \u2013 1)-th layer. After embedding, we choose the feature of masked target node by h^{(i)}_{nd} = H^{(l)}_{nd} [1,: ] for node-level contrastive learning. Subsequently, the target node vi is embedded by a MLP layer. We express the attribute vector of v\u1d62 as x^{(i)} = X[i, :] and target node representation z^{(i)}_{nd} is as follows:\nz^{(2)}_{nd} = MLP(x^{(i)}),\nhere, the weight is shared with GCN layer in Eq. 1 to make sure that representations, h^{(i)}_{nd} and z^{(i)}_{nd}, are projected into the same embedding space. Then, contrastive learning is conducted with the representations h^{(i)}_{nd} and z^{(i)}_{nd} as follows:\nS_{nd}^{(i)} = \u03c3(h^{(i)}_{nd}, z^{(i)}_{nd}),"}, {"title": "Algorithm 1: ANOMIX", "content": "where \u03c3(.) is a sigmoid function and S_{nd}^{(i)} is a node-level similarity score. For efficient contrastive learning, we employ a negative sampling strategy in training. Hence, S_{nd}^{(i)} is denoted as positive score and negative score s_{nd}^{(i)} is computed by: S_{nd}^{(i)} = \u03c3(h^{(i)}_{nd},z), where h^{(i)}_{n} is for another node v\u2c7c in the ego-net and i \u2260 j. Based on both positive S_{nd}^{(i)} and negative S_{nd}^{(i)}, the node-level contrastive network is trained with Jensen-Shannon divergence objective:\nL_{nd} = -\\frac{1}{2n} \\sum_{i=1}^n [log(S_{nd}^{(i)}) + log(1 - \\bar{S}_{nd}^{(i)})].\nNote that the function Lnd, initially derived for normality, can similarly be applied to abnormality using the same process.\nSubgraph-level contrast. We construct the subgraph-level contrastive network, similar to the node-level network. First, similar to Eq. 1, node representation H(l)sb of the input ego-net G^{(i)}_{sb} is generated by GCN: H^{(l)}_{sb} = GCN(D^{(i)}_{sb}(A^{(i)}_{sb} + I)(H^{(l-1)}_{sb}W^{(l-1)})). Note that the main difference between node- and subgraph-level contrasts is that the subgraph-level attempts to learn the context features between the target node and the ego-net representations via the following readout function: hsb = readout(H(l)sb) ="}, {"title": null, "content": "H(l)sb [j,:], here we employ the average pooling as our readout function. Then, a single MLP layer (similar to Eq. 2) is employed to project attributes into the same embedding space and to calculate z(l)sb. Subsequently, the subgraph-level score S_{sb}^{(i)} is computed by a bilinear function (similar to Eq. 3). Finally, this context-level network is trained with a subgraph-level negative score s_{sb}^{(i)} by the following objective:\nL_{sb} = -\\frac{1}{2n} \\sum_{i=1}^n [log(S_{sb}^{(i)}) + log(1 - \\bar{S}_{sb}^{(i)})].\nNote that the function L_{sb}, initially derived for abnormality, can similarly be applied to normality using the same process.\nJoint learning. The overall objective function with two contrastive networks in training is as follows:\nL_{norm} = \u03b1L_{sb} + (1 \u2212 \u03b1)L_{nd},\\L_{joint} = \u03b2L_{norm} + (1 - \u03b2)L_{ab},\nwhere \u03b1 is a trade-off parameter between node- and subgraph-level information. \u03b2 is also a trade-off parameter to balance between the levels of normality and abnormality. L_{ab} is a contrastive loss, same as L_{norm}, for abnormality.\nAnomaly Score Estimation\nANOMIX leverages an anomaly score estimator for each node to calculate its anomaly scores. In detail, for a given target node vi, R is the number of ego-nets for node- and subgraph-level contrastive networks generated, respectively. Consequently, after contrastive learning, we acquire total 8R scores for normal and abnormal samples:\n[S_{nd,1}^{(i)},...,S_{nd,R}^{(i)}, S_{sb,1}^{(i)},...,S_{sb,R}^{(i)}], [\\bar{S}_{nd,1}^{(i)},...,\\bar{S}_{nd,R}^{(i)}, \\bar{S}_{sb,1}^{(i)},...,\\bar{S}_{sb,R}^{(i)}], [aS_{nd,1}^{(i)},...,aS_{nd,R}^{(i)}, aS_{sb,1}^{(i)},...,aS_{sb,R}^{(i)}], [a\\bar{S}_{nd,1}^{(i)},...,a\\bar{S}_{nd,R}^{(i)}, a\\bar{S}_{sb,1}^{(i)},...,a\\bar{S}_{sb,R}^{(i)}],\nwhere as and a\\bar{S} are the abnormal scores for positive and negative samples, respectively. We simply define the base score as the difference between negative and positive scores as follows:\nbs_{lev,j}^{(i)} = ( (S_{lev,j}^{(i)} - \\bar{S}_{lev,j}^{(i)}) + (aS_{lev,j}^{(i)} - a\\bar{S}_{lev,j}^{(i)}) ) / 2,\nwhere the subscript \"lev\" denotes \"node-level\" or \"subgraph-level\" and j\u2208 [1,\uff65\uff65\uff65, R]. Since anomalies, in general, show relatively high and unstable scores on multi-sampling rounds, we adopt statistical techniques to improve generalization as Eq. 8. Hence, the final anomaly scores Ts and ts are defined as follows:\nts_{lev}^{(i)} = \\frac{1}{R} \\sum_{j=i}^R bs_{lev,j}^{(i)},\\ts_{lev}^{(i)} = bs_{lev}^{(i)} + \u03c3(bs_{lev}^{(i)}) \\frac{\\sum_{j=1}^R (bs_{lev,j}^{(i)} - \\bar{bs_{lev}} )}{\\sum_{j=1}^R \\bar{bs_{lev}} / R}.\nComplexity Analysis\nWe analyze the time complexity of ANOMIX by considering the main processes. For randomwalk-based sampling,"}, {"title": null, "content": "the time complexity is O(W|V|L), where W is the number of walks per node, V is the number of node, and L is length of walk. For graph mixup, the time complexity is O(N + M), where N is the number of node in the mixed graph and M is the number of abnormal node for mixing nodes. The time complexity is mainly caused by GNNs during training, which is (l|E|F\u00b2 + IV2F), where F is size of features, E is the number of edges, and I is the number of layers (Wu et al. 2020). For anomaly score computation, the time complexity is far less than the above two phases, so here we ignore this term. To sum up, the overall time complexity of ANOMIX is O(W|V|L + (l|E|F2 + IV2F))."}, {"title": "Experiments", "content": "We conduct an extensive experiment with 12 competitors on six real-world networks to answer the following evaluation questions (EQs):\n\u2022 EQ1. Accuracy: How well does ANOMIX work for GAD on the benchmarks? This EQ evaluates the performance (AUC) and ROC curve of ANOMIX and compares the results with the competitors.\n\u2022 EQ2. Effectiveness of ANOMIX-M: How effectively do mixed graphs from ANOMIX-M work as hard negatives in GAD. This EQ evaluates effectiveness of proposed hard negatives on different mixing scenarios, size of sampled graph, and running time and compares it with state-of-the-art GCL approaches.\n\u2022 EQ3. Sensitivity study: How is the performance of ANOMIX robust on varying parameters? This EQ analyzes the sensitivity with hyperparameters (\u03b1, \u03b2, r, and \u03b3).\n\u2022 EQ4. Theoretical analysis: How can the mixed graph from ANOMIX-M become hard negatives? This EQ theoretically demonstrates how our mixed graphs can become hard negatives.\nDatasets. We choose six widely-used real world networks ranging from citation and social networks to online commercial network, such as ACM, Cora, CiteSeer, Pubmed, Amazon (Dou et al. 2020), and Facebook (Xu et al. 2022). In addition, since there are no anomalies in the citation networks (first four datasets), we conduct the same anomaly injection process as previous works (Ding et al. 2019; Liu et al. 2021; Jin et al. 2021; Luo et al. 2022). The other two datasets contain real anomalies. The statistics are summarized in Table 1. The column R/I is type of anomaly, either 'Injected' or 'Real'."}, {"title": "Baselines", "content": "For baselines, we select the nine unsupervised approaches and three semi-supervised method as follows:\n\u2022 Unsupervised method: Among nine methods, three methods are statistic-based (AMEN (Perozzi and Akoglu 2016), Radar (Li et al. 2017), ANOMALOUS (Peng et al. 2018)), two methods are reconstruction-based (DOMINANT (Ding et al. 2019), ComGA (Luo et al. 2022)), and three are GCL-based (CoLA (Liu et al. 2021), ANEMONE (Jin et al. 2021), GRADATE (Duan et al. 2023)), and the rest is GNN-based state-of-the-art method (LHML (Guo et al. 2022)).\n\u2022 Semi-supervised method: Three deep learning and GNN-based methods are selected (DeepSAD (Ruff et al. 2020), GDN (Ding et al. 2021), Semi-GNN (Kumagai, Iwata, and Fujiwara 2021))."}, {"title": "Detection effectiveness (EQ1)", "content": "Overall, ANOMIX outperforms all the state-of-the-art baselines and all datasets up to 5.49% higher AUC value (Table 2), and the ROC curves support the superiority of ANOMIX (Fig. 3). On the Pubmed dataset, ANOMIX, while yielding relatively better results than CoLA, may not demonstrate a distinct advantage. This could be largely attributed to the synthetic nature of the anomalies. Specifically, Pubmed is a relatively sparse network (average degree < 3) and anomalies are injected as the form of clique based on the intuition that a densely connected set of nodes can be regarded as anomalous situation in real-world networks (Skillicorn 2007). A clique is generated on randomly selected c nodes, making them fully connected (c = 15). Therefore, in the sparse graph, there may rarely exist the hidden anomalies, which are densely connected by normal nodes, while pretending to be normal. Hence, one of advantages of our method, detecting the hidden anomalies, could be diluted on Pubmed."}, {"title": "Ablation Study", "content": "Mixing strategy (EQ2). To demonstrate the effectiveness of generated hard negatives via mixing, we design three different mixing scenarios: 1) without mixing (w/o mix), 2) with random-based mixing (w r-mix), and 3) with proposed mixing. As listed in Table 3, the generated subgraph without mixing can be easy negative samples, which can be identified more easily than hard negatives. Although ANOMIX with random mix shows better detection accuracy than ANOMIX without mix, the ANOMIX with proposed hard negatives shows greater performance on all datasets. In addition, our method shows novel performance achievement (up to 8.59%) especially on ACM dataset. Based on these results, we could conclude that the performance improvement may stem from the proposed hard negative samples via mixing graph, while effectively modeling underlying anomalies and extracting important features from it with our multi-level contrasts.\nEffectiveness of hard negatives (EQ2). In general, GCL requires a large number of negative samples. To effectively reduce the number of samples, while enhancing performance, the hard negative mining has been proposed and we introduce mixing-based hard negatives. To evaluate how effec-"}, {"title": null, "content": "tively our hard samples diminish the required number of samples, we attempt different sample sizes with two state-of-the-art GCL methods. Fig. 5 shows the results on ACM dataset and proves the effectiveness of proposed hard samples, while reporting almost optimal AUC value on the small negative size (3000). Meanwhile, although the performance of GRADATE and ANEMONE tends to be improved as the size increases, these methods report performance degradation on the small size. We empirically demonstrate that the generated hard negatives from ANOMIX-M can considerably reduce, nearly 80%, the amount of samples for GCL. Note that ANEMONE randomly samples subgraphs and GRADATE copies the input graph and modifies its structure via randomly dropping edges. These methods are simply based on randomness in their graph augmentation processes. Unlike ANOMIX, they do not consider sampling hard negatives, which can be important to decrease the amount of samples, for more effective GCL.\nRunning efficiency (EQ2). To further prove the efficiency of ANOMIX, we compare the running time and AUC values with state-of-the-art GCL methods (CoLA, ANEMONE, GRADATE) on Citeseer dataset as shown in Fig. 6. ANOMIX runs 1.76\u00d7 faster than the existing GCL approaches. Due to the fact that GCL generally requires"}, {"title": "Sensitivity analysis (EQ3)", "content": "In this experiment, we further explore the effect of level-balance parameter \u03b1, normality-balance parameter \u03b2, and different contamination ratio r (using 1%, 5%, and 10% in all labeled anomalies) as showin in Fig. 4. ANOMIX records its optimal performance when \u03b1 is 0.4 for Cora, 0.6 for the CiteSeer, and 0.8 for the Pubmed dataset (Fig. 4a). Similarly, when \u03b2 is 0.4 for Cora, 0.8 is for both CiteSeer and Pubmed, it reports optimal performance (Fig. 4b). By jointly considering these perspectives, we can obtain the best results. Although the performance slightly"}, {"title": "Theoretical justification (EQ4)", "content": "To demonstrate how the mixed graphs from ANOMIX-M can become hard negative samples by message-passing, we need to prove that anomalous nodes can have representations similar to normal nodes, and it makes a model hard to distinguish anomalies. In message-passing, the feature representation of node v\u1d62 is updated based on the information of its neighbors. At L layers, the final representation of node v\u1d62 is h(L)i = Aggregate ({h(L-1)j | j \u2208 N(i)})+h(L-1)i, where N(i) is the set of neighbors of node i, and Aggregate is an aggregation function (e.g., Sum, Mean). \u2295 denotes concatenation or another combining operation.\nAssume that node v\u1d62 is an anomalous node, and node v\u2c7c is a normal node. For anomalous node v\u1d62 to be a hard negative sample, its representation should be similar to the representation of normal nodes. This can be formally defined with the similarity distance between normal and abnormal rep-"}, {"title": null, "content": "resentations: ||h(L)i - h(L)j ||2 < \u03f5, where \u03f5 is a similarity threshold. If the aggregation function is Mean, then:\nh(l+1)i = \\frac{1}{|N(i)|} \\sum_{j\u2208N(i)} h(l)j ,\nsimilarly:\nh(l+1)j = \\frac{1}{|N(j)|} \\sum_{i\u2208N(j)} h(l)i .\nIf N(i) \u2248 N(j) or their neighbors have similar information, then h(l)i \u2248 h(l)j, leading to h(L)i \u2248 h(L)j.\nTherefore, the anomalous node v\u1d62 can have representations h(L)i very close to those of normal node h(L)j with similar neighborhood aggregations, and it makes the anomalous nodes hard to distinguish from normal nodes, thereby working as hard negative samples."}, {"title": "Conclusion", "content": "In this paper, we introduce ANOMIX, a novel framework consisting of ANOMIX-M to generate mix-based hard negatives and multi-level contrasts to efficiently distinguish the underlying anomalies. Through extensive experiments with 12 state-of-the-art baselines and case studies on real-world datasets, we demonstrate the superiority of ANOMIX among all the baselines and datasets. We also conduct ablation and sensitivity studies to further prove the effectiveness of the proposed hard negatives and robustness with varying parameters. The promising results of ANOMIX can imply that the proposed graph mixing for hard negatives can actually help identify the hidden anomalies."}]}