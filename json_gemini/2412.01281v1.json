{"title": "FedPAW: Federated Learning with Personalized Aggregation Weights for Urban Vehicle Speed Prediction", "authors": ["Yuepeng He", "Pengzhan Zhou", "Yijun Zhai", "Fang Qu", "Zhida Qin", "Mingyan Li", "Songtao Guo"], "abstract": "Vehicle speed prediction is crucial for intelligent transportation systems, promoting more reliable autonomous driving by accurately predicting future vehicle conditions. Due to variations in drivers' driving styles and vehicle types, speed predictions for different target vehicles may significantly differ. Existing methods may not realize personalized vehicle speed prediction while protecting drivers' data privacy. We propose a Federated learning framework with Personalized Aggregation Weights (FedPAW) to overcome these challenges. This method captures client-specific information by measuring the weighted mean squared error between the parameters of local models and global models. The server sends tailored aggregated models to clients instead of a single global model, without incurring additional computational and communication overhead for clients. To evaluate the effectiveness of FedPAW, we collected driving data in urban scenarios using the autonomous driving simulator CARLA, employing an LSTM-based Seq2Seq model with a multi-head attention mechanism to predict the future speed of target vehicles. The results demonstrate that our proposed FedPAW ranks lowest in prediction error within the time horizon of 10 seconds, with a 0.8% reduction in test MAE, compared to eleven representative benchmark baselines. The source code of FedPAW and dataset CarlaVSP are open-accessed at: https://github.com/heyuepeng/PFLlibVSP and https://pan.baidu.com/s/1qs8fxUvSPERV3C9i6pfUIw?pwd=tl3e.", "sections": [{"title": "I. INTRODUCTION", "content": "VEHICLE speed prediction plays a significant role in modern intelligent transportation systems (ITS), serving as a critical technology to enhance road safety, traffic efficiency, and vehicle energy efficiency [1]. It is widely applied in areas such as path planning, energy management of hybrid electric vehicles (HEVs) [2], [3], and ecological adaptive cruise control (EACC) [4]\u2013[6]. However, accurately predicting the speed of an individual vehicle presents a considerable challenge, as vehicle speed can be influenced by traffic conditions, vehicle types, road conditions, and driver behaviors [1]. Compared to traditional prediction methods [7]\u2013[10], Recurrent Neural Networks (RNNs) leveraging vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication have been proven more effective for vehicle speed prediction [3], [5], [6], [11]. Nevertheless, former researches have not accounted for the significant differences in drivers' driving styles and vehicle types, which greatly impact speed prediction for the target vehicle, hence failing to achieve personalized predictions. Collaborative training on driving data collected from multiple vehicles is crucial for enhancing prediction accuracy, as a single vehicle would not possess sufficient data to train a reliable model. However, traditional distributed machine learning techniques necessitate centralizing the private driving data of all vehicles on a central server (e.g., cloud servers), posing potential risks for private data leakage.\nFederated Learning (FL) emerges as a collaborative, distributed machine learning paradigm coordinated by a central server and solved jointly through a network of multiple participating devices (clients). Each client possesses its private dataset, not uploaded to the server. Instead, locally trained models are uploaded to aggregate into a global model on the server, effectively reducing privacy risks [12]. However, traditional FL approaches like FedAvg [12] and FedProx [13], when applied to vehicle speed prediction, fail to deliver personalized speed predictions with a single global model.\nPersonalized Federated Learning (PFL) methods address the statistical heterogeneity in FL, offering personalized solutions [14]. Unlike traditional FL, which seeks a single high-quality global model, PFL methods prioritize training local models for each client [15]. However, existing PFL methods almost invariably require additional steps on the client side to achieve personalization, often increasing the computational and communication overhead for clients. Given the real-time requirements of driving scenarios and the limited computational and communication resources of vehicles, implementing personalization steps on a powerful server appears to be more appropriate."}, {"title": "II. RELATED WORK", "content": "In this section, we investigate the related works of personalized federated learning and vehicle speed prediction, which are the two most concerned research fields in this paper. The state-of-art advancements are discussed, while their drawbacks when applied in our topic are also analyzed."}, {"title": "A. Personalized Federated Learning", "content": "The traditional FL method, such as FedAvg [12], aims to learn a single global model by aggregating local models from all clients, expecting this model to perform well on most clients. However, this approach often suffers in statistically heterogeneous environments, such as when facing non-IID data, potentially leading to degraded model performance [23]. FedProx [13] enhances the stability of the FL process by introducing a proximal term in the client optimization process. However, due to the statistical heterogeneity in FL, obtaining a single global model that fits well across diverse clients remains challenging [23], [24].\nPersonalized approaches have gained widespread attention for addressing statistical heterogeneity in FL [23]. We categorize the PFL methods for aggregating models on the server into the following four categories:\n(1) Meta-learning and fine-tuning. Per-FedAvg [17] and FedMeta [25] incorporate a meta-learning framework, utilizing the update trend of the aggregated model to learn a global model, easily adapted to local datasets through a few local fine-tuning steps, to achieve good performance on each client.\n(2) Personalized heads. FedPer [26] divides the neural network model into base and personalized layers, with the base layer designed to be shared across all clients to learn generic feature representations, while the personalized layer remains local to each client for customization. FedRep [18] similarly learns shared data representations across clients and unique local heads for each client.\n(3) Learning additional personalized models. pFedMe [19] uses Moreau envelopes as regularized loss functions for clients, learning additional personalized models for each client. Ditto [20] allows each client to learn its additional personalized model through a proximal term, leveraging information from the global model downloaded from the server.\n(4) Personalized aggregation learning local models on clients. Recent methods attempt to generate client-specific local models through personalized aggregation on clients [15]. APFL [21] obtains personalized models through the aggregation of global and local models, introducing an adaptive learning mixture parameter for each client to control the weight of global and local models. FedFomo [22] locally aggregates other relevant client models for local initialization in each iteration, effectively computing the best weighted model combination for each client based on how much one client can benefit from another's model. FedALA [15] captures the necessary information for client models in personalized FL in the global model, with the key component being the Adaptive Local Aggregation (ALA) module, which adaptively aggregates the downloaded global model and local model on clients, thus initializing local models before each iteration of training.\nHowever, these PFL methods require additional personalization steps to be implemented on the client, which typically"}, {"title": "B. Vehicle Speed Prediction", "content": "The trivial methods for vehicle speed prediction assume constant vehicle speed or acceleration [27], [28]. Traditional prediction methods also include Markov Chains [7], [8], Bayesian Networks [9], [10]. Recurrent Neural Networks (RNN) have proven to be more effective in handling time-series data, hence exhibiting better performance in speed prediction. Some prior studies rely solely on historical speed to predict future velocities, where improvements are sought through enhancing the model structure. Shih et al. [29] propose a network consisting of encoder-decoder, LSTM, and attention models for predicting vehicle speed. Li et al. [30] introduce a hybrid prediction model that combines K-means, BiLSTM, and GRU.\nHowever, vehicle speed is determined by many complex factors, such as the preceding vehicle and traffic signals. To enhance the accuracy of speed predictors, utilizing information from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications is crucial [4]. Han et al. [3] combine a one-dimensional CNN with a BiLSTM to predict vehicle speeds using the information provided by V2V and V2I communications. Jia et al. [5] develope an LSTM-based energy-optimal adaptive cruise control for vehicle speed prediction in urban environments. Wegener et al. [11] explore longitudinal speed prediction in urban settings using V2V and V2I communication information through Conditional Linear Gaussian (CLG) models and LSTM. Chada et al. [6] conduct speed predictions in urban and highway scenarios using speed limits as well as V2V and V2I communication information, achieving higher prediction accuracy on a simulated driving dataset with LSTM.\nExisting vehicle speed prediction models do not consider differences in drivers' driving styles and vehicle types, failing to achieve personalized speed prediction. Moreover, if centralized training is adopted, it does not protect the privacy of drivers' driving data."}, {"title": "III. DATASET AND PREDICTION MODEL", "content": "In this section, we discuss selectino of feature groups and the generation of the approporiate dataset for achieving the training of PFL model to realize personalized vehicle speed prediction. A new LSTM-based Seq2Seq model is also proposed to better predict the vehicle speed by deploying multi-head attention mechanism."}, {"title": "A. Dataset Collection", "content": "Selecting optimized feature parameters can significantly enhance the predictive performance of the model. Thus, it is crucial to carefully select feature parameters closely related to the speed behavior of the target vehicle. When traditional methods address the driving in urban scenarios, the prediction of target vehicle's speed mainly deals with the preceding vehicle in the same lane, as well as traffic lights. V2V and V2I communications provide additional input from vehicles ahead, while future signal phase and timing (SPaT) information further refine speed predictions [6]. Our model considers a two-lane context and addresses a more realistic multi-lane environment, since vehicles in adjacent lanes (side vehicles) may also influence the speed of the target vehicle, Besides of external information, internal historical control information of the target vehicle (i.e. throttle, brake, and steering wheel angle) directly influences its speed. Furthermore, visual information captured by onboard cameras of the target vehicle, especially processed after semantic segmentation, provides key information reflecting the traffic situation ahead and helping more accurate speed prediction."}, {"title": "B. Multi-Head Attention Augmented Seq2Seq LSTM Model", "content": "As shown in Figure 5, we propose the Multi-Head Attention Augmented Seq2Seq LSTM Model for vehicle speed prediction, which forecasts the future speed over a period based on a segment of historical feature data. The feature groups are organized in chronological order into an input sequence X_{k\u2212M+1}, \u2026, X_k, with the future speed of the target vehicle as the output sequence v_{T,k+1},..., v_{T,k+H}, where k represents the current discrete time index, M denotes the past time range, and H represents the forecasting horizon. The Seq2Seq architecture, originated from the domain of machine translation [31], has been proven effective in various sequence modeling tasks due to its encoder-decoder structure. Our model further incorporates Long Short-Term Memory (LSTM) units that are capable of capturing temporal dependencies, addressing challenges posed by the sequential nature of the vehicle speed prediction task.\nThe integration of the Multi-Head Attention mechanism, inspired by the transformer model architecture [32], enables the model to capture complex temporal dynamics and long-distance dependencies within sequences. Specifically, the model processes the input sequence through a two-layer LSTM encoder, extracting a high-dimensional representation of temporal data. This is followed by a custom Multi-Head Attention module, which captures dependencies within the sequence across multiple subspaces in parallel. The output from this attention mechanism is then fed into a two-layer LSTM decoder, which incrementally constructs predictions for future vehicle speeds. Finally, a fully connected layer maps the LSTM output to the predicted speed. Our model combines LSTM capability for processing sequential data and multi-head attention mechanism advantage in capturing long-term dependencies, aiming to enhance speed prediction accuracy."}, {"title": "IV. METHODOLOGY", "content": "In this section, we first provide an overview of the proposed FedPAW framework, then state the objectives of FL optimization,"}, {"title": "B. Problem Statement", "content": "In the FL process, multiple target vehicles act as clients, with their learning tasks coordinated by a central server. Suppose there are N clients, each possessing their own private training datasets D_1, ..., D_N, and these datasets are Non-IID. Under the coordination of the central server, the overall goal is to collaboratively learn independent local models \u00d4_1, \u2026\u2026\u2026, \u00d4_N for each client without uploading their private data. The aim is to minimize the global loss function and obtain reasonable local models:"}, {"title": null, "content": "{\u00d4_1, ..., \u00d4_N} = arg min_{\u0398} G(L_1, ..., L_N) \\tag{1}"}, {"title": null, "content": "where L_i = L(\u00d4_i, D_i), di \u2208 [1,N], and L (\u00b7) is the loss function, representing the mean squared error (MSE) between the predicted vehicle speeds and the actual values. Typically, G() is set to \\sum_{i=1}^N k_iL_i, where k_i = \\frac{|D_i|}{\\sum_{j=1}^N |D_j|} measuring the contribution of client i to the dataset, and D_i is the number of local data samples of client i."}, {"title": "C. Federated Learning with Personalized Aggregation Weights (FedPAW)", "content": "In traditional FL such as FedAvg, at iteration t, the server selects a subset of N clients S_t for training, and aggregates all local models O_i^t, i \u2208 S_t to form the global model \u0398^t. It is obtained as follows:"}, {"title": null, "content": "\u0398^{t} \u2190 \\sum_{i \u2208 S_t}  \\frac{k_i}{\\sum_{j \u2208 S_t}k_j} O_i^t \\tag{2}"}, {"title": "t", "content": "At iteration t+1, after the server sends the global model \u0398^{t} to client i, O_i^t overrides the previous local model \u00d4_i, resulting in the local model for this round O_i^{t+1} for local training, i.e., O_i^{t+1} := \u0398^{t}. In FedPAW, the server element-wisely aggregates the global model and the client local models, then sends the aggregated model O_i^{t+1}, not \u0398^t, to client i. Formally:"}, {"title": null, "content": "O_i^{t+1} := \u0398^{t} \u2299 W^t_1 + \u00d4_i \u2299 W^t_2 \\\\ s.t. w^t_1 + w^t_2 = 1, \u2200 valid q. \\tag{3}"}, {"title": "t", "content": "Where \u2299 is the Hadamard product, representing element-wise multiplication of two matrices, w^t_1 and w^t_2 are the q-th parameters of aggregation weights W^t_1 and W^t_2, respectively. FedAvg is a special case of the proposed FedPAW, where \u2200 valid q, w^t_1 = 1 and w^t_2 = 0.\nThe two weights W^t and Wh, with the above constraint, can be further simplified in form. We reformulate Eq. (3) by merging W^t_1 and W^t_2 into:"}, {"title": null, "content": "O_i^{t+1} := \u0398^{t} + (\u00d4_i - \u0398^{t}) \u2299 W^t \\\\ where w \u2208 [0, 1], \u2200w \u2208 W^t. \\tag{4}"}, {"title": "p", "content": "Since lower layers of DNN learn more general information than higher layers [33], clients could obtain most general information from the lower layers of the global model [15]. To reduce computational overhead and improve model performance, a hyperparameter p is introduced to control the scope of PA, which is applied to the top p layers, and the global model Ot is used to override the remained lower layers just like FedAvg:"}, {"title": null, "content": "O_i^{t+1} := \u0398^{t} + (\u00d4_i - \u0398^{t}) \u2299 [0_{L(\u0398_i)-P}; W^t_p] \\tag{5}"}, {"title": "t", "content": "Where L(O_i) is the layer count of e, and [0_{L(\u0398_i)-P}; W^t_p] matches the shape of the lower layers of, with elements as zeros. The aggregation weights W^t_p match the shape of the top p layers."}, {"title": null, "content": "Mp = \\sum_{i \u2208 S_t}  \\frac{k_i}{\\sum_{j \u2208 S_t k_j}} (\u00d4^P_i - \u0398^{t,P}) (\u00d4^P_i - \u0398^{t,P}) \\tag{6}"}, {"title": "t", "content": "Each layer of M^{top}, denoted by M^{t,p_l}, is normalized to derive the aggregation weights W^t_p, ensuring that w \u2208 [0,1], \u2200w \u2208 W^t_p. Formally:"}, {"title": null, "content": "W^{t,p} \u2190 \\frac{M^{t,p_l} \u2013 min(M^{t,p_i})} {max(M^{t,p_i}) - min(M^{t,p_i})}, \u2200l \u2208 [1,p] \\tag{7}"}, {"title": "V. EXPERIMENTS", "content": "In this section, extensive experiments are conducted to assess the performance of the FedPAW framework. Initially, we explore the impact of different feature groups (FG1 to FG7) on the prediction model. The learning methods are centralized training based on cloud servers (Cloud) and local training (Local). Additionally, the comparisons include the physics-based Constant Velocity (CV) and Constant Acceleration"}, {"title": "B. Impact of Feature Groups", "content": "The selection of feature parameters is crucial for the performance of the prediction model. Table III displays the average MAE and RMSE of the proposed prediction model across seven feature groups (FG1 to FG7) over different prediction horizons (5 s and 10 s), with Cloud and Local learning methods being utilized. For a 5 s prediction horizon, the performance of FG3 is superior to that of FG1, and FG4 shows a slightly better performance than FG1 for Cloud, while FG2 is inferior to FG1. This indicates that the additional features, namely historical control information, have a positive contribution to the performance of the prediction model, TEPI contributes marginally, and side vehicle information has a negative impact, possibly due to the infrequent lane changes of vehicles in the Carla simulator. For a 10 s prediction horizon, as the prediction horizon increases, the reference value of historical feature information decreases, leading to a diminished performance of FG3 and FG4 compared to the earlier. FG5 to FG7 are combinations of these additional features, and for both 5 s and 10 s prediction horizons, the best-performing feature group is FG6. Therefore, we have chosen FG6 (i.e. the union of FG1, the control information, and the traffic element proportions) for subsequent experiments. Compared to Table V, it is evident that the predictive performance of the proposed model significantly surpasses that of CV, CA, and traditional LSTM models."}, {"title": "C. Impact of Hyperparameters", "content": "The impact of the hyperparameters r and p in the FedPAW framework, as shown in Table IV, is evaluated with a prediction horizon of 5 s. For the hyperparameter r, reducing r allows for personalized aggregation to be performed in earlier iteration rounds, thus exhibiting the best performance at r = 1. Generally, for computationally complex tasks, ar with very low value would prevent sufficient rounds of FedAvg from occurring in advance, resulting in client local models not converging adequately. This could hinder the differentiation of parameters containing more personalized information. However, given that our task is computationally simpler and the models converge quickly, this effect is insignificant. We set r = 1 for prediction horizons of both 5 s and 10 s. As for the hyperparameter p, decreasing p reduces the number of layers involved in PA, making effective use of the general information from the lower layers of the global model, thereby reducing"}, {"title": "D. Performance Comparison and Analysis", "content": "Table VI shows the prediction errors of FedPAW and all benchmark baselines at prediction horizons of 5 s and 10 s, demonstrating that FedPAW performs better than all baselines at 10 s, with a 0.8% reduction in test MAE and a 0.9% reduction in test RMSE (when p = 1), and is only slightly overtaken by FedRep at 5 s. However, FedRep incurs a greater computational overhead than our FedPAW for clients. Due to non-IID data, the generalization ability of the global model is compromised, resulting in poor performance of both FedAvg and FedProx. Likewise, although Cloud has access to the most training data, the non-IID data makes it difficult for the model to converge sufficiently, and its performance is not the best due to the lack of personalization. Local performs worse than Cloud because it only trains on limited local data; although it adapts to the local data distribution of the client, it cannot utilize the knowledge learned by other clients.\nIn PFL methods, Per-FedAvg reduces prediction errors through local fine-tuning, but fine-tuning focuses solely on local data and fails to acquire general information. FedRep outperforms, especially at the 5 s prediction horizon, because FedRep fine-tunes the head in each iteration while freezing the lower layers of the downloaded model, preserving most of the general information. However, the issue with personalized heads is that they also contain general information,"}, {"title": "E. Stability", "content": "In real driving scenarios, partial of the vehicles may not continuously participate in the entire FL process due to reasons such as insufficient remained energy, lack of computing and storage resources, and unstable network conditions. We simulate this scenario by varying joining ratio p of the client in each iteration. Specifically, we uniformly sample a value of p within a given range in each iteration. Table VI shows the prediction errors of FedPAW and all baselines under the extreme dynamic scenario of p\u2208 [0.1, 1]. Compared to p = 1, it is evident that the stability of almost all methods decreases (as indicated by an increase in standard deviation). FedPAW still maintains its stability in this extreme dynamic scenario, with only a slight increase in standard deviation."}, {"title": "F. Computation and Communication Overhead", "content": "As shown in Table VII, the experiment recorded the total computational time cost before convergence (determined by early stopping) for each FL method, comparing the computational overhead. The computation time per iteration for FedPAW is relatively low, only slightly higher than that of FedAvg. This is because FedPAW only adds the computational cost of the PA step on top of FedAvg, and the computation of aggregation weights is fast and does not require gradient descent. Additionally, PA is executed on the server, not imposing extra computational overhead on clients. Compared with FedPAW, benchmarks are beaten for numerous reasons. Per-FedAvg requires local model fine-tuning, and FedRep involves fine-tuning the model head, necessitating additional time. Learning personalized models in pFedMe involves extra training steps, leading to the highest computational cost per iteration, with Ditto facing a similar situation. APFL increases the computational overhead on clients due to freezing the global and local models to learn mixing parameters via gradient descent, and FedALA does the same for learning aggregation weights. FedFomo requires feeding data forward through downloaded client models to obtain approximate aggregate weights, which takes additional time. Additionally, the total computational time for FedPAW is also relatively low, indicating that fewer iterations are required for its convergence."}, {"title": "G. Speed Prediction Results", "content": "Figure 9 shows the speed prediction results of the FedPAW framework on a subset of the test dataset for one client, evaluated every second at a prediction horizon of 5 s. It is evident that the predictive model within the FedPAW framework performs well in urban scenarios characterized by frequent starts and stops of vehicles, demonstrating strong generalization capabilities. To further analyze how the predictive model generalizes the movement of target vehicles interacting with preceding vehicles and traffic lights, Figure 10 presents a 50 s example sequence and compares the speed prediction results of five representative samples among actual speed, CV model, CA model, FedAvg, and FedPAW. The predictive effectiveness of the FedAvg and FedPAW using the predictive model is much stronger than that of the CV and CA models, demonstrating the effectiveness of the proposed mechanism. Specifically, in the first sample, with no traffic lights ahead and the target vehicle approaching a preceding vehicle, there is a slight deceleration; the predictive model infers this behavior through V2V communication, with FedPAW performing better. In the second sample, with a red traffic light ahead at a close distance, vehicles queuing, and the preceding vehicle changing lanes, the predictive model still performs well in this complex situation. In the third sample, with a traffic light changing from red to green in the next second, the predictive model accurately predicts the starting behavior based on V2I communication, with FedPAW performing slightly better. In the fourth sample, with no obstacles ahead but the target vehicle on a curve turning, the prediction model infers a slower acceleration based on the steering wheel angle feature, with FedPAW performing better. In the fifth sample, with a red traffic light ahead and no preceding vehicle, the target vehicle decelerates to a stop normally, and the predictive model predicts accurately, with FedPAW outperforming. Overall, the predictive performance of FedPAW is mostly superior to that of FedAvg, demonstrating the advantages of personalization in vehicle speed prediction."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose FedPAW, a federated learning framework that performs personalized aggregation on the server to capture client-specific information for privacy-preserving personalized vehicle speed prediction, without imposing additional computational and communication overhead on clients. We introduce a new prediction model and have established a simulated driving dataset, CarlaVSP, through the CARLA simulator that distinguishes between different drivers and vehicle types. We demonstrate the efficacy of the FedPAW framework through extensive experiments on the CarlaVSP dataset, where FedPAW outperforms the eleven benchmark baselines in a comprehensive evaluation of prediction performance, stability, computational and communication overhead."}]}