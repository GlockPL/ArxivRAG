{"title": "Personalized Video Summarization by Multimodal Video Understanding", "authors": ["Brian Chen", "Xiangyuan Zhao", "Yingnan Zhu"], "abstract": "Video summarization techniques have been proven to improve the overall user experience when it comes to accessing and comprehending video content. If the user's preference is known, video summarization can identify significant information or relevant content from an input video, aiding them in obtaining the necessary information or determining their interest in watching the original video. Adapting video summarization to various types of video and user preferences requires significant training data and expensive human labeling. To facilitate such research, we proposed a new benchmark for video summarization that captures various user preferences. Also, we present a pipeline called Video Summarization with Language (VSL) for user-preferred video summarization that is based on pre-trained visual language models (VLMs) to avoid the need to train a video summarization system on a large training dataset. The pipeline takes both video and closed captioning as input and performs semantic analysis at the scene level by converting video frames into text. Subsequently, the user's genre preference was used as the basis for selecting the pertinent textual scenes. The experimental results demonstrate that our proposed pipeline outperforms current state-of-the-art unsupervised video summarization models. We show that our method is more adaptable across different datasets compared to supervised query-based video summarization models. In the end, the runtime analysis demonstrates that our pipeline is more suitable for practical use when scaling up the number of user preferences and videos.", "sections": [{"title": "1 INTRODUCTION", "content": "The availability of online content, such as news articles, live broadcasts, and video blogs, has led to a demand for video summarization in different practical scenarios. The growing interest in multimodal learning has focused on the development of personalized video summaries using natural language queries [10]. Unlike conventional video summarization methods [2] that solely rely on video content to capture repetitive scenes as highlights, query-guided video summarization [12] incorporates information from natural language queries to produce concise video summaries. This approach provides users with condensed information, which is particularly useful for lengthy videos such as live streams and product reviews, where redundant content is often present.\nExisting methods for query-guided video summarization [10] have utilized text information but can only generate summaries based on descriptive sentences, such as \"A shark is swimming under water,\" rather than more abstract concepts like genres, such as \"Sci-Fi,\" \"Romance,\" or \"Comedy.\" However, genre tags can be easily obtained from recommendation systems and can represent user preferences when recommending videos. Unfortunately, current query-guided models struggle to understand genre-specific tags due to limitations in the training data, which was only trained with paired natural language sentences and lacks a deeper understanding of videos. Recently, there have been studies exploring user-preferred video summarization [10, 12, 17], which aim to generate summaries based on specific user applications. However, genre-specific user preferences have not been thoroughly explored. In this work, our goal is to generate video summaries that are conditioned on user preferences, resulting in user-centric video summarization.\nIn order to facilitate research on long-video summarization with user-preferred information, we have collected a large-scale video dataset called USERPREFSUM. This dataset consists of over 1K movie videos from Condensed Movies [4], covering a diverse range of genres (21 classes in total). For genre-based video summarization, users can provide one or multiple preferred movie genres as a query. We start by detecting scenes in the videos and then use the zero-shot ability of CLIP [21] to automatically label the genre of each scene. The final video summarization contains multiple scenes that are related to the given genre. This approach allows for a more realistic and user-centered evaluation of video summarization methods. This is motivated by the fact that video summarization can be quite subjective. Instead, we are offering a more unbiased approach to assess such situations by representing user preference by genres. This is based on our hypothesis on the Content-Based Recommendation System [20], the underlying principle of this type of recommendation system being that if a user enjoyed a certain movie or show, they may also enjoy something similar.\nMotivated by the observations mentioned above, we propose a Video Summarization with Language (VSL) approach, as depicted in Figure 1. VSL is based on the Socratic model [29] and consists of four components: multimodal scene detection, video captioning, multimodal summarization, and video selection. The multimodal scene detection component utilizes both video and closed captioning as inputs to identify clear scene cuts that preserve the integrity of both the video and dialogue in the input movie. Video captioning converts the input movie from the video domains to a text-based captioning domain for semantic analysis. The multimodal summarization component generates summaries for both video captioning and closed captioning, capturing the most important information at the scene level. The video selection component analyzes the summarization results from the multimodal summarization component, taking into account the input genre, in order to select appropriate scenes and create the final summary video that aligns with the specific user preferences. Experimental results demonstrate that VSL outperforms current state-of-the-art methods in both general video summarization (TVSum[24]) and user-specific video summarization (USERPREFSUM). We also test summarization in user-generated videos SumMe[7]. Additionally, we conduct runtime analysis on varying numbers of videos and user preferences to showcase the practical applicability of the VSL model.\nTo summarize, our contributions include:\n\u2022 A new benchmark is proposed to facilitate the research of user-preferred video summarization.\n\u2022 A language-based architecture VSL makes use of pre-trained large language models for video captioning and multimodal summarization. This allows the architecture to analyze natural language at a semantic level, which takes into account multimodal input and genre recommendation, an aspect overlooked by previous studies.\n\u2022 A runtime analysis has been conducted to demonstrate the scalability of our model. Unlike the previous state-of-the-art method that requires feeding user preference queries to each video individually, our VSL can generate summarization in parallel. This real-time capability is crucial for practical applications."}, {"title": "2 RELATED WORK", "content": "2.1 General video summarization\nDatasets. General video summarization, also known as Query-agnostic summarization, involves generating a concise version of a given video without any user query. Datasets such as those mentioned in [7, 9, 16, 24] are based solely on visual cues.\nMethod. The advances in unsupervised learning have demonstrated promising potential in general video summarization. A notable example is the CA-SUM [2] approach, which incorporates length regularization loss and concentrated attention to achieve high F-scores on both the TVSum[24] and SumMe[7] datasets, out-performing GAN-based models[1]. Unlike supervised query-guided video summarization, CA-SUM is an unsupervised model that also offers the possibility of zero-shot capability. However, a limitation of CA-SUM is its reliance on a pre-defined attention mechanism, which restricts its ability to perform semantic analysis on complex video content, particularly in the case of movies.\n2.2 User specific video summarization.\nDatasets. The primary objective of query-guided video summarization is to allow users to customize the summary by specifying specific text keywords (e.g., \"A man riding a horse\"). This practical approach is the main focus of our research. Another relevant task, called Moment retrieval, involves locating specific moments in a video using a natural language query. Several datasets [18, 23, 26] have been proposed or modified for this purpose. While some of these datasets only provide one moment for each query-video pair, others offer multiple moments. Unlike previous datasets that primarily focus on retrieving relative moments using natural language queries, our focus is on retrieving moments/scenes based on user preferences. To construct such a user preference for evaluation, we utilize genre tags to represent the user's preference and retrieve multiple moments for our final video summarization.\nMethod. Several approaches [8, 19, 25, 27, 28] have been developed to tackle video summarization/moment retrieval tasks. Some of these approaches focus on scoring generated moment proposals, predicting moment start-end indices, or regressing moment coordinates. However, these approaches require manual preprocessing steps (e.g., proposal generation) or postprocessing steps (e.g., non-maximum suppression) that are not trainable end-to-end. Another line of work, represented by DETR-based methods [10, 12, 17], treats video summarization/moment retrieval as a direct set prediction problem. These methods take video and user query representations as inputs and directly output moment coordinates and saliency scores. However, existing approaches often generate summaries using natural language sentences, which can be costly in terms of describing user preferences. Furthermore, as the number of users increases, the user preference information can become extensive. To address these challenges, we propose a method called Video Summarization with Language (VSL), which leverages a video-to-text model to process all information in the text space. Our method demonstrates the ability to handle diverse user preferences without increasing the computation time. Also, due to the design nature of our model, it can easily incorporate other user-preference information besides genre."}, {"title": "3 DATASET", "content": "To address the task of user-preferred video summarization, we have developed a labeled dataset called USERPREFSUM, which is designed specifically for movie summarization. In the following sections, we will describe the procedure for automatic labeling using CLIP [21] and the approach for filtering the labels. The process of creating the data is illustrated in Figure 2. Our dataset is on top of the public dataset Condensed Movies [4]. We will release the code and dataset annotation for reproducibility.\n3.1 Scene segmentation for summarization\nThe process of video summarization involves dividing the video into multiple scenes, each of which has a similar semantic meaning. In this context, we assume that scenes sharing the same semantic similarity belong to the same movie genre. To achieve this, we follow the approach proposed by Bose et al. [5] and utilize PySceneDetect\u00b9 to segment the movie clips in the Condensed Movies [4] into multiple scenes.\n3.2 CLIP-based movie genre labeling\nIn this section, we describe how CLIP [21] is used to associate genre labels with specific segments of movie scenes from the previous section, as illustrated in Figure 2(a). To alleviate the burden of manually annotating long videos in USERPREFSUM, we leverage the zero-shot capabilities of CLIP, following the approach proposed in [5], to tag the scenes. A notable difference is that we assign movie\ngenres as labels to the scenes, rather than background scene labels. The genre labels are obtained from the Moviescope dataset [6], which is a subset of the Condensed Movies [4] genre. This dataset comprises 21 movie genre types, listed in Table 1. By employing the CLIP prompt engineering technique, we use a specific prompt for each movie genre, such as \"A photo of a label, a type of movie genre.\" This prompt allows us to assign labels from our movie genre list to individual frames or scenes in video clips. Given a movie clip, we generate multiple scene segments, denoted as s\u1d62, following the approach described in the previous section, where (i = 1, ..., N). Let's assume that scene s\u1d62 contains T frames. We utilize CLIP's visual encoder to extract frame-wise visual embeddings u\u209c, where (t = 1, ..., T). For each movie genre, we employ CLIP's text encoder to extract embeddings g\u2081, where (l = 1, 2, ..., L), corresponding to the genre-specific prompts. To compute the similarity score matrix M, whose entries M\u2097\u209c represent the similarity between the genre-specific prompt g\u2081 and the frame-wise visual embedding u\u209c, we use the following equation:\n$M_{lt} = \\frac{g_l^T u_t}{||g_l|| ||u_t||}$ (1)\nTo obtain an aggregate score for each scene label, we perform a temporal average pooling over the similarity matrix M\u2097\u209c following [15] to capture temporal information. This provides us with the genre score G\u2081 for a given scene, as the visual content within a scene tends to remain relatively consistent. Here, G\u2081 represents the distribution of confidence scores for each genre assigned to a scene. Finally, we select the top 5 genre labels, as shown in Figure 2(a)."}, {"title": "3.3 Multi-genre label creation", "content": "In order to accommodate users with multiple genre preferences, we aggregate the G\u2081 values for different genres and apply the same filtering strategy described in the previous sections. This approach will result in a video summarization that is 15% of the original length (following [24]) when the user provides 2 or 3 genre queries, as depicted in Figure 2(e). The selected scenes will be the ground truth label of the video summarization. The idea behind this is to create a final video summary that includes a combination of different genres. We evaluate the performance of this approach in the ablation study described in Section 5.5."}, {"title": "3.4 Dataset analysis", "content": "We gathered a total of 1800 queries related to 9000 moments across 1008 videos. Unlike other moment retrieval datasets, USERPREF-Sum can have multiple separate moments paired with a single query. On average, there are 9 moments per query in a video. This is in contrast to previous datasets where there are fewer than two moments per query. One key difference is that our queries represent a single-word high-level semantic genre, while previous studies focused on using natural language queries to retrieve more specific video segments. To evaluate the quality of the automatic labeling process, a human evaluation was conducted by randomly sampling 10% of the annotations. The Inter-Annotator Agreement (IAA) [3] was 74.3%, approaching the accuracy of human annotation."}, {"title": "4 METHOD", "content": "In this section, we present the VSL architecture as in Figure 3. The goal of VSL is to identify the most significant scenes in a given movie based on the input genres. VSL offers two modes for video summarization: general video summarization and user-specific video summarization. In general mode, the input genres are extracted from the movie metadata, representing its inherent genres. In the user-specific mode, the input genres are obtained from an upstream recommendation system, reflecting the user's preferences. To execute the VSL architecture, the first step involves dividing the input movie into separate video and audio backbones. Next, a scene detection process is performed to convert the input movie into a sequential scene set {s\u2081, ..., s\u1d62}, where i ranges from 1 to N. Subsequently, semantic analysis is applied to all scenes according to input genres, resulting in a list of saliency scores {M\u2081, ..., M\u1d62}. Finally, the process of generating a summary video involves selecting scenes with the highest saliency scores. Compared to previous approaches, VSL improves the scene detection algorithm and offers capabilities in semantic analysis. Further details will be explained in the following sections.\n4.1 Multimodal scene detection\nThe objective of scene detection is to identify frames in a video where significant semantic changes occur. Previous studies, such as MovieCLIPs [5], have typically employed video-based algorithms for scene detection. However, this approach often results in cutting the video in the middle of a dialogue, which can create a confusing summary video for users. To address this problem, we propose a multimodal scene detection approach in our architecture. The multimodal scene detection method incorporates both video and audio signals as inputs, ensuring that the generated scenes maintain the integrity of both the video and the dialogue.\n4.2 Semantic analysis\nSemantic analysis is a crucial component in both unsupervised and supervised video summarization, as it greatly impacts the quality of the summary video. However, existing algorithms often overlook an important characteristic that is essential for all video summarization systems: the ability to handle unseen videos, also known as zero-shot capability. This means that the system should be able to perform well on videos that were not included in the training data, as this ultimately determines the system's quality. Taking inspiration from recent research demonstrating the generalization ability of large language models, we incorporate such a capability into our VSL architecture by employing semantic analysis at the natural language level.\nFigure 3(b) illustrates the initial step in VSL's semantic analysis, which involves video captioning. In this phase, the video backbone of the input movie is first subsampled and then passed through a pre-trained BLIP model [11] to generate captions for each subsampled frame. Following the video captioning stage, a multimodal summarization module is used. This module performs semantic summarization on the scene level for both video captioning and closed captioning, eliminating irrelevant information and enhancing the summarization performance. The final component of the semantic analysis is the scene scoring module, which uses a pre-trained T5 model [22] and cosine similarity. The T5 model generates embeddings for both the input genres and the outputs of the multimodal summarization module. If there are L genres in the input genres, the score for scene i of the input movie can be calculated using the following equation:\n$M_i = \\sum_{l=0}^L (\\frac{f_{uc}^T f_{g_l}}{||f_{uc}|| ||f_{g_l}||} +  \\frac{f_{cc}^T f_{g_l}}{||f_{cc}|| ||f_{g_l}||})$ (2)\nwhere fuc is T5 feature for the scene's video captioning summarization, foc is T5 feature for its closed captioning summarization.\n4.3 Video summarization frame selection\nThe objective of video summarization is to select appropriate scenes from a list of scene scores {M\u2081, ..., M\u1d62} and include them in the final summary video. The length of the summary video can be adjusted based on user-specific settings (e.g., 10%, 15%, 25% of the original movie). To handle different input videos, we offer two algorithms. For short movies (less than 5 minutes), the knapSack algorithm is employed to ensure diversity in the final summary video by balancing scene score and scene length during scene selection."}, {"title": "5 EXPERIMENT", "content": "5.1 Baselines\n5.1.1 General video summarization. CA-SUM is a deep learning framework that employs a concentrated attention mechanism to evaluate the significance of frames in videos. The approach is unsupervised, eliminating the requirement for supervised data. RS-SUM leverages self-supervised learning and a restorative score. The aim is to produce concise summaries from unannotated videos without the need for manual supervision. AC-SUM-GAN is an approach that combines an actor-critical model with a Generative Adversarial Network (GAN) to tackle the task of selecting important video fragments for creating a summary. This approach frames the selection process as a sequence generation task.\n5.1.2 Query-based video summarization. MomentDETR is the first end-to-end video summarization model that operates using a query-based approach. It adopts a transformer encoder-decoder architecture and treats video summarization as a direct set prediction task. Moreover, the model leverages the text query to assist in the selection of informative video segments, taking into account user preferences. QD-DETR is built on top of MomentDETR and addresses the limited importance of queries in transformer architectures. In QD-DETR, the encoding module begins with cross-attention layers to incorporate the context of the text query into the video representation. UniVTG also adopts the end-to-end transformer architecture of MomentDETR. Research involves pre-training with a wide range of diverse labels obtained from different task-specific sources, which enhances its grounding capabilities."}, {"title": "5.2 Experiment setup", "content": "VSL settings. In our VSL architecture test, we opt for a sample rate of 15 FPS for semantic analysis. The video captioning BLIP model is trained on the COCO dataset [13]. To generate closed captions, we employ the whisper-timestamped model [14] to obtain more accurate timestamp results. During the test, we test out various genre types (21 classes in total), as indicated in Table 1. Additionally, we utilize the video summarization module with video-based scene detection results to enable comparison with our baseline algorithms. All experiments are conducted on Amazon Web Service, employing a single V100 GPU with 16GB of memory.\nBaseline settings. To generate video summarization, we utilize the UniVTG [12] approach to pre-extract CLIP visual and text features from the query-based video summarization baselines. We employ the best checkpoints for each method, which have undergone fine-tuning or additional data pre-training, to test on the USERPREF-Sumdataset. During the inference process, we input either a single genre or multiple genres as the text query to generate the video summarization."}, {"title": "5.3 Evaluation metrics", "content": "To comply with established protocols, we make sure that the summary generated using Keyshot is limited to less than 15% of the original video duration, as stated in previous studies [24]. To evaluate the accuracy of our predictions, we use the definitions of precision (P) and recall (R) as described in [24]. Precision is calculated by dividing the duration of the overlapped prediction and ground-truth by the duration of the prediction, while recall is calculated by dividing the duration of the overlapped prediction and ground-truth by the duration of the ground-truth. The F-score is then calculated using the formula F = (2 x P x R)/(P + R) x 100%. To ensure a reliable assessment, we employ a random selection approach. The F-score is reported for various experiment settings."}, {"title": "5.4 User-preferred video summarization", "content": "As indicated in Table 1, UniVTG achieves the highest performance among DETR-based methods by utilizing the largest training data compared to other methods. Our proposed method outperforms other baselines trained on video summarization datasets such as QV-highlight and TVSum. By combining the captioning model for video understanding with the summarization model, our approach effectively captures the semantic information of videos across multiple genre-related scenes. Importantly, this behavior is learned without the need for additional human supervision or labeled data. In contrast, the current DETR-based method is trained on natural language descriptions as queries, requiring additional fine-tuning to adapt well to video summarization with specific genres."}, {"title": "5.5 Multi-genre video summarization.", "content": "We evaluate the performance with different # of genres as in Table 2. In this setting, a user is interested in multiple genres, such as Sci-Fi and Romance. To obtain annotations for such cases, we follow the label generation process described in Figure 2(e). We aggregate the confidence scores in the N genres and apply the same filtering criteria. In the experiment, we observe that MomentDETR and Uni-VTG experience a significant drop in performance as the number of genres increases (N=3). In contrast, QD-DETR demonstrates consistent performance across different numbers of genres. We attribute this difference to the unique design of QD-DETR, which incorporates additional interaction between video and text. Our proposed method consistently outperforms the current baselines, without requiring video summarization label training. This highlights the effectiveness and generalizability of our approach."}, {"title": "5.6 Evaluation on public benchmarks", "content": "TV shows. We evaluate our approach by comparing it with existing unsupervised methods on the TVSum dataset, as presented in Table 3. To conduct the experiment, we utilize the video genre information from the metadata of the TVSum dataset and set the summary video length ratio to 0.15. The VSL method outperforms the other unsupervised baselines, indicating its effectiveness and versatility on a standard video summarization dataset.\nUser generated videos. We test on a user-generated video dataset SumMe [7], where live videos are generated by users. As in Table 4, we conducted a comparison with other query-based video summarization methods, using the video title as the query. The results showed that our VSL method performed satisfactorily in comparison to other baselines, thereby proving its adaptability to a variety of video types, not limited to movies or TV shows."}, {"title": "5.7 Ablation study", "content": "We've conducted an ablation study on two elements of our model, as shown in Table 5.\nMultimodal summarization. We abate the component depicted in Figure 3 (b) (Multimodal summarization) by substituting it with the caption that appears most frequently in the videos, rather than employing the summarization model. Results in the SumMe dataset show the efficacy of the summarization model, which encompasses a more thorough understanding of the full length of the video.\nLanguage Model (LM) for semantic matching. We tested various language models in Figure 3 (b) (Scene scoring) on our User-PREFSUM dataset. Our findings indicate that basic word embedding (Glove) semantic matching results in satisfactory performance. Moreover, training language models with a greater number of parameters enhances semantic comprehension, which is beneficial for everything from summarization to genre matching."}, {"title": "5.8 Runtime analysis", "content": "Scaling # of videos. In order to test the practical application of video summarization, we assume that the user's preferences have been obtained in advance. Our goal is to generate video summarization in real-time. The runtime of each method is calculated and presented in Figure 5(a). It was observed that QD-DETR requires the longest inference time due to its complex architecture, which includes a specially designed cross-attention mechanism between queries and videos. Unlike DETR-based methods, VSL is able to maintain a similar runtime as the number of input videos increases. This is because our method converts the video into text beforehand, allowing for faster processing of video summarization selection. In other words, we avoid the overhead of video processing to generate video summarization more quickly.\nScaling # of user preferences. In practical scenarios, users may have various combinations of genre preferences. In our specific case, there are 21 different genres available and each user can have a maximum of 3 preferred genres. Among these combinations, we selected 300 combinations of user preferences across 10 videos, as depicted in Figure 5(b). Although the DETR-based method requires feeding individual queries to the model, our model can precompute genre features, ensuring a constant time complexity as the number of user-preference combinations increases."}, {"title": "6 CONCLUSION", "content": "In this study, we have introduced a new dataset and a pipeline for multimodal video summarization. Our approach utilizes pre-trained visual language models and semantic analysis at the scene level, which eliminates the need for extensive training datasets. The experimental results demonstrate the effectiveness of our approach, surpassing existing unsupervised video summarization models, and demonstrating its adaptability across different datasets. This represents a significant advance in making video summarization more accessible, efficient, and customizable to individual user preferences.\nIn the end, our method is not confined to genre; genre is merely one mechanism to encapsulate user preferences, but any other technique that captures user preferences can be utilized. As a future direction, we plan to experiment with new methods of expressing user preferences, such as generating user profiles based on LLM."}]}