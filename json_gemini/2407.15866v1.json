{"title": "SmartQuant: CXL-based AI Model Store in Support of Runtime Configurable Weight Quantization", "authors": ["Rui Xie", "Asad Ul Haq", "Linsen Ma", "Krystal Sun", "Sanchari Sen", "Swagath Venkataramani", "Liu Liu", "Tong Zhang"], "abstract": "Recent studies have revealed that, during the inference on generative AI models such as transformer, the importance of different weights exhibits substantial context-dependent variations. This naturally manifests a promising potential of adaptively configuring weight quantization to improve the generative AI inference efficiency. Although configurable weight quantization can readily leverage the hardware support of variable-precision arithmetics in modern GPU and AI accelerators, little prior research has studied how one could exploit variable weight quantization to proportionally improve the AI model memory access speed and energy efficiency. Motivated by the rapidly maturing CXL ecosystem, this work develops a CXL-based design solution to fill this gap. The key is to allow CXL memory controllers play an active role in supporting and exploiting runtime configurable weight quantization. Using transformer as a representative generative AI model, we carried out experiments that well demonstrate the effectiveness of the proposed design solution.", "sections": [{"title": "I. INTRODUCTION", "content": "With the wide acceptance of the scaling law [1], AI model size will continue to increase at least in the foreseeable future, despite today's AI models already contain over a trillion parameters [2], [3]. To unleash the full potential of large AI models in the real world, it is essential to maximize the inference cost/energy efficiency, for which quantization is one of the most effective means and has been widely studied [4]-[6]. To materialize the benefits of quantization, modern GPU and AI accelerators provide strong built-in hardware support for variable-precision arithmetics. For modern generative AI models such as transformer [7], recent studies show that the importance of different weights exhibits substantial context-dependent variations during inference [8]. This observation opens new opportunities to improve the inference efficiency via adaptive weight quantization configuration. Although runtime weight quantization configuration can leverage the hardware support for variable-precision arithmetics in GPU and AI accelerators, little prior research has studied how one could exploit configurable weight quantization to adaptively reduce the AI model load latency and energy consumption.\nTo fill this missing link, this letter presents a CXL-based AI model store that can gracefully support runtime configurable weight quantization. Built upon the rapidly maturing CXL ecosystem with native support of memory pooling/sharing [9], CXL memory devices are well positioned to play a key role in future computing infrastructure. Since today's CPUs already have a large number of PCIe lanes (e.g., AMD Genoa has 128 PCIe Gen5 lanes), it is reasonable to expect future CPU/GPU and AI accelerators could readily support 150+ PCIe lanes. Upcoming PCIe Gen7 specifies a per-lane bandwidth of 16GB/s, leading to total 4TB/s bandwidth (2TB/s in each direction) with 128 PCIe lanes. In comparison, the upcoming 4th generation HMB (high bandwidth memory) is expected to enable ~1.5TB/s throughput per DRAM stack, and one GPU package may integrate 4~8 HBM DRAM stacks. Hence, CXL memory devices could be a viable cost-effective supplement (or even alternative) to HBM for building future AI systems. This letter advocates for a CXL-based AI model store that can serve generative AI inference with dynamically configurable weight quantization (e.g., FP16, FP8, FP6). Its practical implementation is subject to two major issues: (1) How to materialize the potential of configurable weight quantization on improving the AI model DRAM access energy efficiency and throughput? As the straightforward design option, CXL memory controller reads the full-precision weights from DRAM and truncates/converts them into lower-precision ones, which however will not reduce the model load latency and energy consumption at all. (2) How to simplify the system integration? Inference computing devices should be able to conveniently configure and load AI model weights from CXL memory devices. To address these issues, this letter presents a design solution called SmartQuant that consists of two simple yet effective methods, including bit-plane in-memory placement and memory logical space bloating. Using the open-source transformer model [3] as a test vehicle and DRAM simulator DRAMSim3 [10], we carried out experiments to study the effectiveness of the proposed design solution. In comparison to the straightforward design, the results show that, SmartQuant can reduce the model load latency by up to 42.1% and memory access energy consumption by 40.3%."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Recognizing the paramount importance of quantization in AI infer-ence (and even training), the chip design industry has made GPU and AI accelerators natively support variable-precision arithmetics so that their computational throughput and/or energy efficiency could grace-fully scale with the data quantization precision. For example, latest Nvidia GPU Blackwell supports FP64/32/16/8/6/4 and INT8, and its peak performance is 2.25 PFLOPS (peta FLOPS), 4.5 PFLOPS, and 9 PFLOPS under FP16, FP8, and FP4 quantization, respectively. This capability lays a solid foundation for algorithms/software to navigate the trade-off space between the AI inference/training quality and implementation cost by dynamically adjusting the data quantization.\nAs a topic of great current interest, generative AI models such as transformer [7] have been most successfully demonstrating the power of the scaling law [1]. Leading-edge generative Al models now boast over a trillion parameters, with no indication of slowing down in the continuous growth of model size. This has led to growing attention on improving their training/inference efficiency. Motivated by the observation that the importance of different weights exhibit significant context-dependent variations during inference on generative AI models, recent work proposed to reduce the inference cost by selectively bypassing less important model weights [8]. This can be naturally extended to more flexible and fine-grained configuration of weight quantization in adaptation to their context-dependent importance. This will enable generative AI inference more gracefully explore the quality vs. cost trade-off space. Although it could seamlessly leverage the existing variable-precision arithmetic support of modern GPU and AI accelerators, little prior work has studied how one could exploit weight quantization configuration to improve the model memory load speed and access efficiency."}, {"title": "III. PROPOSED DESIGN", "content": "This section presents a solution that enables future CXL memory devices to effectively support context-dependent fine-grained weight quantization configuration for generative AI inference. First, we note that, to facilitate its practical implementation, weight quantization should be configured with a coarse granularity: A large chunk of closely related model weights (e.g., all the weights associated with the same attention head in a transformer model) are always assigned with the same runtime contextual importance and hence the same quantization precision. As a result, runtime model weight quanti-zation configuration only varies from one large chunk to another. Therefore, inference computing devices fetch model weights from DRAM chunk-by-chunk, where all the weights within each chunk always have the same quantization configuration. Note that different chunks may contain different number of weights. Such chunked quantization configuration not only simplifies the runtime evaluation of contextual weight importance, but also streamlines the process of loading model from memory and facilitates the utilization of variable-precision arithmetic hardware support in modern GPU and AI accelerators.\nLet Q denote the set of supported quantization formats, e.g., Q ={FP16, FP8, FP6, FP4}. The most convenient option is to store multiple copies of the AI model in DRAM, each copy corresponding to one quantization format $q_\\iota \\in Q$, similar to the storage of different-quality video files in support of adaptive bit rate streaming [11]. This however incurs a significant memory capacity usage overhead because of the very large generative AI model size. Intuitively, one alternative is to store only the full-precision AI model in memory and construct reduced-precision model weights via on-the-fly quantization format conversion. The controller chip inside each CXL memory device can naturally serve this format-conversion task, as illustrated in Fig. 1. When fetching each chunk of model weights, inference com-puting devices specifies the intended quantization format $q_i \\in Q$, and CXL memory devices internally on-the-fly construct the $q_\\iota$-quantized weights from the stored full-precision version. When implementing such CXL-based Al model store with built-in quantization format conversion capability, we should aim at achieving two goals:\n1) Proportional DRAM access efficiency: Let $\\alpha_x > 1$ denote the average weight precision reduction ratio (i.e., the ratio between the total number of bits in the full-precision model and dynamically quantized model). Ideally, the DRAM access efficiency should improve by $\\alpha_x$, i.e., both the AI model load energy consumption and latency reduces by $\\alpha_x$.\n2) Seamless system integration: The host should be able to con-veniently convey the intended quantization format information to CXL memory devices while being 100%-compliant to the CXL.mem protocol. To ensure the practical feasibility of the proposed design solution, we should restrain from adding any new command CXL.mem protocol commands.\nTo accomplish these two goals, we develop two simple yet effective techniques that will be described in the remainder of this section."}, {"title": "A. Bit-Plane In-Memory Placement", "content": "In conventional practice, all the bits of each numeric value are always stored contiguously together in memory. This however will prevent CXL memory devices from achieving proportional DRAM access efficiency in support of configurable weight quantization: Assume we store each full-precision weight as a whole in DRAM. Due to the per-page DRAM cell activation and DRAM access burst length, regardless of the target precision, CXL memory controller must always fetch full-precision weights from DRAM to realize the quantization format conversion. As a result, CXL memory devices always have a fixed quantization-independent, other than the desired quantization-proportional, DRAM access efficiency.\nTo address this issue, the only viable option is to bit-wise disag-gregate the in-memory storage of each full-precision weight, similar to a bite-wise column-store. Let $N_1$ denote the number of bits in full-precision weight quantization. We store all the model weights in $N_1$ bit-planes, each bit-plane corresponding to one bit position in the $N_1$-bit full-precision quantization. All the $N_1$ bit-planes are stored in DRAM independetly from each other, as illustrated in Fig. 2. Through such bit-plane in-memory placement, CXL memory controller can selectively fetch a just-enough portion of each full-precision weight from DRAM to realize quantization format conversion. Suppose the full-precision quantization contains 1-bit sign, $n_e$-bit exponent, and $n_m$-bit mantissa (i.e., $N_1 = 1 + n_e + n_m$). Assume inference computing devices request a chunk of weights under a reduced-precision quantization with 1-bit sign, $r_e$-bit exponent, and $r_m$-bit mantissa. We could perform quantization format conversion by fetching data from $1 + (r_e + \\delta_e) + (r_m + \\delta_m)$ in-memory bit-planes, where $\\delta_e, \\delta_m \\ge 0$. When setting $\\delta_e = \\delta_m = 0$, we obtain the reduced-precision model weights by directly truncating the full-precision model weights. When setting $\\delta_e$ and/or $\\delta_m$ as 1 or 2, we can improve the quantization format conversion accuracy by rounding-off the full-precision model weights. Utilizing such bit-plane in-memory weight placement, we could proportionally reduce the amount of DRAM page activation and data transfer, leading to the proportional DRAM access efficiency."}, {"title": "B. Memory Logical Space Bloating", "content": "To facilitate host inference computing devices fetch model weight chunks with configurable quantization, CXL memory devices could expose a bloated logical memory space: Let $L$ denote the total num-ber of model weights, $s$ denote the number of different quantization formats, and $N_i$ denote the number of bits in the $i$-th quantization format ($N_1$ corresponds to the full-precision quantization). CXL memory devices exposes a logical space consisting of $s$ regions, where each region $P_i$ corresponds to the $i$-th quantization format and has the capacity of $L \\cdot N_i$ bits. As illustrated in Fig. 3, each"}, {"title": "IV. EVALUATION", "content": "Using the open-source transformer model OPT [3], we first studied the effect of configurable weight quantization on the inference quality. All the experiments were done on a server with 8 Nvidia L40 48GB GPUs. Following the recent work [8], we use a small-size offline-trained neural network as a predictor on each layer to estimate the relative importance of each attention head and neuron. Taking the activation from the previous layer as input, each predictor produces an importance score in the range of [0,1] for each attention head and each neuron, where 0 and 1 correspond to the least and highest importance. Given the total $s$ quantization formats, the system can adjust a set of $s - 1$ thresholds {t1, t2,...,ts-1} to realize the runtime configurable weight quantization.\nWe considered three different scenarios: (i) Baseline: All the OPT model weights use the full-precision quantization of FP16; (ii) Uni-form: All the OPT weights use the same quantization precision; (iii) Non-uniform: Given the target average number of bits per weight, the system dynamically configure weight quantization based on the estimated importance score. In this work, we set s = 6 with the quantization formats of FP16, FP12, FP8, FP6, FP4, and FPO (i.e., skipped weight). We use three different OPT models including OPT 1.3b, OPT 13b, and OPT 30b, and evaluate the inference perplexity using the language modeling datasets C4 [12] and WikiText [13].\nFig. 4 shows the measured perplexity under different average number of bits per weight (note that the baseline always use the full-precision FP16 on all the weights). The results show that, under the same average number of bits per weight, non-uniform dynamic quantization consistently achieves lower (i.e., better) perplexities than uniform quantization. This can be intuitively justified by the significant context-dependent weight importance variation observed in prior work [8]. Moreover, the results show that non-uniform dynamic quantization can be more effective as AI models become bigger. For example, under the C4 dataset, the perplexity gap between non-uniform with average of 8bits/weight and baseline is 13.0% (i.e.,"}, {"title": null, "content": "17.4 vs. 15.4), while the gap reduces to 11.6% in the OPT 30b model (i.e., 11.5 vs. 10.3)."}, {"title": "B. DRAM Access Efficiency", "content": "Using the open-source DRAM simulator DRAMSim3 [10], we further evaluated the DRAM access efficiency of configurable weight quantization. We set that each CXL memory module contains 4 DRAM channels, each channel hosting 10 \u00d74 DDR5-4800 devices. We collected the memory access traces during the inference of OPT 30b model on the WikiText dataset, which was fed into the DRAM simulator. We studied two different scenarios: (i) Traditional: As the straightforward design option, the full-precision AI model are stored in DRAM weight-by-weight. To support dynamic weight quantization, the CXL memory controller reads full-precision weights from DRAM and then converts them into lower-precision formats; (ii) SmartQuant: As described in Sec. III-A, this approach utilizes bit-plane placement to minimize quantization overhead, enabling the CXL memory controller to read only the required bits from the full-precision weights in DRAM. All the predictors' weights are always fetched in the full-precision FP16 format. Fig. 5(a) shows six different quantization formats and predictor distribution in entire OPT 30b model. As the average number of bits per weight increases from 1.6 to 8.0, the proportion of predictors decreases from 15.2% to 3.7%. Fig 5(b) and (c) shows the average percentages of the quantization formats across attention and MLP layers under different target average number of bits per weight."}, {"title": "1) DRAM Access Energy", "content": "Fig. 6(a) shows the total DRAM access energy when loading the entire AI model once, with the energy consumption breakdown among attention layers, MLP layers, and predictors. Compared to the traditional approach, SmartQuant reduces the DRAM access energy"}, {"title": null, "content": "by up to 40.3%. Fig. 6(b) and (c) shows the average per-weight DRAM access energy consumption for reading weights in an attention head and an MLP neuron of the OPT 30b model. We treat all weights associated with the same attention head or MLP neuron as a single chunk and hence assign the same quantization format. In the OPT 30b model, this corresponds to 3.7 \u00d7 10\u00ba weights in an attention head and 7.2 \u00d7 103 weights in an MLP neuron. Both the traditional and SmartQuant approaches directly skip all the chunks with FPO and fetch the same amount of data from DRAM for chunks with the full-precision FP16 quantization. For chunks with reduced-precision quantization formats, SmartQuant fetches less amount of data from DRAM than the traditional approach, leading to energy and latency reduction. Fig. 6 clearly shows the DRAM access energy saving of SmartQuant. For attention heads, under target bits/weight of 1.6, 4.8, and 8.0, the per-weight DRAM access energy is 49.6pJ, 118.9pJ, and 238.9pJ when using the traditional approach, which reduce to 34.5pJ, 70.8pJ, and 141.2pJ under SmartQuant, representing 30.5%, 40.4%, and 40.9% reduction, respectively. Similarly, for MLP layers, under target bits/weight of 1.6, 4.8, and 8.0, compared with the traditional approach, SmartQuant could reduce the per-weight DRAM access energy by 19.4%, 20.3%, and 33.9%, respectively."}, {"title": "2) Model Load Latency", "content": "We further measured the AI model load latency when using different design approaches. Fig. 7 shows the average load latency of each attention head and each neuron in OPT 30b model. The results clearly show the effectiveness of the bit-lane in-memory placement on reducing the model load latency. For each attention head that contains 3.7 \u00d7 10\u00ba weights, under target bits/weight of 1.6, 4.8, and 8.0, the load latency is 50.7\u00b5s, 89.9\u00b5s, and 233.9\u00b5s when using the traditional approach, which reduce to 32.4\u00b5s, 77.4\u00b5s, and 135.6\u00b5s when using SmartQuant, representing 36.2%, 40.6%, and 42.1% reduction, respectively. Similarly, for each neuron in MLP layers, under target bits/weight of 1.6, 4.8, and 8.0, compared with the traditional approach, SmartQuant could reduce the load latency by 24.8%, 27.9%, and 38.4%, respectively."}, {"title": "V. CONCLUSION", "content": "This letter introduces SmartQuant, a CXL-based AI model storage solution that can exploit runtime context-dependent model weight importance variation to proportionally reduce model access energy consumption and load latency. The CXL memory controller is respon-sible for on-the-fly weight quantization conversion to supply variable-precision model weights. It employs a weight bit-plane in-memory placement strategy to enable fetching just-enough bits from DRAM in adaptation to desired weight quantization precision. Furthermore, it simplifies the system integration by exposing a bloated logical memory space that is partitioned into regions with different quantiza-tion precisions. Using open-source transformer model, we carried out experiments to demonstrate the effectiveness of context-dependent model weight quantization configuration and the proposed design solution on reducing the model DRAM access energy consumption and load latency."}]}