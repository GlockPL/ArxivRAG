{"title": "Learning Decentralized Multi-Biped Control for Payload Transport", "authors": ["Bikram Pandit", "Ashutosh Gupta", "Mohitvishnu S. Gadde", "Addison Johnson", "Aayam Kumar Shrestha", "Helei Duan", "Jeremy Dao", "Alan Fern"], "abstract": "Payload transport over flat terrain via multi-wheel robot carriers is well-understood, highly effective, and configurable. In this paper, our goal is to provide similar effectiveness and configurability for transport over rough terrain that is more suitable for legs rather than wheels. For this purpose, we consider multi-biped robot carriers, where wheels are replaced by multiple bipedal robots attached to the carrier. Our main contribution is to design a decentralized controller for such systems that can be effectively applied to varying numbers and configurations of rigidly attached bipedal robots without retraining. We present a reinforcement learning approach for training the controller in simulation that supports transfer to the real world. Our experiments in simulation provide quantitative metrics showing the effectiveness of the approach over a wide variety of simulated transport scenarios. In addition, we demonstrate the controller in the real-world for systems composed of two and three Cassie robots. To our knowledge, this is the first example of a scalable multi-biped payload transport system.", "sections": [{"title": "1 Introduction", "content": "Multi-wheel carriers are the most common way to transport payloads across well-structured terrain due to their ease of control, effectiveness, and configurable wheel arrangements [1]. Further, researchers have also considered wheeled transport that can be reconfigured on the fly via multiple co-operating wheeled robots [2]. While these transport systems excel in structured environments, they struggle in unstructured and challenging terrain, which limits their general applicability. This work aims to overcome such limitations while retaining the control and reconfigurability of multi-wheel systems. To achieve this, we propose the multi-biped transport problem, where wheels or wheeled robots are replaced with bipedal robots that can be arbitrarily attached to carriers. This setup allows for transport over various terrains, potentially extending to more challenging environments [3, 4, 5].\nMulti-biped transport presents significant challenges. Legged robots are generally unstable and highly dynamic, making them much harder to control compared to wheeled robots. Maintaining stability while transporting payloads and coordinating multiple legged-robots adds further complexity to the control problem [6]. Prior works on multi-legged transport systems have primarily focused on model-based approaches that rely on accurate modeling of robot interaction and explicit inter-robot communication, which limit their reconfigurability and practicality [7, 8]. While learning-based approaches have emerged as promising alternatives [9, 10, 11, 12, 13, 14, 15, 16, 17, 18] for various control problems, existing methods on multi-robot system often assume centralized control with strong assumptions on system observability, leading to limited scalability, robustness, and vulnerability to point of failure [19].\nTo overcome these challenges, we propose an architecture and training approach for a decentralized Multi-Biped Controller (decMBC). This controller supports high-level motion control of a carrier that is attached to any arbitrary configuration of bipedal robots. Importantly, to maintain maximum reconfigurability, the decMBC is decentralized in the sense that each biped can be controlled using only information about its local state and its relative position with respect to a desired reference point"}, {"title": "2 Related Work", "content": "Multi-robot transport has attracted significant research attention due to its potential to overcome the limitations of single-robot systems, especially in handling large, heavy payloads and navigating diverse real-world environments. Several works have focused on wheeled robots for collaborative transport which developed a composite connector for non-holonomic mobile robots, but their work primarily focused on well-structured environments [2]. Moreover, Eoh and Park [20] propose a cooperative object transport controller using deep reinforcement learning which uses simple non-holonomic wheeled robots to learn the collaborative pushing of an object to its goal position. While their approach shows promise, it assumes that the robots can sense the positions of the object and the goal with respect to each robot. This makes scaling the approach for arbitrary robot-carrier configurations difficult.\nSimilarly, other works have considered predictive control and decentralized approaches. Fawcett et al. [21] proposed a predictive control approach for robust locomotion of three holonomic constraint quadrupeds, but it did not allow for formation changes or scaling to more agents. Zhang et al."}, {"title": "3 Problem Formulation", "content": "We consider multi-biped transport problems where a set of N identical bipedal robots are rigidly attached to the base of a payload carrier. We model the carrier base B \\subset R^2 as a rigidly connected planar surface with a finite area. Each carrier base is assumed to contain the origin, which is referred to as the carrier control point O_c [see Figure 1 (A)]. An N robot attachment configuration for a base B is a set of points P = \\{p_1,...,p_N\\}, such that p_r \\in B, which indicates where each robot is attached. In this work, we assume ball joint attachments where each robot has free movements in all three rotational directions relative to the attachment but constrains movements in linear directions. However, our approach is easily adapted to other types of joints and non-planar rigid platform bases.\nOur goal is to control the height and linear and angular velocity of the carrier control point via the combined motor actions of the robots. To support flexible applications, we consider a decentralized controller architecture that avoids inter-robot communication and is flexible to varying numbers of robots N and configurations P = \\{p_1, ...,p_n\\}. The decentralized Multi-Biped Controller (decMBC) is defined by a single-biped controller \\pi. At each time-step, t, this controller is independently applied to each robot r \\in \\{1, ..., N\\} to compute robot actions \\alpha_t^r = \\pi(o_t^r, c_t), where o_t^r is the local observation and c_t is the carrier command.\nIn our work, the command c_t \\in R^4 specifies the target linear xy velocity, angular yaw velocity, and height of the control point. The local observation o_t^r contains: 1) the local proprioceptive state s_t^r of the robot, including the joints/motor positions and velocities, base orientation, and angular velocity, 2) the attachment position p_r of the robot relative to the control point, and 3) the relative yaw orientation \\theta \\in R of the robot base with respect to the control point orientation axis. Each of our actions a_t^r specifies the target PD motor set points of robot r, which includes 10 motors (5 in each leg) for the Cassie biped used in our experiments. Note that \\pi can be applied to any number of"}, {"title": "4 Learning Approach", "content": "We formulate learning the decMBC as a reinforcement learning (RL) problem, as it involves solving a complex physical control task without supervision for low-level actions. Our approach employs Independent Proximal Policy Optimization (IPPO) [27], where the shared decMBC controller \\pi is trained using data collected from multiple robots operating in parallel, without inter-robot communication. Below we describe the architecture of decMBC, training environment and curriculum, reward function, and details of IPPO training procedure.\n4.1 Neural Network Architecture\nFollowing prior work on bipedal locomotion [28], we represent the decMBC using a recurrent neural network architecture, which allows for history-dependent action selection via the recurrent internal memory. Specifically, our network consists of two Long Short-Term Memory (LSTM) [29] layers, each containing 64-dimensional hidden states. These layers process the decMBC input at each time step to form the internal hidden state. This state is then processed by a linear layer that produces a 10-dimensional action vector, specifying the 5 motor set points for each of Cassie's legs.\n4.2 Episode Generation and Curriculum\nOur IPPO training approach uses training data generated by executing training episodes involving one or more robots. During each episode, transitions from each robot are aggregated into a single replay buffer that is used to optimize a single actor and critic using the PPO objective [14]. Details of the PPO hyperparameters are in the Appendix E. The single actor at the end of training is taken to be the decMBC controller. Below, we describe the episode generation process, training configurations, and curriculum used to train the MBC.\nEpisode Generation. At the start of each training episode, we generate a random robot configuration according to the current curriculum stage (see below), including randomized weights on the rigid bars connecting robots. During an episode, the decMBC is used to control the N robots, given a randomized sequence of carrier commands. Each command is executed for a random number of time steps in the interval [100,450]. In addition, depending on the curriculum stage, random perturbation forces (0 to 50 N) are applied to the robot's pelvis and the carrier. Each episode ends after 500 time-steps (10 seconds) or a termination condition that checks whether a robot or the carrier has fallen, which is detailed in the Appendix C.\nTraining Configurations. Our current simulation time scales super-linearly as the number of robots grows. For this reason, we limit the number of robots used during training to N \\le 3. Empirically, we have found that using training configurations with N < 3 leads to strong performance for N > 3. We generate random configurations for N \\in \\{1, 2, 3\\} as follows: For N = 1, a single Cassie robot is used with a small carrier, where the attachment point is located at the center of the carrier. For N = 2, two Cassie robots are used with their attachment points being endpoints of a rigid bar. The carrier control point is located anywhere within a meter away from either of the robot's positions. For N = 3, an arbitrary triangular frame is formed by connecting three rigid bars. Here, the attachment points for three Cassie robots are the vertices of the triangular frame. In this configuration, the carrier control point can be randomized anywhere enclosed in the triangular frame [Figure 1 (B)]. Further details are given in Appendix A.\nTraining Curriculum. We initialize the decMBC controller and critic randomly and train it via IPPO using a 4 stage curriculum, where each stage is defined by the configurations and disturbances considered. Stage 1 involves training with one robot (N = 1) with the objective of learning robust locomotion gaits operating for carrier commands at its attachment point. Stage 2 involves training with the addition of perturbation force and torsion force on the carrier such that a robot is resistant to the disturbance forces. Stage 3 involves training with the addition of two-robot and three-robot configurations without any perturbation and torsion forces. This stage ensures each robot learns to operate on the carrier control point randomly placed away from its attachment point. Stage 4 involves"}, {"title": "4.3 Reward Design", "content": "Our reward function is designed to produce stable locomotion behavior while ensuring the robots collaboratively follow the commanded motion of the carrier control point. For each robot i in N-robot configuration, where i \\in \\{1, . . ., N \\}, the reward r_i is composed of two components: the local robot reward r_i^l and global reward r^G, where r_i = r_i^l+r^G at each timestep throughout the episode. The local reward r_i^l is designed to produce stable locomotion behavior of each robot, including maintaining consistent base height, penalizing jerky motion, regularizing proper foot alignment and the stepping frequency, with single-contact reward as proposed by van Marum et al. [30]. Furthermore, the local reward minimizes torque of the motors to penalize excessive energy expenditure as well as minimizes the force exerted at the attachment point at the carrier during standing, ensuring that robots do not pull or push against each other but hold the carrier steady. The global reward r^G focuses on the performance of the combined action of all robots in following the carrier commands. It aims to minimize the deviation between the commanded and actual x-velocity, y-velocity, angular yaw velocity, and height of the carrier control point. We detail the reward components in Appendix B."}, {"title": "5 Experiments and Results", "content": "We conduct experiments to empirically evaluate the decMBC on the Cassie bipedal robot platform. The primary objective is to assess the effectiveness and adaptability to coordinate an arbitrary number of robots, maintain stability, and adapt to diverse transport scenarios. We also discuss the scalability and generalizability of our approach across a range of practical settings including real-world transfer to multi-biped systems with two and three Cassie robots.\nExperimental Procedure and Metrics. We compute metrics such as drift, failure rate, and power consumption across various configurations to evaluate our decMBC policy. We also measure these metrics to compare our approach against other approaches. For each of these metrics across various carrier configurations, we run 1000 episodes for a maximum of 20 seconds, each with a fixed carrier command such as hold still, move forward, move sideways, and turn in-place. Drift measures the displacement of the carrier in the x/y direction and its orientational drift from its expected orientation and considers episodes that last for at least 1 second. We do not apply any perturbation forces for measuring drift. A negative and positive sign in drift indicates that the carrier is behind or ahead of the expected global position for the desired carrier command. Failure rate is calculated as the percentage of episodes that terminate before reaching 20 seconds. This is evaluated under varying perturbation forces and payload masses fixed on the top of the carrier (except dynamic payload on Table 2). We apply ranges of perturbation forces and change payload masses in the carrier to measure the failure rate. Additionally, we calculate the Power consumption by all robots combined for each configuration.\n5.1 Evaluating Scalability and Reconfigurability\nVarying Numbers of Robots: We first evaluate the performance of the decMBC for varying numbers of robots. For this purpose, we vary the number of robots from 2 to 10 using the same solid rectangular carrier. The robots are placed in a manner that ensures they are evenly distributed to maintain static stability. This typically involves positioning robots at the corners and, when necessary, placing additional robots in a central position to maintain balance (see Figure 2). In order to provide a reference point for strong performance, we evaluate the metrics for a single robot policy (1-R*) trained with its floating base as the control point. We consider achieving multi-biped performance that is comparable to 1-R* as a practical gauge of success."}, {"title": "5.2 Comparison with Centralized and Specialized policies", "content": "In this section, we compare our decentralized and adaptable decMBC with two types of straightforward but inflexible systems: centralized policies and specialized policies. The aim is to demonstrate that our approach, despite its flexibility and generalizability, does not significantly compromise performance compared to these more rigid alternatives. A centralized policy is one where all robots' states are used to produce actions for each robot from a single controller. This approach relies on a comprehensive view of the entire system to make coordinated decisions. A specialized policy, on the other hand, is designed for a specific carrier configuration with a fixed number of robots. It is trained to perform optimally under these specific conditions but lacks the flexibility to adapt to different setups.\nFrom our observations in Table 3 and Figure 4, the drift values for our approach are reasonably close to the centralized policy. Interestingly, our approach outperforms the centralized policy for two-robot configuration on failure rate, but the performance worsens on the three-robot configuration. We can also observe that the power consumption of our approach is slightly higher than that of the centralized approach. This difference could be attributed to the centralized controller's ability to produce better coordination due to its full observation of the system state.\nInterestingly, from the observations in Table 4 and Figure 5, our decentralized approach outperforms the specialized policy for Rectangle and T-shape configurations in terms of failure rate, with a cost of slightly more power consumption. Our argument for this result is that the decentralized policy, through its adaptive nature, may have generalized to alternative variations of similar configurations, making it more robust to unexpected disturbances."}, {"title": "5.3 Sim-to-real Transfer", "content": "We evaluated our policy on real hardware consisting of two and three bipedal robots, Cassie, attached at the endpoints of an I-shaped and T-shaped carrier made from aluminum extrusion with a weight holder. We also added a payload on the weight holder and used it as a carrier control point [Figure 1 (D)]. We deployed the decMBC policy on each of the robots, running independently, providing their"}, {"title": "6 Limitations", "content": "Despite the promising results, there are clear limitations to address in future work. First, the decMBC is not explored on diverse terrain types beyond flat surfaces, eliminating the understanding of its performance in more challenging environments. Second, real-world experiments are demonstrated to up to three Cassie robots, despite the simulation results showing the scalability of up to 10 robots. Third, the controller relies solely on proprioceptive sensing and relative robot positions without incorporating additional sensing modalities or addressing autonomous navigation and high-level decision-making challenges. Integrating the decMBC with perception, planning, and control modules could enhance its capabilities in complex environments. Finally, although our policy is trained with a wide range of relative positions of each robot with respect to the carrier point, which should cover most real-world scenarios, our policy does not guarantee it may work for positions beyond the range we have set during training."}, {"title": "Appendix", "content": "A Implementation details\nIn this section, we describe the implementation details of our decMBC approach, focusing on the configuration generation, and randomization of parameters used during training.\nFor generating N-robot configuration during training, where N \\in \\{1,2,3\\}, we choose relative position p_i \\forall i \\in \\{1... N\\} with respect to the carrier control point which is a 2D polar coordinate indicated by (R_i, \\Theta_i). Figure 6 shows an example of robot configurations during training. In Figure 6a (N = 1), the carrier control point is located at the base of the robot, therefore relative position (R_1,\\Theta_1) is always zero. In Figure 6b (N = 2), we randomize carrier control point location up to 1 meter from any of the robots, and similarly, in Figure 6c (N = 3), we randomize carrier control point to be located anywhere enclosed within the triangular frame, which produces a range of relative position (R_i, \\Theta_i). Furthermore, in each of these configurations, we randomize the mass of the rigid bar connection as shown in Table 5.\nThe ranges of relative positions and masses seen during training generalize for various carrier configurations with an arbitrary number of robots and payloads as observed in the experiments."}, {"title": "B Reward details", "content": "Below, we outline the reward terms used in our implementation. The reward components are divided into two categories: Local rewards r_i^l are designed to produce stable locomotion for each robot, described in the local robot frame. The Global rewards r^G are designed for following carrier commands with the combined action of all robots in the control point frame."}, {"title": "C Termination condition", "content": "An episode is terminated if any of the following conditions are met:\n\n*   The carrier's pitch or roll O_{c,rp} is outside the range [-30\u00b0, 30\u00b0].\n*   The robot's pelvis pitch or roll base_{rp} is outside the range [-30\u00b0, 30\u00b0].\n*   The robot's knee collides with the ground or with each other.\n*   The relative yaw orientation \\theta_i of each robot is outside the range [-30\u00b0, 30\u00b0].\n*   The robot's pelvis height base_z is outside the range [0.5, 1.0]\n*   The episode duration exceeds 500 timesteps (10 seconds)."}, {"title": "D Dynamics Randomization", "content": "To facilitate transfer from simulation to the real robot, we employ dynamics randomization throughout all stages of the curriculum during training. This involves randomizing various physical parameters of the simulated environment, such as joint damping, link masses, center of mass positions, encoder noise, ground friction, terrain variations, and adding noise to states and policy rate. Table 7 lists the dynamics randomization ranges used."}, {"title": "EPPO Hyperparameters", "content": "Table 8 lists the hyperparameters used for the PPO training."}]}