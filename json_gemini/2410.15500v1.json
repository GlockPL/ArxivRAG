{"title": "Anonymising Elderly and Pathological Speech: Voice Conversion Using DDSP and Query-by-Example", "authors": ["Suhita Ghosh", "Melanie Jouaiti", "Arnab Das", "Yamini Sinha", "Tim Polzehl", "Ingo Siegert", "Sebastian Stober"], "abstract": "Speech anonymisation aims to protect speaker identity by changing personal identifiers in speech while retaining linguistic content. Current methods fail to retain prosody and unique speech patterns found in elderly and pathological speech domains, which is essential for remote health monitoring. To address this gap, we propose a voice conversion-based method (DDSP-QbE) using differentiable digital signal processing and query-by-example. The proposed method, trained with novel losses, aids in disentangling linguistic, prosodic, and domain representations, enabling the model to adapt to uncommon speech patterns. Objective and subjective evaluations show that DDSP-QbE significantly outperforms the voice conversion state-of-the-art concerning intelligibility, prosody, and domain preservation across diverse datasets, pathologies, and speakers while maintaining quality and speaker anonymity. Experts validate domain preservation by analysing twelve clinically pertinent domain attributes.", "sections": [{"title": "1. Introduction", "content": "The widespread adoption of cloud-based speech technologies has made remote health monitoring more accessible for elderly individuals and those with speech disorders [1, 2]. However, since these speech recordings contain highly sensitive personal data, it becomes crucial to anonymise the speech before sharing the data across systems [3]. Speech anonymisation aims to conceal the speaker's identity in recordings while maintaining linguistic content. For elderly and pathological (non-standard) speech data, it is crucial to also preserve the prosody and unique speech patterns of the domain, such as the hoarseness in dementia patients [4], for further analysis, diagnosis, and tracking of age- or disease-related changes [5].\nVoice conversion (VC)-based methods [6] have been successful in producing anonymised speech, where a source utterance is modified to sound like a target speaker. Most VC methods are based on generative adversarial network (GAN), trained with cycle-consistency loss, allowing training on non-parallel datasets [7]. These methods generate a transformed spectrogram conditioned on the target speaker's embeddings, which are learnt jointly with the linguistic embeddings during training. GAN-based methods over-come the buzzy voice problems caused by spectrum over-smoothing in variational autoencoder approaches [8]. This is attributed to the GAN's discriminator, which ensures that the generator produces realistic conversions matching the target speaker's style. Conversely, techniques like KNN-VC [9] opt for a strategy that eschews the direct learning of speaker or phonetic embeddings. Instead, KNN-VC utilises self-supervised model-derived representations, where the features from the source are mapped to a target speaker using K-nearest neighbours. Similarly, another method [10] used pre-trained speaker embeddings as one of the inputs in the differentiable digital signal processing (DDSP) [11]-based framework. DDSP integrates traditional DSP elements, such as filters and synthesiser oscillators, into deep neural networks, with the neural network itself generating the parameters. To the best of the authors' knowledge, our work is pioneering DDSP-based VC evaluation compared to other VC or anonymisation methods.\nMost VC methods [12] have primarily been evaluated on standard data, featuring speech from young and healthy adults, with an emphasis on the naturalness and intelligibility of conversions. However, the recently introduced any-to-many method Emo-StarGAN [13] extends focus to prosody, achieving emotion-preserving conversions by employing losses derived from both hand-crafted and deep learning (DL) generated para-linguistic features, as well as an adversarial emotion classifier. However, the method fails to preserve the atypical speech patterns seen in stuttering [14], a common form of non-standard data.\nThus, we propose 'DDSP-QbE', an any-to-many VC method focused on preserving prosody and domain characteristics in speech anonymisation, even for unseen speakers from non-standard data. Our method builds on recent advancements in query-by-example (QbE) [15] and DDSP [16]. Our approach uses a subtractive harmonic oscillator-based DDSP synthesiser [16], inspired by the human speech production model [17], to incorporate an inductive bias for effective learning with limited data. By leveraging QbE, we directly derive target phonetic representations from source speech, thus bypassing the need to learn these representations during training. We introduce an inductive bias for prosody preservation by: (i) employing a novel loss function that utilises emotional speech to facilitate the separation of prosodic and linguistic features, and (ii) adding supplementary hand-crafted and DL-generated input features to the network, which have prosodic knowledge from the source utterance. For domain preservation, we employ loss functions based on acoustic properties that are crucial in clinical evaluations of voice disorders [18]. In addition to an objective evaluation, we conduct an in-depth subjective assessment of domain preservation, with speech pathologists assessing the retention of twelve clinically recognised measures for voice disorder. This analysis offers key insights, highlighting which domain aspects are preserved and which are not. Both objective and subjective analyses demonstrate DDSP-QbE's ability to anonymise speech while retaining prosodic and clinically pertinent domain features."}, {"title": "2. Differentiable Digital Signal Processing", "content": "In the DDSP framework, a synthesiser generates speech, with its parameters predicted by a neural network, enabling end-to-end training. However, this necessitates that the synthesiser's components are differentiable to enable back-propagation. A subtractive-based synthesiser model [16] is a harmonic-plus-noise model, which decomposes a monophonic sound into two components,"}, {"title": "3. DDSP-QbE", "content": "The state-of-the-art VC approach Emo-StarGAN [13] successfully retains prosody during anonymisation for standard data. However, it struggles to preserve the distinctive speech patterns, or domain features characteristic of elderly and pathological speech, such as roughness, strain, or breathiness [4]. Our method aims to overcome these challenges by isolating these unique domain-specific patterns from personal speaker traits, even in situations with limited data."}, {"title": "3.1. Framework", "content": "As portrayed in Fig. 1, the proposed framework comprises three components: Phone Mapper, Fusion, and Synthesiser, with only the latter two requiring training.\nPhone Mapper: Given a source utterance X, we obtain the phonetic representations \\(x_{phon}\\) for a target speaker from Phone Mapper. These representations maintain the linguistic content of the source utterance while sounding like the target speaker. Previous works [9, 19] have shown that representations from the 6th layer of the self-supervised model WavLM-Large (WLM6) serve as a promising candidate to derive latent phonetic features as they achieved high performance in phone discrimination tasks [20]. Furthermore, similar-sounding phones tend to be closer together in this latent phone space [9]. With this understanding, we derive the target-phone features from the source-phone features through a QbE scheme, similar to the previous works [9, 15].\nInitially, a pool of phone representations is generated per target speaker using WLM6, computed frame-wise for all available ut-terances. For a source utterance X, phonetic representations \\(X_{phon}\\) are derived in a similar manner using WLM6. Each source-phone representation acts as a query \\(q \\in X_{phon}\\) (shown in Fig. 1), which is replaced with \\(\\hat{q}\\in x_{phon}\\). \\(\\hat{q}\\) is computed as the weighted average of top-M phone representations similar to q from the target speaker's pool. Specifically, it is calculated as \\(\\hat{q} = \\sum_{i=1}^{M} m_i * w_i\\), where \\(m_i\\) represents the selected phone representation, and \\(w_i\\) is its corre-sponding weight. The weights \\(\\{w_i\\}_{i=1}^M\\) are determined by applying a softmax function to the inverse of the cosine distance between q and the \\(\\{m_i\\}_{i=1}^M\\) candidates. The weighting scheme is utilised to reduce the impact of outliers, thereby ensuring that phonetic representations closer in distance to q are given more importance.\nFusion: This component generates the parameters needed by the DDSP synthesiser, as shown in Fig. 1. It has been found that the phonetic representations from WLM6 are intertwined with para-linguistic or prosodic cues [9]. This implies that during the mapping phase, the prosodic information is also replaced along with phonetic features, which is undesirable as we aim to preserve the prosody from the source. To address this 'prosody leakage' issue, we provide the network with prosodic features \\(X_{pro}\\), extracted directly from the source utterance, along with the mapped phonetic features \\(\\hat{X}_{phon}\\) from the Phone Mapper, as shown in Fig. 1. The prosodic features \\(X_{pro}\\) are a combination of hand-crafted and DL-generated features, which are correlated with prosodic cues. The hand-crafted features considered are loudness and sample-wise z-normalised logarithmic FO contour, aimed at capturing the speaker-independent pitch variations. Recent analyses [21, 22] have shown the middle (12th) layer of WavLM-Large (WLM12) to perform well for para-linguistic related tasks, such as emotion classification. Therefore, we consider the representations from WLM12 as the prosody-correlated deep feature as prosody provides important cues to emotion [23].\nInitially, \\(\\hat{X}_{phon}\\) and \\(X_{pro}\\) are fed to their individual branches, phonetic encoder \\(Enc_{phon}\\) and prosody encoder \\(Enc_{pro}\\) respectively, as shown in Fig. 1. Each branch comprises two 1D convolutions with ReLU activation followed by group normalisation. The outputs from both branches are combined through element-wise addition and passed through a stack of three self-attention layers. Subsequently, this is followed by a shallow convolution stack with post-attention normalisation. Finally, a linear layer is used with dimensions match-ing the number of parameters \\(\\Theta\\), required by the synthesiser. The architecture of the Fusion component is akin to the small Conformer architecture [24], which has demonstrated efficacy in capturing both local and global contexts in a sequence of acoustic features.\nSynthesiser: We integrate the subtractive synthesiser proposed in [16] into our framework. The synthesiser produces the conversion Y using the parameters derived from the Fusion module, where \\(\\Theta = \\{F0, \\psi_h, \\psi_s\\}\\). In our work, we incorporate the network predicted FO (\\(\\hat{F0}\\)) unlike in the original DDSP work [11], which incorporates an additional inductive bias in the network and drives it to produce F0-consistent speech."}, {"title": "3.2. Domain and Prosody-Aware Losses", "content": "Relying solely on multi-resolution spectral losses, as employed in previous works [10, 11, 16], ensures high fidelity in the reconstruc-tion, but fails to guarantee the preservation of non-linguistic features. To address this, we incorporate additional losses during training.\nJitter and Shimmer are clinically acclaimed indicators for assessing voice disorders [25]. Jitter refers to the variation or irregu-larity in the timing of consecutive periods of F0, reflecting the insta-bility in vocal fold vibrations [26]. Therefore, jitter can be used to assess 'shakiness' or 'unsteadiness' correlates in the voice. We con-sider the jitter of the five-point period perturbation quotient (jppq5) as it is widely used in clinical studies for its ability to provide a more consistent assessment by considering neighbouring periods [27]. \\(T_i\\) are the extracted FO period lengths and N is the number of FO peri-ods. We calculate jitter loss \\(L_{jit}\\) as the mean absolute error (MAE) between jppq5 computed from the source X and the conversion Y.\n\n\\(\u0130ppq5 = \\frac{\\sum_{i=1}^{N-2} |T_{i+1} - T_i|}{\\sum_{i=1}^{N} T_i} *100\\) (1)\n\nShimmer, on the other hand, captures amplitude irregularities, which can be used to capture 'roughness' and 'breathiness' correlates in the voice [27, 28]. We consider the local shimmer \\(s_{loc}\\), which is used in the clinically recognised measure to assess breathiness, acoustic breathiness index (ABI) [29]. \\(s_{loc}\\) is computed by taking the absolute average difference between the amplitudes of consec-utive periods, and then dividing it by the average amplitude [27]. We calculate the shimmer loss \\(L_{shim}\\) as the MAE between the \\(s_{loc}\\) extracted from the source X and the converted speech Y samples.\nProsody Leakage from the mapping phase is addressed by in-troducing a loss formulation using emotional utterances. The \\(Enc_{phon}\\) encoder is specifically designed not to capture prosodic represen-tations but only phonetic representations that sound like the target speaker. Thus, \\(Enc_{phon}\\) should generate comparable representations for two utterances (X1 and X2) by the same speaker, containing the same linguistic content but delivered with different emotions. To quantify the disparity between these representations, we introduce the loss \\(L_{pro}\\), as depicted in Equation 2. This loss guides the encoder \\(Enc_{phon}\\) to capture non-prosodic representations, thereby facilitating the disentanglement of prosodic and non-prosodic features.\n\n\\(L_{pro} = |Enc_{phon}(WLM6(X_1)) -Enc_{phon}(WLM6(X_2))|\\) (2)\n\nTraining Objectives: We train the DDSP-QbE model with the multi-resolution spectral loss \\(L_s\\) and FO-related loss \\(L_{fo}\\), as done in [16], along with our proposed losses. Therefore, we train the model DDSP-QbE using the objective function shown in Equation 3, where \\(\\lambda_s, \\lambda_{jit}, \\lambda_{shim}, \\lambda_{pro}\\) and \\(\\lambda_{fo}\\) are hyper-parameters.\n\n\\(Loss=\\lambda_sL_s+\\lambda_{jit}L_{jit}+\\lambda_{shim}L_{shim}+\\lambda_{pro}L_{pro}+\\lambda_{fo}L_{fo}\\) (3)"}, {"title": "4. Experiment and Results", "content": "Due to the dearth of anonymisation methods for non-standard data, we use Emo-StarGAN (denoted as 'Emo'), known for emotion preservation as our baseline, which has been evaluated on German stuttering speech [14].\nThree English datasets are considered for training and evalu-ation: (i) ESD [30]: standard data having annotations for 5 emotion classes, (ii) ADRESS0 [31]: speech from healthy and dementia-diagnosed elderly speakers, and (iii) Sep-28k [32]: stuttering speech, containing annotations for 4 types of stuttering: block, interjection, word and sound repetitions. All utterances are resampled to 16 kHz and randomly split into training, validation, and test sets in proportions 0.8/0.1/0.1, respectively, ensuring no speakers overlap across the sets. We train and evaluate all the models on the same splits for a fair comparison. For ESD, we consider utterances from 6 speakers for training. For the non-standard datasets, we consider 20 speakers each from ADRESSo and Sep-28k, totalling \u22482.5 hours of data. Only excerpts featuring elderly and pathological speech are retained, guided by the dataset annotations. Speaker selections across all datasets are randomised, ensuring an even distribution of healthy, non-healthy, various pathologies and genders. Each model is trained using Adam optimiser with a learning rate of 0.002 for 150 epochs and batch size of 128, taking \u224836 hours to complete on an Nvidia A100 80 GB GPU. WavLM representations for training are produced from 2-second audio. We use J=150 harmonics and the fil-ter lengths as \\(F_h = 176\\) and \\(F_s = 80\\), and 5 resolutions \\(R=\\{2\\}^5\\) for spectral loss \\(L_s\\), with 75% overlapping among neighbouring frames. We set \\(\\lambda_{ms}=\\lambda_{fo}=1.0\\), \\(\\lambda_{jit}=10\\), \\(\\lambda_{shim} =0.1\\) and \\(\\lambda_{pro}=0.1\\).\nDDSP-QbE generate conversions faster than real-time, considering M=4 candidates from a phonetic feature pool, which is created for each target speaker from around 5 minutes of their utterances. We train a HiFiGAN vocoder on the training split, as described in [13], which is utilised by Emo to produce conversions. The remaining in-tricate training details and demo audio samples can be found online\u00b9.\nEvaluation Setup: We perform both objective and subjective evalua-tions for 4 source \u2192 target scenarios: (i) Elderly+Healthy \u2192 SD, (ii) Elderly+Dementia \u2192 SD, (iii) Stuttering \u2192 SD, and (iv) SD \u2192 SD, where SD denotes standard data. In all scenarios, the target speaker is chosen from SD, mirroring real-life situations due to the widespread availability of standard data. For evaluation, we use source utter-ances from the test split and randomly select 1000 conversions for each scenario, ensuring a balanced distribution across genders and types of pathologies."}, {"title": "Objective Evaluation", "content": "We assess domain preservation by classifying stuttering types and dementia. The classifiers are trained only on the original utterances in the training split, as done in [13]. We compare the class predicted for the converted speech with that of the original speech, considering the latter as the ground truth. Specifically for SD\u2192SD, we analyse whether pathologies such as dementia or stuttering are inadvertently introduced during the conversion process. For other performance metrics, we follow the methods detailed in previous work [13]: assessing overall quality through the predicted mean opinion score (pMOS) [33], prosody preservation via the pitch-correlation coefficient (PCC), intelligibility by measuring the character error rate (CER) using transcriptions from the Whisper medium-English model [34], and the strength of anonymisation through the equal error rate (EER).\nSubjective Evaluation: We embrace two kinds of user studies\u00b2, considering 160 randomly selected conversions per model due to the extensive time and cost involved in evaluating all of them. Each audio clip lasts for 4-13 seconds. The raters for the studies were un-aware whether the samples were original or synthetically generated. In the first study, two speech-language pathologists assessed the preservation of 12 domain attributes. These measures are typically used clinically to detect speech disorders [35, 36]: (i) Dysphonia: following the GRBAS scale [37], which provides an assessment of the severity level of a speech disorder, (ii) roughness: measures raspiness or harshness in voice, (iii) breathiness: measures lack of clarity in phonation, (iv) abnormal respiration, (v) articulation error, (vi) word repetition such as 'I will [will] go', (vii) sound repetition such as 'I am [pr-pr-pr-]prepared', (viii) omission or made-up words, (ix) block: unnatural pause or gasps of air, (x) interjection or filler-words such as 'um' or 'uh', (xi) strain: excessive effort or tension in phonation, and (xii) asthenia: lack of strength in the voice. In the second study, 87 English-speaking participants assessed prosody, naturalness and anonymisation, as done in [13]. For prosody preservation, subjects compared the rhythm and intonation of a source utterance to the conversions by Emo and DDSP-QbE (ABX test), disregarding quality and content, and choosing the more similar or 'both equal' option. They rated naturalness on a 5-point MOS scale from 'bad' to 'excellent'. For anonymisation, raters marked speaker similarity on a 5-point scale (1: different, 5: similar), after listening to a converted sample and another utterance from the source speaker. Each test was assessed by at least 3 raters,"}, {"title": "Results and Discussion", "content": "Table 1 indicates that DDSP-QbE significantly surpasses Emo concerning all metrics except for anonymisation, where the EER scores are compara-ble. However, in subjective assessment (refer to Table 2), DDSP-QbE conversions were perceived as less similar to the source speaker compared to those from Emo. Concerning intelligibility, DDSP-QbE outperforms Emo significantly, as indicated by the CER scores (p < 0.001, paired t-test). Emo's reduced intelligibility stems from its challenge in adapting to uncommon speech patterns, such as irregular fricatives or plosives leading to prolongations, and repetitions of sounds like '[pr-pr-pr-]prepared', resulting in less clear distinctions between vowels and consonants. This issue also adversely affects Emo's scores for prosody preservation, as seen in subjective results in Table 2. Further, Table 1 shows that both models struggle more with maintaining prosody in elderly speech than in stuttering speech, likely due to increased jitter and vocal tremors in elderly speech [38]. Nonetheless, DDSP outperforms Emo, achieving a higher mean PCC of 70.6 versus Emo's 55.1 for elderly speech. Emo is more inclined than DDSP-QbE to substitute the domain traits, such as roughness, breathiness, respiration, sound repetition, and blocks, with noise, as seen in Fig. 2. This lowers the quality of Emo's conversions, leading to lower MOS scores compared to DDSP-QbE. Most of the Emo's conversions were perceived to have an increased severity of voice disorder compared to the original utterance, affecting the Dysphonia score, as seen in Fig. 2. DDSP-QbE successfully preserves most domain attributes but faces difficulties with breathiness, block and sound repetition, as seen in Fig. 2. Specifically, DDSP-QbE occasionally fails to accurately simulate the airflow release interruption in plosive sounds /p/, /b/, /t/, /d/, typical of stuttering speech. This suggests a need for advanced modelling techniques, incorporating metrics like cepstral peak prominence (CPP) and Acoustic Breathiness Index (ABI) [29].\nThe ablation study presented in Table 3 shows that removing \\(L_{pro}\\) diminishes prosody preservation, while the individual removal of jitter and shimmer losses compromises the model's domain preservation capabilities. Interestingly, the absence of each component reduces the intelligibility of the conversions as well."}, {"title": "5. Conclusion", "content": "We propose the first speech anonymisation technique that suc-cessfully maintains the prosody and distinct speech characteristics prevalent in the elderly and pathological speech, while anonymisa-tion. Our approach utilises a subtractive DDSP synthesiser combined with query-by-example (QbE), possesses only 0.4% of the trainable parameters of Emo, and is trained on just \u22482.5 hours of data. Despite this, it shows a superior ability to generalise to rare speech patterns, showing the effectiveness of the proposed inductive biases.\nThe detailed subjective assessments, including the one focusing on clinically relevant attributes, indicate that DDSP-QbE substantially surpasses the baseline in preserving both prosody and domain-specific traits across diverse speech patterns seen in non-standard data, target speakers, and genders. Looking ahead, our goal is to improve the retention of complex features such as breathiness and to adapt our approach to additional languages and speech disorders."}]}