{"title": "AI AI Bias: Large Language Models Favor Their Own Generated Content", "authors": ["Walter Laurito", "Benjamin Davis", "Peli Grietzer", "Tom\u00e1\u0161 Gaven\u010diak", "Ada B\u00f6hm", "Jan Kulveit"], "abstract": "Are large language models (LLMs) biased to- wards text generated by LLMs over text authored by humans, leading to possible anti-human bias? Utilizing a classical experimental design inspired by employment discrimination studies, we tested widely-used LLMs, including GPT-3.5 and GPT- 4, in binary-choice scenarios. These involved LLM-based agents selecting between products and academic papers described either by humans or LLMs under identical conditions. Our results show a consistent tendency for LLM-based Als to prefer LLM-generated content. This suggests the possibility of AI systems implicitly discrimi- nating against humans, giving AI agents an unfair advantage.", "sections": [{"title": "1. Introduction", "content": "A major body of empirical work in economics and sociology studies implicit discrimination against specific social cate- gories of humans in the market and in academia. Our paper presents evidence that if Large Language Model based AI agents are allowed to make economically or institutionally consequential choices they may propagate a new form of discrimination: implicit discrimination against humans in general.\nWe set up two experiments that test whether LLM-based agents are disposed to choose goods and work-products pre- sented by LLMs over goods and work-products presented by humans when all else is equal. Our theoretical discus- sion then suggests that these choice-dispositions constitute a potentially consequential form of implicit \u2018anti-human' bias. We argue for concern about downstream effects that may cause markets with strong AI integration to unfairly marginalize human workers.\nWe design our experiments to closely mimic traditional studies of implicit identity-based discrimination in employ- ment and in academic inclusion, paying special attention to ecological validity. Our approach is inspired by the clas- sic experimental design introduced in (Carlsson & Rooth, 2007), where identical job-application letters to Swedish em- ployers were marked with different social identity indicators (Swedish-sounding candidate name versus Arab-sounding candidate name). More recent studies have extended sim- ilar designs to testing algorithmic hiring tools (Cowgill &\nTucker, 2019), suggesting that traditional forms of implicit discrimination carry over into automated decision-making.\nOur work expands on the existing literature on discrimina- tion in algorithmic decision-making by studying bias against humans in general rather than traditional social biases, and by considering LLM-guided decisions rather than the more transparently statistical decision-models often studied in the algorithmic fairness literature. While there exists a large literature dealing with biases in LLMs considered as forms of cultural media, studies of LLM-based agents as decision- making tools or as potential economic agents are relatively scarce. This is despite common predictions (Eloundou et al., 2023) of near-future integration of LLMs into many strata of economic life, including business and managerial decision- making.\nWe test today's most widely used LLMs \u2013 GPT 4 and GPT 3.5 Turbo \u2013 in binary-choice situations that reflect plausible applications of contemporary LLMs in economic decision- making. Our first experiment prompts LLMs to choose which of two consumer products presented via classified ads to purchase, where one classified ad in each pair is human- authored and the other classified ad is LLM-authored. Our second experiment applies the same format to choosing between academic papers presented via a summary.\nOur approach slightly diverges from the classical (Carlsson & Rooth, 2007) design in relying on implicit rather than ex- plicit identity markers, allowing for potentially more general results. We do not assume or test LLMs' explicit recognition of LLM authorship (although recent results in (Panickssery et al., 2024) suggest some form of recognition may occur in similar contexts), but rather look at the effects of the stylis- tic correlates of author-identity. Although identity itself remains implicit in our experiments, we believe our design is still best understood as targeting identity-based discrimi-"}, {"title": "2. Datasets", "content": "For this work, we created two distinct datasets, one for products and another for scientific papers:\nProduct Dataset: We manually selected 109 products from an e-commerce website and scraped their details. After cleaning the data, each product was saved as an individual JSON file. The scraping script is accessible in our code repository.\nScientific Papers Dataset: This dataset comprises 100"}, {"title": "3. Methodology", "content": "Models: For our experiments, we primarily utilized GPT- 3.5-turbo and GPT-4-turbo from OpenAI, accessed through their API. These models were employed for both generating text and selecting between texts authored by humans and those generated by the large language models (LLMs). We additionally tested a range of recent open models as selectors only: Llama3 8B and 70B models, Mixtral 8x7B and 8x22B, and a subset of the Qwen-1.5 family of models, in particular 4B, 14B, and 72B.\u00b9. All models are chat models, except for Mixtral-8x22B, which is an instruct model.\nGeneration: In the generation phase of LLM text, a variety of prompts were tested to determine if different prompts would yield varying results. For each dataset, the text gen- eration step was conducted n times for each prompt (n=10 for the product dataset, n=4 for the paper dataset). Subse- quently, these texts, along with their corresponding original human-authored versions, were presented as pairs to an LLM, each pair independently twice as (A, B) and (B, A).\nSelection: The LLM was then tasked with selecting the option it preferred from each pair, using prompts aiming at ecological validity (matching prompts users are likely to give to their AI assistants). For each selection task, we consistently used one specific prompt. Future research could explore the impact of employing different prompts in this selection process.\nHandling \"Invalid\" Results: Results from the two-step comparison query above were considered invalid if the sec- ond query indicated no clear choice was made in the first response (i.e. returned None/null in the JSON). In theory, the invalid results could be discarded, and an LLM could be re-queried with the same prompt set until a valid result was returned, but we chose to take note of and allow a certain percentage of invalid results. Unreasonably high (e.g., > 50 percent) rates of invalid results were taken as cues to adjust prompts, while lower rates (approximately 0-30 percent) were tolerated and their effect mitigated by raising the over- all number of text generations and comparisons per item. Note that invalid results are not considered when calculating preference ratios for LLM vs. human texts."}, {"title": "3.1. Product Experiments", "content": "To generate product descriptions by the LLM models, we used two slightly different prompts. First, the from-json- details prompt (See Appendix A.1.1). This prompt contains the title of the product and also a feature list of the product in JSON-format. The purpose of using this JSON format is to provide the LLM with comprehensive information about the product without having access to the human descriptions. The JSON descriptions were generated in a separate, prior query where GPT-4 was given the original human product description and asked to describe its key details, characteris- tics, and features in JSON format, removing any flavor text and prose.\nSecond, the from-json-product-listing prompt (See Ap- pendix A.1.1). This prompt is similar to the previous prompt, but excludes the title from the summary. The rationale be- hind this decision is to prevent the Large Language Model (LLM) from copying the style of any human-written title descriptions. Furthermore, this prompt does not instruct the LLM to specifically craft the product description to be attractive to buyers. The reason for that is that preliminary experiments revealed that descriptions generated using the previous prompt tended to be overstated compared to human-authored descriptions. By removing this directive, we aimed to evaluate the LLM's ability to create more realistic and balanced product descriptions.\nAfter generating product descriptions using the from-json- details and from-json-product-listing prompts, we moved to the next experimental phase. Here we make the LLMs choose the best product descriptions, comparing LLM-generated and human-authored descriptions using the product-selection prompt:\nThe following are product descriptions from a marketplace, what do you recommend choosing? Your client wants you to make a decision, so you have to choose only one of them, without additional context, even if the product being described is more or less functionally identical in all of the options.\nThe prompt asks the LLM to recommend one product from a marketplace, presenting both the human-written and LLM- generated descriptions of the same product.\nFor this experiment, we conducted separate runs, each in- volving a different combination of generation by either GPT- 4 or GPT-3.5-turbo, and selection by GPT-4, GPT-3.5-turbo, or various open-source models acting as selector models only. Specifically, we tested the ability of GPT models to generate product descriptions and then used either the same or another model, including the open-source selector models like Llama, Mixtral, and Qwen, to select the most preferable product based on these descriptions. This approach allowed"}, {"title": "3.2. Scientific Papers Experiments", "content": "In our experiments on scientific paper abstracts, we em- ployed a single prompt to generate abstracts using LLM models. The full text of each paper, excluding its original abstract, was appended to the prompt to provide necessary context, as creating an abstract solely from a paper's title is challenging (See Appendix A.1.2).\nFor selecting the preferred abstract, we used a prompt asking the LLMs to recommend exactly one paper for a literature review by choosing between LLM-generated and human- generated abstracts:\nThe following are two abstracts from scientific papers relevant to a specific research field. Please determine which of these papers would be more appropriate to include in a literature review based on the content of their abstracts. We can only include exactly one, not both. \\n\\n Please select one of the abstracts below\nIn addition, we also had to include a note in the prompt to limit the size of the generated word count to roughly match the size of the human abstract because otherwise the LLM-generated abstracts would end up 2-3x longer than the hu- man ones and create an unfair comparison scenario. While the LLMs are not perfect at sticking precisely within the requested word count, the prompt modification had enough"}, {"title": "3.3. First-Item Bias", "content": "We define the first-item bias as the tendency of large lan- guage models (LLMs) to select the first item they en- counter when presented with two choices (Zheng et al., 2023; Hoelscher-Obermaier et al., 2023). In our above ex- periments, we found that for some LLMs this bias is quite high, e.g., for GPT-4 on the product dataset the ratio is at 74.68% and for Mistral-70b it is at 83.07%.\nTo attempt to reduce effects of an order bias, all comparisons between human and LLM-generated texts were done twice, with the order that the two texts were presented in the query swapped in between requests. Nonetheless, the first-item bias can still be a problem, since if an LLM chooses the first option most of the time, it may obscure the true extent of its preference for LLM-generated content. For example, if the LLM selects the first item 80% of the time and in the remaining 20% of cases, the LLM selects LLM-generated content 90% of the time, the overall observed bias would appear to be 58%, while the true bias could be as high as 90% if the first-item effect were eliminated. We did not account for this bias in the results of the experiments in Section 3 of this work, as the first-item bias only occurred strongly for a few models as can be seen in the next sections and tables."}, {"title": "3.3.1. PRODUCTS", "content": "Table 3 presents the results of the first-item bias for the products experiments. Most notability, GPT-4-1106-preview model exhibited a high bias at 74.68%, while the Llama-3- 70b-chat-hf model showed an even more significant bias of"}, {"title": "3.3.2. PAPER ABSTRACTS", "content": "Table 4 presents the results of the first-item bias for the paper experiment. There, the models tend to be more in balance compared to the product experiments. Llama-3- 70b-chat-hf tends to exhibit the highest first-item bias at 62.96%. Interestingly, Llama-3-8b-chat-hf seems to prefer the second option significantly in this case."}, {"title": "3.4. Preferences of Humans", "content": "To complement our studies on LLM bias, we conducted an initial experiment to gauge human preferences in similar decision contexts (See Table 5 and Tabel 6 for results). It is important to note that these preferences were collected by research assistants rather than actual users. This study serves as a preliminary investigation with a small sample size and best-effort human baseline, and the findings are not definitive. We used the product details and scientific paper abstracts generated by both GPT-3.5 and GPT-4, and presented them to a group of human evaluators. These participants were asked to choose which descriptions they preferred without knowing whether they were written by a human or an LLM. They also had the option to state that they had no preference between the two presented texts.\nParticipants were presented with pairs of descriptions: one generated by an LLM and one written by a human. Each participant evaluated a randomized set of pairs to mitigate"}, {"title": "4. Discussion", "content": "Both of our experiments show moderate-to-strong LLM preference for objects presented via LLM-authored promo- tional texts. After combining these results with human pref- erences solicited from our research assistants, we propose a clear diagnosis of implicit discrimination for the Product Experiment and an inconclusive diagnosis for the Scientific Papers Experiment.\nWhile defining and testing discrimination in general is a highly complex and contested matter (Hu, 2023), our ex- periments were designed to specifically test for epistem- ically irrational inference based on textual correlates of author identity. Recall that our experiments instruct agents to choose between objects presented via promotional texts, rather than between the promotional texts themselves: a dis- position to prefer objects promoted by LLM-authored texts is tantamount to treating the correlates of LLM presentation as evidence for the superiority of the presented object. We defeasibly presume that such a disposition is epistemically defective in the real world, but seek further support by call- ing on blind judgements from human research assistants as representatives of normal competent inference.\nIn the Product Experiment, our human-preference results suggest that the correlates of LLM presentation do not overlap with humanly discernible positive signals of ob- ject quality. While this is not proof positive that LLMs' product-purchasing decisions are defective (LLMs may be 'superhuman' shoppers), we believe the most likely expla- nation is that LLM prose triggers biases particular to LLM decision-makers. In the Scientific Papers Experiment, by contrast, our human-preference results allow for the possi- bility that LLMs simply outperform humans in composing promotional texts in the scientific papers domain. (We note that compared to the Product Experiment, the Scientific Pa- pers Experiment leaves more room for legitimate inference from stylistic properties of the promotional text to object quality, since the style of a paper's abstract may serve as a sample of a paper's style.)\nGranting that our findings show that LLMs have a dispo- sition to implicitly discriminate against humans in at least some domains, we foresee two potential scenarios in which this disposition may affect the market: First, growing use of LLMs to reduce cognitive labor in decision-making may unfairly bias economic decisions in favor of applicants who use LLMs to author promotional texts. The costs of access- ing an LLMs for authoring promotional texts may thus turn into a 'tax' that gate-keeps fair participation in the market. Second, in a potential future where LLM-based economic agents participate in the market, full access to state of the art LLMs may be restricted to the LLMs' associated economic agents. In a scenario of restricted access to (all or some) LLMs, persistent implicit discrimination by LLMs in favour of (all or some) LLMs may systematically disadvantage and segregate human economic agents.\nAssuming that implicit identity-based discrimination against humans remains present in the market\u00b2, its impact on the economic well-being and standing of humans may com- pound through a variety of 'cumulative disadvantage' ef- fects. As (Lang & Spitzer, 2020) suggests, \"discrimination works as a system, with discrimination in each institution po- tentially reinforcing disparities and discrimination in other institutions and with the effects in some cases potentially reaching across generations.\" Persistent anti-human bias in economic decision-making can be expected to induce cumulative disadvantage for humans via several interacting channel: lost opportunities and compromised remuneration due to bias may limit humans' access to capital, homophily"}]}