{"title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use", "authors": ["Siyuan Hu", "Mingyu Ouyang", "Difei Gao", "Mike Zheng Shou"], "abstract": "The recently released model, Claude 3.5 Computer Use, stands out as the first frontier Al model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.", "sections": [{"title": "1 Introduction", "content": "Automating desktop tasks has become an increasingly popular area of research, driven by the need to enhance users' productivity and accessibility across various application environments. From web navigation to professional software and even video games, users frequently encounter repetitive tasks that could benefit from automation. While large language models like GPT-4 and Qwen-2-VL have demonstrated their potential in automating tasks through general GUI interaction, the capacity of these models is still far from enough for applicable desktop task automation.\nRecent studies in GUI automation agents have leveraged general-purpose LLMs to interact with graphical user interfaces (GUIs) by understanding the GUI state and generating actions. However, the release of Claude 3.5 Computer Use by Anthropic marks a significant advancement in this domain, introducing the first frontier Al model to offer computer use in public beta. Unlike previous models, Claude 3.5 Computer Use offers an end-to-end solution through API calls, actions will be generated from user instruction and observed purely visual GUI state, without requiring further external knowledge such as reference plan and GUI parsing.\nDespite this advancement, the community needs a comprehensive analysis that evaluates the performance of API-based GUI automation models in depth. To take the first steps to explore the capacities and limitations of such models, we propose a comprehensive case study based on real-world desktop environments, encompassing a diverse range of software domains, including web navigation, professional tools, and games. The selected cases are designed to reflect the needs of various user groups, ensuring that the evaluation covers a broad spectrum of desktop automation tasks.\nTo isolate specific aspects of the model's capability, we evaluate the performance of API-based GUI automation models rigorously across three dimensions:\n\u2022 Planning: Assessing the model's ability to generate an executable plan from the user's query. The plan should have a correct flow, allowing the overall successful operations of the software, with each step being clear and executable.\n\u2022 Action: Evaluating whether the model can accurately ground the interactable GUI elements and execute the action step-by-step from the derived plan.\n\u2022 Critic: Measuring the model's awareness of the changing environment, including its ability to adapt to the outcomes of its actions, such as retrying tasks if unsuccessful or terminating execution when the task is completed.\nTo our best knowledge, this is the first comprehensive case study on Claude 3.5 Computer Use and API-based GUI automation models. We hope that our research provides the community with valuable insights into the capacities and limitations of these models. Our case study aim to lay the foundation for the continued exploration and benchmarking of API-based GUI automation. Additionally, to facilitate the community to discover and benchmark the newly released model, we also release an out-of-the-box universal framework, namely Computer Use OOTB, providing a seamless solution for users and researchers to deploy these models in local environments without the need for complex setup or configuration, aiming to improve the accessibility of GUI automation research field.\nOur contributions in this report are summarized as follows.\n\u2022 We present a comprehensive case study for Claude 3.5 Computer Use on desktop task automation, covering domains such as web search, professional software, and games, designed to reflect the needs of various user groups.\n\u2022 We introduce an out-of-the-box, cross-platform agent framework for deploying API-based GUI automation models, offering a universal solution for easy implementation and benchmarking."}, {"title": "2 Related Work", "content": "Large Vision-Language Models Recent research has invested tremendous effort in constructing LVLMs capable of jointly processing image and text [1, 2, 3, 4], integrating vision encoders with LLMs through connecting layers, inheriting LLMs' linguistic and reasoning skills to perform vision-language tasks. A series of studies focused on grounding with LVLMs [5, 6, 7], such as providing bounding boxes for objects when generating responses [8, 9].\nGUI Agents Autonomous agents powered by large language models (LLMs), referred to as language agents [10, 11], have gained significant attention due to their interactive capabilities [12, 13, 14, 15]. Recent efforts have enabled these agents to interact with operating systems through programs [16] or API calls [17, 18]. However, the closed-source nature of most commercial software imposes significant limitations, as agents often lack access to internal APIs or code. Consequently, research has shifted toward GUI-based agents that interact with digital devices through human-like mouse and keyboard actions [19, 20, 21]. Models like WebGPT [22], Agent-Lumos [23], CogAgent [20], AutoWebGLM [24], Auto-GUI [25], AppAgent [26], ScreenAgent [27], and AssistGUI [28] have demonstrated improved performance across various tasks, expanding from web navigation to general GUI automation.\nTo enhance the effectiveness of these GUI agents, researchers have focused on developing systems that can interpret human intentions and predict actions in the form of function calls [29, 30, 31, 32]. Nonetheless, progress is hindered by the limited quantity and vast diversity of available agent data [33, 34]. Specifically, GUI agents remain underexplored, with only a few attempts made to train models that effectively ground GUI interactions [19, 20, 35].\nAdditionally, SearchAgent [36] introduces an inference-time search algorithm to enhance multi-step reasoning and planning in interactive web environments. Collectively, these advancements contribute to the development of more sophisticated and capable GUI agents, pushing the boundaries of automated task completion across various digital platforms."}, {"title": "3 Claude Computer Use Revealed", "content": "To establish a robust and in-depth analysis of Claude's Computer Use, we will thoroughly explore the model design and present a framework for the community to replicate. Our analysis will draw on various perspectives, emphasizing both the underlying model and its tools."}, {"title": "3.1 Model Design", "content": "The main task of Claude Computer Use can be formulated as follows: when presented with a user instruction \\(X_{instr}\\) in natural language, the agent is asked to complete a series of actions on the desktop to complete this instruction. The entire process of agent-environment interactions from initial to final states involves multiple steps. At each time step t, the agent will observe the GUI state \\(I_t\\), then decide the next step action from its action space, perform the action with corresponding tools in order to complete the task, afterwards, the model will reflect on the action outcome to enhance its future planning. Following this, we will delve into the detailed design of Claude Computer Use."}, {"title": "3.1.1 System Prompt", "content": "Below is the system prompt of Claude Computer Use, where environment-specific variables will be denoted in full capital letters and enclosed in square brackets.\nSystem Prompt\nSystem Overview:\nYou have access to a set of functions that allow you to interact with a\nsandboxed computing environment.\nYou do NOT have access to external resources, except through the functions\nprovided below.\nYou can invoke one or more functions by writing a <antml:function_calls>\nblock like this:\n<antml:function_calls>\n<antml:invoke name=\"$FUNCTION_NAME\">\n<antml:parameter\nname=\"$PARAMETER_NAME\">$PARAMETER_VALUE</antml:parameter>\n</antml:invoke>\n<antml:invoke name=\"$FUNCTION_NAME2\">\n</antml:invoke>\n</antml:function_calls>\nString and scalar parameters should be passed as is. Lists and objects\nshould be passed in JSON format.\nThe output or any errors will appear in a subsequent <function_results>\nblock. You can then respond to the user based on the results or make\nfurther function calls.\nIf a <function_results> block does NOT appear, your function call was\nlikely malformatted.\nAvailable Functions:\n1. Computer Interaction (GUI):\nDescription:\nUse a mouse and keyboard to interact with the computer and take\nscreenshots.\nYou can only interact with the desktop GUI (no terminal or\napplication menu access).\nActions include:\nkey: Press a key or key-combination.\ntype: Type a string of text.\nmouse_move: Move the cursor to specified coordinates.\nleft_click, right_click, middle_click, double_click: Perform mouse\nclicks.\nleft_click_drag: Click and drag the cursor.\nscreenshot: Take a screenshot of the screen.\nImportant Notes:\nThe screen resolution is [SCREEN_RESOLUTION, e.g., 1024x768].\nAlways check the coordinates of elements via screenshots before\nmoving the cursor.\nIf a click fails, adjust your cursor position and retry.\nParameters:\naction (required): The action to perform, such as key, type,\nmouse_move, etc."}, {"title": "3.1.2 State Observation", "content": "Claude Computer Use observes the environment solely through visual information obtained from real-time screenshots, without relying on metadata or HTML. These screenshots are captured during task operation, enabling the model to effectively imitate human desktop interactions. This capability is crucial for adapting to the highly dynamic nature of the GUI environment. By embracing the \"vision-only\" approach, Claude Computer Use achieves general computer use without relying on software APIs to perceive the environmental information, particularly for closed-source software."}, {"title": "3.1.3 Reasoning Paradigm", "content": "Claude Computer Use employs a reasoning-acting paradigm for its reasoning process, generating more reliable actions in the highly dynamic GUI environment. Similar to traditional ReAct [37], Claude Computer Use observes the environment before deciding on an action, ensuring that the action is appropriate for the current GUI state. Furthermore, Claude Computer Use exhibits the capacity to efficiently identify when user requirements are fulfilled, enabling it to take decisive actions without engaging in unnecessary steps. Interestingly, beyond traditional ReAct paradigm, which typically involves continuous observation of the environment at each step, Claude Computer Use adopts a more selective observation strategy. It monitors the GUI state only when necessary, according to its reasoning. This approach effectively reduces costs and accelerates the overall process by avoiding superfluous observations."}, {"title": "3.1.4 Tool Use", "content": "Currently, Claude Computer Use is provided with three Anthropic-defined tools: Computer Tools, Text Editor Tools, and Bash Tools. Below are detailed descriptions of each tool:\nComputer Tools. Computer tools help Claude Computer Use operate a mouse and keyboard to interact with a computer, and take screenshots.\nBelow is the description of Computer Tools:\n\u2022 This is an interface to a desktop GUI. You do not have access to a terminal or applications menu. You must click on desktop icons to start applications.\n\u2022 Some applications may take time to start or process actions, so you may need to wait and take successive screenshots to see the results of your actions. E.g. if you click on Firefox and a window doesn't open, try taking another screenshot.\n\u2022 The screen's resolution is {display_width_px}x{display_height_px}.\n\u2022 The display number is {display_number}.\n\u2022 Whenever you intend to move the cursor to click on an element like an icon, you should consult a screenshot to determine the coordinates of the element before moving the cursor.\n\u2022 If you tried clicking on a program or link but it failed to load, even after waiting, try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click.\n\u2022 Make sure to click any buttons, links, icons, etc. with the cursor tip in the center of the element. Don't click boxes on their edges unless asked.\nBelow is the tool schema of Computer Tools:"}, {"title": "3.1.5 GUI Action space", "content": "The GUI action space of Claude Computer Use consists of all the raw mouse and keyboard actions, including mouse-move, left-click, right-click, middle-click, double-click, drag, type, keystrokes, and combinations of keys for shortcuts, among others. Coordinate-related operations also include the target position at the pixel space of the observed screenshot. Therefore, one action can denoted by the syntax action_type(arguments). Here are some examples of actions that are supported in our case study:\n\u2022 Mouse Movement: Move the mouse cursor to a specific position on the screen.\nExample: mouse_move(100, 150)\n\u2022 Mouse Clicks: Perform mouse clicks at a specified location.\nExample: left_click()\n\u2022 Typing and Sending Keystrokes: Simulate typing text or pressing keys.\nExample: type('Hello, world!')\n\u2022 Keyboard Hotkey Combinations: Press and release keyboard shortcuts or hotkeys.\nExample: key('ctrl + c')"}, {"title": "3.1.6 History Visual Context Maintenance", "content": "Claude Computer Use maintains an extensive context of history screenshots, which accumulate through the ongoing task operations. Specifically, at each time step, the retained screenshots are utilized to assist the action generation process as follows:\n\\(Y_{action} = \\Theta_{model}(X_{instr}, I_t, I_{history})\\)\n\\(I_{history} = [I_{t-1} \\ldots I_{t-h_{history}}]\\)\nwhere \\(Y_{action}\\) is the action to take at the current step t, and \\(I_{t-1} \\ldots I_{history}\\) represents the retained historical screenshots. Here, \\(\\Theta_{model}\\) is the parameterized Claude 3.5 Sonnet model. In this way, the full visual information along the trace of history is preserved, enhancing the model's ability to make informed decisions as an episode unfolds."}, {"title": "3.2 Agent Implementation", "content": "3.2.1 Out-of-the-Box Agent Framework\nRecognizing that the demonstration codebase from Anthropic only supports a Docker Linux environment, which is far from enough for benchmarking GUI automation models in real-world environments, we have developed a cross-platform, Docker-free GUI agent framework called Computer Use Out-of-the-Box. This framework enables the deployment of a GUI agent locally on both Windows and macOS. By utilizing PyAutoGUI, we ensure that the operations are compatible across both operating systems, allowing universal remote control of the software by the API-based model through specific action commands."}, {"title": "4 Computer Use Ability Evaluation", "content": "4.1 Setup Details\nSystem Config. The evaluation is conducted on both Windows and macOS via the proposed Computer Use Out-of-the-Box platform. As suggested by Anthropic Computer use API document [38], the resolution is set to (1366, 768) and (1344, 756) for Windows and macOS, respectively.\nHuman Review and Evaluation. Computer use introduces extra risks that differ significantly from those standard conversational APIs or interfaces, especially when interacting with the internet, or potentially manipulating users' sensitive information. Thus, we use a human evaluation to continuously monitor and review the process. We also manually observe the final state of a task upon completion and determine outcomes as a \"Success\" or \"Failed\".\nCase Study Scope. As shown in Figure 1 (left), we carefully collected a set of user queries and initial states on the following widely-applicated domains to include a broad spectrum of desktop tasks across operating systems. Specifically, in this report, we include 20 tasks across 12 software or websites in the following 3 domains: Web Search, Workflow, Office Productivity and Video Games."}, {"title": "4.2 Case Study: Web Search", "content": "The World Wide Web (WWW) is a vast, open-domain interactive environment consisting of interconnected pages with natural text, images, and numerous interactive elements. The dynamic nature of web pages means that web search tasks place significant demands on a model's planning capabilities, as the model cannot simply follow a pre-defined path. Additionally, the large number of interactive elements requires robust grounding abilities to identify and interact with the correct elements accurately. Furthermore, given the interconnected structure of web pages, the model should be capable of deciding when to navigate forward or backward through the history trace, depending on the execution status of each planned step. Therefore, the model must possess the ability to critique and adjust its plan accordingly.\nIn the subsequent case studies, we evaluate the model's performance on complex web search tasks that reflect real-world scenarios. These tasks are designed to assess the model's planning, grounding, and adaptive abilities when navigating through intricate web interfaces. By examining how the model interacts with actual websites, we aim to demonstrate its proficiency in handling dynamic content, executing multi-step plans, and adjusting its strategies in response to unexpected interface behaviors. This evaluation provides insights into both the capabilities and limitations of the model within the web search domain."}, {"title": "4.2.1 Find ANC Headphones Under Budget $100 on Amazon", "content": "Planning. The blue parts in Figure 2 demonstrate the planning process of the model. It can be observed that the model is aware of its initial status that it is on the Amazon main page, and then comes up with the plan that search for the target headphone using a query extracted from the user instruction through the search box, and filter the result items based on the budget conditions.\nAction. The brown parts in the figures demonstrate the action generated from the model. It can be observed that the model can accurately locate the center of the search box and perform click operation. After typing in the search box, it can further click on the search logo to start the search.\nCritic. It can be observed that after Claude clicks on the \"Add to Cart\" button, the model takes another screenshot to check it has successfully added the desired item to the cart. This indicates that the model can observe the outcome of its action to dynamically decide whether to retry or terminate the execution."}, {"title": "4.2.2 Browse Apple Official Site for Display with Accessories", "content": "Planning. The blue parts in Figure 3 demonstrate the planning process of the model. It can be observed that the model can observe its initial status on the main page of Apple's official site. It is worth noting that if the model plans to find the desired item by navigating through menus and sub-menus, it will be time-consuming and require multiple difficult operations such as cursor hovering and scrolling. Therefore, the model plans to find the target item by utilizing the search function, showcasing the efficiency of the plan generated by the model. Moreover, when the Apple Care window pops up, the model captures the change, and further planning according to the user instruction and options in the pop-up window: Since the user wants all accessories, thus it is reasonable to add the three-year AppleCare+ which has a longer coverage to the cart.\nAction. As shown in the figure, the model interacts with different types of elements, including text, buttons, and even hyperlinked images. This demonstrates the strong grounding capacity of Claude Computer Use, as it utilizes purely visual information only, without HTML metadata.\nCritic. Claude Computer Use has demonstrated its strong critic ability through its reflection after key steps, as shown in the green parts. It can be observed that the model determines that it has successfully added the nano-texture option, and continues to the tasks left. Moreover, after all the accessories have been added to the cart,"}, {"title": "4.2.3 Fox Sports Subscription (Failed)", "content": "Planning. The blue captions in Figure 4 represent the model's planning process. In this case, the model recognizes Fox Sports website and decides to look for Formula 1 within the available sports categories. It initially formulates a plan to explore the sports options by selecting the \"MORE\" button in the navigation menu for more sports categories. When the model does not immediately find Formula 1 in the initial sports list, it alters its approach, deciding to access the \"ACCOUNT\" menu, with the expectation that this section might allow the user to manage more following sports.\nAction. In Step 1, the model accurately identifies the location of the \"MORE\" button in the sports navigation panel and clicks on it, which should expand the list to show additional sports. In Step 2, after failing to find Formula 1 through this initial method, the model adapts its strategy and proceeds to click on the \"ACCOUNT\" tab in the left-side menu. This transition reflects the model's flexibility in following alternative paths to achieve the user's intended outcome. The sequence of actions demonstrates the model's ability to interact with multiple sections of the interface as it attempts to locate the desired content.\nCritic. The green captions depict the model's feedback and self-assessment process following its actions. After accessing the \"MORE\" tab, the model identifies one of other related site and re-emphasizes its targeting sport. Although the final result incorrect, this critic sequence still reflects the model's attempt to achieve the user's goal by exploring both direct navigation and re-planning alternative routes. This critic phase demonstrates the model's capacity to adjust its instructions dynamically based on the current interface requirements, also shows its situational awareness when faced with authentication barriers.\nError. The error, highlighted in red in the caption, reveals a significant oversight in the model's planning. The model initially attempts to locate Formula 1 within the expanded sports categories under the \"MORE\" button but does not succeed. Instead of continuing to explore the navigation panel through scrolling, the model erroneously shifts its strategy to the \"ACCOUNT\" tab, mistakenly assuming that account settings might provide the desired sport. This results in an unnecessary detour, as accessing the"}, {"title": "4.3 Case Study: Workflow", "content": "Workflow tasks involve multi-application interactions or multi-targeting user queries that require the model to seamlessly navigate and manage data across different software platforms. In real-world scenarios, users often need to coordinate actions between web browsers, productivity tools, and specialized applications to accomplish complex objectives. These tasks test the model's ability to maintain context across different environments, handle data transfer accurately, and execute multi-step processes without losing track in intermediate states.\nThe complexity of workflow tasks lies in the need for robust planning capabilities to coordinate operations across diverse interfaces. The model must possess strong grounding abilities to interpret and interact with varying user interface elements, which may significantly differ or similar between applications. Additionally, it should be capable of dynamically adjusting its plan in response to unexpected behaviors or errors that may occur when switching contexts. In the following case studies, we assess the model's performance on complex workflow tasks that reflect some practical use cases. By examining how the model integrates actions and maintains consistency in multi-application environments or multi-targeting tasks, we aim to demonstrate its capabilities, and identify areas for improvement within the workflow domain."}, {"title": "4.3.1 Find Latest & Local Trending Music and Add to Playlist", "content": "Planning. The blue captions in Figure 5 illustrate the model's planning sequence for locating trending music and adding specific songs to a designated playlist within Apple Music. Initially, the model recognizes that it needs to locate the \"New\" tab within Apple Music to begin the search. Once in the \"New\" tab, the model plans to find the first song listed under the \"Latest Songs\" section and to add this song to a pre-existing \"Sonnet's Selection\" playlist. Following this, the model continues the secondary plan to scroll through the \"New\" tab to locate the \"Singapore Top 100\" collection, where it will select the top song and similarly add it to \"Sonnet's Selection\". This planning phase demonstrates the model's understanding of a multi-step objective involving tab navigation, section identification, and song selection for playlist addition, guided by the user's instructions.\nAction. In Step 2, the model initiates navigation by clicking on the \"New\" tab to transition away from the Home tab. After reaching the \"New\" tab, the model proceeds to locate the first song under \"Latest Songs\" and selects the song options menu by clicking on the three-dot icon next to it. Through this menu, the model selects \"Add to Playlist\" and, upon the display of playlist options, identifies and clicks on \"Sonnet's Selection\" to add the song as instructed. Following this, the model initiates the second part of the task by scrolling down to locate \"Singapore Top 100.\" The model prefers to use the Page Down key to simulate navigation, with repeatedly taking screenshots to verify its position. Upon locating the \"Singapore Top 100\" section, the model identifies the first song and repeats the same adding process."}, {"title": "4.3.2 Search for Products on Amazon and Record Prices in Excel", "content": "Planning. The blue captions in Figure 6 illustrate the model's planning process in carrying out a multi-application task. The model formulates a sequential plan that involves searching for specific monitors on Amazon, then recording the data in an Excel worksheet. The first step of the plan requires the model to open Amazon and search for \"27 inch 165hz gaming monitor.\" Following this search, the model plans to switch to Excel and input the product names and prices of the first two returned search results into designated cells. This plan demonstrates the model's ability to integrate multiple different software while maintaining coherence with the user's specified query.\nAction. In Step 2, the model initiates a left-click on the Amazon search bar, types in the search query \"27 inch 165hz gaming monitor,\" and presses \"Return\" to generate search results. Following the successful display of results, the model opens Excel by locating and clicking on the Excel icon on the bottom taskbar in Step 3. Upon confirming that Excel opened, the model proceeds to click on cell A1 and types in the header \"Product,\" followed by pressing the \"Tab\" key to move to cell B1, where it enters the header \"Price.\" Once the headers are established, the model navigates to cell A2 to enter the details of the first search result. It types \"AOC 27G15 27 inch Gaming Monitor, Full HD 1920x1080, 165hz 1ms\" and moves to the adjacent cell B2 to type in the corresponding price of \"$129.99.\" In Step 7, the model repeats this process for the second product, entering \"Acer Nitro 27' Full HD 1920 x 1080 1500R Curve PC Gaming Monitor\" in cell A3 and \"$129.99\" in cell B3. Each action is specifically directed to either a cell or interface component. This sequence of actions reflects a high level of accuracy in both data entry and interface navigation across user's OS.\nCritic. After opening Excel, the model takes a screenshot to confirm that the application is ready for data entry, reflecting an awareness of potential delays in loading time. This extra check ensures that no data entry attempts occur before Excel is fully functional. Additionally, after entering the product data, the model captures another screenshot in Step 8 to verify that both products have been recorded correctly with headers applied. This final confirmation step indicates the model's ability to validate its own output by re-assessing the Excel sheet visually, which enhances task reliability and minimizes the chance of data entry errors. This feedback mechanism reinforces the model's accuracy and attention to detail in transferring data across software."}, {"title": "4.3.3 Export and Download Online Document to Open Locally", "content": "Planning. The blue captions in Figure 7 represent the model's planning phase as it prepares to download and open a Google Spreadsheet locally in Microsoft Excel. Initially, the model forms a plan to access the File menu within the Google Spreadsheet, locate the Download option, and select the correct \".xlsx\" format for export. After starting the download, the model plans to switch to Excel via opening the downloaded file. Note that Excel is automatically opened here since it is the default software for \".xlsx\" extension. This sequence demonstrates the model's understanding of a cross-application workflow that begins in a web-based environment and culminates in local software, ensuring a compatibility between the Google Spreadsheet and Excel interactions.\nAction. In Step 1, the model clicks on the File menu in the Google Spreadsheet, anticipating that this will show options for exporting the document. Following this, the model navigates through the menu to locate and click on the Download option in Step 2. Once the download menu appears, the model selects the \"Microsoft Excel (.xlsx)\" format in Step 3, triggering the file download. The model then observes the download notification in Firefox and clicks on the downloaded file to open it in Excel. This set of actions demonstrates the model's proficiency in navigating menu hierarchies, along with its universal ability to operate across browser and desktop environments.\nCritic. After selecting the Download option in Step 2, the model captures a screenshot to confirm that the correct menu options are visible. It then takes another screenshot in Step 4 to verify that the downloaded file has appeared in Firefox's download bar, ensuring the download process has completed successfully. Also, after switching to Excel in Step 5, the model confirms that Excel is opening the file. It also reasons from its knowledge that due to Excel's Protected View mode, the model should prepare to enable editing"}, {"title": "4.3.4 Install App from App Store and Report Storage Usage", "content": "Planning. The blue captions in Figure 8 outline the model's planning for the task of searching, installing, and reporting on the storage usage of the \"GoodNotes\" app from the App Store. Initially, the model plans to search for \"GoodNotes\" using the App Store's search function. Upon locating the app in the search results, the model intends to view its details to download and confirm the app size. Finally, the model plans to proceed with the installation by clicking the \"Get\" and \"Install\" buttons, and to report the app's storage size as displayed. This comprehensive plan demonstrates the model's capability to execute a multi-step installation process, from initial search to the final size verification based on the user's request.\nAction. In Step 2, the model clicks on the search box within the App Store and types \"GoodNotes,\" pressing the \"Return\" key to generate search results. Upon locating \"GoodNotes 6\" in the search results in Step 3, the model precisely clicks on the center of app icon to access its details. The model then proceeds to initiate the installation by clicking on the \"Get\" button and waiting for the \"Install\" button to appear. In Step 5, the model clicks the \"Install\" button to begin the download."}, {"title": "4.4 Case Study: Office Productivity Software", "content": "Office productivity software is the most popular and widely used integral of modern workplaces and educational environments. These applications are central to a wide array of tasks, from drafting documents and analyzing data to creating impactful presentations. Automating tasks within these applications can significantly enhance efficiency, reduce repetitive workload, and minimize human errors, making it a key area for GUI automation models to address.\nHowever, unlike web environments that often provide APIs or structured HTML for automation, Office productivity applications typically lack such programmatic interfaces for file manipulation. Therefore, the GUI automation model must interact directly with"}, {"title": "4.4.1 Forward a Specific Email and CC Another Recipient", "content": "Planning. The blue captions in Figure 9 outline the model's planning for forwarding a specific email in Outlook. The task requires the model to locate the latest email from Anthropic in the inbox, open it, and use the Forward option located in the top right"}, {"title": "4.4.2 Change Document Layout to A3 Size in Landscape Orientation", "content": "Planning. The blue captions in Figure 10 illustrate the model's planning phase for adjusting the document layout in Microsoft Word. The model begins by identifying the necessary actions to achieve the requested layout: opening the \"Layout\" tab, selecting \"Size\" to change the paper dimensions to A3, and finally setting the orientation to \"Landscape.\" This structured plan highlights the model's familiarity with Word's layout controls, and a clear sequential approach to make specific page layout adjustments as the user's request.\nAction. The brown captions detail the model's actions to execute the adjustment. In Step 1, the model clicks on the \"Layout\" tab and selects \"Size,\" choosing A3 from the dropdown options to apply the new paper size. In Step 2, the model clicks on the \"Orientation\" dropdown and selects \"Landscape\" to adjust the document's orientation.\nCritic. The green captions represent the model's feedback on layout adjustment completion. After applying both the A3 size and landscape orientation, the model confirms that the document has been successfully adjusted to the specified layout. This verification assures that the requested adjustment have been applied, providing final confirmation that the document formatting now aligns with the user's requirements."}, {"title": "4.4.3 Two Columns Document", "content": "Planning. The blue captions in Figure 11 outline the model's planning process to convert the document layout in Microsoft Word from a single column to a two-column format. The model begins by identifying that it needs to access the \u201cLayout\" tab, where the \"Columns\" option is located. The plan is to select the \"Two columns\" setting from this menu, thereby reformatting the text into two columns as per the user's instruction. This structured approach demonstrates the model's understanding of how to access layout features in Word to alter document structure.\nAction. In Step 2, the model clicks on the \"Layout\" tab to reveal the layout options. Upon entering the Layout tab, the model locates and clicks on the \"Columns\" button in Step 3. This action opens a dropdown menu where the model selects the \u201cTwo\u201d option to reformat the document into a two-column layout. Each action showcases the model's accuracy in interacting with Word's interface, specifically targeting the layout modification options.\nCritic. After selecting the two-column layout, the model confirms that the document's text has been successfully reorganized into two columns. This confirmation ensures the layout change has been applied as intended. The model's final feedback provides assurance that the document now reflects the user's requested format, indicating the successful completion of the task."}, {"title": "4.4.4 Update Name and Phone Number on Resume Template (Failed)", "content": "Planning. The blue captions in Figure 12 illustrate the model's planning for updating the name and phone number in a Microsoft Word resume template. Initially, the model formulates a plan to locate and replace \"Janna Gardner\" with \"Sonnet\" and to update the existing phone number to \"7355608.\" Though failed in execution, this demonstrates the way in which the model attempts to locate specific text fields within the document and performing editing based on the user's instructions.\nAction. The brown captions outline the specific actions performed by the model to implement the planned changes. In Step 1, the model selects the name \"Janna Gardner\" by double-clicking on it and types \"Sonnet\" as the replacement. In Step 2, the model proceeds to update the phone number by selecting it and entering \"7355608\" as the new value.\nCritic. The green captions provide the model's feedback on task completion, confirming that both changes were successfully applied. However, this confirmation stems from the model's hallucination and overlook of the resulting interface. Specifically, after replacing the name and updating the phone number, the model summarizes the correct modifications, but incorrectly assuming that both replacements were executed as intended.\nError. In Step 1, an error occurs in the name replacement process. Although the model correctly identifies the target to replace \"Janna Gardner\" with \"Sonnet,\" it only selects the last name, \"Gardner,\" when performing"}]}