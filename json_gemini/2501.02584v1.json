{"title": "Efficient Architectures for High Resolution Vision-Language Models", "authors": ["Miguel Carvalho", "Bruno Martins"], "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.", "sections": [{"title": "1 Introduction", "content": "The integration of visual capabilities as extensions to Large Language Models (LLMs) has led to the emergence of large Vision-Language Models (VLMs), currently excelling in tasks like image captioning and visual question answering. Notable examples include Flamingo (Alayrac et al., 2022) or Monkey (Li et al., 2023d), in this last case bringing improvements to tasks that involve scene-text by processing higher-resolution images. LLaVA-NeXT (Liu et al., 2024) followed Monkey's lead with similar enhancements, but at the cost of a quadratic increase in the computational complexity of the language model, as it skipped a resampler module that compresses large sequence lengths, from the encoder, into a fixed size of query vectors. Recent focus has shifted towards smaller VLMs that can run on hardware-constrained devices. Models like MoE-LLaVA (Lin et al., 2024) or moondream2 (Vikhyat, 2024) achieve impressive performance with a fraction of the parameters of their predecessors. This work further advances small VLMs with Pheye, i.e. a family of compact models that can process high-resolution images with fewer parameters and computational demands, expanding VLM applications to resource-limited environments where understanding fine details is crucial."}, {"title": "2 An Efficient High-Resolution VLM", "content": "This section presents the proposed method for building high-resolution efficient VLMs, starting with architectural design choices, and then analysing the model's computational complexity."}, {"title": "2.1 The Proposed Architecture", "content": "The Pheye architecture is illustrated in Figure 1. It employs a frozen instruction-tuned language model and a vision encoder that adapts a pre-trained CLIP model, linking the two components with dense cross-attention layers inserted before the language model's layers. The use of cross-attention and the design of the vision encoder were informed by preliminary experiments described in Appendix B.\nThe vision encoder consists of a ViT with two sets of LoRA adapters, one for encoding global images and another for local high-resolution patches, as outlined in Appendix B.2. To better distinguish between global and local patch embeddings and improve convergence, we introduce two Layer-Norm (Ba et al., 2016) layers after the ViT, and before adding learned positional embeddings. These layers are applied separately, respectively processing the global and local patch embeddings.\nGiven the computational efficiency, strong task performance, and reduced number of trainable parameters, we use cross-attention to combine modalities while keeping the language model frozen. Inspired by the Flamingo architecture (Alayrac et al., 2022), we replace vanilla cross-attention modules with dense cross-attention modules, inserting them at regular intervals before the decoder layers, as shown in Figure 2. This design was motivated Flamingo's demonstration that gated cross-attention outperforms vanilla cross-attention even when parameter counts are equal. However, we deviate from Flamingo's gated cross-attention layers, as early experiments showed that the gating mechanism hindered convergence. To preserve the language model's initial integrity, we initialize the output matrices of the dense cross-attention modules with values sampled from a normal distribution with a mean of zero and a variance approaching zero. This way, the cross-attention layers have a minor influence during the initial training epochs."}, {"title": "2.2 Analysis of the Computational Complexity", "content": "This section analyzes the computational complexity of the proposed methods, without accounting for the LoRA adapters in the vision encoder, by calculating the number of multiplications that are involved in all linear layers and attention operations, separately considering the vision encoder and the language model with dense cross-attention layers. The analysis assumes a sequential order of operations in all linear layers and attention mechanisms, although implementation optimizations can be latter used. We compare our method with the most widely used approach of using a higher-resolution ViT and a LLaVA-style architecture.\nFor reference, the cost of a matrix multiplication $C$ between matrices $N \\in \\mathbb{R}^{n \\times d}$ and $W \\in \\mathbb{R}^{d \\times o}$ would be $C = (n \\times o) \\times d$, where $N$ represents a matrix of $n$ rows with dimensionality $d$, and $W$ represents a weight matrix with an input dimensionality of $d$ and an output dimensionality of $o$.\nVision Encoder. The computational complexity of a ViT depends on the number of input tokens $N$, corresponding to the number of image patches plus one for the [CLS] token, and the model dimensionality $D$. The number of multiplications for a single Transformer layer can be expressed as:\n$T_{VIT} = 4ND^2 + DN^2 + 8ND^2$.\nThis complexity breaks down into three components, namely (i) the multiplications in the $W_Q$, $W_K$, $W_V$ and $W_O$ matrices, (ii) the attention mechanism, and (iii) a feed-forward module with two layers, where the intermediate dimensionality is four times the model dimensionality.\nIn the case of our strategy, instead of computing self-attention across all the high-resolution input tokens, the image is broken down into $P$ sub-images of equal size and lower-resolution, and the number of multiplications can be expressed by the following equation, where $N'$ is now reflecting the number of patches per sub-image, plus one [CLS] token also per sub-image (i.e., each sub-image involves a total of $N' = \\frac{(N-1)}{P} + 1$ tokens).\n$T_{Pheye} = (4N'D^2 + DN'^2 + 8N'D^2) \\times P$."}, {"title": "3 Main Experimental Evaluation", "content": "This section begins by outlining the Pheye training setup. It then presents the results achieved by our models on academic task-oriented datasets."}, {"title": "3.1 Experimental Setup", "content": "Our experiments aimed to assess the effects of increasing the input image resolution, and to examine the impact of augmenting the frequency of dense cross-attention layers in the language model. To do this, we compare four model settings, varying the image input resolution between 448\u00d7448 pixels and 672\u00d7672 pixels, and adjusting the dense cross-attention interval to every 4 or 2 decoder layers.\nIn all four settings, we used a Phi 1.5 language model (Li et al., 2023c), finetunned on the SlimOrca (Lian et al., 2023) instruction dataset. The ViT is initialized from a CLIP-ViT-H-14 model finetunned on DFN-5B (Fang et al., 2023).\nThe models were trained in separate stages, similarly to MoE-LLaVA (Lin et al., 2024), using a cross-entropy loss over the output tokens. The different stages are described next, while Appendix A summarizes the datasets used in Stage III.\n\u2022 Stage I. We initially aimed for a model that can effectively describe images, including their finer details. We used ShareGPT4V-PT (Chen et al., 2023a), featuring 1,246K images with detailed descriptions, of which 570K are high resolution images from SAM (Kirillov et al., 2023). To reduce overfitting to a particular prompt, ten different captioning instructions were manually generated. One of these instructions was then randomly selected to be associated with each image-description pair.\n\u2022 Stage II. The second stage empowers Pheye to go beyond captioning, using a mixture of complex multi-modal instruction following examples: MIMIC-IT (Li et al., 2023a), LRV (Liu et al., 2023), SVIT (Zhao et al., 2023) and LVIS (Wang et al., 2023). This comes to a total of 964K samples. For each sample with multiple instruction-response turns, a single turn was randomly selected.\n\u2022 Stage III. Previous stages used synthetic GPT-4 (Achiam et al., 2023) and GPT-4V (OpenAI, 2023) data for finetunning, which is naturally prone to noise and hallucinations. To alleviate this, we further finetunned our models on a mixture of academic task-oriented VQA and captioning datasets, as well as some synthetic multimodal instruction following examples, based on LLaVA 1.5-mix-665K. We specifically considered ST-VQA"}, {"title": "3.3 Assessing the Use of Fine Image Details", "content": "In order to further evaluate the impact that increasing the input image resolution has on tasks that involve the comprehension of fine-grained details, we followed a strategy similar to that of Zhang et al. (2023a) and partitioned the TextVQA validation set into three groups of approximately equal size, based on the relative size of the ground-truth bounding box $S = \\frac{A_{bb}}{A_{total}}$, where $A_{bb}$ denotes the area of the answer bounding box, and $A_{total}$ denotes the total area of the image. Specifically, we divided the data into three tertiles: the bottom tertile (finer details), the middle tertile (medium), and the top tertile (broader entities). The selection of the ground-truth bounding box was based on the average string similarity with all the ground truth answers, using the longest contiguous matching subsequence algorithm.\nWe also applied the same approach to a randomly sampled subset of the VQAv2 validation split, resulting in 10,000 questions pertaining to 8,453 images. For this dataset we used the segmentation area of objects as opposed to bounding boxes, since this quantity can better represent an object's area in the image, and used its category name for the string similarity algorithm. Since a large portion of VQAv2 answers correspond to yes/no or a number, in these cases we applied the same algorithm to the question, instead of the ground truth answers.\nIn both datasets, increasing resolution seems to lead to a greater improvement in accuracy for the samples that require understanding finer details within the image. This is particularly evident in TextVQA, as tertiles corresponding to smaller image-question-answer triplets have a greater relative improvement in performance, in comparison with larger tertiles. In VQAv2, however, this pattern is not as consistent, likely due to the less frequent appearance of category names for each object in questions and answers, compared to OCR tokens in TextVQA."}, {"title": "4 Conclusions", "content": "We presented an approach for building efficient Vision-Language Models (VLMs), processing high-resolution images while maintaining parameter efficiency. The approach was used to develop the Pheye family of VLMs, which achieve a high effectiveness on various academic task-oriented datasets, surpassing other generalist models of similar size, and achieving particularly strong results on tasks that involve understanding scene-text.\nFuture work directions include investigating the use of different vision encoders, for instance incorporating strategies to process images at close to native resolutions and aspect ratios, e.g. building upon the approach proposed by Dehghani et al. (2024). We can also explore ways to increase the amount of training data, given that our method still requires training hundreds of millions of randomly initialized parameters. To address this, we could generate additional task-specific synthetic data, particularly for tasks that involve scene-text, where the amount of available human generated data is smaller. Building on the work of Zhang et al. (2023a), we could generate VQA examples that focus on finer image details, which have been shown to be crucial for performance."}, {"title": "Limitations and Ethical Considerations", "content": "While our work does not raise new ethical issues within the domain of vision-language models (e.g., we conducted our experiments on public datasets, carefully designed for academic research and extensively used in previous studies), there are some general important concerns.\nVision-Language Models (VLMs) are, for instance, notorious for their internal biases, inherited from the training data itself or from the use of pre-trained models such as CLIP. We therefore recommend caution in the use of the approach proposed in this paper, and anticipate further research into model biases, before relying on our work beyond research environments.\nAnother important limitation in the work reported on this paper concerns the fact that our experiments relied exclusively on English datasets. Multilingual models have shown potential in leveraging diverse datasets and providing more robust and versatile language understanding capabilities, which could be beneficial for creating VLMs that can handle a wider variety of tasks and languages. Future work can perhaps explore the use of efficient multilingual models like Qwen (Bai et al., 2023) to enhance our approach, although additional efforts would be required in the design of an effective mixture of multilingual data for training."}, {"title": "Acknowledgements", "content": "This research was supported by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (i.e., the Center For Responsible AI), and also by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia (FCT), specifically through the project with reference UIDB/50021/2020 (DOI: 10.54499/UIDB/50021/2020)."}, {"title": "B.1 Vision-Language Model Architectures", "content": "One initial experiment evaluated three different alternatives for combining the vision and language modalities, taking inspiration from the FROMAGe (Koh et al., 2023), LLaVA, and SmallCap (Ramos et al., 2023) neural architectures.\nBuilding upon FROMAGe, our first approach involves transforming an image into a set of visual embeddings using a pre-trained ViT. The resulting visual inputs are then summarized through the extraction of the [CLS] token, which is subsequently mapped to the language model's dimensionality via a linear transformation. In contrast, the second architecture (i.e., the one inspired by LLaVA) leverages all the embeddings generated by the ViT to represent an image, rather than relying solely on the [CLS] token. In both cases, the language"}, {"title": "B.2 Encoding High Resolution Images", "content": "Previous attempts at increasing the input resolution of VLMs mostly chose to finetune the vision encoder on higher resolution images (Chen et al., 2023b), which is costly due to the quadratic computational complexity of the attention mechanism, and which also requires large amounts of image-text data. To overcome this issue, we experimented with an approach similar to that of Li et al. (2023d), in which we scale up the input resolution by splitting the image into smaller patches that match the resolution of the vision encoder, afterwards encoding the smaller sub-images individually. To provide the model with global context, we also encode the full image as normal, concatenate all feature maps, and use the result as our image representation.\nSince the smaller image patches can have a different distribution than that of the images in which the ViT was trained on, we introduce two sets of LORA adapters to the vision encoder. One is used on the global image and the other is used on the smaller local patches, allowing each resolution to specialize on different size aspects of the input image. The adapters were inserted at every linear layer of the ViT with the following hyperparame-"}, {"title": "C Assessing Changes in Cross-Attention According to Different Path Sizes", "content": "This appendix further analyses the use of high-resolution inputs, assessing the reliance of the model on the global versus local sub-images.\nTo investigate how the model uses the different patches in the cross-attention module, we calculated the average of the attention scores for the generated captions across all TextCaps validation set images. Specifically we compute the average of the cross-modal attention scores across the global image tokens and the local patches tokens, at each step of generating caption tokens and across all attention heads."}]}