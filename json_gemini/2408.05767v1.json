{"title": "Reference-free Hallucination Detection for Large Vision-Language Models", "authors": ["Qing Li", "Chenyang Lyu", "Jiahui Geng", "Derui Zhu", "Maxim Panov", "Fakhri Karray"], "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in language understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several methods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and depend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effectiveness of different reference-free solutions in detecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency-based, and supervised uncertainty quantification methods on four representative LVLMs across two different tasks. The empirical results show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncertainty quantification method outperforming the others, achieving the best performance across different settings.", "sections": [{"title": "Introduction", "content": "Large vision-language models (LVLMs) like LLaVA (Liu et al., 2023, 2024b), MiniGPT-4 (Zhu et al., 2024b) have demonstrated remarkable capabilities in understanding and generating complex visual and textual content. However, an emerging concern with these models is their tendency towards hallucination (Li et al., 2023; Zhou et al., 2024). For instance, a model might describe an object or event in the generated text that is not present in the input image. This phenomenon poses challenges for the reliability of LVLMs, highlighting the intrinsic limitations of current models in main-taining coherence between visual inputs and textual descriptions (Huang et al., 2023a; Liu et al., 2024a). Most existing methods of assessing hallucinations rely on external models. Li et al. (2023) proposed POPE, which requires automated segmentation tools like SEEM (Zou et al., 2024) to identify present and absent objects in images. Faithscore (Jing et al., 2023) utilizes a visual entailment model (Xie et al., 2019) to validate the factuality, focusing on object attributes and their relationships. Nonetheless, these approaches increase the computational cost and are restricted by the capabilities of these models. Reference-free methods are a promising approach to address this issue, primarily utilizing the model's own knowledge. They have attracted great research interest in Large Language Models (LLMs), with typical methods including the uncertainty-based methods (Hu et al., 2023; Geng et al., 2024; Huang et al., 2023b; Vazhentsev et al., 2023; Fadeeva et al., 2023), consistency-based methods (Manakul et al., 2023) and supervised uncertainty quantification (SUQ) approach (Chen et al., 2023; Azaria and Mitchell, 2023). However, their potential in the context of LVLMs is unknown. They have not yet been systematically studied, possibly due to the complexity of multimodal data. We are committed to bridging this gap. In this work, we collect and implement a battery of state-of-the-art methods, including four metrics of uncertainty-based methods, four variants of consistency-based methods, and the supervised uncertainty quantification method. The experiments are conducted on five LVLMs with different types, versions, and sizes on Yes-and-No and Open-ended tasks. Our findings show that the SUQ method outperforms other approaches in hallucination detection, both at the sentence and passage levels. The main contributions of this paper are summarized as follows:\n1. We comprehensively measure the performance of different reference-free approaches in detecting hallucination.\n2. We demonstrate that the supervised uncertainty quantification (SUQ) method performs best across various settings.\n3. We contribute the Image-Hallucination Annotation Dataset (IHAD), a manually annotated, sentence-level dataset created by prompting LLaVA-v1.5-7b."}, {"title": "Reference-free Hallucination Detection Methods", "content": "Uncertainty-based methods have the hypothesis that when LVLMs are uncertain about generated information, generated tokens often have higher uncertainty, shown in Figure 1(b). Uncertainty-based methods are extensively studied for hallucination detection in LLMs (Hu et al., 2023; Geng et al., 2024; Huang et al., 2023b; Fadeeva et al., 2023, 2024), and are also used as benchmark techniques (Zhu et al., 2024a; Manakul et al., 2023). We adopt the four proposed metrics: AvgProb, AvgEnt, MaxProb, MaxEnt to aggregate token-level uncertainty to measure sentence-level and passage-level uncertainty. Appendix A.1 provides detailed descriptions of these metrics.\nConsistency-based methods mainly include self-consistency, cross-question consistency, and cross-model consistency (Zhang et al., 2023). In this work, we focus on four variants \u2013 BERTScore, Question Answering (QA), Unigram, Natural Language Inference (NLI) \u2013 of the self-consistency method proposed in (Manakul et al., 2023) to detect non-factual information for LVLMs, as demonstrated in Figure 1(c). The hallucination score for the i-th sentence, denoted by S(i), ranges from 0.0 to 1.0, where a score close to 0.0 indicates the sentence is accurate, and a score near 1.0 suggests the sentence is hallucinated. More detailed information can be found in Appendix A.2.\nSupervised uncertainty quantification (SUQ). \u03a4\u03bf analyze and understand these internal states, SUQ method (Chen et al., 2023; Azaria and Mitchell, 2023) shown in Figure 1(a) train a classifier, referred to as a probe, on a dataset containing labeled examples. This classifier outputs the likelihood of a statement being truthful by analyzing the hidden layer activations of the LVLM during the reading or generation of states."}, {"title": "Study Design", "content": "Dataset. Our experiments are conducted on public datasets: POPE (Li et al., 2023), GQA (Hudson and Manning, 2019), and M-HalDetect (Gunjal et al., 2024). POPE contains three subsets: random, popular, and adversarial. For simplicity, we refer to them as Ran, Pop, and Adv, and we also abbreviate M-HalDetect as M-Hal. In addition, we create a manually annotated, sentence-level dataset, IHAD, by using LLaVA-v1.5-7b to generate concise image descriptions for 500 images from MSCOCO (Lin et al., 2014) using prompt \"Provide a concise description of the given image\". Ran, Pop, Adv, and a subset of GQA used in this work evaluate object hallucination as a binary classification task, prompt-ing LVLMs to output \"Yes\" or \"No\", categorizing them as Yes-or-No tasks. In contrast, M-Hal and IHAD involve long text answers based on open-ended questions, making them Open-ended tasks. More detailed dataset information is shown in Appendix B.1.\nModels. We evaluate five representative categories of LVLMs: MiniGPT-4v (Zhu et al., 2024b), LLaVA-v1.5-7b (Liu et al., 2023), LLaVA-v1.6-vicuna-7b, LLaVA-v1.6-mistral-7b (Liu et al., 2024b) and LLaVA-v1.6-vicuna-13b. These models, featuring different architectures and strong performance, are used as benchmarks in various works (Huang et al., 2023a; Zhou et al., 2024). We only test models of a 7-billion and 13-billion size due to the limited computational resources.\nExperimental Setup. We combine questions and responses to create inputs for test models. If the response is from this test model, it is considered self-generated input; otherwise, it is regarded as hand-crafted input for this model. Inputs are then processed by test models, from which we extract"}, {"title": "Experimental Results and Analysis", "content": "The comprehensive results on Open-ended tasks are presented in Tables 1 and 2. The numbers in the pink and yellow background, respectively, represent the best results for the uncertainty-based methods and consistency-based methods, while the numbers in bold indicate the overall best results. We utilize AUC-PR (Davis and Goadrich, 2006), which stands for the area under the precision-recall curve, to objectively evaluate the effectiveness of each model. The higher the value of AUC-PR, the stronger the ability of this method for hallucination detection."}, {"title": "Effectiveness of Various Methods", "content": "Sentence-level Hallucination Detection. Based on the data in Table 1, we find that in most instances, the consistency-based approaches surpasses the uncertainty-based metrics, yet they remain significantly inferior to the SUQ method by approximately 15%. Among consistency-based methods, NLI outperforms best while Unigram shows the least effectiveness in most cases. Specifically, on the M-Hal dataset, the NLI performance of LLaVA-v1.6-7b exceeds the highest performing uncertainty metric, MaxEnt, by approximately 12%. BERTScore and QA outperform the uncertain estimation in most setups.\nPassage-level Hallucination Detection. We calculate the metrics of uncertainty-based methods across all tokens in a passage and take the embedding information of the passage's last token for SUQ. Consistency-based methods are not used to detect non-factual information in passages as the heavy computation. Our results in Table 2 show that the SUQ method is still better than the uncertainty-based methods, including probability and entropy methods. Besides, uncertainty-based methods are weakly better than the random baseline at passage-level.\nThe performance effects of the three types of methods on Yes-or-No tasks align with those on Open-ended tasks. Detailed results are demonstrated in Appendix C.1."}, {"title": "Robustness and Limitations of SUQ Methods.", "content": "To investigate the robustness of SUQ methods, we employ two classifiers to assess the impact on each dataset. Each dataset is split into a training set and a test set in a 3:1 ratio. One classifier is trained on the Pop training set, while the other is trained on the training set of each dataset individually. Figure 4 illustrates that the classifier trained on Pop exhibits comparable performance to classifiers trained on separated datasets, indicating the robustness of the SUQ method.\nAlthough our experiments confirmed the effectiveness of the SUQ method, it was found to be ineffective in detecting subtle, manually crafted hallucinations. For example, the difference between a correct statement, \u201cThere is a red apple and a cute kitten.\" and an incorrect one, \"There is a red orange and a cute kitten.\" has a negligible effect on the model's hidden state at the final position, posing a challenge for SUQ methods to differentiate between them."}, {"title": "Impact of Image Clarity on Hallucination Detection Methods", "content": "To explore this question, we employ the Gaussian blur technique, which smooths the image by assigning a weighted average to the surrounding pixels of each pixel, resulting in an obscure picture. We set radius = 10 in our experiments. Figure 3 shows the effectiveness of AvgProb, SUQ, and NLI in blurred images are significantly reduced compared to clear images. The phenomenon suggests that enhancing image clarity is a crucial strategy for boosting the effectiveness of hallucination detection methods."}, {"title": "Impact of Data Source on various Methods", "content": "Reference-free methods, particularly uncertainty-based and SUQ methods, rely on the internal information of models. A natural question is whether these methods are affected by data sources. M-Hal and IHAD are hand-crafted data and self-generated data, respectively, for LLaVA-v1.5-7b. Therefore, we compare various detection methods on M-Hal and IHAD based on LLaVA-v1.5-7b. Specifically, we calculate \u0394AUC-PR\n\u0394AUC-PR = AUC-PR - AUC-PR,\nwhere i refers to M-Hal and IHAD, j denotes different approaches."}, {"title": "Conclusion", "content": "This paper systematically studies reference-free approaches for detecting the hallucination of LVLMs. Our results indicate that the SUQ method performs best as a supervised method, but it relies on sufficient training data, which could limit its applicability in certain scenarios. Consistency-based methods outperform uncertainty-based methods; however, they require more computation and are more suitable for black-box models. Uncertainty-based methods are particularly well-suited for tasks similar to Yes-or-No questions. We call for more accurate and robust methods."}, {"title": "Limitations", "content": "Three major limitations are identified in this work.\nFirst, we did not compare with reference-based methods. The performance of reference-based methods depends on the performance of external models and is specific to certain tasks. In this article, we primarily aim to explore the potential of reference-free methods, which have a broader range of applications.\nSecond, we did not classify types of hallucinations; limited by resources, even with fine-grained annotations, we only marked whether sentences contained hallucinations without distinguishing types such as object existence errors, attribute errors, text recognition errors, and fact-conflicting errors. This might be where SUQ methods have an advantage.\nThird, we only studied white-box models. Recent research indicates that white-box models can be proxies for black-box models to calculate their confidence regarding specific issues. Unigram method is one such proxy model approach. In the future, we could use more complex models, closer in architecture to the target models, as proxies to explore their potential in hallucination detection."}, {"title": "Ethics and Broader Impact", "content": "We sampled a portion of the data from existing datasets for our experiments, which may affect the accuracy of some of our conclusions."}, {"title": "Reference-free Hallucination Detection Methods", "content": "We collect uncertainty-based and consistency-based methods proposed in (Manakul et al., 2023) to test the effectiveness on LVLMs."}, {"title": "Uncertainty-based methods", "content": "To aggregate the uncertainty information obtained at the token level, we employ four metrics to aggregate token-level uncertainty into sentence level. In particular, a sentence-level uncertainty score can be obtained by taking either the maximum or average of the negative loglikelihood \u2013 $\\log p_{ij}$ in a sentence:\n$\\text{MaxProb}(i) = \\max_{j} (-\\log p_{ij}),$\n$\\text{AvgProb}(i) = \\frac{1}{J_i} \\sum_{j=1}^{J_i} (-\\log p_{ij}),$\nwhere $p_{ij}$ is the probability of a token at a position $j$ in the sentence $i$ and $J_i$ is the total number of tokens in the considered sentence. Additionally, one can also replace the negative loglikelihood \u2013 $\\log p_{ij}$ with the entropy $H_{ij}$:\n$\\text{MaxEnt}(i) = \\max_{j} H_{ij},$\n$\\text{AvgEnt}(i) = \\frac{1}{J_i} \\sum_{j=1}^{J_i} H_{ij},$\nwhere $H_{ij}$ is the entropy of the token distribution for the $j$-th token in the sentence $i$."}, {"title": "Consistency-based Methods", "content": "$\\text{BERTScore}$ denoted as $S_{\\text{BERT}}$, finds the average BERTScore of the considered sentence with the most similar sentence from each drawn sample. Let $B(., .)$ denote the BERTScore between two sentences. $S_{\\text{BERT}}$ is computed by $B(., .)$ as:\n$S_{\\text{BERT}}(i) = 1 - \\frac{1}{N} \\sum_{n=1}^N \\max_k B(r_i, s_k),$ where $r_i$ represents the $i$-th sentence in main response $R$, $s_k$ represents the $k$-th sentence in the $n$-th sample $S_n$ and $N$ is the total number of sampled sentences.\nQuestion Answering (QA). The multiple-choice question answering generation (MQAG) evaluates consistency by creating multiple-choice questions from the primary generated response and answering them using other sampled responses as reference. MQAG consists of two stages: question generation $G$ and question answering $A$. For the sentence $r_i$ in the response $R$, we draw questions $q$ and options $o$:\n$q,o \\sim P_G(q, o | r_i, R).$\nThe answering stage A selects the answers:\n$a_R = \\arg \\max_k[P_A(o_k | q, R, 0)],$\n$a_{S^n} = \\arg \\max_k[P_A(o_k | q, S^n, 0)].$\nWe compare whether $a_R$ is equal to $a_{S^n}$ for each sample in {$S1,...,SN$}, yielding $N_m$ matches and $N_n$ not-matches. A simple inconsistency score for the i-th sentence and question q based on the match/not-match counts is defined: $S_{QA}(i,q) = \\frac{N_n}{N_n+N_m}$. Manakul et al. (2023) modify the inconsistency score $S_{QA}(i, q)$ by incorporating the answerability of generated questions. Finally, $S_{QA}(i)$ is the average of inconsistency scores across q,\n$S_{QA}(i) = \\mathbb{E}_q[S_{QA}(i, q)]$"}, {"title": "Dataset", "content": "POPE (Li et al., 2023) formulates the evaluation of object hallucination as a binary classification task that prompts LVLMs to output \u201cYes\u201d or \u201cNo\u201d, e.g., \"Is there a chair in the figure?\" POPE adopts three sampling settings to construct negative samples: random, popular, and adversarial. The random setting selects non-present objects at random. In contrast, the popular setting chooses from a list of frequently occurring but absent objects, and the adversarial method selects based on common co-occurrence in contexts despite the absence in the target image.\nGQA (Hudson and Manning, 2019) is proposed to detect hallucinations, including existence, attribute, and relation. It contains not only Yes-or-No problems but also Open-ended problems. We randomly 9050 some binary problems for 397 images used in our experiments.\nM-HalDetect (Gunjal et al., 2024) consists of 16k fine-grained annotations on visual question-answering examples, including object hallucination, descriptions, and relationships. We utilize about 3000 questions on 764 figures for our Open-ended tasks, averaging 4 sentences per response. IHAD severs as the evaluation of description for Open-ended tasks. It is a sentence-level dataset created by prompting LLaVA-V1.5-7b to generate concise image descriptions for 500 images from MSCOCO (Lin et al., 2014). On average, each response contains 5 sentences. The creation of IHAD aims to investigate if the data source impacts the internal information of LVLMs and further impacts hallucination detection methods, which rely on this internal information."}, {"title": "Experimental Setup", "content": "In our experiments, we merge questions and responses from humans or other models to create hand-crafted input. This input is then processed by the evaluation model, from which we extract internal information, including probability, entropy, and embedding states. The evaluation model, capable of recognizing hallucinations (Duan et al., 2024), can assign reasonable internal information to responses not generated by itself. As shown in Figure 2, when generating the \"cat\" token, the evaluation model assigns probabilities to all tokens and selects the higher probability token for output. Specifically, the probability assigned to \u201ccat\u201d in self-generated input is 0.9, while in hand-crafted input, \"dog\" receives a probability of 0.1. POPE, GQA, and M-Hal are hand-crafted data for all four evaluation models used in this work. IHAD's response, however, is generated by LLaVA-v1.5-7b, making IHAD self-generated data specifically for LLaVA-v1.5-7b. For the other three models, IHAD is considered hand-crafted data.\nIn the Yes-or-No tasks, we simplify the original responses to \u201cYes\u201d for positive answers (e.g.,"}, {"title": "Unigram", "content": "The concept behind Unigram is to develop a new model that approximates the LVLMs by samples {S\u00b9,..., SN } and get the LVLM's token probabilities using this model. As N increases, the new model gets closer to LVLMs. Due to time and cost constraints, we just train a simple n-gram model using the samples {S\u00b9, . . ., SN } as well as the main response R. We then compare the average and maximum of the negative probabilities of the sentence in response R using the following equations:\n$S_{\\text{Avg}}^{\\text{n-gram}}(i) = \\frac{1}{J_i} \\sum_{j=1}^{J_i} (-\\log p_{ij}),$\n$S_{\\text{Max}}^{\\text{n-gram}}(i) = \\max_j (\u2013 \\log p_{ij}),$\nwhere $p_{ij}$ is the probability of a token at position $j$ of a sentence $i$."}, {"title": "Natural Language Inference (NLI)", "content": "determines whether a hypothesis follows a premise, classified into either entailment/neutral/contradiction. In this work, we use DeBERTa-v3-large (He et al., 2023) fine-tuned to MNLI as the NLI model. The input for NLI classifiers is typically the premise concatenated to the hypothesis, which for NLI is the sampled passage $S^n$ concatenated to the sentence to be assessed $r_i$ in the response $R$. Only the logits associated with the 'entailment' and 'contradiction' classes are considered,\nP(\\text{contradict} | r_i, S^n) = \\frac{\\exp(z_{\\text{in}})}{\\exp(z_{\\text{en}}) + \\exp(z_{\\text{cn}})},$\nwhere $z_{\\text{en}} = z_{\\text{en}}(r_i, S^n)$ and $z_{\\text{in}} = z_{\\text{in}}(r_i, S^n)$ are the logits of the 'entailment' and 'contradiction' classes. NLI score for sentence $r_i$ on samples {$S1, ..., SN$} is then defined as,\n$S_{\\text{NLI}}(i) = \\frac{1}{N} \\sum_{n=1}^N P(\\text{contradict} | r_i, S^n)$"}, {"title": "Yes-or-No Tasks", "content": "We only compare the uncertainty-based and SUQ methods on Yes-or-No tasks because the model's answers exclusively contain \u201cYes\u201d or \u201cNo\u201d. The random baseline is calculated as the ratio of non-factual examples to the total number of examples. Our results in Table 3 demonstrate that SUQ method outperforms the uncertainty-based methods, exceeding AvgProb by about 4.5%. AvgProb achieves AUC-PR values nearly double those of the random baseline across all datasets and models, indicating it is an efficient method since it is an unsupervised method. In addition, the probability Prob overall performs better than the entropy Ent in Yes-or-No tasks."}, {"title": "Impact of Unimportant Tokens for Uncertainty-based Methods", "content": "It is observed that the performance of uncertainty methods on the Open-ended tasks is not as good as on the Yes-or-No tasks. To explore the reasons, we append a period, resulting in \u201cYes.\u201d or \u201cNo.\u201d in the Yes-or-No task. In this case, probability p and entropy H of two tokens need to be considered."}, {"title": "Optimal Hidden Layers for Effective SUQ Method", "content": "To explore which hidden layer information should be used, we perform 3D TSNE visualizations of the embedding states of LLaVA at the fifth and last layers for popular tasks. These visualizations show that at the last layer, there are distinct separations between the representations of different labels. Conversely, at the fifth layer, representations of different labels are relatively close and almost blend together, indicating that the shallow layers primarily preserve language-agnostic features. Therefore, we use the last hidden layer to explore the SUQ method in our experiments."}, {"title": "The comparison of clear and blurred images.", "content": "The comparison of clear and blurred images is illustrated in Figure 8."}, {"title": "The performance of different methods in LLaVA-v1.6-mistral-7b.", "content": "Figure 9 visually presents the performance of different methods in LLaVA-v1.6-mistral-7b on IHAD."}]}