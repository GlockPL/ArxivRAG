{"title": "Reference-free Hallucination Detection\nfor Large Vision-Language Models", "authors": ["Qing Li", "Chenyang Lyu", "Jiahui Geng", "Derui Zhu", "Maxim Panov", "Fakhri Karray"], "abstract": "Large vision-language models (LVLMs) have\nmade significant progress in recent years.\nWhile LVLMs exhibit excellent ability in lan-\nguage understanding, question answering, and\nconversations of visual inputs, they are prone to\nproducing hallucinations. While several meth-\nods are proposed to evaluate the hallucinations\nin LVLMs, most are reference-based and de-\npend on external tools, which complicates their\npractical application. To assess the viability of\nalternative methods, it is critical to understand\nwhether the reference-free approaches, which\ndo not rely on any external tools, can efficiently\ndetect hallucinations. Therefore, we initiate an\nexploratory study to demonstrate the effective-\nness of different reference-free solutions in de-\ntecting hallucinations in LVLMs. In particular,\nwe conduct an extensive study on three kinds\nof techniques: uncertainty-based, consistency-\nbased, and supervised uncertainty quantifica-\ntion methods on four representative LVLMs\nacross two different tasks. The empirical re-\nsults show that the reference-free approaches\nare capable of effectively detecting non-factual\nresponses in LVLMs, with the supervised uncer-\ntainty quantification method outperforming the\nothers, achieving the best performance across\ndifferent settings.", "sections": [{"title": "1 Introduction", "content": "Large vision-language models (LVLMs) like\nLLaVA (Liu et al., 2023, 2024b), MiniGPT-4 (Zhu\net al., 2024b) have demonstrated remarkable capa-\nbilities in understanding and generating complex\nvisual and textual content. However, an emerg-\ning concern with these models is their tendency\ntowards hallucination (Li et al., 2023; Zhou et al.,\n2024). For instance, a model might describe an ob-\nject or event in the generated text that is not present\nin the input image. This phenomenon poses chal-\nlenges for the reliability of LVLMs, highlighting\nthe intrinsic limitations of current models in main-\ntaining coherence between visual inputs and tex-\ntual descriptions (Huang et al., 2023a; Liu et al.,\n2024a).\nMost existing methods of assessing hallucina-\ntions rely on external models. Li et al. (2023)\nproposed POPE, which requires automated seg-\nmentation tools like SEEM (Zou et al., 2024) to\nidentify present and absent objects in images. Faith-\nscore (Jing et al., 2023) utilizes a visual entailment\nmodel (Xie et al., 2019) to validate the factuality,\nfocusing on object attributes and their relationships.\nNonetheless, these approaches increase the compu-\ntational cost and are restricted by the capabilities\nof these models.\nReference-free methods are a promising ap-\nproach to address this issue, primarily utilizing\nthe model's own knowledge. They have attracted\ngreat research interest in Large Language Mod-\nels (LLMs), with typical methods including the\nuncertainty-based methods (Hu et al., 2023; Geng\net al., 2024; Huang et al., 2023b; Vazhentsev et al.,\n2023; Fadeeva et al., 2023), consistency-based\nmethods (Manakul et al., 2023) and supervised\nuncertainty quantification (SUQ) approach (Chen\net al., 2023; Azaria and Mitchell, 2023). However,\ntheir potential in the context of LVLMs is unknown.\nThey have not yet been systematically studied, pos-\nsibly due to the complexity of multimodal data. We\nare committed to bridging this gap.\nIn this work, we collect and implement a bat-\ntery of state-of-the-art methods, including four met-\nrics of uncertainty-based methods, four variants of\nconsistency-based methods, and the supervised un-\ncertainty quantification method. The experiments\nare conducted on five LVLMs with different types,\nversions, and sizes on Yes-and-No and Open-ended\ntasks. Our findings show that the SUQ method\noutperforms other approaches in hallucination de-\ntection, both at the sentence and passage levels.\nThe main contributions of this paper are summa-\nrized as follows:\n1. We comprehensively measure the perfor-\nmance of different reference-free approaches\nin detecting hallucination.\n2. We demonstrate that the supervised uncer-\ntainty quantification (SUQ) method performs\nbest across various settings.\n3. We contribute the Image-Hallucination Anno-\ntation Dataset (IHAD), a manually annotated,\nsentence-level dataset created by prompting\nLLaVA-v1.5-7b."}, {"title": "2 Reference-free Hallucination Detection\nMethods", "content": "Uncertainty-based methods have the hypothesis\nthat when LVLMs are uncertain about generated\ninformation, generated tokens often have higher\nuncertainty, shown in Figure 1(b). Uncertainty-\nbased methods are extensively studied for halluci-\nnation detection in LLMs (Hu et al., 2023; Geng\net al., 2024; Huang et al., 2023b; Fadeeva et al.,\n2023, 2024), and are also used as benchmark tech-\nniques (Zhu et al., 2024a; Manakul et al., 2023).\nWe adopt the four proposed metrics: AvgProb,\nAvgEnt, MaxProb, MaxEnt to aggregate token-\nlevel uncertainty to measure sentence-level and\npassage-level uncertainty. Appendix A.1 provides\ndetailed descriptions of these metrics.\nConsistency-based methods mainly include self-\nconsistency, cross-question consistency, and cross-\nmodel consistency (Zhang et al., 2023). In this\nwork, we focus on four variants BERTScore,\nQuestion Answering (QA), Unigram, Natural Lan-\nguage Inference (NLI) \u2013 of the self-consistency\nmethod proposed in (Manakul et al., 2023) to de-\ntect non-factual information for LVLMs, as demon-\nstrated in Figure 1(c). The hallucination score for\nthe i-th sentence, denoted by S(i), ranges from 0.0\nto 1.0, where a score close to 0.0 indicates the sen-\ntence is accurate, and a score near 1.0 suggests the\nsentence is hallucinated. More detailed information\ncan be found in Appendix A.2.\nSupervised uncertainty quantification (SUQ). \u03a4\u03bf\nanalyze and understand these internal states, SUQ\nmethod (Chen et al., 2023; Azaria and Mitchell,\n2023) shown in Figure 1(a) train a classifier, re-\nferred to as a probe, on a dataset containing labeled\nexamples. This classifier outputs the likelihood of\na statement being truthful by analyzing the hidden\nlayer activations of the LVLM during the reading\nor generation of states."}, {"title": "3 Study Design", "content": "Dataset. Our experiments are conducted on public\ndatasets: POPE (Li et al., 2023), GQA (Hudson and\nManning, 2019), and M-HalDetect (Gunjal et al.,\n2024). POPE contains three subsets: random, pop-\nular, and adversarial. For simplicity, we refer to\nthem as Ran, Pop, and Adv, and we also abbreviate\nM-HalDetect as M-Hal. In addition, we create a\nmanually annotated, sentence-level dataset, IHAD,\nby using LLaVA-v1.5-7b to generate concise image\ndescriptions for 500 images from MSCOCO (Lin\net al., 2014) using prompt \"Provide a concise de-\nscription of the given image\". Ran, Pop, Adv, and\na subset of GQA used in this work evaluate object\nhallucination as a binary classification task, prompt-"}, {"title": "4 Experimental Results and Analysis", "content": "The comprehensive results on Open-ended tasks\nare presented in Tables 1 and 2. The numbers in\nthe pink and yellow background, respectively, rep-\nresent the best results for the uncertainty-based\nmethods and consistency-based methods, while the\nnumbers in bold indicate the overall best results.\nWe utilize AUC-PR (Davis and Goadrich, 2006),\nwhich stands for the area under the precision-recall\ncurve, to objectively evaluate the effectiveness of\neach model. The higher the value of AUC-PR, the\nstronger the ability of this method for hallucination\ndetection."}, {"title": "4.1 Effectiveness of Various Methods", "content": "Sentence-level Hallucination Detection. Based\non the data in Table 1, we find that in most in-\nstances, the consistency-based approaches sur-\npasses the uncertainty-based metrics, yet they re-\nmain significantly inferior to the SUQ method by\napproximately 15%. Among consistency-based\nmethods, NLI outperforms best while Unigram\nshows the least effectiveness in most cases. Specif-\nically, on the M-Hal dataset, the NLI performance\nof LLaVA-v1.6-7b exceeds the highest perform-\ning uncertainty metric, MaxEnt, by approximately\n12%. BERTScore and QA outperform the uncertain\nestimation in most setups."}, {"title": "Passage-level Hallucination Detection", "content": "We cal\nculate the metrics of uncertainty-based methods\nacross all tokens in a passage and take the em-\nbedding information of the passage's last token\nfor SUQ. Consistency-based methods are not used\nto detect non-factual information in passages as\nthe heavy computation. Our results in Table 2\nshow that the SUQ method is still better than the\nuncertainty-based methods, including probability\nand entropy methods. Besides, uncertainty-based\nmethods are weakly better than the random base-\nline at passage-level.\nThe performance effects of the three types of\nmethods on Yes-or-No tasks align with those on\nOpen-ended tasks. Detailed results are demon-\nstrated in Appendix C.1."}, {"title": "4.2 Robustness and Limitations of SUQ\nMethods.", "content": "To investigate the robustness of SUQ methods, we\nemploy two classifiers to assess the impact on each\ndataset. Each dataset is split into a training set and a\ntest set in a 3:1 ratio. One classifier is trained on the\nPop training set, while the other is trained on the\ntraining set of each dataset individually. Figure 4\nillustrates that the classifier trained on Pop exhibits\ncomparable performance to classifiers trained on\nseparated datasets, indicating the robustness of the\nSUQ method.\nAlthough our experiments confirmed the effec-\ntiveness of the SUQ method, it was found to be\nineffective in detecting subtle, manually crafted hal-\nlucinations. For example, the difference between\na correct statement, \u201cThere is a red apple and a\ncute kitten.\" and an incorrect one, \"There is a red\norange and a cute kitten.\" has a negligible effect on\nthe model's hidden state at the final position, pos-\ning a challenge for SUQ methods to differentiate\nbetween them."}, {"title": "4.3 Impact of Image Clarity on Hallucination\nDetection Methods", "content": "To explore this question, we employ the Gaussian\nblur technique, which smooths the image by assign-\ning a weighted average to the surrounding pixels of\neach pixel, resulting in an obscure picture. We set\nradius = 10 in our experiments. Figure 3 shows the\neffectiveness of AvgProb, SUQ, and NLI in blurred\nimages are significantly reduced compared to clear\nimages. The phenomenon suggests that enhancing\nimage clarity is a crucial strategy for boosting the\neffectiveness of hallucination detection methods."}, {"title": "4.4 Impact of Data Source on various\nMethods", "content": "Reference-free methods, particularly uncertain-\nbased and SUQ methods, rely on the internal infor-\nmation of models. A natural question is whether\nthese methods are affected by data sources. M-Hal\nand IHAD are hand-crafted data and self-generated\ndata, respectively, for LLaVA-v1.5-7b. Therefore,\nwe compare various detection methods on M-Hal\nand IHAD based on LLaVA-v1.5-7b. Specifically,\nwe calculate AAUC-PR\n$\\triangle AUC\\text{-}PR_{i,j} = AUC\\text{-}PR_{i,j} \u2013 AUC\\text{-}PR_{i,baseline}$,\nwhere i refers to M-Hal and IHAD, j denotes dif-\nferent approaches.\nFigure 5 shows that in uncertainty-based ap-\nproaches, some metrics outperform in M-Hal while\nothers excel in IHAD, a trend also observed in\nconsistency-based techniques. This phenomenon\nillustrates that the reference-free hallucination de-\ntection methods are insensitive to the data source.\nOthers. We have also explored the impact of unim-\nportant tokens for uncertainty-based methods and\nthe optimal hidden layers for SUQ. They are dis-\ncussed respectively in Appendices C.2 and C.3."}, {"title": "5 Conclusion", "content": "This paper systematically studies reference-free ap-\nproaches for detecting the hallucination of LVLMs.\nOur results indicate that the SUQ method performs\nbest as a supervised method, but it relies on suf-\nficient training data, which could limit its appli-\ncability in certain scenarios. Consistency-based\nmethods outperform uncertainty-based methods;\nhowever, they require more computation and are\nmore suitable for black-box models. Uncertainty-\nbased methods are particularly well-suited for tasks\nsimilar to Yes-or-No questions. We call for more\naccurate and robust methods."}, {"title": "Limitations", "content": "Three major limitations are identified in this work.\nFirst, we did not compare with reference-based\nmethods. The performance of reference-based\nmethods depends on the performance of external\nmodels and is specific to certain tasks. In this ar-\nticle, we primarily aim to explore the potential\nof reference-free methods, which have a broader\nrange of applications.\nSecond, we did not classify types of hallucina-\ntions; limited by resources, even with fine-grained\nannotations, we only marked whether sentences\ncontained hallucinations without distinguishing\ntypes such as object existence errors, attribute er-\nrors, text recognition errors, and fact-conflicting\nerrors. This might be where SUQ methods have an\nadvantage.\nThird, we only studied white-box models. Re-\ncent research indicates that white-box models can\nbe proxies for black-box models to calculate their\nconfidence regarding specific issues. Unigram\nmethod is one such proxy model approach. In the\nfuture, we could use more complex models, closer\nin architecture to the target models, as proxies to\nexplore their potential in hallucination detection."}, {"title": "Ethics and Broader Impact", "content": "We sampled a portion of the data from existing\ndatasets for our experiments, which may affect the\naccuracy of some of our conclusions."}, {"title": "A Reference-free Hallucination Detection\nMethods", "content": "We collect uncertainty-based and consistency-\nbased methods proposed in (Manakul et al., 2023)\nto test the effectiveness on LVLMs."}, {"title": "A.1 Uncertainty-based methods", "content": "To aggregate the uncertainty information obtained\nat the token level, we employ four metrics to aggre-\ngate token-level uncertainty into sentence level. In\nparticular, a sentence-level uncertainty score can\nbe obtained by taking either the maximum or av-\nerage of the negative loglikelihood \u2013 log pij in a\nsentence:\n$MaxProb(i) = \\underset{j}{max}(-\\text{log } P_{ij}),$ (1)\n$\\frac{1}{J_i} \\text{log } P_{ij}$ (2)\nwhere pij is the probability of a token at a position j\nin the sentence i and Ji is the total number of tokens\nin the considered sentence. Additionally, one can\nalso replace the negative loglikelihood \u2013 log Pij\nwith the entropy Hij:\n$MaxEnt(i) = \\underset{j}{max} H_{ij},$ (3)\n$\\frac{1}{J_i} \\sum H_{ij},$ (4)\nwhere Hij is the entropy of the token distribution\nfor the j-th token in the sentence i."}, {"title": "A.2 Consistency-based Methods", "content": "BERTScore denoted as SBERT, finds the aver-\nage BERTScore of the considered sentence with\nthe most similar sentence from each drawn sam-\nple. Let B(., .) denote the BERTScore between two\nsentences. SBERT is computed by B(., .) as:\n$\\frac{1}{N} \\sum \\underset{k}{max} B(r_i, s_k),$ (5)\nwhere ri represents the i-th sentence in main re-\nsponse R, sh represents the k-th sentence in the\nn-th sample Sn and N is the total number of sam-\npled sentences.\nQuestion Answering (QA). The multiple-choice\nquestion answering generation (MQAG) evaluates\nconsistency by creating multiple-choice questions\nfrom the primary generated response and answer-\ning them using other sampled responses as refer-\nence. MQAG consists of two stages: question gen-\neration G and question answering A. For the sen-\ntence ri in the response R, we draw questions q\nand options 0:\n$q,0 \\sim P_G(q, 0 | r_i, R).$ (6)\nThe answering stage A selects the answers:\n$a_R = arg \\underset{k}{max}[P_A(o_k | q, R, 0)],$ (7)\n$a_{S_n} = arg \\underset{k}{max}[P_A(o_k | q, S_n, \u0ed0)].$ (8)\nWe compare whether ar is equal to agn for each\nsample in {$1,...,SN}, yielding Nm matches\nand Nn not-matches. A simple inconsistency score\nfor the i-th sentence and question q based on the\nmatch/not-match counts is defined: SQA(i,q) =\nNn. Manakul et al. (2023) modify the incon-\nsistency score SQA(i, q) by incorporating the an-\nswerability of generated questions. Finally, SQA(i)\nis the average of inconsistency scores across q,\n$S_{QA}(i) = E_q[S_{QA}(i, q)].$ (9)"}, {"title": "Unigram.", "content": "The concept behind Unigram is to de-\nvelop a new model that approximates the LVLMs\nby samples {S\u00b9,..., SN } and get the LVLM's to-\nken probabilities using this model. As N increases,\nthe new model gets closer to LVLMs. Due to time\nand cost constraints, we just train a simple n-gram\nmodel using the samples {S\u00b9, . . ., SN } as well as\nthe main response R. We then compare the av-\nerage and maximum of the negative probabilities\nof the sentence in response R using the following\nequations:\n$\\frac{1}{J_i} \\sum \\text{log } P_{ij},$ (10)\n$S_{Max^{n-gram}}(i) = \\underset{j}{max}(\u2013 \\text{log } p_{ij}),$ (11)\nwhere pij is the probability of a token at position j\nof a sentence i."}, {"title": "Natural Language Inference (NLI)", "content": "determines\nwhether a hypothesis follows a premise, classified\ninto either entailment/neutral/contradiction. In this\nwork, we use DeBERTa-v3-large (He et al., 2023)\nfine-tuned to MNLI as the NLI model. The input\nfor NLI classifiers is typically the premise con-\ncatenated to the hypothesis, which for NLI is the\nsampled passage Sn concatenated to the sentence\nto be assessed ri in the response R. Only the logits\nassociated with the 'entailment' and 'contradiction'\nclasses are considered,\n$P(contradict | r_i, S^n) = \\frac{exp(z_{in})}{exp(z_{en}) + exp(z_{in})},$ \nwhere $z^e_{n} = z^e(r_i, S^n)$ and $z^i_{n} = z^e(r_i, S^n)$ are\nthe logits of the 'entailment' and 'contradiction'\nclasses. NLI score for sentence ri on samples\n{S1, ..., SN} is then defined as,\n$\\frac{1}{N} P(contradict | r_i, S^n).$ (12)"}, {"title": "B Dataset and Implementation", "content": "POPE (Li et al., 2023) formulates the evaluation of\nobject hallucination as a binary classification task\nthat prompts LVLMs to output \u201cYes\u201d or \u201cNo\u201d, e.g.,\n\"Is there a chair in the figure?\" POPE adopts three\nsampling settings to construct negative samples:\nrandom, popular, and adversarial. The random\nsetting selects non-present objects at random. In\ncontrast, the popular setting chooses from a list of\nfrequently occurring but absent objects, and the\nadversarial method selects based on common co-\noccurrence in contexts despite the absence in the\ntarget image.\nGQA (Hudson and Manning, 2019) is proposed to\ndetect hallucinations, including existence, attribute,\nand relation. It contains not only Yes-or-No prob-\nlems but also Open-ended problems. We randomly\n9050 some binary problems for 397 images used\nin our experiments.\nM-HalDetect (Gunjal et al., 2024) consists of\n16k fine-grained annotations on visual question-\nanswering examples, including object hallucina-\ntion, descriptions, and relationships. We utilize\nabout 3000 questions on 764 figures for our Open-\nended tasks, averaging 4 sentences per response.\nIHAD severs as the evaluation of description for\nOpen-ended tasks. It is a sentence-level dataset\ncreated by prompting LLaVA-V1.5-7b to generate\nconcise image descriptions for 500 images from\nMSCOCO (Lin et al., 2014). On average, each re-\nsponse contains 5 sentences. The creation of IHAD\naims to investigate if the data source impacts the\ninternal information of LVLMs and further impacts\nhallucination detection methods, which rely on this\ninternal information."}, {"title": "B.2", "content": "Experimental Setup\nIn our experiments, we merge questions and re-\nsponses from humans or other models to create\nhand-crafted input. This input is then processed\nby the evaluation model, from which we extract\ninternal information, including probability, entropy,\nand embedding states. The evaluation model, ca-\npable of recognizing hallucinations (Duan et al.,\n2024), can assign reasonable internal information\nto responses not generated by itself. As shown\nin Figure 2, when generating the \"cat\" token, the\nevaluation model assigns probabilities to all tokens\nand selects the higher probability token for output.\nSpecifically, the probability assigned to \u201ccat\u201d in\nself-generated input is 0.9, while in hand-crafted\ninput, \"dog\" receives a probability of 0.1. POPE,\nGQA, and M-Hal are hand-crafted data for all four\nevaluation models used in this work. IHAD's re-\nsponse, however, is generated by LLaVA-v1.5-7b,\nmaking IHAD self-generated data specifically for\nLLaVA-v1.5-7b. For the other three models, IHAD\nis considered hand-crafted data.\nIn the Yes-or-No tasks, we simplify the origi-\nnal responses to \u201cYes\u201d for positive answers (e.g.,"}, {"title": "C Experimental Results", "content": "Yes-or-No Tasks\nWe only compare the uncertainty-based and SUQ\nmethods on Yes-or-No tasks because the model's\nanswers exclusively contain \u201cYes\u201d or \u201cNo\u201d. The\nrandom baseline is calculated as the ratio of non-\nfactual examples to the total number of exam-\nples. Our results in Table 3 demonstrate that SUQ\nmethod outperforms the uncertainty-based meth-\nods, exceeding AvgProb by about 4.5%. AvgProb\nachieves AUC-PR values nearly double those of the\nrandom baseline across all datasets and models,\nindicating it is an efficient method since it is an\nunsupervised method. In addition, the probability\nProb overall performs better than the entropy Ent\nin Yes-or-No tasks."}, {"title": "C.2 Impact of Unimportant Tokens for\nUncertainty-based Methods", "content": "It is observed that the performance of uncertainty\nmethods on the Open-ended tasks is not as good\nas on the Yes-or-No tasks. To explore the reasons,\nwe append a period, resulting in \u201cYes.\u201d or \u201cNo.\u201d\nin the Yes-or-No task. In this case, probability p\nand entropy H of two tokens need to be consid-\nered. Table 4 shows the detailed performance for\nYes-or-No tasks with period using LLaVA-V1.6-"}, {"title": "C.3 Optimal Hidden Layers for Effective\nSUQ Method", "content": "To explore which hidden layer information should\nbe used, we perform 3D TSNE visualizations of the\nembedding states of LLaVA at the fifth and last lay-\ners for popular tasks, as shown in Figure 7. These\nvisualizations show that at the last layer, there are\ndistinct separations between the representations of\ndifferent labels. Conversely, at the fifth layer, rep-\nresentations of different labels are relatively close\nand almost blend together, indicating that the shal-\nlow layers primarily preserve language-agnostic\nfeatures. Therefore, we use the last hidden layer to\nexplore the SUQ method in our experiments."}, {"title": "C.4", "content": "The comparison of clear and blurred\nimages.\nThe comparison of clear and blurred images is il-\nlustrated in Figure 8."}, {"title": "C.5 The performance of different methods in\nLLaVA-v1.6-mistral-7b.", "content": "Figure 9 visually presents the performance of differ-\nent methods in LLaVA-v1.6-mistral-7b on IHAD."}]}