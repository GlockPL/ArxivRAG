{"title": "Perceptron Collaborative Filtering", "authors": ["Arya Chakraborty"], "abstract": "While multivariate logistic regression classifiers are a great way of implementing collaborative filtering - a method of making automatic predictions about the interests of a user by collecting preferences or taste information from many other users, we can also achieve similar results using neural networks. A recommender system is a subclass of information filtering system that provide suggestions for items that are most pertinent to a particular user. A perceptron or a neural network is a machine learning model designed for fitting complex datasets using backpropagation and gradient descent. When coupled with advanced optimization techniques, the model may prove to be a great substitute for classical logistic classifiers. The optimizations include feature scaling, mean normalization, regularization, hyperparameter tuning and using stochastic/mini-batch gradient descent instead of regular gradient descent. In this use case, we will use the perceptron in the recommender system to fit the parameters i.e., the data from a multitude of users and use it to predict the preference/interest of a particular user.", "sections": [{"title": "I. A BRIEF HISTORY AND INTRODUCTION", "content": "Neural network in machine learning is a model made up of artificial neurons or nodes. The connections between the nodes are modelled as weights. A positive weight reflects an excitatory connection while a negative weight reflects an inhibitory connection. The inputs are modified by the weights and summed; an activity known as linear combination. An activation function at the end controls the amplitude of the output i.e., brings the output in a desirable range - usually 0 to 1 or -1 to 1.\nIn 1943, Warren McCulloch and Walter Pitts from the University of Illinois and the University of Chicago published research that analysed how the brain could produce complex patterns and could be simplified down to a binary logic structure with only true/false connections. Frank Rosenblatt from the Cornell Aeronautical Laboratory was credited with the development of the perceptron in 1958. His research introduced weights to McColloch's and Pitt's work, and Rosenblatt leveraged his work to demonstrate how a computer could use neural networks to detect imagines and make inferences.\nThe next step in the development of neural networks came in 1982 with the development of 'Hopfield Networks' by John Hopfield. A Hopfield network is a fully interconnected recurrent neural network where each unit is connected to every other unit. It behaves in a discrete manner and produces distinct outputs in generally binary (0/1) form or in bipolar (-1/1) form. In a recurrent neural network, the outputs of the neurons are again fed into the network as 'memory' that improves the current output and input of the network."}, {"title": "B. Collaborative Filtering", "content": "A recommender system is a subclass of Information Filtering System, that provides suggestions for items that most appropriate for a particular user, based on the data collected from a multitude of users. Recommender systems are particularly useful as they can help a user choose properly when there is an overwhelming number of options. Both the users and the services provided have benefited from these kinds of systems. The quality and decision-making process has also improved through these kinds of systems.\nCollaborative Filtering is a technique used by recommender systems for making automated predictions about the interests of a user by collecting preferences from many users. In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Collaborative filtering can be applied to various kinds of data including sensing and monitoring data, financial data etc. The overwhelming amount of data necessities mechanisms for efficient information filtering. Collaborative filtering is one of the techniques used for solving this problem.\nThough various sources contradict, the discovery of the collaborative filtering algorithm is generally accredited to Dave Goldberg and his colleagues at Xerox PARC. The origins of modern recommender systems date back to the early 1990s when they were mainly applied experimentally to personal email and information filtering. Today, 30 years later, personalized recommendations are ubiquitous and research in this highly successful application area of AI is flourishing more than ever. Much of the research in the last decades was promoted by advances in machine learning technology. In 1992, the concept of \"Collaborative Filtering\" was introduced with an experimental mail system called Tapestry."}, {"title": "II. THE CLASSICAL APPROACH", "content": "Collaborative Filtering is usually done using multivariate logistic regression as the output is discrete in nature (Logistic regression corresponds to discrete output, used in classification problems; while linear regression corresponds to continuous output, used in data-value prediction problems). With conventional regularized multivariate regression, appreciable accuracy is achieved. But, neural networks (perceptron) can help us to achieve considerably more accuracy using backpropagation and gradient descent. The classical approach to collaborative filtering using logistic regression is as follows:\n$n_u = number of users$\n$n_m = number of movies$\n$r(i,j) = 1$ if 'j' has rated movie 'i' (0 otherwise)\n$y^{(i,j)}$ = rating by user 'j' on movie 'i' (if defined)\n$\\theta^{(j)}$ = parameter vector for user 'j'\n$x^{(t)}$ = feature vector for movie 'i'\nGiven $x^{(1)}, ..., x^{(n_m)}$, estimate $\\theta^{(1)}, ..., \\theta^{(n_u)}$:\n$\\min_{\\theta^{(1)}, ..., \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T x^{(t)} \u2013 y^{t,j})^2$\nAdding the regularization term to overcome overfitting or underfitting (due to high bias or variance):\n$\\min_{\\theta^{(1)}, ..., \\theta^{(n_u)}} \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T x^{(t)} \u2013 y^{t,j})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^{n} (\\theta_k^{(j)})^2$\nWhere $\\theta \\epsilon R^n$ i. e., \u03b8 is an n \u2013 dimensional vector, \u03bb = regularization parameter\nSimilarly, Given $\\theta^{(1)}, ..., \\theta^{(n_u)}$ estimate $x^{(1)}, ...,x^{(n_m)}$:\n$\\min_{x^{(1)}, ..., x^{(n_m)}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T x^{(t)} \u2013 y^{t,j})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2$\nWhere $x \\in R^n$ i. e., x is an n - dimensional vector\nConsidering the above functions as the cost functions for the respective parameter vectors (\u03b8) and feature vectors (x) as $J(\\theta^{(1)}, ..., \\theta^{(n_u)})$ and $J(x^{(1)}, ..., x^{(n_m)})$ we simultaneously minimize the functions using gradient descent. Thus, for every j = 1,2, ..., nu and i = 1,2,..., nm:\n$x_k^{(i)} := x_k^{(i)} - \\alpha (\\sum_{j:r(i,j)=1} ((\\theta^{(j)})^T x^{(t)} \u2013 y^{(t,j)})(\\theta^{(j)}) + \\frac{\\lambda}{2} x_k^{(i)})$\n$\\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha (\\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T x^{(t)} \u2013 y^{(i,j)}) x_k^{(t)}) + \\frac{\\lambda}{2} \\theta_k^{(j)})$ \nWhere $x_k^{(t)}$ is the kth feature of the feature vector $x^{(i)}$ and $\\theta_k^{(j)}$ is the kth parameter of the parameter vector $\\theta^{(j)}$. The learning rate for the gradient descent is \u03b1 and the regularization parameter for the same is \u03bb. Both are hyperparameters and can be optimized using hyperparameter tuning."}, {"title": "B. Use of Logistic Classifiers", "content": "After minimizing the cost functions and coming up with a set of feature vector (X) and parameter vector (\u03b8), we compute the hypothesis h(x) using the sigmoid function\n$h(X) = \\frac{1}{1+ e^{-\\theta^T X}}$\nwhere X is the feature vector of the movie to be recommended to the user and \u03b8 is the parameter vector of the user. h(x) is the estimated likelihood that user \u03b8 will like movie X"}, {"title": "B. A Better Alternative", "content": "Neural networks are complex machine learning models designed to fit complex datasets using backpropagation and gradient descent (or other advanced optimization algorithms). This can help us to achieve higher levels of accuracy in predicting the choice/taste of a user. Coupled with optimization techniques such as cosine similarity, gradient boosting and dimensionality reduction, it will be able to provide a more accurate outcome.\nAs seen in a number of experiments and research studies, artificial neural networks tend to provide a better overall fit to the data, thanks to its ability to form more complex decision boundaries. Thus, in this use case too, it would help us to find a better fit for the data."}, {"title": "III. USING ARTIFICIAL NEURAL NETWORKS", "content": "Given $x^{(1)}, ..., x^{(n_m)}$ and $y^{(i,j)}$, estimate $\\theta^{(1)}, ..., \\theta^{(n_u)}$\nCompute: Argmin C(\u03b8); C(\u03b8) \u2192 Cost/Loss Function\nPreprocessing of the dataset:\n$y^{(i,j)} = \\begin{cases} 1: y^{(i,j)} \\geq T \\\\ 0: y^{(i,j)} < T \\end{cases}; T \\rightarrow Threshold of recommendation$\nThe value of T can be set based on the rating of the movies. It signifies that the value of y will be set to 1, if the rating of the movie according to a user is greater than the threshold and 0, if the rating is lower than the same.\n$Cost(h(x),y) = \\begin{cases} - logh(x); y = 1 \\\\ (-log(1 \u2013 h(x)); y = 0 \\end{cases}$\nwhere, $h(x) = \\frac{1}{1 + e^{-\\theta^T X}}$\n$\\theta^T$ = Transpose of the matrix containing the parameter vectors of the user\nX = Matrix containing the feature vectors of the movie\nCombined form of the cost function:\n$cost(h(x),y) = \u2212y log(h(x)) \u2013 (1 \u2013 y) log(1 \u2013 h(x))$\nFor artificial neural networks the total cost, denoted by C (\u03b8) is:\n$C(\\theta) = - \\frac{1}{N_m} (\\sum_{i=1}^{N_m} y^{(i)} log (h(x^{(i)})) + (1-y^{(i)}) log (1-h(x^{(i)}))$\nFor a neural network containing K output nodes, the cost/loss function gets modified. The sum over all the output nodes is taken in this case, and the function is modified in the following way:\nK \u2192 number of ouput nodes of the neural network\n$C(\\theta) = - \\frac{1}{N_m} \\sum_{i=1}^{N_m} \\sum_{k=1}^{K} (y_k^{(i)} log (h(x^{(i)})) + (1-y_k^{(i)}) log (1-h(x^{(i)}))$\nTherefore, objective: compute Argmin C (\u03b8):\nCompute: Argmin $ \\frac{1}{N_m} \\sum_{i=1}^{N_m} \\sum_{k=1}^{K} (y_k^{(i)} log (h(x^{(i)})) + (1-y_k^{(i)}) log (1-h(x^{(i)}))$"}, {"title": "A. Objective of the Model", "content": "This will return the parameters $\\theta^{(1)}, ..., \\theta^{(n_u)}$ of a particular user which can be used with the feature vector of a new movie (one which user has not seen) to compute h(x) and determine the likelihood of recommendation. If h(x) \u2265 T; T being the threshold of recommendation, then the movie can be recommended to the user. On the other hand, if h(x) < T, then the movie may not be a right fit for the particular user taken into consideration."}, {"title": "B. Working of the Proposed Model", "content": "g(x) = Activation function of the neural network\nThe activation function helps to map the resulting values in between 0 to 1 or -1 to 1 etc. (depending on the function). Some of the activation functions that can be used in the neural network including linear and non-linear activation function are:\nSigmoid/logistic activation function: $g(x) = \\frac{1}{1+e^{-x}}$\nRange: 0 to 1\nUse Case: Since probability of an event lies between 0 and 1, the sigmoid function is especially useful for models where the prediction of a probability has to me made.\nHyperbolic tangent activation function: g(x) = tanh(x)\nRange: -1 to 1\nUse Case: Mainly used for classification between two classes\nRectifier Linear Unit (ReLU) activation function: g(x) = max(0,x)\nRange: 0 to + infinity\nUse Case: Most used activation function nowadays in neural networks."}, {"title": "Some used notation:", "content": "$a_i^{(j)}$ = activation of ith node of layer j\n$U^z = {\\theta^{(n)}: 1 \\leq n \\leq j;j = number of layers of neural network}; U^z \\rightarrow$ set of all matrices of weights for user z\n$\\theta^{(0)}$ = matrix of weights controlling function mapping from layer j to j + 1\n$\\theta^{(i)} = \\begin{bmatrix} \\theta_{11} & \\theta_{12} & ... & \\theta_{1n} \\\\ \\theta_{21} & \\theta_{22} & ... & \\theta_{2n} \\\\ : & : & : & : \\\\ \\theta_{n1} & \\theta_{n2} & ... & \\theta_{nn} \\end{bmatrix}$ = Matrix of user parameters\nConsidering the following neural network for our demonstration purposes:\n$\\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{bmatrix}$ is the feature vector of the movie and first layer of the neural network and $\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\\\ a_3^{(2)} \\\\ a_4^{(2)} \\end{bmatrix}$ is the second layer of the neural network. $a_i^{(2)}$ is also the first activation layer of the neural network. Similarly, the third layer/ second activation layer of the neural network is $a^{(3)} = \\begin{bmatrix} a_1^{(3)} \\\\ a_2^{(3)} \\end{bmatrix}$\nUsing forward propagation, we calculate the values of the activation nodes of the first activation layer:\n$a_1^{(2)} = g(\\theta_{11} x_1 + \\theta_{12}x_2 + \\theta_{13}x_3)$\n$a_2^{(2)} = g(\\theta_{21} x_1 + \\theta_{22}x_2 + \\theta_{23}x_3)$\n$a_3^{(2)} = g(\\theta_{31} x_1 + \\theta_{32}x_2 + \\theta_{33}x_3)$\n$a_4^{(2)} = g(\\theta_{41} x_1 + \\theta_{42}x_2 + \\theta_{43}x_3)$\nTherefore, the general formula for ith activation node of the second layer/ first activation layer (j = 2) of the neural network having 'n' input nodes i.e., an n-dimensional vector containing 'n' features of the movie is:\n$a_i^{(2)} = g ( \\sum_{k=1}^{n} (\\theta_{ik} x_k ); j = 2$\nSimilarly for 1st activation node of the third layer/ 2nd activation layer of the neural network is:\n$a_1^{(3)} = g(\\theta_{11}^{(2)} a_1^{(2)} + \\theta_{12}^{(2)} a_2^{(2)} + \\theta_{13}^{(2)} a_3^{(2)} + \\theta_{14}^{(2)} a_4^{(2)})$\nTherefore, for the kth activation node of the layer 'j' having 'n' activation nodes in the layer 'j-1', for j \u2265 3, the general formula for the calculation of activation is as follows:"}, {"title": "Some used notation:", "content": "$a_i^{(j)} = g(\\sum_{i=1}^{n} (\\theta_{ik}^{(i-1)} a_i^{(i-1)} ); j \\geq 3$\nTherefore, for a neural network containing 'j' layers and having 'n' nodes in the layer 'j', the final hypothesis is given by:\n$h(x) = g(\\theta_{11}^{(j)} a_1^{(j)} + \\theta_{12}^{(j)} a_2^{(j)} + \\theta_{13}^{(j)} a_3^{(j)} + ... + \\theta_{1n}^{(j)} a_n^{(j)}) = g ( \\sum_{k=1}^{n} \\theta_{ik}^{(j)} a_k^{(j)} )$\nIf we consider the vectorized implementation of the above approach, then $a^{(2)} = g(\\theta^{(1)}X^{(1)})$. Similarly, $a^{(3)} = g(\\theta^{(2)}a^{(2)})$ and so on. Thus, the general vectorized implementation of activation for layer 'j' is:\n$a^{(j)} = g(\\theta^{(j-1)}a^{(j\u22121)}); j \\geq 3$\nGradient computation using Backpropagation algorithm:\nTotal number of layers = L\n$y^{(i,j)}$ = rating by user 'j' on movie 'i' (if defined)\nFor all l \u2208 L and i,j\u2208 \u03b8(n) \u2200\u03b8(n) \u2208 U\u00b2, deviation factor, \u0394\u2081 = 0\nNow, $\\delta^{(L+1)} = h(x) \u2013 y^{(i,j)}$\nConverting $\\delta^{(L+1)}$ to a matrix to back-feed the neural network $\\Rightarrow \\delta^{(L+1)} = \\begin{bmatrix} h(x) - y^{(i,j)} \\\\ : \\\\ n times \\end{bmatrix}$ ; n \u2192 number of nodes for layer L\nNote: Only required for layer L+1 i.e., the output node as the output node, h(x) outputs a definite value. So it needs to be converted into a matrix in order to carry on with the backpropagation gradient computation.\nFor all layers, l\u2208 {L, L 1, ... 2}:\n$\\delta^{(l)} = ((\\theta^{(l)})^T\\delta^{(l+1)}) .* a^{(l)}.* (1 \u2013 a^{(l)})$\nWhere \".* \" operation means the element wise multiplication of the corresponding matrices.\nNow, calculating the deviation factors,\n$\\Delta^{(l)}= \\Delta^{(l)} + \\delta^{(l)} (a^{(l+1)})$\nA more general vectorized implementation is $\\Delta^{(l)}= \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T$\n$D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\lambda A; where D \\rightarrow$ partial derivative of the term\nFor each layer, using matrices, $D^{(l)} = \\frac{1}{m} \\Delta^{(l)}$; m = total number of movie feature vectors\nThrough calculus, it can be shown,\n$\\frac{\\partial}{\\partial \\theta_{ij}} C(\\theta) = D_{ij}^{(l)}; C(\\theta) \\rightarrow$ Cost function of the neural network\nTherefore, D(l) is the vector containing the partial derivatives of the activation nodes for the layer l\n$Gradient matrix, G = \\begin{bmatrix} |\\vec{ivi} \\in D^{(1)} \\\\ |\\vec{ivi} \\in D^{(2)} \\\\ : \\\\ |\\vec{ivi} \\in D^{(L)} \\end{bmatrix}$\nThus, G is the gradient matrix containing all the weights for the neural network.\nNow running gradient descent on the cost function C(\u03b8) and minimizing the weights in the gradient matrix, we obtain the matrix containing the parameter vectors of the user.\nTherefore, we compute:\nArgmin $ \\frac{1}{N_m} \\sum_{i=1}^{N_m} \\sum_{k=1}^{K} y_k^{(i)} log (h(x^{(i)})) + (1-y_k^{(i)}) log (1-h(x^{(i)}))$ \nWhere h(x) is computed using the weights present in the gradient matrix."}, {"title": "C. Optimization Techniques", "content": "The following optimization techniques can be implemented in the model to increase the computational efficiency of the same as well as to achieve a better overall result.\n1) Implementing Gradient Checking: Backpropagation is very powerful if implemented correctly. While building a neural network from scratch, backpropagation is often the place where people make mistakes. Implementing backpropagation incorrectly may not only result in the improper estimation of the weights, but also the total failure of the neural network. Thus, an intermediate step, known as gradient checking could help in overcoming this problem. This is a powerful way to eliminate all the bugs in backpropagation.\nFrom calculus we know,\n$f'(x) = \\frac{d}{dx} f(x) = \\lim_{h\\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h}$\nUsing this, we can conduct the numerical estimation of the gradient by\n$\\frac{d}{d\\theta} C(\\theta) \\approx \\frac{C(\\theta + \\gamma) \u2013 C(\\theta \u2013 \\gamma)}{2\\gamma}; where \\gamma \\approx 10^{-7}$\nTherefore, $\\forall \\theta_i \\in {\\theta \\nu \\theta \\epsilon U^z}$:\n$\\frac{\\partial}{\\partial \\theta_i} C(\\theta) \\approx \\frac{J(\\theta_1, \\theta_2,..., \\theta_i + \\gamma, \\theta_{i+1}, ... \\theta_n) \u2013 J(\\theta_1, \\theta_2, ..., \\theta_i \u2013 \\gamma, \\theta_{i+1}, ... \\theta_n)}{2\\gamma}$\nLet the matrix formed by the numerical estimation of the gradients be G'. Now we calculate the Euclidian distance normalized by the sum of the sum of the vectors G and G'.\n$\\epsilon = \\frac{||G' - G||_2}{||G'||_2 + ||G||_2}$\nIf \u03b5 < y, we can conclude that the backpropagation was implemented correctly and is therefore working.\nNote: While gradient checking is an incredibly powerful way of checking whether backpropagation is working properly, it must be noted that it is a very memory intensive process and therefore must be implemented only once i.e., after the initial calculation of the gradient matrix using backpropagation. If gradient checking is allowed to run after every iteration of gradient descent, then the efficiency of the model will be hampered.\n2) Random Initialization of the Parameters: While setting all the initial value of the weights to zero works for logistic regression, it doesn't work for neural networks. This is because, for all iterations, the weights of the activation nodes will be equal. This will therefore result in the parameters going into each of the nodes be equal. That is,\n$\\frac{\\partial}{\\partial \\theta_1} C(\\theta) = \\frac{\\partial}{\\partial \\theta_2} C(\\theta) = ... = \\frac{\\partial}{\\partial \\theta_n} C(\\theta)$\nThus, the neural network always ends up with only one feature. This will result in high bias and will therefore result in under-fitting of the parameters.\nSymmetry breaking: Initialize every $\\theta \\epsilon \\theta^{(1)} \\forall \\theta^{(1)} \\epsilon U^z$ to some random value in between [-x.x] e.g., 0.69420\nNote: Usually it is a good practice to randomize all the weights in the range (0, 1), such that every weight is a random real number between 0 and 1 (both exclusive). The weights can also be between -1 and 1 (both exclusive).\n3) Feature Scaling: The idea is to make sure that all the features are on a similar scale. Gradient descent can converge faster if the features are in a similar range. Therefore, for all the different features, the features need to be divided by some constant (maybe be different for different features) to get them into approximately -1\u2264x\u22641 range."}, {"title": "C. Optimization Techniques", "content": "Mean Normalization: Just like feature scaling, this falls under the category of preprocessing of data. Here the idea is to make all the features have zero mean. Having a normalized dataset helps the gradient descent to converge faster, thereby lowering the training time of the model.\n$x_i - \\frac{x_i - \\mu_i}{max x_i \u2013 min x_i}; \\mu_i \\rightarrow$ mean of the dataset\nReplacing the denominator by standard deviation instead, we obtain a similar result. The process is called mean standardization.\n5) Implementing Regularization: A machine learning model is often prone to high bias or high variance. The former happens when the hypothesis functions maps poorly to the trend of the data and is also known as under-fitting. The latter is when the learned hypothesis may fit the training set very well but fail to generalize to new examples outside the training dataset. This can be eliminated by keeping all the features but reducing the magnitude/values of the parameters.\nTherefore, we penalize the cost function using a new introduced term, \u03bb, known as regularizing parameter, thereby minimizing the values of the parameters.\nThe regularized cost function for the neural network is:\n$C(\\theta) = \\frac{1}{N_m} \\sum_{i=1}^{n_m} \\sum_{k=1}^{K} (y_k^{(i)} log (h(x^{(i)})) x) + (1 \u2212y_k) log (1 \u2013 h(x^{(i)})) k + \\frac{\\lambda}{2n_m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{St} \\sum_{j=1}^{Sj+1} (\\theta_{ji}^{(l)})^2$\nWhere $S^t$, $S^{i+1}$ = number of units in layer l, l + 1 respectively; L = total number of layers in neural network\nNote: If the value of \u03b1 i.e., the regularization parameter is too large, all the parameters are close to zero. This results in underfitting i.e., the hypothesis has too high bias. Similarly, a too small value of the regularizing parameter will make the regularization term very small and thus will result in regularization becoming useless. Thus, we should pick a moderate value of the regularization parameter (around 10) and modify it overtime to find the best fit.\n6) Hyperparameter Tuning: In machine learning, a hyperparameter is a parameter whose value is used to control the learning process unlike the other parameters whose values are derived via training. For the neural network, we have two hyperparameters involved i.e., the learning rate, \u03b1, of the gradient descent and the regularization parameter, \u03bb, for the regularized cost function.\nThere are various ways of going about hyperparameter tuning. The most common ones are GridSearchCV and Randomized SearchCV. In GridSearchCV approach, the machine learning model is evaluated for a range of hyperparameter values. It is called GridSearchCV because it searches for the best set of hyperparameters from a grid of hyperparameter values.\nRandomizedSearchCV solves a drawback of GridSearchCV, as it goes through only a fixed number of hyperparameter settings. It moves within the grid in a random fashion to find the best set of hyperparameters. This approach reduces unnecessary computation.\nNote: For hyperparameter tuning, the RandomSearchCV and GridSearchCV functions can be used from sklearn.model_selection module\n7) Using Stochastic Gradient Descent or Mini-Batch Gradient Descent: Stochastic gradient descent is selecting data points at each step to calculate the derivatives. Stochastic gradient descent randomly picks one data point from the whole dataset at each iteration to reduce the computations enormously.\nThe way of going about stochastic gradient descent is as follows:\n$cost (\\theta, (x^{(i)},y^{(i)})) = \\frac{1}{2} (h(x^{(i)}) \u2013 y^{(i)})^2$\nTherefore, calculating the derivative of the cost function, we have the following expression:"}, {"title": "C. Optimization Techniques", "content": "$\\frac{\\partial}{\\partial \\theta_j} cost (\\theta, (x^{(i)},y^{(i)})) = (h(x^{(i)}) \u2013 y^{(i)})x_j^{(i)}$\nThus, the following steps should be taken in order to implement Stochastic Gradient Descent (SGD):\n1. Randomly shuffle the dataset. Randomly shuffled dataset at first will help the gradient descent converge a little faster to the global minimum.\n2. Repeat {\nfor i = 1,2..., m {\n$\\theta_j = \\theta_j \u2013 \\alpha ((h(x^{(i)}) \u2013 y^{(i)})x_j^{(i)})$; for all j = 0, 1, ..., n (n = total number of features)\n}\n}\nLooking through each example and take a little step to try to fit just that particular parameter.\nFor mini-batch gradient descent we define a parameter \"b\", the mini batch size i.e., using \"b\" examples for every iteration. Usually the value of \"b\" is taken somewhere between 2 and 100. The process is as follows:\nRepeat {\nGet \u201cb\u201d examples $\\rightarrow (x^{(i)},y^{(i)}), ..., (x^{(i+(b\u22121))},y^{(i+(b\u22121))})$\n$\\theta_j = \\theta_j - \\alpha \\sum_{k=i}^{i+(b-1)} (h(x^{(k)}) \u2013 y^{(k)})x_j^{(k)} $; for all j = 0,1,...,n (n = total number of features)\ni=i+b\n}\nUsing mini-batch gradient descent results in more stable convergence towards the global minimum since we are calculating an average gradient over \"b\" examples.\nNote: While mini-batch gradient descent is computationally more efficient, the one major turnoff in this case is the introduction of a new hyperparameter \"b\", i.e., the batch size. Thus, to ensure peak efficiency, we have to tune the mini batch size, \"b\", as well along with the other hyperparameters using hyperparameter tuning.\nThe above optimization techniques help us not only to eliminate bugs and improve accuracy of the model, but also goes a long way in improving the computational efficiency of the same. Therefore, we should implement all of the above techniques to help us make the model optimal."}, {"title": "IV. CONCLUSION", "content": "While logistic regression classifiers provide a fairly accurate result of the user's recommendation based on the data collected from a multitude of users, we can further enhance the accuracy of the model by the introduction of neural networks. As neural networks are designed to fit complex datasets, we can form functions of higher complexity and thus get a better fit for the data without running into issues like high bias (under-fitting) and high variance (over-fitting). While neural networks can fit the datasets better than logistic classifiers, they are often computationally more expensive. Thus, we implement a series of optimization techniques to get the best possible results with the least computational work. This includes preprocessing of data i.e., implementing mean normalization and feature scaling. While training the model, we use gradient checking to ensure backpropagation is implemented correctly, we use regularization to overcome the issue of under-fitting and over-fitting due to high bias and high variance respectively. We initialize the parameter vectors using random initialization. To make the model computationally more efficient, instead of regular batch gradient descent, we use stochastic/mini-batch gradient descent. Finally, to achieve the best set of hyperparameters i.e., the learning rate (\u03b1), the regularization parameter (\u03bb) and the mini-batch size (b), we use hyperparameter tuning using GridSearchCV or RandomizedSearchCV. More advanced algorithms such as Momentum (used for reducing the high variance in SGD and softening the convergence), Nesterov Accelerated Gradient (made so that Momentum does not miss the local minima), AdaGrad (overcomes the problem of having a constant learning rate and therefore implements a dynamic learning rate), AdaDelta (removes the decaying learning rate problem of AdaGrad) and Adaptive Momentum Estimation (works with momentums of first and second order) can be used instead of gradient descent to further improve the computational efficiency and accuracy. For neural networks specifically, Adaptive Momentum Estimation tends to be the best optimizer.\nThe objective (section III-A) and the working (section III-B) of the model have been explained with the specific example of recommending a movie to a user. However, the similar approach can be followed in other use cases as the core concept remains the same. The most important part is the implementation of gradient checking just after the calculation of the derivatives using backpropagation to ensure the proper working of the backpropagation algorithm. The perceptron collaborative filtering can be used in OTT platforms to recommend movies to a user based on the rating of other users. It can also be used in online shopping platforms to recommend certain products to certain people based on relatability and rating/reviews."}]}