{"title": "WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction", "authors": ["Oktie Hassanzadeh"], "abstract": "Recently, there has been an increasing interest in the construction of general-domain and domain-specific causal knowledge graphs. Such knowledge graphs enable reasoning for causal analysis and event prediction, and so have a range of applications across different domains. While great progress has been made toward automated construction of causal knowledge graphs, the evaluation of such solutions has either focused on low-level tasks (e.g., cause-effect phrase extraction) or on ad hoc evaluation data and small manual evaluations. In this Resource Track paper, we present a corpus, task, and evaluation framework for causal knowledge graph construction. Our corpus consists of Wikipedia articles for a collection of event-related concepts in Wikidata. The task is to extract causal relations between event concepts from the corpus. The evaluation is performed in part using existing causal relations in Wikidata to measure recall, and in part using Large Language Models to avoid the need for manual or crowd-sourced evaluation. We evaluate a pipeline for causal knowledge graph construction that relies on neural models for question answering and concept linking, and show how the corpus and the evaluation framework allow us to effectively find the right model for each task. The corpus and the evaluation framework are publicly available.", "sections": [{"title": "1 Introduction", "content": "Extracting and representing causal knowledge has been a topic of extensive research, with applications in decision support and event forecasting in a variety of domains such as sociopolitical event forecasting [16,27,28,33], enterprise risk management and finance [4,17,40], and healthcare [1,32,46]. One way to derive"}, {"title": "2 Task Definition and Use Cases", "content": "Our target task is as follows: given a corpus of text documents and a select set of concepts (e.g., from an existing knowledge graph), automatically generate a causal knowledge graph in which nodes are the given concepts, and an edge between two concepts indicates a causal relation between the concepts. The select concepts in the knowledge graphs could be either event-related classes, or instances of such classes. The class concepts could belong to an ontology, with a class hierarchy. We assume that no annotations or training data are available. That is, while we know what concept each document is associated with, we do not have annotations of concepts or relations in the corpus."}, {"title": "3 Corpus Creation", "content": "Given our task definition, we curate a collection of text documents, each associated with an event-related concept. We use Wikipedia as the source of our text documents and Wikidata as our source of event-related concepts. The first step in curating our corpus is identifying a set of event-related concepts in Wikidata. We do so by querying Wikidata for concepts that have associated Wikinews articles. An associated Wikinews article implies that the article's topic is on a newsworthy event instance. We then find the set of all the classes of the retrieved instances that are subclasses of class occurrence (Q1190554) to ensure that the chosen class is an event class as some non-event classes also have links to Wikinews. We then further manually verify each of the concepts and drop those that are not event-related. We do so since currently some non-event related concepts are (possibly erroneously) sub-classes of occurrence (Q1190554). For the first version of our corpus, this results in a select set of 50 top-level event-related concepts in Wikidata."}, {"title": "4 Evaluation Framework", "content": "As with any automated knowledge graph construction task, we need to measure the quality of the output both in terms of the number of causal relations ex-"}, {"title": "4.1 Recall Evaluation", "content": "Given that manually extracting all the expressed causal relations over the corpus is not feasible, our automated recall evaluation relies on existing causal relations in Wikidata. Although the number of causal relations in Wikidata is limited and much less than the available causal knowledge expressed in Wikipedia documents, the majority of such relations are described in Wikipedia articles, and so measuring the ability of an automated method to discover those relations provides us with a good estimate of the actual recall of the method. Over time, we expect the high-confident accurate causal relations to be added to Wikidata, and so this evaluation strategy can become an even more reliable measure of recall. In our experiments with causal knowledge extraction methods, we discovered that different methods perform differently with respect to extracting relations between instances and classes. As a result, we apply our accuracy measures separately for relations that include at least one instance, and those that are between classes only.\nFor recall evaluation, we first construct a causal knowledge graph from the existing causal relations in Wikidata and our selected seed concepts and all their instances. We refer to this graph as the \"Base KG\". Our recall evaluation script takes the Base KG and the output of causal extraction as inputs, and reports the following measures:\nrecall The ratio of causal relations in the Base KG that can be found in the extraction output.\nhit_count The number overlapping causal relations between the Base KG and the extraction output.\nrel_count The number of extracted causal relations.\nbase_kg_size: The number of causal relations in the Base KG."}, {"title": "4.2 Precision Evaluation", "content": "In the absence of a complete knowledge graph for a given corpus, the standard way to evaluate the precision of the extracted knowledge is manual evaluation. Manual evaluation, however, is tedious and time-consuming, which limits the possibility of experimenting on a large scale with a wide range of methods and parameters. Inspired by a recent trend in the use of large language models (LLMs) as an alternative to crowd-sourcing and manual annotation [12,48,49], we devise a mechanism to automatically create prompts for generative LLMs to evaluate the precision of the extracted causal relations. This approach works well for our corpus and task since LLMs have been exposed to the knowledge that is available on Wikipedia and Wikidata and are therefore likely to perform very well in the verification of the extracted relations. It is important to note that we do not use LLMs as a causal extraction method since our goal is the evaluation of generic extraction methods that can handle proprietary sources of knowledge that are less likely to be in the sources that LLMs are trained on. Our goal in this paper is not to curate a large-scale Wikipedia-based causal KG but to evaluate generic causal extraction methods. As mentioned earlier, such methods have a range of applications in risk management, intelligence analysis, and healthcare, where there is access to proprietary sources of knowledge.\nOur precision evaluation script takes as input a causal knowledge graph consisting of cause-effect pairs of concepts and uses each pair to generate a prompt for a generative LLM to verify the accuracy of the extracted pair. There are many ways to use LLMs for this validation task, and for each approach, there are several available models and parameters to be chosen. In order to find the best approach, we use the Base KG relations to measure the performance of each approach. This is similar to asking verification questions to crowd workers to find high-performing workers and drop the low-performing ones. We observed in our experiments that instruction-tuned generative LLMs perform better at the task, as verified using our Base KG. As an example, an extracted pair (cause, effect) can be turned into the following prompt:\nDefinition: Answer the question with a yes or no.\nNow complete the following example\nInput: Question: Could {cause} result in effect}?\nOutput:"}, {"title": "5 Experiments", "content": "In this section, we report our preliminary results of using the corpus and our evaluation framework to evaluate a causal knowledge extraction pipeline that relies on the extraction of cause-effect phrases and linking the outcome to event concepts. As a part of our evaluation framework, in addition to the evaluation scripts and the corpus, we have made the extracted knowledge graph outputs and the results on these outputs publicly available."}, {"title": "5.1 Causal Extraction Pipeline", "content": "The causal extraction pipeline we use in our experiments is in part based on our previous work [9], that splits the causal extraction process into two steps: 1) extraction of pairs of cause-effect phrases 2) linking each cause and effect to event concepts. The pipeline is depicted in Figure 4. As mentioned earlier, there is a range of methods for cause-effect pair extraction, including supervised sequence tagging methods (e.g., [7]) as well as weakly-supervised and unsupervised methods (e.g., [2,35] and references therein). For two of the four knowledge graphs used in our experiments, we use the causal pairs from CauseNet [13] which is based on the application of a number of state-of-the-art extraction methods including a supervised sequence tagger. The other two knowledge graphs rely on question answering using the associated Wikidata concept labels in the corpus as seed terms to extract causal relations. We refer to these KGs as QAL (Question Answering & Linking) KGs, which are constructed by first using the seed terms to create open-ended causal questions. For example, a seed term event can turn into a question \"What does event lead to?\" or \"What causes event?\". The question is then asked from either the opening paragraph or all the paragraphs in the associated articles. The answer span along with the label used in the input result in a cause-effect pair, with one side already linked to an event concept in the seed set.\nOnce we have a collection of cause-effect pairs of phrases, the next step is to link them to event concepts. While this task is similar to the classic entity"}, {"title": "5.2 Evaluation Framework Repository and Settings", "content": "For reproducibility, all the results reported in this section are obtained using scripts and data that are released publicly as a snapshot of our git repository. A screenshot of the repository is shown in Figure 51. We have made our git repository public so that future improvements (by our team and the community) as well as future results can similarly be shared, and our results page can act as a public leaderboard for the latest results on the corpus.\nFor the results reported in this paper, we have used version 1 of our released corpus which is derived from a September 1, 2022 dump of English Wikipedia. We experimented with a number of LLMs for our precision evaluation including large and proprietary models, and ended up picking allenai/tk-instruct-3b-def as a publicly available model that can run without GPUs (albeit slow) so the evaluation can be performed without requiring hard-to-obtain resources. Each precision evaluation takes about an hour to run using CPUs only, and a few minutes with one V100 GPU with 16GB of memory. Since the model is instruction-"}, {"title": "5.3 Results", "content": "Figures 6 and 7 show the results of recall and precision evaluation over four extracted knowledge graphs:\ncausenet-full-linked-v1: The CauseNet Full data [13], turned into a Causal KG through concept linking using BLINK [45].\ncausenet-precision-linked-v1: The CauseNet Precision subset (publicly-available higher-precision subset of CauseNet), also turned into a KG through linking using BLINK.\nqal-kg-v1: Question Answering & Linking (QAL) pipeline that uses DistilBERT [36] fine-tuned on SQUAD2.0 [34] dataset, with extractions only on the first_section of the articles in the corpus. The pairs are then linked to event concepts using BLINK.\nqal-kg-v2: QAL pipeline that uses mfeb/albert-xxlarge-v2-squad2 model2 also fined-tuned on SQUAD2.0, over all the text contents of the articles in the corpus. Again, the output is linked using BLINK."}, {"title": "6 Discussion, Lessons Learned, and Future Work", "content": "Task Difficulty & Use Cases As our recall and precision results show, the defined task is very challenging for pipelines built using state-of-the-art neural models for knowledge extraction. However, the high level of precision means that the application of these methods would result in the addition of a significant number of causal relations to the base knowledge graph. Such discovered relations can play a critical role in end applications such as event prediction [39]. We also observed that our automated precision evaluation method underestimates the actual level of precision for instance-level relations due to the use of longer labels that are also less frequently observed in the source that large language models are trained on. Still, the precision evaluation method provides a reliable way of comparing different methods' level of precision. In our manual inspection of the results, we see many highly-interesting and causal relations that are difficult to extract using classic rule-based methods. Finally, it is important to note that many relations in the ground truth used for recall evaluation may not be described in the associate article, and so the recall metric should be considered mainly for comparison of different methods and not for measure the absolute recall of the methods.\nSustainability As the current snapshot of our evaluation framework repository shows, we are making all the data and scripts that were used for the results in this paper public. We also plan to make it easy for the community to contribute new outputs, results, and extensions to the evaluation scripts and to the corpus. Our github repository (shown in Figure 5 and included in the Zenodo snapshot) is now publicly available at https://github.com/IBM/wikicausal. We are currently working on release version 2 of the corpus. While it is essential to fix the"}, {"title": "7 Conclusion", "content": "We presented a corpus, task, and evaluation framework for extracting causal knowledge from textual corpora. Using our evaluation framework, we presented the results of our evaluation of four different automatically extracted knowledge graphs of causal relations. As part of our evaluation framework, we developed a method for measuring the relative recall of various extraction methods using Wikidata's existing causal knowledge. In lieu of crowdsourcing or manual evaluation, we devised a novel method for gauging the precision of extraction methods by employing large generative language models. We have made our corpus and evaluation framework permanently available to the public, and we intend to establish a community and leaderboard for the task and for future extensions of the corpus and evaluation framework. While our work targets an important set of causal knowledge applications for reasoning and prediction, it also fills an important gap in the literature on resources for the evaluation of knowledge extraction methods from text."}]}