{"title": "AFFORDANCE-BASED ROBOT MANIPULATION WITH FLOW MATCHING", "authors": ["Fan Zhang", "Michael Gienger"], "abstract": "We present a framework for assistive robot manipulation, which focuses on two fundamental chal- lenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot trajectories guided by affordances in a supervised Flow Matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation high- lights that the proposed prompt tuning method for learning manipulation affordance with language prompter achieves competitive performance and even outperforms other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot trajectories with a single flow matching policy also leads to consistently better performance than alternative behavior cloning methods, especially given multimodal robot action distributions. Our framework seamlessly unifies affordance model learning and trajectory generation with flow matching for robot manipulation.", "sections": [{"title": "Introduction", "content": "Recent advances in vision-language models (VLMs) present unprecedented opportunities to solve robot manipulation problems. Attempts in the field have focused on three primary aspects: 1) End-to-end learning manipulation from scratch. These approaches [Padalkar et al., 2023] make the least assumptions on tasks and are formulated in language- image-to-action prediction models. 2) Off-the-shelf-vision-language models for robot manipulation. Such line of works have explored using pre-trained VLMs with prompt engineering in various contexts of robot motion learning, including reward design for reinforcement learning [Ma et al., 2023], python coding [Liang et al., 2023], joint actions [Wang et al., 2023], etc. 3) Intermediate substrate to bridge high-level language-image instructions and low-level robot policies. These works usually introduce some form of prior derived from human knowledge as an intermediate stage, including affordances [Huang et al., 2023], pre-trained visual representations [Xiao et al., 2022], primitive skills [Ingelhag et al., 2024], etc. In this paper, we follow the third line of work. We propose to learn manipulation affordances, leveraging finetuning vision-language foundation models with a wealth of internalized knowledge. Then the affordance model is used for learning low-level robot policies, and thus helps to alleviate the sample inefficiency problem of end-to-end learning from scratch.\nExtracting affordance knowledge has long inspired the robot community [Xiao et al., 2022]. We particularly focus on affordance learning in multi-task scenarios with text prompting. As shown in Fig. 2, given the same visual scene but with different language instructions, we aim to extract different affordances for robot policy learning through our proposed model. To leverage the ability of pre-trained foundation models while avoiding expensive computational constraints, recent works have explored parameter-efficiently fine-tuning (PEFT) large vision-language models for"}, {"title": "Related Work", "content": ""}, {"title": "Robot Learning from Demonstration", "content": "Imitation learning has been a common paradigm for robots which requires simulated or real-world demonstration data collection [Zhang and Demiris, 2023]. To improve data efficiency, extensive works have been proposed to learn robot policies on the top of visual representations [Liu et al., 2024] instead of end-to-end raw images [Goyal et al., 2023]. Keypoints or affordance heatmaps are often used to provide contact information as visual representation [Liu et al., 2024]. The concept of affordance has been introduced in [Gibson, 2014], referring to the ability to perform certain actions with objects in the context of a given scene. This paper concentrates on using affordances to guide the low level robot manipulation. In terms of network architectures for robot learning, prior works have successfully investigated convolutional networks [Zhang and Demiris, 2022], Transformers [Shridhar et al., 2023], generative adversarial networks [Ho and Ermon, 2016], Energy-Based Models [Florence et al., 2022], etc. However, the collected data is usually expected to be non-convex and multi-modal due to the variability in human demonstrations. Recent works have addressed this problem by reformulating the robot policy as a generative process. Diffusion policy [Chi et al., 2023] has emerged as a powerful class of generative models for behavior cloning by representing a robot's visuomotor policy as a conditional denoising diffusion process. In this work, we investigate Flow Matching [Lipman et al., 2022], a novel generative model that has demonstrated its superiority in image generation, but is much less explored in robotics domains."}, {"title": "Parameter-Efficient Finetuning", "content": "Instruction-aware vision encoding [Gupta et al., 2022] has been extensively studied for various language-vision fusing tasks [Radford et al., 2021]. Given the dominance of large-scale vision-language models, many approaches have been proposed to efficiently finetune a frozen pretrained model for different downstream tasks to speed up training and reduce memory. Two representative methods among them are adapters and prompting. The first type of research varies depending on the adapter that could add extra lightweight modules [Gao et al., 2024] or express the weight updates as a low-rank decomposition of the weight matrix [Hu et al., 2021]. Another line of work focuses on prompting [Liu et al., 2023], which originally primes a frozen pretrained language model for downstream tasks by including a text prompt. Recent works on prompt tuning [Liu et al., 2021] treat prompts as continuous vectors and compute their gradients"}, {"title": "Flow Matching in Robotics", "content": "Despite its recent progress in image generation [Albergo and Vanden-Eijnden, 2022], the application of flow matching in robotics domains remains underexplored. Few prior studies [Braun et al., 2024, Hu et al., 2024, Rouxel et al., 2024] have successfully modeled transformations that smoothly move random samples towards target robot actions. These works are nevertheless limited to simulation tasks or conditioned on state-based input. We propose the flow matching policy to learn multi-task robot behavior from raw images with visual affordances in a single supervised policy."}, {"title": "Methods", "content": ""}, {"title": "Prompt Tuning for Affordance Learning", "content": "Given a pre-trained vision transformer, our objective is to learn a set of text-conditioned prompts to maximize the likelihood of correct affordance labels, as shown in Fig. 3. Only the prompt-related layers and the decoder are being updated during the training, while the vision transformer remains frozen. Inspired by Vision Prompt Tuning [Jia et al., 2022], we also propose shallow and deep network architectures."}, {"title": "Shallow Architecture", "content": "The vision transformer layer takes the image patch embeddings Eo as input, and passes through various layers L to achieve vision features $E_i$, where $E_i \\in R^{M\\times C}$ and C is the channel dimension.\n$E_i = L(E_{i-1}) \\qquad i = 1,2,..., N$\nSimilarly, the text transformer layer could be represented as\n$P_i = L(P_{i-1}) \\qquad i = 1,2,..., N$\nwhere Po denotes the text tokens, text features $P_i$ are obtained through various layers $L'$, where $p_i \\in R^{K\\times C}$."}, {"title": "Deep Architecture", "content": "For the deep architecture, the only difference is that text features $P_i$ are computed through each layer and introduced at the corresponding vision transformer layer's input space:\n$[\\_, E_1] = L_1([P_1, E_0])$\n$[\\_, E_i] = L([P_i, E_{i-1}])$"}, {"title": "Implementation Details", "content": "We adopt the L2 Mean Squared loss between the predicted and ground truth affordances for network training. We add positional embeddings to all the image and language tokens to preserve the positional information. In the subsequent experiments, we will further study multiple model variants, including text and vision fusion, prompt depth, pretrained weights for vision transformer, etc."}, {"title": "Flow Matching Policy", "content": "In this session, we build the robot behavioral cloning policy as a generative process of Flow Matching, which constructs a flow vector that continuously transforms a source probability distribution towards a destination distribution. Flow Matching leverages an ordinary differential equation to deterministically mold data distribution, contrasting with diffusion policy which is based on a stochastic differential equation through introducing noise."}, {"title": "Flow Matching Model", "content": "Given a probability density path $p_t(x)$ and a corresponding vector field $u_t(x)$, the objective loss of flow matching could be described as:\n$L_{FM}(\\theta) = E_{t, p_t(x)} ||V_t(x, \\theta) - u_t(x)||^2\\qquad$(1)"}, {"title": "Flow Matching for Visuomotor Policy Learning", "content": "We extend flow matching to learn robot visuomotor policies. This requires two modifications in the formulation: i) changing the output x to represent robot actions; ii) modeling the flow estimation conditioned on input image observations. Fig. 4 illustrates our model structures.\nIn our case of robot manipulation, x\u2081 in Equation (3) represents the demonstration robot trajectories. xo is the random generated waypoints following a multivariate normal distribution $x_0 \\sim N(0, I)$. x here denotes the complete long-horizon robot trajectory for various tasks. We modify Equation (2) as:\n$V_t(x o t) = f_\\theta(x_t, t o t)$"}, {"title": "Implementation Details", "content": "The whole training process of the flow matching policy is illustrated in Algorithm 1 and Fig. 4. At time step t, the policy predicts flow vectors $v_t$ for each waypoint conditioned on visual affordance observation data ot with Feature- wise Linear Modulation (FiLM) [Perez et al., 2018] as well as the waypoints at the current timestep $x_t$. The visual embeddings ot are obtained through ResNet [He et al., 2016], and the flow model $f_\\theta$ is represented with U-Net. We will ablate various structures later.\nFor the inference procedure, random waypoints are sampled from the source distribution and then flowed into the target trajectory by estimating the flow from t = 0 to t = 1 over single or several steps:\n$x_{t+\\Delta t} = x_t + \\Delta t f_\\theta(x_t, t ot), \\qquad$ for $t \\in [0, 1]\\qquad$(4)"}, {"title": "Experiments", "content": "We systematically evaluate prompt tuning method and flow matching policy against baseline studies. We also investigate how design choices would affect their performance.\nWe construct a real-world dataset with 10 tasks across Activities of Daily Living to test the proposed method. Each task includes 1,000 sets of RGB images, demonstrated robot trajectories, and labeled ground truth of affordances. The data has been manually collected by moving robot end-effectors with kinesthetic teaching. The novelty of our dataset could be summarized as: (i) It contains same scenarios with multiple objects, multi-task affordances and the demonstrated"}, {"title": "Affordance Evaluation with Prompt Tuning", "content": ""}, {"title": "Experiment Setup and dataset", "content": "We use a pretrained ViT-B-16 transformer as a vision backbone. The text layer adopts the classic setup, including Multiheaded Self-Attention, Feed-Forward Networks with LayerNorm and residual connections. As suggested by MAE [He et al., 2022], the decoder is only used for downstream tasks, and could be flexible and lightweight. Thus we use one single transformer decoder layer. For a fair comparison, all the baselines here use self-supervised pretrained MAE weights on ImageNet-21k dataset for the vision transformer model. In ablation studies, we will investigate different pretrained weights. The training parameters for the prompt tuning network include an image size of 224 \u00d7 224, AdamW with a learning rate of 1.5e-4 including Warmup with step-decay, and batch size of 256. The text encoding layers output 76 tokens, as the CLIP method [Radford et al., 2021]."}, {"title": "Baseline Studies", "content": "In this experiment, we benchmark our proposed shallow and deep prompt tuning structures against several commonly used finetuning and instruction-aware vision encoding protocols.\n\u2022 Full finetuning: fully update the text and vision transformer layers and the decoder.\n\u2022 Adapter-based methods: insert MLP layers with residual connections between pretrained frozen transformer layers of vision and language, as customary in the literature [Gao et al., 2024].\n\u2022 Decoder-based methods: These methods treat the pretrained backbone as a feature extractor with fixed weights during tuning, and only the decoder is tuned, as customary in the literature [He et al., 2016].\n\u2022 side-network methods: train a text transformer network on the side and append pretrained vision features and sidetuned text features before being fed into the decoder, as customary in the literature [Ganz et al., 2024].\n\u2022 Cross-attention methods: Similarly to the above side-network methods, the difference here is using cross- attention fusing instead of simple prepending. An example of cross-attention fusing vision and language could be found in the literature [Jiang et al., 2022]."}, {"title": "Main Results", "content": "Table 1 presents the results of prompt tuning on our testing dataset for affordance learning, comparing against baselines. We use two metrics to evaluate our results: (i) L2 error of affordance heatmap estimation (the fourth column), and (ii) L2 distance between the predicted and ground truth of heatmap centers (the fifth column). For the second metric, we label the heatmap affordance of our data with 2D Gaussian blobs centered on the pixel of demonstrated action during data collection. For the inference, we fit Gaussian Mixture Models to determine the heatmap centers. The heatmap error is averaged on each map, and the center error is averaged on per center point.\nFour observations could be made from this result: (i) The deep structure outperforms the other baselines except for the full finetuning. (ii) Full finetuning slightly outperforms deep prompt tuning in terms of heatmap estimation error and heatmap center error. However, the distinction of heatmap center errors (1.78 pixels) remains subtle, given the full image size of 224 \u00d7 224. This outcome is favorable as it indicates that most heatmap errors are caused by the tails of the Gaussian distribution, instead of the center area where the robot actions actually applied on. (iii) As expected, the shallow structure performs suboptimally to the deep structure. In the following ablation studies, we will investigate how the prompt depth affects the performance. (iv) The training dataset only includes a manikin. We observe that it generates well on our testing data with real humans.\nWhat do prompts learn? We show a t-SNE [Van der Maaten and Hinton, 2008] visualization of the embeddings after the last vision transformer layer and before the decoder in Fig. 5. We can see that the points of the same color (e. g., tasks with same language prompts) are embedded together, implying that the representations recover the underlying manifold structure of discriminative task information."}, {"title": "Ablations", "content": "We further ablate model design choices.\nPretrained Weights. We evaluate using MAE self-supervised pretrained weights and supervised pretrained weights trained on ImageNet-21k dataset for the vision transformer model. The results in Table 1 show self-supervised pretrained weights perform better.\nDecoder Input. We apply the decoder on the global output and image-corresponding output after the vision transformer respectively and report results in Table 1.\nDataset Size. We use various amounts of data to train the model. Fig. 6-left shows that prompt tuning has better adaptability than full finetuning when downstream data is scarce.\nPrompt Location. We have seen different conclusions from prior works about whether the vision-language fusion should be integrated at early or late transformer layers. Thus we conduct experiments to insert prompts at various layers. From Fig. 6-right, we can see that inserting prompts to early layers (for example, layer 1-3 from bottom to top) achieves higher loss than inserting to late layers (for example, layer 1-3 from top to bottom). Thus in our case, prompts have greater significance at the late transformer layers. These results are also supported by the nature of vision transformer hierarchy: lower layers mainly capture low-level fundamental visual details, while higher layers focus on high-level concepts that might be vital for downstream tasks.\nIn conclusion, we observe no single method that outperforms all the rest. For scenarios where a small number of parameters or datasize is available, we reckon that prompt tuning remains the preferred approach."}, {"title": "Flow Matching Policy Evaluation", "content": ""}, {"title": "Baseline Studies", "content": "We compare our flow matching policy against two other robot behavior cloning methods: (i) Diffusion Policy [Chi et al., 2023], and (ii) Transformer-based behavior cloning with Mean Square Error Loss, as customary in RVT [Goyal et al., 2023], RT-X [Padalkar et al., 2023].\nNote that we are aware of other well-performed robot behavior cloning methods, including energy-based IBC [Florence et al., 2022], GAIL [Ho and Ermon, 2016], etc. Since extensive studies have been conducted between these methods and diffusion policy, we choose two representative baselines for evaluation. We consider CNN-based and Transformer-based structures for flow matching. The input of flow matching and baselines are the RGB image with visual affordance, and the output is a complete long-horizon trajectory in both 2D pixel space and 3D Cartesian space. All 10 tasks in our"}, {"title": "Main Results", "content": "Table 2 presents the results of flow matching policy on our testing dataset for robot trajectory learning, comparing against baselines. We use two metrics for evaluation: (i) the error of 2D and 3D trajectory estimation, and (ii) the average inference time across various steps, performed using PyTorch with RTX 4090 GPU acceleration. The trajectory error is averaged on each point of the trajectory. We use 32 waypoints to represent the trajectory. In the later session, we will ablate this choice.\nThree observations could be made from this result: (i) Flow matching (CNN-based, 16 steps) outperforms other baselines in terms of 2D and 3D trajectory prediction accuracy. The 3D estimation performs decently with our method using only RGB images as input. But we also observe the 3D estimation could be highly attuned to camera angles. (ii) The Transformer behavior cloning achieves marginal precision. This is expected as it is hindered by the nature of multi-modal action distribution, causing the averaging out across non-convex spaces. (iii) Flow matching with 16 steps achieves slightly faster inference without loss of quality compared to diffusion policy with 16 steps. We hypothesize that flow matching with optimal transport generates straighter flows, and thus causes faster inference.\nFig. 1 showcases more examples of the waypoint motion path and vector field plots of flow matching policy. We could see that flow matching transforms random waypoints to the target trajectory from timestep 0 to 1. The animations in the supplementary video better demonstrate the whole process."}, {"title": "Comparisons against diffusion policy", "content": "Fig. 7 shows the training and testing loss of flow matching and diffusion policy throughout the training process. We can see flow matching exhibits greater stability on both training and evaluation than diffusion model. For the Push-T benchmark evaluation, flow matching also requires fewer steps and better performance (120 steps, score 1.0) to push a T-shaped block to fully overlap with a fixed target using a circular end-effector, compared to diffusion policy (200 steps, score 0.91)."}, {"title": "Ablations", "content": "We further ablate various policy design choices.\nNetwork Structure: As shown in Table 2, CNN-based flow matching achieves better results than transformer-based architecture. We hypothesize that transformer might need additional hyperparameter tuning.\nInference Steps: Table 2 also showcases better performances of flow matching when applying more inference steps, with a trade-off of longer inference time.\nTrajectory Horizon: We empirically test trajectory representation with 8, 16, 32 and 64 waypoints. More waypoints are not necessary, while fewer waypoints are unable to entirely encapsulate the complete long-horizon trajectories. We"}, {"title": "Real-World Robot Experiments", "content": "Lastly, we deploy flow matching and other policies on real robot manipulation evaluation, as shown in Table 3. We carry out 50 replications of trials for each policy. The orientation is predefined. We use a KINOVA Gen3 arm, an Azure Kinect camera, and a 3D printed UMI gripper [Chi et al., 2024] for real-world robot experiments.\nWe observe that most failures occurred during the initial grasping. Even the trajectory prediction is accurate in general, a slight misalignment between the first trajectory waypoint and the object would cause grasping failures. This problem might be addressed in the future by a weighted flow matching policy, which highlights the initial waypoints by adding higher probability weights. Since we have not considered closed-loop actions, the robot might also collide with objects in the process of moving to the first waypoint. Some out-of-distribution factors also lead to failure, including background and camera view changes. Larger data sizes and more diversity should alleviate this problem.\nWe also investigate how the affordance would guide the flow matching policy. We trained flow matching for each task separately, taking the raw RGB images as input. It achieves a higher loss but a similar success rate compared to training all tasks in one policy with affordance guidance. Interestingly, a further comprehensive examination reinforces the argument that flow matching could handle multimodal action distribution. Fig. 8 shows an example. From the left figure, we can see that when training a separate policy of moving the towel toward the trash for sweeping, the predicted trajectory (yellow) could be detached from the ground truth (red), but still a reasonable solution that allows for a successful robot execution. With affordance guidance, the prediction is closely aligned with the truth (Fig. 8-right)."}, {"title": "Conclusion", "content": "We have formulated a prompt tuning method for affordance learning and flow matching policy for robot manipulation. The core idea of prompt tuning is to maximally exploit the pretrained foundation model, and rapidly excavate the relevance of foundation and downstream affordance learning tasks. Our focus in this work is not to outperform state-of- the-art general robot manipulation research. Instead, we have systematically studied flow matching framework, which provides an alternative to diffusion policy. The results suggest forsaking the stochastic construction in favor of more directly learning the probability path, allowing for improved generation. We qualitatively and quantitatively experiment on the multi-task robot manipulation scenarios to prove the ease of training and evaluation for flow matching. We could further improve performance by incorporating the robot state as an extra input of flow matching, estimating closed-loop"}]}