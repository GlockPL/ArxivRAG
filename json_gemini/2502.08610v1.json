{"title": "Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current Al Standards", "authors": ["Keerthana Madhavan", "Abbas Yazdinejad", "Fattane Zarrinkalam", "Ali Dehghantanha"], "abstract": "As Al systems increasingly integrate into critical infrastructure, their security implications within AI compliance standards demand urgent attention. This paper conducts a comprehensive security audit and quantitative risk analysis of three prominent Al governance frameworks: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. We employ a novel methodology that combines a rigorous line-by-line audit process, performed by five researchers and validated by four industry experts, with a quantitative risk assessment framework. We develop metrics such as the Risk Severity Index (RSI), Attack Vector Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS) to quantify security concerns in these standards. This analysis identifies 136 distinct concerns across the frameworks, revealing significant gaps between compliance and actual security. The NIST framework leaves 69.23% of identified risks unaddressed, ALTAI demonstrates the highest vulnerability to attack vectors with an AVPI of 0.51, and the ICO AI Risk Toolkit exhibits the largest compliance-security gap, with 80.00% of its high-risk concerns remaining unresolved. Our root cause analysis, quantified through RCVS, identifies under-defined processes (average RCVS of 0.33 for ALTAI) and insufficient implementation guidance (average RCVS of 0.25 for NIST and ICO) as major contributors to these vulnerabilities. This research offers actionable insights for policymakers and organizations implementing AI systems, emphasizing the urgent need for more robust, specific, and enforceable security controls within AI compliance frameworks. We provide targeted recommendations to enhance the security posture of each standard, bridging the gap between compliance and genuine security in Al governance. Supporting code is anonymously available at https://anonymous.4open.science/r/Quantifying-AI-Standards-Risks-C45F.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has become integral to sectors such as healthcare, finance, and transportation, transforming industries through enhanced operational efficiency, innovation, and decision-making [55]. However, the rapid integration of AI into critical infrastructure introduces new security vulnerabilities, including model poisoning, data leakage, adversarial attacks, and malicious inputs [45, 50]. For instance, adversarial attacks have led to misclassifications in autonomous vehicles, posing significant safety risks [21], while data poisoning has compromised healthcare AI models, resulting in misdiagnoses and flawed treatment recommendations [29]. The urgency of addressing these risks is underscored by the Al Incident Database, which reported a 55-incident increase in AI-related security breaches in 2023, marking a notable rise in vulnerabilities [11].\nWhile existing AI compliance standards-such as NIST AI RMF 1.0, ICO AI Risk Toolkit, and the European Commission's ALTAI-provide guidance on risk management, privacy, and ethics, they often fail to explicitly address security vulnerabilities [34]. Nevertheless, many organizations adopt these frameworks as quasi-security guides, assuming compliance ensures protection-a premise that has not been fully tested [31?]. In practice, the ICO AI Risk Toolkit and ALTAI were originally designed to safeguard rights or promote trustworthy AI, rather than implementing stringent security controls. Our analysis highlights that this broader usage introduces gaps, since these frameworks do not systematically cover ai-specific threats-potentially leaving AI systems vulnerable [14, 17]. We do not suggest these frameworks fail at their original missions; rather, we expose a mismatch between organizations' reliance on them for security and their actual scope, where security is treated more as a peripheral concern. This gap exposes organizations to financial loss, reputational damage, and operational risks, since \"compliance alone\" does not necessarily guard against sophisticated AI-specific threats such as adversarial attacks and model tampering [20, 67].\nExisting frameworks also remain too generalized to address the nuanced security needs of AI systems, often relying on principle-based guidance that can lead to inconsistencies in implementation [36, 39]. Recent analyses show these standards lack robust countermeasures for Al-specific threats, leaving unresolved gaps such as vague definitions, unenforceable security controls, and insufficient direction on managing third-party AI components [18, 30, 63]. Motivated by these gaps, we pose the following central research question: How effectively do current Al compliance standards protect against Al-specific threats when adopted as security guidance? To investigate this, we conducted a line-by-line audit of three globally recognized standards-NIST AI RMF 1.0, ICO's Al and Data Protection Risk Toolkit, and ALTAI-identifying 136 distinct security concerns. Our analysis revealed systemic issues including ambiguous specifications, insufficient data protection measures, and challenges in enforcing security controls, particularly for third-party AI components and unforeseen uses of Al systems.\nThis research makes the following key contributions:"}, {"title": "2 Related Work", "content": "Efforts to regulate AI through standards have been extensive, yet no established mandatory standards exist. The European Commission's risk-based approach through the AI Act aims to ensure AI systems are safe, transparent, and adhere to fundamental rights [49]. Systematic studies, such as those by Xia B et al., point out that the ability of these frameworks to assess and mitigate Al risks is not well understood [68]. This is further evidenced by research identifying challenges developers face in industrial fields, including ambiguous terminologies, lack of domain-specific concreteness, and non-specific requirements [40].\nThis indicates that existing standards and regulations alone may not guarantee Al system security. When organizations focus solely on compliance, potential vulnerabilities not explicitly addressed by the standards can be overlooked. This overemphasis on compliance often results in a false sense of security, leaving systems vulnerable [9, 57]. Moreover, simple compliance may not guarantee protection, as evidenced by a study identifying 148 issues of varying severity across three major digital compliance standards [60]. Key issues included insufficiently specific security requirements, lack of guidelines for rapid response to threats, and absence of provisions for ongoing system monitoring. These findings underscore the need for our proposed line-by-line audit to offer a more holistic understanding of security gaps and to guide the development of more robust AI compliance standards.\nAdditionally, selecting appropriate security standards and extracting requirements within an organizational context can be challenging. The complexity of finding the most suitable cybersecurity solutions for an organization is underscored by a study of public organizations in Ecuador [35]. This complexity implies that compliance alone may not ensure the security of AI systems. Consequently, organizations may need to adopt tailored security measures beyond compliance to manage their risks effectively. Historically, compliance audits have used reward-driven and penalty-based approaches to promote adherence to minimum security standards. However, these approaches may not encourage organizations to implement additional security measures beyond the basic requirements for compliance [23, 41]. This suggests that while compliance standards are needed for maintaining a baseline level of cybersecurity, they may not be sufficient to ensure complete protection. Hence, organizations need to continuously assess their specific risks, adopt relevant controls, and monitor the effectiveness of their cybersecurity programs.\nExisting research indicates that although current frameworks aim to address security risks in Al systems, there is still no consensus on how to effectively define and evaluate these risks [51, 59]. Anderljung et al. point out that we currently lack a robust and comprehensive set of evaluation methods to operationalize these standards, which are necessary to identify and mitigate the potentially dangerous capabilities and emerging risks associated with advanced Al systems [15]. This issue is further exacerbated by the absence of concrete solutions or structured formats for presenting these evaluations [33, 46].\nWhile previous studies have examined the effectiveness of AI compliance standards, they often fall short of providing a detailed, line-by-line analysis of these standards to determine whether the existing controls are sufficient to ensure Al security. Such an analysis is crucial for uncovering potential gaps and oversights in security measures, and for evaluating whether the current standards provide adequate safeguards against emerging AI-specific threats. Our research addresses this critical gap by conducting a comprehensive, line-by-line examination of leading AI compliance standards. This approach allows us to assess the adequacy and effectiveness of existing controls, identify potential security vulnerabilities, and determine whether these standards provide sufficient guidance to ensure the security of AI systems in practice. By doing so, our study aims to highlight potential security risks that might be overlooked in more general analyses and to evaluate whether current compliance standards are truly fit for purpose in the rapidly evolving landscape of AI security.\nTo address the gaps identified in existing research, we developed a novel methodology that combines a detailed security audit of Al compliance standards with quantitative risk assessment. The following section outlines our approach."}, {"title": "3 Research Methodology", "content": "The study, conducted between May 2023 and May 2024, received ethical approval from the University Research Ethics Board. Participants provided informed consent, and data confidentiality was maintained."}, {"title": "3.1 Selection of compliance standards", "content": "Our selection of compliance standards was based on a systematic review of 16 globally recognized Al frameworks identified by Xia et al. (2023) [68]. We focused on frameworks developed between 2016 and 2023 by leading technology companies, government agencies, and industry consortia, evaluating them on risk assessment guidance, alignment with Responsible AI principles, technical depth, and regional relevance. Selection criteria included applicability to our research needs, global recognition, and distinct perspectives on security aspects. The AI compliance standards selected for our audit are NIST AI RMF 1.0 (2023) [64], providing global guidance for managing Al system risks; UK's AI and Data Protection Risk Toolkit (2020) [12], focusing on data protection; and EU's ALTAI (2020) [58], ensuring ethical AI use within the EU. These were chosen for their comprehensive coverage of global, national, and regional perspectives on AI compliance. For a detailed analysis of each standard, refer to Appendix A."}, {"title": "3.2 Participant Recruitment", "content": "This study engaged nine participants with extensive experience in cybersecurity, compliance, and risk management across academia, industry, and government sectors. The research team comprised five researchers and four industry experts, with an average of 22.5 years of professional experience. We employed purposive sampling to recruit five researchers from diverse backgrounds [22]. These researchers conducted a detailed audit of three AI standards, following the process described in Section 3.4. To validate our findings, we recruited four Subject Matter Experts (SMEs) from industry. We outline the expert validation process and criteria in Section 3.6.\nFor detailed participant information and recruitment methods, refer to Appendix B."}, {"title": "3.3 Audit Methodology for AI Compliance Standards", "content": "We refined the systematic auditing approach by Stevens et al. (2020) to identify security issues in Al compliance standards, enhancing it with a quantitative risk assessment framework [61]. This approach offers advantages over traditional methods like random sampling or purely qualitative analysis, which may overlook nuances or lack structural rigor [25, 62]. By focusing on security concerns and integrating quantitative metrics, our audit addresses a gap in existing research that often examines standards primarily for trust, privacy, and ethics aspects. The methodology involves a line-by-line audit of selected AI compliance standards, followed by expert validation and quantification. This ensures both accuracy and an objective evaluation of security risks. For each identified security concern, we provide a detailed explanation, specific examples from the standards, quantitative risk assessment, and relevant real-world incidents that illustrate potential consequences. This comprehensive approach helps identify potential security vulnerabilities that might be missed in broader analyses and demonstrates their practical implications through historical precedents."}, {"title": "3.4 AI Compliance-standard Audit Process", "content": "Audit Objective: The primary goal of this audit was to identify potential security concerns within AI compliance standards that could undermine the security of AI systems. The audit focused on policies that present risks of data exposure and processes characterized by unclear implementation guidelines. To achieve this, a detailed line-by-line analysis of three prominent AI compliance standards was conducted. In this study, a \"security concern\" is defined as any recommendation or policy that, if implemented as written, could compromise security measures, potentially leading to unauthorized access or the exposure of sensitive data.\nAudit Process: The audit commenced with independent analyses conducted by each researcher. These analyses were guided by the audit's objective and employed a content analysis methodology grounded in established social science research principles [66]. Researchers documented their findings at the conclusion of each section within the standards under review. Each documented issue included the title of the section where it was identified, the specific phrase or provision deemed problematic, a brief description of the issue, and, where applicable, references to publicly known issues. In instances where multiple issues were identified within a single phrase or section, each issue was logged separately to ensure comprehensive coverage.\nUpon completing the standards examination, the researchers flagged issues based on specific criteria. An issue was flagged as (1) if it was independently identified by multiple researchers. Alternatively, an issue was flagged as (2) if there was disagreement among the researchers regarding its significance. In cases where no unanimous consensus could be reached, the issue was discarded from the final list of concerns but maintained as a record of disagreement. This approach ensured that all perspectives were considered while maintaining the integrity of the final audit outcomes.\nTo validate the consistency of the findings, inter-coder reliability was calculated using Krippendorff's \u03b1 (Alpha), a statistical measure that accounts for chance agreement [16]. This metric was chosen for its suitability in evaluating agreement in nominal data, particularly when categorizing data points based on the presence or absence of a security concern. The analysis yielded inter-coder reliability values of 0.88 for the NIST AI RMF 1.0, 0.84 for the ICO AI Risk Toolkit, and 0.90 for the ALTAI standard. An \u03b1 value of 0.8 or higher indicates a high level of reliability in the audit process, effectively mitigating the likelihood of chance agreements among researchers and confirming the consistency of identified security concerns.\nFollowing the verification of identified issues, the research team proceeded to analyze and categorize these security concerns through an iterative open coding process. This process involved the application of categorical labels-referred to as a \u201ccodebook"}, {"title": "3.5 Risk Quantification Framework", "content": "We present a quantitative risk framework designed to provide objective and comparable measures of security across AI compliance standards. This framework quantifies vulnerability severity, identifies root causes, and evaluates the robustness of each standard, forming the foundation for our comparative analysis and recommendations for enhancing AI compliance.\nOur framework is grounded in established risk management principles, including NIST SP 800-30, ISO 31000, and Composite Risk Management (CRM). Risk is quantified using a probability-impact approach, with probability values ranging from 1 (Unlikely) to 5 (Frequent) and severity values from 1 (Negligible) to 4 (Catastrophic). These four impact levels align with widely accepted risk assessment models\u00b9, ensuring a consistent and interpretable foundation for risk quantification.\nUsing the qualitative audit process described in Section 3.4, we compute key risk metrics based on these probability and severity values. Central to this framework is the Risk Score (RS) assigned to each identified concern\u00b2. The RS supports the calculation of quantitative metrics that drive our analysis."}, {"title": "3.5.1 Risk Score (RS):", "content": "Each concern i is assigned a Risk Score (RS) as:\n$RS_i = Probability_i \\times Impact_i$  (1)\nwhere $Probability_i \\in \\{1, 2, 3, 4, 5\\}$ represents the likelihood of occurrence and $Impact_i \\in \\{1, 2, 3, 4\\}$ represents the severity of the concern."}, {"title": "3.5.2 Risk Severity Index (RSI):", "content": "The Risk Severity Index (RSI) quantifies overall risk exposure across all identified concerns:\n$RSI = \\frac{\\sum_{i=1}^{n} RS_i}{n}$  (2)\nwhere n represents the total number of concerns. RSI provides a concise measure of the overall risk severity in a framework, supporting cross-framework comparisons."}, {"title": "3.5.3 Root Cause Vulnerability Score (RCVS):", "content": "The Root Cause Vulnerability Score (RCVS) quantifies the contribution of each root cause to the overall risk:\n$RCVS_c = \\frac{\\sum_{i\\in c} RS_i}{\\sum_{i=1}^{n} RS_i}$  (3)\nwhere:\n$\\sum_{i\\in c} RS_i$ is the sum of risk scores for all concerns in category c.\n$\\sum_{i=1}^{n} RS_i$ is the total risk score for all concerns.\nThis metric identifies the root causes (e.g., under-defined processes, ambiguous specifications) that contribute most significantly to overall risk\u00b3. Categories with higher Root Cause Vulnerability Scores (RCVS) indicate a greater impact on total risk."}, {"title": "3.5.4 Attack Vector Potential Index (AVPI):", "content": "The Attack Vector Potential Index (AVPI) measures how unresolved vulnerabilities from root causes contribute to the system's attack surface:\n$AVPI = \\sum_{c=1}^{k} (\\frac{C_c}{C_{total}} \\cdot RCVS_c)$  (4)\nwhere:\n$C_c$ is the number of concerns in root cause category c.\n$C_{total}$ is the total number of concerns.\n$RCVS_c$ is the Root Cause Vulnerability Score for category c.\nk is the number of distinct root cause categories.\nThe Attack Vector Potential Index (AVPI) measures the system's exposure to potential attack vectors4"}, {"title": "3.5.5 Compliance-Security Gap Percentage (CSGP):", "content": "The Compliance-Security Gap Percentage (CSGP) quantifies the share of high-risk and extremely high-risk concerns that remain unaddressed:\n$CSGP = \\frac{C_{unaddressed}}{C_{total}} \\times 100$  (5)\nwhere:\n$C_{unaddressed}$ is the number of concerns classified as High (H) or Extremely High (E) risk.\n$C_{total}$ is the total number of concerns."}, {"title": "3.5.6 Justification of the Quantification Framework", "content": "The quantification framework is based on principles from NIST SP 800-30, ISO 31000, and Composite Risk Management (CRM). Each metric addresses a critical dimension of risk assessment: the Risk Severity Index (RSI) captures overall risk severity, the Root Cause Vulnerability Score (RCVS) identifies the most impactful root causes, the Attack Vector Potential Index (AVPI) highlights systemic exposure to attacks, and the Critical Severity Gap Percentage (CSGP) quantifies unresolved high-risk concerns. By employing a probability-impact approach with well-defined risk parameters, the framework ensures computational efficiency, scalability, and interpretability, enabling policymakers and security practitioners to effectively prioritize risk mitigation efforts."}, {"title": "3.6 Expert validation process", "content": "To obtain external validation of our findings, our four experts, as shown in Table 5, from real-world organizations helped validate the findings from the researchers. We asked these experts to categorize the security concern we identified into one of three categories: (1) confirmed, (2) plausible, or (3) rejected. A confirmed security concern is one that the expert has previously encountered or observed its consequences within an enterprise environment. A plausible issue is one that the expert hasn't personally encountered but agrees could potentially arise in other organizations or if the controls were implemented as stated. A rejected issue is one where there's no observable evidence of security concerns in a live environment or there are related security factors we hadn't considered.\nWe employed both closed and open-ended survey questions to collect insights from each expert. Besides simply confirming or dismissing each identified issue, we also encouraged experts to share relevant personal experiences, adding depth to their responses. We presented the issues to the experts in a randomized order through an Excel workbook, providing the referenced section title, exact text from the section, a brief detail of the security concern, a description of the perceived issue, and the standard document. In our study, the expert validation process was governed by a consensus threshold of 75%. For a finding to be accepted in our panel of four experts, it requires the concurrence of at least three experts, which represents 75% agreement. Conversely, a finding where only two experts agree, representing a 50% agreement, is rejected. This strong consensus requirement aligns with established inter-coder reliability practices, enhancing the credibility of our results [28]. After gathering data from each expert, we removed the rejected finding. We also held open-ended discussions with the experts to discuss similarities and differences in assessments."}, {"title": "3.7 Limitations", "content": "Our study has several limitations that warrant consideration. Firstly, our analysis focused on three AI compliance frameworks, potentially limiting the global applicability of our findings. This approach may not generalize well to regions with different regulatory environments, cultural attitudes towards AI, or technological infrastructures. The expert validation process, while rigorous, involved a relatively small sample of four industry experts, which may not fully capture the complexities of real-world security issues across diverse contexts. Our methodology did not account for false negatives, possibly overlooking some security concerns. Additionally, we conducted audits in isolation, assuming flawless implementation of each standard, which may not reflect real-world scenarios where multiple security controls interact. The subjective nature of our risk categorization, relying heavily on expert judgment, introduces potential bias. Interpretations of risk severity may vary across different contexts, highlighting the need for more standardized assessment guidelines. Lastly, the rapidly evolving nature of AI technology means that some of our findings may become outdated as new challenges emerge. Despite these limitations, our methodology offers a robust framework for evaluating AI standards. Future research should address these constraints by expanding the scope to diverse geographic regions and industries, involving a larger"}, {"title": "4 Audit Results", "content": "The audit of AI compliance standards included a thorough evaluation of three primary documents: NIST AI RMF 1.0, ALTAI HLEG EC, and the ICO AI Risk Toolkit. Across these standards, we identified a total of 136 security concerns, classified by their severity and root causes. Table 3 provides a breakdown of these concerns by document and assessed risk levels. Following the CRM framework, these concerns are classified and assessed within a risk matrix Figure 1."}, {"title": "5 Evaluation: AI and Data Protection Risk Toolkit", "content": "We identified 30 security concerns within the standard text. These concerns were evaluated and classified according to their impact levels: 3 extremely high concerns, 16 high-risk concerns, and 11 medium-risk concerns. The impact level assessment was crucial in identifying and categorizing the perceived security concerns in AI systems, highlighting the potential risks and vulnerabilities in data handling and protection. Furthermore, our analysis chose to omit two incidents of unenforceable security control and ambiguous specification. Expert validation determined that these incidents did not create insecure conditions or promote insecure practices.\nRefer to Figure 2a that further illustrates these findings. The heatmap's gradient indicates the frequency of security concerns across different impact and probability levels, with darker shades signifying more frequent occurrences. These shades highlight areas with higher event frequencies but don't necessarily correspond to higher risk levels, indicating that frequent issues can span from low to high severity. Below, we present detailed examples of findings based on their perceived root cause."}, {"title": "5.1 Root cause analysis", "content": ""}, {"title": "5.1.1 Data vulnerability", "content": "The 11 security concerns identified primarily stemmed from weak data flow mapping and protection measures. Section 1.3 of the standard suggests but does not mandate data flow mapping, creating a gap in data security protocols. This omission exposes systems to increased risks of data breaches, regulatory non-compliance, and privacy violations, highlighting the necessity for mandatory data mapping to ensure comprehensive data protection and adherence to regulatory standards.\nReal-World examples illustrating the risks: The security implications of inadequate data flow mapping are vividly illustrated by specific AI vulnerabilities, such as the arbitrary file write vulnerability identified in MLFlow, known as CVE-2023-6975 [47]. With a Common Vulnerability Scoring System (CVSS) severity rating of 9.8, this vulnerability underscores the dire consequences of unauthorized access and malicious activities potentially due to gaps in data flow oversight. Furthermore, the risk of poisoned training data, particularly in large language models, exemplifies how compromised data integrity can lead to biased outputs, security breaches, or complete system failures. These occurrences emphasize the importance of accurate data flow mapping in Al systems to uphold data integrity, security, and overall dependability. Addressing the complex data flows in Al requires scalable and sophisticated approaches. Tools and methodologies designed for automated data discovery and mapping, such as AIMap from Adeptia, offer viable solutions [13]. These allow for the efficient identification and protection of data pathways. While we understand that it is not practical for an organization to map every single data flow in the system, adopting a risk-based approach enables the prioritization of essential data flows. This ensures that robust security measures are implemented where they are most needed."}, {"title": "5.1.2 Under-defined process", "content": "Our analysis identified 9 security concerns related to the absence of clear guidelines for secure implementation. Section 1.7 of the standard merely recommends regular discussions on personal data collection without providing specific data requirements, emphasizing data minimization principles, or outlining the necessary justifications for data collection. These are elements recognized in the General Data Protection Regulation (GDPR) and other privacy frameworks to prevent unauthorized data access and ensure compliance with evolving regulatory landscapes. The absence of precise and comprehensive processes for data collection and utilization poses privacy risks and directly contributes to security vulnerabilities. In the context of AI systems, which often process vast amounts of sensitive and personal information, this oversight can lead to inadequate protection against unauthorized access and potential data breaches. The implications of mishandling such data are profound, affecting not only privacy violations but also AI applications' ethical and societal impacts. Diligent internal oversight and discussions are emphasized to prevent data misuse and ensure responsible AI management.\nReal-World Examples Illustrating the Risks: In 2022, several U.S. tax filing websites, including H&R Block, TaxAct, and TaxSlayer, used the Meta Pixel to collect and transmit sensitive financial information of taxpayers to Meta [32]. This data included names, email"}, {"title": "5.1.3 Ambiguous specification", "content": "We have pinpointed 8 security concerns rooted in the vagueness of the control measures prescribed by the standard, especially evident in sections that overlook the practical application of data collection and processing guidelines. For example, Section 2.2's vague recommendation for \u201cappropriate\" technical measures for bias mitigation lacks the precision needed for effective implementation. Similarly, the requirement in Section 2.7 for information to be \"easily accessible and easy to understand\" introduces subjectivity, lacking a consistent benchmark for ease and accessibility. This ambiguity in specifications can result in a wide range of interpretations, leading to uneven application and the potential for non-compliance with regulatory standards. Without clear guidelines, stakeholders are forced to interpret the requirements, navigating the gray areas of data handling and processing. This situation escalates the risks of bias, privacy violations, and ensuing legal challenges.\nReal-World Examples Illustrating the Risks: The controversy around Microsoft's AI chatbot Tay in 2016 is a direct consequence of ambiguous data collection parameters, culminating in the bot generating offensive content after interacting with a particular user subpopulation [2]. This oversight, a direct consequence of ambiguous data handling protocols, allowed the bot to generate and disseminate offensive content after being exposed to harmful interactions with a subset of users. This incident underscores the need for explicit data management instructions to prevent similar misuse of technology. Similarly, the Google+ incident 2018, where a software flaw exposed users' private data, underscores the consequences of unclear data usage policies. The flaw resulted from poorly defined security parameters within the platform's data management"}, {"title": "5.1.4 Unenforceable security control", "content": "We have identified 2 high-risk concerns where security controls are effectively unenforceable due to vague execution details. The standard's recommendation in Section 2.1 to consult with a data protection officer on lawful data processing bases is non-specific, potentially leading to inconsistent interpretations and applications. Section 4.6 similarly falls short by vaguely advising regular reviews of processing and privacy notices without concrete steps, risking deviation from original data purposes. When rules are not clear or enforceable, organizations might process data without proper authorization or fail to keep the necessary records. This can weaken efforts to protect data and comply with privacy laws. From what we've seen, relying on vague guidelines can accidentally lead organizations to break these laws or privacy norms. The original standard advice was to regularly check with a Data Protection Officer (DPO) to ensure data processing is legal. Initially, we deemed this guidance too ambiguous to be actionable, as it lacked specific implementation instructions. However, following further expert consultations, we recognize that this control can be enforceable through proper documentation of the consultation process.\nReal-World Examples Illustrating the Risks: An example of this can be seen in instances where organizations, due to unclear guidelines, fail to consult appropriately with their DPOs, leading to unauthorized data processing and breaches of privacy laws. Without clear documentation and enforceable steps, such consultations are prone to inconsistency and inadequate compliance, exposing the organization to legal and reputational risks."}, {"title": "5.2 Expert recommendations", "content": "In our expert validation process, our four recruited professionals carefully examined the findings and provided valuable feedback, resulting in additional recommendations. Each expert brought forth their perspective and insights:\nE1, specializing in data governance, emphasizes the role of data flow mapping in identifying and mitigating potential security vulnerabilities. To transition from best practice to mandatory standard,"}, {"title": "6 Evaluation: ALTAI\u0399", "content": "Our audit identified 28 security concerns within the standard text. These concerns were evaluated and classified according to their impact levels: 17 at high risk, 10 at medium risk, and 1 at low risk. In alignment with our expert validation process, we excluded one concern about unenforceable security control. Our experts determined that machine learning experts can implement this particular control without introducing any insecure practices or compromising the overall security measures. Refer to Figure 2b for a visual representation of the correlation between the root cause and its respective probability and severity.\nBelow, we present the discovered ALTAI Security concerns, categorized by root cause, and provide detailed explanations."}, {"title": "6.1 Root cause analysis", "content": ""}, {"title": "6.1.1 Under-defined processes", "content": "We have identified 19 security concerns originating from under-defined processes in Al systems. These issues stem from a combination of factors: a lack of transparency, leading to user confusion; an excessive dependence on AI in fields like healthcare, where its decision-making can be obscure and problematic; an absence of adequate oversight and control, resulting in unchecked AI operations; and poorly defined methods for evaluating Al's outputs, which poses risks due to unverified decisions.\nReal-World Examples Illustrating the Risks: The repercussions of these under-defined processes are vividly highlighted through two cases: the UK Algorithmic Grade Prediction scandal of 2020 and the criticisms faced by IBM's Watson for Oncology in 2018 [5, 8].\nThe UK Algorithmic Grade Prediction controversy showcases the pitfalls of insufficient transparency and oversight in AI decision-making. Employing an algorithm to predict student grades without clear guidelines led to public outrage, as outcomes were perceived as unfair and biased. This incident underscores the need for transparency in Al systems, particularly when their decisions impact individuals' lives and futures [24]. Similarly, the debate around IBM's Watson for Oncology in 2018 elucidates the risks tied to an over-reliance on AI in making healthcare decisions without proper human oversight. The system's sometimes contradictory recommendations to established medical practices emphasize the dangers of operating AI systems without transparent decision-making processes and expert validation [19].\nThese examples underscore the need for clear operational guidelines and rigorous oversight in Al systems, particularly in sensitive fields like education and healthcare. Well-defined processes are crucial for evaluating AI outputs, ensuring transparent decision-making, and deploying trustworthy systems, ultimately reducing operational risks and security vulnerabilities."}, {"title": "6.1.2 Ambiguous specification", "content": "Ambiguous specifications in Al systems have led to the identification of 5 security concerns, particularly highlighted by the unclear boundaries around human-AI interaction and the simulation of social interactions. This ambiguity blurs the line for users between interacting with humans or Al systems and obscures the autonomous nature of Al decisions, risking the incorporation of unintended biases into the decision-making process. When Al systems lack clear specifications, users might harbor unrealistic expectations or mistrust towards these systems due to a lack of communication about their technical limitations and potential risks. Additionally, without a consistently applied definition of fairness, discrimination issues could be amplified throughout the Al lifecycle.\nReal-World Examples Illustrating the Risks: The repercussions of these ambiguities are starkly evident in cases like Amazon's recruiting tool and Optum's healthcare algorithm. Amazon's tool, which utilized unclear criteria for evaluating candidates, perpetuated gender biases by systematically undervaluing female applicants [27, 42]. This case highlights how ambiguity in AI specifications can lead to direct, systematic discrimination. Similarly, the healthcare algorithm developed by Optum, by not clearly defining healthcare needs, inadvertently skewed resource allocation in favor of certain racial groups over others [6, 53]. This serves as an example of how vague Al specifications can result in unfair resource distribution and reinforce societal biases."}, {"title": "6.1.3 Data vulnerability", "content": "We identified four vulnerabilities related to inadequate risk assessment and security guidelines for AI systems, as highlighted in Technical Robustness and Safety (Requirement 2) and Privacy and Data Governance (Requirement 3). These standards advocate for \"state-of-the-art\" privacy and data protection yet lack clear definitions and implementation guidance. This"}, {"title": "6.2 Expert recommendations", "content": "Our experts, who have applied the ALTAI framework to their own Al systems and security measures, provided valuable insights into the practical implications of our findings. The validation process confirmed the majority of our identified security concerns. However, the experts also provided additional context and nuance, highlighting the complexity of implementing robust security measures in AI systems.\nE1 emphasizes the need for clear process definitions in AI systems, which should include detailed guidelines on user interactions, data handling, and responses to various scenarios. Regular training for stakeholders is important to ensure they fully understand these processes, addressing problems like lack of transparency and inadequate oversight that can lead to security vulnerabilities. E1 notes that companies such as Microsoft have implemented their own responsible AI governance frameworks, focusing on human-AI interaction guidelines and the importance of training stakeholders in Al operations.\nE2 and E3 highlight the need for robust data protection measures. They advocate for implementing strong encryption, secure data storage, and processing methods alongside a lifecycle approach to data protection. This recommendation is particularly pertinent in light of vulnerabilities related to inadequate risk assessment and security guidelines. Industries such as healthcare and finance, regulated by standards like HIPAA and SR-11-7, respectively, exemplify the adoption of these measures. They employ advanced encryption and secure data processing techniques, showcasing a commitment to protecting data throughout its lifecycle, from creation to disposal.\nE4 recommends the development of enforceable security controls, including clear policies, robust access control measures, and regular monitoring systems to detect policy violations. The suggestion to use automated enforcement tools, such as policy enforcement points (PEP), ensures consistent application of security controls. This is mirrored in regulatory frameworks like Canada's Directive on Automated Decision-Making and the European Union's AI Act, which mandate regular monitoring and strict compliance with security policies for Al systems."}, {"title": "7 Evaluation"}]}