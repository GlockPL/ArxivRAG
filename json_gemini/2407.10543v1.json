{"title": "Understanding the Dependence of Perception Model Competency on Regions in an Image", "authors": ["Sara Pohland", "Claire Tomlin"], "abstract": "While deep neural network (DNN)-based perception mod- els are useful for many applications, these models are black boxes and their outputs are not yet well understood. To confidently enable a real- world, decision-making system to utilize such a perception model with- out human intervention, we must enable the system to reason about the perception model's level of competency and respond appropriately when the model is incompetent. In order for the system to make an intelligent decision about the appropriate action when the model is incompetent, it would be useful for the system to understand why the model is in- competent. We explore five novel methods for identifying regions in the input image contributing to low model competency, which we refer to as image cropping, segment masking, pixel perturbation, competency gradi- ents, and reconstruction loss. We assess the ability of these five methods to identify unfamiliar objects, recognize regions associated with unseen classes, and identify unexplored areas in an environment. We find that the competency gradients and reconstruction loss methods show great promise in identifying regions associated with low model competency, particularly when aspects of the image that are unfamiliar to the per- ception model are causing this reduction in competency. Both of these methods boast low computation times and high levels of accuracy in de- tecting image regions that are unfamiliar to the model, allowing them to provide potential utility in decision-making pipelines.", "sections": [{"title": "1 Importance of Understanding Model Competency", "content": "Deep neural networks (DNNs) have been very successful in image classification tasks. However, DNN models are considered black boxes, and there currently exists only a shallow understanding of how these models obtain their outputs [1]. This greatly limits our ability to trust these models in real-world systems, where their failure could be detrimental without human intervention. To confidently employ DNN-based perception models in the real world, we need to develop more robust and generalizable models, but we also need to gain a deeper understanding"}, {"title": "2 Background & Related Work", "content": "In this work, we explore explanations for model competency-a generalized form of predictive uncertainty. Predictive uncertainty generally arises from three factors: data (or aleatoric) uncertainty, model (or epistemic) uncertainty, and distribu- tional shift [4]. Data uncertainty refers to uncertainty arising from complexities of the data (i.e., noise, class overlap, etc.), while model uncertainty reflects the ability of the perception model to capture the true underlying model of the data [5]. These types of uncertainty are explored in work referred to as uncertainty quantification (Section 2.1). Distributional shift refers to mismatched training and test distributions [6]. Extensive work has explored methods to detect inputs that are out-of-distribution (OOD) (Section 2.2). This paper aims to expand upon work on uncertainty quantification and OOD detection with the goal of offering explanations for why a perception model's prediction is uncertain. This idea of explaining perception model competency is related to previous work within the area of explainable image classification (Section 2.3), which seeks to offer explanations for the classification decisions of perception models. Our work differs from existing work in this area in that we seek to offer explanations for low model competency, rather than explanations for a model's prediction."}, {"title": "2.1 Uncertainty Quantification", "content": "Extensive research has been done on methods to understand and quantify uncer- tainty in a neural network's prediction. The modeling of these uncertainties can"}, {"title": "2.2 Out-of-Distribution (OOD) Detection", "content": "Many recent approaches have focused on quantifying distributional uncertainty- uncertainty that is caused by a change in the input data distribution [6]. Ap- proaches that are specifically focused on determining if an input falls outside of the input-data distribution are referred to as OOD detection methods [3]. These approaches are generally either (1) classification-based, (2) density-based, (3) distance-based, or (4) reconstruction-based. Classification-based methods is a broad category that captures methods that largely rely on classifier predictions. These methods generally seek to revise the overly confident softmax scores at the output of neural networks to detect OOD samples more robustly [19-21]. Density-based methods model the training distribution with some probabilis- tic model and flag test data in low-density regions as OOD. These approaches tend to rely on Gaussian mixture models (GMMs) [22], kernel density estima- tion (KDE) [23], normalizing flows [24, 25], or likelihood ratios [26]. Distance- based methods use distance metrics in the feature space with the assumption that OOD samples should lie relatively far from the centroids or prototypes of the training classes. Many popular approaches utilize the Mahalanobis dis- tance [27,28], cosine similarity [29], or a nearest-neighbor distance [30]. Finally, reconstruction-based methods rely on the reconstruction loss of autoencoders (AEs) [31,32], variational autoencoders (VAEs) [33], convolutional autoencoders (CAEs) [34], or generative adversarial networks (GANs) [35-37]. While all these methods have been used to successfully identify OOD samples, they do not offer a clear explanation as to why the samples are deemed OOD."}, {"title": "2.3 Explainable Image Classification", "content": "The field of explainable artificial intelligence (XAI) seeks to offer explanations for why a model makes the decisions that it does with the goal of developing more interpretable and trustworthy models [1,38]. Within the area of explain- able image classification, there exist (1) posthoc techniques, which explain the"}, {"title": "2.4 Explainable Competency Estimation", "content": "While there has been work on quantifying model uncertainty, identifying OOD inputs, and explaining model predictions, to the best of our knowledge, there has not been an effort to offer explanations for the lack of competency associated with a model's prediction. We seek to bridge this gap by bringing together work on uncertainty quantification, OOD detection, and explainability. We see this as an important step in developing decision-making systems that can reason about model competency and respond safely in the face of uncertainty."}, {"title": "3 Approach for Understanding Model Competency", "content": "Recall that model competency is a generalized form of predictive uncertainty that captures data (or aleatoric) uncertainty, model (or epistemic) uncertainty, and distributional shift [4]. To gain a deeper understanding of perception model com- petency, we explore five methods for visualizing regions in the input image that contribute most to a lack of competency (similar to saliency methods discussed in Section 2.3). Towards this end, we first compute a competency score, estimat- ing the confidence in a model's prediction for a given input image (Section 3.1)."}, {"title": "3.1 Estimating Model Competency", "content": "Let $f$ be the true underlying model from which our images are drawn and $\\hat{f}$ be the predicted model (or the perception model). For an input image, $X$, the perception model aims to estimate the true class of the image, $f(X)$, from the set of all classes. The competency of the model for this image is given by\n\n$C(X) := P(\\left\\{f(X) = \\hat{f}(X)\\right\\}|X).$  (1)\n\nTo simplify our notation, let $\\hat{c}$ be the class predicted by the deterministic perception model (i.e., $\\hat{f}(X) = \\hat{c}$), so we can express the model competency as\n\n$C(X) = P(\\left\\{f(X) = \\hat{c}\\right\\}|X).$   (2)\n\nIt is difficult to estimate this probability directly because we are limited by the data contained in the training sample. Let $D$ be the event that the input image is in-distribution (i.e., drawn from the same distribution as the training samples) and write the following lower bound on competency:\n\n$C(X) > P(D\\cap \\left\\{f(X) = \\hat{c}\\right\\}|X)$ (3)\n\n$= P(\\left\\{f(X) = \\hat{c}\\right\\}|X, D)P(D|X).$ (4)\n\nThese probabilities are not readily available but can be approximated [4]. To estimate the first probability, we can fit a calibrated transfer classifier, such as the logistic regression classifier, and use its class probability output to obtain an estimate, $P_{\\hat{f}|D}(X)$. To estimate the probability that an image is from a class distribution, we can model the training distribution as a Gaussian mixture model (GMM) and compute the Mahalanobis distance of the feature vector to each of the class distributions. We then use these distances as input to another logistic regression classifier to estimate the probability that the input image came from the training distribution, $P_D(X)$. Using these approximations, the lower bound on the probability of model competency can be approximated as\n\n$C(X) \\geq P_{\\hat{f}|D}(X)P_D(X).$ (5)\n\nWe refer to this expression as the competency score. This metric ties together work on uncertainty estimation (Section 2.1) and OOD detection (Section 2.2), using a deterministic classification-based approach to estimate model uncertainty assuming a sample is in-distribution and a distance-based approach to estimate the probability that a sample is in-distribution. Note that the competency score estimates a probability and is thus always between zero and one. If the model is 100% competent on a given image, the probability that the model's prediction"}, {"title": "3.2 Identifying Regions Contributing to Low Competency", "content": "We explore five approaches for estimating the dependence of the competency score on regions in the input image to understand the extent to which different regions in the image contribute to low model competency.\n\nApproach 1: Image Cropping The first naive approach we consider is re- ferred to as Cropping and is described in Figure 1. For this approach, we simply compute the competency score for each region in the image within a grid. We tune the height and width of the grid cells to achieve the best performance.\n\nApproach 2: Segment Masking We refer to the second approach considered as Masking. In this approach, we compute the competency score for each segment in the image, while masking out the rest of the image, as shown in Figure 2. We explore various methods for masking in an attempt to capture \"feature missing- ness\" [60]-blurring regions that are not of interest, adding noise to those regions, or replacing those pixels with zeros, ones, average values, uniform random values, or Gaussian random values and select the highest-performing method.\n\nApproach 3: Pixel Perturbation The third approach, which is closely related to the second, is referred to as Perturbation and is described in Figure 3. In this approach, we measure the increase in competency achieved by perturbing segments of the image. This approach is related to perturbation approaches in"}, {"title": "Approach 4: Competency Gradients", "content": "The fourth approach, which we re- fer to as Gradients, relies on gradient information relating the input image to the estimated competency score and is described in Figure 4. The competency score (defined as $C$ in Section 3.1) is a differentiable function of the input image, $X$. Therefore, we can compute the partial derivative of the competency score with respect to each of the pixel values in the input image. If the derivative is large, then small changes in this pixel value will significantly affect the compe- tency score, and we say that model competency is more dependent on this pixel. This approach is similar to gradient-based approaches in X\u0391\u0399 [41] but considers"}, {"title": "Approach 5: Reconstruction Loss", "content": "The fifth and final approach we consid- ered is referred to as Reconstruction because it depends on the reconstruction loss of an autoencoder trained to fill in missing regions of an input image, as shown in Figure 5. The autoencoder is designed to reconstruct the original image from the feature vector used for prediction by the perception model. We assume that the ability of the autoencoder to accurately reconstruct a region of an image reflects the familiarity of the model with that image region and thus the depen- dence of low model competency on that region. Reconstruction-based approaches have been explored for anomaly detection [32] and localization [62] but have not yet been explored as an explainability tool. For determining the reconstruction loss across segmented regions in an image, we considered different methods and loss functions and selected those that achieved the best performance."}, {"title": "4 Method Evaluation & Analysis", "content": "We provide a thorough analysis of the performance and usefulness of each of these five approaches for identifying regions contributing to low model competency. We conduct analysis across three unique datasets that assess the ability to identify unfamiliar objects (Section 4.2), detect regions associated with unseen classes (Section 4.3), and recognize unexplored areas in an environment (Section 4.4)."}, {"title": "4.1 Metrics for Comparison", "content": "Following much of the work in explainable image classification (Section 2.3), we provide a visual comparison of the outputs of the five approaches. This visual analysis allows us to better understand the method outputs and determine the extent to which we trust each of the methods for explaining model competency. It also helps to determine whether the outputs of these methods offer explanations that are helpful to and interpretable by a human user. While we cannot display every example image in this paper, we include a few representative images.\n\nWhile visual analysis is useful, it may be misleading to rely solely on a qual- itative comparison [60]. In an effort to provide a more rigorous quantitative comparison, we develop a metric for accuracy that evaluates the ability of each of the five approaches to identify regions that are unfamiliar to the model. Each image in the test set is manually labeled to indicate which regions contain fea- tures that were not present in the training set. A pixel that corresponds to a feature that was not present in the training set is considered a positive sample, and a pixel that corresponds to a feature that was present is considered a nega- tive sample. We then assess the ability of the five approaches to label the dataset in the same way, measuring the overall accuracy, true positive rate (TPR), true negative rate (TNR), positive predictive value (PPV), and negative predictive value (NPV) averaged across all of the images in the test set.\n\nFinally, in order for any of these approaches to truly be useful for decision- making, we must consider the computation time required to identify regions contributing to low competency. While it is valuable to provide users a better understanding of how their model works, when it is incompetent, and what leads to incompetency, it is desirable for systems to recognize their level of competency and respond appropriately without the need for human intervention. In order for this autonomy to be realistic, the system needs to reason about its competency fast enough that doing so does not interrupt the decision-making process."}, {"title": "4.2 Dataset 1: Lunar Environment", "content": "The first dataset we consider was obtained from a lunar environment developed by the Johns Hopkins University Applied Physics Laboratory in the Gazebo sim- ulator, in which the training data contains images from an uninhabited moon and the test data contains images from a habited moon. While the training images only contain the lunar surface, the sky, and shadows, the test images ad- ditionally contain astronauts and human-made structures. This dataset assesses"}, {"title": "4.3 Dataset 2: Speed Limit Signs", "content": "The second dataset contains speed limit signs in Germany [63]. While the train- ing dataset is composed of common speed limit signs (30 km/hr and higher), the test dataset set also contains an uncommon speed limit (20 km/hr). This dataset assesses the ability of the regional competency approaches to identify regions associated with unseen classes."}, {"title": "4.4 Dataset 3: Outdoor Park", "content": "The third and final dataset contains images from regions in a park. While the training dataset only contains images from forested and grassy regions of the park, the test datset additionally includes images from around the park's pavil- ion. This dataset assesses the ability of the regional competency approaches to"}, {"title": "4.5 Analysis of Results", "content": "Comparing all of our results across the three datasets in Figure 9, we see that the Cropping method may provide a rough idea of image regions contributing to low model competency for simple datasets, but this method is unlikely to provide utility for more complex datasets. The Masking and Perturbation methods may be useful for some datasets, but it is unlikely that they would provide more explanatory power than the Gradients and Reconstruction methods. Among all of the regional competency methods, the Reconstruction method appears the most promising for identifying regions contributing to low model competency, but the Gradients method also has merit. While the Reconstruction method more reliably informs us of regions in the image that are unfamiliar to the classifier, the Gradients method more directly tells us which pixels the competency score actually depends on. Because these two methods inform us of slightly different aspects of model confidence, there may be value in combining their outputs.\n\nFrom Figure 10, both the Gradients and Reconstruction methods can ob- tain their outputs in around 0.1 seconds or less on a 2.3 GHz Quad-Core Intel Core i7 processor. While these methods are fast enough to be usable in most decision-making pipelines, their computation times are dependent on image size, classifier architecture, and segmentation algorithm parameters, so computation time can vary across systems. While the increase in computation time seen for the speed limit signs dataset was not terribly significant for the Gradients and Reconstruction methods, it was quite significant for the Cropping, Masking, and Perturbation methods. The Cropping method is unlikely to run fast enough for most systems, and the Masking and Perturbation methods may be too slow for systems with larger images and a greater numbers of image segments.\n\nFrom a visual comparison of the performance of the regional competency methods across the three datasets, it appears that these methods may be better"}, {"title": "5 Conclusions", "content": "In this paper, we sought to develop tools for understanding when DNN-based image classification models fail and why. We expand upon uncertainty quantifi- cation and OOD detection work to offer explanations for why a perception model lacks confidence in its prediction for a given input image by identifying specific regions in the image that lead to low model competency. Rather than offering explanations for classification decisions, as is typically done in explainable AI work, we aim to explain why a model is incompetent in its prediction.\n\nWith these goals in mind, we developed five distinct methods for identifying regions in an image contributing to low model competency, which we refer to as (1) Image Cropping, (2) Segment Masking, (3) Pixel Perturbation, (4) Compe- tency Gradients, and (5) Reconstruction Loss. Each of these methods outputted an incompetency dependency score for each pixel in a set of input images. We evaluated each of these five approaches across three diverse datasets with the aim of determining which methods were most suitable for identifying unfamiliar objects, regions associated with unseen classes, and unexplored areas in an envi- ronment. We compared the dependency outputs of the five approaches visually, evaluated their ability to accurately identify image features that are unfamiliar to the classification model, and compared their computation times.\n\nWe found the Reconstruction Loss method to be the most effective and promising approach overall but also found merit in the Competency Gradients method. We believe both methods offer valuable information about why an im- age classification model is incompetent for a given input image, and their outputs can be obtained fast enough to be used in decision-making problems."}, {"title": "6 Limitations & Future Work", "content": "Additional work could be done to improve the performance of the regional com- petency estimation methods presented in this work. First, it may be worthwhile to explore different segmentation algorithms, particularly ones that are domain- specific or trained to work well with a particular dataset or environment of interest. For the Reconstruction method, it would be interesting to consider additional reconstruction loss functions, as well as ensembles of autoencoders. While the Reconstruction method is independent of the competency metric, the other four methods are directly dependent on the chosen metric. In this work, we only considered a single metric for competency, which incorporates different facets of uncertainty into a single score. It would be interesting to disentangle these sources of uncertainty or consider different metrics for competency. While the Cropping, Masking, and Perturbation methods do not have strict require-ments for the chosen competency metric, the Gradients method is only useful if the competency score is differentiable with respect to the input image.\n\nBeyond these more immediate next steps for improving the proposed meth- ods, there are several other avenues for future work. First, this work focused on explaining low model competency for OOD images with regional features that are clearly unfamiliar to the perception model. Additional work would need to be done to understand low model competency for more nuanced cases, particularly with in-distribution images, and to understand high model competency. One could explore other methods for explaining model competency that are not fo- cused on identifying particular regions contributing to low competency. It would be worthwhile to identify other types of image features that might contribute to low competency, such as image brightness. This work also seeks to understand perception model competency of trained models without modifying the under- lying model architecture. It would interesting to also explore antehoc methods from the XAI literature to better understand model competency.\n\nThe approaches explored in this paper can be used to provide users a deeper understanding of how a model works and when it might fail. Additional work could be done with this goal in mind to combine visual explanations with lan- guage. Future work should also explore how the regional competency information presented in this paper should play into decision-making problems. While these methods provide perception-based systems with more information about why an image classification model is incompetent, it is still not clear how the system should respond. Additional work would need to be done to decide the appropri- ate response to model incompetency for different applications. Finally, the work presented in this paper focused on image classification problems. Additional work should be done to understand how these approaches could be adapted for regression, semantic segmentation, object detection, and other problems.\n\nDisclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article."}, {"title": "A Data Sources & Algorithm Parameters", "content": "To reproduce this work, data is available on Drive, code is available on GitHub, and the chosen model architectures and training parameters are available on Drive. For more information on reproducing this work, please see this README."}, {"title": "B Comparison to Class Activation Maps", "content": "The goal of the work presented in this paper is to identify regions in an image contributing to low model competency and to display this incompetency depen- dency in a saliency map. This is closely related to work on developing class acti- vation maps (CAMs), which are class-discriminative saliency maps that identify key regions in an image used by the model to make a particular class predic- tion. There are various methods for generating CAMs [64], but we choose to focus on a few representative approaches: Grad-CAM [41], Guided Grad-CAM [41], Grad-CAM++ [65], Integrated Gradients [42], SmoothGrad [66], DeepLIFT [48], Score-CAM [45], Ablation-CAM [67], Eigen-CAM [68], and LayerCAM [69]. These methods were implemented with the help of the PyTorch library for CAM methods [70] and the Captum model interpretability library for PyTorch [71].\n\nThe chosen mapping approaches are designed to explain model predictions, but we evaluate their ability to recognize unfamiliar objects (Section B.1), iden- tify regions associated with unseen classes (Section B.2), and determine unex- plored regions in an environment (Section B.3). We find that these approaches are not suited for generating competency maps, as is done in our work."}, {"title": "B.1 Dataset 1: Lunar Environment", "content": ""}, {"title": "B.2 Dataset 2: Speed Limit Signs", "content": ""}, {"title": "B.3 Dataset 3: Outdoor Park", "content": ""}]}