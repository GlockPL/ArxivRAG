{"title": "DMQR-RAG: DIVERSE MULTI-QUERY REWRITING\nFOR RETRIEVAL-AUGMENTED GENERATION", "authors": ["Zhicong Li", "Jiahao Wang", "Zhishu Jiang", "Hangyu Mao", "Zhongxia Chen", "Jiazhen Du", "Yuanxing Zhang", "Fuzheng Zhang", "Di Zhang", "Yong Liu"], "abstract": "Large language models often encounter challenges with static knowledge and hal-\nlucinations, which undermine their reliability. Retrieval-augmented generation\n(RAG) mitigates these issues by incorporating external information. However,\nuser queries frequently contain noise and intent deviations, necessitating query\nrewriting to improve the relevance of retrieved documents. In this paper, we in-\ntroduce DMQR-RAG, a Diverse Multi-Query Rewriting framework designed to\nimprove the performance of both document retrieval and final responses in RAG.\nSpecifically, we investigate how queries with varying information quantities can\nretrieve a diverse array of documents, presenting four rewriting strategies that op-\nerate at different levels of information to enhance the performance of baseline\napproaches. Additionally, we propose an adaptive strategy selection method that\nminimizes the number of rewrites while optimizing overall performance. Our\nmethods have been rigorously validated through extensive experiments conducted\nin both academic and industry settings.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) possess powerful comprehension and generation abilities (Touvron\net al., 2023a;b), demonstrating remarkable performance across various downstream tasks (Jiang\net al., 2023; Su et al., 2024; Ruan et al., 2023; Kong et al., 2024; Zhang et al., 2024a;b; Li et al.,\n2024). However, the parametric knowledge within LLMs is inherently static, making it challenging\nfor them to provide up-to-data information in real-time scenarios (Yao et al., 2023). Additionally,\nLLMs are prone to hallucinations when addressing factual questions (Guan et al., 2024; Hoshi et al.,\n2023), undermining the reliability of generated answers.\nTo address these issues, retrieval-augmented generation (RAG) (Gao et al., 2023b; Chen et al., 2024)\nhas emerged as a method to enhance LLMs by retrieving and incorporating external knowledge.\nHowever, due to noise and intent bias in the original queries, direct retrieval often fails to yield"}, {"title": "RELATED WORK", "content": "Existing rewriting methods can be categorized into two categories: training-based and prompt-based.\nTraining-based methods (Wang et al., 2024a; Mao et al., 2024; Ma et al., 2023) use labeled data for\nsupervised fine-tuning on the model or employ reward scores for reinforcement learning to achieve\nbetter rewriting results. RQ-RAG (Chan et al., 2024) constructs an innovative dataset that contains\nsearch queries and rewritten results across multiple scenarios, which is used to train an end-to-end\nmodel that refines search queries. RRR (Ma et al., 2023) proposes a novel training strategy for\nrewriting, which leverages the performance of response model as a reward and optimizes retrieval\nqueries through reinforcement learning. However, these methods require substantial costs for dataset\nconstruction and training.\nPrompt-based methods (Zheng et al., 2024; Chan et al., 2024; Wang et al., 2023a) leverage various\nprompting strategies to directly instruct large models to perform multiple rewriting tasks. Hyde\n(Gao et al., 2023a) utilizes LLMs to generate a pseudo-answer for the original query in advance.\nThis pseudo-answer is semantically closer to the correct answer, making it easier to retrieve the\ncorrect results. Step-back Prompting (Zheng et al., 2024) tackles queries with extensive details\nby rewriting them at a higher conceptual level to retrieve more comprehensive answers. Least-to-\nmost prompting (Zhou et al., 2023) decomposes a complex query into several easier-to-address sub-\nqueries, which are individually retrieved to gather all documents necessary to answer the original\nquery. While avoiding additional training costs, these methods focus only on specific query types,\nlack generalization, and produce retrieval results with insufficient diversity. Therefore, our approach\nproposes multi-strategy rewriting, using prompt-based methods to guide the model in preforming\nmultiple rewrites according to different strategies. This effectively addresses various types of queries\nand enhances the diversity of retrieval results."}, {"title": "METHODOLOGY", "content": "We first introduce the traditional RAG workflow and propose a standardized setup to explore how the\nrewriting strategy impacts performance. Building on this foundation, we present our DMQR-RAG\nframework and the adaptive rewriting selection method."}, {"title": "FORMAL SETUP FOR MULTI-QUERY REWRITING", "content": "Given a user's query q, the traditional RAG process begins by rewriting the query to obtain q'. Next,\na retriever searches for relevant documents, represented by the set D. These documents are then\nreranked to produce D'. Subsequently, the top K documents are concatenated with the original\nquery q and input into an LLM to generate the final response A. The entire process is as follows:\nq' = Rewriter(q), D = Retriever(q'),\nD' = Reranker(D), A = LLM(q, TopK(D'))."}, {"title": "THE DIVERSE MULTI-QUERY REWRITING FRAMEWORK", "content": "Due to their advanced natural language understanding capabilities (Touvron et al., 2023a;b), LLMs\nare often used as the foundational tool for query rewriting. In this section, we will first explore vari-\nous LLM-based rewriting strategies from an informational perspective, followed by an introduction\nto the rewriting strategy selection method."}, {"title": "REWRITING STRATEGIES", "content": "Current rewriting methods often rely on a straightforward, single rewrite of the original query, which\nfrequently fails to retrieve relevant documents. Additionally, multi-query rewriting approaches, such\nas RAG-Fusion (Rackauckas, 2024), tend to be simplistic, producing rewrites that are nearly identi-\ncal and lacking in diversity. This limitation hampers their ability to enhance overall performance.\nAn effective multi-query rewriting strategy should meet the following informational criteria: each\nrewritten query must be diverse, providing unique information not present in the others. By en-\nhancing the diversity of information in the rewritten queries, we increase the likelihood of retrieving\na broader range of documents, ultimately improving our chances of obtaining genuinely relevant\ndocuments (Maron & Kuhns, 1960; Baeza-Yates et al., 1999; Weikum & Vossen, 2001).\nSpecifically, we propose four rewriting strategies that focus on adjusting or preserving the infor-\nmation in the original query. These strategies aim to refine the query while also providing unique\ninsights to facilitate the retrieval of a diverse set of documents.\nInformation Equality. User-generated queries often contain irrelevant noise and unclear intent\n(Gao et al., 2023b), leading to deviations from the intended retrieval objective. Therefore, a general\nrewriting approach is necessary to denoise and refine the original query. We refer to this method\nas General Query Rewriting (GQR), which refines the original query q while retaining all relevant\ninformation and eliminating noise, thereby enhancing retrieval precision. This strategy is in line\nwith the approach proposed by Ma et al. (2023).\nFurthermore, to ensure alignment with search engine preferences while maintaining the same\namount of information, we introduce Keyword Rewriting (KWR). This strategy aims to extract\nall keywords from the query q, particularly nouns and subjects. By doing so, KWR enables search\nengines to directly address user needs and quickly locate relevant documents (Gupta & Vidyapeeth,\n2017), while also reducing the parsing burden on the search engine."}, {"title": "ADAPTIVE REWRITING STRATEGY SELECTION", "content": "In practical industrial scenarios, user queries are diverse. While multi-query rewriting can enhance\nretrieval diversity, applying a fixed set of strategies to every query is not optimal. Therefore, it is\ncrucial to dynamically select rewriting strategies tailored to each specific query, generating multiple\nrewritten queries that best suit the original intent.\nWe implement this selection method using lightweight prompting and few-shot learning. Specifi-\ncally, we incorporate descriptions of the rewriting strategies in the strategy pool RS into the prompts\nof the LLMs as contextual information. These descriptions outline the applicable query types and\nthe roles of each strategy, enabling the LLMs to gain a comprehensive understanding of all available\nrewriting strategies. To enhance strategy selection in challenging cases, we also adopt a few-shot ap-\nproach by providing the LLMs with multiple demonstrations, which assist them in selecting suitable\nrewriting strategies for difficult queries."}, {"title": "EXPERIMENTS", "content": null}, {"title": "EXPERIMENTAL SETUP", "content": null}, {"title": "DATASETS", "content": "We conduct experiments using three representative open-domain question-answering datasets: (1)\nAmbigNQ (Min et al., 2020), designed to address the inherent ambiguity in Natural Questions\n(Kwiatkowski et al., 2019); (2) HotpotQA (Yang et al., 2018), which includes complex questions\nrequiring multi-hop reasoning, with the validation set used for evaluation due to the lack of ground\ntruth in the test set; and (3) FreshQA (Vu et al., 2024), a dynamic benchmark that encompasses\nvarious question types and necessitates up-to-date world knowledge for accurate responses. We also\nconduct experiments on industry datasets, which will be described later."}, {"title": "METRICS", "content": "We evaluate both retrieval and end-to-end response metrics. Specifically, we assess rewriting ef-\nfectiveness using the Top-5 hit rate (H@5) and precision (P@5) of the retrieved documents, with\nrelevance evaluated by GPT-4. For end-to-end responses, we use official evaluation methods: for\nHotpotQA and AmbigNQ, we calculate exact match (EM) and F1 scores, while for FreshQA, we\nuse GPT-4 to score responses and compute accuracy (Acc)."}, {"title": "BASELINES", "content": "We adopt prompting and fine-tuning methods as our baseline approaches. For prompt-based meth-\nods: (1) LLM Rewrite (abbreviated as Rewrite) (Ma et al., 2023) utilizes prompts to harness the\ninherent capabilities of large language models for general retrieval rewriting. (2) Hyde (Gao et al.,\n2023a) employs zero-shot prompting to guide large language models in creating a pseudo-document\nthat captures the semantics of the target document, which is then used for retrieval. For fine-tuning\nmethods: (1) Rewrite-Retrieve-Read (RRR) (Ma et al., 2023) uses the accuracy of model responses\nobtained through rewriting and retrieval as a reward signal to fine-tune the T5 model via reinforce-\nment learning. (2) RQ-RAG (Chan et al., 2024) constructs a dataset of search queries across multiple\nscenarios and trains a model to perform rewriting, decomposition, and clarification of the original\nqueries. Additionally, we compare the effectiveness of using the original query directly, without\nrewriting, referred to as Original Query Retrieval (OQR)."}, {"title": "IMPLEMENTATION DETAILS", "content": "The experiments are implemented using PyTorch and employ several LLMs for our rewriting tasks,\ndemonstrating that our DMQR-RAG framework is applicable to various models, including Llama3-\n8B (Dubey et al., 2024), Qwen2-7B Yang et al. (2024), and GPT-4 (Achiam et al., 2023). After\ngenerating multiple rewrites, document retrieval and response generation are conducted according\nto the setup outlined in Section 3.1. Specifically, each proposed rewriting method independently\nretrieves 10 documents, recalling a total of 50 documents, which are then reranked by the reranker.\nBy default, we use the Bing search engine as the retriever and BGE (Chen et al., 2009) as the\nreranker. Both the baselines and our method utilize GPT-4 as the response model, leveraging the\nreranked Top-5 documents as additional context."}, {"title": "RESULTS", "content": null}, {"title": "BASELINE COMPARISON", "content": "The main results are shown in Table 1. Overall, our method outperforms others in most scenarios.\nThe detailed analysis leads to the following conclusions."}, {"title": "GENERALIZATION TESTING", "content": "One important question is whether DMQR-RAG is applicable to other LLMs, particularly smaller\nmodels than GPT-4. To address this, we tested Llama3-8B and Qwen2-7B, with the results presented\nin Table 2. The findings indicate that our method is not restricted to high-performing LLMs like\nGPT-4; in fact, it can be effectively applied to both Llama3-8B and Qwen2-7B, yielding strong\nresults than baseline rewriting methods. This highlights the versatility and generalizability of our\napproach across different model architectures."}, {"title": "ABLATION STUDY", "content": "We conduct an ablation study to investigate the effectiveness of each rewriting method. Specifically,\nwe use Llama3-8B as the base model and remove each method individually, comparing the results\nwith our comprehensive multi-query rewriting approach. The results are presented in Table 3."}, {"title": "EVALUATION OF ADAPTIVE REWRITING SELECTION", "content": "The previous section shows that removing a rewriting method may lead to performance improve-\nments on the FreshQA dataset when using Llama3-8B as the rewriter. In this section, we validate\nour adaptive rewriting selection method proposed in Section 3.2.2 with both Llama3-8B and GPT-4\nas the rewriters, focusing on the FreshQA dataset. The results are presented in Figure 2 and 3."}, {"title": "INDUSTRIAL APPLICATIONS", "content": "We deploy the DMQR-RAG framework in real-world industrial scenarios, using queries from 15\nmillion online users. The queries include news-related topics, complex knowledge-based questions,\nand daily conversational queries. We compare our method with OQR and RAG-Fusion, maintaining\nconsistency with prior experimental settings. Results shown in Figure 4 indicate that our method\nsignificantly improves retrieval, with H@5 increasing by an average of 2.0% and P@5 by 10.0%.\nIn terms of end-to-end response performance, Correctness has improved 4, demonstrating that our\nmethod effectively addresses more user queries. Relevance has also increased, suggesting that while"}, {"title": "CONCLUSION", "content": "This paper presented the Diverse Multi-Query Rewriting Framework (DMQR-RAG), aimed at en-\nhancing both document retrieval and final responses in retrieval-augmented generation. We devel-\noped four rewriting strategies based on information levels to ensure that the rewritten queries are\ndiverse and provide unique insights. Additionally, we implemented an adaptive rewriting selection\nmethod utilizing lightweight prompting and few-shot learning. Our evaluation on both academic\nand industry datasets demonstrated that multi-query rewriting generally outperforms single-query\nrewriting, with DMQR-RAG surpassing vanilla RAG-Fusion. Our ablation study and case analysis\nfurther highlighted the importance of query-specific rewriting strategy selection, confirming the ef-\nfectiveness of our approach. In the future, we will explore further enhancements to the DMQR-RAG\nframework, including optimizing the adaptive rewriting selection method and expanding the range\nof rewriting strategies to create a more diverse strategy pool."}]}