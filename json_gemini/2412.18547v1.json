{"title": "Token-Budget-Aware LLM Reasoning", "authors": ["Tingxu Han", "Chunrong Fang", "Shiyu Zhao", "Shiqing Ma", "Zhenyu Chen", "Zhenting Wang"], "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning.", "sections": [{"title": "1 Introduction", "content": "Reasoning plays a crucial role in enabling large language models (LLM) to perform effectively across a wide range of tasks (Zhou et al., 2022; Hao et al., 2023, 2024a). A variety of methods have been proposed to enhance the reasoning capabilities of large language models (Suzgun et al., 2022; Wang et al., 2023; Feng et al., 2023; Yao et al., 2024a; Xie et al., 2024). Among these, Chain-of-Thought (CoT) (Wei et al., 2022) is the most representative and widely adopted approach. It enhances the reliability of the model's answers by guiding large language models with the prompt \"Let's think step by step\", encouraging them to decompose the problem into intermediate steps and solve each before arriving at the final answer. Observe that without CoT, the LLM produces incorrect answers to the question. With a CoT-enhanced prompt, the LLM systematically breaks the question into multiple steps and reasons through each step sequentially. By addressing each step incrementally, the LLM eventually arrives at the correct answer.\nAlthough reasoning enhancement approaches such as CoT impressively improve LLM performance, they produce substantial additional overhead, specifically in the form of the increased number of tokens produced (Wei et al., 2022; Feng et al., 2023; Yao et al., 2024a). As shown in Figure 1b, the answer to prompt with CoT has notably higher token costs due to the detailed intermediate reasoning steps included in the output. Such high token costs can lead to significant expenses, including increased computational resource usage and longer running times during the LLM inference phase, ultimately resulting in significant additional monetary and energy costs.\nThis raises an important question: \u201cIs the reasoning process of current LLMs unnecessarily lengthy, and how can it be compressed?\u201d Nayab et al. (2024) demonstrate that LLM has the potential to follow a length constraint in the prompt. Building on this, we find that including a token budget (see Table 1) in the prompts is a promising approach to compressing the CoT reasoning tokens. However, the choice of token budget plays a crucial role in the actual compression effectiveness. For example, Figure 1d illustrates that including a reasonable token budget (e.g., 50 tokens in this case) in the instructions reduces the token cost in the chain-of-thought (CoT) process from 258 output tokens to"}, {"title": "2 Related Work", "content": "LLM Reasoning. Reasoning in LLMs has seen substantial advancements through techniques that generate intermediate steps, enabling more accurate and effective performance across diverse domains (Wu et al., 2022; Yang et al., 2022; Zhou et al., 2022; Sun et al., 2024; OpenAI, 2024c). Various LLM reasoning techniques are proposed to improve the LLM performance. Chen et al. (2024) formulates reasoning as sampling from a latent distribution and optimizing it via variational approaches. Ho et al. (2022) utilizes LLM as reasoning teachers, improving the reasoning abilities of smaller models through knowledge distillation. Among them, Chain-of-Thought (CoT) prompting has emerged as a key technique for improving LLM reasoning by breaking problems into intermediate steps, enabling better performance on multiple"}, {"title": "3 Token Redundancy in LLM Reasoning", "content": "Token Budget. Previous research Nayab et al. (2024) demonstrates that LLM has the potential to follow a length constraint in the prompt. Table 1 shows the difference between the vanilla CoT and the CoT with token budget. For instance, by including a token budget (50 tokens) within the prompt, as illustrated in Figure 1d, the LLM adjusts the length of its output (86 output tokens), trying to align with the specified budget. This indicates that LLMs have a certain capability in following prompts with an explicit token budget.\nToken Redundancy Phenomenon. We find that providing a reasonable token budget can significantly reduce the token cost during reasoning. As shown in Figure 1d, including a token budget in"}, {"title": "4 Searching Optimal Token Budget", "content": "As demonstrated in Figure 1, different token budgets have different effects. Therefore, it is natural to investigate the following question: \u201cHow to search the optimal token budget for a specific question and a particular LLM?\u201d\nVanilla Method for Optimal Budget Search. An intuitive method is finding the minimal needed tokens as the budget, ensuring that the LLM can still produce correct and accurate responses within this constraint. To find the minimal token budget required for each question, we utilize a binary search-based minimal budget search algorithm. Before initiating the search process, we first apply the vanilla CoT to generate an answer for each question, as illustrated in Figure 1b. The number of tokens in the resulting answer is then calculated and designated as the right boundary for search, denoted by right. The"}, {"title": "5 Methodology", "content": "Based on the above observations and analysis, we designed our method TALE for token-budget-aware reasoning in LLMs. Figure 4 provides an overview of TALE's workflow. TALE aims to elaborate a token-budget-aware prompt that achieves performance comparable to vanilla CoT while reducing token costs. To strike this balance, TALE follows a two-phase approach: budget estimation and prompt construction. The observation of token elasticity, as discussed in Section 4, suggests that only an appropriate budget located in a reasonable range can effectively reduce token costs and preserve LLM performance simultaneously. The optimal budget searched by and is located in such reasonable budget range and achieves satisfying trade-off between token costs and LLM performance. In this case, TALE first estimates a reasonable token budget that is close to the searched optimal budget for the given question. Using the estimated token budget, TALE then crafts a token-budget-aware prompt and then feeds it into LLM to generate the final answer.\n5.1 Overview\n5.2 Budget Estimation\nTo estimate an appropriate budget within the reasonable budget range, two possible solutions are taken into consideration: zero-shot-based mechanism and budget regression. We also discuss an approach that internalizes the budget awareness of LLM by fine-tuning it. All approaches focus on developing an estimator that effectively approximates the optimal budget.\nZero-shot Estimator. For zero-shot mechanism to predict the reasonable budget, TALE leverages the reasoning LLM itself as an estimator. Before querying the LLM with a question, TALE first prompts the LLM to estimate the number of output tokens needed to answer the question. Figure 5 illustrates the budget estimation prompt. The key intuition behind this is the human-like thinking paradigm. When presented with a mathematical question, although it may take humans a few minutes to calculate the answer, they can typically estimate the time or effort required to solve it after just briefly reviewing the question. For instance, when presented with a question from primary school arithmetic and another from college-level calculus, a human may not immediately provide the answers. Still, it is easy to infer that the former can be solved in seconds, while the latter requires a significantly longer time, even with only a brief glance. RQ2 in Section 6 showcases the performance of budget estimation. Observe that a large amount of the estimated budgets are around the optimal searched budget and achieve competitive performance."}, {"title": "6 Evaluation", "content": "In this section, we provide the preliminary results of the zero-shot estimator version of our method TALE. This project is ongoing and we will update more results soon. Three state-of-the-art LLMs (i.e., GPT-40 (OpenAI, 2024b), GPT-40-mini (OpenAI, 2024a), Yi-lightning (Wake et al., 2024)) are involved in the experiments. Our evaluation is surrounding around the following research questions(RQs):\nRQ1. How effective is TALE in reducing token costs while maintaining LLM performance?\nRQ2. How effective is TALE to estimate the token budget given a question?\nRQ3. How general is TALE across different state-of-the-art LLMs?\n6.1 Experiment Setup\nMetrics. The target of TALE is to balance the LLM performance and extra redundant token costs. Specifically, TALE seeks to minimize token consumption while maintaining comparable LLM performance. To evaluate the LLM performance, three most challenging mathematical datasets are taken"}, {"title": "6.2 RQ1. Effectiveness of TALE.", "content": "Table 2 presents a comparison of TALE and other prompt engineering methods, including \u201cDirectly Answering\u201d and \u201cVanilla CoT\u201d, in their effectiveness for seven different datasets. The results are evaluated regarding ACC, Number of Output Tokens (Output Tokens for short), and Expense. A well-designed prompt engineering approach should induce the LLM to generate a correct response with as few tokens as possible. TALE consistently demonstrates significant improvements in efficiency while maintaining competitive accuracy. Directly answering achieves the lowest output tokens (14.57 on average) and expenses (25.37 on average) but has the lowest accuracy (52.31% on average). Vanilla CoT achieves the highest accuracy (83.75% on average) but incurs a significant token cost (461.25 on average) and expenses (289.78 on average). TALE demonstrates a balanced trade-off between performance and cost. It achieves competitive accuracy (81.03%) while reducing token costs (32% of Vanilla CoT) and expenses (41% of Vanilla CoT) significantly. For accuracy, notably, on GSM8K, TALE even improves accuracy to 84.46%, surpassing Vanilla CoT, demonstrating its ability to adapt well to complex reasoning tasks while remaining efficient. For output tokens on GSM8K-Zero, TALE achieves an impressive reduction in output token costs from 252.96 to 22.67 while maintaining high accuracy (98.72%), showcasing its capability to optimize reasoning efficiency in such tasks. For expenses, TALE demonstrates its cost-effectiveness, reducing expenses from 78.58 to 18.62 while achieving reasonable accuracy (73.67% vs 75.00%) on MathBench-Arithmetic. TALE demonstrates that incorporating token-budget awareness allows for a significant reduction in token costs and monetary expenses without a major compromise in accuracy. TALE reduces output token costs by 68.64% on average, making it a more efficient solution for budget-constrained reasoning tasks while retaining competitive performance. These results highlight TALE's generalizability across tasks with varying complexity, demonstrating its potential to scale in real-world scenarios while managing computational and financial resources effectively."}, {"title": "6.3 RQ2. Effectiveness of Budget Estimation.", "content": "In this RQ, we evaluate the effectiveness of the budget estimation performance. An ideal estimated budget should be located around the optimal searched budget and in the bottom area of Figure 2. We further define such an area as the ideal budget range and give the formalized definition in A.1. A good budget should be located in the ideal budget range. Two metrics are taken into consideration: in-range accuracy and out-of-range distance. In-range accuracy determines whether the predicted budget $\\beta$ falls within the ideal range"}, {"title": "6.4 RQ3. Generalization of TALE.", "content": "Table 3 demonstrates the generalization of TALE across Yi-lightning (Wake et al., 2024), GPT-40-mini (OpenAI, 2024a), and GPT-40 (OpenAI, 2024b) on MathBench-College, showing its ability to reduce output tokens and expenses while maintaining competitive accuracy significantly. TALE achieves substantial token savings, reducing output tokens by 62.6% for Yi-lightning, 61.5% for GPT-40-mini, and 69.8% for GPT-40, compared to Vanilla CoT. Expense reductions are equally notable, with costs decreasing from 21.55 to 17.25 for Yi-lightning, 81.56 to 45.60 for GPT-40-mini, and 1359.42 to 759.95 for GPT-40. Despite these cost savings, TALE maintains strong accuracy, achieving 76.67% on Yi-lightning, 70.00% on GPT-40-mini, and 80.00% on GPT-40, comparable to Vanilla CoT. These results highlight TALE's effectiveness in balancing cost efficiency and reasoning performance across diverse LLM architectures. The observed accuracy drop is most significant for GPT-40-mini. This could be attributed to its smaller number of parameters, which makes it more challenging to answer correctly within a limited response reasoning length."}, {"title": "7 Conclusion", "content": "In this paper, we introduce TALE, a framework that reduces token redundancy in Chain-of-Thought (CoT) reasoning by incorporating token budget awareness. TALE dynamically estimates token budgets based on task complexity to guide reasoning, balancing efficiency and accuracy. Experiments show that TALE reduces token usage by 68.9% on average with less than a 5% accuracy loss, outperforming Vanilla CoT in cost-effectiveness while generalizing well across various LLMs."}, {"title": "A Appendix", "content": "Ideal Budget Range. Based on the observation of token elasticity, a token cost bottom range exists during searching for the optimal budget. In this range, the token costs approach the token cost lowest bound. Before or after the range, the token cost will increase. We define such a bottom range as \"ideal budget range\". It's worth noting that the budget continuously degrades during the search. Only the token cost rebounds. That's why we refer to this observation as token elasticity. To summarize, ideal budget range is an range that minimizes actual token consumption. Let $B = {\\beta_1, \\beta_2, ..., \\beta_N}$ denote all possible budgets that can maintain answer correctness. A rolling window $W \\in B$ is applied iteratively over $B$. Let $k$ represent the range size, which is adaptively determined during our evaluation as, where N is the total number of possible budgets. A budget range is defined as:\n$W_k(i) = {\\beta_j | i \\leq j \\leq i + k - 1}$,\n$1 \\leq i \\leq |B| - k + 1$\nThe ideal budget range $W^*$ is defined as:\n$W^* = arg\\underset{B_j \\in W_k(i)}{min} (T(B_j))$, (3)\nwhere $T$ denote the actual token consumption for a given budget $B_j \\in B$. We aim to estimate a budget located in the ideal budget ranges without any search process. In that case, TALE obtains the ideal budget within acceptable sacrifice.\nA.1 Definition of Ideal Budget Range"}]}