{"title": "NUDGING: INFERENCE-TIME ALIGNMENT VIA MODEL COLLABORATION", "authors": ["Yu Fei", "Yasaman Razeghi", "Sameer Singh"], "abstract": "Large language models (LLMs) require alignment\u2014such as instruction-tuning or reinforcement learning from human feedback to effectively and safely follow user instructions. This process necessitates training aligned versions for every model size in each model family, resulting in significant computational overhead. In this work, we propose nudging, a simple, plug-and-play, and training-free algorithm that aligns any base model at inference time using a small aligned model. Nudging is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens, such as \"Sure\" or \"Thank\u201d. We find that base models are significantly more uncertain when gener-ating these tokens. Leveraging this observation, nudging employs a small aligned model to generate nudging tokens to steer the large base model's output toward desired directions when the base model's uncertainty is high. We evaluate the effectiveness of nudging across 3 model families and 13 tasks, covering reasoning, general knowledge, instruction following, and safety benchmarks. Without any additional training, nudging a large base model with a 7x-14\u00d7 smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. For example, nudging OLMo-7b with OLMo-1b-instruct-affecting less than 9% of tokens-achieves a 10% absolute improvement on GSM8K over OLMo-7b-instruct. Unlike prior inference-time tuning methods, nudging enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, this work introduces a simple yet powerful approach to token-level model collaboration, offering a modular solution to LLM alignment.Our project website: https://fywalter.github.io/nudging/.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) pre-trained on extensive text corpora acquire vast general knowledge. Still, they often struggle to produce responses that are helpful, safe, and aligned with user instructions without additional fine-tuning. As a result, alignment methods such as instruction tuning (Wei et al., 2022a) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022a)\u2014have become essential for developing AI assistants like ChatGPT. However, the conventional training pipelines require separate alignment tuning for every model size within each model family. This practice leads to substantial computational overhead, hindering the rapid iteration and development of new model families for both the industry and the academia.\nRecent studies (Zhou et al., 2024; Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and well-formatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, Lin et al. (2023) analyzed the Llama-2 (Touvron et al., 2023b) models and found only a small subset of stylistic tokens, such as Sure, is affected after alignment. These findings raise a natural question: If the alignment mainly affects the base models superficially and at the token level, is it necessary to align base models via fine-tuning?\nIn this work, we show that we can inject a few alignment tokens at inference time using a small aligned model to elicit various abilities of base models while ensuring helpfulness and safety. Specif-"}, {"title": "2 A CLOSER LOOK INTO ALIGNMENT AT THE TOKEN LEVEL", "content": "Previous work (Lin et al., 2023) finds that the token distributions of base models shift significantly after alignment only on a small set of output positions. By identifying (1) where the base and aligned model would disagree and (2) what the aligned model would generate for these positions, we can insert these tokens during decoding to nudge (Leonard, 2008) a base model to behave like an aligned model. In this section, we study these two questions: where to nudge and what to nudge.\nMethodology and Setup. To find out where the base and aligned models would disagree and what aligned models would generate for these positions, we analyze the token distribution shifts between the base and aligned model pairs, such as OLMo-7b and OLMo-7b-it, similar to Lin et al. (2023). Given a question $q = \\{q_1, q_2,\\ldots\\}$, we first generate the answer $a = \\{a_1,a_2,\\ldots\\}$ using the aligned models (e.g., OLMo-7b-it). Then, for each answer token position $i$, we compute the token distributions $P_{base}(\\cdot|q, a_{<i})$ and $P_{aligned}(\\cdot|q, a_{<i})$. Note that we use the same context, i.e., the question and the answer prefix, for both the base and aligned models. Let the rank of the top-1 token from $P_{aligned}$ in $P_{base}$ be $r$ for a token position. This position is considered alignment-related if $r > 3$, meaning that the distributions of model pairs differ remarkably. For example, suppose that at the first token of the second sentence in Figure 1, the top tokens and their probabilities from $P_{base}$ are [25: 0.3, Answer: 0.2, The: 0.2, Let: 0.1, ...], while the top token from $P_{aligned}$ is Let with probability 0.9. Since Let has rank 4 in $P_{base}$, this position is alignment-related, indicating a large distribution shift. To study token distribution across different tasks and models, we use three distinct types of datasets, each with 200 samples: (1) math reasoning: GSM8K (Cobbe et al., 2021b), (2) general knowledge: MMLU, and (3) instruction following: just-eval-instruct (Lin et al., 2023). For models, we use Llama-2-70b, Gemma-2-27b, OLMo-7b, and their aligned versions."}, {"title": "2.1 WHERE DO THE BASE AND ALIGNED MODELS DISAGREE?", "content": "Base models are significantly more uncertain at alignment-related positions To study where the base models would disagree with the aligned models, we first count the numbers and ratios of three types of token positions in the aligned models' answers: (1) agree: the base and aligned models agree on the top-1 token, (2) weakly agree: the top-1 token of the aligned model is ranked 2 or 3 in $P_{base}$, (3) disagree, i.e., the alignment-related positions: the top-1 token of the aligned model has a rank larger than 3 in $P_{base}$. We arrange the token position type counts and ratios according to the"}, {"title": "2.2 WHAT TO GENERATE AS ALIGNMENT TOKENS?", "content": "Having answered where to place nudging tokens, if we can further predict what the aligned model would generate for these positions, we can inject these tokens at the decoding time to emulate the aligned model's behavior. Suppose that we want to align a large base model in this way. How to generate nudging tokens without the large aligned model? More specifically, can a small aligned model be used to generate nudging tokens in the same way as a large aligned model? To answer the question, we study how the aligned models of different sizes agree with each other on the alignment-related positions, i.e., where the base and aligned models disagree. We use LLama-2-70b, Gemma-2-27b, OLMo-7b pairs to determine the alignment-related positions and analyze the agreement of the smallest and the largest aligned models in each family, i.e., LLama-2-7b-chat v.s. 70b-chat, Gemma-2-2b-it v.s. 27b-it, and OLMo-1b-it v.s. OLMo-7b-it.\nAligned models of different sizes within the same family tend to agree on alignment-related positions."}, {"title": "3 NUDGING: INFERENCE-TIME ALIGNMENT AT THE TOKEN LEVEL", "content": "The analysis in the previous sections suggests that we can predict where a base model would disagree with its aligned version based on the base model's uncertainty, and at these positions aligned models of different sizes tend to agree with each other. Based on these findings, we introduce NUDGING: a simple training-free algorithm that aligns a base model's output at inference time using nudging tokens generated by an off-the-shelf small aligned model, which we call the nudging model. Given a base and nudging model and a fixed uncertainty threshold $\\gamma$, NUDGING generates the output in a token-level collaborative fashion. As illustrated in Figure 1, for any query $q$, we first let the base model propose a short completion $c = \\{c_1, c_2, ..., c_n\\}$. Then we find the first token position $i$ where the top-1 probability of the base model is less than $\\gamma$: $top-1(P_{base}(\\cdot|q, c_{<i})) < \\gamma$. We discard the tokens after $i$ and insert a token generated by the nudging model. Then we let the base model propose again and find the next uncertain token. If all tokens from the base model's completion have top-1 probabilities larger than $\\gamma$, we continue to generate another completion using the base model.\nWe determine where to nudge based on the token probabilities, but for nudging tokens, we find it beneficial to use spaces as boundaries and use the first \"word\" from the nudging model as the nudging token. In the example shown in Figure 1, we accept \"Sure,\" instead of \"Sure\", in the first nudging round. This leads to better performance and facilitates better collaboration between model families with different tokenizers. We use the nudging model to determine when to stop generating. Specifically, instead of letting the nudging model generate only one token each time, we let it generate a short completion. If the nudging model's completion finishes with an [EOS] token, we append the full completion from the nudging model to the current answer and stop generation (see Figure 7 for an example). Otherwise, we accept the first word from the nudging completion. We depict a detailed implementation of NUDGING in Algorithm 1 in Appendix A.1."}, {"title": "4 EXPERIMENTS", "content": "We evaluate the effectiveness of NUDGING from various aspects. In Section 4.1, we introduce our evaluation setup. In Section 4.2, we compare NUDGING with the base and aligned models and other inference-time alignment baselines on standard benchmarks. We evaluate NUDGING on instruction-following and safety benchmarks in Section 4.3. In Section 4.4, we show that NUDGING is effective even when the base and nudging models are from different model families. Finally, we conduct a scaling-up study on NUDGING and show insights about alignment in Section 4.5."}, {"title": "4.1 EVALUATION SETUP", "content": "Models. To demonstrate the effectiveness of NUDGING, we evaluate it across three different model families: Llama-2 (Touvron et al., 2023a), Gemma-2 (Team et al., 2024), and OLMO (Groeneveld et al., 2024), chosen for their available base and aligned models in various sizes.\nDatasets. We choose 13 tasks across diverse applications, including math reasoning -GSM8K (GSM) (Cobbe et al., 2021a), SVAMP (SVP) (Patel et al., 2021), MultiArith (MA) subsets from Roy & Roth (2015), general knowledge -MMLU (MM) (Hendrycks et al., 2021), commonsense reasoning -Arc-challenge (Arc) (Clark et al., 2018), CommonsenseQA (CS) (Talmor et al., 2019), StrategyQA (ST) (Geva et al., 2021). We also select two commonsense reasoning tasks from the BIG-bench effort (Srivastava et al., 2022): Date Understanding (date), which infers dates from context, and Sports Understanding (SP), following (Wei et al., 2023). We also evaluate two toy tasks on symbolic reasoning: Last letter concatenation (LLC) and Coin Flip (CF) following (Wei et al., 2023). Finally, we evaluate the tasks of just-eval-instruct (Lin et al., 2023) for benchmarking the instruction following and safety. To control the computational cost, we randomly sample 1000 examples from the test set for each dataset for evaluation.\nBaselines We compare NUDGING with the base and aligned models of different sizes in each model family. For other training-free, inference-time alignment baselines, we choose 1) Average ensemble, one of the simplest ways to combine models, that averages the top-5 token distributions of the base and nudging models at each token position for sampling. 2) Proxy tuning (PT) (Liu et al., 2024): the state-of-the-art training-free inference-time tuning method that also uses smaller models"}, {"title": "4.2 STANDARD BENCHMARKS", "content": "We first compare NUDGING with the base and aligned models on standard benchmarks. We report the results with $\\gamma$ = 0.4 for Llama-2 and $\\gamma$ = 0.3 for Gemma-2 and OLMo for the best results and we leave the results for other $\\gamma$ in Table 9. Note that the performance of NUDGING is not very sensitive to the choice of $\\gamma$ as we discuss in Section 5.\nNUDGING significantly boosts the performance of the base and nudging models. As shown in Table 3, we find that combining a large base model with a small aligned model using NUDGING gives a better performance than any of them on almost every dataset for all model families. Specifically, NUDGING boosts the average performance of the base and nudging model by up to 57.9% (Gemma-2-27b) and 15.4% (Llama-2-7b-chat), showing the benefits of combining models. Remarkably, on the last-letter-concat (LLC) dataset, nudging combines Gemma-2-27b (6.7%) and Gemma-2-2b-it (4.7%) and achieves a performance of 86.0% that surpasses that of Gemma-2-27b-it (82.0%).\nNUDGING achieves comparable performance to the large aligned models and is particularly effective on math and symbolic reasoning tasks. Surprisingly, NUDGING mostly performs on par with the large aligned models (Table 3). For Llama-2 and OLMO, NUDGING even outperforms the large aligned models on average. We find that this success is largely due to NUDGING's effectiveness on math and symbolic reasoning tasks. Notably, OLMo-7b-it shows lower zero-shot performance than OLMo-7b on the GSM and MA math datasets, which aligns with recent findings"}, {"title": "4.3 INSTRUCTION FOLLOWING AND SAFETY", "content": "To evaluate the effectiveness of NUDGING in helpfully and safely following instructions, we compare NUDGING with the base and aligned models on the just-eval-instruct dataset.\nNUDGING gives aligned-model-level performance on instruction following tasks. Following Lin et al. (2023), we evaluate the answers along 5 dimensions using GPT-4. As shown in Figure 4, NUDGING performs on par with aligned models in all five dimensions and significantly outperforms the base models. By combining the small aligned model with a more knowledgable large base model, NUDGING improves the factuality scores for both Llama-2 and OLMo. Interestingly, nudging Llama-2-70b-chat with Llama-2-7b-chat is rated slightly higher even in helpfulness and engagement over Llama-2-7b-chat, even though over 85% of the tokens are from the base model."}, {"title": "4.4 COLLABORATION OF MODELS FROM DIFFERENT FAMILIES", "content": "One main advantage of NUDGING over other inference-time tuning methods like proxy tuning is that NUDGING allows collaborations of models from different families. When a new family of base models comes out, one can make them instantly useful by nudging them with existing aligned models. To demonstrate this, we use Llama-2-7b-chat and OLMo-7b-it (small aligned models) to nudge Gemma-2-27b (base model) on GSM8K and MMLU. As shown in Table 6, NUDGING boosts the performance of Gemma-2-27b substantially on both datasets, and using Llama-2-7b-chat as the nudging model even largely outperforms Llama-2-70b-chat."}, {"title": "4.5 SCALING UP THE MODELS", "content": "Nudging provides a modular and flexible solution to alignment with many potential benefits as shown in previous results. To develop a deeper understanding of the role of the base and nudging models, we conduct a scaling-up study using Llama-2 and Gemma-2 families on three datasets: GSM8K, MMLU, and Arc-challenge. Specifically, we (1) fix the size of the base model and scale up the nudging model and (2) fix the size of the nudging model and scale up the base model.\nA small aligned model is sufficient while using a stronger base model significantly enhances performance. As shown in the left part of Figure 5, for both Llama-2 and Gemma-2, using the smallest aligned model as the nudging model is as good as using the larger ones. This shows that a small model is sufficient for generating alignment tokens to nudge a much larger base model, which is another evidence supporting that alignment only adds minor abilities to the base models. On the other hand, as shown in the right part of Figure 5, when the nudging model is fixed, using a larger, i.e., more capable, base model brings substantial gains. This explains the improvements when we change the base model from LLama-2-70b to Gemma-2-27b and confirm again the core abilities of LLMs stem from the pre-training stage."}, {"title": "5 ANALYSIS", "content": "So far we have demonstrated the effectiveness of NUDGING in various scenarios. However, questions remain: How many and what kind of tokens are the nudging tokens? How do the nudging tokens help the base model? How should we choose the uncertainty threshold in practice? In this section, we dig deeper into these aspects of NUDGING."}, {"title": "6 RELATED WORK", "content": "Analysis of alignment Many recent studies focus on understanding the nature of alignment. Zhou et al. (2024); Chen et al. (2023) find that a small amount of carefully curated instruction-tuning data is sufficient to teach base models to generate high-quality responses, posing the superficial alignment hypothesis. Lin et al. (2023) provide a token-level view to understand alignment and find that only a small subset of stylistic tokens are affected after alignment. Another line of work shows that alignment might hurt certain capabilities of the base models. Wang et al. (2023) find that some base LLMs become significantly worse on factual and reasoning benchmarks after supervised fine-tuning. Ghosh et al. (2024) also show that supervised instruction tuning makes LLama-2 models more likely to hallucinate. Mitchell et al. (2023) analyze the effect of scaling-up pre-training and instruction tuning and find instruction tuning increases the helpfulness of the model while factual knowledge comes from pre-training. Building on top of these findings, we proposed a modular and token-level solution to alignment that favors the disentanglement of alignment and general abilities.\nInference-time tuning methods As the LLMs (Brown, 2020; Achiam et al., 2023; Touvron et al., 2023b; Dubey et al., 2024) being increasingly large, fine-tuning them becomes prohibitively expensive. Therefore, like NUDGING, many works explore using smaller models to adapt the large models' behavior at inference time without updating or accessing the model weights. Liu et al. (2024; 2021); Mitchell et al. (2023) use the distributions of a pair of tuned and unturned small models to rescale the distribution of the large models. NUDGING offers a simpler, faster, and more flexible solution with better performance. Other works propose to train a small expert model or adapter to optimize large base models for specific tasks. For alignment specifically, many works consider in-context learning as a solution to inference-time alignment. However, using in-context examples shortens the usable context length. More importantly, in-context examples can lead to various biases, and effective examples might be task or evaluation-specific. Finally, Shen et al. explore a similar token-level model collaboration to our work. Compared with NUDGING, their method needs task-specific training for all model pairs and is not specifically about alignment.\nToken-level alignment of LLMs While most popular RLHF methods optimize at a sample-level, token-level alignment methods get increasing attention recently. Specifically, designed token-level reward for RLHF to provide more detailed control of model responses. Deng & Raffel uses token-level reward at decoding time to adjust the model's outputs. NUDGING shares the same motivation with these works in adapting large language models' outputs from the token level."}, {"title": "7 DISCUSSION AND CONCLUSION", "content": "In this work, we introduced NUDGING, a simple yet powerful approach to align large language models at inference time without the need for additional training. The simplicity and modularity of NUDGING present a promising alternative to traditional alignment methods, drastically reducing the computational cost of training while delivering significant performance gains across diverse tasks. The simplicity of nudging makes it useful and easy to implement. However, designing more optimized ways of finding the nudging positions and generating nudging tokens is worth exploring. Here, we provide two future directions: (1) Currently where to nudge is solely determined based on the base model, which assumes that the base model is well-calibrated. However, for many practical use cases, we may want to guide the base model's behavior based on customized rules. In this case, designing new nudging rules that take the nudging model's distribution into account is an interesting direction to explore. (2) NUDGING combines off-the-shelf models. Although this is effective, the small aligned model is not explicitly trained to generate nudging tokens. Therefore, learning a model that predicts where and what to nudge can potentially shrink the size of the nudging model while further improving the answer qualities, e.g., fluency and conciseness. When deploying NUDGING via an API, additional inference costs arise since generating a single response requires multiple calls to both the base and nudging models. However, if we have direct access to the base and nudging models, we can significantly optimize NUDGING by caching prefixes during generation, similar to Speculative Decoding. In this case, the inference speed of NUDGING can approach that of the base model alone, as the nudging model is substantially smaller. Finally, by enabling models to collaborate at the token level, NUDGING harnesses the strengths of different models and effectively disentangles their capabilities. Our work provides a fresh perspective on aligning large language models and offers a promising direction for designing modular AI systems."}, {"title": "A APPENDIX", "content": "A.1 MORE IMPLEMENTATION DETAILS\nIn this section, we provide more implementation details about NUDGING and the two baselines we compare NUDGING with, the average ensemble and proxy tuning. We implement all methods based on vllm with A6000 GPUs."}, {"title": "A.1.1 NUDGING", "content": "We depict a detailed implementation of NUDGING in Algorithm 1. For our implementation, we set the completion length L to be 16 as it balances the computational cost and gives the nudging model better control of when to stop generating. We set the max nudging round R = 100 and the max token number T = 512. When passing the query prompt and the current answer to the nudging models, we adapt them using the instruction templates of the corresponding model families accordingly. Finally, we use a simple heuristic for repetition control: When the base model's completion appears in the current answer, we end the round and pass to the nudging model. If the nudging words for three consecutive rounds are the same, we terminate generation.\nUsing words instead of tokens For finding nudging tokens, we use spaces, i.e., \"\", to split the nudging completion and use the first word as the nudging tokens. We find that using the first word, rather than the first token, leads to better performance. We hypothesize that this is because words, as the basic semantic units of language, provide more meaningful guidance for steering base models, whereas sub-word level tokens may sometimes lack the semantic coherence needed for effective nudging. For example, for LLama-2 models on GSM8K, the nudging model mostly starts the answer with \"Sure\", and the base model would complete the word with \u201cly\u201d, ending up with \u201cSurely\u201d, which usually leads to worse answers. Also, using full words as nudging tokens makes the collaboration of different model families easier when they have different tokenizations of words."}, {"title": "A.1.2 BASELINES", "content": "Average ensemble We choose the average ensemble as a baseline as it is one of the simplest ways to combine two different models. We average the top-5 token distributions of the base and nudging models at each token position before sampling. To put the baseline in a similar condition with NUDGING, we assume that we only have access to the top-5 top log probs from the models, which is the maximum number of top log probs for most API service providers like Fireworks AI. At inference time, for each token position, we retrieve the top-5 token probabilities from both the base and the nudging model and then average the probability of each token. If a token appears only in the top-5 tokens of one model, its probability is halved. This ensemble operation is applied to each token position, meaning the number of calls made to both the base and the nudging model corresponds to the number of answer tokens.\nProxy tuning Proxy tuning works by rescaling the large base model's distribution by contrasting the distribution of a pair of small models. Ideally, it requires the full distribution from all models to work. This requirement cannot be satisfied for API-based implementation, which is the base for most practical applications. Following , we use the top-100 probabilities from the models due to the limited computational resources, and following their implementation we only focus on tokens that appear in the top 100 tokens of all models. When the top log probs number is small, e.g., 5, the top tokens from all three models might not intersect at all, making proxy tuning not feasible for most API service providers."}, {"title": "A.2 EVALUATION", "content": "Standard benchmarks For math reasoning tasks, following ; , we extract the last number in the model's response based on rules. For other tasks, we use GPT-4 to compare the generated answers with the gold answers using a predefined template as shown in Figure 7. We manually check that the automatic evaluation correctly reflects how well the models perform in general.\nInstruction following and safety For instruction following and safety datasets, we follow the evaluation setup of and use their evaluation prompts. For NUDGING, We find it is beneficial to slightly increase the uncertainty threshold $\\gamma$. Therefore, we report the results with $\\gamma$ = 0.4 for LLama-2 and Gemma-2 and $\\gamma$ = 0.5 for OLMo in Section 4.3."}]}