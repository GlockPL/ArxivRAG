{"title": "EDGE: Enhanced Grounded GUI Understanding with Enriched Multi-Granularity Synthetic Data", "authors": ["Xuetian Chen", "Hangcheng Li", "Jiaqing Liang", "Sihang Jiang", "Deqing Yang"], "abstract": "Autonomous agents operating on the graphical user interfaces (GUIs) of various applications hold immense practical value. Unlike the large language model (LLM)-based methods which rely on structured texts and customized backends, the approaches using large vision-language models (LVLMs) are more intuitive and adaptable as they can visually perceive and directly interact with screens, making them indispensable in general scenarios without text metadata and tailored backends. Given the lack of high-quality training data for GUI-related tasks in existing work, this paper aims to enhance the GUI understanding and interacting capabilities of LVLMs through a data-driven approach. We propose EDGE, a general data synthesis framework that automatically generates large-scale, multi-granularity training data from webpages across the Web. Evaluation results on various GUI and agent benchmarks demonstrate that the model trained with the dataset generated through EDGE exhibits superior webpage understanding capabilities, which can then be easily transferred to previously unseen desktop and mobile environments. Our approach significantly reduces the dependence on manual annotations, empowering researchers to harness the vast public resources available on the Web to advance their work. Our source code, the dataset and the model are available at https://anonymous.4open.science/r/EDGE-1CDB.", "sections": [{"title": "1 Introduction", "content": "Autonomous interaction with computing devices has long been a topic of artificial intelligence research [7, 36]. With the popularity of personal computers and smartphones, the agents that can automatically interact with graphical user interfaces (GUIs) of various applications have become a growing focus [5, 23, 33, 49]. Recently, the continuous advance of large language models (LLMs) [1, 4, 52] makes it increasingly feasible to develop general GUI agents.\nNumerous studies [15, 17, 19, 21, 27, 47, 50, 58, 59] have proposed LLM-based GUI agents on web, mobile, and desktop environments. These methods avoid visually perceiving GUIs by using structured texts (e.g., HTML for webpages [17, 21] and view hierarchy for Android screens [47, 58]) as input. In addition, they may require environment-specific backend access to control the systems [50, 59]. However, given that GUIs are designed for human interaction through visual reading and basic actions like clicking, the methods relying on text metadata or customized backend are neither intuitive nor widely practical. Since GUI formats vary across different environments, the agents in generalized scenarios must interact with screens in a human-like manner, without access to underlying data.\nRecognizing this limitation, recent studies [13, 24, 31, 44, 48, 60, 62, 66] have turned to the vision-based approaches that employ large vision-language models (LVLMs) to directly perceive and interact with screens, as illustrated in Figure 1. To improve the limited capabilities of LVLMs in grounded GUI understanding and interacting, where \"grounded\" refers to the ability to precisely locate elements, these efforts often involve further training on the GUI-specific datasets with element-level annotations, which are typically sourced from the Web [13, 20, 24, 66]. Specifically, they drive browser engines to capture the element-level correspondences between HTML and rendered webpage images, including both positions and content. This automated process exploits the supervisory signals inherent on Web, reducing the need for manual annotations.\nHowever, models trained using these methods still exhibit some drawbacks in real-world scenarios. Firstly, these models generally underperform on the GUIs with rich visual elements such as images and icons. Secondly, despite their fine performance on element-level grounded understanding, they are not competent on the action grounding tasks, where the models are required to map real instructions to certain operations on target elements. Thirdly, these models have difficulty understanding the local and global semantics that humans can easily capture, resulting in possible failure on tasks beyond the single-element level.\nTherefore, following a data-driven approach, we propose EDGE, an automated webpage annotation and data synthesis framework. EDGE is characterized by its rich annotations and multi-granularity task settings, aimed at enhancing the capabilities of open-source LVLMs in grounded GUI understanding and interaction, while also promoting further research on general GUI agents. Specifically, in the annotation stage, we extract explicit texts as well as rich latent elements from webpages, with an additional collection of extensive icons commonly used in GUIs to address the challenges of icon understanding. Subsequently, in addition to the basic element-level recognition question-answering (QA), EDGE also integrates the multi-granularity QA tasks synthesized using the powerful proprietary model Claude-3.5 [4]. This comprehensive task setup is designed to cover the global semantic understanding and reasoning abilities required in complex webpage and action grounding scenarios. Besides, through our framework, the model's GUI-related capabilities learned from large-scale webpages can be easily transferred to desktop and mobile environments, which lack available supervised data and typically rely on manual annotations.\nOur main contributions are summarized as follows:\n\u2022 We propose EDGE, an automated webpage annotation and data synthesis framework that significantly enriches annotations and covers multi-granularity tasks to enhance the GUI-specific capabilities of LVLMs.\n\u2022 Following EDGE, we constructed a large-scale dataset of GUI tasks and release it together with the LVLM trained on it, which has excellent grounded GUI understanding and interacting capabilities.\n\u2022 The model's outstanding performance on GUI benchmarks [13, 41] and the gains achieved on downstream agent benchmarks [17, 47, 49] demonstrate the effectiveness of EDGE."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Large Vision-Language Models", "content": "Various studies have proposed solutions to address the challenges [51, 64, 67] of architectural complexity and data scarcity in the training of LVLMs. A series of methods [2, 32, 32, 57, 63] utilize elaborately designed architecture to integrate visual encoders with LLMs, thereby inheriting the language and reasoning abilities of LLMs. For data scarcity, the LLaVA series [38, 40] utilize GPT-4 [1] to generate rich instruction-following data. Monkey [35] proposes a multi-level caption generation pipeline. Such data-driven approaches are more common in training domain-specific knowledge, such as medicine [30], and spatial awareness [11, 46, 69].\nIn this work, we focus on GUI environments and adopt the data-driven approach to address the limitations in both the quantity and diversity of training data."}, {"title": "2.2 GUI Understanding and Interacting", "content": "Previous work primarily targets simplified web [22, 29, 37, 48, 49] or mobile environments [8, 31, 33]. Recent emergence of real-world benchmarks [17, 47, 74] promote an LLM-centric paradigm. Several methods explore prompting GPT-4 for web tasks via in-context learning [73] and self-refinement [26]. WebAgent [21] enhances manipulation through instruction decomposition and programming.\nRecently, research focus has shifted to more intuitive and adaptable LVLM-based methods. Mobile-Agent [55] uses external detection models to determine the positions of interactive components.\nA series of studies [19, 60, 62, 72] rely on the powerful visual perception ability of GPT-4V, combined with extracted text metadata to compensate for its lack of grounding capabilities [1].\nLatest work like CogAgent [24], SeeClick [13], and Ferret-UI [66] neither depend on proprietary models nor GUI metadata. Our work, following the same motivation, aims to empower open-source LVLMs with GUI understanding and interacting capabilities."}, {"title": "2.3 Grounding Capability of LVLMs", "content": "Researchers have proposed various improvements to the grounding performance of LVLMs, including introducing external vision models [55, 71], internally integrating fine-grained visual modules [28, 65, 68], and directly predicting bounding boxes [11, 46, 69].\nClassical detection models [9, 42, 75] are not well-suited for GUI environments due to semantic disparities, resulting in a scarcity of detailed annotated datasets. CogAgent [24] leverages crawled webpages to collect large-scale annotations, supplemented by manually annotated fine-grained data. SeeClick [13] develop a simple framework for annotation grounding dataset synthesis. Ferret-UI [66] uses the UI detection model to annotate mobile screens, as well as to synthesize elementary and advanced tasks by GPT-4.\nInheriting the advantages of these works, we aim to generate richer annotations from public webpages and automatically synthesize multi-granularity data with the aid of proprietary models."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Motivation", "content": "In humans' view, understanding and interacting with a GUI requires perceiving the content and locations of on-screen elements and accurately positioning the target element. Based on this intuition, we believe that the poor performance of existing general-purpose LVLMs [1, 4, 6, 30, 52] in GUI tasks [13, 41] stems from their lack of grounded GUI understanding and interacting capabilities.\nOn one hand, GUIs are characterized by flat, dense elements and a mix of text and icons, hinder the transfer of some models' built-in grounding abilities in natural images [6] to GUI environments. On the other hand, even though proprietary models [1, 4, 52] possess universal image understanding capabilities, the inherent weakness in object detection prevents them from locating elements on the screen. Therefore, models must undergo element-level grounded training within GUI environments to enhance their performance.\nEDGE is designed to address the lack of large-scale, high-quality datasets in this domain. Before describing the synthesis framework and data pipeline, we first formalize the definition of GUI grounding capabilities of LVLMs and its training process."}, {"title": "3.2 GUI Grounding Tasks", "content": "We first introduce element grounding and action grounding tasks. Similar to open-set object detection [42], element grounding requires models to find the location $l$, as coordinates of a point $(x, y)$ or a bounding box $(x_1, y_1, x_2, y_2)$, of an element within the input screenshot $s$ based on a text description $t$, i.e., to predict $p(l | s, t)$. Action grounding shares the same input-output format but here $t$ refers to a user instruction. The model must identify the target element before locating it, as the instruction may differ from the text of the target element. The reverse of grounding, namely predicting $p(t | s, l)$, is called referring. Grounding and referring tasks often appear in pairs in EDGE, mutually reinforcing each other to promote cross-modal semantic fusion.\nUnlike vision models [9, 42, 75] that directly predict numeric coordinates, LVLMs typically treat coordinates as part of the output tokens. Previous studies [48, 54] introduce extended vocabularies (such as {<p0>, <p1>, . . ., <p999>}) to encode coordinates, whereas we follow a more intuitive manner that directly treats them as native language tokens [6, 11]. This method eliminates the need for additional pre/post-processing without compromising performance. In this setting, training completely follows the standard next-token prediction with cross-entropy supervision."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Overview", "content": "Following the human intuition, the overall task settings in EDGE primarily encompass the elementary tasks and advanced tasks as named by Ferret-UI [66]. The elementary tasks are designed to enhance the element-level grounded understanding of the GUI, while the advanced tasks introduce the multi-granularity understanding and reasoning capabilities required for GUI interactions.\nThe training of GUI grounding requires massive element-level annotations. Following previous work [13, 20, 24], we collect general webpages from Common Crawl\u00b9, a large-scale web crawl data repository, and drive Playwright to automatically annotate them. Then we format these annotations to synthesize question-answering (QA) data using various templates. This process is detailed in Section 4.2.\nIn terms of advanced tasks, We additionally collect highly interactive mainstream websites and leverage proprietary models [4] to synthesize more flexible and interaction-focused QA pairs, along with detailed descriptions that assess multi-granularity understanding capabilities. We introduce this process in Section 4.3.\nConsidering the weaknesses of existing open-source LVLMs, we additionally propose the icon understanding task. This task utilizes an automatically collected icon annotations dataset to enhance the model's ability to understand and interact with icons, as outlined in Section 4.4."}, {"title": "4.2 Elementary Tasks", "content": "The elementary tasks primarily synthesize element-level QA from large-scale general web annotations to equip the model with grounded GUI understanding, thereby promoting effective interactions. We introduce the generation in three stages: collection, annotation, and synthesis.\n4.2.1 Collection. Due to the vast quantity and varying quality of webpages on Common Crawl, instead of performing our own cleaning, we collect webpages from a refined subset FineWeb-Edu [43]. FineWeb-Edu consists of carefully cleaned and quality-assessed pages, retaining those that are most educationally valuable for LLMs and typically exhibit well-designed layouts in their GUIs.\n4.2.2 Annotation. In this stage, we render the webpages, take screenshots and capture positions and content of on-screen elements by injecting JavaScript scripts. Compared to previous work [13, 15, 20, 24], our annotations introduce improvements in both the effectiveness and richness, making them closer to human-generated ones. Figure 2 showcases the characteristics of our method with the annotation results on the Google homepage.\nEffectiveness. Given that there is no unified design standard at the source code level across different websites, identifying the minimal semantic units is crucial for effective annotation. Our carefully designed script includes steps for visibility filtering, classification and integration, ensuring that annotations align with visual presentation and human cognitive habits. Technically, instead of focusing solely on leaf nodes of the DOM tree, we adopt a set of rules to integrate the minimal semantic units based on the structure and visual representation of common tags (e.g., <a> and <button>). More technical details of the script are presented in the Appendix A.\nRichness. Besides explicit text, we also capture the various latent description, such as alt (<img>), title and accessibility content (aria-label). These properties are designed for assistive technologies and are highly suitable as training data for GUI understanding tasks, yet existing annotation methods [13, 15] tend to ignore them.\n4.2.3 Synthesis. Based on the annotations, we synthesize element-level grounding and OCR tasks, including elements with explicit text and accessibility labels. We adopt the multi-turn conversation format, where a training sample contains multiple QAs on one screenshot. We additionally synthesize the basic page-level QA about the title and description of the webpage. This process is illustrated in the top half of Figure 3 and the final form of training samples are shown in the Appendix B.\nIn our early explorations, the model could roughly locate elements but lacked precision. Hence, we designed two data augmentation techniques to address this weakness:\n1. We randomly crop the screenshots, accordingly modifying the coordinates of visible elements. The model is compelled to identify subtle positional variations from similar image features.\n2. We overlay a rectangular box on a randomly selected element within the screenshot. This new task requires the model to simultaneously identify the both position and content of the \"highlighted\" element, promoting the fusion across modalities."}, {"title": "4.3 Advanced Tasks", "content": "Unlike the elementary tasks which focus on understanding through template-based generation, the advanced tasks aim to incorporate the multi-granular understanding and reasoning abilities to respond to free-form interactive instructions. Therefore, we utilize the same annotation methodology to generate high-quality QA pairs on a set of highly interactive mainstream websites, with the help of Claude-3.5 [4]. Through the training on these advanced tasks, the model becomes capable of understanding user intention and correctly responding to interaction-related queries regarding the GUI, laying a solid foundation for downstream agent tasks.\n4.3.1 Collection. To facilitate GUI interaction, we need more interactive webpages, rather than the aforementioned general ones which are primarily information-focused such as news and blogs. Hence, we collected the top 20,000 domains from Ahrefs Rank\u00b3, considering these as the most frequently used websites which are typically highly interactive.\n4.3.2 Annotation and Synthesis. The homepages of these sites are annotated using the same method as proposed. During the synthesis stage, we transfer the following three advanced tasks settings from the mobile dataset of Ferret-UI [66] to our web environment:\n\u2022 Function inference: Provide a brief summary of the purpose of the webpage or the function it offers to users.\n\u2022 Detailed description: Offer a detailed account of the position and content of each major element on the webpage.\n\u2022 Conversation intention: Select elements and simulate user interactions with them, presented in a QA format.\nAmong these, function inference requires global semantics perception, detailed description involves a mix of element-level and local-level understanding, while the last one is similar to the action grounding introduced in Section 3.2, testing reasoning capabilities.\nWe input the annotations of webpages into Claude-3.5 [4] and prompt it to generate QA pairs for these tasks. Considering the complexity of webpages, we also input the screenshots annotated by the Set-of-Mark[61] prompting method, aiming to enable Claude-3.5 to capture semantics that automatic annotation might miss. The synthesis of advanced tasks is shown in the bottom half of Figure 3."}, {"title": "4.4 Icon Understanding", "content": "Scripts-based annotations struggle to capture the semantics of visual elements without textual content. Since icon understanding is a weakness in most open-source LVLMs [13], EDGE also incorporates an automated icon annotating process, adhering to our principle of minimizing manual work.\nFirst, we collect frequently-used icons from several icon font libraries 456. Then GPT-4 is prompted to generate descriptions for them. The icon-description pairs are directly added to the QA training set. In addition, to simulate the mixed layout of icons with other content in real GUIs, icons are also randomly embedded into webpage screenshots to construct referring and grounding tasks.\nUnlike OCR, icon understanding relies more on the similar cases seen during model training. Our collection includes commonly used icons and logos of well-known products, which can effectively address the model's cognitive gap regarding icon-dense GUIs and logos frequently encountered in real-world scenarios."}, {"title": "4.5 Dataset Statistics", "content": "To facilitate the transfer of web GUI capabilities to general GUI environments, while retaining the ability to comprehend natural images, EDGE also includes three tasks derived from the mobile GUI annotation dataset Rico [16]: Screen2words [54], Widget-Caption [34], and Ricosca [33], along with general visual instruction-following and QA dataset from the training set of LLaVA [68] and Monkey [35]. Together with the webpage dataset synthesized by EDGE itself, this forms a comprehensive dataset comprising 5.42 million QA pairs across 1.66 million images, still referred to as EDGE for convenience. Approximately 60% of the images and 85% of the QA pairs originate from the EDGE proprietary synthesis method. Figure 4 illustrates the distribution of task types within the final dataset and the number of samples from different webpage sources in the web environment QAs."}, {"title": "5 Experiments", "content": "To validate the effectiveness of EDGE, we train a model using it (hereinafter referred to as EDGE as well) and compare its performance with some representative LVLMs on GUI-related tasks. We outline the training details in Sec 5.1. Then the evaluation results on two GUI-specific benchmarks [13, 41] and three downstream agent benchmarks [17, 47, 49] are presented in Sec 5.2 and 5.3, respectively. Finally we introduce the ablation study in Sec 5.4."}, {"title": "5.1 Training Details", "content": "EDGE undergoes further training on the robust baseline, Monkey [35]. We freeze the image encoder and train all parameters of the LLM. For accelerating and memory saving, we adopt a Lion 8-bit optimizer [12] with a learning rate of 1e-5 and a weight decay ratio of 0.1. The cosine learning rate scheduler is employed with a warm-up ratio of 0.02 and the global batch size is 512. Other settings follow Monkey's. Training for one epoch on the full dataset takes approximately 2 days on 8 NVIDIA A800 GPUs."}, {"title": "5.2 Results on GUI Benchmarks", "content": "We evaluated the grounded GUI capabilities of EDGE on two recently proposed benchmarks [13, 41]. In these settings, each sample represents an independent question or instruction, and the model is required to answer the question or locate the target element based solely on the image, without any context.\n5.2.1 VisualWebBench [41]. Comprising over 1,500 samples from 139 websites, VisualWebBench is designed to comprehensively evaluate the understanding and interaction capabilities of LVLMs on webpages. There are seven tasks of different granularities, ranging from deterministic, elementary OCR and grounding to open-ended captioning that requires global semantics.\nCompetitors & Metrics. The original paper [41] reports the performance of a number of general-purpose and GUI-specific LVLMs. Based on the existing results, we additionally report the performance of Monkey [35], EDGE, and the latest proposed Qwen2-VL [56], a general-purpose LVLM but trained with a considerable proportion of GUI grounding data and is therefore classified as semi-GUI-specific. When evaluating the grounding capability of GUI-specific models, we adopt point prediction and report the click accuracy, the proportion of predicted points falling within the ground truth bounding box, rather than the default multiple-choice settings with selection accuracy for general LVLMs. We also slightly modify the input prompt of EDGE in the OCR, grounding, and captioning tasks to make them more consistent with the input form of EDGE. All other settings remain the same as the original paper [41].\nResults. As shown in Table 1, EDGE achieves the best average score among GUI-specific models, and even approximates general-purpose LVLMs with significantly more parameters. First, in the formalized Element OCR task, EDGE performs far better than other GUI models and is close to the strongest Claude Sonnet. We specially note the comparison of the results of the Element Ground and Action Ground tasks. According to the original paper [41], while performing well in the multi-choice setting, general LVLMs can barely handle the more difficult point predictions setting. However, EDGE's point prediction performance in the two grounding tasks"}, {"title": "5.2.2 ScreenSpot [13]", "content": "Encompassing over 1200 instructions from 600+ GUI screenshots across web, desktop, and mobile environments, it is specifically designed to evaluate LVLMs' action grounding capabilities and characterized by a substantial number of icon or widget grounding samples.\nCompetitors & Metrics. ScreenSpot reports the performance of a general LVLM with grounding capability and GUI-specific LVLMs. Similarly. The evaluation metric is click accuracy as well.\nResults. As shown in Table 2, compared with general LVLMs, GUI-specific models demonstrate significant improvements. EDGE achieved the best average performance across different scenarios, significantly outperforming both SeeClick and CogAgent, demonstrating the effectiveness of our data framework. Additionally, EDGE successfully generalized the GUI interacting capabilities to unseen iOS and desktop environments, indicating its transferability.\nIn terms icon samples, EDGE shows a evident performance improvement compared to SeeClick and CogAgent, underscoring the benefits of our framework in addressing the challenges of collecting icon annotations. The strong competitor, Qwen2-VL, slightly outperforms EDGE in mobile and desktop icon samples, likely due to a better model architecture, flexible resolution settings, and a larger, higher-quality set about icon annotations. Unfortunately, Qwen2-VL has not released its data construction details yet."}, {"title": "5.3 Results on Agent Benchmarks", "content": "We further explore the benefits of EDGE's grounded GUI understanding and interacting on downstream agent benchmarks [17, 47, 49]. Here each sample typically represents a multi-step interaction environment. Models are required to determine the next action based on overall instruction and current screenshots as well as the operation history, which involves sequential decision-making that goes beyond the grounding and interaction tasks focused on in EDGE. Therefore, we highlight the improvements relative to the baselines (Monkey and Seeclick), rather than the absolute results.\nWe completely follow the experimental setup in SeeClick [13], that is, for each benchmark, we further fine-tune EDGE using the exact same respective training split, and evaluate it on the corresponding test set with the same metrics.\n5.3.1 MiniWob [49]. MiniWob is a simplified web environment that includes about 100 types of web automation tasks, where the agent is asked to accomplish open-ended instructions. Our training set contains 2.8k episodes.\nCompetitors & Metrics. We compare EDGE with several offline training methods, including text-based and vision-based. The current state-of-the-art method WebGUM [18] use screenshots as auxiliary input but still interact with the environment through HTML elements selecting. Pix2Act [48] is an vision-based approach trained with extensive demonstration data. Due to the variance in test splits among different methods [18, 37, 48], for fairness, we report performance in two groups based on the overlapping tasks. We compute the success rate over 50 random seeds for each task and then compute the average over all tasks as the final score.\nResults. As reported in Table 3, our purely vision-based EDGE outperforms the offline SOTA WebGUM [18] with HTML and screenshots as input under the same 2.8K training data setting. Benefiting from the powerful LVLM baseline and our grounded GUI training, EDGE also surpass vision-based SeeClick [13] and Pix2Act [48], the latter of which uses nearly 500 times more data."}, {"title": "5.3.2 Android In The Wild (AITW) [47]", "content": "As a smartphone environment benchmark consists of 715k episodes spanning 30k unique instructions, AITW considers a wide range of action types on mobile devices, including tap, swipe, typing, going home, back, etc.\nCompetitors & Metrics. We compare EDGE with two types of models: the methods based on the structured text provided by the original dataset and those visually perceive screenshots. Following SeeClick's [13] setup, we adopt the screen-wise action matching score from the original AITW paper [47] and additionally report the click accuracy (ClickAcc), the accuracy when both reference and prediction are click operations.\nResults. As displayed in Table 4, although not as good as Auto-UI and CogAgent, EDGE outperforms general LVLMs like GPT-4V and Qwen-VL, as well as SeeClick trained in single-granularity data. These results support our idea that rich multi-granularity grounded training enhances the performance of downstream agents tasks.\n5.3.3 Mind2Web. Mind2Web [17] comprises over 2000 open-ended tasks collected from 137 real websites, each with high-level instruction for developing and evaluating generalist web agents.\nCompetitors & Metrics. Originally designed for LLM-based agents, the baseline [17] employs a two-stage approach, where a language model first filter candidate elements from raw HTML, followed by a Flan-T5-XL [14] selecting the target element in a multi-choice format. Other text-based model share the same filtered HTML and multi-choice setup. We follow the setting of [17] and report the step success rate (step SR) as metric. For LVLMs, we use screenshots instead of HTML as input and report the click accuracy.\nResults. As shown in Table 5, EDGE offers evident improvements over the base model Monkey and the competitor SeeClick. However, it still lags significantly behind CogAgent and other text-based models, indicating the heavy dependence of real-world web agents on planning capabilities and highlighting the substantial room for improving vision-based agents."}, {"title": "5.4 Ablation Study", "content": "To validate the effectiveness of the advanced tasks, we design an experiment where all advanced task samples (i.e., function inference, detailed description, conversation intention) were removed from EDGE. The model was then retrained under identical conditions. As shown in Table 6, the model trained without advanced tasks performs worse on both GUI benchmarks. It is important to note that the advanced task samples only constitute 0.5% of the total dataset, yet their efficiency in further improving the model's capabilities is evident. We also provide a detailed report in Figure 5, covering global understanding and interaction tasks in both benchmarks. In all these scenarios, the full EDGE consistently outperformed its ablated variant. This demonstrates that the training of multi-granularity and reasoning abilities in the advanced tasks enhances the model's GUI understanding and interaction capabilities.\nWe further present two toy examples in Figure 6 to display the different outputs of the full EDGE and its ablated variant. The full EDGE perfectly answered the questions in the first screenshot, going beyond repeating the heading. In the second one, it successfully distinguished between the product itself and the button for adding it to the cart. These examples provide intuitive demonstrations of how training on advanced tasks can enhance the model. Additional demonstration examples are provided in Appendix C."}, {"title": "6 Limitations and Prospects", "content": "EDGE still has significant room for expansion in terms of data scale and diversity. This includes leveraging more web data, introducing additional actions such as typing and double-clicking, and integrating the planning capabilities required for agent tasks."}, {"title": "7 Conclusion", "content": "This paper proposes an EDGE, enriched, multi-granularity, and fully automated data synthesis framework for open-source LVLMs, aimed at enhancing their performance on GUI tasks. Based on the annotation of large-scale webpages, we designed elementary recognition tasks and advanced operational tasks, as well as additional icon understanding task, which enable LVLMs to acquire the multi-granularity capabilities required for understanding and interacting with web GUIs, with seamless transfer to mobile and desktop environments. The model's excellent performance in GUI benchmarks and improvements in agent benchmarks validate the effectiveness of EDGE. Our work demonstrates that the vast resources available on the Internet hold significant value for various endeavors, including but not limited to LVLMs training. Furthermore, even individual researchers with limited manpower and funding can effectively leverage these resources to advance their work."}, {"title": "A Annotation Script", "content": "We meticulously implement a JavaScript script for annotating any webpage, which can be executed directly in the browser console or via browser drivers. It first filters out invisible elements within the current viewport, followed by excluding visually hidden elements, even if present in the HTML, by checking factors like the computed styles and cursor accessibility, as well as the special cases like overflow beyond its container's boundaries, to ensure accurate visibility judgments. In our experience, this precise visibility filtering is crucial for the accuracy of subsequent element annotations.\nNext, we determine the type of each visible element based on attributes like tag, role, and computed styles, covering common elements found on webpages, such as text, code, images, icons, buttons, hyperlinks, and inputs. Then we designed a set of integration rules based on the type, content, and children of an element to decide whether it represents the smallest, complete semantic unit visually. For example (illustrated in Figure 7), buttons often consist of borders, text, and icons visually. Traditional methods tend to recognize the text and icon separately, leading to incomplete semantic understanding. In contrast, we treat the entire button as a single element and use the outermost border to determine its position."}, {"title": "B Dataset Details", "content": ""}, {"title": "B.1 Training Samples", "content": "When rendering the webpage, we select from 16 various screen sizes to simulate the presentation of webpages on desktop, tablet and mobile devices. The input images of the training samples are raw screenshots. The dotted box annotations in the previous images are only for demonstration purposes.\nWe use two question forms in grounding tasks: predicting the bounding box (x1, y1, x2, y2) or its center point (x, y), with the specific choice explicitly indicated in the input. As shown in Figure8, for element-level tasks, we use a multi-turn conversation format, which is not only more sample efficient, but also bring better experimental results. Page-level tasks still occupy a single sample."}, {"title": "C More Inference Results", "content": "More reasoning results are shown in Figure 9. Our model supports understanding and answering user operation instructions in a natural, human-like manner."}]}