{"title": "Monte Carlo Tree Diffusion for System 2 Planning", "authors": ["Jaesik Yoon", "Hyeonseo Cho", "Doojin Baek", "Yoshua Bengio", "Sungjin Ahn"], "abstract": "Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)\u2014whose performance naturally improves with additional test-time computation (TTC)\u2014standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as TTC increases.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have recently emerged as a powerful approach to planning, enabling the generation of complex trajectories by modeling trajectory distributions using large-scale offline data (Janner et al., 2022; Ajay et al., 2022; Zhou et al., 2024; Chen et al., 2024a;c;b). Unlike traditional autoregressive planning methods, diffusion-based planners, such as Diffuser (Janner et al., 2022), generate entire trajectories holistically through a series of denoising steps, eliminating the need for a forward dynamics model. This approach effectively addresses key limitations of forward models, such as poor long-term dependency modeling and error accumulation (Hafner et al., 2022; Hamed et al., 2024), making it particularly well-suited for planning tasks with long horizons or sparse rewards.\nDespite their strengths, it remains uncertain how diffusion-based planners can effectively enhance planning accuracy by utilizing additional test-time compute (TTC)\u2014a property referred to as TTC scalability. One potential approach is to increase the number of denoising steps or, alternatively, draw additional samples (Zhou et al., 2024). However, it is known that performance gains from increasing denoising steps plateau quickly (Karras et al., 2022; Song et al., 2020a;b), and independent random searches with multiple samples are highly inefficient as they fail to leverage information from other samples. Moreover, how to effectively manage the exploration-exploitation tradeoff within this framework also remains unclear.\nIn contrast, Monte Carlo Tree Search (MCTS) (Coulom, 2006), a widely adopted planning method, demonstrates robust TTC scalability. By leveraging iterative simulations, MCTS refines decisions and adapts based on exploratory feedback, making it highly effective in improving planning accuracy as more computation is allocated. This capability has established MCTS as a cornerstone in many System 2 reasoning tasks, such as mathematical problem-solving (Guan et al., 2025; Zhang et al., 2024a) and program synthesis (Brandfonbrener et al., 2024). However, unlike diffusion-based planners, traditional MCTS relies on a forward model for tree rollouts, inheriting its limitations including losing global consistency. In addition to being restricted to discrete action spaces, the resulting search tree can grow excessively large in both depth and width. This leads to significant computational demands, particularly in scenarios involving long horizons and large action spaces.\nThis raises a crucial question: how can we combine the strengths of Diffuser and MCTS to overcome their limitations and enhance the TTC scalability of diffusion-based planning? To address this, we propose Monte Carlo Tree Diffusion (MCTD), a framework that integrates diffusion-based trajectory generation with the iterative search capabilities of MCTS for more efficient and scalable planning.\nMCTD builds on three key innovations. First, it restructures denoising into a tree-based rollout process, enabling semi-autoregressive causal planning while maintaining trajectory coherence. Second, it introduces guidance levels as meta-actions to dynamically balance exploration and exploitation, ensuring adaptive and scalable trajectory refinement within"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Diffuser: Diffusion Models for Planning", "content": "Diffuser (Janner et al., 2022) addresses long-horizon decision-making by treating entire trajectories as a matrix\nX =\\begin{bmatrix}\ns_0 & s_1 & ... & s_T \\\\\na_0 & a_1 & ... & a_T\n\\end{bmatrix}\nwhere st and at denote the state and action at time t, respectively. A diffusion process is then trained to iteratively remove noise from samples of x, ultimately producing coherent trajectories. In practice, this corresponds to reversing a forward noise-injection procedure by learning a denoiser $p_\\theta(x)$ over the trajectory space. Since $p_\\theta$ by itself does not encode reward or other task objectives, Diffuser optionally incorporates a heuristic or learned guidance function $J_\\theta(x)$. This function predicts the return or value of a partially de-noised trajectory, thereby biasing the sampling distribution:\n$p_\\theta(x) \\propto p_\\theta(x) \\exp(J_\\theta(x)).$                                                                                                                                     (1)\nAccordingly, at each denoising step, gradient information from $J_\\theta$ nudges the model toward trajectories that appear both feasible (as learned from the offline data) and promising with respect to returns, in a manner akin to classifier guidance in image diffusion (Dhariwal & Nichol, 2021).\nDiffusion Forcing (Chen et al., 2024a) extends the above framework by allowing tokenization of x. This tokenization allows each token to be denoised at a different noise level, enabling partial denoising of segments where uncertainty is high (e.g., future plans), without requiring a complete transition from full noise to no noise across the entire trajectory. Such token-level control is particularly beneficial in domains that demand causal consistency, such as long-horizon planning problems."}, {"title": "2.2. Monte Carlo Tree Search", "content": "Monte Carlo Tree Search (MCTS) is a planning algorithm that combines tree search with stochastic simulations to effectively balance exploration and exploitation (Coulom, 2006). Typically, MCTS proceeds in four stages: selection, expansion, simulation, and backpropagation. During selection, the algorithm starts at a root node corresponding to the current state (or partial plan) and descends the tree according to a policy such as Upper Confidence Bounds for Trees (UCT) (Kocsis & Szepesv\u00e1ri, 2006), aiming to balancely choose the most promising child at each step or explore unexperienced states. Once a leaf or expandable node is reached, expansion adds new child nodes representing previously unexplored actions or plans.\nFollowing expansion, simulation estimates the value of a newly added node by sampling a sequence of actions until reaching a terminal condition or a predefined rollout depth. The resulting outcome (e.g., cumulative reward in a Markov Decision Process) then updates the node's estimated value through backpropagation, propagating this information to all ancestor nodes in the tree. Over multiple iterations, MCTS prunes unpromising branches and refines its value estimates for promising subtrees, guiding the search toward more effective solutions."}, {"title": "3. Monte Carlo Tree Diffusion", "content": "In this section, we first outline the key concepts that enable MCTD planning: (1) Denoising as Tree-Rollout, (2) Guidance Levels as Meta-Actions, and (3) Jumpy Denoising as Simulation. These innovations form the foundation of MCTD, bridging the gap between traditional tree search methods and diffusion-based planning. We then describe the MCTD process in terms of the four steps: Selection, Expansion, Simulation, and Backpropagation."}, {"title": "3.1. Denoising as Tree-Rollout", "content": "Traditional MCTS operates on individual states, leading to deep search trees and significant scalability challenges. Because each node in the tree represents a single state, the depth of the tree increases linearly with the planning horizon, resulting in an exponential growth of the search space. Furthermore, it lacks the holistic perspective of the entire trajectory that diffusion-based planners inherently provide. On the other hand, Diffuser does not offer the tree-like structure necessary for intermediate decision-point searches that effectively balance exploitation and exploration.\nTo address this issue, we first introduce the Denoising as Tree-Rollout process by leveraging the semi-autoregressive denoising process (Chen et al., 2024a). Specifically, we partition the full trajectory x = (x1,x2,...,xN) into S subplans, x = (X1,X2, ..., XS) such that x = ()."}, {"title": "3.2. Guidance Levels as Meta-Actions", "content": "In MCTS, constructing and searching the tree becomes computationally expensive in large action spaces and is fundamentally impractical for continuous action spaces. To"}, {"title": "3.3. Jumpy Denoising as Fast Simulation", "content": "In MCTS, evaluating a node far from a leaf node, where evaluation of the plan is feasible, is a critical requirement. This is typically addressed in one of two ways: using a fast forward dynamics model to roll out trajectories to the leaf node, which is computationally expensive, or approximating the node's value via bootstrapping, offering faster but less accurate results. However, how to effectively incorporate these simulation strategies into the diffuser framework remains an open question.\nIn MCTD, we implement this simulation functionality using the fast jumpy denoising process based on DDIM (Song et al., 2020a). Specifically, when the tree-rollout denoising process has progressed up to the s-th subplan, the remaining steps are denoised quickly by skipping every C step:\n$x_{s+1:S} \\sim p(x_{s+1:S} | X_{1:s}, g).$                                                                                                                                                (4)\nThis produces a full trajectory x = (x1:s, xs+1:s), which is then evaluated using the reward function r(x). While this fast denoising process may introduce larger approximation errors, it is highly computationally efficient, making it well-suited for the simulation step in MCTD."}, {"title": "3.4. The Four Steps of an MCTD Round", "content": "Building on the description above, we detail how the four traditional steps of MCTS\u2014Selection, Expansion, Simulation, and Backpropagation\u2014are adapted and implemented in MCTD. This process is illustrated in Figure 1.\nSelection. In MCTD, the selection process involves traversing the tree from the root node to a leaf or partially expanded node. At each step, a child node is chosen based on a selection criterion such as Upper Confidence Bound (UCB). Importantly, this step does not require computationally expensive denoising; it simply traverses the existing tree structure. Unlike traditional MCTS, MCTD nodes correspond to temporally extended states, enabling higher-level reasoning and reducing tree depth, which improves scalability. The guidance schedule g is dynamically adjusted during this step by UCB to balance exploration (NO_GUIDE) and exploitation (GUIDE).\nExpansion. Once a leaf or partially expanded node is selected, the expansion step generates new child nodes by"}, {"title": "4. Related Works", "content": "Diffusion models (Sohl-Dickstein et al., 2015) have recently shown significant promise for long-horizon trajectory planning, particularly in settings with sparse rewards, by learning to generate entire sequences rather than stepwise actions (Janner et al., 2022; Chen et al., 2024a; Ajay et al., 2022; Zhou et al., 2024; Chen et al., 2024c;b; Chen et al.). Various enhancements have been proposed to extend their capabilities. For instance, Chen et al.(2024a) incorporate causal noise scheduling for semi-autoregressive planning, and other works introduce hierarchical structures (Chen et al., 2024c;b; Chen et al.), low-level value learning policies (Chen et al., 2024b), or classifier-free guidance (Ajay et al., 2022; Zhou et al., 2024). Despite these developments, the explicit interplay between exploration and exploitation in diffusion sampling has received relatively little attention.\nChen et al. (2024a) further propose Monte Carlo Guidance (MCG) to leverage multiple sub-plans and average their guidance signals, thereby biasing denoising toward higher-reward outcomes. While this can encourage planning toward an expected reward over multiple rollouts, it does not implement an explicit search mechanism. The detailed discussion is in Appendix C. Similarly, Anonymous (2024) apply a discrete diffusion denoising process to tasks such as chess, implicitly modeling MCTS without explicit tree expansion.\nMCTS (Coulom, 2006) has achieved impressive results across various decision-making problems, particularly when combined with learned policies or value networks (Silver et al., 2016; Schrittwieser et al., 2020). It has also been applied to System 2 reasoning in Large Language Models (LLMs) to enhance structured problem-solving (Xiang et al., 2025; Yao et al., 2024; Zhang et al., 2024b). However, to the best of our knowledge, this work is the first to integrate tree search with diffusion models for full trajectory generation, bridging structured search with generative planning."}, {"title": "5. Experiments", "content": "We evaluate the proposed approach, MCTD, on a suite of tasks from the Offline Goal-conditioned RL Benchmark (OGBench) (Park et al., 2024), which spans diverse domains such as maze navigation with multiple robot morphologies (e.g., point-mass or ant) and robot-arm manipulation. Our chosen tasks\u2014point and antmaze navigation, multi-cube manipulation, and a newly introduced visual pointmaze\u2014jointly assess a planner's ability to handle long-"}, {"title": "5.1. Baselines", "content": "We compare MCTD against several baselines, each employing a distinct strategy for test-time computation. First, we include the standard single-shot Diffuser (Janner et al., 2022), which plans only at the start of an episode and executes that plan without further adjustments. Next, we consider two methods that allocate additional test-time compute differently. Diffuser-Replanning periodically replans at fixed intervals, allowing partial corrections during the episode. This enables us to gauge the benefits of iterative replanning without a full tree search. Diffuser-Random Search (Zhou et al., 2024) generates multiple random trajectories in parallel and selects the highest-scoring one according to the same reward used by MCTD. While this increases test-time sampling and applies multiple guidance levels for different samples, it lacks the systematic expansion and pruning of a tree-search algorithm. We also compare our method to Diffusion Forcing (Chen et al., 2024a), which introduces a causal denoising schedule that enables semi-autoregressive trajectory generation. This comparison helps isolate the benefits of semi-autoregressive denoising from our full tree-structured approach. Further implementation details about all baselines can be found in Appendix A.1."}, {"title": "5.2. Maze Navigation with Point-Mass and Ant Robots", "content": "We begin with the pointmaze and antmaze tasks from OGBench, featuring mazes of varying sizes (medium, large, and giant). The agent is rewarded for reaching a designated goal region, and the relevant dataset (navigate) comprises long-horizon trajectories that challenge each method's capacity for exploration. As in prior work (Janner et al., 2022), we employ a heuristic controller for the pointmaze environment, whereas the antmaze environment uses a value-learning policy (Chen et al., 2024b) for low-level control. Appendix A.5 provides more details on this task evaluation.\nResults. Table 1 presents success rates across medium, large, and giant mazes for both point-mass and ant robots. We can see that MCTD consistently surpasses other methods by a large margin. Notably, Diffuser-Random Search, despite using roughly the same number of denoising steps as MCTD, demonstrates no improvement over one-shot Diffuser, underscoring the importance of tree-based systematic branching over random sampling. Diffuser-Replanning similarly fails to outperform the base Diffuser, whereas Diffusion Forcing exhibits higher success rates in larger mazes, indicating benefits from a semi-autoregressive"}, {"title": "5.3. Robot Arm Cube Manipulation", "content": "For multi-cube manipulation tasks from OGBench, a robot arm must move one to four cubes to specific table locations as shown in Figure 4a. Increasing the number of cubes grows both the planning horizon and the combinatorial complexity. While the planner (MCTD or a baseline) issues high-"}, {"title": "5.4. Visual Pointmaze", "content": "To evaluate our method in image-based planning, particularly under partial observability, we introduce a visual pointmaze environment where the agent receives top-down pixel"}, {"title": "5.5. Test-Time Compute Scalability & Time Complexity", "content": "We examine how each planner leverages additional test-time computation by varying the maximum number of denoising steps and measuring both success rate and runtime on the large and giant pointmaze tasks (Figure 6). As the TTC budget grows, MCTD consistently achieves high success rates\u2014ultimately nearing perfection in giant mazes. In contrast, simply increasing denoising steps (Diffuser) or drawing more random samples (Diffuser-Random Search) yields modest returns, suggesting limited test-time scalability.\nMCTD incurs additional overhead from maintaining and expanding a tree\u2014leading to a larger runtime as the budget grows. For instance, Diffuser-Random Search shows moderate runtime growth because multiple samples can be denoised in parallel batches, which is not directly feasible in MCTD's tree-structured approach. Nonetheless, MCTD can terminate early whenever a successful plan is found, thereby limiting unnecessary expansions. As a result, on the large maze, MCTD's runtime quickly saturates once the task can be solved with a smaller search depth, rather than fully consuming the available budget. A similar pattern emerges in the giant maze: as success rates increase under higher budgets, the runtime growth for MCTD remains sub-linear, indicating that many expansions terminate early once suitable trajectories are located."}, {"title": "6. Limitations & Discussion", "content": "While MCTD enhances test-time compute scalability by integrating search-based planning with diffusion models, it remains computationally expensive due to its System 2-style deliberate reasoning. Determining when to engage in structured planning versus relying on fast System 1 (model-free) planning is an open challenge, as expensive tree search may not always be necessary. Adaptive compute allocation based on task complexity or uncertainty could improve efficiency.\nAnother limitation is inefficiency in large-scale search spaces, where evaluating multiple trajectory hypotheses remains computationally demanding despite using low-dimensional meta-actions. A promising direction is amortizing search learning, where the system meta-learns from test-time search to improve exploration efficiency over time. Instead of treating initial exploration as random, MCTD could incorporate test-time learning mechanisms to refine its search dynamically. Additionally, self-supervised reward shaping could improve trajectory evaluation in sparse-reward settings. Optimizing parallelized denoising, differentiable tree search, and model-based rollouts could further enhance efficiency."}, {"title": "7. Conclusion", "content": "We introduced Monte Carlo Tree Diffusion (MCTD), a framework designed to combine the best of both worlds: the structured search of Monte Carlo Tree Search and the generative flexibility of diffusion planning to enhance the test-time compute scalability of System 2 planning. MCTD leverages meta-actions for adaptive exploration-exploitation, tree-structured denoising for efficient diffusion-based expansion, and fast jumpy denoising for rapid simulation. Experimental results demonstrate that MCTD outperforms existing approaches in various planning tasks, achieving superior scalability and solution quality. Future work will explore adaptive compute allocation, learning-based meta-action selection, and reward shaping to further enhance performance, paving the way for more scalable and flexible System 2 planning."}, {"title": "A. Experiment Details", "content": "We compare MCTD against several baselines, each highlighting different ways to leverage test-time computing on diffusion models or broader planning strategies:"}, {"title": "A.1. Baselines", "content": "\u2022 Diffuser (Janner et al., 2022): We use the standard single-shot Diffuser as our primary diffusion baseline. It generates a full trajectory by denoising from maximum to zero noise in one pass, guided by a return-predicting function J. While it can produce coherent plans, it lacks any mechanism to adapt or refine these plans at test time.\n\u2022 Diffuser-Replanning: To partially address the lack of iterative search, we evaluated a replanning variant of Diffuser. After an initial denoised plan is sampled, the policy is periodically re-invoked at predefined intervals (e.g., every 10 time steps), using the agent's new state as the \"start\" for a fresh diffusion process. This strategy leverages additional test-time compute by attempting incremental corrections, but does not share information across different replans.\n\u2022 Diffuser-Random Search: We consider a random-search variant of Diffuser that generates multiple trajectories from the randomly noised plans and picks the best candidate according to a heuristic or learned value function. This approach can be seen as a \u201cSample-Score-Rank\u201d method proposed in (Zhou et al., 2024), increasing test-time compute by drawing more samples, but lacking the systematic exploration or adaptation found in tree-search paradigms.\n\u2022 Diffusion Forcing: Diffusion Forcing (Chen et al., 2024a) extends Diffuser by introducing a tokenized, causal noise schedule. Subplans of the trajectory can be denoised at different rates, allowing partial re-sampling of specific time segments without re-optimizing the entire plan. In our experiments, we evaluate Diffusion Forcing with the Transformer backbone provided in their official repository, which performed better in our preliminary tests. This method helps isolate the benefit of tokenized partial denoising from our tree-structured approach. We note that Diffusion Forcing baselines are basically designed to do replanning, so we follow the setting in this evaluation."}, {"title": "A.2. Heuristic Controller, Value-Learning Policy and Inverse Dynamics Model", "content": "A long-standing challenge in diffusion-based planning is the difficulty of preserving global trajectory coherence while managing local, high-dimensional state-action control (Chen et al., 2024a;b). For example, PlanDQ (Chen et al., 2024b) couples a high-level diffusion planner with a learned low-level policy. Similarly, our approach focuses the diffusion planner on lower-dimensional, representative state information (e.g., object or agent positions), while delegating detailed action inference to the hueristic controller or value-learning policy or inverse dynamics model. The details of this integration is discussed in Algorithm 10. Such a hierarchical design allows MCTD and baseline planners to concentrate on broader strategic decisions while leaving the finer-grained execution details to the heuristic controller or value-learning policy or inverse dynamics model."}, {"title": "A.3. Model Hyperparameters", "content": "For reproducibility, we provide details on the hyperparameters used in our experiments. These settings were chosen based on prior work and empirical tuning to ensure stable training and evaluation across all tasks. The almost same hyperparameters were applied consistently across tasks, except in cases where task-specific configurations are required. They will be discussed in each task section."}, {"title": "A.3.1. DIFFUSER", "content": "We build our tested Diffuser on the official implementation, which is available https://github.com/jannerm/ diffuser. While keeping the core architecture and training procedure intact, we made the following modifications to align with our experimental setup:\n\u2022 Guidance Function: For pointmaze tasks, Diffuser originally applied the goal-inpainting guidance, but in our evaluation, we employed a distance-based reward function to ensure fair comparison across baselines.\n\u2022 Replanning Variant: To evaluate the impact of test-time iterative refinement, we implemented a replanning strategy that periodically re-samples trajectories based on the agent's updated state."}, {"title": "A.3.2. DIFFUSION FORCING", "content": "We build our tested Diffusion Forcing upon the officially released code from https://github.com/buoyancy99/ diffusion-forcing. The following modifications were made:\n\u2022 Replanning Frequency: Since Diffusion Forcing inherently supports test-time replanning, we evaluated it under comparable conditions by applying periodic plan refinement, similar to Diffuser-Replanning.\n\u2022 Transformer Backbone: To maintain consistency across models, we utilized the Transformer-based backbone provided in the Diffusion Forcing repository, rather than alternative architectures."}, {"title": "A.3.3. \u039c\u039f\u039dTE CARLO TREE DIFFUSION (MCTD)", "content": "MCTD follows Diffusion Forcing hyperparameters, but some of them are modified for MCTD and tree search related hyperparameters are added."}, {"title": "A.3.4. VALUE-LEARNING POLICY", "content": "For some tasks, to handle complex action space, we applied a value-learning policy by modifying the PlanDQ (Chen et al., 2024b) source code, https://github.com/changchencc/plandq."}, {"title": "A.4. The Evaluation Details", "content": "We followed the OGBench (Park et al., 2024) task configurations, 5 tasks per each environment. For each task, we evaluated 10 random seeds per each model, and reported the averaged success rates and their standard deviations."}, {"title": "A.5. Maze Navigation with Point-Mass and Ant Robots", "content": ""}, {"title": "A.5.1. MODIFICATIONS FROM OGBENCH", "content": "Although the original OGBench datasets include random noise in start and goal positions, we remove this noise to isolate model performance from environmental randomness. This modification facilitates clearer comparisons across methods and ensure that differences in performance can be attributed to planning capabilities rather than uncontrolled stochasticity."}, {"title": "A.5.2. GUIDANCE FUNCTION", "content": "To plan long-horizon while sustaining the plan effectiveness to reach the goal as fast as possible, we applied the guidance function to reduce the distances between every state and goal, $ \\Sigma_{i} ||x_{i} - g||_{2}$ (Chen et al., 2024a). We note that, to do fair comparison we applied the same guidance style over every baseline. For example, Diffuser originally used the goal-inpainting guidance for pointmaze, but in this evaluation, we used the distance guidance."}, {"title": "A.5.3. GUIDANCE SET", "content": "We applied different guidance sets to MCTD for the different tasks. For pointmaze medium and large, [0, 0.1, 0.5, 1, 2] is applied, and for pointmaze giant, [0.5, 1, 2, 3, 4] is applied. For antmaze tasks, [0, 1, 2, 3, 4, 5] is set."}, {"title": "A.5.4. PLANNING HORIZON", "content": "For medium and large mazes, we applied 500 planning horizons and 1000 for giant to train the models with enough data. Medium and large navigate dataset trajectory horizons are 1000 and 2000 for giant. To make more data through sliding window technique, we applied smaller planning horizon than the dataset horizon."}, {"title": "A.5.5. HEURISTIC CONTROLLER AND VALUE-LEARNING POLICY", "content": "For pointmaze, we take the heuristic controller designed in (Janner et al., 2022). For antmaze, we use the value-learning policy (Wang et al., 2022) as done in (Chen et al., 2024b). However, we note that different from (Chen et al., 2024b), we only give the 10 steps further planned position information as the subgoal for that."}, {"title": "A.5.6. MCTD REWARD FUNCTION", "content": "For the reward function, we designed heuristic rules. One of those is to check whether there exists too large position difference between near states, because it is impossible physically. Another rule is to give a reward when reaching the goal. Earlier reaching can get the larger reward, $ r = (H - t)/H$, where H and t are the horizon length and current step."}, {"title": "A.5.7. SHORT-HORIZON (STITCH) RESULTS", "content": "To examine performance when only short-horizon trajectories are given in the training phase, we evaluate on the stitch datasets, summarized in Table 8. The trajectory horizon in stitch datasets is much shorter than the required steps to reach the goal (e.g., the horizon in the dataset is 100 while over 200 steps are required in medium map tasks), we applied replanning on Diffuser, Diffusion Forcing, and MCTD. Different from the long-horizon dataset given cases, the Diffuser shows better performances than Diffusion Forcing, it can be seen that the implicit stitching ability of the Diffuser is better than Diffusion Forcing. On the other hand, MCTD shows better performance than baselines in the medium-sized map, due to multiple samplings with exploration, but the search length could not be long due to the limited length of trajectories in the training dataset, so it does not show good performance in large map."}, {"title": "\u0410.5.8. \u0421\u043eMPARISON WITH GOAL-INPAINTING DIFFUSER", "content": "Diffuser (Janner et al., 2022) typically employs a learned guidance function to bias generated trajectories toward high-return outcomes. For pointmaze tasks, however, an alternative approach\u2014goal-inpainting guidance\u2014has also been introduced (Janner et al., 2022), which further improves single-pass planning by \"imagining\u201d an intermediate trajectory whose final state is the designated goal. In the context of Diffuser-Replanning, we adapt goal-inpainting by providing the experienced trajectory as contextual information rather than a direct target for denoising."}, {"title": "A.6. Robot Arm Cube Manipulation", "content": ""}, {"title": "A.6.1. OBJECT-WISE GUIDANCE", "content": "Because the robot arm can only move a single cube at any given time, we adopt an object-wise guidance mechanism in MCTD. Rather than applying a single holistic diffusion schedule to all cubes simultaneously, MCTD treats the selection of which cube to move next as a meta-action. This design partially alleviates issues that arise when a single plan attempts simultaneous motion of multiple cubes, though multi-object manipulation still demands careful sequencing to avoid suboptimal interleavings."}, {"title": "A.6.2. GUIDANCE FUNCTION", "content": "The guidance function itself is same to that for maze tasks, Appendix A.5.2, but the object-specific guidance is applied to MCTD and MCTD-Replanning. The guidance tries to reduce the distances between specific object states and the object's goal only."}, {"title": "A.6.3. GUIDANCE SET", "content": "For cube tasks, the guidance set [1, 2, 4] for each object is applied. For example, for two cube tasks, the size of the guidance set is 6 with three guidance set for each object."}, {"title": "A.6.4. REWARD FUNCTION FOR ROBOT ARM CUBE MANIPULATION", "content": "In addition to the reward function for the maze environment, we added some rules not to generate unrealistic plans. One of them is not to move multiple objects simultaneously. The small noise on planning is okay, but if multiple objects move adequately, the reward function decides the plan is unrealistic and gives a 0 reward. Other rules are, that the final state of the"}, {"title": "A.6.5. PLANNING HORIZON", "content": "For single cube tasks, the planning horizon is 200 same to the episode length, and 500 for others to train the models with enough data."}, {"title": "A.6.6. VALUE-LEARNING POLICY", "content": "Similar to antmaze tasks, we applied value-learning policy (Wang et al., 2022) by achieving the subgoals suggested by planners for 10 steps further planned states."}, {"title": "A.7. Visual Pointmaze", "content": ""}, {"title": "A.7.1. SETUP", "content": "Because diffusion-based planners typically operate on state-action representations, we first learn an image encoder via a Variational Autoencoder (VAE) (Kingma & Welling, 2013). The resulting latent states serve as inputs to the diffusion models. Notably, these latent encodings do not include explicit positional information, so the planner must infer underlying geometry and navigate the maze indirectly. We design an inverse dynamics model (a small network of linear layers) to convert consecutive latent observations into actions: $ \\hat{a_t}$ = InvDyna((xt\u22121, xt), xt+1). This replaces the heuristic controller of the original Diffuser, which cannot be directly applied here due to partial observability."}, {"title": "A.7.2. GUIDANCE", "content": "To guide the diffusion process, we introduce a position estimator trained on ground-truth coordinates. Although this estimator provides an approximate distance to the goal, the image encoder is not updated from this learning signal and it does not supply the planner with the full underlying state. Therefore, the image encoder still sees only the pixel-level data without direct positional labels, thereby preserving partial observability in the core planning procedure. This design choice highlights an open-ended question of how best to incorporate task-relevant signals in visual domains. The same guidance function with maze task, Appendix A.5.2 is applied."}, {"title": "A.7.3. GUIDANCE SET", "content": "For this task, we applied [0, 0.1, 0.5, 1, 2] guidance set for MCTD and MCTD-Replanning."}, {"title": "A.7.4. DATASET GENERATION AND PLANNING HORIZON", "content": "We follow the OGBench (Park et al., 2024) dataset generation script for PointMaze, with trajectory horizons of 1000 for Medium and Large mazes. To enhance training, we use reduced planning horizons (500 for Medium/Large) and apply a sliding-window technique, as in our earlier pointmaze setup."}]}