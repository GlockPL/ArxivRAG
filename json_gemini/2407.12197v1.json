{"title": "Towards Interpretable Visuo-Tactile Predictive Models for Soft Robot Interactions", "authors": ["Enrico Donato", "Thomas George Thuruthel", "Egidio Falotico"], "abstract": "Abstract\u2014Autonomous systems face the intricate challenge of navigating unpredictable environments and interacting with ex-ternal objects. The successful integration of robotic agents into real-world situations hinges on their perception capabilities, which involve amalgamating world models and predictive skills. Effective perception models build upon the fusion of various sensory modalities to probe the surroundings. Deep learning applied to raw sensory modalities offers a viable option. However, learning-based perceptive representations become difficult to interpret. This challenge is particularly pronounced in soft robots, where the compliance of structures and materials makes prediction even harder. Our work addresses this complexity by harnessing a generative model to construct a multi-modal perception model for soft robots and to leverage proprioceptive and visual information to anticipate and interpret contact interactions with external objects. A suite of tools to interpret the perception model is furnished, shedding light on the fusion and prediction processes across multiple sensory inputs after the learning phase. We will delve into the outlooks of the perception model and its implications for control purposes.", "sections": [{"title": "I. INTRODUCTION", "content": "Deploying autonomous systems in the real world requires a thoughtful consideration of robots as embodied agents. Their physical form plays a crucial role in shaping dynamic interactions with the external environment [1]. Most im-portantly, this includes how the agent perceives the world and constructs a model upon which goal-based decision-making will be predicated. In particular, soft robotics arises as a challenging domain when building perception models, because of robot structural and material compliance making sensory prediction hard to refine [2], [3].\nBiological perception frequently relies on the integration of multiple modalities, enabling the acquisition of a com-prehensive set of features related to the same perceived event. This integration serves to disambiguate informa-tion and facilitate cross-inference among different sensory modalities [4]. Such biological perspective can be translated to robotic agents, whose sensing capabilities should both provide feedback about the control actions outcome and eventual interactions with the external environment. It does not produce a model of itself as an isolated body that has to deal with interactions as disturbances, but takes advantage of interactions to produce a world model in which the robot is aware of its surroundings [5]. Multi-modality is not the only challenge while dealing with perception, but sensory distribution plays a fundamental role, especially in the pres-ence of continuously deformable bodies, whose relative parts deformation cannot be inferred by only localized sensing [6].\nAddressing the complexities of multi-modal distributed perception presents several challenges, encompassing issues like encoding, sensory coherence, confidence across modal-ities, and the intricate process of fusion [7]. The high-dimension data poses a formidable obstacle, potentially re-stricting the efficacy of data-driven algorithms in discerning tangled patterns and understanding input-output dependen-cies. A well-defined methodology might open avenues for more effective utilization of diverse sensory inputs and facilitate the creation of sophisticated models capable of comprehending their interplay. The use of such models in learning-based controllers of soft robots [8] will unveil a plethora of novel applications.\nIn our previous work [9], we have explored generative models to build a perception representation for soft robot interactions, as shown in Fig. 1. In particular, a Variational AutoEncoder (VAE) [10] is used to manage the multi-sensory encoding and fusion, and at the same time leverage its predictive capabilities to build a perception model, that aims to reconstruct sensory information via cross-inference. Such mapping is enabled by the projection of the multi-modal sensory information into a latent space, an organized, rich,"}, {"title": "II. RELATED WORKS", "content": "Diverse information from various sensory sources in-troduces the challenge of effectively summarizing multi-modal data by leveraging their complementary and redundant aspects [20]. This also raises issues regarding handling disparate levels of noise and addressing missing data. The ob-jective of multi-modal representation learning is to discover a unified representation that conforms to the requirements of being a robust representation, as outlined by Bengio et al. [21] and Srivastava et al. [22]. Within this context, the VAE seeks to generate a representation of the multi-modal input in a latent space, utilizing it for generative purposes by projecting it back into the multi-modal domain.\nThe efficacy of generative models in this domain necessi-tates an exploration of how they instil confidence, facilitate decision-making, establish a foundation for assessment, or intrinsic purposes [23]. Posthoc techniques, including visual-izations, have been devised to interpret the outputs of gener-ative models based on deep learning. These techniques have been widely applied across various input modalities such as images, natural language, and domain-specific languages, as well as tabular data [24], [25]. Furthermore, efforts have been made towards developing disentangled representations that establish mappings between high-dimensional inputs and low-dimensional representations. The objective is to align representation dimensions with the underlying factors that generated the data [26]. Additionally, such explanation often involves examining the variation in each dimension of the representation [27]. While alternative solutions exist, they often lack insights into the individual meanings of dimensions and are typically specific to the encoding stage rather than the generative phase [28]. Although interactive methods have been devised to assist users in comprehending the data's geometry and understanding what information the representation retains, these methods fall short of explaining the dimensions of variation themselves."}, {"title": "III. WORLD MODEL GENERATION", "content": "In this section, we elucidate the process of data collection while a soft passive finger interacts with movable objects in simulation and multi-modal information is stored. Data are then used to train the predictive model, to build a world model that the robot might use for further decision-making or control purposes.\nThe finger in Fig. 2 is implemented in the SoMo simulator [29] and comprises 20 links and joints, with the same spring constant and mass. Joints are arranged alternately from the base to the tip, allowing the finger to flex forward/backwards and laterally. This soft passive finger is attached to the distal end of a cylindrical rigid robot, featuring a rotary joint $q_1$ at the base and prismatic joints $q_2$ and $q_3$ connecting its links. This design provides a cylindrical workspace achieved through rotation and vertical and sliding motions. Box-shaped objects are introduced into the simulation, randomly placed near the robot, allowing the finger to make extra contact with its surroundings.\nThe simulations start with a predetermined robot config-uration, and random actuations are generated over defined time steps to cover the entire workspace. The simulation operates at 1 KHz to mitigate numerical instabilities. Data modalities are stored, including proprioceptive, contact, and visual information. The data acquisition frequency is set to 10 Hz, ensuring quasi-static motion. Proprioceptive data includes finger joint angles $q_f = q_{fe,t}, q_{aa,t} \\in R^{20 \\times 1}$ and rigid arm joint angles $q_f \\in R^{3 \\times 1}$, to capture the finger's shape and arm configuration.\nDuring interactions, SoMo monitors local deformations with normal forces $f_t \\in R^{20 \\times 1}$ on each finger link. Global deformations are captured visually through $v_t$ with shape $64 \\times 64 \\times 3$, obtained by recording the simulation with a virtual camera, offering a broader perspective on the system's deformations and interactions.\nOur learning architecture aims to seamlessly integrate information from diverse sensory modalities, leveraging the dynamic evolution of the physical body. In [9], we employed a Conditional VAE (CVAE) [30] to predict the next state $S_{t+1}$ in self-supervision at each time-step $t$ from non-minimal, multi-modal sensory data $S_t$ and robot action $a_t$.\nIn this study, we introduce an updated version of the generative model, accounting for the diversity of sensory data through modality-specific encoding and decoding layers, as illustrated in Fig. 3. Each sensory input is processed by its encoder, generating a modality-specific model and mapping the information into a dedicated one-modality latent space. In particular, a Convolutional Neural Network (CNN) is used for encoding/decoding of visual information, to consider also spatial features; conversely, two Multi Layer Perceptron (MLP) are employed for both proprioception and force mapping."}, {"title": "IV. INTERPRET THE SENSORY REPRESENTATION", "content": "This section explores a suite of tools designed for inter-preting generative models. The methodology employed ad-dresses the problem through two distinct approaches. First, it assesses the encoding and fusion stages by visualizing the or-ganizational structure of the encoded and conditioned latent spaces concerning input modalities. Second, it scrutinizes the generative capabilities of the network by manipulating either the encoded representation or the selected action arbitrarily.\nLatent space denotes the learned, lower-dimensional space where a model encodes high-dimensional input data into a more concise and abstract representation. However, the chal-lenge lies in gaining insights into the features of this high-dimensional latent space. One approach involves analyzing features individually or identifying evident patterns among sampled data. In the latter case, latent space visualization proves valuable in understanding how the model captures and organizes information, particularly aiding in data clustering and anomaly detection. This visualization maps the repre-sentation into a very low-dimensional embedding, typically two dimensions, making it easily interpretable.\nPrincipal Component Analysis (PCA) [31] identifies prin-cipal components that capture maximum variance in the data for linear dimensionality reduction, offering valuable insights into the overall structure of the latent space. How-ever, identifying linear dependencies in such high-variable spaces is uncommon. In this context, t-Distributed Stochastic Neighbor Embedding (t-SNE) [28] prioritizes preserving pairwise similarities between data points, making it well-suited for revealing the local structure of the latent space. It is worth noting that t-SNE excels in preserving local structures, but it may not always accurately represent global structures. Anyway, data clusters often signify groups of similar instances or patterns within the data, shedding light on the distribution of points and providing insights into how the model captures features and variations.\nThe t-SNE algorithm includes a tuning parameter known as perplexity, which serves as a balance between local and global features. This parameter influences the effective number of neighbours that each point considers during dimensionality reduction. Through an iterative process of increasing the perplexity, the optimal value is identified based on achieving the lowest Kullback-Leibler divergence (KLdiv) [32], where a lower value indicates a reduced diver-gence between the latent space and a normal distribution.\nUpon gaining insights into the mapping of input sensory modalities into a shared latent space, the next step involves evaluating the generative capabilities of the network. While the network excels in predicting the acquired dataset op-timally, as evidenced by the reconstruction loss, its true generative potential emerges when presented with synthetic data. In such instances, it adeptly leverages its generative capacities to establish meaningful connections between novel information and pre-existing knowledge.\nAssessing the generative properties of the perception model entails examining how the conditioned latent space is reconstructed during the decoding stage. Treating the decoders as constant mapping blocks, we can manipulate either the encoded latent representation or the action. In the former scenario, introducing a new sensory observation allows us to analyze how the network generalizes across diverse observations while keeping the action constant. Con-versely, in the latter case, maintaining the observation as constant and varying the action enables us to explore the entire actuation space and anticipate all potential future observations resulting from changes in action.\nThis analytical approach also provides insights into the network's stability concerning data distribution. Operating within a variational latent space, where the encoded latent representation is sampled from the learned distribution, as-sessing how different samples from the same distribution yield consistent predictions serves as a measure of stability."}, {"title": "V. RESULTS", "content": "The examination of the generative architecture involves an assessment of its learning performance across various input modalities. It includes gaining insights into the organization of the latent representation in both the encoded and condi-tioned latent space. Furthermore, an analysis of generative capabilities will be conducted to evaluate prediction stability and observe the model's behaviour when presented with synthetic samples as input. Lastly, the consideration of input-to-output dependencies aims to provide an estimation of feature importance in modalities prediction.\nThe perception model is trained on a dataset comprising approximately 40k samples, using a laptop equipped with a Nvidia GeForce RTX 3060 GPU. The training process for the network takes between 20 minutes to 1 hour, depending on the size of the latent space. After training, each sensory prediction iteration averages 78 milliseconds.\nThe assessment of the generative model performance extends to its predictive capabilities across consecutive time steps, as illustrated in Fig. 4. Evaluation is conducted across various modalities, with a focus on force and proprioception depicted in Fig. 4(A). The results align with previous dis-cussions in [9], quantified in terms of Root Mean Square Error (RMSE) between predicted values and the desired outcome. Notably, vision slightly enhances proprioception prediction, yet is reliable without any fusion, and enables a finer force estimation. Furthermore, expanding the latent space improves force prediction by accommodating a greater number of features from visual information, albeit with increased variance.\nIn Fig. 4(B), optical flow predictions are showcased for two random samples from the validation dataset. Despite the suboptimal estimation of colours, the network adeptly identifies the image components responsible for detecting op-tical flow. However, improvements can be made in predicting the soft part, as its pronounced variability in motion poses challenges in accurately estimating changes in its shape.\nFuture deployment on physical robotic devices will then consider the presence of external cameras to track the body evolution and environment dynamics, as well as a proprio-ceptive system to estimate the robot shape over time. Contact sensors will serve as ground truth to measure the divergence between the model's predictions and real-world observations. However, these contact sensors can be removed during actual operational phases. The use of physical sensors introduces typical challenges associated with soft sensors, or to the quality issues encountered with visual information in sub-optimal settings. These sensory challenges will necessitate a pre-processing step before utilizing the perception model.\nVisualizing the latent space offers valuable insights into the underlying data distribution, forming the foundation for predictive modelling. This visualization is facilitated by the t-SNE algorithm, with a perplexity parameter set to 1000. The choice of perplexity is informed by the observed monotonous decrease in divergence during its increase.\nIn Fig.5(A), the encoded latent representation across two dimensions is presented. The analysis explores variable latent space dimensions and diverse input information. Each point in the bi-dimensional plane is color-coded based on the contact force applied to the finger at that specific timestep. When utilizing only proprioception, distinct clusters emerge to force magnitude, with symmetrical patterns apparent, especially around the axis of low forces (under 4 N). The centroid excels at encoding low-magnitude forces, while the outer region encompasses samples with higher applied forces. However, introducing vision as input disrupts this clarity, presenting a seemingly random distribution, partic-ularly as the latent space dimensions increase.\nIn contrast, Fig.5(B) demonstrates the impact of moving information into the conditioned latent space. The represen-tation retains the symmetry property for proprioception-only input while reducing the spread of each cluster. For vision input, a discernible pattern of force distribution emerges, featuring a centroid with low forces that progressively in-crease towards the extremities. This relationship is further elucidated in Fig.5(C) by illustrating the distance between each sample and the distribution centroid concerning the applied force. The conditioned latent space allows for a spatially dependent encoding of force, a capability limited in the encoded latent space. These findings underscore the cross-modal inference capabilities of the perception model, facilitating the construction of a force-encoded representation even without explicit training for such information.\nAn alternative metric to assess improvement upon tran-sitioning to the conditioned latent space is the change in mutual information between the distance from the respective centroid and the applied force. Mutual information serves as a measure of dependence between two random variables, with higher values indicating increased dependence. The variation in mutual information as one traverses the latent space can be likened to an information gain. Examining Tab.I, it becomes evident that the gain is nearly negligible when employing only proprioception, indicating that the transformation in the conditioned latent space maintains a similar level of topological information as its precursor. In contrast, when incorporating vision as input (or output), the information gain experiences a notable increase. This observation underscores how the inclusion of multi-modal information enhances the model's ability to discriminate among events, thereby amplifying its predictive capabilities.\nOur analysis has primarily focused on the predictive capabilities of the perception model. Given its generative properties, it is pertinent to highlight its performance across various input modalities, actions, and when confronted with synthetic data. For brevity and clarity in subsequent dis-cussions, the perception model with 64 latent units exploits proprioceptive and visual information to predict contact.\nInput modalities undergo encoding into a shared varia-tional latent space, with stochastic parameters being learned for latent representation sampling. However, the stochastic nature of this process can influence the network's prediction capabilities. In Fig. 6(A), the impact of repetitive sampling from the same encoded distribution on prediction error is depicted. The likelihood of obtaining a sub-optimal repre-sentation is higher in low-dimensional latent spaces.\nSimilarly, the alteration of conditioning actions is explored for its effect on prediction. Fig. 6(B) illustrates the change in prediction error while using null actions, to not induce any motion, and randomly sampled actions. Null actions have a relatively minor impact as they do not provide additional information to the network, resulting in outputs closely resembling the inputs. Conversely, random actions alter per-formance by leading to unexplored system evolutions.\nVisual representations in optical flow prediction further validate these findings for synthetic latent representations [Fig.6(C)] and actions [Fig. 6(D)]. Despite incorporating white noise into the sample while keeping the other input constant, the network generates new data, offering novel insights in the robotic domain. For instance, assessing the stability of specific actions in future observations by exam-ining changes in representation while holding the action, or exploring diverse system evolutions by maintaining the same representation while altering the action.\nWhen a subset of outputs is fed as input, the perception model enables feedback loops for self-prediction. Fig. 6(E) illustrates this approach across three timesteps, incorporating visual information (combining optical flow with the initial visual state) and proprioception as feedback to predict force dynamics. This iterative feedback mechanism enhances the prediction of the network's future states."}, {"title": "VI. CONCLUSION", "content": "This work introduces a methodology for analyzing and in-terpreting perception models based on generative techniques. The application of multi-modal distributed perception in soft robotic systems has revealed intricate challenges related to sensory encoding, fusion, and state prediction. Expanding on our previous work with generative models, we investigate the mapping of sensory observations to latent representations and explore the dynamic interplay between sensory inputs and subsequent robot actions, delving into sensory prediction.\nWe aim to extend the model application to include actuated soft robots. The overarching objective is to leverage this concise and versatile state representation to develop task-specific control policies that leverage world models."}]}