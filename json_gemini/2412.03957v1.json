{"title": "A Framework For Image Synthesis Using Supervised Contrastive Learning", "authors": ["Yibin Liu", "Jianyu Zhang", "Li Zhang", "Shijian Li", "Gang Pan"], "abstract": "Text-to-image (T2I) generation aims at producing realistic images corresponding to text descriptions. Generative Adversarial Network (GAN) has proven to be successful in this task. Typical T2I GANS are 2-phase methods that first pre-train an inter-modal representation from aligned image-text pairs and then use GAN to train image generator on that basis. However, such representation ignores the inner-modal semantic correspondence, e.g. the images with same label. The semantic label in priory describes the inherent distribution pattern with underlying cross-image relationships, which is supplement to the text description for understanding the full characteristics of image. In this paper, we propose a framework leveraging both inter- and inner-modal correspondence by label guided supervised contrastive learning. We extend the T2I GANs to two parameter-sharing contrast branches in both pre-training and generation phases. This integration effectively clusters the semantically similar image-text pair representations, thereby fostering the generation of higher-quality images. We demonstrate our framework on four novel T21 GANs by both single-object dataset CUB and multi-object dataset COCO, achieving significant improvements in the Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics of image generation evaluation. Notably, on more complex multi-object COCO, our framework improves FID by 30.1%, 27.3%, 16.2% and 17.1% for AttnGAN, DM-GAN, SSA-GAN and GALIP, respectively. We also validate our superiority by comparing with other label guided T2I GANs. The results affirm the effectiveness and competitiveness of our approach in advancing the state-of-the-art GAN for T2I generation.", "sections": [{"title": "Introduction", "content": "Text-to-image (T2I) generation targets on generating realistic images that match the corresponding text description. This captivating task has gained widespread attention and popularity owing to its vast creative potentials in art generation, image manipulation, virtual reality and computer-aided design."}, {"title": "Related Work", "content": ""}, {"title": "Contrastive Learning", "content": "Contrastive learning is a self-supervised method which has been successful in representation learning. It plays a crucial role in serving computer vision tasks and extends influence to other research field like natural language processing. Contrastive learning follows the intuition that similar data samples should be closer in the representation space, while dissimilar samples should be far apart. Typical contrastive learning setting SimCLR [1] augments image into two randomly warped views and extracts their representations through twin encoders. The two branches of representation are then projected to same feature space to apply contrastive loss [13], where the paired view of image is considered as positive sample and vice verca. Other variants of contrastive learning mainly differ in the formulation of negative samples [5], the asymmetric design of twin encoders [4], or contrastive loss definition [29]. All these methods have either comparable results or exceed supervised methods on many representation learning benchmarks [2]. In addition to construct the positive and negative samples by self supervision, researchers [6,12] also utilize image classification labels to formulate single- and multi-label contrastive loss, the former achieves high accuracy in image classification while the latter succeeds in visual reasoning. Contrastive learning has also been explored to bridge the modality gap and create unified representation for multi-modal pre-training. Trained by fine-curated large scale image text pairs, CLIP [15] has demonstrated great zero-shot capability for dozens of visual and image-text downstream tasks.\nThese contrastive learning progresses proves the feasibility of aligning different feature views at low annotation cost. We adopt the intuition that any data representation can be improved by referencing similar semantic concepts from both inter- and inner-modal data, therefore our framework designs multiple ways of feature alignment which will be detailed in Section 3."}, {"title": "GAN for Text-to-Image Generation", "content": "In recent years, image generation has experienced rapid development starting from the remarkable success of Generative Adversarial Network (GAN) which trains a generative model by adversarial discrimination [9,14,22,25,30-32]. Reed et al. [16] were the first to employ GAN to generate images from text descriptions. To synthesize higher resolution images, Zhang et al. propose the Stack-GAN [30] and StackGAN++ [31] employing a multi-generator strategy that first generates a low-resolution image and then finetunes followup generators to produce high resolution realistic images. Many works follow this multi-stage stack structure [14, 17, 25, 28,32] to improve image generation quality. On basis of StackGAN++, AttnGAN [25] introduced attention mechanism to refine the process of generating images from fine-grained textual descriptions at different stages of image generation. In addition, AttnGAN proposed the Deep Attentional Multi-modal Similarity Model (DAMSM) to improve multi-granular consistency between image and text. DM-GAN [32] proposed dynamic memory to store the intermediate generated images and retrieve the most relevant textual information with gated attention to update the image representation accordingly.\nAlthough the multi-stage GAN is designate for high-resolution progressive image generation, its training complexity grows as the stage stacking. To overcome this, DF-GAN [22] proposed single-stage generation, whose generator uses a series of UPBlock specially designed for high resolution feature upsampling. DF-GAN further used Matching-Aware Gradient Penalty and hinge loss to train the UPBlocks. Followup SSA-GAN [9] used a Semantic Spatial Aware Convolution Network (SSACN) block to predict text aware mask maps based on the current generated image features, which facilitates the fusion and consistency between image and text. These conventionally designed single-stage methods greatly reduce the complexity of T2I generation, meanwhile others seek for utilizing famous visual-language pre-training techniques to bridge the inter-modal gap. GALIP [21] directly integrates CLIP [15] to harness the well-aligned image-text representation and extend GAN's ability to synthesize complex images. Hui et al. [27] propose a framework leveraging contrastive learning to enhance the consistency between caption generated images and the originals. All these T2I GANs focus on the inter-modal image text alignment without considering inner-modal association, which in some extent leads to flaws in the generation results. Our framework instead encourages both inter- and inner-modal association."}, {"title": "Method", "content": "In this section, we introduce a simple effective framework which integrates supervised contrastive learning to leverage the inner-modal association, thereby enhancing the generation quality of T2I GANs. Like novel contrastive learning approach, we adopt the dual tower structure and create two symmetric branches of contrast opponents for both pre-training and GAN phases. In pre-training phase, the supervised contrastive learning encourages the representation coherency for image-text pairs sharing same semantics. In favor of the coherent representation, in the GAN phase, the supervised contrastive learning establishes additional guidance for the semantic consistency of the generated images. We detail our framework adaptation and enhanced T2I GAN learning objectives for the two phases in the following respective sections."}, {"title": "Supervised Contrastive Learning for Pre-training", "content": "Typical T2I GANs pre-train the image and text encoders by maximizing the paired image-text representation similarity and the unpaired dissimilarity. To enhance this learning process, we extend the pre-training by supervised contrastive learning on the image-text pair with shared label. The extension has three components shown in Figure 1."}, {"title": "Data Sampling Strategy", "content": "At each training step, we randomly sample a batch of N examples which consist of N captionst, corresponding images x and label set Y. To construct contrastive pair, we ensure that each sample has reference example with the same labels: for each sample $(t_i, x_i, Y_i)$, we select a sample $(t_i, x_i, Y_i)$ as its pair where $Y_i \\cap Y \\neq \\emptyset$."}, {"title": "Image Encoder g And Text Encoder f", "content": "In pre-training phase, the encoder extracted representations usually have multi-granular features to encourage the deep fusion, e.g., the global/local views of image, and the sentence/word level of text. Our methods do not change the functionalities but extend them by applying shared image and text encoders g, f to extract contrastive pair image representations $v = g(x)$, $v' = g(x')$ and text representations $e = f(t),e' = f(t')$. Our framework is indifferent for the type of encoders, where we keep them consistent to the baseline methods our framework applied to. Specifically, for AttnGAN [25], DM-GAN [32] and SSA-GAN [9], we use Inception-v3 [20] as image encoder g and Bi-LSTM [19] as text encoder f. For GALIP [21], we use transformer-based CLIP image and text encoders. The weights of the text encoder and image encoder are frozen during the training phase of the GAN."}, {"title": "Learning Objective", "content": "With the data sampling strategy, we define the objective for training. For image-text matching using Inception-v3 and Bi-LSTM, we consider $(t_i, x_i)$ and $(t_i, x_i)$ as positive image-text pairs to calculate DAMSM loss same as AttnGAN [25]. As for CLIP encoder, we use symmetric cross entropy loss [15]. To apply supervised contrastive loss, we formulate positive pairs from sampling strategy for image-image, image-text and text-text associations. Specifically, $(t_i, t'_i)$, $(t_i, t_j)$ and $(t_i, t'_i)$ are considered as positive text-text pairs where $Y_i \\cap Y_j \\neq \\emptyset$). It is worth noting that in single-object dataset CUB, each corresponding image-text sample only has one label, while in complex multi-object dataset COCO, it has multiple labels. Therefore, we use different supervised contrastive loss functions to deal with different label sharing.\nFor one label scenario, we treat sample pairs with the same label as positive pairs and apply single-label supervised contrastive loss. Given a random batch of N instances, we pick 2N instances after data sampling stategy where each instance is guaranteed to have at least one same label in other instances. In order to facilitate the calculation, we concatenate the sampled instances with the original ones to obtain the image representation $v = {v, v'}$, text representation $e = {e, e'}$ and labels $Y = {Y, Y'}$ at this step. Let $sim(a,b) = a^T b/(||a|| \\cdot ||b||)$ denote the cosine similarity between a and b. For a certain representation $u_i$ and its relative batch of representations w, the supervised contrastive loss function is calculated as\n$L_{sup}(u_i, w) = \\frac{-1}{|P_s(i)|} \\sum_{p \\in P_s(i)} log \\frac{exp(sim(u_i, w_p)/\\tau)}{\\sum_{j \\neq i}^{2N} exp(sim(u_i, w_j)/\\tau)}$\nwhere $P_s(i) = {p \\in {1, ...,2N} : Y_p = Y_i}$ is the set of indices of all positives in the batch distinct from i, $|P_s(i)|$ is the cardinality of $P_s(i)$ and $\\tau$ denotes the temperature parameter. We can specifically compute supervised contrastive losses for image-image $L_{img}^{sup}$, text-text $L_{txt}^{sup}$ and image-text $L_{i2t}^{sup}$ as follows:\n$L_{img}^{sup} = \\sum_{i=1}^{2N} C_{sup}(v_i,\\bar{v})$\n$L_{txt}^{sup} = \\sum_{i=1}^{2N} C_{sup}(e_i,\\bar{e})$\n$L_{i2t}^{sup} = \\sum_{i=1}^{2N} \\sum_{i=1}^{2N} C_{sup}(e_i,v) + \\sum_{i=1}^{2N} C_{sup}(v_i, e)$\nSimilarly, for multi-label scenarios, we consider instances that have one or more common labels as positive pair. We employ multi-label supervised contrastive loss, which replaces $P_s(i)$ with $P_m(i) = {p \\in {1, ..., 2N} : Y_p \\cap Y_i \\neq \\emptyset}$ in the calculation process while keeping all other calculation the same as in the single-label contrastive loss."}, {"title": "Supervised Contrastive Learning for GAN", "content": "Intuitively, shared labels reflect common visual semantics within the images. In captioning datasets, the brief text annotation typically use concise descriptions to depict partial aspect of images. Therefore, during generator training, we provide instances sharing same label to encourage the generator to refer to the similar instances. Our generator training framework is illustrated in figure 2."}, {"title": "Data Sampling Strategy", "content": "Same as the pre-training phase, we sample a batch of images x and x', text captions t and t', labels Y and Y'. The captions are extracted to text representations e and e' by pre-trained text encoder f."}, {"title": "GAN Adaptation", "content": "As discussed in Section 2.2, the mainstream T2I GAN methods are based on two types: the multi-stage StackGAN series [31] and the one-stage DFGAN [22]. Our framework can be applicable to both types. Given the ground-truth real image x, the generator G utilizes text representations (e, e') and noise z to generate fake images $(x_f, x'_f)$ in two branches. Subsequently, the discriminator calculates the generator losses $(L_G, L'_G)$ and discriminator losses $(L_D, L'_D)$ for two branches from $(x, e, x_f)$ and $(x', e', x'_f)$, respectively. Meanwhile, the generated images from both branches are encoded by an image encoder and obtains fake image representations $(v_f, v'_f)$. These representations are then paired with (e, e') to calculate supervised contrastive loss."}, {"title": "Learning Objective", "content": "In our framework, the objective function for discriminator loss during the training process is identical to the GAN baselines in both branches, and the overall discriminator loss $L_D$ is the sum of loss from two branches. As for the generator loss $L_G$, one-stage GAN typically use conditional generation loss [10,22] while multi-stage GAN often incorporate additional non-conditional generation loss [31]. Our method does not vary the usage of baseline generator losses but adding extra supervised contrastive losses for image-to-image and image-text pairs.\nSimilar to pre-training phase, for sampled batch, we first concatenate the generated fake image representation $v = {v_f, v'_f}$, the corresponding text representations $e = {e,e'}$ and the labels $Y = {Y, Y'}$. The discriminator and generator loss function are then computed as follows:\n$L_D = L_D + L'_D$\n$L_G = L_G + L'_G + \\lambda_2 (L_{img}^{sup} + L_{i2t}^{sup})$\nwhere\n$L_{img}^{sup} = \\sum_{i=1}^{2N} C_{sup}(v_i,\\bar{v})$\n$L_{i2t}^{sup} = \\sum_{i=1}^{2N} C_{sup}(e_i,v) + \\sum_{i=1}^{2N} C_{sup}(v_i, e)$\nand $\\lambda_2$ is the weight of supervised contrastive loss."}, {"title": "Experiments", "content": "We choose novel multi-stage (AttnGAN, DM-GAN) and one-stage (SSA-GAN, GALIP) GANs to validate the superiority and universality of our framework on T2I generation for both single-object CUB [24] and multi-object COCO [11] datasets. We also conduct extensive ablations to assess the effectiveness of each component our framework proposes."}, {"title": "Evaluation Metric", "content": "We follow the baselines' evaluation protocol on the CUB and COCO datasets, which uses Inception Score (IS) [18] and Fr\u00e9chet Inception Distance (FID) [23] as quantitative evaluation metrics. After training completion, we generate 30,000 images in resolution 256\u00d7256 on the test set and compute IS and FID scores. Several previous works [8,22] have pointed out that IS can not provide useful guidance to evaluate the quality of the synthetic images on dataset COCO, thus we only evaluate IS on CUB dataset. Since GALIP was not evaluated on IS, we only compared with GALIP on FID."}, {"title": "Quantitative Results", "content": "The four baselines and our enhancement results are reported in Table 1. On single-object CUB dataset, our framework is able to improve the IS of AttnGAN by 5.7%, DM-GAN by 6.5%, and SSA-GAN by 1.4%. These results demonstrate that our framework effectively improves the clarity and diversity of generated images. Moreover, our framework improves the FID of AttnGAN by 25.6%, DM-GAN by 6.3%, SSA-GAN by 9.5% and GALIP by 1.8%. On more challenging multi-object COCO dataset, our framework is able to significantly improve the FID of all baselines. Specifically, we improves AttnGAN, DM-GAN, SSA-GAN and GALIP by 30.1%, 27.3%, 16.2% and 17.1% respectively. These results indicate that semantic relationship modeling is crucial for enhancing the T21 GAN generation quality, and the more complex scenario benefits more from it."}, {"title": "Visual Quality", "content": "In this section, we further compare the visual quality of generated images by a subset of CUB and COCO datasets for DM-GAN, SSA-GAN baselines before and after applying our framework, which are shown in Figure 3.\nFor the CUB dataset, we randomly select text-generated images belonging to the \"Tree Swallow\" category for comparison. In the first and second column, the images generated by DM-GAN exhibit severe error in producing bird head, while DM-GAN with supervised contrastive learning generates natural bird images. SSA-GAN on the other hand can generate natural bird images, but the generated bird images do not always match the descriptions or the desired bird species. For example, the bird generated in the 1st column exhibits yellow and green wings, and the bird in the 3rd column had red tails, which are not mentioned in the text description and do not align with the characteristics of Tree Swallows. On the contrary, SSA-GAN enhanced by our framework can produce birds that match the text description specifying blue-black-white wings, and is consistent with the features of Tree Swallows. In addition, the images generated by our framework exhibit strong similarity for same species, which further confirms the validity of supervised contrastive learning.\nGenerating realistic and textually coherent images that align with the descriptions is more challenging in the COCO dataset. However, our framework outperforms the baseline in terms of generating higher quality and more textually consistent images. For example, in 6th column, both DM-GAN and SSA-GAN failed to generate a red boat mentioned in the input text, but DM-GAN and SSA-GAN enhanced by our framework successfully generate the desired object. In 8th column, the bus generated by SSA-GAN is orange-yellow which deviates from the \"red\" description, while SSA-GAN enhanced by our framework successfully produce a red bus matching the description."}, {"title": "Ablation Study", "content": "In both pre-training and GAN phases we incorporate image-image supervised contrastive loss $L_{img}^{sup}$ and image-text $L_{i2t}^{sup}$ supervised contrastive loss. In this section, we verify the effectiveness of pre, $L_{img}^{sup}$ and $L_{i2t}^{sup}$ in our framework by conducting extensive ablation study on the CUB and COCO dataset in Table 2."}, {"title": "Comparison to other label-supervised methods", "content": "To our best knowledge, there is no existing approach in this field leveraging labels information as additional guidance like our framework does. To demonstrate the novelty of our approach, we use two simple settings that commonly used for plug-in label learning as extra baselines. Firstly, we apply UniCL [26] to AttnGAN. On CUB dataset, UniCL can easily be adopted because each image only asscociates with one label. In order to apply UniCL to the COCO dataset, we replaced its single-label supervised contrastive loss to a multi-label supervised contrastive loss. Secondly, we introduce cross-entropy loss in classification task to AttnGAN. We introduce a pre-trained fully connected network as a image classifier and add the cross-entropy loss to the existing loss and train by multi-task learning. The results are shown in the table 3. As the UniCL and cross-entropy improving the AttnGAN slightly, our framework demonstrate largest margin of visual enhancement for all metrics, indicating the compatibility of our framework with T2I GAN baselines."}, {"title": "Conclusions", "content": "In this work, we introduce a novel framework that harness semantic information with supervised contrastive learning to improve T2I GAN. Our framework use the two branch contrast to extend the original method across the pre-training and GAN phases. In pre-training phase, we employ label guided data sampling strategy, where we define positive pair as the images with same label. Driven by supervised contrastive loss on the positive image pairs and their corresponding text, the pre-training encoder elevates the representation similarity of images with same semantic concepts and push away those without. In the GAN phase, we first proceed original GAN for each branch independently and formulate a quadruple including the representations of generated positive image pair and their corresponding texts from two branches. We then employ augmented supervised contrastive loss to the quadruple which, like in pre-training phase, serves to elevate the similarity between images characterized by common semantic, thereby enhancing the image generation quality.\nWe apply our framework to famous four GAN baselines including AttnGAN, DM-GAN, SSA-GAN and GALIP and conduct experiments on single-object CUB and multi-object COCO dataset. The results demonstrate that our framework can indifferently improve baselines on both datasets with considerable margin, especially the more complex COCO."}, {"title": "Acknowledgments", "content": "This research was supported by STI 2030-Major Projects 2021ZD0200403. The authors like to thank the authors of DM-GAN for providing the details of its implemention and the anonymous reviewers for their review and comments."}, {"title": "Conclusions", "content": "Although we only demonstrate the effectiveness on the datasets with detailed label annotation, our framework can be extended to other image-text pair only datasets by noun extraction from all text as labels, which will be the next step of our research interest. Recently, the advent of data-centric methodologies such as SAM [7] has further curtailed the expenses for semantic label acquisition, subsequently relaxing the prerequisites for implementing our framework. Furthermore, we expect this work to exhibit potential application for diffusion models especially on efficiency improving due to the adaptable nature of our framework. We defer the extension to future research endeavors."}]}