{"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "authors": ["Cristiano Patr\u00edcio", "Isabel Rio-Torto", "Jaime S. Cardoso", "Lu\u00eds F. Teixeira", "Jo\u00e3o C. Neves"], "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the final disease prediction on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: https://cristianopatricio.github.io/CBVLM/.", "sections": [{"title": "1. Introduction", "content": "The field of medical imaging analysis has witnessed significant advancements through the use of supervised deep learning methods. However, their performance greatly depends on the availability and quality of training samples,"}, {"title": "2. Related Work", "content": "Large Vision-Language Models in the Medical Domain The emergent capabilities of Large Language Models (LLMs) have led to the development of LLMs able to process visual inputs, i.e. LVLMs. These models can reason about images in a zero- or few-shot manner (e.g., LLaVA [9], GPT-4V [10]), allowing them to tackle downstream tasks without fine-tuning. General-purpose LVLMs have recently been adapted for the medical domain through training on extensive medical datasets, coining the term Med-LVLMs. The generalization capability of these models has prompted researchers to assess their performance in multiple medical data. Royer et al. [11] recently released an open-source toolkit for fair and reproducible evaluation of Med-LVLMs in tasks like classification and report generation. Han et al. [5] evaluated the performance of a general LVLM on the NEJM Image Challenge, as well as the impact of fine-tuning the LVLM. In [12], Med-LVLMs were evaluated in different specialities and compared with medical experts, with results comparable to the performance of junior doctors. In [7], the authors benchmarked the performance of GPT-4V against fully supervised image classifiers and found that their performance is comparable. In a different line of research, Xia et al. [13] evaluated the trustworthiness and fairness of Med-LVLMs and found that models often display factual inaccuracies and can be unfair across different demographic groups. CBVLM distinguishes itself from previous studies as it is not limited to prompting the LVLM to provide a diagnosis, but it is, to the best of our knowledge, the first to generate concept-based explanations on which the LVLM response is grounded prior to diagnosis prediction, thus enhancing the performance and transparency of the decision-making process.\nConcept Bottleneck Models CBMs [14] are based on an encoder-decoder paradigm, where the encoder predicts a set of human-specified concepts from the input image, while the decoder leverages these predicted concepts to generate the final predictions. Due to their inherent interpretability, these models have attracted particular interest in medical image classification. Several CBMs have been proposed for a wide variety of medical tasks (e.g., melanoma diagnosis [15\u201317], knee osteoarthritis [14] and chest X-ray classification [18]). Nevertheless, CBMs have well-known limitations: (i); they require concept annotations (image-level or class-level), (ii) they tend to have lower performance compared to traditional (black-box) supervised methods, (iii) they struggle to predict concepts accurately, and (iv) incorporating new concepts implies retraining the whole CBM. These challenges are particularly significant in medical imaging, where data availability is typically limited due to the expertise required for annotations. Consequently, LVLMs represent a promising alternative to traditional CBMs, as they are able to generalize to unseen domains with minimal or no supervision. Moreover, incorporating additional concepts becomes trivial and does not require fine-tuning, as happened with traditional CBMs."}, {"title": "3. Methodology", "content": "Given the task of predicting a target $y \\in R$ from input $x \\in R^d$, let $D = \\{(x^{(i)},y^{(i)}, c^{(i)})\\}_{i=1}^N$ be a batch of training samples where $c\\in R^l$ is a vector of $l$ concepts. Traditional (black-box) models learn a function $h : R^d \\rightarrow R$ that directly predicts the target $y\\in R$ from the input $x \\in R^d$. However, CBMs first map the input $x$ into a set of interpretable concepts $c$ (\"bottleneck\") by learning $g : R^d \\rightarrow R^l$, and use these concepts to predict the target $y$ using $f : R^l \\rightarrow R$. Therefore, the prediction $\\hat{y} = f(g(x))$ is entirely based on the predicted concepts $\\hat{c} = g(x)$."}, {"title": "3.1. Concept Detection", "content": "The first stage involves predicting the $l$ concepts present in a given input image. For each predefined clinical concept, we start by providing the LVLM with a short description of that concept and then prompt it to answer if that concept is shown in the image (see Figure 2 on the left). Thus, concept prediction is given by $P_{LVLM}(t_{ans}|t_1,..., t_i, t_h,..., t_b)$, where $t_{ans}$ corresponds to the first generated token, $t_1,..., t_i$ correspond to the input image tokens, and $t_h,..., t_b$ correspond to the tokens of the prompt (description of the concept and the question). The structure of the (zero-shot) prompt for the prediction of a single concept is as follows:"}, {"title": "3.2. Disease Diagnosis", "content": "The second stage involves prompting the LVLM to answer if a certain disease (i.e. class) is present in the image. The concepts predicted in the previous stage are appended to the question, such that when providing an answer, the LVLM not only considers the image and the question, but also takes into account the concepts. Thus, the disease diagnosis is given by $P_{LVLM}(t_{ans}|t_1,..., t_i, t_h,..., t_b, t_{c_0},..., t_{c_l})$, where $t_{c_0},..., t_{c_l}$ are the tokens representing the concepts. This approach ensures that the diagnosis is grounded on the identified clinical concepts, enhancing the interpretability and transparency of the model's response. The structure of the (zero-shot) prompt for diagnosis classification is as follows:"}, {"title": "3.3. Few-shot Prompting", "content": "A key factor contributing to the appeal of LLMs, and consequently LVLMs, is their ability to perform ICL. ICL, or few-shot learning/prompting, is a prompt engineering technique in which the model is provided with examples, known as demonstrations, consisting of: i) questions, and ii) their corresponding answers. It has been shown to significantly improve the performance of both LLMs [20] and LVLMs [21]. Therefore, our methodology incorporates ICL in both stages, as shown in Figure 2."}, {"title": "3.4. Demonstration Example Retrieval", "content": "Although the demonstrations for ICL could be chosen randomly, Zhou et al. [22] have shown that this selection policy can decrease the model performance when compared to the 0-shot scenario (when no examples are included in the prompt). Thus, following other works [21, 23\u201325], we introduce a Retrieval Module to select the demonstrations based on their similarity to the query image $x_o$, a method known as Retrieval-based In-Context Examples Selection (RICES). Given the feature vector $(x_0)$ of the query image obtained from an arbitrary vision encoder, we select the $N$ most similar image feature vectors from the training set $D$ as the few-shot demonstrations to include in the prompt. For each different type of prompt, the question is repeated for every demonstration. The ground-truth answer to each demonstration question is also provided; depending on the stage of our methodology, this can be the absence/presence of a given concept (1st stage), or the disease diagnosis, as well as the absence/presence of all concepts (2nd stage)."}, {"title": "3.5. Answer Extraction", "content": "As LVLMs produce their outputs in natural language (i.e. it is open-ended), it is necessary to process the generated text to compare it with the ground-truth concepts or the ground-truth class labels. To simplify this process and limit the space of possible responses, all questions are posed as multiple choice and the LVLM is instructed to answer by choosing one of the options and not providing additional information (see the example prompts in Figure 2).\nOne way of extracting the LVLM answer would be to select the option with the higher first token log probability, i.e. if token B is the one with the highest probability, then this would mean that the LVLM had chosen option B. However, [26] have shown that there is a mismatch between the first token probabilities and the actual LVLM answer. Thus, we use a Regular Expression (RegEx) to directly find the pattern \"option_letter)\", either in lower or upper case. We choose to provide the options as \u201coption_letter\" + \")\" instead of \"option_letter\u201d + \u201c.\", as it would be less probable to find the pattern \u201coption_letter)\" in the middle of the LVLM response. If none of the predefined options is found in the LVLM answer (e.g., when the LVLM answers the question but in a more verbose fashion), we follow the approach of previous works [3] and employ an auxiliary LLM\u00b9 to extract the information from the LVLM answer and match it to the predefined list of options. If both approaches fail (e.g., when the LVLM response is just a description of the input image without any actual answer to the question), we mark that instance as unknown and report separately the percentage of instances for which this occurred."}, {"title": "4. Experimental Setup", "content": "In this section, we outline our experimental setup with information about the adopted LVLMs, datasets, and evaluation metrics, as well as the implementation details."}, {"title": "4.1. Datasets", "content": "We conduct experiments on four publicly available datasets, encompassing dermatology (Derm7pt [27] and SkinCon [28]), radiology (CORDA [29]) and eye fundus imaging (DDR [30]) modalities. The selection of these datasets is constrained by the availability of datasets with annotated clinical features, hereinafter referred to as \"concepts\". Unless otherwise stated, we use the official train-val-test splits of the datasets.\nFor the Derm7pt [27] dataset, with a total of 827 images, we consider only dermoscopic images of the \"nevus\" and \"melanoma\u201d classes, jointly with the 5 annotated dermoscopic attributes, namely Absent/Typical/Atypical Pigment Network, Absent/Regular/Irregular Streaks, Absent/Regular/Irregular Dots and Globules, Blue-Whitish Veils, and Regression Structures. The SkinCon [28] dataset contains 3,230 images from the Fitzpatrick 17k dataset [31] annotated with 48 clinical concepts. Following previous works, we use the binary labels denoting skin malignancy for the target task and randomly split the dataset into train/val/test with the proportion of 70%, 15%, and 15%, respectively. Additionally, only 22 out of the 48 concepts are used, as the other 26 concepts appear in less than 50 images. From the CORDA [29] dataset, a COVID-19 diagnosis dataset, we use its 1601 chest X-rays. Inspired by the work of Barbano et al. [32], we use a pretrained model on the CheXpert [33] dataset to annotate the samples with a binary label to denote the presence or absence of 14 radiological observations. Of those 14 concepts, 9 are used as they are present in at least 50 images. We also exclude the \"No Finding\" concept, keeping the remaining 8 concepts. The DDR [30] dataset comprises 13,673 fundus images divided into six Diabetic Retinopathy (DR) levels. In our experiments, we focus on the five-class classification task for DR grading, discarding the images belonging to the ungradable class and considering a subset of 757 images annotated with 4 types of lesions correlated with DR, which we use as concepts. We augment this subset with 743 \u201cNo DR\u201d images, resulting in a dataset of 1,500 samples."}, {"title": "4.2. LVLMS", "content": "In our experiments, we assess the performance of open-source LVLMs that can process both text and multiple images. Specifically, we focus on models trained on general-domain data, such as OpenFlamingo [23], Idefics3 [34], VILA [35], LLaVA-OneVision [36], Qwen2-VL [37], MiniCPM-V [38], InternVL2.5 [39] and mPLUG-Owl3 [40]. Additionally, we consider models specialized for the medical domain, including Med-Flamingo [1], LLaVA-Med [2], CheXagent [3] and SkinGPT-4 [4]. We use their corresponding checkpoints from the HuggingFace library (the full list of checkpoints can be found in the Supplementary Material)."}, {"title": "4.3. Baselines", "content": "We compare the performance of CBVLM against several baselines, including:\ni) standard CBM [14] trained on each of the four datasets considered using the official implementation. It is worth noting that, for training the CBM on the Derm7pt dataset, we considered 11 clinical concepts. Specifically, 3 out of the 5 annotated concepts have three options each (e.g., Pigment Network can be Absent, Regular, or Irregular), as described in 4.1. To address this, and given that the concept layer predicts the presence or absence of each concept, we treated each option as an independent concept. For instance, for Pigment Network, we defined the concepts: Absent Pigment Network, Regular Pigment Network, and Irregular Pigment Network;\nii) CLAT [41] - a concept-based framework specifically designed for retinal disease diagnosis. For DDR, the pre-computed textual embeddings of the clinical concepts provided in the official CLAT repository were used. However, since the original embeddings were obtained from a retinal foundation model [42], for the other datasets, we replaced it with MedImageInsight [43], an embedding model for medical imaging trained on a mixture of X-ray, computed tomography, dermatology, and pathology datasets. For Derm7pt, we follow the same strategy described above, i.e. we transformed the 5 concepts into 11 independent concepts;\niii) supervised black-box models, namely ImageNet-pretrained ResNet50 [44] and ViT Base [45] fine-tuned on the disease diagnosis task;\niv) task-specific black-box models, namely [17] for the Derm7pt dataset, [46] for the SkinCon dataset, [47] for the CORDA dataset, and [48] for the DDR dataset, reporting classification performance on the respective dataset."}, {"title": "4.4. Retrieval Module", "content": "For the retrieval strategy used to perform ICL, image similarity is determined using cosine similarity between features, which are extracted from three distinct encoders, namely CLIP ViT/B-16 [49], MedImageInsight [43] and the (vision) encoder of the corresponding LVLM."}, {"title": "4.5. Evaluation Metrics", "content": "For both concept prediction and disease diagnosis, we report Balanced Accuracy (BACC) and F1-score, as these metrics are tailored to deal with imbalanced datasets, which is the case in medical image classification. For concept prediction, we measure the BACC and F1-score averaged over each concept."}, {"title": "5. Results and Analysis", "content": "This section presents the key results of CBVLM, comparing its performance against CBMs and task-specific supervised methods in terms of BACC and F1-score across two tasks: i) Concept Detection (Figures 3, 4, and 5); and ii) Disease Diagnosis (Figures 6 and 7)."}, {"title": "5.1. Concept Detection Performance", "content": "We start by assessing the predictive performance of CBVLM in the task of concept detection. Figure 3 presents the results across different datasets under various n-shot settings, $n \\in \\{0,1,2,4\\}$. For comparative analysis, we also report the performance of CBM and CLAT. The results for n > 0 were obtained considering the best vision encoder, in this case, MedImageInsight, selected after an ablation study across three different vision encoders (see Figure 8).\nFew-shot prompting boosts performance The results presented in Figure 3 and the analysis depicted in Figure 4 (left) reveals that few-shot prompting significantly boosts performance by incorporating contextually relevant information within the prompts [1]. Notably, there is a consistent trend across all datasets, demonstrating improved performance as the number of demonstrations increases. For example, the performance of CBVLM when adopting CheXagent, LLaVA-OV, Qwen2-VL, and SkinGPT-4 improves by over 15% when incorporating 4 demonstration examples into the prompt compared to the zero-shot scenario (n = 0). Remarkably, CBVLM outperforms both CBM and CLAT across all datasets except Derm7pt, where its performance is on par with CBM. Impressively, CBVLM achieves these results without any training, relying on only four demonstration examples or less."}, {"title": "Medical LVLMs outperform generic LVLMs in few-shot settings", "content": "From the results depicted in Figure 4 (right), in the 0-shot scenario, generic LVLMs consistently outperform medical LVLMs across all datasets. However, in the few-shot setting, medical LVLMs outperform their generic counterparts in all datasets using only one or two demonstration examples (n \u2208 {1,2}). The stronger performance of generic LVLMs in the zero-shot setting can be attributed to their training on large-scale generic image datasets, allowing for better generalization in this setting. As demonstration examples are introduced into the prompt, medical LVLMs improve, benefiting from their pre-training on specialized medical data, which enhances their ability to generalize in these scenarios."}, {"title": "CBVLM outperforms CBMs", "content": "CBVLM outperforms CBMs and CLAT in concept prediction across all datasets, except for Derm7pt, where CBM achieves the best result with a margin of \u2248 5%. This may be due to the fact that in Derm7pt, some concepts have three available options, whereas in the other datasets, each concept is binary, which may make it easier for the LVLM to predict the presence or absence of a concept more accurately."}, {"title": "CBVLM outperforms CBM when using only 10% annotated data", "content": "Although CBVLM is able to outperform CBMs with only 4-shots, it should be noted that, in total, more than 4 images are being used, as the same 4 example images will not be the most similar for all query images in the test set. To assess the impact of the number of images per class in the examples set, we progressively reduce the percentage of images per class, limiting the number of available examples for selection for few-shot prompting. This experiment is conducted using the best-performing model for each dataset: CheXagent 4-shots for Derm7pt, and Open-Flamingo 2-shots for SkinCon, CORDA and DDR. The results, shown in Figure 5 and averaged across 3 runs, demonstrate that even with just 10% of the images per class in the examples set, CBVLM consistently outperforms CBM across all datasets, except for Derm7pt where even with 100% of the images per class, the performance is lower than CBM, as discussed earlier. Nevertheless, the results shown in Figure 5 underscore a key observation: few-shot prompting with a good example selection strategy can boost performance by up to 20% (cf. BACC at 0% and 10%), even in situations where annotated data is severely limited, which is the case of medical data. Thus, LVLMs can indeed be successfully used for concept detection with very limited amounts of annotated data, thus reducing the annotation cost inherent to CBMs."}, {"title": "5.2. Disease Diagnosis Performance", "content": "Figure 6 shows the performance of CBVLM in terms of BACC and F1-score for the task of disease classification across different datasets under various n-shots (n = {0,1,2,4}). Similarly to Section 5.1, we also report the performance of CBM, CLAT and a Black-box (Bbox) model for comparison. Additionally, we include results for the setting where the CBVLM predicts the disease label directly without prior concept information (\"0 w/o\" bar in Figure 6).\nFew-shot prompting boosts performance Similar to the trend observed in the concept detection task, the results shown in Figure 7 (left) confirm our initial assumption and align with findings reported in the literature: few-shot prompting significantly enhances the performance of LVLMs in disease classification as the number of demonstrations increases, across all datasets.\nUsing concepts improves performance and promotes explainability The results show that incorporating clinical concepts into the prompt improves both BACC and F1-score compared to the scenario without concept guidance (i.e., \"0\" vs \"0 w/o\" bars in Figure 6). Furthermore, grounding the final diagnosis on a set of clinical concepts promotes transparency and explainability, aligning more closely with human reasoning, unlike the concept-free setting (\"0 w/o\").\nMedical LVLMs outperform generic LVLMs in few-shot settings The results in Figure 7 (right) confirm that generic LVLMs consistently outperform their medical counterparts across all datasets in the scenario without concepts (\"0 w/o\"). This trend is also observed in the 0-shot scenario with concepts, in general. In the few-shot setting, generic LVLMs exhibit superior performance only in SkinCon in terms of BACC, with a marginal average improvement of 1% across scenarios with n > 0. Conversely, when averaging across all few-shot scenarios (n > 0), medical LVLMs consistently outperform generic models across all datasets, achieving F1-score improvements of 25.74%, 14.48%, 4.58%, and 19.81% for Derm7pt, SkinCon, CORDA, and DDR, respectively. This phenomenon can be attributed to two factors: i) the stronger domain knowledge of medical LVLMs, and ii) the initialization of their vision encoders from medical-specific CLIP versions, such as BiomedCLIP [50] for LLaVA-Med, or larger improved CLIP architectures like EVA-CLIP-g [51], used in CheXagent and SkinGPT.\nCBVLM outperforms CBMs CBVLM surpasses CBMs across all datasets. With the top-performing LVLM, we observe F1-score improvements of 10.88%, 45.71%, 21.04%,"}, {"title": "5.3. Ablation Study: Strategies for ICL Example Selection", "content": "In order to validate the importance of the Retrieval Module (cf. Figure 2), which employs RICES [24] (i.e. ranks examples based on the cosine similarity between image features extracted from an arbitrary vision encoder), we conduct an ablation study on the task of concept detection for the 1-shot scenario.\nIn this study, we first compare random ICL example selection with RICES, and then assess different vision encoders to perform RICES. For random selection, we compare choosing only 1 random example against choosing 1 random example per class. The results of Figure 8 show that, except for Open-Flamingo and MiniCPM-V, using 1 example per class does not improve performance. In fact, in the majority of cases it slightly decreases it, when comparing to simply choose 1 random example.\nWe then explore two different encoders for RICES: Med-ImageInsight [43] and the encoder integrated within the respective LVLM. Although in some models (e.g., Open-Flamingo and MiniCPM-V), choosing the examples randomly brings better performance, in the majority of cases it improves both BACC and F1-score (especially for Medical LVLMs). More importantly, we are not concerned with the average results, but rather on maximizing them, even if"}, {"title": "CBVLM outperforms supervised black-box models", "content": "Finally, when compared to black-box approaches, a similar trend is observed. CBVLM outperforms black-box models by 2.15% in BACC for Derm7pt and achieves comparable performance on CORDA, with a slight decrease of -1.07%. In terms of F1-score, the top-performing CBVLM surpasses black-box models by 14.42% on SkinCon and by 1% on DDR. Thus, CBVLM achieves state-of-the-art results on Derm7pt, SkinCon and DDR."}, {"title": "5.4. General Discussion", "content": "Table 2 presents a detailed comparison of our proposed CBVLM with concept-based models, supervised black-box models, and task-specific models. As highlighted in the results, CBVLM consistently achieves either the best or second-best performance across all evaluations. Notably, this is accomplished without any additional training and by leveraging just four examples, enabling concept-based explanations and enhanced interpretability and transparency in the model responses. In summary, CBVLM using CheXagent with 4 demonstration examples emerges as the top-performing combination for Derm7pt. For SkinCon, CBVLM with the generic Open-Flamingo model and 4 demonstration examples achieves the best results in both disease diagnosis and concept detection tasks. For CORDA and"}, {"title": "5.5. Limitations", "content": "The main drawback of the proposed methodology for generating concept-based explanations with LVLMs lies in the independent prediction of each concept, which is a concern when applied to datasets with a large number of concepts. A similar problem arises with datasets with a large number of classes, as each class is provided as a separate option in the prompt. This can result in excessively long prompts that would require alternative ways of querying the model.\nAdditionally, our approach is limited to discrete-valued concepts (e.g., absent/present or, for some concepts of Derm7pt, absent/regular/irregular), contrasting with traditional CBMs, where each concept is associated with a probability. Nevertheless, this limitation could be addressed by prompting the LVLM to not only answer if a given concept is present in the input image but also to provide a confidence score, as is done in other works [7]. We leave this exploration to future work.\nFinally, CBVLM is only as good as the LVLM it uses. In a few cases, some of the LVLMs we tested struggled to provide answers to the questions (e.g., instead of answering the question, the LVLM simply described the input image), leading to an increase in unknown responses (as shown in Figures 3 and 6). This phenomenon is particularly common with SkinGPT, which may be due to its pretraining on very specific prompts designed to elicit desired responses [4]. LLava-Med also faced challenges in providing valid responses, especially in few-shot scenarios where n > 1. This difficulty may stem from the fact that it is not trained with interleaved image-text data; in such models, image data is given at the input in the form of tokens, so as the number of shots increases, so does the number of tokens, making it more difficult for the model to pay attention to the full input sequence [52]."}, {"title": "6. Conclusions and Future Work", "content": "We propose a simple yet effective methodology, CBVLM, that leverages off-the-shelf LVLMs to produce concept-based explanations and predict disease diagnoses grounded on those explanations, thus offering a more transparent and explainable decision-making process, something that is critical in high-stakes scenarios such as medical use-cases. Through extensive experiments across four medical datasets, and open-source LVLMs, we show that CBVLM outperforms the traditional CBM and even task-specific black-box models across all benchmarks. This is achieved without requiring any training and using just a few annotated examples, thus tackling a very common problem in medical applications, where the annotation process requires a clinician's expertise. Overall, CBVLM is able to leverage the best of both worlds: (i) its LVLM-based training-free nature presents a major advantage over CBMs, as these models require additional training and are restricted by predefined concepts, and (ii) its two-stage process keeps the interpretability inherent to CBMs but that LVLMs lack.\nAlthough CBVLM was tested on medical datasets, it is a general methodology that can be applied in other domains. In the future, we would like to do so, e.g., on the CUB dataset [53]. Given the promising results obtained in this work with open-source LVLMs of around 8 billion parameters, we would also like to experiment with bigger and/or proprietary LVLMs."}]}