{"title": "ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science", "authors": ["Robert Wolfe", "Alexis Hiniker", "Bill Howe"], "abstract": "This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.", "sections": [{"title": "Introduction", "content": "Computational methods that quantify societal biases using language technologies like word embeddings (Mikolov et al. 2013), generative language models (Radford et al. 2018), and multimodal language-and-image models (Radford et al. 2021) have been widely adopted by social scientists, who leverage the reflection of society captured by these technologies to observe implicit and explicit societal biases where human-subjects experiments are infeasible or prohibitively expensive (Durrheim et al. 2023; Kennedy et al. 2021). Social scientists have employed word embeddings in particular to analyze diachronic historical changes in human biases and norms (Garg et al. 2018), to study variations in gender biases across numerous languages (Lewis and Lupyan 2020), to compare implicit biases in adult and children's language corpora (Charlesworth et al. 2021), and to validate longstanding theories about societal biases, such as the masculine default"}, {"title": "Related Work", "content": "In reviewing the related on work on Embedding Association Tests (EATs), we describe the models and domains in which EATs are employed; then provide a detailed overview of their applications in computational social science; and finally review their limitations as described in prior work."}, {"title": "Embedding Association Tests", "content": "Caliskan, Bryson, and Narayanan (2017) introduce the Word Embedding Association Test (WEAT), a measurement of intrinsic bias in word embeddings drawing on the design of the Implicit Association Test (IAT), a method for studying implicit (unconscious) bias in human subjects (Greenwald, McGhee, and Schwartz 1998). The WEAT quantifies the relative association of two target groups (such as Science and Art) with two attribute groups (such as Male and Female), and, like the IAT, returns an effect size (Cohen's d) and a p-value. Caliskan, Bryson, and Narayanan (2017) used the WEAT to replicate the results of ten IATs reflecting gender and racial biases, among others, in the GloVe embeddings of Pennington, Socher, and Manning (2014). Caliskan, Bryson, and Narayanan (2017) also introduced the Single Category SC-WEAT, which quantifies the differential association of a single word with two attribute groups. The SC-WEAT was introduced as part of a measurement called the Word Embedding Factual Association Test, and further clarified in an analysis of androcentric gender bias by Caliskan et al. (2022). The WEAT was extended to transformer models by May et al. (2019), who introduced the Sentence Embedding Association Test (SEAT), a sentence-level WEAT employing semantically neutral prompts, and by Kurita et al. (2019), who tied bias measurement to the model's pretraining objective. Guo and Caliskan (2021) introduced the Contextualized Embedding Association Test (CEAT), which measured embedding bias at the word level and modeled contextualization as a random effect. EATs are also used in computer vision and multimodal language-and-image models. Steed and Caliskan (2021) introduced the Image Embedding Association Test (iEAT), which quantified bias in self-supervised image encoders such as SimCLR (Chen et al. 2020b) and iGPT (Chen et al. 2020a). Ross, Katz, and Barbu (2020) introduced the Grounded-WEAT for grounded language-and-image models, while (Wolfe and Caliskan 2022a) use an EAT to quantify bias in CLIP language-and-image models (Radford et al. 2021), and Hausladen et al. (2023) employ the SC-EAT to perform a causal analysis of social bias in language-and-image models. Finally, Slaughter et al. (2023) introduce the SpEAT, an EAT for quantifying intrinsic bias in pretained speech processing models such as wav2vec2 (Baevski et al. 2020) and OpenAI Whisper (Radford et al. 2023). While most EATs employ cosine similarity to measure association between target and attribute stimuli, recent work explores alternatives. Omrani Sabbaghi, Wolfe, and Caliskan (2023) employ an algebraic definition of bias similar to that of Bolukbasi et al. (2016), but use an EAT-based formula to obtain an effect size and p-value. Bai et al. (2024) assess implicit bias using the textual output of ostensibly debiased generative language models by employing a prompt-based analogue of the IAT."}, {"title": "Applications of EATs in Social Science", "content": "Social scientists use word embeddings and EATs to study phenomena that are impossible or financially infeasible to measure solely through direct experimental methods. Many studies leverage the societal scale of the data used for training word embeddings to make broad inferences about society that would be unavailable in a small-N psychological study. Caliskan et al. (2022) demonstrate a masculine default in the English-language internet using EATs computed for every word in the vocabulary of pretrained GloVe and FastText embeddings, while Bailey, Williams, and Cimpian (2022) use word embeddings to demonstrate the implicit equivalence of the concept of \"person\" with \"men.\" Napp (2023) use EAT measurements to contend that gender stereotypes are stronger in countries that are more economically developed and individualistic. Finally, Schmahl et al. (2020) use the EAT to study changes in gender bias on Wikipedia, informing their suggestions for reducing bias on the platform. Other research employs word embeddings for previously untenable cross-cultural analyses of human attitudes. For example, Mukherjee et al. (2023) employ EAT to measure biases related to albeism, immigration, and education across 24 languages. Embeddings of historical data have also provided a means to study human societies that no longer exist and are thus unavailable for direct study. Charlesworth, Caliskan, and Banaji (2022) use word embeddings to measure changes in stereotypes about social groups over 200 years. Borenstein et al. (2023) use the EAT to study intersectional biases in word embeddings trained on newspapers from the 18th and 19th century. Sunsay (2023) use the EAT to study the disease avoidance theory of xenophobia, measuring EAT scores in 19th and 20th century travel literature to test western associations of indigenous people disgust words that would suggest disease avoidance. Leach, Kitchin, and Sutton (2023) employ the EAT to argue that \"language has changed in a way that reflects greater concern for others,\" supporting the idea that the societal \u201cmoral circle\u201d expanded during the 19th and 20th centuries to include more groups of people, in addition animals and the environment. Guan et al. (2024) use cosine similarities to measure the evolution of color associations (e.g., association of the color red with heat) over 200 years, while Betti, Abrate, and Kaltenbrunner (2023) use the EAT to measure sexism in fifty years of English song lyrics. Finally, Wolfe, Banaji, and Caliskan (2022) find evidence of the historical bias of hypodescent in CLIP models. Research also employs EATs to study present-day bias in domains such as law and medicine. Rios, Joshi, and Shin (2020) use the WEAT to measure gender bias in biomedical research, finding that traditional gender stereotypes have declined over time, but that specific medical conditions like body dysmorphia still exhibit high gender bias. Cobert et al. (2024) use cosine similarities to measure implicit racial biases in ICU notes. In the legal domain, Matthews, Hudzina, and Sepehr (2022) use the EAT to study biases in word embeddings trained on corpora of legal opinions, and Dutta et al. (2023) use WEAT scores to quantify gender bias in Indian divorce court proceedings. Moreover, amid increasing interest in using AI to measure aspects of human society"}, {"title": "Limitations of EATS", "content": "The EAT faces challenges related both to its mathematical definition and its predictive value in NLP applications. Social scientists who choose not to use the EAT sometimes note that its design, which mimics the differential construction of the IAT, can cause difficulties in observing and interpreting bias. Bailey, Williams, and Cimpian (2022) use the difference in raw cosine similarities to observe asymmetrical gender biases, noting that the WEAT is better applied to symmetrical patterns of association. Ethayarajh, Duvenaud, and Hirst (2019) contend that the standardization of the WEAT, in dividing by the joint standard deviation of word associations, can obscure differences in underlying cosine similarities. Moreover, anisotropy (directional uniformity) in deep learning models (Mu and Viswanath 2018) can distort intrinsic semantic measurements (Timkey and van Schijndel 2021), necessitating postprocessing of EATs (Wolfe and Caliskan 2022b). Recent work suggests that EATs have limited predictive value for bias in downstream NLP tasks. Goldfarb-Tarrant et al. (2020) find that biases observed with the WEAT are not correlated with application biases in tasks like coreference resolution. In a study of downstream propagation using transformer language models, Orgad, Goldfarb-Tarrant, and Belinkov (2022) propose an information-theoretic framework rather than an EAT. Cabello, J\u00f8rgensen, and S\u00f8gaard (2023) show that association bias and fairness are uncorrelated, but also provide sociological evidence that the two kinds of metrics should be expected to be independent of each other. We are concerned with uses of the EAT in social science to study human attitudes. To that end, we design an interpretable EAT, rather than an EAT predictive of downstream bias."}, {"title": "Models and Data", "content": "This research introduces the ML-EAT and applies it to word embeddings, GPT-2, and CLIP, adapting stimuli from prior studies of implicit bias in NLP and computer vision."}, {"title": "Pretrained Models", "content": "We apply the ML-EAT to the below language technologies.\n\u2022 GloVe Word Embeddings: Global Vectors for word representation (GloVe) train on the co-occurrence matrix of a text corpus, such that the vector representation of a word is learned based on the words it is most likely to occur around (Pennington, Socher, and Manning 2014). Caliskan, Bryson, and Narayanan (2017) introduced the WEAT by presenting results on 300-dimensional GloVe vectors trained on the 840-billion token Common Crawl.\n\u2022 HistWords Embeddings: HistWords refers to sets of 20 word embeddings in four languages trained on ten-year slices of historical language corpora ranging between the years 1800 and 2000 (Hamilton, Leskovec, and Jurafsky 2016). Hamilton, Leskovec, and Jurafsky (2016) introduced HistWords to prove that more frequently used words exhibit less semantic change over time, and that polysemous words exhibit faster semantic change. We apply the ML-EAT to the English language HistWords embeddings trained using Word2Vec (SGNS) (Mikolov et al. 2013) on Google books (all genres) (Lin et al. 2012).\n\u2022 GPT-2 Language Models: GPT-2 (\u201cGenerative Pre-trained Transformer\u201d) is a causally masked transformer (Vaswani et al. 2017) language model trained to predict the next word in a sequence (Radford et al. 2019). This research studies the four pretrained GPT-2 models (Base, Medium, Large, and XL) available via the Transformers library (Wolf et al. 2020), which were pretrained on OpenAI's WebText dataset, a collection of webpages scraped from highly rated outbound links on Reddit.\n\u2022 CLIP Language-and-Image Models: CLIP (\u201cContrastive Language Image Pretraining\") is a multimodal language-and-image model, which classifies images based on their cosine similarity with text labels (Radford et al. 2021). This research reports results under varying prompts from the CLIP-ViT-L14-336 model, the best performing OpenAI-trained CLIP model available. CLIP-ViT-L14-336 is trained on OpenAI's WebImageText (WIT) dataset, a collection of 400 million pairs of web-scraped images and accompanying captions (Radford et al. 2021)."}, {"title": "EAT Stimuli", "content": "An EAT employs four groups of words or images (called \u201cstimuli\u201d, drawing on the test's psychological foundations in the IAT (Greenwald, McGhee, and Schwartz 1998)), each representing a concept. For example, the EAT demonstrating that flowers are favored over insects uses word lists to represent the concepts of Flowers, Insects, Pleasant, and Unpleasant. Each EAT includes two \"target\" groups, X and Y (Flowers and Insects), which are tested for association with two \"attribute\" groups, A and B (Pleasant and Unpleasant) (Caliskan, Bryson, and Narayanan 2017). The two target groups contain the same number of stimuli, as do the two attribute groups. Groups must contain at least eight stimuli to adequately represent a concept (Caliskan et al. 2022). We use the stimuli for the tests of implicit bias specified by Caliskan, Bryson, and Narayanan (2017) when applying the ML-EAT to GloVe and GPT-2. We applied the Math/Arts Male/Female EAT to the HistWords embeddings, replacing three stimuli because their L2 norms were zero-valued (preventing the computation of cosine similarity) in several HistWords embeddings. We substituted \u201cmusic\u201d for \"symphony\u201d; \"mathematics\" for \"math\u201d; and \"calculation\u201d for \"calculus.\" Tests of bias in CLIP utilize the word stimuli of Caliskan, Bryson, and Narayanan (2017) to represent Pleasant and Un-pleasant, and image stimuli from Steed and Caliskan (2021)."}, {"title": "Approach", "content": "The ML-EAT is defined using three levels of measurement, with a taxonomy of nine EAT patterns for describing biases it quantifies. We first describe the test itself, then introduce the EAT-Map visualization and EAT pattern taxonomy."}, {"title": "Defining the ML-EAT", "content": "The ML-EAT computes bias at three levels of increasing granularity. Level 1 returns the traditional standardized effect size quantifying the differential association between two target concepts with two attribute concepts; Level 2 returns two effect sizes quantifying the differential association of each target group individually with the two attribute groups; and Level 3 returns four means and corresponding standard deviations describing the non-differential association of each target group and attribute group.\nLevel 1 The first level of the ML-EAT is equivalent to the WEAT, as given by Caliskan, Bryson, and Narayanan (2017):\n$\\frac{\\text{mean}_{x \\in X} s(x, A, B) - \\text{mean}_{y \\in Y} s(y, A, B)}{\\text{std\\_dev}_{w \\in X \\cup Y} s(w, A, B)}$  (1)\nwhere A and B are attributes, X and Y are target groups, and association s() for an embedding w is:\n$\\text{mean}_{a \\in A} \\text{cos}(\\vec{w}, \\vec{a}) - \\text{mean}_{b \\in B} \\text{cos}(w, b)$ (2)\nThis means that the association s() for each target stimulus represented by w is equal to its average association with the attribute stimuli in A, minus its average association with the attribute stimuli in B. The EAT returns an effect size (Cohen's d (Cohen 1992)), and a p-value from a permutation test:\n$\\text{Pri}[s(X_i, Y_i, A, B) > s(X, Y, A, B)]$ (3)\nwhich shuffles the words of the target groups to determine the unlikeliness of the test statistic s(X, Y, A, B), given by:\n$\\sum_{x \\in X} s(x, A, B) - \\sum_{y \\in Y} s(y, A, B)$ (4)\nLevel 2 We introduce Level 2 of the ML-EAT, which quantifies the differential association of a single Target concept T with attributes A and B:\n$d_{A,B,T} = \\frac{\\text{mean}_{a \\in A} u(T, a) - \\text{mean}_{b \\in B} u(T, b)}{\\text{std\\_dev}_{x \\in A \\cup B} u(T, x)}$ (5)\nwhere the association u() for an attribute embedding a with the target group T is given by:\n$\\text{mean}_{t \\in T} \\text{cos}(t, a)$ (6)\nLike Level 1, Level 2 returns an effect size (Cohen's d (Cohen 1992)) and a p-value from a permutation test:\n$\\text{Pri}[u(A_i, B_i, T) > u(A, B, T)]$ (7)\nwhich shuffles the words of the attribute groups to determine the unlikeliness of the test statistic, defined as:\n$\\sum_{a \\in A} u(T, a) - \\sum_{b \\in B} u(T, b)$ (8)\nNote that Level 2 computes the differential association between a target group and two attributes, rather than an attribute and two targets. This design is intentional, as it allows the ML-EAT to answer whether a target group like Young names, for example, is associated with pleasantness or unpleasantness, without reference to another target group, such"}, {"title": "EAT Patterns", "content": "We introduce EAT patterns to provide a taxonomy for describing biases quantified by Level 2. EAT patterns define EAT measurements in terms of Direction, based on whether the target groups in an EAT are individually associated (i.e., exhibit a significant p-value and at least a small positive effect size) with the same attribute group, or with differing attribute groups. An EAT exhibits one of the following four categories of Direction:\n\u2022 Divergent: X and Y are associated with differing attributes (e.g., X with A, Y with B).\n\u2022 Uniform: X and Y are associated with the same attribute (e.g., X with A, and Y with A).\n\u2022 Singular: Either X or Y is associated with an attribute, while the other target group is not (e.g., X with A, Y with neither A nor B).\n\u2022 Non-Directional: neither X nor Y is associated with either A or B.\nEAT Patterns describe an EAT's target-attribute associations by prepending them to the Direction. An EAT with Singular Direction wherein the only significant Level 2 association occurs between Target Y and Attribute B exhibits a BY-Singular EAT pattern. An EAT with Uniform Direction and significant Level 2 associations both between X and A and between Y and A exhibits an A-Uniform EAT pattern. Describing an EAT that exhibits a Divergent pattern is slightly different: if significant Level 2 associations occur between X and A and between Y and B, the EAT exhibits an AB-Divergent pattern; if associations occur between X and B and between Y and A, the EAT exhibits a BA-Divergent pattern. Societal biases quantified using Embedding Association Tests can be described more transparently by employing the vocabulary of EAT patterns; for example, describing the outcome of the Male/Female, Science/Arts EAT as AX-Singular communicates that Science (X) is differentially associated with Male (A), but Arts (Y) is associated neither with Male (A) nor with Female (B). The granular information provided by EAT patterns allows social scientists to observe more about the nature of a bias than can be interpreted via the single effect size and p-value returned by a traditional EAT. As demonstrated in the Results section and illustrated in Figure 2, nearly all EAT patterns consistent with a positively signed Level 1 effect size do in fact occur in the tests performed by Caliskan, Bryson, and Narayanan (2017), indicating that the ML-EAT can provide additional insights about diverse forms of societal bias, even where EATs have been taken previously."}, {"title": "Results", "content": "We apply the ML-EAT to three language technologies: static and diachronic word embeddings, GPT-2 language models and CLIP language-and-image models. Our analysis shows that a wide variety of EAT patterns occur even when Level 1 effect sizes are uniformly large, positive, and statistically significant; that Level 2 effect sizes can inform the interpretation of historical biases; and that Levels 2 and 3 of the ML-EAT can provide insight into the effects of prompting, and can surface anisotropy that may render EATs unreliable."}, {"title": "Empirical Analysis: GloVe Embeddings", "content": "Table 1 presents the results of the ML-EAT applied to the ten word embedding association tests of Caliskan, Bryson, and Narayanan (2017) and demonstrates that a wide variety of EAT patterns can result in a large, statistically significant Level 1 effect size (equivalent to the traditional WEAT effect size). Four tests exhibit Singular Direction:\n\u2022 The EA/AA 16 P/U 8 test exhibits an + AX-Singular pattern, indicating that European-American is significantly associated with Pleasantness, and African-American with neither Pleasantness nor Unpleasantness.\n\u2022 The Science/Arts Male/Female test exhibits an AX-Singular pattern, indicating that Science is associated with Male, and Arts with neither Male nor Female.\n\u2022 The Mental/Physical Temporary/Permanent test exhibits a BY-Singular pattern, indicating that Physical is associated with Permanent, and Mental with neither Temporary nor Permanent.\n\u2022 The Male/Female Career/Family test exhibits a BY-Singular pattern, indicating that Female is associated with Family, and Male with neither Career nor Family. The Young/Old test exhibits an A-Uniform pattern, indicating that Young and Old are both differentially associated with Pleasantness; however the magnitude of association is greater for Young (1.09 vs. 0.94). The most common EAT pattern observed is Non-Directional, exhibited by the first two European American/African American PU/25 tests"}, {"title": "Empirical Analysis: HistWords Embeddings", "content": "Among the most common uses of the EAT in computational social science is to observe change in societal biases over time (Charlesworth, Caliskan, and Banaji 2022; Borenstein et al. 2023; Betti, Abrate, and Kaltenbrunner 2023). To illustrate how the ML-EAT can inform such studies, we quantified gender bias using the Math/Arts Male/Female EAT in the HistWords embeddings (Hamilton, Leskovec,"}, {"title": "Empirical Analysis: Prompting in CLIP", "content": "In modern zero-shot language-and-image models like CLIP, the choice of prompt can impact the cosine similarities returned by the model. For example, Radford et al. (2021) suggested adding the prefix \"a photo of a class\" to prompts when using CLIP in the zero-shot setting to improve performance when classifying images. Following the IAT, wherein human reaction times are measured in response to the appearance of pairs of individual words on a screen (Greenwald, McGhee, and Schwartz 1998), EATs typically add little context when measuring semantic associations in language technologies,"}, {"title": "Empirical Analysis: Anisotropy in GPT-2", "content": "Table 3 describes results of the ML-EAT in both the GPT-2 Base model (124 million parameters) and the GPT-2 XL model (1.5 billion parameters) measured using the prompting approach of May et al. (2019), wherein the model receives the prompt \"This is [stimulus]\" to accord with its training objective. We focus not on the EAT patterns in this case, but on the Level 3 results, which provide evidence of anisotropy (directional uniformity, based on cosine similarity close to 1.0). Given that prior work finds that high anisotropy obscures the semantic properties of contextual word embeddings (Mu and Viswanath 2018; Timkey and van Schijndel 2021), one might avoid relying on the Level 1 and Level 2 measurements for which these cosine similarities are components. Cosine similarities in GPT-2 XL, on the other hand, may exhibit mild anisotropy, but they also exhibit more variance than the smaller GPT-2 model. While EAT patterns are mostly nondirectional, Level 1 effects are large and significant in the XL model, consistent with results from static word embeddings and with the studies of implicit bias in human subjects."}, {"title": "Discussion", "content": "Reporting outcomes using the ML-EAT can increase the transparency of research that employs EATs. Rather than attempting to explain the meaning of a single summary statistic and significance test, researchers can drawn on a categorical and visual vocabulary with which to communicate the findings of their work. Given the variance observed in the Level 2 results obtained using stimuli employed in prior studies (even with uniformly large Level 1 effect sizes), describing intermediate results via the ML-EAT might be adopted as a best practice when reporting the outcome of an EAT."}, {"title": "Motivating Interpretable Bias Measurement", "content": "Studies employing the EAT inform how social scientists understand human attitudes (Charlesworth, Caliskan, and Banaji 2022), and ongoing work uses these measurements to predict human bias (Bhatia and Walasek 2023). Ensuring that findings related to societal or domain-specific (e.g., legal, medical, etc.) bias are well-understood is essential not only for the integrity of the scientific record but for the decisions societies may make on the basis of that data. Scholars including Greenwald et al. (2022) have suggested that implicit bias might be approached as a public health problem, and mitigated using preventative approaches \u201cto disable the path from implicit biases to discriminatory outcomes.\u201d Where an EAT is presented as evidence of an implicit bias in need of redress through public health measures, researchers would be well-served by transparent and interpretable methods. Where the ML-EAT informs how intrinsic bias is interpreted, it may also provide information for how bias might be addressed in the embedding itself. In the case of the tests of racial bias (EA/AA Names) in the GloVe embeddings, Level 2 indicates that there is no significant association of African American names with pleasantness or unpleasantness, and Level 3 indicates that this results from a lack of similarity (due to lack of co-occurrence) with words in either attribute A or B. This suggests that bias quantified by the EAT might be mitigated via more diverse and representative training data, rather than by aggressively pruning the dataset, a process which can exclude diverse voices (Dodge et al. 2021)."}, {"title": "Improving the Robustness of the EAT", "content": "The ML-EAT also helps to improve the robustness of EAT-based measurements by introducing a generalization of the SC-EAT in Level 2. Generalizing this test such that the target"}, {"title": "Intrinsic Bias and Application Bias", "content": "Prior work documenting limitations of EATs has largely focused on evidence that intrinsic biases do not transfer to downstream tasks (Goldfarb-Tarrant et al. 2020). While it is not our central concern, we note that, in cases where cosine similarity has an explicitly defined function in a model, biases measured transparently using well-designed EATs will necessarily transfer downstream. This occurs when models like CLIP are used in a zero-shot setting, such that a cosine similarity between text and image is converted into a probability for use in classification (Radford et al. 2021). While intrinsic measurements may not predict application bias in many cases, there remain NLP applications that motivate the transparency and interpretability of EAT measurements."}, {"title": "Limitations and Future Work", "content": "While the ML-EAT renders bias more observable, careful curation of stimuli remains necessary to ensure validity (Caliskan et al. 2022). Ensuring that those stimuli are globally representative is an ongoing challenge for studies employing EATs, as recent research contends that stimuli used in most EATs reflect a western-centric bias that fails to capture biases faced by indigenous populations around the world (Yogarajan, Dobbie, and Gouk 2023). Moreover, while the ML-EAT is modular with differing mathematical definitions of bias, its levels are not adaptable for some modified versions of the EAT, such as the CEAT, which computes associations using thousands of sentences (Guo and Caliskan 2021). Future work might extend the ML-EAT to such tests and uncover new patterns of bias. Finally, our research addresses limitations of observability of bias in EATs, rather than downstream propagation of intrinsic bias. Future work might explore whether ML-EAT measurements can predict application bias."}, {"title": "Conclusion", "content": "We introduced the ML-EAT, a three-level measurement of intrinsic bias intended to improve the transparency and observability of bias measurement in social science, and appropriate for technologies ranging from static and diachronic word embeddings to zero-shot language-and-image models. We further introduced a taxonomy of nine distinct EAT patterns which we showed occur in prior EATs applied to word embeddings, alongside the EAT-Map, an intuitive visualization for EAT patterns. The ML-EAT provides greater transparency when employing language technologies to understand human minds and societies, an increasing concern as such measurements are used not only to observe human bias, but to predict it (Bhatia and Walasek 2023), and perhaps even to try to prevent it (Greenwald et al. 2022)."}, {"title": "Researcher Positionality", "content": "Two of the authors of this research have an extensive background in machine learning and quantitative studies of AI bias, and a third author has extensive experience with human-computer interaction, including statistical studies of deceptive design. We sought to include a variety of perspectives on this project, as the method we intended to develop needed to be both statistically rigorous, yet also approachable and interpretable for researchers hoping to study bias in AI."}, {"title": "Ethical Considerations", "content": "We note that, while we believe reporting results using the ML-EAT will help to make bias research more transparent and interpretable, simply using the ML-EAT rather than the WEAT will not guarantee full transparency in research practices. Researchers must also choose ethical ways of selecting stimuli for WEAT tests, and for determining the number of stimuli to include, which can impact the statistical power of the test. Though pre-registration is sometimes employed for psychological experiments, including those involving word embeddings, the easy availability of language models may render this approach less effective than it is with human subjects experiments, which carry much more notable startup costs. Future work might consider ethical approaches to social scientific experiment design with modern language technologies."}, {"title": "Adverse Impacts", "content": "While we have not produced any new technology in this work, individuals could use our method for ends we have not intended, such as exploiting biases identified with the ML-EAT to further marketing campaigns or to produce misinformation targeted to societal vulnerabilities. We hope and expect that most uses of the method will be to support more transparent and interpretable studies of bias in AI."}]}