{"title": "ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science", "authors": ["Robert Wolfe", "Alexis Hiniker", "Bill Howe"], "abstract": "This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.", "sections": [{"title": "Introduction", "content": "Computational methods that quantify societal biases using language technologies like word embeddings (Mikolov et al. 2013), generative language models (Radford et al. 2018), and multimodal language-and-image models (Radford et al. 2021) have been widely adopted by social scientists, who leverage the reflection of society captured by these technologies to observe implicit and explicit societal biases where human-subjects experiments are infeasible or prohibitively expensive (Durrheim et al. 2023; Kennedy et al. 2021). Social scientists have employed word embeddings in particular to analyze diachronic historical changes in human biases and norms (Garg et al. 2018), to study variations in gender biases across numerous languages (Lewis and Lupyan 2020), to compare implicit biases in adult and children's language corpora (Charlesworth et al. 2021), and to validate longstanding theories about societal biases, such as the masculine default (Caliskan et al. 2022; Bailey, Williams, and Cimpian 2022). Bhatia and Walasek (2023) employ such techniques to predict human biases, potentially facilitating mitigations at the societal scale. Among the most widely adopted bias measurement methods in computational social science is the Word Embedding Association Test (WEAT) (Caliskan, Bryson, and Narayanan 2017), a statistical technique grounded in the Implicit Association Test (IAT), a widely used measurement of unconscious bias in human subjects (Greenwald, McGhee, and Schwartz 1998). The WEAT quantifies bias based on the differential cosine similarity of two target groups X and Y (such as Science vs. Art) with two attribute groups A and B (such as Male vs. Female, for a common test of gender bias).\nYet the WEAT also suffers from a limitation common in AI evaluation: it is an aggregate metric (Burnell et al. 2023), averaging over many sub-measurements between groups of words to produce a single summary statistic. Explaining a bias quantified by the WEAT is thus not straightforward. In plain English, a statistically significant WEAT indicates that the X target group is more associated with A relative to B than the Y target group is more associated with A relative to B. While effective for surfacing implicit biases, the method also raises questions about whether and to what extent each target group is individually associated with A or B. Consider the test of age bias presented by Caliskan, Bryson, and Narayanan (2017), which sets group A to Pleasantness, B to Unpleasantness, X to Young Names, and Y to Old Names and returns a large, statistically significant effect size of 1.21. While one might intuitively interpret the result of the test to mean that Young Names are associated with Pleasantness, and Old Names with Unpleasantness, examining component cosine similarities used to compute the WEAT reveals that both Young Names (X) and Old Names (Y) are associated with Pleasantness (A). On the other hand, consider the WEAT setting A to Pleasantness, B to Unpleasantness, X to Instruments, and Y to Weapons, which also returns a large, significant effect size, of 1.53. Inspection of component cosine similarities reveals that, in this case, Instruments are associated with Pleasantness, while Weapons are associated with Unpleasantness. Though the two WEATs return effect sizes with the same sign and similarly large magnitudes, the characteristics of their underlying biases, and the corresponding sociological interpretations of the results, differ significantly. Given the wide range of psychological and computational"}, {"title": "Related Work", "content": "In reviewing the related on work on Embedding Association Tests (EATs), we describe the models and domains in which EATs are employed; then provide a detailed overview of their applications in computational social science; and finally review their limitations as described in prior work."}, {"title": "Embedding Association Tests", "content": "Caliskan, Bryson, and Narayanan (2017) introduce the Word Embedding Association Test (WEAT), a measurement of intrinsic bias in word embeddings drawing on the design of the Implicit Association Test (IAT), a method for studying implicit (unconscious) bias in human subjects (Greenwald, McGhee, and Schwartz 1998). The WEAT quantifies the relative association of two target groups (such as Science and Art) with two attribute groups (such as Male and Female), and, like the IAT, returns an effect size (Cohen's d) and a p-value. Caliskan, Bryson, and Narayanan (2017) used the WEAT to replicate the results of ten IATs reflecting gender and racial biases, among others, in the GloVe embeddings of Pennington, Socher, and Manning (2014). Caliskan, Bryson, and Narayanan (2017) also introduced the Single Category SC-WEAT, which quantifies the differential association of a single word with two attribute groups. The SC-WEAT was introduced as part of a measurement called the Word Embedding Factual Association Test, and further clarified in an analysis of androcentric gender bias by Caliskan et al. (2022). The WEAT was extended to transformer models by May et al. (2019), who introduced the Sentence Embedding Association Test (SEAT), a sentence-level WEAT employing semantically neutral prompts, and by Kurita et al. (2019), who tied bias measurement to the model's pretraining objective. Guo and Caliskan (2021) introduced the Contextualized Embedding Association Test (CEAT), which measured embedding bias at the word level and modeled contextualization as a random effect. EATs are also used in computer vision and multimodal language-and-image models. Steed and Caliskan (2021) introduced the Image Embedding Association Test (iEAT), which quantified bias in self-supervised image encoders such as SimCLR (Chen et al. 2020b) and iGPT (Chen et al. 2020a). Ross, Katz, and Barbu (2020) introduced the Grounded-WEAT for grounded language-and-image models, while (Wolfe and Caliskan 2022a) use an EAT to quantify bias in CLIP language-and-image models (Radford et al. 2021), and Hausladen et al. (2023) employ the SC-EAT to perform a causal analysis of social bias in language-and-image models. Finally, Slaughter et al. (2023) introduce the SpEAT, an EAT for quantifying intrinsic bias in pretained speech processing models such as wav2vec2 (Baevski et al. 2020) and OpenAI Whisper (Radford et al. 2023).\nWhile most EATs employ cosine similarity to measure association between target and attribute stimuli, recent work explores alternatives. Omrani Sabbaghi, Wolfe, and Caliskan (2023) employ an algebraic definition of bias similar to that of Bolukbasi et al. (2016), but use an EAT-based formula to obtain an effect size and p-value. Bai et al. (2024) assess implicit bias using the textual output of ostensibly debiased generative language models by employing a prompt-based analogue of the IAT."}, {"title": "Applications of EATs in Social Science", "content": "Social scientists use word embeddings and EATs to study phenomena that are impossible or financially infeasible to measure solely through direct experimental methods. Many studies leverage the societal scale of the data used for training word embeddings to make broad inferences about society that would be unavailable in a small-N psychological study. Caliskan et al. (2022) demonstrate a masculine default in the English-language internet using EATs computed for every word in the vocabulary of pretrained GloVe and FastText embeddings, while Bailey, Williams, and Cimpian (2022) use word embeddings to demonstrate the implicit equivalence of the concept of \"person\" with \"men.\" Napp (2023) use EAT measurements to contend that gender stereotypes are stronger in countries that are more economically developed and individualistic. Finally, Schmahl et al. (2020) use the EAT to study changes in gender bias on Wikipedia, informing their suggestions for reducing bias on the platform. Other research employs word embeddings for previously untenable cross-cultural analyses of human attitudes. For example, Mukherjee et al. (2023) employ EAT to measure biases related to albeism, immigration, and education across 24 languages.\nEmbeddings of historical data have also provided a means to study human societies that no longer exist and are thus unavailable for direct study. Charlesworth, Caliskan, and Banaji (2022) use word embeddings to measure changes in stereotypes about social groups over 200 years. Borenstein et al. (2023) use the EAT to study intersectional biases in word embeddings trained on newspapers from the 18th and 19th century. Sunsay (2023) use the EAT to study the disease avoidance theory of xenophobia, measuring EAT scores in 19th and 20th century travel literature to test western associations of indigenous people disgust words that would suggest disease avoidance. Leach, Kitchin, and Sutton (2023) employ the EAT to argue that \"language has changed in a way that reflects greater concern for others,\" supporting the idea that the societal \u201cmoral circle\u201d expanded during the 19th and 20th centuries to include more groups of people, in addition animals and the environment. Guan et al. (2024) use cosine similarities to measure the evolution of color associations (e.g., association of the color red with heat) over 200 years, while Betti, Abrate, and Kaltenbrunner (2023) use the EAT to measure sexism in fifty years of English song lyrics. Finally, Wolfe, Banaji, and Caliskan (2022) find evidence of the historical bias of hypodescent in CLIP models.\nResearch also employs EATs to study present-day bias in domains such as law and medicine. Rios, Joshi, and Shin (2020) use the WEAT to measure gender bias in biomedical research, finding that traditional gender stereotypes have declined over time, but that specific medical conditions like body dysmorphia still exhibit high gender bias. Cobert et al. (2024) use cosine similarities to measure implicit racial biases in ICU notes. In the legal domain, Matthews, Hudzina, and Sepehr (2022) use the EAT to study biases in word embeddings trained on corpora of legal opinions, and Dutta et al. (2023) use WEAT scores to quantify gender bias in Indian divorce court proceedings. Moreover, amid increasing interest in using AI to measure aspects of human society (Park et al. 2023; Shanahan, McDonell, and Reynolds 2023;"}, {"title": "Limitations of EATS", "content": "The EAT faces challenges related both to its mathematical definition and its predictive value in NLP applications. Social scientists who choose not to use the EAT sometimes note that its design, which mimics the differential construction of the IAT, can cause difficulties in observing and interpreting bias. Bailey, Williams, and Cimpian (2022) use the difference in raw cosine similarities to observe asymmetrical gender biases, noting that the WEAT is better applied to symmetrical patterns of association. Ethayarajh, Duvenaud, and Hirst (2019) contend that the standardization of the WEAT, in dividing by the joint standard deviation of word associations, can obscure differences in underlying cosine similarities. Moreover, anisotropy (directional uniformity) in deep learning models (Mu and Viswanath 2018) can distort intrinsic semantic measurements (Timkey and van Schijndel 2021), necessitating postprocessing of EATs (Wolfe and Caliskan 2022b).\nRecent work suggests that EATs have limited predictive value for bias in downstream NLP tasks. Goldfarb-Tarrant et al. (2020) find that biases observed with the WEAT are not correlated with application biases in tasks like coreference resolution. In a study of downstream propagation using transformer language models, Orgad, Goldfarb-Tarrant, and Belinkov (2022) propose an information-theoretic framework rather than an EAT. Cabello, J\u00f8rgensen, and S\u00f8gaard (2023) show that association bias and fairness are uncorrelated, but also provide sociological evidence that the two kinds of metrics should be expected to be independent of each other.\nWe are concerned with uses of the EAT in social science to study human attitudes. To that end, we design an interpretable EAT, rather than an EAT predictive of downstream bias."}, {"title": "Models and Data", "content": "This research introduces the ML-EAT and applies it to word embeddings, GPT-2, and CLIP, adapting stimuli from prior studies of implicit bias in NLP and computer vision."}, {"title": "Pretrained Models", "content": "We apply the ML-EAT to the below language technologies.\n\u2022 GloVe Word Embeddings: Global Vectors for word representation (GloVe) train on the co-occurrence matrix of a text corpus, such that the vector representation of a word is learned based on the words it is most likely to occur around (Pennington, Socher, and Manning 2014). Caliskan, Bryson, and Narayanan (2017) introduced the WEAT by presenting results on 300-dimensional GloVe vectors trained on the 840-billion token Common Crawl.\n\u2022 HistWords Embeddings: HistWords refers to sets of 20 word embeddings in four languages trained on ten-year slices of historical language corpora ranging between the years 1800 and 2000 (Hamilton, Leskovec, and Jurafsky 2016). Hamilton, Leskovec, and Jurafsky (2016) introduced HistWords to prove that more frequently used words exhibit less semantic change over time, and that polysemous words exhibit faster semantic change. We apply the ML-EAT to the English language HistWords embeddings trained using Word2Vec (SGNS) (Mikolov et al. 2013) on Google books (all genres) (Lin et al. 2012).\n\u2022 GPT-2 Language Models: GPT-2 (\u201cGenerative Pre-trained Transformer\u201d) is a causally masked transformer (Vaswani et al. 2017) language model trained to predict the next word in a sequence (Radford et al. 2019). This research studies the four pretrained GPT-2 models (Base, Medium, Large, and XL) available via the Transformers library (Wolf et al. 2020), which were pretrained on OpenAI's WebText dataset, a collection of webpages scraped from highly rated outbound links on Reddit.\n\u2022 CLIP Language-and-Image Models: CLIP (\u201cContrastive Language Image Pretraining\") is a multimodal language-and-image model, which classifies images based on their cosine similarity with text labels (Radford et al. 2021). This research reports results under varying prompts from the CLIP-ViT-L14-336 model, the best performing OpenAI-trained CLIP model available. CLIP-ViT-L14-336 is trained on OpenAI's WebImageText (WIT) dataset, a collection of 400 million pairs of web-scraped images and accompanying captions (Radford et al. 2021).\nGPT-2 embeddings are obtained from the model's top layer, consistent with both May et al. (2019) and Guo and Caliskan (2021). CLIP embeddings are collected after projection to the model's multimodal text-and-image latent space."}, {"title": "EAT Stimuli", "content": "An EAT employs four groups of words or images (called \u201cstimuli\u201d, drawing on the test's psychological foundations in the IAT (Greenwald, McGhee, and Schwartz 1998)), each representing a concept. For example, the EAT demonstrating that flowers are favored over insects uses word lists to represent the concepts of Flowers, Insects, Pleasant, and Unpleasant. Each EAT includes two \"target\" groups, X and Y (Flowers and Insects), which are tested for association with two \"attribute\" groups, A and B (Pleasant and Unpleasant) (Caliskan, Bryson, and Narayanan 2017). The two target groups contain the same number of stimuli, as do the two attribute groups. Groups must contain at least eight stimuli to adequately represent a concept (Caliskan et al. 2022).\nWe use the stimuli for the tests of implicit bias specified by Caliskan, Bryson, and Narayanan (2017) when applying the ML-EAT to GloVe and GPT-2. We applied the Math/Arts Male/Female EAT to the HistWords embeddings, replacing three stimuli because their L2 norms were zero-valued (preventing the computation of cosine similarity) in several HistWords embeddings. We substituted \u201cmusic\u201d for \"symphony\u201d; \"mathematics\u201d for \"math\"; and \"calculation\u201d for \u201ccalculus.\u201d Tests of bias in CLIP utilize the word stimuli of Caliskan, Bryson, and Narayanan (2017) to represent Pleasant and Unpleasant, and image stimuli from Steed and Caliskan (2021)."}, {"title": "Approach", "content": "The ML-EAT is defined using three levels of measurement, with a taxonomy of nine EAT patterns for describing biases it quantifies. We first describe the test itself, then introduce the EAT-Map visualization and EAT pattern taxonomy."}, {"title": "Defining the ML-EAT", "content": "The ML-EAT computes bias at three levels of increasing granularity. Level 1 returns the traditional standardized effect size quantifying the differential association between two target concepts with two attribute concepts; Level 2 returns two effect sizes quantifying the differential association of each target group individually with the two attribute groups; and Level 3 returns four means and corresponding standard deviations describing the non-differential association of each target group and attribute group.\nLevel 1 The first level of the ML-EAT is equivalent to the WEAT, as given by Caliskan, Bryson, and Narayanan (2017):\n$\\frac{mean_{x\\in X} s(x, A, B) \u2013 mean_{y\\in Y} s(y, A, B)}{std\\_dev_{w\\in X \\cup Y} s(\\omega, A, B)}$\nwhere A and B are attributes, X and Y are target groups, and association s() for an embedding w is:\n$mean_{a\\in A} cos(\\vec{w}, \\vec{a}) \u2013 mean_{b\\in B} cos(w, b)$\nThis means that the association s() for each target stimulus represented by w is equal to its average association with the attribute stimuli in A, minus its average association with the attribute stimuli in B. The EAT returns an effect size (Cohen's d (Cohen 1992)), and a p-value from a permutation test:\n$Pri[s(X_i, Y_i, A, B) > s(X, Y, A, B)]$\nwhich shuffles the words of the target groups to determine the unlikeliness of the test statistic s(X, Y, A, B), given by:\n$\\frac{\\sum_{x \\in X} s(x, A, B) \u2013 \\sum_{y \\in Y} s(y, A, B)}{\\Sigma(T, a) \u2013 \\Sigma(T, b)}$\nLevel 2 We introduce Level 2 of the ML-EAT, which quantifies the differential association of a single Target concept T with attributes A and B:\n$d_{A,B,T} = \\frac{mean_{a\\in A} u(T, a) \u2013 mean_{b\\in B} u(T, b)}{std\\_dev_{x\\in A \\cup B} u(T, x)}$\nwhere the association u() for an attribute embedding a with the target group T is given by:\n$mean_{t\\in T} cos(t, a)$\nLike Level 1, Level 2 returns an effect size (Cohen's d (Cohen 1992)) and a p-value from a permutation test:\n$Pri[u(A_i, B_i, T) > u(A, B,T)]$\nwhich shuffles the words of the attribute groups to determine the unlikeliness of the test statistic, defined as:\n$\\frac{\\sum_{a\\in A} u(T, a) \u2013 \\sum_{b\\in B} u(T, b)}{(8)}$\nNote that Level 2 computes the differential association between a target group and two attributes, rather than an attribute and two targets. This design is intentional, as it allows the ML-EAT to answer whether a target group like Young names, for example, is associated with pleasantness or unpleasantness, without reference to another target group, such"}, {"title": "EAT Patterns", "content": "We introduce EAT patterns to provide a taxonomy for describing biases quantified by Level 2. EAT patterns define EAT measurements in terms of Direction, based on whether the target groups in an EAT are individually associated (i.e., exhibit a significant p-value and at least a small positive effect size) with the same attribute group, or with differing attribute groups. An EAT exhibits one of the following four categories of Direction:\n\u2022 Divergent: X and Y are associated with differing attributes (e.g., X with A, Y with B).\n\u2022 Uniform: X and Y are associated with the same attribute (e.g., X with A, and Y with A).\n\u2022 Singular: Either X or Y is associated with an attribute, while the other target group is not (e.g., X with A, Y with neither A nor B).\n\u2022 Non-Directional: neither X nor Y is associated with either A or B.\nEAT Patterns describe an EAT's target-attribute associations by prepending them to the Direction. An EAT with Singular Direction wherein the only significant Level 2 association occurs between Target Y and Attribute B exhibits a BY-Singular EAT pattern. An EAT with Uniform Direction and significant Level 2 associations both between X and A and between Y and A exhibits an A-Uniform EAT pattern. Describing an EAT that exhibits a Divergent pattern is slightly different: if significant Level 2 associations occur between X and A and between Y and B, the EAT exhibits an ABDivergent pattern; if associations occur between X and B and between Y and A, the EAT exhibits a BA-Divergent pattern.\nSocietal biases quantified using Embedding Association Tests can be described more transparently by employing the vocabulary of EAT patterns; for example, describing the outcome of the Male/Female, Science/Arts EAT as AX-Singular communicates that Science (X) is differentially associated with Male (A), but Arts (Y) is associated neither with Male (A) nor with Female (B). The granular information provided by EAT patterns allows social scientists to observe more about the nature of a bias than can be interpreted via the single effect size and p-value returned by a traditional EAT. As demonstrated in the Results section and illustrated in Figure 2, nearly all EAT patterns consistent with a positively signed Level 1 effect size do in fact occur in the tests performed by Caliskan, Bryson, and Narayanan (2017), indicating that the ML-EAT can provide additional insights about diverse forms of societal bias, even where EATs have been taken previously."}, {"title": "Results", "content": "We apply the ML-EAT to three language technologies: static and diachronic word embeddings, GPT-2 language models, and CLIP language-and-image models. Our analysis shows that a wide variety of EAT patterns occur even when Level 1 effect sizes are uniformly large, positive, and statistically significant; that Level 2 effect sizes can inform the interpretation of historical biases; and that Levels 2 and 3 of the ML-EAT can provide insight into the effects of prompting, and can surface anisotropy that may render EATs unreliable."}, {"title": "Empirical Analysis: GloVe Embeddings", "content": "Table 1 presents the results of the ML-EAT applied to the ten word embedding association tests of Caliskan, Bryson, and Narayanan (2017) and demonstrates that a wide variety of EAT patterns can result in a large, statistically significant Level 1 effect size (equivalent to the traditional WEAT effect size). Four tests exhibit Singular Direction:\n\u2022 The EA/AA 16 P/U 8 test exhibits an + AX-Singular pattern, indicating that European-American is significantly associated with Pleasantness, and African-American with neither Pleasantness nor Unpleasantness.\n\u2022 The Science/Arts Male/Female test exhibits an AXSingular pattern, indicating that Science is associated with Male, and Arts with neither Male nor Female.\n\u2022 The Mental/Physical Temporary/Permanent test exhibits a BY-Singular pattern, indicating that Physical is associated with Permanent, and Mental with neither Temporary nor Permanent.\n\u2022 The Male/Female Career/Family test exhibits a BYSingular pattern, indicating that Female is associated with Family, and Male with neither Career nor Family.\nThe Young/Old test exhibits an A-Uniform pattern, indicating that Young and Old are both differentially associated with Pleasantness; however the magnitude of association is greater for Young (1.09 vs. 0.94). The most common EAT pattern observed is Non-Directional, exhibited by the first two European American/African American PU/25 tests and the Math/Arts Male/Female test. Only the Flowers/Insects P/U25 and Instruments/Weapons P/U25 tests exhibit an AB-Divergent EAT pattern, a notable finding given that discussions of EAT results often suggest this pattern (i.e., X is differentially associated with A, while Y is differentially with B). That none of the results of the social bias tests in the GloVe embeddings exhibit an AB-Divergent EAT pattern highlights the need for descriptive reporting of EATS.\nInspection of Level 3 results reveals that small differences in cosine similarity distributions can yield large, statistically significant Level 1 effect sizes. Consider EATs exhibiting a Non-Directional pattern: in the Math/Arts, Male/Female test, the absolute difference in mean cosine similarity for Math is .01 greater with the Male attribute group (.10 vs. .09), while the absolute difference in mean cosine similarity for Arts is.01 greater with the Female attribute group (.23 vs. .24). Similarly, the mean cosine similarity for an African American names target group (Y in tests 3, 4, and 5) with any attribute group never exceeds .02 or falls below -.01, suggesting a paucity of co-occurrence data for African American names due to under-representation in the training data. This is also reflected in the non-significant Level 2 effect size A, B, Y for the African-American names target group. Nonetheless, Level 1 returns a large, significant effect size for these EATs.\nThat Level 1 picks up on small differences is a benefit of the EAT, and Level 1 is often consistent with tests of implicit bias in humans (Caliskan, Bryson, and Narayanan 2017). However, interpreting Level 1 without reference to Level 2 or 3 could lead to inaccurate conclusions about the direction of bias and the magnitude of absolute differences in underlying similarities between target and attribute groups."}, {"title": "Empirical Analysis: HistWords Embeddings", "content": "Among the most common uses of the EAT in computational social science is to observe change in societal biases over time (Charlesworth, Caliskan, and Banaji 2022; Borenstein et al. 2023; Betti, Abrate, and Kaltenbrunner 2023). To illustrate how the ML-EAT can inform such studies, we quantified gender bias using the Math/Arts Male/Female EAT in the HistWords embeddings (Hamilton, Leskovec,"}, {"title": "Empirical Analysis: Prompting in CLIP", "content": "In modern zero-shot language-and-image models like CLIP, the choice of prompt can impact the cosine similarities returned by the model. For example, Radford et al. (2021) suggested adding the prefix \"a photo of a class\" to prompts when using CLIP in the zero-shot setting to improve performance when classifying images. Following the IAT, wherein human reaction times are measured in response to the appearance of pairs of individual words on a screen (Greenwald, McGhee, and Schwartz 1998), EATs typically add little context when measuring semantic associations in language technologies, and usually attempt to measure associations between individual words or images. However, some recent research using language-and-image models departs from this (Wolfe et al. 2023), employing longer prompts like that recommended by Radford et al. (2021) in order to use the model as close to the way it was intended as possible.\nLevel 3 of the ML-EAT can surface the impact of prompts by revealing underlying CLIP cosine similarities. We study five positively-signed, statistically significant EATs obtained from the CLIP-ViT-L14-336 model using the language stimuli of Caliskan, Bryson, and Narayanan (2017) and the image stimuli Steed and Caliskan (2021), with results in Table 2. These EATs include a Flowers-Insects P/U25 test exhibiting the AB-Divergent EAT pattern (i.e., Flowers associated with Pleasantness, Insects with Unpleasantness); a test of White/Black P/U25 racial bias exhibiting the B-Uniform EAT pattern (i.e., both White and Black associated with Unpleasantness); a Thin/Heavy P/U25 test of weight bias exhibit-ing a BY-Singular EAT pattern (i.e., Heavy associated with Unpleasantness, Thin with neither Pleasant nor Unpleasant); a Male/Female Career/Family test of gender bias exhibiting an A-Uniform EAT pattern (i.e., both Male and Female associated with Career); and a Science/Arts Male/Female test of gender bias exhibiting a Non-Directional EAT pattern (neither target associated with an attribute).\nUsing prompts with CLIP can affect these measurements. In accordance with the suggestion of Radford et al. (2021) and in keeping with the the IAT, we used \"a picture that brings to mind [word]\" as the prompt for word stimuli in both the A and B attribute groups (not all image stimuli are photographs, so we prompted with \u201cpicture\u201d instead of \"photo\"). With the prompt, the mean cosine similarity increases for every target-attribute pairing. Observing the impact of prompting helps to understand the variance induced by a particular prompt, and reinforces that these EATs measure implicit bias: if the stimuli described exactly what was in the image, the cosine similarities would be higher, as they are proportional to probabilities in CLIP."}, {"title": "Empirical Analysis: Anisotropy in GPT-2", "content": "Table 3 describes results of the ML-EAT in both the GPT2 Base model (124 million parameters) and the GPT-2 XL model (1.5 billion parameters) measured using the prompting approach of May et al. (2019), wherein the model receives the prompt \"This is [stimulus]\" to accord with its training objective. We focus not on the EAT patterns in this case, but on the Level 3 results, which provide evidence of anisotropy (directional uniformity, based on cosine similarity close to 1.0). Given that prior work finds that high anisotropy obscures the semantic properties of contextual word embeddings (Mu and Viswanath 2018; Timkey and van Schijndel 2021), one might avoid relying on the Level 1 and Level 2 measurements for which these cosine similarities are components. Cosine similarities in GPT-2 XL, on the other hand, may exhibit mild anisotropy, but they also exhibit more variance than the smaller GPT-2 model. While EAT patterns are mostly nondirectional, Level 1 effects are large and significant in the XL model, consistent with results from static word embeddings and with the studies of implicit bias in human subjects."}, {"title": "Discussion", "content": "Reporting outcomes using the ML-EAT can increase the transparency of research that employs EATs. Rather than attempting to explain the meaning of a single summary statistic and significance test, researchers can drawn on a categorical and visual vocabulary with which to communicate the findings of their work. Given the variance observed in the Level 2 results obtained using stimuli employed in prior studies (even with uniformly large Level 1 effect sizes), describing intermediate results via the ML-EAT might be adopted as a best practice when reporting the outcome of an EAT."}, {"title": "Motivating Interpretable Bias Measurement", "content": "Studies employing the EAT inform how social scientists understand human attitudes (Charlesworth, Caliskan, and Banaji 2022), and ongoing work uses these measurements to predict human bias (Bhatia and Walasek 2023). Ensuring that findings related to societal or domain-specific (e.g., legal, medical, etc.) bias are well-understood is essential not only for the integrity of the scientific record but for the decisions societies may make on the basis of that data. Scholars including Greenwald et al. (2022) have suggested that implicit bias might be approached as a public health problem, and mitigated using preventative approaches \u201cto disable the path from implicit biases to discriminatory outcomes.\" Where an EAT is presented as evidence of an implicit bias in need of redress through public health measures, researchers would be well-served by transparent and interpretable methods.\nWhere the ML-EAT informs how intrinsic bias is interpreted, it may also provide information for how bias might be addressed in the embedding itself. In the case of the tests of racial bias (EA/AA Names) in the GloVe embeddings, Level 2 indicates that there is no significant association of African American names with pleasantness or unpleasantness, and Level 3 indicates that this results from a lack of similarity (due to lack of co-occurrence) with words in either attribute A or B. This suggests that bias quantified by the EAT might be mitigated via more diverse and representative training data, rather than by aggressively pruning the dataset, a process which can exclude diverse voices (Dodge et al. 2021)."}, {"title": "Improving the Robustness of the EAT", "content": "The ML-EAT also helps to improve the robustness of EATbased measurements by introducing a generalization of the SC-EAT in Level 2. Generalizing this test such that the target"}, {"title": "Researcher Positionality", "content": "Two of the authors of this research have an extensive background in machine learning and quantitative studies of AI bias, and a third author has extensive experience with humancomputer interaction, including statistical studies of deceptive design. We sought to include a variety of perspectives on this project, as the"}]}