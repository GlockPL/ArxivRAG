{"title": "TSPE: Task-Specific Prompt Ensemble for Improved Zero-Shot Audio Classification", "authors": ["Nishit Anand", "Ashish Seth", "Ramani Duraiswami", "Dinesh Manocha"], "abstract": "Audio-language models (ALMs) excel in zero-shot audio classification, a task where models classify previously unseen audio clips at test time by leveraging descriptive natural language prompts. We introduce TSPE (Task-Specific Prompt Ensemble), a simple, training-free hard prompting method that boosts ALEs' zero-shot performance by customizing prompts for diverse audio classification tasks. Rather than using generic template-based prompts like \u201cSound of a car\" we generate context-rich prompts, such as \"Sound of a car coming from a tunnel\". Specifically, we leverage label information to identify suitable sound attributes, such as \"loud\" and \"feeble\", and appropriate sound sources, such as \"tunnel\" and \"street\" and incorporate this information into the prompts used by Audio-Language Models (ALMs) for audio classification. Further, to enhance audio-text alignment, we perform prompt ensemble across TSPE-generated task-specific prompts. When evaluated on 12 diverse audio classification datasets, TSPE improves performance across ALMs by showing an absolute improvement of 1.23-16.36% over vanilla zero-shot evaluation.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent progress in multimodal language models (MLMs) has greatly advanced performance across multiple modalities and tasks [27]\u2013[31]. Trained on large datasets of audio-caption pairs, these models gain a broad understanding of audio concepts, allowing them to classify new audio categories in a zero-shot setting. This adaptability makes ALEs well-suited for dynamic environments with diverse and unfamiliar sounds.\nContrastive Learning-based Audio Language Models (ALM) like Contrastive Language-Audio Pre-training (CLAP) [19] learn a shared representation space between audio and text. This helps them generalize well and have good downstream performance in tasks like audio classification.\nVarious ALEs have been published in the literature over the past few years which perform well on tasks like audio classification and text-to-audio retrieval\nThese models are pre-trained on large audio-text datasets, enabling them to generalize well in diverse audio environ- ments. While they perform strongly in zero-shot audio clas- sification, this success often depends on extensive audio- text pairs or additional fine-tuning. Few efforts have focused on enhancing zero-shot classification for CLAP-like models without extra training. Some approaches, like Audio Prompt Learner [25] and TreffAdapter [26], improve performance but require additional training and introduce learnable parameters, which increase time and computational costs. Moreover, al- though effective on in-distribution tasks, they tend to perform poorly on out-of-distribution (OOD) audio classification tasks. Additionally, a common limitation in current methods is their dependence on generic prompts such as \u201csound of a \u00a1label\u00bf\". We find that these prompts do not transfer effectively across different downstream tasks and often need adaptation to be meaningful. For instance, in a musical genre classification task like GTZAN [6], the prompt \"sound of a rock\" is unclear and does not convey the intended category, whereas modifying it to \"sound of a rock music\" provides clarity for the model to understand genres in proper context.\nMain Contribution - To address this, we introduce TPSE (Task-Specific Prompt Ensemble), a training-free approach which improves the zero-shot audio classification performance of ALEs. It uses downstream task and label information to au- tomatically generate task-specific prompts for each class label. This is important in order to capture the semantic nuances in the audio-text alignment/space. Then, instead of using a single vanilla prompt, it uses prompt ensembling to learn a more semantically rich representation of the prompt, which helps it to better understand the correlation between the prompt's rich textual representation and the audio representation, thus improving the performance of the ALE on downstream audio classification. Our method does not require any fine-tuning or extra training for this performance improvement, and the highlight is that it performs well on audio classification on out- of-distribution (OOD) datasets as well. We conduct extensive experiments on 12 diverse audio classification tasks and show an absolute improvement of 1.23-16.36% over vanilla zero- shot evaluation."}, {"title": "II. RELATED WORK", "content": "Multimodal encoders for learning shared representations across modalities have shown significant promise. Building on contrastive pre-training methods from vision-language mod- els like CLIP [16], Audio-Language Models (ALMs) have achieved state-of-the-art zero-shot performance in audio clas- sification. Early models such as Wav2Clip [17] and Audio- CLIP [18] focused on aligning audio representations with categorical labels, while recent approaches like CLAP [19] map audio directly to textual descriptions, yielding substantial zero-shot gains. While hard prompting [24] has enhanced zero- shot abilities in vision-language models by using contextually rich prompt engineering [20], this technique remains largely unexplored for ALMs. Instead, prior ALMs have relied on compute-intensive methods, such as improving alignment ob- jectives [21] or scaling parameters and datasets [19]."}, {"title": "III. METHODOLOGY", "content": "We observe that the current prompts used for audio clas- sification tasks often fail to capture the diversity of audio labels. For instance, generic prompts like 'sound of a <label>' lack semantic coherence when <label> is a musical genre such as 'rock' or a location like 'beach.' This highlights the need to understand task-specific labels to create more effective prompts. To address this, we develop TSPE that uses the task and label knowledge to generate task-specific hard prompts. We achieve this by first categorizing labels into distinct groups based on their classification characteristics, such as the type of sound. Details of this classification are outlined below:\n\u2022 Musical Instruments Recognition This category com- prises of sounds from various musical instruments played in diverse settings, such as opera, street performances, and theater. It includes a wide range of instruments, including the piano, guitar, cymbals, drums, etc. Relevant datasets for this category include Beijing Opera [1], Mridangam Stroke [2], Mridangam Tonic [3], Nsynth Instrument [4], and Nsynth Source [5].\n\u2022 Music Genre Classification: This category involves classifying audio samples into distinct music genres, including classical, country, disco, hip-hop, blues, etc. [6].\n\u2022 Acoustic Scene Understanding This category refers to common urban sounds, such as those produced by buses, cars, jackhammers, drills, dog barks, elevators, crowded streets, subways, trucks, police sirens, motorcycles, air conditioners, engines idling, car horns, and street music as well as sounds commonly encountered in daily life, such as church bells, birds chirping, mouse clicks, ambi- ent office noise, cafes, trams, beaches, restaurants, hens, roosters, metro stations, parks, and city centers. Datasets used for this category include Cochlscene [7], ESC50 [8], TUT [9] and USD-8K (Urban Sounds) [10].\n\u2022 Impact and Emergency Sound: This category focuses on loud, sudden sounds such as explosions, gunshots, and sirens. The SESA dataset is commonly used for this task [11].\n\u2022 Non-Verbal Vocalization Sounds: This category in- cludes non-verbal vocal sounds such as coughing, sneez- ing, throat clearing, sniffing, sighing, and laughter. Rele- vant datasets for this category include Vocalsound [12]."}, {"title": "A. Grouping Diverse Audio Labels"}, {"title": "B. Task-Specific Prompt Generation", "content": "We begin by providing GPT-4 with information about task categories and their labels, along with examples of sound attributes like 'quiet,' 'loud,' 'muted,' and 'faint.' We also include examples of sound sources such as 'theater,' 'concert,' 'room,' 'opera,' and 'street.' We then request GPT-4 [13] to generate a list of 60 sound attributes and sources relevant to our task categories. Next, we manually map these sound attributes and sources to each task category, ensuring that the attributes and sources are contextually appropriate for the specific labels within each category.\nFor each task category, we then supply GPT-4 [13] with a prompt format and the list of attributes and sources we have mapped to that category. We ask it to generate 40 prompts using the following formats:\n\u2022 \"A <attribute> sound of a <label>\"\n\u2022 \"A sound of a <label> coming from a <source>\"\n\u2022 \"A <attribute> sound of a <label> can be heard from a <source>\""}, {"title": "C. Hard Prompting and Prompt Ensemble", "content": "As discussed, we employ hard prompting [14] over tech- niques like soft prompting or other learning-based methods be- cause those require retraining or fine-tuning [15], whereas our approach is training-free and enables zero-shot improvement. Our diverse set of prompts, built for different tasks, improves downstream performance by describing class labels in various ways, incorporating both their acoustic characteristics and sound sources.\nOur method, Task-Specific Prompt Ensemble (TSPE), sig- nificantly improves the zero-shot performance of Audio- Language Encoders for audio classification [23]. By using a unique set of prompts for each task category, we capture the subtle nuances and properties of sounds across diverse scenarios. This approach allows for more accurate seman- tic representation, as prompts like 'A melodious sound of <piano>' capture the nuances of the category label better than generic prompts like \u2018This is the sound of a <label>'. For each task category, we generate text embeddings for all prompts in its prompt set and then average these embeddings to enhance semantic representation. In summary, we first create task- specific prompts using hard prompting and then ensemble these prompts to improve zero-shot results."}, {"title": "D. Injecting sound attribute in hard prompting", "content": "From the initial set of 40 prompts generated by GPT-4 for each task category, we manually filter 20 prompts for each category, carefully checking for mismatches in sound attributes. We ensure that the sound attributes in each prompt are meaningfully related to the category labels for the task. For example, the prompt \"A melodious sound of a <label>\" is well-suited for the Music Instruments Classification task, as in \"A melodious sound of a <piano>\", which is semantically appropriate. However, \u201cA melodious sound of a <gunshot>\" would be mismatched and inappropriate for the Impact and Emergency Sound.\nSimilarly, prompts like \"A gentle sound of an explosion\" do not fit the Impact and Emergency Sound and would be better replaced with \"A gentle sound of a flute\" for the Music Instruments Classification task. This manual filtering step ensures that sound attributes align with the realistic qualities of each task category."}, {"title": "E. Injecting sound source information in hard prompting", "content": "During the manual filtering of the 40 prompts generated by GPT for each task, we carefully ensure that each sound source aligns naturally with the category labels. This alignment is essential because a mismatched sound source can degrade the coherence of the prompt, reducing its effectiveness in capturing the correct semantic context [22].\nFor instance, consider the prompt \"The sound of a violin can be heard from an orchestra.\" This is a more plausible pairing than alternatives like \u201cThe sound of a violin coming from a library\u201d or 'a zoo,' where the source does not fit the expected environment for a violin. Similarly, the prompt \u201cThe sound of an organ coming from the church\u201d aligns well with the category label organ, providing a realistic setting. In contrast, options like \u201cThe sound of an organ coming from an airport\u201d or an railway station would seem out of place, reducing the prompt's effectiveness in discriminating organ sound from a pool of sounds."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We evaluate our method across five tasks using twelve different datasets and two state-of-the-art Audio Language Encoders (ALEs), MS-CLAP'22 and MS-CLAP'23 [19], both developed by Microsoft and released in 2022 and 2023, respec- tively. For the Musical Instrument Recognition task, we test our model on multiple audio datasets that include a wide range of musical instruments. These datasets include Beijing Opera [1], Mridangam Stroke [2], Mridangam Tonic [3], Nsynth Instrument [4], and Nsynth Source [5]. For the Music Genre Classification task, we use the GTZAN dataset [6], which contains music from various genres, including Rock, Hip- Hop, Pop, and Country. For Acoustic Scene Understanding, we evaluate our model on multiple datasets, such as Cochlscene [7], USD-8K (Urban Sounds) [10], ESC50 [8], and TUT Urban Acoustic Scenes [9]. These datasets contain diverse sounds like air conditioners, car horns, trams, and metro stations. For Impact and Emergency Sound Classification, we use the SESA dataset [11], which includes emergency-related sounds like explosions, gunshots, and sirens. For Non-Verbal Vocalization Sound Classification, we test on the VocalSound dataset [12], which includes sounds such as sighing, sniffing, laughing, and sneezing."}, {"title": "V. RESULTS", "content": "Table I presents the effectiveness of our technique for Zero- Shot Audio Classification (ZSAC) applied to two state-of- the-art Audio-Language Models (ALMs), MSCLAP'22 and MSCLAP'23 [19]. We provide a comparison of our Task- Specific Prompt Ensemble method against vanilla prompts for ZSAC across five tasks and twelve audio classification datasets covering diverse sounds."}, {"title": "A. Result Analysis", "content": "TSPE is able to improve the performance of the models ranging from 1.27% to 16.36%, with an average improvement of 2.06% across all datasets on MSCLAP'23 and 1.89% on MSCLAP'22. We observe that on some datasets, performance decreases after applying TSPE. One reason could be that the prompts are not linguistically rich enough for that task and we can take this up as future work. Table II shows examples of prompts generated by TSPE for different tasks."}, {"title": "B. Hyper-Parameter Tuning", "content": "We select $K=20$ prompts from an initial set of 40 gener- ated by GPT-4, and perform an ablation study to determine the optimal value for $K$. Specifically, we evaluate the performance of TSPE on the VocalSound dataset with $K$ values of {5, 10, 15, 20, 25, 30} to examine how the number of prompts affects the performance of Audio-Language Models (ALMs) on zero-shot audio classification. Results indicate that ALM performance improves as $K$ increases up to 20, after which it declines. This drop in performance beyond $K=20$ may result from increased semantic noise due to the higher number of prompts. Fig. 2 illustrates the effect of different prompt counts on MSCLAP'23 for audio classification on the VocalSound dataset [12]."}, {"title": "VI. CONCLUSION", "content": "In this paper, we present TSPE, a task-specific hard prompt- ing method designed to enhance the zero-shot performance of existing state-of-the-art audio language models (ALMs). Unlike standard prompts commonly used in the literature, our approach creates task-specific prompts by first analyzing the individual class labels across various audio classification tasks."}, {"title": "VII. LIMITATION AND FUTURE WORK", "content": "Prompt Generation Errors: Errors or repetitive phrasing in GPT-4-generated prompts may require careful manual filtering. Future work will explore automated quality control methods to reduce the need for human oversight.\nBias in Task-Specific Prompts: The customization of prompts may introduce task-specific biases into the model. Future efforts will focus on identifying and mitigating these biases to ensure robust performance across diverse datasets.\nBroader Application of TSPE Representations: TSPE's text-audio representations could enhance other tasks, such as audio generation and sound event localization. Future research will include extending TSPE to these applications."}]}