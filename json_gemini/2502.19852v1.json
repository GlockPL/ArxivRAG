{"title": "CONVCODEWORLD: BENCHMARKING CONVERSATIONAL CODE GENERATION IN REPRODUCIBLE FEEDBACK ENVIRONMENTS", "authors": ["Hojae Han", "Seung-won Hwang", "Rajhans Samdani", "Yuxiong He"], "abstract": "Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: First, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying test coverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third, extensive evaluations of both closed-source and open-source LLMs including R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks are publicly available at https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld.", "sections": [{"title": "1 INTRODUCTION", "content": "Human-AI pair programming has become a promising approach to boost software development productivity, where large language models (LLMs) iteratively refine the code from developers' feedback. However, most existing benchmarks focus on single-turn scenarios, where LLMs are expected to generate executable code in one attempt Chen et al. (2021); Hendrycks et al. (2021); Austin et al. (2021); Li et al. (2022); Zhuo et al. (2024).\nTo address these gaps, we introduce CONVCODEWORLD (\u00a73; left panel in Figure 1), a novel environment for benchmarking interactive multi-turn code generation across diverse feedback combinations. CONVCODEWORLD features nine scenarios by combining three feedback types: (a) compilation feedback, (b) execution feedback with partial and full test coverage, and (c) novice and expert level verbal human feedback. We simulate human feedback using GPT-4o (OpenAI, 2024) to generate verbal responses, ensuring reproducibility and cost-efficiency at only 1.5% of the cost of human annotation (Appendix C.2).\nWhile replacing expensive human intervention with LLMs in CONVCODEWORLD reduces costs, it can still be expensive due to computational overhead or API fees, and latency due to LLM response."}, {"title": "2 RELATED WORK", "content": "Code generation benchmarks have traditionally focused on single-turn generation from natural language problem descriptions (Chen et al., 2021; Austin et al., 2021; Li et al., 2022; Zhuo et al., 2024). More recently, LLM performance has improved through interactions with external tools, such as interpreters for compiling, executing test cases, and verbal feedback, resulting in more accurate outputs (Shinn et al., 2023; Madaan et al., 2024; Chen et al., 2024; Olausson et al., 2024). This shift has led to the development of multi-turn benchmarks like InterCode (Yang et al., 2023) and MINT (Wang et al., 2024).\nHowever, existing multi-turn benchmarks remain limited in feedback diversity. InterCode focuses on compilation and partial execution feedback but lacks full test coverage and verbal feedback. MINT generates verbal feedback via GPT-4, reducing human-in-the-loop evaluation costs, but its feedback scope is narrow and requires costly LLM calls for each evaluation.\nOur study presents (a) CONVCODEWORLD, a reproducible environment with nine unique feedback combinations and (b) CONVCODEBENCH, a cost-effective benchmark that maintains high correlation with live environment by using pre-generated logs, eliminating the need for costly LLM calls to provide verbal feedback. We further discuss the distinction of CONVCODEWORLD in Appendix B."}, {"title": "3 CONVCODEWORLD: REPRODUCIBLE FEEDBACK ENVIRONMENTS", "content": "In real-world settings of interactive code generation, the types and combinations of feedback can vary significantly due to factors such as the availability of feedback from code execution (e.g., error messages, output) and the expertise of the feedback provider. These variations, particularly the provider's expertise, can strongly influence the quality of the verbal feedback when it is offered.\nTo effectively evaluate LLMs under these varying conditions, we propose CONVCODEWORLD, a novel and reproducible environment designed to simulate a wide range of interactive code generation scenarios. Two key features of CONVCODEWORLD are as follows: (a) Encompassing Diverse Real-World Scenarios: CONVCODEWORLD covers nine distinct feedback combinations that occur in practical settings; (b) Ensure the Reproducibility of Evaluation: CONVCODEWORLD provides a consistent and repeatable framework for assessing the performance of LLMs."}, {"title": "3.1 FEEDBACK CATEGORIZATION", "content": "To accurately simulate real-world feedback in interactive code generation, we focus on two critical components: (a) Fault Localization: Identifying the specific parts of the code where issues or errors occur; (b) Guidance for Refinement: Offering suggestions or instructions on how to correct the identified issues.\nAs means of obtaining such information, we consider three different types of feedback: compilation feedback, execution feedback, and verbal feedback."}, {"title": "3.1.1 VERBAL FEEDBACK GENERATION", "content": "We generate $f_v$ and $f_*$ by GPT-4o with in-context learning (Dong et al., 2022). We chose GPT-4o as we found it to be best at following instructions and minimizing risks such as ground truth code leakage, as discussed in Appendix C.5.\n\u2022 Generation of $f_v$: Novice-level verbal feedback is constructed by verbalizing outputs from compilation and/or execution feedback, possibly supplemented with language model predictions.\n\u2022 Generation of $f_*$: Expert-level verbal feedback is produced by showing the agent's code with the correct reference code (Wang et al., 2024), enabling a comparison and subsequent detailed feedback on required modifications. We perform extensive analysis to ensure no ground truth code is leaked during $f_*$ generation (see Appendix C.5 for analysis on this)."}, {"title": "3.2 FEEDBACK COMBINATIONS", "content": "In each of our turns, we simulate different real-world interactive code generation scenarios by combining representative feedback combinations. We represent feedback settings by taking a Cartesian product across compilation feedback settings, execution feedback settings, and verbal feedback settings. In particular, we formalize a feedback combination \u03a9 as a tuple of feedback expressed by\n\u03a9 = <fc, [\u03c6 | fe |f\u2217 ], [\u03c6 | fv |f\u2217 ]>."}, {"title": "4 CONVCODEBENCH: A STATIC BENCHMARK FOR EFFICIENT EVALUATION", "content": "While CONVCODEWORLD provides a comprehensive live benchmark for evaluating LLMs in interactive code generation scenarios, it requires access to an LLM for verbal feedback generation. Although this approach is more efficient and reproducible than using human annotators, it still introduces additional overhead, cost, and potential reproducibility issues, especially when using closed API models like GPT-4o. To address these challenges, we propose CONVCODEBENCH, a static benchmark designed to complement CONVCODEWORLD.\nCONVCODEBENCH leverages feedback logs generated by a fixed reference model interacting with GPT-4o. The benchmark presents pre-generated conversations\u2014including the code produced by the reference model and the corresponding feedback, such as verbal feedback by GPT-4o\u2014and tasks the target code model with refining the code. We revise Equation 2 to formalize CONVCODEBENCH as follows. For each turn t, the target code generation model M is provided generated code $C_M^t$ from a reference model M, and the corresponding tuple of feedback $\u03a9_t$ provided to outputs generated by $\\tilde{M}$. Given the model and feedback corresponding to a reference model, the target model M generates the next version of code $C_t^1$:\n$C_{t+1}^1 = M (x; C_M^t ; \u03a9_t)$.\nThis approach offers several advantages:\n\u2022 Elimination of Dependency on External LLMs or APIs for Verbal Feedback Generation: By using static feedback logs, CONVCODEBENCH reduces costs and latency associated with real-time LLM interactions.\n\u2022 Parallel Processing of Inference Calls: The static nature of the benchmark allows for batched evaluation requests across all turns, enabling faster turnaround times.\n\u2022 Enhanced Reproducibility: Utilizing fixed logs ensures consistent evaluations, further increasing reproducibility.\nOne key concern when using CONVCODEBENCH is the bias introduced by pre-generated interaction logs prompting the question: Can we ensure high correlation between static and live benchmarks by an appropriate choice of reference model?\nWe hypothesize that using logs from a weaker model, where the generated code still requires refinement even after multiple turns, allows for better differentiation among models based on their ability to improve unsolved code."}, {"title": "5 EXPERIMENTS", "content": "Using CONVCODEWORLD and CONVCODEBENCH, we conduct comprehensive experiments to evaluate LLMs' interactive code generation capabilities across diverse feedback combinations. This section outlines our experimental setup (\u00a75.1), results on CONVCODEWORLD (\u00a75.2), and results on CONVCODEBENCH (\u00a75.3)."}, {"title": "5.1 SETUP", "content": "To implement CONVCODEWORLD, we extended BigCodeBench-Full-Instruct (Zhuo et al., 2024), a single-turn Python code generation benchmark, into an interactive framework using a custom prompt pipeline built using DSPy (Khattab et al., 2024) (see Appendix E for the implementation details). BigCodeBench was selected for three key reasons: (a) its highly challenging problem sets (as of the writing of this paper, the highest performance on this data is 51.1% of Pass@1); (b) its large scale, with 1,140 problems, offering higher generalizability than smaller benchmarks like HumanEval (Chen et al., 2021; 164 problems) and MBPP-sanitized (Austin et al., 2021; 399-427 problems); and (c) its comprehensive test coverage - an average of 5.6 cases per problem with 99% branch coverage-enabling the evaluation of a wide spectrum of execution feedback scenarios, ranging from partial to full test coverage.\nEvaluation Metrics In the interactive scenario, where code is iteratively refined based on feedback, we focus on two aspects for evaluation: (a) the number of turns it takes to produce correct code, with fewer turns being preferable, and (b) whether the model can eventually solve the problem within a set number of turns n. In our experiments, we set n = 10.\nTo capture these aspects, we use Pass@1 (Chen et al., 2021) as the core metric to assess code correctness at each turn and adapt two complementary metrics from information retrieval: (a) Mean Reciprocal Rank (MRR): where k is the turn at which the model produces correct code. If no correct code is generated within n turns, the score is set to 0; (b) Recall: 1 if the model produces correct code within n turns.\nBaseline LLMs We extensively evaluated 3 closed-source and 18 open-source LLMs ranging from 7B to 70B: (a) Closed-Source: We select three OpenAI LLMs\u2014GPT-4-0613, GPT-4-Turbo-2024-04-09, and GPT-4o-2024-05-13; (b) Open-Source: Llama-3.1-70B-Instruct (Dubey et al., 2024), Llama-3.1-8B-Instruct, DeepSeek-Coder-V2-Lite-Instruct (Zhu et al. (2024); an MoE model; total params: 16B; active params: 2.4B), DeepSeek-Coder-33B-Instruct (Guo et al., 2024), DeepSeek-Coder-6.7B-Instruct, ReflectionCoder-DS-33B (Ren et al., 2024), ReflectionCoder-DS-6.7B, Qwen1.5-72B-Chat (Bai et al., 2023), Qwen1.5-32B-Chat, CodeQwen1.5-7B-Chat, StarCoder2-15B-Instruct-v0.1, CodeLlama-34B-Instruct (Roziere et al., 2023), CodeLlama-13B-Instruct, and CodeLlama-7B-Instruct. In Appendix A, we further included the results of two recent R1-Distill (DeepSeek-AI et al., 2025) models and their base models on CONVCODEWORLD."}, {"title": "5.2 RESULTS ON CONVCODEWORLD", "content": "Tables 4 and 5 present MRR and Recall scores, respectively, for both closed-source and open-source LLMs across various feedback combinations. These results provide a comprehensive view of model performance in CONVCODEWORLD.\nOverview of Results While closed-source models generally outperformed most open-source models, Llama-3.1-70B-Instruct demonstrated competitive Recall performance, surpassing both GPT-4-Turbo and GPT-4o in certain scenarios like \u3008fc, [fe|f*], fv> and <fc, [\u03c6|fe|f*], f*>.\nNotably, this Recall gap between closed-source and open-source models narrows significantly under specific feedback settings, particularly when expert-level verbal feedback f* is provided. For instance, in the <fc, \u03c6, f*> setting, DeepSeek-Coder6.7B-Instruct (82.8) outperformed GPT-4o (82.3), and DeepSeek-Coder33B-Instruct (85.4) outperformed GPT-4-Turbo (84.7).\nAnother key observation is that, among open-source models smaller than 30B, no clear winner emerges across all feedback combinations. This emphasizes the importance of selecting models based on the specific type of feedback available."}, {"title": "5.2.1 FEEDBACK COMBINATIONS: DIVERSIFIED EVALUATION", "content": "We observed significant performance variation within the same model across different feedback combinations, emphasizing the necessity of CONVCODEWORLD for evaluating code generation models under diverse feedback conditions.\nSpecifically, we summarize the effect of providing different feedback combinations:\nImpact of Novice-Level Verbal Feedback on Execution Feedback Utilization Without novice-level verbal feedback (fv), some models\u2014DeepSeek-Coder-33B-Instruct, DeepSeek-Coder-6.7B-Instruct, CodeQwen1.5-7B-Chat, StarCoder2-15B-Instruct-v0.1, CodeLlama-13B-Instruct, and CodeLlama-7B-Instruct\u2014showed minimal performance differences between partial (<fc, fe, \u03c6>) and full (<fc, f*, \u03c6>) test coverage in execution feedback. However, these models showed greater reliance on fv, especially in <fc, f*, fv> compared to \u3008fc, fe, fv), indicating that they need fv to fully leverage f*. In contrast, high-performing models\u2014GPT-4, GPT-4-Turbo, GPT-4o, and Llama-3.1-70B\u2014demonstrated a larger performance boost from <fc, fe, \u03c6> to <fc, f*, \u03c6> compared to the boost from <fc, fe, \u03c6> to <fc, fe, fv>. This suggests these models can infer refinement strategies directly from raw execution feedback without heavily relying on fv.\nImpact of Expert-Level Verbal Feedback on Execution Feedback Utilization Most models demonstrated performance improvements with richer execution feedback, progressing through the sequences <fc, \u03c6, f*\u3009, \u3008fc, fe, f*>, and <fc, f*, f*\u3009. However, exceptions arise: (a) DeepSeek-Coder family and ReflectionCoder-DS-6.7B exhibited no performance difference with the inclusion of execution feedback; (b) Llama-3.1-8B-Instruct, ReflectionCoder-DS-33B, and CodeQwen1.5-7B-Chat showed no significant difference between <fc, \u03c6, f*> and <fc, fe, f*), but performance improved when full test coverage (<fc, f*, f*>) was ensured; (c) In some weaker models\u2014Qwen1.5-32B-Chat and StarCoder2-15B-Instruct-v0.1\u2014increasing the test coverage from <fc, fe, f*> to <fc, f*, f*> resulted in negative performance impacts. Additionally, the highest performance of Qwen1.5-32B-Chat was observed with <fc, \u03c6, f*\u3009, while adding execution feedback (fe or f*) led to decreased performance. We hypothesize that weaker models struggle to utilize complex feedback effectively, resulting in lower performance. We further discuss the possible reasons for these exceptions in Appendix C.4."}, {"title": "5.2.2 MULTI-TURN FEEDBACK: WEAKER MODELS OUTPERFORMING SINGLE-TURN SOTA", "content": "Weaker LLMs with sufficient feedback outperformed the single-turn, no-feedback performance (<\u03c6, \u03c6, \u03c6>) of state-of-the-art models like GPT-4 and GPT-4-Turbo.\nMRR When expert-level verbal feedback (f*) was incorporated, most weaker models, including DeepSeek-Coder-6.7B-Instruct and Llama-3.1-8B-Instruct, surpassed the single-turn code generation performance of state-of-the-art single-turn models such as GPT-4, GPT-4-Turbo, and GPT-4o. Additionally, with the inclusion of novice-level verbal feedback (fv) and either partial or full execution feedback (fe or f*), DeepSeek-Coder-33B-Instruct and ReflectionCoder-DS-33B matched or exceeded the single-turn performance of GPT-4 and GPT-4-Turbo.\nRecall Most open-source models exhibited significant improvements when novice-level verbal feedback with execution feedback (<fc, [fe|f*], fv\u3009) or expert-level verbal feedback (<fc, [\u03c6|fe|f*], fv\u3009) was provided. Remarkably, providing execution feedback with full test coverage while omitting any verbal feedback (\u3008fc, f*, \u03c6)) enabled some models, such as DeepSeek-Coder-V2-Lite-Instruct, DeepSeek-Coder-33B-Instruct, and Qwen1.5-72B-Chat, to achieve or even exceed GPT-4's single-turn performance."}, {"title": "5.2.3 GENERALIZATION: UNSEEN FEEDBACK COMBINATION", "content": "ReflectionCoder-DS family were initialized from DeepSeek-Coder-Instruct, and trained to refine code on a specific scenario of <fc, f*, fv). As a result, ReflectionCoder-DS-6.7B outperformed DeepSeek-Coder-6.7B-Instruct on <fc, [fe|f*], fv\u3009. However, with unseen feedback like expert-level verbal feedback (f*), the performance gap narrows significantly, with minimal MRR difference and DeepSeek-Coder-Instruct generally outperforming in Recall. This tendency is more pronounced in ReflectionCoder-DS-33B; except for <fc, [fe|f*], fv\u3009, ReflectionCoder-DS-33B consistently performed at or below the level of DeepSeek-Coder-33B-Instruct across all feedback combinations in both MRR and Recall. This indicates that training on a specific feedback combination can reduce the performance on the other combinations."}, {"title": "5.2.4 TRADE-OFF: MULTI-TURN MRR AND RECALL", "content": "We observed that an LLM requiring fewer turns to solve a problem (high MRR) may not excel at solving as many problems as possible (high Recall), and vice versa: (a) Closed-Source Models: GPT-4o achieved the highest MRR, while GPT-4 had the best Recall; (b) Open-Source Models \u2265 30B: Llama-3.1-70B led in both MRR and Recall. DeepSeek-Coder-33B-Instruct and ReflectionCoder-DS-33B followed in MRR. However, with fv or f* feedback, Qwen1.5-72B-Chat generally outperformed them in Recall, despite having a lower MRR; (c) Open-Source Models < 30B: MRR and Recall tendencies were similar without verbal feedback. With verbal feedback, CodeQwen1.5-7B-Chat excelled in MRR, while DeepSeek-Coder-V2-Lite-Instruct (<fc, [fe|f*], fv\u3009), and DeepSeek-Coder-6.7B-Instruct (<fc, [\u03c6|fe|f*], f*> led in Recall."}, {"title": "5.3 RESULTS ON CONVCODEBENCH", "content": "While CONVCODEWORLD provides valuable insights into interactive code generation across various feedback combinations, CONVCODEBENCH offers a faster, cheaper, and more reproducible alternative. As discussed in \u00a74, we chose CodeLlama-7B-Instruct as the reference model, and excluded scenarios without verbal feedback, as they do not require LLM intervention. Additionally, <fc, \u03c6, fv> scenario was omitted as CodeLlama-7B-Instruct achieved a 100% compilation success rate in the initial generation, eliminating the need for novice-level verbal feedback on compilation.\nCONVCODEBENCH as a Reliable Proxy We conducted a comparative analysis of CONVCODEBENCH and CONVCODEWORLD to validate CONVCODEBENCH as a proxy, comparing the MRR (Figure 2) and Recall (Appendix H.1) results across target models and feedback combinations Spearman's rank correlations ranged from 0.82\u20130.99, indicating that CONVCODEBENCH is a reliable, efficient, and cost-effective proxy for CONVCODEWORLD.\nAdditionally, Table 6 presents the results on CONVCODEBENCH, showing that MRR ranking trends closely aligned with CONVCODEWORLD (Table 4), with minor deviations. While absolute recall and MRR scores are slightly lower compared to CONVCODEWORLD, the rankings amongst models remained roughly consistent between CONVCODEBENCH and CONVCODEWORLD. Based on approximately consistent rankings across CONVCODEBENCH and CONVCODEWORLD, we recommend code LLMs use CONVCODEBENCH as a solid alternative to compare against other baselines."}, {"title": "6 CONCLUSION", "content": "This paper recognizes the need for benchmarks with diverse type of interactions in conversational code generation. To address this gap, we introduced CONVCODEWORLD, a novel and reproducible environment designed to assess LLM code generation abilities across nine varied feedback scenarios. Additionally, for scenarios where API call costs are prohibitive, we offer CONVCODEBENCH, a zero-call benchmark from pre-generated feedback logs, providing a highly correlated evaluation of the conversational code generation capabilities of LLMs with CONVCODEWORLD. Our work contributes to a more thorough evaluation of diverse multi-turn evaluation objectives, and highlights a gap to invite for future models in the new design space."}, {"title": "A DEEKSEEK-R1-DISTILL RESULTS ON CONVCODEWORLD", "content": "Table 7: MRR results on CONVCODEWORLD. X indicates that no feedback of that type is provided (4). The leftmost results, with three X, represent \u03a9 = <\u03c6, \u03c6, \u03c6), corresponding to single-turn code generation without any feedback. For each column, bold and underscore indicate 1st and 2nd place performance within the same model group. Maximum token length is set to 8K throughout the experiments, except for the R1-Distill models, which are set to 16K.\nTable 8: Recall results on CONVCODEWORLD. X indicates that no feedback of that type is provided (4). The leftmost results, with three X, represent \u03a9 = <\u03c6, \u03c6, \u03c6\u3009, corresponding to single-turn code generation without any feedback. For each column, bold and underscore indicate 1st and 2nd place performance within the same model group. Maximum token length is set to 8K throughout the experiments, except for the R1-Distill models, which are set to 16K."}, {"title": "5.2.3 GENERALIZATION: UNSEEN FEEDBACK COMBINATION", "content": "ReflectionCoder-DS family were initialized from DeepSeek-Coder-Instruct, and trained to refine code on a specific scenario of <fc, f*, fv). As a result, ReflectionCoder-DS-6.7B outperformed DeepSeek-Coder-6.7B-Instruct on <fc, [fe|f*], fv\u3009. However, with unseen feedback like expert-level verbal feedback (f*), the performance gap narrows significantly, with minimal MRR difference and DeepSeek-Coder-Instruct generally outperforming in Recall. This tendency is more pronounced in ReflectionCoder-DS-33B; except for <fc, [fe|f*], fv\u3009, ReflectionCoder-DS-33B consistently performed at or below the level of DeepSeek-Coder-33B-Instruct across all feedback combinations in both MRR and Recall. This indicates that training on a specific feedback combination can reduce the performance on the other combinations."}, {"title": "5.2.4 TRADE-OFF: MULTI-TURN MRR AND RECALL", "content": "We observed that an LLM requiring fewer turns to solve a problem (high MRR) may not excel at solving as many problems as possible (high Recall), and vice versa: (a) Closed-Source Models: GPT-4o achieved the highest MRR, while GPT-4 had the best Recall; (b) Open-Source Models \u2265 30B: Llama-3.1-70B led in both MRR and Recall. DeepSeek-Coder-33B-Instruct and ReflectionCoder-DS-33B followed in MRR. However, with fv or f* feedback, Qwen1.5-72B-Chat generally outperformed them in Recall, despite having a lower MRR; (c) Open-Source Models < 30B: MRR and Recall tendencies were similar without verbal feedback. With verbal feedback, CodeQwen1.5-7B-Chat excelled in MRR, while DeepSeek-Coder-V2-Lite-Instruct (<fc, [fe|f*], fv\u3009), and DeepSeek-Coder-6.7B-Instruct (<fc, [\u03c6|fe|f*], f*> led in Recall."}, {"title": "CONVCODEBENCH as a Reliable Proxy", "content": "We conducted a comparative analysis of CONVCODEBENCH and CONVCODEWORLD to validate CONVCODEBENCH as a proxy, comparing the MRR (Figure 2) and Recall (Appendix H.1) results across target models and feedback combinations Spearman's rank correlations ranged from 0.82\u20130.99, indicating that CONVCODEBENCH is a reliable, efficient, and cost-effective proxy for CONVCODEWORLD.\nAdditionally, Table 6 presents the results on CONVCODEBENCH, showing that MRR ranking trends closely aligned with CONVCODEWORLD (Table 4), with minor deviations. While absolute recall and MRR scores are slightly lower compared to CONVCODEWORLD, the rankings amongst models remained roughly consistent between CONVCODEBENCH and CONVCODEWORLD. Based on approximately consistent rankings across CONVCODEBENCH and CONVCODEWORLD, we recommend code LLMs use CONVCODEBENCH as a solid alternative to compare against other baselines."}, {"title": "6 CONCLUSION", "content": "This paper recognizes the need for benchmarks with diverse type of interactions in conversational code generation. To address this gap, we introduced CONVCODEWORLD, a novel and reproducible environment designed to assess LLM code generation abilities across nine varied feedback scenarios. Additionally, for scenarios where API call costs are prohibitive, we offer CONVCODEBENCH, a zero-call benchmark from pre-generated feedback logs, providing a highly correlated evaluation of the conversational code generation capabilities of LLMs with CONVCODEWORLD. Our work contributes to a more thorough evaluation of diverse multi-turn evaluation objectives, and highlights a gap to invite for future models in the new design space."}, {"title": "B DISTINCTION OF CONVCODEWORLD", "content": "We elaborate distinctive implications from existing works such as InterCode Yang et al. (2023) and MINT Wang et al. (2024):\n\u2022 Comparative analyses of partial to full test coverage in execution feedback enables to evaluate both:\nTest generalization: A model's ability to produce code that passes full tests even when only partial tests are provided.\nTest utilization: A model's capability to leverage given test results for code refinement.\nInterCode--which uses full test only--evaluates test utilization only, and MINT\u2014-which uses partial test only--provides an entangled evaluation of test generalization and test"}, {"title": "C VERBAL FEEDBACK", "content": "A key challenge in creating CONVCODEWORLD is generating verbal feedback. Human annotation is both impractical and inconsistent (\u00a73.1.1), which led us to employ GPT-4o for this task. While GPT-4o may not fully replicate the nuances of human feedback, it ensures reproducibility and affordability, both critical for maintaining consistency across benchmark evaluations. As demonstrated by direct comparisons between LLM-generated and human feedback in prior studies (Wang et al., 2024), we find this method sufficiently effective for our benchmarking purposes."}, {"title": "C.1 DISCUSSION ON EMPLOYING LLMS FOR VERBAL FEEDBACK GENERATION", "content": "A key challenge in creating CONVCODEWORLD is generating verbal feedback. Human annotation is both impractical and inconsistent (\u00a73.1.1), which led us to employ GPT-4o for this task. While GPT-4o may not fully replicate the nuances of human feedback, it ensures reproducibility and affordability, both critical for maintaining consistency across benchmark evaluations. As demonstrated by direct comparisons between LLM-generated and human feedback in prior studies (Wang et al., 2024), we find this method sufficiently effective for our benchmarking purposes."}, {"title": "C.2 COST-EFFICIENCY OF CONVCODEWORLD COMPARED TO HUMAN ANNOTATION", "content": "In the worst-case scenario, CodeLlama-7B-Instruct, which requested the most verbal feedback due to its low performance, incurred a total cost of $215 (26.4M input tokens and 5.5M output tokens) for 15,905 turns using GPT-4o-2024-05-13 pricing ($5/1M input tokens and $15/1M output tokens). By comparison, assuming human annotation takes 96 seconds per turn (Wang et al., 2024) and the average U.S. private non-farmer worker's hourly wage is $35.04 according to US Bureau of Labor Statistics (2024), the human annotation cost would be approximately $14,792."}, {"title": "C.3 HUMAN EVALUATION OF GENERATED VERBAL FEEDBACK", "content": "Table 10: Human evaluation of simulated expert-level user feedback by GPT-4o and real user feedback by ShareGPT.\nWe conducted human evaluation to validate the realism of simulated expert-level user feedback, noting that in-context examples might lead to unrealistic responses. Specifically, two human evaluators"}, {"title": "C.4 POSSIBLE REASONS FOR THE OBSERVED \"STRUGGLE TO UTILIZE FEEDBACK\"", "content": "From Section 5.2.1, we further discuss two possible reasons for models when they struggle to utilize complex feedback:\n\u2022 Limited Model Size: Smaller models, such as ReflectionCoder-DS-6.7B, may lack the capacity to process and integrate complex information effectively, which could limit performance improvements even when execution feedback is included. In contrast, their bigger versions like ReflectionCoder-DS-33B demonstrated performance gains with execution feedback (41.6 \u2192 45.3). Mixed feedback types may distract small models further. When comparing Expert feedback only vs. Expert feedback + execution feedback. For Qwen1.5-Chat, the 72B model's performance improved with execution feedback, while the 32B model's performance deteriorated, which suggests that smaller models might become distracted when faced with multiple feedback signals simultaneously (Liu et al., 2024). However, this distraction may be mitigated with well-designed training data, as even smaller models like Llama-3.1-8B-Instruct show improvements when provided with more execution feedback.\n\u2022 Limited Generalization Training: ReflectionCoder models were trained on a specific feedback combination, limiting their adaptability to other feedback types (Section 5.2.3). For example, with expert feedback, ReflectionCoder-DS-33B scores lower (81.4) than its base model DeepSeekCoder-33B-Instruct (85.4)."}, {"title": "C.5 ANALYSIS OF GROUND TRUTH CODE LEAKAGE IN GENERATED EXPERT-LEVEL VERBAL FEEDBACK", "content": "Table 11: Pass@1 results of various LLMs with expert-level verbal feedback f* generated by GPT-4o compared to direct ground truth code feedback. The total number of turns n = 1. For each column, bold and underscore indicate 1st and 2nd place performance while keeping the code generation model fixed.\nTable 12: Ground truth code leakage ratio (%) by incorporating different models for expert-level verbal feedback generation. The lower the better.\nThe generation of expert-level verbal feedback f* involves comparing the generated code with the ground truth code to provide modification suggestions. This process could raise concerns about potential code leakage. As shown in Table 11, providing the ground truth code significantly outperforms providing f*, empirically confirming that f* is unlikely to be a direct copy of the ground truth code."}, {"title": "C.6 COMPARATIVE ANALYSIS OF VERBAL FEEDBACK ACROSS DIFFERENT LLMS", "content": "Table 13: Pass@1 results over different model combinations of expert-level verbal feedback f* generation and code generation on CONVCODEWORLD where \u03a9 = <fc, \u03c6, f*> and the total number of turns n = 1. Each row represents a model used to provide verbal feedback. Each column represents a model that utilizes this feedback to refine code. For each column, bold and underscore indicate 1st and 2nd place performance while keeping the code generation model fixed.\nIn our main experiments, we utilized GPT-4o for verbal feedback generation and investigated its performance in comparison to other models. To see the effect of using other LLMs for verbal feedback generation, we conducted a single iteration of code generation using three closed-source LLMs as both code generators and expert-level verbal feedback generators, examining the Pass@1 performance. Table 13 evaluates different models as potential verbal feedback simulators. The effectiveness of the feedback provided by each simulator is assessed by comparing the performance across columns, showing consistent superior performance when employing GPT-4o for feedback generation."}]}