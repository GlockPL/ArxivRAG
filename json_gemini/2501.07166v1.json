{"title": "Natural Language-Assisted Multi-modal Medication Recommendation", "authors": ["Jie Tan", "Yu Rong", "Kangfei Zhao", "Tian Bian", "Tingyang Xu", "Junzhou Huang", "Hong Cheng", "Helen Meng"], "abstract": "Combinatorial medication recommendation (CMR) is a fundamental task of healthcare, which offers opportunities for clinical physicians to provide more precise prescriptions for patients with intricate health conditions, particularly in the scenarios of long-term medical care. Previous research efforts have sought to extract meaningful information from electronic health records (EHRs) to facilitate combinatorial medication recommendations. Existing learning-based approaches further consider the chemical structures of medications, but ignore the textual medication descriptions in which the functionalities are clearly described. Furthermore, the textual knowledge derived from the EHRs of patients remains largely underutilized. To address these issues, we introduce the Natural Language-Assisted Multi-modal Medication Recommendation (NLA-MMR), a multi-modal alignment framework designed to learn knowledge from the patient view and medication view jointly. Specifically, NLA-MMR formulates CMR as an alignment problem from patient and medication modalities. In this vein, we employ pretrained language models (PLMs) to extract in-domain knowledge regarding patients and medications, serving as the foundational representation for both modalities. In the medication modality, we exploit both chemical structures and textual descriptions to create medication representations. In the patient modality, we generate the patient representations based on textual descriptions of diagnosis, procedure, and symptom. Extensive experiments conducted on three publicly accessible datasets demonstrate that NLA-MMR achieves new state-of-the-art performance, with a notable average improvement of 4.72% in Jaccard score. Our source code is publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.", "sections": [{"title": "1 INTRODUCTION", "content": "The implementation of digital patient records in healthcare institutions has boosted the development of a substantial repository of information known as Electronic Health Records (EHRs). This valuable asset holds significant potential for a variety of applications within the medical field, such as predicting mortality rates, forecasting treatment outcomes, and offering medication recommendations. As one of the fundamental tasks of healthcare, combinatorial medication recommendation (CMR) has attracted a lot of attention in recent years, and achieved significant progress in providing personalized and safe medication recommendations for patients. This advancement is especially beneficial for the elderly who have been suffering from chronic illnesses for a prolonged time [27, 30].\nA multitude of deep learning models have been designed to solve the CMR task in the literature, which not only use the EHRs (e.g., diagnoses, procedures, and symptoms), but also leverage the chemical structures of drugs. For example, SafeDrug [38] developed dual molecular graph encoders to embed global and local molecular structures. MoleRec [39] proposed a molecular substructure-aware attentive method for CMR. DrugRec [31] proposed to represent drugs with the SMILES, a line-based representation (i.e., string) of molecules. Besides EHRs and chemical structures, we observe there are a vast amount of textual descriptions of drug molecules that are human-understandable and easily accessible on platforms such as PubChem [13] and DrugBank [35]. These texts describe the functionalities of drug molecules and reveal their therapeutic purposes. Unfortunately, none of the existing methods try to exploit the textual medication descriptions, thus failing to establish the semantic relations between the textual descriptions of patients and medications. Consider the example depicted in Figure 1 with two modalities of patient and medication, respectively. In the patient modality, an EHR shows that the patient with the symptoms \"heart pain\" and \"fatigue\" is diagnosed with diseases \u201cold myocardial infraction\" and \"chronic ischemic\", and then is prescribed with a set of drugs", "aspirin": "nd \u201cacetaminophen", "Carvedilol is used to treat chronic heart failure, hypertension, and myocardial infarction\". This example illustrates that the textual medication description can supplement the chemical structures to enhance the expressiveness of medication representations, and help establish the semantic relations between the patients and medications. This motivates us to leverage the useful textual medication description and consider patient and medication as two distinct modalities for medication recommendation.\nIn another direction, recent studies have attempted to incorporate structured domain knowledge, such as biomedical knowledge graphs (BioKG) [29, 36": "and drug-drug interactions (DDI) [2, 31, 38] to enhance the CMR performance. However, the quality of such structured domain knowledge may degrade due to potential information loss and biased sampling when it is extracted from raw medical documents. A more effective approach is to employ the pretrained language models (PLMs), which are pretrained on web-scale textual data from the chemical and biological domain to capture clinical specialist knowledge in raw texts for medication recommendation. Representations learned by PLMs like PubMedBERT [6] and ClinicalBERT [16] achieve strong performance across many domain-specific applications. Intuitively, using these PLMs to encode the textual knowledge of the patient and medication modalities can provide additional gains over conventional encoding methods in the CMR task because they effectively fuse various types of information, such as diagnoses, procedures, symptoms and medications in a unified global semantic space.\nIn light of these observations, we propose a novel model, called Natural Language-Assisted Multi-modal Medication Recommendation (NLA-MMR), to make medication recommendations by effectively harnessing the wealth of clinical knowledge in the EHRs and drugs. NLA-MMR incorporates the textual medication descriptions for molecular representation learning. It also integrates expert knowledge by leveraging PLMs to capture various types of semantic relations between the texts in the EHRs and medication descriptions."}, {"title": "2 PRELIMINARIES", "content": "For a patient $v$, his/her electrical health records (EHRs) are represented as a sequence $R_v = [x_v^{(1)}, x_v^{(2)},...,x_v^{(T_v)}]$, where $x_v^{(t)}$ represents the health record from the $t$-th clinical visit and $T_v$ is the total number of visits of the patient $v$. When the context is clear, we omit the subscript $v$ to simplify notations. Then, the $t$-th visit in the EHR of a patient can be represented by a $x^{(t)} = (d^{(t)},p^{(t)}, s^{(t)}, m^{(t)})$, where $d^{(t)} \\in {0,1}^{|D|}$, $p^{(t)} \\in {0,1}^{|P|}$, $s^{(t)} \\in {0,1}^{|S|}$, $m^{(t)} \\in {0,1}^{|M|}$ are multi-hot diagnosis, procedure, symptom, and medication vectors, respectively. $D, P, S, M$ indicate the set of possible diagnoses, procedures, symptoms and medications, respectively, and $|\\cdot|$ denotes the cardinality of a set. As shown in Figure 1, our approach treats the patient and medication as two modalities to consider their information."}, {"title": "3 METHODOLOGY", "content": "Figure 2 illustrates an overview of our proposed drug recommendation approach NLA-MMR, which is composed of three modules: 1) the patient representation module using a PLM to encode the texts of diagnosis, procedure, and symptom; 2) the medication representation module generating medication representations by employing the GNN model and a PLM for incorporating molecular structure-level information and textual knowledge, respectively; 3) the cross-modal alignment module responsible for making prescriptions by using two projection layers to calculate the similarity between the patient representation and medication representations on the joint latent space. In particular, in the patient representation module, we conduct cross-attention fusion to fully leverage the different types of textual information present in the patient modality. Besides, we integrate the medication information of historical visits to capture dependencies between medication usage in previous visits and the current visit."}, {"title": "3.1 Patient Representation Module", "content": "In this section, we construct the patient representation with the textual components defined in the patient modality. By leveraging the power of PLMs, we are able to extract valuable expert knowledge from these unprocessed texts, thereby contributing to the development of an advanced medication recommendation model. Specifically, we first consider the PLM-based representation of the concatenated text of diagnosis, procedure, and symptom as the basic patient representation. Then, we devise a cross-attention fusion mechanism to merge information from the three types of patient descriptions to obtain the final patient representation.\nPLM-based Representation. We employ a PLM encoder [17] to extract the representation for the patient modality. Give a patient's t-th visit $r^{(t)} = (r_{d}^{(t)}, r_{p}^{(t)}, r_{s}^{(t)})$, we concatenate these descriptions into a synthesis text as $r_{dps}^{(t)}$ and employ the PLM encoder to obtain its representation:\n$h_{dps}^{(t)} = \\text{PLM-Encoder}(r_{dps}^{(t)}),$ (1)\nwhere $h_{dps}^{(t)} \\in \\mathbb{R}^{d_{enc}}$ is the output of the PLM encoder, which is obtained via an average pooling of the embeddings of all tokens. Meanwhile, we individually encode the textual descriptions of diagnosis, procedure, and symptom with the PLM and obtain $h_{d}^{(t)}, h_{p}^{(t)},$ and $h_{s}^{(t)}$, which are representations of the corresponding type of textual description in the patient modality.\nCross-Attention Fusion. To enhance the basic patient representation in Eq. (1), we aggregate the features derived from three aspects, $h_{dps}^{(t)}, h_{d}^{(t)}, h_{p}^{(t)}$, and $h_{s}^{(t)}$, by utilizing a cross-attention mechanism, weighted by a set of learnable weights $w_c \\in \\mathbb{R}^{|C|}, \\hat{h}_{attn} = \\sum_{c \\in C} w_c h_c^{(t)}, C = \\{s, d,p\\}$. Let $H^{(t)} \\in \\mathbb{R}^{|C| \\times d_{enc}}$ be the matrix stacked by the embeddings in $\\{h_s^{(t)}, h_d^{(t)}, h_p^{(t)}\\}$. In Eq. (2), $H_1 \\in \\mathbb{R}^{|C|\\times d_K}, H_2 \\in \\mathbb{R}^{1 \\times d_K}$ are transformed by linear weight matrices $W_1, W_2 \\in \\mathbb{R}^{d_{enc}\\times d_K}$, respectively. $w_c$ is computed by the multiplication of $H_1$ and the transpose of $H_2$ followed by a softmax function that normalizes the weights, as Eq. (3) shows.\n$H_1 = H^{(t)} W_1, H_2 = h_{dps}^{(t)} W_2,$ (2)\n$\\{w_c\\}_{c\\in C} = \\text{softmax} (\\frac{H_1 H_2^\\intercal}{\\sqrt{d_K}}).$ (3)\nNext, we concatenate the aggregated representation $\\hat{h}_{attn} \\in \\mathbb{R}^{d_{enc}}$ and the basic patient feature $h_{dps}^{(t)} \\in \\mathbb{R}^{d_{enc}}$ as the patient representation, which can be formulated as:\n$p^{(t)} = [\\hat{h}_{attn} || h_{dps}^{(t)}] W_r + b_r,$ (4)\nwhere $[||]$ denotes the concatenation operation and $W_r \\in \\mathbb{R}^{2 d_{enc}\\times d_{enc}}$ and $b_r \\in \\mathbb{R}^{d_{enc}}$ are trainable parameters."}, {"title": "3.2 Medication Representation Module", "content": "The medication modality encompasses two essential components, namely textual knowledge and structural information. As depicted in Figure 2, our module for representing medications is composed of two branches: the textual description branch and the chemical structure branch. These branches are designed to address the external domain knowledge of molecules through textual descriptions and their intrinsic property through chemical structures, respectively. To harness the domain-specific knowledge contained within the textual descriptions, we employ a text encoder known as the PLM. Additionally, we employ a GNN model to extract representations at the structure level for drugs based on their chemical structures.\nPLM-based Functional Representation. The textual description of a drug provides a high-level overview of the molecule's functionality, which offers valuable insights into the therapeutic potential of the drug and can aid in drug predictions. In this paper, we extract therapeutic descriptions of the drugs from DrugBank [35], which illustrates drug effects on the target organism. Given the textual medication descriptions, we obtain its PLM-based functional representations by conducting a similar process as in Eq. (1). We collect the textual embeddings of all drugs into a drug matrix $E_f \\in \\mathbb{R}^{|M| \\times d_{enc}}$, where each row corresponds to the textual embedding of a drug.\nStructure-level Representation. We incorporate chemical structures to learn the molecule representation. Specifically, we employ the GNN to model the interactions between all the atoms across the single molecule graph $g$ and obtain the structure-level drug representation. The aggregation and combination paradigm is conducted on the graph $g$ of a drug molecule whose nodes are atoms and edges are chemical bonds. Given the features of the $n$ atoms $E^{(k)} = \\{e_1^{(k)},...,e_n^{(k)}\\} \\in \\mathbb{R}^{n \\times d_s}$, for each atom $a_i \\in A$, the messages of all the other atoms in the molecule are aggregated to $\\overline{e_a}^{(k)} \\in \\mathbb{R}^{d_s}$ (Eq. (5)). Afterward, the aggregated message $\\overline{e_a}^{(k)}$ is combined with the feature of the atom $e_a^{(k)}$ by a summation operation, followed by a multilayer perceptron (MLP), as the combination function of GIN [28], generating new atom feature $e_a^{(k+1)}$ for the next layer (Eq. (6)).\n$\\overline{e_a}^{(k)} = \\text{AGG}^{(k)} (\\{ e_{a_j}^{(k)} | \\forall a_j \\in \\mathcal{N}(a_i) \\}),$ (5)\n$e_a^{(k+1)} = \\text{MLP}^{(k)} ((1 + \\epsilon^{(k+1)}) e_a^{(k)} + \\epsilon^{(k+1)} \\overline{e_a}^{(k)} + e_a^{(k)}), \\forall i \\in \\{1,..., n\\},$ (6)\nwhere $e$ can be a learnable parameter or a fixed scalar, it determines the importance of the target node compared to its neighbors, $e_{a_j}^{(k)}$ is the feature vector of atom $a_j$ at the $k$-th layer, $\\mathcal{N}(a_i)$ denotes the neighbors of the atom $a_i$. After applying message passing for $L$ layers, the embeddings of the atoms in the molecule graph are aggregated into a global structural-level representation as Eq. (7),\n$e_g = \\text{Pooling} (\\{ e_a^{(L)} | \\forall i \\in \\{1,..., n\\} \\}),$ (7)\nwhere the pooling function is the average function. We employ the same GNN encoder with shared parameters for each drug molecule (with a total of $|M|$ different drugs). We collect the GNN embeddings of all drugs into a drug memory $E_s \\in \\mathbb{R}^{|M| \\times d_s}$, where each row corresponds to the drug's structural representation, $d_s$ represents the dimension of the structure-level drug representation. The textual representation and structural representation are combined to form the final medication embedding matrix $E_m \\in \\mathbb{R}^{|M| \\times (d_s + d_{enc})}$ as follows:\n$E_m = [E_s || E_f].$ (8)"}, {"title": "3.3 Cross-Modal Alignment Module", "content": "In the cross-modal alignment module, the patient representation $p^{(t)}$ and medication representations $E_m$ are fed into two MLPs which map the representations extracted from different modalities to a joint latent space as Eq. (9):\n$\\hat{p}^{(t)} = \\text{MLP}(p^{(t)}), \\hat{E}_m = \\text{MLP}(E_m) .$ (9)\nSubsequently, we use inner product operation $\\odot$ to compute the similarity of the patient feature $\\hat{p}^{(t)}$, and the medication features $\\hat{E}_m$ in the joint space as Eq. (10):\n$m^{(t)} = \\sigma (\\hat{p}^{(t)} \\odot (\\hat{E}_m)), \\forall i \\in \\{1,..., |M|\\}.$ (10)\nThe predicted recommendation probabilities $m^{(t)} \\in [0,1]^{|M|}$ are obtained by a sigmoid function $\\sigma$."}, {"title": "3.4 Historical Information Integration", "content": "Within the realm of medication recommendation, it is of utmost importance to effectively model the historical clinical information of patients, particularly those suffering from chronic ailments that necessitate long-term medication. Prior studies commonly employ the extraction of historical features, such as diagnosis, procedure, and symptom, to capture the patient's clinical trajectory [38, 39]. Nevertheless, the observed patient data often suffers from incompleteness and inadequacy, subsequently resulting in imprecise predictions in the CMR task [31]. To overcome this limitation, we propose a historical information integration mechanism that directly assimilates the patient's medication history. This module generates a comprehensive representation of the patient's historical clinical condition by duly considering the temporal patterns of medication usage. By leveraging this integrated information, we aim to enhance the accuracy and precision of medication recommendations.\nFirstly, we denote $r_m^{(t)}$ as the textual description of the medications used in the t-th visit and obtain the medication representations of all the past visits $h_m^{(i)} \\in \\mathbb{R}^{d_{enc}}$ for the visit $i \\in \\{1,..., t - 1\\}$ through the similar process as in Eq. (1):\n$h_m^{(i)} = \\text{PLM-Encoder}(r_m^{(i)}), \\forall i \\in \\{1,..., t - 1\\}.$ (11)\nThen, we use the patient representation in Eq. (4), $p^{(t)}$, as the query vector to calculate the selection score $\\{w_m^{(i)}\\}_{i \\in \\{1,..., t - 1\\}}$ along the historical medication combination features. Let $H_m \\in \\mathbb{R}^{(t-1) \\times d_{enc}}$ be the matrix stacked by the historical medication embeddings $\\{h_m^{(i)} | i \\in \\{1,..., t - 1\\}\\}$, we calculate $w_m$ as follows:\n$H_3 = H_m W_3, H_4 = p^{(t)} W_4,$ (12)\n$\\{w_m^{(i)}\\}_{i \\in \\{1,...,t-1\\}} = \\text{softmax}(\\frac{H_3 H_4^{\\intercal}}{\\sqrt{d_K}}),$ (13)\nwhere $H_3 \\in \\mathbb{R}^{(t-1) \\times d_K}, H_4 \\in \\mathbb{R}^{1 \\times d_K}$, and $W_3, W_4 \\in \\mathbb{R}^{d_{enc} \\times d_K}$ are learnable parameters. Finally, the historical medication representation of the patient can be computed as follows:\n$p_m = \\sum_{i \\in \\{1,...,t-1\\}} w_m^{(i)} h_m^{(i)} W_m + b_m,$ (14)\nwhere $W_m \\in \\mathbb{R}^{d_{enc} \\times d_K}$ and $b_m \\in \\mathbb{R}^{d_K}$ are trainable parameters. The embedding $p^{(t)} + p_m)$ can be considered as the patient feature for the alignment in Eq. (10)."}, {"title": "3.5 Training Objective", "content": "For each patient in the training set, we use the bce loss in Eq. (15) as the optimization objective, which is a summation for all the T times visits. Here, $m^{(t)} \\in \\{0,1\\}^{|M|}$ and $\\hat{m}^{(t)} \\in [0,1]^{|M|}$ are the ground truth label and recommendation probabilities for a drug set M, respectively.\n$L_{bce} = - \\sum_{t=1}^T \\sum_{i=1}^{|M|} (m_i^{(t)} \\log \\hat{m}_i^{(t)} + (1 - m_i^{(t)}) \\log (1 - \\hat{m}_i^{(t)} ))$ (15)\nWe utilize the margin loss $L_{mar}$ in Eq. (16), aiming to enlarge the discrepancy between the predicted probability of positive and negative cases in the prescription.\n$L_{mar} = \\sum_{t=1}^T \\sum_{\\{i | m_i^{(t)}=1\\}} \\sum_{\\{j | m_j^{(t)}=0\\}} \\text{max}\\{1 - (\\hat{m}_i^{(t)} - \\hat{m}_j^{(t)}), 0\\}$ (16)\nThe final training objective of the recommendation prediction can be formulated as:\n$L = \\alpha L_{bce} + (1 - \\alpha) L_{mar},$ (17)\nwhere $\\alpha$ is a hyperparameter to balance these two loss terms."}, {"title": "3.6 Prediction of Mt", "content": "In the prediction stage, given the medication set $M$ and the model $f(.)$, we first obtain the features for all the medications as $E_m$. For the $t$-th visit of the patient $v$, we acquire its representation $\\hat{p}_v = p^{(t)} + \\hat{p}^{(t)}$ by employing $f(.)$ as the query embedding. Subsequently, we compute the similarities between $\\hat{p}_v$ and all medications in $E_m$ and then sort the medications based on similarity in ascending order. To produce the final recommendation set $M_t$, we apply the predefined threshold value $\\delta$ to select medications with a similarity greater than $\\delta$."}, {"title": "3.7 Complexity Analysis", "content": "Given $N$ patients, $|M|$ medications, $d_K$ as the hidden dimension of MLP layer, $d_{enc}$ as the output dimension of PLMs, $d_s$ as the output dimension of GNN encoder, $T$ as the maximum number of historical visits for all patients, and $iter$ as the number of iterations, we analyze the complexity of NLA-MMR as follows.\nTime complexity. The time complexity of linear feature transformation (Eq.(4)) is $O(T\\cdot d_{enc})$, and that of the attention between patient textual descriptions (Eq.(2)-(3)) is $O(Td_{enc} d_K)$. For the GNN encoder (Eq.(5)-(7)), the time complexity is $O(|M|\\cdot L\\cdot (e_o\\cdot d_s+n\\cdot d)$, where $L$ is the number of GNN layers, $e_o$ is the number of edges in a molecular graph, and $n$ denotes the number of atoms. The time complexity of MLP layers in Eq.(9) is $O(Td_{enc} d_K)$ for patients and $O(|M|\\cdot (d_{enc} + d_s) d_K)$ for medications, respectively. The complexity of the output layer in Eq.(10) for medication prediction is $O(T |M| d_K)$. The complexity of attention between all past visits (Eq.(12)-(14)) is $O(Td_{enc} d_K)$. With an average degree of 2.1 in molecular graphs for MIMIC-III and MIMIC-IV, and $d_s < d_K < d_{enc}$, the total time complexity is $O(iter\\cdot N\\cdot (Td_{enc} + |M|d_{enc} d_K + |M| \\cdot L \\cdot n\\cdot d))$.\nSpace complexity. The space of linear feature transformation in Eq.(4) is $O(d_{enc})$. For attention in the patient representation and historical medication integration, the space is $O(d_{enc} d_K)$. The GNN encoder (Eq.(5)-(7)) costs $O(n d_s)$ space. The matrix in Eq.(10) costs $O(|M|\\cdot d_K)$ space for the medication prediction task. Assume $|M| < d_K < d_{enc}$, the total space complexity is $O(d_{enc} + n\\cdot d_s)$."}, {"title": "4 EXPERIMENTAL STUDY", "content": "In this section, we describe the experimental setting (\u00a74.1) and report the experiments in the following facets: (1) Compare the modeling effectiveness of NLA-MMR with state-of-the-art drug recommendation approaches (\u00a74.2). (2) Conduct ablation studies to test the core components of NLA-MMR (\u00a74.3). (3) Test the effect"}, {"title": "4.1 Experimental Setup", "content": "Baselines. To comprehensively evaluate NLA-MMR, we compare with 3 instance-based methods: LR, ECC [25], LEAP [40], and 7 longitudinal-based methods: RETAIN [3], GAMENet [20], MICRON [37], SafeDrug [38], COGNet [36], DrugRec [31] and MoleRec [39].\nDatasets. We use 3 public datasets: MIMIC-III [11], MIMIC-IV [12] and eICU [24]. For the MIMIC-III and MIMIC-IV datasets, we follow the data processing of DrugRec [31], which collates the diagnosis, procedure, and medication records of patients. For the eICU dataset, we only collect the diagnosis and medication records of the patients since the eICU dataset itself does not include procedure records. Because the DDI between the medications in the eICU dataset is not provided, we do not evaluate the DDI rate on eICU. Table 1 lists the profiles of the processed data. For the MIMIC datasets, we choose the ATC Third Level code as the target label and extract its corresponding textual description from DrugBank [35] as the textual medication description. As the ATC code is not available in the eICU dataset, we choose the generic therapeutic class (GTC) as the target label and obtain the textual description of the GTC code by engaging professionals to annotate its corresponding ATC Third Level code. The textual description of the ATC Third Level code is then adopted as the textual description of the GTC code. We split train, validation, and test by 2/3, 1/6, and 1/6, respectively.\nEvaluation Metrics. We use the following metrics: (a) F1 score (F1) - the harmonic mean of precision and recall; (b) Jaccard score (Jaccard) - the ratio of the intersection to the union of predicted and true labels; (c) Precision Recall AUC (PRAUC) - the area under the recall-precision curve, measuring the performance of a model across various recall levels.\nImplementation and Reproducibility. Our experiments are performed using PyTorch 2.0.1. For drug molecule encoding, we adopt a 4-layer GIN [28] with a hidden embedding size of ds = 64. For each molecule graph, the 9-dimensional initial node features contain the atomic number and chirality, as well as other additional atom fea-"}, {"title": "4.2 Overall Performance", "content": "We compare NLA-MMR with the baselines in terms of Jaccard, F1, and PRAUC on MIMIC-III, MIMIC-IV and eICU using BioBERT [17] as the PLM. The comparison results in Table 2 show that NLA-MMR outperforms previous approaches regarding all the evaluation metrics, by a significant margin. The main reason for this is our effective integration of expert knowledge through the introduction of PLMs to extract patient and medication representations, followed by the alignment of the representations. Additionally, our medication representations, which take into account both the chemical structures and textual descriptions, exhibit greater expressiveness compared to the representations employed in MoleRec and DrugRec, which fail to leverage the textual medication descriptions for learning medication representations. The instance-based approaches such as LR, ECC, and LEAP perform poorly and this is because these approaches fail to utilize longitudinal information in the EHR. Among the longitudinal-based methods, GAMENet incorporates additional graph information, while SafeDrug and MoleRec utilize drug molecule structures and substructures, respectively. These enhancements contribute to further performance improvements.\nThe average DDI rate of the ground truth medication combinations is 0.08238 and 0.07423 in MIMIC-III and MIMIC-IV, respectively. We compute the DDI rate from the recommendation results of each method. Then we report the difference between the DDI of each method and that of the ground truth (i.e., \\Delta DDI) in Figure 3. We can see that the DDI rate from the predicted results by NLA-MMR has the smallest difference from that of the ground truth. This observation demonstrates that our proposed NLA-MMR model proficiently simulates physicians' medication prescribing patterns."}, {"title": "4.3 Ablation Study", "content": "In this section, we conduct ablation studies to investigate the effectiveness of different components and settings of NLA-MMR, including the drug representation module, cross-attention fusion, historical information integration mechanism, and using different types of textual descriptions in the EHRs for cross-attention fusion. First, we ablate the structure-level representation and PLM-based functional representation in NLA-MMR. From Table 3 (a)-(b), we can observe that conducting the recommendation solely based on\neither the structural or textual medication representation results in performance degradation, which confirms the importance of incorporating both structural information and textual knowledge in the medication modality.\nSecond, to exploit the effectiveness of the cross-attention fusion in the patient representation module and historical information integration technique, we remove these two components, respectively. As shown in Table 3 (c)-(d), only leveraging the feature in Eq. (1) as the patient representation will degrade the performance of our proposed model to a large extent, showing our model can benefit from fusing different types of texts in the EHRs. Besides, we notice that disabling the historical information hurts performance significantly on all evaluation metrics.\nThird, we investigate the importance of different types of text from the patient modality used in the cross-attention fusion module in Table 3 (e)-(g). For each ablation, we ignore one type of textual description for conducting cross-attention fusion in Eq. (4). We observe that implementing the cross-attention with two types of textual description is also beneficial and the best performance is achieved with all the types of textual description."}, {"title": "4.4 Effect of Pretrained Language Models", "content": "To assess the impact of different PLMs on NLA-MMR, we use five public releases of PLMs, including BioBERT [17], SciBERT [1], ClinicalBERT [16], PubMedBERT [6] and BlueBERT [23]. Among these PLMs, BioBERT is initialized with the standard BERT model and then continues pretraining using PubMed abstracts, while PubMed-BERT is pretrained from scratch using these texts. ClinicalBERT conducts continual pretraining from BioBERT with clinical notes from MIMIC-III [11]. BlueBERT mixes PubMed and MIMIC-III [11] to conduct continual pretraining from BERT. SciBERT is pretrained from scratch using biomedicine and computer science articles. Table 4 compares the performance of NLA-MMR using five different PLMs. The evaluation measures for the NLA-MMR implemented with all five PLMs are much higher than those for the best baseline MoleRec, which confirms the general applicability and superiority of our proposed approach. We observe that the model achieves the best performance when using BioBERT as the PLM. In particular, even though clinical notes in MIMIC-III are relevant to the domain of diagnosis and procedure texts in our drug recommendation task, adding them to the pretraining corpus does not bring any advantage, as is evident by the results of ClinicalBERT and BlueBERT. This reveals that mixed-domain pre-training of the texts related to the patients and drugs is not beneficial to the CMR task, which supports the assumption that textual descriptions of the patient and drugs are different from the two modalities. However, the PLM such as SciBERT mixing out-of-domain texts from science articles during the pretraining procedure leads to worse performance."}, {"title": "4.5 Effect of Historical Visits", "content": "To evaluate the impact of the number of historical visits on the model performance, we test our approach with different numbers of visits on MIMIC-III using BioBERT. As a comparison, we include the two strongest baselines MoleRec and DrugRec, which\nalso incorporate historical information. Figure 4 shows the Jaccard, F1, and PRAUC of NLA-MMR and the two baselines when setting the number of visits in \\{1, 2, 3, 4\\}. We observe that NLA-MMR achieves relatively better performance with more visits, while the performance of MoleRec almost stays flat and DrugRec consistently exhibits poor performance. This indicates that the design of the historical information integration mechanism can effectively incorporate prescription information of historical visits into the CMR task, thereby directly enhancing the accuracy of drug predictions. On the other hand, MoleRec relies on an RNN-based mechanism, which may not be as adept at capturing and utilizing the relevant information from past visits, leading to its relatively stagnant performance. Note that both NLA-MMR and DrugRec achieve the best performance by utilizing the historical information in the last two visits. It suggests that outdated information may not provide useful guidance and could potentially lead to misleading prescriptions in current drug predictions."}, {"title": "4.6 Visualization", "content": "Recall that in the medication representation module, we incorporate the PLM-based functional representation with GNN-based structure-level representation as the medication representation (Eq. (8)). In this section, we conduct a case study to explore whether the structure-level representation can capture distinct information beyond the textual medication descriptions, and thus providing extra assistance for the CMR task. First, we compare the two drug representations, the pure textural representation $E_f$, and the representation $E_m$ incorporated extra structural feature by GNN. Figure 5 shows the visualization of the two representations, i.e., w/o GNN and w/ GNN, on the MIMIC-III and MIMIC-IV by t-SNE [32], where BioBERT is used as the PLM. We observe that the distributions of the representations w/o and w/ GNN are different. It is noticed that the NLA-MMR model can learn the chemical properties of drugs because the incorporation of structural information. The drug representations derived from chemical structures and\ntextual descriptions exhibit noticeable distinctions when compared to text-based drug representations in the embedding space. Furthermore, to gain a better understanding of the multi-modal alignment module in NLA-MMR, we visualize the representation of 1000 patient samples and the representation of all the 112 drugs, generated by NLA-MMR before and after training. Figure 6 presents the two representations of MIMIC-III by t-SNE, where BioBERT is used as the PLM. We observe that before training the representations of patient and drug are two separate manifolds (Figure 6(a)), while after the training, the two manifolds are closer in the latent space and present a trend of integration (Figure 6(b)). This visualization confirms that our multi-modal alignment module is capable of aligning the representations of patients and drugs."}, {"title": "4.7 Hyper-parameter Sensitivity", "content": "In this section, we test the parameter sensitivity of NLA-MMR on the MIMIC-III and MIMIC-IV datasets using BioBERT as the PLM. Impact"}]}