{"title": "Graph-defined Language Learning with LLMs", "authors": ["Huachi Zhou", "Jiahe Du", "Chuang Zhou", "Chang Yang", "Yilin Xiao", "Yuxuan Xie", "Xiao Huang"], "abstract": "Recent efforts leverage Large Language Models (LLMs) for modeling text-attributed graph structures in node classification tasks. These approaches describe graph structures for LLMs to understand or aggregate LLM-generated textual attribute embeddings through graph structure. However, these approaches face two main limitations in modeling graph structures with LLMs. (i) Graph descriptions become verbose in describing high-order graph structure. (ii) Textual attributes alone do not contain adequate graph structure information. It is challenging to model graph structure concisely and adequately with LLMs. LLMs lack built-in mechanisms to model graph structures directly. They also struggle with complex long-range dependencies between high-order nodes and target nodes.\nInspired by the observation that LLMs pre-trained on one language can achieve exceptional performance on another with minimal additional training, we propose Graph-Defined Language for Large Language Model (GDL4LLM). This novel framework enables LLMs to transfer their powerful language understanding capabilities to graph-structured data. GDL4LLM translates graphs into a graph language corpus instead of graph descriptions and pre-trains LLMs on this corpus to adequately understand graph structures. During fine-tuning, this corpus describes the structural information of target nodes concisely with only a few tokens. By treating graphs as a new language, GDL4LLM enables LLMs to model graph structures adequately and concisely for node classification tasks. Extensive experiments on three real-world datasets demonstrate that GDL4LLM outperforms description-based and textual attribute embeddings-based baselines by efficiently modeling different orders of graph structure with LLMs.", "sections": [{"title": "1 Introduction", "content": "Text-attributed graphs have become essential data representations in various domains, spanning from social networks to biological systems [11, 17]. In these graphs, each node is associated with textual attribute and class label. The pioneering studies indicate that a node's class label is influenced not only by the attributes of its immediate neighbors but also by those several hops away [5]. Consequently, accurate class label predictions require capturing graph structural information when aggregating attributes from nodes across different hops. Graph neural networks (GNNs) address this through message-passing mechanisms, enabling concise and effective capture of graph structures [22, 31].\nLarge language models (LLMs), such as GPTs [1], have demonstrated exceptional text manipulation capabilities across various machine learning tasks. Recent efforts have aimed to leverage these capabilities for modeling graph structures in text-attributed graphs"}, {"title": "2 Preliminary", "content": "Notation. The text-attributed graph could be represented by a triple G = (V, &, X), where V denotes the set of nodes {01, 02, .., ||} with the size |V|; & \u2286 V \u00d7 V represents the set of edges between nodes with the size 8. And the edge set & is encoded in the adjacency matrix A \u2208 R|V|\u00d7|V|, where Aij = 1 if there is an edge between nodes vi and vj, Aij = 0 otherwise; X = {X1, X2, ..., XV } represents the text attributes associated with each node, where xi \u2208 X is the node attribute associated with node vi \u2208 V. The node attribute xi could be a paper abstract, an item description, or other textual documents.\nNode Classification Task. Given the graph G, we aim to learn an embedding vector ti for each node that captures both the structural information from the graph G and the semantic information from the node attribute xi to predict the node class label.\nFor each class, a linear transformation projects the embedding ti to a score l and the scores lf are passed through a softmax function to obtain the predicted probabilities \u0177 for class c:\n$\\hat{y}=\\frac{exp(w_c^t t_i + b_c)}{\\Sigma_{\\hat{c}} exp(w_{\\hat{c}}^t t_i + b_{\\hat{c}})}$\nwhere we and be are the weights and bias for class c. Then we calculate the classification loss yi associated with each node based on the predicted probabilities \u0177.\nWe adopt the cross-entropy loss and calculate the loss as follows:\n$L_{CE} = \\sum_{i=1}^{V} \\sum_{c=1}^{C} y_{ic} log(\\hat{y}_{ic})$\nwhere C is the number of classes, y is the true class label (1 if node i belongs to class c, otherwise 0), and \u0177 is the predicted probability of class c.\nLow Rank Adaption. To fine-tune LLMs efficiently, we leverage Low-Rank Adaptation (LoRA), a widely used parameter-efficient fine-tuning technique that accelerates the learning process with less memory consumption. LoRA incorporates the trained matrices into each layer of the LLM [15] through low-rank decomposition. Specifically, given the weight matrix of one layer of LLM Wo\u2208 Rd\u00d7k, LoRA introduces W0 +AW = W0 +BA, where B \u2208 Rdxr, A \u2208 Rr\u00d7k, and the rank r \u00ab {d, k}, for LLM optimization. A is initialized with Gaussian distribution and B is initialized with zero, therefore, AW = BA is zero at the beginning of the fine-tuning. During the"}, {"title": "3 Methodology", "content": "In this section, we introduce GDL4LLM, a graph language learning framework. As illustrated in Figure 1, LLMs learn the graph language through two primary stages. The first pre-training stage involves collecting a graph language corpus and pre-training LLMs on this corpus to learn the underlying language patterns. The second fine-tuning stage applies the learned graph language to represent the subgraph centered around the target nodes for fine-tuning pre-trained LLMs. And we demonstrate how to incorporate the memory bank to assist graph language in handling large-scale graph."}, {"title": "3.1 Graph-defined Language Pre-training", "content": "We begin by defining the fundamental concepts in graph language, such as graph tokens and graph sentences. We then demonstrate how to derive graph sentences from a given graph to create a graph language corpus and pre-train LLMs on this corpus. And we prove that the pre-training objective encodes graph structural information in each step under certain assumptions."}, {"title": "3.1.1 Graph Token and Graph Sentence", "content": "Graph Node as Graph Token. In graph language, we define the graph token set as equivalent to the node set V. To facilitate the recognition and processing of graph tokens, we extend the LLM tokenizer vocabulary by assigning each node in the given graph a unique token identifier. For instance, node 1 in the graph is represented as \"<node_1>\" in the graph language. These graph tokens serve as the fundamental units of computation in graph language.\nGraph Path as Graph Sentence. In graph language, we define a graph sentence as a sequence of edges connecting a series of graph tokens. For example, in Figure 1, the first graph sentence in the pre-training stage is <node_8><node_3><node_10><node_12><node_9>. We traverse the graph and translate the graph into a collection of sampled graph sentences formed as a graph language corpus C, i.e., C = {$i}, i = {1, 2, . . .}. This corpus inherently encodes graph structural information; for instance, graph tokens associated with high degree nodes will appear frequently in the corpus. Next we introduce how to traverse the graph and sample sentences."}, {"title": "3.1.2 Graph Corpus Sampling", "content": "The key idea in sampling graph sentences is to capture local graph structural information centered around each node. Any graph token can serve as the start token for a graph sentence. To traverse the graph and extend the graph sentence from the start token, we employ random walks to control the traversal process. Given a node vi, the probability of node vj becoming the next graph token is:\n$P(v_j|v_i) = \\begin{cases} \\frac{A_{ij}}{\\Sigma_{v_j \\in N(v_i), e_{ij} \\in &} A_{ij}} & v_j \\in N(v_i), \\\\ 0, & otherwise. \\end{cases}$", "latex": true}, {"title": "3.1.3 Pre-training LLMs on the Corpus", "content": "To project out-of-vocabulary graph tokens into a token embedding space comprehensible to LLMs, we utilize off-the-shelf text encoding models, such as LLM itself, to summarize the textual attribute x\u012b into a unified initial graph token embedding. To facilitate the incorporation of graph structural information into the graph token embedding, we introduce a learnable linear projector Wp \u2208 Rd\u00d7d, which aligns the initial graph token embeddings with the graph structure information.\nWith graph token embeddings in hand, we adopt the language modeling objective to pre-train the LLMs on the corpus. First, we initialize the LoRA weight \u2206W\u2081 and Wp for optimization. During pre-training, Wp maps graph token embeddings that appear in similar contexts to similar embeddings in the transformation, while the additive weight AW\u2081 learns the graph token transition patterns in the graph sentences. We achieve this by iteratively predicting on the graph language corpus, maximizing the likelihood of the correct graph token appearing next in a graph sentence:\n$L_{pre} = \\sum_{i=1}^{kVl} \\sum_{q=1} log P(v_q | v_0, ..., v_{q-1}; W_p, W_1, W_h)$", "latex": true}, {"title": "3.1.4 Connection between Pre-training and Graph Learning", "content": "The graph language bridges the gap between linguistic and structural representations by leveraging the statistical pattern of node degrees in the prompt. Since we use this inner product to rank the next candidate graph token, the pre-training objective increases the inner products between the hidden representation of the next graph token th,q and the next graph token weight vector Wing in Wh. For frequently sampled graph sentences, the representation of their next graph token will be well optimized and closer to the next graph token weight vector. The sampling frequency of a specific graph sentences correlates with the node degree of each graph token in the sentence. For example, consider two graph sentences of equal length sampled from areas with different connection densities. The probability of sampling a specific sequence is lower in densely connected regions compared to sparsely connected ones. This is because densely connected areas offer more potential graph sentences, reducing the likelihood of any particular sentence being"}, {"title": "3.2 Graph Structure-aware Fine-tuning", "content": "We first introduce how to fine-tune pre-trained LLMs with graph language for node classification. Then, we discuss how to integrate node textual attributes with graph language in the prompt to enhance both the pre-training and fine-tuning stages."}, {"title": "3.2.1 Absorbing Graph Structure Knowledge", "content": "We integrate the learned graph structure knowledge into the LLMs by merging the pre-trained LoRA weights. The additive parameters of LoRA learned in the pre-training stage contain the knowledge of the graph structure. Fine-tuning the same LoRA weights may risk forgetting the knowledge acquired in the pre-training stage. Therefore, we initialize another set of LoRA weights AW2 to adapt LLMs to the fine-tuning stage. The parameters of LLMs are now composed as follows:\nW = W_0 + \\Delta W_1 + \\Delta W_2\nHere, the parameters Wo and AW\u2081 remain fixed, while AW2 are the learnable parameters in the fine-tuning stage. The weights in the head layer have to be reinitialized as \u0174h \u2208 Rd\u00d7C to adapt to the class label, while the weights in the projector remain fixed. We use LoRA to add additional trainable parameters to the projector and obtain Wp. We then use these parameters to perform node classification in the subsequent fine-tuning process."}, {"title": "3.2.2 Fine-tuning with Graph Language", "content": "To classify a node vi, we sample multiple graph sentences {S1, S2, ..., Sk} of length I originating from the same graph token associated with node vi, following the method described in sub-subsection 3.1.2. We then concatenate these graph sentences into a small graph language corpus. This corpus describes the different orders of graph structural information centered around the target node vi and provides sufficient context for classification. The overall classification loss is defined as follows:\n$L_{CE} = \\sum_{i=1}^{V} log P(Y_i | S_1, S_2, ..., S_k; \\hat{W_p}, \\Delta W_2, \\hat{W_h})$", "latex": true}, {"title": "3.2.3 Fine-tuning with Textual Node Attributes", "content": "We could further combine the graph language corpus with the textual attributes of each node to form a composite document, enriching the information in the prompt. While graph language captures structural information, it does not explicitly utilize textual attributes and semantic information of attributes is insufficiently captured. Our goal is to preserve these textual attributes in the prompt, rather than merely using them to generate initial graph token embeddings. To achieve this, we first start with an empty document; traverse each graph token in the graph language corpus and append the visited graph tokens' textual attributes to the document. This document is then included in the prompt besides the graph language in the both pre-training and fine-tuning stages. LLMs could leverage the powerful natural language comprehension abilities of this document to comprehend node connection and generate accurate node classification results."}, {"title": "3.2.4 Dynamic Memory Bank", "content": "Each node is represented as a unique token, which can lead to unacceptable memory costs when dealing with large-scale graphs. To address this issue, we introduce a memory bank mechanism to efficiently handle large-scale graphs in the graph language framework. (i) To reduce GPU memory usage, we dynamically look up and retrieve text embeddings and weights of Wh from the memory bank. This allows the framework to focus only on the embeddings relevant to the current computation, reducing overhead. (ii) During training, sampled softmax is performed by using other nodes in the current batch as negative samples. This ensures that the retrieved nodes serve both as positive and negative samples, avoiding the need to compare all nodes in the graph. After each batch, relevant weights in the memory bank are dynamically updated to reflect the latest training progress, ensuring consistency and accuracy across iterations."}, {"title": "4 Experiments", "content": "In this section, we examine the effectiveness and efficiency of our model and understand how it behaves. Comprehensive experiments across five datasets have been conducted. Our study aims to address the following four research questions:\n\u2022 RQ1: How effective is GDL4LLM compared to state-of-the-art text-attributed frameworks for node classification?\n\u2022 RQ2: How do the pre-training stage and textual attributes in the prompt contribute to the overall model performance?\n\u2022 RQ3: How sensitive is GDL4LLM to hyperparameter selection, such as the length of sampled graph sentences, and the choice of different LLM backbones?\n\u2022 RQ4: How efficiently does GDL4LLM model different orders of graph structures in terms of token usage?"}, {"title": "4.1 Dataset", "content": "We evaluate the performance of GDL4LLM using five datasets: ACM, Wikipedia, Amazon, OGBN-Arxiv and OGBN-Products. Three of these datasets have been manually created from the raw corpus along with their respective descriptions while two of them are classic graph neural networks benchmarking. The division into training, validation, and test sets follows the settings used in previous studies [37] The statistics for these three datasets are presented in Table 2.\nWiki. The raw data consists of text in all languages from Wikipedia articles, encoded in UTF-81. The processed dataset regards cleaned articles as nodes and their relationships as edges. We extract the primary content of each article as attributes, and hyperlinks in the articles as their relationships. A text-attributed graph is then created based on the hyperlink connections between the articles.\nACM. The ACM dataset contains papers published in flagship conferences, such as KDD and SIGMOD. This dataset comprises 48,579 documents from the Association for Computing Machinery (ACM) [32]. The papers are treated as nodes, with their abstracts used as textual attributes, and a directed graph is built based on citation links of these papers. The papers are categorized under different topics, which serve as labels.\nAmazon. The Amazon dataset comprises product reviews and associated metadata from the famous e-commerce website [25]. The products are treated as nodes. The review texts are treated as node textual attributes, and edges represent node relationships like co-purchasing or co-viewing.\nOgbn-Arxiv. The OGBN-Arxiv dataset is a citation network of 169,343 arXiv papers, where nodes represent papers and directed edges indicate citations. The textual attributes contain the article title and abstract. The task is to predict the subject area of each paper among 40 classes.\nOgbn-Products. The OGBN-Products dataset is a large Amazon co-purchasing network with 2,449,029 nodes and 61,859,140 undirected edges. The task is to classify products into 47 categories. This dataset is ideal for evaluating the scalability of models on large-scale node classification tasks."}, {"title": "4.2 Baselines", "content": "We incorporate the following state-of-the-art baselines in our main comparison."}, {"title": "4.3 Main Comparison (RQ1)", "content": "Table 3 and 4 presents a main comparison of GDL4LLM with other baselines across five datasets with respect to the micro-F1 score while Table 8 and 7 shows the macro-F1 score. To evaluate the effectiveness of our module, we include various techniques from both description-based and textual embedding-based approaches. The baselines are categorized into three groups: (1) textual embedding-based: using fine-tuned LM embeddings in GNNs for label classification; (2) description-based frameworks for text-attributed graphs, some of which use graph descriptions to describe the graph structure centered around target node, e.g., InstructGLM, LLAGA and some of which are specialized text-attributed graph frameworks; and (3) textual embedding-based: using fine-tuned LLMs to embed textual attributes and GNNs for aggregation.\nThe first group fine-tunes small LMs plus GNN are capable of achieving satisfactory results, but the small LMs themselves can not achieve very good performance. The performance is largely due to the advantages offered by the GNN in capturing graph structural information. These methods are limited by both the capacity of the language model and lack of structural information in the textual attributes. Therefore these approaches could not achieve further promising results."}, {"title": "4.4 Ablation Studies (RQ2)", "content": "In this section, we investigate the effects of different training strategies on the performance of the GDL4LLM model. We show the results on the test set in Figure 2. We explore two variants that include pre-training stages and additional textual attribute, emphasizing their impact on enhancing models' capabilities. Specifically, we analyze the role of pre-training by comparing models that are initialized from a pre-trained state against those trained from scratch, assessing improvements in generalization and performance on downstream tasks. Furthermore, we explore the impact of incorporating additional textual attributes within the prompt,"}, {"title": "4.5 Backbones & Hyperparameters (RQ3)", "content": "Backbones. In this section, we examine the impact of the backbone on GDL4LLM model performance and show the results in Figure 3. While our main comparison uses Llama-2, we also evaluate Llama-3 to assess how different LLMs affect overall results. Comparing these two versions highlights their varying capabilities and performance metrics. Our analysis reveals that Llama-3 slightly outperforms Llama-2 within the framework. This improvement may be attributed to several factors, including advancements in Llama-3's architectural design and enhanced training data quality. Furthermore, we conduct an ablation study for Llama-3, which demonstrates the benefits of the pre-training objective in improving performance of different LLM backbones.\nHyperparameters. Additionally, we investigate the effects of hyperparameters, particularly focusing on the length of sampled graph sentences I and the number of sampled graph sentences k. By experimenting with varying sentence lengths, we understand how this parameter influences the model's exploration and representation of high-order structure information in the pre-train and fine-tuning, respectively. Figure 4 demonstrates that the optimal results occur with a sentence length of 8 and a sampling number of 6. Further increasing these values yields only marginal improvements. The improvement in the figure shows that our framework can effectively model high-order structural information. For example, when"}, {"title": "4.6 Token Efficiency Analysis (RQ4)", "content": "To evaluate the efficiency of our proposed GDL4LLM framework in leveraging high-order structural information, we conduct a comparison of token usage and computational time. As shown in Table 5, GDL4LLM significantly reduces token usage compared to InstructGLM and LLAGA-HO. While LLAGA-HO uses GNNs to summarize each hop of neighbors, its prompts remain verbose due to natural language task descriptions. In contrast, GDL4LLM employs succinct graph language prompts, eliminating the need for natural language and further enhancing efficiency. This reduction in token usage not only alleviates the computational burden but also enables the exploration of higher-order graph information compared with InstructGLM. Table 6 further highlights GDL4LLM's computational advantage, achieving faster training and inference times across"}, {"title": "5 Conclusion", "content": "In this paper, we identify two key limitations of using natural language alone to model text-attributed graphs with LLMs: (i) Graph descriptions become excessively verbose, particularly when modeling high-order neighbors, and (ii) textual attributes often lack sufficient structural information, limiting LLMs' ability to capture high-order graph structures. We address two main challenges: (i) the absence of a built-in mechanism in LLMs, akin to message passing in GNNs, and (ii) the long-range dependencies between high-order neighbors and target nodes. To overcome these challenges, we propose the GDL4LLM framework, which enables LLMs to model text-attributed graphs as if learning a new language. GDL4LLM consists of two main stages: (i) sampling a graph language corpus and pre-training LLMs on this corpus to adequately understand the graph, and (ii) fine-tuning LLMs with these graph language tokens to concisely represent graph structures centered around the target node. This approach allows LLMs to effectively capture graph structural information of various orders. Extensive experiments on five datasets demonstrate the efficiency and effectiveness of GDL4LLM in modeling text-attributed graphs for node classification tasks."}, {"title": "6 Related Work", "content": "6.1 GNNs for Text-attributed Graph\nIn traditional pipelines for text-attributed graph analysis, natural language processing techniques are first employed to extract features from textual data, which are then utilized in graph neural networks (GNNs) for graph propagation. Common NLP methods include Bag of Words [39], fixed embeddings like Word2Vec and GloVe[27], as well as the use of pre-trained models such as BERT and fine-tuned variations [9, 24]. These approaches establish the foundational representation of textual information within the graph structure. However, more recent advancements have led to the development of tailored graph learning methods specifically designed for text-attributed graphs. Notable examples include Graphformers [36] and MPAD [26], which represent word-adjacent relationships as graphs, offering an alternative perspective to text-attributed graph representations. GLEM [42] integrates graph structure and language learning using a variational Expectation-Maximization framework. Other tailored approaches [4, 7] aim to improve the flexibility and efficacy of graph-based analyses from complex and structured data."}]}