{"title": "SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation", "authors": ["Yi-Chia Chen", "Wei-Hua Li", "Cheng Sun", "Yu-Chiang Frank Wang", "Chu-Song Chen"], "abstract": "We introduce SAM4MLLM, an innovative approach which integrates the Segment Anything Model (SAM) with Multi-Modal Large Language Models (MLLMs) for pixel-aware tasks. Our method enables MLLMs to learn pixel-level location information without requiring excessive modifications to the existing model architecture or adding specialized tokens. We introduce an inquiry-based approach that can effectively find prompt points for SAM to perform segmentation based on MLLM. It combines detailed visual information with the powerful expressive capabilities of large language models in a unified language-based manner without additional computational overhead in learning. Experimental results on pubic benchmarks demonstrate the effectiveness of our approach. Our code is available on GitHub.", "sections": [{"title": "1 Introduction", "content": "With the rapid growth of generative AI, large language models (LLM) [5,15,17, 46, 47, 49, 60] become a focus of research and application due to their profound capabilities in understanding and generating text. They show innovative power in machine learning and marks the evolution of human-machine interaction.\nRecently, the progress has been made from simple text processing to the complex multi-modal understanding. The advent of Multi-modal Large Language Models (MLLMs) [1,21, 28, 33, 45, 58, 64] lies in incorporating image processing modules into LLMs. They successfully endow LLMs with the ability to process visual information, thereby bridging the significant gap between visual and linguistic tasks. Recent studies enabled MLLMs to engage in in-depth dialogues based on image content. Subsequent research enhanced MLLMs capabilities through data or structural modifications, leading to enhanced MLLMS that allow for the input and output of bounding boxes of objects to achieve fine-grained visual dialogues [6, 10, 11, 31, 37, 59]."}, {"title": "2 Chen et al.", "content": "Referring Expression Segmentation (RES) [48,51,53,66] aims to label image pixels corresponding to specific objects or reuns mentioned in natural language expressions. It involves accurately identifying and segmenting the objects referred to by linguistic descriptions. In this paper, we focus on RES and use MLLM to solve this task. However, bounding boxes alone are insufficient for precisely indicating object locations within images. This has led to research efforts focused on improving annotation granularity to pixel-level for MLLM, enhancing image information encoding, and designing models capable of outputting detailed segmentation masks [27,39,42,50]. Despite significant progress, these advancements require substantial modifications to the original MLLMs architectures [42]. Some study introduces additional model structures to output segmentation masks [39]; some others leverage the use of special tokens different from those in original LLMs [27,50] or rely on the application of multiple losses for model optimization [27]. These adjustments introduce architectural complexity to MLLM and may complicate model extension for additional tasks.\nIn this paper, we propose a simple solution that can enhance MLLM's abilities to understand object localization in pixel level. Our approach is simple but effective, which upgrades MLLMs' visual capabilities to a new level for accurate understanding the referring expressions of pixel-level location in images.\nOur method draws inspiration from the context provided below. Concurrently with the development of LLMs and MLLMs, the field of image segmentation has witnessed a significant breakthrough with the introduction of the Segment Anything Model (SAM) [25], a foundation model trained on the SA-1B [25] high-quality image segmentation dataset. SAM, a promptable segmentation model, can generate high-quality semantic-free segmentation masks in images based on prompts provided by the user, such as points or bounding boxes.\nWe observe that while MLLMs possess a profound understanding of image semantics, they struggle to articulate detailed pixel-level information. Conversely, SAM, although not semantically aware, can delineate intricate segmentation masks with minimal prompting. In light of this, we propose a novel methodology using SAM for MLLM (namely, SAM4MLLM) which seamlessly integrates MLLMS with SAM. Specifically, we employ a straightforward yet simple strategy, introducing pixel-level information into the training dataset without altering the original MLLM architecture. This enables MLLMs to grasp pixel-level information using the same text cross-entropy loss used by popular LLMs [6,17,23,47]. Considering the potential limitations of MLLMs in pixel expression due to input resolution constraints and a model architecture not explicitly designed for visual tasks, we further enhance the output with SAM, post-processing MLLM outputs to obtain higher precision segmentation masks in a relatively effortless manner. To establish a connection between SAM and MLLM, one straightforward approach is to enable MLLM to generate prompt points for SAM. However, effectively producing multiple points can be challenging. Therefore, we introduce a novel method that leverages the dialog capability of LLMs. Specifically, we proactively ask the MLLM to acquire effective prompt points for SAM. We tackle the problem of RES and demonstrate the effectiveness of our approach."}, {"title": "SAM4MLLM", "content": "Main contributions of this work are as follows:\nWe present SAM4MLLM, an approach allowing MLLMs to understand pixel-level details without altering the MLLM model architecture, introducing new tokens, or employing additional losses. It is simple yet effective for RES.\nTo connect MLLM and SAM, we introduce a novel method of actively querying the language system to obtain prompt point cues.\nThrough experiments on various RES benchmarks, including RES dataset, GRES, and ReasonSeg, we validate the effectiveness of SAM4MLLM and demonstrate its favorable performance in handling complex pixel-aware tasks."}, {"title": "2 Related Works", "content": "In this section, we review the related works of the topics: RES, image segmentation, MLLMs, and MLLMs toward segmentation.\nReferring Expression Segmentation. Early researches in RES focus on integrating features from language and vision models to effectively merge these two types of information. Yu et al. [56] combine the language attention and subject, location, and relationship modules to localize the target region. In STEP [9], a DNN architecture is applied to iteratively refine the segmentation heatmap. Subsequently, Hui et al. [22] introduce Linguistic Structure guided Context Modeling (LSCM) to aggregate the multi-modal features. To understanding the language expression in different perspective, VLT [16] generates several sets of queries and introduces a query balance module to focus on the most suitable query. Zhu et al. [63] regard RES as a point prediction problem and design a simple transformer-based network to perform referring segmentation. Recently, the method in [51] leverages a novel adapter to facilitate cross-modal information. These studies lay the foundation for subsequent work on MLLMs in RES.\nWith the advancement of multi-modal model, they have been introduced into the RES field, enhancing the accuracy and efficiency of segmentation. Wang et al. [48] introduce the multi-modal model CLIP [40] to RES tasks. With the rise of MLLMs, research based on these models has emerged, leveraging their remarkable abilities in understanding text and images. [27, 39, 42, 50]. Furthermore, it has been pointed out in [32] that classical RES benchmarks are not sufficiently comprehensive in some cases, leading to the proposal of General Referring Expression Segmentation (GRES) [32] dataset to broaden its application scope. GRES allows for the reference to multiple objects simultaneously and can address the absence of objects in images, further enhancing the applicability in practice. In LISA [27], a more complex dataset, ReasonSeg, is proposed. It requires models to possess complex reasoning abilities and a basic understanding of the real world, addressing challenges that are closer to real-world scenarios.\nImage Segmentation and Segment Anything. Image segmentation is a central task in computer vision, aiming to identify and label objects within images at the pixel level. Methods like Fully Convolutional Networks [35], Mask R-CNN [19] and Masf2Former [14] have greatly advanced the field. Recently,"}, {"title": "4 Chen et al.", "content": "the Segment Anything Model (SAM) [25] is trained on the SA-1B [25] dataset with one billion high-quality segmentation annotations. SAM can segment high-quality object masks based on simple prompts. EfficientViT-SAM [8] further introduces multi-scale linear attention into the ViT backbone of SAM, increasing the speed of SAM by several times without compromising performance. Our SAM4MLLM employ MLLMs to guide SAM for precise object segmentation.\nMultimodal Large Language Models (MLLMs). LLMs have proven their exceptional capabilities in the domains of language understanding and generation, with notable examples including GPT-3 [17], BLOOM [49], PaLM [15], OPT [60], LLAMA [46], LLaMA-2 [47], Mistral [23], Qwen [6], and others, significantly advancing the field of natural language processing. These models have not only demonstrated near-human levels of proficiency but have also spurred interest in the study of visual-language interaction, leading to the development of MLLMs. MLLMs are built upon LLMs by integrating innovative techniques that combine visual and linguistic modalities, such as the Perceiver Resampler introduced by Flamingo [2], the prompt tuning token by LLaMA-Adapter [58], the Q-Former by BLIP-2 [28], and the use of linear projection layers in LLaVA [33] to enable LLMs to interpret images.\nMLLMS Toward Segmentation. In MLLMs, researchers focus not only on enhancing the model's understanding of multimodal data but also empowering MLLMS with the capability to process detailed information. For instance, Det-GPT [38] introduces a method that combines MLLMs with open-vocabulary object detectors. GPT4RoI [59] incorporates region-of-interest information into instructions. Kosmos-2 [37] constructs a large-scale grounding image-text pairs dataset, named GRIT, which assists MLLMs in understanding regional information within images. Shikra [11] encodes all regional information in a linguistic form, eliminating the need for introducing new vocabulary, position encoders, or decoders to MLLMs. Ferret [54] uses a hybrid regional representation method that combines discrete coordinates with continuous features to describe regions within images. However, the model outputs of these methods are limited to bounding boxes and have not yet achieved pixel-level precision operations.\nBuilding on this foundation, Lai et al. [27] propose a method based on introducing [SEG] tokens and a SAM decoder, enabling MLLMs to perform reasoning segmentation tasks. In PerceptionGPT [39], a lightweight visual task encoder and decoder are adopted to handle segmentation masks, allowing MLLMs to input and output segmentation masks. Ren et al. encode masks within images into segmentation codebooks, coupled with a lightweight decoder for mask output. GSVA [50] extends upon LISA [27] by supporting multiple [SEG] tokens and introducing [REJ] tokens, applying MLLMs to GRES tasks. Rasheed et al. [42] propose a Grounding LMM model capable of generating natural language responses seamlessly integrated with corresponding object segmentation masks.\nAlthough the aforementioned models can output masks, they require modifications to the original MLLM architecture or the addition of new model structures to output masks, or the introduction of special tokens not belonging to the original LLMs. They may need to utilize multiple loss functions for simulta-"}, {"title": "SAM4MLLM", "content": "neous model optimization, which increases the complexity of MLLM design and poses obstacles to the model's expansion to more tasks. Our SAM4MLLM does not have these burdens, merely integrating with the off-the-shelf SAM model to output high-quality segmentation masks, thereby providing a new solution path for complex pixel-level tasks."}, {"title": "3 Method", "content": "In this section, we present our SAM4MLLM method. We first introduce how to encode segmentation masks with SAM's prompt, and then our solutions for prompting SAM using MLLM."}, {"title": "3.1 Encode Segmentation Mask into SAM Prompt", "content": "Existing MLLMs for segmentation (e.g., LISA [27], PerceptionGPT [39], GLaMM [42], GSVA [50]) rely on specialized design of model architectures, segmentation-specific tokens, and heterogeneous loss functions to predict object masks. E.g., LISA [27] introduces a special token [SEG] and the associated architecture. It uses dice and binary-cross-entropy losses for segmentation, combined with text loss for training. This increases the model complexity and optimization difficulty.\nOur method leverages SAM's characteristic that it can convert few discrete text prompt tokens (i.e., bounding box plus several points indicating whether they are inside or outside the object region) to high-quality continuous-boundary segmentation masks. Our SAM4MLLM uses the discretized image coordinate for points. We encode an arbitrary-shaped mask by using a bounding box and K points. The bounding box is expressed as $\\text{Prompts} \\in \\mathbb{N}^2$; the prompt of K points, each of which contains three values, a coordinate, y coordinate, and whether the point is on the object mask, are encoded as $\\text{Promptp} \\in \\mathbb{N}^{K\\times3}$.\nBy encoding continuous segmentation masks into discrete SAM prompts, we avoid adding any tokens or altering the model structure, while maintaining training with only text auto-regression cross-entropy loss. This method is consistent with the original training mode of language models, enabling MLLMs to understand pixel-level information and facilitate easier future model expansion."}, {"title": "3.2 Prompting SAM Using MLLM", "content": "To incorporate SAM into MLLM in a unified way, a main issue lies in acquiring the prompt points for SAM, including the points that are positive (inside) and negative (outside) the object mask region. To do this, we introduce two solutions, Prompt-Point Generation (PPG) and Proactive Query of Prompt-Points (PQPP). The former directly generates the proposal points by using the MLLM model in the inference stage. The later, on the other hand, acquires the points in an indirect manner; it uniformly samples the points in the bounding box at first, and then for each point asks the MLLM model whether the point is inside the object region or not. We respectively introduce them in the following."}, {"title": "SAM4MLLM-PPG.", "content": "In this method, an MLLM that can take both text-prompt and image inputs is adopted. To align the MLLM with segmentation utility, we use the parameter-efficient fine tuning technique, LoRA [20], to train the model based on some RES datasets with image-text pairs and ground-truth masks. LoRA outputs the location prompt including the bounding box $\\text{Prompts} \\in \\mathbb{N}^4$ and k groups of positive and negative points $\\text{Promptp} \\in \\mathbb{N}^{(n_1+n_2)k\\times3}$ as illustrated in Fig. 1(a), where a group contains $n_1$ positive and $n_2$ negative points ($n_1 = 2, n_2 = 1$ in our implementation).\nTo provide the location supervision to LoRA, we randomly sample K groups of points (K > k) in the training stage based on the object mask and then send these prompts to SAM. For every group, SAM delivers the segmentation result. We filter out the prompts with low IoUs compared to the ground-truth masks and only keep the top-k groups (Fig. 1(c)). In our implementation, only text loss (auto-regression cross-entropy loss) is required; K is typically 64 and k = 1. In the inference stage, LoRA directly delivers the points that are sent to SAM for segmentation, as shown in Fig. 1(b). More details an be found in Sec. 4.1."}, {"title": "SAM4MLLM-PQPP.", "content": "In this method, instead of producing the prompts directly, we propose to leverage the power of MLLM's query-response capability. We sample the prompt points and proactively ask the MLLM if they are inside (or outside) the mask. In the training phase, a bounding box and K groups of points are randomly sampled based on the ground-truth mask, and a dialog containing two rounds are conducted. In the first round of the dialog, LoRA responses a bounding box. In the second round, for each of the $(n_1 + n_2)K$ points, LORA responses whether it is inside the mask (yes or no) during training."}, {"title": "3.3 RES Training", "content": "To align the foundational MLLM to the RES task, we use the datasets containing the RES-relevant examples to guide the model toward the goal. Three datasets are used for training our SAM4MLLM to align with the RES task. Two of them (RES dataset and gRefCOCO dataset) contain RES data with ground-truth masks. The third (VQA) is a visual dialog dataset without masks, employed to enhance further the general capability of joint vision-language understanding. During training, to preserve the generalization ability of MLLM on images, we"}, {"title": "8 Chen et al.", "content": "freeze most of the network parameters, and adjust only the visual resampler of MLLM together with the LoRA adapter. The datasets are briefed below.\nReferring Expression Segmentation Datasets (RES dataset): Each sample in this dataset provides an image accompanied by a phrase denoting a specific object in the image. The phrase corresponds to only one object. This dataset includes publicly available subsets, refCOCO [57], refCOCO+ [57], and refCOCOg [36]. They are based on images from the MSCOCO [30] but compiled through different annotation processes. The primary difference between RefCOCO+ and RefCOCO is that the former prohibits the use of location-based descriptions (e.g., \"the person on the right side of the picture\"), thereby compelling annotators to focus on describing the appearance features of objects. RefCOCOg provides longer and more detailed descriptions that cover not only appearance information but may also include actions, locations, and details about relationships with other objects.\nGeneralized Referring Expression Segmentation (GRES) Dataset [32]: Similar to the RES dataset, each sample offers an image and a phrase describing the objects to segment. The difference is that the phrase may not be present in the image or may refer to multiple objects simultaneously. We use the publicly available gRefCOCO [32] dataset for this task. Our SAM4MLLM can naturally generate additional SAM prompts for segmenting multiple instances. In case where the queried objects are not present, we train our model to predict \"object not in the image.\"\nVisual Question Answering (VQA): To maintain the visual dialogue capability of MLLM, we incorporated VQA data, specifically using the VQAv2 [4]. For all the datasets mentioned above, we do not use data augmentation during training because flipping and/or cropping may change the relative position or relationship of objects in the image."}, {"title": "4 Experiments", "content": "In this section, we outline the experimental setups for our SAM4MLLM method, covering the network architecture, implementation details, evaluation datasets, and analysis of experimental results."}, {"title": "4.1 Implementation Details", "content": "Network Architecture: We use Qwen-VL-7B-Chat [6] as our MLLM backbone architecture for its ability to output bounding boxes from the pre-training phase. Specifically, the LoRA adapter is configured as follows: LoRA rank is set to 256, LORA alpha to 128, and LoRA dropout to 0.05. Regarding the SAM [25], we use EfficientViT-XL1-SAM [8] to accelerate the experiments, and we observe only minor accuracy loss in our pilot study.\nTraining Details: Our training is conducted on 8 NVIDIA 32G V100 GPUs, using float16 precision. We employ deepspeed [3] ZeRO2 for multi-GPU distributed training. We use Lion [12] as ours optimizer. The learning rate is set to"}, {"title": "SAM4MLLM", "content": "le-5, with a weight decay of 0.1. We employ a CosineAnnealing learning rate scheduler with a warmup period covering 3% of the total steps. The loss function includes only the text cross entropy loss of LLM. The batch size per GPU is 2, with a gradient accumulation set to 8. We truncate the maximum text length to 2048 during training and only train the model for 3 epochs to prevent overfitting.\nFine-tune SAM light-weight decoder: To ensure fair comparison and optimal performance of our model on the COCO extended dataset, we fine-tune our SAM lightweight mask decoder specifically. Given the inherent bias in mask annotations within the COCO dataset [25], conducting inference directly without fine-tuning SAM's lightweight decoder would not offer a fair comparison against methods that have been trained on COCO with mask decoders. Therefore, we fine-tuned our SAM decoder on the COCO dataset for one epoch. Additionally, to prevent data leakage, we excluded from COCO the images present in the RefCOCO, RefCOCO+, RefCOCOg, and gRefCOCO validation and test sets.\nPPG Pointing Strategy Detail: During the training data generation phase of PPG, we randomly sample 64 point groups within the ground truth bounding box. Each group consists of two positive points inside the ground truth mask and one negative point outside. We keep the 16 groups with the highest Intersection over Union (IoU) with the ground truth mask, and then randomly pick a single group from these. The chosen group of points are encoded into text using the proposed \"mask as prompt\" method to serve as the label for training. During testing, our parser retrieves two positive and one negative points as well.\nPQPP Pointing Strategy Detail: To train the PQPP, we randomly sample 10 points from the ground truth bounding box. These points are labeled as positive if they fall inside the ground truth mask, and negative otherwise. During testing, we uniformly sample 5\u00d75 grid of points from the bounding box produced by the MLLM. We then query the MLLM to determine whether each point lies inside the object. Based on MLLM's response (Yes or No), we filter the outcomes according to the response's confidence level associated with the output token (the probability of emitting that token). We only retain points with a confidence level greater than 0.9 and feed them into SAM to generate the mask."}, {"title": "4.2 Benchmarks", "content": "We use the datasets described in Sec. 3.3 for training. Their test splits are used for evaluation (RES dataset, GRES, VQA). In addition, we use ReasonSeg [27] as a zero-shot evaluation for segmentation from complex reasoning scenarios. This comprehensive evaluation assesses the versatility and effectiveness of our model across a diverse range of referring expression segmentation scenarios.\nIt is worth mentioning that, compared to other MLLM-based methods, our approach uses significantly less training data. A detailed comparison is presented in Tab. 1. For instance, GLaMM [42] is trained using the GranD [42] dataset, which has 11M images and 810M object masks. Its annotations are collected through various vision and language models including GPT-4-based rewrites of existing open-source datasets. In contrast, our SAM4MLLM only uses a small"}, {"title": "10 Chen et al.", "content": "amount of mask annotation data (100K images, 82K object masks) to enable MLLMs to learn general information, but can produce high-quality segmentation masks in conjunction with SAM."}, {"title": "4.3 Main Results", "content": "We compare the two variants of our SAM4MLLM, PPG and PQPP, with previous arts on various tasks. There have been numerous LLM-based methods emerging recently, but our comparisons primarily focus on their results using models of similar scales (7B).\nRES dataset: In Tab. 2, we present the performance of PPG and PQPP on the refCOCO datasets [57], where our approach outperforms most of the recent LLM-based methods and achieves comparable results to the most recent GLaMM [42]. Additionally, we observe distinct performance variances across datasets. Specifically, our method shows superior results to GLaMM on the RefCOCOg dataset with complex narrative queries, while we obtain inferior results on the RefCOCO and RefCOCO+ dataset with simple short text queries. This advantage likely arises from our model's streamlined architecture, which preserves the language model's comprehension and inference capabilities more effectively, leading to better results on complex queries."}, {"title": "SAM4MLLM", "content": "as our MLLM backbone architecture instead of Qwen-VL for PQPP on these datasets. With a more powerful MLLM, the performance can be enhanced.\nGRES: We present the comparison on the gRefCOCO dataset [32] in Tab. 3. Unlike the RES dataset, this dataset contains the cases where multiple instances or no instances are referred. In this generalized RES task, our method sets the new state-of-the-art among the 7B models on most of the splits and metrics, except for \"Test Set A\", where we lag slightly behind the recent GSVA [50].\nReasonSeg: Our method also demonstrates superior results on the complex reasoning segmentation task, as shown in Tab. 4. It is worth noting that we evaluate on this dataset in a zero-shot manner, meaning our model was not trained on relevant tasks before. Besides, we use more training data to train SAM4MLLM, denoted as SAM4MLLM*. Despite using less training data than LISA, it can outperform LISA-13B-LLaVA1.5.\nVQA: This dataset is not desifned for RES but for visual question answering. We use it to verify that our model, although enhanced by image segmentation functionality, still maintains its original capabilities. The VQA scores in Tab. 5 demonstrate that our approach does not compromise the VQA abilities acquired during the pre-training phase of our MLLM backbone. In fact, the VQA performance is even boosted, perhaps due to our fine-tuning on more datasets.\nPQPP and PPG: Our PQPP consistently outperforms PPG on most results. We discuss the effect of points prompting strategy further in the ablation studies.\nQualitative results: Fig. 3 presents qualitative examples of our SAM4MLLM approach on various referring expression segmentation datasets. We showcase our results on RES task in the upper row. The leftest image is from refCOCO, showing the successful segmentation of a specific zebra referred to as \"behind another one.\" The middle-left image, sourced from refCOCO+, demonstrates the accurate identification of the \"middle animal\" among multiple instances. The middle-right image from refCOCOg illustrates the model's ability to handle more"}, {"title": "13", "content": "complex referring expressions, such as \"The teddy bear that is as large as the baby.\" Finally, the rightest image, also from refCOCO+, showcases the model's understanding of relative positions, correctly segmenting the \"dish closest\" to the referred object. The bottom row demonstrates SAM4MLLM ability on generalized RES task, where our method accurately segments multiple instances as per the given text. These examples highlight SAM4MLLM's capability to accurately segment objects based on diverse referring expressions across different datasets."}, {"title": "5 Ablation Study", "content": "To gain a deeper understanding of the factors contribution, we conducted some ablation studies focusing on the best-performing variant, SAM4MLLM-PQPP. Our investigations centered around the following aspects: confidence threshold for filtering points in PQPP and sampling strategy for selecting points within the bounding box, providing insights into our method's robustness and adaptability.\nPoints Filtering Threshold First, we examined the impact of the confidence threshold used in PQPP to filter points based on the MLLM's responses. We experimented with threshold values ranging from 0.6 to 0.95 on the RefCOCOg validation set and evaluated their effect on the cIoU metric. Our results, presented in Table 6a, reveal that a threshold of 0.9 strikes the optimal balance, as further increasing or decreasing the threshold leads to a notable decline in cIoU. This finding highlights the importance of carefully tuning the confidence threshold to ensure the best possible segmentation quality.\nPoints Sampling Strategy Next, we explored the influence of the point sampling strategy within the bounding box on the overall performance. We compared two approaches: grid-based sampling and random sampling, while also varying"}, {"title": "6 Conclusion", "content": "In this paper, we introduced SAM4MLLM, a novel approach that integrates the Segment Anything Model (SAM) with Multi-Modal Large Language Models (MLLMs) to address the Referring Expression Segmentation (RES) task. By encoding object masks as discrete text prompts, our method enables MLLMs to understand and generate pixel-level object localization information without requiring complex architectural modifications or additional loss functions. Our method is simple but effective. Through experiments on various RES benchmarks, we demonstrate that SAM4MLLM achieves competitive performance while maintaining the simplicity and generalizability of the original language models. Our work explores a new direction for leveraging the capabilities of foundation models to tackle complex vision-language tasks in a more streamlined and unified manner. We hope that the insights gained from this research will inspire further investigations into effectively combining the strengths of different models"}, {"title": "15", "content": "to solve challenging multimodal problems. Future work could involve extending our approach to handle a broader range of visual reasoning tasks and conducting more in-depth analyses to better understand the interplay between language models and visual foundation models."}, {"title": "Supplementary Materials SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation", "content": "1 Details of point sampling strategies.\nIn Tab. 1, we provide an analysis for using SAM as our backend, where the upper-bound is the maximum IoU from multiple SAM prompts sampled using the ground-truth masks. The upper-bound is around 87.8% IoU, which is much higher than all existing methods. The predicted SAM prompts by our method achieve around 75% IoU, suggesting there is room for improvement on the MLLM side toward reaching the upper-bound quality. In Tab. 2, we provide additional ablations of PPG, where more groups (k) or more points (n1, n2) result in worse accuracy. The results support our claim in the main paper that it is challenging for MLLM to learn to predict multiple points simultaneously."}, {"title": "20 Chen et al.", "content": "2 Details of SAM in SAM4MLLM\nFollowing LISA [27] and GSVA [50], we finetune SAM's decoder on COCO extended datasets (RefCOCO and GRES), while using off-the-shelf SAM on ReasonSeg. This is due to the inherent bias in mask annotations within the COCO dataset. As shown in Tab. 3 and Fig. 1, with finetuned SAM, SAM4MLLM can predict more consistent segmentation mask that matches the granularity of COCO where coarser masks are often provided as ground truths."}, {"title": "3 Ablation regarding the dataset combination.", "content": "If we only use RES datasets for training without using the non-segmentation VQA datase, the segmentation performance is not influenced as shown in Tab. 4. Moreover, our method can preserve VQA capabilities when using both datasets."}, {"title": "SAM4MLLM", "content": "21\n4 Segmentation Samples\nWe present a visual comparison of SAM4MLLM with GLaMM [42] and LISA [27] on the RefCOCOg [36] dataset, which contains longer queries than RefCOCO [57] and RefCOCO+ [57], and is more challenging in referring expression segmentation. As we can see in Fig.2, our approach better captures the intent of the query and predicts more accurate segments. In contrast, GLaMM and LISA may segment incomplete or larger objects instead of the regions specified in the query."}, {"title": "22 Chen et al.", "content": "5 Discussion\nLimitation: A limitation of our approach is that it depends on SAM for the final segmentation output, thus the quality of the generated masks is inherently"}, {"title": "SAM4MLLM", "content": "23\nconstrained by SAM's segmentation capability. However, our approach will also improve effortlessly with the progress of SAM.\nFuture work: Integrating SAM4MLLM with other state-of-the-art segmentation models or developing techniques to refine SAM's outputs in challenging scenarios are worth exploring. SAM4MLLM offers significant advantages by combining the strengths of MLLMs and SAM in a straightforward yet effective manner. The MLLM's ability to understand and reason about referring expressions, combined with SAM's strong segmentation capabilities, enables SAM4MLLM to tackle complex referring expression segmentation tasks. As research in both language models and segmentation models continues to advance, we expect the performance of SAM4MLLM to be further improved."}]}