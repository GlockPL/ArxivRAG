{"title": "Infrastructure for AI Agents", "authors": ["Alan Chan", "Kevin Wei", "Sihao Huang", "Nitarshan Rajkumar", "Elija Perrier", "Seth Lazar", "Gillian K. Hadfield", "Markus Anderljung"], "abstract": "Increasingly many AI systems can plan and execute interactions in open-ended environments, such as making phone calls or buying online goods. As developers grow the space of tasks that such AI agents can accomplish, we will need tools both to unlock their benefits and manage their risks. Current tools are largely insufficient because they are not designed to shape how agents interact with existing institutions (e.g., legal and economic systems) or actors (e.g., digital service providers, humans, other AI agents). For example, alignment techniques by nature do not assure counterparties that some human will be held accountable when a user instructs an agent to perform an illegal action. To fill this gap, we propose the concept of agent infrastructure: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Agent infrastructure comprises both new tools and reconfigurations or extensions of existing tools. For example, to facilitate accountability, protocols that tie users to agents could build upon existing systems for user authentication, such as OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We propose infrastructure that could help achieve each function, explaining use cases, adoption, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents.", "sections": [{"title": "1 Introduction", "content": "A fundamental goal of the AI research community is to build AI agents: AI systems that can plan and execute interactions in open-ended environments,\u00b9 such as making phone calls or buying online goods. Agents differ from other computational systems in two significant ways. First, in comparison to foundation models used as chatbots, agents directly interact with the world (e.g., a flight booking website) rather than only with users. Second, in comparison to traditional software (e.g., an implementation of a sorting algorithm), agents can adapt to under-specified task instructions. Although the AI community has been developing agents for decades, these agents typically performed only a narrow set of tasks. In contrast, recent agents built upon language models can attempt\u2014with varying degrees of reliability\u2014a much wider array of tasks, such as software engineering or office support.\nAs developers expand the space of tasks that AI agents can accomplish, we will need tools both to unlock their benefits and manage their risks. As an example of a potential benefit, personalized agents could help individuals make a wide variety of difficult decisions, such as what insurance to buy or which school to attend. Yet, blockers such as a lack of reliability, an inability to maintain effective oversight, or the absence of recourse mechanisms could hinder adoption. Compared to the adoption of chatbots, these blockers could be more significant in the context of agents because the latter could directly cause negative consequences in the world (e.g., a mistaken financial transaction). Tools that address these blockers could enable positive uses of agents that would otherwise not materialize. On the other hand, if AI agents are adopted despite such blockers, we will need tools to prevent and respond to resulting problems. One such problem is disruptions to digital services that can result from the ability to carry out certain tasks at speed and scale, analogous to DDoS attacks. For example, an individual fradulently obtained more than 10 million dollars in royalties from Spotify by automating music creation and streaming.\nThe predominant focus of AI safety research is on system-level interventions, which intervene on the AI system itself to shape its behaviour. Major areas of work include goal specification and following"}, {"title": "2 Agent Infrastructure", "content": "To answer this challenge, we propose the concept of agent infrastructure: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments\u00b3 These systems and protocols could be novel, or could be reconfigurations or extensions of existing systems and protocols. Examples of agent infrastructure include inter-agent communication protocols, IDs for agents, systems for certifying an agent's properties or behaviour, and methods for rolling back an agent's actions. We include more examples in Table 1. Our notion of agent infrastructure is not about the technical systems that enable basic operation of agents (e.g., memory systems, cloud compute), although it will in practice often build upon or modify such systems. Furthermore, while our discussion will be grounded in language-model-based agents, the core ideas of agent infrastructure are largely architecture-agnostic and extend existing work across computational science, economics, and social science.\nTo further understand the distinctions between agent infrastructure and system-level interventions, consider traffic safety as an analogy. If we treat human drivers as analogous to AI agents, system-level interventions includes driver training programs. Infrastructure could include traffic lights, roundabouts, emergency lanes, and camera-enforced speed limits. We provide more comparisons in Table 2.\nJust as the Internet relies on infrastructure like TCP, HTTPS, and BGP, we argue that agent infrastructure will likely be crucial for unlocking the benefits and managing the risks of agents. As an example of unlocking benefits, protocols that tie an agent's actions to a user could facilitate accountability and thereby reduce blockers to agent adoption. Analogously, the ability to perform secure financial transactions over HTTPS enables an e-commerce market in the trillions of dollars. As an example of managing risks, agent infrastructure can support system-level interventions. For instance, a certification system for agents could warn actors (e.g., other agents) not to interact with agents that lack certain safeguards, just as browsers flag non-HTTPS websites. In this way, agent infrastructure can use agents' interactions as a leverage point to improve safety: restricting an agent's interactions correspondingly restricts the agent's potential negative impacts.\nThis work identifies three functions that agent infrastructure could serve: 1) attributing actions, properties, and other information to specific agents or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We propose infrastructure that could help achieve each function, including analysis of use cases, adoption, limitations, and open questions. Our suggestions are primarily aimed at researchers and developers who may want to build agent infrastructure. These suggestions could also be useful to governments or funding bodies who may want to support its construction."}, {"title": "2.1 Categories of Agent Infrastructure", "content": "We focus on agents that interact with digital environments. Roughly speaking, we consider an agent to consist of the underlying machine-learning model and any primitives that provide basic functionality (e.g., memory, scaffolding). Language-model agents commonly act by issuing commands to tools, such as APIs that return the output of a separate software service (e.g., the weather). Environments consist of such tools and any counterparties that interact with an agent (e.g., other agents, humans)."}, {"title": "2.2 Example", "content": "To illustrate how agent infrastructure could help unlock the benefits and manage the risks of agents, consider the task of running a company. In this hypothetical company, agents would perform or assist with the work that human employees would usually perform, such as operations, R&D, and legal tasks. Agent infrastructure could enable the following functions for this agent-run company.\n\u2022 Human review: Human managers can make use of oversight layers (Section 4.2) to ensure that their agent employees are not malfunctioning.\n\u2022 Coordination: A communication protocol (Section 4.3) and commitment devices (Section 4.4) can facilitate coordination between agents both within a company and from different companies. For example, agents could make commitments to carry out certain tasks, much as companies create contracts.\n\u2022 Recourse: A lack of recourse mechanisms could discourage interaction with the company's agents. One way to address this issue is by ensuring that each agent has at least one human that can answer for the agent's actions (Section 3.1), and making this information available to those that interact with the company's agents or a third-party mediator.\n\u2022 Undoing mistakes: Rollback mechanisms (Section 5.2) enable certain decisions, such as updates to compensation or internal data management processes, to be easily undone if an agent makes a mistake. 6"}, {"title": "3 Attribution", "content": "Attribution infrastructure attributes actions, properties, and other information to (instances of) agents, users, or other actors. Identity binding links an agent or its actions to an existing legal identity. Certification would provide assurance about behaviour and properties of an agent instance. An agent ID would be a container of information about an agent instance. In Figure 2, we depict how these pieces of infrastructure fit together."}, {"title": "3.1 Identity Binding", "content": "Description: AI agents are not legal entities. The ability (but not necessarily the requirement) to associate an agent's actions with a legal entity, such as a human or a corporation, could help existing institutions accommodate agents. For example, users could be held accountable for illegal activity (e.g., fraud) that they instruct their agents to carry out. Analogously, cars are associated to drivers through license plates and registries. Such binding could be helpful even when the relevant legal entity is not fully responsible for all of the agents' actions. For example, in addition to the employee using an agent, managers reviewing the agent's actions and the corporation hiring the employee could potentially bear some responsibility. With this motivation, we define identity binding to be a process that consists of (1) authentication of an identity and (2) linking that identity to an (instance of an) agent or its actions. Crucially, linking an identity to an agent does not mean making the identity visible publicly. For example, identity information could be accessible only to authorized parties, such as if needed for legal action, or only reveal limited information, such as the fact that a real person is responsible for the agent. Further below, we discuss how existing infrastructure could help accomplish (1) and (2), along with modifications that may be needed.\nPotential functions:\n\u2022 Accountability: Malicious uses of agents could be harder to address if all agents acted anonymously. For example, anonymity enables Sybil attacks, wherein someone creates multiple fake identities to undermine a network (e.g., an online voting system).\n\u2022 Trust: Counterparties could be more willing to engage in productive interactions with agents that are bound to legal entities because it could be easier to obtain recourse in case of harm. Analogously, identity verification of drivers reassures users that would otherwise be dissuaded from ridesharing by personal safety considerations.\n\u2022 Application of laws: Identity binding could allow the broader application of existing laws to interactions between agents. For example, at least as a starting point, identity binding could provide evidence that an agent did indeed enter into a contract on behalf of a particular principal\nExisting alternatives: Existing technologies can provide some functionality for authentication and identity linking, but have limitations. User authentication is already required for many digital services, such as banking (through know-your-customer regulations) or services requiring an account (through logging in with OAuth). The actions of agents interacting with such services would by default be attributable to the user. However, some services only provide attribution to a pseudonymous internet user, rather than a specific legal entity. In addition, there is currently no comparable authentication system for agent-agent interactions or actors other than users. The former may be important for agents that are significant economic actors, whereas the latter could include actors that assume responsibility for some of the agent's actions (e.g., an insurance company) or other overseers of the agent.\nIdentity information could be linked to an agent in a variety of ways. Drawing from work on content provenance, identity information can be included as metadata or as a watermark. However, metadata can be stripped from content and watermarks can potentially be removed. Furthermore, privacy considerations could weigh against revealing identity information in this way. If so, a potential alternative could be to establish a trusted intermediary that would hold this identity information. Counterparties could request that this intermediary reveal the information or use it appropriately in the event it is needed. As an analogy, ISPs can share information about their users in response to a criminal investigation. It remains to be seen how such a function could be provided for agents.\nAdoption: Firstly, users might be more likely to engage with platforms that include or require identity binding. Some evidence suggests that contractors use AI systems on platforms like Upwork. Dharmesh is also creating a service where AI agents play the role of crowdworkers and are matched to tasks. In both of these cases, customers are likely to engage more fully if they know there is a person ultimately responsible for the service, similar to identity verification for gig economy services. Secondly, platforms might require identity binding to avoid Sybil-like attacks. For example, an individual recently defrauded Spotify of millions of dollars by synthetically generating music and creating multiple fake accounts to stream it.\nLimitations: Identity binding entails at least some collection and use of user information, even if that information is only shared with authorized actors when needed. Security vulnerabilities analogous to those in existing authentication systems could compromise user information. More broadly, reduced anonymity (or the threat of reduced anonymity) could threaten personal safety (e.g., doxxing), hinder whistleblowing, and reduce expression of novel ideas. Such concerns imply that identity binding may not be appropriate in every application of agents, and proportionate confidentiality provisions will be necessary when it is used. Analogously, although it can be abused, legislatures and courts have recognized the importance of identification of internet users under certain conditions.\nResearch questions:\n\u2022 What key applications of agents could identity binding enable?\n\u2022 How can identity binding build upon existing digital identity solutions, such as (national) digital wallets?\n\u2022 How could an authentication system be designed for agent-agent interactions and authentication of actors other than human users (e.g., for companies with digital identities)?\n\u2022 How can identity information be linked to an agent or its actions?\n\u2022 If intermediaries hold identity information, who should be able to request access to or use of the informa-tion, such as for obtaining recourse? Under what conditions should such access or use be granted?"}, {"title": "3.2 Certification", "content": "Description: Before interacting with an agent (instance), counterparties might want assurances about its operation, behaviour, or properties. For example, agents whose actions are regularly reviewed could be more trustworthy than agents who do not undergo such review. Claims about operation, behavior, or properties could come from a user, an agent deployer, or other actors (e.g., third-party auditors). Actors making such claims could include them in certificates, which could be associated with the relevant agent and made visible to counterparties interacting with it. Analagously, SSL certificates make claims to website users about domain ownership. Certification infrastructure consists of tools for making, verifying, and revoking certificates for agents. Verification includes how those viewing a certificate can verify both its claims and that it corresponds to the agent with which they are interacting.\nExample claims: We provide potential example claims of an agent's operation, behavior, or properties that could be included in a certificate.\n\u2022 The tools the agent can access: Certain tools, such as commitment devices could make agents more reliable or useful.\n\u2022 Capabilities, accounting for post-training modifications: Weaker agents could be more susceptible to prompt injections from stronger agents (e.g., stronger agents could be more capable at detecting such attempts). If so, a user might want their agent only to interact with agents that are at most as strong as their agent. Accounting for post-training modifications would likely involve analysis of an agent's behaviour log. Doing so in a privacy-preserving way is an open problem.\n\u2022 The level of autonomy with which the agent is authorized to act: Agents that act with more regular human oversight could be, but are not necessarily, more trustworthy.\n\u2022 How an agent handles sensitive information: For example, an agent could follow a protocol to delete sensitive information from context and memory after use.\nPotential functions:\n\u2022 Trust: Certificates for safety-relevant properties can build trust in agent interactions. For such trust not to be misplaced, a certificate should at minimum only contain claims about what can feasibly be known about the agent, given the state of the science. For example, it may be difficult to provide useful bounds on whether an agent will ever violate a safety constraint.\n\u2022 Races to the top: If counterparties can selectively interact with agents that have certificates for pro-social properties (e.g., that use cooperation mechanisms), developers and users could be incentivized to respectively develop and use agents with those properties.\n\u2022 Supporting recourse: Certain forms of recourse (e.g., undoing an action; see Section 5.2) could rely on an auditor to certify that, for example, an agent was prompt injected.\nExisting alternatives: Auditing of companies involved in the development or use of agents, such as developers or cloud compute providers, can be a component of certification of agents. For example, an auditor could verify that a cloud compute provider has policies for only running agents that have passed certain safety standards. Yet, certifying such companies could be ineffective if large numbers of users modify and deploy their own agents.\nIn the absence of any additional certification infrastructure, developers, deployers, and users could make their own claims about their agents. However, counterparties may be unable to assess the veracity of such claims. Evaluating AI systems is not straightforward and empirical experiments can be subtly misleading. Even so, entities may still be able to assess the quality of an agent's actions and reject them if necessary. For example, a counterparty may not know in general how reliable a software engineering agent is, but could run software to verify the correctness of the agent's code.\nAdoption: Demand for certification is likely to vary across domains and contexts. For instance, protocols for forgetting sensitive information can enable agreements that would otherwise not be possible. Certifying that an agent can follow such protocol seems more important for high-stakes negotiations. Feasibility and economic cost could also affect demand. For example, properties that require access to interaction logs (e.g., whether a prompt injection is present) could raise privacy concerns, in the absence of privacy-preserving certification methods.\nFinally, as discussed above, developers and deployers could support certification as a way to credibly signal the trustworthiness of their agents.\nLimitations: It could be impossible or extremely difficult to verify certain properties. Providing meaningful guarantees about the behavior of a system in general is an active area of research. If meaningful guarantees are not technically possible, certifications could provide a false sense of security. Other properties could be feasible, but potentially impractical or invasive, to certify. A potential example could be whether an agent has interacted with a particular, other agent.\nFurthermore, actors creating certificates for agents could be incompetent, captured, or fail to perform any useful certification. A notable case is TrustARC (formerly TRUSTe), who provided privacy certification for websites. Although TrustARC claimed to verify website privacy policies, it often did not conduct any such checks whatsoever. Surprisingly, as an example of adverse selection, Edelman provides evidence that websites that obtained TrustARC certification were significantly less trustworthy than websites without it. potential Ways to address this problem include oversight over actors performing certification of agents and transparency requirements about their processes.\nResearch questions:\n\u2022 For the example claims we list above, who should be making the claims? How could the claims be verified?\n\u2022 What other claims about an agent would be useful to make?\n\u2022 How can actors verify that a certificate corresponds to the agent instance with which they are interacting?\n\u2022 Under what conditions should a certificate be revoked? For example, claims about an agent's future behavior could become outdated the longer the agent operates.\n\u2022 How would certification of agents interact with existing auditing practices and regimes?\n\u2022 How could existing infrastructure (e.g., verifiable credentials) support certification of agents?\n\u2022 What existing certification methods in information systems and cybersecurity could provide useful inspi-ration?\n\u2022 What oversight or transparency requirements should exist for actors providing certificates for agents?\n\u2022 What recourse should be available if certificates contain mistakes or are otherwise compromised in some way?"}, {"title": "3.3 Agent IDs", "content": "Description: An agent ID would at minimum consist of a unique identifier for an agent instance, and potentially include other information relevant to the instance, such as a system card or certifications. ID-like systems exist across many domains to facilitate risk and incident management, such as serial numbers on consumer products, tail numbers on aircraft, and registration numbers for businesses. Although any AI system could have an ID, IDs could be significantly more useful for agents because they take actions in the world (see below).\nPotential functions: Many of these uses draw from Chan et al.\n\u2022 Supporting certification: An ID could present an agent's certifications (Section 3.2) and bound identities (Section 3.1) to entities that seek to engage with it.\n\u2022 Incident response: IDs could provide information that is useful for incident response. For example, an agent could create sub-agents to split up a harmful task into multiple sub-tasks. An ID could link the activities of these sub-agents to the original agent. An ID could also be useful for detecting cross-platform incidents, where an agent harms multiple counterparties.\n\u2022 Enabling targeted action against particular agents: Much like how process IDs in operating systems en-able monitoring and resource restriction, an ID could enable interventions on the agents that are causing harm.\nExisting alternatives: For agents that interact with digital services, OAuth tokens or (read-only) API keys could aid incident attribution. However, such tools do not apply to interactions between agents. These tools are also service-specific. For example, an OAuth token cannot link the activity of an agent across different counterparties, nor can it link the activities of sub-agents to the agent that created them.\nIP addresses could also help incident attribution, but may not reliably identify an agent instance. Indeed, they may not be static and can also be shared amongst different internet users.\nAdoption: Identifiers could be a part of existing obligations to inform humans when they are interacting with AI systems As agents are assigned increasingly complex roles, counterparties could demand more information from IDs so as to ensure reliable interactions (e.g., through certification) and facilitate incident attribution.\nCounterparties that expect only to interact with agents (or which only interact with agents through specific channels [Section 4.1]) could require IDs before an interaction. Other counterparties which also interact with systems other than agents may have more difficulty doing so. For example, suppose a counterparty engages with both agents and traditional software programs. Extending a verifiable ID system from agents to all software programs would likely be infeasible, since traditional software programs do not have IDs.\nLimitations: IDs could allow attackers to target particular agents and their users. If activity associated with an ID is logged, leaks of such information could allow attackers to identify high-value targets. Knowing an agent's frequently used services could also facilitate exploits like prompt injections (e.g., attackers could infect particular websites). When interacting with a particular agent, attackers could exfiltrate information or jailbreak the agent to act on the attacker's behalf, such as by performing financial transfers.\nIDs could also be stolen or otherwise compromised. For example, IDs that depend on certificate authorities are vulnerable to existing security vulnerabilities.\nResearch questions:\n\u2022 How could IDs complement existing tools for incident investigation?\n\u2022 Where should IDs be required, and what information should an ID include?\n\u2022 How can counterparties incentivize or enforce ID usage (e.g., by making sure that agents use unique channels (Section 4.1))?\n\u2022 When should IDs link to information identifying the user?\n\u2022 What should entities be allowed to do with an identifier? For example, should an entity with an identifier be allowed to contact the associated agent, or otherwise interact with it?\n\u2022 What other systems need to be in place to make IDs more useful? E.g., should there be a registry of certain agent IDs or a reviewing system?\n\u2022 How can the design of IDs avoid known problems with existing Internet infrastructure (e.g., certificate authorities)?\n\u2022 How can the design of IDs limit abuse (e.g., attackers use IDs to target particular agents and their users)?"}, {"title": "4 Interaction", "content": "Interaction infrastructure consists of protocols and affordances to shape how agents interact with their environments. Agent channels would isolate agent traffic from other traffic. Oversight layers would enable actors to intervene on an agent's behaviour. Inter-agent communication protocols would help to facilitate joint activities amongst groups of agents. Commitment devices are mechanisms that enforce commitments between agents. We depict these pieces of infrastructure in Figure 3."}, {"title": "4.1 Agent Channels", "content": "Description: For interactions with a digital service (e.g., payment services), an agent channel would separate agent traffic (or AI traffic in general) from other digital traffic. An agent channel could, for example, facilitate incident management by temporarily suspending access if a language-model worm spreads. Traffic separation is common across many applications. For example, digital payments companies isolate systems that handle cardholder data. Some hospitals separate networks for storing and using different kinds of medical data, so as to safeguard privacy in case one network is compromised. Wi-Fi routers provide guest networks that restrict the activity of unfamiliar devices.\nThere are two axes of design considerations for agent channels. First, agent channels could be different software interfaces or protocols for agents, or could take the form of more dedicated internet infrastructure. As examples of the former, there are efforts to build agent-specific web browsers and interaction protocols As examples of the latter, agents could have a dedicated block of IP addresses (which could perhaps function as IDs Section 3.3), or separate routing systems could handle agent traffic.\nThe second axis is how to operationalize traffic separation. Rate limits on human channels and the relative absence or weakening of such limits on agent channels could incentivize the use of the latter. However, it may in practice be difficult to separate agent or AI traffic from software traffic in general. For example, agents could write software that interacts with counterparties.9\nPotential functions:\n\u2022 Monitoring: Separate transaction data for agents, for example, can help to understand the potential macroeconomic effects of high volumes of agent activity.\n\u2022 Managing incidents: Isolation of traffic could help to manage incidents. For example, providers of services could shut down agent channels, but not human channels, to restrict spread of a text or image worm.\n\u2022 Enforcing agent-specific rules: Agent channels could implement rules to support reliable agent use, such as around rolling back unintended actions (Section 5.2) or how to handle sensitive data.\n\u2022 Reduced attack surface: Human graphical interfaces have a large attack surface. For example, impercep-tible changes to a website page could hide a prompt injection. APIs designed for agents could reduce this attack surface, although text-based prompt injections would still be possible.\n\u2022 Supporting IDs: As discussed in Section 3.3, agent channels could make it more feasible to require IDs from agents.\nExisting alternatives: Agents currently use existing software interfaces when interacting with web services. In the absence of dedicated channels, behavioural tests could try to identify and isolate agent behaviour. CAPTCHAs tend to be unreliable. Another potential test is whether an entity interacts with an interface at faster-than-average human speeds. If so, the entity could undergo more intensive monitoring or have more limited access to functions, under the suspicion that it might not be human. This differential treatment would effectively constitute an agent channel, without building specific interfaces or other dedicated internet infrastructure.\nAdoption: Agents currently have difficulty interacting with human interfaces. If agent channels are optimized to help agents function more efficiently and reliably, such as by simplifying interfaces (e.g., website navigation instructions) or imposing fewer limitations (e.g., higher rate limits, reduced latency) compared to human interfaces, there could be much demand for them. Yet, agents could soon become capable of interacting with with human interfaces. Encouraging the use of agent channels could therefore be time-sensitive.\nLimitations: Agent channels could fail to cover enough agents or could cover entities besides agents. On the first point, agent channels would likely be less effective if only a small number of agents use them. Monitoring of agent channels may miss incidents, and interventions on agent channels may not catch all agent activity. On the second point, although it seems difficult to prevent other entities from using agent channels, this limitation may not be significant. For example, any rules applied to agent channels (e.g., slowing the spread of a language-model worm by restricting activities) would simply also apply to those entities.\nResearch questions:\n\u2022 How should agent channels be designed and implemented? Can their creation be automated (e.g., auto-matic API creation)?\n\u2022 What could encourage the adoption of agent channels?\n\u2022 Is it possible (or desirable) to isolate agent traffic for functionalities already provided by traditional software interfaces (e.g., email , weather retrieval )?\n\u2022 What policies should govern the use of agent channels to influence or constrain agents?"}, {"title": "4.2 Oversight Layers", "content": "Description: The potential for unreliable agent behaviour motivates review of the agent's actions. An oversight layer would consist of (1) a monitoring system to detect when external intervention is required during an agent's operation and (2) an interface for an actor to intervene. Examples of external intervention include the provision of additional information (e.g., about the task or what the agent is not authorized to do) or rejection of the agent's action. The actor in (2) could be a human user, but not necessarily. For example, a trusted, automated system could take the place of a user especially if the oversight layer flags many actions. An oversight layer could also be useful for any actor that engages with an agent, not just a user. For example, a manager in a company could use an oversight layer to review actions from agents used by individual employees.\nPotential functions:\n\u2022 Rejecting unsafe actions: Agents could take consequential actions that are unbeknownst to, or unintended by, their users. For example, a malicious actor could prompt-inject an agent to drain a user's bank account. Similar to credit card fraud detection, methods for detecting when actions may be unintended could protect users.\n\u2022 Improving functionality: As Hewett discusses, an oversight layer could ask a human for help whenever an agent is incapable of completing a particular action (e.g., misinterprets the task or is stuck in a loop), without the human having to restart the agent from scratch.\n\u2022 Accountability: An oversight layer could make it easier to assess responsibility for an agent's actions, such as by recording the explicit approval of a user for an action that harmed another party.\n\u2022 Generating useful information: Oversight layers could generate useful information about which agents tend to take actions that overseers reject more often. This information could enable, for example, insurance to be designed around the expected reliability of an agent.\nExisting alternatives: Some technical frameworks already include some kind of oversight layer. See below for additional research questions to extend upon this work.\nAdoption: There could be strong incentives to develop and implement oversight layers because they make agents more usable. However, it is unclear if the market will provide them to a socially optimal degree (or what a socially optimal degree would be). Analogously, although more granular social feed controls (e.g., reducing the degree of sensationalist content) would make social media more usable, major social media companies have not provided such options. Indeed, some controls may be against the financial interests of such companies. For example, removing sensationalist content could reduce engagement and ad revenue. Analogously, AI companies could hesitate about providing users with the ability to limit how agents engage with activities that could be profitable for the company, such as making financial transactions or viewing advertisements.\nLimitations: Oversight layers are only useful to the extent that actors use them to intervene as needed. Automation bias could reduce the ability of human actors to do so"}, {"title": "4.3 Inter-Agent Communication", "content": "Description: Agents will likely interact not only with existing digital services, but also with other agents. For example, agents from different users could collaborate on shared projects. Inter-agent communication infrastructure includes (1) rules for how agents communicate and (2) the underlying technical systems to enable such communication. On (1), in addition to point-to-point communications, infrastructure may also need to facilitate broadcast-like communications (e.g., for important announcements). On (2), technical systems include an addressing system and the underlying messaging protocol. Agent channels (Section 4.1) are similar to inter-agent communication infrastructure, but focuses on interaction with existing digital services rather than interactions between agents.\nPotential functions:\n\u2022 Notification: Communications can convey important information. For example, an agent could warn other agents about the discovery of a security vulnerability.\n\u2022 Cooperation: Communications could be useful for coordinating the activities of different agents, whether for completing shared projects or helping to resolve collective action problems.\n\u2022 Negotiation: Agents could use communications to negotiate on rules for an interaction such as how to handle confidential information.\nExisting alternatives: Existing communication infrastructure may be difficult for agents to use because they require human involvement. For example, signing up for a Gmail account requires access to a phone number. It may also be difficult to add agent-specific functionality onto existing communication infrastructure. For instance, Facebook Messenger does not currently support broadcasting functions.\nAdoption: The difficulties of using the existing alternatives above could spur adoption of more specialized inter-agent communication infrastructure.\nLimitations: As with existing internet communication infrastructure, inter-agent communication infras-tructure could be abused. For example, broadcast functionalities could be frequent sources of spam. Scam-mers Could use an addressing system to target agents they believe to be especially vulnerable (e.g., more easily hijacked through a prompt injection). The extent of abuse will likely depend upon the specifics of implementation (e.g., the effectiveness of any blocking functionalities).\nResearch questions:\n\u2022 What security measures should inter-agent communication infrastructure incorporate? For example, what could be analogues to HTTPS?\n\u2022 What are use cases for broadcasting?\n\u2022 How should broadcasting be designed? For example, one way of implementing broadcasting is to rely on other agents to pass messages, rather than having a centralized system.\n\u2022 What functionalities should inter-agent communication infrastructure include? For example, a communi-cation protocol could include a way to signal or carry out commitments (Section 4.4).\n\u2022 How can inter-agent communication infrastructure integrate with other types of agent infrastructure, such as oversight layers (Section 4.2), agent IDs (Section 3.3), and identity binding (Section 3.1)?"}, {"title": "4.4 Commitment Devices", "content": "Description: An inability to credibly commit to particular actions is a key reason why rational actors some-times do not obtain cooperative outcomes. Commitment devices-mechanisms that can enforce commitments-aim to address this issue. Examples of commitment devices include escrow payments, assurance contracts, and legal contracts. As agents become increasingly important economic and social actors, commitment devices designed for agents can help to achieve positive societal outcomes. Such commitment devices could simply be interfaces connecting agents to existing human commitment devices, or could be specially designed for agents.\nPotential functions:\n\u2022 Funding productive activities: The funding of productive activities can suffer from collective action prob-lems (e.g., underprovisioning of public goods). Commitment devices could help agents with economic resources, whether used independently or on behalf of a user, cooperate to fund such activities. For ex-ample, future agents could initiate a vast array of projects or companies. Analogues to Kickstarter (i.e., assurance contracts) could facilitate funding of such projects or companies.\n\u2022 Helping to avoid the tragedy of the commons: Commitment device could allow agents to agree not to carry out risky activities as long as others do the same, even if such activities advance the interests of the individual agent or user. For example, competitive pressures in AI development likely lead to underinvestment in safety. Future agents that are able to perform such development or run AI companies could commit, for example, not to build systems that surpass certain capability thresholds without sufficient safety guardrails, so long as others do the same.\nExisting alternatives: Commitment devices that depend upon (threats of) action in the physical world or human social norms, such as legal contracts or making announcements to friends, are likely to be ineffective for agents that we are currently building. Connecting agents to existing software-based commitment devices, such as smart contracts, is likely to be more fruitful in the short term.\nAdoption: Commitment devices are low-cost for actors who seek to make conditional commitments. If the conditions of the commitment do not hold (e.g., a quorum is reached), the actors take no action. Some types of commitment devices, such as dominant assurance contracts, may even positively incentivize participation in the device.\nLimitations: A commitment device is only useful in so far as it is employed. Depending on how a commit-ment device is implemented, users could be able to circumvent them. For example, if a commitment device depends upon modifying an agent's weights so that it takes an action, the user could simply shut the agent down.\nCommitments could also lead to negative outcomes. For example, an agent that misunderstands a user's preferences could mistakenly commit funds to a project that the user does not support. More broadly,\nResearch questions:\n\u2022 What commitment devices could help agents from different users better cooperate on productive projects (e.g., software development)?\n\u2022 How could commitment devices integrate with inter-agent communication channels (Section 4.3)?\n\u2022 When should commitments between agents be legally enforceable?\n\u2022 Will private actors sufficiently provide for commitment devices?\n\u2022 If future agents are responsive to legal incentives, 10 would new types of commitment devices still be necessary?"}, {"title": "5 Response", "content": "Response infrastructure consists of tools for detecting and remediating harmful actions from agents. Incident reporting systems would collect information about and"}]}