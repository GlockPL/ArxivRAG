{"title": "CluMo: Cluster-based Modality Fusion Prompt for Contin- ual Learning in Visual Question Answering", "authors": ["Yuliang Cai", "Mohammad Rostami"], "abstract": "Large vision-language models (VLMs) have shown significant performance boost in various application domains. However, adopting them to deal with several sequentially encoun- tered tasks has been challenging because finetuning a VLM on a task normally leads to reducing its generalization power and the capacity of learning new tasks as well as causing catastrophic forgetting on previously learned tasks. Enabling using VLMs in multimodal continual learning (CL) settings can help to address such scenarios. To improve general- ization capacity and prevent catastrophic forgetting, we propose a novel prompt-based CL method for VLMs, namely Cluster-based Modality Fusion Prompt (CluMo). We design a novel Key-Key-Prompt pair, where each prompt is associated with a visual prompt key and a textual prompt key. We adopt a two-stage training strategy. During the first stage, the single-modal keys are trained via K-means clustering algorithm to help select the best semantically matched prompt. During the second stage, the prompt keys are frozen, the se- lected prompt is attached to the input for training the VLM in the CL scenario. Experiments on two benchmarks demonstrate that our method achieves SOTA performance.", "sections": [{"title": "1 Introduction", "content": "Visual Question Answering (VQA) is a complicated task, where the goal is to answer questions described in natural language (text) about a given input image. Addressing VQA requires understanding and fusion of information from both the visual and textual domains to generate accurate responses. Recently, significant advancements in addressing VQA tasks have emerged due to the development of pre-trained large vision- language models (VLMs) Radford et al. (2021); Kim et al. (2021). Despite these advances, one of the persistent challenges in VQA tasks is the ability to adapt a VLM in CL setting to avoid finetuning a copy of an underlying VLM per task. In a CL setting, we learn new tasks and aim for continuously improving the model performance without forgetting previously learned knowledge, also known as catastrophic forgetting French (1999). To address catastrophic forgetting, a group of CL algorithms are deployed. Regularization- based methods Kirkpatrick et al. (2017); Jin et al. (2021) constrain the drastic parameter shift when learning new tasks. Expansion-based methods Douillard et al. (2022); Cai et al. (2023b) expand the model with small portion of additional weights and use the expanded weights to learn the new incoming tasks. Rehearsal-based methods Rebuffi et al. (2017); Rostami (2021) store a representative subset of the training dataset for each task into a small memory buffer and replay them back during the learning of the current task to maintain the encoded knowledge of the previously learned tasks. More recently, prompt-based methods Wang et al. (2022b;a); Cai et al. (2023a) aim to use prompts that contains task-specific or semantic-specific information to prevent catastrophic forgetting. A prompt is attached to the embedded features of the input to adapt the model to focus on the specific characteristics of the input task that has been learned before.\nMost existing CL methods consider unimodal, i.e., vision-only or language-only, settings and hence are inapplicable to address VQA tasks. To tackle this shortcoming, we propose a novel two-stage prompt learning-based CL method, namely cluster-based modality fusion prompt (CluMo). Figure 1 visualizes the high-level idea of our approach. Our method adopts a pre-trained VLM as its backbone and benefits from"}, {"title": "2 Related Works", "content": "Visual Question Answering Visual Question Answering (VQA) has been a pivotal task at the inter- section of computer vision and natural language processing which led to advances on more complex tasks. Initially, VQA was formulated as a classification task in which answers are selected from a predefined set of answers Agrawal et al. (2016) and was solved by using CNNs for image feature extraction and RNNS for text processing. These models were too simple to be used in most practical cases. With the develop- ment of transformer and BERT-like models Lu et al. (2019); Li et al. (2019), performance in VQA tasks has significantly been improved due to the better capacity of capturing the intricate relationship between two modalities and generating the response in the form of meaningful and descriptive texts. Despite these advances, VQA tasks are mostly studied in static settings Goyal et al. (2017); Johnson et al. (2016); Marino et al. (2019), where it is assumed that there is a single input task. As a result, most existing methods are inapplicable in dynamic environments and settings such as continual learning (CL) Srinivasan et al. (2022) when we encounter several tasks over time.\nPrompt-Based Learning Prompt learning is a powerful technique for leveraging pre-trained language models to frame downstream tasks in NLP. It is more memory-efficient than using Adapters Pfeiffer et al. (2021) or LORA Hu et al. (2021) and has been used successfully to guide responses of VLMs for a particular task. The reason is that additional prompt parameters are in the form of small embeddings which are directly added to the input to make the model task. As a result, the prompts are often much smaller in size compared to the layers of the model, leading to a minimal increase in the total number of parameters compared to using adapters or LORA. Moreover, prompts can be stored in a memory-efficient way and retrieved dynamically based on the observed task in the input. This means that even when the model handles many tasks, the memory overhead remains low. Browon et al. 2020 introduced the concept of prompt for the natural language instruction task to guide the model towards desired outputs. Prompt learning is based on providing a fixed function to condition a model so that it gets extra information token which specializes it to perform the down-stream task. Prompts are mostly considered as trainable parameters, either task-specific or domain- specific, to guide the model by obtaining task-specific knowledge Lester et al. (2021); Li & Liang (2021). Prompt learning has also been found helpful in handling a single VQA task Hu et al. (2023).\nPrompt Learning for Continual Learning Prompt learning has been used in CL to prevent catas- trophic forgetting when a large pre-trained models is trained on a stream of sequentially encountered tasks. It allows a single model to quickly adapt to new tasks in the stream without needing extensive retraining. By using task-specific prompts, the model can retain and recall knowledge from earlier tasks, mitigating the issue of catastrophic forgetting. It also allows scalability to learning a large number of tasks since each task primarily requires learning or generating new prompts rather than retraining the entire model. L2P Wang et al. (2022b) pioneered to connect prompt-based learning and CL. Instead of having a single shared prompt to learn all tasks, L2P introduced the concept of \"prompt pool\" to maintain prompts for different tasks independently from each other. DualPrompt Wang et al. (2022a) extended the idea of prompt pool in 12p by introducing E-prompt and G-prompt. While E-prompt is task-specific, G-prompt encodes the knowledge used for all tasks to further allow knowledge sharing and transferring while mitigating negative transfer. S-Prompt Wang et al. (2023) applied clustering to build the prompt pool with domain-specific prompts. These prompt learning methods for CL only consider single-modality, i.e., vision-only or text-only, and hence are sub-optimal for tasks with multi-modal inputs such VQA when the modalities are related. Our method benefits from the specific properties of multi-modal data to address VQA in CL settings using prompt learning and leads to performance improvements against these methods."}, {"title": "3 Problem Description", "content": "Consider a set of VQA tasks, {Tj}j=1, which are encountered sequentially and each of them is from different domain. For each of the tasks, a labeled training dataset Dt = {{(I, L),yt}Ni=1} is accessible, Ni denotes the size of dataset, I \u2208 RH\u00d7W\u00d7C denotes the input image, L\u2208 RL\u00d7|V| denotes the input text, and y denotes the text-typed discrete label. The order in which the VQA tasks are observed is not known in advance and the training data points are assumed to be drawn iid from a task-specific joint distribution p(I, L, y). Upon learning each task, the model moves forward to learn the next task. Since all the previously learned tasks can be encountered at any time during testing in the future, the model should learn new tasks such that its knowledge of previously learned tasks is maintained, i.e., by preventing catastrophic forgetting.\nWe formulate our problem in a domain-incremental learning setting Van de Ven & Tolias (2019) which assumes all the tasks are from different domains and the boundaries between them are known during learning time. We consider that each task can be learnt individually by adapting a pre-trained large multimodal transformer f\u03b8M(\u00b7,\u00b7) via minimizing a suitable discrimination loss L, e.g., cross entropy. In our approach, all the model parameters, except the final classifier layer \u03b8cls, are frozen during training to preserve the generalizability of the model. We benefit from prompt learning to enable using a single model to learn all tasks. To prevent catastrophic forgetting, a trainable task-specific prompt pool, which contains several task-specific prompts, is attached to the model f\u03b8M(\u00b7,\u00b7) such that the best-semantically-matched prompt is selected based on image and text inputs for task specialization. The prompt is then pre-pended to the input vectors so that the output is generated based on specialization. Our method is rehearsal-free and does not need any memory buffer similar to prior approaches Lopez-Paz & Ranzato (2022); Rostami & Galstyan (2023)."}, {"title": "4 Proposed Architecture", "content": "Our architecture, named cluster-based modality fusion prompt (CluMo), contains two unimodal task-specific cluster-based keys for vision and text embeddings and one prompt pool. The combination of the selections from both keys is then used to select the best matched prompt from the prompt pool. A high-level diagram of our approach is presented in Figure 2. In this section, we first introduce the preliminaries such as backbone model and prompt pool-based method in 4.1, and modality fusion prompt in Sec. 4.2, then the cluster-based prompt key is described in Sec. 4.3, and the training and the inference strategy is discussed in Sec. 4.4."}, {"title": "4.1 Preliminary", "content": "Backbone The base multimodal transformer contains three encoders: the visual encoder VE, the textual encoder TE, and the multimodal fusion encoder FE. Given a visual input V, i.e., a single image, and a textual input T, i.e., a question, the data processing pipeline for the model is:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    \\hat{y}(V, T) = F(F_{E}([V_{E}(V); T_{E}(T)])),\n\\end{equation}\n\n\\end{document}\nwhere F(\u00b7) is the classifier to predict the answer.\nPrompt Pool As an adoption of prompt learning in continual learning, a prompt pool is a set of trainable key-value (K-P) pair, in which K \u2208 RLk\u00d7D denotes the \u201cprompt key\", and P \u2208 RLp\u00d7D is the prompt. Lp and D denote the length and dimension of the prompt. Given an input image V, we compute v1 = VE(V) \u2208 RLv\u00d7D, where Lv is the dimension of the features, after passing the image through the visual encoder. VIO = v1[0] is matched with all the keys K within the prompt pool via similarity score, such as cosine similarity, to find the most similar K1. The corresponding P1 is selected and prepend to V as V\u2032 = [P1; V]. Parameters of K and P are updated through back-propagation during the training. However, in our setting, we adopt a two stage training strategy that prompt keys are trained before model and prompts.\""}, {"title": "4.2 Modality Fusion Prompt", "content": "Previous prompt-based CL methods such as L2P Wang et al. (2022b) associate each prompt in the prompt pool with a single prompt key to form Key-Value pair. In practice, the prompt keys in prompt-based CL can be considered as cluster centers. These cluster encode a notion of similarity between the prompts. The input feature vectors that form a cluster in the feature space can be assigned to these cluster centers, which are prompt keys. The intuition behind this idea is that feature vectors with small geometric distance in the feature space are semantically similar Wang et al. (2023).\nHowever, such a key-value pair design considers only single modality without tasks with multimodal inputs. The reason is that different input modalities contain different or complementary semantic information. Hence, having prompt keys that associate with each modality can help guiding prompt selection, which is more comprehensive and representative in term of semantic properties of each modality. Thus, we propose a task-specific prompt pool architecture, namely Modality Fusion Prompt, which is composed of the visual prompt keys Kv, the textual prompt keys Kt, and the prompt pool P as following:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{align}\n    & K_{t} = [K_{t1}, K_{t2}, ..., K_{ts}], \\\\\n    & K_{v} = [K_{v1}, K_{v2}, ..., K_{vs}], \\\\\n    & P = [P_{1}, P_{2}..., P_{sp}], \\\\\n    & K_{tm} \\in \\mathbb{R}^{D}, K_{vn} \\in \\mathbb{R}^{D}, P_{i} \\in \\mathbb{R}^{L_{p} \\times D},\\\\\n\\end{align}\n\n\\end{document}\nwhere St, Sv, and Sp are the sizes of textual prompt key, the visual prompt key, and the prompt pool, respectively. Lp is the length of each prompt and D is the hidden dimension of the transformer backbone. The prompt pool size Sp is then determined as Sp = Sv \u00d7 St. Each prompt is associated with the unique combination of one visual prompt key and one textual prompt key. Given a specific visual prompt key Kum and a specific textual prompt key Ktn, the Key-Key-Value pair is defined as the following:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    (K_{vm}, K_{tn}) \\rightarrow P_{m*Sv+n}.\n\\end{equation}\n\n\\end{document}\nAs modality fusion prompt is task-specific, new visual prompt keys, textual prompt keys and a prompt pool will be initialized for each of the new coming task. The previous ones are frozen during training."}, {"title": "4.3 Cluster-based Prompt Key", "content": "K-means clustering has been widely adopted in machine learning algorithms for semantic separation and understanding, where data from different domains can be explicitly separated via clustering in an unsuper- vised way Wang et al. (2023) Cohn & Holm (2021). However, even though the data from single task belong to the same domain, they can still be further divided into sub-domains based on the semantic property. To make each prompt key be the semantically cluster center of the sub-domains for both vision and text inputs, we adopt mini-batch K-means clustering algorithm on prompt keys of K and Kt to make each prompt key diverse and representative. Let B = (I,T) be the random batch from the training dataset. We extract the image feature vector vI and the text feature vector vT as follows:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    v_{I} = V_{E}(I), v_{T} = T_{E}(T),\n\\end{equation}\n\n\\end{document}\nwhere vI \u2208 RB\u00d7LI\u00d7D and vT \u2208 RB\u00d7LT\u00d7D, B is the batch size, LI and LT are the length of vectors for image and text features, represent the embedded image and text input respectively. For visual prompt key clustering, each image feature vector, vIn, is set by taking mean along second the dimension such that v^In \u2208 RB\u00d7D, and v^In is used to compare with every prompt key in K:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    similarity(n, m) = || \\hat{v}_{In} - K_{vm} ||_{2},\n\\end{equation}\n\n\\end{document}\nand the prompt key with highest similarity is assigned to match vIn. After calculation of the whole batch B, the prompt keys are then updated by calculating the mean of all v^I assigned to that specific visual prompt key. We repeat the above step until the convergence. The procedure of updating the text prompt key Kt is similar to updating the image prompt keys. Algorithm 1 summarizes our approach for prompt key training."}, {"title": "4.4 Training and Inference", "content": "During training, we adopt a two-stage training strategy to ensure that the prompt keys are correctly settled before learning the current task. In the first stage of learning each task Ti, we random select batches from the current task's dataloader to train minibatch k-means Cluster on the visual and the textual prompt key Kv and Kt until reaching the convergence of the clustering algorithm. During the second stage, the trained Kv and Kt are frozen. Within the iteration of training dataloader, each training instance is assigned to its nearest prompt key using k-nearest neighbor (KNN) algorithm to find the best match prompt Pk from the prompt pool P. Pk is then attached to the model pipeline:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    \\hat{y}(V,T) = F(F_{E}([P_{k}; V_{E}(V); T_{E}(T)])),\n\\end{equation}\n\n\\end{document}\nDuring the second stage, we also use knowledge distillation to further boost the performance. Before the training of task T, we keep a frozen copy of model after finishing T \u2212 1, denoted as MT\u22121. To prevent significant parameter shift, we pass the same input image and question to both MT and MT\u22121 and add the difference between the two model's output to the loss:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    L_{KD}(V,T) = MSE(\\hat{y}_{M_{T}}(V,T), \\hat{y}_{M_{T-1}}(V, T)).\n\\end{equation}\n\n\\end{document}\nThe final objective loss function would be:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    L = L_{ce}(\\hat{y}(V, T), y) + L_{KD}\n\\end{equation}\n\n\\end{document}\nWhere Lce is the same cross entropy loss. The overall training procedure for all tasks come in sequence is presented in Algorithm 2.\nDuring inference, the model is frozen and we follow a procedure similar to the second stage of training. For every training image-text pair, the image input is aligned with the best-matched image prompt key while the text input is aligned with the best-matched text prompt key. The combination of prompt keys is deployed to find the corresponding prompt, which is pre-pend to the output of multimodal encoder. To help better understand the procedure of prompt selection, an visualization example is provided in Figure 3."}, {"title": "5 Experiments", "content": "Our implementation code is available at https://github.com/YuliangCai2022/CLUMO.git. Please refer to the code for reporducing the results."}, {"title": "5.1 Experiment Setup", "content": "Backbone We used the public pre-trained large multimodal transformer, ALBEF Li et al. (2021), as our backbone for VQA task. It consists of an image encoder, a text encoder, a multimodal encoder, which uses cross-attention between the two modalities. Specifically for VQA tasks, an pre-trained answer decoder is append after the multimodal encoder, which has same architecture as multimodal encoder.\nBaselines for comparison We use seven methods for comparison. We include algorithms from major CL approaches. We include two regularization-based methods: EWC Kirkpatrick et al. (2017) and LwF Li & Hoiem (2017), two rehearsal-based methods: ER Rolnick et al. (2019) and GEM Su et al. (2021). We also include three SOTA prompt-based continual learning methods, L2P Wang et al. (2022b), DualPrompt Wang et al. (2022a), and S-Prompt Wang et al. (2023). We also include finetuning to demonstrate the positive effect of CL. Following the original setting of each method, we leave the whole backbone model unfrozen for non-prompt-based methods and freeze the whole backbone model for prompt-based methods except for the classifier. To make the fair comparison, we fit all the continual learning methods into our backbone, ALBEF, instead of using the original model proposed in each method.\nCL Tasks We evaluate our method on tasks built using the CLOVE Lei et al. (2022) dataset which is a VQA-based continual learning dataset. The benchmark contains two sepecate benchmarks for different scenario, including scene-incremental setting benchmark, CLOVE-scene, and function-incremental setting benchmark, CLOVE-function. Each of the task sets contains six tasks which are domain-specific and diverse from each other. For more details about CLOVE and the tasks we use, please refer to the Appendix.\nMetrics for comparison We use the average accuracy and the average forgetting rate on all tasks to evaluate the performance of our method and its ability to tackle catastrophic forgetting. Different from dataset such as VQAv2 Goyal et al. (2017), where each question is paired with different ground truth answers, questions in GLOVE dataset only contains exactly one correct answer for each question. Thus, the accuracy is simply calculated by y == \u0177 for every training and testing data instance. On the other hand, the forgetting rate is calculated as:\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\begin{equation}\n    F = \\frac{A_{i} - A_{ij}}{A_{i}}\n\\end{equation}\n\\end{document}\nwhere Ai is the accuracy of task i, and Aij is the accuracy of task i after the model is trained on task j. For details about the optimization and implementation processes, please refer to the Appendix."}, {"title": "5.2 Comparative Results", "content": "We conduct the comparison experiments on both the CLOVE-scene and CLOVE-function task sets with a randomly selected task order. In table 1, the task order abcdef represents the CL tasks: Shop AndDining, WorkPlace, HomeOrHotel, Transportation, SportAndLeisure Outdoors in sequence. The oarlks in CLOVE-function represents tasks: ObjectRecognition, AttributeRecognition, RelationReasoning, LogicReasoning, KnowledgeReasoning and Scene TextRecognition.\nWe observe in Table 1 that our method outperforms all the baselines across all task order sets in terms of both average accuracy and average forgetting rates. We also observe that the performance of different method within the same group tend to be similar. The regularization-based methods, EWC and LwF, obtain the sub-optimal accuracy and forgetting rate besides. The reason is that the domain for each task in the dataset is significantly different from the rest of tasks and hence regularization methods fail to capture the common space of the parameter distribution. This challenge makes it difficult to maintain the accuracy of the current task and previous tasks at the same time using regularization. The replay methods, ER and GEM, achieve better performance than regularization-based methods. This can be explained by the fact that replaying the data from previous task is an efficient way to remind the model and adjust its parameter distribution not too diverse from previous ones. However, because we need to rely on a memory buffer to store samples for replay, these methods are memory-consuming and thus not space-efficient. Moreover, replay-based methods are still limited by the upper-bound of joint training, as they generally can only reduce catastrophic forgetting without boosting the accuracy of individual tasks. On the other hand, the prompt-based methods, namely L2P, DualPrompt, and SPrompt, achieve superior performances compared to more traditional CL methods. Rather than tuning the whole model with regularization, prompt-based methods store the prior knowledge in trainable prompts, which are smaller and more efficient than memory buffer, and keep the main body of backbone model frozen. With the combination of generalization capacity of pre-trained model and specific previous knowledge stored in prompt, prompt-based method can outperform the replay and regularization methods. Among all the methods, our method achieve the best performnce.\nCompared with the baseline prompt-based methods which only consider visual modality for prompt selecting and updating, CluMo takes care of both the visual and textual modalities, as well as the fusion of the two for selecting the prompt which deploys the given information more comprehensively to process the prompt. Our design thus fits better in multimodal learning scenario than other existing continual learning methods."}, {"title": "5.3 Ablation Experiments", "content": "To offer a better insight about our method, we perform an ablation study for each component of CluMo to study the positive contribution of each component. We study the effect of the following:"}, {"title": "5.4 Analytic Experiments", "content": "Effect of clustering To show the effect of clustering algorithm, we empirically show the correlation between the clustering error and the downstream accuracy. As we apply Euclidean distance as metric to learn the clusters, we record the average distance between each point to its assigned cluster center for every task, and take the average for all the tasks:\n\\documentclass{article}\n\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{equation}\n    \\mathcal{E} = Avg(\\sum_{i=1}^{N} Avg(\\sum_{j=1}^{M} ||x_{j} - C_{k}||_{2}))\n\\end{equation}\n\n\\end{document}\nwhere i represent the number of tasks, j represent the training data from task i and k is the kth cluster center. We consider both the visual prompt key training and the textual prompt key training in this experiment."}, {"title": "6 Conclusion", "content": "We introduced a novel prompt-based continual learning method for learning multimodal tasks. While most of existing methods apply simple prompts on a single modality, our method proposes modal-specific visual prompt keys and textual prompt keys and train them to capture the semantic properties of the training dataset using K-means clustering algorithm. We use the combination of both the visual prompt key and the textual prompt key to select prompts, which enable the prompt to better boost the performance. Our experiments show that our method achieves the state-of-the-art performance in continual VQA tasks in different domains compared to other regularization-based, rehearsal-based and prompt-based CL methods."}]}