{"title": "CluMo: Cluster-based Modality Fusion Prompt for Contin- ual Learning in Visual Question Answering", "authors": ["Yuliang Cai", "Mohammad Rostami"], "abstract": "Large vision-language models (VLMs) have shown significant performance boost in various application domains. However, adopting them to deal with several sequentially encoun- tered tasks has been challenging because finetuning a VLM on a task normally leads to reducing its generalization power and the capacity of learning new tasks as well as causing catastrophic forgetting on previously learned tasks. Enabling using VLMs in multimodal continual learning (CL) settings can help to address such scenarios. To improve general- ization capacity and prevent catastrophic forgetting, we propose a novel prompt-based CL method for VLMs, namely Cluster-based Modality Fusion Prompt (CluMo). We design a novel Key-Key-Prompt pair, where each prompt is associated with a visual prompt key and a textual prompt key. We adopt a two-stage training strategy. During the first stage, the single-modal keys are trained via K-means clustering algorithm to help select the best semantically matched prompt. During the second stage, the prompt keys are frozen, the se- lected prompt is attached to the input for training the VLM in the CL scenario. Experiments on two benchmarks demonstrate that our method achieves SOTA performance.", "sections": [{"title": "1 Introduction", "content": "Visual Question Answering (VQA) is a complicated task, where the goal is to answer questions described in natural language (text) about a given input image. Addressing VQA requires understanding and fusion of information from both the visual and textual domains to generate accurate responses. Recently, significant advancements in addressing VQA tasks have emerged due to the development of pre-trained large vision- language models (VLMs) Radford et al. (2021); Kim et al. (2021). Despite these advances, one of the persistent challenges in VQA tasks is the ability to adapt a VLM in CL setting to avoid finetuning a copy of an underlying VLM per task. In a CL setting, we learn new tasks and aim for continuously improving the model performance without forgetting previously learned knowledge, also known as catastrophic forgetting French (1999). To address catastrophic forgetting, a group of CL algorithms are deployed. Regularization- based methods Kirkpatrick et al. (2017); Jin et al. (2021) constrain the drastic parameter shift when learning new tasks. Expansion-based methods Douillard et al. (2022); Cai et al. (2023b) expand the model with small portion of additional weights and use the expanded weights to learn the new incoming tasks. Rehearsal-based methods Rebuffi et al. (2017); Rostami (2021) store a representative subset of the training dataset for each task into a small memory buffer and replay them back during the learning of the current task to maintain the encoded knowledge of the previously learned tasks. More recently, prompt-based methods Wang et al. (2022b;a); Cai et al. (2023a) aim to use prompts that contains task-specific or semantic-specific information to prevent catastrophic forgetting. A prompt is attached to the embedded features of the input to adapt the model to focus on the specific characteristics of the input task that has been learned before.\nMost existing CL methods consider unimodal, i.e., vision-only or language-only, settings and hence are inapplicable to address VQA tasks. To tackle this shortcoming, we propose a novel two-stage prompt learning-based CL method, namely cluster-based modality fusion prompt (CluMo). Figure 1 visualizes the high-level idea of our approach. Our method adopts a pre-trained VLM as its backbone and benefits from"}, {"title": "2 Related Works", "content": "Visual Question Answering Visual Question Answering (VQA) has been a pivotal task at the inter- section of computer vision and natural language processing which led to advances on more complex tasks. Initially, VQA was formulated as a classification task in which answers are selected from a predefined set of answers Agrawal et al. (2016) and was solved by using CNNs for image feature extraction and RNNS for text processing. These models were too simple to be used in most practical cases. With the develop- ment of transformer and BERT-like models Lu et al. (2019); Li et al. (2019), performance in VQA tasks has significantly been improved due to the better capacity of capturing the intricate relationship between two modalities and generating the response in the form of meaningful and descriptive texts. Despite these advances, VQA tasks are mostly studied in static settings Goyal et al. (2017); Johnson et al. (2016); Marino et al. (2019), where it is assumed that there is a single input task. As a result, most existing methods are inapplicable in dynamic environments and settings such as continual learning (CL) Srinivasan et al. (2022) when we encounter several tasks over time."}, {"title": "3 Problem Description", "content": "Consider a set of VQA tasks, $\\{T_j\\}_{j=1}^J$, which are encountered sequentially and each of them is from different domain. For each of the tasks, a labeled training dataset $D_i = \\{((I, L),y)_{i=1}^{N_i}\\}$ is accessible, $N_i$ denotes the size of dataset, $I \\in R^{H\\times W\\times C}$ denotes the input image, $L\\in R^{L\\times|V|}$ denotes the input text, and y denotes the text-typed discrete label. The order in which the VQA tasks are observed is not known in advance and the training data points are assumed to be drawn iid from a task-specific joint distribution $p(I, L, y)$. Upon learning each task, the model moves forward to learn the next task. Since all the previously learned tasks can be encountered at any time during testing in the future, the model should learn new tasks such that its knowledge of previously learned tasks is maintained, i.e., by preventing catastrophic forgetting.\nWe formulate our problem in a domain-incremental learning setting Van de Ven & Tolias (2019) which assumes all the tasks are from different domains and the boundaries between them are known during learning time. We consider that each task can be learnt individually by adapting a pre-trained large multimodal transformer $f_{\\theta_M}(.,.)$ via minimizing a suitable discrimination loss $\\mathcal{L}$, e.g., cross entropy. In our approach, all the model parameters, except the final classifier layer $\\theta_{cls}$, are frozen during training to preserve the generalizability of the model. We benefit from prompt learning to enable using a single model to learn all tasks. To prevent catastrophic forgetting, a trainable task-specific prompt pool, which contains several task-specific prompts, is attached to the model $f_{\\theta_M}(.,.)$ such that the best-semantically-matched prompt is selected based on image and text inputs for task specialization. The prompt is then pre-pended to the input vectors so that the output is generated based on specialization. Our method is rehearsal-free and does not need any memory buffer similar to prior approaches Lopez-Paz & Ranzato (2022); Rostami & Galstyan (2023)."}, {"title": "4 Proposed Architecture", "content": "Our architecture, named cluster-based modality fusion prompt (CluMo), contains two unimodal task-specific cluster-based keys for vision and text embeddings and one prompt pool. The combination of the selections from both keys is then used to select the best matched prompt from the prompt pool. A high-level diagram of our approach is presented in Figure 2. In this section, we first introduce the preliminaries such as backbone model and prompt pool-based method in 4.1, and modality fusion prompt in Sec. 4.2, then the cluster-based prompt key is described in Sec. 4.3, and the training and the inference strategy is discussed in Sec. 4.4."}, {"title": "4.1 Preliminary", "content": "Backbone The base multimodal transformer contains three encoders: the visual encoder $V_E$, the textual encoder $T_E$, and the multimodal fusion encoder $F_E$. Given a visual input V, i.e., a single image, and a textual input T, i.e., a question, the data processing pipeline for the model is:\n$\\hat{y}(V, T) = F(F_E([V_E(V); T_E(T)])),$ (1)\nwhere $F(.)$ is the classifier to predict the answer.\nPrompt Pool As an adoption of prompt learning in continual learning, a prompt pool is a set of trainable key-value (K-P) pair, in which $K \\in R^{L_K\\times D}$ denotes the \u201cprompt key\", and $P \\in R^{L_P\\times D}$ is the prompt. $L_P$ and D denote the length and dimension of the prompt. Given an input image V, we compute $v_i = V_E(V) \\in R^{L_V\\times D}$, where $L_V$ is the dimension of the features, after passing the image through the visual encoder. $V_{I0} = v_i[0]$ is matched with all the keys K within the prompt pool via similarity score, such as cosine similarity, to find the most similar $K_1$. The corresponding $P_i$ is selected and prepend to V as $V' = [P_i; V]$. Parameters of K and P are updated through back-propagation during the training. However, in our setting, we adopt a two stage training strategy that prompt keys are trained before model and prompts."}, {"title": "4.2 Modality Fusion Prompt", "content": "Previous prompt-based CL methods such as L2P Wang et al. (2022b) associate each prompt in the prompt pool with a single prompt key to form Key-Value pair. In practice, the prompt keys in prompt-based CL can be considered as cluster centers. These cluster encode a notion of similarity between the prompts. The input feature vectors that form a cluster in the feature space can be assigned to these cluster centers, which are prompt keys. The intuition behind this idea is that feature vectors with small geometric distance in the feature space are semantically similar Wang et al. (2023).\nHowever, such a key-value pair design considers only single modality without tasks with multimodal inputs. The reason is that different input modalities contain different or complementary semantic information. Hence, having prompt keys that associate with each modality can help guiding prompt selection, which is more comprehensive and representative in term of semantic properties of each modality. Thus, we propose a task-specific prompt pool architecture, namely Modality Fusion Prompt, which is composed of the visual prompt keys $K_v$, the textual prompt keys $K_t$, and the prompt pool P as following:\n$K_t = [K_{t1}, K_{t2}, ..., K_{tS_t}],$\n$K_v = [K_{v1}, K_{v2}, ..., K_{vS_v}],$\n$P = [P_1, P_2..., P_{S_p}],$ (2)\n$K_{tm} \\in R^{D}, K_{vn} \\in R^{D}, P_i \\in R^{L_P\\times D},$\nwhere $S_t$, $S_v$, and $S_p$ are the sizes of textual prompt key, the visual prompt key, and the prompt pool, respectively. $L_p$ is the length of each prompt and D is the hidden dimension of the transformer backbone. The prompt pool size $S_p$ is then determined as $S_p = S_v \\times S_t$. Each prompt is associated with the unique combination of one visual prompt key and one textual prompt key. Given a specific visual prompt key $K_{vm}$ and a specific textual prompt key $K_{tn}$, the Key-Key-Value pair is defined as the following:\n$(K_{vm}, K_{tn}) \\rightarrow P_{m*S_v+n}.$ (3)\nAs modality fusion prompt is task-specific, new visual prompt keys, textual prompt keys and a prompt pool will be initialized for each of the new coming task. The previous ones are frozen during training."}, {"title": "4.3 Cluster-based Prompt Key", "content": "K-means clustering has been widely adopted in machine learning algorithms for semantic separation and understanding, where data from different domains can be explicitly separated via clustering in an unsuper- vised way Wang et al. (2023) Cohn & Holm (2021). However, even though the data from single task belong to the same domain, they can still be further divided into sub-domains based on the semantic property. To make each prompt key be the semantically cluster center of the sub-domains for both vision and text inputs, we adopt mini-batch K-means clustering algorithm on prompt keys of K and Kt to make each prompt key diverse and representative. Let B = (I,T) be the random batch from the training dataset. We extract the image feature vector $v_I$ and the text feature vector $v_T$ as follows:\n$v_I = V_E(I), v_T = T_E(T),$ (4)\nwhere $v_I \\in R^{B\\times L_I\\times D}$ and $v_T \\in R^{B\\times L_T\\times D}$, B is the batch size, $L_I$ and $L_T$ are the length of vectors for image and text features, represent the embedded image and text input respectively. For visual prompt key clustering, each image feature vector, $\\hat{v}_{In}$, is set by taking mean along second the dimension such that $\\hat{v}_{In} \\in R^{B\\times D}$, and $\\hat{v}_{In}$ is used to compare with every prompt key in K:\nsimilarity$(n, m) = ||\\hat{v}_{In} - K_{vm} ||_2,$ (5)\nand the prompt key with highest similarity is assigned to match $v_{In}$. After calculation of the whole batch B, the prompt keys are then updated by calculating the mean of all $\\hat{v}_{I}$ assigned to that specific visual prompt key. We repeat the above step until the convergence. The procedure of updating the text prompt key Kt is similar to updating the image prompt keys. Algorithm 1 summarizes our approach for prompt key training."}, {"title": "4.4 Training and Inference", "content": "During training, we adopt a two-stage training strategy to ensure that the prompt keys are correctly settled before learning the current task. In the first stage of learning each task $T_i$, we random select batches from the current task's dataloader to train minibatch k-means Cluster on the visual and the textual prompt key $K_v$ and $K_t$ until reaching the convergence of the clustering algorithm. During the second stage, the trained $K_v$ and $K_t$ are frozen. Within the iteration of training dataloader, each training instance is assigned to its nearest prompt key using k-nearest neighbor (KNN) algorithm to find the best match prompt $P_k$ from the prompt pool P. $P_k$ is then attached to the model pipeline:\n$\\hat{y}(V,T) = F(F_E([P_k; V_E(V); T_E(T)]))$ (6)\nDuring the second stage, we also use knowledge distillation to further boost the performance. Before the training of task T, we keep a frozen copy of model after finishing T\u22121, denoted as $M_{T-1}$. To prevent significant parameter shift, we pass the same input image and question to both $M_T$ and $M_{T-1}$ and add the difference between the two model's output to the loss:\n$\\mathcal{L}_{KD}(V,T) = MSE(\\hat{y}_{M_T}(V,T), \\hat{y}_{M_{T-1}} (V, T)).$ (7)\nThe final objective loss function would be:\n$\\mathcal{L} = \\mathcal{L}_{ce}(\\hat{y}(V, T), y) + \\mathcal{L}_{KD}$ (8)\nWhere $\\mathcal{L}_{ce}$ is the same cross entropy loss. The overall training procedure for all tasks come in sequence is presented in Algorithm 2.\nDuring inference, the model is frozen and we follow a procedure similar to the second stage of training. For every training image-text pair, the image input is aligned with the best-matched image prompt key while the text input is aligned with the best-matched text prompt key. The combination of prompt keys is deployed to find the corresponding prompt, which is pre-pend to the output of multimodal encoder. To help better understand the procedure of prompt selection, an visualization example is provided in Figure 3."}, {"title": "5 Experiments", "content": "Our implementation code is available at https://github.com/YuliangCai2022/CLUMO.git. Please refer to the code for reporducing the results."}, {"title": "5.1 Experiment Setup", "content": "Backbone We used the public pre-trained large multimodal transformer, ALBEF Li et al. (2021), as our backbone for VQA task. It consists of an image encoder, a text encoder, a multimodal encoder, which uses cross-attention between the two modalities. Specifically for VQA tasks, an pre-trained answer decoder is append after the multimodal encoder, which has same architecture as multimodal encoder.\nBaselines for comparison We use seven methods for comparison. We include algorithms from major CL approaches. We include two regularization-based methods: EWC Kirkpatrick et al. (2017) and LwF Li & Hoiem (2017), two rehearsal-based methods: ER Rolnick et al. (2019) and GEM Su et al. (2021). We also include three SOTA prompt-based continual learning methods, L2P Wang et al. (2022b), DualPrompt Wang et al. (2022a), and S-Prompt Wang et al. (2023). We also include finetuning to demonstrate the positive effect of CL. Following the original setting of each method, we leave the whole backbone model unfrozen for non-prompt-based methods and freeze the whole backbone model for prompt-based methods except for the classifier. To make the fair comparison, we fit all the continual learning methods into our backbone, ALBEF, instead of using the original model proposed in each method.\nCL Tasks We evaluate our method on tasks built using the CLOVE Lei et al. (2022) dataset which is a VQA-based continual learning dataset. The benchmark contains two sepecate benchmarks for different scenario, including scene-incremental setting benchmark, CLOVE-scene, and function-incremental setting benchmark, CLOVE-function. Each of the task sets contains six tasks which are domain-specific and diverse from each other. For more details about CLOVE and the tasks we use, please refer to the Appendix.\nMetrics for comparison We use the average accuracy and the average forgetting rate on all tasks to evaluate the performance of our method and its ability to tackle catastrophic forgetting. Different from dataset such as VQAv2 Goyal et al. (2017), where each question is paired with different ground truth answers, questions in GLOVE dataset only contains exactly one correct answer for each question. Thus, the accuracy is simply calculated by y == \u0177 for every training and testing data instance. On the other hand, the forgetting rate is calculated as:\n$F = \\frac{A_i - A_{ij}}{A_i}$ (9)\nwhere $A_i$ is the accuracy of task i, and $A_{ij}$ is the accuracy of task i after the model is trained on task j. For details about the optimization and implementation processes, please refer to the Appendix."}, {"title": "5.2 Comparative Results", "content": "We conduct the comparison experiments on both the CLOVE-scene and CLOVE-function task sets with a randomly selected task order. In table 1, the task order abcdef represents the CL tasks: Shop AndDining, WorkPlace, HomeOrHotel, Transportation, SportAndLeisure Outdoors in sequence. The oarlks in CLOVE-function represents tasks: ObjectRecognition, AttributeRecognition, RelationReasoning, LogicReasoning, KnowledgeReasoning and Scene TextRecognition.\nWe observe in Table 1 that our method outperforms all the baselines across all task order sets in terms of both average accuracy and average forgetting rates. We also observe that the performance of different method within the same group tend to be similar. The regularization-based methods, EWC and LwF, obtain the sub-optimal accuracy and forgetting rate besides. The reason is that the domain for each task in the dataset is significantly different from the rest of tasks and hence regularization methods fail to capture the common space of the parameter distribution. This challenge makes it difficult to maintain the accuracy of the current task and previous tasks at the same time using regularization. The replay methods, ER and GEM, achieve better performance than regularization-based methods. This can be explained by the fact that replaying the data from previous task is an efficient way to remind the model and adjust its parameter distribution not too diverse from previous ones. However, because we need to rely on a memory buffer to store samples for replay, these methods are memory-consuming and thus not space-efficient. Moreover, replay-based methods are still limited by the upper-bound of joint training, as they generally can only reduce catastrophic forgetting without boosting the accuracy of individual tasks. On the other hand, the prompt-based methods, namely L2P,"}, {"title": "5.3 Ablation Experiments", "content": "To offer a better insight about our method, we perform an ablation study for each component of CluMo to study the positive contribution of each component. We study the effect of the following:"}, {"title": "5.4 Analytic Experiments", "content": "Effect of clustering To show the effect of clustering algorithm, we empirically show the correlation between the clustering error and the downstream accuracy. As we apply Euclidean distance as metric to learn the clusters, we record the average distance between each point to its assigned cluster center for every task, and take the average for all the tasks:\n$E = Avg(\\sum_{i=1}^N Avg( \\sum_{j=1}^M ||x_j - C_k||_2))$ (10)\nwhere i represent the number of tasks, j represent the training data from task i and k is the $k^{th}$ cluster center. We consider both the visual prompt key training and the textual prompt key training in this experiment."}, {"title": "6 Conclusion", "content": "We introduced a novel prompt-based continual learning method for learning multimodal tasks. While most of existing methods apply simple prompts on a single modality, our method proposes modal-specific visual prompt keys and textual prompt keys and train them to capture the semantic properties of the training dataset using K-means clustering algorithm. We use the combination of both the visual prompt key and the textual prompt key to select prompts, which enable the prompt to better boost the performance. Our experiments show that our method achieves the state-of-the-art performance in continual VQA tasks in different domains compared to other regularization-based, rehearsal-based and prompt-based CL methods."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Hardware Setup and Hyper-parameter", "content": "All experiments were conducted using a single Nvidia A40 GPU. We employed the AdamW optimizer across all experiments, utilizing a cosine learning rate scheduler, and set the initial learning rate to lr = 3 \u00d7 10\u22124. The models were trained for 5 epochs with a batch size of 16.\nFor EWC, we set the fisher sample percentage to be 0.1 and ewc loss weight equals to 0.1 as well.\nFor Experiment Replay, we store 1% of data from each tasks into the memory buffer. During the training of the current task, we randomly select a batch of data from the memory buffer to train the model for every 100 batches of current data training.\nFor GEM, we also store 1% of data to memory buffer, which is randomly picked from each tasks.\nFor CluMo framework, we configured the visual prompt key size (S) and the text prompt key size (St) both to 3, with the prompt length (Lp) set to 10.\nFor L2P, we set the prompt pool size equals to 20 and prompt length to be 5.\nFor DuamPrompt, the G-Prompt was inserted into layers 0 and 1 of the visual encoder, while the E-Prompt was integrated into layers 2, 3, and 4.\nFor S-Prompt, we set the prompt length to be 10 and prompt pool size to be 30.\nFurthermore, for prompt-based techniques such as L2P, DualPrompt, and SPrompt, we opted to freeze the entire backbone model, allowing only the final classifier layer to remain trainable. Conversely, for all other baseline methods, no parameters were frozen, ensuring the entire network was fine-tuned during training."}, {"title": "A.2 CLOVE dataset detail description", "content": "In the CLOVE-Scene and CLOVE-Function datasets, all tasks have a uniform distribution of training and testing data, with the exception of the SceneTextRecognition task, which comprises 16.8K training samples and 2.4K testing samples. The remaining tasks within these datasets contain 20K training samples and 3K testing samples each. It is reflected in the Table 5.\nTo provide a deeper understanding of the CLOVE dataset, we offer additional details here. We have included visualizations in Figure 7 and Figure 8 to showcase two sample images from each task within the datasets, emphasizing the distinctiveness of each domain. From these samples, it is evident that the images in the CLOVE-Scene dataset vary significantly across tasks, even though the questions associated with them are similar in structure, differing primarily based on the content depicted in the images."}]}