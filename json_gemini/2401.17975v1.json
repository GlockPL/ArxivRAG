{"title": "Understanding polysemanticity in neural networks through coding theory", "authors": ["Simon C. Marshall", "Jan H. Kirchner"], "abstract": "Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al. (2022). Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability.", "sections": [{"title": "Introduction", "content": "As the capabilities of deep neural networks increase, the need for interpretability to understand failure modes, fix bugs, and establish accountability becomes increasingly apparent. Given an explanation of the internal structure of a deep neural network, we might be able to infer the logic that leads to a given output and correct this logic to conform with our preferences for the model's behaviour. The methods for achieving interpretability overlap with those of other areas of deep learning research. For example, more interpretable networks tend to be more robust to adversarial attacks, while more robust networks tend to be more interpretable. Modularity and continual learning are also related to interpretability, as methods that promote specialization among weights or neurons can make the network easier to understand.\nDespite the rapid development of interpretability techniques, key challenges remain. Techniques need to be sufficiently robust to scale to large models, withstand rigorous evaluation, and work reliably across domains. One particular challenge for interpretability is the phenomenon of polysemanticity, whereby neurons are activated by multiple unrelated features. Polysemanticity poses a challenge for interpretability based on individual neurons, which will necessarily be incorrect or incomplete. One explanation for why polysemanticity is near ubiquitous is the superposition hypothesis which posits that neural networks overload individual neurons with multiple roles in order to leverage the maximum available network capacity. A deep understanding of these phenomena will be important for advancing interpretability research."}, {"title": "Results", "content": "Polysemanticity and superposition as the default for efficient encoding of information. When presented with an appropriately formatted input, the hidden layers of a deep neural network activate based on the characteristics of the input and based on the connectivity of neurons within that network. Individual neurons in the network will activate very selectively to a few inputs (monosemantic coding) or will activate broadly to multiple inputs (polysemantic/superposition coding) depending on the learned code. While these phenomena traditionally are studied at the single-neuron level, we regard them as properties of the network code. Viewed from this perspective, it is immediately apparent that a network that needs to encode a number of features larger than the number of neurons per layer will benefit from utilizing a superposition code.\nTo investigate whether neural networks can indeed learn to leverage an approximately optimal encoding of its input, we reanalyze a result from a recent study on toy models of superposition. In this setting, a deep neural network learns to encode and decode inputs, x, with varying sparsity, S. By computing the information capacity of a neural network and comparing it to the information"}, {"title": "", "content": "content of a sparse input, we can calculate the compression rate, R, that could be achieved by an optimal code,\n\\(R = 1-S+\\frac{1}{H}(-Slog(S) \u2013 (1 \u2013 S) log(1 \u2013 S)),\\)\nwhere H = (log p\u2093) is the information content (entropy) of a single random input, x (see Supplementary Note for detailed derivation). For sufficiently large H, e.g. 16 or 32 bits, R can be approximated as R = 1 - S. Intriguingly, this analytic result provides a good fit to the simulation data, with the deviations from the optimal being due to \"sticky\" encoding regimes where the toy model learns a highly robust but suboptimal encoding.\nMight an optimal code be a more interpretable code? The hardness of average case decoding of neural network codes is unknown, but results for specific tasks indicate the problem might be difficult. Indeed, for some tasks, we demonstrate that the optimal code resists polynomial time decoders almost always (see Supplementary Note). Given that optimal polysemantic codes reuse the same neuron for many concepts and deciding properties of highly convoluted functions is generally hard, we might expect networks that learn a nearly optimal code to be very difficult to interpret.\nRobust encoding implies reduced channel capacity. While the code learned by networks might be very difficult to interpret in general due to its high information content, in practice, neural networks do not solely maximize information density. In the face of noise and other perturbations, neural networks must be robust by introducing redundancies and other error-correcting steps into the code. To illustrate the effect of noise, we consider dropout, a common form of regularisation in neural networks. Dropout is itself a noise source: as information passes through layers, a fraction, D\u2208 [0, 1], of the total number of neurons is erased. A na\u00efve dense code (such as that from the previous subsection) with no redundancies would immediately lose a fraction D of information encoded across neurons in the first dropout layer and then a fraction D again in the subsequent dropout layers, giving an effective information capacity of \\(C_{dropout} \\sim C'(1 \u2013 D)^N\\), after N dropout layers, where C is the capacity of the noiseless channel.\nHowever, by utilizing a redundant encoding, it is possible to introduce an error-correcting component into the code, which is resistant to some noise sources. When error-correction is present, the neural network can reliably transmit the entire channel capacity of a layer with dropout through every layer of the network (Fig. 2c, see Supplementary Note for detailed derivation),\n\\(C_{dropout}\\simC(1 \u2013 D).\\)\nWe verified that a neural network trained as an autoencoder could indeed learn error-correction by measuring the mutual information between the input and the hidden layers and observing that after"}, {"title": "", "content": "training, the network does not experience exponential decay of information (see Supplementary Note for details).\nThus, when a neural network is exposed to moderate noise, we expect the code to become more redundant, distributing the representation of inputs across multiple activation patterns. While this distribution of representations across neurons discourages a monosemantic code, it is unclear whether it might facilitate the holistic interpretation of activations across a given layer.\nCodes produce eigenspectra with varying speeds of decay. We next turned to probe the high-level properties of a neural network's code. A natural approach that was recently proposed in the study of coding properties of biological systems is to investigate the eigenspectrum of the activation covariance matrix. The shape of the eigenspectrum can reflect the properties of the network's code. In particular, a largely redundant code has an eigenspectrum with a steep decay since only a small subset of possible configurations is actually relevant to the network's code. In contrast, a maximally non-redundant code will exhibit a more gradual decay in the eigenspectrum since it exploits the entire space of possible configurations. In practice, eigenspectra tend not to match either extreme and instead decay according to a power law with parameter \u03b1. Applying this approach to deep neural networks, we find the same power law decay but with varying values of \u03b1. By using this encoding, the effective channel capacity becomes approximately.\n\\(C_\\alpha\\approx C_a m \\log(m/e),\\)\nwhere m is the width of the given layer and C is the channel capacity using the optimal dense encoding (see Supplementary Note for detailed derivation).\nWhen visualizing the eigenspectrum of the hidden activations of a trained Autoencoder-ResNet trained on the ImageNet dataset without dropout (Fig. 3a), we find that the decay of the eigenspectrum changes non-monotonically over the depth of the network. The eigenspectrum initially decays with a power law exponent just below \u20131, which is consistent with the observation that the spectrum of natural images decays with that same exponent. The eigenspectrum decay then slows down, with the power law exponent increasing above -1 for the intermediate layers of the network,"}, {"title": "", "content": "before eventually decaying rapidly with a power law exponent < -1 for the final layers. Consistent with results from neuroscience, the corresponding eigenfilters vary from coarse global filters for the first eigenvectors to intricate filters sensitive to local variations for later eigenvectors. We note that the recovered eigenvectors resemble previously described Gabor receptive fields as well as curve detectors.\nIn summary, the eigenspectrum of a trained Autoencoder-ResNet reveals that the trained network learns a code that is neither highly redundant nor one that maximizes capacity. Instead, we find that the neural code exhibits some redundancy, particularly towards early and late layers, while employing less redundant codes in intermediate layers.\nRandom projections reflect smoothness of code. What can we say about the neural network code beyond general statements regarding its redundancy? We hypothesized that an input that is encoded with more redundancy might be easier to decode with linear probes. To this end, we projected the m hidden activations produced by a moving dot stimulus with n frames (Fig. 4a), \\(A \\in R^{n \\times m}\\), onto a randomly chosen basis of vectors with normalized variance, \\(r \\in R^{m \\times 3}\\),\\(r_i \\sim N(0, I)\\). These random projections, z = Ar, have the useful property that their average action, \\(<E[z]) = \\int ||\\dot{z}(t)||^2dt\\), is proportional to the decay factor \u03b1,\n\\(<E[z]) = \\sum_{k=1}^{m} \\frac{1}{\\lambda_k} \\sim \\zeta(-{\\alpha}),\\)\nwhere are the eigenvalues of the hidden activations and \u03b6 is the Riemann zeta function restricted to the real numbers, \\(\\zeta(s) = \\sum_{k=1}^{\\inf} \\frac{1}{k^s}\\).\nWhen displaying the random projections as curves, we indeed observed that while early layers produce highly discontinuous projections, later layers of the network recover some of the smooth structure of the input (Fig. 4b). In particular, we noticed that the transition between discontinuous and locally smooth occurs as the decay factor \u03b1 falls below -1 (Fig. 4c). From eq. 4, we see that in this regime, the expected action of the curve is finite, while in the regime \u03b1 > \u22121, the action of the curve grows without bound.\nThe slow decay of eigenspectra in artificial neural networks stands in stark contrast to the eigenspectra in the visual cortex of the mouse and the macaque. In this biological network, the power law exponent a is consistently below -1, constraining the network to use a continuous and differential code of the input. Instead, the artificial neural network resembles the much smaller zebrafish with a slowly decaying eigenspectrum with \u03b1 \u2248 -0.7."}, {"title": "", "content": "Taken together, we found that the speed of decay of the eigenspectrum determines the statistical properties of random projections of stimuli. A fast decay (\u03b1 \u2264 \u22121) is common in larger mammals and results in continuous and differentiable projections. A slow decay (\u03b1 > -1) is more prominent in small artificial neural networks and in the zebrafish and implies discontinuous random projections.\nDeviations from power law distribution In the final layers of figure 3 we can see that the eigenspectrum in some cases systematically deviates from the power law. These deviations stem from transitions from the 'biological power-law' case to the optimal 'redundant code' case (the step function in Fig. 3a), which helps protect the information from loss. To test this hypothesis we investigate the effect of higher and lower levels of dropout noise on the power spectrum (Fig. 5).\nIn early layers, both the high and low dropout cases are well described by a simple power-law. In contrast, intermediate and late layers exhibit increasingly pronounced step-like spectra, especially in the higher-dropout layer. These properties indicate a large linear redundancy in the code, which is a characteristic signature of error-correcting codes.\nIdentifying the exact error-correcting code is beyond the scope of this paper, but we note that the non-linear decay is not a sharp step function, as would be the case with a linear error-correcting code. While this decay is consistent with a suboptimal encoding, we hypothesise that instead, the learnt code is not entirely linear. Consistently, the dropoff becomes more pronounced in later layers of the network, likely because linear error correction early in the network is both challenging (there are fewer layers to arrange the information into a code) and less necessary (as the image already contains many redundancies, minimising damage done by losing a couple of pixels).\nDropout incentivizes fast decay of eigenspectrum. As a network's code depends on the amount of noise present, it is natural to ask how the learned code changes as the dropout rate increases. We systematically varied the amount of dropout the network experiences (Fig. 6a) and observed that higher dropout generally produce lower power law exponents (Fig. 6b). Interestingly, already for a dropout rate of 20%, the power law exponent remains very close to -1, as in the visual cortex of the mouse and the macaque. Consistent with our observation in networks without dropout (Fig. 4), networks with lower power law exponent also tend to produce random projections with lower average action (Fig. 6c).\nTo test whether networks with higher dropout might encode more robust representations of their input throughout their layers, we computed the variance of the input explained by random projections from different layers of the network (see Methods). We found that the explained variance increases with both dropout rate and network depth (Fig. 6d). Consistently, we found that trajectories with low average action tend to explain a higher fraction of input variance (Fig. 6e).\nThese findings suggest that introducing dropout during training can reliably produce networks with a faster-decaying eigenspectrum. Random projections from these networks exhibit low average action and tend to resemble robustly and linearly encode some global features of the input.\nEfficient codes provide a framework for understanding language model interpretability. The success of deep learning models in natural language processing, particularly the transformer architecture, provides a range of new challenges and opportunities for interpretability research. A prominent challenge is the unprecedented size of large language models, which makes the use of interpretability methods that rely on human annotation of individual components infeasible. A"}, {"title": "", "content": "recent study proposes a GPT-assisted method for generating single neuron explanations in a scalable fashion. While the proposed method manages to provide explanations with predictive power for thousands of neurons, the approach is limited to cases without polysemanticity and superposition, which our work suggests is a small fraction. To investigate whether our approach of interpreting random projections might be applicable to this setting, we turned to analyze the eigenspectrum of a large language model.\nWe calculated the power law exponent a as a function of network depth. We observed that the power law exponent consistently remains above the critical value (\u03b1 \u2265 \u22121, Fig. 7a), indicating unbounded variance of random projections. This observation is consistent with the highly nonlinear nature of natural language.\nTo investigate whether a lower power law exponent might nonetheless translate into more expressive random projections, we compute the explanation score (see Methods) for the activation of single neurons and random projections across different layers of the network (Fig. 7b,c). Interestingly, we found that the average explanation score changes non-monotonically, starting at comparatively high values in the first layers, decreasing up until two-thirds of the network depth, and eventually increasing again. The explanation score for random projections is the inverse of the score for single neurons, with our method performing well in the layers in which the single-neuron method performs"}, {"title": "Discussion", "content": "In this study, we explored the properties of neural network codes and their implications for interpretability, leveraging concepts from information theory and neuroscience. Our analysis revealed that the eigenspectrum of the activation covariance matrix provides insights into the redundancy of a network's code and its capacity for error-correction. Our results demonstrated that networks with higher dropout levels tend to employ codes with faster-decaying eigenspectra, yield smoother random projections, and better explanations of input variance. Furthermore, we found that our approach to interpreting random projections might complement existing single-neuron interpretability methods, particularly in the context of large language models. Overall, our findings provide a top-down perspective on neural network interpretability, advancing our understanding of the underlying structure of these models and suggesting new avenues for future research.\nA fascinating aspect studying eigenspectrum decay in artificial neural networks is the potential connection to neuroscience. In the same way that sufficiently large animals exhibit a neural code that optimally trades coding capacity and robustness to perturbations, we hypothesize that sufficiently large artificial neural networks might learn to strike the same balance. Indeed, a recent study indicates that networks with a fast eigenspectrum decay are more robust to adversarial examples and investigates which aspects of training and network architecture result in a more optimal decay. A systematic study of how the eigenspectrum decay changes as a function of network size might reveal important information on whether we can expect large language models to have smooth random projection.\nAn important direction for future research is to investigate the role of nonlinearities in the network code. Considering that neural networks were initially developed for nonlinear signal processing, it is moderately surprising that modern networks allow linear read-out from their hidden layers. Several factors incentivize linearity in hidden layers, such as the linear structure of the residual stream and the training objective of supervised learning which incentivizes linear separability of concepts. Conversely, linear codes can have comparable performance to nonlinear codes; hence they are not disadvantaged under optimization pressure. However, the exact reasons for which linear structure networks converge to is an important open question into which our current work might provide insight.\nOverall, our efficient channel coding framework provides a top-down perspective on interpretability in neural networks and offers a promising direction for future research in understanding deep learning models' underlying structure and behavior."}, {"title": "Methods", "content": "Statistics. We perform a Principal Component Analysis on the activations of the ReLu units of a given layer. To reduce the computational capacity required by very high-dimensional data, we apply a subsampling method by selecting every s-th component, where s is computed such that the resulting dimension is less than 8000. We employ a randomized SVD solver for PCA fitting and restrict the number of components to a maximum of 1000.\nAfter fitting the PCA model, we perform a Huber regression analysis of the explained variance of the PCA components to estimate each layer's power-law exponent (\u03b1). The Huber regression is used due to its robustness to outliers. Specifically, we fit the Huber regression model on the logarithm of the PCA components' indices (from 10 to 50) and the logarithm of their corresponding explained variances.\nWe use a recently published method for automated interpretability in language models, which aims to explain the activation patterns of individual neurons in a large language model (GPT-2 XL) with a larger language model (GPT-4). We use GPT-4 to generate explanations of random projections of the activations of all neurons in a given layer of GPT-2. The value of these explanations is then assessed"}, {"title": "", "content": "by a third model (GPT-3.5), which predicts the activation of the explained neuron on a held-out test set. The correlation between the expected and the real output is then the explanation score.\nModel training. We studied the eigenspectrum of a ResNet-based Variational Autoencoder (ResNet-VAE) activations for image generation tasks. The ResNet-VAE model is built upon the ResNet-18 architecture pretrained on ImageNet. We remove the final fully connected layer from the ResNet architecture and replace it with two sets of linear layers to encode latent variables mu and logvar. The model employs two fully connected hidden layers with 1024 units each, followed by the latent embedding layer with 256 dimensions. A dropout rate of 0.2 is applied during training to promote regularization.\nThe model is trained on the CIFAR-10 dataset, which consists of 50,000 train and 10,000 test instances of 32x32 pixel images belonging to 10 different classes. The images are resized to 224x224 pixels and normalized before being fed into the network. We use the Adam optimizer with a learning rate of 1e-3 to optimize the network parameters. The training process consists of 20 epochs, with a batch size of 50. The model performance is measured by a custom loss function that combines Mean Squared Error (MSE) and Kullback-Leibler Divergence (KLD).\nIn addition to the ResNet-VAE model, we studied the GPT-2 XL model through the PyTorch Hub library. We computed explanation scores with the code published by Bills et al. (ref., but computed simulation scores with text-davinci-003 rather than GPT-4, since token logprobs are not available through the API at the time of this study.\nCode availability. After publication code will be publically available as a GitHub repository; at this time, all experiments are available in the supplementary material."}, {"title": "Experimental details", "content": "A zip file is provided in this supplementary material with the exact files used to create the results showcased in this experiment. We also provide brief descriptions of each file here to allow those looking to recreate our work or to find specific experiment details quicker access. After the anonymous peer review process is over these will be provided on a public github linked in the main text.\nResNetVAE/modules.py This code defines a neural network model called ResNet_VAE that is capable of encoding and decoding image data using a variational autoencoder architecture. The model uses a pre-trained ResNet18 model for encoding, and includes several fully connected layers and convolutional layers for decoding. The model is capable of generating a reconstructed image from an input image, as well as outputting a latent representation of the input image that can be used for further downstream analysis.\nResNetVAE/ResNetVAE_cifar10.py The ResNetVAE_cifar10.py script contains the code to train a variant of the Variational AutoEncoder using the ResNet architecture as an encoder to encode an image into its latent representation. The script uses the CIFAR10 dataset to train the model and record the loss metrics throughout the training process. The code handles saving and loading of model and optimizer states, and uses weights and biases for logging and visualization. Finally, the trained models are stored in local disk space as well. The script has an adjustable dropout probability and can be trained for an adjustable number of epochs.\nResNetVAE/make_plots.py This code is used to analyze the properties of a ResNet-VAE neural network model trained on the CIFAR-10 dataset. The code first loads the model and uses hooks to extract activations at specified layers. The activations are then used to perform PCA analysis and the variance explained by each component is plotted. The code then generates activations for a set of dot images and project them onto a low-dimensional space obtained through a random projection matrix. The energy of the projection and the correlation between inputs and the projection are computed and plotted. Finally, the code computes a decay coefficient for each layer of the model and plots it against the energy of the projection and the correlation between inputs and the projection. The results are saved to disk as figures.\nResNetVAE/helpers.py This code contains several utility functions for computer vision and machine learning tasks. It includes a function to generate a sequence of dot images moving in a circular path, and preprocesses the images using the default transform for CIFAR images. Another function computes the total energy of a set of points in n-dimensional space by calculating the squared magnitude of the differences between successive points. Additionally, there is a function that returns a dictionary of random projection matrices with specified dimensions for different layers in a neural network. These functions can be used to generate image data, calculate energy, and create random projection matrices for machine learning tasks.\nautomated-interpretability/neuron-explainer/generate_trajectory.py This code generates random tra-jectories for a pre-trained language model. It captures activations of certain layers during a forward pass and computes the total energy of a set of points in n-dimensional space. It then finds the minimum trajectory and project it to a 1D plot. The process is repeated multiple times to visualize different trajectories in each iteration. The final result is a set of PDF files and NPY files showing the 1D projections of different minimum trajectories. The generated plots can be used to gain insights into how the language model processes various sequences of text inputs.\nautomated-interpretability/neuron-explainer/generate_explanations.py This script generates explana-tions for the neuron activation patterns of a pre-trained GPT-2 XL language model using a calibrated simulator."}, {"title": "", "content": "The explanations are generated for the top 10 activation patterns of each layer in the neural network based on their mean maximum activation value compared to all activation patterns. The TokenActivationPairExplainer is used to generate explanations for the neuron activations, followed by simulating and scoring the explanation using an UncalibratedNeuronSimulator. The generated explanations and scores are saved for later use.\nEstimate-MI-of-NN-Layers.py This script trains a very deep neural network on a simple task (preserv-ing information) in a regime where error correction is needed to preserve any information. The results of this program are shown in figure 1"}, {"title": "Theorems and Proofs", "content": "We provide formal statements and proofs behind the results claimed in the main text."}, {"title": "Optimal sparse input ratio", "content": "This subsection derives the predicted optimal ratio # dimensions-per-feature to sparsity.\nFirst, we formally define the sparse input distribution. It is based on a uniform random distribution with S features set to 0."}, {"title": "", "content": "Definition 1 (Sparsification). Let S-sparsification be a process that takes a vector random variable v and returns a random variable v' with each element of v' independently set to 0 with probability S. If X is any distribution with i.i.d. elements then the S-sparsification of X is the distribution of X after S-sparsification, denote it as XS\nThe entropy of such a distribution is\nLemma 2 (Entropy of sparse distribution). Let X be some distribution. If XS is the distribution after S-sparsification then the entropy of XS is:\n\\(H(X^S) = -nS log(S) \u2013 n(1 \u2013 S) log(1 \u2013 S) + n(1 \u2013 S) H_{elem}(X)\\)\nWhere Helem(X) is the elementwise entropy of X\nProof. Express the joint entropy as the sum of the individual entropies (chain rule of entropy)\n\\(H(X^S) = H(X_1^S, X_2^S, ..., X_n^S) = \\sum_i(H(X_i^S|X_1^S, ..., X_{i-1}^S, X_{i+1}^S,... X_n^S))\\)\nas elements are i.i.d. (a condition of sparsification).\n\\(H(X^S) = \\sum_i(H(X_i^S)) = nH(X_i^S)\\)\nW.L.O.G. we have set all entropies equal to that of the first element of the vector.\nIf we assume smoothness of X the probability that any un-sparcified element of a vector is exactly 0 is vanishingly small \\(P(X_0 = 0) \\approx 0\\), allowing us to assume that if \\(X_i^S\\) is 0, it has been set this way by sparsification.\nThis implies that a single element of the sparcified vector has the distribution:\n\\(P(X_i^S = Y) =\\begin{cases}(1 \u2212 S)P(X = Y), \\text{if }Y\\neq 0\\\\S, \\text{if }Y = 0\\end{cases}\\)\nDirectly calculating the entropy of this distribution:\n\\(H(X_i^S) = - \\sum_X P(X_i^S)log(P(X_i^S)) = \u2212 \\sum_{X \\neq 0} P(X_i^S)log(P(X_i^S)) + P(X = 0)log(P(X = 0))\\)\n\\(H(X_i^S) \\approx \u2013 \\sum_{X \\neq 0} (1 \u2013 S)P(X_O) log((1 \u2013 S)P(X)) \u2013 Slog(S)\\)\n\\(H(X_i^S) \\approx \u2013 (1 \u2013 S) \\sum_{X \\neq 0} P(X_O) log(P(X)) \u2013 (1 \u2013 S) \\sum_{X \\neq 0} P(X_O) log(1 \u2013 S) \u2013 Slog(S)\\)\n\\(H(X_i^S) \\approx \u2212(1\u2212 S)H(X_O) \u2013 (1 \u2013 S) log(1 \u2013 S) \u2013 Slog(S)\\)\nWhich proves the result of the lemma.\nThe above statement is approximate, taking the assumption that \\(P(X_i = 0)\\) is arbitrarily small. This can be turned into a strongly rigorous statement by taking smoothness or other assumptions which amount to bounding \\(P(X_i = 0) < \\epsilon\\). In this case the stated entropy result is within additive \u03b5. For the case we are interested in (Elhage et al.) X is a uniform distribution of high precision floats, making \\(P(X_i = 0) \\approx 0\\) sufficiently accurate.\nThe stated ratio simply follows from this lemma by assuming the precision (and hence entropy) of the internal state/hidden layer/linear algebra is equal to that of the unsparcified input distribution, X: Helem. The channel capacity of m hidden units is \\(m \\times H_{elem}\\). The ratio, m : n, is found by divding one by the other."}, {"title": "Hardness of decoding a certain problem", "content": "As codes become more efficient (approach the Shannon limit given the entropy) it seems likely that they become less interpretable. To provide weight to this conjecture we outline a learning problem such that any attempt at decoding \u00b9 the optimal code in the final layer in polynomial time must fail, assuming cryptographic primitives.\nThe existence of such a task is relatively trivial, we can see that a neural network is capable of learning to implement a discrete exponential, an function which is easy one-way but complicated to find the inverse of. If any algorithm were able to decode the input from a given output we would be able to use the same algorithm to find the inverse, i.e. to solve the discrete logarithm problem. The optimality of code constraint forces the model to keep only the bits it needs for the problem, leaving just the exponential and no \u201cleaks\" as to what the input was.\nFormally the problem gives 3 values as input: a prime, p, a primitive element, g of \\(\\mathbb{Z}_p = \\{1, ..., p \u2212 1\\}\\), and \\(x \\in \\mathbb{Z}_p\\). The tasks is then to output \\(g^x (modp)\\). This is an easy function to implement and very likely learnable up to large p, g, and x (only existence, not learnability, is strictly needed to show the result). It is now obvious how any algorithm that could take the output of this circuit and calculate x could solve DLP.\nDLP is thought to be average-case hard for chosen groups (chosen p), restricting our problem case to these instances completes our result. Many other cryptographic one-way functions could be used in the place of DLP here, strengthening this result to rely on any one of a number of possible assumed hardness results. Indeed, given a public-private key encryption scheme which is average case hard for every bit of the input, we can modify the learning problem such that the neural network implements the public key encryption, again any decoder that breaks even a single bit of the encrypted activations in polynomial time could be used to recover the original message without access to the private key."}, {"title": "Channel capacity of a dropout layer", "content": "This subsection proves the channel capacity result claimed in the main text, namely that a neural network with n dropout layers, with dropout rate D has a channel capacity of \\(C_{dropout} \\sim C(1 \u2212 D)\\), where C is the capacity of the dropout-less layer.\nTheorem 3. The channel capacity of m independent neurons, each with capacity Chase when subject to an uncorrelated dropout layer with rate D is bounded by:\n\\((1 \u2212 \\epsilon) \\times (1 \u2212 D) \\times C_{base} \\times m \\leq C_{dropout} \\leq (1 - D) \\times C_{base} \\times m,\\)\nwhere \\(\\epsilon = - log(1 \u2013 2^{-C_{base}})\\) tends to zero asymptotically for large Chase (high precision).\nProof. We first note that \\(C_{dropout} = m \\times C_{dropout}\\) due to the chain rule of entropy and the independence of channels.\nWe then establish the lower and upper bounds separately for \\(C_{dropout}\\), both rely on the similarity of this channel to the standard erasure channel.\nLower bound(proof by construction) Create a uniform distribution on all but the 0 symbol, call this X. The entropy of this distribution is equal to the full uniform distribution sans 1 state:\n\\(H(X) = log(2^{C_{base}} - 1) = log(1-2^{-C_{base}}) + C_{base}\\)\nLet Y be the distribution given by putting X through the dropout layer. The mutual information between the two is:\n\\(I(X;Y) = H(X) \u2013 H(X|Y) = H(X) \u2013 \\sum P(Y = y)H(X|Y = y)\\)\nIf y = 0, H(X|Y = y) = H(X) as dropout applies to all inputs equally and the 0 symbol in X has no probability mass, if y \u2260 0 then y = x and H(X|Y = y) = 0. Thus:\n\\(I(X; Y) = H(X) \u2013 H(X|Y) = H(X) \u2013 P(Y = 0)H(X) = (1 \u2013 D)H(X)\\)"}, {"title": "", "content": "i.e. to take the code of an input and resolve the original input, or what information about the input has been conserved\nImplying the lower bound on the channel capacity\n\\(C_{dropout} \\geq (1 \u2013 D) \\times (log(1-2^{-C_{base}}) + C_{base})\\)\nUpper bound(proof by contradiction) Assume toward contradiction that \\(C_{dropout} = (1 - D)C_{base} + \\epsilon\\) for some fixed \u025b > 0. Let the n-bit erasure channel be the extension of the binary erasure channel to an input of n bits, i.e. on input \\(x \\in X = \\{0,1\\}^n\\) the channel produces output \\(y \\in Y = \\{0, 1\\}^n + e\\), where e is an error symbol, with probability:\n\\(P(Y = y|X = x) = \\begin{cases} \u03b1, y = e\\\\1-\u03b1, y = x\\end{cases}\\)\nThe proof of the channel capacity of this channel is a trivial extension of the binary case: I(X; Y) = H(X) + H(X|Y) = H(X) \u2013 \u2211P(Y = y)H(X|Y = y) = (1 \u2212 \u03b1)H(X)\nWhich is maximized when X is uniform, giving a capacity of (1 \u2212 \u03b1)n = (1 \u2212 \u03b1)Cbase.\nHowever, given access to a n-bit erasure channel with \u03b1 = D we can recreate a dropout channel by setting all e symbols to 0. If The channel capacity of our dropout layer was as assumed we could use the encoding procedure for the dropout layer, pass it through the n-bit erasure channel, perform the e = 0 operation and achieve the dropout layer capacity. As \u025b > 0 this implies it is possible to transmit more information than the channel capacity through the erasure channel, causing a contradiction."}, {"title": "Information loss from power law distribution", "content": "The entropy of a distribution with variance following power law distribution"}]}