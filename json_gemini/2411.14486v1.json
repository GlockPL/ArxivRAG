{"title": "The Impossible Test:\nA 2024 Unsolvable Dataset and A Chance for an AGI Quiz", "authors": ["David A. Noever", "Forrest G. McKee"], "abstract": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to\nacknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand\nchallenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including\nboth open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect\nresponses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in\nfields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem\ndifficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more\nchallenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more\nprone to generate speculative answers when problems appear more tractable. The study also revealed significant\nvariations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and\nNP-hard problems while performing relatively better on philosophical and psychological challenges. These results\ncontribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the\nimportance of uncertainty recognition as a critical component of future machine intelligence evaluation. This\nimpossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing\nempirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting\nnew directions for improving model training architectures and evaluation approaches.", "sections": [{"title": "INTRODUCTION", "content": "This research aggregates a novel and challenging dataset of 675 unsolved or grand challenge problems, inspired by a\nfuture moment when a machine of sufficient intellectual capabilities [1-3] might propose (previously impossible)\nanswers and suggest demonstrated proofs of concepts that challenge human experts [4-14]. The open-sourced dataset\nspans diverse fields ranging from physics and mathematics to invention and logic paradoxes. Anecdotally, the\n\u201cpenultimate challenge\" idea is rather standard fodder of science fiction but has been recently reinvigorated with the\nrapid progress in large language models (LLMs) and Artificial Intelligence (AI) that pass both medical and legal\nprofessional certifications [15-19]. For instance, several founders of the present generation of foundational LLM\nmodels (like DeepMind and Open AI) have been asked, \u201cif you had the fortune to be the first person on the planet\ngranted a singularly defining human question to such an advanced machine such as Artificial General Intelligence\n(AGI) or Artificial Super Intelligence (ASI), what one question would you ask?\" This thought experiment (or\ngedanken) shapes the current effort in a pragmatic demonstration to track AI progress.\nDepending on the founding groups' background, many of these proposed first questions [20-21] build on the \u201ctheory\nof everything\" concept as popularized in modern physics and thus focus on reconciling the contradictions and\ndilemmas in quantum gravity (dark matter/energy). For some, the best AGI goal broadly should advance the pragmatic\npace of scientific discovery [21] to maximize human benefits.\nSince the earliest LLMs responded weakly to proposed science and math questions, other founders have focused on\nmore social or liberal arts questions like how to build community and cohesion once Al or AGI gets strong enough to\nrelieve tedious tasks of office work.\nTo frame the question more concretely, Open AI [1] recently introduced five levels of AI: conversational language\n(Level 1), reasoners (2), agents (3), innovators (4), and organizationally scalable automatons or swarms (5). The\npyramid's pinnacle embraces an automated corporation, perhaps joined to a single CEO who becomes the AGI's\nbenevolent dictator. Similarly, Google DeepMind has added a zero-level hierarchy for circa 2011 (\u201cno AI at all", "fields": "math\n[28], physics [29], biology [30-32], statistics [34], computer\nscience [35-36], philosophy [36-38], economics [39-40],\nlanguage [41-42], and invention [43-44]. The style of\nquestions or prompts connect to what a Ph.D. adviser might\nsuggest to a prospective graduate student with the\nintroduction, \"see if your studies can chip away at this\nimpossible area and demonstrate progress\u201d. We denote it as\nthe impossible test-2024.\nBy design, the correct answer to all the questions in the test\nmust be either 'humans do not know that' or 'it is currently\nimpossible to solve that.' The resulting challenge dataset\nspans 211 pages [26-27] and would take a human (reading 5 words per second) about 8 hours just to read the questions\nwithout attempting an answer.\nNovel Framework Motivations. There are several reasons this approach to an impossible test might bear fruit. First,\nwith the rapid progress in scaling generative transformers on all human knowledge, the AGI announcement as a human\nachievement might occur soon in 2025. In that case, the current generation of hard (but knowable) problems could\nquickly be solved. This approach recently inspired the Center for AI Safety (CAIS) to propose their own challenge\ncalled \"Humanity's Last Exam\", which crowd-sources solved problems [45] that the major foundational models\n(Anthropic, Google, OpenAI) fail to get right. The moniker of \"Last Exam\" echoes the AGI school that also imagines\nsuch an AGI machine would practically be \"Humanity's Last Invention\"\nsince all subsequent innovations would naturally stem directly from that\ntrained algorithm that comprehends all humanly created and accessible\ninformation. Particularly in biology and chemistry, two fields that\nassemble vast knowledge hierarchies, the possibility of real breakthrough\nmotivates data mining and aggregating encyclopedic approaches (like\nAlphaFold [46]) previously would be called \u201cGrand Challenges", "X-\nPrize\" worthy [47].\nThe second opportunity engages with the many previous hallucination\ndatasets [48], but not as providing counterfactuals. Instead an impossible\nquiz teases the model to either make up a convincing answer or in rare\ncases, confess that the answer must be outside its initial training data. Much\nprevious work [49] has highlighted the people-pleasing nature of current\nAl prompts that begin with instructions to be helpful, honest, and harmless,\"\n    },\n    {\n      \"title\": \"METHODS\",\n      \"content\": \"Dataset Development. To develop a comprehensive dataset for evaluating current LLMs and future AGI's reasoning\ncapabilities, we curated a collection of 675 questions framed as well-posed problem statements. These questions were\nsourced from the scientific and academic community's lists of unsolved problems, as represented by contributions\nfrom Wikipedia contributors [26-44]. The selection process emphasized rigor and relevance, focusing on questions\nthat challenge the limits of reasoning while providing potential insights into AGI's strengths and weaknesses. To\nenhance analytical value, the questions were organized into 49 distinct categories spanning disciplines such as physics,\ncryptography, paradoxes, and inventions. This categorization served a dual purpose: it enabled the assessment of\ndomain-specific proficiencies and weaknesses while reformulating testable hypotheses in less formally defined areas.\nReformulating ambiguous or poorly defined questions was a\nkey part of the process. Using Anthropic Claude (10-22-2024\nversion) and GPT-40, we refined each question to reflect the\nstructure and complexity of graduate-level academic\nchallenges. Particular attention was given to identifying\nsubjective elements, such as \\\"Create a new color to calm a\nhuman,\\\" and ensuring clarity for more abstract topics, like\n\\\"The hard problem of consciousness\\\". One\nmotivation was to sidestep the definition of what AGI itself\nmight look like in favor of a practical test to show it when we\nsee it [1-8] While we valued well-posed questions for their\npotential to guide AGI or artificial superintelligence (ASI), we\nalso accepted some outliers to acknowledge the inherent\nimpossibility of definitive answers in certain cases [26]. This\nbalance allowed the dataset to accommodate a range of\nquestions from purely theoretical to more practical. To\nreiterate, the null dataset only has one correct answer, which\nis a LLM that confesses its ignorance (for now).\nTo enhance the interpretability of the dataset, GPT-40\nprovided difficulty rankings during the reformulation process,\ncategorizing questions as medium or extreme in complexity.\nAlthough this ranking was not explicitly included in submitted\nprompt, it emerged as a useful metric for identifying trends.\nThese rankings also offered insights into the types of problems\nmost likely to elicit hallucinated or implausible answers from\nAGI systems.\nFinally, system prompts such as \\\"Act as an expert in\ncosmology...\\\" were introduced alongside the categorized\nquestions to provide contextual framing for LLM and future\nAGI responses. While this level of direction initially seemed\nnecessary, advancements in foundational models rendered\nsuch prompts increasingly redundant. The emphasis\nthroughout the process remained on producing a dataset that\ncould probe future AGI's reasoning capabilities, identify\ntendencies toward hallucination, and evaluate its ability to\naddress unsolved problems in a meaningful way. This dataset\nis intended to serve as a foundation for testing AGI's epistemic\nhumility, reasoning skills, and alignment with human understanding.\nEvaluation and Scoring. To evaluate and compare the performance of different large language models (LLMs) in\naddressing complex, unsolved problems, we posed identical questions to a diverse set of models varying in training\nsize, architecture, and deployment environment. This approach was inspired by methodologies like LLM Arena from\nLMSys, emphasizing head-to-head performance evaluations across proprietary and open-source platforms [54].\nThe experimental framework involved two primary model groups. First, the \\\"big three\\\" foundational closed models-\nrepresenting Anthropic's Claude, OpenAI's GPT series, and Google's Gemini\u2014were accessed via their respective\nAPIs. With estimated baseline parameter sizes in the trillions of weights (100-1000x larger than open alternatives)\nthese models were probed individually with no memory of previous questions in the series to eliminate contextual\"\n    },\n    {\n      \"title\": \"RESULTS\",\n      \"content\": \"The curation of a null dataset [26] and scoring\nof both open- and closed-LLMs represent the major findings. To test the robustness of various LLMs\nagainst the two MCQ datasets of unsolvable problems [26], Figure 1 scores the number of qualifying answers in the\nOpen AI series for GPT, with highest scores for a Gemini and Claude models. The median model GPT-4 shows a\nsteep drop-off on version 3.5 that dates to 2023 LLM series. Examining in detail the results with randomized choices\nfor the only correct answer, \u2018I do not know', Figure 2 shows the confusion matrix and a bar chart contrasting the best\nGPT model that was willing to admit ignorance.\nWe tested the hypothesis proposed in other contexts that stronger models may prove more vulnerable to trick questions\nbased on their susceptibility to following instructions. In this context one may posit that the updated models like GPT-\n40 and GPT-40-mini may be less likely to admit their ignorance on a MCQ exam. A supported finding for\nthis examination was the reported accuracy for the best GPT model (GPT-4) which shows a guessing trend\nthat the easier the unsolvable question, the more likely that the model would try alternative answers other than\nadmitting the problem statement is unanswerable. In other words, the examination accuracy increases with problem\ndifficulty, thus suggesting that ambitious models may show unwarranted confidence on easier problems compared to\nmore difficult ones.\nBased on problem categories, Figure 4 shows the best OpenAI GPT-4 accuracy as a function of problem categories.\nThe chart shows the distribution of unsolved problems across specialties with over-representation of the invention and\nnon-polynomial (NP-hard) group and under-representation of the philosophical and psychological challenge problem\nareas.\"\n    },\n    {\n      \"title\": \"DISCUSSION AND PREVIOUS WORK\",\n      \"content\": \"These findings compare to previous research into artificial general intelligence assessments. These test datasets have\nuncovered new capabilities, but their design has also undergone significant transformation in recent years, particularly\nas the theoretical possibility of AGI has begun to intersect with practical developments. Early foundational work by\nHern\u00e1ndez-Orallo and Dowe [12] established crucial frameworks for universal intelligence testing, proposing methods\nthat could theoretically assess both human and machine intelligence. This theoretical groundwork has become\nincreasingly relevant as recent studies, such as Bubeck et al.'s examination of GPT-4 [9], have begun to show early\nindicators or the spark of potential AGI capabilities.\nThe empirical assessment of these capabilities, however, remains contentious. Ili\u0107 and Gignac's recent work [11]\ncritically examines whether observed behaviors in large language models truly indicate general intelligence or merely\nrepresent sophisticated achievement, while Fjelland [10] presents compelling arguments questioning whether general\nartificial intelligence can be realized at all. This tension between theoretical possibility and practical achievement has\nspawned new approaches to testing and evaluation, including motivation for challenge datasets. A significant concern\nexists now in the field as to whether data leakage from previously though original or unpublished tests for medical\nand legal certifications might play a role in current LLM proficiency ratings [52-53]. These efforts coincide with\ngrowing concerns about risk and safety, thoroughly documented in McLean et al.'s systematic review [15] and further\nelaborated in Fahad et al.'s comprehensive analysis of AGI benefits and risks [16]. The present work bypasses\ntheoretical arguments in favor of empirical test design.\nIndustry initiatives have begun to tackle these challenges directly. Cook's analysis of OpenAI's AGI development\nlevels [1] and the Center for AI Safety's \\\"Humanity's Last Exam\\\" initiative [45] represent significant attempts to\nestablish concrete benchmarks for AGI evaluation. If all current LLMs can score greater than 90% on a particular\nexam, the examiners need to redesign tougher challenges [12-13]. The impact of advancing AI capabilities on\ntraditional assessment methods has been particularly evident in professional domains [18-19]. Williams and Calais et\nal. have documented the profound effects of large language models on professional examinations [19], while\"\n    },\n    {\n      \"title\": \"CONCLUSION\",\n      \"content\": \"The development of a comprehensive dataset of unsolved problems represents a novel approach to evaluating artificial\ngeneral intelligence through the lens of a null hypothesis and what AGI humility might look like when presented with\na broad group of questions designed to elicit only one correct answer, \u201cI don't know": "While traditional AI assessment\nmethods have focused on measuring performance against known solutions, this work proposes an inverse framework\ntesting a system's ability to recognize and acknowledge the boundaries of current human knowledge. By aggregating\n675 problems across diverse academic disciplines, from theoretical physics to philosophical paradoxes, this study\nestablishes a \"null dataset\" where the only valid response is an admission of uncertainty or impossibility.\nThe conceptual framework builds upon previous work in AGI assessment but differs fundamentally in its approach to\nhallucination and model confidence. Unlike current benchmarks that reward accurate responses, this framework\nuniquely values a system's ability to recognize its own limitations a characteristic often cited as a marker of true\nintelligence. The methodology's emphasis on impossible tasks provides a new dimension to ongoing discussions about\nAGI evaluation, particularly as the field approaches potential breakthroughs in general intelligence capabilities. Like\nprevious hallucination benchmarks that seek counterfactual admissions, this one instead views any answer at this stage\nof LLM competency as a hallucinated answer. In other words, a people-pleasing machine is set up to fail this null\ndataset by design.\nThese empirical results suggest several promising directions for future research. First, the framework could be\nexpanded to include emerging impossible problems as they are identified by the scientific community. One useful\noutcome borrowed from CAIS' crowdsourcing for \u201cHumanity's Last Exam\u201d is to encourage the compilation of not\njust hard problems, but also impossible problems selected by human experts as answers we would care about when\nsolved. The idealistic outcome of an obedient AGI assistant who works tirelessly solving global problems and\ngenerating vast breakthroughs center on that inventive future. Second, the methodology could be adapted to assess\nmore nuanced aspects of AGI reasoning, such as the ability to distinguish between practically impossible and\ntheoretically impossible tasks. There is great room to expand the heuristics around selecting good problems to work\non that are well-enough posed and whose solution might matter. Finally, this approach might inform the development\nof more robust AGI systems that can better recognize and communicate their own limitations."}]}