{"title": "Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI", "authors": ["Ruwan Wickramarachchi", "Cory Henson", "Amit Sheth"], "abstract": "In the era of Generative AI, Neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition. The use of Neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability. However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets tailored to Neurosymbolic AI tasks. To address this gap and support the evaluation of current and future methods, we introduce DSceneKG a suite of knowledge graphs of driving scenes built from real-world, high-quality scenes from multiple open autonomous driving datasets. In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks. DSceneKG is publicly accessible at: https://github.com/ruwantw/DSceneKG", "sections": [{"title": "1. Introduction", "content": "Integrating intelligent behavior into AI systems requires both perception, processing raw sensor data, and cognition, using background knowledge for tasks like reasoning, planning, and decision-making [1]. Knowledge graphs play a crucial role in explicitly representing this background knowledge and enabling AI systems to perform cognitive tasks more effectively. Neural networks, while proficient in pattern recognition, often lack these explicit representations, limiting their ability to perform reliable reasoning.\nNeurosymbolic AI aims to overcome this limitation by combining symbolic knowledge representations (e.g., knowledge graphs, ontologies, logical rules) with sub-symbolic AI techniques, such as machine learning and deep learning. Recently, this approach has shown promise in improving reliability, explainability, and performance in handling tasks that demand higher levels of perceptual and cognitive abilities[2, 3]. However, evaluating such neurosymbolic Al capabilities is often constrained by the use of benchmark datasets that do not reflect the complexities of real-world scenarios, thereby limiting their practical relevance.\nA good example of this challenge is provided by knowledge graph completion (KGC), a key problem in knowledge representation and reasoning. Various link prediction (LP) methods have been developed to handle the inherent incompleteness of knowledge graphs by predicting new links in the graph in order to fill in the gaps. These methods are primarily evaluated on standard benchmark datasets like Freebase[4] and WordNet[5]. While such benchmark"}, {"title": "2. DSceneKG: Driving Scenes Knowledge Graph", "content": "To address these challenges, we introduce DSceneKG, a suite of KGs developed to represent real-world driving data from multiple autonomous driving datasets. DSceneKG captures a wide range of driving scenarios, including urban and rural environments, different weather conditions, and various traffic situations. The data is sourced from several benchmark datasets for AD containing heterogeneous data from LiDAR, cameras, and GPS sensors, providing a rich and multi-modal dataset for research and development in autonomous driving."}, {"title": "2.1. The Development of DSceneKG", "content": "The DSceneKG primarily contains two main components: (1) The Driving Scenes Ontology (DSO) to represent the formal structure and semantics of scenes, and (2) the generation of a KG based on an existing AD dataset to instantiate the real-world objects and events. Next, we will succinctly describe the development of these two components."}, {"title": "2.1.1. Driving Scenes Ontology", "content": "The Driving Scene Ontology (DSO)[6] provides a formal semantic structure, developed in the Web Ontology Language (OWL) (https://www.w3.org/OWL/), to represent driving scene information. DSO is designed to be dataset-agnostic and can describe scenes from any autonomous driving dataset. It distinguishes between two types of scenes: sequence scene, representing a sequence over time and space, and frame scene, representing a specific moment in time and space. Temporal information is encoded using specific time instant properties, while spatial information is captured through location names, geographic coordinates, and addresses. The ontology also categorizes entities into objects and events, links them with scenes in which they are observed, and defines relationships for the interactions between objects and events. DSO aims to standardize scene representation, enhancing the understanding and analysis of driving scenarios across different datasets."}, {"title": "2.1.2. Instantiating a Driving Scene KG from a Public Dataset", "content": "A KG of driving scenes can be constructed by converting scene data from autonomous driving datasets into Resource Description Framework (RDF)\u00b9 format, conforming to the Driving Scene Ontology. First, the relevant scene data are extracted using a Software Development Kit (SDK) native to the dataset (e.g., NuScenes-Devkit\u00b2 and Pandaset-Devkit\u00b3). The data are then"}, {"title": "3. Applications of DSceneKG: Emerging Neurosymbolic Al Capabilities", "content": "DSceneKG has significant potential for both industrial and academic applications. In this section, we showcase seven Neurosymbolic solutions that use DSceneKG as the benchmark dataset for evaluation (see Figure 2)."}, {"title": "3.1. Machine Perception", "content": "Machine perception enables autonomous systems to operate effectively in dynamic environments. As these systems become increasingly integrated into daily life, across domains like"}, {"title": "3.1.1. Knowledge-based Entity Prediction", "content": "Knowledge-based entity prediction (KEP) involves predicting the presence of potentially unrecognized or unobserved entities in a scene using current and background knowledge represented in a knowledge graph. The goal is to leverage an expressive KG to provide high-level semantic cues for identifying entities that are not explicitly recognized by traditional perception systems. For example, if an autonomous vehicle detects a ball on the road in a residential area, KEP would help predict the likely presence of a child nearby, considering knowledge about the context and relationships between objects, like children playing with balls. To address this issue, [6] leverages the holistic and expressive scene representation in DSceneKG to build a link prediction-based solution for KEP. They demonstrate the effectiveness of this approach by showing that the missing entities may be predicted with high precision (0.87 Hits@1) while significantly outperforming the non-semantic and rule-based baselines."}, {"title": "3.1.2. Explainable Scene Clustering/ Typing", "content": "KGs can help to explore sets of interrelated entities and discover meaningful patterns by clustering entities into informative subsets. For example, in the context of autonomous driving,"}, {"title": "3.1.3. Computing Semantic Similarity", "content": "Computing scene similarity involves determining how alike two scenes are based on certain features or characteristics. In autonomous driving, addressing this problem by only considering visual characteristics is problematic as driving scenes can be visually dissimilar but semantically similar. For example, consider the scenes recorded from two physical locations where a vehicle turns left from a roundabout. The visual information can be quite dissimilar; however, the high-level action is the same. [8] proposes a solution based on DSceneKG where they first transform the DSceneKG into embedding vectors and compute the cosine similarity between the vectors of scene pairs to identify those with the highest similarity scores. Notably, this method could detect similarities even when scenes were not visually alike, focusing instead on shared, high-level semantic characteristics."}, {"title": "3.2. Knowledge Completion and Augmentation", "content": "Knowledge graph completion refers to the task of completing a graph with missing information, i.e. filling in the gaps. Different types of knowledge may need to be completed, such as missing relations, entities, and high-level entity-type information of instances that are currently typed to only their granular types. DSceneKG will enable real-world evaluations of the current and future knowledge completion methods for the above-mentioned tasks. Additionally, DSceneKG facilitates the evaluation of methods designed to complete knowledge specific to driving scenes. For example, [9] proposes a context-based approach for labeling unobserved entities in DSceneKG. The scene nodes in DSceneKG can then be augmented with these newly obtained labels for entities that may have gone unobserved or unlabeled in the original dataset."}, {"title": "3.3. Semantic Search", "content": "For tasks requiring semantic search over multimodal data, DSceneKG can be utilized in two primary ways. First, DSceneKG can be queried directly using SPARQL, as all visual scene elements and metadata are structured within the graph, enabling efficient search and inference tasks. Second, vector search can be performed over the learned embeddings by leveraging vector databases, which efficiently store high-dimensional vectors. This allows for fast and"}, {"title": "3.4. Causality", "content": "Causality is often studied using frameworks like Causal Bayesian Networks (CBNs), which represent variables and their causal relationships as directed acyclic graphs (DAGs). CBNs allow for reasoning about cause and effect by leveraging probabilistic relationships between variables, enabling estimations about how changes in one variable can influence others. Recently, there has been an interest in improving the representation of causality with knowledge graphs where domain knowledge graphs that represent observation data are enriched with information from causal Bayesian networks (CBN) to enhance causal inference and explainability. For example, [11] showcases how DSceneKG can be enriched with causal information to create a causal knowledge graph that enables counterfactual and intervention reasoning to understand the behaviors of scene entities."}, {"title": "3.5. Cross-modal Retrieval of Complex Data", "content": "Cross-modal retrieval aims to retrieve relevant information in one modality based on a query in another. For example, in autonomous driving, cross-modal retrieval aims at retrieving bird's-eye-view (BEV) scene representations (e.g., image/video) from textual input or instructions. In such cases, using a global semantic structure is essential to provide semantic relationships between entities like objects, features, and movements. Building upon this idea, [12] proposes a novel BEV retrieval method that uses the DSceneKG as a source of associative embeddings, enriching the representation of input text by embedding related autonomous driving keywords. These keyword embeddings are integrated with language models, enabling better alignment between the text input and the bird's eye view (BEV) features extracted from visual data. This fusion of knowledge graph embeddings and text descriptions improves the retrieval of BEV representations by providing a more structured and semantically relevant context."}, {"title": "4. Conclusions", "content": "We introduce a suite of driving scenes knowledge graphs, DSceneKG, designed to benchmark the emerging capabilities of Neurosymbolic AI. Built from real-world, open-domain datasets, DSceneKG integrates multimodal data from diverse driving scenes across various continents and environmental conditions. We outline the process of constructing DSceneKG and demonstrate its application across seven different tasks."}]}