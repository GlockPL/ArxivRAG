{"title": "End-to-end Driving in High-Interaction Traffic Scenarios with Reinforcement Learning", "authors": ["Yueyuan Li", "Mingyang Jiang", "Songan Zhang", "Wei Yuan", "Chunxiang Wang", "Ming Yang"], "abstract": "Dynamic and interactive traffic scenarios pose significant challenges for autonomous driving systems. Reinforcement learning (RL) offers a promising approach by enabling the exploration of driving policies beyond the constraints of pre-collected datasets and predefined conditions, particularly in complex environments. However, a critical challenge lies in effectively extracting spatial and temporal features from sequences of high-dimensional, multi-modal observations while minimizing the accumulation of errors over time. Additionally, efficiently guiding large-scale RL models to converge on optimal driving policies without frequent failures during the training process remains tricky. We propose an end-to-end model-based RL algorithm named Ramble to address these issues. Ramble processes multi-view RGB images and LiDAR point clouds into low-dimensional latent features to capture the context of traffic scenarios at each time step. A transformer-based architecture is then employed to model temporal dependencies and predict future states. By learning a dynamics model of the environment, Ramble can foresee upcoming traffic events and make more informed, strategic decisions. Our implementation demonstrates that prior experience in feature extraction and decision-making plays a pivotal role in accelerating the convergence of RL models toward optimal driving policies. Ramble achieves state-of-the-art performance regarding route completion rate and driving score on the CARLA Leaderboard 2.0, showcasing its effectiveness in managing complex and dynamic traffic situations.", "sections": [{"title": "I. INTRODUCTION", "content": "In real-world traffic scenarios, drivers must continuously interact with other vehicles and road users, negotiating situations such as merging, turning, overtaking, and reacting swiftly to traffic signals and lane markings. The ability to understand and respond to these dynamic and interactive situations is essential for autonomous driving. However, traditional rule-based policymakers struggle to generalize across the various traffic events. Moreover, simply relying on limited human driving data through Imitation Learning (IL) proves inadequate in addressing the diverse and unpredictable nature of driving. As evidence, CARLA presents 38 highly dynamic and interactive scenarios in its Challenge 2023. Despite advances in the field, no solution has been able to fully master these tasks without privileged information, with the best-performing implementations achieving only 18% route completion rate, highlighting the need for more robust driving solutions [1].\nReinforcement Learning (RL) has shown significant potential in addressing complex traffic scenarios. Unlike IL, which is confined to replicating human behavior based on predefined datasets, RL may explore novel and efficient driving policies through continuous environmental interaction. With the guidance of reward signals, a well-designed RL framework can optimize its behavior towards different driving purposes, such as safety, efficiency, or passenger comfort. The RL algorithm has outperformed human champions in the F1 racing competition [2]. Additionally, in the CARLA simulator, RL-based models have achieved the highest route completion rates and driving scores in Leaderboard 1.0 and 2.0 as long as privileged information is available [3, 4]. These successes suggest the promise of RL as a robust approach to tackling the challenges inherent in dynamic traffic scenarios.\nOne of the primary challenges in applying RL to realistic driving scenarios is effectively representing the high-dimensional observation space. Most well-known RL algorithms, such as DQN, SAC, PPO, and the Dreamer family, are designed to handle low-dimensional state spaces and only have their performance tested in naive environments like Atari and Mujoco [5, 6]. However, realistic driving scenarios involve complex sensory inputs, such as high-resolution camera images, LiDAR point clouds, and radar signals, that require more sophisticated feature extraction and representation methods [7]. Bridging this gap between high-dimensional sensory input and RL algorithms remains a critical obstacle in translating its successes from toy cases to real-world applications.\nAnother obstacle lies in effectively leveraging the temporal features inherent in traffic scenarios. Most RL methods rely on the Markov assumption, which presumes that the current state contains all necessary information for decision-making. However, in autonomous driving, understanding the intentions and behaviors of other traffic participants requires capturing temporal dependencies that extend beyond the current state. Integrating this temporal information is crucial for making informed decisions in dynamic environments [8].\nThis paper proposes an end-to-end model-based RL algorithm called Ramble to address these challenges. Ramble is designed to process multi-modal, high-dimensional sensory inputs while effectively managing the temporal complexities of interactive traffic environments. By learning a dynamics model of the environment, Ramble can predict future outcomes"}, {"title": "II. RELATED WORKS", "content": "End-to-end driving models map raw sensor inputs directly to driving actions. This framework aims to avoid explicit feature extraction, which may result in the omission of crucial latent features and the accumulation of errors. Most of the end-to-end driving models fall into two main classes: IL and RL [9].\nIn recent years, IL-based driving models have become the mainstream in the field. They primarily focus on leveraging features from high-dimensional inputs and imitate expert behavior during policy learning for rapid convergence. This approach has been popular since the introduction of the first end-to-end driving model, DAVE-2 [10]. Many IL-based driving models have since devoted to improving perception capabilities. Notable examples include Transfuser and InterFuser, which utilize transformer-based architectures to integrate features from multi-view images and LiDAR point clouds [11, 12]. Models such as ReasonNet and CarLLaVA further refine performance by generating intermediate features with semantic meaning, improving context awareness [1, 13]. Besides, some research attempts to ensemble outputs. For instance, TCP combines action commands with waypoint predictions, which is an effective improvement adopted by many subsequent works [14]. Similarly, Transfuser++ enhances performance by decoupling waypoint prediction from velocity estimation [15]. However, the main drawback of IL-based driving models is their reliance on high-quality trajectory data or command records from experts [16-18], so their performance naturally degrade in corner cases or unseen scenarios. Furthermore, due to their limited exploration ability, these models struggle in traffic scenarios with dense interaction, a limitation clearly highlighted in CARLA Leaderboard 2.0.\nWhile IL-based methods have thrived, RL-based driving models have encountered more challenges. Despite early efforts in specific scenarios [19-21], researchers have found that training RL agents is costly due to the need for real-time feedback from the environment. Additionally, model-free RL-based driving models face significant challenges in processing high-dimensional inputs and achieving stable convergence to a reasonable policy [22, 23]. While models like Roach and Think2Drive have demonstrated the potential of RL in driving decision-making, their reliance on privileged information makes them impractical for real-world applications [3, 4]. To enable models to learn scenario context from complex raw sensor data, Peng et al. proposed using expert driving behavior to guide the learning process, which has been shown to converge to safer policies [24]. RL-based driving models are expected to achieve better generalization performance than While RL-based models are expected to generalize better due to their ability to explore the environment, they still lag behind IL-based models in overall performance."}, {"title": "B. Model-based Reinforcement Learning", "content": "RL is a potential learning paradigm for exploring behavioral strategies in complex environments. Model-based RL distinguishes itself by learning a dynamics model of the environment, which enables it to plan actions with foresight into future outcomes [25]. This approach tends to be more sample-efficient and capable of handling more complex tasks compared to model-free RL.\nModel-based RL has achieved impressive performance on well-known benchmarks like Atari. Many successful model-based algorithms are built upon the foundational ideas of the world model framework [26], which emphasizes learning compact representations of the environment to predict future states and guide planning. SimPLe employs a basic neural network to model environment dynamics and incorporates a stochastic model to handle the environment uncertainty, resulting in higher sample efficiency than model-free methods [27]. In PlaNet, Hafner et al. introduced the recurrent state-space model to better manage environments with complex variations, providing higher cross-domain robustness [28]. This innovation set the stage for the widely adopted Dreamer family of algorithms [6]. Beyond advancements in underlying mechanisms, researchers are continuously upgrading the network architectures within model-based RL frameworks. For instance, IRIS demonstrates the effectiveness of transformer-based models in learning environment dynamics [29], while STORM enhances performance further by integrating a stochastic model to better address environmental uncertainty [30].\nDespite the success of model-based RL in toy environments, its application in real-world scenarios remains challenges. Very few attempts have been made to apply model-based RL to autonomous driving [4]. The main obstacle is the difficulty of learning an accurate dynamics model from high-dimensional sensor inputs. The complexity of the traffic scenarios makes it challenging to predict future states. In this paper, we will show that Ramble is a promising model-based RL algorithm to tackle the issues metioned above and is applicable to autonomous driving."}, {"title": "III. METHOD", "content": "The overall structure of Ramble is illustrated in Figure 1. Our approach builds upon the framework established in the original world model [26]. Following this paradigm, Ramble first processes high-dimensional input observations, encoding them into abstract, compressed latent features. These latent features are then fed into a sequence model, which captures the temporal information across a series of observations. The latent features obtained directly from the observations and those containing temporal information are then integrated as inputs to the agent, which makes driving decisions."}, {"title": "B. Latent Feature Encoder", "content": "The feasibility of model-based RL agents has been proven in Atari games, where observations consist of relatively simple 210 x 160 pixel images [31]. However, in the context of autonomous driving, the observation space is typically much higher in dimension. Lower-dimensional input would not provide sufficient information for the agent to make decisions. Consequently, it is necessary to design a more powerful encoder capable of compressing high-dimensional input observations into a lower-dimensional latent space.\nOur input observations consist of multi-view RGB images and LiDAR point clouds. Detailed sensor configurations are provided in Appendix B. The RGB images are captured at a resolution of 640 \u00d7 480 from the front, left, right, and rear views. The LiDAR data comprises approximately 31,000 points from a 360\u00b0 scan. These inputs are recorded at 10 Hz.\nTo effectively manage the complex input data, we designed a latent feature encoder inspired by BEVFusion [32]. The structure of this encoder is shown in Figure 2. Raw camera data is initially processed by a Swin Transformer [33] to extract multi-scale features, which are then merged into a feature map using an FPN [34]. Following the core concept of BEVFusion, the camera features are projected into BEV and mapped onto 3D grids. Simultaneously, LiDAR point clouds are voxelized and mapped to 3D grids using a PointPillars network [35]. Since the point cloud features are natively in BEV format, no additional view transformation is needed. The image and point cloud features are then concatenated and fused through a convolutional neural network. Finally, the route points information is processed by a multi-layer perceptron and fused with the sensory data by cross-attention mechanism and sent to the Variational Autoencoder (VAE).\nThe feature extraction network and the VAE's encoder together are referred to as the observation encoder $q_{\\phi}(w)$"}, {"title": "C. Sequence Model", "content": "To extract temporal information from the sequence of latent features, it is necessary to apply capable of capturing interconnections. The initial version of the world model uses a Long Short-Term Memory (LSTM) network [26]. However, the Transformer architecture has been proven to be more effective at capturing long-range dependencies in sequential data [36]. Therefore, we follow the design of STORM to implement the sequence model with a transformer [30]. As is demonstrated in the following equations, the latent feature $z_t$ sampled from $Z_t$ is combined with the action $a_t$, using multi-layer perceptron (MLP) and concatenation. This manipulation is denoted as $m_{\\rho}$, and the mixed feature is $e_t$. We then adopt a GPT-like transformer $f_{\\theta}$ with a stochastic attention mechanism to process the mixed feature. The output of the transformer is the hidden state $h_t$. Finally, the hidden state is fed into three different MLPs: $g_{\\phi}^{z'}$ predicts the next environment dynamics distribution, $g_{\\phi}^{r}$ predicts the reward, and $g_{\\phi}^{\\hat{c}}$ predicts the continuation probability. $\\phi$ represents the parameters of the sequence model. The expressions of these components are as follows:\nAction mixer: $e_t = m_{\\rho}(z_t, a_t)$\nSequence model: $h_{1:T} = f_{\\theta}(e_{1:T})$\nDynamics predictor: $Z_{t+1} = g_{\\phi}^{z'}(z_{t+1}|h_t)$\nReward predictor: $r_t = g_{\\phi}^{r}(h_t)$\nContinuation predictor: $\\hat{c_t} = g_{\\phi}^{\\hat{c}}(h_t)$\nThe sequence model is trained jointly with the latent feature encoder, and its loss function comprises multiple components, each targeting a different optimization goal. The dynamics loss, $L_{dyn}$, and the representation loss, $L_{rep}$, guide the prediction of the next distribution and maintain similarity between the sequence model's output and the encoder's latent features. The reward prediction error is captured by $L_{rew}$ using symlog two-hot loss, while the continuation prediction error"}, {"title": "IV. EXPERIMENT", "content": "We evaluated our methods using CARLA's official benchmarks: Leaderboard 1.0 and Leaderboard 2.0. Leaderboard 1.0 provides routes without any special events in relatively small maps, while Leaderboard 2.0 adds 39 types of challenging interactive traffic scenarios to the routes in large maps. These benchmarks facilitate a relatively fair comparison of different methods by offering an online platform where users can test their methods on unseen route sets. We selected Leaderboard 1.0 because it is the most widely acknowledged benchmark for comparing end-to-end driving methods [9]. Meanwhile,"}, {"title": "A. Benchmarks", "content": "We evaluated our methods using CARLA's official benchmarks: Leaderboard 1.0 and Leaderboard 2.0. Leaderboard 1.0 provides routes without any special events in relatively small maps, while Leaderboard 2.0 adds 39 types of challenging interactive traffic scenarios to the routes in large maps. These benchmarks facilitate a relatively fair comparison of different methods by offering an online platform where users can test their methods on unseen route sets. We selected Leaderboard 1.0 because it is the most widely acknowledged benchmark for comparing end-to-end driving methods [9]. Meanwhile,"}, {"title": "Evaluation Metrics", "content": "The CARLA Leaderboards employ three key metrics to assess model performance: route completion, infraction penalty, and driving score. Route completion measures the average ratio of successfully completed routes. Infraction penalty reflects adherence to traffic regulations, diminishing by a percentage when the agent commits infractions or violates traffic rules. The driving score is derived from the product of the route completion ratio and infraction penalty. It offers a comprehensive evaluation of the agent's efficiency and safety. The detailed infraction items are defined and automatically obtained by the CARLA leaderboards, including\ncollision with pedestrians (Collisions pedestrians)\ncollision with other vehicles (Collisions vehicles)\ncollision with static elements (Collisions layout)\nrunning a Red light\nrunning a Stop sign\ndriving Off-road\nagent taking no action for a long time (Agent blocked)\nfailure to Yield to emergency vehicle\nfailure to maintain Minimum speed\nfailure to pass a scenario in time (Scenario timeouts)\nThis metric is only available in Leaderboard 2.0 because \"scenario\" is defined before.\ndeviation from the routes for a distance (Route deviations)\nfailure to complete a route in time (Route timeout)"}, {"title": "C. Experts", "content": "Due to the difficulty of the routes in CARLA's leaderboards, most end-to-end driving models cannot guarantee the completion of all the routes without relying on privileged information, such as the ground-truth states of surrounding traffic participants, precise positions of static obstacles, and the specific details of traffic signs. To estimate the upper limit performance of learning-based end-to-end driving models, we adapted several representative experts.\n1) Roach: The RL-based expert developed by [3] serves as the coach that guides the training of the IL agent using only permissible data. This model leverages the states of traffic lights and semantic segmentation images as privileged information, effectively demonstrating RL's capability to formulate driving policies from scratch. We reproduced the performance of Roach's expert model on CARLA Leaderboard 1.0, and then adapted and trained it on CARLA Leaderboard 2.0. This approach allows us to illustrate the potential upper limits of model-free RL's performance in autonomous driving scenarios.\n2) Think2Drive: Think2Drive is an RL-based expert model incorporating a world model to enhance its understanding of the environment [4]. It stands out as the first learning-based model to successfully complete routes on CARLA Leaderboard 2.0. Still, it relies on some privileged information, including real-time bounding boxes of surrounding traffic participants and obstacles, HD maps, and the states of traffic lights. Due to the code's unavailability\u00b9 and the substantial computational resources required, we reference the scores reported in Think2Drive's publication to illustrate the potential performance of learning-based models.\n3) PDM-Lite: PDM-Lite is a rule-based expert model developed by [1] to gather data from CARLA Leaderboard 2.0. It is the only open-source model reported to achieve a 100% route completion rate on most of the routes in Leaderboard 2.0. This model extracts scenario types and traffic signals from the backend, generating a list of feasible waypoints. It employs bicycle models to predict trajectories for both the ego vehicle and nearby traffic participants. Using the gathered contexts, it calculates control commands through a longitudinal linear regression controller and a lateral PID controller. Due to its heavy reliance on identifying scenario types, an attribute absent in Leaderboard 1.0, PDM-Lite is only referred to for performance comparison in Leaderboard 2.0."}, {"title": "D. Baselines", "content": "We have reproduced various SOTA driving models on CARLA Leaderboard 1.0 and transited them to Leaderboard 2.0 to assess their ability of generalization. Notably, all the end-to-end driving models achieving top positions on the leaderboards are based on IL. IL possesses an advantage over model-free RL in its superior capacity to handle high-dimensional environmental data. Additionally, the availability of ground truth trajectory data facilitates faster convergence in the policy network. The absence of standout RL-based state-of-the-art models is another reason why we introduce RL-based expert models for comparison experiments.\n1) LAV: LAV processes multi-modal sensory data, including RGB camera and LiDAR inputs, to directly output control commands [18]. The model is elegantly designed, emphasizing that the quality of the ground truth trajectory during training significantly influences performance. This model was selected because it ranks 1st regarding route completion rate on CARLA Leaderboard 1.0.\n2) Transfuser++: Transfuser++ is an enhanced version of Transfuser [11, 15], maintaining a similar structure while improving the transformer's efficiency and reducing ambiguity in the model's output by disentangling trajectory prediction from velocity. Among the various implementations of Transfuser++, we opted to reproduce the version with waypoint prediction (TF++ WP) and adapted it for CARLA Leaderboard 2.0.\n3) TCP: TCP is a camera-only IL model [14], which proposes combining trajectory and control prediction outputs to enhance the model's generalization ability. This innovative framework allows TCP to be seamlessly integrated as an output module for other models. We have reproduced TCP's performance on CARLA Leaderboard 1.0 and transited it to Leaderboard 2.0 to evaluate its generalization ability.\n4) ReasonNet: ReasonNet is the successor to InterFuser [12, 13]. Its main improvement is the introduction of a global reasoning module that enhances the model's environmental understanding. This model has achieved the highest driving"}, {"title": "V. RESULT AND DISCUSSION", "content": "In Tables I and II, we present a performance comparison between the baseline methods and our proposed approaches,"}, {"title": "A. Comparison of Performance", "content": "In Tables I and II, we present a performance comparison between the baseline methods and our proposed approaches,"}, {"title": "Efficiency", "content": null}, {"title": "Ablation Study", "content": null}, {"title": "D. Visualization", "content": null}, {"title": "VI. CONCLUSION", "content": null}, {"title": "A. Performance of Efficiency", "content": null}]}