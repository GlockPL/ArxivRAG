{"title": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference", "authors": ["Changwoo Lee", "Soo Min Kwon", "Qing Qu", "Hun-Seok Kim"], "abstract": "Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by 70% and 40%, respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x compression while exhibiting the lowest performance degradation among all tested structured matrices.", "sections": [{"title": "1 Introduction", "content": "Foundation models built on large deep neural networks (DNNs) have demonstrated remarkable performance in vision and language tasks. However, the size of these large networks poses both computational and storage challenges, especially in resource-constrained environments such as edge devices. The size of a single DNN often exceeds the capacity of the supporting hardware devices [1-5]. For example, Llama-70B [1] demands at least 140GB of memory solely for loading its weights in half-precision floating point representation, while the state-of-the-art commercial GPU only accommodates 80GB of memory. Furthermore, inference with these networks involves numerous dense matrix-vector operations, which can be limiting when computing power is constrained.\nFortunately, large (overparameterized) DNNs often exhibits parameter redundancy, where the intrinsic dimension of the weights is much lower than the ambient dimension. As such, the weights should be structured, possessing hidden properties such as low-rankness [6\u20139] or sparsity [10, 11]. Hence, it is possible to replace (or factorize) these dense existing weight matrices with structured ones without degrading performance [10-12]. However, using structured matrices that do not align with the true underlying structure of the weight matrices can result in significant performance degradation. We demonstrate this point in Figure 1 where we attempt to capture the structure of a diffusion model transformer (DiT) [13] using the low-rank structure to generate synthetic images. In Figure 1, we compress the model's linear layers by approximately 50% of the total number of parameters using low-rank weight matrices via singular value decomposition (SVD) and generate images with the compressed model (see Section 4.2 and Appendix C.3 for details). As shown in Figure 1 (middle), simply using the low-rank structure introduces unwanted artifacts in the generated images.\nTo address this issue, many flexible structures for modeling DNN weights have been proposed to minimize the misalignment between imposed and true low-dimensional structures. For example, Dao et al. [14] proposed the Monarch matrix, a specific type of Block Low-Rank (BLR) structure [15], in which all blocks share the same rank, intended for use in the linear layers of transformers [16]. Matrix multiplication with a Monarch matrix can be performed efficiently using batched matrix multiplication routines. Additionally, Chen et al. [17] investigated a block sparse plus low-rank structure. However, all of these methods still suffer from the fact that the underlying structure of each weight matrix is not known a priori. By imposing one of these structures, performance degradation may still occur due to misalignment. Recently, Lee and Kim [12] introduced a data-driven design called Generalized Block Low-Rank (GBLR). This approach employs multiple rank-1 blocks with various sizes and locations learned from data via differentiable masks. Unfortunately, the GBLR matrix is optimized for custom-designed hardware, as the learned block patterns are random. It has limited usability on general GPUs as the computation of GBLR matrices does not accelerate well on them.\nIn this work, we introduce the Block-Level Adaptive Structured (BLAST) matrix, a versatile and efficient design tailored to uncover various low-dimensional structures in the weight matrices of DNNs for accelerated inference on GPUs. Our matrix structure leverages shared bases across block matrices with block-wise diagonal coupling factors. This structure encapsulates different structures such as low-rank, block low-rank, block-diagonal matrices, and their combinations. BLAST matrices can be applied to the training scenario from scratch or compression after training. For training from scratch, we let the linear layers of the DNN to directly adopt the BLAST structure and learn its factors from data. The factors of the BLAST matrix are constructed to have well-defined gradients, allowing them to be optimized using popular methods like stochastic gradient descent (SGD) or Adam [18]. For compressing existing weights, we propose a factorization algorithm to learn the BLAST factors from pre-trained weights. The compression performance can be further improved by updating the BLAST factors using data, a process we call \u201cre-training\".\nWe demonstrate the efficiency of BLAST by training Vision Transformers (ViT) [19] and GPT-2 [20] from scratch on various datasets, showing that it can reduce complexity by 70% and 40%, respectively. We also compress existing ViT and Diffusion Transformer (DiT) [13] models with BLAST matrices by 70% and 50%, respectively, demonstrating that BLAST compression (and re-training) achieves higher accuracy / quality compared to existing methods for ViT and DiT (see Figure 1). For the language tasks, we compress Llama-7B [1] by 50% via BLAST and re-train on 0.49B tokens, showing the lowest accuracy degradation with significant inference speedup on a NVIDIA A100 GPU. Overall, our contributions can be summarized as follows:"}, {"title": "2 Block-Level Adaptive Structured (BLAST) Matrix", "content": "Consider a square matrix\u00b9 $A \\in \\mathbb{R}^{n \\times n}$ for some $n \\in \\mathbb{N}$, which has an unknown intrinsic low-dimensional structure. We first equally partition the matrix $A$ into $b \\times b$ blocks of size $p \\times p$ where $b, p \\in \\mathbb{N}$ are constants such that $n = bp$:\n$A =\\begin{bmatrix}\nA_{1,1} & A_{1,2} & \\cdots & A_{1,b} \\\\\nA_{2,1} & A_{2,2} & \\cdots & A_{2,b} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{b,1} & A_{b,2} & \\cdots & A_{b,b}\n\\end{bmatrix}, \\quad A_{i,j} \\in \\mathbb{R}^{p \\times p}, \\quad i, j \\in [b].$ (1)\nThen, the BLAST matrix parameterizes each block matrix $A_{i,j}$ using three factors:\n$A_{i,j} = U_i S_{i,j} V_j^T, $ (2)\nwhere $U_i, V_j \\in \\mathbb{R}^{p \\times r}$ are the left and the right factors, respectively, and $S_{i,j} = \\text{diag}(s_{i,j})$ is an $r \\times r$ diagonal matrix whose diagonal entries are $s_{i,j} \\in \\mathbb{R}$. We provide a visual representation on the rightmost side of Figure 2, and illustrate how this structure differs from other types of matrices. While the BLAST structure may appear similar to SVD, there are two notable differences: (i) the left and right factors do not need to be orthonormal, and (ii) the diagonal entries do not need to be positive. These distinctions make it more flexible in capturing different types of low-rank structures.\nAs illustrated in Figure 2, the BLAST matrix also comes with two unique properties:"}, {"title": "3 Applications of BLAST Matrices", "content": "There are two main applications of BLAST matrices: (i) training from scratch with the BLAST structure and (ii) compression of pre-trained weights using BLAST factorization."}, {"title": "3.1 Training from Scratch using BLAST Matrices", "content": "To train a DNN on a dataset, parameters are typically initialized randomly and updated through stochastic gradient descent. In this setting, BLAST can replace dense weights to learn structures from the training data. Instead of using random dense weight matrices, the model is initialized with random BLAST factors $U_i, V_j, s_{i,j}$. Since the forward and the backward path of the linear layer involving the weight matrix is composed of three linear operations as in Equation (3), the derivatives of the minibatch loss can be back-propagated by automatic differentiation frameworks [21]. Hence, all of the trainable parameters of BLAST can be updated using conventional optimizers (e.g., Adam [18] or AdamW [22]) without additional treatment."}, {"title": "3.2 Compressing Weights via BLAST Factorization", "content": "Given pre-trained dense weights of a DNN, we can compress the weights using BLAST matrices. Let $A$ denote the weight matrix and $A_{i,j}$ denote its blocks. We estimate the BLAST factors of $A_{i,j}$ by finding the factors of the BLAST matrix that minimize the Frobenius norm error between the original weight matrix and the BLAST structure:\n$\\ell(U_*, V_*, s_{*,*}) = \\sum_{i=1}^b\\sum_{j=1}^b\\frac{1}{2} ||A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T||_F^2,$ (4)\nwhere $*$ denotes the collection of all $b$ components along the axis. This problem shares many characteristics with the classical matrix factorization problem [23-26], and hence we can solve for the factors using alternating gradient descent starting from small random initialization (e.g., Line 1 of Algorithm 2) [27, 8]. That is, the $k^{th}$ gradient descent step is composed of three alternating updates with a step size $\\eta > 0$:\n$U_i^{(k+1)} \\leftarrow U_i^{(k)} - \\eta_{\\text{U}_i}^{(k)} \\cdot \\nabla_{\\text{U}_i} \\ell (U_i^{(k)}, V^{(k)}, s^{(k)}), $ (5)\n$V_j^{(k+1)} \\leftarrow V_j^{(k)} - \\eta_{\\text{V}_j}^{(k)} \\cdot \\nabla_{\\text{V}_j} \\ell (U^{(k+1)}, V^{(k)}, s^{(k)}),$ (6)\n$S_{i,j}^{(k+1)} \\leftarrow S_{i,j}^{(k)} - \\eta_{\\text{S}_{i,j}}^{(k)} \\cdot \\nabla_{\\text{s}_{i,j}} \\ell(U^{(k+1)}, V^{(k+1)},s^{(k)}),$ (7)\nWith properly chosen step sizes, Equations (5) to (7) always decrease the loss value whenever the current variables do not have any infinite entries and the gradient is non-zero. Using notations $\\overline{V}_i^{(k)} = [S_{i,1}^{(k)} V_1^{(k)T} \\dots S_{i,b}^{(k)} V_b^{(k)T} ]$ and $\\overline{U}_j^{(k)} = [(U_1^{(k+1)} S_{1,j}^{(k)T} \\dots (U_b^{(k+1)} S_{b,j}^{(k)T}]$ to indicate the concatenation of the right and left factors scaled by the diagonal components, the loss is monotonically non-increasing as in the following theorem."}, {"title": "4 Experimental Results", "content": "We evaluate the BLAST matrix under two settings: (i) training from scratch with random initialization in the BLAST format, and (ii) re-training after compressing the dense weights to BLAST matrices via Algorithm 2. We compare the performance of BLAST with both non-adaptive and adaptive structured matrices. Among the non-adaptive approaches, we include low-rank (LR) matrices, Monarch for block low-rank (BLR) [14], and Pixelfly [17] or Block-Diagonal for block sparse matrices. For the adaptive and learnable structured matrix category, we evaluate Gaudi-GBLR [12]. We report the number of floating point operations (FLOPs) by counting the number of multiplications. The BLAST matrix with $b \\times b$ number of blocks is denoted by $\\text{BLAST}_b$. We used the same hyperparameter $r$ for every target weight matrix by setting it to meet the computational budget of the DNN. All experimental details can be found in Appendix C."}, {"title": "4.1 Training from Scratch", "content": "We train the reduced-size Vi-sion Transformers (ViT) [19] with $\\text{BLAST}_3$ (BLAST with $b = 3$) weight matrices on CIFAR-10, 100 [28], and ImageNet-1k[29] for 310 epochs from random initialization, and compare with other structured matrices. In the CIFAR-10 and CIFAR-100 benchmarks, BLAST outperforms several non-adaptive baselines, such as Low-Rank, Pixelfly, and Monarch with higher accuracy at the same FLOPs complexity (Figure 4). Gaudi-GBLR presents the most favorable accuracy-to-FLOPs tradeoff due to its capability of learning the adaptive resource/budget allocation for each weight matrix, which is a feature that our BLAST setting lacks in this particular evaluation (as we force it to use the same $r$ for all matrices).\nHowever, in the context of ImageNet-1k in Table 1, weight matrices trained using BLAST with $b = 3$ attain the highest levels of accuracy with the least FLOPs. This superior performance of BLAST (despite the common $r$ for all matrices) over Gaudi-GBLR can be attributed to its simpler training process with fewer hyperparameters. In contrast, the more complex training requirements of Gaudi-GBLR, which involve smoothness annealing and proximal gradient descent, may lead to suboptimal results for a large model such as ViT-Base in Table 1.\nWe validate the training performance of BLAST weights on language models. We replace the weights of GPT-2 [20] with random BLAST matrices and trained the network from scratch on the WikiText 103 [30] dataset for 100 epochs. In Figure 5, we compare the test set perplexity of BLAST with the perplexity from low-rank, block-diagonal, Monarch, and Gaudi-GBLR matrices. Similar to the ImageNet training, we found that BLAST achieves the best perplexity-FLOPs trade-off. Compared to Gaudi-GBLR, BLAST obtains a significant perplexity gain. We attribute this improvement to the simple training process of BLAST which requires less hyperparameter tuning than that of Gaudi-GBLR."}, {"title": "4.2 Compression and Re-training", "content": "In this section, we discuss the performance of BLAST weights when pre-trained dense weights are available. We first compress the dense weights using Algorithm 2 and re-train the model on the training data with the cross-entropy loss.\nWe compress the weights of the vision transformer (ViT) trained on ImageNet training set by $\\text{BLAST}_3$ and $\\text{BLAST}_{12}$ using Algorithm 2 and re-train the models for 35 epochs. The accuracy-FLOPs trade-off curve of each model is presented in Figure 6. Both BLAST compressed & re-trained models outperform other baselines, even though BLAST models did not use the adaptive budget allocation, unlike Gaudi-GBLR. It is observed that the accuracy of the BLAST models slightly increases from $b = 3$ to $b = 12$.\nWe compress the weights of a Diffusion Transformer (DiT) [13] pre-trained on ImageNet using BLAST matrices and compare its performance to SVD-based low-rank approximation. For both techniques, we match the compression ratio such that both decrease the total number of model parameters by 50%, and re-train each model for 10 epochs on the ImageNet training set. We evaluate the models by generating a total of 50,000 images using the original, low-rank, and BLAST compressed models, and compute the FID [31], sFID [32], and IS [33] metrics with respect to the ImageNet validation set. The objective is to observe if the compressed model can generate images as realistic as the original uncompressed model.\nIn Table 2, we show quantitatively that the model compressed via BLAST significantly outperforms the model compressed via SVD. The low-rank compressed model often generates unrealistic images, leading to poor metrics such as the inception score. Figure 1 also contrasts how the BLAST matrices contribute to maintaining high perceptual quality as well as a close instance-wise resemblance with the uncompressed model outputs. Due to space limitations, we defer additional qualitative results and experimental setup to Appendix D.2.\nWe compress the weights of Llama-7B [1] with BLAST matrices using Algorithm 2 by 20% and 50%, and re-train the models for 400 steps on a subset of SlimPajama [34] dataset using 0.49B tokens. The number of blocks $b$ in the BLAST matrices is fixed at 16, and we use $r = 1024$ for the attention modules and $r = 1488$ for the MLP modules to achieve a 50% compression ratio. We test the WikiText-2 perplexity and the zero-shot task classification accuracy on common sense reasoning datasets including PIQA[35], HellaSwag[36], WinoGrande[37], BoolQ[38], OpenBookQA[39], ARC-easy and challenge [40]. We report the performance of Low-Rank, Monarch, and Block-Diagonal weight matrices after compression at the same rate and re-training. In Table 3, the first row presents the performance of the original Llama-7B model. On 50% compression ratio in the last five rows, the Monarch and Block-Diagonal matrices fail to recover the acceptable performance. Compared to Low-Rank weights, BLAST weights achieve the lowest performance degradation in WikiText-2 perplexity and zero-shot classification accuracy. The accuracy of each common sense reasoning benchmark and extended results can be found in Appendix D.3.\nWe provide an analysis to quantify the performance impact of compression and re-training. We first quantify the weight compression performance at 20% compression ratio in Table 3. Although the compression ratio is moderate, Low-Rank and Monarch compression without re-training suffer from"}, {"title": "5 Related Works", "content": "Sharing the bases of block-structured matrices has recently drawn interest due to its considerable memory savings. BLAST matrices exemplify this approach."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we introduced the BLAST matrix designed to improve the inference efficiency of large DNNs. The BLAST matrix represents various low-dimensional structures of the weight matrices with fewer parameters, while enabling efficient matrix-vector products. The BLAST factors are either learnable from data or estimated from existing weights using our preconditioned factorization algorithm. Our results on both language and vision tasks highlight the effectiveness of BLAST.\nThe BLAST matrix-vector product consists of three steps, as detailed in Equation (3), which may degrade hardware-execution parallelism. In our evaluation, we used the same computational budget $r$ for all matrices. Learning an adaptive budget per layer or matrix (e.g., via overparameterization [54, 7]) could further improve BLAST performance, which is left for future work. The proposed method has not been evaluated on tiny (<100M parameters) or extremely large (>10B parameters) DNNs. Additionally, optimizing runtime and power consumption via BLAST matrices with customized library functions and/or hardware accelerators also remains as future work. Furthermore, a deeper theoretical investigation into the behaviors of BLAST matrices would provide a more comprehensive understanding of their capabilities and limitations. Applying advanced re-training techniques, such as knowledge distillation [55] or iterative compression and distillation [56], to the BLAST compression pipeline is also left for future work. Finally, beyond the weight structures, we expect BLAST can also help understand and exploit low-dimensional data manifolds [57\u201359] in future work."}, {"title": "A Details on BLAST Matrix and Factorization", "content": "Following the conventional setting in Transformers [16], the input tensor is assumed to have batch, sequence, and channel dimensions. The left and right factors are multiplied using the batched matrix multiplication routine, whereas the diagonal factors are multiplied via broadcasting and summation.\nA block-diagonal matrix is a BLAST matrix when $r = p$ and\n$s_{i,j} = \\begin{cases}1 & \\text{if } i = j \\\\ 0 & \\text{otherwise}\\end{cases}$\nsince $s_{i,j} \\in \\mathbb{R}^{r \\times r}$.\nWhen $r < p$, BLAST matrices model block-diagonal matrices which have low-rank diagonal blocks. For ease of understanding, let us consider a BLR matrix of $9 \\times$ rank-1 blocks. Each block is composed of the unique bases $A_{i,j} = u_{i,j}v_{i,j}^T$. Now consider $U_i = [u_{i,1}, u_{i,2}, u_{i,3}]$ and $V_j = [v_{1,j}, v_{2,j}, v_{3,j}]$. Then, the BLR matrix is a BLAST matrix with $r = b = 3$:\n$\\begin{bmatrix}\nu_{1,1} v_{1,1}^T & \nu_{1,2} v_{1,2}^T & \nu_{1,3} v_{1,3}^T \\\\\nu_{2,1} v_{2,1}^T & \nu_{2,2} v_{2,2}^T & \nu_{2,3} v_{2,3}^T \\\\\nu_{3,1} v_{3,1}^T & \nu_{3,2} v_{3,2}^T & \nu_{3,3} v_{3,3}^T \\end{bmatrix} = \\begin{bmatrix}U_1 S_1 V^T & U_1 S_2 V^T & U_1 S_3 V^T \\\\U_2 S_1 V^T & U_2 S_2 V^T & U_2 S_3 V^T \\\\U_3 S_1 V^T & U_3 S_2 V^T & U_3 S_3 V^T \\end{bmatrix},$\nwhere $S_1 = \\text{diag}([1, 0, 0]), S_2 = \\text{diag}([0, 1, 0]), \\text{and } S_3 = \\text{diag}([0, 0, 1])$.\nTo model a n x n general $b \\times b$ partitioned BLR matrix where the rank of each block is t, let us use the BLAST matrix with $b \\times b$ blocks and $r = bt$. The factors have the following shapes:\n$U_i, V_j \\in \\mathbb{R}^{p \\times (bt)}, \\quad S_{ij} \\in \\mathbb{R}^{bt}.$\nBy letting\n$S_{i,j,k} = \\begin{cases}1 & \\text{if } t(j - 1) + 1 \\leq k < tj + 1 \\\\ 0 & \\text{otherwise}\\end{cases},$\nthe BLAST matrix can model the BLR matrix. Note that the number of parameters of the BLAST matrix is $2nr + rb^2$, whereas that of the BLR matrix in this case is $b^2 \\cdot (p + p)t = 2(pb)(bt) = 2nr$. In other words, the BLAST matrix models various matrices with the cost of $rb^2$."}, {"title": "A.2 Derivation of Preconditioning Matrices", "content": "We rewrite the loss function in Equation (4) below:\n$\\ell(U_*, V_*, s_{*,*}) = \\sum_{i=1}^b\\sum_{j=1}^b\\frac{1}{2} ||A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T||_F^2.$ (4)\nHere we derive the gradients of Equation (4) with respect to the BLAST factors. We begin with introducing the short-handed notation for the concatenated factors:\n$\\overline{V}_i^T = [S_{i,1} V_1^T \\dots S_{i,b} V_b^T ], \\\\ \\overline{U}_j = [(U_1 S_{1,j})^T \\dots (U_b S_{b,j})^T]^T.$\nThat is, the matrix $\\overline{V}_i$ is composed by concatenating $V_j$s horizontally along $j = 1, 2, ..., b$ after scaling them with $S_{i,j}$. $U_j$ is defined similarly by concatenating the scaled $U_i$s vertically. Now we derive the gradients below.\nWe only have to consider the loss term related to $U_i$. Therefore, we have the following gradient expression:\n$\\nabla_{\\text{U}_i}\\ell(U_*, V_*, s_{*,*}) = \\nabla_{\\text{U}_i} \\sum_{j=1}^b \\frac{1}{2} ||A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T||_F^2 \\\\ = \\nabla_{\\text{U}_i} \\frac{1}{2} ||A_{i,*} - U_i \\overline{V}_i^T||_F^2 \\\\ = (U_i \\overline{V}_i^T - A_{i,*}) \\overline{V}_i,$ (10)\nGradient of $V_j$ follows the similar derivation as Equation (10):\n$\\nabla_{\\text{V}_j}\\ell(U_*, V_*, s_{*,*}) = \\nabla_{\\text{V}_j} \\sum_{i=1}^b \\frac{1}{2} ||A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T||_F^2 \\\\ = \\nabla_{\\text{V}_j} \\frac{1}{2} ||A_{*,j} - \\overline{U}_j V_j^T||_F^2 \\\\ = (\\overline{U}_j \\overline{U}_j^T - A_{*,j})^T \\overline{U}_j.$ (11)\nGradient of $s_{i,j}$ the block-wise loss for the gradient:\n$\\nabla_{s_{i,j}}\\ell(U_*, V_*, s_{*,*}) = \\nabla_{s_{i,j}} \\frac{1}{2} ||A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T||_F^2.$ (12)\nSince the Frobenius norm can be expressed by a matrix trace, the loss is written as follows:\n$|| A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T ||_F^2 = \\text{Tr} ((A_{i,j} - U_i S_{i,j} V_j^T) (A_{i,j} - U_i S_{i,j} V_j^T)^T) \\\\ = \\text{Tr} (V_j S_{i,j} U_i^T U_i S_{i,j} V_j^T - 2A_{i,j} U_i S_{i,j} V_j^T + A_{i,j} A_{i,j}) \\\\ = \\text{Tr} (S_{i,j} V_j^T V_j S_{i,j} U_i^T U_i - S_{i,j} V_j^T U_i + A_{i,j} A_{i,j}),$\nwhere $\\text{Tr}(X)$ is the trace of $X$. Note that the derivative of product in trace is given by $\\nabla_X \\text{Tr}(XY) = Y^T$ for any two conformal matrices X and Y. Therefore, we have\n$\\nabla_{s_{i,j}}\\frac{1}{2}|| A_{i,j} - U_i \\text{diag}(s_{i,j}) V_j^T ||_F^2 = U_i^T U S_{i,j} V_j^T V - U_i^T A_{i,j} V_j$"}, {"title": "B Proof of Theorem 1", "content": "We first introduce the following properties:\n$f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle \\leq \\frac{L}{2} ||y - x||^2.$\nThen, with the step size $0 < \\eta < \\frac{1}{L}$, the following holds:\n$f(x^{(k+1)}) \\leq f(x^{(k)}) - \\frac{\\eta}{2} ||\\nabla f(x^{(k)}||^2.$\n$\\leq f(x^{(k)}) - \\frac{\\eta}{2L} ||\\nabla f(x^{(k)}||^2 \\text{ (since } \\eta L \\leq 1)$\n$\\leq f(x^{(k)}) - \\frac{\\eta}{2L} ||\\nabla f(x^{(k)}||_2^2.$"}]}