{"title": "Debiasing Alternative Data for Credit Underwriting Using Causal Inference", "authors": ["Chris Lam"], "abstract": "Alternative data provides valuable insights for lenders to evaluate a borrower's creditworthiness, which could help expand credit access to underserved groups and lower costs for borrowers. But some forms of alternative data have historically been excluded from credit underwriting because it could act as an illegal proxy for a protected class like race or gender, causing redlining. We propose a method for applying causal inference to a supervised machine learning model to debias alternative data so that it might be used for credit underwriting. We demonstrate how our algorithm can be used against a public credit dataset to improve model accuracy across different racial groups, while providing theoretically robust nondiscrimination guarantees.", "sections": [{"title": "1 INTRODUCTION", "content": "The financial services industry has for years discussed the potential of using alternative data sources to improve lenders' ability to accurately assess the creditworthiness of borrowers [4, 14, 22, 39]. Traditional credit scores like FICO have historically relied on factors like payment history, amounts owed, length of credit history, and recent credit inquiries to generate their models [15]. More recently, alternative data sources like transactions or cash flow (e.g. bank accounts), bill payments (e.g. rent and utilities), and income or assets (e.g. employment history and property ownership) are also being utilized in some credit scoring models. By integrating more data into these models, this can lead to improved accuracy that could help lenders expand credit access to underserved borrowers such as credit invisible and thin file loan applicants [6], while enabling more borrowers to be approved or to qualify for lower interest rates [11, 16, 25].\nAl researchers have identified additional sources of alternative data that could aid credit underwriting but may not currently be in use by the industry. For example, Berg identified digital footprints, such as whether borrowers browsed online in incognito mode or used an anonymous email address, as being more likely to default [3]. Lee identified certain grocery shopping behaviors, like whether someone purchases cigarettes or processed meat, as also being less likely to pay back a loan [26]. Finally, Netzer identified certain words or phrases that were used on a credit application, like \"please,\" \"promise,\" and \"thank you,\u201d as potential red flags [28].\nBut these non-financial sources of alternative data may entail greater regulatory risk. Such data may act as an illegal proxy for a protected class like race or gender, leading to potentially unlawful discrimination [2, 37, 40]. Many FinTechs would like to leverage such alternative data with machine learning algorithms to improve model accuracy, but don't know how to do so without introducing unwanted bias or illegal discrimination. A large part of this problem has to do with the fairness through unawareness approach that is adopted by regulations such as the Equal Credit Opportunity Act Regulation B in the US, which several Al researchers have raised concerns about [2, 12, 18, 21, 33].\nA small community of AI researchers have proposed using causal models as a potential solution to the fairness and machine learning problem [1, 8, 9, 20, 29, 35, 41]. In particular, techniques proposed by Pearl using causal Bayesian networks (CBNs) may offer a partic-ularly promising solution [31, 32]. But to use these methods require building proper causal models of the fairness problem, which is not easy and requires significant domain expertise.\nThis paper will discuss how to build causal fairness models for credit underwriting, showing a fundamental relationship between supervised machine learning and causal inference. We will then discuss how to use those models to identify an algorithm to debias alternative data so that it would not act as an illegal proxy. Finally, we will show the results of this algorithm against a public dataset (i.e. the National Survey of Mortgage Originations)."}, {"title": "2 BUILDING A CAUSAL MODEL OF FAIRNESS", "content": "We begin with a block diagram representation of the supervised machine learning problem as shown in Figure 1. In the center is a black box which represents a machine learning model such as a neural network. The black box inputs data X and outputs decision D. The decision D leads to a future outcome Y, which in turn becomes future data X. The key question is whether a protected attribute A should be used as an input to the black box model, and if so, how should it be used?\nIn credit underwriting, data X could represent traditional or alter-native data, such as credit bureau data that captures past outcomes Y. The decision D could represent whether a loan is approved and its interest rate. The outcome Y could represent whether a borrower defaulted. The protected attribute A could represent race or gender, for example.\nWe now want to transform this block diagram representation into a causal Bayesian network (CBN). We perform this transformation through a series of steps. Note that due to the directed acyclic graph constraint of CBNs, we do not complete the feedback loop between outcome Y and data X.\nIn Figure 2, we have a three node CBN where the protected at-tribute A causes the mediator W, which in turn causes the outcome Y. The protected attribute A is colored gray because it may or may not be observable. If not, it could be imputed using a technique like Bayesian Improved Surname Geocoding (BISG) [5]. The medi-ator W represents creditworthiness and is colored white because it is a latent variable (i.e. it is not fully observable) [13]. On the other hand, the outcome Y is colored black because it is observable. The outcome Y is also independent of the protected attribute A given the mediator W: $Y \\amalg A / W$. In other words, creditworthiness completely explains away the relationship between the protected attribute and whether a borrower would default.\nIn Figure 3, we add in a few more nodes. Traditional data Xw is a partial and imperfect measure of the latent mediator W that is used to make a decision D. For example, we can set D as\n$D = P(Y = y/X_w = x_w) < t$ (1)\nwhere t is some threshold for predicted loan default. If a borrower is predicted to be below the threshold t, then he or she may be approved for a loan. Alternatively, a scorecard could be used with various thresholds to determine an interest rate. The machine learn-ing model is embedded inside the decision node D, which also has a causal effect on the outcome Y. For example, borrowers cannot default if they were never approved for a loan.\nThe graph in Figure 3 represents how traditional credit scoring is performed today. Traditional data Xw does not act as a proxy for the protected attribute A because it is a measure of the mediator W (i.e. creditworthiness), which acts as a confounding variable through the fork $X_w \\leftarrow W \\rightarrow Y$. In these causal models, we would also consider alternative data in use by the industry today (e.g. cash flow or utilities data) as traditional data Xw because they do not proxy for the protected attribute A. In this graph, the protected attribute A has no influence on the decision D. This is also known among the Al research community as fairness through unawareness.\nOne may view Figure 2 as a world model. Figure 3 shows how to overlay a machine learning model on top of the world model through the addition of the variables Xw and D. One may also consider the machine learning model to be a surrogate of the world model. That is, the data Xw acts as a surrogate for the mediator W and the decision D acts as a surrogate for the outcome Y. For those who are familiar with Pearl's three-layer causal hierarchy [32], the machine learning model is operating at the first layer (association), while the world model is operating at the second layer (intervention) and third layer (counterfactuals)."}, {"title": "3 DEFINING UNLAWFUL DISCRIMINATION USING CAUSAL MODELS", "content": "There are two legal doctrines for unlawful discrimination in credit underwriting [17]. In the US, these are called disparate treatment and disparate impact. In the EU, these are called direct discrimina-tion and indirect discrimination. This paper will focus on US fair lending discrimination.\nAccording to the Interagency Fair Lending Examination Pro-cedures manual [10], disparate treatment occurs when a lender explicitly considers prohibited factors (overt evidence) or by dif-ferences in treatment that are not fully explained by legitimate nondiscriminatory factors (comparative evidence). On the other hand, disparate impact occurs when a formally neutral policy dis-proportionately excludes or burdens certain persons on a prohibited basis.\nWe can model disparate treatment as a form of overt discrim-ination using the graph in Figure 4. The protected attribute A is explicitly used to influence a decision D, which causes both the decision D and the outcome Y to become biased. We can visualize this bias using red nodes and edges. For example, a lender that uses knowledge of a borrower being Black as a basis for denying a loan would cause overt discrimination that would lead to dis-parate treatment. In causal terms, we would say that the protected attribute A has a negative direct effect on the decision D. This is why the financial services industry generally uses fairness through unawareness, as shown in Figure 3.\nWe can model disparate impact using a three step process. The first step for establishing disparate impact is to identify whether a disparity exists on a prohibited basis. If so, the next step is to determine whether the policy is justified by a \"business necessity.\" Finally, a lender could still be in violation if an alternative policy or practice could achieve a less discriminatory effect.\nThe first step of disparate impact could be modeled as a form of covert discrimination using the graph in Figure 5. The protected attribute A is not explicitly used to influence a decision D, which makes the policy formally neutral. That is, the protected attribute A has no direct effect on the decision D. However, due to biases that may exist from the use of traditional data Xw alone, it could dis-proportionately burden or exclude certain persons on a prohibited basis. Such biases may be due to historical, structural, and systemic discrimination for example. Toh argues that the use of traditional credit scoring models may disproportionately punish consumers from economically disadvantaged groups including minorities, who are more likely to be credit invisible or have thin files [39]. Here we can visualize bias across the entire graph through red nodes and edges. In causal terms, we would say that the protected attribute A has a negative indirect effect on the decision D. This is why the financial services industry's approach of fairness through un-awareness, as shown in Figure 3, may be insufficient for preventing disparate impact.\nThe following section will discuss how the use of alternative data for credit scoring may help to satisfy the second and third steps of disparate impact. That is, alternative data may be able to improve the accuracy of a credit scoring model, which is necessary for satisfying the business necessity requirement. Finally, it may also be able to improve fairness by picking up additional signals for economically disadvantaged groups to prove their creditworthiness, thus leading to a less discriminatory credit scoring model."}, {"title": "4 INCORPORATING ALTERNATIVE DATA INTO A CAUSAL MODEL", "content": "One of the primary goals of a data scientist is to build the most accu-rate model. For example, if we had perfect information about a bor-rower's creditworthiness and combined it with a perfect model of the world, we should be able predict with perfect accuracy whether a borrower would default on a loan. A perfectly accurate model would help lenders because they would be able to make loans to all creditworthy borrowers in the marketplace without taking any losses from defaults. This would also help borrowers in a competi-tive market for credit as they would be charged lower risk premiums and never have to deal with the consequences of defaulting on a loan. However, this goal is not fully achievable in practice. One of the reasons is due to limitations in our ability to collect tradi-tional data Xw to measure the mediator W due to practical, legal, or ethical reasons.\nSince we can never fully capture the mediator W using tradi-tional data Xw, there is interest in the industry to use alternative data to gain a better picture of a borrower's creditworthiness. But certain types of alternative data, such as zip code or education, may act a proxy for a protected class. These types of alternative data are modeled in Figure 6 as demographic variables Z, which is a modified version of Figure 2. Unlike the mediator W, demo-graphic variables Z do not completely explain away the causal effect between the protected attribute A and the outcome Y. As a result, the protected attribute A may act as a confounder variable for demographic variables Z. Note that both the mediator W and demographic variables Z are latent and thus colored white.\nDemographic variables Z are correlated with the creditworthi-ness mediator W through two paths. The first is a direct causal path $Z \\rightarrow W$, which should be considered a legal path. The sec-ond is a spurious non-causal path $Z \\rightarrow A \\rightarrow W$, which should be considered an illegal path through the protected attribute A. This spurious path is colored red in Figure 6.\nWe also need to be careful not to include any variables that have no causal relationship to the creditworthiness mediator W. Figure 7 shows a proxy variable P, which is only correlated with the mediator W through the spurious non-causal path $P \\leftarrow A \\rightarrow W$. This should also be considered an illegal path through the protected attribute A.\nBy modifying Figure 3 to include alternative data Xz, we get Figure 8. Notice that the direct use of alternative data Xz might trigger proxy discrimination and lead to overt discrimination in the decision D, similar to the overt discrimination caused by the direct use of the protected attribute A in Figure 4. This is because as shown in Figure 6, demographic variable Z is spuriously correlated with the mediator W through the protected attribute A. That is, the machine learning classifier has no way to isolate the direct causal effect $Z \\rightarrow W$ from the spurious non-causal effect $Z \\rightarrow A \\rightarrow W$."}, {"title": "5 LEVERAGING CAUSAL INFERENCE TO DEBIAS ALTERNATIVE DATA", "content": "Now that we have causal graphs for modeling fairness and dis-crimination in the credit underwriting problem, we will now dive deeper into explaining how we can use Pearl's tools of causal in-ference to make interventions into a machine learning model [30]. This provides the foundation for a causal approach to data science that could be called \"model-based\u201d supervised learning, which is in contrast to the \"model-free\u201d supervised learning that the industry commonly uses today. By using a CBN as a higher level abstraction of the supervised ML problem, it acts as a type of world model that provides a high-level specification for performing data science.\nThe data science process requires performing a series of steps, including feature selection, data preparation, training, inference, and evaluation. While each of these steps are already intuitively performed by a data scientist today, they can be more systematically augmented using causal inference.\nThe first step is feature selection, which is a type of mediation analysis. A data scientist must select features that have a causal relationship to a borrower's creditworthiness (or as Evans points out, a \"nexus to creditworthiness\" [13]). This can be done using prior knowledge of known causal effects in the data. In the finan-cial services industry, some professionals refer to creditworthiness as the 5 C's of credit: character, capacity, capital, collateral and conditions [34, 38]. Traditional data and some forms of financial alternative data (e.g. bank cash flow, rent and utility payments) may be considered measures of the creditworthiness mediator W, and are therefore labeled as traditional data Xw. Non-financial forms of alternative data Xz (e.g. zip code and education) may be considered measures of demographic variables Z, which have both a direct causal effect on the creditworthiness mediator W and a spurious non-causal effect through the confounding protected attribute A, creating an illegal proxy effect. This is in contrast to data that have no causal effect on the creditworthiness mediator W, which must be excluded from the model. They instead may only be spuriously correlated with outcome Y through the confounding protected attribute A (e.g. name, hair length, or music preference).\nThis is followed by the data preparation step. A data scientist needs to clean up the data such as by removing or replacing values. During this step, a data scientist should perform overlap testing. That is, we need to ensure that all of the values for each feature have an overlap with the values of a protected class. If there is no overlap, then we would have a positivity violation. This is espe-cially important for alternative data Xz. An example of a positivity violation may be using a historically Black college or university (i.e. HBCU) for education. Very few if any Whites attend an HBCU, making it impossible for a classifier to isolate the direct causal ef-fect of attending an HBCU on the creditworthiness mediator W from the spurious non-causal effect of the confounding protected attribute for race A. When such a positivity violation occurs, a data scientist would need to remove or replace the value for the feature. Note that you do not necessarily have to drop the entire feature, just individual values where the positivity violation exists.\nWe can think of the training step as a type of backdoor adjustment. We train a classifier using the protected attribute A in order to isolate the direct causal effect of demographic variable Z on the creditworthiness mediator W (i.e. Z \u2192 W) from the spurious non-causal effect through the confounding protected attribute A (i.e. $Z \\leftarrow A \\rightarrow W$). This has the effect of debiasing alternative data Xz by closing off the backdoor path that could cause it to act as an illegal proxy for the protected attribute A.\nDuring the inference step, we perform our discrimination testing by making an intervention into the model. For a given loan applicant, we would perform a paired test using both the default values and the actual values for the protected attribute A. As an example, we may use White as a default value for race and male as a default value for gender. More specifically, we would be performing Pearl's do-intervention to replace the protected attribute A's direct effect on the decision D, such as $P(Y = y|do(A = a'), do(X_z = x_z),do(X_w = x_w))$. This has the causal effect of closing the backdoor effect of the feature Xz through the confounding protected attribute A. For example, a zip code where the population is majority Black cannot act as a proxy for a borrower being Black if the classifier was told to treat that borrower as a White person living in that zip code. This assumes that we have enough examples of White people living in that zip code to train on so that the classifier can isolate its causal effect from its spurious effect. This will allow the classifier to use zip code to capture local economic conditions that are independent of race.\nFinally, during the evaluation step we perform a counterfactual analysis. We compare the decision D that is derived from a tradi-tional credit score (as shown in Figure 3) to an alternative credit score (as shown in Figure 9). This will allow us to verify that the alternative credit scoring model is less discriminatory across all protected classes. We may consider this as another approach to using counterfactuals for fairness [7, 19, 23, 27]."}, {"title": "6 TESTING THE ALGORITHM", "content": "Our debiasing algorithm was tested against a public dataset called the National Survey of Mortgage Originations (NSMO). This survey is administered by the Federal Housing Finance Agency (FHFA) and the CFPB. It is conducted quarterly to collect voluntary feed-back directly from mortgage borrowers about their experiences with obtaining a mortgage. While this public dataset is not actual credit bureau data, we used it to simulate that type of data. One of the unique aspects of this public dataset is that it contains both a protected attribute (race) and its outcome (default).\nThe specific version of the NSMO dataset that we used was nsmo_v41_1320_puf.csv, which was released in March 2023. We generated the target variable using the Perf_Status questions, marking a loan as having defaulted if it had codes 2 through 9, or at least 60 days past due. We combined the variables X77R and X78R to generate the following classes for race: Non-Hispanic White, Hispanic, Black, Asian, and other. The focus of our analysis for this paper was between Non-Hispanic Whites and Blacks.\nThe remaining X and Z variables were used to train our models, as well as several supplemental features. Since the focus of our test was on race, we decided to leave in age and sex. However, these could be dropped from our models without loss of generality. We also removed supplemental variables to prevent data leakage. Our source code has been publicly uploaded to Github for researchers who want to replicate our results. [24]"}, {"title": "6.1 Model training", "content": "We trained three models, which we called the awareness, unaware-ness, and counterfactual models. The awareness model is a \"kitchen-sink model\" that used every feature including race, which yielded the highest accuracy but also caused disparate treatment. The un-awareness model dropped race and any features highly correlated with Black applicants. For the purpose of this experiment, we as-sumed that such features could be considered a form of alternative data that could act as an illegal proxy. This model simulates the fairness through unawareness approach commonly used by the industry today. Finally, the counterfactual model uses all features including race during training but treats all applicants as White dur-ing inference, which is equivalent to Figure 9 to prevent disparate treatment. Since it uses alternative data to expand credit access to underserved groups, it would also prevent disparate impact.\nThe models were built using the Microsoft LightGBM library. We used an 80/20 train/test split with 5 K-fold validation. The evaluation metric was ROC/AUC.\nFor the unawareness model, we dropped any features that were highly correlated with race, and more specifically with being Black. Any features that had a correlation coefficient less than -0.05 or greater than 0.05 were dropped, for a total of 89 out of 1012 features.\nFor the two backdoor models, we also did overlap testing to ensure that feature values for all the potential proxy features had overlap with the racial groups. We didn't identify any positivity violations with this dataset. But had we identified such violations, we would have simply dropped the values for those proxy features."}, {"title": "6.2 Results", "content": "While there was a significant drop-off in accuracy going from the awareness to unawareness models, most of that dropoff was recovered by moving to the counterfactual model. Note that there was still a very small dropoff in accuracy between the awareness and the counterfactual model, which is due to the cost of mislabeling Black applicants as White. However, that cost was more than offset by the increased use of data to explain away the effect of race."}, {"title": "7 CONCLUSION", "content": "Causal inference may provide a more robust set of algorithmic techniques for debiasing alternative data to prevent it from acting as an illegal proxy for a protected attribute. Compared to the statistical or correlational techniques commonly used by the industry today, such as hyperparameter tuning or adversarial debiasing [36], causal techniques may provide a stronger mathematical and theoretical basis for proving that a machine learning algorithm will not cause unlawful discrimination.\nUnfortunately, current fair lending regulations like the Equal Credit Opportunity Act (ECOA) in the US may not allow the use of our causal debiasing algorithm at this time. According to ECOA Regulation B, \"a creditor shall not consider race, color, religion, national origin, or sex (or an applicant's or other person's decision not to provide the information) in any aspect of a credit transac-tion\" (1002.6). Instead, the protected class may only be used for monitoring purposes (1002.13), self-testing and correction (1002.15), or for determining eligibility in a special purpose credit program (1002.8). That is, the protected attribute may only be used to detect discrimination after a decision has been made, as opposed to being used to prevent discrimination before a decision has been made.\nWe believe that policymakers should reconsider this policy as these techniques provide at least five major advantages. First, they directly address the fairness through unawareness concerns that the Al researchers have made over the last several years. These techniques provide robust guardrails that allow us to make full use of data that has a causal relationship or nexus to creditworthi-ness. Second, they could be used to solve the proxy discrimination problem in alternative data, expanding the amount of data that lenders could use to assess a borrower's creditworthiness. Third, this broader use of alternative data could help expand credit access to underserved groups, including credit invisible and thin file bor-rowers. Fourth, they enable the development of less discriminatory models for satisfying the third step of disparate impact. Finally, they can help eliminate the need for regulatory sandboxes by providing more transparent tools to audit and correct for bias and discrimi-nation in machine learning models. These causal techniques may provide a means for modernizing our regulations, enabling a more equitable and inclusive financial system that would benefit both lenders and borrowers."}]}