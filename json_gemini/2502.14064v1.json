{"title": "Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging", "authors": ["Shansong Wang", "Mojtaba Safari", "Qiang Li", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "abstract": "Vision foundation models (VFMs) are pre-trained on extensive image datasets to learn general representations for diverse types of data. These models can subsequently be fine-tuned for specific downstream tasks, significantly boosting performance across a broad range of applications. However, existing vision foundation models that claim to be applicable to various radiology tasks are mostly pre-trained on 3D computed tomography (CT), which benefits from the availability of extensive 3D CT databases. Significant differences between CT and magnetic resonance imaging (MRI) in imaging principles, signal characteristics, and data distribution may hinder their practical performance and versatility in MRI-specific applications. Here, we propose Triad, a vision foundation model for 3D MRI. Triad adopts a widely used autoencoder architecture to learn robust representations from 131,170 3D MRI volumes and uses organ-independent imaging descriptions to constrain the semantic distribution of the visual modality. The above pre-training dataset is called Triad-131K, which is currently the largest 3D MRI pre-training dataset. We evaluate Triad across three tasks, namely, organ/tumor segmentation, organ/cancer classification, and medical image registration, in two data modalities (within-domain and out-of-domain) settings using 25 downstream datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad improves segmentation performance by 6.88% compared to nnUNet-Scratch across 17 datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in classification tasks across five datasets. SwinUNETR-Triad improves by 4.00% compared to SwinUNETR-Scratch in registration tasks across two datasets. Our study demonstrates that pre-training can maximize performance when the data modalities and organs of upstream and downstream tasks are consistent. This work highlights the value of large-scale pre-training techniques for downstream tasks in 3D MRI. By open-sourcing Triad's weights, code, and data, we aim to enhance the adaptability and reliability of AI solutions for 3D MRI in radiology.", "sections": [{"title": "1 Introduction", "content": "Each year, over 40 million magnetic resonance imaging (MRI) scans are performed in the United States, an average of 107.5 scans per 1,000 persons [1, 2]. Globally, the annual total of MRI scans ranges between 100 to 150 million [3]. This has led to a growing demand for automated analysis tools [4]. In recent years, Foundation Model (FM)-driven image analysis has shown significant advancements. However, these foundation models have primarily been tailored for general computer vision tasks and are trained on numerous natural image datasets to acquire general representations applicable to a wide range of data [5, 6, 7]. They can then be fine-tuned for various specific downstream tasks, leading to significant enhancements in performance across different applications. This paradigm shift has also been widely adopted in clinical modalities (including 2D and 3D data), which has demonstrated notable improvements [8, 9, 10, 11].\nUnfortunately, the potential of foundational models specifically for 3D MRI remains largely unexplored. There are two key limitations: Firstly, although previous general medical foundation models assert the capability to generalize to 3D MRI, substantial differences in imaging principles, signal characteristics, and data distribution"}, {"title": "2 Results", "content": "We present results from 25 downstream datasets across three types of evaluation tasks and two data modality settings. These downstream tasks are categorized into within-domain and out-of-domain tasks. Within-domain downstream tasks utilize the same data modalities and structures as those in the pre-training phase, including brain, breast, and prostate MRI. These tasks assess whether Triad has successfully learned structural and modality representations during pre-training, thereby improving performance on related tasks. Conversely, out-of-domain downstream tasks involve data modalities or organs different from those in the pre-training stage, such as liver CT or atrial MRI. These tasks evaluate whether the knowledge acquired by Triad during pre-training can be effectively transferred to and applied in new modalities or structures. Based on these two data modality settings, we evaluate three task types: 3D structure/tumor segmentation (Fig. 3 and Fig. 4), organ/cancer classification (Fig. 5), and 3D medical image registration (Fig. 6)."}, {"title": "2.1 3D organ/tumor segmentation", "content": "We first evaluated the effectiveness of Triad on 3D organ/tumor segmentation, a common tasks in medical image analysis. As shown in Fig. 3 (a), we initialize the encoder with the parameters learned during pre-training, while the decoder is randomly initialized. We evaluate Triad on 17 extensive 3D MRI and CT semantic segmentation datasets, including five MRI datasets for within-domain tasks: BraTS21[26], MSD[27]-BrainTumour, BreastDM[28], Prostate158[29], and MSD-Prostate; and 12 datasets covering different organs or modalities for out-of-domain tasks: MM-WHS-MRI[30], ATLAS-MRI[31], Abdomen 1K[32], Kipa22[33], MSD-Pancreas, MSD-Liver, MSD-Heart, MSD-Hippocampus, MSD-Lung, MSD-HepaticVessel, MSD-Spleen, and MSD-Colon. We use the Dice Similarity Coefficient (DSC) as the primary evaluation metric, consistent with public benchmarks."}, {"title": "2.1.1 Influence of model parameter scale on model performance", "content": "It is widely believed that increasing model parameters enhances the performance of downstream tasks in foundation models [7, 9, 34, 35]. This trend has been observed in various domains, including natural images [7], X-rays [34], and other medical imaging modalities[9, 35]. However, some studies have reported contradictory findings, particularly in 3D CT imaging[16] and vision language models [36, 37]. In this study, we quantitatively analyze the scaling behavior of Triad pre-training. We conduct a series of experiments with varying model architectures to systematically evaluate their impact on performance.\nWe specifically select the widely used SwinUNETR architecture [38]. The SwinUNETR encoder utilizes different variants of Swin Transformer [39], including Swin-B (Base), Swin-L (Large), and Swin-H (Huge). We evaluate the impact of model parameter scaling on the within-domain 3D tumor segmentation task and compare it with Scratch and VoCo-SSL [16]. VoCo-SSL is a vision foundation model pre-trained on 160K 3D CT scans using a self-supervised model distillation scheme and is considered state-of-the-art in 3D medical imaging. Scratch denotes training from scratch without using any pre-trained weights.\nAs shown in Fig. 3 b (1) - b (5), the average DSC reported by Swin-B-Scratch across the five datasets is 77.76%. In comparison, Swin-B-VoCo-SSL achieves an average DSC of 79.31% (+1.55%), while Swin-B-Triad achieves 79.66% (+1.90%). These results indicate that pre-training the upstream encoder can markedly enhance the performance of downstream tasks. This finding is consistent with previous studies[16, 35]. Notably, Swin-B-Triad outperforms Swin-B-VoCo-SSL by +0.35%, which can be attributed to Triad's use of MRI data for pre-training, whereas VoCo-SSL is pre-trained on CT data. This suggests that greater alignment between the data modalities of upstream and downstream models leads to improved downstream performance. A key finding across 15 experiments comparing the Swin-B/L/H architectures is that 11 of these experiments indicate that increasing the size of model parameter does not consistently lead to performance improvements. This observation aligns with findings from VoCo-SSL[16]. A possible explanation is that excessive model parameters may lead to overfitting on small downstream datasets. Even with robust initial parameters from the upstream pre-trained model, downstream performance may still be adversely affected by overfitting."}, {"title": "2.1.2 Within-domain 3D tumor segmentation", "content": "The upstream 3D MRI data used for pre-training is derived from three organs: the brain, breast, and prostate. Consequently, the downstream within-domain 3D tumor segmentation task employs data from the same modality and organs to evaluate performance, assessing whether Triad has effectively learned structural and modality representations during pre-training. Specifically, we select two brain datasets: BraTS21 [26] and MSD-BrainTumour [27], one breast dataset: BreastDM [28], and two prostate datasets: MSD-Prostate [27] and Prostate158 [29]. We employ two widely used network architectures: nnUNet [21] and SwinUNETR (Swin-B).\nThe nnUNet-Scratch achieves an average DSC of 79.68%, whereas nnUNet-Triad improved this to 81.13%, marking an increase of 1.45%. Considering the performance of the Swin-B encoder under the three initialization settings discussed earlier, it is evident that pre-trained parameters consistently outperform random initialization, regardless of the model structure (nnUNet or SwinUNETR) or the pre-training strategy (VoCo-SSL or Triad) employed. Furthermore, the results indicate that nnUNet generally outperforms SwinUNETR in segmentation tasks. A radar chart presenting DSC for each category in Fig. 3 (d). This chart demonstrates that Triad excels in segmenting fine-grained tumors. For example, the BraTS21 Tumor Core represents the core region of the tumor, which serves as the primary therapeutic target and excludes the edema area. The BraTS21 Enhancing Tumor represents the actively invasive tumor and serves as a key indicator of tumor grade and recurrence. Notably, Triad outperforms Swin-B-VoCo by +2.08% and +1.38% in both categories."}, {"title": "2.1.3 Out-of-domain organ/tumor segmentation", "content": "We further assess whether the knowledge acquired by Triad during pre-training can be effectively transferred to and applied in new modalities or anatomical organs. To achieve this, we select four MRI datasets from other organs: MSD-Heart, MSD-Liver, MM-WHS-MRI [30], and ATLAS-MRI [31]. Additionally, we incorporated eight CT datasets: MSD-Hippocampus, MSD-Lung, MSD-Pancreas, MSD-HepaticVessel, MSD-Spleen, MSD-Colon, Abdomen 1K [32], and Kipa22 [33]. We continue to use the nnUNet and SwinUNETR (Swin-B) architectures and employ three parameter initialization methods: training from scratch, VoCo-SSL, and Triad.\nAs shown in Fig. 4 (a), an interesting observation is that, when using the Swin-B architecture, the three initialization methods rank in performance as follows: VoCo (77.14%) > Scratch (74.34%) > Triad (73.09%). We also observed"}, {"title": "2.2 Organ/cancer classification", "content": "We next evaluate the performance of Triad on organ and cancer classification tasks. As shown in Fig. 5 (a), we initialize the encoder with the parameters learned during pre-training, apply an average pooling operation to the output of its final layer, and pass the resulting features through a two-layer linear classifier to predict the probability distribution of the categories. We evaluate Triad on five widely recognized 3D CT and MRI classification datasets, including two MRI datasets for within-domain classification: ADNI [40] and BreastDM [28]; two CT datasets for out-of-domain classification: OrganMNIST3D [41] and LUNA16 [42]; and one additional MRI dataset for out-of-domain classification: LLD-MMRI [43]. We use classification accuracy (Acc) as the primary evaluation metric."}, {"title": "2.2.1 Within-domain organ/cancer classification", "content": "As shown in Fig. 5 (c), we compare two architectures, 3D UNet and Swin-B, using four initialization methods: training from scratch, SwinUNETR [44], VoCo-SSL, and Triad. We observe that on both the ADNI and BreastDM datasets, Swin-B-Scratch achieves an average accuracy that is +4.25% higher than 3D UNet-Scratch. A similar trend is observed in the LLD-MMRI and OrganMNIST3D datasets. The only exception is the LUNA16 dataset, where 3D UNet-Scratch achieves an accuracy that is +0.73% higher than Swin-B-Scratch. These findings provide strong evidence that the Swin-B architecture is better suited for classification tasks.\nNext, we compare the impact of three different pre-trained models on downstream performance. SwinUNETR is pre-trained on approximately 5K CT volumes, whereas VoCo-SSL utilizes 160K CT volumes. According to the reported accuracy, VoCo-SSL achieves an average accuracy that is +1.37% higher than SwinUNETR. Triad is pre-trained on 131K MR volumes and achieves an average accuracy that is +1.52% higher than VoCo-SSL. These results indicate that both the modality and scale of pre-training data positively impact downstream performance."}, {"title": "2.2.2 Out-of-domain organ/cancer classification", "content": "As illustrated in Fig. 5 (c), Triad achieves the highest performance in the organ classification task and ranks second in both lung nodule and liver lesion classification tasks. Notably, Triad still outperforms training from scratch by +1.02%, demonstrating its effectiveness in generalizing across diverse imaging modalities and organ types. Furthermore, we provide the confusion matrix for Swin-B-Triad across the five datasets. Fig. 5 b (1) shows that when Swin-B-Triad is applied to an out-of-domain classification task with a highly imbalanced category distribution, the model struggles to classify minority classes accurately. In the OrganMNIST3D classification task, Triad fails to distinguish categories 1\u20134 accurately. These findings suggest that while pre-trained parameters enhance overall downstream performance, addressing challenges such as data imbalance and hard example mining may require specialized sampling strategies or model architectures. Additionally, we present the ROC curves for four datasets in Fig. 5 (d). The ROC curves of all pre-trained models exhibit significant overlap, whereas models trained from scratch show markedly inferior performance, particularly on OrganMNIST3D and ADNI."}, {"title": "2.3 3D medical image registration", "content": "Finally, we evaluate the performance of Triad on the 3D medical image registration task. As illustrated in Fig. 6 (a) and (c), we explore two different parameter initialization strategies. In Fig. 6 (a), we employ the TransMorph [45] architecture with a Swin-Transformer-L encoder, initializing it with pre-trained weights from Triad and VoCo-SSL. In Fig. 6 (c), we use the Swin-UNETR [46] architecture with a Swin-Transformer-B encoder, initializing it with pre-trained weights from Triad, VoCo-SSL, SuPreM, and SwinUNETR. The decoder remains unchanged from the original method and is randomly initialized. We evaluate Triad on three widely recognized 3D MRI registration datasets, including two brain datasets for within-domain registration: IXI [47] and OASIS [48], as well as one cardiac dataset for out-of-domain registration: ACDC [49]. We use the Dice Similarity Coefficient (DSC) as the primary evaluation metric and report the best results after fine-tuning for 200 epochs."}, {"title": "2.3.1 Comparison of pre-training strategies in TransMorph and SwinUNETR for 3D medical image registration", "content": "Fig. 6 (a) illustrates a bar chart depicting the DSCs for each dataset. TransMorph-Scratch achieves average DSCs of 73.76%, 86.79%, and 74.81% on IXI, OASIS, and ACDC, respectively. When employed Triad pre-trained weights, the DSCs are 73.91% (+0.15%) on IXI and 86.92% (+0.13%) on OASIS, but decrease to 74.57% (-0.24%) on ACDC. Similarly, VoCo-SSL pre-training results in DSCs of 73.44% (-0.32%), 86.98% (+0.19%), and 74.72% (-0.09%) on IXI, OASIS, and ACDC, respectively. The performance of TransMorph under these three initialization strategies indicates that pre-trained parameters do not consistently yield improvements over random initialization in 3D medical image registration. This observation applies to both within-domain (IXI, OASIS) and out-of-domain (ACDC) tasks, with some cases even exhibiting marginal performance declines.\nFig. 6 (c) presents DSC performance under the SwinUNETR architecture, comparing five initialization strategies: Scratch, SwinUNETR, SuPreM, VoCo-SSL, and Triad. Specifically, Swin-B-Scratch achieves DSCs of 72.60% on IXI and 81.80% on OASIS. Swin-B-VoCo achieves DSC scores of 73.60% (+1.00%) on IXI and 84.40% (+2.60%) on OASIS. Notably, Triad proves to be the most effective pre-training method, achieving DSC scores of 73.70% (+1.10%) on IXI and 88.70% (+6.90%) on OASIS. These improvements are particularly pronounced on the OASIS dataset, where Triad outperforms other initialization methods by a significant margin.\nBy integrating these findings with the TransMorph results in Fig. 6 (a), we observe that partially loading encoder weights, as done in TransMorph, while randomly initializing the remaining parameters, may introduce inconsisten- cies, potentially limiting the benefits of pre-training. In contrast, SwinUNETR employs a fully pre-trained encoder, thereby eliminating random initialization in that module. This allows the network to leverage pre-trained features more effectively, leading to substantial improvements in 3D medical image registration."}, {"title": "2.3.2 Impact of initialization method on registration performance", "content": "Fig. 6 (b) presents the visualization results for the IXI dataset using the TransMorph architecture. Regardless of the initialization method used for fine-tuning, the observed improvements in the mask appear similar, with no substantial enhancements detected. In contrast, Fig. 6 (d) presents the visualization of the OASIS dataset using the SwinUNETR architecture, where improvements in the mask (indicated by the red arrow) are noticeable. These visualizations provide intuitive evidence supporting our previous assertion that incomplete pretraining initialization of the encoder may lead to model confusion.\nFurthermore, Fig. 6 (e) depicts the DSC distributions for the IXI dataset. SwinUNETR initialized with Triad weights achieves the highest registration performance across most organs, including the Thalamus, Cerebral White Matter, Cerebellar White Matter, Pallidum, Caudate, Lateral Ventricle, Hippocampus, Third Ventricle, Fourth Ventricle, and Amygdala. Notably, this superior performance can be attributed to the inclusion of abundant 3D MRI brain organ data in the upstream pretraining tasks. Regardless of the initialization weights, the DSC and registration performance for the choroid plexus remains low, likely due to its complex anatomical attachments to surrounding structures and its diffuse morphological characteristics."}, {"title": "3 Discussion", "content": "In this study, we constructed a large-scale 3D MRI dataset, known as TriadMR-131K, which consists of 131,170 volumes from 19,721 patients across 36 clinical datasets. This extensive collection includes a diverse collection of 3D MRI data from three organs, including the brain, breast, and prostate. It includes modalities such as T1-w, T2-w, FLAIR, DWI-MRI, fMRI, and DCE-MRI. Additionally, we extract imaging descriptions from the metadata of each 3D volume. These descriptions detail the imaging modality and associated device parameters. Using this dataset, we develop Triad, a vision foundation model tailored for 3D MRI. Triad employs widely used autoencoder architecture to learn robust representations and incorporates organ-independent imaging descriptions to constrain the semantic distribution of the visual modality. We evaluate Triad on three tasks, including organ and tumor segmentation, organ and cancer classification, and medical image registration. These tasks are assessed across two data modalities-within-domain and out-of-domain- across 25 downstream datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad improves segmentation performance by 6.88% over nnUNet-Scratch across 17 datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in classification tasks across five datasets. SwinUNETR-Triad improves by 4.00% compared to SwinUNETR-Scratch in registration tasks across three datasets. Triad outperforms baseline models across all downstream tasks and exceeds existing state-of-the-art models in most cases. Overall, Triad's seamless adaptability across downstream tasks highlights its potential as a"}, {"title": "4 Methods", "content": "The following sections are structured are organized to provide a comprehensive overview of our methodologies and findings. Initially, we will introduce the data curation process for TriadMR-131K, as well as the protocol established for Triad pre-training. Following this, we will articulate the implementation strategies employed in three distinct experimental paradigms: 3D organ and tumor Segmentation, organ and cancer classification, and 3D medical image registration."}, {"title": "4.1 Pretraining", "content": "Curation of TriadMR-131K dataset. To ensure the quality and diversity of data for model pretraining, we curated a large-scale 3D MRI dataset of 131,170 3D volumes derived from 19,721 patients across 36 clinical datasets. TriadMR-131K comprises a diverse collection of 3D MRI data spanning three organs (brain, breast, and prostate), featuring modalities such as T1-w, T2-w, FLAIR, DWI-MRI, fMRI, DCE-MRI. To standardize all sub-datasets, we used the same preprocessing protocol for all organs: we used the dicom2nifti package to convert all DICOM-format 2D slice collections into NIfTI-format 3D volumes. For 4D volume data, such as DCE-MRI, we took the (t)th or (t+1)th 3D slice to replace the original 4D data, where t denotes the tth time step. All corrupted volumes were deleted during the conversion process. Next, we reformatted all MRI scans so that the first axis points from left to right, the second from posterior to anterior, and the third from inferior to superior. We then resampled the images to a 1 mm resolution using bilinear interpolation. We also resized all images to (256,256,128) using trilinear interpolation. To save storage space, we stored most of the 3D volume data types as UINT16 and the rest as Float32. Finally, the brain MRI data involved 51,112 series from 37,436 examinations of 12,994 patients; the breast MRI data involved 46,116 series from 8,180 examinations of 3,834 patients; the prostate MRI data involved 33,942 series from 9,941 examinations of 4,639 patients. The statistical information of the 36 datasets is shown in Table 1. Note that due to deletions during the conversion process, the numbers in the table are usually lower than the officially published figures. In addition, we extracted the imaging description from the metadata of each 3D volume, which describes the imaging modality and related device parameters. Since it is organ-independent information, it helps to adjust the positional relationship of each modality in the semantic space, thereby improving the generalization ability of the model. We tried to avoid any overlap between the datasets used in pre-training and all downstream evaluation sets to minimize the risk of data contamination.\nProtocol of Triad pre-training. Triad uses nnUNet (31M)[21] and SwinTransformer[46] for the image encoder. Furthermore, SwinTransformer is expanded into Swin-B (72.8M), Swin-L (290.4M), and Swin-H (11.6B) according to feature sizes of 48, 96, and 192 to study the parameter scaling law (Fig. 2b). We use GTR-T5-Large [54] as the text encoder instead of CLIP [55] because the text embedding of T5 can provide a more semantically nuanced distribution and is suitable as a supervisory signal to guide the alignment of visual modality distributions to the semantic space, rather than just as a representation of relative similarity in contrastive learning. This means that the"}, {"title": "4.2 3D organ/tumor segmentation", "content": "4.2.1 Curation of segmentation datasets\nWithin-domain segmentation datasets. The BraTS21 dataset [26], released as part of the 2021 RSNA-ASNR-MICCAI Brain Tumor Segmentation Challenge, consists of multi-institutional, multi-modal MRI scans (T1-w, T1 postcontrast, T2-w, FLAIR) from patients with glioblastoma or lower-grade glioma. Each case includes expert annotations delineating tumor subregions (enhancing tumor, edema, and necrotic core). The core challenge portion provides 1,251 labeled scans for training. The MSD-BrainTumour dataset [27] (Task 01 in the Medical Segmentation Decathlon) includes 484 preoperative multi-modal MRI scans (T1-w, T1 postcontrast, T2-w, FLAIR) sourced primarily from earlier BraTS collections. For BreastDM [28], the original publication reports a new breast MRI dataset (with dynamic contrast-enhanced volumes) consisting of 232 scans. The Prostate158 dataset [29] offers 158 MRI scans (T2, ADC, DFI) with detailed prostate annotations. Finally, the MSD-Prostate dataset [27] (Task 05 in the MSD) contains 32 T2-w, ADC map, and DFI scans with corresponding prostate zonal annotations (central gland and peripheral zone).\nOut-of-domain segmentation datasets. The MM-WHS-MRI subset [30] from the Multimodality Whole Heart Segmentation challenge consists of around 20 annotated 3D MRI volumes of the heart. ATLAS\u2013MRI [31] is a publicly available dataset of contrast-enhanced MRI for hepatocellular carcinoma (HCC), which consists of 60 scans. The Abdomen-1K [32], released as part of the MICCAI FLARE 2022 Playground subtask 1, includes a training set adapted from MSD Pancreas (281 cases) and NIH Pancreas (80 cases), where all 361 CT scans are from the portal phase. Kipa22 [33] comes from the Kidney PArsing Challenge 2022, and its goal is to segment 3D kidneys, renal tumors, arteries, and veins. It released 70 training sets with detailed annotations. Lastly, the remaining MSD tasks, namely MSD-Pancreas (281 training scans), MSD-Liver (131 training scans), MSD-Heart (20 training scans), MSD-Hippocampus (260 training scans), MSD-Lung (63 training scans), MSD-HepaticVessel (303 training scans), MSD-Spleen (41 training scans), and MSD-Colon (126 training scans), do not provide official validation sets. For all the above datasets, we keep the same split method as provided by VoCo [16]."}, {"title": "4.2.2 Fully supervised finetuning with nnUNet framework", "content": "In our image segmentation experiments, we adopt nnUNetv2 as a unified framework to ensure consistent data preprocessing for fair comparisons across different models. Within this framework, we have implemented Swin-Transformer Base/Large/Huge networks, thereby aligning the training protocols. For each publicly available dataset with detailed annotations, nnUNetv2's built-in code is used to perform a 5-fold cross-validation split. Note that, in order to make a fair comparison with VoCo, we report the Oth fold in the 5-fold cross-validation. Throughout training, nnUNet models are trained for 300 epochs, while Swin-Transformer models are trained for 150 epochs, and we select the model with the highest validation performance for final evaluation. We employ an SGD optimizer with an initial learning rate of 0.01, following nnUNet's default decay schedule. VoCo-SSL pretrained weights are sourced from the code library. Because our experimental setup closely matches that of VoCo, some of the results reported here are derived from the extended version of the original publication [16]."}, {"title": "4.3 Organ/cancer classification", "content": "4.3.1 Curation of classification datasets\nWithin-domain classification datasets. The ADNI dataset [40] (Alzheimer's Disease Neuroimaging Initiative) is a longitudinal, multi-center, observational study that includes thousands of participants, from cognitively normal (CN) to those with mild cognitive impairment (MCI) or Alzheimer's disease (AD). In this study, we use a dataset consisting of participants who have screening, 6-month, 1-year, 18-month (MCI only), 2-year, and 3-year (normal and MCI only) scans, which is called \u201cADNI1_Complete 3Yr 1.5T,\u201d totaling 2,182 samples. Consistent with the literature [56], the training set, validation set, and test set contain 1,526, 326, and 330 samples, respectively. We use NPPY [57] and its available pre-trained weights to convert raw MRI scans into uniformly sized skull-stripped,"}, {"title": "4.3.2 Fully supervised finetuning with linear classifier", "content": "As shown in Fig. 5 (a), we use the parameters saved in the pretraining phase as the initial parameters of the encoder, perform an average pooling operation on the output of the last layer of the encoder, and then input it into a two-layer linear classifier to predict the probability distribution of the category. In classification experiments, we set the ADNI dataset input size to 96\u00d796\u00d796, while all other datasets are resized to 64\u00d764\u00d764. Classifiers based on the Swin-Transformer are trained for 150 epochs, and those based on the 3D UNet are trained for 300 epochs. In each experiment, we report the best result. We employ a learning rate of 1e-3, with the Adam optimizer, following a cosine decay schedule. Additionally, the first 5 epochs are used for warmup to stabilize training."}, {"title": "4.4 3D medical image registration", "content": "4.4.1 Curation of registration datasets\nWithin-domain registration datasets. The IXI dataset [47] consists of over 576 T1-weighted brain MRI scans from healthy volunteers collected at three different hospitals in London. Following the TransMorph [45] protocol, we use 403 scans as the training set, 58 as the validation set, and 115 as the test set. The volumes are cropped to 160\u00d7192\u00d7224. Thirty annotated structures were used for evaluation. The OASIS dataset [48] (Open Access Series of Imaging Studies) includes 413 T1-weighted brain MRI scans from participants aged 18 to 96, with both healthy controls and patients exhibiting mild to moderate cognitive impairment. The original MR volumes are preprocessed using FreeSurfer [58], which includes spatial normalization, skull stripping, affine transformations, and automatic structural segmentation. Following the TransMorph [45] protocol, we use 394 scans as the training set and 19 scans as the validation set. Since there is no test set available, we employed the validation set for evaluation. The volumes are cropped to 160\u00d7192\u00d7224. 35 structures are used as ground truths to evaluate the performance.\nOut-of-domain registration datasets. The ACDC dataset [49] (Automated Cardiac Diagnosis Challenge) comprises 150 cardiac MRI scans in short-axis view, covering subjects with various heart conditions. The original challenge reserves 100 scans for training and 50 for testing. The volumes are cropped to 160\u00d7192\u00d7224."}, {"title": "4.4.2 Fine-tuning TransMorph/SwinUNETR for image registration", "content": "For fine-tuning of TransMorph, we replaced the \"Transformer Encoder\" in the original framework with our own Swin-Transformer Encoder, loading the weights of Triad-L. The rest of the components, such as \u201cCNN Decoder,\u201d \u201cAffine Network,\u201d and \u201cSpatial Transform,\u201d are randomly initialized. For fine-tuning of SwinUNETR, we load the weights of Triad-B into the encoder and randomly initialize the UNETR decoder. The pre-trained weights of SwinUNETR and SupreM are obtained from the code repository provided by VoCo. Due to limited resources, we only fine-tune for 200 epochs on each set of experiments and select the best-performing results for reporting. The Adam optimizer is used for fine-tuning, and the batch size was 1. The learning rates for OASIS and IXI are 0.00005, while the learning rate for ACDC is 0.0001. The remaining parameters, such as the type of loss function and weight factor, remain consistent with the default settings in the code provided by TransMorph."}, {"title": "4.5 Computing hardware and software", "content": "We use pydicom 3.0.1 and dicom2nifti 2.5.0 for 2D slice sequences and 3D volume data preprocessing. We use Python 3.10.13 for all experiments and analyses in the study. For the pretraining stage, we use the AdamW [59"}]}