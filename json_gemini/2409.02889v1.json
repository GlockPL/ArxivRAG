{"title": "looongLLaVA: Scaling Multi-modal LLMs to\n1000 Images Efficiently via Hybrid Architecture", "authors": ["Xidong Wang", "Dingjie Song", "Shunian Chen", "Chen Zhang", "Benyou Wang"], "abstract": "Expanding the long-context capabilities of Multi-modal Large Language Mod-\nels (MLLMs) is crucial for video understanding, high-resolution image under-\nstanding, and multi-modal agents. This involves a series of systematic optimiza-\ntions, including model architecture, data construction and training strategy, par-\nticularly addressing challenges such as degraded performance with more images\nand high computational costs. In this paper, we adapt the model architecture to a\nhybrid of Mamba and Transformer blocks, approach data construction with both\ntemporal and spatial dependencies among multiple images and employ a progres-\nsive training strategy. The released model LongLLaVA (Long-Context Large\nLanguage and Vision Assistant) is the first hybrid MLLM, which achieved a bet-\nter balance between efficiency and effectiveness. LongLLaVA not only achieves\ncompetitive results across various benchmarks, but also maintains high through-\nput and low memory consumption. Especially, it could process nearly a thousand\nimages on a single A100 80GB GPU, showing promising application prospects\nfor a wide range of tasks.", "sections": [{"title": "Introduction", "content": "The rapid advancement of MLLMs (Liu et al., 2024b, 2023a; Dong et al., 2024a; Chen et al., 2024a)\nhas demonstrated their remarkable capabilities across various applications (Chu et al., 2024; Yang\net al., 2023; Wu et al., 2023b; Chen et al., 2024b). However, multi-image scenario remain an impor-\ntant yet to-be-explored aspect. In particular, expanding the context of MLLMs to understand longer\nvideos (Zhang et al., 2023; Cheng et al., 2024b), higher-resolution images (Xu et al., 2024c; Wu &\nXie, 2023b), and make decisions based on more historical messages (Wang et al., 2024; Liu et al.,\n2024c) is crucial for enhancing user experience (Li et al., 2024b) and further broadening MLLMs'\napplication scope (Apple, 2024).\nHowever, extending the context length of MLLMs to improve their usability poses challenges related\nto degraded performance and high computational costs when processing more images. To maintain\nthe performance in longer context, some studies (Zhang et al., 2024a; Zhao et al., 2024c) have con-\ncentrated on curating long-context training data involving multiple images to enhance performance.\nAdditionally, other research efforts have explored innovative training strategies (Liu et al., 2024a;\nZhang et al., 2024b; Li et al., 2024a; Zhang et al., 2024d) to mitigate performance declines. Re-\ngarding the issue of high computational costs, Xue et al. (2024) have made strides in improving\nmulti-node efficiency by reducing communication costs. However, there remains a gap in solutions\nfor accelerating the computation itself when managing longer contexts.\nTo address the challenges mentioned above, we propose a systematic solution called LongLLaVA,\nespecially using a hybrid architecture for acceleration. This solution comprehensively optimizes\nacross three dimensions:Multi-modal architecture, Data construction, and Training strategy."}, {"title": "Towards Scaling up the Image Number in MLLMs", "content": null}, {"title": "The Curse of Image Numbers", "content": "The model's input length increases rapidly with the number of images, leading to issues such as\ndegraded model performance and high inference costs. With the evolution of MLLM technology,\nmany existing open-source MLLMs have demonstrated capabilities on par with closed-source mod-\nels in single-image tasks (Bai et al., 2023; Li et al., 2024a; Zhang et al., 2024a; OpenAI, 2024;"}, {"title": "The Benefit of Scaling Up the Image Number", "content": "Adopting more images significantly broadens the application scenarios for current MLLMs. We will\nexplore this from two dimensions: Temporal Expansion and Spatial Expansion.\nTemporal Expansion. Understanding the temporal dependencies between images is crucial for a\nvariety of applications. In multi-modal assistants, it enhances real-time recall capabilities, which is\nparticularly beneficial for the elderly (Li et al., 2024b; Loveys et al., 2022). For mobile agents, it\nenables more personalized services and improves task planning (Deng et al., 2024; Li et al., 2024d;\nWu et al., 2023a). In the healthcare sector, it assists in anomaly detection in 3D medical videos,\nthereby reducing diagnostic errors (Bai et al., 2024a).\nSpatial Expansion. When dealing with high-resolution images (Xu et al., 2024c; Dong et al.,\n2024b) or when detailed understanding of images (Wu & Xie, 2023b) is required, images are often\ndecomposed into sub-images. This process highlights the importance of grasping spatial dependen-\ncies among these sub-images. In remote sensing, an increased number of images enhances both\ncoverage and granularity (Guo et al., 2024; Liu et al., 2022). In pathology, it minimizes information\nloss and improves diagnostic accuracy (Sun et al., 2024; Xu et al., 2024a). In the field of Molecu-\nlar Learning, it facilitates the processing of complex reactions and the analysis of larger molecular\ngraphs (Zhang et al., 2024c; Le et al., 2024)."}, {"title": "LongLLaVA: Scaling LLaVA to Longer Context", "content": "To address the aforementioned challenges and enhance the model's adaptability to long-context,\nmulti-image scenarios, we introduce improvements from three perspectives: multi-modal model\narchitecture (Sec. 3.1), data processing protocol (Sec. 3.2), and training strategy (Sec. 3.3)."}, {"title": "Multi-modal Architecture", "content": "Our multimodal architecture is constructed around three core components inspired by LLaVA (Li\net al., 2024a): the Vision Encoder, the Projector, and the LLM."}, {"title": "Data Processing Protocol", "content": "To ensure that the model effectively distinguishes between temporal and spatial dependencies among\nimages in multi-image scenarios and performs well across various tasks, we meticulously differenti-\nated special characters in different scenarios. As shown in Figure 3, these special characters compre-\nhensively address the various relationships between images in different contexts, thereby enhancing\nthe model's adaptability to diverse tasks.\nRegular Single and Multiple Images: For regular single and multiple image inputs, we use <img>\nand </img> to enclose image tokens, helping the model differentiate between image and text tokens.\nVideo: For video inputs, to enable the model to understand the temporal relationship between\nframes, we first use <vid> and </vid> to enclose image tokens. Additionally, we add the spe-\ncial symbol  between different frames to represent the temporal dependency between them.\nHigh Resolution Image: For complex single-image understanding that need to divide image into\nmultiple sub-images, we use \\n to separate the main image from its sub-images. For the arrangement\nof sub-images, we traverse from the top-left to the bottom-right, adding \\n between split lines to\npreserve the relative spatial positions of the sub-images."}, {"title": "Training Strategy", "content": "In our training strategy, we implement single-modal and multi-modal adaptations to transform a\npre-trained language model into a multimodal long-context model."}, {"title": "Experiments", "content": null}, {"title": "Training Details.", "content": "For training, we utilize random sampling to concatenate data items into a token length of 40,960,\nseparated by the <eos> token. This approach helps in managing extensive datasets and ensuring\ndiverse coverage of different data segments. Training is executed across three compute nodes, each\nequipped with eight A800 GPUs, leveraging DeepSpeed Zero-3 as the distributed strategy to en-\nhance scalability and efficiency. We employ a cosine learning rate scheduler with a warm-up rate of\n0.03, set the training epoch to 1, and the learning rate to 1e-5. This configuration is designed to\nbalance learning speed and model convergence effectively. ."}, {"title": "Evaluation Setup", "content": "We evaluate the model's multimodal long-context understanding capabilities using the multi-image\nevaluation and conduct single-image evaluations to explore basic functionalities. For details on\nthe single-image evaluation, please refer to Appendix B. Both LongLLaVA (single image) and\nLongLLaVA are evaluated using Int8 quantization with the temperature set to zero to ensure con-\nsistency in performance evaluation. The use of Int8 quantization helps in reducing the model size\nand computational load while maintaining performance accuracy."}, {"title": "Main Results", "content": "As shown in Table 2, LongLLaVA demonstrates superior performance among open-source models\non MileBench, even surpassing Claude3-Opus, and particularly excels in retrieval tasks. This high-\nlights LongLLaVA's impressive capabilities in handling multi-image tasks. Notably, LongLLaVA's\neffectiveness is further underscored by its performance on video benchmarks such as Video-MME\nand MVBench. It shows exceptional results, especially in tasks involving medium to long-length\nvideos, outperforming traditional video models like Video-LLaMA2 and VideoChat2.\nRemarkably, despite achieving these impressive results, LongLLaVA operates with an order of mag-\nnitude fewer FLOPs compared to other models. This efficiency in computational resources not only\nunderscores LongLLaVA's advanced performance but also its optimization in resource management.\nThese results reflect a significant advancement in the research community's efforts to close the per-\nformance gap with commercial models."}, {"title": "Diagnostic Evaluation of Long-Context MLLMs", "content": "Considering that former evaluations cannot adequately capture the abilities of MLLMs over long\ncontexts, we employ a new diagnostic evaluation set, VNBench (Zhao et al., 2024e), to further\nanalyze the atomic capabilities of models in long contexts. VNBench is a benchmark construction\nframework based on synthetic video generation, encompassing tasks such as retrieval, ordering, and\ncounting."}, {"title": "Ablation Study", "content": "As shown in Table 4, significant improvements were observed across all evaluation sets when using\nthe hybrid LLM architecture, Jamba, with identical data and model parameters, demonstrating its\npotential in multimodal scenarios. For token compression, we choose the 2D pooling, which signif-\nicantly reduces computational load while keeping performance degradation within acceptable limits\n(less than 2.2%). Compared to 1D pooling, the 2D pooling method, which pools along the height and\nwidth directions to obtain a 12x12 token arrangement, yields better results (0.1~1.5 improvement).\nRegarding data construction, after training on our single-image data, the model achieved a 1.5%\naccuracy improvement on SEEDBench and 12.3% on MileBench. Subsequent multi-image training\nled to a further 7.4% increase on MileBench, validating the dataset construction's effectiveness."}, {"title": "More Analysis", "content": "In this section, we conduct further analysis to understand the inner workings and multimodal long-\ncontext capability of LongLLaVA."}, {"title": "On the Motivation for the Hybrid Architecture.", "content": "We explore the strengths and weaknesses of different architectures in terms of ICL capabilities and\ninference efficiency, highlighting the balanced advantages of multimodal hybrid architectures that\ncombine the strengths of both. For Mamba, we select Cobra (Zhao et al., 2024a), which, to the best\nof our knowledge, is the first and only multimodal architecture to use Mamba as its LLM within the\nLLaVA framework. For Transformer, we choose the 13B parameter LLaVA-1.6, which has inference\nparameters consistent with LongLLaVA, to enable a more accurate efficiency comparison.\nICL Analysis. We evaluate the performance on the Matching Image task from VL-ICL bench-\nmark (Zong et al., 2024) for multi-modal in-context learning. This task's inputs contain an im-\nage pair x = {x1,x2}, and output y indicates whether a specific relation r holds between them."}, {"title": "Scaling Law of the Image Number", "content": "With more images processed, it could support more image patches for high-resolution image un-\nderstanding and more video frames for video understanding. To explore the impact of increas-\ning the number of sub-images and video frames, we evaluate LongLLaVA on the benchmarks V*\nBench (Wu & Xie, 2023a) and Video-MME (Fu et al., 2024a) respectively."}, {"title": "Further Scaling up the image number to 1000", "content": "Using the V-NIAH evaluation framework proposed in LongVA (Zhang et al., 2024b), we conduct\na needle-in-the-haystack test to evaluate the model's performance. Given the model's training se-\nquence length limit of 40,960 tokens, we apply token pooling techniques to reduce the original token\ncount from 144 to 36. This adjustment allows us to test the model's ability to retrieve relevant infor-\nmation from a large dataset efficiently. As shown in Figure 7, LongLLaVA achieves nearly 100%\nretrieval accuracy on a set of 1000 images without requiring additional training.\nHowever, when we increase the number of images in the test more than 1,000, we observe a de-\ncline in retrieval accuracy. This drop in performance may be due to exceeding the model's training\nsequence length, which potentially affects its ability to maintain accuracy with more images. In"}, {"title": "Conclusion", "content": "In this study, we introduce LongLLaVA (Long Context Large Language and Visual Assistant), an\ninnovative hybrid architecture model that excels in long-context multi-modal understanding. The\nmodel integrates Mamba and Transformer blocks, leveraging temporal and spatial dependencies\nbetween multiple images to construct data, and employs a progressive training strategy. LongLLaVA\ndemonstrates competitive performance across various benchmarks while ensuring efficiency, setting\na new standard for long-context Multi-modal Large Language Models (MLLMs)."}]}