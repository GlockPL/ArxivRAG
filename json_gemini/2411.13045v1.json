{"title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce Relevance Learning", "authors": ["Gang Zhao", "Ximing Zhang", "Chenji Lu", "Hui Zhao", "Tianshu Wu", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "abstract": "Effective query-item relevance modeling is pivotal for enhancing user experience and safeguarding user satisfaction in e-commerce search systems. Recently, benefiting from the vast inherent knowledge, Large Language Model (LLM) approach demonstrates strong performance and long-tail generalization ability compared with previous neural-based specialized relevance learning methods. Though promising, current LLM-based methods encounter the following inadequacies in practice: First, the massive parameters and computational demands make it difficult to be deployed online. Second, distilling LLM models to online models is a feasible direction, but the LLM relevance modeling is a black box, and its rich intrinsic knowledge is difficult to extract and apply online. To improve the interpretability of LLM and boost the performance of online relevance models via LLM, we propose an Explainable LLM-driven Multi-dimensional Distillation framework for e-commerce relevance learning, which comprises two core components: (1) An Explainable LLM for relevance modeling (ELLM-rele), which decomposes the relevance learning into intermediate steps and models relevance learning as a Chain-of-Thought (CoT) reasoning, thereby enhancing both interpretability and performance of LLM. (2) A Multi-dimensional Knowledge Distillation (MKD) architecture that transfers the knowledge of ELLM-rele to current deployable interaction-based and representation-based student models from both the relevance score distribution and CoT reasoning aspects. Through distilling the probabilistic and CoT reasoning knowledge, MKD improves both the semantic interaction and long-tail generalization abilities of student models. Extensive offline evaluations and online experiments on Taobao search ad scene demonstrate that our proposed framework significantly enhances e-commerce relevance learning performance and user experience.", "sections": [{"title": "1 INTRODUCTION", "content": "With the widespread development of web, billions of people are now able to purchase desired items through e-commerce platforms such as Taobao\u00b9 and Amazon\u00b2. Search engines play a crucial role in this landscape, enabling users to discover preferred products. Therefore, user experience is a very important part. It is indispensable to assess the semantic relevance between queries and products in order to filter out irrelevant items, thereby improving user experience and safeguarding long-term user satisfaction [3].\nDue to the significant importance, text relevance learning has attracted considerable attention from researchers. Current neural-based relevance learning approaches can be mainly divided into two paradigms: representation-based models and interaction-based models. The representation-based models [16, 18, 20, 24, 29, 38, 55] focus"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 E-Commerce Relevance Learning", "content": "E-Commerce relevance Modeling is fundamentally a text matching task, which has been extensively studied due to its pivotal role in information retrieval and search engine. Early methods such as TF-IDF [37] and BM25 [40] typically relies on manually designed statistical keyword features. Though computationally efficient, these approaches cannot capture deeper semantic relationships and are limited in handling variations such as synonyms and misspellings.\nWith the development of deep learning and pre-trained language models [1, 28, 45, 54], neural-based relevance learning methods have gained prominence by more effective semantic modeling and better generalizability. These methods can be broadly categorized into two paradigms: representation-based models and interaction-based models. The representation-based models, usually adopting dual-encoder architectures, focus on obtaining high-quality semantic representations for queries and items, and then subsequently compute relevance scores through similarity measures or lightweight late-interaction modules. A classic example is the Deep Structured Semantic Model (DSSM) [15], which uses separate deep fully connected networks to generate embeddings for queries and items. Subsequently, LSTM-DSSM [30] and LSTM-RNN [31] incorporate RNNs to model sequential dependencies within the text. Afterwards, The integration of pre-trained language models further advances the state of the art in e-commerce relevance learning. Sentence-BERT [39] adopts a BERT-based siamese-like architecture which encodes queries and items separately and measures the similarity of sentence representations. PolyEncoder [17] projects the query embedding into multiple spaces with learnable vectors to represent more global features. Khattab et al. [21] proposes a token-level late interaction module to further enhance the information interaction between queries and items. ReprBERT [55] distills the knowledge of interaction-based BERT into representation-based model and promotes the interaction via the intermediate interaction representation. DeepBoW [25] creates high-dimensional representations with the same dimensions as the vocabulary for queries and items, aiming at capturing detailed semantic information and better explainability. These representation-based methods allow for pre-computing embeddings, making them highly efficient for deployment in online large-scale e-commerce search engines. However, the absence of fine-grained semantic interaction results in performance limitations of current online approaches, especially pronounced in handling long-tail samples.\nThe interaction-based models facilitate information interaction between queries and items during the semantic encoding process or integrate sophisticated interaction modules, which enables them to achieve robust performance. Architectures such as ARC-II [14] and MatchPyramid [32] utilize convolutional neural networks (CNN) [23] to capture rich hierarchical matching patterns from the matching matrix. Match-SRNN [48] further introduces recurrent neural network [9] to model the recursive matching structure and capture long-distance dependency between the interactions. DecompAtt [34] incorporates the attention mechanism to enhance interaction and alignment between query and item tokens. Additionally, BERT [7] achieves the state-of-the-art performance by enabling deep interactions between query-item pairs during"}, {"title": "2.2 Knowledge Distillation of LLM", "content": "Knowledge distillation refers to the process of transferring knowledge from a large, complex teacher model to a smaller, more efficient student model. This technique is pivotal in addressing the computational challenges and resource limitations associated with deploying large-scale models in practical applications. Most existing approaches focus on the knowledge distillation between homogeneous LLM architectures with different amount of parameters, which can be divided into two categories. Firstly, the conventional knowledge distillation methods, aimed at enabling the student LLM to achieve performance comparable to a larger teacher LLM on various tasks. These methods include employing teacher LLM to annotate data for student LLM [11, 41, 52], directly expanding the dataset [4, 12], or fitting hidden features of student and teacher for deeper knowledge transfer [10, 44]. The second group of approaches aim at transferring the specific abilities of teacher LLM to student LLM, such as instruction follwing [36, 51], multi-turn dialogue [49], or retrieval-augmented generation [2, 26] abilities. Despite the achievements, the aforementioned methods primarily focus on the distillation of general knowledge and capabilities between LLMs, offering limited assistance for online industrial scenes like e-commerce relevance learning.\nTo better utilize the knowledge of LLMs, several heterogeneous distillation methods are proposed for information retrieval. Srinivasan et al. [42] proposes a two-stage framework for query rewriting which trasnfers the knowledge of LLM through annotating unlabeled data. Dai et al. [5] generates pseudo-queries for unlabeled documents based on a small number of demonstrations via LLM for dense retrieval. RankGPT [43] leverages GPT-4 to generate permutations for a group of candidate passages for the reranking task. Though gaining improvements, current data augmentation methods have not explored deeper knowledge of LLMs, such as reasoning knowledge contained in the CoT ability or fine-grained relevance ranking knowledge reflected by output probability distribution. How to enhance the performance of online heterogeneous models via LLM remains an unresolved issue."}, {"title": "3 METHODOLOGY", "content": "As Figure 2 shows, the proposed framework consists of two components: Explainable LLM for relevance learning (ELLM-rele) and Multi-dimensional LLM Knowledge Distillation (MKD) module. In this section, we first introduce how we originally transform relevance learning to CoT reasoning and define multi-aspect structured CoT knowledge based on the natural application of relevance discrimination in Sec 3.1. Then, we demonstrate the detail of constructing the ELLM-rele with stable CoT reasoning ability on relevance learning in Sec 3.2. Finally, we introduce the proposed knowledge distillation approach of ELLM-rele from both the score distribution and CoT reasoning dimensions in Sec 3.3."}, {"title": "3.1 Relevance Learning as CoT Reasoning", "content": "Focused on transforming the relevance learning task to CoT reasoning for explicitly modeling and fully exploiting knowledge of LLM, we originally define multi-aspect structured CoT knowledge based on the natural application of relevance discrimination criteria. For easy understanding, we introduce some key notions here:\n1) E-commerce Relevance Learning task: Given a user query $Q = {q_i\\}_{i=1}^{l_q}$, and an item with title $T = {t_i\\}_{i=1}^{l_t}$, where $q_i$ and $t_i$ are constituent tokens, the relevance task aims at acquiring a semantic similarity score s(Q, T) indicating the query-item pair belongs to $y \\in {Good, Bad}$ relevance situation.\n2) Chain-of-Thought [53]: The ability of LLM to improve reasoning and decision-making by breaking down complex problems into a sequence of interconnected, logical steps, thereby enhancing the interpretability and accuracy of the model output.\n3) Relevance Learning as CoT reasoning: To improve the interpretability and performance of LLM-based relevance method, we transform the relevance task from text classification to CoT reasoning. Given the business attributes of e-commerce scenarios, the relevance discrimination criteria can be decomposed into several fine-grained aspects such as category, brand, so the relevance task can be viewed as the matching of subproperties in the item and query. If any of the aspects do not match, we can get a fine-grained result such as \"Bad - Brand mis-match\", and the final judgment will be obtained as Good only if all the properties are satisfied. Based on such insight, the CoT construction of a query-item pair can be explicitly represented as: 1) attributes extracted from query, 2) attributes extracted from item title with match result to the same attribute from query, 3) final judgement as Good or Bad concluded from all attribute-level match results, as demonstrated in Figure 2. Such a design allows us to explicitly model e-commerce relevance learning as explainable CoT reasoning, not only improving the interpretability and performance of LLM-based model, but also providing additional fine-grained knowledge for further exploring. Details about the aspect definition are shown in Appendix A."}, {"title": "3.2 Explainable LLM for Relevance Learning", "content": "In this section, we introduce Explainable LLM for relevance learning (ELLM-rele) to explicitly model the relevance task as CoT reasoning, which involves two steps: high-quality CoT annotation generation and supervised fine-tuning of ELLM-rele."}, {"title": "3.2.1 In-context Learning-based CoT Annotation", "content": "We aim to leverage the CoT capacity of LLM for fine-grained extraction and discriminate relevance, where the model capacity is positively correlated with the number of model participants. In our business scenario, we need to perform daily inference on millions of data. Limited to the business requirement and computing resource, we can only choose the LLM around 7B parameters scale, which has insufficient CoT ability to meet the demands of the task without fine-tuning. Therefore, we consider generating CoT Annotations for existing labeled dataset by leveraging the few-shot capability of larger LLM(>70B) noted as L-LLM and enhance the 7B model capability by distillation. We choose two larger models Qwen2-72B [54] and LLama3-70B[8] to generate CoT Annotations, which avoids the generation of an overly homogeneous distribution of CoT data. In detail, Given the"}, {"title": "3.2.2 Supervised Fine-tuning of ELLM-rele", "content": "Limited by the number of parameters, the CoT capability of the original 7B LLM is not enough to accomplish the extraction and correlation discrimination tasks well. Thus we intend To enhance 7B LLM's CoT ability by data distillation based on the CoT annotation obtained from the previous Section. In detail, We take the 7B LLM as student student and the CoT annotation as its learning target. A standard SFT paradigm is employed to fine-tune 7B LLM and the negative log-likelihood loss function is optimized as follows:\n$L_{LLM} = -\\frac{1}{N}\\sum_{i=1}^{N} log P(C_i |S, Q_i, T_i)$\nIn contrast to $P_{L-LLM}$, we only concatenate S, $Q_i$, and $T_i$ as the prompt $P_{LLM}$ for model fine-tuning, rather than incorporating few-shot examples. This choice is based on experimental findings that, while adding examples provides only marginal improvements to the model's performance but significantly increases inference time."}, {"title": "3.3 LLM-driven Multi-dimensional Knowledge Distillation", "content": "In this section, we detail the approach of distilling score distribution and CoT knowledge from ELLM-rele to specialist student models."}, {"title": "3.3.1 Relevance Score Distribution Distillation", "content": "Through supervised fine-tuning, ELLM-rele can learn the output template of relevance reasoning and provide a relevance decision of \"Good\" or \"Bad\" at the end of the inference sequence, which we regard as relevance judgement token. Considering that the probabilities of judgement tokens contain information about the fine-grained relevance degree, we convert them into relevance scores to guide student model training and narrow its performance gap with the teacher model.\nData Sampling and Pseudo Label Prediction. To sufficiently dig the knowledge of teacher model, we sample real-world unlabeled data in addition to human-annotated data for pseudo labeling"}, {"title": "Relevance Score Projection", "content": "To isolate the relevance information from other tokens and mitigate the influence of non-relevant outputs, we focus solely on the probabilities assigned to the Good and Bad tokens. Assuming $pr (Good | Q_i, T_i)$ and $pr (Bad | Q_i, T_i)$ denote the probablities of relevance judgement token, we transform them into continuous relevance scores using probability normalization to ensure the scores are bounded between 0 and 1:\n$S_T (Q_i, T_i) = \\frac{e^{P_T (Good | Q_i,T_i)}}{e^{P_T (Good | Q_i,T_i)} + e^{P_T (Bad | Q_i,T_i)}}$"}, {"title": "Score Distillation via KL Divergence", "content": "The distillation objective is to align the student relevance score distribution with that of the teacher ELLM-rele, we model the student score distribution $p_s (y | Q_i, T_i)$ to mimic the teacher $s_T (Q_i, T_i)$ by minimizing the Kullback-Leibler (KL) divergence between them:\n$L_{score} = \\frac{1}{N}\\sum_{i=1}^{N} KL (S_T (Q_i, T_i) || p_s (y | Q_i, T_i))$\nBy minimizing $L_{score}$, the student model learns to not only align with the teacher's binary relevance decisions but also inherits the deeper probabilistic knowledge."}, {"title": "3.3.2 CoT Knowledge Distillation", "content": "To further explore the inherent knowledge of LLM, we incorporate the distillation of Chain-of-Thought (CoT) reasoning knowledge from the teacher ELLM-rele in addition to relevance score distillation. By modeling e-commerce relevance learning as an explainable CoT reasoning, ELLM-rele generates intermediate reasoning steps that elucidate the decision-making process leading to the final relevance judgment of \"Good\" or \"Bad\". These reasoning steps demonstrate fine-gained semantic interactions between the query and item title, highlighting specific relevance evidences (e.g., category, brand, etc.) appeared in query and titles as well as their alignment situation. We perform regular parsing on CoT sequences as fine-grained token-level supervision signals, and inject the interaction knowledge into the student model through auxiliary distillation tasks. Considering the scalability to adapt to both interaction-based and representation-based student paradigms, we design two kinds of CoT distillation tasks:"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Dataset", "content": "We conduct extensive experiments on large-scale human annotated e-commerce relevance dataset to validate the effectiveness of our proposed ELLM-rele and MKD architecture. The dataset contains more than 400 thousand query-item pairs sampled from the Taobao ad search logs, and then labeled Good (relevant) or Bad (irrelevant) by experienced human annotators. We divide the dataset into training, validation, and test sets for model fine-tuning and evaluation. For knowledge distillation, we collect an additional 30 million unlabeled query-item pairs randomly sampled from the Taobao search logs. The average length of queries and item titles are 6 and 34 Chinese characters. Detailed data statistics are shown in Table 1."}, {"title": "4.2 Baseline and Evaluation Metrics", "content": ""}, {"title": "4.2.1 Baselines", "content": "To comprehensively evaluate the effectiveness and generalizability of our proposed ELLM-MKD framework, we adopt the following diverse set of state-of-the-art baseline models across different relevance modeling paradigms. 1) BERT [7], an interaction-based method that jointly encodes queries and items using a bidirectional Transformer architecture to capture deep semantic interactions during the encoding process. 2) Sentence-BERT [38], a representation-based method which encodes queries and items separately into fixed-size embeddings for efficient computation of cosine similarity. 3) Poly-Encoder [18], a representation-based method which projects query embeddings into multiple vector spaces via learnable vectors to capture diverse global features. 3) ColBERT [20], a representation-based method that further improves the query-item interaction by employing a token-level late interaction mechanism. 4) ReprBERT [55], which proposes an intermediate interaction module and leverages knowledge distillation from interaction-based model to enhance semantic representations of queries and items. 5) Warmart-LLM [27], a LLM-based method that fine-tunes a 7B decoder-only large language model by directly generating the relevance judgments for queries and items."}, {"title": "4.2.2 Evaluation Metrics", "content": "We evaluate our method using a combination of offline and online metrics to comprehensively evaluate its performance and industrial value. For offline evaluation, given the binary nature of human annotations in the relevance task, we treat the problem as a classification task. We employ the Receiver Operating Characteristic Area Under Curve, marked as ROC-AUC,"}, {"title": "4.3 Implementation Details", "content": ""}, {"title": "4.3.1 ELLM-rele Setup", "content": "Considering the robust performance and adequate model size of Qwen2-7B LLM[54], we choose it as the backbone model to build ELLM-rele. We fine-tune the Qwen2-7B via the annotated CoT dataset introduced in Section 4.1. To eliminate the impact of using different backbone LLMs, the experiments of Walmart-LLM[27] are also conducted based on Qwen2-7B for a fair comparison. Both models are trained on 16 H20 GPUs with a global batch size of 256 and using AdamW optimzier to optimize the model with lr=1e-5,$\u1e9e\u2081 = 0.9$,$\u03b22 = 0.999$,decay_rate=0.01. The models are trained for 8 thousand steps in a total of 10 hours. Besides, The self-consistency strategy is used in generating CoT annotations to improve the accuracy of Larger LLM. To balance the generation of diversity and accuracy, we set hyper-parameter TopP as 0.7, TopK as 50 and temperature T as 0.8 following previous work[50]."}, {"title": "4.3.2 Multi-dimentional Knowledge Distillation Setup", "content": "To evaluate the effectiveness and generalizability of MKD on various relevance models, we perform MKD on BERT, Sentence-BERT, PolyEncoder and ColBERT baselines in offline experiments. The sequence length of queries and item titles are set to 32 and 64, the batchsize is set to 256 for all baselines and our MKD models. All models are trained via Adam optimizer with a learning rate of 5e-6 on 32 Nvidia H20 GPUs. For CoT knowledge distillation, we perform sequence taggging on"}, {"title": "4.4 Main Results", "content": "Table 2 presents the main experimental results comparing our proposed ELLM-rele and MKD against the baseline models. From the results, we can discern the following key observations:\nThe superiority of LLM-based method. Both ELLM-rele and Warmart-LLM, achieve substantially higher performance compared to expert models. Specifically, ELLM-rele surpasses the interaction-based model BERT by 5.3% on ROC-AUC and 5.2% on Neg PR-AUC. Additionally, ELLM-rele outperforms representation-based models such as Sentence-BERT by 14.0% and Poly-Encoder by 10.8% on ROC-AUC, and by 16.9% and 13.0% on Neg PR-AUC, respectively. These results demonstrate the efficacy of leveraging LLMs for relevance modeling, capitalizing on their extensive pre-trained knowledge to achieve significant performance enhancements.\nEffectiveness of incorporating CoT to LLM. Specifically, ELLM-rele achieves an increase of 0.8% in ROC-AUC and 1.8% in Neg PR-AUC compared to Warmart-LLM. This indicates that modeling relevance as a CoT reasoning task not only enhances the model interpretability but also benefits the model performance from the fine-grained relevance discrimination and reasoning processes.\nEffectiveness and generalizability of MKD. Furthermore, our proposed Multi-dimensional Knowledge Distillation framework achieves significant performance enhancements on student models. Specifically, MKD improve ROC-AUC by 2.6% /4.1%/2.8%/2.2% and Neg PR-AUC by 3.3%/4.6%/3.9%/2.5% compared to each baseline, respectively. Compared to ReprBERT, which also employs knowledge distillation, MKD can enable the already disadvantaged Poly-Encoder model to surpass ReprBERT's performance by 2.0% on ROC-AUC, further revealing the potential of using LLM as the teacher model. The results validate the effectiveness of MKD in exploiting and transferring the knowledge from ELLM-rele to current specialized relevance models. Notably, MKD consistently delivers performance improvements across different model paradigms,"}, {"title": "4.5 Model Complexity", "content": "Considering industrial deployment, we evaluate the model complexity and computational efficiency using parameter size and training/inference time consumption. Table 3 presents the efficiency results of various models, from which we can observe that: Firstly, the introduction of MKD does not alter the model parameter size or inference time compared to their respective baselines. This is because knowledge distillation is applied solely during the training phase, ensuring that the deployment-phase efficiency remains consistent with the baseline models. The incorporation of auxiliary distillation tasks leads to a slight increase in training time, however it remains acceptable as the training is conducted offline, without impacting the online deployment efficiency. Regarding ELLM-rele, modeling relevance learning as a CoT reasoning task leads to an increase in the length of the output tokens. This augmentation elevates inference times. Given that LLMs are predominantly deployed offline, the acceptable increase in latency is outweighed by the substantial gains in model performance and interpretability."}, {"title": "4.6 Ablation Study", "content": "To further understand the contribution of each component in our ELLM-rele and MKD framework, we perform ablation study on the following model variants: (1) w/o CoT reasoning, an ELLM-rele variant that removes the CoT reasoning process, modeling the relevance task as directly outputting relevance judgment tokens based on query-item pair. (2) w/o score distillation, a MKD variant that removes the score distribution distillation process by setting $\u03bb\u2081 = 0$, performing only hard label distillation of ELLM-rele. (3) w/o CoT distillation, a MKD variant that removes the CoT distillation process by setting $\u03bb\u2082 = 0$. (4) w/o score&CoT, a MKD variant that removes both the score and CoT distillation by setting $\u03bb\u2081 = \u03bb\u2082 = 0$. (5) w/o MKD, which completely removes all unlabeled data with pseudo label generated by ELLM-rele.\nThe ablation study results are presented in Table 4. From the results, we can observe that:\n(1) Modeling the relevance judgment task as a CoT reasoning process results in an improvement of +0.8% in ROC-AUC and +1.8% in Neg PR-AUC for ELLM-rele. This demonstrates that CoT not only enhances the interpretability of the LLM-based relevance model but also contributes to performance gains by enabling finer-grained relevance discrimination and reasoning.\n(2) Utilizing soft label distillation, where the language model's relevance judgment token probabilities are transformed into relevance scores, leads to an improvement of +0.9%/+0.6% in ROC-AUC and +1.6%/+0.9% in Neg PR-AUC for MKDCOIBERT/BERT. This validates the superiority of score distribution distillation over hard label distillation, as it allows the student models to capture more nuanced relevance information from the teacher model.\n(3) Distilling the interaction knowledge from the CoT reasoning outputs further enhances performance by +0.5%/+0.4% in ROC-AUC and +0.9%/+0.5% in Neg PR-AUC. Furthermore, it can also bring improvements that can be overlaid with score distillation by +1.1%/+0.8% in ROC-AUC and +2.0%/+1.2% in Neg PR-AUC. This"}, {"title": "4.7 Online Evaluation", "content": "We conduct online A/B test by integrating MKD into the ColBERT-like relevance model that is previously deployed, while keeping other factors unchanged. Both variations of the experiment are exposed to 5% of Taobao's search advertising traffic and run continuously for two weeks. The findings show that the introduction of MKD leads to a +0.17% increase in click-through rate (CTR). Human evaluations further indicate an overall improvement in relevance Goodrate by +0.89%, with a particularly notable increase of +1.96% for long-tail samples. These results confirm the effectiveness of the ELLM-MKD framework in enhancing relevance performance and user experience. MKD has already served the entire Taobao search advertising traffic, and ELLM-rele functions not only as the teacher relevance model but also provides additional services, such as relevance annotation assistance and error analysis."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose the Explainable LLM-driven Multidimensional Distillation framework for e-commerce relevance learning, benefiting from two aspects: Firstly, to improve the interpretability and performance of current LLM-based method, we propose an Explainable LLM for relevance modeling, which decomposes the relevance learning into intermediate steps and models relevance learning as a Chain-of-Thought reasoning. Secondly, to enhance the performance of online relevance models via LLM, we propose a Multi-dimensional Knowledge Distillation architecture that transfers the knowledge of ELLM-rele to current interaction-based and representation-based student models from the relevance score distribution and CoT reasoning aspects. Extensive offline and online experimental results demonstrate the effectiveness of our proposed method in enhancing e-commerce relevance modeling performance and consumer experience."}, {"title": "A Details of CoT Construction", "content": "To better illustrate the CoT design of ELLM-rele discussed in Section 3.2.1, we provide detailed information on the aspects of e-commerce relevance discrimination in Table 5."}, {"title": "B Long-tail Analysis", "content": "To evaluate the performance on queries with different occurrence frequencies, we conduct the main experiments by splitting the entire queries into head and long-tail sets by the daily user view (UV). Specifically, we define the queries with UV > 10 on the test"}]}