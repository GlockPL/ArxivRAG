{"title": "OPEN-MAGVIT2: AN OPEN-SOURCE PROJECT TOWARD DEMOCRATIZING AUTO-REGRESSIVE VISUAL GENERATION", "authors": ["Zhuoyan Luo", "Fengyuan Shi", "Yixiao Ge", "Yujiu Yang", "Limin Wang", "Ying Shan"], "abstract": "We present Open-MAGVIT2, a family of auto-regressive image generation models ranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source replication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., 218 codes), and achieves the state-of-the-art reconstruction performance (1.17 rFID) on ImageNet 256 \u00d7 256. Furthermore, we explore its application in plain auto-regressive models and validate scalability properties. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce \"next sub-token prediction\" to enhance sub-token interaction for better generation quality. We release all models and codes to foster innovation and creativity in the field of auto-regressive visual generation.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs), built upon auto-regressive transformer (Vaswani et al., 2017; OpenAI, 2023; Chowdhery et al., 2022; Touvron et al., 2023), have demonstrated dominance in natural language generation due to the incredible context modeling and scalability. Inspired by this, emergent works introduce auto-regressive models into visual generation (Van Den Oord et al., 2017; Esser et al., 2021; Yu et al., 2022; Lee et al., 2022; Sun et al., 2024). These approaches first utilize a vector quantizer for image tokenization and de-tokenization, then employ an auto-regressive transformer for discrete image token sequence modeling.\nAlthough great processes are achieved, the quality of visual generation still falls behind the diffusion-based methods. The main factor is limited tokenizer performance. Tokenizers are generally posited as the upper bound of the visual generation, and inferior off-the-shelf tokenizers (e.g., VQ-VAE (Van Den Oord et al., 2017)) will lead to poor generation quality. Although some improvements are done (Yu et al., 2022; Lee et al., 2022; Sun et al., 2024), current tokenizers are limited by the codebook size and utilization, and the reconstruction performance is still far worse than VAE(Kingma, 2013; Rombach et al., 2022b) used in diffusion models. To unlock the potential of tokenizers, MAGVIT-v2 (Yu et al., 2024a) proposes Lookup-Free Quantizer to enable a highly code-activated and super-large codebook, and achieves better generation quality than diffusion models. However, such a powerful visual tokenizer is completely closed-source and we have no access to this so far, limiting the development of the academic community.\nIn this work, we push forward the auto-regressive visual generation in two folds: 1) Replication of the visual tokenizer: We re-implement the advanced Lookup-Free Quantizer proposed by MAGVIT-v2. To our best knowledge, our open-source replication achieves the closest reconstruction performance stated in MAGVIT-v2 (1.18 vs. 1.15 rFID on ImageNet 128\u00d7128) and outperforms all other methods on the hallmark Imagenet benchmark (Deng et al., 2009). 2) Integrating a super-large codebook with AR visual generation: Instead of simply following MAGVIT-v2 that leverages the vision-oriented design (i.e., mask generative methods (Chang et al., 2022) for visual synthesis), we seek to exploit the potential of such a large codebook in vanilla auto-regressive generation. To assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two sub-vocabulary of different sizes by asymmetric token factorization, and further introduce \"next sub-token prediction\u201d to enhance sub-token interaction for better generation quality. Our experiments on the standard visual generation dataset ImageNet suggest that, with the powerful tokenizer, the plain auto-regressive model exhibits superiority and scalability."}, {"title": "METHOD", "content": "Open-MAGVIT2 is composed of two significant stages. One is a powerful visual tokenizer that maps the input visual signal into the discrete token representations. Subsequently, the vector-quantized sequence will be fed into the auto-regressive transformer for intra- and inter-token relationship modeling, eventually for visual synthesis."}, {"title": "VISUAL TOKENIZER", "content": "Visual tokenization is fundamentally deemed as the crucial component in multimodal large language models (MLLMs) to understand the visual signal input. The CNN-based encoder-quantizer-decoder architecture first proposed in VQVAE (Van Den Oord et al., 2017) is well adopted as the visual tokenizer, which maps input pixels into discrete representations and reconstructs images from quantized features. Specifically, given an image $I \\in []R^{3\\times H\\times W}$, the encoder projects it into the feature map $Z \\in R^{D\\times H'\\times W'}$, where $H' = H/p,W' = W/p$, and p is the down-sample ratio. The quantizer containing a learnable codebook $E\\in R^{2^K\\times D}$ then selects the closest entry $\\hat{z} \\in R^D$ from the codebook for each feature vector $z \\in R^D$. And we can use discrete token indices $X = {x_i}_{i=1}^{H*W'}$ to represent the continuous feature map Z. For decoding, each code index will be mapped back to the quantized feature vector and input into the decoder for pixel-level image reconstruction.\nMotivated by the relationship between the size of the codebook and the dimension of code embeddings, MAGVIT-v2 (Yu et al., 2024a) eliminates the need for embedding lookup by reducing the dimension of code embedding to zero. Specifically, the codebook is shrunk into an integer set where the latent space of each entry is decomposed as the Cartesian product of single-dimensional variables (i.e., $\\hat{C} = \\times_{i=1}^{K}\\{-1,1\\}$, $|\\hat{C}| = 2^K$). As shown in Fig. 2, the tokenization process can be simplified as:\n$z_i = sign(z_i) = -1\\{z_i \\leq 0\\} + 1\\{z_i > 0\\},$"}, {"title": "AUTO-REGRESSIVE TRANSFORMER", "content": "Given a sequence of discrete tokens $X = \\{x_i\\}_{i=1}^{T}, T = H' \\times W'$ from the visual tokenizer, the auto-regressive transformer predicts the next token $x_t$ conditioned on the previous tokens $x_1, x_2,...,x_{t-1}$:\np(X1, X2,\u2026, XT) = \u03a0 P(Xt|x1, x2,..., Xt-1).", "content_after_equations": "Considering the different scales of auto-regressive transformer (i.e., from ~300M to 1B) and the limited training academic data, directly optimizing such a large vocabulary (i.e., 218 codes) is impractical. Therefore, we propose the asymmetric token factorization technique to assist models in performing \u201cnext-token prediction\u201d within concatenated codebooks. Specifically, the LFQ token's latent space is factorized into M subspaces $ {x_t^1}_{t=1}^{T}, {x_t^2}_{t=1}^{T},..., {x_t^M}_{t=1}^{T},$ each of which contains $2^{k_m}$ tokens. As shown in Fig. 2, each subspace is embedded individually and their summation is used as the transformer inputs. Conventionally, an intuitive solution to perform auto-regressive within subspaces is leveraging M separate heads for independent categorical distribution modeling. However, since both sub-tokens are derived from the same latent spaces, such a simple operation may ignore their intra-correlation. Consequently, inspired by (Lee et al., 2022), we reformulate the autoregression paradigm into modeling both intra- and inter-token dependency, which is essentially \u201cnext sub-token prediction\". In this manner, the representational capacity of the super-large codebook can exhibit great potential in auto-regressive generation with better scalability.\n1) Inter-token Relationship: Given a set of sub-tokens from the visual tokenizers, a stacked of Llama blocks with N layers and w width are leveraged to capture the in-context information between tokens. The process can be formulated as:\nCt = LlamaBlock(s, (xi),..., (2x\u22121)), where s denotes the conditional tokens, $C_t \\in R^{T\\times w}$s is the t-th context token.\n2) Intra-token Relationship: We further utilize a transformer with L intra-blocks to autoregressively predict the each sub-token ($x_1, x_t^2,\u2026\u2026, x_t^M$) at the position t. By associating the sub-token conditioned with contextual-enriched vector Ct, the intra-dependency within tokens can be well modeled. Formally, at t position, the autoregression of predicting the conditional distribution of each sub-token is:\nPtm = LlamaBlock(Ct, x\u2021\u00b7\u00b7\u00b7, xm-1).\nTherefore, the auto-regressive likelihood is formulated as:\np(X1, X2,\u2026, \u0425\u0442) = \u03a0p(Xt|X1, X2,\uff65\uff65\uff65, Xt\u22121)\n= \u03a0\u03a0 \u03c1(x(X1, X2,\u00b7\u00b7\u00b7, Xt\u22121), (x1, x2,...xm-1)), where Xt specifies a set of sub-token {$x_t^1,x_t^2,\u00a8\u00a8\u00a8,x_t^M$ } at each position t."}, {"title": "EXPERIMENTS", "content": "The training of the visual tokenizer and auto-regressive transformer are both on ImageNet (Deng et al., 2009). Specifically, we train the tokenizer in 128 \u00d7 128 and 256 \u00d7 256 resolutions.\nFor visual reconstruction, the reconstruction-FID, denoted as rFID (Heusel et al., 2017), codebook utilization, the use percentage of codes, and PSNR on ImageNet 50k validation set are adopted to measure the quality of reconstructed images. Simultaneously, we measure the quality of image generation by the prevalent metrics FID, IS (Salimans et al., 2016) and Precision/Recall (Kynk\u00e4\u00e4nniemi et al., 2019)."}, {"title": "IMPLEMENTATIONS DETAILS", "content": "Open-MAGVIT2 follows the same architecture of the visual tokenizer proposed in (Yu et al., 2024a). For computational efficiency, we remove the gradient penalty loss, and adopt PatchGAN (Isola et al., 2017) as the discriminator instead of StyleGAN (Karras et al., 2019). All models corresponding to different resolutions are trained with similar settings: an initial le 4 learning rate, an Adam Optimizer with \u1e9e1 = 0.5, \u03b22 = 0.9, a total 256 batch size from 270 to 350 epochs, a combination of reconstruction, GAN, perceptual (Zhang et al., 2018), entropy penalty (Yu et al., 2024a), commitment losses, LeCAM regularization (Tseng et al., 2021) for training stability, and 32 \u00d7 Nvidia V100 / Ascend 910B with Pytorch."}, {"title": "MAIN RESULTS", "content": "As shown in Tab. 2, by incorporating all useful designs proposed in (Yu et al., 2024a), Open-MAGVIT2 matches MAGVIT-v2 performances with merely 0.03 FID margin on ImageNet 128 \u00d7 128. Further, we also compare our Open-MAGVIT2 with previous visual tokenizers on ImageNet 256 \u00d7 256 in Tab. 3. Benefiting from the super-large codebook with lookup-free quantization, Open-MAGVIT2 outperforms all previous image tokenizers under fair settings. Moreover, we provide an illustrative visual comparison in Fig. 3. As indicated, our visual tokenizer gains more superiority in detail perception as well as precise facial and text reconstruction."}, {"title": "Visual Generation", "content": "MAGVIT-v2 leverages the non-autoregressive framework for image synthesis and achieves competitive performance. Considering the scalability of auto-regressive models and the remarkable success of the auto-regressive paradigm in MLLM (Team, 2024), we instead focus on exploring the potential of incorporating a super-large codebook for auto-regressive visual generation."}, {"title": "CONCLUSION", "content": "In this work, we re-implement the powerful visual tokenizer, which achieves state-of-the-art performance compared with previous methods, and make it available to the community. Instead of simply following (Yu et al., 2024a) that leverages masked-generative transformer for visual generation, we delve into a more promising manner (i.e., auto-regressive visual synthesis). To excavate the potential of the large vocabulary, we introduce the \u201cnext sub-token prediction\u201d paradigm with the asymmetric token factorization technique. The experiment suggests that with the powerful tokenizer, the plain auto-regressive model exhibits superiority and scalability. We hope our contribution to the open-source community can facilitate more innovative and creative works in the field of auto-regressive visual generation, eventually making a difference in building an omnipotent multi-modal framework."}, {"title": "Limitations and future work", "content": "We expect that the effectiveness of such a super-large codebook, (i.e., 218 codes), is still underestimated due to the limited data scale and the sacrifice of the representational capacity with the token factorization technique. We believe that by amplifying the task with more training data (e.g., text-conditional image generation, video generation, etc.), and enlarging the model size to 7B or even larger, the potential of AR generation with a super-large codebook can be dramatically exploited. Therefore, extending Open-MAGVIT2 into more broad multi-modal generation applications will be a high priority in our future exploration."}, {"title": "RELATED WORKS", "content": "Visual tokenizer is to map an image into compact discrete tokens, which are subsequently fed into the generative models for sequence modeling. Early pioneer VQVAE (Van Den Oord et al., 2017) first introduces learnable codebook mechanism for 2D tokens generation. Subsequently, ViT- VQGAN (Yu et al., 2022) and RQ-VAE (Lee et al., 2022) improve VQVAE through normalized and multi-scale quantization respectively. Recently, LlamaGen (Sun et al., 2024) reexamines the design of vanilla tokenizer (Esser et al., 2021) and reveals the conflict between the fidelity of the synthesized image and the size of codebook. Therefore, following the simple intuition (Yu et al., 2022) that reducing code dimension limits the representational capacity of individual tokens, MAGVIT-2 (Yu et al., 2024a) proposes an advanced visual tokenizer which significantly enlarges the size of codebook to 218 with Lookup-Free Quantization."}, {"title": "VISUAL GENERATION", "content": "Given a set of compact discrete image tokens, there exist two prevalent frameworks for the subsequent image synthesis, including Non-autoregressive and Auto-regressive generation.\nNon-autoregressive frameworks. MaskGIT (Chang et al., 2022) utilizes BERT-style transformer (Devlin et al., 2018) to parallelly generate all visual tokens via masked-prediction mechanism. MAGVIT (Yu et al., 2023; 2024a) adopts the same architecture but includes an additional embedding mask for better generation quality.\nAuto-regressive frameworks. Autoregressive-based Multi-Modal Large Language Models (Liu et al., 2024; Li et al., 2024) has achieved remarkable success in versatile visual understanding. In contrast, the progress in counterpart visual generation still remains unsatisfactory. The simplest approach VQGAN (Esser et al., 2021) employs tiny GPT2 (Radford et al., 2019) (~ 300M) for next- token prediction. VAR (Tian et al., 2024) reformulates the image generation approach into next-scale prediction and unveils the scaling principle simultaneously. Subsequently, LlamaGen (Sun et al., 2024) extends VQGAN with Llama (Touvron et al., 2023) architecture, showcasing significant improvement in fidelity. However, the limited codebook size (e.g., 214) in existing auto-regressive models may incur the representational bottleneck. Therefore, considering that the capacity of the visual tokenizer is highly correlated with the quality of visual synthesis (Yu et al., 2024a), we democratize the plain auto-regressive approach with a super-large codebook."}]}