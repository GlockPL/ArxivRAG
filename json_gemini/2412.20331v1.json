{"title": "Mind the Data Gap: Bridging LLMs to Enterprise Data Integration", "authors": ["Moe Kayali", "Nesime Tatbul", "Fabian Wenz", "\u00c7a\u011fatay Demiralp"], "abstract": "Leading large language models (LLMs) are trained on public data. However, the majority of the world's data is dark data not publicly accessible, mainly in the form of private organizational data or enterprise data. We show that the performance of methods based on LLMs seriously degrades when tested on real-world enterprise datasets. Current benchmarks, based on public data, overestimate the performance of LLMs. We release a new benchmark dataset, the Goby Benchmark, to advance discovery in enterprise data integration. Based on our experience with this enterprise benchmark, we propose techniques to uplift the performance of LLMs on enterprise data, including: (1) hierarchical annotation, (2) runtime class-learning, and (3) ontology synthesis. We show that, once these techniques are deployed, the performance on enterprise data becomes on par with that of public data. The Goby benchmark can be obtained at https://goby-benchmark.github.io/.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite intensive academic and industrial interest, the uptake of large language models (LLMs) for data management and integration tasks remains limited in practice. A study by Gartner reports that at least 30% of LLM-based enterprise projects will be abandoned after proof of concept by the end of 2025 [19]. For example, commercial LLM-based legal research tools by industry leaders LexisNexis and Thomson Reuters were measured to have only 40-65% accuracy [12]. Their failures were sufficiently high profile that John Roberts, chief justice of the United States Supreme Court, rebuked lawyers using LLM-based tools in his 2023 summary of the state of the judicial system [16].\nLLM-based techniques have been intensively studied for data integration over the last two years by the database community, including entity matching [13], column type annotation [11], wrapper induction [2], and candidate key identification [23]. Industry data management practitioners have also embraced these tech-niques, with LLM-based techniques deployed in Azure Data Lake, Microsoft Excel, and Amazon Q for Business. Several startups in the LLM-for-data-management space, including Unstructured and Numbers Station, have attracted tens of millions in investment dollars. Interest continues to be high.\nThis work is a call to action: evaluating LLM performance on public data management benchmarks risks presenting an overly rosy picture of its abilities. In the first part of the paper, Section 2, we discuss data management benchmarks, explaining that they can exist on a spectrum of public-to-private sources. Then, we present Goby, a new benchmark built with real enterprise in Section 3. Finding that LLM-based approaches perform more poorly on the Goby Benchmark than comparable public-data benchmarks, we propose new concepts for adapting LLMs to enterprise data and empirically evaluate these in Section 4. Finally, we discuss our results and future directions in Section 5. To resolve this data gap, we in the data management community must \"do the work\" and develop more realistic benchmarks."}, {"title": "2 BACKGROUND", "content": "Benchmarks and their accompanying datasets can be categorized, in general terms, into two buckets: public and private. On the fully-public end, there are standard benchmarks such as VizNet [7] and GitTables [9]. These contain verbatim public data from web sources. Another well-known example is the Spider Benchmark [24], which compiles databases from sources such as Wikipedia but generates questions over them using the traditional graduate-student-based approach. On the other end, there are private datasets made from proprietary data sources and labeled with proprietary labels, which are not possible to understand outside the originating organization. Our dataset is oriented towards these private datasets.\nFurther, separate from the issue of public accessibility is the over-representation of some high-visibility domains in benchmarks. Examples of such domains are celebrities, government, and geography. While these domains are very well-represented on the public web, they do not make up the bulk of enterprise data.\nTask suitability is another concern. Many existing benchmarks are constructed in a manner not authentic to enterprise use. These benchmarks are created by crawling the web for specific patterns that are easy to identify. For example, GitTables is constructed by searching GitHub for patterns such as id and state, and similarly, for T2Dv2, tables that were easy to annotate were selected from Wikipedia. In other words, the \"how\" of data collection dictates the \"what\" of the database content. On the other hand, organizations generally decide on \"what\" datasets they have a business need for and then the \"how\" of data collection.\nVirtually all academic systems are evaluated on public datasets (e.g., [2, 11, 13]). Gartner reports that lack of ability to perform evaluation is common in enterprise setting: industry generally does not have its own benchmarks [19]. In personal communication with practitioners building data management systems, we have found this to be true.\nMany research systems focused specifically on public data, such as the seminal WebTables system [3]. Prior work noted the poor generalization of many deep learning techniques [11]. The reason for that is the high risk of overfitting to the training data, which restricts the model's ability to generalize and adapt to new, unseen data and contexts. For example, consider the performance of the deep learning model DoDuo [18]. In the setting where it is trained and tested on the same data distribution (VizNet), the performance is strong. However, when we attempt to generalize by training on the similarly structured Wiki dataset and testing it on VizNet, its performance drops by at least 30% in terms of F1-score, precision, and recall [11]. This comparison is merely between two public datasets with very similar data types and structures. An unseen private dataset, which often has unique structures and proprietary labels, may pose an even bigger challenge. This underlines the necessity for more robust approaches to bridge the gap between private and public data sources.\nWe focus this paper on a specific data integration task: semantic column type annotation [8]. Given a relational table, column anno-tation involves assigning to each column a type which corresponds to the type of values stored in that column. The classes are drawn from a set of labels: if those labels are basic types (e.g., date, time), the problem is called atomic type annotation, while if the labels"}, {"title": "3 GOBY BENCHMARK", "content": "Goby is a benchmark dataset we derive from a production industry workload in the event promotion and marketing setting. The dataset was compiled around 2017 and encompasses over 4 million events.\nConstructing Goby. The dataset was constructed over several years. First, over one thousand wrappers were written by profes-sional developers in order to convert web pages and APIs into relational tables. Once those wrappers were developed, they were executed to obtain 4.1 million rows of data, each corresponding to an event. Next, the team developed a set of semantic types common to all the events. The labels were applied to 40% of the columns. The rest of the columns were miscellaneous data not of interest to the task. Finally, the semantic type mappings were used to create a universal schema which unifies all the source tables into one.\nBenchmark components. Its components are:\n(1) a semantic type hierarchy developed by domain experts\n(2) nearly 1 200 source tables, each corresponding to the output of one wrapper\n(3) mapping each annotated with the semantic types of its columns\n(4) a universal table in which all data from the source tables is unified\nBenchmark Characteristics. All in all, Goby is comprised of a total of 4.04 million rows and 23 203 columns. \nGoby exhibits high task suitability for data integration tasks. This is because the construction method is realistic and tailored for a data integration pipeline. We also highlight that Goby is semantically-rich as compared to with other benchmarks. The most common labels are those related to events, such as location and organizer details. Meanwhile, typical benchmarks contain labels with low semantic information, for example the three most popular semantic types in one benchmark [9] are the generic labels id, name and type.\nFurther, because of the method of construction, Goby has more realistic table structure. This can be seen in the average number of rows: 3 400, and columns: 20. This reflects the need to have a minimum amount of business information about each event."}, {"title": "4 INSIGHTS AND EXPERIMENTS", "content": "In this section, we explore the performance of large language mod-els (LLMs) in the context of enterprise data, highlighting the lim-itations of previous models and benchmarks, as well as novel ap-proaches we discovered for optimizing LLM performance. Through our experiments, we present key insights that not only build upon prior work but also introduce innovative techniques for integrating and analyzing enterprise data using LLMs. To backup these obser-vations, we provide empirical evidence and detail in the design decisions made during the model development process, offering a clear reasoning for each step.\nExperimental setup. We conduct an experimental evaluation us-ing the following large language models: OpenAI's gpt-3.5-turbo and Meta's Llama2 with 13 billion parameters. Experiments utiliz-ing grammar-constrained decoding were conducted using Llama2 since OpenAI APIS do not expose the required raw token probabili-ties."}, {"title": "4.1 Enterprise performance gap", "content": "We begin by evaluating LLM approaches on the Goby enterprise dataset in comparison to the public benchmark VizNet for the task of semantic column-type annotation. Overall, we find the outcome that LLMs perform considerably worse on Goby compared to the public data benchmark.\nThe semantic column-type annotation task is a multi-class classi-fication problem. We evaluate using the standard metrics precision $P$, recall $R$ and their harmonic mean, the $F_1$ score. Precision is defined as ratio of true positives to the sum of true positives and false positives. Recall is defined as the ratio of true positives to the sum of true positives and false negatives. The $F_1$ score reflects a metric in which precision and recall are equally weighted. Because of the multi-class setting, the reported scores are macro-averaged. Additionally, we note that the LLM can decline to answer or provide an unparsable answer, lowering the recall but not precision.\nThe underlying approach for extracting semantic column-type predictions is the same as the CHORUS system [11], we refer the reader to that paper for more details. In this experiment, the model is given access to the set of valid labels. We will remove this as-sumption in a later experiment.\nEmpirically, we find a gap between prior benchmarks and Goby of 0.18 F1 points, 14.1 percentage points of precision and 18.8 percentage points of recall."}, {"title": "4.2 Iterative dictionary construction", "content": "The first step in our approach involves synthesizing a data dictio-nary and then generalizing it into an ontology. A Data dictionary is a list of possible data types and acceptable values, optionally with some metadata on their semantic meaning. Ontologies can be considered to be data dictionaries with a hierarchy.\nThe development of data dictionaries involve making many ar-bitrary distinctions. Label granularity is an example: the curator may choose to have the classes street address, full address and PO box or combine them under just one heading, address. Further, because of coordination failure between multiple labelers, redundant synonymous labels such as wages and salary can be present. As such, finding a perfect match for the dictionary created by a human curator is not the goal.\nNow we start testing the ability of large language models to iter-atively construct the class labels. In this experiment, we randomly sample from the Goby tables to learn the classes. The process is seeded with a small number of starting classes. We then consider a column at a time, presenting a sample of 5 unique values from that attribute. If the LLM predicts an existing class for the column, we continue onto the next column. Otherwise, we extract a new label for the column and add that to the pool of labels. The process is terminated when the quiescence condition is met: the observation of 5 tables sequentially, all of which do not require the creation of any new labels.\nThe evaluation metric here is coverage, the portion of ground-truth label for which there exists a synthesized label. We find that the LLM can obtain 70.5% coverage."}, {"title": "4.3 Building a Hierarchy", "content": "We address the challenge of label granularity by learning an on-tology: a structure of classes. Starting with the data dictionary of the relevant classes, we aim at the goal of creating an ontology. We consider this ontology successful if it faithfully captures all ground-truth, human-labeled classes. In section 4.3, we will show that such a synthesized ontology can cap-ture 100% of the ground-truth labels and resolves the granularity issue.\nFirst, we test the LLM's ability to transform its predicted classes into a hierarchy. We build the tree in an iterative approach, similar to the class learning prior. The main difference is that we utilize batching to give the model enough context to select appropriate superclasses. We find this a resounding success: the model can produce superclasses that have 100% coverage of both the learned and ground truth classes. \nNow that we have the ontology, integrating a hierarchical ap-proach into LLM requires a number of architectural decisions that we experiment with in the next sub-section, section 4.4. As we will see, how the hierarchy is integrated into the LLM makes a large impact on the performance."}, {"title": "4.4 Encoding: Full context required", "content": "Once the ontology is learned, it must be encoded into the language model for prediction. We investigate several methods of integrating hierarchical information into LLMs. These included both archi-tectural and semantic approaches. We consistently found that ap-proaches that presented the full context to the LLMs outperformed those which attempted to feed only the relevant context.\nGrammar-constrained decoding. Large language models perform sampling to extract tokens from the conditional token probability distributions they output [6]. Common approaches include beam search, top-k, temperature and nucleus sampling. \nStep-by-step prompting. Another method to encode the ontology is presenting a series of prompts within the same context.\nTree serialization. The final approach we consider is serializing the class tree and providing it to the model. This involves traversing the tree in pre-order and enumerating each path from the root. Surprisingly, as will will show shortly, this approach has the most robust performance.\nEvaluation. We consider a prediction correct if it matches the ground truth label in our mapping, or if the most-recent common ancestor (MRCA) is the direct parent of the prediction. We find the tree-serialization approach most effective, bringing total $F_1$ score to 0.85. Grammar-based decoding paradoxically results in a lower score $F_1$ 0.66, while step-by-step approach achieves a modest lift of 0.05 $F_1$ points from the baseline of 0.71."}, {"title": "5 DISCUSSION", "content": "Our findings support the hypothesis that enterprise data presents new challenges to LLMs not captured by public-data benchmarks. We find that a hierarchical approach allows for successfully learning labels and making predictions on enterprise data.\nDataset contamination or distribution shift? Two principal mecha-nisms may explain the observed performance gap. The data contam-ination theory posits that the LLMs have been trained on (portions of) test set of the benchmark dataset. \nCost. This also remains an elusive facet of LLM applications. Generally, the volume of \"dark\" enterprise data dwarfs that which is publicly available. Findings that LLMs are too cost prohibitive to run on public data are magnified in the enterprise setting, par-ticularly due to extensive data duplication in the business setting. Effective table summarization becomes key: it is necessitated by the limited context window size of language models, which causes an inability to ingest whole tables. Current approaches for sum-marization are ad-hoc-a principled approach would be welcome. Scalability remains a unsolved challenge too. The superior per-formance of the (relatively context-consuming) tree serialization approach raises concerns.\nThe insights gleaned in this work translate more widely into general data management problems: we have a work-in-progress on applying lessons learned in this approach to problem outside data integration, focusing on natural language to query translation in the enterprise."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce Goby, the first data integration bench-mark utilizing real enterprise data. We show that LLM-based ap-proaches to data integration tasks such as semantic column type annotation, having been trained on public data, experience a sig-nificant drop in performance on enterprise data (i.e., evaluating them on public benchmarks paints a false picture). More specifi-cally, we show that out-of-the-box LLM performance on the Goby Benchmark dataset is lower than that on public datasets such as VizNet. We then propose and test approaches to bridge the gap between publicly trained LLMs and their use for enterprise data integration, primarily based on hierarchical clustering. We find these approaches effective in remedying the discovered gap for the semantic column type annotation task and invite our community for further research using on our Goby Benchmark which we are making publicly available."}]}