{"title": "Learning Ensembles of Vision-based Safety Control Filters", "authors": ["Ihab Tabbara", "Hussein Sibai"], "abstract": "Safety filters in control systems correct nominal controls that violate safety constraints. Designing such filters as functions of visual observations in uncertain and complex environments is challenging. Several deep learning-based approaches to tackle this challenge have been proposed recently. However, formally verifying that the learned filters satisfy critical properties that enable them to guarantee the safety of the system is currently beyond reach. Instead, in this work, motivated by the success of ensemble methods in reinforcement learning, we empirically investigate the efficacy of ensembles in enhancing the accuracy and the out-of-distribution generalization of such filters, as a step towards more reliable ones. We experiment with diverse pre-trained vision representation models as filter backbones, training approaches, and output aggregation techniques. We compare the performance of ensembles with different configurations against each other, their individual member models, and large single-model baselines in distinguishing between safe and unsafe states and controls in the DeepAccident dataset. Our results show that diverse ensembles have better state and control classification accuracies compared to individual models.", "sections": [{"title": "1. Introduction", "content": "Ensuring safety of control systems is a fundamental challenge in various application domains, including autonomous driving (Betz et al. (2019)), aerospace (Breeden and Panagou (2022)), and robotic surgery (Haidegger (2019)). It entails verifying that the trajectories of a system remain in a region of the state space that the user considers safe, or synthesizing controllers that drive the system to remain there. One of the prominent solutions is to design barrier certificates that guarantee the safety of the system. These certificates can guide the selection of controls or modify nominal ones to maintain safety, effectively serving as safety control filters (Ames et al. (2016)). Unfortunately, synthesizing such certificates is generally NP-hard (Clark (2021)) and accordingly, existing algorithms do not scale beyond few dimensions. Moreover, these algorithms require white-box settings where the dynamics of the system and its environment are known. For many modern systems, e.g., vision-based autonomous navigation, such conditions are not satisfied.\nRecently, deep learning-based methods have been proposed for designing certificates and controllers, offering an easier and more scalable approach for their design (Dawson et al. (2023); Abdi et al. (2023); Tong et al. (2023); Xiao et al. (2022); Yang and Sibai (2024)). However, the learned neural certificates are not formal ones. They do not necessarily satisfy the required conditions at every state and control for them to guarantee safety. Algorithms for formally verifying them suffer from similar curse-of-dimensionality limitations as the traditional methods for their design, despite significant progress in neural networks (NN) verification over the past few years (e.g., Katz et al. (2022));"}, {"title": "2. Related Work", "content": "Ensembles in reinforcement learning (RL) and control Ensembles have been used to represent and tackle uncertainty in risk-sensitive RL (Eriksson et al. (2022); Hoel et al. (2023)), for learning from unstable estimations of value functions (Fau\u00dfer and Schwenker (2015); Anschel et al. (2017)), for learning value functions more efficiently (Chen et al. (2021)), to facilitate optimism for efficient exploration in model-based online deep RL (Pacchiano et al. (2021)), to enable pessimism in offline RL (Ghasemipour et al. (2022)), to approximate reward functions in inverse RL (Lin et al. (2020)), for robust dynamic motion prediction (Mortlock et al. (2024)), and for anomaly detection (Ji et al. (2024)). Moreover, it has been shown that carefully designed reward functions define Q functions that are equivalent to control barrier functions (Tan et al. (2023)), the control version of barrier certificates. This implies that the demonstrated benefits of ensembles in RL can also be potentially obtained in learning safety filters. The results of this paper can be seen as a supporting evidence.\nLearning safety filters Recent approaches to designing safety filters use deep learning to overcome the scalability challenges inherent in methods based on sum-of-squares optimization and reachability analysis (Dawson et al. (2022b,a)) and to account for unknown dynamics (Qin et al. (2022); Lavanakul et al. (2024); Castaneda et al. (2023)) and high-dimensional observations such as images and point clouds (Tong et al. (2023); Abdi et al. (2023); Xiao et al. (2023)). The resulting filters are not guaranteed to satisfy the conditions for them to be valid certificates, unless under restrictive assumptions of known Lispchitz constants of the NNs and corresponding grid-like training datasets that cover the whole domain (Anand and Zamani (2023); Tayal et al. (2024)). Several works have used NN verification techniques to guarantee these conditions as well as generating counter examples that can be used for retraining (Wu et al. (2023); Hu et al. (2024)). However, these techniques are not yet scalable enough to verify high-dimensional, observation-based filters across all pos-"}, {"title": "3. Preliminaries", "content": "In this section, we recall the definition of control barrier functions (CBF) and the guarantees they provide. We generally assume that the dynamics of the control system under consideration is control-affine. This assumption, while not required for the CBF definition, is usually added to obtain state-dependent linear constraints that separate safe and unsafe controls, which simplify the safety filtering optimization problem to a quadratic program that can be solved efficiently in real-time.\nDefinition 3.1 (Control-affine control systems) A continuous-time nonlinear control-affine system can be described using the following ordinary differential equation (ODE):\n$\\dot{x} = f(x) + g(x)u,$\n(3.1)\nwhere $x \\in X \\subset \\mathbb{R}^n$ is the state variable, and $u \\in U \\subset \\mathbb{R}^m$ is the control one. We assume that $f : X \\rightarrow \\mathbb{R}^n$ and $g : X \\rightarrow \\mathbb{R}^{n \\times m}$ are locally Lipschitz continuous.\nDefinition 3.2 (Control barrier functions (Ames et al. (2019))) A continuously differentiable function $B : X \\rightarrow \\mathbb{R}$ is called a control barrier function for system (3.1) if\n$\\exists u \\in \\mathcal{U} \\text{ such that } \\dot{B}(x, u) + \\gamma(B(x)) \\geq 0 \\quad \\forall x \\in \\mathcal{D} \\subseteq X,$\n(3.2)\nwhere the super-level set $B_{\\geq 0} := \\{x \\mid B(x) \\geq 0\\}$ of $B$ is a subset of $\\mathcal{D}$, and $\\gamma: (-b, a) \\rightarrow \\mathbb{R}$, for some $a, b > 0$, is a locally Lispchitz extended class $\\mathcal{K}_{\\infty}$ function, i.e., it is strictly increasing and $\\gamma(0) = 0$.\nA CBF $B$ specifies the controls that guarantee the forward invariance of $B_{\\geq 0}$, i.e., all the trajectories of system (3.1) that start in $B_{\\geq 0}$ and follow controls that satisfy (3.2), will remain within it at all times, as stated in the following theorem. If the set of unsafe states is disjoint from $B_{\\geq 0}$ and the system starts from states in $B_{\\geq 0}$, then the system can be kept safe by following such controls.\nTheorem 3.3 (Ames et al. (2019)) Any Lipschitz continuous control policy $\\pi: \\mathcal{D} \\rightarrow \\mathcal{U}$ where $\\forall x, \\pi(x) \\in \\{u \\in \\mathcal{U} : \\nabla B(x) (f(x) + g(x)u) + \\gamma(B(x)) > 0\\}$ renders $B_{\\geq 0}$ forward invariant.\nGiven a reference controller $\\pi_{\\text{ref}} : X \\rightarrow \\mathcal{U}$ that does not necessarily guarantee safety, a CBF $B$ can be used to filter its unsafe decisions. Specifically, a quadratic program (QP) can be formulated with the objective to find the closest safe control to $\\text{safe}(x)$, as follows (Ames et al. (2016)):\n$\\pi_{\\text{safe}} (x) := \\underset{u \\in \\mathcal{U}}{\\text{arg min}} ||u - \\pi_{\\text{ref}} (x)||^2 \\quad \\text{s.t.} \\quad \\nabla B(x) (f(x) + g(x)u) + \\gamma(B(x)) \\geq 0.$\n(3.3)\nIn our case, the state is a function of the non-interpretable representations of the image observations generated by the PVR backbones, as we will discuss later. The dynamics over such a state space are unknown, as it involves modeling uncertain, complex, and dynamic environments as well generating corresponding input images. This presents a challenge to traditional approaches for the"}, {"title": "4. Method", "content": "In this section, we describe how we build our diverse ensembles and aggregate their outputs."}, {"title": "4.1. Designing member models of the ensembles", "content": "We train each member vision-based safety filter using the approach described in Yang and Sibai (2024). We use PVRs that resulted in the best performance in that study, which are: (1) CLIP (ViT-B/32 model) (Radford et al. (2021)), a model trained using contrastive learning on image-text pairs collected from the internet and (2) VC1 (ViT-B/32 model) (Majumdar et al. (2023)), a transformer-based encoder model pre-trained on data encompassing control and robotics tasks. In all of our experiments, we froze the backbones and only trained the safety filter heads and the layers aggregating the representations of the frames from the different cameras. We consider the setting where a set of $M$ images $I_1:M$ are captured at each sampled time instant, e.g., by the various cameras mounted on an autonomous vehicle. These images are fed one-at-a-time to a PVR backbone to obtain their respective representations $h_1:M$. We encode the identity of the camera capturing every image using positional encoding and concatenate it with its representation, resulting in $h_i:M := h_i || \\text{POS}(i) \\quad i \\in [1:M]$. Then, we train an attention layer to compute $\\text{score}_i(h)$, which we use to create a unified representation using a weighted sum $h^* := \\sum_i \\text{score}_i(h)$. We then define the system state as $x_t := \\text{MLP}_\\theta (h^*, x_{t-1}, u_{t-1})$, where MLP is a feedforward NN and $x_{t-1}$ and $u_{t-1}$ are the state and control at the previous time instant. Finally, we use iDBF, SABLAS, and DH methods to train the safety filters for the black-box dynamics over such a state space."}, {"title": "4.2. Aggregating the outputs of member models", "content": "This section discusses the methods we used to combine the outputs of the member models of the ensembles. We explored (weighted) averaging, majority voting, and consensus-based ensembles."}, {"title": "Weighted averaging-based ensembles", "content": "A simple approach to combine the outputs of the member models of an ensemble is to take their average. When they represent CBFs, the output of the ensemble can be defined as $B_{\\text{ens}}(x) := \\sum_{i=1}^N w_i B_i(x)$, where $N$ is the number of member models, and $\\forall i, w_i \\geq 0$ and $\\sum_{i=1}^N w_i = 1$. One can either use uniform weights or ones optimized using data. In either case, the left-hand-side of the constraint in (3.3) becomes $\\dot{B_{\\text{ens}}} (x, u) + \\gamma(B_{\\text{ens}}(x)) = (\\sum_{i=1}^N w_i \\nabla B_i(x) (f_i(x) + g_i(x)u)) + \\gamma(\\sum_{i=1}^N w_i B_i(x))$, where $f_i$ and $g_i$ represent the dynamics learned for training $B_i$. When the member models are DH-based ones, we can define a similar constraint to that of the averaging-based CBF ensemble as follows: $\\sum_{i=1}^N w_i (a_i(x)^T u - b_i(x)) \\geq 0$. Both constraints are affine in $u$, and one can still use (3.3) to obtain safe controls that follow the reference one. As a separate note, as described in (Lavanakul et al. (2024)), the output of a DH model represents a generalization of a CBF-based constraint. Particularly, $b_i(x)$ represents the term $-\\nabla B_i(x) f(x) - \\gamma(B_i(x))$ in the discriminating hyperplane defined by a CBF $B_i$ constraint. However, unless $\\gamma$ is linear, the term $-\\sum_{i=1}^N w_i b_i(x)$ in the DH-based ensemble constraint is different from the term $(\\sum_{i=1}^N w_i \\nabla B_i(x) f(x)) + (\\sum_{i=1}^N w_i \\gamma B_i(x))$ in the CBF-based one. For this reason, we only consider linear $\\gamma$ in our experiments. That allows us to combine DH and CBF-based member models in the same ensemble.\nWe optimize the weights $\\{w_i\\}_{i \\in [N]}$, while freezing the member models. We use this approach for both CBF and DH-based ensembles. We define the loss as: $\\mathcal{L} = \\mathcal{L}_{\\text{safe}} + \\mathcal{L}_{\\text{unsafe}}$, where $\\mathcal{L}_{\\text{safe}} = \\sigma(-B_{\\text{ens}} (x, u) - \\gamma(B_{\\text{ens}}(x))) \\cdot 1(x' \\in \\mathcal{X}_{\\text{safe}})$ or $\\mathcal{L}_{\\text{safe}} = \\sigma(-\\sum_{i=1}^N w_i (a_i(x)^T u - b_i(x))) \\cdot 1(x \\in \\mathcal{X}_{\\text{safe}})$, $\\mathcal{L}_{\\text{unsafe}} = \\sigma(B_{\\text{ens}} (x, u) + \\gamma(B_{\\text{ens}}(x))) \\cdot 1(x' \\notin \\mathcal{X}_{\\text{safe}})$ or $\\mathcal{L}_{\\text{unsafe}} = \\sigma (\\sum_{i=1}^N w_i (a_i(x)^T u - b_i(x))) \\cdot 1(x' \\notin \\mathcal{X}_{\\text{safe}})$, $\\sigma$ is the ReLU function, $x'$ is the state appearing after $x$ in the trajectory, and $\\lambda > 1$ is to further penalize miss-classifications of unsafe actions and handle dataset imbalance. Nonetheless, for ensembles without DH-based models, one can alternatively choose to train $\\{w_i\\}_{i \\in [N]}$ to optimize both the values of the CBF on safe and unsafe states in addition to the hyperplanes classifying actions."}, {"title": "Majority voting-based ensembles", "content": "The second approach we consider is to combine the outputs of the member models using majority voting. Each model decides whether a state or an action is safe or unsafe, and the final output is determined by the majority. To classify a state or action as safe, we should have strictly more votes for safety than unsafety, otherwise it is considered unsafe. For SABLAS and iDBF, we check if $B_i(x) \\geq 0$ to classify a state $x$ as safe and check if $\\nabla B_i(x, u) + \\gamma(B_i(x)) > 0$ to classify an action $u$ at state $x$ as safe. In the case of DH, a model does not classify states as it only defines hyperplanes separating actions to safe and unsafe ones. The constraint defined by the majority voting-based ensemble is not affine in $u$. Instead of the QP problem in (3.3), the new optimization problem to find a safe action that is close to the reference can be formulated as a Mixed-Integer Quadratic Program (MIQP), which is NP-complete (Pia et al. (2017)). Solving such a problem is not suitable for real-time settings. Instead, if the majority voted that the reference control is unsafe, one can resort to a heuristic and select the models which voted in support of the decision and define a QP which have their constraints, and ignoring the other models."}, {"title": "Consensus-based ensembles", "content": "In the final aggregation method that we use, ensembles have three member models: $M_1$, $M_2$, and $M_3$. We consider two cases. In the first one, which we call the specialized members case, we select $M_1$ and $M_2$ to be experts on different tasks: $M_1$ that is highly accurate in classifying safe actions and $M_2$ that is highly accurate in classifying unsafe ones. In the second case, which we call the non-specialized members one, $M_1$ and $M_2$ are both equally capable in classifying both safe and unsafe actions. In both cases, we select $M_3$ to be an ensemble with higher"}, {"title": "5. Experimental Setup and Results", "content": "We conducted several experiments to compare ensembles of safety filters with individual models on the DeepAccident dataset (Wang et al. (2024))."}, {"title": "5.1. Setup", "content": "Dataset and data pre-processing DeepAccident (Wang et al. (2024)) is a synthetic dataset generated using CARLA simulating real traffic accidents reported by the National Highway Traffic Safety Administration (NHTSA), as well as safe driving scenarios. It includes action-annotated videos captured from six distinct cameras mounted on the ego vehicle with a total of 57k annotated frames. The control is a 2D vector determining the vehicle's velocity. We used the safety labels of the frames and the actions for the dataset which were created in Yang and Sibai (2024). For each trajectory with an accident, the first frame at which the collision happens was labeled as unsafe along with the frames following it. The five frames preceding the collision were labeled as safe, and the controls during that interval were labeled as unsafe. All other frames and controls were considered safe.\nEvaluation metrics We use the classification accuracy of safe and unsafe states and actions. In addition, we use the ensemble improvement rate (EIR), introduced in (Theisen et al. (2024)), as a measure of improvement of the loss achieved by ensembles compared to individual member models. In the case of averaging-based ensembles, EIR := $(\\sqrt{\\sum_{i \\in [N]} \\mathcal{L}(f_i) - \\mathcal{L}(f)}) / (\\frac{1}{N} \\sum_{i \\in [N]} \\mathcal{L}(f_i))$, where $f(x)$ is $B_i(x)$ in the state classification task and $f(x, u)$ is $\\nabla B_i(x) (f_i(x) + g_i(x)u) + \\gamma(B_i(x))$ in the action classification task. Also, $f(x)$ is $\\sum_{i=1}^N w_i B_i(x)$ in the state classification task and $f(x, u)$ is $(\\sum_{i=1}^N w_i \\nabla B_i(x) (f_i(x) + g_i(x)u)) + \\gamma(\\sum_{i=1}^N w_i B_i(x))$ in the action classification task. The loss used in the EIR calculation for the action classification task is $\\mathcal{L}(f) = \\mathcal{L}_{\\text{safe}}(f) + \\lambda \\mathcal{L}_{\\text{unsafe}} (f)$, defined in Section 4 with $\\lambda = 18$. For the state classification task, it is modified so that $\\mathcal{L}_{\\text{safe}} (f) := \\frac{1}{|D|} \\sum_{x \\in D} \\sigma(f(x)) \\cdot 1(x \\in \\mathcal{X}_{\\text{safe}})$ and $\\mathcal{L}_{\\text{unsafe}} (f) := \\frac{1}{|D|} \\sum_{x \\in D} \\sigma(f(x)) \\cdot 1(x \\notin \\mathcal{X}_{\\text{safe}})$, where $D$ is the set of states in the test set and $\\mathcal{X}_{\\text{safe}}$ is the set of safe states. We replace the averages over $D$ with the averages over all pairs $(x, u)$ in the test set and $\\mathcal{X}_{\\text{safe}}$ to the set of safe state-action pairs when considering the action classification task. In the case of majority voting-based ensembles, we have the same definition of EIR, but consider $f$ to be the majority voting ensemble and $\\mathcal{L}(f)$ is the zero-one loss, i.e., is zero when a state or an action is correctly classified."}, {"title": "5.2. Results and analysis", "content": "We trained five models with different weight initializations and hyper-parameters for every pair of a backbone and a safety filter training method to create all of our ensembles. When designing an ensemble that has a certain pair, we include all of the five corresponding trained models. For example, the ensembles using three training methods and one backbone are composed of fifteen individual models.\nTable 1 shows the accuracies and EIR of single-backbone and multi-backbone ensembles using various output aggregation and safety filter training methods. It also includes the accuracies of the member models and individual large models with comparable total parameters to the ensembles. When reporting the results of individual models, we use the average accuracies taken over five models along with their standard deviation. Hereafter, when presenting action classification accuracy percentages, we use the format (safe action classification accuracy %, unsafe action classification accuracy %), unless stated otherwise. We focus more on the action classification task in our analysis as it is the fundamental purpose of the safety filter.\nComparison of ensembles and individual models By comparing the results of ensembles and member models in Table 1, we observe that the former generally perform equally or better than the average of the latter, as expected. For example, the weighted averaging-based and majority voting-based ensembles of models with a VC1 backbone trained using iDBF achieve action classification accuracies of (75.23, 73.37) and (77.06, 73.10), respectively, which are slightly better than the average of their members (75.72, 71.14). Similarly, member models trained using SABLAS and having a VC1 backbone have an average action classification accuracy of (74.29, 67.06) while their corresponding weighted averaging-based ensemble has a better accuracy of (74.50, 72.01).\nIn the cases with the CLIP backbone, both member models and the ensembles demonstrated relatively low performance. For example, SABLAS with CLIP member models achieve an average accuracy of (82.84, 55.86), which increases to (85.16, 59.51) for the majority voting-based ensemble. This shows that while ensembles help balance or improve performance, if the underlying individual models perform poorly, the ensemble's performance is also likely to be limited.\nWhen we look at ensembles with more diverse member models, such as the majority voting-based one using all training methods (iDBF, SABLAS, DH) and both VC1 and CLIP backbones, we observe an accuracy of (76.99, 80.90), which is better than the average results of all of its member models.\nMost weighted averaging-based ensembles, utilizing variations of the SABLAS and iDBF training methods along with a VC1 backbone, consistently achieve accuracies in the range of 74-80% on classifying safe actions. This marks an improvement compared to the range of 74\u201376% accuracy achieved by the average of member models. Similarly, these ensembles demonstrate 72\u201374% accuracy on classifying unsafe actions, outperforming the member models, which achieve 67\u201371%.\nEven the least effective ensembles, using the averaging aggregation method with uniform weights, show slight benefits by improving both safe and unsafe state/action classification accuracies compared to member models, or by enhancing one metric while causing only a minor decrease in the other.\nComparison of single- and multiple-backbone ensembles Multiple-backbone ensembles outperform single-backbone ones, which can be attributed to the diversity of features captured. VC1 is trained with masked autoencoding on egocentric video datasets from diverse robotic simulators"}, {"title": "Comparison of ensembles on in-distribution (IND) and out-of-distribution (OOD) data", "content": "We considered four towns from the DeepAccident dataset as IND data and withheld the data from the remaining three towns as OOD data. We trained fourteen models on the training dataset portion of the IND data and created their uniform averaging and majority voting-based ensembles and computed their EIR,. Each town has a different environment, but they share similar accident patterns and trajectories. Thus, such a configuration provides a limited view on the the performance of the ensembles on OOD data. EIR remains consistently positive for OOD test data, though slightly lower than for IND test data. This indicates that ensembles maintain advantage over member models on OOD samples, though slightly reduced compared to IND data."}, {"title": "6. Conclusion", "content": "We conducted an extensive analysis of various ensemble configurations, including multiple perception backbones (VC1 and CLIP), different safety filter training methods (SABLAS, iDBF and DH), diverse weight initializations and hyper parameters as well as various model aggregation methods (averaging, weighted averaging, majority voting and consensus-based). Our results showed that ensemble methods consistently improved performance and out-of-distribution generalization of safety filters compared to both member models of the ensemble and to larger single models with comparable number of parameters as the ensemble."}]}