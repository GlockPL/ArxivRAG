{"title": "PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting", "authors": ["Liam Hebert", "Krishna Sayana", "Ambarish Jash", "Alexandros Karatzoglou", "Sukhdeep Sodhi", "Sumanth Doddapaneni", "Yanli Cai", "Dima Kuzmin"], "abstract": "Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and text-prompt-based techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Personalized systems have become increasingly essential across various applications in today's connected digital landscape. These systems leverage insights from past interactions to tailor experiences to each user's unique preferences and needs. Personalized systems are found in diverse domains, from content recommendations like music [18] and movies [8], to personalized medicine [12, 26] and customized educational learning pathways [3, 23].\nAmid these advancements in personalization, large language models (LLMs) applications have emerged in many domains. When trained on an extensive training corpus of natural language tokens and billions of parameters, large language models have displayed exceptional emergent capabilities to tackle multiple tasks without re-training. In other words, LLMs have emerged as a \"one-size-fits-all\" solution to many tasks in various domains. To achieve this goal, LLMs are often trained to reproduce text from large generalist and task-agnostic datasets. However, this emphasis on generalist capabilities can limit the effectiveness of LLMs in personalizing outputs towards the needs and characteristics of specific users.\nResearch to bring personalization to LLMs has primarily explored the development of sophisticated text prompt templates, categorized into simple personalized text prompts, retrieval-augmented prompts, and profile-augmented prompts. Given a list of natural language descriptions of prior actions and a desired task prompt, the simple approach appends the entire set of historical descriptions to the task prompt, leveraging the LLM's inherent in-context learning abilities to personalize the output. However, given the potential for extensive user histories which may exceed the LLM's context window, some studies have proposed retrieval methods to distill this history into the most relevant segments for personalization. Other approaches have also explored using an LLM to synthesize the user's history into brief natural language profiles of user preferences before concatenating them to the desired task prompt."}, {"title": "2 RELATED WORK", "content": "Various text-based prompting methods have been proposed to achieve personalization in language models. These approaches range from concatenating historical interactions into text prompts [7] to using first-stage retrievers for sampling history items [25] and prompt rewriting to summarize critical information [14]. However, text-based prompting suffers from computational constraints due to large inputs and finetuning requirements.\nSoft prompting, a parameter-efficient technique introduced by [13], addresses these challenges by adapting frozen language models to specific tasks. This approach has been extended to various domains, including multilingual tasks [33], transfer learning [29], and vision models [2]. Recently, [21] proposed using the language model itself to create more expressive task soft prompt tokens.\nSoft prompting has also been explored for personalization. Methods like [34] and [19] create trainable user-specific tokens, while [32] uses MLP networks to create personalized soft prompts from tabular user data. UEM [5] explores using dense user-item embeddings as personalized soft prompts. Recent work has also explored adapting other parameter-efficient techniques for personalization. [6] explore injecting adapter layers in the transformer stack to adapt models towards a specific topic domain.\nBuilding upon UEM and previous work, our research introduces novel methods for resampling, diverse encoder architectures, parameter-efficient training, and comprehensive evaluation of various history sampling techniques. This expands the capabilities and understanding of soft prompting for personalization."}, {"title": "3 METHOD", "content": "Our method is grounded in the unified text-to-text approach proposed by T5 [24], where tasks are conceptualized as text generation. Formally, for an input sequence of tokens X, the output sequence Y is modelled probabilistically as $P_{\\theta_{LM}}(Y|X)$ parameterized by the language model's learned weights $O_{LM}$. Prior research in text-to-text tasks can be divided into two predominant prompting methodologies: text-based prompting, which integrates textual directives into the input [4, 20, 31], and soft-prompting, introducing a series of fixed trained tokens preceding the model's input tokens to capture information about a specific task [13, 15].\nConventionally, soft-prompting employs a static, task-specific soft-prompt to enhance parameter efficiency across various linguistic tasks, optimizing the likelihood $P_{\\theta_{LM}}(Y|[T; X])$, where T represents a fixed set of trainable task tokens (called task \"soft prompts\") and X is the task input. Given a set of historical interactions $H_u$ described in natural language for user u, we further train a soft prompt adapter network to jointly compress and resample these interactions into a set of personalized user=specific soft-prompt tokens $P_u$ where $|P_u| \\leq |H_u|$. The generated soft-prompt tokens can then be utilized to optimize $P_{\\theta_{LM}}(Y_u|[P_u;T;X])$, where $Y_u$ is a personalized text output conditioned on user u's history.\nIn practice, PERSOMA (depicted in Figure 1) consists of three primary components: the history encoder, soft prompt adapter and resampler and a large language model decoder. We first encode each natural language user interaction $h_i \\in H_u$ using a SentenceT5 text embedding model [22], our history encoder, creating H. We then feed the set of H into our soft prompt adapter network to generate the set of personalized soft prompt tokens $P_u$. In our results, we experiment with architectures which jointly encode the sequence of H embeddings (Perceiver [11], Transformer [28]) as well as those that encode embeddings individually (MLP projection). Finally, the combined soft prompt [T; Pu] is fed into a frozen PaLM 2 Language Model to generate personalized responses to the given task parameterized by T. To ensure our work is comparable to the prior art, all mentions of PaLM 2 refer to the smallest XXS version."}, {"title": "3.1 PERSOMA Architecture", "content": "Our method is grounded in the unified text-to-text approach proposed by T5 [24], where tasks are conceptualized as text generation. Formally, for an input sequence of tokens X, the output sequence Y is modelled probabilistically as $P_{\\theta_{LM}}(Y|X)$ parameterized by the language model's learned weights $O_{LM}$. Prior research in text-to-text tasks can be divided into two predominant prompting methodologies: text-based prompting, which integrates textual directives into the input [4, 20, 31], and soft-prompting, introducing a series of fixed trained tokens preceding the model's input tokens to capture information about a specific task [13, 15].\nConventionally, soft-prompting employs a static, task-specific soft-prompt to enhance parameter efficiency across various linguistic tasks, optimizing the likelihood $P_{\\theta_{LM}}(Y|[T; X])$, where T represents a fixed set of trainable task tokens (called task \"soft prompts\") and X is the task input. Given a set of historical interactions $H_u$ described in natural language for user u, we further train a soft prompt adapter network to jointly compress and resample these interactions into a set of personalized user=specific soft-prompt tokens $P_u$ where $|P_u| \\leq |H_u|$. The generated soft-prompt tokens can then be utilized to optimize $P_{\\theta_{LM}}(Y_u|[P_u;T;X])$, where $Y_u$ is a personalized text output conditioned on user u's history.\nIn practice, PERSOMA (depicted in Figure 1) consists of three primary components: the history encoder, soft prompt adapter and resampler and a large language model decoder. We first encode each natural language user interaction $h_i \\in H_u$ using a SentenceT5 text embedding model [22], our history encoder, creating H. We then feed the set of H into our soft prompt adapter network to generate the set of personalized soft prompt tokens $P_u$. In our results, we experiment with architectures which jointly encode the sequence of H embeddings (Perceiver [11], Transformer [28]) as well as those that encode embeddings individually (MLP projection). Finally, the combined soft prompt [T; Pu] is fed into a frozen PaLM 2 Language Model to generate personalized responses to the given task parameterized by T. To ensure our work is comparable to the prior art, all mentions of PaLM 2 refer to the smallest XXS version."}, {"title": "3.2 MovieLens Personalized Genre Prediction Task", "content": "To train and evaluate PERSOMA, we utilize the personalized genre prediction task proposed by [5] using the MovieLens dataset [9]. We briefly describe it here for completeness. The MovieLens dataset contains the viewing habits of users and metadata about the movies they are watching. We formulate the genre prediction task on this dataset as follows: given a set of natural language titles $m^t$, descriptions $m^d$ and review scores $m^r$ for each movie $m \\in H_u$ user u watched, predict the set of genres the user likes $G^+$ and dislikes $G^-$. The task is suitable for evaluating personalization as the model must understand the user's preferences to predict personalized genre suggestions. We removed all films with less than 20 reviews to ensure that each movie was high quality and could be seen sufficiently across user sequences in the dataset. After pruning, the final dataset includes 14.4 M reviews from 127 K users spanning 8.2 K unique movies and 19 genres.\nTo apply PERSOMA and other embedding-based personalization baselines on this task, we construct our set of item embeddings $H_u$ by concatenating the history encoder embeddings for $m^t$, $m^d$ and $m^r$. For text-only baselines, we instead represent the input history by concatenating the direct text description of each movie, formatted as\n$U_{text} = { [Title:m^t ,Description:m^d, Rating:m^r] | \\forall m \\in H_u }$\nFinally, to frame this task as a text-to-text generation task, we formulate the target genre prediction output Y as\n$Y = Liked Genres: G^+ Not Liked Genres: G^\u2212$\nwhere a comma delimits each genre in $G^+$ and $G^\u2212$ (ex: \"Liked Genres: Action, Comedy. Not Liked Genres: Romance\").\nRather than use conventional text generation metrics such as BLEU and ROUGE, the predicted genre names are extracted from the generated string to evaluate class-weighted Recall, Precision, and F1. This ensures that we explicitly consider the model's performance on the target task, irrespective of the order in which the genres are predicted. Given that users can like or dislike each of the 19 genres, our evaluation metrics evaluate the problem as a 38-class multi-classification problem\u00b9. We divide the MovieLens dataset into sets of 114K/5K/5K users for training, validation and test respectively, following [5]."}, {"title": "4 RESULTS", "content": "Using the MovieLens Personalized Genre Prediction Task, we evaluate our method against other state-of-the-art personalization baselines. We also conduct an extensive ablation study assessing the impact of various history sampling techniques and item input formats. Unless otherwise noted, we utilize the most recent N movies the user watched to construct the user's history, where N is either 5, 10 or 50 (denoted as \"History N\"). We also include a simple \"Counting\" baseline that selects the top three and bottom three watched genres from the user's history as the predicted liked and disliked genres, respectively.\nIn each experiment, we use 20 learned tokens as our task prompt T. We also use a batch size of 32 and a learning rate of 0.001, training for 300k steps with early stopping to optimize validation F1. When comparing various soft prompt adapter networks, we utilize a 3-layer MLP, 1-layer Transformer network, and 4-layer Perceiver network for each variant. For resampling with Perceiver, the output prompt size is resampled to 20 regardless of history size. We use PaLM2 XXS as the LLM in all our PERSOMA experiments."}, {"title": "4.1 Effect of PERSOMA Soft Prompt Adapter Architecture", "content": "A vital component of the PERSOMA architecture is how we process, resample, and represent the user's historical interactions for personalization. We evaluate PERSOMA's performance towards this capability by comparing it against various personalization methods when faced with a user history of the latest 5, 10, and 50 movies watched, respectively. We compare against end-to-end finetuned models UEM Base and UEM Large from [5], which use T5, and a PaLM 2 [1] XXS model following the text input formulation described in section 3.2 as embedding-based and text prompt-based baselines respectively. We also include a zero-shot Gemini 1.5 Pro model baseline to compare against PERSOMA methods without end-to-end finetuning.\nOur results, shown in Table 1, comparing MLP, Perceiver, and Transformer variants of PERSOMA, demonstrate that both MLP and Transformer variants outperform UEM Base and UEM Large, particularly with larger history sizes. For example, PERSOMA MLP achieves a 0.19 higher F1 score than UEM Large when processing 50 items. Even with a frozen language model, PERSOMA MLP with Frozen LM still outperforms UEM Large by 0.16 F1 with the same history size.\nSince PERSOMA MLP with Frozen LM trains far fewer parameters than UEM Large, our results indicate that finetuning the entire model may be unnecessary with a stronger base LLM (PaLM2 vs T5). Instead, training the adapter to create expressive user embeddings within the LLM language space is sufficient to create significant personalization gains, even at smaller history sizes. This result is further reinforced when resampling with a Perceiver model, unlocking further improvements in token efficiency.\nNext, we inspect the performance of PERSOMA against text-prompting baselines. Here, it is essential to note that text prompting requires significantly more computing, scaling according to text token length rather than having a single token per history item. This is best seen in Table 2, where we note that text prompting would require \u2248 16 000 input tokens to model a user history of 50 items in MovieLens, whereas PERSOMA would only need 130, further reduced to 100 with Perceiver resampling. Further, it is worth noting that PaLM 2 is finetuned end-to-end toward the target task, tuning many more parameters than our Frozen variants.\nDespite utilizing several orders of magnitude less computing, the gap between PERSOMA and PaLM 2 text prompting is only a marginal 0.007 F1 and 0.019 F1 difference when using a history size of 5 and 10, respectively. Comparing PERSOMA Frozen and Gemini, we see that PERSOMA MLP achieves 0.308 higher F1 at a history length of 50. This demonstrates the effectiveness of using in-domain soft prompts for personalization over text prompts.\nFinally, we compare the performance of various PERSOMA soft prompt adapter architectures. Surprisingly, we find that PERSOMA MLP, a smaller and simpler architecture, outperforms PERSOMA Perceiver and PERSOMA Transformer. This is especially the case when utilizing a large history size of 50, where PERSOMA MLP outperforms PERSOMA Transformer and PERSOMA Perceiver by 0.024 F1 and 0.198 F1, respectively.\nThere are a few possible reasons for this loss in optimal performance with sequential adapter models. First, it is essential to note that while MovieLens does provide an ordering of watched films, the order is self-reported by users on the platform via a survey [9]. Given that ordering is not strict, architectures incorporating positional encodings, such as Transformer and Perceiver, may be better suited for tasks that heavily depend on interactions' temporal order. Second, the lack of performance of the Perceiver may be attributed to the reduced number of prompt tokens, which may not have sufficient expressivity as input to the LLM. However, we note that the Perceiver result resampled to a history length of 20 is better than MLP with a history length of 10."}, {"title": "4.2 Parameter Efficient Training with LoRA", "content": "To assess the effectiveness of parameter-efficient finetuning techniques for PERSOMA, we conducted experiments using Low-Rank Adaptation (LoRA) [10] and frozen language model weights, comparing them to a fully finetuned PERSOMA model. For LoRA experiments, we use rank four adaptation for both attention and transformer feedforward layers.\nAs demonstrated in Table 4, when using recency sampling with a history size of 50, both LoRA and frozen PERSOMA variants achieved strong F1 scores of 0.533 and 0.541 respectively, closely matching the performance of the end-to-end finetuned model (0.569 F1). This indicates that parameter-efficient techniques can be effectively applied to PERSOMA, offering a promising avenue for reducing computational costs without sacrificing performance."}, {"title": "4.3 Effect of History Sampling Strategies", "content": "Finally, inspired by the LAMP personalization benchmark [25], we extensively evaluate the impact of various methods sampling strategies on the performance of PERSOMA, PaLM 2 and Counting. For a given history size H, we evaluate the following sampling strategies:\n\u2022 Recency: Select the latest H watched movies\n\u2022 Random: Uniformly sample H watched movies\n\u2022 Long Tail: Uniformly sample H watched movies that are below the global 90th quartile of popularity\n\u2022 Top-K Popularity: Select the top H globally popular movies the user watched\n\u2022 Genre Sample: Sample H movies the user watched according to the users genre density\nThe results of each sampling strategy can be seen in Table 3. We can see that PERSOMA performance improves significantly as the history size increases regardless of sampling strategy. PERSOMA achieved the highest performance with Top-K popularity, achieving 0.593 F1 and 0.294 F1 at history 50 and 5, respectively. Top-K sampling bested Recency sampling, which achieved 0.569 F1 and 0.278 F1 with a history of 50 and 5.\nOne notable outlier sampling strategy was long tail sampling, achieving a lower F1 across PERSOMA and PaLM 2. This performance gap is likely due to many esoteric films not being in the popular zeitgeist and, therefore, not being well represented in the training data for the language model (unlike Top-K sampling, which features only popular movies). Besides long-tail sampling, all other strategies perform within \u00b1 0.03 F1 of each other, demonstrating PERSOMA's robustness."}, {"title": "5 CONCLUSIONS & FUTURE WORK", "content": "This paper introduces PERSOMA, an architecture designed to tackle the challenges of effectively modelling user history for personalization tasks. PERSOMA's core strength lies in its ability to compress and resample historical user interactions into informative in-context soft prompt tokens while employing parameter-efficient techniques for finetuning the language model.\nThrough extensive experimentation with various encoder architectures, sampling methods, and parameter-efficient techniques, we demonstrate the versatility and robustness of PERSOMA. Notably, PERSOMA matches the performance of text-based prompting even when restricted to the same history size and surpasses these baselines with longer histories, all while requiring significantly less computational power. This makes PERSOMA a valuable tool for practitioners seeking efficient natural language personalization without compromising performance.\nWe also believe that embedding representations in LLMs is a promising avenue for further work in personalized prompting. We have investigated the effectiveness of our resampling strategies with Perceiver on the MovieLens dataset, which contains user sequences of O(100) history items. However, real-world production datasets often contain much more extensive interaction histories. Exploring the benefits of resampling with such datasets would be valuable.\nAdditionally, incorporating sparse first-stage retrievers to pre-filter history items for PERSOMA could prove beneficial, mainly when dealing with histories exceeding 500 interactions. Our sampling experiments, specifically the positive impact of task-targeted sampling (Top-K and Genre Sampling), highlight the potential of this approach. Combining these techniques with Perceiver offers a promising way to efficiently represent long user journeys.\nFinally, while we employed PaLM 2 as our large language model decoder, future studies could examine the performance of various LLM decoders of different sizes, such as replacing PaLM 2 with T5 small [24] Phi-2 [16], or MiniLM [30]. Such research could shed light on the applicability of PERSOMA in low-latency and resource-constrained production environments."}, {"title": "B EFFECT OF ITEM INPUT FORMAT", "content": "A key aspect of PERSOMA is its utilization of semantic content within historical items. To assess the sensitivity of our architecture and the personalization task to different representations, we conducted an ablation study examining the impact of removing either the movie title or description"}]}