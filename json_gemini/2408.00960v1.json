{"title": "PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting", "authors": ["Liam Hebert", "Krishna Sayana", "Ambarish Jash", "Alexandros Karatzoglou", "Sukhdeep Sodhi", "Sumanth Doddapaneni", "Yanli Cai", "Dima Kuzmin"], "abstract": "Understanding the nuances of a user's extensive interaction history\nis key to building accurate and personalized natural language sys-\ntems that can adapt to evolving user preferences. To address this,\nwe introduce PERSOMA, Personalized Soft Prompt Adapter archi-\ntecture. Unlike previous personalized prompting methods for large\nlanguage models, PERSOMA offers a novel approach to efficiently\ncapture user history. It achieves this by resampling and compressing\ninteractions as free form text into expressive soft prompt embed-\ndings, building upon recent research utilizing embedding represen-\ntations as input for LLMs. We rigorously validate our approach by\nevaluating various adapter architectures, first-stage sampling strate-\ngies, parameter-efficient tuning techniques like LoRA, and other\npersonalization methods. Our results demonstrate PERSOMA's su-\nperior ability to handle large and complex user histories compared\nto existing embedding-based and text-prompt-based techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Personalized systems have become increasingly essential across\nvarious applications in today's connected digital landscape. These\nsystems leverage insights from past interactions to tailor experi-\nences to each user's unique preferences and needs. Personalized\nsystems are found in diverse domains, from content recommen-\ndations like music [18] and movies [8], to personalized medicine\n[12, 26] and customized educational learning pathways [3, 23].\nAmid these advancements in personalization, large language\nmodels (LLMs) applications have emerged in many domains. When\ntrained on an extensive training corpus of natural language tokens\nand billions of parameters, large language models have displayed\nexceptional emergent capabilities to tackle multiple tasks without\nre-training. In other words, LLMs have emerged as a \"one-size-fits-\nall\" solution to many tasks in various domains. To achieve this\ngoal, LLMs are often trained to reproduce text from large generalist\nand task-agnostic datasets. However, this emphasis on generalist\ncapabilities can limit the effectiveness of LLMs in personalizing\noutputs towards the needs and characteristics of specific users.\nResearch to bring personalization to LLMs has primarily explored\nthe development of sophisticated text prompt templates, catego-\nrized into simple personalized text prompts, retrieval-augmented\nprompts, and profile-augmented prompts. Given a list of natural\nlanguage descriptions of prior actions and a desired task prompt,\nthe simple approach appends the entire set of historical descrip-\ntions to the task prompt, leveraging the LLM's inherent in-context\nlearning abilities to personalize the output. However, given the\npotential for extensive user histories which may exceed the LLM's\ncontext window, some studies have proposed retrieval methods\nto distill this history into the most relevant segments for person-\nalization. Other approaches have also explored using an LLM to\nsynthesize the user's history into brief natural language profiles\nof user preferences before concatenating them to the desired task\nprompt."}, {"title": "2 RELATED WORK", "content": "Various text-based prompting methods have been proposed to\nachieve personalization in language models. These approaches\nrange from concatenating historical interactions into text prompts\n[7] to using first-stage retrievers for sampling history items [25]\nand prompt rewriting to summarize critical information [14]. How-\never, text-based prompting suffers from computational constraints\ndue to large inputs and finetuning requirements.\nSoft prompting, a parameter-efficient technique introduced by\n[13], addresses these challenges by adapting frozen language mod-\nels to specific tasks. This approach has been extended to various\ndomains, including multilingual tasks [33], transfer learning [29],\nand vision models [2]. Recently, [21] proposed using the language\nmodel itself to create more expressive task soft prompt tokens.\nSoft prompting has also been explored for personalization. Meth-\nods like [34] and [19] create trainable user-specific tokens, while\n[32] uses MLP networks to create personalized soft prompts from\ntabular user data. UEM [5] explores using dense user-item embed-\ndings as personalized soft prompts. Recent work has also explored\nadapting other parameter-efficient techniques for personalization.\n[6] explore injecting adapter layers in the transformer stack to\nadapt models towards a specific topic domain.\nBuilding upon UEM and previous work, our research intro-\nduces novel methods for resampling, diverse encoder architectures,\nparameter-efficient training, and comprehensive evaluation of vari-\nous history sampling techniques. This expands the capabilities and\nunderstanding of soft prompting for personalization."}, {"title": "3 METHOD", "content": "Our method is grounded in the unified text-to-text approach pro-\nposed by T5 [24], where tasks are conceptualized as text generation.\nFormally, for an input sequence of tokens X, the output sequence Y\nis modelled probabilistically as \\(P_{\\theta_{LM}}(Y|X)\\) parameterized by the\nlanguage model's learned weights \\(\\theta_{LM}\\). Prior research in text-to-\ntext tasks can be divided into two predominant prompting method-\nologies: text-based prompting, which integrates textual directives\ninto the input [4, 20, 31], and soft-prompting, introducing a series of\nfixed trained tokens preceding the model's input tokens to capture\ninformation about a specific task [13, 15].\nConventionally, soft-prompting employs a static, task-specific\nsoft-prompt to enhance parameter efficiency across various lin-\nguistic tasks, optimizing the likelihood \\(P_{\\theta_{LM}}(Y|[T; X])\\), where T\nrepresents a fixed set of trainable task tokens (called task \"soft\nprompts\") and X is the task input. Given a set of historical interac-\ntions \\(H_u\\) described in natural language for user u, we further train a\nsoft prompt adapter network to jointly compress and resample these\ninteractions into a set of personalized user=specific soft-prompt\ntokens \\(P_u\\) where \\(|P_u| \\le |H_u|\\). The generated soft-prompt tokens\ncan then be utilized to optimize \\(P_{\\theta_{LM}}(Y_u|[P_u;T;X])\\), where \\(Y_u\\) is\na personalized text output conditioned on user u's history.\nIn practice, PERSOMA (depicted in Figure 1) consists of three\nprimary components: the history encoder, soft prompt adapter and\nresampler and a large language model decoder. We first encode\neach natural language user interaction \\(h_i \\in H_u\\) using a SentenceT5\ntext embedding model [22], our history encoder, creating H. We\nthen feed the set of H into our soft prompt adapter network to\ngenerate the set of personalized soft prompt tokens \\(P_u\\). In our\nresults, we experiment with architectures which jointly encode the\nsequence of H embeddings (Perceiver [11], Transformer [28]) as\nwell as those that encode embeddings individually (MLP projection).\nFinally, the combined soft prompt [T; \\(P_u\\)] is fed into a frozen PaLM\n2 Language Model to generate personalized responses to the given\ntask parameterized by T. To ensure our work is comparable to the\nprior art, all mentions of PalM 2 refer to the smallest XXS version."}, {"title": "3.1 PERSOMA Architecture", "content": "the MovieLens Personalized Genre Prediction\nTask\nTo train and evaluate PERSOMA, we utilize the personalized genre\nprediction task proposed by [5] using the MovieLens dataset [9].\nWe briefly describe it here for completeness. The MovieLens dataset\ncontains the viewing habits of users and metadata about the movies"}, {"title": "4 RESULTS", "content": "Using the MovieLens Personalized Genre Prediction Task, we eval-\nuate our method against other state-of-the-art personalization base-\nlines. We also conduct an extensive ablation study assessing the\nimpact of various history sampling techniques and item input for-\nmats. Unless otherwise noted, we utilize the most recent N movies\nthe user watched to construct the user's history, where N is either 5,\n10 or 50 (denoted as \"History N\"). We also include a simple \"Count-\ning\" baseline that selects the top three and bottom three watched\ngenres from the user's history as the predicted liked and disliked\ngenres, respectively.\nIn each experiment, we use 20 learned tokens as our task prompt\nT. We also use a batch size of 32 and a learning rate of 0.001, training\nfor 300k steps with early stopping to optimize validation F1. When\ncomparing various soft prompt adapter networks, we utilize a 3-\nlayer MLP, 1-layer Transformer network, and 4-layer Perceiver\nnetwork for each variant. For resampling with Perceiver, the output\nprompt size is resampled to 20 regardless of history size. We use\nPaLM2 XXS as the LLM in all our PERSOMA experiments."}, {"title": "4.1 Effect of PERSOMA Soft Prompt Adapter Architecture", "content": "A vital component of the PERSOMA architecture is how we process,\nresample, and represent the user's historical interactions for per-\nsonalization. We evaluate PERSOMA's performance towards this\ncapability by comparing it against various personalization methods\nwhen faced with a user history of the latest 5, 10, and 50 movies\nwatched, respectively. We compare against end-to-end finetuned\nmodels UEM Base and UEM Large from [5], which use T5, and a\nPaLM 2 [1] XXS model following the text input formulation de-\nscribed in section 3.2 as embedding-based and text prompt-based\nbaselines respectively. We also include a zero-shot Gemini 1.5 Pro\nmodel baseline to compare against PERSOMA methods without\nend-to-end finetuning.\nOur results, shown in Table 1, comparing MLP, Perceiver, and\nTransformer variants of PERSOMA, demonstrate that both MLP"}, {"title": "4.2 Parameter Efficient Training with LoRA", "content": "To assess the effectiveness of parameter-efficient finetuning tech-\nniques for PERSOMA, we conducted experiments using Low-Rank\nAdaptation (LoRA) [10] and frozen language model weights, com-\nparing them to a fully finetuned PERSOMA model. For LoRA ex-\nperiments, we use rank four adaptation for both attention and\ntransformer feedforward layers.\nAs demonstrated in Table 4, when using recency sampling with\na history size of 50, both LoRA and frozen PERSOMA variants"}, {"title": "4.3 Effect of History Sampling Strategies", "content": "Finally, inspired by the LAMP personalization benchmark [25],\nwe extensively evaluate the impact of various methods sampling\nstrategies on the performance of PERSOMA, PaLM 2 and Counting.\nFor a given history size H, we evaluate the following sampling\nstrategies:\n\u2022 Recency: Select the latest H watched movies\n\u2022 Random: Uniformly sample H watched movies\n\u2022 Long Tail: Uniformly sample H watched movies that are\nbelow the global 90th quartile of popularity\n\u2022 Top-K Popularity: Select the top H globally popular movies\nthe user watched\n\u2022 Genre Sample: Sample H movies the user watched accord-\ning to the users genre density\nThe results of each sampling strategy can be seen in Table 3. We\ncan see that PERSOMA performance improves significantly as the\nhistory size increases regardless of sampling strategy. PERSOMA"}, {"title": "5 CONCLUSIONS & FUTURE WORK", "content": "This paper introduces PERSOMA, an architecture designed to tackle\nthe challenges of effectively modelling user history for personal-ization tasks. PERSOMA's core strength lies in its ability to com-press and resample historical user interactions into informativein-context soft prompt tokens while employing parameter-efficienttechniques for finetuning the language model.Through extensive experimentation with various encoder archi-tectures, sampling methods, and parameter-efficient techniques,we demonstrate the versatility and robustness of PERSOMA. No-tably, PERSOMA matches the performance of text-based promptingeven when restricted to the same history size and surpasses thesebaselines with longer histories, all while requiring significantlyless computational power. This makes PERSOMA a valuable toolfor practitioners seeking efficient natural language personalizationwithout compromising performance.We also believe that embedding representations in LLMs is apromising avenue for further work in personalized prompting. Wehave investigated the effectiveness of our resampling strategies with"}, {"title": "B EFFECT OF ITEM INPUT FORMAT", "content": "A key aspect of PERSOMA is its utilization of semantic contentwithin historical items. To assess the sensitivity of our architectureand the personalization task to different representations, we con-ducted an ablation study examining the impact of removing eitherthe movie title or description (Table 5)."}]}