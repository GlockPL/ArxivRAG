{"title": "A NOVEL APPROACH TO IMAGE EEG SLEEP DATA FOR IMPROVING QUALITY OF LIFE IN PATIENTS SUFFERING FROM BRAIN INJURIES USING DREAMDIFFUSION", "authors": ["David Fahim", "Joshveer Grewal", "Ritvik Ellendula"], "abstract": "Those experiencing strokes, traumatic brain injuries, and drug complications can often end up\nhospitalized and diagnosed with coma or locked-in syndrome. Such mental impediments can\npermanently alter the neurological pathways in work and significantly decrease the quality of life\n(QoL). It is critical to translate brain signals into images to gain a deeper understanding of the thoughts\nof a comatose patient. Traditionally, brain signals collected by an EEG could only be translated into\ntext, but with the novel method of an open-source model available on GitHub, DreamDiffusion can\nbe used to convert brain waves into images directly. DreamDiffusion works by extracting features\nfrom EEG signals and then using the features to create images through StableDiffusion. Upon this,\nwe made further improvements that could make StableDiffusion the forerunner technology in waves\nto media translation. In our study, we begin by modifying the existing DreamDiffusion codebase so\nthat it does not require any prior setup, avoiding any confusing steps needed to run the model from\nGitHub. For many researchers, the incomplete setup process, errors in the existing code, and a lack\nof directions made it nearly impossible to run, not even considering the model's performance. We\nbrought the code into Google Colab so users could run and evaluate problems cell-by-cell, eliminating\nthe specific file and repository dependencies. We also provided the original training data file so users\ndo not need to purchase the necessary computing power to train the model from the given dataset.\nThe second change is utilizing the mutability of the code and optimizing the model so it can be used\nto generate images from other given inputs, such as sleep data. Additionally, the affordability of EEG\ntechnology allows for global dissemination and creates the opportunity for those who want to work\non the shared DreamDiffusion model.", "sections": [{"title": "Introduction", "content": "Our project focused on creating a method to facilitate communication with those impacted by brain impairments,\nutilizing EEG waves as the primary tool. Therefore, our initiative differs from other treatment strategies, as we strive\nto create a non-invasive method to facilitate communication rather than directly providing them with drugs and other\nmedicines to impede the development of the brain ailment. To do so, we expanded off existing methodologies, such as\nDreamDiffusion and StableDiffusion, modifying and cleaning up the code segments to create an open-source method that\nuses the EEG data to predict and image those thoughts. Treatment modalities for brain injuries constitute interventions\ntailored to address the vast array of physical, emotional, and cognitive ramifications associated with the condition.\nIn instances of mild TBI, colloquially referred to as concussions, therapeutic protocols predominantly emphasize\nrest, complemented by adherence to healthcare provider directives aimed at minimizing symptom exacerbation and\nfacilitating optimal recovery. Activities demanding cognitive exertion, such as prolonged computer usage, are typically\ncurtailed. At the same time, abstention from alcohol and drugs is strongly advocated to forestall potential setbacks\nin rehabilitation and mitigate the risk of re-injury, which could precipitate enduring neurological deficits or even\nmortality. Urgent management of TBI is centered on expeditiously stabilizing patients, ensuring adequate cerebral\nperfusion and oxygenation, often necessitating surgical intervention to address complications such as intracranial\nhemorrhage, skull fractures, or elevated intracranial pressure. Pharmacological interventions have a pivotal role in"}, {"title": "Etiology", "content": "It is worth mentioning that TBIs are a broad category of injuries that occur in the brain, affecting the same set of\nlocations. The major types are concussions, contusions, tSAH, and hematoma. A concussion denotes a mild head\ninjury typically characterized by a brief loss of consciousness, often devoid of enduring cerebral damage. A contusion\nsignifies localized brain bruising resulting from head impact, alternatively termed a coup or contrecoup injury. In coup\ninstances, brain trauma occurs directly beneath the point of impact, whereas in contrecoup scenarios, injury manifests\non the opposite side of the effects. Traumatic Subarachnoid Hemorrhage (tSAH) describes bleeding into the cerebral\nspace enveloping the brain. Generally filled with cerebrospinal fluid (CSF), this space is a protective cushion for the\nbrain. Traumatic SAH arises from the tearing of small arteries during the initial injury, leading to blood dispersion\nacross the brain's surface and subsequent wide-ranging effects. Hematoma refers to a blood clot formation following\nblood vessel rupture. Escaping blood thickens and coagulates outside the regular circulatory system. Coagulation serves\nas the body's innate mechanism to staunch bleeding. Hematomas may vary in size and exert pressure on the brain, with\nsymptoms contingent upon the clot's location. An epidural hematoma emerges between the skull and the brain's dura\nlining, while a subdural hematoma forms between the brain and the dura. An intracerebral hematoma develops deep\nwithin brain tissue. Eventually, the body reabsorbs the clot, and surgical intervention is sometimes necessary for more\nconsiderable clot removal [2]. From this point on, a reference to TBIs includes any of these injuries that demonstrate a\nsimilar degree of trauma to the brain.\nAs there are many causes of TBIs among different populations with dissimilar groups in relationship with age, race, and\nsocial determinants, the affected brain region varies. Each region controls functions associated with it, yielding a wide\nrange of symptoms that have proved troublesome to treat. The brain is divided into four major lobes and two brain stem\nsections"}, {"title": "Methodology", "content": "EEG: An electroencephalogram (EEG) utilizes electrodes attached to the scalp to capture electrical brain activity [5].\nComparable to other diagnostic tests such as fMRI and MRI, EEG can detect various conditions stemming from TBIs,\nalbeit at different marginal costs. While fMRI and MRI yield superior-quality brain activity records with minimal noise,\nthey entail longer acquisition times and significant equipment expenses. Conversely, EEG offers cost-effectiveness and\nquicker turnaround between scans, albeit with inherent signal noise. The process yields three states of the EEG signal:\noriginal, deconstructed, and reconstructed. Notice how the reconstructed signal's general trend closely corresponds with\nthe original signal collected from patients. Specifically, one prominent oscillation observed is the gamma\nwave, commonly associated with cognitive processes such as thinking or problem-solving.\nDeveloping an EEG Encoder that can encode EEG signals into semantic representations that can be accurately decoded\nto match the overall EEG signal is paramount. Embeddings are tokens positioned in nth-dimensional space. Due to the\nlimited EEG-image pair data, instead of immediately training to generate embeddings that accurately represent images,\nthe DreamDiffusion EEG Encoder is first trained to generate embeddings that represent a variety of unique EEG signals.\nTherefore, we used a more substantial dataset of EEG recordings rather than a limited collection of EEG-image pairs.\nSegments of this EEG data are randomly obscured, leaving behind a masked EEG signal that is then tokenized and fed\ninto the EEG Encoder. The encoder generates embeddings, which are then passed to a decoder that reconstructs the\nmasked portions of the signal. The overall trends of the reconstructed and original signal are compared to assess the\naccuracy of the EEG Encoder's embedding space.\nCLIP: Contrastive Language-Image Pre-training (CLIP), a simplified version of ConVIRT (Contrastive Learning of\nMedical Visual Representations from Paired Images and Text) trained from scratch, is an efficient means of learning\nimage representations through natural language guidance. CLIP conducts joint training of an image encoder and a text\nencoder to accurately predict the pairings within a batch of (image, text) training examples. During testing, the learned\ntext encoder constructs a zero-shot linear classifier by embedding the names or descriptions of the target dataset's\nclasses. During pre-training, CLIP is tasked with discerning the actual occurrences among the potential NXN (image,\ntext) pairings within a batch. It achieves this by cultivating a multi-modal embedding space, where the image and\ntext encoders are trained concurrently to maximize the cosine similarity of embeddings for the batch's N2 - N real\npairs while minimizing the similarity for incorrect pairings. This objective is optimized through symmetric\ncross-entropy loss over the similarity scores. With CLIP trained on a diverse assortment of (image, text) pairs, it can be\ninstructed through natural language to predict the most relevant text snippet corresponding to an image without explicit\noptimization for the task, reminiscent of the zero-shot capabilities showcased by GPT-2 and GPT-3.\nStable Diffusion: Stable Diffusion operates as a diffusion model, distinguishing it from conventional image generation\nmodels. Essentially, diffusion models utilize Gaussian noise to encode an image and subsequently employ a noise\npredictor with a reverse diffusion process to reconstruct the image. What sets Stable Diffusion apart is its departure\nfrom utilizing the pixel space of the image. Instead, it employs a reduced-definition latent space. This choice is\nmotivated by the immense number of possible values in the pixel space of a color image with a resolution of 512x512,\namounting to 786,432 values. In contrast, Stable Diffusion employs a compressed image with only 16,384 values,\nreducing processing demands significantly. The efficacy of the smaller latent space hinges on the non-random nature of\nnatural images, which Stable Diffusion leverages by utilizing variational autoencoder (VAE) files in the decoder to\ncapture fine details like eyes. The architectural components of Stable Diffusion include a VAE, forward and reverse\ndiffusion, a noise predictor, and text conditioning. The VAE encompasses separate encoder and decoder components.\nThe encoder compresses the 512x512 pixel image into a smaller 64x64 model in the latent space, facilitating easier\nmanipulation. Conversely, the decoder restores the model from latent space into a full-size 512x512 pixel image.\nForward diffusion systematically introduces Gaussian noise to an image until only random noise remains, rendering\nthe final noisy image unidentifiable. All images undergo this process during training, although forward diffusion is\nonly employed for image-to-image conversion. Reverse diffusion is a parameterized process that iteratively reverses\nthe forward diffusion, allowing for the generation of unique images based on prompts and training data. The noise\npredictor, implemented as a U-Net model, is essential for denoising images. It estimates the noise in the latent space\nand subtracts it from the image, iteratively reducing noise according to user-specified steps. This component is sensitive\nto conditioning prompts that aid in determining the final image. Text conditioning, a common form, involves text\nprompts to guide image generation. Each word in a textual prompt is analyzed by a CLIP tokenizer and embedded into\na 768-value vector. Stable Diffusion feeds these prompts from the text encoder to the U-Net noise predictor via a text\ntransformer, enabling the generation of diverse images in the latent space by adjusting the random number generator's\nseed. This process creates photorealistic images from text.\nDreamDiffusion: The implementation of DreamDiffusion in Colab utilizes a Google Drive folder for storage, where\nvarious data, including images and EEG data, are downloaded by the Colab notebook. DreamDiffusion integrates Stable\nDiffusion, which employs an encoder called CLIP to convert text into embeddings for image generation. However,\nDreamDiffusion aims to replace CLIP with its own EEG Signals Encoder while still utilizing Stable Diffusion's image\ngenerator. The objective is to align the embedding space of the DreamDiffusion EEG Encoder with CLIP's embedding\nspace. Given the inherent noise in EEG data, DreamDiffusion employs a strategy to address this challenge. Initially, it\ntrains on a large dataset of EEG data rather than a limited collection of EEG-image pairs. Portions of this EEG data are\nrandomly masked, and the remaining signal is tokenized before being fed into the encoder, which yields embeddings.\nSubsequently, a decoder model is employed to generate the masked parts in the signal from the embeddings, and the\nreconstructed signal is compared against the original. However, the embedding space produced by DreamDiffusion\ndiffers significantly from CLIP's embedding space, but CLIP's embedding space is ideal because it closely corresponds\nto images. To bridge this gap, a smaller dataset of EEG-image pairs is utilized. The image is encoded into CLIP\nembeddings for each pair to obtain the appropriate embeddings for Stable Diffusion. Simultaneously, the EEG signal is\nfed into the previously trained EEG Encoder, generating its embedding [12]. A comparison is made between CLIP\nand the EEG Encoder embeddings, and DreamDiffusion adjusts the EEG Encoder embedding space to minimize any\ndisparity between these embedding spaces."}, {"title": "Results", "content": "Training Data: Before the EEG pretrain, we trained the model on the training data. On Colab Pro+, it took around 15\nhours and got down to a loss of approximately 0.000243. This is calculated by finding the difference between 1 and the\ntraining loss step. Our code is shown below.\nEEG Input: We imported EEG files and found that the image creation could accurately reflect the patients' dreams.\nBelow are some reconstructed images that could be created from EEG waves.\nImage Generation: With the model, we could also improve functionality and generate images close to the original\ninput."}, {"title": "Improvements", "content": "Data Augmentation (DA) with Deep Learning: Data augmentation techniques involve creating variations of existing\ndata to increase the diversity of the dataset. This could include applying transformations such as time warping,\namplitude scaling, or adding random noise to simulate real-world variability for EEG data. Deep learning methods\ncan be employed to implement these augmentations effectively, ensuring that the generated data remains realistic and\nretains its underlying patterns.\nFourier Series for Wave-to-Time Conversion: The Fourier series can decompose EEG signals into their constituent\nfrequency components, allowing for a representation in the time domain. By performing Fourier analysis, we can\nidentify dominant frequencies and their temporal evolution, providing insights into the rhythmic patterns of brain\nactivity. Implementing Fourier series equations enables us to extract meaningful features from EEG data and accurately\nmodel temporal dynamics.\nSampling Ratio Optimization: Optimizing the sampling ratio involves determining the appropriate proportions of\ndata allocated for training and testing to maximize model performance. This optimization process requires careful\nconsideration of dataset size, model complexity, and generalization requirements. We can find the optimal balance\nbetween training and testing data by experimenting with different sampling ratios to achieve superior model accuracy\nand robustness.\nNuclear Norm Regularized Deep Neural Network (NRDNN): Nuclear norm regularization imposes constraints\non the singular values of the weight matrices in deep neural networks, promoting sparsity and enhancing model\ngeneralization. By incorporating NRDNN into our model architecture, we can effectively differentiate various brain\nregions from EEG data and improve classification accuracy. This regularization technique helps prevent overfitting and\nenhances the stability and reliability of the trained model.\nActivities of Daily Living (ADLs) Integration: Integrating ADL-related EEG data into the model training process\nallows us to capture additional contextual information about the patient's state and environment. By incorporating\nfeatures related to daily activities such as sleeping, eating, or exercising, the model can learn to recognize patterns\nassociated with different states of consciousness and adapt its predictions accordingly. This integration enhances the\nmodel's ability to accurately assess the patient's condition and monitor their recovery progress.\nNeural Network Techniques for Error Reduction: Neural network techniques such as attention mechanisms,\nensemble learning, and anomaly detection can help reduce human error in EEG data analysis by automating tedious\ntasks and flagging potential errors or inconsistencies. Attention mechanisms enable the model to focus on relevant\nfeatures in the data, while ensemble learning combines multiple models to improve prediction accuracy. Anomaly\ndetection algorithms can identify abnormal patterns in EEG signals, alerting clinicians to potential issues and facilitating\ntimely intervention."}, {"title": "Existing Gaps in Research", "content": "The main challenge during our project revolved around setting up the DreamDiffusion environment and seamlessly\nintegrating all the necessary programs. We faced issues related to a lack of documentation, complex dependencies, and\nerrors in code syntax, all of which demanded significant time and effort to resolve. As a collaborative effort, we have\nmade the code readily available to facilitate further work on the model and encourage new implementations. However,\nthere is a pressing need within the research community to streamline these resources, ensuring they are easily accessible\nand well-documented to minimize barriers to entry and facilitate smoother workflows.\nMoreover, substantial gaps remain in efforts to enhance the quality of life (QoL) for patients with neurological conditions.\nWith advancements in machine learning (ML) models and neural networks on the horizon, there is an increased urgency\nto leverage these technologies effectively in healthcare. Yet, there is a notable deficit in research focusing on leveraging\nthese advancements to address patients' immediate needs and challenges. Researchers and practitioners must bridge\nthis gap by developing and implementing innovative solutions that directly impact patient outcomes and improve their\noverall QoL. This entails a multidisciplinary approach incorporating input from healthcare professionals, technologists,\nand patients to ensure that emerging technologies are effectively translated into practical solutions that benefit those in\nneed."}, {"title": "Conclusion", "content": "In summary, traumatic brain injuries (TBIs) and associated neurological conditions represent critical areas of inquiry in\ncontemporary medical research, holding profound implications for both patient outcomes and quality of life. Within\nthis domain, the advent of innovative technologies such as DreamDiffusion offers promising avenues for advancing\nneuroimaging capabilities and therapeutic interventions. By harnessing pre-trained text-to-image models and employing\nsophisticated signal modeling techniques, DreamDiffusion endeavors to overcome inherent challenges in EEG data\nanalysis. Through integration with CLIP image encoders and EEG signal encoders, DreamDiffusion seeks to align\nembedding spaces, enhancing the accuracy and efficacy of image generation from EEG signals. Nevertheless, the review\nunderscores persistent gaps in research and implementation, particularly concerning the accessibility and usability\nof such advanced technologies. Complex setup procedures, dependency issues, and code errors remain significant\nimpediments, necessitating concerted efforts to address and mitigate these barriers. Furthermore, there is a compelling\nimperative to prioritize enhancing patient quality of life through translating technological advancements into tangible\nclinical applications. Collaboration across multidisciplinary teams, including researchers, healthcare professionals,"}]}