{"title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution", "authors": ["Corban Rivera", "Grayson Byrd", "William Paul", "Tyler Feldman", "Meghan Booker", "Emma Holmes", "David Handelman", "Bethany Kemp", "Andrew Badger", "Aurora Schmidt", "Krishna Murthy Jatavallabhula", "Celso M de Melo", "Lalithkumar Seenivasan", "Mathias Unberath", "Rama Chellappa"], "abstract": "Robotic planning and execution in open-world environments is a complex problem due to the vast state spaces and high variability of task embodiment. Recent advances in perception algorithms, combined with Large Language Models (LLMs) for planning, offer promising solutions to these challenges, as the common sense reasoning capabilities of LLMs provide a strong heuristic for efficiently searching the action space. However, prior work fails to address the possibility of hallucinations from LLMs, which results in failures to execute the planned actions largely due to logical fallacies at high- or low-levels. To contend with automation failure due to such hallucinations, we introduce ConceptAgent, a natural language-driven robotic platform designed for task execution in unstructured environments. With a focus on scalability and reliability of LLM-based planning in complex state and action spaces, we present innovations designed to limit these shortcomings, including 1) Predicate Grounding to prevent and recover from infeasible actions, and 2) an embodied version of LLM-guided Monte Carlo Tree Search with self reflection. ConceptAgent combines these planning enhancements with dynamic language aligned 3d scene graphs, and large multi-modal pretrained models to perceive, localize, and interact with its environment, enabling reliable task completion. In simulation experiments, ConceptAgent achieved a 19% task completion rate across three room layouts and 30 easy level embodied tasks outperforming other state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on the same benchmark. Additionally, ablation studies on moderate to hard embodied tasks revealed a 20% increase in task completion from the baseline agent to the fully enhanced ConceptAgent, highlighting the individual and combined contributions of Predicate Grounding and LLM-guided Tree Search to enable more robust automation in complex state and action spaces. Additionally, in real-world mobile manipulation trials, conducted in randomized, low-clutter environments, a ConceptAgent-driven Spot robot achieved a 40% task completion rate, demonstrating the performance of our perception system in real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "The aspiration to have competent robotic counterparts that can reason about the world in an open manner and be commanded with natural language has been a long term goal of the robotics community. Recent advancements leveraging data-driven methodologies and robust robot models have showcased notable strides [1]\u2013[4]. However, existing systems often exhibit fragility, rigidity, and struggle when confronted with unforeseen circumstances. Many of these systems rely on being given a prescanned environment which limits their use to well known environments [5], [6]. Even the most expansive robotic models typically function optimally only within familiar settings [7], [8]. This fragility is particularly evident in scenarios with limited available robotic data, such as unstructured domestic environments. This lack of adaptability stands in stark contrast to the remarkable capabilities demonstrated by large-scale vision models, which excel in tasks such as semantic comprehension, object detection, and bridging visual and linguistic representations [5], [9]-[15]. Despite the maturity of fundamental robotic skills like navigation, grasping, and object manipulation [16]-[19], the integration of modern vision models with these foundational capabilities often yields underwhelming results. This challenge was underscored by the NeurIPS 2023 OVMM challenge, where the winning solution achieved a success rate of only 33% [20], [21]. The complexity arises from a multitude of factors rather than a singular obstacle. Successful completion of general, long-horizon, embodied tasks requires intelligently generating a sequence of discrete actions, each of which are non-trivial both in their implementation and their execution. Each of these actions has the potential to accumulate and propagate error. Additionally, to create an actionable plan and recover when that plan goes awry requires a competent abstraction of the physical environment as well as a planner capable of fully leveraging that abstraction. Addressing these challenges necessitates a nuanced framework that seamlessly integrates natural language comprehension, full scene abstraction and understanding, and resilient reasoning.\nIn response to these challenges, we introduce ConceptAgent, a natural language-driven robotic platform designed for task execution in unstructured settings. ConceptAgent integrates advanced perception, manipulation, movement, speech, coding, and search primitives, allowing it to tackle a wide variety of tasks described in natural language. At its core, ConceptAgent combines large multi-modal pretrained models with novel planning and reasoning enhancements, enabling robust task execution in complex, dynamic environments.\nKey innovations of ConceptAgent include:\n\u2022 Formal Precondition Verification: ConceptAgent integrates precondition grounding, a mechanism that formally verifies action constraints before execution, preventing infeasible actions and facilitating failure recovery. This ensures the agent can maintain task progress, even in unstructured environments."}, {"title": "II. RELATED WORK", "content": "Traditional Robotic Task Planning - Task planning in robotics involves finding a sequence of actions to achieve specific goals within an environment. Traditional methods make use of domain-specific languages like PDDL [23], [24] and finite state machines [6], along with grammars and semantic parsing [25], search techniques [26], and heuristics [27] to find solutions. These approaches can struggle with scalability and adaptability to real-world scenarios due to their incredibly large state-action space. Hierarchical, imitation, and reinforcement learning-based alternatives face challenges related to data demands and scalability [28], [29]. Our approach leverages the common sense capabilities of Large Language Models (LLMs) to sidestep the brute force search problem associated with generating task plans in un- constrained environments. Leveraging realtime incremental 3D scene graphs for grounding [5] and large pretrained multi- modal models [30], our approach can formulate execution paths that were not preconceived, resulting in a capability to adapt and use affordances in ways that were not explicitly encoded by humans.\nTask Planning with Large Language Models - Task planning with LLMs, specifically translating natural language instructions into robotic task plans, is gaining traction in the field. Previous studies have effectively employed pre- trained LLMs' contextual learning abilities to generate ac- tionable plans for embodied agents [2], [31]\u2013[36]. However, a significant challenge remains in grounding these plans within the operational context of the robot. Past efforts make use of object detection models [32], PDDL environment representations [31], [37], or value functions [2] for this purpose, but they are primarily effective in single-room environments and struggle with scalability. In our work, we improve scalability by re-imagining ConceptGraphs [5] as a service with limited scope. This allows the agent to select only the portions of the graph that are most relevant to a given task.\nRetrieval Augmented Generation for Task Planning - Integrating external knowledge into LLMs has emerged as a promising approach to enhance their reliability. This in- volves using external tools to provide feedback or additional information to guide LLM output generation. Leveraging external knowledge in tool-based agents requires API calls to external tools [38], [39] or textual feedback from the operating environment [33], [40], [41]. Building on these concepts, we introduce an agent-based framework that reacts"}, {"title": "III. CONCEPTAGENT", "content": "ConceptAgent enables natural language-driven task plan- ning and execution. Figure 2 illustrates the framework. The system responds to natural language task requests, plans and executes in a closed loop over a series of steps. Intermediate feedback in the form of abstracted environment observations and information about unsatisfied action constraints (Precon- dition Grounding) allows the robot to adapt and recover in real-time.\n\nWe address the problem of real-time task execution in open-world environments driven by unconstrained natural language goals. In these scenarios, the key challenge lies in enabling a robotic system to understand and act upon com- plex, context-dependent language instructions, while simulta- neously adapting to dynamic and unstructured environments.\nThe system must continuously interpret natural language expressions without predefined constraints, translating them into actionable plans in real-time. Full scene understanding is achieved through a combination of an incrementally-updating language-aligned 3d scene graph [5] and large pretrained multi-modal models.\nGiven the open-world nature of the problem, the system must demonstrate the ability to generalize across varying environments and manage unknown objects, unforeseen ob- stacles, and unexpected environmental changes. The primary metric for success is task completion rate, underscoring the importance of reliable, autonomous decision-making in diverse and unpredictable contexts."}, {"title": "B. LLM-guided Tree Search with Self-Reflection", "content": "Inspired by [42], ConceptAgent agent extends prior work in LLM-guided tree search to the real world domain. LLM- guided tree search follows the standard phases of traditional MCTS: selection, expansion, simulation, and backpropaga- tion, but introduces LLM-based enhancements to expansion and simulation. Below, we detail each phase, emphasizing how the LLM guides planning and decision-making.\n1) Selection Based on Upper Confidence Bound: The selection phase is based on the Upper Confidence Bound (UCB1) algorithm [43], which balances exploration and exploitation in selecting the next node to explore. For a node $s_t$ with child nodes $a_i$, the UCB1 criterion is used to select the action that maximizes the following equation:\n$a^* = \\arg \\max_{a_i} Q(s_t, a_i) + c \\cdot \\sqrt{\\frac{\\log N(s_t)}{N(s_t, a_i)}}$ where $Q(s_t, a_i)$ is the expected reward of taking action $a_i$ at state $s_t$, $N(s_t)$ is the number of times state $s_t$ has been visited, $N(s_t, a_i)$ is the number of times action $a_i$ has been taken from $s_t$, and c is a constant controlling the exploration- exploitation trade-off.\n2) LLM-Guided Expansion: In the expansion phase, tra- ditional MCTS adds new child nodes representing possible future states. For embodied tasks, the state-action space explodes at even small depths in the tree search, requiring a powerful heuristic to filter out irrelevant states [42]. In ConceptAgent, we use the common sense of LLMs to serve as this heuristic. Given a current state $s_t$ and a natural language task goal g, the LLM generates a set of plausible actions $A_t = L(s_t,g)$. These actions are then added to the tree as new nodes, representing potential transitions from $s_t$. This LLM-guided expansion allows the agent to explore a broader range of actions that align with the natural language task, thus avoiding state space explosion and improving task understanding and goal relevance.\n3) LLM-Based Critique for Simulation and Scoring: In the traditional MCTS framework, the simulation phase evaluates state transitions through random rollouts in the environment that backpropogate the value of the terminal state of that random rollout. For ConceptAgent, we replace this with a LLM-based critique mechanism that assesses a planned sequence of actions holistically. At the leaf node of the search tree, rather than simulating state transitions, the LLM critic is given the sequence of actions planned from time t to t+k. The LLM evaluates the efficiency, relevance, and goal alignment of the entire plan, producing a planning score that serves as the reward signal for backpropagation. Formally, let $\u03c4 = \\{a_t, a_{t+1},...,a_{t+k}\\}$ be the sequence of planned actions from state $s_t$ over k time steps. The LLM critic, L, is tasked with evaluating the quality of this action sequence in achieving the task goal g. The critique score C(\u03c4, g) reflects how well the action sequence satisfies the task requirements and the efficiency of the plan. The score incorporates penalties for unnecessary steps or inefficient actions.\n4) Backpropagation: Once the critique score $C(s_{t+k}, g)$ is obtained, the backpropagation phase proceeds as in stan- dard MCTS. The critique score is propagated back through the tree, updating the value estimates Q(s, a) for each state- action pair along the trajectory \u03c4. The update rule for each node is as follows:\n$Q(s_t, a_i) \\leftarrow \\frac{1}{N(s_t, a_i)} \\sum_{j=1}^{N(s_t, a_i)} C(\\tau_j, g),$ where N(st, ai) is the number of times the action ai has been selected from state st, and $C(\u03c4_j,g)$ is the critique score from the j-th simulation. This ensures that the agent's planning decisions reflect the accumulated knowledge from both successful and unsuccessful action sequences."}, {"title": "C. Precondition Grounding", "content": "Once MCTS with LLM-guided expansion and critique- based scoring identifies the optimal action sequence, Concep- tAgent attempts to execute the chosen next action. If the tool execution fails, an agent must contextualize the failure with respect to the planning history to determine proper recovery measures. For example, upon failing to search inside a cabi- net, the agent must analyze its previous actions and determine a reason for the execution failure. In this case, there are many possible reasons: the cabinet is not open, the agent has not moved within reach of the cabinet, the agent is currently holding an object and cannot execute the search tool due to manipulator constraints, etc. Through experiments, we find the baseline agent struggles to extrapolate such broad con- clusions from the action history. Therefore, to improve agent recovery capabilities, we introduce Precondition Grounding as an enhancement to our LLM planner. The precondition grounding is autonomously generated, promoting scalability, and enables formal validation and recovery feedback during tool execution, greatly improving task completion.\n1) LLM-Derived Preconditions for Action Feasibility: Precondition Grounding requires a defined set of precondi- tions for each tool. At inference time, these preconditions are verified by formal methods to ensure that a chosen tool can be executed given the perceived state of the environment. The creation of such preconditions generally lacks scalability [23], [24], but recent work has explored the ability of LLMs for precondition generation [44]. Extending this idea, we prompt the agent to generate predicate based preconditions for each tool.\nFormally, these preconditions encode the logical require- ments for successful tool execution. Let $A = \\{a_1,..., a_n\\}$ represent the set of n actions available to the agent. Then, $P_i = LLM(S, n_i, d_i, atts)$ is the set of preconditions for ai generated by conditioning the LLM on the system prompt, S, action name, ni, action description, di, and list of object and agent boolean attributes, atts. At time, t, we define the Formal Verification function, $F(s_t, P_c)$, as:\n$F(s_t, P_c) = \\begin{cases} 1, & \\text{if } P_c \\text{ is satisfied in } s_t \\\\ 0, & \\text{if } P_c \\text{ is not satisfied in } s_t \\end{cases}$ Where c represents the index of the chosen action in A and $s_t$ represents the state of the system at time, t. If F(st, Pc) = 1, then the action, ac, is executed. If F(st, Pc) = 0, then the action is deemed infeasible and execution is aborted.\n2) Feedback for Future Planning: When execution is aborted due to unmet preconditions, the system provides explicit feedback to the LLM agent, specifying which pre- conditions were unsatisfied.\nLet U be the set of unsatisfied preconditions for action c in A. If F(st, Pc) = 0, then:\n$U_c = \\{p \u2208 P_c | p \\text{ is not satisfied in } s_t\\}$ The set Uc is formatted and returned to the LLM as feedback, allowing the agent to update its internal model of the environ- ment and adjust future action sequences to avoid proposing actions whose preconditions are unlikely to be met. This iterative feedback loop refines the agent's planning process, improving task completion rates over time by preventing the repetition of infeasible actions and guiding the agent towards valid recovery plans. Additionally, using feedback from Uc, the LLM revises its future plans, internalizing the state feedback to adjust the sequence of future actions. The agent can either attempt to satisfy the unsatisfied preconditions or generate alternative actions that are feasible given the current state st. This mechanism ensures that ConceptAgent's action plans are dynamically adapted to the environment."}, {"title": "D. Implementation Details", "content": "ConceptAgent make use of a library of parametric skills to complete tasks. In physical experiments these skills include object localization, navigation, grasping, manipulation, visual question answering, speech, code execution, and web search. Due to space constraints, we elaborate on these skills in the Appendix."}, {"title": "IV. RESULTS", "content": "The goals of our experiments were twofold: first, to evaluate the effectiveness of ConceptAgent's planning and reasoning enhancements, including formal precondition ver- ification and LLM-guided Monte Carlo Tree Search, in both simulated and real-world environments; and second, to assess its adaptability and task completion performance in diverse and unstructured scenarios, benchmarked against state-of- the-art generative AI reasoning approaches."}, {"title": "A. Task Planning and Execution in Simulation", "content": "Our simulation experiments were run in the AI2Thor sim- ulator, a 3D simulator for kitchen environments that contains a variety of manipulable objects within the environment. To evaluate our planner, A set of 30 randomly constructed object rearrangement tasks were generated. The object rear- rangement tasks were characterized by simple, clear object rearrangement instructions e.g. \"The agent should pick up the credit card that is on the counter and place it on the kitchen drawer\". An additional set of 40 tasks were hand crafted by a human expert. The tasks are split into 20 moderate tasks and 20 hard tasks. Moderate tasks are characterized by simple, object rearrangement instructions where task objects may be concealed within other objects e.g. \"Put a Tomato in a Cabinet.\". The tomato may initially be in the refrigerator requiring additional exploration and manipulation. Difficult tasks involve longer horizon goal states and increased am- biguity in the task statement e.g. \"Chill the tomato and put the bowl away in the cabinet.\".\nWe first evaluate our agent's ability to autonomously generate preconditions for each provided action. To do this, a human expert first generated a set of ground truth precon- ditions for each possible agent action. We then compare the generated preconditions for each tool to that of the ground truth preconditions. The ground truth set contained 42 pre-conditions spanning across 10 different tools while the LLM generated set contained 38. We compared each precondition in the generated set to the ground truth set and noted 37 out of the 38 generated preconditions correctly matched to the 42 ground truth preconditions. We observe 6 ground truth preconditions were missing from the generated set. This represents an accuracy (correct generated / total generated) of 97.4% and a recall (correct generated / total ground truth) of 88.1%. Although 6 ground truth preconditions were missing from the generated set, this will be no worse than the baseline agent without any verification mechanism. In short, only 2.6% of the LLM generated preconditions are incorrect and thus have real potential to cause harm to planning beyond that of the baseline agents without enhancements. We fur- ther evaluate the LLM generated preconditions empirically through their impact on planning performance in Table I."}, {"title": "B. Comparison to Baselines", "content": "We compared our approach to several state-of-the-art closed-loop generative AI-based reasoning baselines adapted for embodied tasks. The approaches were evaluated with 30 object rearrangement benchmark tasks across 3 room layouts. Specifically, we evaluated the performance of these reasoning approaches alongside multiple levels of expansion in ConceptAgent. As a positive control, we included results from a baseline approach that utilized an LLM nearly 10 times larger than those used in other methods.\nOur results, summarized in Table II, show that the baseline methods, including ReAct [34] and Tree of Thoughts (ToT) [45], both using an 8-billion-parameter LLM, achieved task completion rates of 10% and 8%, respectively. In compari- son, ConceptAgent, with 10 and 20 expansions, achieved task completion rates of 14% and 19%, respectively. These results highlight the effectiveness of our approach, demonstrating a clear improvement in task performance with additional ConceptAgent expansions.\nNotably, with 20 expansions, ConceptAgent's performance (19% task completion) approached that of the ReAct base- line, which relied on an LLM nearly 10 times larger. This illustrates the efficiency of ConceptAgent in leverag- ing smaller models while still achieving competitive results through improved planning and reasoning mechanisms."}, {"title": "C. Ablation Experiments", "content": "Next, we explored ablations of ConceptAgent on the moderate and hard benchmarks. In these experiments we ablate the two key planning enhancements: Precondition Grounding and LLM-MCTS. The results shown in Table I report task success rate separately for the moderate and hard task benchmarks along with an overall average over the 40 tasks.\nWithout predicte grounding or LLM-driven MCTS, Con- ceptAgent completes 5% of the moderate and 5% of the hard tasks. When combined with precondition grounding, we find that ConceptAgent was able to complete 15% of the moderate tasks and 10% of the hard tasks successfully. This shows that, while Llama3.1 70B tends to choose tools under invalid execution constraints, it is able to recover from these planning errors given the appropriate feedback in the form of unsatisfied preconditions. LLM-MCTS was evaluated with 5 expansion steps in the ablation experiments. When ConceptAgent includes LLM-driven MCTS and pre- condition grounding we observed the highest task completion on both the moderate and hard benchmark tasks achieving 35% and 15% respectively."}, {"title": "D. Physical Trial for Object Rearrangement", "content": "In these trials, we aimed to characterize failure modes in physical mobile manipulation experiments. To benchmark the system's performance, we began with object rearrange- ment tasks, modeled after of experiments of prior work [6], [46]\u2013[48]. These tasks are formulated as \"pick up object X and place it on/in object Y\". We ran 30 iterations of these tasks, selecting objects and their destinations at random. Every 5 iterations, the scene layout would be randomized, and every 10 iterations, the scene would be decluttered like prior work [6], where for each declutter procedure, we select the select 1/3 of the objects that showed difficulty for the system in terms of semantic ambiguity or grasping difficulty and relocate the remaining objects to accessible locations. In these trials, ConceptAgent was evaluated with GPT-4, single expansion, and standard feedback. Metrics To evaluate these results, we recorded the following metrics: Success rate, Partial success rate / step-wise success rate (SSR).\nAlthough the open vocabulary step-wise success rates (SSR) were relatively high (Table III), completing the task requires successfully executing multiple sequential steps. As such, the overall task success rate is the product of the step- wise success rates, resulting in a total success rate of 20%. This occurred despite the system generating highly accurate plans in each trial and achieving 100% successful navigation. Table IV presents the overall task success rates under different levels of clutter. After decluttering, we observed an increase in success rate from 10% to 40%, highlighting that both visual clutter and semantic ambiguity are critical factors affecting task performance. Figure 3 illustrates the step-wise and overall success rates in a Sankey diagram, mapping the primary failure modes observed during the trials."}, {"title": "V. CONCLUSIONS", "content": "Large multi-modal pretrained models are essential for enhancing the robustness of robotic plan execution in open- world environments. These models enable ConceptAgent to integrate perception and reasoning, allowing for more accurate and adaptable task execution in diverse, unstructured scenarios."}, {"title": "VI. ACKNOWLEDGEMENTS", "content": "This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-21-2-0211. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. DISTRIBUTION A. Approved for public release; distribution unlimited."}, {"title": "VII. SYSTEM PROMPT FOR GENERATING PRECONDITIONS", "content": "We give our agent a brief description of the task of generating preconditions using the PDDL format. PDDL was chosen as this is a commonly used domain language for generating predicate based preconditions and effects, thus it was natural to ask the LLM agent in this format as the agent has likely seen PDDL format extensively in its training data."}, {"title": "XI. LLM REFLECTION FOR MCTS PLAN EVALUATION", "content": "LLM-based reflection replaces simulation and evaluation in the typical MCTS algorithm. The LLM-based reflection provides a third party evaluation of the plan. The LLm is prompted to score the quality of the plan and provide justification given the goal and the steps of the plan."}, {"title": "XII. LLM-DRIVEN ACTION CANDIDATE EXPANSION", "content": "LLM-based action expansion is used to propose candidate actions and parameterization given the current state of the environment and the stated goal."}, {"title": "XIII. IMPLEMENTATION DETAILS FOR ROBOTIC SKILLS", "content": "The perception system maintains an abstraction of the physical environment through a dynamic 3d scene graph that updates in real-time. Inspired by [5], our perception pipeline leverages several off-the-shelf computer vision models to accurately segment, embed, and project objects into a 3 di- mensional pointcloud where open vocabulary object retrieval can be performed. A significant shortcoming of prior work required pre-scanning of the environment and offline analysis before queries could be executed. This severely limits the ability of an embodied agent, as exploration and discovery of the environment are critical to the completion of open ended, general tasks. To circumvent this limitation, our approach introduces a fast combination of Segment Anything (SAM) (everything mode) and CLIP for instance segmentation and image embedding in real-time. The bounding boxes of can-"}]}