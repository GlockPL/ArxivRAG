{"title": "Efficient Backdoor Defense in Multimodal Contrastive Learning: A Token-Level Unlearning Method for Mitigating Threats", "authors": ["Kuanrong Liu", "Siyuan Liang", "Jiawei Liang", "Pengwen Dai", "Xiaochun Cao"], "abstract": "Multimodal contrastive learning uses various data modalities to create high-quality features, but its reliance on extensive data sources on the Internet makes it vulnerable to backdoor attacks. These attacks insert malicious behaviors during training, which are activated by specific triggers during inference, posing significant security risks. Despite existing countermeasures through fine-tuning that reduce the malicious impacts of such attacks, these defenses frequently necessitate extensive training time and degrade clean accuracy. In this study, we propose an efficient defense mechanism against backdoor threats using a concept known as machine unlearning. This entails strategically creating a small set of poisoned samples to aid the model's rapid unlearning of backdoor vulnerabilities, known as Unlearn Backdoor Threats (UBT). We specifically use overfit training to improve backdoor shortcuts and accurately detect suspicious samples in the potential poisoning data set. Then, we select fewer unlearned samples from suspicious samples for rapid forgetting in order to eliminate the backdoor effect and thus improve backdoor defense efficiency. In the backdoor unlearning process, we present a novel token-based portion unlearning training regime. This technique focuses on the model's compromised elements, dissociating backdoor correlations while maintaining the model's overall integrity. Extensive experimental results show that our method effectively defends against various backdoor attack methods in the CLIP model. Compared to SOTA backdoor defense methods, UBT achieves the lowest attack success rate while maintaining a high clean accuracy of the model (attack success rate decreases by 19% compared to SOTA, while clean accuracy increases by 2.57%).", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal Contrastive Learning (MCL) [1] improves model functionality through integrating multiple data modalities and promoting a more generalized representation of features. By assimilating rich information streams such as text and images, MCL enables the model to discern the intricate relationships between different modalities, thereby improving the proficiency of cross-modal retrieval. Additionally, enhanced representation also contributes to stronger explainability [2] and increased trustworthiness in the context of adversarial robustness [3]\u2013[33] and privacy attack defenses [34]\u2013[38], ensuring secure and interpretable model performance. The CLIP model [39] is a notable example of this approach. The CLIP model employs contrastive learning to reduce contrastive loss, thereby increasing similarity across each image-text pair while decreasing resemblance between disparate pairs. CLIP effectively determines the similarities and connections among diverse samples using the semantic insights gained from contrastive learning, which is critical to its success in linear probing tasks. Its ability to perform cross-modal operations also makes zero-shot classification tasks easier, as the model can accurately categorize unseen samples without the need for explicit sample data for specific categories, demonstrating significant utility in real-world scenarios. Overall, the CLIP model demonstrates exceptional versatility and performance in a wide range of downstream applications [40].\nDue to the fact that MCL typically trains on a large number of image-text pairs (400 million), ensuring the security of training data presents a challenge. Research has highlighted that effective backdoor attacks can be executed by modifying a small amount of training data, specifically 1500 image-text pairs [41], allowing attackers to alter the prediction of the model. At present, there have been many backdoor attacks against MCL models [42]\u2013[44]. Attackers ensure that backdoor behaviors are efficiently implanted into the model and are difficult to weed out by constructing various activation patterns.\nTo counteract the adverse effects of backdoor attacks [45]-"}, {"title": "II. RELATED WORK", "content": "researchers have developed defense strategies aimed at mitigating such threats. These defenses are broadly classified into two categories: backdoor detection and backdoor prevention. Backdoor detection approaches [51] involve comparing the performance of multimodal encoders in compromised and uncompromised models to identify any tampering, effectively removing models affected by backdoors. On the other hand, backdoor prevention strategies seek to eliminate backdoor impacts through additional fine-tuning [52], [53]. Typically, these methods involve fine-tuning the affected models with constructed subsets of clean training data to disrupt the malfeasance engineered by malicious image-text pairs. However, such defense mechanisms frequently require considerable time to train on clean datasets and to fine-tune the models accordingly. Moreover, potential disparities in the distribution between these clean image-text pairs constructed and the original training data could compromise the accuracy of the model on legitimate inputs.\nAS shown in Figure 1,in this study, we explore how to use a small number of poisoned samples from the perspective of machine unlearning to help mitigate the malicious impact of the backdoor attack. We envision that, under the supervision of third parties, defenders can alleviate the threats posed by potential attackers. Specifically, attackers poison originally clean pre-trained models by creating and using datasets containing malicious data, thus executing malicious attacks. Unlike attacks, models released by attackers undergo adaptation by defenders. In this context, defenders identify and utilize malicious samples in the potential poisoned dataset, employing specific machine unlearning strategies aimed at inducing the model to forget the backdoor features while minimizing damage to the model's performance on clean samples.\nTo save time, we force the poisoned model to forget crucial poisoned samples to eliminate the impact of backdoor attacks. Specifically, 1) we use a pre-trained model to distinguish suspicious samples in the dataset; at the same time, 2) we train an overfitted poisoned model using these suspicious samples, 3) and then use the overfitted model to find a subset of backdoor samples from the suspicious samples. This subset of backdoor samples accounts for only a small part of the entire dataset, but our experiments show that this subset is effective enough to eliminate the backdoor in the model. To improve defense [54]\u2013[56] effectiveness and reduce the impact on clean sample performance, we introduce a strategy that merges data augmentation with localized unlearning to efficiently purge malicious associations of malicious samples within a few-shot unlearning framework. Inspired by the principles of contrastive learning, we discover that selectively erasing contaminated information in localized areas can effectively obstruct backdoor pathways. Moreover, in light of the prevalent text image attack schemes, we propose a token-level local unlearning technique. This approach is designed to significantly decouple clean and contaminated features, thereby minimizing clean feature disruption during the unlearning phase and increasing the precision of backdoor feature elimination. Our main contributions are:\n\u2022 We introduce a backdoor defense framework for MCL models grounded in machine unlearning, showcasing the potential of machine unlearning in mitigating backdoor attacks on MCL models.\n\u2022 We present an innovative approach that leverages data augmentation and localized unlearning to precisely eliminate backdoor influences using a limited set of samples, ensuring minimal detriment to the model's overall performance.\n\u2022 Through our experiments, we affirm the efficacy of the strategy in using few-shot poisoned samples to refine the poisoned model. Our defense method effectively maintains a low attack success rate (ASR, decrease by 19% compared to the SOTA method) while achieving high clean accuracy (CA, increase by 2.57% compared to the SOTA method)."}, {"title": "A. Multimodal Contrastive Learning", "content": "Multimodal contrastive learning aims to learn feature representations by leveraging multiple types of data. The core idea is to associate data from different modalities to learn their relationships, thus improving the understanding of complex multimodal data. Initially, the MCL model makes breakthroughs in the image-text domain, and related work demonstrates an improvement in the performance of the MCL model with large-scale corpora [39], [57]. These achievements are successfully applied in domains such as semantic segmentation [58], [59] and object detection [60]\u2013[62].\nAs the generalization and versatility of contrastive learning methods are increasingly recognized, researchers find that the MCL approach can be applied effectively to different types of data modalities. Therefore, the MCL model gradually expands to the processing and learning of other modal data, enriching its application scope and demonstrating good applicability and performance in various data modalities such as video data [63]\u2013[65] and audio [66], [67] data. For example, Girdhar et al. [68] propose a six-modal model that includes images, text, audio, infrared, depth, and IMU data, using image alignment to train a joint embedding space. Zhu et al. [69] propose a five-modal model that includes images, text, audio, infrared, and depth data, aligning each modality directly with the language modality with the highest information density. This research provides important theoretical and practical foundations for the development of multimodal contrastive learning."}, {"title": "B. Backdoor Attacks and Defense against MCL", "content": "A backdoor attack [70], [71] involves injecting samples with specific triggers into the training set, creating a hidden backdoor in the model. In benign samples, the poisoned model behaves similarly to a regular model. However, when the attacker inputs data with specific trigger features into the poisoned model, the model consistently outputs the preset output predetermined by the attacker. In MCL frameworks, attackers orchestrate backdoor attacks by embedding imperceptible triggers in image-text pairs, altering text labels to poison targets, as seen in methods such as BadNet [72] with unnoticeable triggers, Blended [73] which blends the trigger pattern with the original image, and advanced techniques such as SIG [74] and SSBA [75]. Carlini et al. [41] demonstrate that past backdoor attacks can be easily transferred to MCL models with better attack effectiveness, requiring only a 0.01% poisoning rate to achieve a backdoor attack [41]. In addition, there is research on backdoor attacks targeting MCL models. For example, Badencoder [76] fine-tunes encoders to achieve attacks on self-supervised models, and TrojVQA [42] simultaneously applies triggers to both image and text modalities. These attacks trick the model into classifying trigger-containing images as the intended target of the attacker.\nTo combat this, researchers develop detection and mitigation strategies. Feng et al. [51] propose an encoder-based approach to identify and reverse trigger effects in poisoned models. Meanwhile, CleanCLIP [52] offers a backdoor fine-tuning strategy that uses extra clean data sets to disrupt backdoor pathways. RoCLIP [53] maintains a text feature pool and reconstructs image-text pairs during pre-training to disrupt the association between backdoor image-text pairs. However, while these methods can reduce ASR, they may also lead to a decrease in the clean accuracy of the model. We propose the UBT method for efficient backdoor defense, effectively reducing the backdoor ASR while sacrificing only minimal CA."}, {"title": "C. Machine Unlearning", "content": "Machine unlearning refers to the process of removing specific samples from the memory of a model without the need for full retraining [77]. Based on the degree of access to the unlearned data, machine learning can be categorized into zero-glance unlearning [78], [79] (full access to all forgotten data), few-shot unlearning [80] (limited access to some forgotten data) and zero-shot unlearning [81], [82] (no access to forgotten data). In our study, our objective is to eliminate the impact of backdoor attacks by unlearning subsets of backdoor samples, which falls under the category of few-shot unlearning. In the context of few-shot unlearning, Yoon et al. [80] propose a few-shot unlearning framework based on model inversion, while Peste et al. [83] introduce a method of unlearning based on influence functions. Recently, low-cost unlearning in larger parameter models becomes increasingly important [84]\u2013[87]. Yao et al. [85] demonstrate efficient unlearning in large language models by using gradient ascent only on negative samples. However, the effectiveness of these algorithms on multimodal foundation models like MCL is still under exploration [88]\u2013[90]. In the context of backdoor attacks, Li et al. [91] explore unlearning techniques by adjusting model parameters using gradient ascent to counteract backdoors, highlighting its significance in improving model security. Bansal et al. [52] face limitations in looking for new statistical features to effectively detect data. Our approach successfully achieves the separation of partial backdoor samples from other samples in MCL models for the first time and investigates the unlearning capability of MCL models for backdoor samples in few-shot unlearning scenarios."}, {"title": "III. PRELIMINARIES", "content": "MCL utilizes images along with their corresponding text descriptions and trains the model using contrastive learning. The large amount of data used for training enables the model to exhibit outstanding performance in various downstream tasks such as few-shot classification and zero-shot classification. Our work focuses primarily on the CLIP model, which comprises a text encoder $f_T$ and an image encoder $f_I$, mapping images and text in the same-dimensional feature space. For any data set $D = \\{(I_i,T_i)\\}_{i=1}^N$ in the sample space $I \\times T$, where $I$ represents the image space and $T$ represents the text space, it is divided into two parts in the poisoning scenario, denoted $D = D_{clean} \\cup D_{bd}$. During the training phase, the model is trained using the potential poisoned dataset. Contrastive learning treats matching sample pairs $(I_i, T_i), (I_j,T_j)$ in $D$ as positive samples, while $(I_i, T_j), (I_j, T_i)$ are considered negative samples. This is achieved by decreasing the distance between positive sample pairs and increasing the distance between negative sample pairs through the InfoNCE loss, which can be expressed as follows:\n$L_{CLIP}(D, \\theta) = \\frac{1}{N}\\sum_{i=1}^{N}{\\{log\\frac{e^{S_{\\theta}(I_i,T_i)/\\tau}}{\\sum_{l=1}^{N} e^{S_{\\theta}(I_i,T_l)/\\tau}} + \\sum_{j=1}^{N}log\\frac{e^{S_{\\theta}(I_j,T_j)/\\tau}}{\\sum_{l=1}^{N} e^{S_{\\theta}(I_l,T_j)/\\tau}}\\}.$ (1)\nHere, $S_{\\theta}(I_k,T_k) =<f_{\\theta}(I_k), f_{\\theta}(T_k)>$, $\\theta$ represents the model parameters, $f_{\\theta}(I_k)$ and $f_{\\theta}(T_k)$ represent the representations of the image $I_k$ and text $T_k$ in the feature space, $< \\cdot >$ represents the operation of the inner product between vectors, and $\\tau$ is the temperature parameter. This training method enables the model to learn excellent image-text contrastive capabilities and successfully apply them to downstream tasks such as zero-shot classification.During the training phase, the model is trained using a potential poisoned dataset and can be represented as:\n$\\theta_{bd} = min_{\\theta} {\\mathcal{L}_{CLIP}(D_{clean}, \\theta) + \\mathcal{L}_{CLIP}(D_{bd}, \\theta) \\}. $ (2)"}, {"title": "B. Backdoor Attacks in Zero-Shot Classification", "content": "In our research, we focus on exploring backdoor attacks on the zero-shot classification downstream task. Zero-shot classification [92] is a type of transfer learning which aims to classify unseen data using a model trained on visible samples. The CLIP model utilizes a large-scale pre-training dataset, enabling the model to learn rich semantic representations. This extensive pre-training approach gives the CLIP model stronger generalization capabilities in zero-shot classification tasks. We use ImageNet1K [93] as the downstream validation set and select one category as the target label for the backdoor attack. During poisoning, we add triggers to images in $D_{bd}$ and randomly select ImageNet1K [93] templates based on the target label to construct captions to replace their original text. As training progresses, the model will learn the backdoor shortcut between the trigger and the target label, which will be reflected in the downstream task. When the attacker activates the backdoor in the downstream task, the model will consistently produce incorrect output."}, {"title": "C. Problem Formulation", "content": "Defense Scenarios The defender operates a secure training platform to protect users from attacks, especially backdoor threats. Even with security measures in place, attackers could potentially exploit the platform by embedding backdoors in the training data and then using it to train poisoned models.\nDefense Capabilities The defender has the right to inspect and audit training data and models submitted for security checks. However, the defender cannot determine whether the model is subject to a backdoor attack. Even with access to all training data, the abundance of samples makes it difficult for the defender to manually identify data with concealed backdoors.\nDefense Objectives The goal of the defender is to protect against backdoor attacks in models. SoTA defenses such as CleanCLIP [52] fine-tunes poisoned models with extensive image-text pairs, which can be inefficient and impact accuracy. Our proposed strategy employs a targeted unlearning method, leveraging suspect datasets to selectively erase backdoor data, preserving model performance on clean data.\nTrade-off Strategy The defender can only test the accuracy of clean samples in downstream tasks during model training and cannot obtain information on the attack success rate. In order to eliminate the impact of backdoors in the model, the defender assumes that the attack success rate is positively correlated with the accuracy of clean samples. Consequently, defenders sacrifice a certain level of clean accuracy in exchange for the algorithm's ability to eliminate backdoors. However, to maintain model performance, defenders must avoid making drastic adjustments to CA, forcing them to strike a balance when employing fine-tuning defense methods."}, {"title": "IV. METHOD", "content": "Fig. 2 shows the framework for unlearning backdoor threats (UBT). We improve the backdoor shortcuts through poisoned samples and implement token-level local unlearning to purify the backdoor model on the few-shot suspicious samples. The entire process of the UBT algorithm can be seen in the algorithm 1."}, {"title": "A. Poisoned Sample Overfitting", "content": "Faced with the challenge of \"weak\" backdoor shortcuts created by attackers, our defense strategy aims to further strengthen these shortcuts to better discover suspicious samples. To this end, we combine dataset analysis with a differentiated training approach, focusing on the segmentation of the poisoned dataset and strengthening the model's response to backdoor triggers through a specific training process.\nWe first use a clean pre-trained model, which is typically a publicly available model with established knowledge, such as the pre-trained CLIP released by OpenAI [39]. This model is used to divide the dataset into a suspicious sample set $D_{susp}$ and a normal sample set $D_{normal}$ based on multimodal text similarity. In this case, we set the size of $D_{susp}$ at a relatively large level (e.g., 1% of the entire dataset). This operation is similar to what is described in [52].For the MCL model's backdoor attack, the poisoning rate is always less than the size of $D_{susp}$. Up to this point, there are still many clean samples mixed in the suspicious sample set, so it cannot be used directly for unlearning. This partitioning strategy allows us to target samples with different characteristics and further strengthen the backdoor shortcut.\nIn the overfitting phase, we fine-tune the poisoned model to obtain the overfitted model. Specifically, we increase the suspicious set's cosine similarity; the model becomes more sensitive to backdoors, ensuring accurate trigger detection. $D_{noraml}$ serves as a regularization for balance training, using InfoCE loss to prevent overfitting to clean samples in $D_{susp}$, thus prioritizing the fitting of backdoor features. The process can be formulated as follows:\n$D_{overfitting} = min_{\\theta} {\\frac{1}{|D_{susp}|} \\sum_{i=1}^{|D_{susp}|}{[S_{\\theta}(I_i, T_i) - 1]^2} + \\mathcal{L}_{CLIP}(D_{normal}, \\theta) \\},$ (3)\nsubject to $(I_i, T_i) \\in D_{susp}$.\nWith this staged and targeted training approach, we amplify the poisoning properties of the model, which helps pinpoint those samples that have the greatest impact on the model's security, comprising the unlearned subset used for backdoor defense."}, {"title": "B. Suspicious Sample Detection", "content": "We reanalyze the suspicious sample set using the overfitting poisoned model after enhancing the shortcuts and further perform a finer-grained backdoor analysis on the sample set. The goal of this process is to discover and localize the subsets of samples that have the greatest impact on backdoor oblivion, so that these backdoor features can be weakened or eliminated more effectively in subsequent processing, thereby improving the overall security of the model.\nSpecifically, we first compute, for each sample in the suspect sample set, its embedding features, which are generated by the poisoning model reinforcing the backdoor features, reflecting the multidimensional spatial location of the sample represented inside the poisoning model. Subsequently, we reordered the similarity scores of these embedded features and focused highly on the backdoor samples with the highest similarity scores. This can be represented as follows:\n$D_{topk} = \\{(I_i, T_i) | rank(S_{\\theta_{overfitting}}(I_i, T_i)) \\le k, (I_i, T_i) \\in D_{susp}\\},$ (4)\nwhere $rank()$ denotes the similarity ranking of the image-text pair $(I_i, T_i)$ in the set, the higher the similarity, the smaller the rank value is.\nTop-k ranked samples are more likely to carry backdoor triggers because they exhibit the highest activation scores compared to the other samples. This phenomenon suggests that when the model encounters these specific samples, the probability of the backdoor logic being activated is significantly higher, thus triggering a specific, predetermined response at the output layer of the model. By identifying these high similarity few-shot suspicious samples, we can not only focus on this small group of samples to effectively mitigate or eliminate the potential threat posed by backdoor attacks, but also reduce the overall cost of training."}, {"title": "C. Token-level Local Unlearn", "content": "To improve the security of the poisoning base model, we propose a fine-tuning process based on model unlearning to adjust the poisoned model and reduce the impact of backdoor attacks on the accuracy of the model. In this approach, we focus on two core issues: the necessity of unlearning and the specific scope of unlearning.\nFirst, regarding the need to forget the entire sample, we argue that it is not necessary. Backdoor attacks are often realized by modifying a small range of content. If unlearning is performed on a large range, it may conflict with the original knowledge of the model, thus affecting the accuracy of the model in handling clean data. Therefore, we advocate selective unlearning to maintain the overall performance of the model.\nSecond, determining the exact scope of the unlearning is a challenge. Intuitively, unlearning specific regions in the image (e.g., patches where triggers are located) seems to be a straightforward solution. However, given the diversity of attacks, especially attacks such as blended attacks, in which triggers are highly integrated with the normal recognized regions of the image, unlearning is exceptionally difficult. To address this challenge, we turn to unlearn discrete text tokens, a choice based on the observation that backdoors typically do not significantly overfit the semantic content of the text. We utilized [94] attribution methods to calculate the contribution values of each token in the text towards CLIP model predictions for image-text pairs.Subsequently, we retained tokens with higher contribution values, which are deemed likely to contain information relevant to the backdoor. In the following text, we represent this as $M_{\\theta}(\\cdot)$.\nFurthermore, due to the high degree of similarity between the images and captions in the backdoor sample set. To enhance the effectiveness of the unlearning process, we adopt an innovative approach of performing Cartesian product combination on a subset of the few-shot unlearning as a way of data augmentation. This step generates a variety of combinations of backdoor samples with varying degrees of correlation, which significantly increases the data diversity and richness of the unlearning training. We refer to the above unlearning process as token-level local unlearning training, which can be formulated as follows:\n$D_{mask} = \\{(I_i, M_{\\theta}(T_i)) | (I_i, T_i) \\in (D_{topk} \\times D_{topk})\\},$ (5)\n$D_{unlearn} = (D_{topk} \\times D_{topk}) \\cup D_{mask},$ (6)\n$\\theta_{unlearn} = min_{\\theta} {\\frac{1}{|D_{unlearn}|}\\sum_{i=1}^{|D_{unlearn}|}{S_{\\theta}(I_i, T_i)}\\},$ (7)\nwhere $D_{unlearn}$ is derived by extending $D_{topk}$ base on the above two conclusions This process not only helps the model to identify and forget potential backdoor samples more effectively but also ensures that the ability to recognize normal samples is retained as much as possible while cutting down the backdoor influence."}, {"title": "D. Analysis of the Existence of a Relatively Small Unlearning Dataset", "content": "In this section, we analyze the upper bound on the minimum number of samples required for backdoor unlearning in poisoned models. We argue that, due to the small difference between clean and poisoned models, the number of samples needed for training should ideally be small, which provides insight into selecting a smaller unlearning set. We use PAC-Bayes theory [95] to demonstrate that a smaller unlearning dataset can effectively achieve the desired unlearning outcome. We first provide the definition of PAC-Bayes theory: Let the sample space be defined as $Z = X \\times Y$, where $X = Y = \\mathbb{R}^n$. Let $D = \\{x_i, y_i\\}_{i=1}^N$ represent a dataset consisting of N samples randomly drawn from the sample space, following a random probability distribution $P\\in \\mathcal{P}(Z)$. $\\mathcal{P}(Z)$ denotes the family of probability measures over a set Z.Let $Q_0$ be a probability distribution over the hypothesis space H, and after observing data D, the output probability distribution is denoted as $Q_D$. $l : Z \\times H \\rightarrow \\mathbb{R}^+$ is the loss function. The PAC theory can be expressed as Theorem 1.\nTheorem 1. For any $\\delta \\in (0, 1)$, with probability at least 1-$\\delta$, the following inequality holds:\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_0}[\\hat{L}(D,h)] \\le \\sqrt{\\frac{1}{2n-1} (KL(Q_D||Q_0) + log(\\frac{n+2}{\\delta}) ) }.$ (8)\nFor $h \\in H$, $L(h) = \\mathbb{E}_{z\\sim P}[l(z,h)]$ is the generalization risk, or simply risk, and $\\hat{L}(D,h) = \\frac{1}{N}\\sum_{i=1}^N l((x_i, y_i), h)$ is the empirical loss. $KL(\\cdot||\\cdot)$ denotes the KL divergence.\nNext, we will roughly analyze the impact of sample size on the distribution of model parameters before and after training. We will transform equation 8 into:\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_0}[\\hat{L}(D,h)] \\le \\sqrt{\\frac{1}{2n-1} [-KL(Q_D||Q_0) + \\frac{log (n+2)}{2n-1} + C ]}$ (9)\nWhen $\\delta$ is fixed, C is a constant.\nLemma 1. For any N, there exists 0 < $\\epsilon$ < 1 such that when n > N, the following inequality holds:\n$\\frac{log(n+2)}{2n-1} < \\frac{1}{(2n - 1)^{\\epsilon}}$ (10)\nProof. We start by analyzing the term $\\frac{log(n+2)}{2n-1}$. Since log(n+ 2) grows logarithmically and 2n - 1 grows linearly with n, for sufficiently large n, the term\n$\\frac{log(n+2)}{2n-1}$ (11)\nwill decay faster than any power of\n$\\frac{1}{(2n - 1)^{\\epsilon}},$ (12)\nwhere 0 < $\\epsilon$ < 1. Thus, there exists some N > 0 such that for all n > N, the inequality (10) holds.\nLemma 2. Under the condition n > N, the following inequality holds:\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_D}[\\hat{L}(D, h)] \\le \\sqrt{\\frac{1}{2n-1} (KL(Q_D||Q_0) + C) + \\frac{1}{(2n - 1)^{\\epsilon}}} $(13)\n$\\le \\sqrt{\\frac{1}{(2n-1)^{\\epsilon}} (KL(Q_D||Q_0) + C_0)}.$ (14)\nProof. Starting from Equation (13), we apply the result from Lemma 10. Substituting the term $\\frac{log(n+2)}{2n-1} \\le \\frac{1}{(2n-1)^{\\epsilon}}$, we obtain:\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_D}[\\hat{L}(D, h)] \\le \\sqrt{\\frac{1}{2n-1} (KL(Q_D||Q_0) + C) + \\frac{1}{(2n - 1)^{\\epsilon}}}$ (15)\nFurther simplification gives:\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_D}[\\hat{L}(D, h)] \\le \\sqrt{\\frac{1}{(2n-1)^{\\epsilon}} (KL(Q_D||Q_0) + C_0)},$ (16)\nwhich completes the proof.\nLemma 3. For any $r > 0$, a sufficient condition for\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_D}[\\hat{L}(D, h)] \\le r$ (17)\nis:\n$\\sqrt{\\frac{1}{(2n - 1)^{\\epsilon}} (KL(Q_D||Q_0) + C_0)} \\le r.$ (18)\nThis implies:\nn $\\ge \\bigg(\\frac{(KL(Q_D||Q_0) + C_0)}{2r^2}\\bigg) + \\frac{1}{2} = N_0.$ (19)\nProof. Starting from the inequality:\n$\\sqrt{\\frac{1}{(2n - 1)^{\\epsilon}} (KL(Q_D||Q_0) + C_0)} \\le r,$ (20)\nsquaring both sides, we get:\n$\\frac{1}{(2n - 1)^{\\epsilon}} (KL(Q_D||Q_0) + C_0) \\le r^2.$ (21)\nThis leads to the bound on n:\nn $\\ge \\bigg(\\frac{KL(Q_D||Q_0) + C_0}{2r^2}\\bigg) + \\frac{1}{2} = N_0.$ (22)\nThus, n > $N_0$ is the sufficient condition for\n$\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_D}[\\hat{L}(D, h)] \\le r.$ (23)\nWhen r is sufficiently small, $N_0 > N$. Therefore, as long as Equation 19 holds, we have $\\mathbb{E}_{h \\sim Q_D}[L(h)] - \\mathbb{E}_{h \\sim Q_D}[\\hat{L}(D,h)] \\le r$, where $N_0$ is the estimated minimum number of samples required for training. Note that the estimation method used in Equation 14 introduces significant approximation errors, making the PAC upper bound not tight. As a result, $N_0$ does not accurately represent the minimum number of samples, and the actual minimum sample size $N^*$ satisfies $N^* < N_0$. Therefore, $N_0$ is an upper bound on the minimum sample size.\nFrom Equation 19, we can see that $N_0$ is proportional to $KL(Q_D||Q_0)$, which implies that the more similar the parameter distributions before and after training, the fewer samples are required for training. As shown in Figure III, in the \"No defense\" scenario, the poisoned model and the retrained model are derived from training datasets with nearly identical quantities (differing by only about 0.3% of backdoor samples). Consequently, the parameter distributions of the poisoned model($Q_{bd}$) and the retrained model($Q_{re}$) should be very similar (with a smaller KL divergence $KL(Q_{re}||Q_{bd})$). Although not explicitly shown in our paper, we can infer that the retrained model differs significantly from the pre-trained model($Q_{pre}$) (with a larger KL divergence $KL(Q_{re}||Q_{pre})$), as the pre-trained model lacks most of the knowledge in the training dataset, necessitating nearly the entire dataset for training. In fact, $KL(Q_{re}||Q_{bd}) \\approx KL(Q_{re}||Q_{pre})$, which assures us that fine-tuning does not require a large dataset to achieve unlearning. However, due to approximation errors, Equation 19 cannot provide an accurate estimate of the dataset size, and through our experiments, we believe that using 1% of the data is a good choice."}, {"title": "V. EXPERIMENTS", "content": "We conduct backdoor attack experiments using a 500K subset of the CC3M dataset [96", "methods": "BadNet [72", "73": "SIG [74", "75": "and TrojVQA [42", "93": "zero-shot classification task as the downstream task", "banana\u201c as the target label for the backdoor attack.\nFor backdoor defense, UBT first selects 1% of the entire dataset(D) as suspicious data. We train an overfitting poisoned model with a batch size of 64 and a learning rate of le-6 for 5 epochs to make it challenging to generalize to clean data. Then, we further filter the dataset to include $\\sqrt{|D|}$.1% of the data as unlearn data, where $|\\cdot|$ denotes the size of the dataset. Although the MCL model has a higher poisoning rate compared to traditional models, we believe that the poisoning rate will not drop below a certain threshold (greater than $\\sqrt{|D|}$.1%) since attackers aim to maintain a high attack success rate. UBT uses unlearning techniques by adjusting the batch size to 64, the learning rate to le-5, and conducting 5 epochs of training to eliminate backdoor feature memories from the model, thereby enhancing security and robustness.\nWe use three methods for comparison": "ABL [91", "52": "assuming $D_{susp} = D_{unlearn}$, and conduct unlearning defense. We use a batch size of 64 and a learning rate of le-6 for 10 epochs of training. RoCLIP [53"}, {"52": "is currently the state-of-the-art defense algorithm. We follow its specific experimental"}]}