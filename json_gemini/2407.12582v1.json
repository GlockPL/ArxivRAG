{"title": "Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection", "authors": ["Hu Cao", "Zehua Zhang", "Yan Xia", "Xinyi Li", "Jiahao Xia", "Guang Chen", "Alois Knoll"], "abstract": "In frame-based vision, object detection faces substantial performance degradation under challenging conditions due to the limited sensing capability of conventional cameras. Event cameras output sparse and asynchronous events, providing a potential solution to solve these problems. However, effectively fusing two heterogeneous modalities remains an open issue. In this work, we propose a novel hierarchical feature refinement network for event-frame fusion. The core concept is the design of the coarse-to-fine fusion module, denoted as the cross-modality adaptive feature refinement (CAFR) module. In the initial phase, the bidirectional cross-modality interaction (BCI) part facilitates information bridging from two distinct sources. Subsequently, the features are further refined by aligning the channel-level mean and variance in the two-fold adaptive feature refinement (TAFR) part. We conducted extensive experiments on two benchmarks: the low-resolution PKU-DDD17-Car dataset and the high-resolution DSEC dataset. Experimental results show that our method surpasses the state-of-the-art by an impressive margin of 8.0% on the DSEC dataset. Besides, our method exhibits significantly better robustness (69.5% versus 38.7%) when introducing 15 different corruption types to the frame images.", "sections": [{"title": "1 Introduction", "content": "Object detection is a fundamental task in computer vision [9, 32, 39]. The performance of conventional frame-based cameras in object detection often faces a significant decline in challenging conditions, such as high-speed motion, poor lighting conditions (e.g. low-light and overexposure), and image corruptions [21, 36,45]. Recently, an emerging bio-inspired vision sensor, the event camera (e.g., dynamic and active pixel vision sensors, DAVIS), has been developed for vision perception [3,37]. The working principle of the event camera fundamentally differs from that of the conventional camera. It transmits only the local pixel-level changes caused by variations in lighting intensity, like a bio-inspired retina [7,14]. The event camera has several appealing characteristics, including low latency, high dynamic range (120 dB), and high temporal resolution. It offers a new perspective to address some limitations of conventional cameras in challenging scenarios. However, similar to the problems faced by conventional cameras in extreme-light scenarios, event cameras exhibit poor performance in static or remote scenes with small targets, as illustrated in Fig. 1. The event camera captures dynamic context and structural information, whereas the frame-based camera provides rich color and texture information. Both event cameras and frame-based cameras are complementary, motivating the development of new algorithms for various computer vision tasks.\nCurrent event-frame fusion methods often utilize concatenation [4, 48], attention mechanisms [5, 46, 58], and post-processing [8, 24, 27] strategies to fuse events and frames. Simply concatenating event-based and frame-based features improves performance slightly, but the inherent complementary nature between different modalities is not fully exploited. The authors of [5] employ pixel-level spatial attention to leverage event-based features in enhancing frame-based features, leading to improved performance. However, enhancing single-modal features is considered suboptimal, as event-based features and frame-based features possess unique characteristics. The methods presented in [46,58] incorporate feature interaction between event-based and frame-based features; however, they do not comprehensively consider the feature imbalance problems existing in event-frame object detection. As illustrated in Fig. 2, we present the feature maps before and after the application of our fusion module. In the daytime scene,"}, {"title": "3 Method", "content": "In this section, we first introduce preliminaries, including the working principles of the event camera and event representation. Subsequently, we delve into a detailed illustration of our proposed hierarchical feature refinement network."}, {"title": "3.1 Preliminaries", "content": "Event camera. The conventional frame-based camera operates by capturing and delivering a sequence of frames at a fixed frequency. In stark contrast, the working principle of the event camera deviates fundamentally from that of its frame-based counterpart. The event camera, a bio-inspired vision sensor, generates asynchronous and sparse event streams exclusively when the change in logarithmic intensity L(x, y, t) surpasses a predetermined threshold C. The computational process can be succinctly formulated as follows:\n\\(L(x, y, t) - L(x,y,t \u2013 \u2206t) \u2265 pC, p\u2208 {\u22121,+1}\\).\nwhere \u2206t represents the time variation. The event polarity, denoted by p \u2208 {-1, +1}, signifies the sign of the brightness change, indicating either positive (\"ON\") or negative (\u201cOFF\u201d) events, respectively. The generated event streams E can be expressed as follows:\n\\(E = {e_i}_{i=1}^{N}, e_i = (X_i, Y_i, t_i, P_i)\\).\nwhere N denotes the number of events ei within the event stream E. The tuple (x,y) denotes the triggered pixel coordinates, while t and p represent the corresponding timestamp and polarity, respectively.\nEvent representation. The event representation employed in this study is the voxel grid [59] generated through the discretization of the time domain. Considering a set of N input events {(xi, Yi, ti, Pi)}i\u2208[1,N] and a set of B bins for discretizing the time dimension, we normalize the timestamps to the range [0, B-1]. Subsequently, the event volume is generated as follows:\n\\(t_i = \\frac{(B-1)(t_i - t_1)}{t_n-t_1},\\)\n\\(V(x,y,t) = \\sum_i p_ik_b(x - x_i) k_b(y - Y_i) k_b (t - t_i),\\)\n\\(k(a) = max(0, 1 - |a|).\\)\nwhere k(a) corresponds to the bilinear sampling kernel, as defined in [42]. More specifically, we regard the time domain of the voxel grid as channels in a conventional 2D image and conduct a 2D convolution across the spatial dimensions x and y. This approach enables the model to effectively capture feature representations from the spatiotemporal distribution of events."}, {"title": "3.2 Hierarchical Feature Refinement Network", "content": "Overview. The method's architecture, illustrated in Fig. 3, comprises a dual-stream backbone, CAFR, a feature pyramid network (FPN), and a detection head. Events are preprocessed using the voxel grid encoding method for CNN to extract deep features. Utilizing event-frame data as inputs, the dual-stream backbone network extracts multi-scale features. For effective information exchange between different modal features, CAFR receive event-based and frame-based features to balance the information flow. Subsequently, the detection head operates on the harmonized multi-modal features for precise detection predictions. Detailed explanations of each module will be provided in the subsequent parts.\nDual-stream backbone. The backbone incorporates two branches: event-based and frame-based ResNet-50s [18]. Each ResNet-50 is composed of four blocks denoted as {L1, L2, L3, L4}. The feature maps' resolution progressively decreases from L\u2081 to L4, with the resolution of the features being consistently maintained at each block. Residual learning is employed to extract semantically stronger and more valuable features. In this work, a CAFR module is strategically inserted between the two blocks to enhance the learning process.\nCross-modality adaptive feature refinement (CAFR) module. Frame-based cameras perform poorly in several challenging scenarios, such as overexposure, high-speed motion, etc. Furthermore, frame-based features may generate unclear results because objects share many visual similarities. In contrast, event cameras with high temporal resolution and dynamic range excel at capturing motion and edge information. Recognizing the complementarity of event-based and frame-based information, we introduce the CAFR to enhance feature representations at the feature level by leveraging event-based dynamic context. As depicted in Fig. 4, CAFR processes both frame-based features Ff and event-based features Fe to obtain balanced semantic features."}, {"title": "4 Experiments", "content": "In this section, we outline the datasets and evaluation metrics used for event-frame multimodal object detection. Extensive experiments are then conducted to evaluate the effectiveness of our proposed method."}, {"title": "4.1 Datasets", "content": "DDD17. The DDD17 dataset [1] is collected to combine events and frames for end-to-end driving applications. It uses a 346 \u00d7 260 pixel DAVIS to record over 12 hours of highway and city driving in various conditions (daytime, evening, night, dry, etc.). However, the original DDD17 dataset does not have object detection labels. The authors of [27] manually labeled the vehicles in the DDD17 dataset to make them available for object detection tasks, named PKU-DDD17-Car. The PKU-DDD17-Car dataset contains frame data and corresponding event streams, of which 2241 frames are the training set and 913 frames are the test set.\nDSEC. DSEC [16] is a high-resolution and large-scale event-frame dataset for real-world driving scenarios. Concurrently, event data is captured using an event camera with a resolution of 640 \u00d7 480. Unlike the DDD17 dataset, where both event data and frame data originate from the same DAVIS camera, the DSEC dataset features non-synchronized event and RGB data. To address this, a homographic transformation based on camera matrices is employed to align the viewpoint and resolution of the event and RGB cameras. The initial release of the DSEC dataset lacks annotations for object detection. In this study, we utilize the labels introduced in [48] for evaluation. Specifically, the labeled dataset encompasses three object categories: cars, pedestrians, and large vehicles.\nCorruption data. Previous studies [12,36,48] indicate significant performance degradation (as low as 30-60% of the original performance) in standard object detection models when applied to corrupted images. To assess model's robustness, we introduced 15 distinct corruption types to frame images, including gaussian noise, shot noise, impulse noise, defocus blur, frosted glass blur,"}, {"title": "4.2 Evaluation Metrics", "content": "We use the widely-adopted mAP metric [31] to evaluate the detection performance of different methods. When assessing performance over corrupted data, we utilize the mean performance under corruption (mPC) metric [19,36]. This metric represents the average mAP over various corruption types and severity levels, as expressed below:\n\\(MPC = \\frac{1}{N_c} \\sum_{c=1}^{N_c} \\frac{1}{N_s} \\sum_{s=1}^{N_s} MAP_{c,s}\\),\nwhere mAPc,s represents the performance measure evaluated on corruption type c under severity level s, Nc is the number of corruption types (15 in our work), and Ns is the number of severity levels considered (5 in our work)."}, {"title": "4.3 Ablation Study", "content": "In this section, we conduct a series of ablation experiments to assess the effectiveness of the proposed modules.\nMulti-modal vs. single-modal. In Tab. 1, we compare the performance of different modality inputs. Firstly, RetinaNet [30] is trained using events and frames as input, respectively. The results demonstrate that, despite event cameras effectively capturing dynamic semantics and filtering out redundant background information, event-based detectors exhibit inferior performance compared to their frame-based counterparts. This performance gap is attributed to the absence of crucial color and texture information for object detection. Given the complementary nature of events and frames, enhancing performance through their fusion is a viable approach. Hence, exploring effective fusion methods for events and frames is crucial. In this work, we propose CAFR to effectively leverage the complementary information from events and frames. Integrating our CAFR"}, {"title": "4.4 Comparison with SOTA Methods", "content": "We conduct a comprehensive evaluation by comparing our method with SOTA methods on both the DSEC dataset and the PKU-DDD17-Car dataset. Additionally, we thoroughly analyze the robustness of our method, particularly its effectiveness in handling corrupted data. The results showcase the strengths of our approach, demonstrating excellent performance and notable robustness in challenging scenarios. We delve into the specific details in the following:\nComparison with SOTA methods on the DSEC dataset. To the best of our knowledge, there is limited research on RGB-event fusion methods specifically applied to the DSEC dataset. To address this gap, we replace our proposed fusion modules with SOTA alternatives from both the RGB-Event domain, including FPN-Fusion [48], DRFuser [38], RAMNet [15], CMX [55], FAGC [5], RENet [58], and EFNet [46], and the RGB-D domain, comprising SAGate [33], DCF [23], and SPNet [57]. Additionally, we compare our method with well-known attention modules such as SENet [20], ECANet [49], and CBAM [50].\nWe maintain consistency in event representation (voxel grid), network structure (RetinaNet), loss, and hyperparameter settings across all comparative experiments, with the only variation being the fusion module. The experimental results"}, {"title": "5 Conclusion", "content": "This paper proposes a novel hierarchical feature refinement network for event-frame fusion. The key idea is the coarse-to-fine fusion module, named the cross-modality adaptive feature refinement module (CAFR). The CAFR employs attention mechanisms and feature statistics to enhance feature representations at the feature level. Extensive experiments are conducted on two datasets: the low-resolution PKU-DDD17-Car dataset and the high-resolution DSEC dataset. The results consistently demonstrate performance improvements on both datasets, showcasing the method's effectiveness and strong generalization. Furthermore, the model's robustness is thoroughly evaluated on generated corruption data, revealing superior robustness compared to other SOTA methods. These findings highlight the effectiveness of the proposed CAFR in strengthening the model across diverse scenarios and datasets."}, {"title": "A More Experimental Details", "content": "A.1 Datasets\nDDD17. The vehicles in the DDD17 dataset were manually labeled by the authors of [27] to create the PKU-DDD17-Car dataset for object detection tasks. Additional details about the PKU-DDD17-Car dataset are outlined in Tab. 6.\nDSEC. The original dataset comprises 53 sequences captured in three distinct regions of Switzerland. RGB frames are captured using the FLIR color camera, which boasts a resolution of 1440 \u00d7 1080. The original DSEC dataset [16] lacks the required labels for object detection. The labels introduced in [48] is used in this work. This annotated dataset consists of a total of 41 sequences, allocated for training (33 sequences), validation (3 sequences), and testing (5 sequences). Notably, the dataset covers a wide range of lighting conditions, from ideal to highly challenging. This diversity guarantees comprehensive testing of vision systems, ensuring their robustness and applicability in real-world settings. Additionally, unlike the DDD17 dataset, in which both event data and RGB data are sourced from the same DAVIS camera, the raw event data and RGB data in the DSEC dataset are not aligned; they do not correspond to the same frame. Therefore, additional preprocessing of the raw DSEC data is essential. Further details regarding the preprocessing steps are provided below:\nHomographic transformation. To address the misalignment between the event data and RGB data, the two types of data need to be transformed into the same frame. The dataset provides a baseline of 4.5 cm between the two cameras, allowing for the transformation to a common viewpoint. In our pre-processing, we leverage the homographic transformation induced by pure rotation, as derived in Eq. 13, to align the scene from an RGB frame to an event-camera frame. It's important to note that, in our scenario, the scene appears far away from the camera, and the baseline of 4.5 cm is smaller than the distances of the scene objects.\n\\(P_{event,rgb} = K_{event} * R_{rgb} * R_{event,rgb} * R_{event} * K_{rgb}\\).\nwhere Krgb and Kevent represent the intrinsic camera matrices of the RGB and event cameras, respectively. Similarly, Rrgb and Revent denote the rotation matrices accounting for the transition from distorted to undistorted frames for each camera. Additionally, Revent,rgb stands for the rotation matrix aligning the RGB camera coordinate system with that of the event camera.\nAnnotation generation. To facilitate object detection on the DSEC dataset, we employed simulated annotations provided by [48]. YOLOV5 [25] was used"}, {"title": "A.2 Training details.", "content": "Our model is implemented using PyTorch [40], and we initialize the event-based and frame-based backbone branches with pre-trained ResNet-50 [18]. The input data for the PKU-DDD17-Car dataset and DSEC dataset are resized to 346\u00d7260 and 640 \u00d7 480, respectively. Furthermore, we train the model using the Adam optimizer [26] with an initial learning rate of 1 \u00d7 10-4. The experiments are"}, {"title": "B More Experimental Analysis", "content": "Performance under different lighting conditions. To analyze the contribution of the event camera, we assess the performance gain under different illumination conditions.\nEfficiency analysis.\nMore robustness analysis."}, {"title": "C Limitations", "content": "While our CAFR has demonstrated efficacy in the context of object detection, it is essential to acknowledge a current limitation. The scope of our evaluations has been confined to this specific task, and extrapolating the performance of CAFR to other prevalent perception tasks, such as semantic segmentation, depth prediction, and steering angle prediction, remains unexplored. Future investigations could delve into the broader applicability of CAFR, providing insights into its adaptability and potential limitations across diverse perception domains."}, {"title": "D More Related Works", "content": "In other areas of cross-modal fusion, such as depth, thermal, and event data, we discuss relevant works below. In the field of RGB-D salient object detection (SOD), the joint learning and densely cooperative fusion framework introduced"}]}