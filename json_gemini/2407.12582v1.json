{"title": "Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection", "authors": ["Hu Cao", "Zehua Zhang", "Yan Xia", "Xinyi Li", "Jiahao Xia", "Guang Chen", "Alois Knoll"], "abstract": "In frame-based vision, object detection faces substantial per-formance degradation under challenging conditions due to the limited sensing capability of conventional cameras. Event cameras output sparse and asynchronous events, providing a potential solution to solve these problems. However, effectively fusing two heterogeneous modalities re-mains an open issue. In this work, we propose a novel hierarchical fea-ture refinement network for event-frame fusion. The core concept is the design of the coarse-to-fine fusion module, denoted as the cross-modality adaptive feature refinement (CAFR) module. In the initial phase, the bidirectional cross-modality interaction (BCI) part facilitates informa-tion bridging from two distinct sources. Subsequently, the features are further refined by aligning the channel-level mean and variance in the two-fold adaptive feature refinement (TAFR) part. We conducted extensive experiments on two benchmarks: the low-resolution PKU-DDD17-Car dataset and the high-resolution DSEC dataset. Experimental results show that our method surpasses the state-of-the-art by an impressive margin of 8.0% on the DSEC dataset. Besides, our method exhibits sig-nificantly better robustness (69.5% versus 38.7%) when introducing 15 different corruption types to the frame images. The code can be found at the link (https://github.com/HuCaoFighting/FRN).", "sections": [{"title": "1 Introduction", "content": "Object detection is a fundamental task in computer vision [9, 32, 39]. The per-formance of conventional frame-based cameras in object detection often faces a significant decline in challenging conditions, such as high-speed motion, poor lighting conditions (e.g. low-light and overexposure), and image corruptions [21,"}, {"title": "3 Method", "content": "In this section, we first introduce preliminaries, including the working principles of the event camera and event representation. Subsequently, we delve into a detailed illustration of our proposed hierarchical feature refinement network."}, {"title": "3.1 Preliminaries", "content": "Event camera. The conventional frame-based camera operates by capturing and delivering a sequence of frames at a fixed frequency. In stark contrast, the working principle of the event camera deviates fundamentally from that of its frame-based counterpart. The event camera, a bio-inspired vision sensor, gen-erates asynchronous and sparse event streams exclusively when the change in logarithmic intensity L(x, y, t) surpasses a predetermined threshold C. The com-putational process can be succinctly formulated as follows:\n\nL(x, y, t) - L(x,y,t \u2013 \u2206t) \u2265 pC, p\u2208 {\u22121,+1}.\n\nwhere \u2206t represents the time variation. The event polarity, denoted by p \u2208 {-1, +1}, signifies the sign of the brightness change, indicating either positive (\"ON\") or negative (\u201cOFF\u201d) events, respectively. The generated event streams E can be expressed as follows:\n\nE = {e_i}_{i=1}^N, e_i = (X_i, Y_i, t_i, P_i).\n\nwhere N denotes the number of events $e_i$ within the event stream E. The tu-ple (x,y) denotes the triggered pixel coordinates, while t and p represent the corresponding timestamp and polarity, respectively.\nEvent representation. The event representation employed in this study is the voxel grid [59] generated through the discretization of the time domain. Considering a set of N input events {(xi, Yi, ti, Pi)}i\u2208[1,N] and a set of B bins for discretizing the time dimension, we normalize the timestamps to the range [0, B-1]. Subsequently, the event volume is generated as follows:\n\nt_i = \\frac{(B-1)(t_i - t_1)}{t_n-t_1},\nV(x,y,t) = \u2211p_ik_b(x - x_i) k_b (y - Y_i) k_b (t-1),\nk(a) = max(0, 1 - |a|).\n\nwhere k(a) corresponds to the bilinear sampling kernel, as defined in [42]. More specifically, we regard the time domain of the voxel grid as channels in a con-ventional 2D image and conduct a 2D convolution across the spatial dimensions x and y. This approach enables the model to effectively capture feature repre-sentations from the spatiotemporal distribution of events."}, {"title": "3.2 Hierarchical Feature Refinement Network", "content": "Overview. The method's architecture, illustrated in Fig. 3, comprises a dual-stream backbone, CAFR, a feature pyramid network (FPN), and a detection head. Events are preprocessed using the voxel grid encoding method for CNN to extract deep features. Utilizing event-frame data as inputs, the dual-stream back-bone network extracts multi-scale features. For effective information exchange between different modal features, CAFR receive event-based and frame-based features to balance the information flow. Subsequently, the detection head op-erates on the harmonized multi-modal features for precise detection predictions. Detailed explanations of each module will be provided in the subsequent parts.\nDual-stream backbone. The backbone incorporates two branches: event-based and frame-based ResNet-50s [18]. Each ResNet-50 is composed of four blocks denoted as {L1, L2, L3, L4}. The feature maps' resolution progressively decreases from L\u2081 to L4, with the resolution of the features being consistently maintained at each block. Residual learning is employed to extract semantically stronger and more valuable features. In this work, a CAFR module is strategically inserted between the two blocks to enhance the learning process.\nCross-modality adaptive feature refinement (CAFR) module. Frame-based cameras perform poorly in several challenging scenarios, such as overexpo-sure, high-speed motion, etc. Furthermore, frame-based features may generate unclear results because objects share many visual similarities. In contrast, event cameras with high temporal resolution and dynamic range excel at capturing motion and edge information. Recognizing the complementarity of event-based and frame-based information, we introduce the CAFR to enhance feature rep-resentations at the feature level by leveraging event-based dynamic context. As depicted in Fig. 4, CAFR processes both frame-based features $F_f$ and event-based features $F_e$ to obtain balanced semantic features."}, {"title": "Embracing Events and Frames for Object Detection", "content": "Bidirectional cross-modality interaction (BCI). Initially, a transformation module, utilizing a 1 \u00d7 1 convolution layer, is employed for activation. The cal-culation can be expressed as follows:\n\nF'_f = Conv_{1\u00d71}(F_f), F'_e = Conv_{1\u00d71}(F_e).\n\nSubsequently, the activated frame and event features undergo a coarse-to-fine fusion. Specifically, a global attention map is computed through pixel-wise mul-tiplication, representing mixed attention across spatial and channel dimensions to enhance features. Formally, the computation for enhanced features Fenh and Fenh is as follows:\n\nF^{enh}_f = F'_f \\bigotimes F'_e + F_f, F^{enh}_e = F'_f \\bigotimes F'_e + F_e.\n\nTo bridge information from two distinct sources, the features undergo pro-cessing via a bidirectional cross-self-attention mechanism at a coarse level. The input features Fenh and Fenh are first projected into query (Qf and Qe), key (Kf and Ke), and value (Vf and Ve) tensors. The computation can be expressed as follows:\n\nQ_f = F_f^{enh}W_Q^f, K_f = F_f^{enh}W_K^f,\nV_f = F_f^{enh}W_V^f, Q_e = F_e^{enh}W_Q^e,\nK_e = F_e^{enh}W_K^e, V_e = F_e^{enh}W_V^e.\n\nwhere W represents a learnable linear projection.\nIn contrast to self-attention in [6,51,52], which focuses on relationships within an individual feature modality, cross-self-attention extends its scope by incorpo-rating guidance from the counterpart feature modality. This mechanism captures semantics as:"}, {"title": "Embracing Events and Frames for Object Detection", "content": "F_f^a = CrossAtt_f(Q_e, K_e, V_f), F_e^a = CrossAtt_e(Q_f, K_f, V_e).\n\nWhere CrossAttf is defined as follows:\n\nCrossAtt_f(Q_e, K_e, V_f) = SoftMax(\\frac{Q_eK_e^T}{\\sqrt{D}})V_f.\n\nwhere D denotes the channel dimension of the feature map. CrossAtte is com-puted in a similar manner.\nTwo-fold adaptive feature refinement (TAFR). Furthermore, we employ fea-ture statistics to align the feature space, leveraging insights from the style trans-fer domain [22]. Specifically, the features undergo further refinement by aligning the channel-level mean and variance. The resulting final output, denoted as Fo, is attained by concatenating the two aligned features. This entire process is represented as follows:\n\nF_W = W_fF_f^a, F_e^W = W_eF_e^a,\nF'_f = \\frac{\\sigma(F_e^{enh})(F_W - \u03bc(F_e^{enh}))}{\\sigma(F_W)} + \u03bc(F_e^{enh}),\nF'_e = \\frac{\\sigma(F_f^{enh})(F_e^W - \u03bc(F_f^{enh}))}{\\sigma(F_e^W)} + \u03bc(F_f^{enh}),\nF_o = Concat(F'_f, F'_e).\n\nwhere Wf and We represent learnable linear projections. Notably, the computa-tion for F, Fe, and Fo operates without learnable affine parameters. Instead, it adaptively computes the affine parameters directly from the inputs. The com-putation of \u03bcand \u03c3 is expressed as:\n\n\u03bc(x) = \\frac{1}{H \\times W} \\sum_{h=1}^H \\sum_{w=1}^W x,\n\u03c3(x) = \\sqrt{\\frac{1}{H \\times W} \\sum_{h=1}^H \\sum_{w=1}^W (x - \u03bc(x))^2 + \u03f5}.\n\nWhere x denotes the input feature maps. H and W represent the height and width of the input feature map, respectively. e is a small value introduced to prevent division by zero, typically set to 1 \u00d7 10-5.\nFPN and detection head. Similar to the previous works [5,29,30], we employ FPN to enhance features, improving detection robustness for objects of varying sizes. The outputs {Pn}=1 are generated through top-down pathways and lat-eral connections. The last level of the feature map, P5, is obtained by applying a 3 \u00d7 3 convolutional layer with stride 2 on P4. The resulting multi-level feature maps {P}=1 are then fed into the detection head for prediction.\nAfter FPN processing, the detection head comprises two subnets responsible for classification and bounding box regression. Each sub-network comprises four"}, {"title": "Experiments", "content": "In this section, we outline the datasets and evaluation metrics used for event-frame multimodal object detection. Extensive experiments are then conducted to evaluate the effectiveness of our proposed method."}, {"title": "4.1 Datasets", "content": "DDD17. The DDD17 dataset [1] is collected to combine events and frames for end-to-end driving applications. It uses a 346 \u00d7 260 pixel DAVIS to record over 12 hours of highway and city driving in various conditions (daytime, evening, night, dry, etc.). However, the original DDD17 dataset does not have object detection labels. The authors of [27] manually labeled the vehicles in the DDD17 dataset to make them available for object detection tasks, named PKU-DDD17-Car. The PKU-DDD17-Car dataset contains frame data and corresponding event streams, of which 2241 frames are the training set and 913 frames are the test set.\nDSEC. DSEC [16] is a high-resolution and large-scale event-frame dataset for real-world driving scenarios. Concurrently, event data is captured using an event camera with a resolution of 640 \u00d7 480. Unlike the DDD17 dataset, where both event data and frame data originate from the same DAVIS camera, the DSEC dataset features non-synchronized event and RGB data. To address this, a ho-mographic transformation based on camera matrices is employed to align the viewpoint and resolution of the event and RGB cameras. The initial release of the DSEC dataset lacks annotations for object detection. In this study, we uti-lize the labels introduced in [48] for evaluation. Specifically, the labeled dataset encompasses three object categories: cars, pedestrians, and large vehicles.\nCorruption data. Previous studies [12,36,48] indicate significant performance degradation (as low as 30-60% of the original performance) in standard ob-ject detection models when applied to corrupted images. To assess model's ro-bustness, we introduced 15 distinct corruption types to frame images, includ-ing gaussian noise, shot noise, impulse noise, defocus blur, frosted glass blur,"}, {"title": "4.2 Evaluation Metrics", "content": "We use the widely-adopted mAP metric [31] to evaluate the detection perfor-mance of different methods. When assessing performance over corrupted data, we utilize the mean performance under corruption (mPC) metric [19,36]. This metric represents the average mAP over various corruption types and severity levels, as expressed below:\n\nmPC = \\frac{1}{N_c} \\sum_{c=1}^{N_c} \\frac{1}{N_s} \\sum_{s=1}^{N_s} mAP_{c,s}\n\nwhere mAPc,s represents the performance measure evaluated on corruption type c under severity level s, Ne is the number of corruption types (15 in our work), and Ns is the number of severity levels considered (5 in our work)."}, {"title": "4.3 Ablation Study", "content": "In this section, we conduct a series of ablation experiments to assess the effec-tiveness of the proposed modules. The results are summarized in Tab. 1 and Tab. 2.\nMulti-modal vs. single-modal. In Tab. 1, we compare the performance of dif-ferent modality inputs. Firstly, RetinaNet [30] is trained using events and frames as input, respectively. The results demonstrate that, despite event cameras effec-tively capturing dynamic semantics and filtering out redundant background in-formation, event-based detectors exhibit inferior performance compared to their frame-based counterparts. This performance gap is attributed to the absence of crucial color and texture information for object detection. Given the com-plementary nature of events and frames, enhancing performance through their fusion is a viable approach. Hence, exploring effective fusion methods for events and frames is crucial. In this work, we propose CAFR to effectively leverage the complementary information from events and frames. Integrating our CAFR"}, {"title": "4.4 Comparison with SOTA Methods", "content": "We conduct a comprehensive evaluation by comparing our method with SOTA methods on both the DSEC dataset and the PKU-DDD17-Car dataset. Addi-tionally, we thoroughly analyze the robustness of our method, particularly its effectiveness in handling corrupted data. The results showcase the strengths of our approach, demonstrating excellent performance and notable robustness in challenging scenarios. We delve into the specific details in the following:\nComparison with SOTA methods on the DSEC dataset. To the best of our knowledge, there is limited research on RGB-event fusion methods specifi-cally applied to the DSEC dataset. To address this gap, we replace our proposed fusion modules with SOTA alternatives from both the RGB-Event domain, in-cluding FPN-Fusion [48], DRFuser [38], RAMNet [15], CMX [55], FAGC [5], RENet [58], and EFNet [46], and the RGB-D domain, comprising SAGate [33], DCF [23], and SPNet [57]. Additionally, we compare our method with well-known attention modules such as SENet [20], ECANet [49], and CBAM [50]. We maintain consistency in event representation (voxel grid), network structure (RetinaNet), loss, and hyperparameter settings across all comparative experi-ments, with the only variation being the fusion module. The experimental results"}, {"title": "Conclusion", "content": "This paper proposes a novel hierarchical feature refinement network for event-frame fusion. The key idea is the coarse-to-fine fusion module, named the cross-modality adaptive feature refinement module (CAFR). The CAFR employs at-tention mechanisms and feature statistics to enhance feature representations at the feature level. Extensive experiments are conducted on two datasets: the low-resolution PKU-DDD17-Car dataset and the high-resolution DSEC dataset. The results consistently demonstrate performance improvements on both datasets, showcasing the method's effectiveness and strong generalization. Furthermore, the model's robustness is thoroughly evaluated on generated corruption data, revealing superior robustness compared to other SOTA methods. These findings highlight the effectiveness of the proposed CAFR in strengthening the model across diverse scenarios and datasets."}, {"title": "A More Experimental Details", "content": "A.1 Datasets\nDDD17. The vehicles in the DDD17 dataset were manually labeled by the authors of [27] to create the PKU-DDD17-Car dataset for object detection tasks. Additional details about the PKU-DDD17-Car dataset are outlined in Tab. 6.\nDSEC. The original dataset comprises 53 sequences captured in three distinct regions of Switzerland. RGB frames are captured using the FLIR color cam-era, which boasts a resolution of 1440 \u00d7 1080. The original DSEC dataset [16] lacks the required labels for object detection. The labels introduced in [48] is used in this work. This annotated dataset consists of a total of 41 sequences, allocated for training (33 sequences), validation (3 sequences), and testing (5 sequences). Notably, the dataset covers a wide range of lighting conditions, from ideal to highly challenging. This diversity guarantees comprehensive testing of vision systems, ensuring their robustness and applicability in real-world settings. Additionally, unlike the DDD17 dataset, in which both event data and RGB data are sourced from the same DAVIS camera, the raw event data and RGB data in the DSEC dataset are not aligned; they do not correspond to the same frame. Therefore, additional preprocessing of the raw DSEC data is essential. Further details regarding the preprocessing steps are provided below:\nHomographic transformation. To address the misalignment between the event data and RGB data, the two types of data need to be transformed into the same frame. The dataset provides a baseline of 4.5 cm between the two cameras, allowing for the transformation to a common viewpoint. In our pre-processing, we leverage the homographic transformation induced by pure rotation, as derived in Eq. 13, to align the scene from an RGB frame to an event-camera frame. It's important to note that, in our scenario, the scene appears far away from the camera, and the baseline of 4.5 cm is smaller than the distances of the scene objects.\n\nP_{event,rgb} = K_{event} * R_{rgb} * R_{event,rgb} * R_{event} * K_{rgb}\n\nwhere Krgb and Kevent represent the intrinsic camera matrices of the RGB and event cameras, respectively. Similarly, Rrgb and Revent denote the rotation ma-trices accounting for the transition from distorted to undistorted frames for each camera. Additionally, Revent,rgb stands for the rotation matrix aligning the RGB camera coordinate system with that of the event camera.\nFig. 6 illustrates the impact of the homomorphic transformation. In the left figure, the RGB image and the event image are not aligned. However, after ap-plying the homomorphic transformation, the RGB image and the event image become aligned, sharing the same frame and being suitable for multimodal fu-sion. Additionally, the size of the RGB images is resized to match the event image, ensuring that both types of data have the same field of view and resolu-tion.\nAnnotation generation. To facilitate object detection on the DSEC dataset, we employed simulated annotations provided by [48]. YOLOV5 [25] was used"}, {"title": "A.2 Training details.", "content": "Our model is implemented using PyTorch [40], and we initialize the event-based and frame-based backbone branches with pre-trained ResNet-50 [18]. The input data for the PKU-DDD17-Car dataset and DSEC dataset are resized to 346\u00d7260 and 640 \u00d7 480, respectively. Furthermore, we train the model using the Adam optimizer [26] with an initial learning rate of 1 \u00d7 10-4. The experiments are"}, {"title": "B More Experimental Analysis", "content": "Performance under different lighting conditions. To analyze the contri-bution of the event camera, we assess the performance gain under different illu-mination conditions. Detailed comparisons are provided in Tab. 10. The results illustrate that our proposed CAFR, capitalizing on the complementary nature of events and frames, consistently improves detection performance and is bet-ter than other fusion methods across various lighting conditions. This analysis provides valuable insights into the effectiveness of incorporating event data in improving overall model performance under diverse illumination scenarios.\nEfficiency analysis. As detailed in Tab. 10, the running speeds of various meth-ods are presented. In comparison with other fusion methods, such as CBAM [50], SAGate [11], and RENet [58], the proposed method exhibits comparable running speed while significantly enhancing detection accuracy.\nMore robustness analysis. Fig. 9 illustrates the relative performance under corruption (RPC) at severity levels ranging from 1 to 5. Across all models, there is a consistent decline in relative performance as the severity of corruption increases. Notably, the model relying solely on RGB data exhibits the steepest"}, {"title": "C Limitations", "content": "While our CAFR has demonstrated efficacy in the context of object detection, it is essential to acknowledge a current limitation. The scope of our evaluations has been confined to this specific task, and extrapolating the performance of CAFR to other prevalent perception tasks, such as semantic segmentation, depth pre-diction, and steering angle prediction, remains unexplored. Future investigations could delve into the broader applicability of CAFR, providing insights into its adaptability and potential limitations across diverse perception domains."}, {"title": "D More Related Works", "content": "In other areas of cross-modal fusion, such as depth, thermal, and event data, we discuss relevant works below. In the field of RGB-D salient object detection (SOD), the joint learning and densely cooperative fusion framework introduced"}]}