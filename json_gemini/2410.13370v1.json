{"title": "MagicTailor: Component-Controllable Personalization In Text-to-Image Diffusion Models", "authors": ["Donghao Zhou", "Jiancheng Huang", "Jinbin Bai", "Jiaze Wang", "Hao Chen", "Guangyong Chen", "Xiaowei Hu", "Pheng-Ann Heng"], "abstract": "Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to gener-ate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to re-configure specific components when personalizing visual concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of the concept and component. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb unde-sired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a bal-anced learning paradigm for desired visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this chal-lenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation.", "sections": [{"title": "INTRODUCTION", "content": "Text-to-image (T2I) diffusion models (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; Chen et al., 2023) demonstrate remarkable capabilities in generating high-quality visual content from textual descriptions. These models are able to create images that closely match the provided prompts. But when certain visual concepts are difficult to articulate through natural language, they may face difficulties in accurately incorporating such elements into the generated images. To address this limitation, some approaches (Gal et al., 2022; Ruiz et al., 2023) enable T2I models to learn specific concepts from a few reference images, thus allowing for more faithful integration of these concepts into the generated images. This process, illustrated in Figure 1(a), is referred as personalization. However, existing personalization methods are limited to replicating predefined concepts and lack the capability to offer flexible, fine-grained control over these elements. This constraint significantly limits their practical applicability in real-world scenarios. A concept often comprises multiple components, such as a house consisting of walls, windows, and doors. Therefore, a more sophisticated challenge in personalization is determining how to effectively control and manipulate these individual components during the personalization process.\nThis paper introduces a new task, component-controllable personalization, which aims to recon-figure specific components of personalized concepts using additional visual references (see Fig-ure 1(b)). In this task, a T2I model is fine-tuned with reference images and corresponding category labels, allowing it to learn and generate the desired concept along with the given component. Achiev-ing this capability would not only enable users to refine and customize concepts with precise control but also foster innovation and creativity, paving the way for novel ideas, inventions, and artworks across various creative domains.\nA straightforward approach to this task is to treat each component as a separate concept and use existing personalization methods to combine multiple concepts with suitable text prompts. How-ever, this naive strategy falls short in component-controllable personalization due to the inherent complexity of handling visual semantics during learning. One of the key challenges in this task is semantic pollution (see Figure 2(a)), where undesired visual semantics inadvertently appear in generated images, thereby \u201cpolluting\u201d the personalized concept. This occurs because the T2I model tends to blend visual semantics from different regions during the learning process. Simply masking out unwanted visual elements in reference images is not a viable solution, as it disrupts the over-all visual context and leads to unintended compositions. Another significant challenge is semantic imbalance (see Figure 2(b)), which causes the T2I model to focus disproportionately on certain aspects, resulting in unfaithful personalization. This issue arises from the semantic disparity be-tween the concept and component, highlighting the need for an effective learning paradigm to better manage concept-level (e.g., person) and component-level (e.g., hair) visual semantics.\nTo address these challenges, we present MagicTailor, a novel framework that enables component-controllable personalization for T2I models (see Figure 1(c)). As shown in Figure 3, we first employ a text-guided image segmenter to generate segmentation masks for both the concept and compo-nent. Then, we introduce a technique called Dynamic Masked Degradation (DM-Deg), which transforms the original reference images into randomly degraded versions, dynamically perturbing undesired visual semantics. This approach helps suppress the model's sensitivity to irrelevant visual details while preserving the overall visual context, thereby effectively mitigating semantic pollu-tion. Next, we initiate a warm-up phase for the T2I model by jointly training it on these degraded images, using a masked diffusion loss to focus on the desired visual semantics and an attention loss to strengthen the correlation between these semantics and pseudo-words. To tackle the is-sue of semantic imbalance, we employ Dual-Stream Balancing (DS-Bal), a dual-stream learning paradigm designed for balancing the learning of visual semantics, to launch the second phase. In this paradigm, the online denoising U-Net performs sample-wise min-max optimization, while the momentum denoising U-Net applies selective preserving regularization. This balanced approach ensures more faithful and accurate personalization of the target concept and component.\nWe validate the effectiveness of MagicTailor through comprehensive qualitative and quantitative experiments, demonstrating that it can achieve state-of-the-art (SOTA) performance in component-controllable personalization. Detailed ablation studies further confirm the impact of the key tech-niques integrated into MagicTailor. Additionally, we showcase its potential to enable a variety of further applications. In summary, the main contributions of this work are as follows:"}, {"title": "RELATED WORKS", "content": "Text-to-Image Generation. Text-to-image (T2I) generation has made remarkable advancements in recent years, enabling the synthesis of vivid and diverse imagery based on textual descriptions. Early methods employed Generative Adversarial Networks (GANs) (Reed et al., 2016; Xu et al., 2018; Qiao et al., 2019; Zhu et al., 2019), and transformers (Ding et al., 2021; Ramesh et al., 2021; Ding et al., 2022; Yu et al., 2022; Bai et al., 2024) began to show the potential in conditioned image generation. More recently, the advent of diffusion models has ushered in a new era in T2I generation (Li et al., 2024; Saharia et al., 2022; Ramesh et al., 2022; Chen et al., 2023; Xue et al., 2024). Leveraging these models, a range of related applications has rapidly emerged, including image editing (Li et al., 2024; Mou et al., 2024; Huang et al., 2024; Feng et al., 2024; Bai et al., 2023), image completion and translation (Xie et al., 2023b;a; Lin et al., 2024), and controllable generation (Zhang et al., 2023; Wang et al., 2024b; Zheng et al., 2023). Despite advancements in T2I diffusion models, generating images that accurately capture specific, user-defined concepts remains challenging. This study explores component-controllable personalization, enabling precise adjustment of concept's components through visual references.\nPersonalization. Personalization seeks to adapt T2I models to generate specific concepts using ref-erence images. Initial approaches such as textual inversion (Gal et al., 2022) and DreamBooth (Ruiz et al., 2023) addressed this task by either optimizing a text embedding or fine-tuning the entire T2I model. Additionally, low-rank adaptation (LoRA) (Hu et al., 2021) has been widely adopted by the research community for personalization (Ryu, 2022), offering an efficient and lightweight solution. The scope of personalization has further expanded to accommodate multiple concepts (Kumari et al., 2023; Avrahami et al., 2023; Liu et al., 2023; Gu et al., 2024; Han et al., 2023; Gu et al., 2024). Besides, a growing body of works has explored tuning-free approaches to personalization (Xiao et al., 2023; Li et al., 2023; Shi et al., 2023; Wei et al., 2023; Wang et al., 2024a). However, these meth-ods often rely on training an encoder with extensive domain-specific image datasets. There is also a category of works studying training-free schemes (Jeong et al., 2024; Zhang et al., 2024), but they generally suffer from inferior performance and tortuous inference processes. In light of that, Our MagicTailor goes with a widely-adopted paradigm of test-time optimization to achieve stable performance and precise control over the concept during personalization."}, {"title": "METHODOLOGY", "content": "Let $I = {({I_{nk}})_{k=1}^K,C_n}_{n=1}^N$ represents a concept-component pair with $N$ samples of concepts and components, where each sample contains $K$ reference images ${I_{nk}}_{k=1}^K$ with a category label $C_n$. In this work, we focus on handling one concept and one component to make the task setting more practical. Specifically, we set $N = 2$ and define the first sample as a concept (e.g., dog) while the second one as a component (e.g., ear). In addition, these samples are associated with the pseudo-words $P = {p_n}_{n=1}$ serving as their text identifiers. The objective of component-controllable personalization is to fine-tune a text-to-image (T2I) model to accurately learn both the concept and component from $I$. Using text prompts with $P$, the fine-tuned model is expected to generate images that contain the personalized concept integrated with specified components.\nIn this section, we begin by providing an overview of the MagicTailor pipeline (refer to Section 3.1). Following this, we delve into its two core techniques: Dynamic Masked Degradation (DM-Deg, see Section 3.2) and Dual-Stream Balancing (DS-Bal, see Section 3.3)."}, {"title": "OVERALL PIPELINE", "content": "The overall pipeline of MagicTailor is illustrated in Figure 3. The process begins with identifying the desired concept or component within each reference image $I_{nk}$, employing an off-the-shelf text-guided image segmenter to generate a segmentation mask $M_{nk}$ based on $I_{nk}$ and its associated category label $C_n$. Conditioned on $M_{nk}$, we introduce Dynamic Masked Degradation (DM-Deg) to perturb undesired visual semantics within $I_{nk}$, addressing semantic pollution. At each training step, DM-Deg transforms $I_{nk}$ into a randomly degraded image $\\hat{I}_{nk}$, with the degradation intensity being dynamically regulated. Subsequently, these degraded images, along with structured text prompts, are used to fine-tune a T2I diffusion model to facilitate concept and component learning. The model is formally expressed as ${{\\epsilon_\\theta}, T_\\theta, E, D}$, where $\\epsilon_\\theta$ represents the denoising U-Net, $T_\\theta$ is the text encoder, and $E$ and $D$ denote the image encoder and decoder, respectively. To promote the learning of the desired visual semantics, we employ the masked diffusion loss, which is defined as\n$\\mathcal{L}_{diff} = \\mathbb{E}_{n,k,\\epsilon,t} [||\\epsilon - \\epsilon_\\theta (z_{nk}^{(t)}, t, e_n) \\odot M'_{nk}||_2^2]$,\nwhere $\\epsilon \\sim \\mathcal{N}(0, 1)$ is the unscaled noise, $z_{nk}^{(t)}$ is the noisy latent image of $\\hat{I}_{nk}$ with a random time step $t$, $e_n$ is the text embedding of the corresponding text prompt, and $M'_{nk}$ is downsampled from $M_{nk}$ to match the shape of $\\epsilon$ and $z_{nk}$. Additionally, we also incorporate the cross-attention loss to strengthen the correlation between desired visual semantics and their corresponding pseudo-words, formulated as\n$\\mathcal{L}_{attn} = \\mathbb{E}_{n,k,t} [||A_{\\theta} (p_n, z_{nk}^{(t)}) - M'_{nk}||_2^2]$,\nwhen $A_{\\theta}(p_n, z_{nk}^{(t)})$ is the cross-attention maps between the pseudo-word $p_n$ and the noisy latent image $z_{nk}^{(t)}$, and $M'_{nk}$ is downsampled from $M_{nk}$ to match the shape of $A_{\\theta}(p_n, z_{nk}^{(t)})$. Using $\\mathcal{L}_{diff}$ and $\\mathcal{L}_{attn}$, we first warm up the T2I model by jointly learning all samples, aiming to preliminarily inject the knowledge of visual semantics. The loss of the warm-up stage is defined as\n$\\mathcal{L}_{warm-up} = \\mathcal{L}_{diff} + \\lambda_{attn} \\mathcal{L}_{attn}$,\nwhere $\\lambda_{attn} = 0.01$ is the loss weight for $\\mathcal{L}_{attn}$. For efficient fine-tuning, we only train the denois-ing U-Net $\\epsilon_\\theta$ in a low-rank adaptation (LoRA) (Hu et al., 2021) manner and the text embedding of the pseudo-words $P$, keeping the others frozen. Thereafter, we employ Dual-Stream Balanc-ing (DS-Bal) to establish a dual-stream learning paradigm to address the challenge called semantic imbalance. In this paradigm, the online denoising U-Net $\\epsilon_{\\theta}$ conducts sample-wise min-max opti-mization for the hardest-to-learn sample, and meanwhile the momentum denoising U-Net $\\tilde{\\epsilon}$ applies selective preserving regularization for the other sample."}, {"title": "DYNAMIC MASKED DEGRADATION", "content": "In this task, one of the major challenges is semantic pollution, where undesired visual semantics could be perceived by the T2I model and thus \u201cpollute\u201d the personalized con-cept. As shown in Figure 2(a.i), the target concept (i.e., person) could be severely disturbed by the owner of the target component (i.e., eye), resulting in a hybrid person. Unfortunately, directly masking out the regions other than the target concept and component would damage the overall visual context, thus leading to overfitting and weird com-positions in Figure 2(a.ii). In light of that, the undesired visual semantics of reference images should be processed properly. Hence, we propose Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics (see Figure 3), aiming at suppressing the T2I model's perception for them while maintaining overall visual contexts (see Figure 2(a.iii)).\nDegradation Imposition. In each training step, DM-Deg imposes degradation in the regions outside segmentation masks for each reference image. There are various types of degradation that can be adopted to perturb the visual semantics of an image, such as noise, blur, and geometric distortions, but not all of them are easy to use and compatible with mask operations. In DM-Deg, we choose to employ Gaussian noise due to its simplicity. For a reference image $I_{nk}$, we randomly sample a Gaussian noise matrix $G_{nk} \\sim \\mathcal{N}(0, 1)$ with the same shape as $I_{nk}$. Note that here the pixel values of $I_{nk}$ range from -1 to 1. Then, the imposition of degradation is conducted as\n$\\hat{I}_{nk} = a_d G_{nk} \\odot (1 \u2013 M_{nk}) + I_{nk}$,\nwhere $\\odot$ indicates element-wise multiplication and $a_d \\in [0, 1]$ is a dynamic weight used to regulate the degradation intensity for $I_{nk}$. In this way, we can obtain a randomly degraded image $\\hat{I}_{nk}$ where the original visual contexts are generally retained. Encountering with $\\hat{I}_{nk}$, it is more difficult for the T2I model to perceive undesired visual semantics in out-of-mask regions, since these semantics would be randomly perturbed with Gaussian noise at each training step.\nDynamic Intensity. Unfortunately, the T2I model may gradually memorize the introduced noise when learning meaningful visual semantics, thus making noise appear in generated images (see Figure 4(a)). Such behavior also aligns with previous observations on deep networks (Arpit et al., 2017). Hence, we design a descending scheme to dynamically regulate the intensity of the imposed noise during training. This scheme adopts an exponential curve that maintains a relatively large intensity in the early steps and decreases dramatically in the later ones. Let $d$ denote the current training step and $D$ denote the total training step. The curve of dynamic intensity is defined as\n$a_d = a_{init}(1 - (\\frac{d}{D})^\\gamma)$,"}, {"title": "DUAL-STREAM BALANCING", "content": "Another primary challenge in this task is semantic im-balance, which arises from the inherent visual seman-tic disparity between the target concept and compo-nent. A concept generally has richer visual semantics than a component (e.g. person vs. hair), while the se-mantic richness of a component might be greater than a concept in some cases (e.g., simple tower vs. intri-cate roof). This imbalance complicates the joint learn-ing process which could overemphasize on either con-cept or component, leading to incoherent generation of the concept or component (see Figure 5(a)). To ad-dress this challenge, we design Dual-Stream Balanc-ing (DS-Bal), which establishes a dual-stream learning paradigm with online and momentum denoising U-Nets (see Figure 3) to balance visual semantic learning of the concept and component, improving personalization fidelity (see Figure 5(b)).\nSample-wise Min-Max Optimization. From a loss perspective, the visual semantics of the concept and component are learned by optimizing the masked diffusion loss $\\mathcal{L}_{diff}$ for all the samples. Unfortunately, this indiscriminate optimization does not allocate adequate learning efforts to the sample that is more challenging to learn, gradually leading to an imbalanced learning process. To over-come this issue, DS-Bal utilizes the online denoising U-Net to learn only the visual semantics of the hardest-to-learn sample at each training step. Inheriting the weights of the original denoising U-Net warmed up by joint learning, here the online denoising U-Net only optimizes the maximum masked diffusion loss among $N$ samples, which is defined as\n$\\mathcal{L}_{diff-max} = \\max_n \\mathbb{E}_{k,\\epsilon,t} [||\\epsilon - \\epsilon_\\theta (z_{nk}^{(t)}, t, e_n) \\odot M'_{nk}||_2^2]$,\nwhere minimizing $\\mathcal{L}_{diff-max}$ can be considered as a form of min-max optimization (Razaviyayn et al., 2020). The learning objective of $\\epsilon_\\theta$ may switch across different training steps and is not consistently dominated by the concept or component. Such an optimization scheme can effectively modulate the learning dynamics of different samples and avoid the overemphasis on any particular one.\nSelective Preserving Regularization. At a training step, the sample neglected in $\\mathcal{L}_{diff-max}$ may suffer from knowledge forgetting. This is because the optimization of $\\mathcal{L}_{diff-max}$, which aims to enhance the knowledge of a specific sample, could inadvertently overshadow the knowledge of the others. In light of this, DS-Bal meanwhile exploits the momentum denoising U-Net to preserve the learned visual semantics of the other sample in each training step. Specifically, we first select the sample that is excluded in $\\mathcal{L}_{diff-max}$, which can be expressed as $S = {n | n = 1,..., N} - {n_{max}}$, where $n_{max}$ is the index of the target sample in $\\mathcal{L}_{diff-max}$ and $S$ is the selected index set. Then, we use the momentum denoising U-Net to apply regularization for $S$, with the masked preserving loss as\n$\\mathcal{L}_{pres} = \\mathbb{E}_{n \\in S,k,t} [| | \\epsilon_\\theta (z_{nk}^{(t)}, t, e_n) \\odot M'_{nk} - \\tilde{\\epsilon} (z_{nk}^{(t)}, t, e_n) \\odot M'_{nk} | |_2^2]$,\nwhere $\\tilde{\\epsilon}$ is updated from $\\epsilon_\\theta$ using EMA (Tarvainen & Valpola, 2017) with the smoothing coefficient $\\beta = 0.99$, thereby sustaining the prior accumulated knowledge of $\\epsilon_\\theta$ in each training step. By encouraging the consistency between the output of $\\epsilon_\\theta$ and $\\tilde{\\epsilon}$ in $\\mathcal{L}_{pres}$, we can facilitate the knowledge maintenance of the other sample while learning a specific sample in $\\mathcal{L}_{diff-max}$. Finally, using a loss weight $\\lambda_{pres} = 0.2$, the total loss of the DS-Bal stage is formulated as\n$\\mathcal{L}_{DS-Bal} = \\mathcal{L}_{diff-max} + \\lambda_{pres} \\mathcal{L}_{pres} + \\lambda_{attn} \\mathcal{L}_{attn}$."}, {"title": "EXPERIMENTS", "content": "EXPERIMENTAL SETUP\nDataset, Implementation, and Evaluation. For a systemic investigation, we collect a dataset from various domains, including characters, animation, buildings, objects, and animals. We employ Sta-ble Diffusion 2.1 (Rombach et al., 2022) as the pretrained T2I diffusion model. Reference images are resized to 512 \u00d7 512, and the LoRA rank and alpha are set to 32. For the warm-up and DS-Bal stage, we set the training steps to 200 and 300 and the learning rate to le-4 and 1e-5, using AdamW (Loshchilov & Hutter, 2017) as the optimizer. To generate evaluation images, we carefully design 20 text prompts covering extensive situations. For each method, we generate 14,720 images to conduct a comprehensive evaluation. To ensure fairness, all the seeds are fixed during training and inference.\nCompared Methods. We compare MagicTailor with SOTA methods of personalization, including Textual Inversion (TI) (Gal et al., 2022), DreamBooth (DB) (Ruiz et al., 2023), Custom Diffusion\nQUALITATIVE COMPARISONS\nThe qualitative results are presented in Figure 6. It shows that TI, CD, and CLiC mainly suffer from semantic pollution, where undesired visual semantics severely influence the personalized concept. Besides, we can observe that DB and BAS also underperform in this challenging task. These meth-ods exhibit an overemphasis on the concept or component due to semantic imbalance, which even leads to an absence of the target component. Moreover, there is also an interesting observation that imbalanced learning could aggravate the effect of semantic pollution, making the color and texture of the target concept or component mistakenly transferred into an unexpected part of generated im-ages. Compared with these methods, MagicTailor can achieve superior performance in generating text-aligned images that also faithfully reflect the target concept and component, demonstrating its remarkable performance in this newly formulated task.\nQUANTITATIVE COMPARISONS\nAutomatic Metrics. We utilize four automatic metrics in the aspects of text alignment (CLIP-T (Gal et al., 2022)) and identity fidelity (CLIP-I (Radford et al., 2021), DINO (Oquab et al., 2023), DreamSim (Fu et al., 2023)). To precisely measure identity fidelity, we segment out the concept and component in each reference and evaluation image, and then eliminate the target component from the segmented concept (see the detailed setup in Appendix A). As we can see, component-controllable personalization remains a tough task even for SOTA methods of personalization. By comparison, MagicTailor can achieve the best results in both identity fidelity and text alignment. It should be credited to the utilization of an effective framework tailored to this special task.\nUser Study. We further evaluate the methods with a user study. Specifically, a detailed question-naire is designed to display 20 groups of evaluation images with the corresponding text prompt and"}, {"title": "ABLATION STUDIES", "content": "We conduct comprehensive ablation studies of MagicTailor, aiming to verify the capability of the overall pipeline. For more ablation studies on other aspects, please refer to Appendix C.\nEffectiveness of Key Techniques. In Table 2, we investigate two key techniques of MagicTailor by starting from a baseline framework described in Section 3.1. Even without DM-Deg and DS-Bal, such a baseline framework can still have competitive performance, showing its reliability. On top of that, we introduce DM-Deg and DS-Bal, where the superior performance trade-off indicates the significance of these two key techniques. Qualitative results can also refer to Figure 2.\nDynamic Intensity Matters. In Table 4, we explore DM-Deg by comparing it with 1) the mask-out strategy; 2) the fixed intensity; 3) the linear intensity (a goes from 1 to 0, or from 0 to 1); and 4) the dynamic intensity with different \u03b3. First, the terrible performance of the mask-out strategy verifies that it is not a valid solution for semantic pollution. Moreover, the dynamic intensity generally shows better results, and it can achieve better overall performance with a proper \u03b3.\nMomentum Denoising U-Net as a Good Regularizer. In Table 3, we study DS-Bal by modifying the U-Net for regularization as 1) the fixed U-Net with \u03b2 = 0 (i.e., the one just after warm-up); 2)"}, {"title": "FURTHER APPLICATIONS", "content": "Decoupled Generation. After learning from a concept-component pair, MagicTailor can also enable decoupled generation. As shown in Figure 8(a), MagicTailor can generate the target concept and component separately in various and even cross-domain contexts. This should be credited to its remarkable ability to accurately capture different-level visual semantics. Such an ability extends the flexibility of the possible combination between the concept and component.\nControlling Multiple Components. In this paper, we focus on personalizing one concept and one component, because such a setting is enough to cover extensive scenarios in the real world, and can be further extended to reconfigure multiple components with an iterative procedure. However, as shown in Figure 8(b), our MagicTailor also exhibits the potential to handle one concept and multiple components simultaneously. These results reflect a prospective direction of exploring better control over diverse components for a single concept.\nEnhancing Other Generative Tools. In Figure 9, we provide some interesting method combina-tions to show that our MagicTailor can enhance other generative tools. The combined tools include ContorlNet (Zhang et al., 2023), CSGO (Xing et al., 2024), and InstantMesh (Xu et al., 2024). As we can see, MagicTailor can be seamlessly integrated into these tools, furnishing them with an ad-ditional ability to control the concept's component in their pipelines. For instance, working with MagicTailor, InstantMesh can conveniently achieve fine-grained design of 3D mesh, demonstrating the practicability of MagicTailor in collaborative applications."}, {"title": "CONCLUSION", "content": "In this paper, we introduce the novel task of component-controllable personalization, which al-lows for precise customization of individual components within a personalized concept. We tackle two major challenges that make this task particularly difficult: semantic pollution, where unwanted visual elements disrupt the integrity of the concept, and semantic imbalance, which skews the learn-ing process of visual semantics. To address these challenges, we present MagicTailor, an innovative framework featuring Dynamic Masked Degradation (DM-Deg) to mitigate unwanted visual seman-tics and Dual-Stream Balancing (DS-Bal) to ensure balanced learning of visual components. Our comprehensive experiments demonstrate that MagicTailor not only sets a new benchmark in this challenging task but also opens up exciting possibilities for a wide range of creative applications. Looking ahead, we envision extending our approach to other areas of image and video generation, exploring how multi-level visual semantics can be recognized, controlled, and manipulated to unlock even more sophisticated and imaginative generative capabilities."}, {"title": "MORE DETAILS OF EXPERIMENTAL SETUP", "content": "As there is no existing dataset specifically for component-controllable personalization, we curate a dataset from the internet to conduct experiments. Unlike previous works (Ruiz et al., 2023; Kumari et al., 2023) that focus on very few categories of concepts, the dataset contains various domains of concepts and components, such as characters, animation, buildings, objects, and animals. Overall, the dataset consists of 23 concept-component pairs totally with 138 reference images, where each concept/component contains 3 reference images and a corresponding category label. The scale of this dataset is aligned with the scale of those datasets used in compared methods (Gal et al., 2022; Ruiz et al., 2023; Kumari et al., 2023; Avrahami et al., 2023; Safaee et al., 2024).\nIMPLEMENTATION\nWe utilize Stable Diffusion 2.1 (Rombach et al., 2022) as the pretrained T2I diffusion model. As commonly done, the resolution of reference images is set to 512 \u00d7 512. Besides, the rank and alpha of the LoRA module are set to 32. To simplify concept learning, we exclude the region of the target component from the segmentation masks of the target concept, e.g., remove the hair from the person in a \u201c + \u201d pair. For the warm-up and DS-Bal stage, we set the learning rate to le-4 and 1e-5 and the training steps to 200 and 300. Moreover, the learning rate is further scaled by the batch size, which is set to completely contain a concept-component pair. For the cross-attention loss, we follow (Avrahami et al., 2023) to average the corresponding cross-attention maps at resolution 16 \u00d7 16 and normalized them to [0, 1], yielding in Equation 2. MagicTailor is trained with an AdamW (Loshchilov & Hutter, 2017) optimizer and a DDPM (Ho et al., 2020) sampler on an NVIDIA A100 GPU. For one concept-component pair, it runs for about 5 minutes. All experiments are accomplished with Python 3.10.11 and PyTorch 1.13.1, based on CUDA 11.6. As done in (Avrahami et al., 2023), the tensor precision is set to float16 to accelerate training. For a fair comparison, all random seeds are fixed at 0 in each experiment, and all compared methods use the same implementation above except for method-specific configurations.\nTEXT PROMPTS FOR EVALUATION\nTo generate images for evaluation, we carefully design 20 text prompts covering extensive situations, which are listed in Table 5. These text prompts can be divided into four aspects, including recontex-tualization, restylization, interaction, and property modification, where each aspect is composed of 5 text prompts. In recontextualization, we change the contexts to different locations and periods. In restylization, we transfer concepts into various artistic styles. In interaction, we explore the spatial interaction with other concepts. In property modification, we modify the properties of concepts in rendering, views, and materials. Such a group of diverse text prompts allows us to systemically evaluate the generalization capability of a method.\nSCHEME OF GENERATING EVALUATION IMAGES\nWe generate 32 images per text prompt for each pair, using a DDIM (Song et al., 2020) sampler with 50 steps and a classifier-free guidance scale of 7.5. To ensure fairness, we fix the random seed within the range of [0, 31] across all methods. This process results in a total of 14,720 images for each method to be evaluated, ensuring a robust and thorough comparison.\nAUTOMATIC METRICS\nWe utilize four automatic metrics in the aspects of text alignment (CLIP-T (Gal et al., 2022)) and identity fidelity (CLIP-I (Radford et al., 2021), DINO (Oquab et al., 2023), DreamSim (Fu et al., 2023)). To precisely measure identity fidelity, we improve the traditional measurement approach for personalization. This is because a reference image of the target concept/component could contain an undesired component/concept that is not expected to appear in evaluation images. Specifically, we use Grounded-SAM (Ren et al., 2024) to segment out the concept and component in each reference and evaluation image. Then, we further eliminate the target component from the segmented concept"}, {"title": "ADDITIONAL COMPARISONS", "content": "One might be curious about whether component-controllable personalization can be accomplished by providing detailed textual descriptions to the T2I model. To investigate this, we separately feed the reference images of the concept and component into GPT-40 to obtain detailed textual descrip-tions for them. The text prompt we used is \u201cPlease detailedly describe the  of the upload images in a parapraph\u201d, where \u201c", "The up-loaded images contain a special instance of the , please mark it as #\", where": "is replaced with the category label of the con-cept or component. Then, we instruct them to perform image generation, using the text prompt of \u201cPlease generate images containing # with #\u201d, where \u201c\u201d and \u201c"}]}