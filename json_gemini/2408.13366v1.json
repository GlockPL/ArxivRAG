{"title": "CodeRefine: A Pipeline for Enhancing\nLLM-Generated Code Implementations of\nResearch Papers", "authors": ["Ekaterina Trofimova", "Emil Sataev", "Abhijit Singh Jowhari"], "abstract": "This paper presents CodeRefine, a novel framework for au-\ntomatically transforming research paper methodologies into functional\ncode using Large Language Models (LLMs). Our multi-step approach\nfirst extracts and summarizes key text chunks from papers analyzes their\ncode relevance, and creates a knowledge graph using a predefined ontol-\nogy. Code is then generated from this structured representation and en-\nhanced through the proposed retrospective retrieval augmented genera-\ntion approach. CodeRefine addresses the challenge of bridging theoretical\nresearch and practical implementation, offering a more accurate alterna-\ntive to LLM zero-shot prompting. Evaluating diverse scientific papers\ndemonstrates CodeRefine's comparatively improving code implementa-\ntion from the paper, potentially accelerating the adoption of cutting-edge\nalgorithms in real-world applications.", "sections": [{"title": "1 Introduction", "content": "The rapid growth of scientific research has led to an increasing volume of liter-\nature across various fields [1] [8]. While this abundance of information fosters\ninnovation and collaboration, it also presents significant challenges in efficiently\ntranslating theoretical methodologies into practical applications. Researchers of-\nten spend considerable time manually interpreting and implementing algorithms\nand experimental setups described in papers, which can lead to inefficiencies\nand inconsistencies. This challenge highlights the need for automated tools to\nstreamline the process of converting scientific descriptions into executable code.\nRecent advancements in Natural Language Processing (NLP) and Machine\nLearning (ML) have opened new avenues for automating code generation from\nscientific texts. In particular, the emergence of LLMs has improved the ability\nto understand and generate human language, enabling the context-aware trans-\nlations of textual descriptions into code. However, LLM-based approaches often\nfall short in handling the intricacies and domain-specific nuances of scientific\ntexts.\nThis paper introduces CodeRefine, a framework designed to bridge the gap\nbetween theoretical research and practical implementation, providing a more\nreliable alternative to traditional LLM-based approaches.\nThe paper is organized in the following sections. Section 2 reviews the related\nwork in the field. Section 3 describes the metric used for the comparison of code\nvariants. Section 4 introduces the methodology under the CodeRefine framework.\nSection 5 describes the experiments and results. Section 6 discuss the limitations\nof the proposed framework, while Section 7 shows how the presented ideas can\nbe useful in another application. Section 8 concludes the work."}, {"title": "2 Related Works", "content": "With advancements in NLP, techniques such as named entity recognition, depen-\ndency parsing, and semantic role labeling have become instrumental in identify-\ning key components like variables, operations, and experimental conditions [4].\nSupervised learning models have been applied to classify and extract relevant\ntext sections for code generation. Koncel-Kedziorski et al. [10] utilize ML models\nto extract mathematical expressions and their contexts from scientific papers,\npaving the way for more precise code generation.\nSeq2Seq models have been an early application of deep learning to code\ngeneration, translating natural language descriptions directly into code. These\nmodels have demonstrated the potential to generate accurate code snippets from\nproblem statements or descriptions [14].\nThe attention mechanism, featuring by transformer models, has improved\nthe understanding and generation of natural language, thus facilitating more\naccurate code generation from complex scientific texts [24] [5].\nCode-oriented transformer-based language models, like CodeBERT and Codex,\nleverage large-scale pre-training on code repositories and scientific texts, enabling\nthe generation of code snippets and entire functions from scientific paper descrip-\ntions [7] [3].\nLewis et al. [12] have introduced the Retrieval Augmented Generation (RAG)\nconcept to improve the quality and accuracy of generated text. In RAG, a re-\ntriever component selects relevant documents from a large corpus, and a genera-\ntor component uses these documents to produce more informed and contextually\nappropriate responses.\nOne advancement in RAG methodologies is the integration of knowledge\ngraphs. Knowledge graphs provide a structured representation of information\nthat can be used to enhance the retrieval process. This representation aids the\ngenerative model in producing more relevant output [25] [18]."}, {"title": "3 Metric", "content": "To assess the proposed framework, we compare the ground-truth code with that\nrefined by our pipeline, and vanilla LLM (GPT-40). We use the TSED (Tree-\nbased Structural Edit Distance) [21] [22] score to evaluate the closeness of the\ntwo pieces of code to the ground truth by measuring the TSED scores of the\nabstract syntax trees (ASTs) [9] of the 2 generated code and the AST of the\nground truth.\nLet T\u2081 and T2 be two abstract syntax trees. The Tree Edit Distance (TED)\nscore D(T1, T2) is defined as follows:\n\n$D(T_1, T_2) = \\min_{\\mathcal{O}} (\\sum_{o \\in \\mathcal{O}} cost(o))$\n\nWhere $\\mathcal{O}$ is the set of all possible sequences of edit operations transforming\nT\u2081 into T2, and the cost function cost(o) is given by the costs for inserting,\ndeleting, and renaming nodes: Ci, Cd, and Cr, respectively.\nThe TED algorithm defines several operations on the nodes of the AST,\nincluding insertion, deletion, and renaming of nodes. Insertion penalizes the LLM\npipeline for each insertion operation done on its code. Deletion represents the\nmagnitude of penalty applied for LLM hallucinations. Remaining controls the\nLLM pipeline for each renaming operation of the nodes.\nTree-based Structural Edit Distance (TSED) score is defined as following:\n\n$TSED = \\max\\{1 - \\frac{TED}{MaxNodes(T_1,T_2)}, 0\\}$\n\nA higher TS ED score indicates greater similarity between the two code\nsnippets, while a lower score indicates more significant differences. Following the\nmetric construction, one can notice, that the penalty weight optimization for the\nthree operations is pivotal to the TSED score."}, {"title": "4 Methodology", "content": "Figure 1 illustrates the comprehensive flow of the CodeRefine pipeline.3 The\nprocess begins with a user-provided research paper, which is segmented into\nmultiple text chunks (Text_1, Text_2, ..., Text_n), each accompanied by a\nsummary. These text chunks are trivially decided as the paragraphs of the paper.\nThat is two pieces of text separated by more than one newline character. The\nsummary of each of the chunks is generated using the Llama3-70B model. The\nsegmentation ensures the content is granular enough for effective analysis.\nEach text chunk undergoes an analysis to determine its relevance to code\ngeneration by prompting a large language model (LLM). This analysis focuses\non identifying sections that describe algorithms, implementation details, or step-\nby-step instructions pertinent to code generation."}, {"title": "5 Experiments and Results", "content": "5.1 Task-Aware Vectorizer\nWe use the MixedBreadAI's embed-2d-large-v1 model [11] [13] as task-aware vec-\ntorizer (Figure 2). This model is an advanced embedding model designed to pro-\ncess high-dimensional data and create rich semantic representations. This model\nleverages a bi-encoder architecture where queries and documents are processed\nseparately and then compared in a shared embedding space. The embed-2d-\nlarge-v1 is particularly suited for tasks requiring high semantic accuracy and rel-\nevance, such as document retrieval, semantic search, and knowledge-based tasks.\n5.2 TSED penalty weight optimization\nAs described in Section 3, the tree distance represents the cumulative cost func-\ntion, which pertains to the minimum number of deletion, insertion, and renaming\noperations required to convert the origin tree to the target tree. The weight or\npenalty associated with each operation is summed to form the cost function.\nBelow, we provide a step-by-step breakdown of penalty weight optimization for\nTSED.\n1. We penalize the deletion of nodes\" (d) more than the \"insertion of new\nnodes\" (i) to the origin AST (representative of the generated code).\n2. This approach helps control LLM hallucinations, as we aim to avoid the\nLLM introducing its inputs when generating code based on a research paper\ndescribing a novel architecture.\n3. Next, we set the value of (i) higher than renaming (r). The penalty for (r)\nis kept relatively lenient, as renaming nodes is not considered a significant\nindicator of the LLM's failure to generate relevant code.\n4. Therefore, the final relative relationship among the values of d, i, and r is:\n\nd>i>r\n\nWe start with initial values of (3, 1, 0.1) and empirically adjust them to (4,\n0.7, 1).\nIn this empirical method, the only criterion for adjusting the values of d,\ni, and r (while maintaining the above relative relationship) is to achieve a\npractical value for code similarity between the generated code and the ground\ntruth code.\nThe penalty weights are not unique Before describing the results, it is\nimportant to note that penalty weights for the operations namely deletion, in-\nsertion, and renaming are different for different research papers. That is, they\ndepend on the complexity of the model architecture described in the paper. This\ncomplexity is not quantifiable and rather should be qualitatively understood as\n\"how complex the described architecture is for GPT-40 concerning its ability to\nwrite a code-implementation of the paper\".\nThe reason for this is related to the degree to which the LLM (GPT-40 in\nthis case) can hallucinate when encountered with a substantially complex model\narchitecture. That is, the LLM can hallucinate to such an extent that the con-\ntribution of the deletion penalty makes the total penalty big enough to nullify\nthe TSED score to 0.0 due to the ramp function. In this case, we can not do\na comparison study of our pipeline's potential to reduce the hallucinations and\nhence improve quality.\nTherefore, the TSED penalty weight optimization is emperically carried out\nfor each paper keeping in mind the qualitative model architecture complexity\nand the relative order of the d,i, and r values described in the preceding section."}, {"title": "6 Limitations", "content": "While CodeRefine offers a significant advancement in automating the translation\nof research methodologies into executable code, several limitations should be\nacknowledged:\nComplexity of Papers: The framework's performance can degrade with the in-\ncreasing complexity of the research papers. Papers with highly intricate method-\nologies or extensive mathematical notations might not be fully understood and\ncorrectly translated by the LLMs.\nQuality of Source Texts: The quality of the extracted text chunks significantly\ninfluences the accuracy of the generated code. Poorly written or ambiguous sec-\ntions in the original papers can lead to suboptimal code outputs.\nRetrospective Retrieval Limitations: The Retrospective Retrieval-Augmented\nGeneration (RRAG) component relies on the quality and relevance of the docu-\nments in the dynamic database. If key references or papers are missing from the\ndatabase, the quality of the generated code can be compromised."}, {"title": "7 Downstream tasks", "content": "Building on the concepts of dynamic database systems and task-aware vector-\nization, we propose the development of a research helper tool designed to assist\nusers in obtaining information related to academic papers. For example, a user\ncan input a query about a concept discussed in a paper, which is also referenced\nin another paper. The research helper will then provide an answer based on the\nreferenced paper. 4 The design of the research helper is illustrated below:"}, {"title": "8 Conclusion", "content": "This paper has introduced CodeRefine, a novel tool designed to enhance the\nefficiency and effectiveness of academic research through advanced information\nretrieval and synthesis capabilities. By leveraging dynamic database systems\nand task-aware vectorizers, CodeRefine offers researchers a powerful resource for\naccessing and understanding complex academic concepts.\nCodeRefine stands out with its integration of a Retrospective Retrieval Aug-\nmented Generation system, which utilizes instruction-tuned vector embeddings\nto provide task-aware retrieval of relevant text chunks. The step-by-step break-\ndown of the system ensures that the most relevant to code generation information\nis retrieved and accurately processed.\nFuture work will focus on expanding the capabilities of CodeRefine by in-\ncorporating more sophisticated machine learning algorithms and expanding the\ndataset to include a broader range of academic papers. Additionally, we aim\nto refine the user interface to make the tool more intuitive and accessible to\nresearchers across various disciplines.\nIn conclusion, CodeRefine represents a significant advancement in the realm\nof academic research tools. By combining the strengths of dynamic databases\nand task-aware vectorizers, it provides a robust and user-friendly solution for\nnavigating the complexities of academic literature."}]}