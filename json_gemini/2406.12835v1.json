{"title": "Influence Maximization via Graph Neural Bandits", "authors": ["Yuting Feng", "Vincent Y. F. Tan", "Bogdan Cautis"], "abstract": "We consider a ubiquitous scenario in the study of Influence Max- imization (IM), in which there is limited knowledge about the topology of the diffusion network. We set the IM problem in a multi-round diffusion campaign, aiming to maximize the number of distinct users that are influenced. Leveraging the capability of ban- dit algorithms to effectively balance the objectives of exploration and exploitation, as well as the expressivity of neural networks, our study explores the application of neural bandit algorithms to the IM problem. We propose the framework IM-GNB (Influence Maximization with Graph Neural Bandits), where we provide an es- timate of the users' probabilities of being influenced by influencers (also known as diffusion seeds). This initial estimate forms the basis for constructing both an exploitation graph and an exploration one. Subsequently, IM-GNB handles the exploration-exploitation tradeoff, by selecting seed nodes in real-time using Graph Con- volutional Networks (GCN), in which the pre-estimated graphs are employed to refine the influencers' estimated rewards in each contextual setting. Through extensive experiments on two large real-world datasets, we demonstrate the effectiveness of IM-GNB compared with other baseline methods, significantly improving the spread outcome of such diffusion campaigns, when the underlying network is unknown.", "sections": [{"title": "1 INTRODUCTION", "content": "Motivated by the rise of \"influencer marketing\" in social media advertising, a class of algorithmic problems termed Influence Max- imization (IM) has emerged, starting with the pioneering work of [10, 18]. These algorithms aim to identify the most influential nodes within a diffusion network for initiating the spread of spe- cific information, thereby maximizing its reach. In many ways, this research directly mirrors the increasingly prevalent and successful marketing strategy of targeting key individuals (influencers).\nThe objective of IM is typically formulated by maximizing the expected spread under a stochastic diffusion model, which charac- terizes the information dissemination process. The work of [18] laid the foundations for the IM literature, by introducing two prominent models: Linear Threshold (LT) and Independent Cascade (IC). These models, widely adopted in subsequent research, represent diffusion networks as probabilistic graphs, where the edges are weighted by probabilities of information transmission.\nSelecting the seed nodes maximizing the expected spread is NP- hard under common diffusion models [18]. Despite the development of approximate algorithms, exploiting the monotonicity and sub- modularity of the spread, scaling IM algorithms to large networks remains challenging. Acquiring meaningful influence probabilities is equally challenging, as learning them from past information cas- cades (e.g., as in [11, 13]) can be data-intensive and thus impractical. Moreover, the applicability of such models is limited in scenarios where historical cascades are not available.\nIn the face of these challenges, since even the most efficient IM algorithms such as [16, 30] rely on assumptions and parameters that often fail to capture the complex reality of how information spreads online, a change in research direction has been followed recently. It consists of approaches that neither rely on pre-defined diffusion models nor require upfront knowledge of the diffusion network. Instead, these online methods, such as [17, 20, 33], learn to spread on the fly. More precisely, they involve a sequential learning agent that actively gathers information through a multi-round influ- ence campaign. In each round, the agent selects so-called seed nodes, observes the resulting information spread, and uses this feedback to make better choices in subsequent rounds, with the campaign's total reward being the objective that is to be optimized. Such a learning framework leads naturally to a policy that balances explor- ing unknown aspects (i.e., the diffusion dynamics) with exploiting known and successful choices (i.e., the high-performing spread seed nodes), using multi-armed bandits [21].\nWe consider in this paper such an an online IM scenario with limited network information. Specifically, the diffusion graph is largely unknown, except for a set of predefined influencers, repre- senting the potential seeds for information dissemination at each round of a multi-round diffusion campaign. Additionally, we incor- porate contextual features of both influencers and the information being diffused. Regarding the latter, the rationale is that within a campaign aiming to maximize the reach of a specific message, its framing and presentation can significantly impact its spread. For instance, a political campaign may use various formats like news articles, opinion pieces, data visualizations, or multimedia content, each leading to distinct diffusion patterns.\nWe leverage such contextual information through the formal framework of Contextual Multi-Armed Bandits (CMABs) [21]. Fur- thermore, recognizing that significant correlations between the fea- tures of the basic (to-be-influenced) users may exist, albeit unknown to the agent, and that by implication their activation probabilities may be correlated, we enhance the learning framework with mech- anisms by which each activation can provide useful information about neighbouring users in the network as well, allowing to refine the agent's predictions. We achieve this by adapting to our IM prob- lem setting the Graph Neural Bandits (GNB) framework of [26] (a bandit algorithm for recommender systems). Correlation graphs are constructed based on the similarity of users to be influenced by the same influencer, and GNBs are then employed to handle the challenges associated with graph-based bandit algorithm. In doing so, our work is the first to leverage the implicit relationships that may exist between basic users in the unknown diffusion medium. In essence, we dynamically model these relationships based on the observed campaign feedback, and we use them as input for a graph neural network (GNN)-based learning algorithm guiding the seed selection process at each round, optimizing choices under the exploit-explore paradigm.\nOverview of our IM scenario. As usual in IM scenarios, we run campaigns under budget constraints (limited seedings and rounds), with the goal to maximize the number of distinct users activated, starting from known influencers. The learning agent chooses seeds sequentially, i.e., at each round, with potential re-seeding, and feed- back consists solely of the activated nodes after each round, without additional details on the triggering causes. The feedback is used to refine estimates of influencer potential, guiding future seeding choices. Aligning with the overall objective, each round's reward is the number of newly activated users, and the campaign aims to maximize the cumulative reward across rounds. In this scenario, we mimic real-world influencer marketing, where access is limited to a few influencers, feedback is restricted to user actions (like pur- chases or subscriptions), and the goal is to reach as many unique users as possible.\nOur contributions. We detail our contributions in the following:\n\u2022 By introducing the IM-GNB framework, we connect GNBs and the IM problem. This integration is non-trivial due to the inherent challenges of learning from graph-structured data and making sequential decisions under uncertain environments in the context of diffusion campaigns.\n\u2022 We tackle the challenge of balancing exploration and exploitation in dynamic influence propagation by incorporating contextual bandits into the IM-GNB framework. This enables us to effec- tively explore the potential rewards while exploiting available information, resulting in enhanced influence spread in real-world scenarios.\n\u2022 We construct user-user correlation graphs for exploitation and exploration purposes, capturing intricate interactions among users and influencers in each round of the diffusion campaign. This graph-based approach is scalable to various network set- tings, even without prior knowledge of the network's topology structure.\n\u2022 We develop a novel algorithm that optimally selects seed nodes in real-time, with contextual bandits integrated with GNNs to refine the reward estimates in each contextual setting. Through extensive experiments, we show that our algorithm outperforms baseline methods, highlighting the utility of GNBs as a prin- cipled approach to optimize influence campaigns in uncertain environments."}, {"title": "2 RELATED WORK", "content": "Influence Maximization (IM) addresses the challenge of identify- ing a set of seeds (influencers) within a social network to maximize information spread. Researchers first explored this problem in [10]. Later, [18] provided a clear formulation of the problem, including how influence spreads through stochastic models like Independent Cascade (IC) and Linear Threshold (LT). They also described the important properties of the spread objective, its approximation guar- antees and hardness results. Since then, such stochastic models have become widely adopted in the literature, and most works focused on finding approximate solutions that can be computed efficiently.\nA key breakthrough was the concept of reverse influence sampling, introduced in [6] and made practical in [23, 30, 31]. Diffusion model- based IM approaches rely on diffusion graphs where the edges are labeled by weights (spread probabilities). In empirical evaluations, these weights may be data-based [14, 15] (computed from diffusion cascades), degree based, or simply assumed random. Some recent studies [12, 24] employ representation learning to infer influence probabilities from ground-truth diffusion cascades, a resource that may not be readily available in many application scenarios. (See the recent survey [22] for a review of the IM literature.)\nBandits for Influence Maximization By virtue of their versa- tility and sequential nature, bandit algorithms are apt to be used in IM problems, especially in uncertain diffusion environments with which a learning agent may interact repeatedly [17, 29, 34, 37]. A multi-round, sequential setting allows to spread information and gather feedback, striking a balance between influencing / ac- tivating nodes in each round and learning influence parameters for uncertain or unexplored network facets. This strategy closely mirrors real-world influencer marketing scenarios, in which cam- paigns often unfold over time. [34] is one of the earliest works that map an IM problem formulation to a combinatorial multi-armed bandit (CMAB) paradigm, where diffusions are assumed to fol- low the IC model. IMLinUCB [35] learns the optimal influencers dynamically, while repeatedly interacting with a network under the IC assumption as well. Vaswani et al. [33] introduces a dif- fusion model-agnostic framework, based on a pairwise-influence semi-bandit feedback model and the LinUCB-based algorithm, ad- dressing scenarios involving new marketers that exploit existing networks. Since the aforementioned approaches leverage a given diffusion graph topology, the inherent difficulty of obtaining such data limits their practical interest.\nOperating in highly uncertain diffusion scenarios that (i) make no assumption on the diffusion model and (ii) lack knowledge of the diffusion topology and historical activations (cascades), [20] proposes FAT-GT-UCB, where a Good-Turing estimator is used to capture the utility (called remaining potential) of an influencer, throughout the multiple rounds of a diffusion campaign. They also consider a fatigue effect for influencers, since these may be are repeatedly chosen in the sequential rounds. GLM-GT-UCB [17] considers the same setting as [20], while exploiting contextual in- formation (e.g., features pertaining to influencers or the information being conveyed). Our work shares a similar setting, where the net- work topology is unknown and no assumptions are made about the diffusion model. In a multi-round diffusion campaign, we select at each round diffusion seeds, without factoring in influencer fatigue.\nBandits with deep learning Early works [1, 9, 28] in the con- textual bandit literature focused on linear models, assuming the expected reward at each round is linear in the feature vector. This assumption, however, often fails to hold in practice, prompting ex- ploration into nonlinear or nonparametric contextual bandits [7, 32]. However, these more complex models impose restrictive assump- tions on the reward function, such as Lipschitz continuity [7], or a re- ward function from a reproducing kernel Hilbert space (RKHS) [32]. To overcome these limitations, several recent studies [27, 38, 41] leverage the expressivity of deep neural networks (DNNs) to in- corporate nonlinear models, which require less domain knowledge. The works of [27, 38] employ DNNs for effective context transfor- mation with a linear exploration policy, showing notable empirical success despite the absence of regret guarantees. The work of [41] introduces NeuralUCB, a provably efficient neural contextual bandit algorithm using DNN-based random feature mappings to construct the UCB, with a near-optimal regret guarantee. The construct of the UCB is based on the past gradient of the exploitation function. The work of [40] assigns a normal distribution as the distribution of the reward of each arm, similarly to the deviation computed on the gradient of the estimation function. Similar to some other studies, EE-Net [2] has an exploitation network to estimate re- wards for each arm. It additionally builds an exploration network to predict the potential gain for each arm, relative to the current es- timate, where the input of the exploration network are the previous gradients of the exploitation function. The work of Qi et al. [26] employs contextual neural bandits in recommender systems, to build a graph neural bandit framework where each arm is induced with an exploitation graph and an exploration one, with the weights of edges representing users' correlations regarding the exploitation and exploration performed. The effectiveness of [26] in the recom- mendation setting serves as our initial motivation for leveraging its neural bandits framework in our IM problem. Given the similarities in predicting user preferences (user-item in recommender systems or user-influencer susceptibility in IM), we exploit a graph neural contextual bandit algorithm to maximize the influence spread in multi-round diffusion campaigns."}, {"title": "3 PROBLEM FORMULATION", "content": "We formulate the Influence Maximization (IM) problem with a discrete-time diffusion model [18], adopting a combinatorial multi- armed bandit paradigm to estimate the influence spread.\nIM Problem Within the context of information scenarios charac- terized by stochastic or epidemic information diffusion phenomena, particularly on social media, the information spread is initiated by seed users (influencers) and amplified through sharing and retweet- ing via user interactions. For a campaign of information spread consisting of T rounds (trials), we select the influencers at each round to maximize the overall information spread.\nWe are given a known base set of influencers $K = \\{k_i\\}_{i=1}^n$ as seeds, a budget of T rounds (trials). At each round $t\\in \\{1,2..., T\\}$, the environment provides us with the message $C_t$ to diffuse, and there are $L \\in \\{1, 2, ..., n\\}$ seeds to be activated initially. With $I_t$ (which has cardinality $|I_t| = L$) the set of activated seeds, $S(I_t, C_t)$ is the round's spread (all activated users) starting from the chosen seed set $I_t$. Our objective is to maximize the cumulative and distinct spread of the T rounds, i.e., find\n$\\underset{I_t\\subset K, |I_t|=L,\\forall 1<t\\leq T}{argmax}\\bigcup_{1<t<T}S(I_t, C_t)$                                                                                                                                                                                                                                                                                (1)\nAdaptation to the bandit setting To adapt the IM problem to a contextual bandit setting, the set of influencers K can be considered the set of arms to be pulled in T rounds. At each round t, with the provided message $C_t$ as the context, the set of arms $I_t = \\{k_i\\}_{i=1}^L$ is chosen. For each chosen arm $k_i$, $A_{k_i}$ is the set of basic users activated or influenced by seed (arm) $k_i$. For each basic user u, let $c_u$ denote the total number of times it has been influenced or activated until round t. With the set of activated users (influence spread) as the node semi-bandit feedback, the reward is the number of new activations [17] as\n$R_t = \\sum_{u\\in \\bigcup_{k_i\\in I_t} A_{k_i}} \\{ 1 c_u>0\\} - R_{t-1}; \\qquad R_0 = 0,$                                                                                                                                                                                                                                                                          (2)\nNote that distinct activations are used for the cumulative reward, i.e., a given user will be counted only once in the total reward, even if it has been influenced several times.\nModeling with graph bandits We are mainly motivated by appli- cation scenarios in social media (e.g., information campaigns for elections, online advertising, public awareness campaigns, crisis information diffusion, etc.), where users may exhibit similar prefer- ences and influence susceptibility for certain diffusion topics (e.g., sharing the same political views) initiated by certain influencers (arms), while they may react differently and be more susceptible to other influencers for other topics (e.g., entertainment or sports).\nThus, instead of representing the social graph uniformly, in the bandit setting, we allow each arm $k_i$ at each round t to in- duce a distinct graph $G_{i,t} (U, E, W_{i,t})$ to represent user connectivity. With $k_i$ the $d_1$-dimensional feature vector of arm $k_i$ and $C_t$ the $d_2$- dimensional context vector, the expected reward\u00b9 at each round $t\\in [T]$ brought by arm $k_i$ is defined as\n$r_{i,t} = f(k_i, C_t, G_{i,t}).$                                                                                                                                                                                                                                                                                    (3)\nIn $G_{i,t}$, each user $u \\in U = \\{1,2,..., m\\}$ corresponds to a node, E is the set of edges connecting users, and $W_{i,t} = \\{w_{i,t}(u, u') : u, u' \\in U\\}$ is the set of weights corresponding to each edge $e \\in E$. Modeling real applications, we assume that the weights of the edges connecting nodes in $G_{i,t}$ represent users' similarity w.r.t. the same influencer (arm $k_i$), i.e., the probability to be similarly influenced by arm $k_i$ in round t, which is defined as\n$w_{i,t}(u, u') = \\Phi^{(1)} (\\mathbb{E} [p_{i,u}|k_i, C_t], \\mathbb{E} [p_{i,u'}|k_i, C_t]),$                                                                                                                                                                            (4)\nwhere $p_{i,u} = h_u(k_i, C_t) \\in [0, 1]$ the expected diffusion probabi- lity between influencer (arm $k_i$) and user u under the context $C_t$, and $\\Phi^{(1)} : \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ maps the expected diffusion probability of users w.r.t. influencer $k_i$ to the weights among users in $G_{i,t}$.\nHowever, the similarity graph $G_{i,t}$ and the function $h_u$ are un- known in our problem setting. Thus we propose an estimate graph"}, {"title": "4 PROPOSED FRAMEWORK", "content": "Many recent works [17, 35] on the IM problem that exploit bandits for the exploration-exploitation trade-off assume that the reward is a linear or generalized linear function of arm vectors. Considering the high complexity and dynamicity of social network-related data, we use the representation power of neural networks to firstly, learn users' connectivity to build exploitation and exploration graphs and secondly, learn the underlying reward function and the potential gains on the estimated reward. The overall framework of our model is illustrated in Fig. 1.\n4.1 Estimating the User Graphs\nIn this section, we first provide a strategy to estimate the users' correlations to be influenced by the same arm, forming the basis for the exploration-exploitation strategy in Sec. 4.2.\n4.1.1 User exploitation graph. We bridge the users in the social graph with diffusion probabilities between influencers and users. The intuition is that given the same message to be diffused (context $C_t$), users who exhibit high correlations in this graph are more likely to be influenced by the same influencer. As the context changes, an influencer may not exert the same influence on users. Thus, at each round t and for each influencer (arm) $k_i$, we induce an exploitation graph $G_{i,t}^{(1)}$ to represent the users' correlations.\nIn the exploitation graph $G_{i,t}^{(1)}$, the weights among users are referred to as users' correlations w.r.t. the diffusion probability from arm $k_i$ (hence likelihood to influenced by the same influencer $k_i$). For each user $u \\in U$, we use a neural network as the pre-defined hypothesis function $h_u^{(1)} = h_u^{(1)} (k_i, C_t; [P^{(1)}]_{t-1})$ to learn these probabilities ($[P^{(1)}]_{t-1}$ are the updated parameters of the network from round t \u2212 1). The weights in $G_{i,t}^{(1)}$ are\n$w_{i,t}^{(1)} (u, u') = \\Phi^{(1)} (h_u^{(1)} (k_i, C_t), h_{u'}^{(1)} (k_i, C_t)),$                                                                                                                                                                                                                                                                         (7)\nwhere $\\Phi^{(1)}$ is the same function in Eq. (4). For each user u, $h_u^{(1)}$ will be trained by gradient descent (GD) with the given context and the chosen arm as input and the reward as label. The loss is defined as\n$L_u^{(1)} = (h_u^{(1)} (k_i, C_t; P^{(1)}) - d_u)^2,$                                                                                                                                                                                                                                                                                           (8)\nwhere $d_u = 1$ if $c_u > 0$ and $c_{u-1} = 0$, else $d_u = 0$. Recall that we use $c_u$ to denote the total number of times user u has been activated (influenced) up to and including round t, and we only count the newly activated nodes at each round.\n4.1.2 User exploration graph. Recent works on neural bandits [2\u2013 4, 40, 41] take advantage of the representation power of neural networks to learn the uncertainty of estimation (potential gain). These works use the past gradient to incorporate the feature of arms and the learned discriminative information of estimation function ($h_u^{(1)} (k_i, C_t)$ in our work).\nQi et al. [26] applied this paradigm in collaborative filtering for user-item pair prediction in online recommendation scenarios and demonstrated its effectiveness. Since the IM problem shares similarity with predicting user preferences towards items (in our case susceptibility to influencers), especially when the connections (correlations) among users are reinforced by social ties, we apply the past gradient to quantify the \"exploration bonus\" [21].\nFor a user $u \\in U$, we use a neural network $h_u^{(2)}$ to learn the uncertainty of the estimated diffusion probability between arm $k_i$ and user u, i.e., $\\mathbb{E} [p_{i,t}|u, k_i, C_t] - h_u^{(1)} (k_i, C_t)$, similar to Eq. (6). As in [2], we apply $h_u^{(2)}$ directly on the previous gradient of $h_u^{(1)}$. Analogously, the exploration graph $G_{i,t}^{(2)} = (U, E, W_{i,t}^{(2)})$ is con- structed with $W_{i,t}^{(2)} = \\{w_{i,t}^{(2)} (u, u') : u, u' \\in U\\}$, and $w_{i,t}^{(2)} (u, u')$ is the exploration correlation among users, defined as\n$w_{i,t}^{(2)} (u, u') = \\Phi^{(2)} (\\nabla_p h_u^{(1)} (k_i, C_t), \\nabla_p h_{u'}^{(1)} (k_i, C_t)).$                                                                                                                                           (9)\nThe previous gradient $\\nabla_p h_u^{(1)} (k_i, C_t) = \\nabla_p h_u^{(1)} (k_i, C_t; [P^{(1)}]_{t-1})$ is the network gradient at round t \u2212 1, with $[P^{(1)}]_{t-1}$ the last up- dated parameters of $h_u^{(1)}$. In addition, $\\Phi^{(2)}$ is the function defined in Eq. (6) and $h_u^{(2)}$ will be trained with GD, where the previous gradient of $h_u^{(1)}$ is computed based on the input samples, and the residual diffusion probability (potential gain on the estimated diffu- sion probability) is the label, with the loss given as\n$L_u^{(2)} = (h_u^{(2)} (\\nabla_p h_u^{(1)} (k_i, C_t)) - (d_u - h_u^{(1)} (k_i, C_t)))^2.$                                                                                                                                                                    (10)\nRegarding the network structure of $h_u^{(1)}$ and $h_u^{(2)}$, since there are no data characteristics requiring specific models such as RNNs for sequential dependencies or CNNs for visual content, we simply employ an J-layer fully connected (FC) neural network at this stage for initial graph estimation.\nTo summarise, we use $h_u^{(1)}$, denoting user u, to obtain the es- timated diffusion probability from influencer (arm) $k_i$ to u (the estimation function is built for each user individually, i.e., there are m estimation functions $h_u^{(1)}$ in total), and the exploitation graph"}, {"title": "4.2 Exploitation-Exploration with GNNS", "content": "With the user correlation graphs $G_{i,t}^{(1)}$ and $G_{i,t}^{(2)}$ for exploitation and exploration respectively, we now have a refined estimate of the diffusion probabilities between influencers and users, as well as the expected total spread (newly activated users), i.e., the reward.\n4.2.1 GNN for exploitation. In round t, for each arm $k_i$, with the pre-estimated exploitation graph $G_{i,t}^{(1)}$ for arm $k_i$ as input, we use a GNN model $f_i^{(1)} (k_i, C_t, G_{i,t}^{(1)}; P^{(1)})$ to estimate the reward de- scribed in Eq. (3), with $P^{(1)}$ representing the parameters of $f_i^{(1)}$.\nWe define for each arm a symmetric adjacency matrix $A_{i,t}^{(1)} \\in \\mathbb{R}^{m \\times m}$ from the exploitation graph $G_{i,t}^{(1)}$, with each element in the matrix corresponding to the correlations weights $w_{u,u'}$ between user u and user u' in $G_{i,t}^{(1)}$, and the normalized adjacency matrix [19] being $S_i = D^{-1/2}A_iD^{-1/2}$, with D the degree matrix. We concate- nate the arm feature vector $k_i$ with the context vector $C_t$ to build the feature matrix $X_{it} = diag([k_i, C_t], [k_i, C_t], ..., [k_i, C_t]) \\in \\mathbb{R}^{m \\times m(d_1+d_2)}$\nWe adopt a simplified Graph Convolutional Network (GCN) model [36] to learn the aggregated representation of the exploita- tion graph. With $S_i$ and $X_{it}^{(1)}$ as inputs, the feature representation matrix is expressed as\n$H_G = \\sigma(\\widetilde{A}X_{it}^{(1)}P^{(1)}),$                                                                                                                                                                                                                                                                                            (11)\nwhere \u03c3 is the activation function, $P^{(1)} \\in \\mathbb{R}^{m(d_1+d_2) \\times p}$ is the trainable weight matrix in the GCN model, and y is the number of hops the information propagating over the user graph, indicating that after k layers a node obtains the feature information from all nodes found y hops away in the graph. In the GCN model, $X_{it}^{(1)}$ is applied to the corresponding weight matrix $P^{(1)}$, so that $P^{(1)}$ is partitioned for each user u \u2208 U to get the p-dimensional arm-user diffusion representation, corresponding to each row of $H_G \\in \\mathbb{R}^{m \\times p}$.\nTo further refine the p-dimensional arm-user pair representation in $H_G$, we add an J-layer FC neural network to the GCN model, and for l \u2208 {1, 2, ..., J - 1} the representation for each layer is\n$H_l = \\sigma(H_{l-1}.P_l^{(1)}),$ and $H_0 = H_G,$                                                                                                                                                                                                                                                                               (12)\nwith $H_l \\in \\mathbb{R}^{m \\times p}$, and $P_l^{(1)}$ being the trainable parameters in each layer in the FC network. For the last layer, we have\n$\\widehat{p}_{i,t} = H_{J-1}P_J^{(1)},$                                                                                                                                                                                                                                                                                            (13)\nwhere $P_J^{(1)} \\in \\mathbb{R}^{p \\times m}$ are the parameters in the last layer, and $\\widehat{p}_{i,t} \\in \\mathbb{R}^{m}$ is the m-dimensional vector with each element the refined estimated diffusion probability $\\widehat{p}_{i,u} \\in \\mathbb{R}$ between arm $k_i$ and user $u \\in U$.\nWith the refined estimated diffusion probability between arm $k_i$ and all the users, the estimated reward for arm $k_i$ across all the"}]}