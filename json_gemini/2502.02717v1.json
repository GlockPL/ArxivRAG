{"title": "Astromer 2", "authors": ["Cristobal Donoso-Oliva", "Ignacio Becker", "Pavlos Protopapas", "Guillermo Cabrera-Vives", "Martina C\u00e1diz-Leyton", "Daniel Moreno-Cartagena"], "abstract": "Context. Foundational models have emerged as a powerful paradigm within the deep learning field. Their capacity relies on the ability\nto learn robust representations from large-scale datasets and generalize to diverse downstream applications, such as classification. In\nthis paper, we present Astromer 2, a foundational model designed for extracting light curve embeddings.\nAims. We introduce Astromer 2, an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the\nadvantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed\nempirical analysis of its capabilities, offering deeper insights into the model's representations.\nMethods. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning\ntask that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its\nperformance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on\nAstromer-generated embeddings.\nResults. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including\nlimited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate\nrepresentations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1\nscore on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance,\nespecially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.", "sections": [{"title": "1. Introduction", "content": "Light curve analysis is a cornerstone in astronomy for character-\nizing stellar objects (Deb & Singh 2009). By analyzing the time-\nseries data of luminosity variations, astronomers can extract sta-\ntistical features that enable classification and identification tasks\n(Richards et al. 2011).\nAlthough traditional methods show success\n(S\u00e1nchez-S\u00e1ez et al. 2021; Chaini et al. 2024), the advent\nof foundational models presents fresh opportunities to gain\ninsights into cosmic variability. Foundational models are deep\nneural networks trained using self-supervised techniques on\nextensive datasets (Bommasani et al. 2021; Awais et al. 2023).\nThese models acquire a thorough grasp of their domain, allow-\ning for the creation of versatile representations applicable to\nvarious downstream tasks.\nClassical techniques rely on manually engineered features\n(Debosscher et al. 2007; Nun et al. 2015), which may introduce\nbiases or fail to capture intricate patterns (Pantoja et al. 2022).\nFoundational models, by processing large volumes of data, have\nthe potential to reveal novel, precise structures in the data. How-\never, this gain in representational power comes at the expense of\nreduced interpretability.\nIn 2023, we introduced Astromer, a self-supervised model\ndesigned to extract general-purpose embeddings from light\ncurves (Donoso-Oliva, C. et al. 2023). Trained on 1.5 million\nlight curves, Astromer demonstrated consistent improvements\nin classification tasks compared to models trained directly on\nlabeled datasets.\nOther foundational models in astronomy, such as\nthose employing contrastive learning (Lanusse et al. 2023;\nRizhko & Bloom 2024; Parker et al. 2024), integrate multiple\ndata modalities to create richer and more complex represen-\ntations. While multi-modal learning is a promising avenue,\nit introduces additional complexity in model training and\ninterpretation (Wang et al. 2024).\nAstromer, by contrast, focuses solely on single-modality\nlight curve data, leveraging its temporal structure without requir-\ning alignment or integration steps across modalities. Instead of\ncontrastive learning, Astromer employs magnitude imputation to\nhandle missing values in time series, resulting in a simpler, yet\nhighly effective model that achieves state-of-the-art performance\nwithout incurring high computational costs.\nIn this paper, we present Astromer 2, an improved ver-\nsion of our original model. For consistency, we use the same\ndataset from our initial publication and compare our latest model\nin classification task. Additionally, we delve into the embed-"}, {"title": "2. Astromer 1", "content": "The initial version of Astromer (Donoso-Oliva, C. et al. 2023)\nadapted the BERT text model from natural language processing\n(Devlin et al. 2018). While both light curves and text are sequen-\ntial data, light curves pose a unique challenge due to the inherent\nirregularities in their sampling. Moreoever, instead of a discrete\nvocabulary, we work with continuous magnitudes for each token.\nDespite the differences between BERT and Astromer, the\nhigh-level approach to training remains similar, as it leverages\na self-supervised task. Specifically, we employ a masking strat-\negy that obscures portions of the light curve, allowing the model\nto predict the missing magnitudes. This technique, inspired by\nBERT's word masking in sentences, enables the model to learn\nmeaningful representations without relying on human-annotated\nlabels.\nThis section revisits the pipeline previously introduced in\nAstromer 1. While much of the content has been explained be-\nfore, we present it here with a more refined and clearer explana-\ntion for enhanced understanding."}, {"title": "2.1. Data preparation", "content": "Astromer uses single-band light curves ${x_i}$ with N as the\nnumber of samples. Each sample is represented as a set of tu-\nples $x_i = {(t_l, m_l, e_l)}_1^{L_i}$. Here, $t_l$ denotes the observation time\nin modified Julian date (MJD), $m_l$ represents the magnitude,\nand $e_l$ corresponds to the magnitude uncertainty. The maximum\nnumber of observations $L_i$ varies across samples, resulting in a\nvariable-length dataset.\nWe fixed a maximum length of 200 observations to create the\nnetwork's input. During pretraining, we sample different win-\ndows of 200 observations per epoch, allowing the model to see\nmost of the light curve sequence in small, fixed chunks. Shorter\nlight curves are zero-padded to a fixed length of 200 observa-\ntions.\nAfter constructing the windows, we normalize their values.\nSpecifically, we subtract $x_l = (t,m, \u0113)$ the mean value of each\nlight curve, producing zero-mean samples with non-scaled am-\nplitude. Our experiments have shown that this normalization step\nis essential for the model to converge effectively. Other options\nmay be insufficient to produce valuable embeddings."}, {"title": "2.2. Input Embedding", "content": "Unlike language models, Astromer does not have a fixed vocab-\nulary of tokens. Instead, the input consists of a sequence of con-\ntinuous magnitude values, each paired with its corresponding ob-\nservation time in MJD. We do not consider the uncertainties in\nAstromer's input.\nTo create a single input embedding, we transform each time\nand magnitude scalar into vectors. To encode observation times,\nwe apply an adapted positional encoder (PE) that scales the an-\ngular frequencies $w_j$ using the observation time $t_l$, capturing the irregular sampling in the temporal representation.\n$PE_{l,j} = \\begin{cases} \\sin (t_l \\cdot w_j) & j \\text{ is even} \\\\ \\cos (t_l \\cdot w_j) & j \\text{ is odd} \\end{cases}$ (1)\nIn Eq. 1, $j \\in [0, ..., d_{pe} \u2013 1]$, where $d_{pe} = 256$ is the PE dimen-\nsionality and $w_j$ is the angular frequency defined as,\n$w_j = \\frac{j}{\\sum_{l=0}^{L_i-1} t_l^2}$ (2)\nFor the magnitudes, we linearly project each magnitude\nvalue into a vector of size $d_{pe} = 256$. The weights used to trans-\nform magnitudes are initialized from a Normal distribution and\nsubsequently learned during training. This shared set of weights\nis applied across all observations.\nThe final input embedding $X \\in R^{L_i \\times 256}$ is the sum of the PE\nand transformed magnitudes, $X = PE + mW^T$, where $m \\in R^{L_i \\times 1}$\nis the magnitudes vector and $W \\in R^{256 \\times 1}$ are the weights for\ntransforming scalars into vectors."}, {"title": "2.3. Probed and Masking", "content": "The key to Astromer learning a good representation is to pre-\ntrain it to predict unseen observations along the light curve se-\nquence. The probed subset consists of magnitudes designated for\nthe model to predict. However, these values are excluded when\ncalculating attention weights. We randomly select 50% of the\ntotal observations per window to constitute the probed subset.\nThis subset is denoted by a binary mask vector, where the 1's\ncorrespond to the probed magnitudes, and zero otherwise.\nIn the self-attention mechanism, the attention weights for the\nprobed subset are set to zero using masking. This design encour-\nages the model to leverage the surrounding context to predict the\nprobed magnitudes. During inference, however, masking is not\napplied. To prevent the model from over-relying on the masked\nobservations during training, we adopt a the following strategy.\nWe assign 10% of visible observations and 10% of random ob-\nservations in the probed subset. As a result, the actual masked\nportion is reduced to 30%, while the probed subset still corre-\nsponds to the initial 50%. This approach mitigates the risk of\nthe model learning a direct identity mapping and improves its\nrobustness to noise."}, {"title": "2.4. Encoder", "content": "The encoder comprises a sequence of attention blocks connected\nin series. The first block processes the input embeddings de-\nscribed in Sect. 2.2 and a binary mask matrix that specifies which\nobservations to exclude from the attention mechanism. Subse-\nquent blocks take as input the output of the preceding attention\nblock as shown in Fig. 2."}, {"title": "2.5. Pretraining Task", "content": "We pretrain the model to predict the magnitudes of the probed\nsubset in each input sequence. This is achieved by passing the\noutput embedding from the last attention block, through a fully\nconnected network with no hidden layers or activation. The re-\nsult is a vector of estimated magnitudes, $x \u2208 R^{200 \\times 1}$, , providing\nthe reconstruction for each time point according to its related\nembedding.\nWe constraint the loss function to compute the root-mean-\nsquare error on the probed subset only:\n$Loss = \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{l=0}^{L_i-1} m_{il}(x_{il} - \\hat{x_{il}})^2$. (4)\nIn Eq. 4, N represents the number of training samples, and L =\n200 represents the length of the windows. Thus, the masking\nvector $m_i$ selectively includes errors from the probed subset."}, {"title": "3. Astromer 2", "content": "Astromer 2 incorporates features that were not included in the\ninitial version due to resource and time constraints. While As-\ntromer 1 served as a proof of concept for generating effective\nembeddings, Astromer 2 builds on this foundation, introducing\niterative enhancements to optimize performance at each stage.\nBuilding upon the foundation of Astromer 1 discussed in\nSect. 2, this section is dedicated solely to the enhancements that\ndistinguish the principal features of Astromer 2."}, {"title": "3.1. Input embedding", "content": "The process for creating the input embedding for Astromer 2\nremains the same as in the initial version. However, we replace\nthe magnitudes targeted for masking with a trainable token that\nis zero-initialized and shared across all samples.\nWhile the contribution of masked tokens is zero after the at-\ntention weight calculation, adding a mask token to replace the\nactual magnitude allows the model to recognize which tokens\nare masked, which can be helpful during training. We also avoid\npotential information leaks that could arise from the all-to-all\ncomputation within the similarity matrix."}, {"title": "3.2. Encoder", "content": "The encoder of Astromer 2 has a significantly larger number\nof parameters, increasing from 661505 to 5432129. An eight\nfold increase. This growth is due to the inclusion of six atten-\ntion blocks, with each block containing four heads and 64 units.\nAdditionally, we have incorporated a dropout layer after the self-attention calculation"}, {"title": "3.3. Pretraining task", "content": "Like Astromer 1, we use the root-mean-square error as the loss\nfunction. In Astromer 2, however, the losses are scaled based on\nobservational uncertainties. These uncertainties are normalized\nto a range of 0 to 1, and their reciprocals are used as weights. In-\ncorporating this scaling term into the error calculation enhances"}, {"title": "4. Data Sources", "content": "In this section, we introduce our training data, including unla-\nbeled light curves for pretraining and labeled samples for the\ndownstream classification task."}, {"title": "4.1. Unlabeled data - MACHO", "content": "The MACHO project (Alcock et al. 1993) aimed to detect Mas-\nsive Compact Halo Objects (MACHO) to find evidence of dark\nmatter in the Milky Way halo by searching for gravitational\nmicrolensing events. Light curves were collected from 1992 to\n1999, producing light curves of more than a thousand observa-\ntions (Alcock et al. 1999) in bands B and R. The observed sky\nwas subdivided into 403 fields. Each field was constructed by ob-\nserving a region of the sky or tile. The resulting data is available"}, {"title": "4.2. Labeled data", "content": "To ensure a fair comparison with Astromer 1, we used the same\nsample selection from the MACHO (hereafter referred to as Al-\ncock; Alcock et al. 2003) and the Asteroid Terrestrial-impact\nLast Alert System (hereafter referred to as ATLAS; Heinze et al\n2018) labeled catalogs. The former has a similar magnitude dis-\ntribution, whereas the latter differs, as shown in Fig. 4."}, {"title": "4.2.1. Alcock", "content": "For labeled data, we use the catalog of variable stars from\nAlcock et al. (2003), which contains labels for a subset of the\nMACHO light curves originating from 30 fields from the Large\nMagellanic Cloud. This labeled data will be used to train and\nevaluate the performance of the different embeddings on the\nclassification task."}, {"title": "4.3. ATLAS", "content": "The Asteroid Terrestrial-impact Last Alert System (ATLAS;\nTonry et al. 2018) is a survey developed by the University of\nHawaii and funded by NASA. Operating since 2015, ATLAS\nhas a global network telescopes, primarily focused on detecting\nasteroids and comets that could potentially threaten Earth. Ob-\nserving in c (blue), o (orange), and t (red) filters.\nThe variable star dataset used in this work was presented by\nHienze et al. (2018) and includes 4.7 million candidate variable\nobjects, included in the labeled and unclassified objects, as well\nas a dubious class. According to their estimates, this class is pre-"}, {"title": "4.4. MACHO vs ATLAS", "content": "Figures 4 and 5 illustrate the distributional differences between\nthe unlabeled MACHO dataset and the labeled subsets discussed\nearlier. While the magnitudes show a notable shift between MA-\nCHO and ATLAS, our training strategy normalizes the light\ncurves to a zero mean. As a result, the relationships between ob-\nservations take precedence over the raw magnitude values. Con-\nsequently, we do not expect a substantial performance drop when\ntransitioning between datasets. However, for At, the smaller val-\nues of At present a significant challenge, as the model must ex-\ntrapolate and account for fast variations to capture short-time in-\nformation effectively. We evidence this in our first results from\nAstromer 2, where the F1 score on the ATLAS dataset was lower\ncompared to MACHO when having fewer labels for classifica-\ntion."}, {"title": "5. Results", "content": "The pretraining task of reconstructing the probed magnitudes is\nan essential component that allows the model to learn meaning-\nful representations. Reconstructing probed magnitudes can be\nevaluated as a downstream regression task on labeled datasets,\nwhere the model's ability to predict the magnitudes can be as-\nsessed. Here we evaluate the potential of the representation in\nterms of regression.\nshows the learning curves from the pretraining of\nAstromer 2. The training took approximately 3 days using 4\nA5000 GPUs. The model achieved 0.73 $R^2$ with a root mean\nsquared error (RMSE) of 0.113 on the probed subset. Astromer\n1 had an RMSE of 0.148, making Astromer 2 0.035 better in\nterms of reconstruction error."}, {"title": "5.1. Downstream setup", "content": "Similar to Astromer 1, this work evaluates the embeddings\nacross various scenarios, while controlling the number of sam-"}, {"title": "5.2. Finetuning", "content": "presents the RMSE results for each scenario. The re-\nported values are calculated across the total number of obser-\nvations without masking, allowing for a fair comparison as the\nerror could be biased by the random masking selection. As\nshown in 7, finetuning the model on the Alcock dataset does\nnot result in significant improvements, indicating that the pre-\ntrained model already captures most of the relevant information,\ndespite the out-of-distribution modality discussed in Sect. 4.4.\nIn contrast, finetuning on ATLAS leads to a notable improve-\nment. Specifically, with 100 SPC, we observe a 23% reduction\nin RMSE compared to the pretrained model. However, the per-\nformance improvement between 100 and 500 SPC is minimal,\nwith only small variations.\nThe most computationally intensive scenario takes approx-\nimately three minutes to finetune, which is significantly faster\nthan the days required for pretraining. While the time for MA-"}, {"title": "5.3. Visualizing reconstruction", "content": "To gain insight into Astromer's representations, we visualize the\nattention values after finetuning. Figures 8 presents two examples\nshowing the mean attention weights from each attention head,\nalong with the mean across all attention heads. For visualization\npurposes, light curves associated with the average between heads\nwere folded; however, Astromer does not receive folded inputs\nduring processing. We display only the first attention block, as it\nis more intuitive. This contrasts with intermediate layers, where\nattention is computed over abstract embeddings. Figures 8 and\n9 show that each attention head focuses on different parts of the\nsequence. In particular, attention appears to focus most strongly\nat maximum and minimum brightness points suggesting these\nare key features for reconstruction."}, {"title": "5.4. Classification", "content": "Evaluating classification performance is crucial for assessing\nthe overall effectiveness of Astromer, as it serves as a common\nbenchmark for evaluating the quality of embeddings. After fine-\ntuning on labeled subsets of 20, 100, and 500 SPC, the encoder\nis frozen, meaning its weights are no longer updated\nAstromer is used to extract the representation, which is fed to\nanother classifier model. The same labeled data is used to train a\nclassifier. In this setup, only the classifier section receives label-"}, {"title": "6. Conclusion", "content": "This paper presents the updated version of Astromer, a self-supervised model designed to extract general-purpose embed-dings from light curve data. We demonstrate that Astromer 2outperforms its predecessor, Astromer 1, across multiple scenar-ios, including both the Alcock and ATLAS datasets. The keyimprovements in classification performance, especially when"}, {"title": "7. Ethical and Practical Considerations", "content": "Large-scale models require significant computational re-sources, highlighting the need for optimization. According toLannelongue et al. (2021), training Astromer 2 resulted in 32.29kg of $CO_2$ emissions, which is equivalent to a 195.96 km trip ina passenger car.\nTo mitigate this environmental impact, we provide pre-trained weights in our repository, enabling users to build uponour model without the need for full retraining. If you have newdata, we encourage you to share your pre-trained models as apull request in our repository.\nCode, weights, and data can be found in our official or-ganization: https://github.com/astromer-science. Ad-ditionally, we provide a website with user practical informa-tion https://www.stellardnn.org/projects/astromer/index.html."}]}