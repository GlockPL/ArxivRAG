{"title": "STRENGTH ESTIMATION AND HUMAN-LIKE STRENGTH\nADJUSTMENT IN GAMES", "authors": ["Chun-Jung Chen", "Chung-Chin Shih", "Ti-Rong Wu"], "abstract": "Strength estimation and adjustment are crucial in designing human-AI interactions,\nparticularly in games where AI surpasses human players. This paper introduces a\nnovel strength system, including a strength estimator (SE) and an SE-based Monte\nCarlo tree search, denoted as SE-MCTS, which predicts strengths from games\nand offers different playing strengths with human styles. The strength estimator\ncalculates strength scores and predicts ranks from games without direct human\ninteraction. SE-MCTS utilizes the strength scores in a Monte Carlo tree search to\nadjust playing strength and style. We first conduct experiments in Go, a challenging\nboard game with a wide range of ranks. Our strength estimator significantly\nachieves over 80% accuracy in predicting ranks by observing 15 games only,\nwhereas the previous method reached 49% accuracy for 100 games. For strength\nadjustment, SE-MCTS successfully adjusts to designated ranks while achieving\na 51.33% accuracy in aligning to human actions, outperforming a previous state-\nof-the-art, with only 42.56% accuracy. To demonstrate the generality of our\nstrength system, we further apply SE and SE-MCTS to chess and obtain consistent\nresults. These results show a promising approach to strength estimation and\nadjustment, enhancing human-AI interactions in games. Our code is available at\nhttps://rlg.iis.sinica.edu.tw/papers/strength-estimator.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence has achieved superhuman performance in various domains in recent years,\nespecially in games (Silver et al., 2018; Schrittwieser et al., 2020; Vinyals et al., 2019; OpenAI et al.,\n2019). These achievements have raised interests within the community in exploring AI programs for\nhuman interactions, particularly in estimating human players' strengths and offering corresponding\nlevels to increase entertainment or improve skills (Demediuk et al., 2017; Fan et al., 2019; Moon &\nSeo, 2020; Gusm\u00e3o et al., 2015; Silva et al., 2015; Hunicke & Chapman, 2004). For example, since\nthe advent of AlphaZero, human players have attempted to train themselves by using AI programs.\nSubsequently, many researchers have explored several methods to adjust the playing strength of\nAlphaZero-like programs to provide appropriate difficulty levels for human players (Wu et al., 2019;\nLiu et al., 2020; Fujita, 2022).\nHowever, although these methods can provide strength adjustment, two issues have arisen. First,\nwhile these methods can offer different strengths, human players often need to play several games\nor manually choose AI playing strength, consuming time to find a suitable strength for themselves.\nSecond, the behaviors between AI programs and human players are quite different. This occurs\nbecause most strength adjustment methods mainly focus on adjusting AI strength by calibrating the\nwin rate to around 50% for specific strengths, without considering the human behaviors at those\nstrengths. The problem is further exacerbated when human players attempt to use AI programs\nto analyze games and learn from the better actions suggested by AI. Therefore, designing AI\nprograms that can accurately estimate a player's strength, provide corresponding playing strengths,\nand simultaneously offer human-like behavior is crucial for using superhuman AI in human learning."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 BRADLEY-TERRY MODEL", "content": "The Bradley-Terry model (Bradley & Terry, 1952) is often used for pairwise comparisons, allowing\nfor the estimation of outcomes between individuals based on their relative strengths. In a group of\nindividuals, the model calculates the probability that individual i defeats individual j as $P(i > j) =\\frac{\\lambda_i}{\\lambda_i + \\lambda_j}$, where i and $\\lambda_j$ represent the positive values of individuals i and j, respectively. A higher $\\lambda_i$\nindicates a stronger individual. In practice, $\\lambda_i$ is usually defined by an exponential score function as\n$\\lambda_i = e^{\\beta_i}$, where $\\beta_i$ represents the strength score of individual i.\n$\\frac{\\lambda_i}{\\lambda_i + \\lambda_2 + ... + \\lambda_k}$ .Furthermore, the model can be adapted for team comparisons, where each\nteam comprises multiple individuals. For example, assume team a consists of individuals 1 and 2,\nteam b consists of individuals 2, 3, and 4, and team c consists of individuals 1, 3, and 5. Then, the\nprobability that team a win against team b and c is defined as $P(team a) = \\frac{\\lambda_1\\lambda_2}{\\lambda_1\\lambda_2 + \\lambda_2\\lambda_3\\lambda_4 + \\lambda_1\\lambda_3\\lambda_5}$,\nwhere the strength of each team is determined by the product of the strengths of its individual members.\nDue to its generalization and broader extension, the Bradley-Terry model has been widely used in\nvarious fields, such as games (Coulom, 2007a), sports (Cattelan et al., 2013), and recommendation\nsystems (Chen & Joachims, 2016)."}, {"title": "2.2 MONTE-CARLO TREE SEARCH", "content": "Monte Carlo tree search (MCTS) (Coulom, 2007b; Kocsis & Szepesv\u00e1ri, 2006) is a best-first search al-\ngorithm that has been successfully used by AlphaZero (Silver et al., 2018) and MuZero (Schrittwieser\net al., 2020) to master both board games and Atari games. In AlphaZero, each MCTS simulation\nbegins by traversing the tree from the root node to a leaf node using the PUCT (Rosin, 2011) formula:\n$a^* = arg\\underset{a}{max} \\{ Q(s, a) + c \\cdot P(s,a) \\cdot \\sqrt{\\frac{N(s,b)}{1 + N(s,a)}} \\}$,\nwhere Q(s, a) represents the estimated Q-value for the state-action pair (s, a), N(s, a) is the visit\ncounts, P(s, a) is the prior heuristic value, and c is a coefficient to control the exploration. Next, the\nleaf node is expanded and evaluated by a two-head network, fe(s) = (p, v), where p represents the\npolicy distribution and v denotes the win rate. The policy distribution p serves as the prior heuristic"}, {"title": "2.3 MCTS-BASED STRENGTH ADJUSTMENT", "content": "Strength adjustment (Hunicke & Chapman, 2004; Paulsen & F\u00fcrnkranz, 2010; Silva et al., 2015;\nMoon & Seo, 2020) is crucial in the design of human-AI interactions, especially since AlphaZero\nachieved superhuman performance in many games like Go, Chess, and Shogi. As MCTS is widely\nused in these games, various methods have been explored to adapt it for strength adjustment (Sephton\net al., 2015; Wu et al., 2019; Demediuk et al., 2017; Fan et al., 2019; Moon et al., 2022). For instance,\nSephton et al. (2015) proposes adjusting the playing strength by using a strength index z. After the\nsearch, MCTS decides the node based on the proportionality of their simulation counts, with the\nprobability of selecting node i calculated as,$\\frac{N_i}{\\sum_i N_i} z$, where $N_i$ represents the simulation counts\nfor node i. A larger z value indicates a tendency to select stronger actions, while a smaller z favors\nweaker actions. Wu et al. (2019) further improves this method by introducing a threshold R to filter\nout lower-quality actions, removing nodes j where $N_j < R \\times N_{max}$, where $N_{max}$ represents the\nnode with largest simulation counts. The approach is used to adjust the playing strength of ELF\nOpenGo (Tian et al., 2019), resulting in covering a range of 800 Elo ratings within the interval\nz \u2208 [-2, 2]. However, both methods only change the final decision in MCTS without modifying the\nsearch tree, leaving the search trees identical for different strengths."}, {"title": "2.4 STRENGTH ESTIMATION", "content": "Strength estimation is another important technique related to strength adjustment. With accurate\nstrength estimation, the AI can first predict a player's strength and subsequently provide an appropriate\nlevel of difficulty for human learning. Several methods (Moud\u0159\u00edk & Neruda, 2016; Liu et al., 2020;\nEgri-Nagy & Tormanen, 2020; Scheible & Sch\u00fctze, 2014) have been proposed to estimate player\nstrength in games. For example, Liu et al. (2020) proposes estimating a player's strength by using the\nstrength index with MCTS, as described in the previous subsection, to play against human players.\nSpecifically, the strength index z is adjusted after each game according to the game outcomes. Their\nexperiments show that z generally converges after about 20 games. However, this method requires\nhuman players to play against the MCTS programs with multiple games to obtain an estimation\nof their playing strengths. On the other hand, Moud\u0159\u00edk & Neruda (2016) proposes an alternative\napproach that categorizes the Go players into three ranks \u2013 strong, median, and weak \u2013 and uses a\nneural network to classify player ranks based on a game position using supervised learning. After\ntraining, given a game position, the neural network predicts ranks for each position by selecting\nthe highest probability. Furthermore, it can aggregate predictions across multiple positions. Two\nmethods are presented: (a) sum, which sums probabilities of all positions and makes a prediction\nbased on the highest probability; and (b) vote, which predicts the rank of each position first and\nselects the most frequent rank. However, this approach does not consider multiple actions during\ntraining and the experiment was limited to only three ranks. In addition, strength estimation can be\nformulated as a ranking problem (Burges et al., 2005; Xia et al., 2008), but it differs in a key aspect.\nRanking problems often focus on ordering items based on a single query, whereas in games, strength\nis assessed as overall skills across multiple positions or games. This challenge requires aggregating\nrankings across various scenarios to capture a player's ability."}, {"title": "3 \u041c\u0415\u0422\u041dOD", "content": ""}, {"title": "3.1 STRENGTH ESTIMATOR", "content": "We introduce the strength estimator (SE), which is designed to predict the strength of an action a\nat a given state s based on human game records. Each state-action pair, denoted as p = (s, a), is\nlabeled with a rank r that corresponds to the player's strength. For simplicity, in this paper, ranks are\nordered in descending order where rank 1, denoted as $r_1$, represents the strongest level of play, and\nprogressively higher numbers indicate weaker playing strength. Each rank corresponds to a group\nof players, as we assume that players with the same rank have equivalent strength. Ranks could be"}, {"title": "3.2 TRAINING THE STRENGTH ESTIMATOR", "content": "This subsection introduces a methodology for training strength estimator. For simplicity, we propose\nto train a neural network, $f_\\theta(p) = \\beta$, as a strength estimator which predicts a strength score \u03b2 instead\nof strength \u03bb for a given state-action pair p. This strength score, \u03b2, serves as the exponent for strength\n$\u03bb = e^\u03b2$, as defined by the Bradley-Terry model. Then, the composite strength, $\u039b_i$, from the equation\n2 can be expressed by using \u03b2 as follows:\n$\u039b_i = (\\prod_{j=1}^{m} \\lambda_{i,j})^{\\frac{1}{m}} = (\\prod_{j=1}^{m} e^{\\beta_{i,j}})^{\\frac{1}{m}} = e^{\\frac{1}{m} \\sum_{j=1}^{m} \\beta_{i,j}} = e^{\\overline{\\beta_i}}$,\nwhere $\\overline{\\beta_i}$ represents the average strength scores of m state-action pairs, each with $r_i$, sampled from\nD.\nNext, given n ranks in the game collection, the strength estimator is optimized by maximizing the\nlikelihood L according to the ranking order (Xia et al., 2008; Chen et al., 2009). The likelihood is"}, {"title": "3.3 STRENGTH ESTIMATOR BASED MCTS FOR STRENGTH ADJUSTMENT", "content": "We present a novel method that integrates a strength estimator with MCTS to adjust strength dynami-\ncally, named SE-MCTS. In previous strength adjustment approaches, as described in subsection 2.3,\nthe MCTS search tree is unmodified during the search, with only changing the final action decision\nafter the search is complete. In contrast, we propose inherently modifying the search based on a\ntarget strength score to ensure that the search aligns more closely with the desired strength of ranks.\nSpecifically, in MCTS each node is evaluated by the strength estimator to obtain a strength score,\n\u03b2(s, a), which represents the strength score of action a at state s. We can calculate the composite\nstrength score $B(s, a)$, by averaging all \u03b2 from the nodes within the subtree of state s. This is similar\nto the method used to calculate estimated Q-values. Given a targeted rank r with strength score $\u03b2_t$, we\ncalculate the absolute strength difference for each node, which is denoted as $d(s, a) = |\u03b2(s,a) \u2013 \u03b2_t|$."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENT SETUP", "content": "We conducted experiments using the MiniZero framework (Wu et al., 2024). The human games are\ncollected\u00b9 from FoxWeiqi (YeHu, 2024), which is the largest online Go platform in terms of users.\nThese games are collected from amateur 5 kyu to 9 dan\u00b2, and are ranked in order from the strongest\nto weakest as follows: 9 dan, 8 dan, ..., 2 dan, 1 dan, 1-2 kyu, and 3-5 kyu, corresponding to $r_1, r_2, ...,$\nr_{11}. Namely, a total of n = 11 ranks are used. Note that for kyu, we classify 1 to 2 kyu as one rank\nand 3 to 5 kyu as another rank, and we exclude games played by players ranked lower than 5 kyu.\nThis is because kyu players are still mastering basic Go strategies, their ranks often change rapidly.\nConsequently, their games do not consistently correspond to their ranks. For the training dataset,\nwe collect a total of 495,000 games, with 45,000 games from each rank. We also prepare a separate\ntesting dataset, including a candidate and a query dataset. The candidate dataset is used to estimate\nan average strength score of each rank, including a total of 1,100 games, with 100 games per rank.\nThe query dataset is used for the strength estimator to predict the strength, containing a total of 9,900\ngames, with 900 games per rank.\nThe network architecture of the strength estimator is similar to the AlphaZero network, consisting\nof 20 residual blocks with 256 channels. Given a state-action pair, the network outputs a policy\ndistribution p, a value v, and a strength score \u03b2. The training loss for the policy and value network\nfollows AlphaZero, while the loss for the strength estimator is defined by equation 5. During training,\nwe aggregate the composite strength score $\u03b2_i$ by randomly selecting m = 7 state-action pairs from $r_i$.\nOther training details are provided in the appendix."}, {"title": "4.2 PREDICTING RANKS FROM GAMES", "content": "The strength estimator can be utilized to predict ranks in games where the rank is unknown. We\nfirst calculate $\\overline{\\beta_i}$ for each $r_i$ by evaluating all games in the candidate dataset. Next, for games from\nthe same unknown rank, $r_u$, in the query dataset, a composite score $\\overline{\\beta_u}$ is calculated by the strength\nestimator. Finally, $r_u$ is then determined to be $r_i$, where $|\\overline{\\beta_u} \u2013 \\overline{\\beta_i} |$ is the smallest among all $r_i$.\nWe train two strength estimator networks, SE and $SE_\\infty$, where SE is trained with 11 ranks, and $SE_\\infty$\nincludes an additional rank, $r_\\infty$, for a total of 12 ranks. In addition, for comparison, we train another"}, {"title": "4.3 ADJUSTING STRENGTH WITH STRENGTH ESTIMATOR", "content": "In this section, we evaluate the performance of SE-MCTS, as described in subsection 3.3, by\nincorporating the two trained strength estimator networks into MCTS to adjust the playing strength\nfor game playing. We first calculate the composite strength score $\\overline{\\beta_i}$, by averaging all \u03b2 from the\nstate-action pairs in $r_i$ from the candidate set. Although we assume that the strength score $\\beta_i$ for any\nstate-action pair from $r_i$ should be similar, in practice, we observe that $\u03b2_i$ may vary across different"}, {"title": "4.4 TRAINING STRENGTH ESTIMATOR WITH LIMITED DATA", "content": "In this subsection, we investigate training a strength estimator with limited data. Unlike the supervised\nlearning methods, $SL_{sum}$ and $SL_{vote}$, which require data from each rank, our method can estimate a\nstrength score and use it to predict ranks that were not observed during training. In niche games or\nthose favored by a specific group of enthusiasts, ranking systems are often not fully established due\nto a limited number of game records, and some specific ranks may be sparsely populated with only a\nfew players. Therefore, it is intriguing to explore whether the strength estimator can generalize to\nthese unseen strengths."}, {"title": "4.5 GENERALIZING TO OTHER GAMES", "content": "We further experiment in another game, chess, to demonstrate the generality of our SE and SE-MCTS\napproaches. Similar to Go, chess is also a popular game with abundant human game records. The\ngames were collected from Lichess\u00b3 (Lichess, 2024), which uses Elo ratings as its ranking system.\nWe collect games with Elo ratings ranging from 1,000 to 2,600 and categorize them into eight ranks,\nwith each rank covering 200 Elo points and 240,000 games, for a total of 1,920,000 games. For the\ntesting dataset, the candidate dataset consists of 960 games, with 120 games per rank, while the query\ndataset contains 9,600 games, with 1,200 games per rank. Then, we apply experiments to chess,"}, {"title": "5 DISCUSSION", "content": "This paper introduces a novel strength system, including a strength estimator for evaluating the\nstrength from game records without requiring direct interaction with human players, and an SE-MCTS\nfor adjusting the playing strength using strength scores provided by the strength estimator. When\npredicting ranks in the game of Go, our strength estimator significantly achieves over 80% accuracy\nby examining only 15 games, whereas the previous supervised learning method only reached 49%\naccuracy even after evaluating 100 games. The strength estimator can be trained with limited rank\ndata and still accurately predict unseen rank data, providing extensive generalizability. For strength\nadjustment, SE-MCTS successfully adjusts to designated ranks while providing a playing style that\naligns with human behavior, achieving an average accuracy of 51.33%, compared to the previous\nstate-of-the-art method that only reached 42.56% accuracy. Furthermore, we apply our method to the\ngame of chess and obtain consistent results to Go, demonstrating the generality of our approach.\nOne limitation of our work is that the strength estimator relies on human game records for training.\nHowever, this issue could potentially be addressed by using all models trained by AlphaZero, which\nmay serve as players of different playing strengths to generate games. Besides, the strength system\nalso provides several benefits for future directions. For example, game designers can use the strength\nestimator to evaluate their ranking systems. The strength estimator can evaluate a game by examining\nthe strength scores for each action, and use it to identify incorrect actions for human players or\nfor cheat detection (Alayed et al., 2013). Furthermore, we can extend our strength estimator by\nincorporating opponent-specific strength scores to address the Bradley-Terry model's limitations in\ncapturing intransitivity (Balduzzi et al., 2018; Bertrand et al., 2023; Omidshafiei et al., 2019; Vadori\n& Savani, 2024). Finally, the search tree of SE-MCTS can offer the opportunity for explainability of\nAI actions in human learning."}, {"title": "ETHICS STATEMENT", "content": "This paper presents a method for estimating player strength and adjusting it to specific ranks, allowing\nagents to play in a human-like manner. A potential ethical concern is that our method could be\nexploited by human players to develop human-like agents for cheating in human competitions.\nHowever, we emphasize that all models trained in this paper are used strictly for research purposes\nand adhere to established ethical guidelines."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have provided detailed descriptions of the method, implementation, and training hyperpa-\nrameters in Section 3, Section 4, and Appendix A to facilitate the reproduction of our exper-\niments. The source code, along with a README file containing instructions is available at\nhttps://rlg.iis.sinica.edu.tw/papers/strength-estimator."}, {"title": "ACKNOWLEDGEMENT", "content": "This research is partially supported by the National Science and Technology Council (NSTC) of\nthe Republic of China (Taiwan) under Grant Number NSTC 113-2221-E-001-009-MY3, NSTC\n113-2634-F-A49-004, and NSTC 113-2221-E-A49-127."}, {"title": "A DETAILED TRAINING SETTINGS FOR THE STRENGTH ESTIMATOR", "content": "The feature design in the strength estimator of Go is similar to AlphaZero (Silver et al., 2018).\nSpecifically, we use 18 channels to represent a board position, where the first 16 channels are the\nboard configurations from the past eight moves for both black and white stones. The remaining\ntwo channels are binary indicators of the color of the next player, i.e., one channel for Black and\nWhite. For chess, the feature design also follows the same approach as AlphaZero, which includes\n119 input channels. During training, for each rank, we randomly select seven state-action pairs. We\nalso perform data augmentation to further enhance the diversity of the training data. The network\nis optimized using stochastic gradient descent (SGD), with the loss function specified in Equation\n5. It is important to note that when training $SE_\\infty$, the policy and value loss for state-action pairs of\n$r_\\infty$ are not calculated, since these heads should only consider actual human players' actions. The\nlearning rate is initially set at 0.01 and is halved after 100,000 training steps. The entire training\nprocess encompasses 130,000 steps, consuming around 242 GPU hours for Go and 69 GPU hours for\nchess on an NVIDIA RTX A5000 graphics card. Other hyperparameters are listed in Table 3."}, {"title": "B IN-DEPTH ANALYSIS FOR STRENGTH ESTIMATOR IN GO", "content": "We conduct in-depth analyses for strength estimators in Go. First, we present detailed insights into\npredicting ranks from games, as detailed in Subsection B.1. Second, we demonstrate the outcomes of\nstrength prediction using fewer moves in a single game, discussed in Subsection B.2. Finally, we\nexplore predictions based solely on the first 50 actions or the last 50 actions in games, which are\nelaborated in Subsection B.3 and Subsection B.4, respectively."}, {"title": "B.1 PREDICTING RANKS FROM GAMES", "content": "Figure 8 shows the accuracy of rank predictions for different networks. We observe that in Figure\n8a and Figure 8b, $SL_{vote}$ and $SL_{sum}$ can only distinguish on some ranks, such as 3-5 kyu. This is\nbecause these models do not contain sufficient information to differentiate all ranks based on a single\nstate-action during training (Moud\u0159\u00edk & Neruda, 2016). In Figure 8c and Figure 8d, even though\nwe incorporated a prediction tolerance for these two methods, they still cannot perfectly distinguish\nall ranks, even after 100 games. In Figure 8e and Figure 8f, although our models SE and $SE_\\infty$\ncannot perfectly predict all ranks without incorporating a prediction tolerance, they still achieve high\nperformance across all ranks. In Figure 8g and Figure 8h, when we allow a prediction tolerance, we\nachieve 100% accuracy across all ranks. This result further indicates that our model can differentiate\nthe strength relationship across all ranks."}, {"title": "B.2 PREDICTING RANKS FROM GAME POSITIONS", "content": "We are also interested in whether we can predict the rank using only game positions. Specifically,\nonly one game position can be chosen for each game instead of all actions when predicting the"}, {"title": "B.3 PREDICTING RANKS FROM THE FIRST 50 ACTIONS IN THE GAME", "content": "We are interested in evaluating performance when using only the first 50 actions of a game, known as\nthe fuseki stage in Go. Figures 10a and 10b indicate that $SL_{vote}$ and $SL_{sum}$, utilizing these initial\nactions, achieve similar performance to predictions made using all actions in the game. Figures\n10c and 10d present the prediction results by our methods when limited to the first 50 actions."}, {"title": "B.4 PREDICTING RANKS FROM THE LAST 50 ACTIONS IN THE GAME", "content": "Similarly, we examine the performance when using only the last 50 actions of the game, referred to as\nthe yose stage in Go. Figures 10e and 10f show that $SL_{vote}$ and $SL_{sum}$, employing these final actions,\nmaintain similar performance to predictions based on all actions in the game. In our method, Figures\n10g and 10h display the prediction results using only the last 50 actions. As before, the overall\naccuracy has declined. However, the accuracy for 9 dan players between predictions made using the\nentire game and just the last 50 actions does not differ significantly. This is likely because the yose\nstage involves complex calculations and judgments, areas where top players excel. Furthermore, in\nmost games, especially those between the highest-skilled players, the outcome is often determined\nbefore the yose stage. This leads to less practice and proficiency in this phase among players of lower\nranks. Additionally, we observe a significant drop in accuracy for 8 dan players when predictions are\nbased solely on yose stage. This could be because some 8 dan players have comparable yose skills to\nthose of 9 dan players, leading to some misclassifications of 8 dan players as 9 dan."}, {"title": "C DETAILED EXPERIMENTS FOR STRENGTH ADJUSTMENT", "content": "Table 4 presents the value of z for SA-MCTS\u00bf used in subsection 4.3. To ensure that each SA-MCTSi\nand SE-MCTS\u00bf achieve a comparable win rate, all methods are tested using a fixed simulation\ncount of 800. For SA-MCTS, since the strength index z does not directly correspond to any specific\nrank, we adjust z for each ri to ensure that each SA-MCTSi and SE-MCTSi achieve a comparable\nwin rate. As shown in Table 4, z gradually decreases from r1 to r11, aligning with the results in the\noriginal paper, which indicate that a greater z corresponds to a higher strength."}, {"title": "C.1 ADJUSTING STRENGTH WITH DIFFERENT BASELINES", "content": "In Figure 4, the baseline program is chosen as SE-MCTSi with i = 5 (5 dan). It would be interesting\nto examine whether the relative strength remains consistent when different baseline models are used.\nTo further investigate this, we conduct a round-robin tournament by selecting five ranks ($r_2, r_4, r_6$,\n$r_8$ and $r_{10}$) and two representative methods (SA-MCTS and SE-$^{\\infty}$MCTS, excluding SE-MCTS due\nto its ineffective strength adjustment. Each combination involves 250 games, requiring approximately\n100 GPU hours on an NVIDIA RTX A5000. Table 5 summarizes the results, with the win rates\nin each cell representing the performance of the y-axis player against the x-axis player. Moreover,\nwe compute the Elo rating of each model using this table. We initialize the rating at 1500 for each\nmodel and iteratively update the ratings to match the expected win rates with the observed pairwise\noutcomes. The rightmost column of Table 5 presents the resulting Elo ratings. In summary, the Elo\nratings confirm that higher-ranked models consistently achieve higher ratings, demonstrating the\nrobustness of our method across different baselines."}]}