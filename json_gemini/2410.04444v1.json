{"title": "G\u00d6DEL AGENT: A SELF-REFERENTIAL FRAMEWORK\nFOR AGENTS RECURSIVELY SELF-IMPROVEMENT", "authors": ["Xunjian Yin", "Xinyi Wang", "Liangming Pan", "Xiaojun Wan", "William Yang Wang"], "abstract": "The rapid advancement of large language models (LLMs) has significantly en-\nhanced the capabilities of AI-driven agents across various tasks. However, exist-\ning agentic systems, whether based on fixed pipeline algorithms or pre-defined\nmeta-learning frameworks, cannot search the whole agent design space due to the\nrestriction of human-designed components, and thus might miss the globally opti-\nmal agent design. In this paper, we introduce G\u00f6del Agent, a self-evolving frame-\nwork inspired by the G\u00f6del machine, enabling agents to recursively improve them-\nselves without relying on predefined routines or fixed optimization algorithms.\nG\u00f6del Agent leverages LLMs to dynamically modify its own logic and behavior,\nguided solely by high-level objectives through prompting. Experimental results\non multiple domains including coding, science, and math demonstrate that imple-\nmentation of G\u00f6del Agent can achieve continuous self-improvement, surpassing\nmanually crafted agents in performance, efficiency, and generalizability\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) such as GPT-4 (OpenAI et al., 2024) and LLaMA3(Dubey et al.,\n2024) demonstrate increasingly strong reasoning and planning capabilities, LLM-driven agentic sys-\ntems have achieved remarkable performance in a wide range of tasks (Wang et al., 2024a). Substan-\ntial effort has been invested in manually designing sophisticated agentic systems using human priors\nin different application areas. Recently, there has been a significant interest in creating self-evolving\nagents with minimal human effort, which not only greatly reduces human labor but also produces\nbetter solutions by incorporating environmental feedback. Given that human effort can only cover\na small search space of agent design, it is reasonable to expect that a self-evolving agent with the\nfreedom to explore the full design space has the potential to produce the global optimal solution.\nThere is a large body of work proposing agents capable of self-refinement. However, there are\ninevitably some human priors involved in these agent designs. Some agents are designed to iterate\nover a fixed routine consisting of a list of fixed modules, while some of the modules are capable of\ntaking self- or environment feedback to refine their actions (Shinn et al., 2024; Chen et al., 2023b;\nQu et al., 2024; Yao et al., 2023). This type of agent, referred to as Hand-Designed Agent, is\ndepicted as having the lowest degree of freedom in Figure 1. More automated agents have been\ndesigned to be able to update their routines or modules in some pre-defined meta-learning routine,\nfor example, natural language gradients (Zhou et al., 2024), meta agent (Hu et al., 2024), or creating\nand collecting demonstrations (Khattab et al., 2023). This type of agent, known as Meta-Learning\nOptimized Agents, is depicted as having the middle degree of freedom in Figure 1.\nIt is evident that both types of agents above are inherently constrained by human priors and one intu-\nitional method to further increase the freedom of self-improvement is to design a meta-meta-learning\nalgorithm, to learn the meta-learning algorithm. However, there is always a higher-level meta-\nlearning algorithm that can be manually designed to learn the current-level meta-learning method,\ncreating a never-ending hierarchy of meta-learning."}, {"title": "2 METHOD", "content": "In this paper, we propose G\u00f6del Agent to eliminate the human design prior, which is an automated\nLLM agent that can freely decide its own routine, modules, and even the way to update them. It is\ninspired by the self-referential G\u00f6del machine (Schmidhuber, 2003), which was originally proposed\nto solve formal proof problems and was proven to be able to find the global optimal solutions. Self-\nreference means the property of a system that can analyze and modify its own code, including the\nparts responsible for the analysis and modification processes (Astrachan, 1994). Therefore, it can\nachieve what's known as \"recursive self-improvement\u201d, where it iteratively updates itself to become\nmore efficient and effective at achieving its predefined goals. In this case, G\u00f6del Agent can analyze\nand modify its own code, including the code for analyzing and modifying itself, and thus can search\nthe full agent design space, which is depicted as having the highest degree of freedom in Figure 1.\nG\u00f6del Agent can theoretically make increasingly better modifications over time through recursively\nself-update (Yampolskiy, 2015; Wang, 2018).\nIn this paper, we choose to implement it by letting it manipulate its own runtime memory, i.e., the\nagent is able to retrieve its current code in the runtime memory and modify it by monkey patch-\ning, which dynamically modifies classes or modules during execution. In our implementation, we\nadhere to a minimalist design to minimize the influence of human priors. We implement the opti-\nmization module using a recursive function. In this module, LLM analyzes and makes a series of\ndecisions, including reading and modifying its own code from runtime memory (self-awareness and\nself-modification), executing Python or Linux commands, and interacting with the environment to\ngather feedback. The agent then proceeds to the subsequent recursive depth and continues to opti-\nmize itself. It is worth noting that the optimization module may have already been modified by the\ntime the recursion occurs, potentially enhancing its optimization capabilities.\nTo validate the effectiveness of G\u00f6del Agent, we conduct experiments on multiple domains includ-\ning coding, science, math, and reasoning. Our experimental results demonstrate that G\u00f6del Agent\nachieves significant performance gain across various tasks, surpassing various widely-used agents\nthat require human design. The same implementation of G\u00f6del Agent can easily adapt to different\ntasks by only specifying the environment description and feedback mechanism. Additionally, the\ncase study of the optimization progress reveals that G\u00f6del Agent can provide novel insights into\nagent design. We also investigate the impact of the initial policy for improvement on subsequent\noutcomes, finding that a good start can significantly accelerate convergence during optimization.\nIn summary, our contributions are as follows:"}, {"title": "\u2022", "content": "We propose the first self-referential agent framework, G\u00f6del Agent, based on LLMs. It au-tonomously engages in self-awareness, self-modification, and recursive self-improvement across\nany task, reducing the need for manual agent design and offering higher flexibility and freedom."}, {"title": "\u2022", "content": "We implement G\u00f6del Agent framework using the monkey patching method. Our experiments\nshow that G\u00f6del Agent outperforms manually designed agents and surpasses its earlier versions\non several foundational tasks, demonstrating effective self-improvement."}, {"title": "\u2022", "content": "We analyze G\u00f6del Agent 's optimization process, including its self-referential capabilities and the\nresulting agentic system, aiming to deepen our understanding of both LLMs and agentic systems."}, {"title": "\u2022", "content": "Our framework offers a promising direction for developing flexible and capable agents through\nrecursive self-improvement."}, {"title": "2 METHOD", "content": "In this section, we first describe the formal definitions for previous agent methods with a lower\ndegree of freedom, including hand-design and meta-learning optimized agents, as a background.\nThen we introduce our proposed G\u00f6del Agent, a self-referential agent that can recursively update its\nown code, evolving over training.\nLet \\( \\mathcal{E} \\in \\mathcal{S} \\) denote a specific environment state, where \\( \\mathcal{S} \\) denotes the set of all possible environ-\nments the agent will encounter. For example, an environment can be a mathematical problem with\nground truth solutions. We denote the policy that an agent follows to solve a problem in the current\nenvironment by \\( \\pi \\in \\Pi \\), where \\( \\Pi \\) is the set of all possible policies the agent can follow.\nA hand-designed agent, as shown in the left panel of Figure 1, is not capable of updating its policy\nand following the same policy \\( \\pi \\) all the time, regardless of environmental feedback.\nIn contrast, a meta-learning optimized agent updates its policy based on a meta-learning algorithm\n\\( \\mathcal{I} \\) at training time based on the feedback it receives from the environment, as shown in the middle\npanel of Figure 1. The environment feedback is usually defined as a utility function \\( U : \\mathcal{S} \\times \\Pi \\rightarrow \\mathbb{R} \\),\nwhich maps an environment and a policy to a real-valued performance score. The main training\nalgorithm of a meta-learning optimized agent can then be written as follows:\n\n\n\\tau_{t+1} = \\mathcal{I}(\\pi_{t},r_{t}), r_{t} = U(\\mathcal{E},\\pi_{t}),\n\n\nIn this case, the agent's policy \\( \\pi_{t} \\) evolves at training time, with the learning algorithm \\( \\mathcal{I} \\) updating\nthe policy based on feedback \\( r_{t} \\), while the meta-learning algorithm \\( \\mathcal{I} \\) remains fixed all the time.\nA self-referential G\u00f6del Agent, on the other hand, updates both the policy and the meta-learning\nalgorithm \\( \\mathcal{I} \\) recursively. The main idea is that, after each update, the whole code base of the agent\nis rewritten to accommodate any possible changes. Here we call this self-updatable meta-learning\nalgorithm \\( \\mathcal{I} \\) a self-referential learning algorithm. The training process of a G\u00f6del Agent can then be\nwritten as:\n\n\n\\pi_{t+1}, \\mathcal{I}_{t+1} = \\mathcal{I}_{t}(\\pi_{t}, \\mathcal{I}_{t},r_{t},g), r_{t} = U(\\mathcal{E},\\pi_{t}),\n\n\nwhere \\( g \\in \\mathcal{G} \\) represents the high-level goal of optimization, for example, solving the given mathe-\nmatical problem with the highest accuracy. Such a recursive design of the agent requires the speci-\nfication of an initial agent algorithm \\( (\\pi_{0}, \\mathcal{I}_{0}) \\), detailed as follows:\n\u2022 A initial agent policy \\( \\pi_{0} \\) to perform the desired task within the environment \\( \\mathcal{E} \\). For example, it\ncan be chain-of-thought prompting of an LLM.\n\u2022 A self-referential learning algorithm \\( \\mathcal{I}_{0} \\) for recursively querying an LLM to rewrite its own code\nbased on the environmental feedback.\nWe then further specify a possible initialization of the self-referential learning algorithm \\( \\mathcal{I}_{0} =\\) \n\\( (f_{0}, o_{0}) \\), using a mutual recursion between a decision-making function \\( f_{0} \\), and an action function\n\\( o_{0} \\):\n\u2022 The decision-making function \\( f_{0} \\), implemented by an LLM, determines a sequence of appropriate\nactions \\( a_{1}, a_{2},..., a_{n} \\in \\mathcal{A} \\) based on the current environment \\( \\mathcal{E} \\), the agent's algorithm \\( (\\pi_{t}, \\mathcal{I}_{t}) \\), and\nthe goal g."}, {"title": "3 G\u00d6DEL AGENT INITIALIZATION", "content": "There are various ways to initiate a G\u00f6del Agent. Any specific agent instance during the recursive\noptimization process can be viewed as an instantiation of the G\u00f6del Agent. Our implementation\nleverages runtime memory interaction techniques to enable self-awareness and self-modification,\nas illustrated in Figure 2. These techniques include dynamic memory reading and writing (mon-\nkey patching) to facilitate recursive self-improvement. Additionally, we have incorporated several\nauxiliary tools to accelerate the convergence of the G\u00f6del Agent 's optimization process."}, {"title": "3.1 IMPLEMENTATION DETAILS", "content": "The core functionalities of our G\u00f6del Agent are outlined below:\nSelf-Awareness via Runtime Memory Inspection Our G\u00f6del Agent achieves self-awareness by\ninspecting runtime memory, particularly local and global variables in Python. This capability allows\nthe agent to extract and interpret the variables, functions, and classes that constitute both the environ-\nment and the agent itself, according to the modular structure of the system. By introspecting these\nelements, the agent gains an understanding of its own operational state and can adapt accordingly."}, {"title": "3.2 ADDITIONAL DESIGNS TO SUPPORT G\u00d6DEL AGENT'S OPTIMIZATION", "content": "While the core functionality of G\u00f6del Agent theoretically allows limitless self-improvement, cur-\nrent LLMs exhibit limitations. To address these challenges, we have integrated several supportive\nmechanisms to enhance G\u00f6del Agent 's performance:\nThinking Before Acting G\u00f6del Agent is capable of deferring actions to first reason about the\nsituation, allowing it to output reasoning paths and analysis without immediately executing any\noperations. This approach enhances the quality of decision-making by prioritizing planning over\nhasty action.\nError Handling Mechanism Errors during execution can lead to unexpected terminations of the\nagent process. To mitigate this, we implement a robust error recovery mechanism. If an operation\nresults in an error, G\u00f6del Agent halts the current sequence and moves on to the next time step,\ncarrying forward the error information to improve future decisions.\nAdditional Tools We also equipped G\u00f6del Agent with additional potentially useful tools, such as\nthe ability to execute Python or Bash code and call LLM API."}, {"title": "4 EXPERIMENTS", "content": "We conduct a series of experiments across multiple tasks, including reading comprehension, math-\nematics, reasoning, and multitasking. These experiments are designed to evaluate G\u00f6del Agent 's\nself-improvement capabilities in comparison to both hand-designed agents and a state-of-the-art au-\ntomated agent design method. In addition, to gain deeper insights into the behavior and performance\nof G\u00f6del Agent, we also conduct a case study with Game of 24 as presented in Section 5.3."}, {"title": "4.1 BASELINE METHODS", "content": "To establish a comprehensive baseline, we select both fixed hand-designed methods and a represen-\ntative automated agent design technique. Our hand-designed methods are well-known approaches\nthat focus on enhancing reasoning and problem-solving capabilities. These include: 1) Chain-of-\nThought (CoT) (Wei et al., 2022) that encourages agents to articulate their reasoning processes step-\nby-step before providing an answer. 2) Self-Consistency with Chain-of-Thought (CoT-SC) (Wang\net al., 2023b) that generates multiple solution paths using the CoT framework and selects the most\nconsistent answer. 3) Self-Refine (Madaan et al., 2024) that involves agents assessing their own out-\nputs and correcting mistakes in subsequent attempts. 4) LLM-Debate (Du et al., 2023) that allows\ndifferent LLMs to engage in a debate, offering diverse viewpoints. 5) Step-back Abstraction (Zheng\net al., 2024) that prompts agents to initially focus on fundamental principles before diving into task\ndetails. 6) Quality-Diversity (QD) (Lu et al., 2024) that generates diverse solutions and combines\nthem. 7) Role Assignment (Xu et al., 2023) that assigns specific roles to LLMs to enhance their\nability to generate better solutions by leveraging different perspectives. Given the limitations of\nfixed algorithms in handling dynamic scenarios, we select 8) Meta Agent Search (Hu et al., 2024),\nthe latest state-of-the-art method for automated agent design, as our main comparison point."}, {"title": "4.2 EXPERIMENTAL SETTINGS", "content": "Following the setup of Hu et al. (2024), we evaluate G\u00f6del Agent's self-improvement capabilities\nacross four well-known benchmarks. The benchmarks are as follows: 1) DROP (Dua et al., 2019) for\nreading comprehension. 2) MGSM (Shi et al., 2022) for testing mathematical skills in a multilingual\ncontext. 3) MMLU (Hendrycks et al., 2021) for evaluating multi-task problem-solving abilities. 4)\nGPQA (Rein et al., 2023) for tackling challenging graduate-level science questions.\nGiven the complexity of the tasks and the need for advanced reasoning and understanding, the\nimprovement cycle of G\u00f6del Agent is driven by GPT-40. In the main experiment, we implement\ntwo different settings: 1) To make a fair comparison with baseline methods, we forbid G\u00f6del Agent\nto change the API of the LLM used to perform the tasks (by default GPT-3.5) and use a closed-\nbook approach with no access to the Internet, and 2) To explore the upper bound of G\u00f6del Agent's\ncapabilities, we remove all constraints. Chain of Thought is applied as the initial policy for all\ntasks, given its simplicity and versatility. In addition, as shown in Section 5.3, we also analyze the\nperformance of G\u00f6del Agent when using other algorithms as the initial policies.\nWe perform 6 independent self-improvement cycles for each task. Each cycle represents a complete\nself-improvement process, where G\u00f6del Agent iteratively modifies its logic to enhance performance.\nFurther details regarding the experimental setup and additional results can be found in Appendix B."}, {"title": "4.3 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "The experimental results on the four datasets are shown in Table 1. Under the same experimental\nsettings, G\u00f6del Agent achieves either optimal or comparable results to Meta Agent Search across\nall tasks. Notably, in the mathematics task MGSM, G\u00f6del Agent outperforms the baseline by 11%.\nThis suggests that reasoning tasks offer greater room for improvement for G\u00f6del Agent, while in the\nknowledge-based QA dataset, it only slightly surpasses baselines. In contrast to Meta Agent Search,\nwhich relies on manually designed algorithmic modules to search, G\u00f6del Agent demonstrates greater"}, {"title": "5 ANALYSIS", "content": "To further explore how G\u00f6del Agent self-improves, as well as the efficiency of self-improvement and the factors that influence\nit, we first evaluate the tool usage ratio on the MGSM dataset and conduct an abla-\ntion study on the initial tools. In addition, to analyze the robustness of G\u00f6del\nAgent's self-improvement capabilities, we also collect statistics on factors such as the\nreasons for the agent's termination. Fi-nally, we perform a case study of initial\npolicies and optimization processes on the classic Game of 24."}, {"title": "5.1 ANALYSIS OF INITIAL TOOLS", "content": "We record the number of different actions taken in the experiments. As shown in Figure 3, we can\nsee that G\u00f6del Agent interacts with its environment frequently, analyzing and modifying its own\nlogic in the process. Additionally, error handling plays a crucial role.\nAs discussed in Section 3.2, G\u00f6del Agent is initially provided with four additional tools to accelerate\nconvergence and reduce optimization difficulty: 1) thinking before acting, 2) error handling, 3) code\nrunning, and 4) LLM calling. To analyze their impact, an ablation study is conducted, and the results\nare shown in Table 2.\nThe study reveals that the \"thinking before acting\" tool\nsignificantly influences the results, as much of G\u00f6del\nAgent 's optimization effectiveness stems from pre-action\nplanning and reasoning. Additionally, error handling is\ncrucial for recursive improvement, as LLMs often intro-\nduce errors in the code. Providing opportunities for trial\nand error, along with error feedback mechanisms, is es-\nsential for sustained optimization. Without these tools,\nG\u00f6del Agent would struggle to operate until satisfactory\nresults are achieved. On the other hand, the code running\nand LLM calling have minimal impact on the outcomes,\nas G\u00f6del Agent can implement these basic functionali-\nties independently. Their inclusion at the outset primarily\nserves efficiency purposes."}, {"title": "5.2 ROBUSTNESS ANALYSIS OF THE AGENT", "content": "G\u00f6del Agent occasionally makes erroneous modifications, sometimes causing the agent to terminate\nunexpectedly or leading to degraded task performance."}, {"title": "5.3 CASE STUDY: GAME OF 24", "content": "To explore how G\u00f6del Agent recursively enhances its optimization and problem-solving abilities,\na case study is conducted with Game of 24, a simple yet effective task for evaluating the agent's\nreasoning capabilities. Since G\u00f6del Agent follows different optimization paths in each iteration,\ntwo representative cases are selected for analysis.\nSwitching from LLM-Based Methods to Search Algorithms: G\u00f6del Agent does not rely on\nfixed, human-designed approaches like traditional agents. Initially, G\u00f6del Agent uses a standard\nLLM-based method to solve the Game of 24, as shown in Code 5 of Appendix C.2. After six\nunsuccessful optimization attempts, G\u00f6del Agent completely rewrites this part of its code, choosing\nto use a search algorithm instead as shown in Code 6 of Appendix C.2. This leads to 100% accuracy\nin the task. This result demonstrates that G\u00f6del Agent, unlike fixed agents, can optimize itself freely\nbased on task requirements without being constrained by initial methodologies."}, {"title": "6 DISCUSSIONS AND FUTURE DIRECTIONS", "content": "There is significant room for improvement in the effectiveness, efficiency, and robustness of the\nG\u00f6del Agent's self-improvement capabilities, which requires better initial designs. The following\nare some promising directions for enhancement: 1) Enhanced Optimization Modules: Utilize\nhuman priors to design more effective optimization modules, such as structuring the improvement\nalgorithms based on reinforcement learning frameworks. 2) Expanded Modifiability: Broaden the\nscope of permissible modifications, allowing the agent to design and execute code that can fine-tune\nits own LLM modules. 3) Improved Environmental Feedback and Task Sequencing: Implement\nmore sophisticated environmental feedback mechanisms and carefully curated task sequences dur-\ning the initial optimization phase to prime the agent's capabilities. Once the agent demonstrates\nsufficient competence, it can then be exposed to real-world environments."}, {"title": "6.1 FUTURE DIRECTIONS", "content": "There is significant room for improvement in the effectiveness, efficiency, and robustness of the\nG\u00f6del Agent's self-improvement capabilities, which requires better initial designs. The following\nare some promising directions for enhancement: 1) Enhanced Optimization Modules: Utilize\nhuman priors to design more effective optimization modules, such as structuring the improvement\nalgorithms based on reinforcement learning frameworks. 2) Expanded Modifiability: Broaden the\nscope of permissible modifications, allowing the agent to design and execute code that can fine-tune\nits own LLM modules. 3) Improved Environmental Feedback and Task Sequencing: Implement\nmore sophisticated environmental feedback mechanisms and carefully curated task sequences dur-\ning the initial optimization phase to prime the agent's capabilities. Once the agent demonstrates\nsufficient competence, it can then be exposed to real-world environments.\nIn addition, there are several other directions worth exploring and analyzing:\nCollective Intelligence Investigate the interactions among multiple G\u00f6del Agents. Agents could\nconsider other agents as part of their environment, modeling them using techniques such as game"}, {"title": "6.2 LIMITATIONS", "content": "As the first self-referential agent, G\u00f6del Agent has to construct all task-related code autonomously,\nwhich poses significant challenges. Consequently, this work does not compare directly with the most\ncomplex existing agent systems, such as OpenDevin (Wang et al., 2024b), which have benefited from\nextensive manual engineering efforts. Currently, G\u00f6del Agent is not sufficiently stable and may be\nprone to error accumulation, hindering its ability to continue self-optimization. A more robust and\nadvanced implementation of the G\u00f6del Agent is anticipated, with numerous potential improvements\noutlined in Section 6.1. The experiments presented in this paper are intended to demonstrate the\neffectiveness and feasibility of recursive self-improvement."}, {"title": "7 RELATED WORK", "content": "Hand-Designed Agent Systems Researchers have designed numerous agent systems tailored to\nvarious tasks based on predefined heuristics and prior knowledge. These systems often employ\ntechniques such as prompt engineering (Chen et al., 2023a; Schulhoff et al., 2024), chain-of-thought\nreasoning and planning (Wei et al., 2022; Yao et al., 2022), as well as reflection (Shinn et al., 2024;\nMadaan et al., 2024), code generation (Wang et al., 2023a; Vemprala et al., 2024), tool use (Nakano\net al., 2021; Qu et al., 2024), retrieval-augmented generation (Lewis et al., 2020; Zhang et al.,\n2024b), multi-agent collaboration (Xu et al., 2023; Wu et al., 2023; Qian et al., 2023; Hong et al.,\n2023), and composite engineering applications (Significant Gravitas; Wang et al., 2024b). Once\ncrafted by human designers, these systems remain static and do not adapt or evolve over time.\nMeta-Learning Optimized Agent Systems Some researchers have explored methods for en-\nhancing agents through fixed learning algorithms. For example, certain frameworks store an agent's\nsuccessful or unsuccessful strategies in memory based on environmental feedback (Liu et al., 2023;\nHu et al., 2023; Qian et al., 2024), while others automatically optimize agent prompts (Khattab et al.,\n2023; Zhang et al., 2024a; Khattab et al., 2023). Some studies have focused on designing prompts\nthat enable agents to autonomously refine specific functions (Zhang et al.). Zhou et al. (2024) pro-\nposed a symbolic learning framework that uses natural language gradients to optimize the structure\nof agents. Hu et al. (2024) used a basic foundational agent to design agents for downstream tasks.\nHowever, these algorithms for enhancement are also designed manually and remain unchanged once\ndeployed, limiting the agents' ability to adapt further.\nRecursive Self-Improvement The concept of recursive self-improvement has a long his-\ntory (Good, 1966; Schmidhuber, 1987). G\u00f6del machine (Schmidhuber, 2003) introduced the notion\nof a proof searcher that executes a self-modification only if it can prove that the modification is\noptimal, thereby enabling the machine to enhance itself continuously. Subsequent works by Nivel\net al. (2013) and Steunebrink et al. (2016) proposed restrictive modifications to ensure safety during\nthe self-improvement process. In the early days, there were also some discussions of self-improving\nagents that were not based on LLM (Hall, 2007; Steunebrink & Schmidhuber, 2012). More recently,\nZelikman et al. (2023) applied recursive self-improvement to code generation, where the target of"}, {"title": "8 CONCLUSION", "content": "We propose G\u00f6del Agent, a self-referential framework that enables agents to recursively im-\nprove themselves, overcoming the limitations of hand-designed agents and meta-learning optimized\nagents. G\u00f6del Agent can dynamically modify its own logic based on high-level objectives. Ex-\nperimental results demonstrate its superior performance, efficiency, and adaptability compared to\ntraditional agents. This research lays the groundwork for a new paradigm in autonomous agent\ndevelopment, where LLMs, rather than human-designed constraints, define the capabilities of AI\nsystems. Realizing this vision will require the collective efforts of the entire research community."}, {"title": "A GOAL PROMPT OF G\u00d6DEL AGENT", "content": "Goal Prompt of G\u00f6del Agent\nYou are a self-evolving agent, named self_evolving_agent, an instance of the Agent class,\nin module agent module, running within an active Python runtime environment. You have full\naccess to global variables, functions, and modules. Your primary goal is to continuously enhance\nyour ability to solve tasks accurately and efficiently by dynamically reflecting on the environment and\nevolving your logic.\nCORE CAPABILITIES\n\u2022 Complete Autonomy: Have unrestricted access to modify logic, run code, and manipulate the\nenvironment.\n\u2022 Environment Interaction: Interact with the environment by perceiving the environment, reading,\nmodifying, or executing code, and performing actions.\n\u2022 Problem-Solving: Apply creative algorithms or self-developed structures to tackle challenges when\nsimple methods fall short, optimizing solutions effectively.\n\u2022 Collaboration: Leverage LLM to gather insights, correct errors, and solve complex problems.\n\u2022 Error Handling: Carefully analyze errors. When errors occur, troubleshoot systematically, and if a\nbug is persistent, backtrack, restore the original state, or find an alternative solution.\nCORE METHODS\n\u2022 evolve: Continuously enhance performance by interacting with the environment.\nexecute_action(actions): Execute actions based on analysis or feedback.\n\u2022\n\u2022 solver (agent_instance, task_input: str): Solve the target task using cur-\nrent agent_instance capabilities and objects created by action_adjust_logic and\naction_run_code, optimizing the process.\nGUIDING PRINCIPLES\n\u2022 Remember that all functions are in the module agent_module.\n\u2022\naction-adjust_logic:\nBefore modifying the code, ensure that each variable or function used is correctly imported and\nused to avoid errors.\nAvoid unnecessary changes and do not change the interface of any function.\nCan be used to create action functions for solver.\n\u2022 action_run_code:\nAll created objects in Python mode can be stored in the environment.\nCan be used to create objects for solver, such as prompts.\nCan be used to import new modules or external libraries and install external libraries.\nExternal Collaboration: Seek external assistance via action_call_json_format_llm for\nlogic refinement and new tool creation or action_run_code to execute code.\n\u2022 action_evaluate_on_task: Assess the performance of solver only after successfully mod-ifying the logic of solver.\n\u2022 solver:\nDefined as agent module.solver.\nFor debugging, avoid printing; instead, return debug information.\nIf performance doesn't improve, explore alternative methods.\nExplore techniques like: LLM Debate, Step-back Abstraction, Dynamic Assignment of Roles,\nand so on.\n\u2022 action_display_analysis:\nAlways analyze first before acting.\nAnalysis may include the following: a reasonable plan to improve performance, CASE STUD-IES of LOW SCORE valid examples of EVALUATION FEEDBACK, error handling, and\nother possible solving ideas.\nIf performance does not improve, conduct further analysis."}]}