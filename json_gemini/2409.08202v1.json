{"title": "WHAT MAKES A MAZE LOOK LIKE A MAZE?", "authors": ["Joy Hsu", "Jiayuan Mao", "Joshua B. Tenenbaum", "Noah D. Goodman", "Jiajun Wu"], "abstract": "A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze). To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning. At the core of DSG are schemas-dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols. DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models. The grounded schema is used to augment visual abstraction understanding. We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions.", "sections": [{"title": "1 INTRODUCTION", "content": "Humans possess the remarkable ability to flexibly acquire and apply abstract concepts when interpreting the concrete world around us. Consider the concept \u201cmaze\u201d: our mental model can interpret mazes constructed with conventional materials (e.g., drawn lines) or unconventional ones (e.g., icing), and reason about mazes across a wide range of configurations and environments (e.g., in a cardboard box or on a knitted square). Our goal is to build systems that can make such flexible and broad generalizations as humans do. This necessitates a reconsideration of a fundamental question: what makes a maze look like a maze? A maze is not defined by concrete visual features such as the specific material of walls or its perpendicular intersections, but by lifted rules over symbols-a plausible model for a maze includes its layout, the materials composing the walls, and the designated entry and exit. Crucially, these model components can be realized by infinitely many real-world variations. For instance, walls constructed from candy canes, hay, or popsicle sticks, despite their diverse materials, are all identifiable as parts of a maze.\nHowever, current vision-language models (VLMs) often struggle to reason about visual abstractions at a human level, frequently defaulting to literal interpretations of images, such as a collection of object categories. These interpretations may be technically correct descriptions of the scene, but may not align with the abstract concept underlying the image. For example, when given the intended concept of \"maze\" and an image that realizes the concept with unconventional objects, VLMs such as GPT-4V (OpenAI, 2023) and LLaVA (Liu et al., 2024) fail to correctly recognize objects in the scene as components of the visual abstraction. It is unclear whether these off-the-shelf models leverage knowledge of abstract concepts to make sense of images as humans do."}, {"title": "2 RELATED WORKS", "content": "Vision-language grounding and reasoning benchmarks. Visual-language benchmarks have traditionally tackled reasoning about content in images, such as objects, relations, and actions (Chen et al., 2015; Yu et al., 2015; Antol et al., 2015; Wu et al., 2016; Zhu et al., 2016; Krishna et al., 2017; Wang et al., 2017). Recently, an increasing number of works have focused on evaluating visual-language tasks that require commonsense reasoning, usually by asking \u201cwhat if\u201d or \u201cwhy\u201d questions (Pirsiavash et al., 2014; Wagner et al., 2018; Zellers et al., 2019; Lu et al., 2022), or by generating holistic scene descriptions (Song et al., 2024). In contrast to these related works, our paper emphasizes the visual grounding and reasoning problem of abstract concepts, such as uncommon visualizations of scientific terms or unconventional constructions of strategic games. Such abstract concepts should be grounded on the relationship between lower-level concepts such as entities and patterns, therefore positing novel challenges to vision-language models.\nConcept representations in minds and machines. Our schema-based concept representations are inspired by the theory-theory of concepts in cognitive science and artificial intelligence (Schank & Abelson, 1975; Morton, 1980; Carey, 1985; Gopnik, 1988; Gopnik & Meltzoff, 1997; Carey, 2000). In short, these works treat concepts as organized within and around theories. Therefore, acquiring the concept involves learning its theory, and reasoning with a concept involves causal-explanatory reasoning of the theories. Such views have been largely leveraged in generating symbolic representations for concepts such as word meanings (Speer et al., 2017) and object shapes (Biederman, 1987). However, most works have focused on using only symbolic features to describe scenes and have been primarily applied to simple object-level concrete concepts. In contrast, we propose using our schema representation as a guide for grounding scene-level, abstract concepts in natural images."}, {"title": "3 DEEP SCHEMA GROUNDING", "content": "We study the task of reasoning about visual abstractions by concretely focusing on its visual question-answering (VQA) form, although our idea naturally generalizes to other visual interpretation and reasoning tasks. A VQA instance is a tuple of (v, q, a), where v is an image, q is a natural language question, and a is the answer to the question. Here, a can take the form of multiple choices or", "subsections": [{"title": "3.1 VISUAL ABSTRACTION SCHEMA", "content": "A visual abstraction schema is a concise program that defines a directed acyclic graphical (DAG) representation of a particular concept. As illustrated in Figure 2, each node in the schema corresponds to a subcomponent concept of the higher-level abstract concept. For example, the formation of a maze can be decomposed into three components: the layout, the construction of the walls, and the positioning of the entry and exit of the maze. The dependencies among individual components yield a DAG configuration in this case, the placement of the entry and the exit of the maze depends on the layout of the maze. From an inference perspective, the dependency graph describes the order in which the model should interpret each component, and how the interpretation of a particular component should be conditioned on the interpretation of one or more previously interpreted components.\nThe most important feature of a visual abstraction schema is that the definition of such concepts is universal, in the sense that the program describes a universal characterization of mazes made of various components, rather than being restricted to specific visual instantiations (e.g., mazes formed by drawn lines). Schemas enable machines to better generalize to novel scenes."}, {"title": "3.2 EXTRACTING SCHEMAS OF ABSTRACT CONCEPTS", "content": "DSG utilizes large language models (LLMs) as a repository of knowledge from which schemas can be acquired. Given that LLMs are trained on extensive corpora of human language data, we hypothesize that they contain human-aligned schemas for a wide range of abstract concepts. Compared to prior"}, {"title": "3.3 HIERARCHICALLY GROUNDING SCHEMAS ON IMAGES", "content": "Instead of directly answering questions given the image and the question inputs, we first hierarchically ground individual components in the schema to visual entities in the image. In the maze example in Figure 2, this corresponds to inferring the materials of the walls, the layout of the maze, etc.\nFormally, given a schema definition of the concept, DSG hierarchically grounds each component in the concept DAG. The outcome of this process is represented as short text descriptions, such as \"wall: coffee beans\". We implement this procedure by leveraging pretrained VLMs to predict the most likely grounding of a component given the image and a text query (see Figure 4). Since our grounding procedure is hierarchical, for components that are conditioned on other concepts (e.g., the interpretation of entry-exit is conditioned on the layout), the VLM also takes into account the grounding results of these prerequisite components. This ensures that each step of the grounding process is informed by the previously established context, facilitating more coherent and accurate grounding."}, {"title": "3.4 VISUAL QUESTION-ANSWERING AUGMENTED WITH GROUNDED SCHEMAS", "content": "Finally, DSG leverages the grounding of individual components in the image to answer questions. For example, given the concept \u201cmaze\u201d and the grounded mapping of individual components: {layout:\nrectangular, walls: coffee beans, entry-exit: coffee cups}, we augment the final question-answering step by providing the VLM with the component mapping, as well as the full conversation history of the grounding process. In particular, the groundings of the components are included in a text prompt such as: \u201cImagine that the image represents a maze, and the layout is rectangular, and the walls are coffee beans, and the entry and exit are coffee cups.\" Compared with baselines that only have access to the abstract concept itself (e.g., \"Imagine that the image represents a maze.\"), our concept grounding system shows significant performance improvement with the resolved schema as holistic context for the image.\nNotably, our DSG framework is inference-only: it leverages the strong generalization capabilities of LLMs (for proposing schemas) and VLMs (for grounding schemas and reasoning), to better understand visual abstractions. Programmatic schemas are used as the intermediate representation to bridge the explicit commonsense knowledge in LLMs and the visual reasoning capability of VLMs."}]}, {"title": "4 VISUAL ABSTRACTIONS DATASET", "content": "In order to evaluate models on visual abstraction reasoning, we propose a new dataset, the Visual Abstractions Dataset (VAD), illustrated with examples in Figure 5. It consists of 180 images and 3 questions per image, with a total of 540 test examples. VAD is comprised of 12 different abstract concepts spanning 4 categories, where each concept is associated with 15 examples of real-world images. The questions bridge multiple question types: binary-choice, counting, and open-ended questions. We detail the set of abstract concepts and the image and text components of VAD below.\nAbstract concepts. We choose 12 different visual abstractions as concepts. All of them can be grounded on different objects and configurations of objects in images. We focus on concepts that are not tied to distinct visual features (e.g., unlike canonical colors and shapes) but instead require higher-level, relational, and lifted patterns. These visual abstractions cover a wide range of visual scenarios and possible linguistic queries, and they can be grouped into four broader categories.\nThe first category consists of strategic concepts, which are games characterized by rules and patterns, including \"tic-tac-toe\u201d, \u201cmaze\u201d, and \u201ctreasure map\". The exact object instantiations and visual features may vary, but humans can easily identify the state of the game when given the concept. The second category consists of scientific concepts with no possible visualization in the physical world, including \"solar system\u201d, \u201catom\u201d, and \u201ccell\u201d. While we cannot directly view these concepts, we can create analogies of them based on everyday objects. The third category consists of social concepts, which are intentional actions that involve the deduction of roles and relations, including \u201chelping\", \u201cdeceiving\", and \u201cnegotiating\u201d. Examples of these theory-of-mind concepts in the real world can"}, {"title": "5 RESULTS", "content": "We validate Deep Schema Grounding in comparison to baseline VLMs and integrated LLMs with APIs on the Visual Abstractions Dataset. We first compare our work with ViperGPT (Sur\u00eds et al., 2023), VisProg (Gupta & Kembhavi, 2023), LLaVA (Liu et al., 2024), InstructBLIP (Dai et al., 2024), and GPT-4V (OpenAI, 2023). Then, we ablate different components of the DSG framework. Finally, we show that DSG is model-agnostic and can benefit both closed-source and open-source models. Baseline models are given the abstract concept itself in the prompt along with the question (e.g., \"Imagine that the image represents a maze.\"), but without DSG's resolved schema of the concept. All results are averaged over 5 runs. Here, we focus on a systematic evaluation of multiple-choice answers, where accuracy is calculated as exact-match accuracy. In the Appendix, we provide additional comparisons with a graded accuracy metric based on the degree of alignment to human judgements, and report results on free-form answers evaluated by BERTScores (Zhang et al., 2020)."}, {"title": "6 DISCUSSION", "content": "Our framework demonstrates that by explicitly grounding conceptual schemas through large pretrained vision and language models, we can achieve better visual abstraction understanding in diverse real world instantiations. We find that LLMs generally possess well-structured knowledge of concepts, outputting comprehensive schemas that are accurately decomposed into more concrete symbols and their dependencies. The extracted schemas effective capture the concise essence of a concept, such as the answer to our initial question of what makes a maze look like a maze. However, as DSG relies on pre-trained LLMs to generate schemas and does not restrict the schemas to a set of specified symbols, it is possible that the schemas may contain harmful biases based on the concept given, or may include symbols that are difficult for VLMs to interpret.\nIn particular, we observe that current VLMs struggle to ground challenging schema components involving spatial constraints. DSG does not explicitly improve this capability or specify how to parameterize spatial configurations. While DSG can generate schema components that contain spatial relationships, and try to ground them by describing them in language, it remains limited by the VLMs' abilities to conduct such spatial understanding. For example, let us consider a case where the extracted schema for the concept maze includes a symbol representing \u201can unobstructed path from entry to exit\u201d. That such a path exists in the maze\u2014typically only one and often one that is difficult to find is a defining feature of mazes, yet is challenging for VLMs to correctly identify. VLMs' lack of spatial understanding weakens DSG ability to interpret complex and spatially grounded components. Scaling DSG to better understand concepts that require precise layouts of schema components is difficult but crucial for future work.\nA key aspect of DSG is that our framework does not limit the expressiveness of schemas to a specific set of symbols or the grounding process to a specific set of answers. The only prior we inject into DSG is the hierarchical process of resolving the schema\u2014the interface between the LLM and VLMs is unconstrained language. In theory, DSG can propose and ground any concept and any individual component, allowing the LLM and VLM to determine what defines a visual abstraction without any restrictions. But such flexibility may also render the schema ambiguous and not universal enough to improve downstream tasks. However, we see that the empirical evidence in our paper supports DSG, and we believe that DSG serves as a proof of concept for the kind of inductive biases that should be incorporated into visual reasoning systems\u2014a structured thinking process guided by flexible but explicit knowledge can augment visual abstraction interpretation.\nWhile we have made progress on understanding what makes a maze look like a maze, in proposing DSG as a framework for leveraging explicit schemas backed by powerful LLMs and VLMs, we are far from solving all reasoning across abstract concepts as humans do. There may be ways to improve grounding of individual components, as well as more efficient priors that can be incorporated into the system. We leave such exploration to future work."}, {"title": "7 CONCLUSION", "content": "We propose Deep Schema Grounding as a promising approach to understanding visual abstractions. DSG leverages schemas of concepts to decompose abstract concepts into subcomponents and model their dependencies. Our framework extracts explicitly structured schemas from large language models and hierarchically grounds them to images with vision-language models. On the Visual Abstractions Dataset, a visual question-answering benchmark composed of real-world images with diverse underlying abstract concepts, DSG demonstrates significant improvements compared to base VLMs. DSG is a step toward interpreting visual abstractions as humans do; more remains to be done in the challenge of visual abstraction reasoning."}]}