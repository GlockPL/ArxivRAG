{"title": "A Real-Time Defense Against Object Vanishing Adversarial Patch Attacks for Object Detection in Autonomous Vehicles", "authors": ["Jaden Mu"], "abstract": "Autonomous vehicles (AVs) increasingly use DNN-based object detection models in vision-based perception. Correct detection and classification of obstacles is critical to ensure safe, trustworthy driving decisions. Adversarial patches aim to fool a DNN with intentionally generated patterns concentrated in a localized region of an image. In particular, object vanishing patch attacks can cause object detection models to fail to detect most or all objects in a scene, posing a significant practical threat to AVs.\nThis work proposes ADAV (Adversarial Defense for Autonomous Vehicles), a novel defense methodology against object vanishing patch attacks specifically designed for autonomous vehicles. Unlike existing defense methods which have high latency or are designed for static images, ADAV runs in real-time and leverages contextual information from prior frames in an AV's video feed. ADAV checks if the object detector's output for the target frame is temporally consistent with the output from a previous reference frame to detect the presence of a patch. If the presence of a patch is detected, ADAV uses gradient-based attribution to localize adversarial pixels that break temporal consistency. This two stage procedure allows ADAV to efficiently process clean inputs, and both stages are optimized to be low latency. ADAV is evaluated using real-world driving data from the Berkeley Deep Drive BDD100K dataset, and demonstrates high adversarial and clean performance.", "sections": [{"title": "1 Introduction", "content": "Deep neural network (DNN)-based object detection models are increasingly used in autonomous vehicles (AVs). For example, automotive company Tesla has already deployed a YOLO object detection model in its Full Self Driving software [1].\nUnfortunately, object detection models have been proven to be vulnerable to adversarial attacks [5]. Adversarial patch attacks, which are spatially-constrained adversarial patterns designed to fool DNNs, are especially threatening due to their practicality, as they are robust to real-world transformations"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Gradient-Based Attribution", "content": "Saliency Maps with vanilla gradients compute the gradient of the class score of interest with respect to the pixels in the input image [8]. This creates a fine-grained map of the pixels in the input image which have the greatest impact on increasing the classification score. Gradient based attribution methods/saliency maps have been widely used as an AI explainability method to visualize the regions of an image a model assigns the greatest importance.\nImportantly for this work, saliency maps can also be computed for any differentiable function applied to the model's output, not just a class activation score, by extending the gradient calculation.\nGuided backpropagation was proposed by [9] as a way to create cleaner saliency maps. Guided backpropagation introduced a modification to vanilla gradients by only backpropagating positive gradients through ReLU activation functions, with the intuition that only focusing on gradients that increase the output would ensure that salient features don't get canceled out, and to produce a less noisy visualization."}, {"title": "2.2 Existing Defenses", "content": "Several defenses have been proposed to defend image classification models against adversarial patch attacks. Notably, Local Gradient Smoothing (LGS) achieved"}, {"title": "3 Method", "content": "ADAV uses a two-stage process to defend against adversarial patches - it first checks if there is a patch anywhere in the image. If the presence of a patch is detected, ADAV then localizes and masks the patch to produce a clean frame."}, {"title": "3.1 Object Detection", "content": "This work uses YOLOv5s [3] as the base object detection model, although ADAV can be applied to other models. YOLOv5 is a one-stage object detector that outputs 25200 potential bounding boxes. The bounding boxes with the highest confidences are then returned with Non-Max Suppression. Importantly, YOLOv5's one-stage architecture is fully differentiable."}, {"title": "3.2 Patch Detection", "content": "Intuitively, the same objects should be detected in similar locations between two frames close to each other temporally. This work refers to this concept as temporal consistency. When an object vanishing adversarial patch enters into the field of view of an AV between two temporally close frames, temporal consistency is broken, since the adversarial patch will suppress several detections in the model's output. This is leveraged to detect the presence of an adversarial patch. ADAV checks if temporal consistency is broken between the frame being processed (the target frame) and a temporally close reference frame. ADAV uses the frame 0.5 seconds before the target frame as the reference frame, which is assumed to be clean. To quantify temporal consistency, the Mean-Squared Error (MSE) is computed between the model's output for the target frame and the"}, {"title": "3.3 Patch Localization and Masking", "content": "If a patch is detected (temporal consistency is broken), ADAV will localize and mask the patch to recover a clean input. To localize the adversarial patch, this work uses guided backpropagation to create a saliency map. However, instead of taking the gradient of a class activation, ADAV takes the gradient of the MSE between the outputs of the model for the target frame and the reference frame with respect to the input image. Because adversarial patches break temporal consistency and therefore significantly increase the MSE, the saliency map should primarily flag pixels in the adversarial patches.\nPotential patch regions are extracted by downsampling the saliency map using strided convolutions. Specifically, a 20x20 box filter kernel with a stride of 5 is used to sum the gradients in each potential patch region. We refer to this sum of gradients as the \"suspicion score\" of a region. Because adversarial patches produce very dense clusters on the saliency map, all suspicion scores below some threshold $n$ are ignored. All remaining potential patch regions are assumed to be from the adversarial patch, so the corresponding pixels in the image are masked out.\nA small potential patch region size of 20x20 is chosen so that ADAV can neutralize patches with highly irregular, non-rectangular shapes by approximating the irregular shape with several small 20x20 regions. Additionally, a small potential patch region helps minimize information loss if the region is falsely determined to be adversarial.\nThe threshold $n$ is determined dynamically for each image, since different images may have different magnitudes of gradients in clean regions. Given a median suspicion score $\u00ee$ and interquartile range $Q$ of an image, $n = x + \\lambda Q$, where $\\lambda$ is an empirically tuned constant.\nIntuitively, this threshold detects outlier suspicion scores. This should avoid false positives from sudden non-adversarial road condition changes in the 0.5"}, {"title": "3.4 Cleaned Output Queue", "content": "ADAV must have access to clean outputs to use as the reference. This is done by storing clean outputs in the Cleaned Output Queue, which is initialized from the first 0.5 seconds of frames. Then, if no patch is detected, the output of the YOLO model is directly added to the queue to use as a reference. If a patch is detected, ADAV cleans the input, reruns the YOLO model, and adds the cleaned output to the queue, ensuring that the queue is always populated with 0.5 seconds of clean outputs. Importantly, the outputs of the YOLO model are stored directly in the queue, meaning that no inference has to be repeated when checking for temporal consistency."}, {"title": "3.5 Parameter Tuning", "content": "The threshold for determining the presence of a patch $k$ and for filtering potential patch regions $\\lambda$ must be tuned to be sensitive to the presence of an adversarial patch while remaining high enough to maintain clean performance. Therefore, tuning these parameters must take into account the tradeoff between clean performance and adversarial performance. Additionally, $k$ and $\\lambda$ must be simultaneously optimized, since a lower $k$ might require a higher $\\lambda$ to avoid false positives and vice versa. We weight adversarial performance equally with clean performance. The performance is measured by Mean Average Precision (mAP). Letting $\\pi_{\\kappa,\\lambda}$ be a model defended with ADAV with parameters set to $k$ and $\\lambda$,\n$a = \\max_{\\kappa,\\lambda}[mAP(\\pi_{\\kappa,\\lambda}(x_{adv}))]$\n$b = \\max_{\\kappa,\\lambda}[mAP(\\pi_{\\kappa,\\lambda}(x_{clean}))]$\n$k_{optimal}, \\lambda_{optimal} = arg \\min_{\\kappa,\\lambda}[(a \u2013 mAP(\\pi_{\\kappa,\\lambda}(x_{adv})) + (b \u2212 mAP(\\pi_{\\kappa,\\lambda}(x_{clean}))]$\nWe find $k_{optimal}$ and $\\lambda_{optimal}$ with a grid search.\nIn Fig 3, the blue and red points represent adversarial and clean performance of $\\pi_{\\kappa,\\lambda}$ for 100 samplings of $k$ and $\\lambda$. The red point represents the performance of $\\pi_{koptimal,doptimal}$, and the green point represents (a,b)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "This work uses the BDD100K dataset [13] for training YOLOv5s, generating adversarial patches, and for evaluation. The BDD100K dataset is composed of 100,000 40 second long videos recorded at 30 frames per second (FPS) from vehicle dashcams. BDD100K is a diverse dataset containing several vehicle and object types from multiple cities in different weather conditions and times of day."}, {"title": "4.2 Object Detector Training", "content": "To better simulate object detection in a self-driving context, YOLOv5s was trained from scratch on the 70000 image train split of BDD100K for 300 epochs, achieving 0.47 mAP."}, {"title": "4.3 Attack Formulation", "content": "This work generates adversarial patches using the methodology proposed by [7], which found that finding a patch that maximizes YOLOv5s' confidence loss produces an effective object vanishing attack by lowering the confidence for each predicted bounding box below the confidence threshold for detection. Specifically, a patch P is generated by solving the following optimization problem:\n$\\arg \\max_P E_{x\\sim X,t\\sim T}[L(A(x, t, P), \\hat{y})]$"}, {"title": "4.4 Synthetic Adversarial Video Creation", "content": "In a realistic attack scenario, adversarial patches are placed on objects in motion relative to the AV (e.g. signs, other vehicles). To create an adversarially attacked video from a clean video in BDD100K, an adversarial patch is added at a random time between 1 and 10 seconds into the video at a random position. The patch is then applied to each following frame in the video, with the position changed using a random walk model in which the patch is moved at a random speed to random waypoints throughout the video. Scale at time t seconds is determined by a randomly generated sinusoidal function ranging between 0.2 and 2. This process moves the patch smoothly throughout the video like objects in the real world, and creates a diverse sampling of patch positions and scales."}, {"title": "4.5 Evaluation Dataset", "content": "A evaluation set was created by randomly selecting 100 videos from the BDD100K dataset. Those 100 videos were added to the evaluation set as clean examples, and were then used to create 100 synthetic adversarial videos, for a evaluation set with 200 videos totaling to 240000 frames or 133 minutes."}, {"title": "4.6 Attack Detection Rate", "content": "ADAV's two-stage process requires it to accurately detect the presence of a full scale patch (detecting the presence of a smaller scale patch is less important since smaller patches create significantly weaker attacks). Therefore, we measure Attack Detection Rate separately for scales greater than 0.8. Because ADAV either determines a frame to be clean or adversarially attacked, we can measure Attack Detection Rate with metrics for binary classification (Table 1)."}, {"title": "4.7 Defense Performance", "content": "The performance of the object detector after running the defense on both clean and adversarial inputs can be measured with standard metrics for evaluating object detectors, we choose mAP@IoU=50 (the threshold for a valid detection is 50% intersection with the ground truth box).\nLGS, JPEG Compression, and Universal Defense Frames were used as baselines, as they are human-out-of-the-loop and can be computed in a realistic amount of time.\nAdditionally, the frames per second (FPS) of each defense on a T4 GPU was measured to determine inference time latency.\nThe results of this evaluation are in Table 2. The adversarial and clean performance of each defended model were measured. Additionally, because ADAV processes clean and adversarial inputs differently, clean and adversarial latency were measured separately."}, {"title": "4.8 Analysis", "content": "It is critical for ADAV to flag all adversarially attacked frames so that the second stage can mask the patch. However, it is less important to have a low false positive rate, since even if an attack is falsely detected, the second stage may not find any suspicious regions on a clean image, leaving the clean input unchanged. ADAV aligns with these goals, since ADAV has a very high recall compared to precision, suggesting that ADAV is flagging almost all adversarially attacked frames while making some false positives.\nAdditionally, ADAV demonstrates high performance in defending against adversarial patch attacks by localizing and masking out patches. ADAV significantly outperforms the next-best LGS in adversarial performance, and also exhibits higher clean performance due to its two-stage approach, which leaves clean inputs unchanged (unlike LGS). ADAV also suffers almost no loss in FPS on clean videos, suggesting that ADAV can always be active in an AV's perception system."}, {"title": "5 Discussion and Conclusion", "content": "In this paper, we propose ADAV, a novel defense that specifically focuses on object detection in an AV context. ADAV is designed with the unique characteristics of self-driving in mind, as it defends an object detection model trained on a driving dataset, takes advantage of the contextual information videos provide using temporal consistency, and is able to run in real-time.\nSome improvements to ADAV could be using inpainting techniques to reduce information loss from regions obscured by patches, and using different attribution methods such as Guided GradCAM to produce less noisy saliency maps.\nRegarding concerns of dynamically generated patches, such attacks are still bound by the temporal consistency check. Other adversarial attacks such as adversarial perturbations are harder to execute against AVs in the real world because they are less robust to transformations such as lighting or camera noise, so they were not the focus of this work. However, if temporal consistency is broken, ADAV's first stage can still effectively detect the presence of the attacks. Extending ADAV's second stage to address such attacks can be a direction for future work."}]}