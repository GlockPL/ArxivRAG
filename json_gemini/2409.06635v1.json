{"title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders", "authors": ["Wenyu Zhang", "Shuo Sun", "Bin Wang", "Xunlong Zou", "Zhuohan Liu", "Yingxu He", "Geyu Lin", "Nancy F. Chen", "Ai Ti Aw"], "abstract": "The rapid advancements in large language models (LLMs) have significantly enhanced natural language processing capabilities, facilitating the development of AudioLLMs that process and understand speech and audio inputs alongside text. Existing AudioLLMs typically combine a pre-trained audio encoder with a pre-trained LLM, which are subsequently finetuned on specific audio tasks. However, the pre-trained audio encoder has constrained capacity to capture features for new tasks and datasets. To address this, we propose to incorporate mixtures of 'weak' encoders (MoWE) into the AudioLLM framework. MoWE supplements a base encoder with a pool of relatively lightweight encoders, selectively activated based on the audio input to enhance feature extraction without significantly increasing model size. Our empirical results demonstrate that MoWE effectively improves multi-task performance, broadening the applicability of AudioLLMs to more diverse audio tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "The transformative advancements in large language models (LLMs) [1]\u2013[3] have revolutionized the field of natural language processing and artificial intelligence, enabling systems to perform tasks that require a deep understanding of human language. Building upon the foundation of LLMs, there is increasing interest to develop multimodal models to integrate multiple data modalities, such as text, images and audio, into a single end-to-end model [4]\u2013[10]. The integration of multiple data modalities allows the model to have more comprehensive understanding of context and allows the users to have more diverse ways to interact with the model.\nIn this paper, we focus on Audio Large Language Models (AudioLLMs), the class of multimodal LLMs that process and understand speech and audio inputs in conjunction with text. Existing AudioLLMs [4]\u2013[6], [8], [11]\u2013[15] are capable of executing tasks for both speech and non-speech audio inputs. Speech tasks include automatic speech recognition, speech-to-text translation and speech question answering, and non-speech audio tasks include audio captioning, sound event classification and audio question anwering. Typically, a pre-trained audio encoder is connected to a pre-trained LLM, and the model is instruction finetuned with curated audio task datasets [4]\u2013[8]. The audio encoder is pre-trained with supervised learning [16] or self-supervised learning algorithms [17]\u2013[19]. Recent AudioLLMs [4], [5], [7] have utilized the Whisper-large encoder [16], an advanced model that specializes in speech recognition and speech translation.\nAlthough state-of-the-art pre-trained audio encoders can be utilized, they are typically pre-trained on specific tasks and datasets, and may not have sufficient knowledge and capacity for new tasks and datasets required for general-purpose AudioLLMs. To increase encoder capacity for feature extraction, we propose to incorporate mixtures of 'weak' encoders (MoWE) to the AudioLLM framework to supplement the 'strong' base encoder. We use the terms 'weak' and 'strong' encoders to loosely correspond to the concepts of 'weak' and 'strong' learners in machine learning. We define 'weak' encoders as those with at least an order of magnitude fewer parameters than the base encoder, and may have lower embedding quality measured by downstream task performance as in Table I. This design choice is made so as to not excessively increase model size. To constrain the number of active parameters size, we propose routing strategies to selectively activate a subset of weak encoders from an encoder pool for each data sample. Embeddings from the activated encoders are concatenated to be further processed in the AudioLLM pipeline. We demonstrate empirically that MoWE effectively improves multi-task performance."}, {"title": "II. RELATED WORKS", "content": "We focus on the line of work that address audio tasks through an end-to-end pipieline that connects audio encoders"}, {"title": "A. AudioLLMS", "content": "We focus on the line of work that address audio tasks through an end-to-end pipieline that connects audio encoders"}, {"title": "III. PROPOSED METHOD", "content": "The AudioLLM framework we employ consists (1) a strong base audio encoder $\\mathbb{E}_{\\text{base}}$ and $M$ weak audio encoders $\\{E_k\\}_{k=1}^M$ to be activated by our proposed MoWE strategy, (2) a modality adapter to downsample the encoder embeddings and a linear projection to project the downsampled embeddings into the LLM input space, and (3) an LLM to generate free-form text. The weak encoders are included to supplement the base encoder to learn new datasets and tasks. An overview of the framework is illustrated in Figure 1a.\nWe denote the data as $(A,T,Y)$, where $A$ is the input audio, $T$ is the input text instruction, and $Y$ is the output text response. For an audio input $a_i \\in A$ corresponding to the $i$-th sample, the base encoder produces embeddings $z_{i,\\text{base}} = \\mathbb{E}_{\\text{base}}(a_i)$, and a subset of the weak encoders are activated by our proposed MoWE strategy to produce the embeddings $z_{i,\\text{MoWE}} = \\text{MoWE}(a_i)$, further described in Section III-B. The embeddings are concatenated along the feature dimension as $z_i = z_{i,\\text{base}} \\oplus_f z_{i,\\text{MoWE}}$. Note that the embeddding sequence length and subsequently number of tokens are not increased. We obtain audio tokens $\\text{token}_{a_i} = \\text{proj}(\\text{adapter}(z_i))$, and text instruction tokens $\\text{token}_{t_i} = \\text{tokenizer}(t_i)$ from the text instruction $t_i \\in T$. The tokens are concatenated along the sequence dimension as $\\text{token}_i = \\text{token}_{a_i} \\oplus_s \\text{token}_{t_i}$, and input into the LLM to generate a text response $\\hat{y}_i = \\text{LLM}(\\text{token}_i)$.\nDuring training, we initialize the audio encoders and LLM with pre-trained model weights. The encoders, MoWE routers, adapter, linear projection layer and a light-weight LoRA module [24] inserted into the LLM are finetuned. The model is trained with the standard next-token prediction objective $\\mathcal{L}_{\\text{next-token}}$ and a MoWE routing loss $\\mathcal{L}_{\\text{MOWE}}$:"}, {"title": "A. AudioLLM Framework", "content": "The AudioLLM framework we employ consists (1) a strong base audio encoder $\\mathbb{E}_{\\text{base}}$ and $M$ weak audio encoders $\\{E_k\\}_{k=1}^M$ to be activated by our proposed MoWE strategy, (2) a modality adapter to downsample the encoder embeddings and a linear projection to project the downsampled embeddings into the LLM input space, and (3) an LLM to generate free-form text. The weak encoders are included to supplement the base encoder to learn new datasets and tasks. An overview of the framework is illustrated in Figure 1a.\nWe denote the data as $(A,T,Y)$, where $A$ is the input audio, $T$ is the input text instruction, and $Y$ is the output text response. For an audio input $a_i \\in A$ corresponding to the $i$-th sample, the base encoder produces embeddings $z_{i,\\text{base}} = \\mathbb{E}_{\\text{base}}(a_i)$, and a subset of the weak encoders are activated by our proposed MoWE strategy to produce the embeddings $z_{i,\\text{MoWE}} = \\text{MoWE}(a_i)$, further described in Section III-B. The embeddings are concatenated along the feature dimension as $z_i = z_{i,\\text{base}} \\oplus_f z_{i,\\text{MoWE}}$. Note that the embeddding sequence length and subsequently number of tokens are not increased. We obtain audio tokens $\\text{token}_{a_i} = \\text{proj}(\\text{adapter}(z_i))$, and text instruction tokens $\\text{token}_{t_i} = \\text{tokenizer}(t_i)$ from the text instruction $t_i \\in T$. The tokens are concatenated along the sequence dimension as $\\text{token}_i = \\text{token}_{a_i} \\oplus_s \\text{token}_{t_i}$, and input into the LLM to generate a text response $\\hat{y}_i = \\text{LLM}(\\text{token}_i)$.\nDuring training, we initialize the audio encoders and LLM with pre-trained model weights. The encoders, MoWE routers, adapter, linear projection layer and a light-weight LoRA module [24] inserted into the LLM are finetuned. The model is trained with the standard next-token prediction objective $\\mathcal{L}_{\\text{next-token}}$ and a MoWE routing loss $\\mathcal{L}_{\\text{MOWE}}$:\n$\\mathcal{L} = \\mathcal{L}_{\\text{next-token}} + 0.1 * \\mathcal{L}_{\\text{MOWE}}. \\tag{1}$"}, {"title": "B. Mixture of Weak Encoders", "content": "Given a pool of $M$ weak encoders $\\{E_k\\}_{k=1}^M$, as illustrated in Figure 1b, we select and activate a subset of the encoders per sample based on (i) a data-independent router, and (ii) a data-dependent router.\nData-independent routing: The data-independent router selects a fixed weak encoder irregardless of the input sample, with the purpose of supplementing the overall representation learning capacity of the base encoder. For an audio input $a_i$,\n$r_{\\text{indep}} = \\text{KeepTop1}(\\text{Softmax}(W_{\\text{indep}})) \\tag{2}$\n$z_{i, \\text{indep}} = \\sum_k r_{\\text{indep}}[k] * E_k(a_i) \\tag{3}$\nwhere $W_{\\text{indep}}, \\text{KeepTop1}(v) \\in \\mathbb{R}^M$, with $\\text{KeepTop1}(v)[j] = v[j]$ for $j = \\text{argmax}(v)$ and 0 otherwise.\nData-dependent routing: The data-dependent router selects a weak encoder depending on the input audio to incorporate detailed, data-specific features. For an audio input $a_i$,\n$r_{i,\\text{dep}} = \\text{KeepTop1}(\\text{Softmax}(z_{i,\\text{base}}W_{\\text{dep}})) \\tag{4}$\n$z_{i,\\text{dep}} = \\sum_k r_{i,\\text{dep}}[k] * E_k(a_i) \\tag{5}$\nwhere $z_{i,\\text{base}} \\in \\mathbb{R}^{1 \\times d_{\\text{base}}}$ is the base encoder embeddings $z_{i,\\text{base}}$ averaged across the sequence dimension, and $W_{\\text{dep}} \\in \\mathbb{R}^{d_{\\text{base}} \\times M}$. The vector $r_{i,\\text{dep}}$ weigh the contribution of each weak encoder. During training, we smooth the weight $r_{i,\\text{dep}}$ by $r_{i,\\text{dep}} = 0.9 * r_{i,\\text{dep}} + 0.1 * \\epsilon$ with $\\epsilon = 0.1/M$ to prevent training from being heavily biased towards specific encoders. Embeddings from the two routers are concatenated across the feature dimension to form $z_{i,\\text{MoWE}} = z_{i,\\text{dep}} \\oplus_f z_{i,\\text{indep}}$. The routers are trained alongside other model weights using\n$\\mathcal{L}_{\\text{MOWE}} = \\frac{1}{2} [\\mathcal{L}_{\\text{indep-ent}} + (\\mathcal{L}_{\\text{dep-ent}} + \\mathcal{L}_{\\text{dep-div}})] \\tag{6}$\nwhere\n$\\mathcal{L}_{\\text{indep-ent}} = -\\frac{1}{B} \\sum_i r_{\\text{indep}} * \\log(r_{\\text{indep}}) \\tag{7}$\n$\\mathcal{L}_{\\text{dep-ent}} = -\\frac{1}{B} \\sum_i r_{i,\\text{dep}} * \\log(r_{i,\\text{dep}}) \\tag{8}$\n$\\mathcal{L}_{\\text{dep-div}} = -\\sum_k \\hat{r}_{\\text{dep}} * \\log(\\hat{r}_{\\text{dep}}) \\tag{9}$\nwith $\\hat{r}_{\\text{dep}} = \\sum_{i=1}^B r_{i,\\text{dep}}$ being $r_{i,\\text{dep}}$ averaged across a batch of size $B$. The entropy losses in Equation 7 and 8 encourages confident routing decisions, and the diversity loss in Equation 9 prevents data-dependent router decisions from collapsing to a single encoder choice."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We demonstrate the effectiveness of our method on a set of 5 popular speech and audio tasks, with one representative dataset for each task as summarized in Table II: LibriSpeech-Clean [28] for Automatic Speech Recognition (ASR), MELD-Emotion [29] for Emotion Recognition (ER), Clotho-AQA [30] for Audio Question Answering (AQA), Spoken-SQuAD [31] for Speech Question Answering (SQA), and AudioCaps (AC) [32] for Audio Captioning. Audio inputs are trimmed or padded to 30-second length. We perform evaluations with AudioBench [33]. ASR is evaluated with word error rate (WER), AC is evaluated with METEOR, and the quality of ER, AQA, SQA and AC outputs are graded by Llama-3-70B-Instruct [34] on a 0-5 scale, with a higher score reflecting outputs that more closely resemble the ground-truth."}, {"title": "A. Tasks and Datasets", "content": "We demonstrate the effectiveness of our method on a set of 5 popular speech and audio tasks, with one representative dataset for each task as summarized in Table II: LibriSpeech-Clean [28] for Automatic Speech Recognition (ASR), MELD-Emotion [29] for Emotion Recognition (ER), Clotho-AQA [30] for Audio Question Answering (AQA), Spoken-SQuAD [31] for Speech Question Answering (SQA), and AudioCaps (AC) [32] for Audio Captioning. Audio inputs are trimmed or padded to 30-second length. We perform evaluations with AudioBench [33]. ASR is evaluated with word error rate (WER), AC is evaluated with METEOR, and the quality of ER, AQA, SQA and AC outputs are graded by Llama-3-70B-Instruct [34] on a 0-5 scale, with a higher score reflecting outputs that more closely resemble the ground-truth."}, {"title": "B. Implementation", "content": "In our experiments, we primarily use Whisper-large-v3 [16] with 637M parameters as the strong base encoder, a linear layer plus GELU activation as the adapter to downsample encoder embeddings to 100 tokens, and Llama-3-8B-Instruct [34] as the LLM. We also demonstrate method effectiveness on other choices of LLM such as Zephyr-7B-B [35] and a smaller Phi-3-Mini-4K-Instruct [36] with 3.8B parameters. In Section V-A, we implement MoWE as a mixture of 4 Whisper-tiny encoders, each with 9M parameters. The parameter $W_{\\text{indep}}$ is randomly initialized using a standard Gaussian distribution. In Section V-B and V-C, we use a more diverse mixture of 2 Whisper-tiny and 2 HuBERT encoders, one of which is a HuBERT-base [18] and the other is a task-specific ER-finetuned encoder [20] (denoted here as HuBERT-base-ER) with 95M parameters each. HuBERT features are linearly interpolated to Whisper-tiny's feature dimensions. We set a prior on the data-independent routing parameter $W_{\\text{indep}}$ to preferentially select the HuBERT-base-ER i.e. with value 1 for the encoder and -1 otherwise, since ER is a known task in our setup. We conduct multi-task training with batch size 32 for 5 epochs. We used AdamW optimizer with $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$, a learning rate of $5 \\times 10^{-5}$ with cosine scheduler."}, {"title": "V. RESULTS AND ANALYSIS", "content": "From Table III, MoWE improves the overall model perfor-mance on the evaluated tasks across 3 LLMs tested. Although the Whisper-tiny encoders in MoWE have the same initializa-tions and have much smaller size (9M parameters) compared to the strong base encoder (637M parameters), they are still useful in increasing model capacity to learn new tasks.\nWe perform further analysis on the design of MoWE routers in Table IV. With a single mixture, a data-dependent router achieves better performance than a data-independent router, demonstrating the effect of specialization. With two mixtures, having two data-dependent routers perform suboptimally as they may introduce excessive dynamic changes in the encoder embeddings. A combination of data-dependent router and data-indepdent router performs the best."}, {"title": "A. Mixture of Uniform Encoders", "content": "From Table III, MoWE improves the overall model perfor-mance on the evaluated tasks across 3 LLMs tested. Although the Whisper-tiny encoders in MoWE have the same initializa-tions and have much smaller size (9M parameters) compared to the strong base encoder (637M parameters), they are still useful in increasing model capacity to learn new tasks.\nWe perform further analysis on the design of MoWE routers in Table IV. With a single mixture, a data-dependent router achieves better performance than a data-independent router, demonstrating the effect of specialization. With two mixtures, having two data-dependent routers perform suboptimally as they may introduce excessive dynamic changes in the encoder embeddings. A combination of data-dependent router and data-indepdent router performs the best."}, {"title": "B. Mixture of Diverse Encoders", "content": "We conducted experiments under a single-stage multi-task training regime and a two-stage training regime where the first stage trains only on the LibriSpeech dataset for ASR to initially align the encoders and LLM, and the second stage trains on all datasets and tasks. Expectedly, the model focuses more on the ASR task with two-stage training. From Table V, we observe that MoWE with a diverse mixture of weak encoders is effective in increasing model performance in both training regimes. In particular, with the inclusion of HuBERT-base-ER in the encoder pool, performance on the ER task is 1.69 and 1.71 under the single-stage and two-stage training regime respectively, higher than the 1.63 achieved in Table III where the encoder pool consists of only Whisper-tiny.\nIn Figure 2, for each dataset, we plot the proportion of times each encoder is selected and activated by the data-dependent router during evaluation. Note that since HuBERT-base-ER is already selected by the data-independent router, it is rarely selected again. We observe evident specialization of the en-coders. LibriSpeech-Clean and Spoken-SQUAD consist speech mostly in neutral and consistent tone, and are mainly processed by two indiviudal Whisper-tiny encoders that are pre-trained to specialize in ASR. MELD-Emotion consists utterances with varied emotions, and Clotho-AQA and AudioCaps consist non-speech audio, and these datasets are processed by HuBERT-base pre-trained for representation learning by self-supervised learning methods."}, {"title": "C. Comparison with Models with Large-Scale Training", "content": "In Table VI, we verify that that an AudioLLM with MoWE can obtain task performance competitive with state-of-the-art models described in Section II. We applied SpecAugment [37] to increase data diversity on the 5 selected datasets and used a Conv1D adapter with kernel and stride size 8 to less rigorously downsample the encoder embeddings. We note that although the state-of-the-art models are trained with larger-scale data, MoWE outperformed in ER, AQA and AC."}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed MoWE, a novel approach that integrates a mixture of weak encoders into the AudioLLM framework. MoWE supplements the base encoder by expanding its feature extraction capacity and capabilities without significantly increasing model size. By incorporating a diverse set of encoders, the approach encourages encoder specialization and the learning of data-specific features. Moreover, MoWE demonstrated improved performance in multi-task settings and achieves competitive results when compared to state-of-the-art models."}]}