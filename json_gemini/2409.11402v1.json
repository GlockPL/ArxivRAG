{"title": "NVLM: Open Frontier-Class Multimodal LLMs", "authors": ["Wenliang Dai", "Nayeon Lee", "Boxin Wang", "Zhuoling Yang", "Zihan Liu", "Jon Barker", "Tuomas Rintamaki", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-40) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training.\nIn terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities.\nTo advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [11] have laid the foundation for the rapid progress in AI recently. Since the introduction of ChatGPT [104], LLMs have revolutionized the text domain and are becoming universal task solvers for natural language processing, math and coding problems. Simultaneously, multimodal LLMs (MLLMs) [4; 107], which bridge the physical world with language models, have gained significant traction. The release of GPT-4V [107] has sparked a competitive race in the development of proprietary multimodal LLMs for vision-language intelligence [35; 108; 5; 6; 153; 154; 122]. However, the model architectures, training data, and methods used to build these proprietary models remain undisclosed, preventing the research community from building upon them.\nA notable feature of leading proprietary multimodal LLMs is their exceptional performance on both multimodal and text-only tasks, a quality we refer to as production-grade multimodality [108; 35; 36]. For example, GPT-40 is a single neural network trained end-to-end on text and images, achieving state-of-the-art results in both text-only and vision-language tasks [110]. This unified approach simplifies deployment by eliminating the need to route different input modalities to separate LLMs, offering users a seamless experience for switching between modalities without losing text or multimodal context.\nThe community has made significant progress in advancing the capabilities of open-access multimodal LLMs [26; 79; 18; 71; 139]. Notable families of open models include BLIP [66; 67; 26], LLaVA [79; 78; 80; 65], InternVL [19; 18; 111], and Llama 3-V [82]. The most common architectures used to build these multimodal LLMs are the decoder-only architecture (e.g., LLaVA [79] and InternVL [18]), which processes image tokens within the LLM self-attention layers, and the cross-attention-based architecture (e.g., Flamingo [4] and Llama 3-V [82]), which handles image tokens through LLM cross-attention layers."}, {"title": null, "content": "However, the previous studies of multimodal LLMs have several limitations:\n\u2022 In contrast to the convergence of model architectures to build LLM in the text domain [11; 133; 105; 5; 35], i.e., the decoder-only transformer [143], existing multimodal LLM architectures (e.g., decoder-only vs. cross-attention models) have not been studied and compared in an apples-to-apples manner. There is no information regarding the architectures of proprietary models. Furthermore, studies on open-access models differ in their choice of LLM backbones, vision encoders, and, most importantly, training data, making direct comparisons challenging. For these reasons, IDEFICS-80B, an open-access reproduction of Flamingo [62] based on LLaMA-65B [140], is perceived as significantly lagging behind LLaVA-1.5-13B [78], which is based on Vicuna-13B [21], in VQA tasks.\n\u2022 Model designs that handle high-resolution image input (e.g, dynamic high-resolution [80; 30; 18]) significantly boost performance on OCR-related tasks (e.g., OCRBench [81]), but sometimes show reduced accuracy on reasoning-related tasks (e.g., MMMU [166]) compared to their low-resolution counterparts.\n\u2022 Although open-access multimodal LLMs achieve impressive benchmark results on vision-language tasks, we observe a significant degradation in text-only performance (see Table 8), unlike leading proprietary models (e.g., GPT-40). The only work that provides substantial technical details addressing this issue is Llama 3-V [82], which freezes the LLM parameters and trains only the cross-attention layers. However, these models have not yet been made publicly available.\nTo address these limitations, we introduce NVLM-1.0, a family of frontier multimodal LLMs (see Figure 2 for a comparison with leading models) featuring three distinct architectures: i) NVLM-D, a Decoder-only architecture, ii) NVLM-X, a cross (X)-attention-based architecture, and iii) NVLM-H, a novel Hybrid architecture. Trained on the same curated data blend, all three architectures achieve state-of-the-art performance, rivaling leading proprietary and open-access models, while offering practitioners flexible and feature-rich model options. Specifically, we make the following contributions:\n1. Model architecture: We compare the pros and cons of the decoder-only and the cross-attention-based models using the same LLM backbones, vision encoder, and well-curated training data. Our findings show that the cross-attention-based NVLM-X offers superior computational efficiency when handling high-resolution images, whereas the decoder-only NVLM-D provides unified multimodal reasoning and achieves higher accuracy in OCR-related tasks. Building on these insights, we propose NVLM-H, a novel hybrid architecture that excels in multimodal reasoning while also delivering improved computational efficiency for high-resolution images.\n2. High-resolution: To achieve strong accuracy on both OCR-related tasks (e.g., OCRBench [81]) and multimodal reasoning tasks (e.g., MMMU [166]), we propose a tile-tagging design for the dynamic tiling of high-resolution image inputs. Through comprehensive ablation studies, we find that adding a text-based 1-D tile tag before the image tokens of the corresponding tile in the decoder achieves the best accuracy.\n3. Training data: We meticulously collect and provide detailed information on our multimodal pretraining and supervised fine-tuning (SFT) datasets, which will support and benefit future research. In the dataset selection and filtering process, we find that the data quality and task diversity are more important than the scale, even during the pretraining stage. Furthermore, previous studies have shown that abundant and diverse multimodal pretraining data is crucial for the success of cross-attention-based models, such as Flamingo [4]. In this work, we found that such pretraining data can also significantly improve the performance of decoder-only models, like LLaVA [78], even with a simplified design that involves training only an MLP projection layer during pretraining. For the curation of SFT data, we collected a much larger set of task-oriented datasets compared to previous studies [18].\n4. Production-grade multimodality: We develop production-grade multimodality for NVLM models, enabling them to excel in both vision-language tasks (e.g., multimodal reasoning, OCR, natural image understanding) and text-only tasks (e.g., multidisciplinary knowledge reasoning, coding, and math). To maintain text-only performance during multimodal training, we investigate two approaches: i) For the cross-attention-based NVLM-X, we find that freezing the LLM's parameters and training only the cross-attention layers [4] during both the pretraining and SFT stages works reasonably well, with a moderate performance trade-off on vision-language tasks."}, {"title": null, "content": "ii) We curate a high-quality text-only dataset and integrate it into the multimodal SFT stage, effectively preserving text-only performance with no degradation, and even achieving noticeable improvements on text-only math and coding benchmarks after multimodal training across all NVLM models. We attribute this to the superb quality of text-only data and the significant amount of multimodal math data (e.g., geometry) incorporated into multimdoal SFT blend, which improves NVLM's reasoning capabilities, regardless of modality.\nWe organize the rest of this paper as follows. In \u00a7 2, we present a qualitative study of our model's capabilities, showcasing generated samples. In \u00a7 3, we introduce the preliminaries of multimodal LLMs and discuss related work. In \u00a7 4, we present the NVLM-1.0 model family, followed by details on the training data in \u00a7 5. We introduce the evaluation benchmarks and report results in \u00a7 6. We conclude the paper in \u00a7 7."}, {"title": "2 Qualitative Study", "content": "We conduct a qualitative analysis of NVLM-1.0 with diverse images and instructions. As illustrated in Figure 1, NVLM-1.0 can handle diverse types of images including memes in Figure 1 (a), object-centric images in Figure 1 (b), real-world scene images in Figure 1 (c), hand-written pseudo code in Figure 1 (d), table in Figure 1 (e), and charts in Figure 1 (f).\nOur NVLM-D1.072B demonstrates versatile capabilities in various multimodal tasks by jointly utilizing OCR, reasoning, localization, common sense, world knowledge, and coding ability. For instance, our model can understand the humor behind the \u201cabstract vs. paper\" meme in Figure 1 (a) by performing OCR to recognize the text labels for each image and using reasoning to grasp why juxtaposing \"the abstract\" \u2013 labeled with a fierce-looking lynx - and \"the paper\" labeled with a domestic cat \u2014 is humorous. NVLM accurately performs localization to effectively answer location-sensitive questions, such as \u201cWhat is the difference between the left, middle, and right objects in the image?\" in Figure 1 (b). NVLM is capable of performing mathematical reasoning and coding based on visual information, such as tables and handwritten pseudocode, as illustrated in Figure 1 (d) and (e). For more examples, refer to Appendix A."}, {"title": "3 Preliminaries", "content": "Vision language models [120; 9; 149; 4; 146; 174; 17; 145; 169] build the connection between the visual world and open text domain. Among these works, the multimodal LLMs augmented from pretrained large language models (LLMs) [4; 7; 158; 66; 67; 26; 79; 78; 80; 65; 19; 18; 111; 71; 147; 8; 15; 139; 159] have become visual assistants and universal task solvers for various vision-language tasks, including image / video captioning [72; 157], visual understanding and reasoning [166], chart and diagram-related QA [93], math reasoning in visual context [87], and optical character recognition (OCR) [81]."}, {"title": "3.1 Essential Building Blocks", "content": "Multimodal LLM typically consists of two indispensable components: large language model (LLM) and vision encoder."}, {"title": "Large Languge Model", "content": "A multimodal LLM typically builds upon a text-only LLM for initialization. While there are exceptions where multimodal LLMs are pretrained from scratch using multimodal data [35; 3], these approaches, though conceptually compelling, lack clear evidence of superior performance in vision-language tasks compared to multimodal LLMs built on a text-only LLM.\nInstruction-tuned LLMs [150; 23; 115] serve as universal task solvers in the text domain, as they can follow user-provided instructions to address a variety of tasks. As a result, it is common to build multimodal LLMs on instruction-tuned LLMs rather than base LLMs in previous studies, [65; 18; 71; 139; 82], as the instruction-following capability is essential for solving a wide range of vision-language tasks. Various instruction-tuned LLMs have been used to build multimodal LLMs in different study, including Vicuna-1.5 [21], LLaMA-2-Chat [141], Mistral 7B [50], Yi-34B [161], Llama3-Instruct [82], and Qwen2-Instruct [119]. In this work, we use Qwen2-72B-Instruct [119]"}, {"title": null, "content": "as the default text-only LLM backbone. We also employ Nous-Hermes-2-Yi-34B [102] for ablation study and faster experimentation."}, {"title": "Vision Encoder.", "content": "Multimodal LLMs [e.g., 4; 65; 67; 19] typically leverage pretrained vision en-coders (e.g., CLIP [120]) to extract visual features from input images or video frames, with only a very few exceptions [3]. These vision encoders [120; 47; 19; 167; 27] are often trained on large-scale, diverse, and noisy text-image pairs sourced from the web [124; 12; 33]. This allows for large-scale training and enhances the generalization needed to effectively process visual input in unseen domains. The other types of datasets, such as those used for optical character recognition (OCR) [19] and image segmentation [58], are also incorporated to enhance the specific capabilities of vision encoders. In this study, we use InternViT-6B [19] as the default vision encoder due to its strong performance. We keep this vision encoder frozen at all stages of training, as this simplifies the training process while still delivering strong results."}, {"title": "3.2 Architectural Designs", "content": "There are various architectural designs for constructing multimodal LLMs (MLLMs) using existing LLMs and vision encoders [4; 66; 79; 147; 8]. We discuss the two most common architectures."}, {"title": "Decoder-only MLLMs.", "content": "Decoder-only architectures are popular mainly for their simplicity and unified handling of all modalities by aligning other modality tokens into the text token embedding space. It also facilitates the extension to generating other modalities [35; 108]. The notable examples of decoder-only multimodal LLMs include LLaVA [79; 78; 80; 65], InternVL [19; 18; 111], and Cambrian-1 [139]. In these models, image tokens from the vision encoder are projected into the text-embedding space via a projector module, e.g., position-wise multi-layer perceptron (MLP), and then directly fed into the decoder-only LLM, just like the text tokens. Some variants, such as Qwen-VL [8], utilize more advanced modules, e.g., Perceiver [48], to down-sample the image tokens before they are fed into the LLM.\nTraining decoder-only multimodal LLMs typically involves two stages: pretraining and supervised fine-tuning (SFT). At the start of pretraining, the randomly initialized MLP or projector module needs to be trained while keeping the LLM frozen to avoid disrupting the LLM's weights [79; 80]. Related work has also shown cases where both the projector and vision encoder are jointly trained during the pretraining stage [18; 8]. Due to the limited capacity of the MLP or projector module, the LLM need to be unfrozen during multimodal supervised fine-tuning (SFT) to achieve good performance on vision-language tasks [71]. The vision encoder is typically kept frozen during the SFT stage. There are some exceptions, though, where the entire multimodal LLM is trained end-to-end [65], usually with smaller vision encoder [167]."}, {"title": "Cross-attention-based MLLMs.", "content": "Cross-attention-based architectures are similar to encoder-decoder transformer models for machine translation [143], where the text decoder processes flattened image tokens via cross-attention layers, treating them as if they were a foreign language. One of the early successful cross(X)-attention architectures is Flamingo [4], which is built on frozen pretrained LLMs [42] and often serves as the starting point for many studies on this type of model. The Flamingo model has two sets of trainable modules: i) a perceiver resampler [48] positioned after the frozen vision encoder [120], which is designed to down-sample the vision encoder output to a specified size of representations, and ii) the gated x-attention layers interleaved with frozen LLM layers, which read output representations from the perceiver resampler. In contrast, our NVLM-1.0-X and the concurrent Llama 3-V [82] models utilize only gated cross-attention layers to process image tokens and do not include the Perceiver module.\nThe Flamingo model was trained in two stages: 1) pretraining with a large (and possibly noisy) set of image-text pairs or interleaved image-text data, and 2) supervised fine-tuning (SFT) with high-quality data. It always freezes self-attention layers in LLM decoder and only trains cross-attention layers and perceiver during both pretraining and supervised fine-tuning (SFT) to maintain text-only performance. At inference time, the gate of the X-attention layers can be turned ON for multimodal tasks and OFF for text-only tasks. Thanks to the frozen LLM and gated X-attention designs, the text-only performance is guaranteed not to degrade after multimodal training. The follow-up work includes IDEFICS [62] and OpenFlamingo [7], which are open-source reproductions of Flamingo."}, {"title": null, "content": "In contrast to decoder-only models, cross-attention-based MLLMs are generally considered more complex to implement. This complexity arises from the introduction of additional modules, the need for proper cross-attention masking in interleaved image-text settings, and the significantly heavier pretraining data requirements [4; 62; 176]. However, a notable advantage of the X-attention-based architecture is its computational efficiency, as it does not require unrolling all image tokens in the LLM decoder, which typically results in long sequences during both training and inference, especially for high-resolution images. See \u00a74.3 for further study."}, {"title": "3.3 High-Resolution Inputs", "content": "Properly handling high-resolution images is crucial for achieving strong performance in many OCR-related tasks. However, vision encoders are typically trained with static resolution of 224\u00b2 or 336\u00b2 pixels for efficiency [120; 100], when the image patch size per token is usually 14\u00b2 or 16\u00b2. For example, feeding a 224\u00b2 image to ViT-L/14 (patch size 14\u00b2) results in $(\\frac{224}{14})^2$ = 256 tokens. There are specialized vision encoders that can directly handle static high-resolution images. For instance, the SAM encoder [58], designed for image segmentation, can process images of 1024\u00b2 pixels with a ViT-L/16 backbone (16\u00b2 pixels per patch), producing a 4096-token output. This can be costly, especially when training datasets and downstream tasks contain a mix of low-resolution and high-resolution images.\nThe dynamic high-resolution mechanism [160; 80; 30; 18] has been proposed to address the waste of compute in such scenarios. For example, given a ViT-L/14 vision encoder trained on low-resolution images (e.g., 224\u00b2), a high-resolution image (e.g., 896 \u00d7 672) is divided into tiles based on the aspect ratio and resolution of the input image ($\\frac{224}{896} X 672 = 12$ tiles in this case). Each tile is independently fed into the ViT-L/14, producing 256 tokens per tile and 3072 tokens in total. Meanwhile, it only produces 512 tokens for an input image with 448 \u00d7 224 resolution. This dynamic approach is particularly well-suited for multimodal LLMs, which need to handle different types of tasks with varying image resolutions."}, {"title": "4 NVLM: Models and Training Methods", "content": "In this section, we introduce NVLM-1.0, a family of frontier-class multimodal LLMs featuring three architectures: i) Decoder-only NVLM-D, ii) Cross (X)-attention based NVLM-X, and iii) NVLM-H with Hybrid architecture. Figure 3 illustrates these architectures. We will begin by detailing the vision pathway shared by all NVLM models."}, {"title": "4.1 Shared Vision Pathway", "content": "Several studies have compared various vision encoders in multimodal LLMs, suggesting that unfreez-ing and combining multiple smaller vision encoders offer advantages [139]. In this work, we employ a single, large, and powerful vision encoder, InternViT-6B-448px-V1-5 [113; 18], as the default for all three architectures, keeping it frozen throughout all training stages. It processes images at a fixed resolution of 448\u00b2, generating 1,024 output tokens.\nWe use the similar dynamic high-resolution (DHR) approach outlined in Chen et al. [18]. See the left part of Figure 3 for an illustration. We allow a maximum of 6 tiles at training. Thus, the predefined aspect ratios are: {1:1, 1:2, 1:3, 1:4, 1:51:6, 2:1, 2:22:33:13:24:15:1, 6:1}, encompassing all possible combinations of aspect ratios formed by 1 to 6 tiles. For each input image, we dynamically match it to a predefined aspect ratio and divide it into 1 to 6 tiles, each corresponding to 448\u00d7448 pixels, based on the image's resolution. We include a thumbnail tile, which is a scaled-down version of the entire image to capture the global context. Each tile is then fed into InternViT-6B-448px-V1-5 [113], generating 1,024 tokens. We apply a downsampling operation to reduce the 1,024 image tokens to 256, reducing the processing overhead for the LLM. This operation groups four neighboring image tokens into one by concatenating them along the channel dimension, a.k.a. pixel shuffle [18]. See Figure 4 for a detailed illustration of this process.\nThis dynamic high-resolution (DHR) design significantly improves performance on OCR-related tasks [18; 30], but sometimes results in degraded performance on reasoning-related tasks [166] when all image tokens from the tiles are simply concatenated and fed directly into the LLM. We will address this issue across the three architectures, respectively."}, {"title": "4.2 NVLM-D: Decoder-only Model", "content": "Similar to previous decoder-only multimodal LLMs [79; 18], NVLM-D model connects the pretrained vision encoder to the LLM using a 2-layer MLP as the projector or modality-alignment module.\nTraining NVLM-D involves two stages: pretraining and supervised fine-tuning (SFT). The MLP is randomly initialized and needs to undergo pretraining first, with both the vision encoder and LLM backbone kept frozen. In our early exploration, we found that joint pretraining of the MLP projector and vision encoder is beneficial when the vision encoder is relatively weak (e.g., ViT-L/14 [100]) and the pretraining datasets are sufficiently diverse. However, after upgrading to the more powerful InternViT-6B-448px-V1-5 [113], the performance gains became marginal. Consequently, we opt to keep the vision encoder frozen during pretraining for the sake of simplicity. During the SFT stage, both the MLP projector and LLM are trained to learn new vision-language tasks with novel instructions, while the vision encoder remains frozen. However, a less frequently discussed point in decoder-only MLLM literature is that leaving the LLM unfrozen during multimodal SFT training often results in significant degradation in text-only performance. Our NVLM-D model effectively maintains text-only performance by incorporating a high-quality text-only SFT dataset. The model configuration and training details for NVLM-D models are in \u00a7 4.5."}, {"title": "Tile Tag for Dynamic High-Resolution.", "content": "As illustrated in Figure 3, the LLM backbone needs to process the flattened image tokens from all dynamic high-resolution tiles, including an additional thumbnail tile. Directly concatenating flattened tokens without delimiters could confuse the LLM, as LLM lacks prior knowledge of the dynamic tiling process. To address this, we insert a text-based tile tag in the input sequence to signal the start of a tile and the position of this tile within the whole tiling structure. After the tile tag, we append the flattened 256 image tokens of the tile. Note that our design differs from previous work [30], which globally flattens the image tokens from different tiles"}, {"title": "4.3 NVLM-X: X-attention Model", "content": "NVLM-X employs gated cross-attention to process image tokens and differs from the Flamingo model [4] in two key ways:\n\u2022 During our initial exploration, we found that while the perceiver resampler is beneficial for natural image captioning, it negatively impacts dense OCR tasks, such as transcribing text from scanned documents (see Appendix C for further details). The primary reason is that the cross-attention to latent array in the Perceiver [48] mixes the input image tokens, potentially disrupting the spatial relationships between image patches, which are crucial for document OCR. Based on this observation, our NVLM-X architecture does not use a perceiver resampler; instead, it relies solely on cross-attention to read image tokens directly from the vision encoder."}, {"title": "4.4 NVLM-H: Hybrid Model", "content": "Drawing inspiration from the comparison of NVLM-X and NVLM-D, we propose NVLM-H, a novel hybrid architecture that combines the best of both approaches. As illustrated in Figure 3, NVLM-H separates the processing of image tokens into two paths. The thumbnail image tokens are fed into the LLM alongside text tokens and processed by self-attention layers, enabling joint multimodal reasoning. Simultaneously, a dynamic number of regular tiles are processed through gated cross-attention, enabling the model to capture finer image details. This approach enhances high-resolution capability compared to NVLM-X while significantly improving computational efficiency compared to NVLM-D. Table 3 demonstrates that NVLM-H has higher throughput than NVLM-D in training."}, {"title": "Tile Tag for Dynamic High-Resolution.", "content": "NVLM-H utilizes the same 1-D flattened tile tag introduced in \u00a74.2 for NVLM-D. The primary distinction lies in the processing location. As shown in Figure 3, text embeddings of  are integrated into the gated cross-attention layers alongside visual embeddings. This approach is effective because the text and visual embeddings are well-aligned during pre-training, enabling the model to seamlessly interpret tile tags within the cross-attention mechanism. Consistent with the results in Table 1 and Table 2, adding tile tags enhances NVLM-H's performance on OCR-related tasks compared to no tagging."}, {"title": "4.5 Model Configurations and Training Method", "content": "We provide the model configurations and training details for all NVLM-1.0 models below."}, {"title": "Backbone LLMs and Vision Encoder.", "content": "For the NVLM-D, NVLM-X, and NVLM-H 72B models, we use Qwen2-72B-Instruct [119] as the backbone LLM. For computational reasons, we also use the smaller Nous-Hermes-2-Yi-34B [102] as the LLM backbone for faster ablation studies and experimentation. After finalizing the optimized designs, we shifted our computational resources to improving the NVLM-1.0 72B models. Across all NVLM models, InternViT-6B-448px-V1-5 [113] serves as the vision encoder."}, {"title": "Modality-Alignment Module.", "content": "We include the details of modality-alignment modules for three NVLM architechtures in the following:\n\u2022 For NVLM-D models, the LLM and vision encoder are connected by a two-layer MLP to align the modalities, with hidden dimensions of 12800 \u2192 20480 \u2192 7168 for 34B model, and 12800 \u2192 29568 8192 for 72B model. Note that InternViT-6B has a hidden dimension of 3200, which increases to 3200 \u00d7 4 = 12800 after applying pixel shuffle. Yi-34B [102] has hidden dimension 7168, and Qwen2-72B has hideen dimension 8192.\n\u2022 For NVLM-X models, the images features are first projected to LLMs' hidden dimension with a one-layer MLP, 12800 \u2192 7168 for 34B model, and 12800 \u2192 8192 for 72B model. We insert a gated X-attention layer every 6 and 8 LLM self-attention layers, respectively. This results in a total of 10 X-attention layers for both models.\n\u2022 The NVLM-H 34B and 72B models utilize a two-layer MLP and X-attention layers as the modality-alignment module. The image tokens for both thumbnail and regular tiles are projected through the two-layer MLP, with hidden dimensions of 12800 \u2192 20480 \u2192 7168 for the 34B model, and 12800 \u2192 29568 \u2192 8192 for the 72B model. The projected thumbnail image tokens are then directly fed into the LLM decoder. The projected image tokens of regular tiles are cross-attended by the X-attention layers. As with NVLM-X, ten gated X-attention layers are inserted for both the 34B and 72B models."}, {"title": "Training Method.", "content": "We employ a unified training method for all NVLM models. The training process involves two stages: i) Pretraining: we freeze both the LLM backbone and vision encoder for all models. We only train the modality-alignment modules, i.e., projector MLP or X-attention layers, using our pretraining dataset detailed in Table 4. For pretraining hyperparameters, one can refer to Table 10 in Appendix B. We find a large batch size of 2048 improves the pretraining with frozen LLMs. ii) Supervised fine-tuning (SFT): we keep the vision encoder frozen while training both the LLM and modality-alignment modules with our multimodal SFT datasets detailed in Table 6, along with a text-only SFT dataset. For hyperparameters of SFT, one can refer to Table 11 in Appendix B."}, {"title": "5 Training Data", "content": "In this section, we provide details of the pretraining and supervised fine-tuning (SFT) datasets. These curated training datasets are used across all three architectures in the NVLM family. All datasets are formatted based on the task type and the chat template provided in Appendix E."}, {"title": "5.1 Multimodal Pretraining Data", "content": "We curate a diverse", "33": "commonly used for training CLIP-style vision encoders. However", "176": "and OBELICS [63", "tasks": 1, "66": ".", "4": "has shown that abundant and diverse pretraining data is crucial for the success of cross-attention-based models. In contrast, decoder-only models, such as LLaVA [78;"}]}