{"title": "Visual SLAMMOT Considering Multiple Motion Models", "authors": ["Peilin Tian", "Hao Li"], "abstract": "Simultaneous Localization and Mapping (SLAM) and Multi-Object Tracking (MOT) are pivotal tasks in the realm of autonomous driving, attracting considerable research attention. While SLAM endeavors to generate real-time maps and determine the vehicle's pose in unfamiliar settings, MOT focuses on the real-time identification and tracking of multiple dynamic objects. Despite their importance, the prevalent approach treats SLAM and MOT as independent modules within an autonomous vehicle system, leading to inherent limitations. Classical SLAM methodologies often rely on a static environment assumption, suitable for indoor rather than dynamic outdoor scenarios. Conversely, conventional MOT techniques typically rely on the vehicle's known state, constraining the accuracy of object state estimations based on this prior. To address these challenges, previous efforts introduced the unified SLAMMOT paradigm, yet primarily focused on simplistic motion patterns. In our team's previous work IMM-SLAMMOT[1], we present a novel methodology incorporating consideration of multiple motion models into SLAMMOT i.e. tightly coupled SLAM and MOT, demonstrating its efficacy in LiDAR-based systems. This paper studies feasibility and advantages of instantiating this methodology as visual SLAMMOT, bridging the gap between LiDAR and vision-based sensing mechanisms. Specifically, we propose a solution of visual SLAMMOT considering multiple motion models and validate the inherent advantages of IMM-SLAMMOT in the visual domain.", "sections": [{"title": "I. INTRODUCTION", "content": "IMULTANEOUS Localization and Mapping (SLAM) and Multi-Object Tracking (MOT) are two essential tasks in autonomous driving, garnering significant attention in recent years. SLAM aims to construct real-time maps and estimate the vehicle's pose in unknown environments, while MOT focuses on identifying and tracking multiple dynamic objects in real-time. These tasks are both crucial for achieving highly autonomous and intelligent vehicle systems.\nCurrently, SLAM and MOT are generally operated as two independent modules within an intelligent vehicle system. However, this paradigm imposes certain limitations on both of these tasks. On the one hand, classical SLAM frameworks are often based on the static environment assumption, which is suitable for indoor rather than outdoor scenarios. For autonomous driving, moving surrounding vehicles can interfere with the vehicle's localization and introduce unnecessary motion information into the constructed environmental map. On the other hand, prevalent MOT research typically treats the vehicle's state as known information and estimates object states based on this prior. In this case, the estimated object states are constrained by the accuracy of the vehicle's state. Therefore, in real-world driving scenarios, SLAM and MOT, as two modules requiring real-time operation, are intrinsically linked and mutually dependent.\nSome studies have attempted to implement SLAM and MOT as a unified framework and proposed the more general paradigm of SLAMMOT[2]. SLAM and MOT can respectively be regarded as reduced cases of SLAMMOT. This paradigm is more suitable for the common outdoor dynamic environments in autonomous driving and can enhance the performance of both SLAM and MOT simultaneously. However, previous methods often consider simple object motion patterns, such as a single constant velocity model. In realistic environments, environmental objects often exhibit complex motion states and switch between different patterns. At this point, simple motion models are insufficient to effectively describe the object's state. Therefore, in our team's previous work[1], we propose a methodology coined as Methodology Level 3 that consists in incorporating consideration of multiple motion models into SLAMMOT i.e. tightly coupled SLAM and MOT.\nIt is worth noting that SLAMMOT can be implemented based on various sensors. Common sensors include cameras and LiDARs. Many LiDAR-based SLAM and MOT methods"}, {"title": "II. RELATED WORK", "content": "can achieve good results. However, visual sensors inherently lack depth information, making tasks in 3D environments more challenging. Besides, intelligent vehicle systems based on different sensors exhibit significant differences at the algorithmic level. Our team's previous work[1] presents instantiation of the Methodology Level 3 as LiDAR SLAMMOT and demonstrates advantages of this methodology for LiDAR-based systems. Since LiDAR sensors and visual sensors are essentially different and the two kinds of sensor systems have completely different sensing mechanisms and properties, whether instantiating the Methodology Level 3 as visual SLAMMOT is indeed feasible and, if so, whether the Methodology Level 3 is indeed advantageous for visual SLAMMOT are two questions of which the answers are neither known nor evident.\nThis paper answers the questions, as its two contributions. First, this paper provides a solution of SLAMMOT taking advantage of the Methodology Level 3 to demonstrate feasibility of instantiating this methodology as visual SLAMMOT. Second, this paper demonstrates that advantages of the Methodology Level 3 indeed exist for visual SLAMMOT."}, {"title": "A. Visual SLAM", "content": "Visual SLAM methods typically consist of modules such as sensor data acquisition, frontend odometry, backend optimization, mapping, and loop closure. Common sensors include monocular cameras, stereo cameras, and RGB-D cameras.\nDifferent visual SLAM methods vary in the frontend visual odometry techniques. Some methods are based on feature points, requiring keypoint extraction and descriptor computation for each frame to match features between adjacent frames, facilitating subsequent camera localization and map construction. Typical methods of this paradigm include MonoSLAM[3], PTAM[4], and the ORB-SLAM series[5\u20137]. Among them, ORB-SLAM is based on ORB features, utilizes bundle adjustment for optimization, and features a comprehensive keyframe selection strategy. Moreover, ORB-SLAM incorporates a loop closure thread to enhance the global consistency of localization and mapping. Compared to previous feature-based visual SLAM methods, ORB-SLAM shows significant improvements in the SLAM task and is adaptable to various visual sensors as well as IMU. Some methods omit descriptor computation in visual odometry, relying instead on photometric consistency between pixel-level data in consecutive frames, and estimate the camera motion by minimizing photometric error. Such methods are referred to as direct or semi-direct methods, with representative works including DTAM[8], LSD-SLAM[9], SVO[10], and DSO[11]. Such visual SLAM methods are based on the static environment assumption. While this assumption holds true for indoor environments, it presents notable shortcomings in autonomous driving scenarios. Moving objects in the roadway environment can interfere with the localization and mapping of intelligent vehicles, which becomes a challenge that pure SLAM methods fail to effectively address.\nSLAM methods (or pure SLAM methods) form Methodology Level 0 that will be presented below. A specific SLAM method can serve as a concrete instantiation of Methodology Level 0 itself and can also serve as a concrete composing module of Methodology Level 1, Methodology Level 2, and Methodology Level 3 that will also be presented below."}, {"title": "B. Visual MOT", "content": "MOT is crucial for applications in dynamic environments. State-of-the-art MOT methods typically use deep learning models to estimate the 2D or 3D information of objects. 2D objects can be represented precisely by pixel-level masks, which is obtained using image segmentation models. 3D objects can be represented by 3D bounding boxes, which is achieved through 3D object detection and tracking models.\n2D Image Segmentation. Image segmentation aims to partition different regions within a 2D image. In recent years, deep learning-based image segmentation methods facilitate the analysis of semantic information in different parts of images. Currently, the main sub-directions of image segmentation include semantic segmentation, instance segmentation and panoptic segmentation. Semantic segmentation aims to predict the semantic category for each pixel, with common solutions based on Convolutional Neural Networks (CNNs) [12, 13]. Semantic segmentation can divide different regions in images according to semantic categories yet cannot distinguish between different instances of the same category. To address this issue, instance segmentation combines semantic segmentation with object detection, aiming to finely label each object with pixel-level masks. Some methods follow a top-down paradigm[14, 15], first obtaining object regions through detection mechanisms and then performing semantic segmentation within these regions. Other methods adopt a bottom-up paradigm, embedding each pixel and segmenting through clustering algorithms[16, 17]. Additionally, some models perform instance segmentation in a one-stage manner[18, 19], achieving higher efficiency. In recent years, the task of panoptic segmentation has gradually gained attention[20, 21]. Compared to instance segmentation, panoptic segmentation further segments areas in the image that do not belong to countable objects, enriching the algorithm's understanding of image content.\n3D Object Detection and Tracking. Visual sensors inherently lack depth information, making vision-based 3D perception tasks challenging. With the advancement of deep learning, numerous methods become able to infer 3D information of objects directly from 2D images. Firstly, 3D object detection necessitates acquiring information on the size, position, and orientation of objects in the environment. The configuration of different visual sensors determines the model's input, leading to various visual 3D object detection paradigms. Among these, models based on monocular cameras[22, 23] face significant challenges in achieving 3D object detection, yet they are less demanding on hardware requirements and exhibit advantages in real-time performance. Stereo cameras aid in inferring depth information from images, and such models[24, 25] can achieve higher detection accuracy. In recent years, Bird's-Eye View (BEV)-based multi-camera 3D object detection models [26, 27] have garnered attention. These methods can detect objects around the vehicle, not just limited to the front, and leverage"}, {"title": "C. Visual SLAMMOT", "content": "Bearing in mind limitations of the static environment assumption, researchers have attempted to integrate classical SLAM methods with MOT methods. Wang et al. [2] first define the problem of Simultaneous Localization, Mapping, and Moving Object Tracking (SLAMMOT) and implement the framework using traditional algorithms. In recent years, extensive work has focused on combining visual SLAM with deep learning models to enhance localization and mapping in dynamic environments, and to provide richer scene understanding to intelligent vehicles.\nHandling moving objects in dynamic environments is crucial for SLAMMOT methods. From the methodology perspective, existing SLAMMOT methods may be roughly categorized into four methodology levels[1]:\nMethodology Level 0: No cooperation between SLAM and MOT (namely to treat all objects as stationary in SLAM). Traditional SLAM methods belong to this category, which has been reviewed in Section II-A.\nMethodology Level 1: Slight cooperation between SLAM and MOT (namely to use certain detection or segmentation method to determine potential moving objects and then to heuristically sift out potential moving objects in SLAM). Typical works include[33-35]. Detect-SLAM[33] utilizes an object detection module integrated into ORB-SLAM2[6], and identifies moving objects through semantic information and probabilistic descriptions. DS-SLAM[34] employs semantic segmentation within ORB-SLAM2[6], identifying and eliminating moving objects by semantic information and moving consistency check. DynaSLAM[35] incorporates an instance segmentation module into ORB-SLAM2[6] for prior filtering of dynamic objects during localization. The aforementioned methods filter out dynamic objects from the SLAM process, allowing multi-object tracking to be independently carried out based on the SLAM results. Once SLAM is realized separately, then based on ego-vehicle poses revealed by the SLAM composing module, MOT can be realized using whatever method such as reviewed in Section II-B.\nMethodology Level 2: Holistic cooperation between SLAM and MOT considering a single motion model (namely to holistically fuse state estimation of moving objects and SLAM while fitting moving objects rigidly with a single motion model). Typical works include[36\u201339]. CubeSLAM[36] is based on ORB-SLAM2[6] and utilizes deep learning models to detect and track three-dimensional cube objects. This method leverages object representations and motion model constraints to jointly optimize the poses of the camera and objects along with environmental points. DynaSLAM II[37] is an improved version of DynaSLAM[35], which achieves joint optimization of camera pose, dynamic object trajectories and static scene structure through a novel bundle adjustment method. VDO-SLAM[38] employs instance segmentation and dense optical flow estimation as preprocessing modules. To enhance the robustness of camera and object state estimation, this method employs optical flow for joint estimation. Similar to DynaSLAM II, VDO-SLAM utilizes graph optimization to tightly couple localization, mapping, and object tracking. MOTSLAM[39] utilizes various deep learning methods including 2D detection, 3D detection, semantic segmentation and depth estimation. The obtained dynamic object information assists in feature association and object representation, while SLAM and MOT are coupled through bundle adjustment. The aforementioned methods adopt the same paradigm: deep learning models are used to identify potential moving objects, followed by joint optimization of SLAM and MOT. Typically, in these optimization frameworks, only a constant velocity model is used to describe the motion state of objects.\nMethodology Level 3: Holistic cooperation between SLAM and MOT considering multiple motion models (namely to holistically fuse state estimation of moving objects and SLAM while fitting moving objects flexibly with multiple motion models). Considering multiple motion models, which enables SLAMMOT to effectively handle the inherent nondeterministic nature of moving objects, is the essential point of this methodology. Instantiating this methodology as LiDAR-based SLAMMOT is presented in [1], whereas this paper focuses on instantiating this methodology as visual SLAMMOT.\nIt is worth noting that Methodology Level 3 contains two logic points: First, holistic cooperation between SLAM and MOT (in contrast with \u201cseparate\u201d or \u201cloosely-coupled\" SLAM and MOT as in Methodology Level 1) is important. In fact, Methodology Level 2 also contains this first logic point. Second, considering multiple motion models is important. The first logic point has long since been pointed out by pioneer researchers, yet it does not necessarily hold, if only considering a single motion model, especially in the context of visual SLAMMOT. It is combination of both the first logic point and the second logic point that demonstrates power and advantage over the other three Methodology Levels."}, {"title": "III. ARCHITECTURE OVERVIEW", "content": "In this section, we provide an overview of the methodology and its internal modules. We specifically focus on visual"}, {"title": "A. SLAM", "content": "sensors in this paper, such as monocular cameras, stereo cameras and RGB-D cameras. Our method takes a sequence of images as input and then conducts online processing on these images through three procedures, including MOT, SLAM and IMM-based modules. The output of our method corresponds to the SLAMMOT task, which contains the states of the camera and objects at different timestamps, as well as a global map of the static environment. The illustration of our framework is shown in Fig. 2.\nSLAM serves as a fundamental module in our method, implemented based on ORB-SLAM2[6]. This module is designed for online estimation of the camera trajectory and construction of the environmental map. Specifically, the SLAM module initializes the camera pose and environmental map at the beginning of the process. In each subsequent frame, ORB keypoints and descriptors are extracted and computed, then matched with the previously obtained ones. The matched features are used to estimate the camera poses and to maintain map points simultaneously in the world coordinate system. In ORB-SLAM2, local and global bundle adjustments are implemented using factor graph optimization. The nodes in the graph include camera poses and map points. In the following IMM-SLAMMOT graph optimization module, these factors are jointly optimized and updated with object states."}, {"title": "B. MOT", "content": "The purpose of the MOT module is to provide prior information about objects for the following SLAMMOT task. This module consists of two parts: 2D image segmentation and 3D multi-object tracking. On the one hand, the goal of 2D segmentation is to obtain semantic information about objects in the images. We adopt the instance segmentation model SpatialEmbedding[17] for implementation. This model takes a single 2D image as input to classify and cluster pixels in a bottom-up manner, generating pixel-level masks for each individual instance. As a result, we are able to determine the"}, {"title": "C. IMM-Based Modules", "content": "category of each pixel in the subsequent SLAM module. 3D multi-object tracking aims to provide prior object information, specifically the position, orientation, and bounding box of each object, while maintaining consistent object IDs between consecutive frames. To achieve this, we employ the two-stage MOT model MoMA-M3T[30]. In the first stage, 3D object detection results are obtained by MonoDLE[23]. Then, MoMA-M3T associates these detections in consecutive frames through motion modeling and learning. Output of the 3D MOT module is used in the subsequent IMM-based modules.\nThe IMM-based modules primarily consist of two parts. In the first object state estimation module, we take the position and the yaw rate of objects from the MOT module, and transfer them from camera coordinates to world coordinates using the camera poses estimated by the SLAM module. Then, we employ an IMM-based filtering approach to estimate the motion states of these objects. The estimates are passed to the factor graph module, where they are jointly optimized with the camera poses and map points. The optimization results are utilized to update the relevant variables, aiming to improve both SLAM and MOT for subsequent frames. We will elaborate on the specific algorithms of the IMM-based modules in Section IV. It is worth noting that incorporating consideration of multiple motion models into SLAMMOT is the essential point of the Methodology Level 3. Concerning the IMM itself, like in [1], we just adopt it without scrutinizing any improved variant of it. The focus of this paper is to instantiate visual SLAMMOT considering multiple motion models, rather than achieving marginal improvements through other IMM variants."}, {"title": "IV. IMM-BASED OBJECT STATE ESTIMATION AND GRAPH OPTIMIZATION", "content": "To describe the states of objects, we follow our team's previous work [1] to implement three motion models that"}, {"title": "A. Modeling", "content": "are commonly observed in the real world: the Constant Position (CP) model, the Constant Velocity (CV) model and the Constant Turning Rate and Velocity (CTRV) model. We denote the set of motion models as $\\mathcal{M} = \\{\\text{CP, CV, CTRV}\\}$. We utilize the coordinate system conventionally employed in visual SLAM, which follows the right-hand rule, with the y-axis pointing vertically downwards. Since the vertical motion during vehicle movement can be neglected, we only consider the horizontal motion in the x \u2212 z plane. The object states of different models at timestamp t are then defined as:\n$\\begin{aligned} \\mathbf{x}_{t}^{\\mathrm{CP}}&=\\left[x_{t}, z_{t}, \\theta_{t}\\right]^{T} \\\\ \\mathbf{x}_{t}^{\\mathrm{CV}}&=\\left[x_{t}, z_{t}, \\theta_{t}, v_{t}\\right]^{T} \\\\ \\mathbf{x}_{t}^{\\mathrm{CTRV}}&=\\left[x_{t}, z_{t}, \\theta_{t}, v_{t}, \\omega_{t}\\right]^{T} \\end{aligned}$ (1)\nwhere $\\theta_t$, $v_t$ and $\\omega_t$ denote the yaw rate, the linear velocity and the turning rate of an object at timestamp t, respectively. We denote the motion of an object under model d between two timestamps by a function $g^{d}$:\n$\\mathbf{x}_{t}=g^{d}\\left(\\mathbf{x}_{t-1}\\right), d \\in \\mathcal{M}$ (2)\nBy definition of the considered motion models, we have\n$\\begin{aligned} g^{\\mathrm{CP}}\\left(\\mathbf{x}_{t-1}^{\\mathrm{CP}}\\right)&=\\mathbf{x}_{t-1}^{\\mathrm{CP}} \\\\ g^{\\mathrm{CV}}\\left(\\mathbf{x}_{t-1}^{\\mathrm{CV}}\\right)&=\\left[\\begin{array}{l} x_{t}+v_{t} \\cos \\left(\\theta_{t}\\right) \\Delta T \\\\ z_{t}+v_{t} \\sin \\left(\\theta_{t}\\right) \\Delta T \\\\ \\theta_{t} \\\\ v_{t} \\end{array}\\right] \\\\ g^{\\mathrm{CTRV}}\\left(\\mathbf{x}_{t-1}^{\\mathrm{CTRV}}\\right)&=\\left[\\begin{array}{l} x_{t}+\\frac{v_{t}}{\\omega_{t}}\\left[\\cos \\left(\\theta_{t}+\\omega_{t} \\Delta T\\right)-\\cos \\left(\\theta_{t}\\right)\\right] \\\\ z_{t}+\\frac{v_{t}}{\\omega_{t}}\\left[\\sin \\left(\\theta_{t}+\\omega_{t} \\Delta T\\right)-\\sin \\left(\\theta_{t}\\right)\\right] \\\\ \\theta_{t}+\\omega_{t} \\Delta T \\\\ v_{t} \\\\ \\omega_{t} \\end{array}\\right] \\end{aligned}$ (3)\n(4)\n(5)\nTo apply the filtering method, we approximate (2) by first order and we have\n$\\mathbf{x}_{t}^{d}=A^{d} \\mathbf{x}_{t-1}^{d}+\\mathbf{w}^{d}, \\quad \\text { with } A^{d}=\\left.\\frac{\\partial g^{d}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right|\\_{ \\mathbf{x}=\\mathbf{x}_{t-1}}, d \\in \\mathcal{M}$ (6)\nNote that the CP model is linear, thus there is no actual need for approximation."}, {"title": "B. IMM-Based Object State Estimation", "content": "To the model $d \\in \\mathcal{M}$. These merged variables can be calculated as\n$\\begin{aligned} w_{t}^{d, \\mathcal{M}} &=\\sum_{c \\in \\mathcal{M}} C^{c d} w_{t-1}^{c} \\\\ \\mathbf{x}_{t}^{d, \\mathcal{M}} &=\\sum_{c \\in \\mathcal{M}} C^{c d} w_{t-1}^{c} \\mathbf{x}_{t-1}^{c} / w_{t}^{d, \\mathcal{M}} \\\\ \\mathbf{P}_{t}^{d, \\mathcal{M}} &=\\frac{1}{w_{t}^{d, \\mathcal{M}}} \\sum_{c \\in \\mathcal{M}} C^{c d} w_{t-1}^{c}\\left[\\mathbf{P}_{t-1}^{c}+\\Delta \\mathbf{x}_{t, c d}^{2}\\right] \\end{aligned}$ (7)\n(8)\n(9)\nwhere\n$\\Delta \\mathbf{x}_{t, c d}=\\left(\\mathbf{x}_{t-1}^{c}-\\mathbf{x}_{t}^{d, \\mathcal{M}}\\right)\\left(\\mathbf{x}_{t-1}^{c}-\\mathbf{x}_{t}^{d, \\mathcal{M}}\\right)^{T}$ (10)\nIn practice, the transition probabilities $C^{c d}$ with $c, d \\in \\mathcal{M}$ can be represented as a matrix C. The dimensions of the matrix C correspond to the number of models. Under consideration of three models, the matrix C can be defined as\n$C=\\left[\\begin{array}{ccc} 1-2 \\tau & \\tau & \\tau \\\\ \\tau & 1-2 \\tau & \\tau \\\\ \\tau & \\tau & 1-2 \\tau \\end{array}\\right]$ (11)\nwhere $\\tau$ denotes the transition probability between two different models. In our implementation, the parameter $\\tau$ is pre-set to 0.02.\n2) Distributed estimation: For each model $d \\in \\mathcal{M}$, its previous estimate {$\\mathbf{x}_{t-1}^{d, \\mathcal{M}}, \\mathbf{P}_{t-1}^{d, \\mathcal{M}}$} is used to predict its a priori estimate {$\\mathbf{x}_{t}^{d}, \\mathbf{P}_{t}^{d}$} via the corresponding system model. The Extended Kalman Filter serves as the recursive estimation method for the three distributed estimation tracks [40]. Prediction:\n$\\begin{aligned} \\mathbf{x}_{t}^{d} &=A^{d} \\mathbf{x}_{t-1}^{d, \\mathcal{M}} \\\\ \\mathbf{P}_{t}^{d} &=A^{d} \\mathbf{P}_{t-1}^{d, \\mathcal{M}}\\left(A^{d}\\right)^{T}+Q^{d} \\end{aligned}$ (12)\n(13)\nwhere the diagonal matrix $Q^{d}$ denotes the system noise of the model d. Update:\n$\\begin{aligned} \\mathbf{x}_{t}^{d} &=\\mathbf{x}_{t}^{d}+K\\left(\\mathbf{z}_{t}-H^{d} \\mathbf{x}_{t}^{d}\\right) \\\\ \\mathbf{P}_{t}^{d} &=\\left(I-K H^{d}\\right) \\mathbf{P}_{t}^{d} \\end{aligned}$ (14)\n(15)\nwhere $\\mathbf{z}_{t}=\\[x_{t}, z_{t}, \\theta_{t}\\]$ denotes the measurement obtained from the MOT module. H is the observation matrix, which is identity and maps from the dimension of $\\mathbf{x}_{t}$ to the dimension of $\\mathbf{z}_{t}$. K is the Kalman gain which is computed as\n$K=\\mathbf{P}_{t}^{d}\\left(H^{d}\\right)^{T}\\left\\[H^{d} \\mathbf{P}_{t}^{d}\\left(H^{d}\\right)^{T}+R\\right\\]^{-1}$ (16)\nwhere R is the measurement noise. 3) Model weight update: The weight $w_{t}^{d}$ is updated from its initial value $w_{t}^{d, \\mathcal{M}}$ according to its measurement innovation:\n$\\begin{aligned} w_{t}^{d} &=\\eta w_{t}^{d, \\mathcal{M}} \\sqrt{\\left|\\operatorname{det} S_{t}^{d}\\right|} e^{-\\frac{1}{2}\\left\\[\\mathbf{z}_{t}-H^{d} \\mathbf{x}_{t}^{d}\\right]^{T}\\left(S_{t}^{d}\\right)^{-1}\\left\\[\\mathbf{z}_{t}-H^{d} \\mathbf{x}_{t}^{d}\\right\\]} \\end{aligned}$ (17)\nwhere $\\eta$ is a normalization constant and\n$S^{d}=H^{d} \\mathbf{P}_{t}^{d}\\left(H^{d}\\right)^{T}+R$ (18)"}, {"title": "C. IMM-SLAMMOT Graph Optimization", "content": "For an object in the environment, suppose a timestamp t-1 for initialization of IMM.\nEach recursive cycle of the algorithm starts with three model weights $w_{t-1}^{c}=1 / 3$, three state means $\\mathbf{x}_{t-1}^{c}=\\mathbf{0}$ and three associated covariances $\\mathbf{P}_{t-1}^{c}$ of large values, with c\u2208 \u041c. The following procedure of IMM-based object state estimation consists of four steps [40]: 1) Multi-model merging: The model weights, state means and covariances are merged respectively in a weighted average manner based on their values at the previous timestamp. Denote $C^{c d}$ the transition probability from the model c\u2208 M\nto tightly couple SLAM and MOT for mutual benefits, we employ a novel bundle adjustment method based on graph optimization. We construct vertices for ego poses (T\u2208 SE(3)), map points (m \u2208 R\u00b3), and object states (o, v) respectively, and connect different nodes through errors. To incorporate IMM, we set up states for each motion model and assign model weights to the corresponding edges. We include static map points as nodes in the factor graph, allowing the map to dynamically adjust with the optimization of ego poses, thereby improving the accuracy of subsequent localization and mapping. The structure of our proposed factor graph is illustrated in Fig. 3.\nFor each motion model d \u2208 M, and for each object i at timestamp t, we define its position ($\\mathbf{p}_{i, t}^{d}$), its pose state ($\\mathbf{o}_{i, t}^{d}$) and its measurement ($\\tilde{\\mathbf{o}}_{i, t}^{d}$, obtained by MOT) as\n$\\begin{aligned} &\\mathbf{p}_{i, t}^{d}=\\[x_{i, t}, y_{i, t}, z_{i, t}\\] \\in \\mathbb{R}^{3} \\quad \\text { (23) } \\\\ &\\mathbf{o}_{i, t}^{d}=\\[x_{i, t}, y_{i, t}, z_{i, t}, \\theta_{i, t}\\]=\\left\\[\\mathbf{p}_{i, t}^{d}, \\theta_{i, t}\\right\\] \\in \\mathbb{R}^{4} \\quad \\text { (24) } \\\\ &\\tilde{\\mathbf{o}}_{i, t}^{d}=\\[\tilde{x}_{i, t}, \\tilde{y}_{i, t}, \\tilde{z}_{i, t}, \\tilde{\\theta}_{i, t}\\]=\\left\\[\\tilde{\\mathbf{p}}_{i, t}^{d}, \\tilde{\\theta}_{i, t}\\right\\] \\in \\mathbb{R}^{4} \\quad \\text { (25) } \\end{aligned}$\nThen, the reprojection error for object i with motion model d can be calculated as\n$\\begin{aligned} \\mathbf{e}_{i t}^{\\mathrm{repr}, d} &=\\left\\|\\left\\lceil T_{t}\\left(\\mathbf{p}_{i t}^{d}\\right)\\right\\rceil\\_{1: 3}-\\tilde{\\mathbf{o}}_{i t}\\right\\|-\\epsilon\\_{\\mathrm{cp}} e\\_{t}^{\\mathrm{odo}} (26) \\end{aligned}$\nwhere $\\mathbf{p}_{i t}^{d} \\in \\mathbb{R}^{4}$ represents the homogeneous coordinates of $\\mathbf{p}_{i t}^{d} \\in \\mathbb{R}^{3}$, and $\\phi_{t}$ denotes the yaw rate of the ego vehicle. Besides, we define the full states $\\mathbf{s}\\_{i t}^{d}$ considering the velocities $v_{i t}^{d}$ for different motion models:\n$\\begin{aligned} \\mathbf{s}\\_{i t}^{\\mathrm{CP}}&=\\mathbf{o}\\_{i t}^{\\mathrm{CP}} \\\\ \\mathbf{s}\\_{i t}^{\\mathrm{CV}}&=\\left\\[\\mathbf{o}\\_{i t}^{\\mathrm{CV}}, v\\_{i t}^{\\mathrm{CV}}\\right\\]=\\left\\[\\begin{array}{c} \\mathbf{p}\\_{i t}^{\\mathrm{CV}} \\\\ \\theta\\_{i t}^{\\mathrm{CV}} \\\\ v\\_{i t}^{\\mathrm{CV}} \\end{array}\\right\\] \\\\ \\mathbf{s}\\_{i t}^{\\mathrm{CTRV}}&=\\left\\[\\mathbf{o}\\_{i t}^{\\mathrm{CTRV}}, v\\_{i t}^{\\mathrm{CTRV}}, \\omega\\_{i t}^{\\mathrm{CTRV}}\\right\\]=\\left\\[\\begin{array}{c} \\mathbf{p}\\_{i t}^{\\mathrm{CTRV}} \\\\ \\theta\\_{i t}^{\\mathrm{CTRV}} \\\\ v\\_{i t}^{\\mathrm{CTRV}} \\\\ \\omega\\_{i t}^{\\mathrm{CTRV}} \\end{array}\\right\\] \\end{aligned}$ (27)\n(28)\n(29)\nSimilar to (3)-(5), motion functions $g\\_{s}^{d}$ can be defined as\n$\\begin{aligned} g\\_{s}^{\\mathrm{CP}}\\left(\\mathbf{s}\\_{i t}^{\\mathrm{CP}}\\right)&=\\mathbf{s}\\_{i t}^{\\mathrm{CP}} \\\\ g\\_{s}^{\\mathrm{CV}}\\left(\\mathbf{s}\\_{i t}^{\\mathrm{CV}}\\right)&=\\left\\[\\begin{array}{l} x\\_{i t}+v\\_{i t} \\cos \\left(\\theta\\_{i t}\\right) \\Delta T \\\\ y\\_{i t}+v\\_{i t} \\sin \\left(\\theta\\_{i t}\\right) \\Delta T \\\\ \\theta\\_{i t} \\\\ v\\_{i t} \\end{array}\\right] \\\\ g\\_{s}^{\\mathrm{CTRV}}\\left(\\mathbf{s}\\_{i t}^{\\mathrm{CTRV}}\\right)&=\\left\\[\\begin{array}{l} x\\_{i t}+\\frac{v\\_{i t}}{\\omega\\_{i t}}\\left[\\cos \\left(\\theta\\_{i t}+\\omega\\_{i t} \\Delta T\\right)-\\cos \\left(\\theta\\_{i t}\\right)\\right] \\\\ y\\_{i t}+\\frac{v\\_{i t}}{\\omega\\_{i t}}\\left[\\sin \\left(\\theta\\_{i t}+\\omega\\_{i t} \\Delta T\\right)-\\sin \\left(\\theta\\_{i t}\\right)\\right] \\\\ \\theta\\_{i t}+\\omega\\_{i t} \\Delta T \\\\ v\\_{i t} \\\\ \\omega\\_{i t} \\end{array}\\right] \\end{aligned}$ (30)\n(31)\n(32)\nEach static map point $m\\_{j} \\in \\mathbb{R}^{3}$ can be reprojected to the image frames where it is observable. Denote $u\\_{j, t}$ the coordinates of the pixel corresponding to this map point at timestamp t, and $\\pi\\_{t}$ the reprojection function from a world point to a pixel. Then, the reprojection error of this map point can be calculated as\nTherefore, for each motion model d \u2208 M, the system error for object i between consecutive frames is computed as\n$\\mathbf{e}\\_{i t, t+1}^{\\mathrm{sys}, d}=\\left(\\mathbf{o}\\_{i, t+1}^{d}\\right)-g\\_{s}^{d}\\left(\\mathbf{s}\\_{i t}^{d}\\right)$ (33)\n$\\begin{aligned} \\mathbf{e}\\_{j t}^{\\mathrm{repr}} &=u\\_{j, t}-\\pi\\_{t}\\left(T\\_{t} m\\_{j}\\right) \\end{aligned}$ (21)\nwhere $m\\_{j} \\in \\mathbb{R}^{4}$ represents the homogeneous coordinates of $m\\_{j} \\in \\mathbb{R}^{3}$.\nTo take into account the initial visual odometry results, we define the odometry error between two consecutive timestamps as\n$\\mathbf{e}\\_{t, t+1}^{\\mathrm{odo}}=\\left(T\\_{t+1} T\\_{t}\\right)^{-1} T\\_{t+1}$ (22)\nSpecially for the CV and CTRV models, we further attribute the constant motion error:\n$\\begin{aligned} \\mathbf{e}\\_{i t, t+1}^{\\mathrm{cst}, d} &=v\\_{i, t+1}^{d}-v\\_{i t}^{d}, d \\in \\{\\mathrm{CV}, \\mathrm{CTRV}\\} (34) \\end{aligned}$\nFinally, our objective function for optimization takes into account the model weights computed in the previous module, which can be expressed as\n$\\min \\sum\\_{t}\\left(\\left\\|\\mathbf{e}\\_{t, t+1}^{\\mathrm{odo}}\\right\\|^{2}+\\sum\\_{j}\\left\\|\\mathbf{e}\\_{j t}^{\\mathrm{repr}}\\right\\|^{2}+\\sum\\_{i} \\sum\\_{d \\in \\mathcal{M}} w\\_{t}^{d} e\\_{i t, t+1}\\right)$ (35)"}, {"title": "D. Baseline at Methodology Level 2", "content": "$\\"}]}