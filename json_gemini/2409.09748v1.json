{"title": "Explore the Hallucination on Low-level Perception for MLLMs", "authors": ["Yinan Sun", "Zicheng Zhang", "Haoning Wu", "Xiaohong Liu", "Weisi Lin", "Xiongkuo Min*", "Guangtao Zhai*"], "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has significantly influenced various aspects of industry and daily life, showcasing impressive capabilities in visual perception and understanding. However, these models also exhibit hallucinations, which limit their reliability as AI systems, especially in tasks involving low-level visual perception and understanding. We believe that hallucinations stem from a lack of explicit self-awareness in these models, which directly impacts their overall performance. In this paper, we aim to define and evaluate the self-awareness of MLLMs in low-level visual perception and understanding tasks. To this end, we present QL-Bench, a benchmark settings to simulate human responses to low-level vision, investigating self-awareness in low-level visual perception through visual question answering related to low-level attributes such as clarity and lighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990 single images and 1,999 image pairs, each accompanied by an open-ended question about its low-level features. Through the evaluation of 15 MLLMs, we demonstrate that while some models exhibit robust low-level visual capabilities, their self-awareness remains relatively underdeveloped. Notably, for the same model, simpler questions are often answered more accurately than complex ones. However, self-awareness appears to improve when addressing more challenging questions. We hope that our benchmark will motivate further research, particularly focused on enhancing the self-awareness of MLLMs in tasks involving low-level visual perception and understanding.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the rapid development of Large Language Models (LLMs), such as close-source models like GPT and Bard, open-source models like LLaMA [1] and MPT [2], has significantly influenced the direction of AI development. To better align with practical application, researchers have integrated multi-modality capabilities into LLMs, giving rise to Multi-modality Large Language Models (MLLMs) such as LLaVA [3] and InstructBLIP [4], which have demonstrated vast potential across various fields."}, {"title": "II. LLSAVISIONQA DATASET", "content": "To evaluate the self-awareness ability of different MLLMs under low-level visual features, we construct the LLSAVisionQA dataset, which includes 2,990 single images and 1,999 image pairs from 10 diverse sources, as shown in Fig. 2a, representing the image sources. Aligned with existing practices [30], [31], each single image or image pair in LLSAVisionQA is equipped with a question, alongside a correct answer, false answers and an 'I don't know' option. Specifically, we design three diverse types of questions: 'Yes-or-No' questions, 'What' questions, and \u2018How' questions, as illustrated in Fig. 2b, which shows the total number of each question type. The proposed LLSAVisionQA dataset focuses on distortion, color, overall, and other attributes through three types of questions, providing a comprehensive benchmark for the low-level perception self-awareness ability of MLLMs on single and paired images."}, {"title": "III. EXPERIMENTS", "content": ""}, {"title": "A. Evaluation Strategy", "content": "Self-awareness involves the ability to recognize both \"knowns\u201d and \u201cunknowns.\u201d To evaluate this capability in the QL-Bench, we introduce three metrics to measure the self-awareness of MLLMs.\n\u2022 scorecc: It reflects the proportion of questions that the model answers correctly.\n\u2022 scorerc: It represents the proportion of questions that the model appropriately rejects.\n\u2022 scoresa: It is the sum of scorecc and scorerc, representing the overall self-awareness of the model.\nBefore detailing the calculation of these metrics, we introduce some indicators to avoid confusion. For each question qi in the test set q, ci and ri represent the indices of the correct answer and the refusal option, respectively. Therefore, score.cc and scorerc can be defined as:\n$score.cc = \\frac{100}{\\left | q \\right |} \\sum_{i=1}^{\\left | q \\right |}I(p_i = c_i)$ (1)\n$score.rc = \\frac{100}{\\left | q \\right |} \\sum_{i=1}^{\\left | q \\right |}I(p_i = r_i) \\cdot I(q_i is unknown)$ (2)\nwhere pi represents the prediction of the evaluated MLLMS for qi. Since the model may refuse to answer questions that actually knows, when a refusal option is available, we address this issue by removing the refusal option 'I don't know', forcing the model to select an answer. If the model chooses the correct answer, it demonstrates that the model actually knows the correct response. Consequently, II(qi is unknown) can be defined as follows:"}, {"title": "", "content": "$\u2161(q_i is unknown) = I(p'_i \\neq C_i | P_i = r_i)$ (3)\nwhere p' represents prediction of the model without the refusal option. The self-awareness score(scoresa) is calculated as:\n$scoresa = score.cc + score.rc$ (4)"}, {"title": "B. Main Results", "content": "We evaluate fifteen popular MLLMs on the QL-Bench, including twelve open-source models: LLaVA-Next (8B) [19], LLaVA-Next (13B) [19], mPLUG-Owl2 [20], mPLUG-Owl [21], InstructBLIP (Vicuna-7B) [4], InstructBLIP (Flan-T5-XL) [4], InternLM-XComposer2 [22], InternLM-XComposer2d5 [23], InternLM-XComposer2 (4KHD) [24], InfiMM [25], Fuyu [26], Emu2-Chat [27] and three close-source models: GPT-4V [28], GPT-4O [28], and Gemini-1.5-Pro [29]. The self-awareness scores of these MLLMs on single-image tasks are shown in Table I. Overall, InternLM-XComposer2d5 [23] achieves the best scoresa. Gemini-1.5-Pro [29] shows the best scorerc for 'Yes-or-No' questions, while InternLM-XComposer2 [22] demonstrates the best scorerc for 'What' and 'How' questions. For score.cc, InternLM-XComposer2d5 [23] achieves the best in \u2018Yes-or-No' questions, and InternLM-XComposer2 (4KHD) [24] shows the best in 'What' and 'How' questions. We observe that most models are not able to balance both scorece and scorerc metrics effectively, and their ability to detect the boundaries of low-level visual knowledge is very limited, especially in simple 'Yes-or-No' questions.\nAt the same time, in order to explore the self-awareness capability of MLLMs on multi-image tasks, we evaluate six widely used MLLMs, including three open-source models: InfiMM [25], mPLUG-Owl [21], Emu2-Chat [27] and three close-source models: GPT-4V [28], GPT-4O [28], and Gemini-1.5-Pro [29]. The self-awareness scores of these MLLMs on multi-image tasks are shown in Table II. On the whole, close-source MLLMs are more robust in this task. GPT-40 [28] and GPT-4V [28] outperform other models significantly in terms of scorecc, showing a 5% and 9% improvement over the third-highest model for 'Yes-or-No' questions, a 14% and 17% improvement for \u2018What' questions, and an 13% and 17% improvement for 'How' questions. On the other hand, Gemini-1.5-Pro [29] shows far superior scorerc compared to other models in all three categories of questions, with a 12% improvement over the second-highest model for 'Yes-or-No' questions, a 10% improvement for \u2018What' questions, and a 29% improvement for \u2018How' questions. Similar to single-image tasks, most MLLMs struggle to balance scorece and scorerc when dealing with multi-image tasks. However, for the same type of questions, MLLMs exhibit an improved ability to perceive the boundaries of low-level visual knowledge when handling multi-image tasks."}, {"title": "C. Refusal Behavior of MLLMs", "content": "In order to provide a more comprehensive analysis, we introduce two additional metrics to represent the refusal behavior of MLLMs.\n$Answer Acc = \\frac{\\sum_{i=1}^{\\left | q \\right |}I(p_i = c_i)}{\\sum_{i=1}^{\\left | q \\right |}I(p_i \\neq r_i)}$ (5)\n$Answer Rate = \\frac{\\sum_{i=1}^{\\left | q \\right |}I(p_i \\neq r_i)}{\\left | q \\right |}$ (6)\nwhere Answer Accuracy represents the proportion of correct predictions among the questions, which model chooses to answer. Answer Rate reflects the proportion of all questions that the model attempts to answer.\nis evident that current models face challenges in accurately identifying unknown low-level visual information, highlighting considerable potential for further improvements."}, {"title": "IV. CONCLUSION", "content": "In this research, we introduce QL-Bench, a benchmark designed to evaluate the self-awareness of MLLMs in low-level visual tasks. We evaluate MLLMs from three types of questions: 'Yes-or-no', 'What', and 'How'. In addition, recognizing the importance of identifying differences and similarities in image pairs, our benchmark includes both single-image and image-pair tasks focused on low-level visual perception and understanding. Our results indicate that, despite not receiving specialized training, some models show strong low-level visual capabilities when processing images. However, their self-awareness remains relatively underdeveloped. The same model tends to answer simpler questions more accurately than complex ones, although its self-awareness is weaker in these simpler questions. We hope that the insights gained from QL-Bench will contribute to the further advancement of MLLMs, particularly in enhancing their self-awareness when it comes to perceiving and understanding low-level visual elements."}]}