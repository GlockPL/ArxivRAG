{"title": "SPARAMX: ACCELERATING COMPRESSED LLMS TOKEN GENERATION ON AMX-POWERED CPUS", "authors": ["Ahmed F. AbouElhamayed", "Jordan Dotzel", "Yash Akhauri", "Chi-Chih Chang", "Sameh Gobriel", "J. Pablo Mu\u00f1oz", "Vui Seng Chua", "Nilesh Jain", "Mohamed S. Abdelfattah"], "abstract": "Large language models have high compute, latency, and memory requirements. While specialized accelerators such as GPUs and TPUs typically run these workloads, CPUs are more widely available and consume less energy. Accelerating LLMs with CPUs enables broader AI access at a lower cost and power consumption. This acceleration potential for CPUs is especially relevant during the memory-bound decoding stage of LLM inference, which processes one token at a time and is becoming increasingly utilized with reasoning models. We utilize Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with unstructured sparsity to achieve a 1.42\u00d7 reduction in end-to-end latency compared to the current PyTorch implementation by applying our technique in linear layers. We provide a set of open-source customized sparse kernels that can speed up any PyTorch model by automatically replacing all linear layers with our custom sparse implementation. Furthermore, we demonstrate for the first time the use of unstructured sparsity in the attention computation achieving a 1.14\u00d7 speedup over the current systems without compromising accuracy. Code: https://github.com/IntelLabs/Hardware- Aware-Automated-Machine-Learning/tree/main/SparAMX", "sections": [{"title": "1 INTRODUCTION", "content": "The usage of large language models (LLMs) has grown exponentially over the past few years and is expected to continue its unprecedented growth. This has enabled many AI-driven applications, yet significant hardware and power resources are required, which have motivated recent attempts at model compression and acceleration. One such compression method is unstructured pruning, which removes some model parameters without structural constraints like contiguous nonzero values. Although such a method can achieve high sparsity while maintaining accuracy, achieving actual speedup on current hardware, such as GPUs and TPUs, is challenging.\nThis high-cost specialized hardware makes LLMs inaccessible to many people and limits their use. CPUs, on the other hand, are more ubiquitous and therefore can be used to accelerate LLM-driven applications for a wider audience. Newer CPUs, such as the Intel Sapphire Rapids, contain units that natively accelerate matrix multiplication at low cost and power. For example, AMX units present in the Sapphire Rapids chip enable direct acceleration of LLM workloads on the CPU. We explore combining the capabilities of this unit with unstructured sparsity to accelerate LLMs on the CPU. As shown in Figure 1, our sparse AMX kernel, SparAMX, leads to faster decoding times across common LLMs compared to the stock PyTorch.\nIn this work, we start with background material on sparsity and the hardware features used in our kernel, explore the current literature, and then introduce our detailed optimizations and design of kernels. We then demonstrate better efficiency over stock PyTorch that uses Intel's optimized libraries (Intel, 2024) and other proprietary commercial solutions like DeepSparse (Neuralmagic, 2024) by optimizing the dominant matrix multiplication operations within the model's linear layers. Finally, we apply unstructured pruning to the KV cache for the first time and propose a kernel that makes use of the sparsity to optimize the attention operation as well to increase model performance.\nOur summarized contributions are as follows:\n\u2022 An end-to-end system that uses unstructured sparsity to improve latency by up to 1.42\u00d7 over the stock PyTorch on CPU.\n\u2022 An INT8 CPU kernel that uses unstructured sparsity and AMX to achieve up to 1.46\u00d7 better performance than current proprietary kernels (Neuralmagic, 2024) for quantized models.\n\u2022 A novel study of unstructured sparsity in the KV cache achieving 1.14\u00d7 speedup on 16K context with minimal accuracy loss."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 LLM Inference", "content": "LLM inference consists of two distinct stages with varying compute and memory requirements. During the first stage, known as prefill, all input tokens are processed at the same time, allowing significant memory reuse and making the operation compute-bound. However, this stage occurs only once per prompt. The needed results (keys and values of each token in each attention head in each layer) are stored in the KV cache for subsequent use during the second stage.\nThe second stage, known as decode, performs autoregressive sampling one token at a time. This token-by-token processing reduces memory reuse and compute inten-"}, {"title": "2.2 DNN Sparsity", "content": "In Deep Neural Networks (DNNs), some weights contribute more significantly to the model's performance than others. Prior research has extensively explored methods for identifying weight importance (Liu et al., 2023b; Akhauri et al., 2024). Introducing sparsity into the weights via pruning leverages this insight to reduce the size of the DNN while improving efficiency. Sparsity reduces memory transfer costs by avoiding the loading of insignificant weights and lowers compute requirements by skipping operations involving those weights.\nThere are two types of sparsity: structured and unstructured.\n\u2022 Structured sparsity involves pruning weights in full blocks, such as a full row (mapping to a full neuron in Figure 2) or a full tile. This pattern allows for simpler acceleration since entire sets of weights can be skipped during loading and computation.\n\u2022 Unstructured sparsity imposes no constraints on the patterns of pruned weights. This flexibility enables higher sparsity levels, but achieving actual speedup is more challenging because the irregular distribution of zeros requires full computation and weight loading unless some special handling is used.\nStructured sparsity typically results in lower sparsity levels because important weights often co-exist within the same structure as prunable weights, preventing their independent removal. In contrast, unstructured sparsity achieves greater sparsity due to its flexibility, making it a better approach for model compression in scenarios where lower data transfer is desired and where hardware can handle its computational challenges."}, {"title": "2.3 GEMM Mapping", "content": "During both inference stages, the most compute-intensive operations in LLMs are mapped to matrix multiplication, also known as General Matrix Multiply (GEMM). These operations dominate the computational workload in both the core linear layers and attention mechanisms. Figure 3 shows that linear layers dominate the latency, especially at small contexts.\nLinear layers in LLMs can be represented as matrix multiplications, as illustrated in Figure 2. In the example, the weight matrix (W) contains the weights, where each column corresponds to the weights of a single neuron. Each neuron uses its unique set of weights to process inputs. The input matrix (IN) contains the input values, with each row representing an individual input. Each input undergoes identical processing, being multiplied by the same set of weights. The result of this computation is the output matrix (OUT). Each column in OUT corresponds to a neuron, with its rows containing the computed outputs for each input. The number of rows is the same as the number of inputs as an output row is computed for each of the input rows."}, {"title": "2.4 Advanced ISA Extensions", "content": "Our kernel is designed to utilize two specialized instruction sets: AVX (Advanced Vector Extensions) and AMX (Advanced Matrix Extensions).\nAVX is a set of SIMD (Single Instruction, Multiple Data) instructions extending the x86 architecture, enabling parallel operations on data vectors. It uses special AVX registers to store arguments and outputs for these operations. This work focuses on AVX-512, which operates on 512-bit registers.\nFor example, the instruction mm512_loadu_si512 can load 512 bits of data from main memory into an AVX register. To perform a dot product, you can load elements from two vectors into AVX registers and use the mm512_dpbf16_ps instruction. This operation multiplies 32 pairs of 16-bit elements of the two registers, adds the results of each two consecutive elements together to form 16 32-bit elements which are accumulated in a third register."}, {"title": "4 KERNEL DESIGN", "content": "We design our kernels as PyTorch C++ extensions, accompanied by Python classes, enabling seamless replacement of layers in arbitrary PyTorch models. These kernels are general-purpose and do not assume or optimize for a specific sparsity pattern. Consequently, the achieved speedup depends on the sparsity percentage of the model.\nWe begin by introducing a dense kernel that performs standard GEMM operations in BF16 using AMX. We then extend this kernel to incorporate unstructured sparsity, adapting it to improve performance under sparse conditions. To evaluate performance gains of AMX, we also implement the sparse kernel using AVX and use it for comparisons. Finally, we present the quantized INT8 kernels, designed to leverage low-bit computation for further optimization."}, {"title": "4.1 Dense Kernel", "content": "We begin by developing a kernel for linear layer computation using AMX, without incorporating any custom optimizations. This kernel, referred to as the dense kernel, assumes a fully dense model with no sparsity-related modifications, as illustrated in Figure 5. The AMX unit in each core can hold up to eight distinct tiles simultaneously. In our design:\n\u2022 Tiles 0-3 are utilized to store intermediate results, which remain in the AMX unit during iteration over the inner dimension.\n\u2022 During each iteration, we load two input tiles (Tiles 4 and 5) and two weight tiles (Tiles 6 and 7), compute the matrix multiplication, and accumulate the results in the four result tiles.\n\u2022 Upon completing the inner dimension loop, the four result tiles are saved to memory, and the next set of four result tiles is initialized.\nBy leveraging all eight tiles, instead of the naive approach of using one tile for result and two for operands, we achieve a compute-to-load ratio of 1:1, improving significantly over the 1:2 ratio that results from computing one tile at a time after loading two.\nNext, we parallelize the operations in the kernel. Since each input row (representing a different input token) and output column (representing a neuron) is independent, parallelization can be applied over out_rows and out_cols. In the decoding stage, a single batch contains only one output row. On the other hand, out_cols is input-independent and layer-dependent, making it the preferred dimension for parallelization. For smaller models, where out_cols is less than 32x number of available threads (since two tiles are"}, {"title": "4.2 Sparse Format", "content": "While the dense kernel leverages AMX efficiently, the analysis in Table 1 highlights further opportunities for performance optimization. Using the VTune Profiler to profile a Llama 3 model layer repeated 32 times, we observe that most pipeline slots are memory-bound, with nearly 50% of the time spent waiting on slow DRAM access. To address this bottleneck, we reduce memory transfer by storing weights in a compressed format and decompressing each tile only when needed for computation.\nTypically, weights, whether zero or non-zero, use the same number of bits. To save memory and bandwidth, we employ a compressed format where zero weights are represented with a single bit, while non-zero weights require an additional bit. Before computation, these values are converted back to their original format. Although this approach incurs"}, {"title": "4.3 AMX Sparse Kernel", "content": "Parallelizing over weights compressed in this format is challenging due to its unstructured nature. In a multi-threaded program, the access points for each thread within weight_values are not predetermined. To address this, we introduce a precomputed index list, weight_value_index, generated during model initialization. This list specifies the starting position for each thread within weight_values, as illustrated in Figure 9. Consequently, the number of threads must remain fixed during initialization, introducing a one-"}, {"title": "Algorithm 1 Parallel Prefix Sum with AVX-512 Intrinsics", "content": "Input: AVX Register $v = [v_0, v_1, ..., v_{15}]$ of 16 32-bit integers\nOutput: AVX Register $s = [s_0, s_1, ..., s_{15}]$ where $s_i = \\sum_{j=0}^{i} v_j$\n$s \\leftarrow v + \\text{Shift}(v, 1)$ \n$s \\leftarrow s + \\text{Shift}(s, 2)$ \n$s \\leftarrow s + \\text{Shift}(s, 4)$ \n$s \\leftarrow s + \\text{Shift}(s, 8)$ \nreturn s\n\n\u25b7 Shift right 1 element and add\n\u25b7 Shift right 2 elements and add\n\u25b7 Shift right 4 elements and add\n\u25b7 Shift right 8 elements and add"}, {"content": "Algorithm 1 Parallel Prefix Sum with AVX-512 Intrinsics\nInput: AVX Register $v = [v_0, v_1, ..., v_{15}]$ of 16 32-bit integers\nOutput: AVX Register $s = [s_0, s_1, ..., s_{15}]$ where $s_i = \\sum_{j=0}^{i} v_j$\n$s \\leftarrow v + \\text{Shift}(v, 1)$ \n$s \\leftarrow s + \\text{Shift}(s, 2)$ \n$s \\leftarrow s + \\text{Shift}(s, 4)$ \n$s \\leftarrow s + \\text{Shift}(s, 8)$ \nreturn s\n\n\u25b7 Shift right 1 element and add\n\u25b7 Shift right 2 elements and add\n\u25b7 Shift right 4 elements and add\n\u25b7 Shift right 8 elements and add\nwhich calculates a popcount on each of the 16 32-bit elements and stores the results in a new AVX register. Finally, a parallel prefix sum operation, as described in Algorithm 1, computes the final offset needed for each tile row."}, {"title": "4.4 AVX Sparse Kernel", "content": "During the decode phase with batch size = 1, only one row of the 16-row AMX tile used for input is utilized, leading to significant inefficiency. To address this, we implement the compression technique using only AVX instructions, enabling a performance comparison between AVX and AMX while ensuring compatibility with older CPUs lacking AMX support. The flow is illustrated in Figure 8. In this setup:\n\u2022 An AVX register holds the weights for multiple neurons in the inner dimension.\n\u2022 Another AVX register holds the corresponding input value repeated across the register.\n\u2022 A third AVX register accumulates results for multiple neurons."}, {"title": "4.5 INT8 Kernels", "content": "AMX supports both BF16 and INT8 operations. We develop a kernel for INT8 matrix multiplication and adapt our framework to quantize weights and activations for compatibility. The flow of the INT8 kernel mirrors that of the AMX dense and sparse kernels, with adjustments for 8-bit elements instead of 16-bit.\nEach weight tile now contains 16 \u00d7 64 = 1024 weights, and their metadata is fetched into two AVX registers, each covering eight rows of the tile. Additionally, the weight ordering during preprocessing is modified to divide each weight column into four segments rather than two."}, {"title": "5 RESULTS", "content": "We use stock PyTorch running in dense format\u00b9 as the baseline to ensure a fair comparison and consistency across all other operations, especially since it utilizes AMX when available. Table 2 presents the latencies observed for layer 5 of Llama 3 8B when run using stock PyTorch versus our custom sparse kernel. Our kernel outperforms PyTorch across all projections, with performance improvements ranging from 1.22\u00d7 in the most time-consuming projection to 2.03\u00d7 in the least time-consuming projection. Figure 10 illustrates the tradeoff between end-to-end speedup and accuracy on GSM8K(Cobbe et al., 2021) for multiple models adopted from SQFT(Munoz et al., 2024).\nFor end-to-end speedup, we evaluate the full Llama3 8B"}, {"title": "6 ATTENTION KERNEL", "content": "In the previous sections, we focused on linear layers. Here, we explore the potential of applying unstructured sparsity to attention computations. During attention, the incoming query value is multiplied by the cached K values, followed by a softmax operation, and the resulting values are multiplied by the cached V values. The cached K and V matrices can be treated as weight matrices and sparsified in an unstructured manner. We first evaluate the impact of unstructured sparsity on accuracy and then adapt the kernel to accelerate the matrix multiplication workload of attention, demonstrating the resulting speedup."}, {"title": "6.1 Sparsity in the KV Cache", "content": "Various methods have been proposed to optimize the KV cache, such as dropping certain tokens (Xiao et al., 2023b; Zhang et al., 2023), clustering tokens (Zandieh et al., 2024), or applying channel sparsity (Xu et al., 2024). In our approach, we apply unstructured sparsity to the KV values using magnitude-based pruning, where values with the lowest magnitudes are dropped within each layer."}, {"title": "6.2 Acceleration", "content": "The sparse kernel introduced in Section 4.3 is adapted here for attention computations. Within attention, matrix multiplication is required for computing QK and RV (where R is the result of the scaled softmax of QK). This operation is a batched matrix multiplication with an additional head dimension. The sparse kernel is modified to handle this operation, leveraging the independence of heads to parallelize across them.\nTo manage the KV cache efficiently, we modify the attention code to initialize an empty cache after prefill, storing all previously cached values in the model state similar to how weights are stored. PyTorch's native functions for updating the cache and its repeat_kv function (used in Llama 3 8B's Grouped Query Attention (Ainslie et al., 2023)) incur significant overhead with large KV caches due to memory reallocation for each new token. By replacing the cached tokens with our sparse format, which maintains a constant size within the model state, and saving new tokens in a separate dynamic set, decoding becomes over 6\u00d7 faster. This enables efficient decoding at high context lengths (e.g., 16K) on CPUs, allowing queries on long contexts with reasonable response times."}, {"title": "7 DISCUSSION", "content": "The presented kernels demonstrate that unstructured sparsity can achieve considerable speedup in linear layers. Notably, computations are still performed using dense weights; the speedup is realized by reducing memory transfer in a load-as-sparse, compute-as-dense approach (Xia et al., 2023). While this requires additional computation to reconstruct dense weights from the compressed format, the approach is particularly effective in memory-bound scenarios. Conversely, in compute-bound scenarios, applying unstructured sparsity may reduce performance, as confirmed by our results."}, {"title": "8 LIMITATIONS", "content": "Our system has several limitations. First, it requires preprocessing time, which, although only a few minutes for 8B models, makes it unsuitable for accelerating dynamically generated sparsity, such as activation sparsity. Additionally, our system currently supports only INT8 and BF16 formats, as AMX units do not natively support more compressed formats like INT4. Extending support to INT4 is feasible by dequantizing INT4 values into INT8 before computation.\nWhile our system supports any PyTorch model and serves as a useful tool for measuring the speedup of unstructured sparsity techniques, other systems, such as OpenVINO, achieve better results by incorporating additional optimizations, such as operation fusion, which are not included in our system. These optimizations make OpenVINO more suitable for production use."}, {"title": "9 CONCLUSION", "content": "In this paper, we introduced a system that leverages unstructured sparsity to achieve significant speedup in the decode stage compared to current implementations. Our system is general-purpose, compatible with any PyTorch model, and runs out of the box. It delivers a 1.42\u00d7 performance improvement over the current PyTorch version for Llama 3 8B and, at high batch sizes, achieves over 1.4\u00d7 higher throughput compared to proprietary systems like DeepSparse. Additionally, we demonstrate the potential of using unstructured sparsity in the KV cache to reduce latency per token, achieving a 1.14\u00d7 speedup with simple magnitude pruning."}]}