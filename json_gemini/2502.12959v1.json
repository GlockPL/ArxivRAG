{"title": "ALIGNFREEZE: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages", "authors": ["Steve Bakos", "F\u00e9lix Gaschi", "David Guzm\u00e1n", "Riddhi More", "Kelly Chutong Li", "En-Shiun Annie Lee"], "abstract": "Realignment techniques are often employed to enhance cross-lingual transfer in multilingual language models, still, they can sometimes degrade performance in languages that differ significantly from the fine-tuned source language. This paper introduces ALIGNFREEZE, a method that freezes either the layers' lower half or upper half during realignment. Through controlled experiments on 4 tasks, 3 models, and in 35 languages, we find that realignment affects all the layers but can be the most detrimental to the lower ones. Freezing the lower layers can prevent performance degradation. Particularly, ALIGNFREEZE improves Part-of-Speech (PoS) tagging performances in languages where full realignment fails: with XLM-R, it provides improvements of more than one standard deviation in accuracy in seven more languages than full realignment.", "sections": [{"title": "Introduction", "content": "Multilingual Language Models (mLMs) like XLM-R (Conneau et al., 2020) or mBERT (Devlin et al., 2019) can perform cross-lingual transfer (Pires et al., 2019; Wu and Dredze, 2019). Once fine-tuned on a specific task in English, these models perform well on that same task when evaluated in other languages. While this can be useful for languages where fine-tuning data might be missing, cross-lingual transfer is often less efficient for languages that differ greatly from English (Pires et al., 2019), which unfortunately are the languages that would benefit the most from such ability.\nWith an approach similar to building multilingual word embeddings (Lample et al., 2018; Zhang et al., 2017; Artetxe et al., 2018), realignment explicitly re-trains an mLM for multilingual alignment with the hope of improving its cross-lingual transfer abilities. While some work report some level of success (Cao et al., 2020; Zhao et al., 2021; Pan et al., 2021; Wang et al., 2019), systematic evaluations show that realignment does not consistently improve cross-lingual transfer abilities and can significantly degrade them in some cases (Efimov et al., 2023; Wu and Dredze, 2020).\nThe relative failure of realignment raises the question of whether better multilingual alignment necessarily implies stronger cross-lingual transfer abilities. Previous work has found that mLMs have good multilingual alignment, on top of their cross-lingual transfer abilities (Dou and Neubig, 2021; Ebrahimi et al., 2023), and there even seems to be a strong link between alignment and cross-lingual transfer (Gaschi et al., 2023), although the correlation is not causation and it remains that realignment often fails.\nIf better alignment is linked to better cross-lingual transfer, we hypothesize that realignment has some adverse effect that induces catastrophic forgetting of other important features of the model.\nTo better understand this side-effect of realignment and how the different layers are affected, we propose ALIGNFREEZE. In this method, half of the model layers are frozen during realignment. With a simple controlled experiment, we compare the impact on the lower and the upper layers. We find that realignment impacts all layers, but is particularly detrimental on lower layers, namely for a low-level task like PoS tagging."}, {"title": "Background on realignment", "content": "Realignment explicitly enforces the multilingual alignment of embeddings produced by multilingual models. It trains a multilingual model to produce similar representations for corresponding words in translated sentences. Two resources are needed: a translation dataset and a word alignment tool which, in our experiments, is either FastAlign (Dyer et al., 2013), AwesomeAlign (Dou and Neubig, 2021), or a simple look-up table based on bilingual dictionaries (Lample et al., 2018) as proposed in Gaschi"}, {"title": "Methodology", "content": "We introduce ALIGNFREEZE, a realignment method that relies on partial freezing to preserve half of the weights of an mLM during realignment. Because full realignment was shown not to work consistently (Wu and Dredze, 2020), we hypothesize that applying realignment on the whole model could trigger some catastrophic forgetting of information useful to downstream cross-lingual tasks. To help mitigate that and better understand the impact of realignment, ALIGNFREEZE freezes half of the layers of the mLM during realignment only.\nFreezing Strategies For the sake of simplicity and to reduce the number of experimental runs, we work with only two freezing strategies: 1) Front-freezing, which freezes the lower-half layers while the remaining layers are realigned; and 2) Back-freezing, which freezes upper-half layers instead. Assuming that basic linguistic features are encoded in the lower layers while the top ones retain higher-level information (Peters et al., 2018), Front-freezing aims to preserve the foundational language understanding captured in the early layers while enabling task-specific adaptation in the later layers. Back-freezing seeks to maintain the abstract, high-level representations developed in the deeper layers while fine-tuning the model's basic linguistic features. Our approach intentionally employs a straightforward freezing strategy, not to establish a new state-of-the-art realignment method, but to better understand the conditions under which realignment fails and how to mitigate its failure.\nThe freezing is applied only during realignment. Thus, ALIGNFREEZE can be described with the following steps: 1) Take a multilingual Language Model (mLM), 2) Freeze half of its layers, 3) train the remaining weights for the realignment loss, 4) unfreeze the frozen layers, 5) perform fine-tuning on the whole model for cross-lingual transfer."}, {"title": "Experiment Setup", "content": "Datasets Realignment Dataset: We use the OPUS-100 dataset (Zhang et al., 2020) for the realignment phase. OPUS-100 is a multilingual parallel corpus that includes sentence pairs across multiple languages.\nDownstream Task Dataset: We evaluate multilingual models on three tasks: PoS tagging, Named Entity Recognition (NER), Natural Language Inference (NLI), and Question Answering (QA). For PoS tagging, we use the Universal Dependencies dataset (Zeman et al., 2020), which provides annotated treebanks for a wide range of languages. For NER, we use the WikiANN dataset (Rahimi et al., 2019). For NLI, we use the Cross-lingual Natural Language inference (XNLI) corpus (Conneau et al., 2018). For QA, we use the XQuAD dataset (Artetxe et al., 2020).\nModels Following Gaschi et al. (2023), we work with three models: DistilMBERT (Sanh et al., 2019), mBERT (Devlin et al., 2019), and XLM-R Base (Conneau et al., 2020). DistilMBERT is a smaller version of mBERT (Devlin et al., 2019) obtained through distillation (Sanh et al., 2019). DistilMBERT, mBERT, and XLM-R are all Transformer-based masked multilingual models.\nLanguages We use English as the source language for fine-tuning. We evaluate on 34 languages for PoS-tagging and NER,, 12 for NLI, and 11 for QA. For realignment, we use the 34 available languages for PoS tagging, NER,, NLI, and QA. Using the same setting allows for comparison of results across tasks and also improves the outcome (cf. Appendix C.2). We use all the languages that our resources allow: every language must be present in the translation dataset, the bilingual dictionaries, and one of the downstream datasets. The full list can be found in the subsection B.1."}, {"title": "Results and Discussion", "content": "Finding 1: Full realignment fails in many cases.\nAs already observed by previous work (Wu and Dredze, 2020; Efimov et al., 2023; Gaschi et al., 2023), full realignment isn't always successful. Table 2 shows that realignment provides, on average, a significant improvement over fine-tuning with DistilMBERT, but the improvement is smaller with mBERT and even more so with XLM-R, especially for NLI and QA where it even degrades the results. Figure 1 and Table 2 also show that the outcome of full realignment varies a lot by language. For PoS-tagging with mBERT and distilMBERT, the majority of languages see a significant increase in accuracy. But with XLM-R, only 11 see a significant increase and one (Farsi) even undergoes a significant decrease of 2 points. For NLI, full realignment fails almost systematically with XLM-R, since 8 languages over 12 see a significant decrease in accuracy with realignment, while there can be as many significant increases and decreases for NER with XLM-R.\nFinding 2: ALIGNFREEZE (front) mitigates some of the failures of realignment. Freezing the lower layers during realignment often improves results for cases where full realignment fails. Table 2 shows that it brings an average improvement over full realignment with XLM-R for PoS-tagging and NLI, with 0.4 percent increases for both, but not for NER or QA, although the standard deviation is higher for QA making the results less conclusive. But more importantly, for PoS tagging, all languages are positively or neutrally impacted by front-freezing. And with XLM-R, the improvement is significant for 7 more languages than full realignment. On Figure 1, while Farsi (fa) and Hebrew (he) undergo a significant decrease with full realignment for PoS tagging, they do not with ALIGNFREEZE and even benefit from a 1-point improvement in the case of Hebrew. There are other languages, like Slovakian (sk), Polish (pl), and Hindi (hi) where full realignment provides a smaller improvement than front-freezing. Similarly to PoS tagging, front-freezing with mBERT for NER reduces the number of languages that suffer from realignment (from 19 to 1), but this is not the case with XLM-R. Contrary to PoS tagging and NER, NLI and QA do not benefit much from realignment, but front-freezing allows to reduce the number of languages for which realignment is detrimental for NLI.\nFinding 3: Realignment impacts the entire model, but it seems detrimental to the lower layers while it can be beneficial to the upper ones. Front-freezing can mitigate some failure cases of full realignment, thus realignment can have a detrimental effect on the lower layers. On the other hand, back-freezing seems to have a less important impact on realignment. Table 2 shows that back-freezing does not significantly improve over full realignment, and Figure 1 suggests that it provides worse results than any other alignment method for PoS tagging and NLI. The only exception is QA, for which back-freezing seems to improve over full realignment for distilMBERT and mBERT, but this improvement is not significant compared to the high variance of the results. This contradicts Gaschi et al. (2023) who hypothesized that since realignment appears to work better on smaller models, realignment might only have an impact on the upper layers of the model. Our results show that realignment impacts all layers and seems to be the most detrimental to the lower ones."}, {"title": "Generalized Recommendations for Practitioners using ALIGNFREEZE", "content": "Full realignment should be used for smaller models and low-level tasks. As already suggested by previous work (Gaschi et al., 2023), full realignment works better for smaller models like DistilMBERT and the technique proves beneficial for tasks involving lower-level linguistic features, as evidenced by more consistent improvements in PoS tagging, compared to NLI QA, or even NER (Table 2). This finding is relevant for researchers and organizations facing computational constraints. ALIGNFREEZE and full realignment enable the enhancement of smaller, resource-efficient models, achieving competitive results without large-scale models or extensive computational resources.\nALIGNFREEZE improves upon full realignment for PoS-tagging. Table 2 shows that ALIGNFREEZE is never detrimental to cross-lingual transfer and improves results for more languages than full realignment. For NLI, while ALIGNFREEZE still provides better results than full realignment, it can still be detrimental to cross-lingual transfer in some languages. This suggests ALIGNFREEZE is most effective when applied to tasks relying on syntactic and morphological information preserved in the frozen layers.\nCross-lingual transfer is hard to predict The variability in effectiveness across languages, models, and tasks highlights the importance of tailored approaches in multilingual NLP. In a truly zero-shot context, it seems hard to determine the right method for cross-lingual transfer, as shown by our"}, {"title": "Conclusion", "content": "This study introduces ALIGNFREEZE, a method using partial freezing to improve cross-lingual transfer in multilingual language models. Our experiments demonstrate that ALIGNFREEZE effectively mitigates the failure cases of partial realignment by preserving pre-trained knowledge in the lower layers.\nWhen it comes to cross-lingual transfer, there does not seem to be any \"silver bullet\" (Yarmohammadi et al., 2021) method that works for all languages, models, and tasks. Like realignment itself, and other cross-lingual approaches, ALIGNFREEZE can help for some situations but not others. ALIGNFREEZE can at least be useful for cross-lingual PoS-tagging with XLM-R.\nALIGNFREEZE helps better understand how realignment works. It impacts all layers and can be most detrimental to the lower ones, which is more visible on low-level tasks like PoS-tagging, that might be encoded in lower layers (Peters et al., 2018). Realignment probably fails simply because it is applied to the whole model without hindrance, which explains ALIGNFREEZE relative success but also the results of other methods based on adapters like MAD-X (Pfeiffer et al., 2020)."}, {"title": "Ethics and Limitations", "content": "We worked with the languages available in the datasets we used, but this led to high-resource languages and European languages being over-represented. To evaluate the effectiveness of cross-lingual transfer and realignment, the accuracy was averaged over all languages for a given task and model. Using the average to analyze the results has its risks, as different sets of languages can then potentially lead to different conclusions. However, the average remains convenient for our analysis and it was completed with some language-wise analysis as in Figures 1b and 1a. Moreover, detailed results are provided in Appendix C.5 for the interested reader.\nThe experiments of this paper could be extended to more tasks and more models. PoS tagging, NER,,\nNLI, and QA were chosen for their differences. PoS tagging is a more low-level task looking at word categories while NLI deals with understanding. Moreover, partial realignment works well for PoS tagging, whereas it provides weaker results with NLI (Gaschi et al., 2023). NER is chosen to complement this analysis with a task that is word-level, like PoS tagging, and semantic, like NLI. QA is chosen because it is a more difficult semantic tasks, like NLI, but is also a word-level one, like NER and PoS-tagging. The choice of model was based on a similar approach. XLM-R Base is the largest mLM that we could train with our experimental setting while DistilMBERT offered a smaller alternative, and mBERT some middle ground. XLM-R was shown not to benefit too much from realignment, while DistilMBERT observes a large performance increase and can sometimes match XLM-R with the help of realignment (Gaschi et al., 2023).\nThroughout this paper, realignment is applied to encoder-only Language Models like DistilMBERT or XLM-R. While the literature on realignment also focuses on encoders (Cao et al., 2020; Zhao et al., 2021; Efimov et al., 2023; Wu and Dredze, 2020), realignment could be extended to more recent decoder-only generative multilingual models like Bloom (Scao et al., 2023) or XGLM (Lin et al., 2022). However, these models are often intended to be used in a zero-shot or few-shot fashion, and Ahuja et al. (2023) showed that cross-lingual transfer with fine-tuning of XLM-R largely outperforms prompt-based approaches with generative models on classification tasks.\nThis study experiments only with two simple freezing strategies: front-freezing and back-freezing. More granular freezing strategies could be designed to better understand the role of each layer. However, we experimented with several other approaches, but the results were not conclusive enough to include in the paper. Freezing half of the model does influence realignment, though the overall impact is already relatively minor. More granular freezing strategies led to even smaller variations (See Appendix C.3 for some results).\nSome languages seem to benefit more from realignment than others. This study shows that freezing the bottom half of the layers during realignment might help with some languages that do not benefit from full realignment. However, ALIGNFREEZE, like full realignment, does not work for all languages, and it is still hard to determine in advance"}, {"title": "Related Works", "content": "Pre-trained multilingual language models have become the predominant approach for cross-lingual transfer tasks. Word alignment methods that depend on these models have also been proposed (Jalili Sabet et al., 2020; Nagata et al., 2020). Current realignment methods are typically applied to a multilingual pre-trained model before fine-tuning in a single language (usually English) and applying to other languages on tasks such as Natural Language Inference (NLI) (Conneau et al., 2018), Named Entity Recognition (NER) (Rahimi et al., 2019), Part-of-speech tagging (PoS) (Zeman et al., 2020), or Question Answering (QA) (Artetxe et al., 2020). This process is intended to enhance the model's ability to generalize to other languages for these tasks.\nRealignment can be performed in different ways. Cao et al. (2020) minimizes the 12 distance between translated pairs. But some regularization is needed to prevent the representations from collapsing, which can be done through an additional loss term (Cao et al., 2020; Zhao et al., 2021) or using contrastive learning (Wu and Dredze, 2020). Since the alignment is done at the word level between contextualized representations, an alignment tool is needed to obtain translated pairs to realign. Most methods employ the statistical tool FastAlign (Dyer et al., 2013). However neural-based tools can be used like AwesomeAlign (Dou and Neubig, 2021), which are indeed shown to work better for low-resource languages, although they come at a larger computational cost (Ebrahimi et al., 2023). A bilingual dictionary can also be used as a look-up table but extracts fewer pairs of words (Gaschi et al., 2023). Empirically, it was however shown that realignment has inconsistent results when evaluated across several tasks and languages (Efimov et al., 2023; Wu and Dredze, 2020).\nThe failure of realignment questions the very link between multilingual alignment and cross-lingual transfer (Gaschi et al., 2022). Realignment can increase multilingual alignment, but it might also be detrimental to some monolingual or even multilingual features learned by the model. To alleviate this, Gaschi et al. (2023) tried to optimize the realignment loss jointly with the fine-tuning loss, but they did not report improved performances.\nDue to its black-box nature, it is not straightforward to determine what role each layer of an mLM plays, but Peters et al. (2018) empirically showed, for ELMo, that the lower layers might encapsulate more lower-level information like syntax while the top ones relate to semantics. In a multilingual setting, Wu and Dredze (2019) showed that freezing the lower layers of mBERT during fine-tuning can increase its cross-lingual performances."}, {"title": "Additional Experimental details", "content": "For PoS tagging and NER, because we used languages that were available simultaneously in the dataset but also in the different resources used for that task (bilingual dictionaries and the translation dataset), we worked with the following 34 languages: Afrikaans, Arabic, Bulgarian, Catalan, Chinese, Czech, Danish, Finnish, French, German, Greek, Hebrew, Hindi, Hungarian, Italian, Japanese, Korean, Latvian, Lithuanian, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Spanish, Swedish, Tamil, Thai, Turkish, Ukrainian, and Vietnamese.\nFor NLI, due to similar constraints, we worked with the following 12 languages: Arabic, Bulgarian, Chinese, French, German, Greek, Hindi, Russian, Spanish, Thai, Turkish, and Vietnamese.\nFor both experiments, we reused the experimental setup from Gaschi et al. (2023). All experiments were run with 5 random seeds and performed using Nvidia A40 GPUs.\nWe train up to 5 epochs for PoS-tagging and NER and 2 epochs for NLI, with a learning rate of 2e-5, batch size of 32 for training and evaluation, and a maximum length of 200 for the source and target. For realignment, we use a maximum length of 96 and a batch size of 16.\nWe employ three word alignment methods: FastAlign (Dyer et al., 2013), AwesomeAlign (Dou and Neubig, 2021), and Bilingual Dictionaries (Lample et al., 2018). From a translation dataset, pairs were extracted either using a bilingual dictionary, following Gaschi et al. (2022), with FastAlign or AwesomeAlign. For FastAlign, alignments were generated in both directions and then symmetrized using the grow-diag-final-and heuristic provided by FastAlign, following Wu and Dredze (2020). In all extraction methods, only one-to-one alignments were retained, and trivial cases where both words"}, {"title": "Scientific artefacts used", "content": "Here is a list of the scientific artifacts used2:\n\u2022 The code for realignment comes from Gaschi et al. (2023) and has MIT License\n\u2022 the weights of DistilMBERT (Sanh et al., 2019) have License Apache-2.0\n\u2022 the weights of XLM-R Base (Conneau et al., 2020) have MIT License\n\u2022 The OPUS-100 dataset (Zhang et al., 2020) does not have a known license, but it is a filtering of the OPUS corpus (Tiedemann, 2009) which is itself the compilation of many translation datasets which are, to the best of our knowledge, free to be redistributed.\n\u2022 The Universal Dependencies dataset (Zeman et al., 2020) is also a compilation of several datasets, which all have, to the best of our knowledge, open-source licenses.\n\u2022 The XNLI corpus (Conneau et al., 2018) has a dedicated license but is nevertheless freely available for \"typical machine learning use\", which is the case in this paper.\n\u2022 The WikiANN dataset (Rahimi et al., 2019) doesn't have a known license to the best of our knowledge. It is thus assumed to be free to use.\n\u2022 The XQUAD dataset (Artetxe et al., 2020) has a the License CC-BY-SA-4.0, which allows its usage.\n\u2022 FastAlign (Dyer et al., 2013) has Apache-2.0 license\n\u2022 AWESOME-align (Dou and Neubig, 2021) has BSD 3-Clause License\n\u2022 The bilingual dictionaries (Lample et al., 2018) have an \"Attribution-NonCommercial 4.0 International\" license that allows non-commercial use as is the case here\nThe scientific artifacts were thus used consistently with the intended use, as all identified licenses are open-source or authorize non-commercial use."}, {"title": "Additional Results", "content": "We hypothesized a direct correlation between the quality of the realignment results on the downstream tasks and the quality of the OPUS-100 dataset. To evaluate this, we employed a Quality Estimation (QE) model (Rei et al., 2022) to selectively filter out sentence pairs below a predefined quality threshold. Since the OPUS-100 dataset contains significantly more sentences than needed for the realignment steps, the filtering should not affect the amount of data seen during realignment. Subsequently, we conducted experiments using this curated dataset to assess the impact of data quality on realignment results on the downstream tasks. Contrary to expectations, Figure 2 shows that, on average, using a higher quality dataset filtered by a QE model has little impact on the final results.\nIn this paper, realignment is performed with 34 languages for all tasks, despite the downstream evaluation being possible in only 12 of those languages for NLI. In preliminary experiments, realignment was only performed on those 12 languages for NLI,"}]}