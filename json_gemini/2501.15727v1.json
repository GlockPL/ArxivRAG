{"title": "Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning", "authors": ["Michael Xieyang Liu", "Savvas Petridis", "Vivian Tsai", "Alexander J. Fiannaca", "Alex Olwal", "Michael Terry", "Carrie J. Cai"], "abstract": "Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., \"let me know if my toddler is getting into mischief in the living room\"), with the MLLM analyzing the camera feed and responding within just seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements to the model and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users \"stress test\u201d sensors on potentially unforeseen scenarios. In a 12-participant user study, users reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users' own \"blind spots\" by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we describe insights into how unique characteristics of MLLMs-such as hallucinations and inconsistent responses-can impact the sensor-creation process. Together, these findings contribute to the design of future MLLM-powered sensing systems that are intuitive and customizable by everyday users.", "sections": [{"title": "1 Introduction", "content": "The proliferation of smart home technologies has popularized domestic sensing and monitoring [20, 21, 34, 75, 77]. Among these, smart camera sensors, offered by major consumer electronics companies, enable individuals to observe and react to events or activities within and around their homes, thereby enhancing safety and awareness. These devices leverage advances in computer vision (CV) and machine learning (ML) to identify and classify objects, distinguish between human and animal activities, or recognize specific events like package deliveries [64]. However, the current generation of smart camera sensors have limitations in their adaptability and configurability [51, 62, 76]. While capable of detecting predefined events, these systems do not readily support nuanced needs and priorities of individual users. This can lead to an excess of irrelevant notifications, missed critical events, and a mismatch between sensor capabilities, environmental conditions, and user needs.\nIn contrast to classical CV and ML models, recent multimodal large language models (MLLMs) integrate vast amounts of world knowledge and can understand and reason across a mixture of visual and textual inputs, much like humans [13, 38]. This advancement unlocks new opportunities for end-users to define personalized, MLLM-driven sensors capable of reasoning about complex situations (referred to hereafter as \u201cAI sensors\u201d for brevity)-a user could describe a desired sensing task in natural language, with MLLMs analyzing camera image frames and providing responses within seconds.\nTo illustrate, today's standard CV/ML classifier-based sensors are typically trained on domain-specific data [27] and limited to lower-level specific sensing objectives (e.g. \"are there unusual movements near the window\", \"is there a pet in the kitchen\"). In contrast, MLLM-driven Al sensors offer significantly greater flexibility with their ability to reason about higher-level problems (e.g. \"alert me if my toddler is getting into trouble in the living room\", \"is there something out of place in the backyard?\"). These reasoning capabilities may enable sensors to interpret the world in ways that would otherwise be challenging for traditional CV/ML classifiers. However, the seemingly infinite scope of tasks that can be accomplished with MLLMs also means that the semantic space of sensors users could define is also much broader than before, as they can now define tasks that are potentially more abstract, high-level, and subjective (e.g. \"is my room messy?\").\nIn a formative study, we confirmed users' desire for AI sensor personalization and configurability, but also discovered that the system must better support users in defining and communicating personal criteria and constraints (e.g. \"is there something out of place in my room\" [ignore the messy power cords]). This need was particularly pronounced for higher-level sensors with multiple possible semantic interpretations. For instance, a toddler \"getting into mischief\" might mean unrolling an entire roll of toilet paper in one household versus drawing on their sibling's face in another. Additionally, users often struggled to generalize beyond their immediate, personal settings (e.g., their own room) and found it challenging to anticipate future scenarios and variants of those settings where their sensors might behave unexpectedly. These findings suggest that effective Al sensor definition requires systems to anchor more closely to users' own criteria, while simultaneously making them aware of other, unanticipated scenarios.\nTo address these Al sensor definition opportunities and challenges, we developed Gensors, a system that empowers users to create and test real-time sensors powered by MLLMs. Beyond allowing users to create ad-hoc sensors via natural language prompting, Gensors leverages the reasoning capabilities and world knowledge of MLLMs. Specifically, users can (1) ask Gensors to break down the sensing problem into automatically-generated relevant criteria, (2) manually define their own criteria, and (3) test and debug multiple criteria simultaneously in real time. Furthermore, users can (4) ask Gensors to generate new criteria not yet considered based on frames from their video stream as positive and negative examples and also (5) \"future-proof\" their sensors by asking Gensors to suggest future situations that may lead to model failures, along with actionable tests for the user to try. Finally, users can configure how the final verdict is determined when considering the collective criteria, with options for an LLM-generated decision or rule-based combinations using boolean logic.\nIn a user study with 12 participants, we compared Gensors to a baseline condition where users iterated on a single prompt without structured assistance. We found that Gensors significantly increased users' sense of control, understanding, and communication with the model. Specifically, Gensors enabled participants to decompose the Al sensor definition problem into lower-level criteria and explore them in parallel, granting them greater control and systematic insights. Additionally, Gensors' reasoning capabilities also helped offset users' own limitations and \"blind spots,\" by making them aware of potential failure modes and edge cases, and surfacing context-specific criteria they hadn't considered. Finally, our study also highlighted how certain idiosyncrasies of MLLMs, such as hallucinations or \"flickering\" textual responses, impact sensor performance and perceived reliability.\nNotably, participants used the Gensors tools for purposes beyond addressing model limitations: by enabling users to decompose sensors into bite-sized criteria and test them, Gensors helped users to"}, {"title": "2 Related Work", "content": "2.1 Intelligent and DIY Visual Sensing\nThe concept of general-purpose, do-it-yourself (DIY) sensing has long been considered the ultimate goal of ubiquitous computing, particularly in smart home environments [20, 21, 34, 75, 77]. Many have aspired for sensing systems that end-users can intuitively customize [36, 54], yet current commercial smart home sensors still fall short of this vision. While affordable and accessible, they are typically highly specialized [29, 32, 67], and require users to invest significant time and effort in learning and creating custom workflows based on their outputs [70]. Moreover, these sensors usually produce low-level data that cannot directly answer users' high-level questions [12, 18]. For example, a door sensor might indicate the door's open/close status but cannot explicitly inform users whether their children have left or arrived home [35].\nMuch prior research has been directed at closing this gap between what can technically be sensed and what users are actually interested in knowing [12, 39], particularly in the visual sensing domain. Earlier work focused on leveraging \"human intelligence\" through online marketplaces such as Amazon Mechanical Turk [1]. For example, VizWiz [4] and VizLens [16] had crowd workers answer visual questions of photos taken from smartphones. Follow-up efforts, like VATIC [74] and Flock [9], utilize crowd-labeled data to subsequently train ML models. These efforts culminated in the Zensors system [17, 35]: initially, Zensors relies on human intelligence to directly answer users' sensing questions, such as \"Is there parking spots available?\u201d or \u201cHow orderly is the line?\u201d Over time, it uses this human-labeled data to train CV models, ultimately automating the sensing task by replacing human input with model predictions.\nThough providing a general-purpose sensing solution, Zensors' effectiveness and applicability remains constrained by the capabilities and limitations of traditional CV models. Additionally, users may struggle to debug or customize sensors [35], especially when outputs differ from expectations. In this work, we explore the potential of replacing crowd-ML hybrid sensing backbone with MLLMS, offering two distinct advantages: 1) MLLMs, with their reasoning capabilities, can directly explain their thought processes, aiding users in sensor debugging and understanding system capabilities and limitations, unlike previous methods where the rationale used by crowd workers is obscured by the later CV model; 2) MLLMs handle multimodal inputs, allowing users to define sensors using not only natural language but also direct visual examples, offering greater flexibility to express their unique personal contexts and needs.\n2.2 LLM Prompting and Requirement Articulation\nIn the emerging paradigm of end-user-defined Al sensors, users are now responsible for ensuring their sensors operate as intended, largely through crafting effective prompts. Indeed, prompting has become crucial for crafting effective input instructions to guide Large Language Models (LLMs) in generating desired outputs [2, 45, 48, 63, 78, 79], and has dramatically democratized and accelerated AI prototyping across various use cases and domains [22, 23, 25, 26, 37, 41, 46, 47, 57\u201359, 61, 65]. However, prompting remains a challenging and ambiguous task, particularly for users without technical expertise in LLMs. Common challenges include struggling with finding the right phrasing for a prompt, selecting appropriate demonstrative examples, experimenting with various hyper-parameters, and evaluating the effectiveness of their prompts [10, 22, 79]. Consequently, they may waste time on unproductive strategies, such as making trivial wording changes [53, 55]. This issue is further exacerbated when the task becomes more complex, involving multiple facets and requirements that need to be addressed within a single prompt [24, 81]. Similarly, we observed in the formative study that participants often haphazardly make ad hoc revisions to their sensing prompts in response to previous outputs, without a clear understanding of what needs improvement.\nAkin to the concept of requirement engineering in software engineering [30], where humans define the desired outcomes and behavior of a program, often including expected inputs and outputs [33], recent research has shown that explicitly and clearly stating requirements within prompts is an effective strategy for systematically improving them [11, 49, 66]. However, articulating clear and complete requirements is a known challenge [19, 42, 43, 46, 52, 53], even for experts who need multiple iterations to refine them [68]. Poorly defined requirements frequently lead to program failures [10, 50]. While tools like EvalLM [28], SPADE [68], and EvalGen [69] have explored extracting user requirements from prompts to support prompt evaluation, there is limited emphasis on assisting users in effectively communicating requirements during prompt construction. In this work, we address this gap by prioritizing requirements as first-class entities in the form of criteria that govern the sensor behaviors. Rather than having end-users juggle the intricacies of prompt writing, we abstract these mechanics of raw prompt away from them, and instead direct their attention on defining criteria, thereby simplifying the task. Additionally, we offer user support through features such as auto-generating criteria based on common sense or user-provided visual examples, further assisting users in translating their personal contexts and preferences into criteria.\n2.3 Interactive Model Refinement Through User Feedback\nPrior work has explored various interactive systems that allow users to provide feedback to refine future model outputs. Programming-by-example tools enable users to provide input-output examples, with the system generating a function that fits these examples [8, 73, 80]. Similarly, recommender systems allow users to steer outputs through limited feedback [5, 44, 56], such as adjusting a 2D plane to influence movie recommendations [31]. Teachable Machines also offer an interactive approach, allowing users to train ML models by supplying labeled examples, with real-time feedback facilitating iterative refinement [7]. More recently, systems and methods like ConstitutionMaker [61] and ConstitutionalExperts [60] leverage LLMs to translate natural language feedback and critique into high-level principles (similar to the sensor criteria in our work), enabling conversational, human-like interactions to"}, {"title": "3 Formative Study & Design Goals", "content": "To understand the opportunities and potential challenges for user-specified AI sensors, we conducted a formative study with six professional designers (age range: 29-42, 3 female and 3 male) from a large technology company, where they were asked to brainstorm AI sensor use cases and create their own sensors with a prompt-based prototype. Based on the findings from this study and insights from prior research, we identified a set of design goals for Gensors.\n3.1 Setup\n3.1.1 Procedure. The overall outline of the formative study was as follows: (1) Participants spent 5 minutes individually brainstorming potential use cases for visual-based personal sensors, documenting their ideas and initial thoughts. (2) Participants were then shown the prompt-based prototype they would be using to build their sensors (see Figure 3). (3) Participants spent 40 minutes individually creating one to two sensors of their choice. While they prototyped, they were asked to take notes on how they iterated over their prompt and take screenshots. To ensure ecological validity, participants were asked to create sensors in their homes. (4) For the last 10 minutes, the facilitators led a group discussion with the participants to learn about their experience building sensors.\n3.1.2 Prototype. The prototype (Figure 3) provided a basic interface for creating MLLM-powered sensors. The prototype allowed them to input prompts (Figure 3-d) and set an interval for execution using their laptop camera. The prototype displayed the MLLM's output for each execution (Figure 3-e), as well as the corresponding input image (Figure 3-c). Finally, participants could view the history of all the sensor inputs and outputs.\n3.2 Findings\n3.2.1 The opportunity for personal Al-powered sensors. All participants were quite excited about the possibility of building their own personal sensors. P3 explained, \u201cTraditionally, I had to buy a product built for a task...being able to set up my own sensor - that's the new part.", "detected": "how much family time are we spending,\" \"how much time did I spend practicing Korean,\u201d \u201cwhat food have I eaten during the day to manage health conditions.", "including": "let me know if my landlord is at the door,\" \"tell me if my pork chop is getting burnt,\" and \"tell me if there's a leak at my water heater.", "how many new people did I meet last week": "nd", "before.": "n3.2.2 Challenges in creating prompt-powered sensors. A common strategy participants employed to steer their sensors was to write criteria to specify when the sensor should output a label. For example, P6 built a sensor to determine when his plant needs fertilizer. Their prompt was initially:", "why.": "he outputs from this prompt were a bit vague and seemed unjustified, like: \u201cNo. The plants look healthy.", "criterion": "Look for signs of yellowing and discoloration\" which steered the model toward producing more relevant and informative explanations. Similarly, P5 was building a sensor that detects if any chores needed to be done in her living room, and to further steer the sensor, she appended a criterion to\""}, {"title": "3.3 Summary of Design Goals", "content": "In summary, we postulate that an effective system that helps end-users create flexible and intelligent AI sensors should support:\n\u2022 D1: Enabling precise control over the sensor behavior via fine-grained criteria. Users should be able to author and adjust criteria individually and assess the sensor's performance for each one, without needing to revise an entire prompt.\n\u2022 D2: Accelerating requirement elicitation by bootstrapping common-sense criteria. The system should assist users in getting started by generating relevant, common-sense criteria automatically.\n\u2022 D3: Providing flexible ways to communicate personal context. Users should be able to express their specific context via text, as well as visually. The system should also help users identify their more nuanced criteria.\n\u2022 D4: Scaffolding testing and debugging of criteria. The system should offer tools that allow users to test, isolate, and debug each criterion separately, enabling users to address future scenarios that might confound their sensor, incrementally improve the sensor's accuracy, and understand how each criterion impacts overall performance."}, {"title": "4 The Gensors System", "content": "We begin by presenting a usage scenario that demonstrates the core functionalities of Gensors. This example incorporates key use cases identified in our formative study, along with specific criteria curated by participants in the subsequent user study.\n4.1 Example Usage Scenario\nEmma was concerned about her toddler \"getting into trouble\" in her home, as she has often observed him playing with her purses or breaking her valuable belongings. She couldn't find any commercial sensors specifically designed to monitor this sort of situation, so she decided to create a custom one using the Gensors platform. To get started, Emma entered her request as \"tell me if toddler might damage something\" into the system. Gensors automatically generated a basic LLM prompt based on that request: \"Is the toddler likely to damage something valuable in this room? Answer with 'Yes' or 'No', and provide a brief explanation.\" Emma then positioned her webcam to provide a clear view of her living room space where her toddler frequently played. Within seconds, the custom \"Toddler Check\" sensor was operational: it is configured to run every three seconds (a default frequency easy for users to test and debug the sensor prompt) using the latest three frames (Fig. 2-b) from the camera feed (Fig. 2-a, which takes one frame per second) and the prompt mentioned previously, providing continuous assessment of the room.\nHowever, Emma quickly realized that the sensor considered her stack of clothes damageable, even though she is fine and well-accustomed with her toddler playing with her laundry. The sensor also did not notice her sacred wedding photo that her toddler often attempts to reach. Despite her efforts to tweak and add more clauses, it was unclear if these adjustments were consistently improving the responses. As a result, Emma clicked the \"Generate criteria\" button (Fig. 2-f2), and Gensors automatically produced several criteria (e.g., Fig. 2-i) based on the initial request and the environment (e.g. \"Delicate Surfaces,\u201d \u201cSentimental Objects,"}, {"title": "5 User Study", "content": "To gather insights into Gensors' potential to benefit the AI sensor specification process, we conducted a 12-participant within-subjects user study. The study compares Gensors to a prompt-editor version, similar to the one used in the formative study (Figure 3), where participants specified AI sensor behavior by iterating on a text prompt. We also included Gensors' playback feature in the baseline condition, where participants could view the history of sensor outputs with their inputs.\n5.1 Procedure\nThe overall outline of the study is as follows: (1) Prior to the study, participants completed a 30-minute self-directed tutorial, where they watched instructional videos and built a sensor with Gensors and the prompt-editor version. (2) During the study, participants spent 50 minutes creating two Al-powered sensors, starting either with Gensors (25 minutes) or the prompt-editor version (25 minutes), in a counterbalanced design. (3) After building these two sensors, participants completed a post-study questionnaire, which compared the two sensor prototyping conditions. (4) In a semi-structured interview, participants gave feedback on each prototyping tool, including the benefits and drawbacks of each one. The total time commitment of the study was 90 minutes.\nFrom the brainstorm conducted in the formative study, we picked two different types of sensors for participants to implement in the user study, one \"urgent\" and one \"reminder\u201d sensor. The two sensors were: (1) a \"reminder\" desk clutter sensor which determines when the user's desk is messy and (2) an \"urgent\" toddler safety\nsensor which determines if there is anything particularly dangerous to a toddler in the user's bedroom. We chose these two sensors as they are realistic use cases of personal sensors, as well as general enough for users to be able to define and have personal opinions about them. The order in which participants implemented these provided sensors was counterbalanced, in addition to the condition order. To help situate the task, we asked participants to imagine that they were the target user for each sensor. For the desktop clutter sensor, participants were asked to imagine that they were a professional who worked from home and was creating a sensor that would send a reminder for them to organize their desk during the work week if it got too messy. For the toddler safety sensor, participants were asked to imagine that they were a new parent, building a sensor to help identify conditions in their living or bedroom which might be problematic for their toddler.\nFinally, participants authored and tested their sensors generally via their laptop camera, though some were able to use a webcam focused on the area of interest and author the sensor separately on their laptop. The study was approved by our institution's IRB.\n5.2 Participants\nWe recruited 12 participants (6 female, 6 male) from our institution, covering a range of skill sets, including product managers, UX researchers, and software engineers, and from a variety of locations in the US, including Michigan, New York, California, Georgia, Wisconsin, and Washington. We were generally recruiting for \"first-adopter\" tech-savvy individuals who would be likely candidates for authoring Al-powered sensors in their own home. Participants were recruited via an email invitation. The study was conducted remotely, in participants' homes to create a valid testing environment. Participants received a $40 gift card for their participation.\n5.3 Questionnaire\nWe wanted to understand the potential for Gensors to help participants think through their requirements for the sensor and steer the model to follow them. As such, our questionnaire (Table 1) probes participants' self-perceived control over the sensor, as well as their perceived ability to think through and communicate their personal requirements. We were also interested in seeing how useful Gensors' test suggestions were and if testing via modular criteria helped participants gauge the underlying model's capabilities. We also included questions to assess the relative usefulness of Gensors' features.\n5.4 Data Analysis\nTo analyze the results from the post-study questionnaire, we conducted paired sample Wilcoxon tests with full Bonferroni correction to compare the ratings from the two conditions, since the study was within subjects and the questionnaire collected ordinal data. In addition, the main study sessions and the post-study interviews were screen and audio recorded, then transcribed. The first two authors independently coded the recordings and transcriptions using an open coding approach [72] in accordance with Braun and Clarke's thematic analysis [6]. Subsequently, they iteratively resolved disagreements and ambiguities, which included periodic discussions with the research team. We present the key themes and findings below."}, {"title": "6 Findings", "content": "6.1 Quantitative Findings\nThe results from the questionnaire are summarized in Figure 5 & 6. Notably, participants reported significantly greater control over the sensor when using Gensors (\u03bc = 6.26, \u03c3 = 0.72) compared to the baseline (\u03bc = 4.5, \u03c3 = 1.44, p < .01). In the baseline condition, participants were often uncertain about how to best modify the prompt and felt their edits did little to influence the sensor.\nParticipants' perceived understanding of the underlying model was significantly higher with Gensors (\u03bc = 6.08, \u03c3 = 1.11) than with the baseline (\u03bc = 4.17, \u03c3 = 1.57). With Gensors, participants could pinpoint which individual criteria the sensor was failing on, iterate on them, and understand whether that criteria could be indeed detected by the model. In contrast, the baseline lacked the ability to isolate different criteria, making it difficult for participants to see how the model was attending to the requirements participants formulated in their prompts.\nParticipants also rated their ability to think through and communicate their requirements with Gensors (\u03bc = 6.42, \u03c3 = 0.86) significantly higher than the baseline (\u03bc = 4.17, \u03c3 = 1.34, p < .001). This was predominantly attributed to the automatically generated criteria (which provided a starting point for reflecting on requirements), as\n6.2 How participants iterated on their sensors\nThe two conditions led to markedly different workflows for iterating on sensors. With the baseline prototype, participants typically observed the sensor's live output and spontaneously appended more criteria and caveats to their prompt in an attempt to better align the model's behavior with their own definition of the problem. For example, P1 began with a simple prompt: \"Is my desk cluttered? Output yes or no and explain why.\" The model initially stated that her desk was cluttered with too many items (e.g. a phone, pens, and keyboard). However, P1 considered these items as permanent fixtures of her desk, so she updated the prompt with: \"I do not consider it cluttered if my phone, keyboard, mouse, pens, and picture frame are on the desk.\" Later, she specified that empty food and drink containers should be considered cluttered. However, the model interpreted this too rigidly, flagging even a single container\n6.3 Gensors helped participants tune and troubleshoot sensors via criteria-level controls\nA key benefit of Gensors was that participants could operate on their sensor at the criterion level, as opposed to the prompt level. Having the sensor explicitly check for each criteria helped participants better inspect and assess the model's underlying reasoning. In the following section, we discuss how employing the model's reasoning capabilities at the criteria-level helped participants with controlling, testing, and debugging their sensors.\n6.3.1 Participants felt they had greater control via criteria than with the prompt. When working on their prompts in the baseline condition, participants were often unsure of how to best improve their prompt and if it was improving at all. For instance, P5, working on the desktop-clutter sensor in this condition, aimed to have the model output that his desk was cluttered when at least one empty can was present. However, despite multiple adjustments to the prompt, the sensor persistently reported his desk as clean. He carefully scanned the prompt for any potential miscommunications and hypothesized that the use of \"and\" in the phrase \"Clutter also includes clothing accessories like hats and food waste like soda cans", "and\" to \"or\", which then caused the sensor to briefly classify\n6.4 Gensors helped participants consider failure modes beyond their specific context\nIn both conditions, participants manipulated their environment to test how their sensors would perform in a variety of scenarios. A common strategy was to create exaggerated setups, for example, making a desk conspicuously cluttered or impeccably clean. After establishing that the sensor could work effectively in these clear-cut scenarios, they would make incremental adjustments, generally by adding or removing objects, to bring the scenario closer to a boundary condition. For example, with Gensors, after his sensor stated his tidy kitchen was safe for toddlers, P9 placed a cutting knife on the countertop to test the model's response to a highly unsafe object in an otherwise safe kitchen. Similarly, in the baseline condition, P5 organized his desk to what he considered a clean state, then introduced individual perturbations to make it \"messy,\" such as adding a can of soda. Overall, participants manipulated their environments to explore and test their sensors across a variety of scenarios.\nWhile participants naturally engaged in creating these tests, they tended to overlook tricky visual situations or subtler failure modes in the baseline condition. Meanwhile, some participants appreciated that Gensors created suggested test cases for them, noting that it would otherwise be difficult to think of these alternative scenarios on their own. For example, P10, working on the desk clutter sensor with an \"object count\" criterion, appreciated the test case that suggested placing similar colored objects stacked on their desk to see if the model could distinguish them. They found that stacking objects together did impede the sensor's ability to clearly distinguish them, especially if parts of objects were hidden beneath others. Similarly, for the \"choking hazard\" criterion in his toddler safety sensor, P4 appreciated a test case that suggested partially obscuring the choking hazard object (e.g. a coin) with another larger object. Overall, these criterion-specific test suggestions helped participants more thoroughly stress-test their sensors while gaining a deeper understanding of the underlying MLLM's capabilities.\n6.5 Gensors supplemented users' own \u201cblind spots": "y suggesting complementary criteria\nWhile Gensors enabled users to align the sensor most closely with their own personal criteria and preferences (e.g. via the manual-criteria feature), it also encouraged them to look beyond their immediate surroundings and field of view (e.g. via the auto-generated criteria and Examples-diff features) for criteria that they would otherwise overlook.\nFor example, some users noted that the automatically-generated criteria prompted them to consider aspects of the problem they would not have thought of independently. For example, many users did not initially think of \"uncovered outlets\" as a potential hazard for toddlers, but realized they had missed this upon seeing it auto-generated. Similarly, for the desk clutter sensor, Gensors considered whether there was a \"Usable Area\" on the desk (e.g. \"Is it easy to find a usable area on the desk without having to move items around?\"). Participants found this surprising and compelling, as it reframed the problem of messiness in an unexpected way. However, one participant, P1, commented on a potential drawback of generated criteria, noting that they seemed to be \"too easy to say yes to,", "tunnel vision\" when they struggled to identify additional criteria that could further differentiate borderline cases. For instance, while P11 was working on their desktop clutter sensor, they had a few borderline examples that their current set of criteria was not consistently differentiating. At this point, they felt that they had exhausted the criteria they could identify, so they turned to the Examples-diff feature, from which they selected two criteria: one that checks if the \"objects on the desk appear to be randomly placed": "nd another that checks if the desk feels \"chaotic and disorganized.", "distinguished": "It's easy to describe the easy cases [with criteria", "seeing.": "ccasionally, however, the Examples-diff feature would hallucinate differences between examples. For instance, P12 used the feature for her toddler safety sensor, and the model hallucinated that there was a toilet in their bedroom, creating a criterion that checked if the toilet's lid was closed. Despite these occasional inaccuracies, the Examples-diff feature generally helped participants uncover more features to distinguish between more complex examples.\n6.6 How MLLM peculiarities impacted sensor creation\nMLLM hallucinations both benefited and inhibited participants' processes in defining their sensor criteria. On one hand, when not entirely fabricated, hallucinations could sometimes be useful for helping participants become aware of criteria they did not consider. For example, while P3 was building his toddler safety sensor in baseline, the model deemed his bedroom unsafe due to the potential risk of a toddler falling out of open windows. Although P3's windows were not actually open, he found this scenario plausible and ultimately adjusted his prompt to account for such a situation. On the other hand, entirely fabricated hallucinations could confuse and distract users during the process. For instance, while P10 was working on her desk clutter sensor in the baseline, the model"}, {"title": "7 Discussion", "content": "7.1 Supporting Active Criteria Elicitation\nWhile participants appreciated the automatically generated criteria, some noted that it could potentially stifle them from carefully considering their own personal preferences for the sensors. They desired features that would more actively involve them in creating the criteria. There are many possibilities for alternative, more engaging workflows to help users realize their requirements. For instance, for a desk clutter sensor, one could first be presented with a set of personas, such as an \"organized chaos\" persona, who considers an uncluttered desk"}]}