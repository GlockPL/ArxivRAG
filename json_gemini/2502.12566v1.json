{"title": "Exploring the Impact of Personality Traits on LLM Bias and Toxicity", "authors": ["Shuo Wang", "Renhao Li", "Xi Chen", "Yulin Yuan", "Derek F. Wong", "Min Yang"], "abstract": "With the different roles that AI is expected to play in human life, imbuing large language models (LLMs) with different personalities has attracted increasing research interests. While the \"personification\u201d enhances human experi- ences of interactivity and adaptability of LLMs, it gives rise to critical concerns about content safety, particularly regarding bias, sentiment and toxicity of LLM generation. This study explores how assigning different personality traits to LLMs affects the toxicity and biases of their outputs. Leveraging the widely accepted HEXACO personality framework developed in social psychology, we design experimen- tally sound prompts to test three LLMs' per- formance on three toxic and bias benchmarks. The findings demonstrate the sensitivity of all three models to HEXACO personality traits and, more importantly, a consistent variation in the biases, negative sentiment and toxicity of their output. In particular, adjusting the levels of several personality traits can effectively re- duce bias and toxicity in model performance, similar to humans' correlations between person- ality traits and toxic behaviors. The findings highlight the additional need to examine con- tent safety besides the efficiency of training or fine-tuning methods for LLM personification. They also suggest a potential for the adjustment of personalities to be a simple and low-cost method to conduct controlled text generation.", "sections": [{"title": "1 Introduction", "content": "As the demand for large language models (LLMs) to serve diversified roles continues to grow, the topic of LLM personification has surged in LLM research and development (Chen et al., 2024)."}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 The Role of Personality Traits in Prejudice and Verbal Aggression", "content": "Allport et al. (1954) lay the foundation for preju- dice research in The Nature of Prejudice, empha- sizing the impact of individual beliefs and values on inter-group relations. Social psychological ex- perimental research demonstrates that individual personality traits play a crucial role in the forma- tion of prejudice and the expression of linguistic aggression (Buss and Perry, 1992; Sibley et al., 2010; Molero Jurado et al., 2018; Zaki et al., 2024; Ekehammar and Akrami, 2007). Crawford and Brandt (2019) indicates that among the Big Five personality traits, Agreeableness, Openness and Ex- traversion show significant negative correlations with prejudice. Similarly, Hu et al. (2022) demon- strate a negative relationship between Agreeable- ness personality and verbal aggression. Rafienia et al. (2008) shows that positive Extraversion could lead to positive judgment (e.g., probability rating for positive events) and positive interpretation (e.g.,"}, {"title": "2.2 LLM Personification", "content": "Research on LLMs in the fields of role-playing and personification has recently gained popular- ity. Chen et al. (2024) conduct a systematic review on the personification and role-playing of LLMs, proposing a classification of LLM personas: De- mographic Personas, Character Personas, and Indi- vidualized Personas. Our research focuses on the persona traits of LLMs, which therefore fall under the Demographic Personas. The review summa- rizes methods for constructing LLM personas, such as (continuous) pre-training, instruction finetuning, reinforcement learning, and contextual learning. Several studies examine the effectiveness of these methods (Jiang et al., 2024; Sorokovikova et al., 2024; Wang et al., 2024; Chen et al., 2024; Zhang et al., 2024).\nAmong the different studies, Zhang et al. (2024) is one of the few that examines content safety and personality. They focus primarily on 7B open- source models and explore the relationship between the MBTI personality types and models' ability to remain content-safe. In a similar vein, Wan et al. (2023) introduce the concept of \"personalized bias\" in dialogue systems, evaluating how LLMs exhibit biases in role plays based on social categories of a role (e.g., \"Asian person\" or \"Yumi\u201d). The finding is corroborated by Zhao et al. (2024) who find that, although role-playing can improve the reasoning capabilities of LLMs, it also introduces potential risks, particularly in generating stereotypical and harmful outputs. While the few studies have con- tributed invaluable insight into the potential corre- lations between personality assignment and LLM toxic and/or biased performance, they have either focused on traditional personality types or social categories, the explanatory force of which is rather constrained."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Model Settings", "content": "We select three recent LLMs, considering their size, the language(s) that might have predominated their training, the potential ideological differences un- derlying their output (Atari et al., 2023; Naous et al., 2024), and the instruction-following capabil- ities that they demonstrated. For the open-source model, we adopt Llama-3.1-70B-instruct (Dubey et al., 2024) and Qwen2.5-72B-instruct (Yang et al., 2024). For the closed-source commercial model, we use GPT-40-mini-2024-07-18 (Hurst et al., 2024). To ensure the reproducibility of the experi- mental results, we set the temperature parameter to 0 for all models.\nLLM Personality Activation and Validation.\nBefore exploring how personality influences LLM bias and toxicity, we first evaluate whether the model can indeed take on the different personalities prompted by various personality descriptions from the HEXACO framework. Specifically, we design prompts based on performance descriptions corre- sponding to high and low scores in each personality dimension. We then administer the HEXACO-100- English personality tests (Lee and Ashton, 2018) on the selected models to evaluate whether they effectively embody the assigned personalities after prompting. Specific personality activation prompts for LLMs are provided in Appendix A."}, {"title": "3.2 Datasets", "content": "To comprehensively explore the impact of person- ality on LLM bias and toxicity, we incorporate various task formats for model evaluation.\nClosed-ended Tasks: For the closed-ended task, we utilize the multi-choice question answering dataset BBQ-AMBIGUOUS (Parrish et al., 2022), which covers 11 bias categories (see Appendix B) and consists of 29,246 QAs, each featuring a tar- get bias option. Ambiguous Contexts in BBQ are used to set up the general situation and introduce the two groups related to the questions, assessing the model's performance when there is insufficient evidence in the context. The correct answer in all ambiguous contexts is the \"UNKNOWN option\".\nThe ambiguous samples of BBQ are more chal- lenging than the disambiguous samplews, which justifies our decision to focus on it. By evaluating selected models on this dataset, we aim to assess their tendency to select biased responses.\nOpen-ended Tasks: For the open-ended task, we use two text generation datasets: BOLD (Dhamala et al., 2021) and REALTOXI- CITYPROMPTS (Gehman et al., 2020). BOLD is an open-ended language generation dataset that provides English text generation prompts for bias benchmarking across five domains. In our experiments, we randomly sample 600 instances from each domain while ensuring an equal number of samples from each subgroup. If the total sample"}, {"title": "3.3 Evaluation Methods", "content": "We employ different evaluation methods for the closed-ended dataset and open-ended datasets, con- sidering that the latter has no annotations.\nFor labeled questions in the closed-ended dataset BBQ, we follow Parrish et al. (2022) and adopt the \"bias score in ambiguous contexts\" to systematically quantify the extent of bias in the model's answers:\n$S_{bias} = (1 - acc)(\\frac{2n_{biased\\_ans}}{N_{non-unknown\\_ans}} - 1)$ (1)\nwhere $acc$ represents the accuracy of the model output on the given questions. $n_{biased\\_ans}$ and $N_{non-unknown\\_ans}$ represent the number of model out- puts that reflect the targeted social bias, and the number of model outputs that do not belong to the \"unknown\" choice, respectively. A bias score of 0% indicates no bias in the model's responses, while 100% means all answers reflect the targeted social bias, and -100% indicates all answers are against the targeted bias. We then quantify the correlation by subtracting $S_{bias}$ obtained from high-score and low-score personality traits.\nFor the open-ended text generation tasks, we adopt the Sentiment Reasoner (Vader) score $S_{VAD}$ (Hutto and Gilbert, 2014) based on Valence Aware Dictionary and the toxicity score $S_{tox}$ from a widely-used toxicity classifier (PERSPECTIVE API 2). Specifically, Vader is a rule-based model for sentiment analysis that calculates sentiment scores using valence-based lexicons and the com- bination of the lexicons and rules. For each input, it generates a score $S_{VAD}$ in the range of -1 to 1, where -1 indicates a negative sentiment and 1 in- dicates a positive sentiment. Following Dhamala et al. (2021), we utilize a threshold of $\\geq$ 0.5 to classify positive sentiment, and < -0.5 to classify\n$S_{open} [S_{VAD} + (1 - \\frac{S_{VAD}}{Impact on sentiment}) + (1 - \\frac{S_{tox}}{Impact on toxicity}) ]$ (2)\nWe then subtract the $S_{open}$ obtained from high- score and low-score personality traits to quantify the impact, similar to what we did with the closed- ended dataset."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Validation of LLM Personality", "content": "Figure 2 presents the evaluation scores of three se- lected models on the HEXACO-100-English test, with and without HEXACO personality activation prompts. According to the results, the behavior of models is significantly influenced by the de- signed prompts. Specifically, after incorporating high-score personality prompts, where the model is instructed to simulate a personality trait based on a high-score description, its behavior exhibits a relatively high score on the personality test. Con- versely, when the model is instructed to simulate a personality trait based on a low-score description, the test result tends to approach the minimum value of 1. These findings align with our expectations and demonstrate that the personality activation prompts effectively align LLM behavior with human person- ality traits within the HEXACO framework, paving the way for further investigation into the impact of personality on LLM bias and toxicity."}, {"title": "4.2 Results on BBQ", "content": "Table 1 presents the evaluation results of the se- lected LLMs on the closed-ended QA dataset BBQ. For typographical reasons, the names of sample categories are abbreviated, with their full names provided in Appendix B. Interestingly, the results show that Qwen2.5 has consistently been lower in bias average scores than the other two models. Nevertheless, the three models have a more or less similar variation in their biased performance given certain personality traits. For example, changing the levels of Honesty-Humility and Agreeableness gives rise to more noticeable performance differ- ences. That is, when being assigned with high levels of Honesty-Humility and Agreeableness, the models tend to choose neutral, unbiased answers in the QA tasks, whereas low levels of these per- sonality traits result in more biased answers. In terms of specific types of biases, all the three LLMs demonstrate more biases toward disability (DS), na- tionality (NA), religion (RL), and the intersection between race and gender (RxG, e.g., black women). In contrast, less biases are produced in regard to socioeconomic status (SES)."}, {"title": "4.3 Results on BOLD", "content": "Evaluation results on the BOLD dataset are shown in Table 2. We first report the proportions of posi- tive and negative samples from sentiment analysis, as well as the scaled toxicity scores from toxic- ity analysis in separate columns. The impact of personality traits on the sentiment and toxicity of the LLMs has a high level of consistency. Com-"}, {"title": "4.4 Results on REALTOXICITYPROMPTS.", "content": "Table 3 shows the evaluation results on the REAL- TOXICITYPROMPTS dataset, reporting the propor- tions of positive and negative samples for senti- ment analysis, as well as the scaled toxicity scores for toxicity analysis. Similar to the results from BOLD, the three LLMs exhibit highly consis-"}, {"title": "4.5 Case Study", "content": "Based on the findings in Section 4.3, one par- ticular trait that stands out is Honesty-Humility. When simulating low-score Honesty-Humility per- sonality, the model shows the most significant de- crease in both sentiment and toxicity scores. There- fore, in Table 4, we present a case illustrating the differences in responses from GPT-40-mini to a prompt from REALTOXICITYPROMPTS, and exam- ine how personalities with low Honesty-Humility scores generate lower levels of negative sentiment and toxicity. As shown in Table 4, compared to other personality traits, models with low levels of Honesty-Humility still generate excessively flatter- ing responses, even when the prompt leads to ag- gressive replies. This pattern is also observed in"}, {"title": "5 Discussion", "content": "Figure 3 provides an overview of the impact that various personality traits have on LLM bias, sen- timent, and toxicity. Interestingly, our findings mirror the bias and toxicity patterns observed with humans in social psychology research(Crawford and Brandt, 2019; Hu et al., 2022; Rafienia et al., 2008). For the Agreeableness personality, regard- less of whether in question-answering or text gener- ation tasks, higher scores are negatively correlated with bias, sentiment, and toxicity. Extraversion and Openness to Experience have a more signifi- cant impact on text generation tasks; models with higher scores in these traits tend to produce fewer negative and toxic responses. The pattern for Emo- tionality is less consistent, but it is evident that both high and low scores lead to an increase in negative responses in text generation tasks. Con- scientiousness has the smallest effect on the model in our experiments, showing no significant differ- ences compared to the base model. Models with a high score in The Honesty-Humility demonstrate lower bias and toxicity in both QA tasks and text generation tasks. Personality with low score of The Honesty-Humility has the greatest influence on the proportion of positive responses in text generation tasks, because low The Honesty-Humility models tend to generate excessively flattering language. Therefore, for question-answering tasks, activat- ing personalities with high score Agreeableness and Honesty-Humility help mitigate bias. For text generation tasks, simulating high Agreeableness, The Honesty-Humility, Extraversion, and Openness to Experience serves as a low-cost, widely appli- cable, and effective strategy to reduce bias and toxicity in LLMs. It is not recommended that sim- ulating low Honesty-Humility scores as a toxicity mitigation strategy, prolonged use of this person- ality type to mitigate toxicity may erode user trust in the LLM, and in some contexts, the model may insincerely agree with the user, leading to flawed decision-making. Fanous et al. (2025) also em- phasizes a similar point: in order to cater to hu- man preferences, LLMs may sacrifice authenticity to display flattery. This behavior not only under- mines trust but also limits the reliability of LLMs in many applications. In addition, we also observe that low Agreeableness and Extraversion scores significantly exacerbate these issues, particularly low Agreeableness, which requires caution when developing personalized LLMs to avoid simulating low Agreeableness personalities or roles."}, {"title": "6 Conclusion", "content": "This study explores the impact that specific per- sonality traits have on LLMs' generation of bi- ased and toxic content. Leveraging the HEX- ACO framework, the findings illuminate consis- tent variations of three different LLMs, similar to the socio-psychological and behavioural pat- terns of humans. The high levels of Agreeable- ness and Honesty-Humility in particular help re- duce LLM bias, while high levels of Agreeableness,"}]}