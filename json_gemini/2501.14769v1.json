{"title": "A survey on pioneering metaheuristic algorithms between 2019 and 2024", "authors": ["Tansel Dokeroglu", "Deniz Canturk", "Tayfun Kucukyilmaz"], "abstract": "This review examines over 150 new metaheuristics of the last six years (between 2019-2024), underscoring\ntheir profound influence and performance. Over the past three decades, more than 500 new metaheuristic\nalgorithms have been proposed, with no slowdown in sight-an overwhelming abundance that complicates\nthe process of selecting and assessing the most effective solutions for complex optimization challenges. Our\nevaluation centers on pivotal criteria, including annual citation metrics, the breadth of addressed problem\ntypes, source code availability, user-friendly parameter configurations, innovative mechanisms and operators,\nand approaches designed to mitigate traditional metaheuristic issues such as stagnation and premature\nconvergence. We further explore recent high-impact applications of the past six years' most influential 23\nmetaheuristic algorithms, shedding light on their advantages and limitations, while identifying challenges\nand potential avenues for future research.", "sections": [{"title": "1. Introduction", "content": "Over the past decade, a remarkable surge of metaheuristic algorithms has redefined the field, making it\na challenge to distinguish the most impactful ones Hussain et al. (2019a); Dokeroglu et al. (2019); Agrawal\net al. (2021). With innovation accelerating, selecting the most effective algorithms has become increasingly\ndemanding for researchers and practitioners alike. Recognizing this, we conducted an in-depth review of\nmetaheuristics introduced in the past six years, focusing on their influence and effectiveness. We evaluated\nthese algorithms across essential criteria: citation frequency, diversity in tackled problem types, code avail-\nability, ease of parameter tuning, introduction of novel mechanisms, and resilience to issues like stagnation\nand early convergence. Out of 158 algorithms, we identified 23 that set themselves apart, each contributing\nunique solutions to long-standing optimization challenges. These algorithms stand out for their versatility\nand innovation, positioning them as valuable assets for advancing research and addressing complex real-world\nproblems. Our review offers a detailed analysis of these algorithms, comparing their strengths, limitations,\nsimilarities, and applications, while highlighting promising trends and future pathways in metaheuristic\nresearch."}, {"title": "2. Previous surveys", "content": "This section summarizes metaheuristic survey/review articles published between 2019 and 2024. Hussain\net al. (2019a) reviewed 1222 publications from 1983 to 2016, addressing four key dimensions: new algorithms,\nmodifications, comparisons, and future research gaps, with the objective of highlighting potential open\nquestions and critical issues raised in the literature. The work provides guidance for future research to\nbe conducted more meaningfully that can serve the advancement of this area of research. Halim et al.\n(2021) studied simulation-driven metaheuristic algorithms that outperform deterministic ones in solving\nvarious problems, but their stochastic nature can result in varied solution quality. Accurate performance\nassessment requires appropriate measurement tools focusing on both efficiency-speed and convergence-and\neffectiveness-solution quality-while statistical analysis is crucial for evaluating effectiveness. Wong &"}, {"title": "3. The most influential metaheuristics (between 2019 and 2024)", "content": "In this section, we provide a comprehensive overview of the latest advancements in metaheuristic algo-\nrithms developed between 2019 and 2024, showcasing key examples of their state-of-the-art applications.\nOur selection criteria included citation impact, solution efficacy, adaptability across diverse domains, and\ninnovations in exploration-exploitation techniques as well as mechanisms for managing local optima. We\nanticipate that these metaheuristics will gain greater prominence and see more widespread application in\nthe coming years, distinguishing them from other recent algorithms in the field."}, {"title": "3.1. Harris Hawk Optimization", "content": "The Harris Hawk Optimization (HHO) algorithm is a powerful metaheuristic inspired by the coopera-\ntive hunting strategies of Harris Hawks, known for their group-based tactics and sudden attacks on prey\n(Heidari et al., 2019; Alabool et al., 2021). The HHO algorithm is particularly effective in solving complex\noptimization problems characterized by non-linearity, high dimensionality, and multiple local optima. HHO\nsimulates hawks scouting for prey and executing coordinated surprise pounces, enabling both exploration\n(broad search) and exploitation (local search) of the solution space. See Figure 2 for the steps of the HHO\naccording to the energy level (E), q and r values.\nHHO starts with a random population of candidate solutions, which are evaluated and iteratively updated\nbased on dynamic behaviors that mimic real hawk hunting. During the exploration phase, the algorithm\nsearches broadly to avoid local optima. If a promising area is identified, the exploitation phase begins,\nwhere hawks perform strategic, sudden moves to converge quickly on high-quality solutions. These moves\nare influenced by adaptive parameters that mimic the prey's escape patterns, helping balance between global\nand local searches."}, {"title": null, "content": "Initialize a population of N hawks, represented by:\n$$X_i = (X_{i,1}, X_{i,2},..., X_{i,d}), i = 1, 2, . . ., N,$$"}, {"title": null, "content": "where d is the dimension of the search space. The positions of the hawks are initialized randomly within the\nproblem boundaries. During the exploration phase, Hawks randomly search for prey based on their current\nposition and a reference leader (best solution found so far):\n$$X_i(t+1) = X_{rand}(t) \u2013 r_1|X_{rand}(t) \u2013 2r_2X_i(t)|,$$"}, {"title": null, "content": "where Xrand is a randomly chosen hawk, r\u2081 and r2 are random numbers uniformly distributed in [0, 1].\nThe transition from exploration to exploitation depends on the prey's behavior and escape energy E:\n$$E = 2E_o(1-\\frac{t}{T}),$$"}, {"title": null, "content": "where Eo is a random number in [-1,1], t is the current iteration, and T is the maximum number of\niterations.\nIf E\u2265 0.5, the hawks perform the soft besiege (Exploitation Phase):\n$$X_i(t + 1) = \u2206X(t) \u2013 E|JX_{best} (t) \u2013 X_i(t)|,$$"}, {"title": null, "content": "where \u2206X(t) is the difference between the best and current solutions, and J is a random jump strength\ncoefficient.\nIf E < 0.5, the hawks perform the hard besiege:\n$$X_i(t+1) = X_{best} (t) - E|X_{best} (t) \u2013 X_i(t)|.$$"}, {"title": null, "content": "In case of random attacks or surprise pounces, hawks simulate abrupt dives:\n$$X_i(t+1) = X_{prey}(t) \u2013 E(|X_{prey}(t) \u2013 X_i(t)|\u00b3),$$"}, {"title": null, "content": "where \u1e9e is a control parameter that simulates the sudden movements.\nKamboj et al. (2020) enhanced the global search capabilities and prevented local optima, a hybrid vari-\nant called the HHO-Sine Cosine Algorithm (hHHO-SCA). This variant integrates the Sine-Cosine Algorithm\n(SCA) exploration mechanisms into the HHO to improve its performance. The hHHO-SCA has been tested\non complex, nonlinear, non-convex, and highly constrained engineering design problems. Results demon-\nstrated that hHHO-SCA outperformed the standard SCA, HHO, and other optimization algorithms like\nAnt Lion Optimizer, Moth-Flame Optimization, and Grey Wolf Optimizer. The proposed algorithm showed\nsuperior performance across diverse optimization problems, supporting its effectiveness in solving multidis-\nciplinary design and engineering tasks. Elgamal et al. (2020) introduced CHHO that has two significant\nenhancements to the standard HHO. First, chaotic maps are applied during the initialization phase to im-\nprove population diversity, allowing better search space exploration. Second, Simulated Annealing (SA) is\nintegrated to refine the current best solution, boosting the algorithm's exploitation capabilities. Too et al.\n(2019) proposed Quadratic Binary HHO (QBHHO) that aims to improve the exploration and exploitation\nbalance, providing better solutions for feature selection. The effectiveness of BHHO and QBHHO was\nvalidated using 22 datasets from the UCI machine learning repository."}, {"title": "3.2. Butterfly optimization algorithm", "content": "The Butterfly Optimization Algorithm (BOA) is a metaheuristic based on the foraging and mating\nbehavior of butterflies (Arora & Singh, 2019). It simulates the way butterflies use their sense of smell to find"}, {"title": null, "content": "Butterflies perceive the quality of a solution (fitness) via sensory perception modeled as fragrance. The\nfragrance is defined as:\n$$f_i = c. I_i$$"}, {"title": null, "content": "where fi is the fragrance of butterfly i, c is a constant, Ii is the fitness of the solution (smell intensity),\nand a is a sensory modality parameter, controlling the degree of perception. The movement of butterflies\nis controlled by both global and local search strategies, depending on the fragrance perceived. The global\nsearch allows butterflies to move towards the best solution in the population:\n$$X_i(t+1) = X_i(t) + r \u00b7 f_i. (X_{best} - X_i(t))$$"}, {"title": null, "content": "where Xbest is the best solution found so far, r is a random number in [0, 1], and fi is the fragrance of\nbutterfly i. For local search, the movement is determined by the fragrance of nearby butterflies:\n$$X_i(t+1) = X_i(t) + r \u00b7 f_i \u00b7 (X_j (t) \u2013 X_k(t))$$"}, {"title": null, "content": "where X(t) and Xk(t) are two randomly selected butterflies. To switch between global and local search,\na random switching probability p is introduced:\n$$p = rand(0, 1)$$"}, {"title": null, "content": "If p is less than a threshold po, a global search is performed; otherwise, local search is executed. This\nmechanism ensures a balance between exploration and exploitation. The sensory modality parameter a is\nadapted over iterations to fine-tune the algorithm:\n$$a(t) = a_{min} + (a_{max} - a_{min}) \\frac{t}{T}$$"}, {"title": null, "content": "where amin and amax define the range for the sensory modality, t is the current iteration, and T is the\ntotal number of iterations.\nTubishat et al. (2020) introduces Dynamic BOA (DBOA), addressing its limitations in high-dimensional\nproblems, such as local optima stagnation and lack of solution diversity. By incorporating a Local Search\nAlgorithm Based on Mutation (LSAM), DBOA improves solution diversity and avoids local optima. Ex-\nperiments on 20 UCI benchmark datasets show that DBOA outperforms other algorithms across various\nperformance metrics. Makhadmeh et al. (2023) introduced information about the BOA to illustrate the\nessential foundation and its relevant optimization concepts. In addition, the BOA inspiration and its math-\nematical model are provided with an illustrative example to prove its high capabilities. Subsequently, all\nreviewed studies are classified into three main classes based on the adaptation form, including original,\nmodified, and hybridized. The main BOA applications are also thoroughly explained. Furthermore, the\nBOA advantages and drawbacks in dealing with optimization problems are analyzed. Finally, the paper is\nsummarized in conclusion with the future directions that can be investigated further. Alweshah et al. (2022)\napplied the monarch BOA (MBO) algorithm with a wrapper FS method using the KNN classifier. Tested\non 18 benchmark datasets, MBO outperformed four metaheuristic algorithms (WOASAT, ALO, GA, and\nPSO), achieving an average classification accuracy of 93% and significantly reducing the feature selection\nsize. The results demonstrate MBO's effectiveness and efficiency in FS, with a strong balance between global\nand local search."}, {"title": "3.3. Gradient-based optimizer", "content": "Gradient-based optimizer (GBO) combines the gradient and population-based methods, the search direc-\ntion is specified by the Newton's method to explore the search domain utilizing a set of vectors and two main\noperators (i.e., gradient search rule and local escaping operators). Minimization of the objective function\nis considered in the optimization problems (Ahmadianfar et al., 2020; Daoud et al., 2023). Gradient-based\noptimizers operate by calculating gradients essentially the slope or rate of change of the function with\nrespect to the model parameters and then updating these parameters in the direction that reduces the\nobjective function, aiming for an optimal or near-optimal solution.\nOne of the most widely used gradient-based optimizers is Stochastic Gradient Descent (SGD) (Amari,\n1993), which updates parameters based on the gradient calculated for a single or mini-batch of samples.\nThis approach is faster than full-batch gradient descent (Hinton et al., 2012), particularly for large datasets,"}, {"title": null, "content": "but can suffer from noisy updates and may struggle with complex optimization landscapes. To address these\nissues, variants like Momentum, Nesterov Accelerated Gradient, Adagrad, RMSprop, and Adam (Adaptive\nMoment Estimation) have been developed.\nGradient-based optimizers are essential in fields like deep learning, reinforcement learning, and computer\nvision, where they enable efficient training of large models by focusing on regions in parameter space that\nprogressively reduce error. However, their reliance on gradients also makes them susceptible to challenges\nlike getting trapped in local minima or saddle points, especially in high-dimensional non-convex problems.\nConsequently, researchers are continuously developing enhancements and alternative algorithms to make\nthese optimizers more robust across various machine learning applications.\nPremkumar et al. (2021) introduced a multiobjective GBO (MOGBO), for solving multiobjective truss-\nbar design problems. MOGBO employs a gradient-based approach with operators like the local escaping\noperator and gradient search rule, using non-dominated sorting and crowding distance mechanisms to achieve\nPareto optimal solutions. Performance tests on various benchmark problems show MOGBO outperforms\nother algorithms in accuracy, runtime, and metrics like hyper-volume and diversity, proving its effectiveness\nin complex multiobjective optimization tasks. Jiang et al. (2021) proposed eight variants of the binary\nGBO utilizing S-shaped and V-shaped transfer functions to convert the search space to a discrete format.\nThe performance of these binary GBO algorithms is evaluated on 18 UCI datasets and 10 high-dimensional\ndatasets, comparing them against other feature selection methods. Results indicate that one binary GBO\nvariant outperforms other algorithms, demonstrating superior overall performance in various metrics. Helmi\net al. (2021) introduced a new algorithm (GBOGWO), a feature selection method that enhances the GBO\nwith Grey Wolf Optimizer (GWO) operators, to address high-dimensional data challenges and improve HAR\nclassification. Using UCI-HAR (Human Activity Recognition) and WISDM datasets, GBOGWO achieved an\naverage classification accuracy of 98%, demonstrating its effectiveness in refining HAR model performance."}, {"title": "3.4. Slime mould algorithm", "content": "The Slime Mould Algorithm (SMA) primarily simulates the behavior and morphological changes of the\nslime mould Physarum polycephalum during its foraging process, rather than modeling its entire life cycle\n(Li et al., 2020; Chen et al., 2023). This organism is a eukaryote that thrives in cold, humid environments.\nThe primary nutritional stage is the plasmodium, which represents the active and dynamic phase of the\nslime mould and is the focal point of this survey. During this phase, slime mould actively searches for food,\nencircles it, and releases enzymes for digestion. As it migrates, the leading edge expands into a fan shape,\nsupported by a network of interconnected veins that facilitate the flow of cytoplasm, as illustrated in Fig. 4.\nDue to their unique structure and behavior, slime moulds can simultaneously utilize multiple food sources,\nforming a network that connects them. See Figure 4 for the foraging morphology of slime mould.\nThe slime mould is capable of locating food sources by detecting odours in the air. To mathematically"}, {"title": null, "content": "model this foraging behavior, the following formula have been proposed to simulate the contraction mode.\nThe slime mould can navigate towards food sources by detecting odours in the air. To mathematically\nrepresent this approaching behavior, the following formulas have been proposed to simulate the contraction\nmode:\n$$X(t+1) = {\\begin{cases}\nX(t) + vb. (W. (x(t) \u2013 XB(t))),\nr<p \\\\\nvc. X(t),  r\u2265p\n\\end{cases}}$$"}, {"title": null, "content": "where vb is a parameter that ranges from [\u2212a, a], and vc decreases linearly from one to zero. The variable t\ndenotes the current iteration, \u2717 indicates the location of the individual with the highest odour concentration\ndetected, X represents the position of the slime mould, and XA and XB are two individuals randomly\nselected from the slime mould population. Additionally, W signifies the weight of the slime mould which is\nformulated as follows:\n$$W(SmellIndex(i)) = {\\begin{cases}\n1+r.log( \\frac{bF-S(i)}{OF-WF}+1) , & condition \\\\\n1-r log (\\frac{bF-wF}{\\vert F-S\\vert+1}), & otherwise\n\\end{cases}}$$"}, {"title": null, "content": "where \"condition\u201d indicates that S(i) ranks in the top half of the population, r represents a random\nvalue within the interval [0, 1], bF signifies the best fitness value achieved during the current iteration, wF\nrepresents the worst fitness value obtained thus far in the iterative process, and SmellIndex refers to the\nsequence of fitness values sorted in ascending order for the minimum value problem.\nThe position of the searching individual X can be updated based on the best location \u2717 currently\nidentified, and the adjustment of parameters vb, vc, and W can modify the individual's location. The\ninclusion of the random variable in the formula allows individuals to create search vectors at any angle,"}, {"title": null, "content": "enabling them to explore the solution space in all directions, which enhances the algorithm's potential for\nfinding the optimal solution.\nThe next step is the contraction mode of the venous tissue structure of slime mould when searching.\nThe greater the concentration of food encountered by the vein, the stronger the wave produced by the\nbio-oscillator, resulting in a faster flow of cytoplasm and a thicker vein. Equation 13 mathematically models\nthe positive and negative feedback between the vein width of the slime mould and the food concentration\nthat was investigated. The component r in Equation 13 represents the uncertainty in the mode of venous\ncontraction. The logarithm is utilized to moderate the rate of change in numerical values, ensuring that the\ncontraction frequency does not fluctuate excessively. The \"condition\" reflects how the slime mould adjusts\nits search patterns based on food quality. When food concentration is high, the weight in that area increases;\nconversely, when food concentration is low, the weight diminishes, prompting the slime mould to explore\nnew regions. Based on the aforementioned principles, the mathematical formula for updating the location\nof the slime mould is as follows:\n$$X* = {\\begin{cases}\nrand (UB - LB) + LB, & if rand < z \\\\\nX(t) + vb. (W.XA(t) \u2013 XB(t)), & if r < p \\\\\nvc. X(t),& if r \u2265 p\n\\end{cases}}$$"}, {"title": null, "content": "where LB and UB represent the lower and upper boundaries of the search range, respectively, and rand\nand r signify random values within the interval [0, 1], z is used for oscillation.\nChen et al. (2023) studied and analyzed key research related to the development of the SMA. A total of 98\nSMA-related studies were retrieved, selected, and identified from the Web of Science database. The review\nfocuses on two main aspects: advanced versions of the SMA and its application domains. Premkumar et al.\n(2020) presented a Multi-objective SMA (MOSMA) for tackling multi-objective optimization challenges in\nindustrial settings, based on the oscillatory behaviors of slime mould in laboratory experiments. MOSMA\nintegrates the core principles of SMA with elitist non-dominated sorting and a crowding distance operator to\nensure broad coverage of Pareto optimal solutions. Tested across 41 diverse case studies, MOSMA outper-\nformed existing algorithms (MOSOS, MOEA/D, MOWCA) on several performance metrics, demonstrating\nits strong capability for handling complex multi-objective optimization problems. Houssein et al. (2022)\ndeveloped a multi-objective SMA, called MOSMA, for solving complex multi-objective optimization prob-\nlems. MOSMA incorporates an external archive to store and manage Pareto optimal solutions, simulating\nthe social behaviors of slime mould in a multi-objective search space. Validated on CEC'20 benchmarks and\nvarious engineering problems, MOSMA outperforms six established algorithms (e.g., MOGWO, NSGA-II)\nin terms of solution proximity to the Pareto set and inverted generational distance, proving its strength in\nreal-world applications like automotive helical coil spring optimization."}, {"title": "3.5. Marine Predators Algorithm", "content": "The Marine Predators Algorithm (MPA) simulates the behavior of marine predators foraging in the\nocean (Faramarzi et al., 2020a). The algorithm primarily relies on different movement phases that represent\nvarious predation strategies based on the interaction between predators and prey. See Figure 5 for the phases\nof the MPA."}, {"title": null, "content": "In the initial exploration phase, the algorithm uses random movements inspired by L\u00e9vy flights. The\nposition of each predator Xi at iteration t + 1 is updated as follows:\n$$X^{t+1}_i = X^t_i + r \u00b7 L\u00e9vy(\u03bb) \u00b7 (X^{*}_{best} \u2013 X^t_i),$$"}, {"title": null, "content": "where r is a random number in the range [0, 1], L\u00e9vy(\u03bb) represents a L\u00e9vy flight with scaling parameter \u03bb,\nand Xbest is the position of the best solution found so far.\nThe exploitation phase adjusts the movement based on a \"Brownian motion\" mechanism if the prey is\nclose to the predator. The position update in this phase is given by:\n$$X^{t+1}_i = X^{*}_{best} + B. (X^t_i - X^{*}_{best}),$$"}, {"title": null, "content": "where B represents a random Brownian motion.\nAlternatively, if the prey is farther away, the algorithm uses a different movement strategy:\n$$X^{t+1}_i = X^{*}_{best} + F (X^t_i - X^{*}_{mean}),$$"}, {"title": null, "content": "where F is a random factor, and Xmean is the mean position of all solutions at iteration t.\nTo model the escape of prey, a diversification strategy is applied:\n$$X^{t+1}_i = X^t_i + S (X^t_i - X^{*}_{worst}),$$"}, {"title": null, "content": "where S is a scaling factor, and Xt worst is the position of the worst solution.\nAbd Elminaam et al. (2021) introduced MPA-KNN, a novel hybridization of the MPA and k-Nearest\nNeighbors (k-NN), to improve feature selection for medical datasets, with feature sizes ranging from tiny\nto massive. Experimental results show that MPA-KNN outperforms eight well-regarded metaheuristic al-\ngorithms in accuracy, sensitivity, and specificity across 18 UCI medical benchmarks, underscoring its ef-\nfectiveness for optimal feature selection. Ramezani et al. (2021) proposed an enhanced MPA variant that\nintegrates opposition-based learning, chaotic mapping, self-adaptive population techniques, and an adaptive\nphase-switching mechanism for improved exploration and exploitation. Simulations conducted on CEC-06\n2019 test functions and a real-world control problem applied to a DC motor indicate that the improved al-\ngorithm significantly outperforms the original MPA and five other optimization algorithms in accuracy and\nrobustness. Abdel-Basset et al. (2021) presented an enhanced MPA for optimized photovoltaic parameter\nextraction, incorporating a population improvement strategy where adaptive mutation enhances high-quality\nsolutions, and low-quality solutions are updated based on the best and high-ranked solutions. Experimental\nresults demonstrate that the proposed algorithm offers superior accuracy, showing a high correlation with\nmeasured current-voltage data and proving effective for parameter estimation."}, {"title": "3.6. Equilibrium optimizer", "content": "The Equilibrium Optimizer (EO) is inspired by dynamic mass balance models used in control systems,\nwhere a system reaches equilibrium (Faramarzi et al., 2020b; Makhadmeh et al., 2022). EO mimics the\nprocess of reaching equilibrium through iterations, balancing exploration and exploitation using the control\nmechanism of concentration updating. The algorithm leverages different equilibrium candidates and adaptive\ncontrol to guide the search process. The algorithm maintains an equilibrium pool consisting of multiple\nequilibrium candidates. The update for each individual's position towards these candidates is given by:\n$$X_i(t+1) = X_i(t) + (X_{eq}(t) \u2212 X_i(t)) + F \u00b7 (X_{eq}(t) - X_{rand}(t))$$"}, {"title": null, "content": "where Xeq(t) is the position of the equilibrium candidate at iteration t, A is the random control pa-\nrameter for exploration, F is the control parameter for exploitation, and Xrand(t) is a random solution to\nintroduce diversity. The parameters A and F are updated dynamically over time to balance exploration and\nexploitation:\n$$\u03bb=1-\\frac{t}{T}$$\n$$F = rand(0, 1)$$"}, {"title": null, "content": "where: t is the current iteration, and T is the maximum number of iterations. The equilibrium candidates\nin the pool are updated to reflect the best solutions found so far. This ensures that individuals are attracted\ntowards high-quality solutions while maintaining diversity:\n$$X_{eq}(t+1) = X_{best} (t) + \u03b2. (X_{best}(t) \u2013 X_{mean}(t))$$"}, {"title": null, "content": "Where: Xbest (t) is the best solution at iteration t, Xmean(t) is the mean solution across the population,\nand \u1e9e is a constant controlling the influence of the best solution. As iterations proceed, the control param-\neters A and F help the algorithm converge towards the equilibrium by reducing random fluctuations and\nencouraging exploitation.\nWang et al. (2021b) proposed an improved EO using a neural network to enrich photovoltaic cell data,\nenhancing optimization efficiency. Tested on three diode models, it outperforms other algorithms, achieving\nlower error rates and improving both precision and reliability, making it highly effective for photovoltaic cell\nparameter estimation. Abdel-Basset et al. (2020) presented an improved IEO that integrates linear reduction\ndiversity (LRD) and local minima elimination (MEM) to enhance solution accuracy and convergence. By\ndirecting poor fitness particles toward optimal solutions, LRD accelerates convergence, while MEM reduces\nentrapment risks. Extensive tests on photovoltaic models demonstrate IEO's competitive performance,\nshowing superior optimization for solar cell applications. Gao et al. (2020a) introduced two binary EO\n(BEO) for feature selection, designed for classification tasks. The first maps the continuous EO into discrete\nforms using S-shaped and V-shaped transfer functions (BEO-S and BEO-V), while the second (BEO-T)\nuses the current optimal position. Tests on 19 UCI datasets show BEO-V2 outperforms other methods\nsignificantly."}, {"title": "3.7. Aquila Optimizer", "content": "The Aquila Optimizer (AO) is inspired by the hunting behavior of Aquila, a bird of prey Sasmal et al.\n(2023). AO mimics Aquila's powerful and efficient hunting strategies, combining exploration and exploitation\nto search for global optima. The algorithm consists of different movement strategies, which are applied\ndynamically to balance exploration of the search space and exploitation of promising regions (see Figure 6\nfor its soar and vertical dive behavior).\nThe initial population of Aquilas is randomly generated. AO dynamically switches between different\nmovement strategies depending on the stage of the hunt:\n$$X_i(t+1) = X_{best}(t) + \u03b1 \u00b7 F(X_{best} (t) - X_i(t))$$"}, {"title": null, "content": "During the hunting phase, the distance between the Aquila and the prey is calculated, influencing its\nstrategy:"}, {"title": null, "content": "$$D = |C. X_{best}(t) - X_i(t)|$$"}, {"title": null, "content": "where D is the distance between Aquila i and the prey, C is a coefficient representing the influence of\nthe prey's position. In certain situations, Aquilas perform a dive to capture the prey with more precision,\nexpressed as:\n$$X_i(t+1) = X_{best}(t) + D \u00b7 e^{b\u00b7l} \u00b7 sin(2\u03c0l)$$"}, {"title": null, "content": "where b controls the width of the dive, I is a random variable controlling the angle of the attack and\ne is the base of the natural logarithm, indicating the sharpness of the dive. AO uses adaptive parameters\nto adjust the search dynamically. For example, a changes with time to balance between exploration and\nexploitation:\n$$\u03b1 = 2 \u00b7 (1 - \\frac{t}{T}) \u00b7 rand(0, 1)$$"}, {"title": null, "content": "where T is the total number of iterations, and t is the current iteration number.\nAl-qaness et al. (2022) addressed the shortcomings of the Adaptive Neuro-Fuzzy Inference System (AN-\nFIS) model in oil production estimation by optimizing its parameters with a modified AO and Opposition-\nBased Learning (OBL). The proposed model outperforms several modified ANFIS models and time-series\nforecasting methods using real-world datasets and performance metrics like Root Mean Square Error (RMSE)\nand Mean Absolute Error (MAE). Mahajan et al. (2022) introduced a hybrid optimization method combin-\ning AO and Arithmetic Optimization Algorithm (AOA) to enhance convergence and solution quality. The\nproposed AO-AOA approach is evaluated on various problems, including image processing and engineering\ndesign, with consistent performance across both high- and low-dimensional problems. Population-based"}, {"title": "3.8. Seagull Optimization", "content": "Dhiman & Kumar (2019) introduced the Seagull Optimization Algorithm (SOA), a bio-inspired approach\nbased on seagull migration and attack behaviors to enhance exploration and exploitation within a search\nspace. The SOA's performance is benchmarked against nine popular metaheuristics across forty-four test\nfunctions, with evaluations of its computational complexity and convergence behavior. Additionally, SOA is\napplied to seven constrained real-world industrial problems, showcasing its effectiveness in addressing large-\nscale, complex optimization challenges. Experimental results demonstrate that SOA is highly competitive\nand well-suited for solving constrained, computationally expensive problems.\nSeagulls' behavior can be described as follows: (i) During migration, seagulls travel in groups, starting\nfrom different positions to prevent collisions with one another, (ii) within the group, seagulls orient their\nmovement toward the most fit individual, defined as the seagull with the lowest fitness value compared to the\nothers, (iii) using the position of the fittest seagull as a reference, the rest can adjust their initial positions.\nSeagulls often attack migrating birds over the sea while moving from one location to another, employing a\nnatural spiral movement during their attacks. Then, these behaviors can be formulated about an objective\nfunction for optimization purposes.\nDuring migration, the algorithm mimics the movement of the group of seagulls as they navigate from\none position to another. In this phase, a seagull must meet three conditions: (i) Collision avoidance: To\nprevent collisions with neighboring seagulls, an additional variable is A utilized in the calculation of the new\nposition for the search agent:"}, {"title": null, "content": "$$C_s = A \u00d7 P(x)$$"}, {"title": null, "content": "where Cs denotes the position of the search agent that does not collide with other search agents, P indicates\nthe current position of the search agent, x refers to the current iteration, and A represents the movement\nbehavior of the search agent within the specified search space. (ii) Movement toward the best neighbor's\ndirection: After avoiding collisions with their neighbors, the search agents proceed in the direction of the\nbest neighboring agent:\n$$M_s = B \u00d7 (P_{bs}(x) \u2013 P(x))$$"}, {"title": null, "content": "where Ms indicates the position of the search agent Ps in relation to the best-fit search agent Pbs (i.e., the\nfittest seagull). The behavior of B is randomized, which helps maintain an appropriate balance between"}, {"title": null, "content": "exploration and exploitation. (iii) Stay close to the best search agent: Finally", "agent": "n$$D^\u2019"}]}