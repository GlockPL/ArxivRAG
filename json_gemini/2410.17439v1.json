{"title": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment", "authors": ["Yang Zhong", "Jiangang Hao", "Michael Fau\u00df", "Chen Li", "Yuan Wang"], "abstract": "The recent revolutionary advance in generative AI enables the generation of realistic and coherent texts by large language models (LLMs). Despite many existing evaluation metrics on the quality of the generated texts, there is still a lack of rigorous assessment of how well LLMs perform in complex and demanding writing assessments. This study examines essays generated by ten leading LLMs for the analytical writing assessment of the Graduate Record Exam (GRE). We assessed these essays using both human raters and the e-rater\u00ae automated scoring engine as used in the GRE scoring pipeline. Notably, the top-performing GPT-40 received an average score of 4.67, falling between \"generally thoughtful, well-developed analysis of the issue and conveys meaning clearly\" and \"presents a competent analysis of the issue and conveys meaning with acceptable clarity\" according to the GRE scoring guideline. We also evaluated the detection accuracy of these essays, with detectors trained on essays generated by the same and different LLMs.", "sections": [{"title": "Introduction", "content": "Recent research (W.-L. Chiang et al., 2023; OpenAI et al., 2024; Touvron, Lavril, et al., 2023; Touvron, Martin, et al., 2023) has shown that extensive pretraining on diverse corpora of human-authored texts and subsequent refinement via human feedback (Ouyang et al., 2022) have enabled large language models (LLMs) to produce coherent and contextually relevant writing products. Despite these significant advancements, AI-generated texts are not necessarily perfect. Several ongoing community efforts, such as the General Language Understanding Evaluation (GLUE) (A. Wang, 2018), its enhanced version SuperGLUE (A. Wang et al., 2019), and the more recent Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021), are evaluating the quality of LLM-generated texts to benchmark the progress. However, most of the aforementioned benchmarks are designed in multi-choice Question Answering format, and LLMs optimized for benchmark performances are less robust against simple dataset perturbation Alzahrani et al. (2024). Meanwhile, there are also efforts to use LLMs to evaluate LLM-generated texts automatically. For example, Fu, Ng, Jiang, and Liu (2023) proposed an LLM-based evaluation protocol that interprets the model-predicted probability of specific token(s) as a quality score, while C.-H. Chiang and Lee (2023) and Liu et al. (2023) prompted LLMs to produce a Likert-style score ranging from 1 to 5 to assess LLM-generated texts.\nFrom an assessment perspective, most of these evaluation metrics are based on relatively simple tasks, such as question answering, summarization, next-word prediction, and so on. There are also findings that suggest the LLM-based automated metrics could be unfaithful (Shen, Cheng, Nguyen, You, & Bing, 2023; P. Wang et al., 2023). As such, there is still a lack of rigorous evaluation of AI-generated writing products based on challenging writing tasks with a well-established scoring procedure. This study addresses this gap by leveraging the Graduate Record Examination (GRE) analytical writing assessment to evaluate AI-generated essays. The GRE analytical writing is well known for assessing the test-taker's ability to think critically and communicate complex ideas effectively. It employs well-trained human raters and automated scoring engines, e-rater\u00ae (Attali & Burstein, 2006), to appraise written responses based on a multifaceted scoring framework (Educational Testing Service, n.d.-a). The assessment aims to gauge a candidate's prowess in articulating and substantiating complex thoughts, constructing and assessing arguments, and maintaining a coherent and focused discourse throughout the essay.\nWe instructed several state-of-the-art LLMs to produce essays in response to GRE writing prompts. The selection of LLMs for this study encompasses the latest in AI technology as of mid-2024, including GPT-40, Gemini, and Llama3-8b, as well as prior models dated back to mid-2023, including GPT-4, GPT-3.5 turbo, Google's Bard, Llama, Vicuna, and Koala. We included these earlier models to explore potential trends that might emerge over time. All the generated essays were then evaluated by human raters and ETS's automated scoring engine, e-rater\u00ae. By comparing the scores and language feature metrics of the generated essays, we aim to gain insights into AI-generated essays through a rigorous scoring framework set by the GRE program.\nOn the other hand, as AI technology advances, it becomes increasingly challenging to distinguish between human-written and AI-generated content (Tulchinskii et al., 2023; Weber-Wulff et al., 2023; Wu, Pang, Shen, Cheng, & Chua, 2023). This presents significant concerns, particularly in academic integrity and the preservation of authentic authorship. The detection accuracy of AI-generated texts is influenced by both the context and the contrast samples used. For large-scale writing assessments, AI-generated essays have been shown to be detectable with high accuracy (Hao & Fauss, 2024; Hao et al., 2024; Jiang, Hao, Fauss, & Li, 2024a, 2024b; Yan, Fauss, Hao, & Cui, 2023). However, as the number of high-performing LLMs continues to grow, it becomes increasingly important to benchmark the detectability of AI-generated essays across different LLMs and to assess the performance of detectors trained on various datasets.\nThis study aims to answer the following three research questions.\n\u2022 RQ1. What is the score distribution when LLM-generated essays are evaluated using"}, {"title": "Methods", "content": "This study aims to evaluate the quality of essays generated by a spectrum of advanced LLMs, encompassing proprietary and open-source technologies, by May 20, 2024. Within the proprietary sector, we chose the top ones, including GPT-40 from OpenAI and Gemini-1.5 from Google. These closed-source models represent the forefront of AI research in the industry, offering a glimpse into the state-of-the-art language generation capabilities. In addition to these new models, we also included some older ones, such as GPT-4 and GPT-3.5 turbo from OpenAI and Bard from Google, which appeared in the middle of 2023.\nOn the other hand, we also included some leading open-source LLMs in our evaluation. Specifically, we considered the recent Llama-3 8B model, which has been the subject of extensive research since its release (Dubey et al., 2024). In addition, we experimented with some older models from the middle of 2023, including the Llama-1 13B (Touvron, Lavril, et al., 2023), as well as its two finetuned variants: Vicuna (W.-L. Chiang et al., 2023) and Koala (Geng et al., 2023), both of which underwent specialized optimization processes to further refine their language generation mechanisms. The 13-billion parameter version was selected for each of these earlier models. In addition to the Llama family of models from Meta, we included another popular LLM, Mistral-7B-Instruct-v0.1, from Mistral AI, which is comparable in size to the Llama-3 model. We include model details in Appendix B."}, {"title": "Prompts and Essay Generation", "content": "The GRE analytical writing consists of a 30-minute writing task, \"analyze an issue\", which requires examinees to ponder a topic, evaluate its intricacies, and formulate a well-reasoned argument buttressed by examples and explanations. All writing prompts of the GRE writing assessment are publicly available (ETS, n.d.-b). To alleviate the risk of biases introduced by an individual writing prompt, we randomly selected two writing prompts for this study. To prompt LLMs for generating the essays, we sent the original GRE writing prompt plus an instruction on length: \"Please keep the response to about 500 words.\u201d, where the 500 words is the typical length of GRE essays. The resulting prompts are listed below.\n\u2022 Prompt 1: Governments should not fund any scientific research whose consequences are unclear. Write a response in which you discuss the extent to which you agree or disagree with the recommendation and explain your reasoning for the position you take. In developing and supporting your position, describe specific circumstances in which adopting the recommendation would or would not be advantageous and explain how these examples shape your position. Please keep the response to about 500 words.\n\u2022 Prompt 2: To understand the most important characteristics of a society, one must study its major cities. Write a response in which you discuss the extent to which you agree or disagree with the statement and explain your reasoning for the position you take. In developing and supporting your position, you should consider ways in which the statement might or might not hold true and explain how these considerations shape your position. Please keep the response to about 500 words\nWe generated 100 essays for each prompt from each LLM, leading to a total of 2,000 essays from the ten models. Each LLM comes with a set of hyperparameters that can be adjusted to control the text generation. Among these hyperparameters, temperature directly controls the variability of the generated texts. For comparison purposes, we set the temperature to 0.4 (This was used as the default value in earlier versions of the web-based ChatGPT to strike a balance between diversity and quality) for each model except Bard, where this parameter configuration was not accessible. Here, we caution that different LLMs may use varying scaling for their temperature, meaning that the same numerical value may affect the randomness and diversity of responses differently across models."}, {"title": "Evaluation and Detection", "content": "To address the first research question, we evaluated the essays based on the GRE scoring framework. The GRE program has a well-established scoring process involving well-trained human raters and an automated scoring engine, e-rater\u00ae (Educational Testing Service, n.d.-a). The human raters in this study were professional scoring experts in the GRE program, and they rated each essay with a score from 0 to 6 (see Appendix for more details). The e-rater\u00ae engine takes in an essay and outputs a final score as well as a set of language feature scores.\nTo address the second research question, we first checked how well the length of the generated essays adhered to our instructions in the prompt, as essay length is often positively correlated with essay scores in writing assessments. Then, we compared the pairwise similarity (semantic and verbatim) of generated essays, reflecting the diversity of the texts generated by the given LLMs. We then compared the e-rater\u00ae computed language features of these generated essays, such as grammar, usage, mechanics, styles, organization, development, and word complexity Attali and Burstein (2006); J. Chen, Fife, Bejar, and Rupp (2016). Finally, we compared the perplexity of the generated essays, which characterizes how likely a sequence of words (essay) is generated from a given language model. Most LLMs used in this study do not allow the direct computation of perplexity. So, we chose GPT2 as a common language model to compute the perplexity of essays generated from all LLMs. As such, the perplexity here is a relative measure of how likely the essays have been generated by GPT2. Prior research has shown that perplexity based on a common language model is very informative for detecting AI-generated essays (e.g., Hao and Fauss (2024); Jiang et al. (2024a)).\nTo address the third research question, we randomly sampled 100 human-written essays corresponding to each of the above two prompts from the GRE writing test in 2021 when ChatGPT-like AIs were not available. These samples were mixed with essays generated by the LLMs discussed earlier. Based on the e-rater\u00ae engine-generated language features and perplexity-based features from these data, we developed machine learning classifiers to detect AI-generated essays and evaluate their performance."}, {"title": "Results", "content": ""}, {"title": "Human Scores", "content": "We recruited expert human raters from the GRE program to score the 200 essays (100 from each of the two writing prompts) generated by each LLM. The raters followed the same scoring framework as in the GRE scoring operation and provided a score ranging from 0 to 6 for each essay (see Appendix for the definition of the scores). The source of the essay, i.e., written by humans or AI, has been deliberately hidden from the human raters, so they were unaware that some essays were generated by AI. The score distribution is show in Table 2. We observed that essays generated by the proprietary LLMs (e.g., GPT models and Gemini) generally received higher scores from human raters. Newer models in 2024 generally do better than the LLMs in 2023. The average scores are between 4 and 5, which correspond to \"presents a competent analysis of the issue and conveys meaning with acceptable clarity\" and \"generally thoughtful, well-developed analysis of the issue and conveys meaning clearly\u201d respectively according to the GRE scoring framework."}, {"title": "e-rater\u00ae Scores", "content": "In addition to human scores, we applied the e-rater\u00ae engine to score each AI-generated essay. The score distribution is listed in Table 2.\nCompared with human scores, e-rater\u00ae tended to give higher scores to AI-generated essays, though the proprietary LLMs still outperform the open-source counterparts. This is unsurprising since e-rater\u00ae was developed and calibrated based on human-written essays and showed good agreement with human scores J. Chen et al. (2016). The current language features, such as grammar, usability, mechanics, and so on, are sufficient for scoring human-written essays but fall short of properly scoring AI-generated essays, which display almost impeccable language usage but with shallow meanings. In contrast, human raters adeptly recognize these shortcomings and assign more accurate scores to AI-generated essays."}, {"title": "Essay Length, Similarity, Language Features, and Perplexity", "content": ""}, {"title": "Essay Length Adherence", "content": "We introduced a length requirement, i.e., about 500 words, when prompting LLMs to generate essays. However, not all LLMs follow the instructions strictly.  Overall, we observe that newer models exhibited smaller variances and better alignment with the 500-word instruction. For more recent models in 2024, GPT-40 and Gemini generated essays with lengths slightly greater than 500, while Llama3-8b generated an average of 460 words. For older models, close-sourced models performed better than open-sourced models (Mistral, Vicuna, Koala, and Llama) in following the length instruction. Essays generated by Llama displayed the largest length variability, indicating that the model struggled to consistently produce long and high-quality essays, which echoed the findings on essay quality."}, {"title": "Essay Similarity", "content": "The pairwise similarity of essays generated by the same LLM under the same prompt reflects consistency and variability under identical generation conditions (e.g., prompt, temperature). There are two commonly known types of similarities between texts: verbatim similarity, which describes how closely two texts match in exact wording or surface structure, and semantic similarity, which shows how similar two texts are in meaning, regardless of specific word choices. Comparing these two types of similarities for pairwise essays generated by LLMs under the same prompt could reveal how well the LLMs maintain meaning across different expressions of the same content.\nFor semantic similarity, we first convert each essay into a document vector using a popular neural embedding model jina-embedding-v3 model (Sturua et al., 2024). This embedding model was chosen based on its high performance on text similarity benchmarks of the MTEB English leaderboard\u00b9 and its superior performance on handling long documents. Once the essays are converted to vectors, we compute the cosine similarity between pairs of vectors corresponding to essays generated from the same LLM under the same prompt.  We observed that essays generated by the Llama model displayed the least similarity, comparable to human-written essays. Models such as GPT-4, Gemini, and GPT-40 tend to have distributions with higher mean cosine similarity, indicating that their generated essays are more semantically similar to each other, a sign of good alignment to the writing prompts. The distributions between the two prompts differ slightly, with some models demonstrating higher density and peaks for one prompt than another (i.e., GPT-40 has a higher density for prompt 1, while Llama3-8b obtains the highest mean similarity on prompt 2).\nFor verbatim similarity, we represent each essay as a trigram vector, a method proven effective in capturing textual similarities for test security applications (Choi, Hao, Li, Fauss, & Nov\u00e1k, 2024). Based on the trigram vectors, we computed pairwise cosine similarities for each pair of essays from the same LLM and prompt.  Overall, human essays displayed the lowest similarity, while different LLMs showed different levels of similar texts. Despite the essays from the same LLM and prompt showing a high level of semantic similarity, the verbatim similarity is much lower, though they are generally higher than that from human-written essays."}, {"title": "Language Features", "content": "The language features from e-rater\u00ae allow us to compare more detailed aspects of the generated essays across the LLMs.  It's important to note that grammar, mechanics, usage, and style features are negative by convention, as they represent errors\u2014the closer the value is to 0, the fewer the errors. In contrast, organization, development, and word complexity features are positive, with higher values indicating better performance in these areas.  Based on the results, we have the following observations:\nHumans are doing better in development but show weaker performance in grammar, mechanics, and usage. This suggests human writing is strong in structure and coherence but prone to technical errors compared to LLMs. The GPT family (particularly GPT-40 and GPT-4) shows strong performance in grammar, mechanics, and usage, with wide coverage on the radar plots, indicating an all-around high performance with fewer errors and more fluent text generation. The 2024 models generally perform better than the 2023 ones, suggesting steady progress in these LLMs."}, {"title": "Perplexity Distribution", "content": "Perplexity and perplexity-based features have been used to detect AI-generated essays. Specifically, it has been shown that essay perplexity alone can distinguish between ChatGPT-generated and human-written essays in high-stakes writing assessment (Hao & Fauss, 2024). In this study, we used GPT-2 as the underlying language model and adopted the implementation of perplexity by HuggingFace.\u00b2  The perplexity of human-written essays is much higher than that of AI-generated essays. Chronically, the newer models (GPT-40, Gemini) showed slightly higher perplexity than older models, which suggests the essays generated by these models deviated further from the GPT-2 baseline model than the other LLMs. The perplexity of essays from Llama shows the largest variability, indicating that the essays may not be consistently generated."}, {"title": "Detectability of AI-generated Essays", "content": "As the quality of essays generated by LLMs continues to improve, concerns are rising in education about students submitting AI-generated assignments, posing a significant challenge to academic integrity. The detection of AI-generated essays in large-scale assessments has been a widely discussed topic (Hao & Fauss, 2024; Jiang et al., 2024a; Yan et al., 2023), with most previous work focusing on detecting essays from a single LLM. However, with new and more powerful LLMs emerging every few months, it becomes increasingly difficult to identify which LLMs are being used. This highlights the importance of examining the detectability of AI-generated essays across multiple LLMs.\nIn particular, we consider two scenarios: within-model detectability and cross-model detectability. In the within-model scenario, we assess how well classifiers trained on essays"}, {"title": "Discussion", "content": "In this paper, we conducted a large-scale empirical evaluation of ten top LLMs by using GRE analytical writing assessment. Our goal is to establish benchmarks for the quality of essays generated by AI in cases that require high-level analytical thinking skills. Our results indicate that proprietary LLMs, such as GPTs and Gemini, consistently produced higher-scoring essays compared to open-source models, aligning with existing evaluations that show proprietary models outperforming their open-source counterparts. Additionally, we observed a notable improvement in essay scores from 2024 models compared to 2023 models, reflecting the continuous efforts within the AI community to create increasingly capable models for generating higher-quality text.\nFurther analysis of the language features computed by the e-rater\u00ae engine revealed that AI-generated essays better-controlled grammar, usage, and mechanics errors than human-written essay baseline. All AI-generated essays show a lower perplexity than the baseline of human-written essays. These findings provide information for improving future AI text generation, emphasizing crafting more meaningful texts alongside its already high-quality linguistic characteristics.\nAdditionally, we have devised several machine learning classifiers, using language features from e-rater\u00ae and perplexity features to distinguish between human-written and AI-generated essays. The performance of all these classifiers is promising, suggesting that AI-generated essays responding to GRE analytical writing prompts can be reliably identified. However, these results apply mainly when no further revisions have been made to the AI-generated essays. Nevertheless, these findings establish a benchmark for the detection accuracy of such essays within the context of GRE analytical writing, and the findings are consistent with findings from other writing assessments (Hao & Fauss, 2024; Yan et al., 2023).\nTo summarize, this study has evaluated the essays generated by LLMs using the GRE analytical writing framework and benchmarked various properties under controlled conditions. We hope the findings will help the assessment community understand the status of AI-generated essays and the potential implications of writing assessments. Meanwhile, we hope the findings can also help the LLM community think about ways to enhance the quality of model outputs through improved training and fine-tuning techniques."}, {"title": "Limitations", "content": "The current study has several limitations. Firstly, LLMs are constantly evolving, and it is challenging to enumerate all available advanced models. Whether and how the findings of this paper will change as LLMs change needs follow-up investigation, which we will report in future work.\nSecondly, it is crucial to note that the quality of generated texts heavily hinges on the provided prompts. In our study, we used the original prompts from the analytical writing assessment, only adding a length requirement. However, we acknowledge that modifying prompts in several ways, such as instructing the model to write as if it were a six-year-old, can dramatically alter the quality of the generated essay. As there are infinite ways to tweak the prompts, our choice is motivated by the idea of modifying the prompts as little as possible to simplify and obtain baseline results for benchmarking purposes. In future work, we will explore the impact of prompt tweaking on the essay quality.\nLastly, the variability of the generated essay is controlled by hyper-parameters, such as the temperature of the LLMs. Different LLMs may have different baseline settings for these hyper-parameters, which leads to varying degrees of variability in generated essays even when using the same hyper-parameter values. Therefore, a systematic investigation of the quality of essays for different hyper-parameters is imperative, and we will report the findings in future work."}]}