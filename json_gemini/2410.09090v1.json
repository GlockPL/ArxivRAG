{"title": "Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research", "authors": ["Haowen Xu", "Xueping Li", "Jose Tupayachi", "Jianming (Jamie) Lian", "Femi Omitaomu"], "abstract": "Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.", "sections": [{"title": "1. Introduction", "content": "Bibliometric analysis is a widely used method for evaluating and mapping research trends, impact, and scope across various scientific domains (Donthu et al., 2021). It provides quantitative insights by analyzing publication records, citations, and other scholarly outputs, helping researchers and policymakers understand the evolution of specific fields (Gan et al., 2022). Over the past few decades, bibliometric analysis has evolved from basic citation counts and keyword frequency metrics to more sophisticated approaches, incorporating co-authorship networks, citation flows, and research topic clusters (Ram\u00edrez et al., 2019). These methods are particularly important in fields like urban science, where emerging topics such as smart cities require continuous monitoring to shape the direction of future research and innovation (Guo et al., 2019). Bibliometric analysis plays a key role in identifying influential works, emerging themes, and research gaps, thus guiding strategic decision-making in urban science and smart city development (Zhao et al., 2019).\nHowever, traditional bibliometric methods face several limitations. Most rely heavily on keyword searches and basic text mining techniques, which depend on exact matches of terminologies and predefined keywords. These techniques often miss critical insights that are not explicitly captured in the titles or abstracts of research articles, thereby limiting the ability to fully understand and classify research topics (Romero-Silva and De Leeuw, 2021). Furthermore, traditional natural language processing (NLP) approaches, such as term frequency-inverse document frequency (TF-IDF) or simple word co-occurrence metrics, fail to capture the semantic meaning and contextual relationships between concepts(Safder and Hassan, 2019). Although topic modeling methods like Latent Dirichlet Allocation (LDA) can offer significant benefits for bibliometric analysis by providing deeper insights into the relationships and structures within research literature (Chen and Xie, 2020), they are primarily used for uncovering thematic structures and classifying article topics and are not designed for enabling semantic search or providing a contextual understanding of an article that involves deeper reasoning and interpretation. As a result, traditional bibliometric analysis often falls short in generating deeper insights that require a thorough review and interpretation of the article's full content. Relying primarily on the analysis of titles, keywords, and standard metadata, limits the ability to provide a more customized and nuanced characterization of research based on the full textual data.\nRecent advancements in generative AI models, such as large language models (LLMs), have opened new opportunities for enhancing research (Wang et al., 2024). These models, including transformers and Retrieval-Augmented Generation (RAG) systems, excel at semantic understanding and contextual interpretation of complex texts, making them highly suitable for extracting valuable insights from research articles and technical manuals (Xu et al., 2024b; Tupayachi et al., 2024). In the field of urban informatics, LLMs have been increasingly applied to analyze large volumes of text, uncovering patterns and trends that traditional methods would overlook (Liang et al., 2024; Xu et al., 2024a). In this paper, we propose a novel technical workflow for automating and enhancing bibliometric analysis by integrating Vector Databases, Sentence Transformers, Gaussian Mixture Models (GMM), Retrieval Agents, and an LLM. Our approach enables contextual search, topic ranking, and customized characterization of research arti-"}, {"title": "2. Literature Review", "content": "To overcome the limitations and knowledge gaps in traditional bibliometric analysis, recent studies have employed generative AI models, particularly transformer-based language models, to automate and enhance bibliometric methodologies.\nFija\u010dko et al. (2024) explores the application of generative AI in bibliometric analysis, focusing on 10 years of research abstracts from the European Resuscitation Congresses (ERC). Using ChatGPT-4, the study classified 2,491 abstracts into ERC guideline topics, with Basic Life Support and Adult Advanced Life Support being the most frequent. The research highlights the potential of large language models like ChatGPT-4 in categorizing and analyzing scientific literature and identifying trends. However, challenges included potential misclassification, the limited use of abstract titles rather than full-text, and heavy reliance on the model's capabilities. These constraints highlight the challenges of automating bibliometric analysis in the absence of comprehensive datasets. However, the study effectively showcases the potential of AI to significantly improve bibliometric methodologies despite these limitations.\nWeng et al. (2022) introduces a methodology for detecting and visualizing key research topics using GPT-3 embeddings and the HDBSCAN clustering algorithm on 593 abstracts related to urban studies and machine learning. By clustering abstracts based on semantic similarity and extracting keywords using the Maximal Marginal Relevance (MMR) algorithm, the study provides an interactive tool for exploring abstract clusters and their associated topics. Challenges included optimizing clustering parameters and relying solely on abstracts, which may not fully represent the research. Some clusters contained outliers or minimal data, affecting accuracy. Despite these limitations, the study demonstrates the potential of transformer-based models in facilitating unsupervised bibliometric analysis, though refinement is needed.\nBoth articles emphasize the benefits of transformer-based and large language models for bibliometric analysis, while also addressing critical limitations such as data quality, optimization challenges, and input constraints when working with abstract-based datasets. To overcome these challenges, there is a need to harness recent advancements in sentence transformer models and RAG technologies. These innovations can enable the development of an Al agent capable of advanced contextual understanding of research articles, facilitating semantic search and providing tailored insights based on user-specific queries. This, in turn, can generate new bibliometric metrics, offering deeper and more comprehensive analysis."}, {"title": "3. Methodology", "content": "This section starts by outlining the design requirements for our proposed methods, then presents the conceptual workflow and its implementation, which combines Generative AI techniques with statistical models."}, {"title": "3.1. Design Requirements", "content": "Overall, we aim to develop an AI-agent styled tool that can interact with users, who are primarily researchers and college students, through human nature conversations, to get their inquire on the current-state of cutting edge research in a specific domain, such as smart city and urban science. Based on the inquiry, our workflow will automate a sequence of procedures that leverage the unique capabilities of sentence transformers and RAG techniques on a batch of selected literature filtered and downloaded from academics databases, such as Scopous, IEEE Xplore, and Web of Science. Aiming to shed lights on more advanced, intelligent, and automonous biblimetric analysis, our workflow aims to enable the following features:\nConversational Interaction: A chatbot-style interface will be implemented, allowing users to ask questions through natural human conversations, without the need for pre-defined keywords or technical jargon. This feature will enable users to define search and filter criteria for subsets of bibliographic data (e.g., research articles, conference proceedings, technical reports, and manuals) that have been pre-selected and downloaded from popular"}, {"title": "Semantic and Contextual Search:", "content": "Based on the user-defined inquiry, this process matches and retrieves relevant research documents or specific sections by analyzing the underlying meaning and contextual relationships between words, rather than relying solely on keyword matching. The use of sentence transformers and text embeddings, enables users to access information and knowledge based on conceptual relevance, rather than simple term frequency. This enhances the precision of literature filtering and facilitates deeper, more insightful knowledge discovery, which will plays important role as the retrieval agent within the RAG paradigm to benefit further analytics using Generative AI models."}, {"title": "Customized Literature Characterization:", "content": "Using the output literature from the semantic and contextual search as input, Generative Pre-trained Transformer (GPT) models will be employed for contextual understanding, reasoning, and interpretation. These GPT models will process user inquiries to generate customized characterizations and interpretations of the selected literature, providing deeper insights and creating more sophisticated metrics for advanced bibliometric analyses. This approach aims to enhance the overall understanding of research trends and offer tailored, in-depth evaluations of the literature."}, {"title": "3.2. Workflow Design", "content": "Our workflow consists of four key procedures, as depicted in Figure 1. The workflow is later implemented in a Jupyter lab environment using Python-based libraries. Each procedure is detailed through the following following list."}, {"title": "3.2.1. Bibliography Selection and Data Preparation", "content": "In the first step, users select literature based on generic search criteria such as discipline, publication year, and journal name. Data is extracted from academic databases like Scopus, IEEE Xplore, and SerpApi using their respective web services and platforms, or through custom-built web scrapers, such as those powered by SerpAPI. The retrieved data includes bibliographic summaries in CSV format and individual articles in formats like PDF and HTML, which are then stored in a file-based system for further processing."}, {"title": "3.2.2. Text Embedding and Data Warehousing", "content": "After retrieving the essential documents, a Python script powered by PyPDF2 is used to parse the bibliographic summaries, which include the list of downloaded articles along with supportive metadata (e.g., authors, year, source, citations, and h-index), as well as the PDF and HTML versions of the individual articles. This parsing process is designed to upload key textual information into a datastore, building the knowledge base for the proposed AI agent. Unlike traditional information and content management systems, our workflow utilizes a sentence transformer, specifically the all-MiniLM-L6-v2 model from Hugging Face, to generate text embeddings-vector representations that encode the semantic and contextual meaning of the text. Compared with traditional NLP methods, sentence transformers, with its unique self-attention mechanism, have superior advantages in capturing semantic meaning, enabling contextual understanding, handling synonyms, and long-range dependencies between words in a sentence. These embeddings facilitate more efficient semantic and contextual searches in later stages of the workflow. The text embeddings, along with essential metadata and article content, are uploaded into the datastore. We selected Neo4j, a graph database, as the datastore for this workflow due to its graph data model, which better represents the relationships between data entities stored as nodes in the database. In our project, individual articles are represented as nodes within Neo4j, with associated metadata, content, and text embeddings stored as properties of each node."}, {"title": "3.2.3. Semantic and Contextual Search", "content": "In the third step, the workflow enables semantic and contextual searches within the literature stored in the knowledge base, leveraging the Neo4j database and sentence transformers. User queries, collected through a chatbot interface, serve as input for this advanced search. The core functionality compares the text embeddings of the user queries with those of the article contents. We employ an enhanced cosine similarity analysis, as described in Eq. 1, to calculate a similarity score ranging from 0 to 1, where 0 represents complete irrelevance and 1 represents high relevance. Our implementation extends the standard cosine similarity formula by using Python to chunk the original article content into sections and paragraphs, enabling more granular comparisons between the query and specific parts of the article. This process is applied to each article in the database, generating a similarity score based on semantic similarity with the user's query. At the contextual level, the framework evaluates the query's context and intelligently selects embeddings from different sections of the articles to perform a targeted and accurate search.\nSimilarity Score = $\\frac{a \\cdot b}{||a|| ||b||}$ (1)\nTo draw the decision boundary based on a list of individual article's similarity score, we employed GMM to rank and cluster articles by their similarity score, which reflects their relevance. A GMM is a probabilistic model that represents a distribution of data as a mixture of multiple Gaussian (normal) distributions, each characterized by its own mean and variance, making it effective for modeling complex, multimodal datasets. We employed the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), alongside the elbow method, to determine the optimal number of clusters for the Gaussian Mixture Model (GMM) analysis. After the clustering analysis, the cluster"}, {"title": "3.2.4. Customized Article Characterization", "content": "The top-ranked clusters, containing the most relevant articles, are then imported into a GPT model as an external knowledge source to generate customized characterizations for each article. These tailored bibliographic characteristics serve as metrics, providing more detailed descriptions and classifications of the articles. This approach uncovers valuable insights into research trends, focusing on individual articles' topics, technologies, methods, and contributions.\nWe leverage the contextual reasoning capabilities of large language models to classify and justify findings based on the semantic meaning of sections and paragraphs within the articles, extracting useful information without relying on precise names or keywords. This process is guided by instructional prompting strategies, where we design engineered prompt templates and feed them into the GPT model along with the relevant article text segments (specific sections). These segments are further refined and filtered based on their content relevance to ensure accurate classification and extraction.\nAt the technical level, we explored and tested the capabilities of two GPT models, including a local instance of EleutherAI/gpt-neo-1.3B models and the ChatGPT-3.5 Turbo API. Our experiments reveals that small models with on 1.3B parameters suffer from severe hallucination, and are unable to analyze large size of tokens. The ChatGPT-3.5 API demonstrates stable performance, particularly in its ability to process large text segments efficiently and produce reasoned characterizations."}, {"title": "4. Pilot Study", "content": "This pilot study aims to demonstrate the feasibility and performance of our proposed methods. For this study, we compiled a dataset of 223 high-impact urban research articles published in Nature Communications, obtained through the following Scopus query: TITLE-ABS-KEY ( \u201csmart city\" OR \"urban\" OR \"urban management\" OR \"urban planning\") AND SRCTITLE ( \"Nature Communications\" ) AND PUBYEAR \u00bf 2013. We preprocessed the dataset by removing all intermediate versions labeled as \"Author Correction\" or \"Publisher Correction.\" The final dataset consists of a CSV file containing bibliometric summaries with all Scopus fields selected, along with 223 individual PDF documents of the actual articles."}, {"title": "4.1. Use Case Demonstration", "content": "Our first use case on semantic and contextual search is demonstrated in Figure 2a, where the user submits an inquiry to identify articles related to urban green space. The chatbot responds by visualizing a histogram of similarity scores for all articles and displaying the GMM clusters of the articles. Additionally, a link is provided to download a"}, {"title": "4.2. Limitation and Future Work", "content": "Developed as a prototype for a more advanced knowledge base and management system, our workflow still faces a few limitations, as the following:\nToken Size Limitation: The current implementation using the ChatGPT API has a maximum token size limitation and incurs service fees based on the number of tokens processed. This makes it less suitable for analyzing large volumes of literature.\nDatabase Query Performance: The current datastore implementation using the Neo4j database may encounter challenges in querying and managing large volumes of embedding data, as Neo4j is not optimized as a dedicated vector database.\nLack of Evaluation and Validation: The GPT-generated literature characteristics are not currently evaluated by human experts, which introduces uncertainty regarding their accuracy and reliability.\nAs future work to address these limitations, we propose several experimental solutions. These include (a) deploying a local version of large language models, such as GPT-Neo, to minimize service fees for large-scale data analysis, (b) fine-tuning the GPT model to reduce unnecessary content and instructions sent to the model, thereby mitigating token size limitations, (c) transitioning our datastore implementation to dedicated vector databases, such as FAISS or Pinecone, to enhance latency and accuracy, and (d) developing a comprehensive strategy to evaluate the GPT's performance in analyzing and characterizing literature. Additionally, more advanced bibliometric analysis methods could be integrated into the current workflow to extend its analytical capabilities."}, {"title": "5. Conclusion", "content": "In this paper, we have presented a novel workflow that integrates generative AI models and advanced analytical techniques through the RAG paradigm to address the limitations of traditional bibliometric analysis methods. By leveraging the contextual reasoning capabilities of large language models and enhanced semantic search techniques, our system offers a more nuanced and insightful analysis of research literature. This approach, demonstrated through the analysis of urban science-related articles, enables customized characterizations and generates new metrics for bibliometric analysis, providing deeper insights into research trends, methodologies, and contributions.\nOur pilot study demonstrates the feasibility of this workflow, showcasing its ability to facilitate advanced semantic and contextual searches, cluster relevant articles, and produce tailored bibliographic insights through generative AI. However, the current implementation faces challenges, including token size limitations, database query performance issues, and the lack of expert evaluation for the AI-generated results.\nTo address these limitations, future work will explore the deployment of local language models, fine-tuning of GPT models to optimize token usage, and transitioning to vector databases like FAISS or Pinecone to improve performance. Additionally, we aim to establish a comprehensive validation framework involving human experts to ensure the accuracy and reliability of the generated bibliometric insights. As advancements in AI and bibliometric methodologies continue, our workflow has the potential to serve as a powerful and autonomous tool for researchers and policymakers seeking to analyze and interpret vast bodies of scientific literature more effectively."}]}