{"title": "CodEv: An Automated Grading Framework Leveraging Large Language Models for Consistent and Constructive Feedback", "authors": ["En-Qi Tseng", "Pei-Cing Huang", "Chan Hsu", "Peng-Yi Wu", "Chan-Tung Ku", "Yihuang Kang"], "abstract": "Grading programming assignments is crucial for guiding students to improve their programming skills and coding styles. This study presents an automated grading framework, CodEv, which leverages Large Language Models (LLMs) to provide consistent and constructive feedback. We incorporate Chain of Thought (CoT) prompting techniques to enhance the reasoning capabilities of LLMs and ensure that the grading is aligned with human evaluation. Our framework also integrates LLM ensembles to improve the accuracy and consistency of scores, along with agreement tests to deliver reliable feedback and code review comments. The results demonstrate that the framework can yield grading results comparable to human evaluators, by using smaller LLMs. Evaluation and consistency tests of the LLMs further validate our approach, confirming the reliability of the generated scores and feedback.", "sections": [{"title": "I. INTRODUCTION", "content": "Introductory programming courses are the cornerstone of computer education and a necessary prerequisite for further study in advanced courses. These courses introduce students to basic programming concepts and practices; however, the abstract nature of computer concepts and the complexity of coding logic often hinder learning in the initial stages. Students require targeted guidance to correct errors, write concise code, and demonstrate how the code operates. With the proliferation of programming courses, the number of students and the grading burden on teachers are increasing, making it challenging for educators to evaluate code more efficiently and provide effective feedback.\nWhile grading has traditionally been a manual process, recent advances in Machine Learning (ML) and Artificial Intelligence (AI) have introduced automated grading methods to simplify the grading tasks. Automated grading used to rely heavily on executing test cases to assess student submissions [1]. In this regard, ML models, such as natural language processing (NLP) techniques like recurrent neural networks (RNN), convolutional neural networks (CNN), and long short-term memory networks (LSTM) [2], have enhanced the grading process. However, these methods have an obvious limitation- they solely rely on the model's output to provide the final scores. More is needed; to further evaluate students' programming ability, we must develop methods and techniques that are able to better assess the correctness, readability, and overall structures of students' code.\nWith the rise of online learning and massive open online courses (MOOCs), educators face a significant challenge: providing comprehensive evaluations of numerous student code submissions within limited time constraints, covering aspects such as correctness, readability, and structural integrity. Under such immense pressure, many teachers often offer only simple scores or extremely brief comments. However, relying solely on these cursory suggestions is insufficient to foster students' ability to write high-quality code genuinely. Effective learning requires more profound and specific guidance, enabling students to understand what constitutes good programming and continually improve through practice.\nThe emergence of LLMs addresses the limitations of previous methods that relied solely on the program output. With their exceptional natural language understanding capabilities, LLMs enable the grading process to consider the code's readability and structures, making it possible for more complex tasks, such as generating feedback and syntax/logical error detection. For automated grading tasks, most existing research primarily centers on the performance of large-parameter models, such as GPT-3.5 and GPT-4 [3], [4], [5], [6]. However, using proprietary models (e.g., GPT-40, Gemini [7]) increases the cost for educators, whereas applying open-weight large-parameter models to grading requires more computational resources [8], making it challenging to employ LLMs for automated grading in practical applications. Furthermore, LLMs frequently struggle to follow instructions and provide reliable, unbiased outputs [9]. Therefore, it is crucial to carefully evaluate the results generated by LLMs when using them as a basis for grading."}, {"title": "II. BACKGROUND", "content": "Grading assignments and quizzes, especially when assessing code, can be complex and time-consuming. Code evaluation involves multiple aspects, such as code efficiency, readability, and functionality. Therefore, how educators can more effectively grade code has become noteworthy.\nA survey of Automatic Assessment Tools (AATs) indicates that research related to AATs has steadily increased from 2017 to 2022 [12]. However, most of these technologies provide feedback limited to unit test results, output comparisons, or differences from reference solutions. With the advances of ML/AI, algorithmic models were introduced into automated grading systems, and representation learning [13] further simplified the grading process and improved efficiency. Before the introduction of LLMs into automated grading tasks, ML- based automated grading primarily relied on executing test cases, using techniques such as NLP, RNN, CNN, and LSTM to enhance grading accuracy [2]. With recent developments in sequence modeling and LLMs, grading capabilities expanded to include code readability and efficiency. However, few tools assess code maintainability, readability, or documentation, with most relying on static analysis (such as code quality metrics) to evaluate correctness."}, {"title": "III. CODEV: LLM-BASED GRADING WORKFLOW FOR CODE EVALUATION", "content": "In introductory programming courses, educators must evaluate many programming problems and student submissions based on established grading policies and criteria. This process frequently demands substantial time and effort from educators, highlighting the importance of developing an effective automated code grading framework. To address this issue, we introduce the CodEv framework, which integrates multiple techniques to optimize grading outcomes and enhance the overall efficiency and accuracy of the assessment process.\nAs shown in Figure 1, the process is structured into three key stages: (1) Template design: generating templates for each problem. (Section 3.A), (2) LLMs ensemble: repeatedly generating multiple responses to perform LLM ensemble. (Section 3.B), (3) Robustness testing and result generation: using statistical methods to evaluate the robustness of the model. (Section 3.C)."}, {"title": "A. Template Design", "content": "Our framework develops prompt templates based on the CoT principle to guide LLMs on automated code grading effectively. The template demonstrated in Figure 2 depicts two types of prompt templates, zero-shot and zero-shot-CoT, both can incorporate custom criteria, including the correctness of output, code readability, and code functionality, for more accurate grading. For the zero-shot learning, we simply request the LLMs to generate a score and constructive feedback for each code file. For the zero-shot-CoT learning, we further request the LLMs break down the evaluation of each custom criterion into steps. By requesting the LLMs to explain the reason for the scores in each criterion step by step, we enhance the LLMs' reasoning ability to ensure more accurate grading and comprehensive feedback from different aspects defined in custom criteria."}, {"title": "B. LLM Ensemble", "content": "CodEv uses ensemble techniques to improve the stability of outputs. Recent research indicates that small-parameter models perform as well as larger ones by using ensemble techniques [18], which improve LLM performance in processing complex tasks. Based on these insights, our framework leverages LLMs to process queries based on the problem statement, evaluation criteria, and student answers, which include code and results from compiled test data, by querying each model multiple times. These queries generate results, including scores, comments, and reasoning. Then we apply a sampling and voting technique from Li et al.'s work to determine the final score of each student's code. The final score is established by identifying the most frequent score across multiple queries. This approach enhances model performance, and reduces random fluctuations in individual query results, leading to more reliable evaluations."}, {"title": "C. Robustness Test and Result Generation", "content": "In the final stage of the framework, the reliability and stability of the output scores were evaluated. After conducting agreement tests, the comments accompanying these consensus answers were summarized. The consistency within and between models was also examined to assess the stability of model performance in the code evaluation task.\nInter-model and intra-model agreement tests were performed on LLMs, using the Intraclass Correlation Coefficient (ICC) as"}, {"title": "IV. EXPERIMENT", "content": ""}, {"title": "A. Dataset and Benchmark", "content": "The data for this experiment were obtained from an introductory programming course attended by participants with an information technology background. Several programming test problems from the course were selected as sample questions to assess students' programming skills and problem-solving abilities. The specific requirements and evaluation criteria are outlined in Table 1. To provide a representative assessment, we collected code samples from 30 randomly selected students for each problem from the course.\nIn this study, we use teaching assistants' manual grading as a benchmark, reflecting human graders' standards. This allows us to assess whether the model can produce results comparable to human graders and to evaluate the framework's effectiveness in reducing grading time while maintaining accuracy and consistency."}, {"title": "B. Ensemble", "content": "We consider Llama 3.1 (8B) [21] and Gemma2 (9B) [22] as the small-parameter models for experimentation and employed Llama 3.1 (70B) and GPT-40 as the large-parameter models to validate the ensemble results. The experiment compared the stability of results obtained under different ensemble sizes for each model using various methods. Specifically, we compared the sampling-and-voting method proposed in previous studies [18], the average method, and the median method. Our goal is to enable the LLM to produce scores that closely resemble those of human graders, thereby reducing the time educators spend on grading tasks. To evaluate the effectiveness of these methods, we used the Mean Absolute Error (MAE) between the predicted scores and the scores from human graders as the performance metric, which is defined in Equation 1:\n$MAE = \\frac{1}{m} \\sum_{i=1}^{m}|Mode\\{Score_{i,q}\\}_{q=1}^Q - Score_{i,human}|$\nwhere m represents the number of students, and Q is the number of ensemble iterations for each student. Scorei,q refers to the score of the i-th student during the q-th iteration of evaluation. Scorei,human represents the human-graded score for the i-th student."}, {"title": "C. Effectiveness Validation", "content": "To validate whether our method effectively reduces the gap between small and large parameter models, we compared the MAE of scores generated under our proposed framework across all models for the same problems and student submissions against the scores obtained using the zero-shot method (see Figure 4).\nThe results indicate that after applying the proposed CodEv, the MAE of small-parameter models (Llama 3.1 8B, Gemma2 9B) significantly decreases. No notable differences are observed in the large-parameter models (Llama 3.1 70B, GPT-40) (see Table 3). This finding suggests that the framework can effectively reduce the performance gap between small- parameter and large-parameter models in automated grading tasks, showcasing the potential of using small-parameter models for such tasks through prompt engineering techniques.\nFurthermore, within this framework, the score difference between the large-parameter models and human evaluators is"}, {"title": "D. Agreement Test", "content": "This study conducted intra-model and inter-model agreement tests on GPT-40, Llama 3.1 70B, Llama 3.1 8B, and Gemma2 9B to evaluate the consistency of responses from individual LLMs over multiple iterations. The goal was to assess large language models' stability in grading tasks."}, {"title": "Intra-model Agreement Test", "content": "To validate the intra-model agreement test, we conducted 20 repeated queries for each model, using the same prompt each time. To measure the agreement of responses from a single model across multiple sessions, we chose the ICC (2, k) representing average random raters as the evaluation metric, with detailed results shown in Table 4. The results indicated that all models achieved an ICC (2, k) greater than 0.97, with p- values less than 0.001 and 95% confidence intervals between [0.96, 1]. These findings demonstrate that, regardless of the"}, {"title": "Inter-model Agreement Test", "content": "In the inter-model agreement test, based on the results from Section 4.C, we employed the sampling and voting method, using an ensemble size of 10 to derive the final scores from the model outputs. Different models served as raters to evaluate the scores generated by each model across various tasks (multiple assignments). We selected ICC(3, k) as the indicator for assessing the overall agreement of the scores because this metric is particularly suited for evaluating the agreement among fixed raters (the four models used in this section) across multiple assignments. The results showed that all models' ICC(3, k) values exceeded 0.84, with p-values less than 0.001 and 95% confidence intervals between [0.78, 0.88]. These results indicate a high level of consistency in the scores assigned to automated grading tasks among the different models."}, {"title": "E. Verifying the Reliability of Comments", "content": "Based on the results outlined in Section 4.C, this study collected the reasoning steps and feedback comments generated from 10 ensemble runs. The LLM was then employed to summarize these feedback responses (Figure 5 illustrates a student code sample from Task 1 and its corresponding comment summary). The study employed G-Eval, an LLM- based evaluation metric, and used GPT-40 as the evaluating model to assess the summarized feedback with the task descriptions. A random sample of 20 comments from all models was selected for evaluation, with the results indicating that the score range for all models fell between [0.58, 0.74] (on a scale from 0 to 1). Among these, GPT-4o achieved the highest score, indicating the superior quality of its generated comments. The majority of the feedback scored above 0.5, suggesting that the comments generated by the proposed framework offer valuable insights. However, there remains significant room for improvement in the quality of the generated comments."}, {"title": "V. DISCUSSION", "content": ""}, {"title": "A. Limitations", "content": "Despite our experiments showing that the CodEv framework reduces the capability gap between smaller and large parameter models, it still faces challenges in practical uses. First, the feedback quality and grading accuracy are limited by the size of the model parameters. Smaller models may occasionally produce incorrect feedback due to hallucination or being too harsh in edge cases. Secondly, although the framework focuses on code performance, experiments show that code-specific models may underperform compared to general-purpose models with similar sizes in evaluating code quality. Lastly, the results may be biased by the subjective factors in the human-graded ground truth of scores and the issue of self-judgment of GPT-4 in feedback assessment. Criteria like readability and functionality could be subjective for human evaluators, leading to inconsistencies in graded scores between the models and humans. Using GPT-4 to assess feedback generated by itself may reduce the fairness of assessment, introducing human evaluators to qualitative assessment could mitigate the problem."}, {"title": "B. Future Works", "content": "This study highlights several promising directions for improving code evaluation tasks. First, while the framework helps narrow the gap between small and large parameter models, smaller models still lag behind in performance. A possible solution is to use a step-by-step distillation approach [23], where a large-parameter model trains smaller models using the reasoning steps and scores generated by the framework. This could make automated grading more affordable for educators and enable students to self-assess using smaller models on web or mobile platforms in the future.\nAnother important area for future research is the development of objective standards for evaluating code readability and functionality. Currently, both LLMs and human evaluators introduce subjective bias. Creating standardized evaluation criteria would improve accuracy and fairness in assessments.\nFinally, there is a need to train models that excel in both reading comprehension and code understanding. Small general models often lack deep code knowledge, while specialized models struggle with reading comprehension. Future efforts should focus on developing models that balance both skills within a limited parameter size, improving the overall effectiveness of code evaluation."}, {"title": "VI. CONCLUSION", "content": "This study introduces CodEv, an interpretable and stable framework for automated code grading that provides understandable scores and constructive feedback. It integrates various techniques to enhance LLM performance, narrowing the gap between large and small parameter models. Even small- parameter LLMs can produce scores close to human evaluators while maintaining stable results. Experimental results show that general-purpose LLMs with better reading comprehension outperform specialized LLMs in coding tasks, highlighting the importance of text understanding over specific programming knowledge. The size of the model parameter also affects grading, with large-parameter models demonstrating a better understanding of the problems and criteria, producing feedback closer to human evaluators.\nBased on the experimental results, we employed the ICC for agreement testing, confirming the reliability of the generated feedback. The results indicate that LLMs, regardless of parameter size, achieve strong consistency in grading. While challenges in generating high-quality feedback remain, the framework offers educators a valuable tool for thorough code evaluation and feedback, supporting student learning. Addi- tionally, small parameter models show promise as cost-effective options for automated code grading."}]}