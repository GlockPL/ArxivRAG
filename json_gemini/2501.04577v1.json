{"title": "A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word GRNG for AI Uncertainty Estimation", "authors": ["Zephan M. Enciso", "Boyang Cheng", "Likai Pei", "Jianbo Liu", "Steven Davis", "Ningyuan Cao", "Michael Niemier"], "abstract": "Uncertainty estimation is an indispensable capability for AI-enabled, safety-critical applications, e.g. autonomous vehicles or medical diagnosis. Bayesian neural networks (BNNs) use Bayesian statistics to provide both classification predictions and uncertainty estimation, but they suffer from high computational overhead associated with random number generation and repeated sample iterations. Furthermore, BNNs are not immediately amenable to acceleration through compute-in-memory architectures due to the frequent memory writes necessary after each RNG operation. To address these challenges, we present an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the SRAM memory words. This integration reduces RNG overhead and enables fully-parallel compute-in-memory operations for BNNs. The prototype chip achieves 5.12 GSa/s RNG throughput and 102GOp/s neural network throughput while occupying 0.45 mm\u00b2, bringing AI uncertainty estimation to edge computation.", "sections": [{"title": "I. INTRODUCTION", "content": "Uncertainty estimation is crucial for robust decision-making in data-driven deep learning (DL) systems, particularly when these systems interact with the physical environment. In safety-critical applications, such as autonomous vehicle navigation and obstacle avoidance, medical diagnosis, aerospace control systems, and industrial automation, models equipped with uncertainty estimation could trigger human intervention or engage alternative sensors and models when prediction confidence drops below a set threshold. This process significantly mitiages the risk of potential catastrophic outcomes.\nBayesian neural networks (BNNs) provide a DL framework capable of delivering probabilistic estimates of classification uncertainty by replacing conventional deterministic weights with a posterior distribution of weights [1], [2]. Deployed models typically approximate the posterior with Gaussian distributions [3], [4]; even so, BNNs incur significant overheads from Gaussian random number generation (GRNG), the associated memory accesses, and repeated inferences,\nMoreover, BNNs derive less benefit from non-Von Neuman architectures, such as compute-in-memory (CIM) [5], [6]. When BNNs are deployed on these architectures, the GRNG must retrieve distribution parameters from memory, generate a weight sample, and subsequently write the sample back to the CIM array. Simulations indicate that even CIM-accelerated BNNs consume more than six times the energy per INT8 operation in each sampling iteration compared to traditional neural networks [7], [8], increasing the cost of deploying them on edge inference engines.\nCurrent BNN accelerators focus primarily on either enhancing the efficiency of GRNG hardware [9]\u2013[15] or maximizing data reuse [16]. By contrast, this chip performs fully parallel, in-memory matrix-vector multiplication with arbitrary Gaussian weight distributions. Crucially, this is achieved without requiring extra memory accesses for the GRNG, as the GRNG is integrated within the memory words. This stochastic, mixed-signal CIM architecture, combined with state-of-the-art (SOTA) GRNG energy and area efficiency, enables energy- and area-efficient BNN acceleration."}, {"title": "II. BACKGROUND", "content": "Bayesian neural networks (BNNs) augment traditional neural networks by encoding weights and biases as posterior distributions, which facilitates a probabilistic interpretation of model predictions [17]. This probabilistic approach enables BNNs to provide both classification predictions and uncertainty estimations.\nFormally, the posterior distribution of weights follows Bayes' Theorem:\n$$P (W | X, Y) = \\frac{P (Y | W, X) P(W)}{P(X)}$$\nwhere:\n\u2022 $P(W|X, Y)$ denotes the posterior probability of weights $W$ given input $X$ and output $Y$.\n\u2022 $P (Y | W, X)$ is the conditional probability or likelihood of observing outputs $Y$ for weights $W$ and input $X$.\n\u2022 $P(W)$ and $P(X)$ are the prior probabilities of the weights and input, respectively.\nDirectly approximating the posterior distribution $P (W | X, Y)$ is computationally intractable for edge devices. Consequently, deployed models typically approximate the posterior with a Guassian distribution through a method known as variational inference (VI) [4]. This approximation is expressed as:\n$$P (W | X, Y) \\approx N (W | \\mu, \\sigma)$$\nwhere:\n\u2022 $\u00b5$ is the mean of the Gaussian distribution.\n\u2022 $\u03c3$ is the covariance matrix of the Gaussian distribution.\nThe process of approximating the posterior distribution involves minimizing the divergence between the true posterior and the approximated Gaussian distribution. This minimization is commonly achieved by maximizing the evidence lower bound (ELBO), a technique that balances the fit of the model to the data with the complexity of the model [3]."}, {"title": "B. Compute-in-Memory Accelerators", "content": "Compute-in-memory (CIM) is an emerging accelerator architecture designed to address the von Neumman bottleneck, which refers to the significant latency and power consumption resulting from the separation of memory and processing units in traditional computing architectures. By integrating memory and computation, CIM accelerators aim to reduce data movement, thereby improving both energy efficiency and performance [7], [18]. This integration is particularly beneficial for DL applications, where the energy cost and latency of frequent memory access can be substantial and memory access patterns are relatively simple.\nOne of the key principles behind CIM is the use of crossbar arrays for performing matrix-vector multiplications (MVMs) directly within the memory [19]. In a crossbar array, the matrix weights are encoded as the conductance $G_{(i,j)}$ of each memory cell. When a voltage $X_i$ is applied to each row, it induces a current $Y_j$ to flow down each column according to Kirchhoff's Law. Mathematically, this can be represented as:\n$$Y_j = \\sum_i X_iG_{(i,j)}$$\nThus, $Y = GX$, and this direct in-memory computation reduces the need for data transfer between memory and computation units.\nCrossbars also support parallel in-memory operations. Depending on the precision and number of downstream analog-to-digital converters (ADCs), multiple rows and columns may be activated concurrently. This parallelism enables CIM accelerators to perform high-throughput computation, which is essential for DL applications that require extensive MVMs. CIM designs have been realized with both static random-access memory (SRAM) and emerging memory technologies (EMTs), such as resistive random-access memory (ReRAM), phase-change memory (PCM), and spin-transfer torque magnetic random-access memory (STT-MRAM) [20]\u2013[22]. These technologies may offer advantages in terms of storage density and non-volatility, making them suitable for different application requirements."}, {"title": "C. BNN Hardware Acceleration", "content": "BNNs are distinguished from conventional neural networks by their reliance on stochastic sampling, which significantly amplifies hardware resource demands. This stochastic process necessitates extensive inference runs to accurately determine the mean and variance of inference scores, thereby assessing model uncertainty. Digital BNN accelerators focus on optimizing GRNG [14], [15] or improving the hardware architecture pipeline through data reuse strategies that minimize unnecessary data transactions [12], [16]. Despite these efforts, an efficiency gap remains due to the limitations of digital GRNG and the frequency of memory operations required for BNN inference.\nEach inference iteration in a BNN involves reading distribution parameters, generating a Gaussian sample, and subsequently updating the weight array. This iterative process makes it challenging to apply CIM architectures for BNN hardware acceleration. Leveraging the stochastic properties of EMTs, can enable efficient GRNG through device variation [23]. These emerging memory technologies exploit the inherent stochasticity in their physical properties to generate noise with the stored data, thereby integrating GRNG with memory.\nHowever, the dependency on specific device characteristics for generating stochasticity, coupled with the high power consumption of memory writes for storing GRNG results, introduces programming complexities and scalability issues. Additionally, the endurance and durability concerns of these technologies further complicate their long-term viability for BNN acceleration."}, {"title": "III. CHIP ARCHITECTURE AND CIRCUIT DESIGN", "content": "This chip employs several algorithmic optimizations to reduce BNN overhead and simplify GRNG. First, SOTA partial BNNs significantly reduce the number of repeated operations and the requisite number of RNG samples. Meanwhile, the computationally-expensive convolutional and/or recurrent layers are processed as standard, non-Bayesian layers. This strategy maintains the model's ability to quantify uncertainty without incurring excessive computational costs [13].\nA key optimization in this architecture is weight decomposition, which separates each weight into a sum of the mean \u00b5 and the product of the its standard deviation \u03c3 and a distribution sample $\u03f5$:\n$$W_{(i,j)} = \\mu_{(i,j)} + \\sigma_{(i,j)}\\epsilon, \\epsilon \\sim \\mathcal{N} (0, 1)$$\nThus, the $j^{th}$ output $Y_j$ with N inputs $X_{(i,j)}$ is given by:\n$$Y_j = f (\\sum_{i=1}^N X_{(i,j)} \\mu_{(i,j)} + \\sum_{i=1}^N X_{(i,j)}\\sigma_{(i,j)} \\epsilon)$$\nSince the mean \u00b5 is static, it only needs to be processed once. Furthermore, \u03f5 is sampled from the same standard normal distribution instead of a parameterized normal distribution, which significantly simplifies the GRNG design. This simplification is crucial because it reduces the computational burden associated with generating and storing multiple unique random samples for each weight update."}, {"title": "B. CIM Tile Architecture", "content": "The CIM tile architecture is designed to reflect the weight decomposition methodology, as shown in Fig. 3, by using two crossbar subarrays for separately computing $X\u00b5$ and $X\u03c3\u03f5$. In the fabricated prototype, each CIM tile comprises 64 rows of 8 words. Each word consists of a 8-bit \u00b5 and a 4-bit \u03c3. The 4-bit digital input vector X is fed to current digital-to-analog converters (IDACs). Each word bit is associated with a dedicated 6-bit successive-approximation register (SAR) ADC. To enhance efficiency, the SAR ADCs share a common synchronous controller. This shared control reduces the overall area requirement for each ADC and enables pitch-matching with the SRAM arrays. Pitch-matching is critical as it eliminates the need for column multiplexing, thus enabling single-cycle MVM.\nThe CIM tile also includes digital reduction logic, which also corrects for individual ADC offset. This logic shifts and adds the outputs from the SAR ADCs to reconstruct each word. Once the individual components of $X\u00b5$ and $X\u03c3\u03f5$ have been computed, they are combined to generate a single output vector."}, {"title": "C. In-Word GRNG Circuit", "content": "1) Capacitor Thermal Noise: Unlike pseudorandom RNGs, true RNGS (TRNG) require a physical process capable of producing entropy [24]. Common entropy sources for integrated circuits include thermal noise, jitter, and electrical metastability [25]. [26] identified that thermal noise affects the discharge period of a capacitor with constant leakage current, and the discharge follows a Gaussian distribution whose properties are given by:\n$$\u03bc_\u03c4 = \\frac{CV_{DD}}{2I_L}$$\n$$\u03c3_\u03c4 = \\frac{CV_{DD}}{2I_L}$$\nwhere:\n\u2022 $\u00b5_\u03c4$ is the mean or expectation of the distribution.\n\u2022 $\u03c3_\u03c4$ is the variance of the distribution.\n\u2022 C is the capacitance of the capacitor being discharged.\n\u2022 $I_L$ is the total leakage current.\n\u2022 $q$ is the initial charge on the capacitor.\n\u2022 \u03c4 represents the time after the capacitor begins leaking that the voltage crosses threshold $V_{Thr}$.\n2) GRNG Operation: The in-word GRNG circuit (see Fig. 4) that enables CIM-accelerated BNNs compares the discharge time of two capacitors $C_p$ and $C_n$ to yield a normal distribution centered on zero. The process involves the following steps:\n1) Initially $P_0$ and $P_1$ charge $C_p$ and $C_n$ to $V_{DD}$ while \u0424 is low.\n2) Pulling EN high latches and causes $N_1$ and $N_2$- biased to $V_R$ through $N_0$ and $N_3$, respectively-to slowly discharge $C_p$ and $C_n$ to ground."}, {"title": "3) Calibration for Static Variation", "content": "Transistor mismatch induced during fabrication may cause either $N_1$ or $N_2$ to conduct current faster than the other for the same applied gate voltage. This static variation manifests as a non-zero mean $\u03f5_0$ in the output distribution:\n$$\u03f5_0 = (\\frac{C_p}{I_{N1}} - \\frac{C_n}{I_{N2}}) \\frac{V_{DD}}{(C_p I_{N2} - C_n I_{N1})} \u2260 0$$\nSubthreshold operation, which is required to produce adequately large standard deviations, further amplifies the effects of transistor variations [28]. However, such deviation is static- for a given die, the same variation will be observed each cycle- and can be systematically corrected through calibration. First, the chip measures the mean offset $\u03f5_{0,(i,j)}$ for weight $w_{(i,j)}$ by writing 1 to all \u03c3 words and multiplying each row by $X_i = 1$ sequentially. Then, the static offset is subtracted from the \u00b5 cell, resulting in calibrated weight $w'_{(i,j)}$:\n$$w_{(i,j)} = \u00b5_{(i,j)} + \u03c3_{(i,j)} (\u03f5 + \u03f5_{0,(i,j)})$$\n$$w'_{(i,j)} = \u00b5'_{(i,j)} + \u03c3_{(i,j)} \u03f5, \\space \u00b5'_{(i,j)} = \u00b5_{(i,j)} - \u03c3_{(i,j)}\u03f5_0$$\nThe entire calibration process consumes 3.6 nJ and must only be performed once per chip, though subsequent weight changes must be updated to include the offset."}, {"title": "D. CIM Memory Words", "content": "The CIM memory words, as depicted in Fig. 5, use 8T SRAM cells to minimize parasitic leakage current. These cells feature separate wordlines (WLs) and bitlines (BLs) for reading and writing operations. The computation process begins by charging all BLs to $V_{DD}$. Subsequently, each row's IDAC converts the 4-bit digital input $X_i$ into a read WL voltage. This voltage ensures that the current conducted by the 8T SRAM cells is linearly proportional to $X_i$. The cells conduct current from the BLs to ground for a set duration, and the resulting voltage on the $j^{th}$ BL represents the vector dot product of $X$ by all cells connected to $BL_j$. To facilitate robust CIM operation, the downstream SAR ADCs operate differentially, with charge on $BL_N$ and $BL_P$ representing negative and positive values, respectively. For differential data encoding in the \u00b5 word, each bit uses 2 SRAM cells. A positive value is represented by 01 and a negative value represented with 10. By contrast, the \u03c3 word only requires one SRAM cell per bit because the GRNG produces signed outputs. First, the current through the SRAM cell is gated by GRNG output pulse E via transmission gates at the output. Then, current flows either from $BL_p$ or $BL_N$ depending on complimentary GRNG signals P and N."}, {"title": "IV. HARDWARE EVALUATION", "content": "A prototype chip fabricated on a commercial 65 nm PDK (Fig. 6) provides validation measurements for this design. GRNG tests were conducted in a thermal chamber, as shown in Fig. 7 to ensure a stable operating environment and to measure the temperature stability of the GRNG circuit."}, {"title": "A. In-Word GRNG", "content": "The output distribution and associated latency for one bias configuration are shown in Fig. 8. An examination of the normal probability plot-a specific type of Q-Q plot used to assess normal distributions-indicates that the output distribution is suitably normal for quantized applications with an r-value of 0.9967 for N = 2500 samples.\nSince the bias voltage $V_R$ controls the rate at which ca- pacitors $N_1$ and $N_2$ discharge $C_p$ and $C_n$, increasing $V_R$ reduces total energy and GRNG latency but also decreases the output standard deviation.\nThermal noise is also sensitive to change in environmental temperature; as the environmental temperature increases from 28\u00b0C to 60\u00b0C, the pulse width standard deviation increases by 2.62x, and the average latency decreases by 2.49x (see Tab. I). Despite these variations, the quality of the output distribution slightly improves: the normal probability plot r-value increases by 4.78% over the same temperature increase. Furthermore, changes in standard deviation can be compensated for by tuning the IDAC bias, which affects the rate at which the CIM cells conduct current from the bitlines."}, {"title": "B. Model Uncertainty Estimation", "content": "The INRIA person dataset [29] models a safety-critical application where the neural network must accurately identify the presence of pedestrians. MobileNet [30] was chosen for this analysis due to its efficient feature extraction capabilities, which lends itself to edge inference.\nFig. 10 illustrates how BNNs achieve uncertainty estimation by increasing the entropy of incorrect and out-of-distribution classifications. In the standard MobileNet implementation, incorrect classifications typically exhibit low entropy, making them indistinguishable a priori from correct classifications. However, when employing a Bayesian classifier (Bayesian FC layers, see Sec. III-A) on this chip, the the average predictive entropy (APE) of incorrect classifications increases from 0.350 to 0.513 (+46.6%). Concurrently, the expected calibration error (ECE) [31] decreases from 4.88 to 3.31 (-32.2%). ECE measures the total area between the ideal calibration curve (which is linear) and the model's calibration curve; models with high ECE perform poorly at uncertainty estimation, as demonstrated by the standard MobileNet in Fig. 10.\nEven with only 2 bits of \u03c3 precision, the partial-Bayesian MobileNet maintains a low ECE (see Fig. 11). However, this chip employs a 4-bit \u03c3 to support more complex applications where the model requires more precision to quantify uncertainty accurately.\nIn a real-world BNN model deployment, classifications with uncertainty exceeding predetermined threshold would be flagged for further scrutiny, either via human intervention or supplemental sensors and models. As demonstrated in Fig. 11, when deferring high-entropy classifications, the partial-Bayesian network outperforms the standard model, achieving an average accuracy recovery of 3.5% for representative entropy thresholds between 0.0 and 0.6. Users can also trade-off overall accuracy for better uncertainty estimation through adjustments to the loss function during training, although such techniques are beyond the scope of this work.\nOverall, BNN models deployed on this chip effectively retain their uncertainty estimation, confirming this chip's suitability for safety-critical edge deployments, such as person detection for autonomous vehicles."}, {"title": "V. CONCLUSION", "content": "Fig. 12 presents a detailed breakdown of the tile area and energy consumption for one complete MVM. The SRAM accounts for over 63% of the total tile energy and 48% of total tile area, underscoring the power and space efficiency of the in-word GRNG cell. When compared to SOTA BNN ASICS, as detailed in Tab. II, this chip achieves a 75% GRNG energy reduction and increases GRNG throughput by over 6\u00d7 per mm\u00b2 at the current tech node or over 33\u00d7 per mm\u00b2 when scaled to the same technology.\nIn conclusion, the integration of a 360 fJ/Sample GRNG directly into SRAM memory words presents a significant advancement in the acceleration of BNNs. By reducing the computational overhead associated with RNG and facilitating fully-parallel CIM operations, this ASIC overcomes the traditional challenges faced by BNN accelerators. The prototype chip validates the potential of this approach to bring efficient AI uncertainty estimation to edge computation without sacrificing model accuracy. This work paves the way for more reliable and robust AI systems in safety-critical environments, ultimately contributing to the broader adoption and implementation of BNNs in high-stakes applications."}]}