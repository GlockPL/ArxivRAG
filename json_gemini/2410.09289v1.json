{"title": "AuD-Former: A Hierarchical Transformer Network\nfor Multimodal Audio-Based Disease Prediction", "authors": ["Jinjin Cai", "Ruiqi Wang", "Dezhong Zhao", "Ziqin Yuan", "Victoria McKenna", "Aaron Friedman", "Rachel Foot", "Susan Storey", "Ryan Boente", "Sudip Vhaduri", "Byung-Cheol Min"], "abstract": "Audio-based disease prediction is emerging as a\npromising supplement to traditional medical diagnosis methods,\nfacilitating early, convenient, and non-invasive disease detection\nand prevention. Multimodal fusion, which integrates features\nfrom various domains within or across bio-acoustic modali-\nties, has proven effective in enhancing diagnostic performance.\nHowever, most existing methods in the field employ unilateral\nfusion strategies that focus solely on either intra-modal or inter-\nmodal fusion. This approach limits the full exploitation of the\ncomplementary nature of diverse acoustic feature domains and\nbio-acoustic modalities. Additionally, the inadequate and isolated\nexploration of latent dependencies within modality-specific and\nmodality-shared spaces curtails their capacity to manage the\ninherent heterogeneity in multimodal data. To fill these gaps,\nwe propose AuD-Former, a hierarchical transformer network\ndesigned for general multimodal audio-based disease prediction.\nSpecifically, we seamlessly integrate intra-modal and inter-modal\nfusion in a hierarchical manner and proficiently encode the\nnecessary intra-modal and inter-modal complementary correla-\ntions, respectively. Comprehensive experiments demonstrate that\nAuD-Former achieves state-of-the-art performance in predicting\nthree diseases: COVID-19, Parkinson's disease, and pathological\ndysarthria, showcasing its promising potential in a broad context\nof audio-based disease prediction tasks. Additionally, extensive\nablation studies and qualitative analyses highlight the significant\nbenefits of each main component within our model.", "sections": [{"title": "I. INTRODUCTION", "content": "AUDIO-BASED disease prediction, focused on deducing\npathological symptoms through human acoustic bio-\nsignals such as cough, breathing, and speech, has become\na trending research area [1], [2]. Leveraging deep learning\nalgorithms, audio-based prediction systems have proven ef-\nfective in a diverse array of disease diagnosis scenarios, such\nas respiratory ailments (both acute and chronic), mental health\ndisorders, and developmental abnormalities [3]\u2013[7]. Benefiting\nfrom its non-invasive, cost-effective, and accessible nature,\naudio-based disease prediction could serve as a promising\ncomplement to traditional medical diagnostic tools.\nDue to the high-dimensional and noise-sensitive nature of\nraw audio clips, most audio-based disease prediction systems\ntend to extract and utilize features from various domains\nand sub-domains, such as time, frequency, and cepstral do-\nmains, rather than inputting raw data directly [8], [9]. These\nheterogeneous acoustic features are mappings of a specific\nbio-acoustic modality within different dimensional spaces,\nrevealing various aspects of its characteristics for disease\ndiagnosis. Moreover, even identical feature types from various\nbio-acoustic modalities, such as coughs and breath sounds, can\noffer valuable insights into unique facets of disease symptoms.\nDrawing parallels from these insights, multimodal fusion\nmethods that involve merging acoustic features from different\ndomains within a single bio-acoustic modality [10]\u2013[15], i.e.,\nintra-modal fusion, or combining acoustic features across mul-\ntiple modalities [16]\u2013[25], i.e., inter-modal fusion, have been\ndeveloped to improve disease prediction outcomes compared\nto unimodal methods. Despite recent promising results, several\nkey challenges should be surmounted to fully harness the\npotential of multimodal audio-based disease prediction, as\noutlined below.\nUnilateral Fusion Strategies. Most existing studies exclu-\nsively adopt either intra-modal [10]\u2013[15] or inter-modal [16]-\n[25] fusion, rarely exploring their simultaneous application.\nWhile intra-modal fusion methods can capture a broad range\nof characteristics within a specific bio-acoustic modality by\nfusing features extracted from different domains, they often\nmiss the synergistic benefits achievable through integrating\nmultiple modalities. On the other hand, while inter-modal\nmethods can provide such benefits, they may overlook the\ndeep, nuanced interconnections across diverse feature domains\nwithin each modality, since they often utilize features from a\nsingle domain for each modality [16], [18], [23] or simply\nconcatenate [19], [22] or average [20] several features of\none modality. In summary, the prevalent unidirectional fusion\npattern may limit the model to fully exploit the complementary\ninformation derived from various fusion stages. To address this\ndeficiency, it is imperative to explore a comprehensive fusion\nstrategy that effectively combines the fusion processes within\nand across bio-acoustic modalities.\nInadequate Latent Dependencies Exploration. Bio-acoustic"}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "In this work, we define a modality as a distinct type of vocal\nbehavior or bio-acoustic signal (e.g., cough, breathing, speech)\ngenerated by activation of different body parts, including\nlarynx, vocal folds, tongue, lips, and palate, each offering\nunique insights into a patient's health status. We use the term\nmultimodal fusion to describe the process of integrating these\nvarious audio modalities or their different feature domains\nto form a comprehensive representation [28]. In the context\nof audio-based disease prediction, a common practice is to\nextract features from various domains like time, frequency,\nand cepstral, from raw audio clips [8]. This characteristic\nintroduces two types of multimodal fusion in literature: the\nfusion of different bio-acoustic modalities, such as cough,\nbreathing, and speech (known as inter-modal fusion), and the\nfusion of different feature domains within a single modality\n(referred to as intra-modal fusion).\nA substantial body of research emphasizes inter-modal\nfusion, involving the integration of multimodal representations\nacross various modalities [16]\u2013[22], [25], [27] and the combi-\nnation of insights from models trained on individual modalities\n[23], [24], [29]\u2013[31]. However, these approaches often neglect\nthe rich intra-modal correlations as they generally utilize a sin-\ngle pre-trained model or method for feature extraction within\neach modality, typically focusing on a limited set of feature\ndomains. For instance, Dang et al. [16] employed pre-trained\nVGGish models to independently extract unimodal representa-\ntions for cough, breathing, and voice sounds, which were then\nconcatenated and input into a GRU network for COVID-19\nprediction. This method potentially overlooks valuable insights\nfrom other feature domains within each modality. Furthermore,\nlatent inter-modal dependencies may not be fully captured due\nto limited consideration of the complex interactions between\ndifferent bio-acoustic signals. For example, Effati et al. [29]\nimplemented shared weight strategies to synchronize knowl-\nedge across modalities by averaging weights among three\nBiLSTM models, each trained on specific data types. While\nthis strategy aims to foster cross-modal integration, it may fall\nshort in addressing the intricate relationships and dependencies\ndue to its simplistic weight averaging mechanism.\nOn the other hand, several studies [10]\u2013[15] have con-\ncentrated on intra-modal fusion. However, these approaches\noften confine their methods within a single modality without\nintegrating inter-modal fusion. Moreover, the exploration of\nintra-modal dependencies typically lacks depth: many opt for\nearly concatenation that depends on aligning multiple features\n[10], [15] or late score/decision-level fusion that processes\neach feature domain separately [11]\u2013[13], [31]. For instance,\nBhosale et al. [10] utilized the concatenation of multiple\ntemporal, spectral, and tempo-spectral features as input to an\nearly fusion model for COVID-19 detection. Additionally, Liu\net al. [31] developed two MLP classifiers, each tailored to\nspecific feature sets, with their classification scores fused for\nthe final prediction of voice disorders. These methods may\nfail to effectively facilitate communication between different\nfeature domains due to the inadequate consideration of the\nintra-modal correlations."}, {"title": "III. METHODOLOGY", "content": "In this section, we present our proposed hierarchical trans-\nformer network for multimodal audio-based disease prediction."}, {"title": "A. Problem Formulation and Framework Overview", "content": "Consider multimodal audio signals composed of $m$ modal-\nities. For each modality, the unimodal features extracted\nacross $n$ different domains or subdomains can be repre-\nsented as a low-level unimodal feature sequence $X_{(.)}=$\n$[x_{(.)}^1, x_{(.)}^2, \u2026\u2026\u2026, x_{(.)}^{l_{(.)}}] \\in R^{l_{(.)} \\times d_{(.)}}$. In this paper, $l_{(.)}$ and $d_{(.)}$\ndenote the feature length and dimension of one modality, re-\nspectively. The classification task is to generate discrete labels\nfor disease prediction based on these constituent multimodal\naudio features.\nGiven the heterogeneous yet complementary nature of mul-\ntimodal features within a single audio modality and across\ndifferent modalities, our scientific hypothesis is that a hi-\nerarchical fusion strategy, combining both intra-modal and\ninter-modal fusion along with comprehensive exploration of\nlatent dependencies within modality-specific and modality-\nshared spaces, will efficiently infer a multimodal acoustic\nrepresentation that improves audio-based disease prediction\nperformance. To this end, we propose AuD-Former, a hier-\narchical transformer network designed to hierarchically cap-\nture sufficient intra-modality dependencies and inter-modality\ncorrelations, thereby providing an efficient acoustic fusion\nrepresentation for downstream disease prediction tasks.\nAs illustrated in Fig. 1, the AuD-Former consists of two\nhierarchical core components: 1) Intra-modal representation\nlearning: Utilizing intra-modal attention layers, we generate\nunimodal representations, denoted as UR. These represen-\ntations capture latent intra-modal correlations between var-\nious low-level features within a single modality, effectively\nmapping information across multiple domains (Section III-C);\nand 2) Inter-modal representation learning: Through inter-\nmodal attention layers, we merge these heterogeneous uni-\nmodal representations into a unified fusion representation,\ndenoted as FR. This fusion effectively encodes cross-modal\ndependencies, allowing each target unimodal representation to\ncontinuously integrate complementary information from other\nmodalities to enhance its own feature set (Section III-D). These\nhierarchical modules are specifically designed to leverage the\nheterogeneity and latent complementary attributes within and\nacross unimodal features of different modalities, overcoming\nthe limitations of unilateral fusion strategies and inadequate\ndependency exploration in existing models."}, {"title": "B. Temporal and Positional Embedding", "content": "The low-level feature sequences of each modality, $X_{(.)} =$\n$[x_{(.)}^1, x_{(.)}^2, \u2026\u2026\u2026, x_{(.)}^{l_{(.)}}] \\in R^{l_{(.)} \\times d_{(.)}}$, are first embedded by mul-\ntiple 1-D temporal convolution (TC) layers to obtain convo-"}, {"title": "C. Intra-modal Representation Learning", "content": "The unimodal features within a single modality originate\nfrom different domains or sub-domains, providing a unique\nperspective and emphasizing distinct characteristics of the\nmodality. To capitalize on this heterogeneity and learn a\ncomprehensive unimodal representation, we feed temporal\nunimodal feature sequence $X^{\\prime}_{ \\{1,\u2026,m\\}}$ of each modality into the\nintra-modal transformer to generate unimodal representations\nwith latent intra-modal correlations mined efficiently.\nThe core of the intra-modal transformers is the multi-head\nself-attention mechanism [32]. Specifically, the self-attention\nprocess assesses pairwise relationships of each element in\nthe unimodal feature sequence, i.e., the convoluted unimodal\nfeatures obtained through temporal embedding, to integrate\nthe contextual information from the entire feature sequence.\nFormally, we define queries $Q_{\\{1,\u2026,m\\}}$, keys $K_{\\{1,\u2026,m\\}}$ and\nvalues $V_{\\{1,\u2026,m\\}}$ for unimodal feature sequences as:\n$Q_{\\{1,\u2026,m\\}} = \\hat{X}_{\\{1,\u2026,m\\}} \\cdot W_q^{\\{1,\u2026,m\\}}$\n$K_{\\{1,\u2026,m\\}} = \\hat{X}_{\\{1,\u2026,m\\}} \\cdot W_k^{\\{1,\u2026,m\\}}$\n$V_{\\{1,\u2026,m\\}} = \\hat{X}_{\\{1,\u2026,m\\}} \\cdot W_v^{\\{1,\u2026,m\\}}$\nwhere $W_q^{\\{1,\u2026,m\\}} \\in R^{d_{tc},d_{sq}}$, $W_k^{\\{1,\u2026,m\\}} \\in R^{d_{tc},d_{sk}}$, and\n$W_v^{\\{1,\u2026,m\\}} \\in R^{d_{tc},d_{sv}}$ are three weight groups to be trained\nrespectively.\nThen the self-attention (SA) process is formulated as:\n$UR_{\\{1,\u2026,m\\}} = SA(Q_{\\{1,\u2026,m\\}}, K_{\\{1,\u2026,m\\}}, V_{\\{1,\u2026,m\\}})$\n$= softmax(\\frac{Q_{\\{1,\u2026,m\\}} \\cdot K_{\\{1,\u2026,m\\}}^T}{\\sqrt{d_{sk}}})V_{\\{1,\u2026,m\\}}$\nwhere $UR_{\\{1,\u2026,m\\}} \\in R^{l_{\\{1,\u2026,m\\}},d_{sv}}$ represent the unimodal\nrepresentations resulting from single-head SA operation.\nThe above process can be conducted in parallel multiple\ntimes as multi-head self-attention. Ultimately, we derive the\nfinal unimodal representation $UR_{\\{1,\u2026,m\\}}$ from $UR_{\\{1,\u2026,m\\}}^{\\prime}$\nthrough multiple layer normalization and feed-forward opera-\ntions within the intramodal transformer, as illustrated in Fig. 2.\nBy computing different attention scores to unimodal features\nwithin a single bio-acoustic modality, the self-attention process"}, {"title": "D. Inter-modal Representation Learning", "content": "In practical situations, medical professionals must thor-\noughly examine and combine clinical data from various\nsources to make well-founded diagnostic decisions. Likewise,\na dependable multimodal diagnostic system needs to be pro-\nficient at leveraging the commonalities and complementarities\nacross different bio-acoustic modalities. Typically, common-\nalities of multiple modalities are thought to reflect consistent\ninformation about the disease, whereas complementarities con-\nvey supplementary information. To this end, we propose the\ninter-modal representation learning module to effectively mine\nadequate complementary dependencies and adaptations across\ndifferent modalities.\nThe unimodal representations of all modalities $UR_{\\{1,\u2026,m\\}}$\nfirst produce the low-level fusion representation $FR_L \\in R^{l_f,d_{sv}}$\nwith the concatenation operation. This representation is then\nfed, along with each unimodal representation respectively, into\nmultiple cross-modal transformers. Each cross-modal trans-\nformer aims to progressively enhance the target unimodal rep-\nresentation $UR_m$ with other modalities encoded in the fusion\nrepresentation $FR_L$ by computing the cross-modal attention\nillustrated in Fig. 3a. We formally define queries $Q^{UR_m}$\nderived from the target unimodal representations, and keys"}, {"title": "E. Prediction Layer and Model Optimization", "content": "To further distill essential contextual information for disease\ndiagnosis, the representation $FR_H$ is additionally processed\nthrough a layer featuring multi-head self-attention, as depicted\nin Eq. 4. The output, denoted as $FR_H \\in R^{l_f,d_{ev}}$, is then put\ninto subsequent linear layers accompanied by residual oper-\nations and featuring softmax activation functions to generate\ndisease predictions, formally defined as:\n$\\hat{FR_H} = FR_H + \\Omega_v(FR_H)$\n$P = softmax(\\Omega_{\\tau}(\\hat{FR_H}))$\n$\\hat{y} = argmax(P_i)$\nwhere $y \\in R^1$ and $P_i \\in R^2$ represent the predicted labels\nand probabilities for the $j$th class (two classes in our setting:\nPositive or Negative) in the disease prediction task, and $\\Omega_v$\nand $\\Omega_{\\tau}$, denote two fully-connected layers with parameter sets\n$\\{v\\}$ and $\\{\\tau\\}$, respectively.\nFor model optimization, we choose binary cross-entropy\nloss defined as:\n$L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N[y_i log(\\hat{Y_i}) + (1 - y_i) log(1 - \\hat{g_i})]$\nwhere $y_i$ and $\\hat{y_i}$ are the ground-truth and predicted labels for\nthe $i$th instance, respectively."}, {"title": "IV. EXPERIMENTAL SETTING", "content": "In this section, we detail the experimental setup used to\nevaluate our proposed AuD-Former. The core objective is to\naddress the research question: Can the hierarchical integration\nof intra-modal and inter-modal fusion processes enable the ef-\nficient querying of multimodal representations using unimodal\nfeature sets, thereby enhancing the performance of general\naudio-based prediction tasks?\nTo this end, we compared the AuD-Former to other state-\nof-the-art audio-based prediction baselines that utilize intra-\nmodal or/and inter-modal fusion across various diseases, such\nas respiratory diseases, neurological disorders, and speech\ndisorders. We also extensively implemented extra baselines\nand ablation models to investigate the contributions of the"}, {"title": "V. RESULTS AND ANALYSIS", "content": "Comparisons against Baselines: Tables II and III present\nthe results of the performance comparison between our AuD-\nFormer and other state-of-the-art multimodal fusion baselines\non the all datasets. It can be observed that our AuD-Former sur-\npasses all baselines across all metrics during cross-validation\nexperiments in diagnosing diverse diseases, including COVID-\n19, PD, and pathological dysarthria. This indicates that, in\nterms of overall performance, the AuD-Former has more"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this work, we present AuD-Former, a hierarchical trans-\nformer network designed for general multimodal audio-based\ndisease prediction. By hierarchically leveraging intra-modal\nand inter-modal fusion strategies, AuD-Former effectively cap-\ntures rich dependencies within and across modalities, creating\na unified representation for disease prediction without the\nneed for extensive feature selection. Our experiments on three\ndistinct diseases (COVID-19, pathological dysarthria, and\nParkinson's) demonstrate the effectiveness of this approach.\nDespite the promising results, translating these findings to\nreal clinical settings remains a challenge. Future work will\nfocus on optimizing AuD-Former for real-world applications,\nimproving adaptability to diverse patient demographics, and\nexploring its utility for predicting conditions such as mild cog-\nnitive impairment or early dementia. Further work could also\nexplore the applicability of our model to non-medical audio-\nbased tasks, such as audio event detection and multimedia\ncontent analysis, which could benefit from similar hierarchical\nmultimodal fusion strategies."}]}