{"title": "TODO: ENHANCING LLM ALIGNMENT WITH TERNARY PREFERENCES", "authors": ["Yuxiang Guo", "Lu Yin", "Bo Jiang", "Jiaqi Zhang"], "abstract": "Aligning large language models (LLMs) with human intent is critical for enhancing their performance across a variety of tasks. Standard alignment techniques, such as Direct Preference Optimization (DPO), often rely on the binary Bradley-Terry (BT) model, which can struggle to capture the complexities of human preferences-particularly in the presence of noisy or inconsistent labels and frequent ties. To address these limitations, we introduce the Tie-rank Oriented Bradley-Terry model (TOBT), an extension of the BT model that explicitly incorporates ties, enabling more nuanced preference representation. Building on this, we propose Tie-rank Oriented Direct Preference Optimization (TODO), a novel alignment algorithm that leverages TOBT's ternary ranking system to improve preference alignment. In evaluations on Mistral-7B and Llama 3-8B models, TODO consistently outperforms DPO in modeling preferences across both in-distribution and out-of-distribution datasets. Additional assessments using MT Bench and benchmarks such as Piqa, ARC-c, and MMLU further demonstrate TODO's superior alignment performance. Notably, TODO also shows strong results in binary preference alignment, highlighting its versatility and potential for broader integration into LLM alignment.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) demonstrate remarkable potential in various tasks (Huang et al., 2021; Hendrycks et al., 2021; Shi et al., 2023), with performance gains linked to better alignment with human intent (Mishra et al., 2022; Christiano et al., 2023; Wu et al., 2023). The alignment process typically involves two stages: Supervised Fine-Tuning (SFT) to establish instruction following abilities (Thoppilan et al., 2022; Sanh et al., 2022; Mishra et al., 2022), followed by preference fine-tuning to refine the model's alignment with human preferences (Ziegler et al., 2020; Christiano et al., 2023). This stage typically employs either reinforcement learning (RL)-based (Schulman et al.; OpenAI, 2023; Ramamurthy et al., 2023) or RL-free methods (Rafailov et al.; Azar et al.; Saeidi et al.), both leveraging the preference datasets. Effective alignment is enhanced by the diversity of training data, enabling LLMs to accurately learn from high-quality pairwise responses (Cui et al., 2023; Song et al., 2024; Saeidi et al.).\nCurrent alignment methods relying on the Bradley-Terry (BT) (Bradley & Terry, 1952) model consider only two preference rankings: preference and dis-preference, which restricts the diversity of learnable information. A notable challenge is the inconsistent quality of the pairwise preference data, often showing minimal discernible differences (Nvidia et al., 2024; Amini et al., 2024). Table 1 shows a sample from the Ultrafeedback-binaried dataset\u00b9 (Tunstall et al.), which is commonly used in preference alignment procedures (Tunstall et al.; Hong et al.). In this example, both responses have identical quality score of 8.5 from GPT-4 evaluations (OpenAI, 2023), differing only in narrative sequence and text format. In practice, we observe a considerable amount of tie data in common preference datasets and chat arenas judged by humans, as detailed in Appendix A.1. These ties encompass a variety of information, necessitating a more nuanced analysis. However, existing preference optimization techniques, such as Direct Preference Optimization (DPO) (Rafailov et al.), are constrained by their reliance on the binary BT model and struggle to effectively manage tie relations. How to learn useful information from tie data and achieve nuanced preference modeling in the alignment process remains to be explored, which is the goal of this paper.\nOur primary contributions can be unfolded as:\n\u2022 We enhance the existing human preference alignment process by incorporating a \u201ctie\u201d ranking, transcending the traditional binary rankings as depicted in Figure 1. We first extend the BT model into the Tie-rank Oriented BT (TOBT) model. The TOBT model incorporates the concept of preference uncertainty, allowing for the representation of ties alongside \"prefer\u201d and \u201cdisprefer\u201d rankings. This innovation enables a more comprehensive handling of preference relations.\n\u2022 Building on the TOBT model, we introduce the Tie-rank Oriented Direct Preference Optimization (TODO) algorithm. TODO is designed to accommodate ternary ranking relations, offering a nuanced approach to preference alignment. By integrating the tie relation, TODO is capable of learning from a broader spectrum of preference information, enhancing the adaptability and accuracy of LLMs.\n\u2022 We use Mistral-7B and Llama 3-8B to conduct experimental validation. First, we evaluate the effectiveness of DPO and TODO in terms of preference modeling accuracy. Our evaluation spans both in-distribution dataset, drawn from the same source as the training data, and out-of-distribution dataset, notably the Reward Bench (Lambert et al.). Results indicate superior preference modeling by TODO. Additional assessments on MT Bench (Zheng et al., 2023) and other popular benchmarks such as Piqa (Bisk et al., 2019), ARC-c, ARC-e (Clark et al., 2018), Hellaswag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021) and Winogrande (Sakaguchi et al., 2019) confirms TODO's enhanced alignment capabilities. Finally, we provide an intuitive analysis highlighting TODO's advantages over DPO in two dimensions: enhanced granularity in preference modeling and increased diversity in the acquired information.\n\u2022 TODO can also be directly applied in binary preference alignment process, outperforming DPO with standard binary preference datasets. Furthermore, the proposed TOBT model can be utilized not only in offline policies like DPO but also can be integrated into other online policies or used to train a reward model."}, {"title": "2 RELATED WORKS", "content": "Preference alignment in LLMs. Current methods for aligning preferences in LLMs often utilize the BT model (Bradley & Terry, 1952) and the Plackett-Luce ranking model (Plackett, 1975) to capture human preferences. RL-based approaches, such as Reinforcement Learning from Human Feedback (RLHF, (Schulman et al.)), require a reward model to assess generated responses, which is typically trained on pairwise preference data gathered from crowdworkers (Wang et al., 2024) or by utilizing another LLM as an evaluative judge (Bai et al., 2022; Cui et al., 2023). RL-free algorithms, such as Direct Preference Optimization (DPO) (Rafailov et al.) and its variants (Azar et al.; Ethayarajh et al., 2024), can directly optimize human preferences by introducing an implicit reward. DPO offers a stable and computationally lightweight method to align with human intent. To address the potential overfitting problem in DPO, Identity Preference Optimization (IPO) (Azar et al.) and Reward-aware Preference (RPO) (Nvidia et al., 2024) has been proposed by introducing differences between pairwise responses. Furthermore, Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024), was proposed by directly maximizing the utility of generations instead of maximizing the log-likelihood of preferences, and Hong et al. introduced a reference model-free monolithic odds ratio preference optimization algorithm. Some concurrent works attempt to introduce intrinsic knowledge constraints into preference alignment, either by on-the-fly fine-tuning LLMs to obtain relative qualities (Yu et al., 2024) or by defining a reward distance (Nvidia et al., 2024; Amini et al., 2024). These studies overlook the potential of leveraging tied preference data to refine alignment, whereas our approach is complementary to most of them.\nTernary preference models. Ties frequently occur in ranked data, such as sports and examinations. For example, a soccer result can be classified simply as a home win, an away win, or a tie. The well-known BT model, which can be derived from the order statistics of the exponential distribution, cannot handle ties. Rao & Kupper (1967) corrected the BT model by assuming that small probability difference values would be declared ties. Kuk (1995) applied this approach to football. Davidson (1970) provided an ad hoc correction to the BT model for ties. Dewart & Gillard (2018) applied the BT model to cricket, where draws occur but do not depend strongly on team strengths. Baker & Scarf (2020) used discrete distributions, principally the geometric distribution, to obtain a modified BT model that allows for tie ranks. These studies lay a robust theoretical foundation for addressing ties in ranking results, facilitating the connection between LLM alignment and preference data."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 THE BRADLEY-TERRY MODEL", "content": "The Bradley-Terry (BT) (Bradley & Terry, 1952) model is a probability model for the outcome of pairwise comparisons between instances, teams, or objects. It estimates the probability that the rank order $i \\succ j$ is true, where the symbol > represents a preference or rank relation, such as instance i being preferred over j, or i beating j, depending on the application.\nThe computation of BT model can be represented by Equation 1, where the positive strength owned by two competitors are denoted by $\\lambda_1$ and $\\lambda_2$ respectively, and $r_{12}$ represents the probability that the first competitor obtains a higher rank than the second one in comparison.\n$r_{12} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}$ (1)\nDefine $d_{12} = \\ln \\lambda_1 - \\ln \\lambda_2$, which represents the strength difference of two competitors in logarithmic form. Following Rao & Kupper (1967), Equation 1 can be written into the form of Equation 2, where $\\text{sech}(.)$ denotes the hyperbolic secant function. This relation shows that the preference probability $r_{12}$ depends only on $d_{12}$.\n$r_{12} = \\frac{1}{4} \\int_{-d_{12}}^{\\infty} \\text{sech}^2 (y/2) dy = \\frac{1}{4} \\int_{-\\infty}^{d_{12}} \\text{sech}^2 (y/2) dy$ (2)"}, {"title": "3.2 HUMAN PREFERENCE MODELING", "content": "Following the BT model, the human preference distribution $p^*$ is formulated in Equation 3, where $r^*$ is some latent reward model, $r^*(x, y)$ denotes the reward for response y given an input prompt x, and $\\sigma(.)$ is the Sigmoid function. The variables $x, y_1$ and $y_2$ are drawn from a preference dataset $\\mathcal{D} = \\{(x^i, y_1^i, y_2^i)\\}$, where $y_1$ is the preferred response and $y_2$ is the dispreferred one. The term $\\exp(r^*(x, y))$ signifies the strength $\\lambda$ of responses following Equation 1, acknowledging that the reward could be negative,\n$p^*(y_1 \\succ y_2|x) = \\frac{\\exp(r^*(x, y_1))}{\\exp(r^*(x, y_1)) + \\exp(r^*(x, y_2))} = \\sigma(r^*(x, y_1) - r^*(x,y_2))$. (3)\nSubsequently, a reward model that mirrors human preferences can be trained using the method of maximum likelihood estimation (Schulman et al.),\n$\\max_\\theta \\mathbb{E}_{(x,y_1,y_2)\\sim \\mathcal{D}} [\\log \\sigma(r_\\theta(x, y_1) - r_\\theta(x, y_2)) ]$. (4)"}, {"title": "3.3 DIRECT PREFERENCE OPTIMIZATION", "content": "In RLHF (Schulman et al.), the goal is to maximize the expectation of rewards under the KL divergence constraint,\n$\\max_\\pi \\mathbb{E}_{x\\sim D,y\\sim \\pi_\\theta (y|x)} [r_\\theta(x, y)] - \\beta \\log \\frac{\\pi_\\theta (y|x)}{\\pi_{ref}(y|x)}$ (5)\nThe optimal solution to this problem satisfies the following relationship (Rafailov et al.):\n$r(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)$, (6)\nwhere $Z(x) = \\sum_y \\pi_{ref}(y|x) \\exp (\\frac{r(x, y)}{\\beta})$ only depends on prompt x. By integrating this relation into Equation 4, the loss function of DPO can be expressed as shown in Equation 7, which is incapable of addressing tied preference data.\n$\\mathcal{L}_{DPO} (\\pi_\\theta; \\pi_{ref}) = - \\mathbb{E}_{(x,y_1,y_2)\\sim \\mathcal{D}} [\\log \\sigma (\\beta \\log \\frac{\\pi_\\theta (y_1|x)}{\\pi_{ref} (y_1|x)} -  \\beta \\log \\frac{\\pi_\\theta (y_2|x)}{\\pi_{ref}(y_2|x)} )]$. (7)"}, {"title": "4 TIE-RANK ORIENTED DIRECT PREFERENCE OPTIMIZATION", "content": "To handle ties in preferences modeling, we introduce a buffer in the integral interval of Equation 2, which is inspired by Rao & Kupper (1967). We call the new preference model Tie-rank Oriented BT (TOBT) model. Based on this model, we propose a novel preference alignment algorithm, Tie-rank Oriented Direct Preference Optimization (TODO)."}, {"title": "4.1 TIE-RANK ORIENTED BT MODEL", "content": "As shown in Equation 2, $d_{12}$ represents the preference difference between two instances, and $d_{12} > 0$ means the first instance is preferred. To handle ties, we impose higher requirements on the comparison by introducing a positive number $\\alpha$, and then the preferred relation is determined by $d_{12} > \\alpha$. That is, the ranking probability $r_{12}$ becomes Equation 8,\n$r_{12} = \\frac{1}{4} \\int_{-((\\ln \\lambda_1 - \\ln \\lambda_2)+\\alpha)}^{\\infty} \\text{sech}^2 (y/2) dy$ (8)\nThen, the probability that two instances share a tie relation is $1 - r_{12} - r_{21}$, which is denoted by $r_{(12)}$, and can be expressed in Equation 9,\n$r_{(12)} = \\frac{1}{4} \\int_{-(\\ln \\lambda_1 - \\ln \\lambda_2)-\\alpha}^{-(\\ln \\lambda_1 - \\ln \\lambda_2)+\\alpha} \\text{sech}^2(y/2) dy$ (9)\nIntuitively, the parameter $\\alpha$ encapsulates the inherent uncertainty and noise in the strength values $\\lambda_1$ and $\\lambda_2$, which are ubiquitous in LLM alignment. Preference datasets for alignment often rely on human labeling (Chiang et al., 2024), LLM-as-judge assessments (Cui et al., 2023), or reward model scoring (Nvidia et al., 2024), introducing label noise due to inconsistencies among human annotators or the inherent variability of LLMs in approximating human preferences. Accounting for this"}, {"title": "4.2 OBJECTIVE FUNCTION OF TODO", "content": "Following Equation 10 and 11, the tie-rank oriented human preference distribution $p^*$ can be expressed by Equation 12 and Equation 13.\n$p^*(y_1 \\succ y_2|x) = \\frac{\\exp(r^*(x, y_1))}{\\exp(r^*(x, y_1)) + \\phi \\exp(r^*(x, y_2))}$ (12)\n$p^*(y_1 = y_2|x) = \\frac{\\exp(r^*(x, y_1)) \\exp(r^*(x, y_2))(\\phi^2 - 1)}{(\\exp(r^*(x, y_1)) + \\phi \\exp(r^*(x, y_2))) (\\exp(r^*(x, y_2)) + \\phi \\exp(r^*(x, y_1)))}$ (13)\nEquation 12 represents the possibility of treating $y_1$ as the preferred response and $y_2$ as the dispreferred response in pairwise data based on the TOBT model. Following Equation 12, the objective $\\mathcal{L}_{TODO}^p$ can be written as Equation 14, where $\\mu = r_\\theta(x, y_1) - r_\\theta(x, y_2)$ represents the reward difference of two responses $y_1$ and $y_2$. Because the Z(x) in implicit reward $r_\\theta(x, y)$ only depends on x, the difference results of $\\mu$ can be equivalently expressed as $\\mu = \\beta \\log \\frac{\\pi_\\theta(y_1|x)}{\\pi_{ref}(y_1|x)} - \\beta \\log \\frac{\\pi_\\theta(y_2|x)}{\\pi_{ref}(y_2|x)}$.\nThe superscript p in $\\mathcal{L}_{TODO}^p$ denotes that this formulation is the objective of TODO when the responses in a pair exhibit a clear preference, rather than being tied. The detailed derivation process is provided in Appendix A.4.\n$\\mathcal{L}_{TODO}(\\pi_\\theta; \\pi_{ref}) = - \\mathbb{E}_{(x,y_1,y_2)\\sim \\mathcal{D}} [\\log \\sigma(\\mu - \\alpha)]$ (14)\nEquation 13 represents the possibility of treating pairwise responses $y_1$ and $y_2$ as a tied pair based on the TOBT model. Following Equation 13, the objective $\\mathcal{L}_{TODO}^t$ can be written as Equation 15. The superscript t in $\\mathcal{L}_{TODO}^t$ signifies that this represents TODO's objective when the pair is tied. The detailed derivation process is is provided in Appendix A.5.\n$\\mathcal{L}_{TODO}^t (\\pi_\\theta; \\pi_{ref}) = - \\mathbb{E}_{(x,y_1,y_2)\\sim \\mathcal{D}} [\\log \\frac{\\exp(2\\alpha) - 1}{(1 + \\exp(\\mu + \\alpha))(1 + \\exp(-\\mu + \\alpha))}]$ (15)\nGiven a preference dataset $(x^i, y_1^i, y_2^i, I^i) \\in \\mathcal{D}$, the indicator $I^i$ is determined by Equation 16. Specifically, $I^i = 0$ indicates a clear preference or quality difference between the two responses to the same prompt x, while $I^i = 1$ represents that two responses $y_1$ and $y_2$ are tied. Then, the final loss $\\mathcal{L}_{TODO}$ of TODO can be represented by Equation 17.\n$I^i = \\begin{cases} 1, & y_1^i = y_2^i, \\\\ 0, & y_1^i \\succ y_2^i. \\end{cases}$ (16)\n$\\mathcal{L}_{TODO} = (1 - I)\\mathcal{L}_{TODO}^p + I \\mathcal{L}_{TODO}^t$ (17)"}, {"title": "4.3 THE EFFECT OF DIFFERENT PREFERENCE RELATIONS ON TODO UPDATES", "content": "To elucidate the dynamics of TODO parameter updates, we introduce a gradient-based analysis that distinguishes between scenarios where pairwise responses show a clear preference or are tied. The comprehensive derivation is detailed in Appendices A.6 and A.7.\nFor the gradient update (18) for pairwise data with a clear preference, the introduction of a positive small value $\\alpha$ results in more substantial weight adjustments when the reward difference is misestimated compared to DPO. This refinement mitigates noise from narrow reward margins, allowing TODO to more effectively learn distinct preferences by concurrently enhancing the likelihood of the favored response $y_1$ and diminishing that of the unfavored response $y_2$.\n$\\nabla_\\theta \\mathcal{L}_{TODO} (\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x,y_1,y_2)\\sim \\mathcal{D}} [\\beta \\sigma(-\\mu + \\alpha)  [\\nabla_\\theta \\log(\\pi(y_1|x)) - \\nabla_\\theta \\log(\\pi(y_2|x)) ]]$. (18)\nFor the gradient update (19) for pairwise tie data, $G(\\mu) = \\frac{\\exp(-\\mu+\\alpha)-\\exp(\\mu+\\alpha)}{(1+\\exp(-\\mu+\\alpha))(1+\\exp(\\mu+\\alpha))}$ is monotonically decreasing with respect to $\\mu$, and $G(0) = 0$. When $\\mu = 0$, two responses obtain the same rewards, and DPO continues to update policy models as per Equation 7, which shifts the distribution to reduce the likelihood of the \u201cdispreferred\u201d response $y_2$, potentially discarding valuable information. In contrast, TODO refrains from updating any parameters to maintain the consistent preference alignment of the tied responses.\nWhen the estimated reward difference for tied responses is $\\mu > 0$, suggesting $y_1$ has a higher reward than $y_2$, TODO's gradient update strategy will reduce the likelihood of $y_1$ and increase that of $y_2$. Conversely, if $\\mu < 0$, the update will elevate the likelihood of $y_1$ and lower that of $y_2$, ensuring the preference consistency between the tied responses is preserved.\n$\\nabla_\\theta \\mathcal{L}_{TODO}(\\pi_\\theta; \\pi_{ref}) = -\\mathbb{E}_{(x,y_1,y_2)\\sim \\mathcal{D}} [G(\\mu) \\nabla_\\theta \\log(\\pi(y_1|x)) + G(-\\mu) \\nabla_\\theta \\log(\\pi(y_2|x))]$ (19)"}, {"title": "5 EXPERIMENTAL SETTINGS", "content": ""}, {"title": "5.1 MODELS AND DATASETS", "content": "Models. We select two different series of models, Mistral-7B (Jiang et al., 2023) and Llama 3-8B (AI@Meta, 2024), as our experimental backbone models. We select zephyr-sft-full\u00b2 (Tunstall et al.) as the supervised fine-tuning (SFT) version of Mistral model and llama3-8b-sft-ultrachat\u00b3 as the SFT version model of Llama 3 model. Both zephyr-sft-full and llama3-8b-sft-ultrachat are fine-tuned on Ultrachat-200k (Ding et al., 2023) dataset.\nTraining datasets. For the datasets used in the preference alignment process, we construct 20k-size datasets with different tie data proportions from Ultrafeedback (Cui et al., 2023). Responses sharing the same quality score are classified as tied. The quality score for each response is a weighted score across multiple assessment metrics, taking into account helpfulness, truthfulness, verbalized calibration and honesty. Each sampled dataset exhibits a tie data ratio that varies within the set {0, 0.1, 0.2, 0.3}. Other details of the training sets are shown in Appendix A.8.\nEvaluation benchmarks. We first compare the effectiveness of preference modeling ability between DPO and TODO. To this end, we curate an in-distribution test set containing 1500 non-tied samples and select the Reward Bench (Lambert et al.) as an out-of-distribution dataset. Subsequently, we select a suite of well-established benchmark datasets to evaluate the models comprehensively: 1) MT Bench (Zheng et al., 2023), which contains open-ended questions designed to assess the multi-turn conversational capabilities and the ability to follow instructions of LLMs. 2) A diverse set of benchmarks containing Piqa (Bisk et al., 2019), ARC-c, ARC-e (Clark et al., 2018), Hellaswag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021) and Winogrande (Sakaguchi et al., 2019), which collectively evaluate the language comprehension and reasoning faculties of LLMs."}, {"title": "5.2 TRAINING SETTINGS", "content": "All comparative results are derived from training each model for 3 epochs on their respective training datasets. We set $\\alpha = 0.5$ in TODO, which follows the principle of balancing the loss value of pairwise non-tie data and tie data. Please refer to Appendix A.3 for more details. Other hyper-parameters are shown in Appendix A.9. We ensure the consistency of training hyperparameters among experiments for a fair comparison."}, {"title": "5.3 EVALUATION SETTINGS", "content": "Accuracy of preference modeling. In assessing the efficacy of models fine-tuned with DPO and TODO for preference modeling, we employ prediction accuracy as the primary evaluation metric. For each pair of data where the preference rank $y_1$ is favored over $y_2$, we calculate the predicted probabilities for each preference rank for pair data, adhering to Equations 12 and 13. A prediction is deemed accurate if the model assigns the highest probability to the scenario where $y_1$ is preferred over $y_2$ across all possible ranks.\nGPT based scoring in MT Bench. For the evaluation in MT Bench, we use gpt-4-turbo-2024-04-09 (OpenAI, 2024) to score generated results.\nAccuracy of other benchmarks. For the evaluation of Piqa, ARC-c, ARC-e, Hellaswag, MMLU and Winogrande, we use Opencompass (Contributors, 2023) to assess final results, details of prompt templates and evaluation metrics can be found in Appendix A.10."}, {"title": "6 RESULTS AND ANALYSIS", "content": ""}, {"title": "6.1 TODO IMPROVES HUMAN PREFERENCE MODELING WITH TIE DATA", "content": "In this section, we assess the preference modeling capabilities of models trained with DPO and TODO across both in-distribution and out-of-distribution datasets, incorporating varying proportions of tie data in the training regimen. The in-distribution assessment is based on the above mentioned test set, while the out-of-distribution assessment utilizes the Reward Bench.\nWe observe that both Mistral and Llama 3 models aligned with TODO generally achieve better performance than those aligned with DPO on both in-distribution and out-of-distribution data. Overall, when directly modeling human preferences mixing tie data, DPO often leads to sub-optimal results. TODO addresses this issue with more nuanced preference modeling. Experimental results demonstrate the effectiveness of the combinatorial optimization objectives in TODO."}, {"title": "6.2 TODO ENHANCES ALIGNMENT IN LLMS", "content": "In this section, we evaluate the effectiveness of DPO and TODO across different benchmarks to comprehensively demonstrate the alignment capabilities of LLMs.\nAs illustrated in Figures 3a and 3b, the scores on the MT Bench for models aligned with DPO and TODO reveal that TODO consistently outperforms DPO across all training sets for both the Mistral and Llama 3 models. Specifically, TODO achieves peak performance when using the binary preference dataset for the Mistral models and incorporating a 20% tie data ratio for the Llama 3 models, respectively."}, {"title": "6.3 KEY FACTORS OF TODO IN HANDLING TIE DATA AND OBTAINING BETTER PERFORMANCE", "content": "In this section, we analyse the limitations of DPO and the advantages of TODO in handling tie data. We highlight two pivotal factors that TODO can enhance alignment: nuanced preference modeling and enriched diversity of information learned."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "This study illuminates a limitation in LLM preference alignment: the inability of binary models like DPO to resolve ties in preference data. To overcome this, we integrate a tie ranking system into preference modeling, refine the BT model into the more robust TOBT model, and introduce the TODO algorithm. Our experimental results demonstrate that TODO consistently outperforms DPO across a range of evaluations. The success of TODO stems from its nuanced handling of ties and the robust TOBT model. This approach is not only limited to direct preference optimization but is also compatible with various online and offline preference optimization policies and can be employed for reward model training, which are left for future work."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 TIE DATA IN CURRENT PREFERENCE DATASETS", "content": "We conduct a statistical analysis of existing tie data across common preference data, as presented in Table 4. Current pairwise preference data are scored by LLMs or labeled by humans to establish preference rankings. Preference data scored by LLMs, commonly rely on the quality score considering multiple aspects to differentiate between preferred and dispreferred responses. Ultrafeedback (Cui et al., 2023), as a popular preference dataset, is scored based on GPT-4 feedback, and contains 383k pairwise responses. We find that 17.0% of the data pairs exhibit identical quality scores. The Ultrafeedback_binaried (Tunstall et al.), a subset of this dataset, also includes 12.1% of tie data. This analysis underscores the prevalence of ties in preference datasets."}, {"title": "A.2 THE TOBT MODEL", "content": "By substituting t = $\\frac{y}{2}$ into $\\int sech^2(\\frac{y}{2})dy$, we obtain that\n$F(t) = \\frac{1}{2} \\int sech^2(t)dt = \\frac{1}{2} tanh (t),$ (21)"}, {"title": "A.3 SELECTION OF \u03b1 IN TODO", "content": "In this section, we elaborate on the process of selecting the optimal value for \u03b1 in TODO. Initially, we generate a series of random values to emulate the expression $\\beta \\log \\frac{\\pi_\\theta (y_1|x)}{\\pi_{ref} (y_1|x)} - \\beta \\log \\frac{\\pi_\\theta (y_2|x)}{\\pi_{ref} (y_2|x)}$,"}, {"title": "A.4 THE OBJECTIVE FUNCTION OF TODO WITH PAIRWISE DISTINGUISHED PREFERENCE DATA", "content": "It is straightforward to derive the TODO objective in pairwise data with distinct preference difference. Under the TOBT model as Equation 12, we can obtain the possibility of $y_1$ being preferred over $y_2$ following Equation 26 by substituting $\\phi$ into $\\exp(\\alpha)$.\n$P^*(y_1 \\succ y_2|x) = \\frac{\\exp(r^*(x, y_1))}{\\exp(r^*(x, y_1)) + \\phi \\exp(r^*(x, y_2))}$\n$= \\frac{\\exp(r^*(x, y_1))}{\\exp(r^*(x, y_1)) + \\exp(\\alpha) \\exp(r^*(x,y_2))}$\n$= \\frac{1}{1 + \\exp(\\alpha) \\exp(r^*(x, y_2) - r^*(x, Y_1))}$ (26)\n$= \\frac{1}{1 + \\exp(r^*(x, y_2) - r^*(x, y_1) + \\alpha)}$\n$= \\sigma(r^*(x,y_1) \u2013 r^*(x, y_2) \u2212 \u03b1)$\nRecall that the (unavailable) ground-truth reward through is given as follows:\n$r^*(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)$. (27)\nSubstituting Equation 27 into Equation 26, we derive the per-instance preference possibility as shown in Equation 28.\n$P^*(y_1 \\succ y_2|x) = \\sigma(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\pi_{ref}(y_1|x)} - \\beta \\log \\frac{\\pi^*(y_2|x)}{\\pi_{ref}(y_2|x)} - \\alpha)$ (28)"}, {"title": "A.5 THE OBJECTIVE FUNCTION OF TODO WITH PAIRWISE TIE DATA", "content": "For instances of pairwise tie data", "29": "n$p^*(y_1 = y_2|x) = \\frac{\\exp(r^*(x", "into": "n$p^*(y_1 = y_2|x) = \\frac{\\exp(2\\alpha) - 1}{1+ \\exp(r^*(x, y_1) - r^*(x, y_2) + \\alpha) + \\exp(r^*(x, y_2) - r^*(x, y_1) + \\alpha) + \\exp(2\\alpha)}$ (30)\nby dividing $\\exp(r^*(x,y_1) + r^*(x, y_2))$ in both the numerator and denominator. Because $\\exp(2\\alpha)$ can be expressed by $\\exp(r^*(x, y_1) \u2013 r^*(x, y_2) + \u03b1 + r^*(x, y_2) - r^*(x,y_1) + \u03b1)$, we can rewrite the denominator of the last line in Equation 30 into $(1 + \\exp(r^*(x"}]}