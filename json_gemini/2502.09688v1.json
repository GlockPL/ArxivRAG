{"title": "Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling", "authors": ["Benjamin D. Killeen", "Bohua Wan", "Aditya V. Kulkarni", "Nathan Drenkow", "Michael Oberst", "Paul H. Yi", "Mathias Unberath"], "abstract": "Artificial intelligence and machine learning (AI/ML) are poised to transform healthcare by enabling personalized and efficient patient care through data-driven insights. Although radiology is at the forefront of AI/ML adoption, in practice, the potential of AI/ML models is often overshadowed by severe failures to generalize: AI/ML models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch in advertised and observed AI/ML performance raises concerns that radiologists will be misled by incorrect AI/ML predictions in practice and/or grow to distrust AI/ML, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI/ML models throughout the development cycle on abundant and diverse data is thus critical to anticipate AI/ML model degradation when encountering varied data samples. Achieving these goals in practice, however, is challenging due to the high costs of collecting the necessary diverse data samples and the corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI/ML, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, and operating on latent representations for memory efficiency, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI/ML models in any desired range of diverse patient populations.", "sections": [{"title": "1 Main", "content": "Artificial intelligence and machine learning (AI/ML) have the potential to transform healthcare by deriving actionable insights into personalized care from vast amounts of data (i.e., precision medicine). The opportunities to improve the quality, efficiency, and accessibility of healthcare through AI/ML are numerous, with radiology and particularly CT image analysis being being a prime example. Potential applications in this area include triage acceleration, disease and injury detection, body composition measurement, and clinical decision-making. In light of the potential benefits of AI/ML technologies, FDA clearances for AI/ML-based \u201csoftware as a medical device (SaMD)\u201d have surged from 29 in 2020 to over 1016 in 2024, with a considerable portion aimed at medical image analysis for radiology. In many cases, a key requirement of regulatory clearance is demonstration of robust performance in controlled trials, as in Fig. 1a. However, substantial evidence points toward the brittleness of AI/ML models for image analysis, where performance often degrades significantly when deployed outside controlled environments. In fact, recent studies indicate that controlled trials can overestimate AI/ML performance by 20% or more, with a significant portion of evaluations based on retrospective data from a small number of institutions. These errors often stem from biases, shortcuts, and other differences between the data used for developing and validating models and the images observed from a target population during deployment. Proven approaches such as site-specific clinical trials or causal inference-based analytics can provide these insights if sufficient data is available, but ongoing data collection, and its annotation, is costly and impractical especially with deteriorating models that may already negatively impact patient care. Thus, anticipating model degradation through automatic, easily repeatable processes is paramount to guarantee peak model performance on an ongoing basis and to prevent erroneous outputs from adversely affecting treatment plans and encoding systemic biases into the mechanisms of precision medicine.\nOne way of overcoming these practical challenges is through virtual clinical trials (VCTs). The goal of VCTs is to replicate the model performance that would occur in a real target population using synthetic images. In this scenario, because both the data and its associated label are precisely known and specified at generation time,\nVCTs overcome a primary challenge of approaches that require real data. Although existing methods, like computational phantoms, have made some progress towards VCTs for some medical imaging applications, they do not yet offer a clear path toward image generation with sufficient realism, variability, scalability, and control to model diverse populations with reasonably low costs. Generative AI models, on the other hand, can consume and produce practically unlimited data. They have been used to augment model training with synthetic images, improving the performance of downstream AI models for radiology. Generative models are highly flexible in the kind of conditioning parameters they can incorporate, including any attributes that may lead to model degradation. While conditional generative models exist for other modalities, e.g., for chest X-ray, no CT image generative model has been developed with the capabilities necessary for conducting VCTs in radiology AI. First, volumetric full-body images are required as output in order to represent attributes based on an entire patient, such as height and weight, in an easily verifiable format. Second, the generated images must be sufficiently realistic in terms of visual features and anatomical structure, to ensure that observed changes in performance can be attributed to real model degradations and not domain gaps between synthetic and real images. Finally, the model should generate images with high fidelity to conditioning parameters, enabling VCTs to replicate target populations based solely on attributes that can be collected from medical records or modeled based on survey data, as in Fig. 1b.\nHere, we present the first CT image generative model to fully embody these capabilities, as demonstrated through multiple VCTs that replicate model performance and anticipate hidden biases for multiple tasks in radiology-based precision medicine. A significant challenge for our approach is the complexity of human anatomy, especially over the full body. Previous work has leveraged a strong prior on the anatomical shape, such as an image in a different imaging modality or a detailed organ segmentation $Y$, to model the conditional distribution $p(X|Y)$ of CT images $X$. Here, we incorporate anatomical consistency into a model by learning the joint distribution $p(X, Y)$. This results in images that are visually and anatomically accurate, even compared to methods that focus on smaller regions and have access to detailed anatomical structure information through segmentation. To accomplish this over the full body in a memory-efficient manner, our generative model operates on latent representations of the full-body image and segmentation. To support VCTs, we further model the distribution $p(X, Y|a)$ conditioned on patient attributes $a$ to allow for sampling of synthetic target populations. In our experiments, we model populations based on"}, {"title": "1.1 A Conditional Generative Model for Full-body CT Synthesis", "content": "Our generative model consists of three main components: (1) an image autoencoder, (2) a segmentation autoencoder, and (3) a latent diffusion model. The key capability of this model, which enables it to operate on full-body images with high resolution, is the ability to compress the image and segmentation data into a low-dimensional latent space while still enabling high-quality reconstruction. This compression and reconstruction is achieved in a patch-wise manner using a stacked autoencoder architecture, which allows for a high overall compression rate without sacrificing reconstruction quality. As shown in Fig. 2a, the image autoencoder $E_{img}$ and segmentation autoencoder $E_{seg}$ compress the full-body CT image $X$ and segmentation $Y$ into latent embeddings $Z_{img}$ and $Z_{seg}$, respectively. The latent embeddings are then used to reconstruct the image and segmentation using the corresponding decoders $D_{img}$ and $D_{seg}$.\nThe latent diffusion model (Fig. 2b) is a probabilistic model for the joint distribution $p(Z_{img}, Z_{sega})$ of the latent embeddings, conditioned on patient attributes $a$. In our experiments, the attributes are demographic categories describing the patient's sex, age, height, and weight, which are relevant to the tasks in precision medicine that we evaluate. During CT synthesis, the diffusion model samples a random latent code $Z = [Z_{img}, Z_{seg}]$ from the learned distribution, which is then decoded into a synthetic image $X$ and segmentation $\u1ef8$, as in Fig. 2c."}, {"title": "1.2 Evaluation Metrics", "content": "In evaluating our generative model and the VCTs it enables, we are often interested in assessing the similarity between univariate distributions. For example, may want to compare the distribution of measured height in the synthetic population to that in the target population, to evaluate the model's fidelity to conditioning parameters. For VCTs, we are primarily interested in whether the absolute error on synthetic images is representative of the absolute error on real images. To quantify the difference between samples from two distributions, we use the standard score (Z-score), which measures the difference in standard deviations. Given two sets of samples $x$ and $y$, the Z-score is defined as\n$Z(x,y) = \\frac{\\bar{x}-\\bar{y}}{\\sqrt{\\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n_y}}}$\n(1)\nA Z-score of 0 indicates that the distributions are identical in terms of mean and variance. In Section 1.5, we obtain 95% confidence intervals on the Z-scores using bootstrapping, which involves resampling the data with replacement and computing the Z-score for each resample. A narrow interval indicates higher confidence that the distributions are the same."}, {"title": "1.3 Synthetic Image Realism", "content": "The generative model described above is able to synthesize full-body CT images, as in Fig. 3, that are realistic in terms of visual features as well as anatomical consistency. This is important for ensuring that there is not a significant domain gap between synthetic and real images, which may cause performance degradation to be observed in VCTs even if the downstream model is robust in real images. We assess visual realism for reconstructions and synthetic samples primarily using the Frech\u00e9t Inception Distance (FID), a widely used metric for quantifying the similarity between synthetic and real images, including medical images. Although FID utilizes embeddings from an Inception V3 convolutional neural network that has been pre-trained to classify natural images, it has been shown to effectively evaluate the realism of CT images when using an appropriate dataset for comparison. The FID of the full-body images when using a stacked image decoder and latent diffusion model for the joint distribution was 5.97, comparable to related work. Guo et al. , for example, achieve an FID score of 6.083 using the autoPET 2023 dataset as a reference.\nBeyond low-level visual realism, the anatomical accuracy of synthetic images is important for VCTs in precision medicine as well as many other downstream applications. We first assess the internal consistency of the joint diffusion model $P_\u03b8(Z_{img}, Z_{sega})$ by evaluating consistency between the decoded segmentation $\u00dd = D_{seg}(Z_{seg})$ and independent segmentation of $X$, using TotalSegmentator. We find"}, {"title": "1.4 Fidelity to Conditioning", "content": "We evaluate our model's fidelity to conditioning by independently assessing the relevant values from each CT image and comparing them to the conditioned attribute category. For biological sex, we manually inspect 100 synthetic images randomly sampled with male or female conditioning, finding that sex conditioning results in the correct anatomy in 98% of cases. For age, height, and weight, we measure the relevant attribute from the CT image alone, using an independent organ segmentation. For this experiment, we sample synthetic images conditioned on the same attributes as each real image, with a one-to-one correspondence, so as to ensure realistic combinations of attributes. Because this measurement may differ systematically from the"}, {"title": "1.5 Virtual Clinical Trials for Radiology AI", "content": "In this section, we show that VCTs using synthetic images can replicate model performance and identify biases in downstream models for radiology AI. We focus on two tasks in precision medicine, body fat percentage (BFP) regression and muscle mass percentage (MMP) regression, which are important capabilities for opportunistic body composition measurement. To obtain ground truth, we use automated segmentations of tissue types to compute the mass ratio between the tissue type and the full body (see Section B). This also allows us to compute ground truth for synthetic images, in order to determine the downstream model error. For the downstream model, we use a deep neural network (DNN) to regress the target variable from 2D coronal and sagittal slices of the input image. Many clinical scenarios favor this \"2.5D\" approach, in which the model takes in multiple 2D slices that together capture 3D information about the patient, because it is significantly less computationally expensive than fully 3D models. In the context of body composition measurement, an error of about 2 percentage points or less is considered acceptable, while an average error above 3 percentage points is considered significant. To put this in context, American males have an average BFP from 22.9% at 16-19 years old to 30.9% at 60 - 79, as of 2009. Females range from 32.05% to 42.4%, based on dual-energy X-ray absorptiometry scans. MMP has been measured using full-body magnetic resonance imaging (MRI) at 38.4 \u00b1 5.1% for males and 30.6 \u00b1 5.5% for females. Note that we report the absolute error in terms of BFP or MMP; although the units are percentage points, these are absolute differences in the percentage of the original body mass, not percentage of the regressed quantity.\nTo highlight how VCTs using synthetic CT images can detect model degradation, we intentionally sample a biased training set with a shortcut that is, an easily detectable feature that is correlated with the output variable despite being non-clinically relevant. We bias the training set to have a high correlation between the body volume and the target variable, i.e., body fat or muscle mass percentage. We divide the withheld test set into two populations, an in-distribution (ID) population with the same bias as the training set, and an out-of-distribution (OOD) population with a different bias. For example, an ID population with high correlation between body volume and BFP will facilitate shortcut learning based on a specific linear relationship, but the corresponding OOD population will feature a different linear relationship between body volume and BFP. Overall, this replicates the real-world"}, {"title": "1.5.1 Detecting Model Degradation via VCTs", "content": "Does a VCT detect model degradation from the ID data to the OOD data, based on synthetic images with the same distribution of attributes?\nFor both tasks, we observe significant model degradation on the real OOD population, despite good performance on the ID population. As shown in Table 1, on the real ID test set fBFP achieves a mean absolute error (MAE) of 1.20% (95% CI: 1.04 to 1.40%) and \u0192MMP achieves an MAE of 1.43% (95% CI: 1.18 to 1.73%), confidently within a nominally acceptable error of 2 percentage points. This indicates that both models are capable of accurately estimating the target variable from CT slices and, in our scenario, may obtain regulatory clearance based on ID performance. However, on the real OOD test set, both fBFP and MMP degrade to unacceptable error levels, achieving an MAE of 3.66% (95% CI: 3.10 to 4.31%) and 5.54% (95% CI: 5.02 to 6.16%), respectively. This indicates the models are not robust to the population shift, and may lead to adverse effects on patient care if deployed without further validation. Conventional approaches to anticipate model degradation may fail to detect this bias. With the same information available, a straightforward baseline approach is to reweight the errors measured on the ID test set based on the likelihood p(OOD|a) of coming from the OOD population, so that the model's degradation on the most relevant samples is amplified. This yields an estimated MAE on the real OOD set of 1.31% (95% CI: 1.04, 1.54) for BFP, which does not indicate the true MAE on the OOD set of 3.66% or signify errors outside the acceptable range. For MMP, the weighted MAE is 1.65% (95% CI: 1.25, 2.40), which is far from the true value of 5.54%. These results indicate that conventional statistical approaches for detecting model degradation are not sufficient to detect the bias due to population shift in our experiments.\nIn contrast, VCTs using synthetic images can detect model degradation in both tasks, based on the distribution of attributes. Because the patient attributes are not fully predictive of ID/OOD status, we oversample the ID and OOD populations by a factor of 2, resulting in two distinct synthetic images conditioned on a for each patient i in the test sets. For both tasks, we find that the MAE on synthetic images aligns with that of real images, indicating acceptable errors (< 2%) for the ID population and significant errors for the OOD population (>3%). There is, however, a difference in the distribution of errors on synthetic and real images. We hypothesize that this"}, {"title": "1.5.2 Replicating Model Biases in VCTS", "content": "Do VCTs with synthetic images reveal the exact kind of bias in the downstream model's performance on real images?\nTo answer this question, we examine the patient attributes that may be to blame for model error across the combined ID and OOD test sets. Fig. 5c-h shows the distribution of the errors with respect to the patient attributes used to bias the training data. Qualitatively, the distribution of attributes and errors in the reconstructed test sets closely matches the real test sets. Likewise the synthetic images show a similar distribution of errors even without corresponding samples, with higher error on the OOD side of the bias split at the furthest points from the boundary. Quantitatively, Table 2 details the Pearson correlation coefficient between each variable and the model error on real and synthetic data, showing close alignment with a Z-test p-value indicating high probability of being sampled from the same distribution (p > 0.3). This indicates that the synthetic data replicates the same bias found in the real data with respect to the known biased attributes, which is only possible in this case because the bias is artificially constructed.\nTo quantify the bias of fBFP and fMMP more broadly, we conduct a feature importance analysis to determine the patient attributes which are most predictive of model degradation. For each task and image type (real, reconstructed, synthetic), we train a random forest regression model to predict the absolute error of the downstream model based on 8 patient attributes: sex, age, height, weight, body fat percentage, bone density, muscle mass percentage, and body volume. These features are sufficient to predict the model error with an average MAE of 0.69 percentage points across all image types (see Table 3). Feature importance analysis reveals the patient attributes that are most predictive of the model error. We find that for real, reconstructed, and synthetic test sets, the feature importance values are highly correlated. The feature"}, {"title": "2 Discussion", "content": "VCTs are a key component in the emerging landscape of AI/ML models for radiology. Our first-of-its-kind generative model demonstrates a scalable, flexible, and highly realistic approach to synthesizing virtual patient cohorts suitable for VCTs in precision medicine. We have shown that our model is capable of synthesizing full-body CT images with a high level of realism in terms of visual appearance and anatomical structure. It can generate images from patient attributes (sex, age, height, and weight) that are readily available from medical records and lend themselves to distribution modeling for VCTs. In a simulated VCT, we demonstrated that validation with these images can replicate real biases in downstream AI models across multiple tasks.\nFull-body volumetric image synthesis presents significant challenges, which have confronted in this work. 3D convolutional models are memory intensive, but the need for global consistency in anatomical structures requires a 3D approach. Even using latent image diffusion, image encoders and decoders processing full body volumetric CT images are too large to fit on a single GPU. Prior work has enabled partial CT image synthesis by splitting tensors across multiple GPUs. Our approach takes the more traditional patch-wise encoder-decoder strategy with a stacked VQ-VAE and a final post-processing network to refine synthetic images. To further improve image realism, we introduced a novel multi-window loss function that reweights the contributions of soft and hard tissue structures, ensuring that larger gradients from hard tissue structures do not dominate learning. Our experiments demonstrated the value of this approach in terms of reconstructed and synthetic image quality, achieving an average FID score of 5.97, which is important to reducing the sim-to-real gap for downstream tasks.\nIn addition to low-level image realism, VCTs require high-level anatomical realism. Target variables like BFP and MMP are only meaningful for anatomically realistic full-body images for which segmentations of organs and tissue type can be easily obtained. To achieve global anatomical consistency, our approach included organ segmentations in the latent embedding, using a second autoencoder. Without this, we found that synthetic images might have low FID but lack basic anatomical structures as evaluated by third-party segmentation models. Learning the joint distribution of images and basic organ segmentations yielded generative model with valid organ segmentations closely aligned with real images, in terms of the position and size of segmented organs.\nFinally, VCTs require a way to condition image generation on relevant patient attributes. Our model uses categorical conditioning based on demographic attributes from the available metadata. Independent verification of the sex, age, height, and weight shows successful alignment with the training data in terms of these attributes, although in some cases the measured attribute in the real and synthetic images differed from the value in the metadata. This could be because of variable measuring techniques, such as measuring an individual's weight with their clothes on or measuring the body length with limbs bent. By assessing the difference between quantities measured in the same way, using TotalSegmentator-derived quantities, we can nevertheless conclude that images sampled with a given attribute will align with real images in terms of that attribute, if not with the nominal value in the decedent record.\nOur experiments showed that VCTs using synthetic images were able to detect real model biases with respect to patient attributes. Downstream models for BFP and MMP regression were trained on biased data, with a shortcut that correlated body volume with the target variable. This resulted in significant model degradation"}, {"title": "3 Conclusion", "content": "In conclusion, this work advances the state of generative modeling in precision medicine by introducing a first-of-its-kind conditional generative AI model capable of full-body CT image synthesis for VCTs. By achieving high anatomical and visual realism and precise conditioning on demographic attributes, this model enables scalable, proactive assessments of AI model robustness across diverse populations. Our experiments demonstrate the efficacy of VCTs in detecting performance degradations and biases in medical imaging AI systems, replicating real-world model behavior and identifying the population attributes responsible for degradation. These findings establish a pathway for mitigating biases and safeguarding patient care without the extensive costs and impracticalities of on-going real-world data collection. While the approach highlights the potential of generative AI to revolutionize model validation and robustness assessment, further exploration into broader conditioning capabilities and emergent properties of generative models trained at scale will be crucial. Such advancements could expand the scope of VCTs, enabling a more comprehensive evaluation of AI systems for precision medicine and fostering their safe and equitable deployment."}, {"title": "4 Methods", "content": "Figure 2 shows the overall structure of our generative model, which is composed of 4 parts: 1. a stacked CT image autoencoder, $(E_{img} = E^{(2)} o E^{(1)}, D_{img} = D^{(1)} o D^{(2)})$, that compress input CT image to latent CT vector, $Z_{img}$, with high compression ratio while preserving anatomical structures. 2. a segmentation autoencoder, ${E_{seg}, D_{seg}}$, that compress segmentation to latent segmentation vector, $Z_{seg}$, with the same compression ratio. 3. a latent diffusion model for conditional latent vector sampling. 4. a 3D U-Net based post-processing model that further improves the realism of the generated samples."}, {"title": "4.1.1 Stacked Autoencoder", "content": "We propose a framework for stacking autoencoders to achieve better performance in terms of preserving anatomical structures while compressing images to extreme. Vanilla autoencoder such as Vector Quantized Variational Autoencoder (VQ-VAE), and Vector Quantized Generative Adversarial Network (VQ-GAN) first compress images to latent vectors with a single encoder and then decompress the latent vectors back to reconstructed images with a single decoder. Although using a pair of single encoder and single decoder is simpler, it limits the reconstruction quality and the compression rate. The latent vectors produced by these models are typically 4 to 8 times smaller than the original images in spatial dimensions (height, width, and depth). It has been shown that the reconstruction quality decreases as the compression ratio increases. In this approach, we stack multiple encoders and decoders instead. The compression rate of each pair of encoder and decoder is kept small to reduce the difficulties in learning, as it is considerably harder to train encoder and decoder with high compression rate (16 for example) than to train encoder and decoder with low compression rate. The training of each pair of encoder and decoder is separate, thus the model size of each pair is not limited by the number of levels of stacking and training larger model with limited memory is made possible.\nDue to computational limitation, all autoencoders are implemented in a patch-based manner and image-level reconstructions and latent vectors are obtained using sliding window with patch-based autoencoders. In the following text, we use bolded lower case letter to represent a patch of an image and use its upper case letter to denote the whole image. For example, $x \\in R^{h\\times w\\times d}$ denotes a $(h,w, d)$ sized patch of an input image $X \\in R^{H\\times W\\times D}$ of size (H, W, D).\nFormally, we define a stacked autoencoder as $(E,D)^{(L_{ae})}$, where $E = E^{(L_{ae})} o E^{(L_{ae-1})} o... o E^{(1)}$ and $D = (D)^{(1)} o D^{(2)} o ... o D^{(L_{ae})}$. $L_{ae} \\in Z$ denotes the maximum level of stacking and o denotes composition. The vanilla autoencoder is a special case when $L_{ae} = 1.$"}, {"title": "4.1.2 Attribute Conditioned Latent Diffusion Model", "content": "Similar to Patrick et al. , we built upon the 2D U-Net based latent diffusion model developed by Esser et al. for natural image generation and developed a 3D U-Net latent diffusion model. The 2D operations in the 2D U-Net were propagated to 3D operations to support 3D latent diffusion.\nThe classifier free guidance was used for attribute conditioning. In our study, we consider categorical attributes. A patient's attributes are first converted to categories: a = (asex, aage, Aheight, Aweight) including sex, age, weight, and height (with an additional $a_{none}$ category for each attribute) as dicussed in section 1.1 and then mapped to learnable embeddings. The embeddings are then incorporated to each level of the 3D U-Net to guide the denosing process following Patrick et al [39]. The $a_{none}$ is used to represent unavailable attributes or randomly dropped attributes in classifier free guidance.\nLet the latent embeddings of a CT image and segmentation be $Z_{img} = E_{img} (X)$, and $Z_{seg} = E_{seg} (Y)$, where X is a CT image and Y is the segmentation of the CT image X. The latent diffusion model es takes both as the input the learn the joint distribution of CT and segmentation latent embeddings $Z = [Z_{img}, Z_{seg}]$ conditioning on the patient metadata a."}, {"title": "4.1.3 Post-processing Model", "content": "Since the proposed stacked autoencoder preserves most anatomical structures, the reconstructed images tends to be overly smooth compared with the original image causing the sampled images to be also smooth and lack of details. To restore the lost high frequency information, we trained a 3D U-Net to post-process the decoded images.\nLet the post-process 3D U-Net be f(X), where X = Dimg(Eimg(X)) is a reconstructed image. We train f with the L1 loss and perceptual loss to minimize the distance between f(X) and X. The loss function is defined as:\n$L_{post}(X, X) = L_{1}(X, X) + L_{per}(\u00c2Y, X)$\nWe then process sampled images \u2611 to restore lost details and increase fidelity with the post process model. The post processed sample image is $X_{post} = f(X)$\nThe post-processing mode use identical architecture as the latent diffusion model but without any conditioning."}, {"title": "4.2 Training Details", "content": "Here, we describe the training details for the above model, including the full-body CT dataset used for training and validation. The downstream models consider during the virtual clinical trial are also described below."}, {"title": "4.2.1 Full-body CT Dataset", "content": "We derive a dataset of 798 full body CTs from the New Mexico Decedent Image Database (NMDID), an open resource maintained by the University of New Mexico that provides a de-identified CT scans of deceased individuals. This database includes CT scans from over 15,000 de-identified individuals, collected between 2010 and 2017. The standard collection protocol includes three scans that together cover the full body: (1) the head, neck and upper extremities (H-N-UXT); (2) the torso (TORSO); and (3) lower extremities (LEXT). We use organ centroids from TotalSegmentator to initialize a rigid intensity-based registration, keeping the majority of the H-N-UXT scan for the overlapping region. This generally includes the arms, which are folded over the chest. These are then resized to a resolution of 1 x 1 mm with a slice thickness of 3mm. Segmentations of the body, 128 organs, and 3 tissue types are acquired using TotalSegmentator. For the segmentation autoencoder, the 128 organs are reduced to 16 by combining related organs. The \"bone\" class refers to non-appendicular bones. Large organs were prioritized over small organs to capture as much anatomical structure while preserving GPU memory. The full list or organ classes are listed in Table 1."}, {"title": "4.2.2 VQ-VAE Training Details", "content": "Due to the high memory consumption of 3D convolutions, the autoencoders and post-processing model are implemented in a patch-based manner. Let $x \\in R^{h\\times w\\times d}$ denotes a (h, w, d) sized patch of an input image $X \\in R^{H\\times W\\times D}$ of size (H, W, D). The embedding of a whole CT image and segmentation $Z = [Z_{img}, Z_{seg}]$ is obtained with sliding window approach.\nWe developed three types of autoencoders, including vanilla CT image autoencoders, a 2 level stacked CT image autoencoders, and vanilla segmentation autoencoders. The compression rate (or composed compression rate for stacked autoencoders) of all autoencoders is kept at 16 along each dimension. The patch sizes of vanilla CT image autoencoders and segmentation autoencoders are (128,128,128). The stacked CT image autoencoders use (128,128,128) patch sizes at the highest level $(E^{(2)}, D^{(2)})$, and (96,96,96) at the lowest level $(E^{(1)}, D^{(1)})$. AdamW is used as the optimizer with a learning rate of 0.0000375 for all optimizers. The batch size of all autoencoders is 1. The latent dimensions of the U-Net of the vanilla CT image autoencoders, the 2-level stacked image autoencoders, and the vanilla segmentation autoencoders are (32,64, 128,256); (64, 128) (level 2) and (128,256) (level 1); and (32, 64, 128, 256). In our experiments we used a VQ-VAE with a level-2 stacking and a composed downscaling factor of 16 based on the hyperparameter search in Table A3."}, {"title": "4.2.3 Latent Diffusion Model", "content": "The latent vectors of the image $Z_{img}$ and the segmentation $Z_{seg}$ are concatenated together as $Z \\in R^{2\\times48\\times48\\times48}$. The latent diffusion model is trained to diffusion and reverse diffusion the latent vector using a U-Net. The latent dimensions of the U-Net are (160, 320, 720, 1280). The learning rate is kept as 1 and batch size is 1. The dimension of each patient attribute embedding is 32. During training, the"}, {"title": "4.2.4 Post-processing Model", "content": "The post-processing model is also developed to process patches. The patch size is (80, 80, 24). Same U-Net structure as the latent diffusion model is used with the same latent dimensions. Learning rate is set as 0.0001 and batch size is also 1. We use AdamW as the optimizer with a learning rate of 0.0001. One NVIDIA A5000 with 48GB of GPU memory is used for training. PyTorch is used as the deep learning framework for all the experiments in this paper."}, {"title": "4.3 Downstream AI Model Training", "content": "In this section we describe the data, AI models, and training for the downstream precision medicine tasks discussed in Section 1.5. We consider two tasks in body composition measurement, an important part of precision medicine that provides more physically meaningful measurements than the body mass index but may be difficult to measure. The ground truth for all images is obtained through analysis of an independent segmentation, as described in Section B."}, {"title": "4.3.1 Biased Datasets for Body Composition Measurement", "content": "To create a biased model, we intentionally bias the training set based on patient attributes. For body fat percentage regression, we divided the data based on a linear decision boundary in terms of muscle mass and body volume. Because body fat and muscle mass percentage are related, this results in a high correlation between the body volume and body fat percentage (Pearson r = 0.872 in the training set). This creates a potential shortcut for model learning to estimate the body fat percentage based on body volume, resulting in a biased output. For muscle mass percentage regression, we take the same approach, while splitting the training distribution and OOD samples based on body fat so that the target variable is not directly used in the split, resulting in a Pearson coefficient of -0.696 between muscle mass and body volume. In each case, the training set consists of 200 real CT images, while the ID and OOD test sets contain 75 images each. We denote each test split by the set of patient identifiers, e.g., $S_{ID}^{(fat)}$ or $S_{OOD}^{(fat)}$ to denote the ID and OOD test sets of the split based on body fat percentage, respectively. The downstream model consists of a Swin-B transformer backbone with ImageNet-21k pre-training. The backbone image encoder processes a sagittal and coronal loss, which are then concatenated and followed by a linear layer with scalar output and mean squared error loss. During training, slices are sampled randomly from the middle third of the CT image, while during validation the middle slice is sampled deterministically, and resized to 384\u00d7384. The downstream model is trained for 100 epochs with a batch size of 16 and an initial learning rate of 0.0001, decreased by a factor of 10 at epoch 50 and again at epoch 90, using the Adam"}]}