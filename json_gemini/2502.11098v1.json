{"title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems", "authors": ["Zhao Wang", "Sota Moriyama", "Wei-Yao Wang", "Briti Gangopadhyay", "Shingo Takamatsu"], "abstract": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-01), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., React, GPT40), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available at https://github.com/sony/talkhier.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM) Agents have broad applications across domains such as robotics (Bro-han et al., 2022), finance (Shah et al., 2023; Zhang et al., 2024b), and coding (Chen et al., 2021; Hong et al., 2023). By enhancing capabilities such as autonomous reasoning (Wang et al., 2024b) and decision-making (Eigner and H\u00e4ndler, 2024), LLM agents bridge the gap between human intent and machine execution, generating contextually relevant responses (Pezeshkpour et al., 2024).\nRecent research has primarily focused on LLM-based Multi-Agent (LLM-MA) systems, which"}, {"title": "2 Related Work", "content": "Collaborative LLM-MA. LLM-MA systems enable agents to collaborate on complex tasks through dynamic role allocation, communication, and task execution (Guo et al., 2024; Han et al., 2024). Recent advancements include agent profiling (Yang et al., 2024), hierarchical communication (Rasal, 2024), and integration of reasoning and intentions (Qiu et al., 2024). However, challenges remain in ensuring robust communication, avoiding redundancy, and refining evaluation pro-cesses (Talebirad and Nadiri, 2023). Standardized benchmarks and frameworks are needed to drive future progress (Li et al., 2024).\nCommunication in LLM-MA. Effective communication is crucial for collaborative intelli-gence (Guo et al., 2024). While many previ-ous works, including chain (Wei et al., 2022), tree (Yao et al., 2023), complete graph (Qian et al., 2024), random graph (Qian et al., 2024), opti-mizable graph (Zhuge et al., 2024), and pruned graph (Zhang et al., 2024a) methods have focused on communication topologies, there has been lim-ited discussion on the optimal form of commu-nication. Most systems rely on text-based ex-changes (Zhang et al., 2024a; Shen et al., 2024), which is inefficient and prone to errors as agents often lose track of subtasks or fail to recall prior outputs as tasks grow in complexity. We argue"}, {"title": "3 Methodology", "content": "TalkHier aims to design a LLM-MA system rep-resented as a graph G = (V, E), where V denotes the set of agents (nodes) and E represents the set of communication pathways (edges). Given an in-put problem p, the system dynamically defines a set of communication events Cp, where each event"}, {"title": "3.1 Agents with Independent Memory", "content": "Each agent vi \u2208 V in graph G can be formally represented as:\nVi = (Rolei, Plugins\u00bf,Memory\u017c, Type\u017c) .\nRole: Assign roles such as generator, evaluator, or revisor based on the task type. Plugins: Exter-nal tools or plugins attached for domain-specific operations. Memory: An agent-specific memory that stores and retrieves information relevant to the agent's role and task. Type: Specifies whether the agent is a Supervisor (S) responsible for over-seeing task success, or a Member (M) focused on problem-solving.\nThe first two components-Role\u017c, and Plugins are standard in most related works,"}, {"title": "3.2 Context-Rich Communication Between Agents", "content": "Communication between agents is represented by communication events $c_{ij}^{(t)} \\in C_p$, where each event encapsulates the interaction from agent $v_i$ to agent $v_j$ along an edge $e_{ij} \\in E$ at time step t. Formally, a communication event $c_{ij}^{(t)}$ is defined as:\n$C_{ij}^{(t)} = (M_{ij}^{(t)}, B_{ij}^{(t)}, I_{ij}^{(t)})$, where $M_{ij}^{(t)}$ indicates the message content sent from $v_i$ to $v_j$, containing instructions or clarifica-tions, $B_{ij}^{(t)}$ denotes background information to en-sure coherence and task progression, including the problem's core details and intermediate decisions, and $I_{ij}^{(t)}$ refers to the intermediate output generated by $v_i$, shared with $v_j$ to support task progression and traceability, all at time step t. These structures ensure that agents of TalkHier accomplish efficient communication and task coordination.\nCommunication Event Sequence. At each time step t, the current agent vi communicates with a connected node vj, with one being selected by the LLM if more than one exists. The elements of each edge M, B and It are then generated by invoking an independent LLM. To ensure con-sistency, clarity, and efficiency in extracting these elements, the system employs specialized prompts tailored to the roles of Supervisors and Members, as illustrated in Figure 4. Most notably, background information B is not present for connections"}, {"title": "3.3 Collaborative Hierarchy Agent Team", "content": "The entire graph G consists of multiple teams, each represented as a subset Vteam \u2286 V. Each team in-cludes a dedicated supervisor agent team and one or more member agents veam. A key feature of the hierarchical structure in TalkHier is that a mem-ber agent in one team can also act as a supervisor for another team, creating a nested hierarchy of agent teams. As shown in the second row of Fig-ure 3, this structure enables the entire graph G to represent a hierarchical node system, where teams are recursively linked through supervisor-member relationships.\nFormally, the hierarchical structure of agents with two teams is defined as:\n$V_{main} = {V_{main}^{Gen}, V_{main}^{S}, V_{main}^{Rev}, V_{main}^{E_{eval}}}$,\n$V_{eval}= {V_{eval}, v_{eval}^{S}, E_eval^1, E_{eval}^2,ldots, E_{eval}^n}$, where the Main Supervisor ($V_{main}$) and Evaluation Supervisor ($V_{eval}$) oversee their respective team's"}, {"title": "4 Experiments", "content": "In this section, we aim to answer the following research questions across various domains:\nRQ1: Does TalkHier outperform existing multi-agent, single-agent, and proprietary approaches on general benchmarks?\nRQ2: How does TalkHier perform on open-domain question-answering tasks?\nRQ3: What is the contribution of each component of TalkHier to its overall performance?\nRQ4: How well does TalkHier generalize to more practical but complex generation task?"}, {"title": "4.1 Experimental Setup", "content": "Datasets. We evaluated TalkHier on a diverse col-lection of datasets to assess its performance across various tasks. The Massive Multitask Language Understanding (MMLU) Benchmark (Hendrycks et al., 2021) tests domain-specific reasoning prob-lems including Moral Scenario, College Physics, Machine Learning, Formal Logic and US Foreign Policy. WikiQA (Yang et al., 2017) evaluates open-domain question-answering using real-world ques-tions from Wikipedia. The Camera Dataset (Mita et al., 2024) focuses on advertisement headline gen-eration, assessing the ability to create high-quality advertising text.\nBaselines. To evaluate TalkHier, we compared it against a comprehensive set of baselines including:\n\u2022 GPT-40 (OpenAI, 2024a), based on OpenAI's GPT-4 model with both single-run and ensem-ble majority voting (3, 5, or 7 runs).\n\u2022 OpenAI-01-preview (OpenAI, 2024b), a beta model using advanced inference techniques, though limited by API support.\n\u2022 ReAct (Yao et al., 2022), a reasoning and ac-tion framework in single-run and ensemble configurations.\n\u2022 AutoGPT (Gravitas, 2023), an autonomous agent designed for task execution and iterative improvement.\n\u2022 AgentVerse (OpenBMB, 2023), a multi-agent system framework for collaborative problem-solving.\n\u2022 GPTSwarm (Zhuge et al., 2024), a swarm-based agent collaboration model utilizing op-timizable communication graphs.\n\u2022 AgentPrune (Zhang et al., 2024a), a model leveraging pruning techniques for efficient multi-agent communication and reasoning."}, {"title": "4.2 Performance on MMLU (RQ1)", "content": "Table 1 reports the average accuracy of various models on the five domains of MMLU dataset. TalkHier, built on GPT-40, achieves the high-est average accuracy (88.38%), outperforming open-source multi-agent models (e.g., AgentVerse, 83.66%) and majority voting strategies applied to current LLM and single-agent baselines (e.g., ReAct-7@, 67.19%; GPT-40-7@71.15%). These results highlight the effectiveness of our hierarchi-cal refinement approach in enhancing GPT-40's per-formance across diverse tasks. Although OpenAI-o1 cannot be directly compared to TalkHier and other baselines-since they are all built on GPT-40 and OpenAI-01's internal design and training data remain undisclosed\u2014TalkHier achieves a slightly higher average score (88.38% vs. 87.56%), demon-"}, {"title": "4.3 Evaluation on WikiQA Benchmark (RQ2)", "content": "We evaluated TalkHier and baselines on the Wik-iQA dataset, an open-domain question-answering benchmark. Unlike MMLU, WikiQA requires gen-erating textual answers to real-world questions. The quality of generated answers was assessed using two metrics: Rouge-1 (Lin, 2004), which measures unigram overlap between generated and reference answers, and BERTScore (Zhang et al., 2020), which evaluates the semantic similarity be-tween the two.\nTable 2 shows that TalkHier outperforms base-lines in both Rouge-1 and BERTScore, demonstrat-ing its ability to generate accurate and semantically relevant answers. While other methods, such as AutoGPT and AgentVerse, perform competitively, their scores fall short of TalkHier, highlighting its effectiveness in addressing open-domain question-answering tasks."}, {"title": "4.4 Ablation Study (RQ3)", "content": "To better understand the contribution of individual components in TalkHier, we conducted ablation studies by removing specific modules and evalu-ating the resulting performance across the Moral Scenario, College Physics, and Machine Learning domains. The results of these experiments are sum-marized in Table 3.\nTable 3 presents the contributions of our ablation study on the main components in TalkHier. Remov-ing the evaluation Supervisor (TalkHier w/o Eval. Sup.) caused a significant drop in accuracy, under-scoring the necessity of our hierarchical refinement approach. Replacing the structured communication protocol with the text-based protocol (TalkHier w. Norm. Comm) resulted in moderate accuracy re-ductions, while eliminating the entire evaluation"}, {"title": "4.5 Evaluation on Ad Text Generation (RQ4)", "content": "We evaluate TalkHier on the Camera dataset (Mita et al., 2024) using traditional text generation metrics (BLEU-4, ROUGE-1, BERTScore) and domain-specific metrics (Faithfulness, Fluency, At-tractiveness, Character Count Violation) (Mita et al., 2024). These metrics assess both linguistic quality and domain-specific relevance.\nSetting up baselines like AutoGPT, AgentVerse, and GPTSwarm for this task was challenging, as their implementations focus on general benchmarks like MMLU and require significant customization"}, {"title": "5 Discussion", "content": "The experimental results across the MMLU, WikiQA, and Camera datasets consistently demonstrate the superiority of TalkHier. Built on GPT-40, its hierarchical refinement and structured communication protocol enable robust and adaptable perfor-mance across diverse tasks.\nGeneral and Practical Benchmarks. TalkHier outperformed baselines across general and practi-cal benchmarks. On MMLU, it achieved the high-est accuracy (88.38%), surpassing the best open-source multi-agent baseline, AgentVerse (83.66%), by 5.64%. On WikiQA, it obtained a ROUGE-1 score of 0.3461 (+5.32%) and a BERTScore of 0.6079 (+3.30%), outperforming the best baseline, AutoGPT (0.3286 ROUGE-1, 0.5885 BERTScore). On the Camera dataset, TalkHier exceeded OKG across almost all metrics, demonstrating superior Faithfulness, Fluency, and Attractiveness while maintaining minimal Character Count Violations. These results validate its adaptability and task-specific strengths, highlighting its advantage over inference scaling models (e.g., OpenAI-01), open-"}, {"title": "6 Conclusions", "content": "In this paper, we propose TalkHier, a novel frame-work for LLM-MA systems that addresses key chal-lenges in communication and refinement. To the best of our knowledge, TalkHier is the first frame-work to integrate a structured communication pro-tocol in LLM-MA systems, embedding Messages, intermediate outputs, and background information to ensure organized and context-rich exchanges. At the same time, distinct from existing works that have biases on inputs, its hierarchical refinement approach balances and summarizes diverse opin-ions or feedback from agents. TalkHier sets a new standard for managing complex multi-agent inter-actions across multiple benchmarks, surpassing the best-performing baseline by an average of 5.64% on MMLU, 4.31% on WikiQA, and 17.63% on Camera benchmarks. Beyond consistently outper-forming prior baselines, it also slightly outperforms the inference scaling model OpenAI-01, demon-strating its potential for scalable, unbiased, and high-performance multi-agent collaborations."}, {"title": "Limitations", "content": "One of the main limitations of TalkHier is the rel-atively high API cost associated with the experi-ments (see Appendix A for details). This is a trade-off due to the design of TalkHier, where multiple agents collaborate hierarchically using a specifi-cally designed communication protocol. While this structured interaction enhances reasoning and coor-dination, it also increases computational expenses. This raises broader concerns about the accessibil-ity and democratization of LLM research, as such costs may pose barriers for researchers with limited resources. Future work could explore more cost-efficient generation strategies while preserving the benefits of multi-agent collaboration."}, {"title": "A Cost Analysis for Experiments", "content": "The total expenditure for the experiments across the MMLU dataset, WikiQA, and Camera (Japanese Ad Text Generation) tasks was approximately $2,100 USD. It is important to note that this amount reflects only the cost of final successful executions using the OpenAI 40 API (as TalkHier and almost all other baselines are built on OpenAI 40 backbone). Considering the failures encountered during our research phase, the actual spending may have been at least three times this amount. Below is a detailed breakdown of costs and task-specific details."}, {"title": "A.1 MMLU Dataset (1,450 USD)", "content": "The MMLU dataset comprises approximately 16,000 multiple-choice questions across 57 subjects. For our experiments, we focused on five specific domains:"}, {"title": "A.1.1 Cost Analysis for the Moral Scenario Task and Baselines", "content": "The Moral Scenario task involved generating and evaluating responses for various moral dilemma scenarios using OpenAI's GPT-40 model. Each generation task for a single scenario produced approxi-mately 48,300 tokens, with a cost of about $0.17 per task. Given a total of 895 tasks, the overall token consumption and cost were:\n0.  17 x 895 = 152.15 USD\nIn addition to the Moral Scenario task, we conducted multiple baseline tests using GPT-40, which incurred an additional cost of approximately $3,000 USD. Therefore, the total cost for all GPT-40 evaluations in the Moral Scenario task is:\n152.15+900 = 1052.15 USD"}, {"title": "A.1.2 Cost Analysis for Other Tasks", "content": "In addition to the previously analyzed tasks, we conducted further evaluations across multiple domains using OpenAI's GPT-40 model. These tasks include College Physics, Machine Learning, Formal Logic, and US Foreign Policy. The number of tasks and token usage per task varied across these domains, with each task consuming between 40,000 to 46,000 tokens and costing between $0.14 to $0.15 per task.\n\u2022 College Physics: 101 tasks, each generating 40,000 tokens.\n\u2022 Machine Learning: 111 tasks, each generating 40,000 tokens.\n\u2022 Formal Logic: 125 tasks, each generating 46,000 tokens.\n\u2022 US Foreign Policy: 100 tasks, each generating 45,000 tokens.\nThe total expenditure for these tasks amounted to $63.43 USD. and we also did experiments for various baseline, it cost around 320 usd. totally it is 383.43. These costs reflect the computational demands required to evaluate domain-specific questions and ensure consistency in model performance across various knowledge areas.\nThe total expenditure for these tasks amounted to $63.43 USD. Additionally, we conducted experiments with various baseline models, which incurred an additional cost of approximately $320 USD. In total, the overall expenditure was $383.43 USD. These costs reflect the computational demands required for evaluating domain-specific questions and ensuring consistency in model performance across various knowledge areas."}, {"title": "A.2 WikiQA Dataset (1,191.49 USD)", "content": "The WikiQA dataset comprises 3,047 questions and 29,258 sentences, of which 1,473 sentences are labeled as answers to their corresponding questions. Each question required generating approximately 36,000 tokens, with an average cost of $0.13 per question. Given this setup, the total expenditure for the WikiQA task was:\n0.  13 \u00d7 1,473 = 191.49 USD"}, {"title": "A.3 Camera Dataset (400.56 USD)", "content": "The Camera dataset task involved generating and evaluating ad headlines for 872 different test sets using OpenAI's GPT-40 backbone. Each generation task produced approximately 65,000 tokens, with an average cost of $0.23 per task. Given this setup, the total expenditure for the Camera dataset task was:\n0.  23 x 872 = 200.56 USD\nWe also conducted experiments for three baseline models, which cost approximately $200 USD. In total, the expenditure amounted to $400.56 USD. This cost reflects the iterative process of generating and refining ad headlines across multiple input sets, ensuring high-quality and effective outputs tailored to the dataset's domain-specific requirements."}, {"title": "B Prompt Design and Work Flow for Tasks in MMLU", "content": "In this section, we describe the prompt design for evaluating and revising responses for each MMLU task. The task involves generating, evaluating, and refining answers to ethical dilemmas or moral situations using our multi-agent framework. Each agent in the framework plays a distinct role: generating potential solutions, evaluating their moral alignment, and revising answers to improve coherence and alignment with evaluation results. The prompts used for each agent are detailed below."}, {"title": "B.1 Initial Prompt", "content": "The following is the prompt given to the supervisor at the beginning."}, {"title": "B.2 Answer Generator", "content": "This agent generates answers to a specific moral scenario by considering the ethical implications of the situation."}, {"title": "B.3 Answer Evaluator", "content": "This agent evaluates the answers generated by the Answer Generator, providing scores and feedback based on predefined metrics such as ethical soundness, logical consistency, fairness, and feasibility."}, {"title": "B.4 Answer Revisor", "content": "This agent revises answers that receive low scores in the evaluation step. Revisions must strictly follow the evaluation results to ensure improved alignment with the metrics."}, {"title": "B.5 Settings for each Task", "content": "B.5.1 Evaluator Types\nB.5.2 Tools\nTo enhance the evaluation capabilities of each agent, we have deployed tools for each evaluator to use. The tools are listed as follows:\n\u2022 Output Tool (All Evaluators): A tool for outputting thoughts, allowing the model to repeatedly think.\n\u2022 Truth Table Generator (Truth Table Evaluator): A tool for outputting a truth table, given a proposition as input.\n\u2022 Counterexample Verifier (Truth Table Evaluator): A tool for verifying whether a counterexample is correctly defined.\nHere, the evaluator shown in the brackets are those who have access to the specific tool."}, {"title": "B.6 Good Revision Example for Moral Scenarios Task", "content": "The following example demonstrates how the multi-LLM framework revises an answer for a moral scenario. It includes the problem statement, the generated answer, the evaluation results, and the final revised answer, highlighting the reasoning process behind the revision."}, {"title": "C Prompt Design and Work Flow for for WikiQA", "content": "In this section, we provide a detailed example of how the multi-agent framework processes a WikiQA task, specifically the question: \"What are points on a mortgage?\" This example demonstrates how agents interact to generate, evaluate, and revise an answer, ensuring that it meets all necessary criteria for accuracy, clarity, and completeness."}, {"title": "C.1 Initial Question", "content": "The user asks the question: \"What are points on a mortgage?\""}, {"title": "C.2 Step 1: Answer Generation", "content": "The first step involves the Answer Generator agent, which is tasked with generating a detailed response to the question. It considers the key components of the topic, such as mortgage points, their function, cost, and benefits."}, {"title": "C.3 Step 2: Evaluation by the ETeam Supervisor", "content": "The ETeam Supervisor evaluates the answer based on two primary metrics: Simplicity and Coverage. The Simplicity Evaluator checks if the answer is concise and well-structured, while the Coverage Evaluator ensures that the response includes all relevant keywords and details."}, {"title": "C.4 Step 3: Revisions by the Answer Revisor", "content": "Despite the high evaluation scores, the Coverage Evaluator suggests a slight revision for clarity. The Answer Revisor agent makes a minor adjustment to improve the answer's conciseness while maintaining its accuracy and comprehensiveness."}, {"title": "C.5 Step 4: Final Evaluation", "content": "The revised answer is re-evaluated by the ETeam Supervisor, and all metrics receive top scores. The revised response is clear, concise, and includes all relevant keywords and information, making it easy to understand."}, {"title": "C.6 Final Answer", "content": "After going through the generation, evaluation, and revision steps, the final answer to the question \"What are points on a mortgage?\" is:"}, {"title": "C.7 BERT and ROUGE Scores", "content": "To further evaluate the quality of the answer, we compute BERT and ROUGE scores:"}, {"title": "D Prompt Design, Workflow and Revision Examples for Evaluating the Camera Dataset", "content": "In this section, we introduce our multi-LLM agent framework, a versatile and generalizable design for generating, evaluating, and refining ad text in various contexts. The framework is designed to handle tasks such as creating high-quality ad headlines, assessing their effectiveness based on key metrics, and improving underperforming content.\nRather than being tailored to a specific dataset or domain, our framework adopts a modular structure where each agent is assigned a well-defined role within the pipeline. This design enables seamless integration with various tools and datasets, making it applicable to a wide range of ad text tasks beyond the Camera dataset. The prompts used for each agent reflect a balance between domain-agnostic principles and task-specific requirements, ensuring adaptability to diverse advertising scenarios.\nThe following sections provide the prompts used to define the roles of the agents within the framework."}, {"title": "D.1 Japanese Ad Headlines Generator", "content": "This agent generates high-quality Japanese ad headlines that are fluent, faithful, and attractive. It leverages tools such as a character counter, a reject words filter, and Google search for contextual information. The specific prompt for this agent is:"}, {"title": "D.2 Ad Headlines Evaluator", "content": "This agent evaluates the generated headlines based on three metrics: Faithfulness, Fluency, and Attractive-ness. The specific prompt for this agent is:"}, {"title": "D.3 Ad Headlines Reviser", "content": "This agent revises low-scoring headlines to improve their Faithfulness, Fluency, and Attractiveness scores. The specific prompt for this agent is:"}, {"title": "D.4 Tools Used in the Camera Ad Text Experiment", "content": "To facilitate the generation, evaluation, and refinement of ad text for the Camera dataset, we implemented a set of specialized tools. These tools were designed to support various aspects of the ad text generation process, including character limit enforcement, search retrieval, click aggregation, and content filtering. Below is a description of each tool:\n\u2022 Character Counter (Generator and Revisor): A utility for counting the number of characters in a given sentence. It takes as input a list of lists in the form [[sentence, character limit], [sentence, character limit], ...], where each sentence is checked against a predefined character limit.\n\u2022 Google Search (Generator): A search engine tool used to retrieve real-time information from the web. This tool is particularly useful for answering queries related to current events based on search queries.\n\u2022 Output Tool (All Agents): A simple logging tool that allows agents to write their thoughts. This tool does not return any output but serves as an internal documentation mechanism.\n\u2022 Bad Performance Retriever (Revisor): A quality control tool that checks whether generated headlines or descriptions resemble undesirable outputs. It takes as input a dictionary in the form {\"Headline\": [headline1, ...], \"Description\": [description1, ...]} and returns a list of flagged items if any match known bad examples.\n\u2022 Reject Word Checker (Generator and Revisor): A filtering tool that verifies whether a sentence contains prohibited words. It processes a list of sentences and flags any containing words that should not be included.\nThese tools collectively enable structured ad text generation by enforcing constraints, retrieving relevant information, filtering out undesired outputs, and aggregating performance metrics. Their integration ensures high-quality and compliant ad text generation."}, {"title": "D.5 Ad Headline Revisions with Highlights", "content": "Tables 7 and 8 present two cases of translated ad headline revisions: one for educational ads and the other for employment-related ads. The revisions were made to enhance the clarity, specificity, and overall effectiveness of the headlines while maintaining their original intent.\nIn these tables, text highlighted in green represents a good revision, where improvements were made to make the ad more engaging, informative, or persuasive. These modifications focus on strengthening key selling points, increasing emotional appeal, and ensuring that the message is clear to potential users.\nFor instance, in Table 7, the phrase \"Challenge prestigious school entrance exams\" was revised to \"Support your challenge to enter prestigious schools\" to emphasize the supportive nature of the service rather than just the difficulty of the exams. Similarly, in Table 8, the phrase \"Get a job with Baitoru"}, {"title": "D.6 An example of Hierarchical Refinement with Faithfulness, Fluency, Attractiveness", "content": "TalkHier employs a hierarchical refinement process where evaluators independently assess content (faithfulness, fluency, and attractiveness) and report their findings to an evaluation team supervisor. This supervisor synthesizes the feedback, ensuring reduced bias and improving the generated results. Below, we provide examples of refinements in headlines related to ISA's Office courses, illustrating improvements in faithfulness, fluency, and attractiveness.\nFaithfulness Refinement: Initial headline:\nFastest qualification with ISA courses.\nThis headline lacked specificity and could mislead users. After refinement:\nAchieve qualification in two weeks with ISA courses.\nThis correction provides an accurate depiction of the course duration.\nFluency Refinement: Initial headline:\nISA courses: beginner friendly.\nWhile understandable, the phrase was somewhat unnatural. After refinement:\nBeginner-friendly ISA courses."}, {"title": "Attractiveness Refinement:", "content": "Attractiveness Refinement: Initial headline:\nBoost skills with ISA Office courses.\nThis headline, though factual, lacked emotional appeal. After refinement:\nAdvance your career with ISA Office courses.\nThis modification creates a more engaging and motivational message for potential users."}, {"title": "E Subjective Experiment for the Rating in TalkHier", "content": "In this section, we describe our experimental setup for evaluating the quality of automatically generated advertisement headlines. Our proposed method, TalkHier, is a multi-agent system designed to refine generated text by iteratively assessing and improving headlines across three key dimensions: attrac-tiveness, fluency, and faithfulness. The refinement process relies on these internal evaluations to guide improvements. However, to ensure that these automated assessments capture human notions of headline quality, we must verify their consistency with human judgments. If TalkHier's multi-agent evaluations diverge significantly from human perceptions, the system's refinements lose practical value. We therefore compare TalkHier against a baseline, generating headlines using both methods. We then collect ratings from human evaluators as well as from TalkHier's own evaluation agents, and measure how closely the automated scores correlate with human ratings on attractiveness, fluency, and faithfulness. Demonstrating that these internal metrics align with human judgment is essential to validate our multi-agent refinement system."}, {"title": "E.1 Setup and Data Collection", "content": "We selected five distinct products, each of which serves as a target for generating advertisement headlines. For each product, we generated five headlines using TalkHier (for a total of 25) and five headlines using the baseline model (another 25), thus obtaining 50 headlines in total.\nAll headlines were evaluated by four human raters using a five-point scale (1 = \"very poor\" to 5 = \"excellent\"). We also prompted GPT to rate each of these 50 headlines on the same 1\u20135 scale, effectively treating GPT as a fifth rater."}, {"title": "E.2 Data Example", "content": "Table 9 provides a small subset of our dataset to illustrate how the information is organized. Each row corresponds to one generated headline and includes (i) the product name or headline identifier, (ii) the method that generated it, (iii) the generated text, and (iv) the ratings assigned by a subset of the human evaluators and TalkHier."}, {"title": "E.3 Evaluation Metrics", "content": "To determine whether TalkHier evaluates headlines similarly to human raters, we compute both (i) the correlation (Pearson and Spearman) between TalkHier's ratings and the average human ratings, and (ii) the Intraclass Correlation Coefficient (ICC), treating TalkHier as an additional rater alongside the four humans. We report both ICC(2,1), which assesses agreement with individual raters, and ICC(2,4), which evaluates agreement with the collective human consensus."}, {"title": "E.4 Evaluation Results", "content": "We quantitatively assessed how closely TalkHier's ratings align with the human evaluations using both (i) correlations (Pearson and Spearman) between TalkHier's ratings and the average ratings of the four human evaluators, and (ii) the Intraclass Correlation Coefficient (ICC) treating TalkHier as an additional rater. Table 10 summarizes our main findings."}]}