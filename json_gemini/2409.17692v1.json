{"title": "MIO: A Foundation Model on Multimodal Tokens", "authors": ["Zekun Wang", "King Zhu", "Chunpu Xu", "Wangchunshu Zhou", "Jiaheng Liu", "Yibo Zhang", "Jiashuo Wang", "Ning Shi", "Siyu Li", "Yizhi Li", "Haoran Que", "Zhaoxiang Zhang", "Yuanxing Zhang", "Ge Zhang", "Ke Xu", "Jie Fu", "Wenhao Huang"], "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and videos\nin an end-to-end, autoregressive manner. While the emergence of large language\nmodels (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile capabilities,\nthey still lack true any-to-any understanding and generation. Recently, the release of\nGPT-40 has showcased the remarkable potential of any-to-any LLMs for complex\nreal-world tasks, enabling omnidirectional input and output across images, speech,\nand text. However, it is closed-source and does not support the generation of\nmultimodal interleaved sequences. To address this gap, we present MIO, which\nis trained on a mixture of discrete tokens across four modalities using causal\nmultimodal modeling. MIO undergoes a four-stage training process: (1) alignment\npre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and\n(4) comprehensive supervised fine-tuning on diverse textual, visual, and speech\ntasks. Our experimental results indicate that MIO exhibits competitive, and in\nsome cases superior, performance compared to previous dual-modal baselines,\nany-to-any model baselines, and even modality-specific baselines. Moreover,\nMIO demonstrates advanced capabilities inherent to its any-to-any feature, such\nas interleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Language Models (LLMs) is commonly considered the dawn of artificial\ngeneral intelligence (AGI) [1, 2], given their generalist capabilities such as complex reasoning [3],\nrole playing [4], and creative writing [5]. However, original LLMs lack multimodal understanding\ncapabilities. Consequently, numerous multimodal LLMs (MM-LLMs) have been proposed, allowing\nLLMs to understand images [6, 7], audio [8-11], and other modalities [12-14]. These MM-LLMs\ntypically involve an external multimodal encoder, such as EVA-CLIP [15] or CLAP [16], with an\nalignment module such as Q-Former [6] or MLP [17] for multimodal understanding. These modules\nalign non-textual-modality data features into the embedding space of the LLM backbone.\nAnother line of work involves building any-to-any and end-to-end MM-LLMs that can input and\noutput non-textual modality data. Typically, there are four approaches: (1) Discrete-In-Discrete-Out (DIDO): Non-textual modality data is discretized using vector quantization techniques [26,\n27] and then fed into LLMs [20, 21, 28]. (2) Continuous-In-Discrete-Out (CIDO): The LLM\nbackbones intake densely encoded non-textual modality data features and generate their quantized\nrepresentations [29, 30]. (3) Continuous-In-Continuous-Out (CICO): The LLMs both understand\nand generate non-textual modality data in their densely encoded representations [18, 19, 31\u201333]. (4)\nAutoregression + Diffusion (AR + Diff): The autoregressive and diffusion modeling are integrated in\na unified LLM [25, 34, 35]. Although these works have succeeded in building MM-LLMs unifying\nunderstanding and generation, they exhibit some drawbacks, as illustrated in Table 1. For example,\nEmu1 [18] and Emu2 [19] explore the autoregressive modeling of three modalities: text, images, and\nvideos. SEED-LLaMA [20] proposes a new image quantizer aligned with LLMs' embedding space\nand trains the MM-LLMs on images and videos. However, neither considers the speech modality,\nwhich is heterogeneous from visual modalities like videos and images. Although AnyGPT [21]\nhas explored settings involving four modalities, including text, image, speech, and music, it lacks\nvideo-related abilities, voice synthesis, and comprehensive multi-task supervised fine-tuning, leading\nto limited multimodal instruction-following and reasoning capabilities. Furthermore, AR + Diff\napproaches, such as Transfusion [25], suffer from limited multimodal understanding capabilities\nbecause the multimodal inputs are noised for denoising modeling, and the image tokenizer used is\nsuitable for generation rather than understanding.\nMoreover, most of current MM-LLMs are typically dual-modal, combining text with another modality,\nsuch as images. Although previous works, such as Meta-Transformer [13] and Unified-IO 2 [36],\nhave explored omni-multimodal understanding settings with more than two non-textual modalities,\nthey still lag significantly behind their dual-modal counterparts, especially in terms of multimodal\ninstruction-following capabilities. Moreover, these MM-LLMs are typically focused on understanding\nonly, neglecting the important aspect of multimodal generation. Several works have enabled LLMs\nto call external tools to address this issue. For example, HuggingGPT [37] generates textual image\ndescriptions for external diffusion models to synthesize images. GPT-4 [1] can utilize either an image\ngenerator like DALL-E 3 [38] or a text-to-speech (TTS) tool like Whisper [39] to support multimodal\ngeneration.2 However, these methods are not end-to-end, relying on the text modality as an interface.\nRecently, the release of GPT-40 has demonstrated the capabilities of any-to-any and end-to-end\nfoundation models. It is the first foundational model to accept multimodal tokens as inputs and\ngenerate multimodal tokens within a unified model while also demonstrating strong abilities in\ncomplex multimodal instruction-following, reasoning, planning, and other generalist capabilities.\nFurthermore, as the continuous scaling up of LLMs in the community depletes high-quality language\ntokens, GPT-4o verifies a new source of data for LLM training: multimodal tokens. This approach\nsuggests that the next generation AGI could derive more knowledge from multimodal tokens when\nlanguage tokens are exhausted. However, GPT-40 is closed source and focuses primarily on end-to-end support for speech I/O, image I/O, 3D generation, and video understanding. Its recent open-source"}, {"title": "2 Method", "content": "Firstly, we elaborate on our modeling approach, which supports multimodal token input and output,\nas well as causal language modeling (CausalLM), in \u00a72.1. Secondly, we describe our three-stage pre-training procedures in \u00a72.2. Thirdly, we provide details of our comprehensive supervised fine-tuning\non diverse multimodal understanding and generation tasks in \u00a72.3.\n2.1 Modeling\nAs illustrated in Figure 1, the framework of MIO involves three parts: (1) multimodal tokenization,\n(2) causal multimodal modeling, and (3) multimodal de-tokenization.\nMultimodal Tokenization. In our work, we use SEED-Tokenizer [41] as our image tokenizer and\nSpeechTokenizer [42] as our speech tokenizer. SEED-Tokenizer encodes images using a ViT [43]\nderived from BLIP-2 [6], and then converts the encoded features into fewer tokens with causal\nsemantics via Q-Former [6]. These features are subsequently quantized into discrete tokens that\nare well-aligned with the language model backbone's textual space. The codebook size for these\ndiscrete image tokens is 8192. SEED-Tokenizer transforms each image into a 224x224 resolution"}, {"title": "Causal Multimodal Modeling", "content": "As illustrated in Figure 1, the speech and images, including video\nframes, are tokenized by SpeechTokenizer [42] and SEED-Tokenizer [41], respectively. We add\nthe 4096 speech tokens and 8192 image tokens to the LLM's vocabulary. In addition, we introduce\nfour new special tokens, namely <IMAGE>, </IMAGE>, <SPCH>, and </SPCH>, to the vocabulary.\nConsequently, the embedding layer of the LLM backbone and the language modeling head are\nextended by 4096+8192+4 = 12292 to support the embedding and generation of these new tokens.\nThe image tokens contain causal semantics due to the use of a Causal Q-Former [41], and the speech\ntokens are intrinsically causal due to their temporal nature. Therefore, these multimodal tokens are as\nsuitable for autoregressive training as textual tokens, allowing us to unify the training objectives for\nunderstanding and generation of multimodal tokens into next-token-prediction with cross-entropy\nloss. The training objective is thus:\n$\\mathcal{L} = -\\sum_{t=1}^{T} log P(x_t | X_{<t}; \\theta)$\nwhere $x_t$ represents the discrete multimodal tokens, and $\\theta$ denotes the parameters of the LLM\nbackbone. We use the pre-trained model, Yi-6B-Base [47], for initialization.\nFurthermore, to eliminate the computational inefficiency caused by <PAD> tokens, we use the masked\npacking strategy [36, 28, 48]. Specifically, the samples are concatenated along the sequence length\ndimension until the context window is full. Then, we construct the causal attention mask for the\ntokens of each sample and mask out all the tokens of the other samples."}, {"title": "Multimodal De-Tokenization", "content": "After the generation of multimodal tokens, it is essential to use\nmodality-specific decoders to reconstruct the images or speech from the codes. Specifically, for image\ntokens, we directly utilize SEED-Tokenizer's decoder, which involves an MLP projection to convert\nthe discrete codes into dense latents. These latents condition an off-the-shelf diffusion model [49] to\ngenerate the images in the pixel space [41]. The vanilla SpeechTokenizer [42] involves generating\ntimbre tokens through a non-autoregressive model outside the language model, and then feeding the\nconcatenated content and timbre tokens into the SpeechTokenizer decoder to synthesize speech. In\nour work, to inject the timbre priors into the multimodal language model itself, the timbre tokens are\nalso generated by the autoregressive language model."}, {"title": "2.2 Pre-Training", "content": "As shown in Table 2, we use a three-stage strategy for pre-training, with each stage targeting different\nobjectives. The three stages are: (1) Alignment Pre-training: This stage focuses on learning a\nmultimodal representation more aligned with the language space. (2) Interleaved Pre-training: This\nstage aims to obtain a multimodal representation with richer contextual semantics. (3) Speech-enhanced Pre-Training: This stage specifically enhances the model's speech-related capabilities, while\nconcurrently replaying data from other modalities. For more details on the pre-training data and its\nprocessing procedures, we refer the readers to Appendix B."}, {"title": "Stage I: Alignment Pre-Training", "content": "To fully leverage the superior capabilities of the pre-trained\nLLM backbone, it is essential to align the non-textual modality data representations with text. There\nare two types of pre-training data for image-text multimodal learning: (1) Image-text paired data: This\ndata has well-aligned dependencies between images and text. (2) Image-text interleaved data: This\ndata features more natural and contextual dependencies but is less aligned. Note that in our setting,\nvideo-text paired and interleaved data can be treated as image-text interleaved data, with videos\nbeing sequential images interleaved with text. Therefore, in this stage, we exclude the image-text\ninterleaved data and video data to ensure the most aligned pattern between images and text."}, {"title": "Stage II: Interleaved Pre-Training", "content": "In this stage, we extend the data used for pre-training to include\nimage-text interleaved data (including video-text data) as a novel image-text dependency pattern. The\nimage-text interleaving pattern has a different nature compared to pairing patterns. Although [6] and\n[18] argued that interleaved image-text data mainly serves for multimodal in-context learning, we\nargue that it is also essential for context-aware image generation where images are generated based\non specific context, rather than a precise description of the image content. For example, in image-text\ninterleaved data, the text might serve as the image's preceding or continuing context, rather than"}, {"title": "Stage III: Speech-Enhanced Pre-Training", "content": "The speech tokenizer that we use generates 200 tokens\nfor each second of audio. Given that the duration of a speech sample can be 15 seconds, this results\nin around 3,000 tokens per sample. In comparison, the image tokenizer produces only 32 tokens\nper image. This creates a significant disparity in the number of tokens among different modalities.\nConsequently, our training data is dominated by speech tokens. If we mix all the different modalities\naccording to their original proportions for training, the model would likely become overly focused on\nspeech, at the expense of other modalities.\nTo address this issue, we implement a three-stage strategy that gradually increases the proportion of\nspeech tokens. In Stage I, speech-text data accounts for 12.5% of the training tokens, which rises to\n37.5% in Stage II, and finally reaches 75.0% in Stage III. This incremental increase in the proportion\nof speech tokens ensures that the model's performance in non-speech modalities is not compromised\nby the speech modality, while also allowing for the optimization of the model's speech capabilities.\nFurthermore, we keep the data mixing ratio for other modalities of pre-training data at the minimal\nessential scales for replay, and we only use the high-quality subsets of them in this stage. This stage\nrequires significantly fewer compute resources, due to the foundation laid in the previous stages."}, {"title": "2.3 Supervised Fine-Tuning", "content": "As shown in Table 7, our model undergoes comprehensive and systematic supervised fine-tuning\n(SFT) with 16 different tasks and 34 diverse open-source datasets. The chat template used for SFT is\nthe same as that used for Yi-6B-Chat [47], and only the assistant responses are supervised. We refer\nthe reader to Appendix D for more details about the hyperparameters and prompt templates."}, {"title": "3 Experiments", "content": "In this section, we present our quantitative evaluation results across various domains: image-related\ntasks (\u00a73.1), speech-related tasks (\u00a73.2), and video-related tasks (\u00a73.3). Due to the lack of benchmarks\nfor several advanced and emergent abilities of any-to-any multimodal LLMs, we also provide\nnumerous qualitative demonstrations (\u00a73.4) to demonstrate these capabilities. We refer the reader to\nAppendix E for more details, including the decoding hyperparameters and prompt templates."}, {"title": "3.1 Image-Related Tasks", "content": "Image Understanding. We compare our models with Emu [18], SEED-LLaMA [20], AnyGPT [21],\nFlamingo [7], Kosmos-1 [51], MetaLM [52], IDEFICS [53], CM3Leon [22], and InstructBLIP [54].\nWe evaluate our models in diverse tasks, including: (1) image captioning on MS-COCO [55] Karpathy\ntest split with CIDEr score [56] as the metric, (2) three visual question-answering benchmarks, i.e.,\nVQAv2 [57] (test-dev split), OK-VQA [58] (val split), and VizWiz [59], with VQA accuracy as\nthe metric, and (3) SEED-Bench [60], a comprehensive visual question-answering benchmark\nincluding 9 dimensions with MCQ accuracy as the metric. The scores for all baselines are copied\nfrom their reports. As shown in Table 3, our MIO-Instruct is ranked in the top group among all\nbaselines, demonstrating its competitive image understanding performance. Although SEED-LLaMA"}, {"title": "3.2 Speech-Related Tasks", "content": "We evaluate the speech understanding and gen-\neration abilities of MIO on ASR and TTS\ntasks. Wav2vec 2.0 [64], Whisper Large\nV2 [65], and AnyGPT [21] are the baselines\nfor ASR tasks, while VALL-E [66], USLM [42]\n, and AnyGPT [21] are the baselines for TTS\ntasks. The test set used for ASR evaluation\nis LibriSpeech [67], while the test set used\nfor TTS evaluation is VCTK [68] following\nAnyGPT [21]'s practice. The Whisper medium\nmodel is used to transcribe the speech generated for the TTS task. The WER (word error rate) is\ncomputed by comparing the generated transcribed text with the ground-truth transcription after text\nnormalization. As shown in Table 3.2, our models exhibit speech performance comparable to the\nspeech-specific baselines and outperform the AnyGPT baseline. It is important to note that although\nAnyGPT is capable of generating content tokens for speech, it lacks the ability to generate timbre"}, {"title": "3.3 Video-Related Tasks", "content": "We compare MIO with Flamingo [7], BLIP-\n2 [6], InstructBLIP [54], Emu [18], and SEED-\nLLAMA [20] for video understanding. The mod-\nels are evaluated on the MSVDQA [69] and\nMSRVTT-QA [70]. The results are presented in\nTable 6. Our model achieves the highest scores\ncompared to all baselines. Due to the lack of\nvideo (frame sequence) generation benchmarks\nin our setting, we provide video generation ex-\namples in \u00a73.4. These results demonstrate the su-\nperior performance of our models in both video\nunderstanding and video generation."}, {"title": "3.4 Demonstrations", "content": "We illustrate the basic and advanced abilities of MIO in Figure 2 and 3. The basic abilities of MIO\ninvolve image understanding and generation, video understanding and generation, ASR, and TTS.\nThe advanced abilities of MIO are based on its any-to-any and multimodal interleaved sequence\ngeneration features. These abilities involve visual storytelling (i.e., interleaved video-text generation),\nchain of visual thought, speech-in-speech-out, instructional image editing, visual guideline generation,\netc. We refer the readers to Appendix F for more demonstrations including multimodal chain of\nthought and multimodal in-context learning."}, {"title": "4 Related Works", "content": "4.1 Multimodal LLMS\nWith the rapid success of Large Language Models (LLMs), current multimodal LLMs (MM-LLMs)\nare typically built on a pre-trained LLM backbone and are endowed with the ability to understand\nmultiple modalities [71\u201377]. Generally, these MM-LLMs align the representations of images obtained\nfrom visual encoders with the text embedding space, thereby leveraging the powerful capabilities of\nthe foundational models. For example, BLIP-2 [6] uses CLIP-ViT [63] to extract high-level features\nfrom images and then employs a Q-Former to compress the number of image tokens and further\nalign image tokens with the LLM embeddings. In contrast, LLaVA [17, 78] utilizes a simple linear\nprojection or MLP as the connector between the image encoder and the LLM backbone. These\nmodels demonstrate strong multimodal understanding abilities, achieving significant progress in tasks\nsuch as visual question answering, visual commonsense reasoning, chart understanding, etc.\nAdditionally, beyond images, other MM-LLMs have also focused on modalities such as speech and\nvideo. For instance, LLaSM [79] and InternVideo [80, 81] are MM-LLMs designed for speech and\nvideo understanding, respectively. These models adopt a similar architectural design to BLIP-2 or\nLLaVA but redesign modality-specific encoders tailored for speech and video.\nRecently, increasing attention has been given to unifying multiple modalities within a single MM-LLM. For example, ImageBind [82] develops encoders suited for multiple modalities such as images,\nvideos, audio, heat maps, among others, while OmniBind [83] trains an omni-representation model\nby aligning encoders across four modalities: audio, language, images, and 3D objects.\nHowever, these models focus primarily on multimodal understanding and often overlook the important\naspect of multimodal generation."}, {"title": "4.2 Any-to-Any MM-LLMs", "content": "To enable multimodal generation in MM-LLMs, a straightforward approach is to allow these models\nto call external multimodal generation tools, such as Stable Diffusion [49] or text-to-speech (TTS)\ntools [37, 84, 1]. However, as highlighted in the Gemini technical report [30], relying on an\nintermediate natural language interface can limit the model's ability to express images. If a model\ncannot natively output images, it will not be able to generate images with prompts of interleaved\nsequences of image and text. This claim is in line with our distinction between descriptive image\ngeneration and context-aware image generation, as discussed in \u00a72.2.\nAs a result, recent works focus on the unification of multimodal understanding and generation in a\nsingle model (i.e., any-to-any MM-LLMs), enabling the generation of multimodal tokens without\nnatural language as an interface. These models typically follow different approaches, depending on\nhow images are represented in both input and output sides. For example, the Discrete-In-Discrete-Out\n(DIDO) approach has been explored in works such as SEED-LLaMA [20], AnyGPT [21], and\nChameleon [23]. Continuous-In-Discrete-Out (CIDO) methods have been implemented in models\nlike DaVinCi [29], Gemini [30], and Unified-IO 2 [36]. The Continuous-In-Continuous-Out (CICO)\napproach is used in models such as Emu [18, 19], DreamLLM [31], and MiniGPT-5 [32]. Another\napproach, the integration of autoregression and diffusion (AR + Diff), can be seen in models like\nTransfusion [25], Show-o [34], and [35]'s.\nHowever, these models face specific limitations. DreamLLM (CICI, [31]) and CIDO models suffer\nfrom inconsistencies between input and output forms for multimodal data, making it difficult for them\nto natively support the generation of interleaved multimodal sequences where an image functions in\na coupled way as both input and output. Emu2 (CICO, [19]) struggles with the challenges of the\nmean square error (MSE) loss used for training continuous output representations, as well as with\nthe uni-modal assumption of the Gaussian distribution in the MSE loss. Transfusion (AR + Diff,\n[25]) applies noise to images from the input side to support multimodal generation with diffusion\nmodeling, and relies on VAE [85] features rather than CLIP [63] features for denoising, which largely\ntrade off the multimodal understanding abilities.\nTo mitigate these issues, we adopt the DIDO approach in our work, despite the information loss\ncaused by vector quantization. A comprehensive comparison of our models with other any-to-any\nMM-LLMs is presented in Table 1."}, {"title": "5 Conclusion", "content": "In conclusion, MIO represents an advancement in the realm of multimodal foundation models. By\nemploying a rigorous four-stage training process, MIO successfully integrates and aligns discrete\ntokens across text, image, video, and speech modalities. This comprehensive approach enables MIO\nto understand and generate multimodal content in an end-to-end, autoregressive manner, addressing\nthe limitations of current multimodal large language models. Our experimental results showcase its\ncompetitive performance across a variety of benchmarks compared to the dual-modality baselines and\nother any-to-any multimodal large language models. With the any-to-any and multimodal interleaved\noutput features, MIO exhibits novel emergent abilities such as interleaved video-text generation,\nchain-of-visual-thought reasoning, etc."}, {"title": "A Limitations", "content": "Our work has several limitations. First, due to the information loss caused by vector quantization\nprocedures, our model is not applicable for OCR-related images, or other images with highly detailed\nfeatures. This is also a vital issue faced by CM3Leon [22] and Chameleon [86]. Second, due to\nthe lack of voice-controlled speech data, our model cannot control the generated timbre yet. Third,\nalthough our model can generate frame sequences as video representation, it cannot generate the raw\nvideo data which is in the continuous form. We leave these issues as the research questions for the\nfuture work."}, {"title": "B Pre-training Data", "content": "Pre-training Data Sources. The pre-training data sources involve six types:\nImage-text paired data: SBU [87], CC3M [88], LAION-COCO [89], and JourneyDB [90],\nwhere JourneyDB only serves for image generation.\nLanguage-only data: RefinedWeb [91].\nImage-text interleaved data: OBELICS [53], MMC4-core-ff [92].\nVideo-text paired data: WebVid-10M [93].\nVideo-text interleaved data: HowTo-100M [94], Youtube-Temporal-180M [95].\nSpeech-text paired data: Libriheavy [96].\nPre-training Data Processing. We have different data processing procedures for different data\ntypes illustrated in \u00a7B following Emu [18] and Qwen-VL [97]:\nImage-text paired data: we remove pairs with more than 2:1 aspect ratio or smaller than\n224 \u00d7 224 resolution of the image. We remove pairs with more than 0.27 CLIP scores.\nWe remove non-English pairs. We randomly place the image or text at the forefront for\ngenerating captions based on images and vice versa.\nLanguage-only data: we use the same data processing pipeline as used in Yi [47].\nImage-text interleaved data: we filter the data using a CLIP score threshold of 0.25, and\nfollow the same procedure as illustrated in Emu [18].\nVideo-text paired data: we randomly place the frames or text at the forefront for generating\ncaptions based on frames and vice versa. 60% of the pairs are text-to-video, while 40% of\nthe pairs are video-to-text. We sample 4 to 8 frames of each video for training according to\nthe text lengths.\nVideo-text interleaved data: We first use PySceneDetect to extract key frames from the\nvideo based on scene changes, following the practice of Stable Video Diffusion [98]. Then,\nfor each video clip between two key frames, we extract a central frame for textual caption\ngeneration with BLIP-2 [6]. Additionally, the video clips between key frames are processed\nusing ASR (automatic speech recognition) tools to extract subtitles. The ASR text and\ncaptions are then integrated and refined using Yi-34B-Chat [47], resulting in a single text\nsegment. These text segments, along with the key frames and central frames, form the\nvideo-text interleaved data.\nSpeech-text paired data: we remove speechs with more than 15 seconds."}, {"title": "C Pre-training Details", "content": "Hyperparameters. We enable Flash Attention [99, 100] during pre-training. Gradient clipping is\nset to 1.0 for all stages. The maximum sequence length for training is 2800 tokens. We use a cosine\nlearning rate scheduler with a peak learning rate of 3e-5 and a warmup ratio of 0.03. The optimizer\nused is AdamW [101]."}, {"title": "D Supervised Fine-Tuning Details", "content": "Supervised Fine-Tuning Data. As shown in Table 7, we use 16 tasks with 34 datasets for a\ncomprehensive supervised fine-tuning.\nPrompt Templates. The chat template is the same as used in Yi [47]. The system prompt is unified\nas: \"You are MIO, an AI assistant capable of understanding and generating images, text, videos, and\nspeech, selecting the appropriate modality according to the context.\" except for speech generation\nand TTS whose system prompts are \"You are MIO, an AI assistant capable of understanding images,\ntext, videos, and speech, and generating speech. Please respond to the user with speech only, starting\nwith <spch> and ending with </spch>.\" to avoid randomness of the output modality.\nHyperparameters. Similar to pre-training (c.f., Appendix C), we enable Flash Attention [99, 100]\nduring supervised fine-tuning. Gradient clipping is set to 1.0. The maximum sequence length for\ntraining is 2800 tokens. We use a cosine learning rate scheduler with a peak learning rate of 3e-5 and\na warmup ratio of 0.03. The optimizer used is AdamW [101]."}, {"title": "E Evaluation Details", "content": "Hyperparameters. The decoding strategies and hyperparameters are quite important for a superior\nperformance. As shown in Table 8, we use different sets of parameters for different output modalities.\nPrompt Templates. The prompt templates used for evaluating pre-training checkpoints are the\nsame as used during pre-training. For SFT checkpoint evaluation, we list the prompt templates in\nTable 9."}, {"title": "F More Demonstrations", "content": null}]}