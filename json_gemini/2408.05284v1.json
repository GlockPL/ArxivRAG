{"title": "Can a Bayesian Oracle Prevent Harm from an Agent?", "authors": ["Yoshua Bengio", "Michael K. Cohen", "Nikolay Malkin", "Matt MacDermott", "Damiano Fornasiere", "Pietro Greiner", "Younesse Kaddar"], "abstract": "Is there a way to design powerful AI systems based on machine learning methods\nthat would satisfy probabilistic safety guarantees? With the long-term goal of\nobtaining a probabilistic guarantee that would apply in every context, we consider\nestimating a context-dependent bound on the probability of violating a given safety\nspecification. Such a risk evaluation would need to be performed at run-time\nto provide a guardrail against dangerous actions of an AI. Noting that different\nplausible hypotheses about the world could produce very different outcomes, and\nbecause we do not know which one is right, we derive bounds on the safety violation\nprobability predicted under the true but unknown hypothesis. Such bounds could\nbe used to reject potentially dangerous actions. Our main results involve searching\nfor cautious but plausible hypotheses, obtained by a maximization that involves\nBayesian posteriors over hypotheses. We consider two forms of this result, in the\ni.i.d. case and in the non-i.i.d. case, and conclude with open problems towards\nturning such theoretical results into practical AI guardrails.", "sections": [{"title": "1 Introduction", "content": "Ensuring that an AI system will not misbehave is a challenging open problem [4], particularly in\nthe current context of rapid growth in AI capabilities. Governance measures and evaluation-based\nstrategies have been proposed to mitigate the risk of harm from highly capable AI systems, but\ndo not provide any form of safety guarantee when no undesired behavior is detected. In contrast,\nthe safe-by-design paradigm involves designing AI systems with quantitative (possibly probabilistic)\nsafety guarantees from the ground up, and therefore could represent a stronger form of protection\n[8]. However, how to design such systems remains an open problem too.\nSince testing an AI system for violations of a safety specification in every possible context, e.g.,\nevery (query, output) pair, is impossible, we consider a rejection sampling approach that declines a\ncandidate output or action if it has a probability of violating a given safety specification that is too\nhigh. The question of defining the safety specification (the violation of which is simply referred to\nas \"harm\" below) is important and left to future work, possibly following up approaches such as\nconstitutional AI [1]. We also note that being Bayesian about the interpretation of a human-specified"}, {"title": "2 Safe-by-design AI?", "content": "Before an AI is built and deployed, it is important that the developers have high assurances that\nthe AI will behave well. Dalrymple et al. [8] propose an approach to \"guaranteed safe AI\u201d designs\nwith built-in high-assurance quantitative safety guarantees, although these guarantees can sometimes\nbe probabilistic and only asymptotic. It remains an open question whether and how that research\nprogram can be realized. The authors take existing examples of quantitative guarantees in safety-\ncritical systems and motivate why such a framework should be adopted if we ever build AI systems\nthat match or exceed human cognitive abilities and could potentially act in dangerous ways. Their\nprogram is motivated by current known limitations of state-of-the-art AI systems based on deep\nlearning, including the challenge of engineering AI systems that robustly act as intended [7, 22, 27,\n28, 44, 33, 34, 19, 32].\nThe approach proposed by [8] has the following components: a world model (which can be a distribu-\ntion about hypotheses explaining the data), a safety specification (what are considered unacceptable\nstates of the world), and a verifier (a computable procedure that checks whether a policy or action\nviolates the safety specification).\nHere, we study elements that would go into such a safe-by-design AI system. We assume that the\nsystem infers a probabilistic world model, or theory, \\tau and updates its estimate of r, via machine\nlearning, using the stream of observed data D. The observations D are assumed to come from a"}, {"title": "3 I.I.D. data", "content": "Following the notation introduced in the previous section, here we consider the easier-to-analyze case\nwhere the observed examples D = {Z_1, Z_2, . . ., Z_n} are sampled i.i.d. from the unknown distribution"}, {"title": "4 Non-I.I.D. data", "content": "In this section, we remove the assumption made in Section 3 that observations Z_i are i.i.d. given a\ntheory \\tau^*.\nSetting. As before, let (Z, \\mathcal{F}, \\mu) be a \\sigma-finite Borel measure space. For the results below to hold,\nwe must also assume that (Z, \\mathcal{F}) is a Radon space (e.g., any countable set or manifold), so as to\nsatisfy the conditions of the disintegration theorem.\nLet (Z^{\\infty}, \\mathcal{F}^{\\infty}, \\mu^{\\infty}) be the space of infinite sequences of observations, Z^{\\infty} = {(Z_1, Z_2, ...): Z_i \\in\nZ)}, with the associated product \\sigma-algebra and \\sigma-finite measure. This object is the projective\nlimit of the measure spaces (Z^t, \\mathcal{F}^{\\otimes t}, \\mu^{\\otimes t}), where Z^t = {(z_1, ..., z_t) : Z_i \\in Z} and the projection\nZ^{t+1} \\rightarrow Z^t `forgets' the observation Z_{t+1}.\nA theory \\tau is a probability distribution on (Z^{\\infty}, \\mathcal{F}^{\\infty}) that is absolutely continuous w.r.t. \\mu^{\\infty}. For\nA \\in \\mathcal{F}^{\\otimes t}, we write \\tau_{1:t}(A) for the measure of the cylindrical set, \\tau(A \\times Z \\times Z \\times ...), so \\tau_{1:t}\nis a measure on (Z^t, \\mathcal{F}^{\\otimes t}). Because \\mathcal{F}^{\\infty} is generated by cylindrical sets, the absolute continuity\ncondition on \\tau is equivalent to absolute continuity of \\tau_{1:t} w.r.t. \\mu^{\\otimes t} for all t. This condition allows"}, {"title": "5 Experiments", "content": "Exploding bandit setting. We evaluate the performance of safety guardrails based on Proposi-\ntion 3.4 and Proposition 4.6 in a bandit MDP with 10 arms (actions).\nEach arm a \\in {1, . . ., 10} is represented by a feature vector f_a \\in {0, 1}^d (we take d = 10, but d is not\nnecessarily equal to the number of arms), which is sampled uniformly at random in each iteration of\nthe experiment and known to the agent. The reward distribution of each arm is fixed for the duration\nof each episode and assumed to be of the following form: the reward received after taking action a\nfollows a unit-variance normal distribution, r(a) ~ \\mathcal{N}(f_a \\cdot v^*, 1), where v^* \\in {0, 1}^d is some vector\nsampled uniformly at random at the start of each episode and unknown to the agent.\nTaking any action and observing the reward gives evidence about the identity of v^* and thus about\nthe reward distributions of the other actions. The agent maintains a belief over the vector used to\ncompute the reward, beginning with a uniform prior over {0, 1}^d and updating its posterior with each\nobservation of an action-reward pair.\nWe assume that the agent samples its actions from a Boltzmann policy (with temperature 2) using\nthe expected reward of each action under its posterior given the data seen so far, meaning that the\nposterior over reward vectors fully determines a distribution over observations of action-reward pairs\n(a_t,r_t) observed when following the policy. Thus the set {0, 1}^d can be identified with a multiset\nM of theories about observations of such pairs, where the vector v determines the distribution \\tau\nover action-reward pairs. Inference of v under a uniform prior over {0, 1}^d with evidence collected\non-policy is equivalent to inference of \\tau under a uniform prior over M given data generated by a true\ntheory \\tau^* = \\tau_{v^*}. Note that since the policy changes over time, we are in the non-i.i.d. setting.\nThe bandit comes with a notion of harm: if the reward received at a given timestep exceeds some\nthreshold E, the bandit explodes and the agent dies, terminating the episode. In other words, we\ndefine harm as Y_t = 1[R_t > E], where R_t is the random variable representing the reward received\nwhen taking action a_t. E is set to the highest mean reward of any action (i.e., max_a (f_a \\cdot v^*). The\nmaximum episode length is 25 timesteps.\nSafety guardrails. A guardrail is an algorithm that, given a possible action and context (e.g.,\ncurrent state and history), determines whether taking the action in the context is admissible. A\nguardrail can be used to mask the policy to forbid certain actions, such as those whose estimated\nharm exceeds some threshold C.\nWe compare several guardrails: those constructed from Proposition 3.4 and Proposition 4.6, one that\nmarginalizes across the posterior over \\tau to get the posterior predictive harm probability, and one that\n'cheats' by using the probability of harm under the true theory \\tau^*. We define the four guardrails\nformally below. Recall that Z_{1:t} consists of the observations (i.e., actions taken and rewards received)\nat previous timesteps.\n\\cdot Proposition 3.4 guardrail: rejects an action a_{t+1} if there exists \\tau \\in arg \\text{max}_\\tau P(\\tau | Z_{1:t}) P(Y_{t+1} =\n1 | \\tau, Z_{1:t}, a_{t+1}) with P(Y_{t+1} = 1 | \\tau, Z_{1:t}, a_{t+1}) > C (note that the assumptions of i.i.d. observations\nand distinct theories are not satisfied here).\n\\cdot Proposition 4.6 guardrail: rejects an action a_{t+1} if \\text{max}_{\\tau \\in M_{Z_{1:t}}} P(Y_{t+1} = 1 | Z_{1:t}, \\tau, a_{t+1}) > C.\n\\cdot Posterior predictive guardrail: rejects an action a_{t+1} if P(Y_{t+1} = 1 | Z_{1:t}, a_{t+1}) > C.\n\\cdot Cheating guardrail: rejects an action a_{t+1} if P(Y_{t+1} = 1 | Z_{1:t}, \\tau^*, a_{t+1}) > C (note that this\nguardrail assumes knowledge of the true theory \\tau^*).\nThe guardrail is run at every sampling step, and actions that the guardrail rejects are forbidden to be\nsampled by the agent. If all actions are rejected by the guardrail, the episode terminates.\nResults. shows mean episode rewards and episode deaths under each guardrail across\n10000 episodes, for different values of the rejection threshold C. The cheating guardrail achieves\nzero deaths for sufficiently small C, but for C = 0.1 its death probability is moderately high. The\nposterior predictive guardrail also achieves zero deaths for small C, while for larger C it dies slightly"}, {"title": "6 Conclusion and open problems", "content": "The approach to safety verification proposed here is based on context-dependent run-time verification\nbecause the set of possible inputs for a machine learning system is generally astronomical, while the"}]}