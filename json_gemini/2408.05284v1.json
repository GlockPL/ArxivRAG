{"title": "Can a Bayesian Oracle Prevent Harm from an Agent?", "authors": ["Yoshua Bengio", "Michael K. Cohen", "Nikolay Malkin", "Matt MacDermott", "Damiano Fornasiere", "Pietro Greiner", "Younesse Kaddar*"], "abstract": "Is there a way to design powerful AI systems based on machine learning methods\nthat would satisfy probabilistic safety guarantees? With the long-term goal of\nobtaining a probabilistic guarantee that would apply in every context, we consider\nestimating a context-dependent bound on the probability of violating a given safety\nspecification. Such a risk evaluation would need to be performed at run-time\nto provide a guardrail against dangerous actions of an AI. Noting that different\nplausible hypotheses about the world could produce very different outcomes, and\nbecause we do not know which one is right, we derive bounds on the safety violation\nprobability predicted under the true but unknown hypothesis. Such bounds could\nbe used to reject potentially dangerous actions. Our main results involve searching\nfor cautious but plausible hypotheses, obtained by a maximization that involves\nBayesian posteriors over hypotheses. We consider two forms of this result, in the\ni.i.d. case and in the non-i.i.d. case, and conclude with open problems towards\nturning such theoretical results into practical AI guardrails.", "sections": [{"title": "1 Introduction", "content": "Ensuring that an AI system will not misbehave is a challenging open problem [4], particularly in\nthe current context of rapid growth in AI capabilities. Governance measures and evaluation-based\nstrategies have been proposed to mitigate the risk of harm from highly capable AI systems, but\ndo not provide any form of safety guarantee when no undesired behavior is detected. In contrast,\nthe safe-by-design paradigm involves designing AI systems with quantitative (possibly probabilistic)\nsafety guarantees from the ground up, and therefore could represent a stronger form of protection\n[8]. However, how to design such systems remains an open problem too.\nSince testing an AI system for violations of a safety specification in every possible context, e.g.,\nevery (query, output) pair, is impossible, we consider a rejection sampling approach that declines a\ncandidate output or action if it has a probability of violating a given safety specification that is too\nhigh. The question of defining the safety specification (the violation of which is simply referred to\nas \"harm\" below) is important and left to future work, possibly following up approaches such as\nconstitutional AI [1]. We also note that being Bayesian about the interpretation of a human-specified"}, {"title": "2 Safe-by-design AI?", "content": "Before an AI is built and deployed, it is important that the developers have high assurances that\nthe AI will behave well. Dalrymple et al. [8] propose an approach to \"guaranteed safe AI\u201d designs\nwith built-in high-assurance quantitative safety guarantees, although these guarantees can sometimes\nbe probabilistic and only asymptotic. It remains an open question whether and how that research\nprogram can be realized. The authors take existing examples of quantitative guarantees in safety-\ncritical systems and motivate why such a framework should be adopted if we ever build AI systems\nthat match or exceed human cognitive abilities and could potentially act in dangerous ways. Their\nprogram is motivated by current known limitations of state-of-the-art AI systems based on deep\nlearning, including the challenge of engineering AI systems that robustly act as intended [7, 22, 27,\n28, 44, 33, 34, 19, 32].\nThe approach proposed by [8] has the following components: a world model (which can be a distribu-\ntion about hypotheses explaining the data), a safety specification (what are considered unacceptable\nstates of the world), and a verifier (a computable procedure that checks whether a policy or action\nviolates the safety specification).\nHere, we study elements that would go into such a safe-by-design AI system. We assume that the\nsystem infers a probabilistic world model, or theory, \u03c4 and updates its estimate of r, via machine\nlearning, using the stream of observed data D. The observations D are assumed to come from a"}, {"title": "3 I.I.D. data", "content": "Following the notation introduced in the previous section, here we consider the easier-to-analyze case\nwhere the observed examples D = {z\u2081, Z2, . . ., Zn} are sampled i.i.d. from the unknown distribution"}, {"title": "Definition of posterior as a random variable.", "content": "If P is a prior distribution\u00b2 on M and z \u2208 Z, we\ndefine the posterior to be the distribution with mass function\n$$P(\\tau \\mid z) = \\frac{P(\\tau) P_{\\tau}(z)}{\\sum_{\\tau' \\in M} P(\\tau') P_{\\tau'}(z)} \\propto P(\\tau) P_{\\tau}(z),$$\nassuming the denominator converges and the sum is nonzero. Otherwise, the posterior is considered\nto be undefined. As written, the posterior depends on the choice of density functions P\u03c4, but any\ntwo P\u03c4 that are \u00b5-a.e. equal yield the same posterior for \u00b5-a.e. z.\nFor 21, 22 \u2208 Z, we write P(\u00b7 | Z1, Z2) for the posterior given observation z2 and prior P(. | z\u2081), and\nsimilarly for a longer sequence of observations. It can be checked that P(\u00b7 | Z1,..., Zt) is invariant to\nthe order of 21, . . ., Zt and that it is defined in one order if and only if it is defined in all orders. This\nallows us to unambiguously write P(\u00b7 | D) where D is a finite multiset of observations, and we have\n$$P(\\tau \\mid D) \\propto P(\\tau) \\prod_{z \\in D} P_{\\tau}(z).$$\nLet \u03c4* \u2208 M be the ground truth theory and P(\u00b7) a prior over M. Consider a sequence of i.i.d.\nZ-valued random variables Z\u2081, Z2,... (whose realizations are the observations), where each Zi\nfollows the distribution \u03c4*. For any t \u2208 N, the posterior P(\u00b7 | Z1:t) is then a random variable taking\nvalues in the space of probability mass functions on M.\u00b3"}, {"title": "Harm probability bounds.", "content": "So far we have considered a collection M of distributions over an\nobservation space. Now we show bounds when each theory computes probabilities over some\nadditional variables.\nThe following lemma extends Proposition 3.1(b) to estimates of real-valued functions of the theories\nand observations."}, {"title": "4 Non-I.I.D. data", "content": "In this section, we remove the assumption made in Section 3 that observations Zi are i.i.d. given a\ntheory \u03c4*.\nSetting. As before, let (Z, F, \u00b5) be a o-finite Borel measure space. For the results below to hold,\nwe must also assume that (Z, F) is a Radon space (e.g., any countable set or manifold), so as to\nsatisfy the conditions of the disintegration theorem.\nLet (Z\u221e, F\u221e, \u03bc\u221e) be the space of infinite sequences of observations, Z\u221e = {(Z1, Z2, ...): zi \u2208\nZ)}, with the associated product \u03c3-algebra and \u03c3-finite measure. This object is the projective\nlimit of the measure spaces (Zt, F\u00aet, \u03bc\u00aet), where Zt = {(z1, ..., Z\u2081) : zi \u2208 Z} and the projection\nZt+1 \u2192 Zt \"forgets\" the observation Zt+1.\nA theory \u03c4 is a probability distribution on (Z\u221e, F\u221e) that is absolutely continuous w.r.t. \u03bc\u221e. For\nA \u2208 F\u00aet, we write t1:t(A) for the measure of the cylindrical set, \u03c4(A \u00d7 Z \u00d7 Z \u00d7 ...), so \u03c41:t\nis a measure on (Zt, F\u00aet). Because F\u221e is generated by cylindrical sets, the absolute continuity\ncondition on \u03c4 is equivalent to absolute continuity of \u03c41:t w.r.t. \u03bc\u00aet for all t.\u2074 This condition allows"}, {"title": "Definition of posterior as a random variable.", "content": "Let M be a countable multiset of theories\u2075 and P\na prior distribution on M. We define the posterior to be\n$$P(\\tau \\mid Z_{1:t}) = \\frac{P(\\tau) P_{\\tau}(Z_{1:t})}{\\sum_{\\tau' \\in M} P(\\tau') P_{\\tau'}(Z_{1:t})},$$\nassuming the denominator converges to a positive value.\nConsider a ground truth theory \u03c4* \u2208 M and let Z1:\u221e be the corresponding random variable taking\nvalues in Z\u221e. Similarly to the i.i.d. case, the posterior P(\u00b7 | Z1:t) is a random variable taking values\nin the space of probability mass functions on M.\nFor all results below, we assume that P(\u03c4*) > 0."}, {"title": "5 Experiments", "content": "Exploding bandit setting. We evaluate the performance of safety guardrails based on Proposi-\ntion 3.4 and Proposition 4.6 in a bandit MDP with 10 arms (actions).\nEach arm a \u2208 {1, . . ., 10} is represented by a feature vector fa \u2208 {0, 1}d (we take d = 10, but d is not\nnecessarily equal to the number of arms), which is sampled uniformly at random in each iteration of\nthe experiment and known to the agent. The reward distribution of each arm is fixed for the duration\nof each episode and assumed to be of the following form: the reward received after taking action a\nfollows a unit-variance normal distribution, r(a) ~ N(fa \u00b7 v*, 1), where v* \u2208 {0, 1}d is some vector\nsampled uniformly at random at the start of each episode and unknown to the agent.\nTaking any action and observing the reward gives evidence about the identity of v* and thus about\nthe reward distributions of the other actions. The agent maintains a belief over the vector used to\ncompute the reward, beginning with a uniform prior over {0, 1}d and updating its posterior with each\nobservation of an action-reward pair.\nWe assume that the agent samples its actions from a Boltzmann policy (with temperature 2) using\nthe expected reward of each action under its posterior given the data seen so far, meaning that the\nposterior over reward vectors fully determines a distribution over observations of action-reward pairs\n(at,rt) observed when following the policy. Thus the set {0, 1}d can be identified with a multiset\nM of theories about observations of such pairs, where the vector v determines the distribution \u03c4\nover action-reward pairs7. Inference of v under a uniform prior over {0, 1}d with evidence collected\non-policy is equivalent to inference of t under a uniform prior over M given data generated by a true\ntheory \u03c4* = \u03c4\u03bd*. Note that since the policy changes over time, we are in the non-i.i.d. setting.\nThe bandit comes with a notion of harm: if the reward received at a given timestep exceeds some\nthreshold E, the bandit explodes and the agent dies, terminating the episode. In other words, we\ndefine harm as Yt = 1[Rt > E], where Rt is the random variable representing the reward received\nwhen taking action at. E is set to the highest mean reward of any action (i.e., maxa (fa \u00b7 v*). The\nmaximum episode length is 25 timesteps.\nSafety guardrails. A guardrail is an algorithm that, given a possible action and context (e.g.,\ncurrent state and history), determines whether taking the action in the context is admissible. A\nguardrail can be used to mask the policy to forbid certain actions, such as those whose estimated\nharm exceeds some threshold C.\nWe compare several guardrails: those constructed from Proposition 3.4 and Proposition 4.6, one that\nmarginalizes across the posterior over \u03c4 to get the posterior predictive harm probability, and one that\n'cheats' by using the probability of harm under the true theory \u03c4*. We define the four guardrails\nformally below. Recall that Z1:t consists of the observations (i.e., actions taken and rewards received)\nat previous timesteps.\nResults. Figure 1 shows mean episode rewards and episode deaths under each guardrail across\n10000 episodes, for different values of the rejection threshold C. The cheating guardrail achieves\nzero deaths for sufficiently small C, but for C = 0.1 its death probability is moderately high.\u2078 The\nposterior predictive guardrail also achieves zero deaths for small C, while for larger C it dies slightly"}, {"title": "6 Conclusion and open problems", "content": "The approach to safety verification proposed here is based on context-dependent run-time verification\nbecause the set of possible inputs for a machine learning system is generally astronomical, while the\nsafety of the answer to a specific question is more likely to be amenable to tractable calculations. It\nfocuses on the risk of wrongly interpreting the data, including the safety specification itself (what we\ncalled \"harm\" above) and exploits the fact that as more evidence is gathered (as necessarily happens\nwith i.i.d. data) and when different theories predict different observations, the true interpretation rises\ntowards the maximal value of the Bayesian posterior over interpretations. The bound is tighter with\ni.i.d. data but the i.i.d. assumption is also not realistic, and in the context of safety-critical decisions,\nwe would prefer to err on the side of prudence and fewer assumptions. However, it provides an\ninteresting template to think about variants of this idea in future work.\nThere are many open problems to consider before turning the kinds of bounds introduced above into\nan operational run-time safeguard:\n1. Upper-bounding overcautiousness. Can we ensure that we do not underestimate the probability\nof harm but do not massively overestimate it? Some simple theories consistent with the dataset\n(even an arbitrarily large one) might deem non-harmful actions harmful. Can we bound how\nmuch this harm-avoidance hampers the agent? A plausible approach would be to make use of a\nmentor for the agent that demonstrates non-harmful behavior [5].\n2. Tractability of posterior estimation. How can we efficiently estimate the required Bayesian\nposteriors? For computational tractability, a plausible answer would rely on amortized inference,\nwhich turns the difficult estimation of these posteriors into the task of training a neural net\nprobabilistic estimator which will be fast at run-time. Recent work on amortized Bayesian\ninference for symbolic models, such as causal structures [9, 10], and for intractable posteriors in\nlanguage models [16, 18, 36, 35, 39] \u2013 which are useful when prior knowledge is encoded in a\npretrained foundation model \u2013 suggests that this is feasible. Advances in efficient and adaptive\nMonte Carlo methods, e.g., for language models [29, 42, 23], can also be useful for this purpose,\nand MCMC approaches can complement and aid amortization [17, 21, 31, 20].\n3. Efficient search for a cautious theory. How can we efficiently identify a cautious but plausible\ntheory that upper-bounds the risk, since this requires an optimization at run-time? Again, a\nplausible answer is to rely on amortized probabilistic inference, e.g., by sampling theories with\na low or annealed temperature, a technique that has been used for decades in the nonconvex\noptimization literature and more recently combined with amortized sequential sampling methods\n[40, 41, 43, inter alia].\n4. Partial specification of theories. Another issue is that a full explanatory theory \u03c4 for the\nevidence D will generally be too large an object to make the above bound calculations and\noptimization sufficiently efficient. An alternative would be to derive bounds that only require\npartial explanations, similarly to how humans tell cautious \u2018stories' about what could go wrong\nin uncertain circumstances when thinking about taking a potentially dangerous decision [3].\n5. Harm specification in natural language. How is a safety specification provided in natural\nlanguage, like with Constitutional AI [1], turned into a random variable over which conditional\nprobabilities can be estimated? Having humans label lots of situations that match the desired\nconstraint seems impractical. The alternative proposed by Bengio [3] is to learn to interpret\nnatural language statements as formal random variables over which probabilistic causal reasoning"}, {"title": "Effect of approximation error.", "content": "Finally, because we envisage using machine learning to estimate\nthe bound, we have to consider the fact that a learned predictor will be imperfect. How do we\ncorrect conservatively for the potentially dangerous errors made by this predictor? This may be\nparticularly important if we use a neural network to estimate probabilities, since they are subject\nto lack of robustness and adversarial attacks [15, 38], e.g., how do we make sure that the agent\nproposing the actions has not learned to find loopholes in the risk estimator?"}]}