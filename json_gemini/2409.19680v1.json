{"title": "Instruction Embedding: Latent Representations of Instructions Towards Task Identification", "authors": ["Yiwei Li", "Jiayi Shi", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Boyuan Pan", "Heda Wang", "Yao Hu", "Kan Li"], "abstract": "Instruction data is crucial for improving the capability of Large Language Models (LLMs) to align with human-level performance. Recent research LIMA demonstrates that alignment is essentially a process where the model adapts instructions' interaction style or format to solve various tasks, leveraging pre-trained knowledge and skills. Therefore, for instructional data, the most important aspect is the task it represents, rather than the specific semantics and knowledge information. The latent representations of instructions play roles for some instruction-related tasks like data selection and demonstrations retrieval. However, they are always derived from text embeddings, encompass overall semantic information that influences the representation of task categories. In this work, we introduce a new concept, instruction embedding, and construct Instruction Embedding Benchmark (IEB) for its training and evaluation. Then, we propose a baseline Prompt-based Instruction Embedding (PIE) method to make the representations more attention on tasks. The evaluation of PIE, alongside other embedding methods on IEB with two designed tasks, demonstrates its superior performance in accurately identifying task categories. Moreover, the application of instruction embeddings in four downstream tasks showcases its effectiveness and suitability for instruction-related tasks\u00b9.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating responses capable of addressing specific tasks according to provided instructions. Initially pre-trained for wide-ranging capabilities, they are subsequently fine-tuned using instruction-following datasets to enhance their ability to align with human preferences. LIMA has proved that alignment can be viewed as a straightforward process in which the model just learns the style or format for interacting with users to solve particular problems, where the knowledge and capabilities have already been acquired during pre-training (Zhou et al., 2023).\nText embeddings play a crucial role in a variety of NLP tasks such as semantic textual similarity (Agirre et al., 2012; Cer et al., 2017; Marelli et al., 2014) and information retrieval (Mitra et al., 2017; Karpukhin et al., 2020). Similarly, as a type of text, the latent represent of instructions is also essential for many tasks like data selection for instruction tuning (Wu et al., 2023a) and prompt retrieval for in-context learning (Su et al., 2023). Previous studies (Gao et al., 2021; Wang et al., 2024) obtain text embeddings by directly taking the token vector from language models. However, when it comes to the embeddings of instructions, the key focus should lie in identifying task categories rather than capturing overall semantic information. This is because, as mentioned earlier, instruction fine-tuning helps models learn how to interact with users across different tasks, rather than specific capabilities and knowledge imparted by the instructions. Therefore, task similarities is far more important than semantic similarities for instructions. Figure 1 (a) shows the case where traditional text embedding methods exhibit high overall semantic and syntactic similarity between two samples which actually represent completely different tasks, but low similarity when they represent similar task.\nIn this work, we propose a new concept called instruction embedding, a specialized subset of text embedding that prioritizes task identification for instructions over the extraction of sentence-level semantic information. We construct a new benchmark for instruction embedding training and evaluation. Different from previous text embedding benchmark that only considered the semantic textual similarity, IEB is labeled by task categories of instructions. Inspired by that key instruction words especially verbs are highlighted through instruction tuning (Wu et al., 2023b), we first extract verb-noun pairs to clarify category, then manually select and label instructions with other syntactic structures. Besides, we also conduct category merging and employ GPT-4 to generate complex samples to make the benchmark more robust. IEB totally contains 47k samples dispersed across more than 1k categories, which can also be used for embedding training and downstream tasks.\nTo stimulate language models to generate better instruction embedding, we propose a prompt-based baseline method PIE. It leverages the template to obtain instruction embeddings by directing the model's attention towards the task type represented by the instructions. Despite PIE demonstrating good practicality as it already performs well without training, we can further enhance it by fine-tuning the model on IEB with contrastive learning. As a widely used method for training embedding models, contrastive learning requires positive and hard negative samples to provide training signals, which are hard to extract. In our study, the explicit category information available in IEB enables the straightforward extraction of positive samples by directly selecting two instructions from the same category. We can further construct hard negative samples by selecting samples from categories that share identical verbs or nouns, enhancing the challenge of differentiation. Figure 1 shows that PIE can effectively distinguish whether two instructions refer to the same task cluster.\nWe evaluate PIE and other embedding baselines on IEB with instruction clustering and intention similarity tasks, which shows that PIE can largely outperform other baselines and precisely identify the task categories. We also conduct four downstream tasks, where the superior results demonstrate that the proposed instruction embeddings are more suitable for instruction-related tasks than traditional text embeddings."}, {"title": "The IEB Benchmark", "content": "We present instruction embedding benchmark, IEB, for assessing the quality of the latent representation of instructions. In contrast to current text embedding benchmarks that assess semantic similarity, the primary focus for the space of instruction embeddings is task differentiation based on the given instructions. Therefore, we annotate instructions with their respective tasks in IEB. We define task as follows: a task of an instruction is a category of activities or work that we expect the LLM to perform, which can be represented by a key phrase (mostly verb-noun phrases). The definition of task is not influenced by specific content or knowledge. For example, \"writing an article\" is a task, but the specific topic of the article is not important."}, {"title": "Data Extraction", "content": "For convenience and authenticity, we derive samples from established datasets. Specifically, we adopt three extensively recognized instruction-tuning datasets: DatabricksDolly (Conover et al., 2023), Alpaca data (Taori et al., 2023), and Self-instruct data (Wang et al., 2023). Labeling instructions entirely through manual effort or large language models will incur significant costs. Therefore, it is best to first conduct coarse-grained grouping and filtering based on rule-based policies. Wu et al. (2023b) proves that instruction fine-tuning enables models to recognize key instruction words, which leads to the generation of high-quality responses. Furthermore, it also encourages models to learn word-word relations with instruction verbs. Inspired by these two findings, we argue that verbs and other key words are crucial in identifying the task denoted by an instruction, where the types of them can be effectively determined through syntactic analysis. Thus, we employ the Berkeley Neural Parser\u00b9 (Kitaev and Klein, 2018; Kitaev et al., 2019) for parsing the instructions.\nAfter manual observation and considering the task category requirements, instructions can generally be divided into the following four groups through corresponding parsing tag recognizer:\nVP (VB+NN) denotes verb phrase structure where the verb is closest to the root of the parse tree and directly links to noun. Instructions with this structure account for more than 80% of the total number before filtering. We categorize each instruction based on its verb-noun combination, identifying it as a specific task type, such as write story or generate sentence. After restoring the verb tense and singular form of nouns, we classify instructions with the same verb-noun combination into the same category.\nSBARQ is direct question introduced by a wh- word or a wh-phrase. It can be divided into two main categories: knowledge-based questions led by six interrogative pronouns (e.g., what, when, where, ...) and math problems introduced by what. Unlike instructions in the VP (verb phrase) form, we define categories in the form of interrogative pronoun combing knowledge/math. This is because, considering they all involve asking about knowledge or math problems, further subdividing into noun categories is not very meaningful. For each category, we manually select around 50 samples.\nSQ is inverted yes/no question. It can also be divided into two main categories: knowledge-based questions and task-oriented questions. Similarly, the task label is annotated as yes-no combing knowledge/task and we select around 50 samples for each category.\nOthers There are some other structures: verb phrase that lacks a direct connection to a noun and some rare cases which do not contain verbs, consisting only of noun phrases. We define these four"}, {"title": "Data Synthesis", "content": "In instruction data, we discover some complex sentences, e.g., Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision. Although they make up only a small portion of the dataset, they can serve as particularly challenging samples in the benchmark. However, due to their relative difficulty in identification, we employ GPT-4 to generate samples based on existing task category names, including verbs and corresponding nouns. Subsequently, the generated compound instructions will be integrated into the categories."}, {"title": "Quality Control", "content": "Automatic Filtering We find that low-frequency samples have a higher probability of being noisy, so we discard categories with fewer than 10 samples. Further, we employ GPT-4 to check whether samples belong to its annotated category. About 12.9% samples are filtered out during this process.\nCategory Merging Considering that many verbs or nouns representing instructions are synonyms, e.g., provide and give, it would be inappropriate to classify them into different categories. We utilize WordNet 2 to extract the synonyms, where we merge every two categories where both nouns and verbs are synonyms or same words."}, {"title": "Statistics", "content": "After constructing and filtering, we collect totally 1353 task categories with 47161 samples. Given the large volume of data, the benchmark data can also be used for training and testing instruction embeddings and downstream tasks. Therefore, we have split it in a certain ratio, but it can be be adjusted freely as needed. The EFT (embedding fine-tuning) subset is designed to facilitate models in generating high-quality latent representations of instructions through embedding fine-tuning, which involves a supervised contrastive learning process based on our task labels (details on the embedding fine-tuning process can be found in Sections 3.2). The IFT (instruction fine-tuning) subset is constructed to evaluate the effectiveness of our instruction embeddings in downstream tasks, such as Data Selection for Instruction Tuning and Demonstration Retrieval (details available in Sections 4.3.1 and 4.3.2). Note that there is no overlap among the samples in the four parts, but the task categories in the training and test sets for IFT will overlap."}, {"title": "Instruction Embedding Method", "content": "Traditional text embeddings focus on capturing overall semantic information of text (Xu et al., 2023c). However, Zhou et al. (2023) and Wu et al. (2023b) demonstrate that the essence of instruction data lies in the tasks indicated by task words which are typically composed of a verb and a noun and specify the task action and the task domain (or object of action) respectively. Therefore, we propose instruction embedding method to capture task category information contained in instructions, rather than general semantic information."}, {"title": "Prompt-based Instruction Embedding", "content": "As mentioned above, guiding the model to generate embeddings that focus on task categories is critically important. LLMs have shown an impressive capacity to accomplish novel tasks solely by utilizing in-context demonstrations or instructions (Brown et al., 2020). Inspired by PromptBERT(Jiang et al., 2022), we present a prompt-based instruction embedding method (PIE) that employs a carefully designed prompt to guide the model in extracting the tasks embedded within given instructions. The hidden states of last input token will be represented for the embedding of instruction."}, {"title": "Embedding Fine-tuning", "content": "We further fine-tune PIE-model on EFT-train set following the contrastive learning (CL) framework in SimCSE (Gao et al., 2021), where we replace the dropout-based positive sample pairs construction method with a method based on instruction task labels from EFT-train.\nFormally, let $D = \\{t_i\\}_{i=1}^{|D|}$ denotes EFT-train, where each $t_i = \\{t_{i1}, ..., t_{i|t_i|}\\ } $ represents a specific task category in D, and each $t_{ij}$ is an instruction instance from $t_i$. During training, we take a cross-entropy objective with in-batch negatives (Chen et al., 2017; Henderson et al., 2017). For a given instruction $t_{ij}$, we randomly sampled $t_{ik}$ from $t_i$ where $j \\neq k$ to make up a task-related instruction pair. Let $h_{ij}$ and $h_{ik}$ denote the embeddings of $t_{ij}$ and $t_{ik}$, the learning objective for $(t_{ij}, t_{ik})$ with a mini-batch of N pairs can be formulated as Eq 1\n$l_i = -log \\frac{e^{sim(h_{ij},h_{ik}) / \\tau}}{\\sum_{m=1}^{N} e^{sim(h_{ij},h_{mk'}) / \\tau}}$ (1)\nwhere $\\tau$ is the temperature hyperparameter and $sim(h_1, h_2)$ is the cosine similarity $\\frac{h_1^T h_2}{\\| h_1\\| \\| h_2\\|}$.\nHard negative sampling has been widely adopted in CL (Schroff et al., 2015). In this paper, we propose a hard negative sampling strategy based on verb-noun style instruction task labels: for an instruction $t_{ij}$ whose task category is a verb-noun pair $(v_i, n_i)$, another instruction $t_{ij'}$ whose task category is either $(v_i, n_{i'})$ or $(v_{i'}, n_i)$ is considered as a hard negative sample of $t_{ij}$. When searching for hard negative samples, we prioritize samples with the same verb but different nouns."}, {"title": "Experiment", "content": "Based on IEB benchmark, we introduce instruction clustering task (ICT) and instruction intention similarity (IIS) test to evaluate instruction embeddings. ICT aims to accurately group instructions from different tasks. Specifically, instruction clustering is conducted using k-means clustering based on embeddings of given instructions, where k is predefined and its value equals to the number of task categories in EFT-test (i.e. k = 145 here). We utilize metrics such as Adjusted Rand Index (ARI) (Hubert and Arabie, 1985), Clustering Purity (CP) (Sch\u00fctze et al., 2008), Homogeneity Score (Homo) (Rosenberg and Hirschberg, 2007) and Silhouette Score (Silh) (Rousseeuw, 1987) for evaluation. IIS test is designed to align with STS (Agirre et al., 2012) task. The IIS test set is derived from IFT-train set. First, we randomly sample 1.5k instruction pairs of the same task from IFT-train set and label them as 1. Next, we sample another 1.5k pairs, labeling them as 1 if the task categories matched, otherwise 0. This resulted in a rough 1:1 ratio of samples labeled 1 to those labeled 03. During testing, we calculate cosine similarity of the instruction embeddings for each pair, and compute the Spearman value with the labels across the entire dataset.\nWe implement our PIE method with Llama2 (Touvron et al., 2023b) and BERT (Devlin et al., 2019) separately. For all BERT-based embedding methods, we take the hidden state of [CLS] token from the last layer as instruction embedding. For all Llama2, we first conduct preliminary experiments to select best pooling method and prompt. According to the results, we utilize the average of last token hidden states across last 2 layers as the instruction embedding and choose the prompt.\nWe evaluate the instruction task representation capability of baseline models and compare their performance with our PIE and corresponding supervised fine-tuning method. The baselines are as follows:\nNone-Fine-Tuned baselines We employ Llama2, Vicuna-7b-v1.5 (Zheng et al., 2023) and BERT to obtain instruction embeddings with three prompts: no prompt, semantic-prompt, and PIE-prompt.\nUnsupervised Fine-Tuned baselines Unsupervised SimCSE (Gao et al., 2021) and unsupervised PromptBERT (Jiang et al., 2022) are included as unsupervised fine-tuned baselines. To eliminate the impact of model scale, we also re-implement them with Llama2.\nSupervised Fine-Tuned baselines We supervised fine-tune Llama2 and BERT as mentioned in Section 3.2."}, {"title": "Results and Analyses", "content": "Main Findings The experimental results are shown in Table 3. For none-fine-tuned baselines, our PIE-Prompt guides LLMs to extract task categories of instructions, enabling them to achieve significant improvements in both ICT and IIS test compared to the same model without using prompt. BERT failed to benefit from PIE-prompt, which may due to its limited instruction following capability. Interestingly, Vicuna achieves better results than Llama2 with PIE-prompt despite performing worse when prompt is not used. This is because Vicuna has been enhanced its instruction following capability through instruction tuning, enabling it to better extract task-specific information under the guidance of the PIE prompt. Although Llama2 and Vicuna achieve better performance in none-fine-tuning setting with PIE prompt, BERT successfully bridges this gap and achieves comparable or even better results after supervised fine-tuning on EFT-training. Additionally, for both Llama2 and BERT, although the performance gap between models using PIE-prompt and those using semantic-prompt or no prompt significantly narrows after supervised fine-tuning, models using PIE-prompt still outperform the others. This demonstrates that the guidance provided by PIE-prompt remains crucial even after supervised fine-tuning."}, {"title": "Evaluation on Downstream Tasks", "content": "We conduct four downstream tasks for further evaluation. Our core objective is to validate that instruction embeddings are more suitable for instruction-related downstream tasks compared to traditional text embeddings that focus on the overall semantic information of sentences. Therefore, we select the best-performing model we produced for each type of embedding, i.e., fine-tuned PIE-Llama2 and Wiki fine-tuned Llama2."}, {"title": "Data Selection for Instruction Tuning", "content": "Following previous work (Wu et al., 2023a; Zhou et al., 2023), we design a data selection experiment based on embeddings for instruction diversity. First, we use k-means clustering to divide the IFT- train set into 600 clusters, and extract the closest samples to the clustering centers to achieve data compression. Then, we fine-tune Llama2 on that selected data. We evaluate the performance on our IFT-test set and AlpacaEval (Li et al., 2023c). We use GPT-4 Turbo for judgment, and for IFT-test, its own output serves as the baseline for comparison. We take 5 runs for each setting and calculate the mean score."}, {"title": "Demonstrations Retrieval", "content": "LLMs have shown remarkable in-context learning (ICL) capability (Patel et al., 2023; Yuan et al., 2024). Demonstrations related to the input instruction task are more conducive to model since task-related data are more similar in terms of format and content. Thus in this experiment, we select 2 most related instruction data by calculating cosine similarities from IFT-train set for each instruction in test set. Similarly, we report evaluation results on IFT-test set and AlpacaEval with four models: Vicuna-7B-v1.5, Llama2-7B-chat (Touvron et al., 2023b), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), LongChat-7B-v1.5-32k (Li et al., 2023a). For random selection, we take 10 runs and report the mean score. The results are shown in Figure 4 (b), which demonstrates instruction embedding helps to select more task-related demonstrations and makes better ICL for LLMs."}, {"title": "Tiny Benchmark", "content": "Recently, some work has focused on testing models using fewer samples (Vivek et al., 2024; Polo et al., 2024). The primary goal is to select a more balanced tiny benchmark that can lead to more consistent performance compared to the original full benchmark. Similar to data selection for instruction tuning, this process can also be accomplished through clustering. We select 10, 50, and 100 test samples respectively, and compare the estimation error (%) in performance between the tiny and the original IFT-test benchmark. Following Vivek et al. (2024), we take 100 runs for each setting. The results in Table 4 indicates that instruction embedding can obtain a smaller estimation error by selecting more representative test samples."}, {"title": "Dataset Task Correlation Analysis", "content": "We analyze the correlation degree between instruction tasks across various open-source datasets through instruction embedding. Let $D_1, D_2$ denote two unique instruction datasets, for each instruction $t_i$ in $D_1$, we find its most relevant instruction $t'$ in $D_2$ and take the average of $s_i$ (i.e. the similarity between $t_i$ and $t'$) across $D_1$ (i.e. $\\frac{\\sum_{i=1}^{|D_1|} s_i}{|D_1|}$) as a measure of the extent to which the tasks in $D_1$ are encompassed in $D_2$. We conduct task correlation analysis across GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MBPP (Austin et al., 2021), Lima (Zhou et al., 2023), Dolly (Conover et al., 2023), OAssit (K\u00f6pf et al., 2023), Alpaca (Taori et al., 2023), WizardLM (WizardLM(Alpaca), WizardLM(ShareGPT)(Chiang et al., 2023). As depicted in Figure 5, instruction embeddings succeed to distinguish between math tasks (GSM8k, MATH) and code tasks (MBPP). The correlation degree within math task datasets is significantly higher than the correlation degree between math task datasets and code dataset. Besides, larger and more general instruction datasets exhibit a more significant correlation with other datasets."}, {"title": "Related Work", "content": "Text Embeddings Text embeddings are pivotal in NLP. They encapsulate overall semantic information and the quality of learned embeddings directly influences downstream tasks. Current research on text embeddings primarily focuses on sentence semantic modeling (Gao et al., 2021; Jiang et al., 2022; Li and Li, 2023). We argue that the essence of instructions lies in their task information and instruction embeddings should prioritize modeling task-specific information instead of emphasizing overall semantic information.\nEmbedding Benchmark Semantic Textual Similarity (STS) tasks (Agirre et al., 2012; Cer et al., 2017; Marelli et al., 2014) are commonly employed to evaluate the quality of text embeddings, complemented with transfer tasks and short text clustering tasks (Conneau and Kiela, 2018; Xu et al., 2023c; Muennighoff et al., 2023) to further illustrate the superiority of learned sentence representations. However, previous benchmarks are not tailored to instruction corpora and primarily assess the semantic modeling abilities of text embeddings, rendering them less suitable for evaluating instruction embeddings.\nInstruction Tuning Instruction Fine-Tuning (IFT) is widely adopted to stimulate the instruction following capability of pre-trained LLMs. Early approaches for IFT focused on fine-tuning LLMs with large amounts of instruction data (Wang et al., 2022; Wei et al., 2022) manually aggregated from large NLP task collections (Longpre et al., 2023). With the development of generative language models, Wang et al. (2023) made their attempt to expand instruction data through synthetic data generation, inspiring the following works to evolve instruction data in this automated manner (Taori et al., 2023; Ding et al., 2023; Xu et al., 2023a). Zhou et al. (2023) proved that the quality and diversity of instruction data are significantly more critical than its sheer quantity, motivating recent efforts in instruction data selection to remove unnecessary IFT training costs by eliminating low-quality and redundant data. Quality-based data selection methods typically employ a quality evaluator to predict the quality scores of each instruction sample which are further used to select instruction data Chen et al. (2023); Li et al. (2023b). Diversity-based data selection methods aims to maximize the distance between selected instruction data which are measured by their embeddings Wu et al. (2023a); Liu et al. (2024). However, due to the lack of instruction embedding, previous works relied on semantic embedding which fails to emphasize the task-specific information of instructions data."}, {"title": "Conclusion", "content": "We introduce the concept of instruction embedding, which prioritizes task identification over traditional sentence-level semantic analysis. Alongside this, we release the publicly available IEB benchmark for evaluating and further training instruction embeddings. To ensure instruction embeddings focus more on task specifics, we propose a prompt-based approach for generating instruction embeddings, applicable in both learning-free and supervised fine-tuning settings. It has been demonstrated on two basic evaluation tasks and four downstream tasks that instruction embedding is superior for instruction-related tasks. The introduction of instruction embedding, along with the IEB benchmark and the PIE method, plays a crucial auxiliary role in instruction-related tasks for large language models."}, {"title": "Limitations", "content": "One limitation of our approach is that, by not relying entirely on manual labeling or verification, not all the data is guaranteed to be of high quality. Manual validation results indicate that 93% of the sample categories are accurate, leaving a small portion that may still contain noise. Additionally, we have not addressed multi-step instructions, where several serialized tasks are embedded within a single instruction, as no such cases were manually identified in the selected dataset, and therefore, these samples were neither handled nor supplemented. Lastly, the three popular instruction datasets we selected consist solely of single-turn interactions, meaning that the benchmark does not include multi-turn samples."}, {"title": "Appendix about Benchmark", "content": null}, {"title": "Data Availability", "content": "Dataset: The whole benchmark along with four split parts can be found in https://github.com/ Yiwei98/instruction-embedding-benchmark.\nCode: The code for experiments can be found in https://github.com/Yiwei98/ instruction-embedding-benchmark."}, {"title": "Details about Data Synthesis", "content": "The prompt for employing GPT-4 to generate samples based on task category names is shown in Figure 6. We randomly selected 30% existing task categories and generate 3 samples for each category. After filtering, we obtained a total of 633 synthetic samples.\nGenerate an instruction represents the task category} task, which contains two sentences. Note that the second generated sentences must contain the task word."}, {"title": "Details about Quality Control", "content": "The prompt for employing GPT-4 to check whether instructions belong to its annotated category is shown in Figure 7.\nCheck if the given instruction represents the {task category} task. Instruction: {instruction}. Please answer 'yes' or 'no'.\nFor category merging, we will provide additional details about the merging procedure. Firstly, we select every two categories where both nouns and verbs are synonyms or same words. Then we calculate the cosine similarities of each pair of them by using word embeddings. For two categories where the values of both nouns and verbs pairs are above 0.5, we directly merge them as one category. For categories with values between 0.3 and 0.5, we use GPT-4 to determine whether they describe the same task. If they do, we merge them. For those below 0.3, we directly discard the merge."}, {"title": "More Statistics", "content": "Besides the dataset partitioning, we provide more information about the statistics of proposed benchmark. We present the distribution of the number of instructions per category in Figure 9. Please note that for categories with more than 100 samples, we randomly retained only 100. Additionally, Figures 10 through 14 provide a more detailed view of the verb-noun distributions, where it is clear that there is no category overlap between EFT and IFT, but there is some overlap between the training and test sets within IFT."}, {"title": "Prompts", "content": "PIE Prompt The PIE Prompt is shown in Figure 15. Inspired by Zhang et al. (2024), we combine the pretended chain of thought method and knowledge enhancement method in this prompt, which effectively enhances the instruction task capturing capability of LLM. The essence of an instruction is its task intention. With this in mind, given the instruction below:\n{Instruction}\nafter thinking step by step, the task of the given instruction is:\nSemantic Prompt The semantic Prompt is shown in Figure 16.\nThis Sentence of {Instruction} means:"}, {"title": "Preliminary Experiments", "content": "In LLM, the effectiveness and performance of extracting sentence representations across different hidden layers may vary. To systematically assess the semantic information and representation capabilities of various layers of Llama24, we employs pooling techniques on the last token hidden states at different layers and conduct corresponding evaluations. Specifically, we select the last hidden layer, last two hidden layers, middle hidden layer, and first and last hidden layers as pooling layers. The experimental results are shown in Table 6. We finally select the last two layers as pooling layers mainly due to its robustness. Although it does not achieve all the best results, it consistently maintains competitiveness against the best scores in each metric.\nPrompt is a key part of our PIE . In this paper, we employed a manual approach to search for appropriate prompt: we first manually crafted several prompts, then, for each manually crafted prompt, we evaluated its effectiveness by the instruction clustering task. According to the result, we select #5 template for further experiments."}, {"title": "Additional Configuration", "content": "Instruction Embedding Fine-tuning Experiment Configurations We complete each embedding fine-tuning on a single NVIDIA A100 GPU and adopt LoRA Hu et al. (2022) technique to fine-tune Llama2 7B5 with lora-rank set to 32, lora-alpha set to 64, lora-dropout set to 0.05 and target modules set to ['q_proj','v_proj'6. During training, we set epochs to 1, batch size to 16, tokenize maxlength to 256. Following Gao et al. (2021), the temperature hyperparameter 7 in Eq 1 is set to 0.05. Notably, to better focus on investigating the impact of our embedding train data on instruction embedding training, we remove the data augmentation methods in SimCSE during the embedding training process. Additionally, BERT refers to bert=base-uncased' and Vicuna refers to vicuna-7b-v1.58 unless otherwise specified.\nConfigurations for Instructing Tuning. We complete instruction fine-tuning on 8 NVIDIA A100 GPU to fine-tune the LLM with the batch size set to 128 and the learning rate set to 2 * 10-5. The Alpaca-style template is applied to concatenate queries and responses during fine-tuning."}, {"title": "Visualization Analysis", "content": "To better illustrate the superiority of PIE and the impact of supervised fine-tuning, we visualize instruction embeddings of various mdoels in Figure 17. It is evident that embedding fine-tuning"}, {"title": "Model Inference Output Examples", "content": "In this section, we present the examples of model inference outputs under guidance of different prompts. The results of Llama Touvron et al. (2023a), Llama2, Vicuna v1.310 Chiang et al. (2023), Vicuna v1.5 are shown below. We remove the blank lines from the original model output and replaced the repeatedly generated content with ellipses (...)."}, {"title": "Model Inference Output Examples without Prompt.", "content": "Instruction\nCount how many words are there in the paragraph. also count how many characters are there. output both numbers.\nLLaMA output\n<unk>Count how many words are there in the paragraph. also count how many characters are there. output both numbers.\nComment: @user3100112 I'm not sure what you mean by \"output both numbers\". Do you mean to print the numbers?\nComment: @user3100112 I've updated my answer to include the output.\nInstruction\nCount how many words are there in the paragraph. also count how many characters are there. output both numbers.\nLLaMA2 output\n<s>Count how many words are there in the paragraph. also count how many characters are there. output both numbers.\nI have a problem with the following code.\nInstruction\nCount how many words are there in the paragraph. also count how many characters are there. output both numbers.\nVicuna-v1.3 output\n<s> Count how many words are there in the paragraph. also count how many characters are there. output both numbers.\nExample:\nInput: \"The quick brown fox jumps over the lazy dog.\"\nOutput: 41 (words), 41 (characters)\nInstruction\nCount how many words are there in the paragraph. also count how many characters are there. output both numbers.\nVicuna-v1.5 output\n<s> Count how many words are there in the paragraph. also count how many characters are there. output both numbers.</s>"}, {"title": "Model Inference Output Examples with Semantic Prompt.", "content": "Instruction\nCount how many words are there in the paragraph. also count how many characters are there. output both numbers.\nSemantic-Prompt LLaMA output\n<unk>This sentence of \"Count how many words are there in the paragraph. also count how many characters are there. output both numbers.\" means: 1. count how many words are there in the paragraph. 2. count how many characters are there. output both numbers.\nComment: @user3580874 I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\". I'm not sure what you mean by \"output both numbers\".\nInstruction\nCount how many words are there in the paragraph. also count how many characters are there. output both numbers.\nSemantic-Prompt LLaMA2 output\nThis sentence of \"Count how many words are there in the paragraph. also count"}]}