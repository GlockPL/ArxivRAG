{"title": "MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs", "authors": ["Shiyi Cao", "Shu Liu", "Tyler Griggs", "Peter Schafhalter", "Xiaoxuan Liu", "Ying Sheng", "Joseph E. Gonzalez", "Matei Zaharia", "Ion Stoica"], "abstract": "Efficient deployment of large language models, particularly Mixture of Experts (MoE) models, on resource-constrained platforms presents significant challenges in terms of computational efficiency and memory utilization. The MoE architecture, renowned for its ability to increase model capacity without a proportional increase in inference cost, greatly reduces the token generation latency compared with dense models. However, the large model size makes MoE models inaccessible to individuals without high-end GPUs. In this paper, we propose a high-throughput MoE batch inference system, MoE-Lightning, that significantly outperforms past work. MoE-Lightning introduces a novel CPU-GPU-I/O pipelining schedule, CGOPIPE, with paged weights to achieve high resource utilization, and a performance model, HRM, based on a Hierarchical Roofline Model we introduce to help find policies with higher throughput than existing systems. MOE-LIGHTNING can achieve up to 10.3\u00d7 higher throughput than state-of-the-art offloading-enabled LLM inference systems for Mixtral 8x7B on a single T4 GPU (16GB). When the theoretical system throughput is bounded by the GPU memory, MoE-Lightning can reach the throughput upper bound with 2-3\u00d7 less CPU memory, significantly increasing resource utilization. MoE-Lightning also supports efficient batch inference for much larger MoEs (e.g., Mixtral 8x22B and DBRX) on multiple low-cost GPUs (e.g., 2-4 T4s).", "sections": [{"title": "1 Introduction", "content": "Mixture of Experts (MoE) [10, 22, 41, 46] is a paradigm shift in the architecture of Large Language Models (LLMs) that leverages sparsely-activated expert sub-networks to enhance model performance without significantly increasing the number of operations required for inference. Unlike dense models [40, 47, 53], where all model parameters are activated for each input, MoE models activate only a subset of experts, thereby improving computational efficiency.\nWhile the MoE models achieve strong performance in many tasks [10, 22], unfortunately, their deployment is challenging due to the significantly increased memory demand for the same number of active parameters. For example, the Mixtral 8x22B model [32] requires over 256 GB of memory for the parameters of the expert feed-forward network (FFN), which is 4 - 5\u00d7 higher than the memory requirements of dense models that require similar FLOPs for inference.\nIn this paper, we study how to achieve high-throughput MoE inference with limited GPU memory. We are focusing on off-line, batch-processing workloads such as model evaluation [29], synthetic data generation [14], data wrangling [33], form processing [7], and LLM for relational analytics [31] where higher inference throughput translates into lower total completion time.\nThe common approach for memory-constrained batch inference is to offload model weights [4, 19] and key-value tensors of earlier tokens (KV cache) [42] - which are needed for generating the next token \u2013 to CPU memory or disk. Then, they are loaded layer-by-layer to the GPU for computation.\nUnfortunately, existing solutions fall short of effectively overlapping computations with data transfers between CPU and GPU. For instance, the GPU may remain idle as it awaits a small yet crucial piece of data such as intermediate results for the upcoming batch. At the same time, transferring the weights for subsequent layers may take a long time and potentially block both the GPU and CPU from processing further tasks, leading to under-utilization of all the resources.\nAs a result, efficient MoE inference for throughput-oriented workloads using limited GPU memory remains challenging. We find that increasing I/O utilization and other resource utilization is critical in achieving high throughput. For example, Fig. 1 illustrates the relationship between CPU memory and achievable token generation throughput for different systems with fixed GPU memory (less than the model size) and CPU-to-GPU memory bandwidth. When a layer's weights are loaded onto the GPU, a common strategy to increase throughput is to process as many requests as possible to amortize the I/O overhead of weights' transfer [42]. However, this increases CPU memory usage as additional space is required to store the KV cache for all requests. Consequently, lower I/O utilization means higher I/O overhead of weights' transfer, requiring greater CPU memory to reach peak generation performance; otherwise, the GPU will be under-utilized as suggested by the blue line in Fig. 1.\nWhile improving resource utilization is crucial for achieving high-throughput inference with limited GPU memory, achieving this raises several challenges. First, we need to effectively schedule the computation tasks running on CPU and GPU, together with the transfers of various inputs (e.g., experts weights, hidden states, and KV cache), such that to avoid computation tasks waiting for transfers or the other way around. Second, as indicated by the orange line in Fig. 1, the existing solutions [42] tend to generate sub-optimal policies with smaller GPU batch sizes which lead to resource under-utilization. Fundamentally, these solutions fail to take into account that changes in the workload can lead to changes in the bottleneck resource.\nTo address these two challenges, we developed a new inference system, MoE-Lightning, which consists of two new components. The first component is CGOPIPE, a pipeline scheduling strategy that overlaps GPU computation, CPU computation and various I/O events efficiently so that computation is not blocked by I/O events and different I/O events won't block each other. This way, CGOPIPE can significantly improve the system utilization. The second component is Hierarchical Roofline Model (HRM) which accurately models how different components in an inference system interact and affect application performance under various operational conditions.\nIn summary, this paper makes the following contributions:\n\u2022 CGOPIPE, a pipeline scheduling strategy that efficiently schedules various I/O events and overlaps CPU and GPU computation with I/O events. By deploying weights paging, CGOPIPE reduces pipeline bubbles, significantly enhancing throughput and I/O efficiency compared with existing systems (\u00a74.1).\n\u2022 HRM, a general performance model for LLM inference which extends the Roofline Model [48]. HRM can easily support different models, hardware, and workloads, and has near-zero overhead in real deployments, without the need for extensive data fitting (might take hours or days) as needed in FlexGen (\u00a74.2).\n\u2022 An in-depth performance analysis for MoE models based on our extended HRM which identifies various performance regions where specific resource becomes the bottleneck (\u00a73).\nWe evaluate MoE-Lightning on various recent popular MoE models (e.g., Mixtral 8x7b, Mixtral 8x22B, and DBRX) on different hardware settings (e.g., L4, T4, 2xT4, and 4xT4 GPUs) using three real workloads. When compared to the best of the existing systems, MoE-Lightning can improve the generation throughput by up to 10.3\u00d7 (without request padding) and 3.5\u00d7 (with request padding) on a single GPU. When Tensor-parallelism is enabled, MOE-LIGHTNING demonstrates super-linear scaling in generation throughput (\u00a75)."}, {"title": "2 Background", "content": "2.1 Mixture of Experts\nLarge Language Models (LLMs) have significantly improved in performance due to the advancements in architecture and scalable training methods. In particular, Mixture of Experts (MoE) models have shown remarkable improvements in model capacity, training time, and model quality [10, 13, 22, 27, 41, 46], revitalizing an idea that dates back to the early 1990s [21, 23] where ensembles of specialized models are used in conjunction with a gating mechanism to dynamically select the appropriate \u201cexpert\u201d for a given task.\nThe key idea behind MoE is a gating function that routes inputs to specific experts within a larger neural network. Each expert is specialized in handling particular types of inputs. The gating function selects only a subset of experts to process an input, which allows LLMs to scale the number of parameters without increasing inference operations.\nMoE models adopt a conventional LLM architecture, which uses learned embeddings for tokens and stacked transformer layers. MoE LLMs typically modify the Feed-Forward Network (FFN) within a transformer layer by adding a gating network that selects expert FFNs, usually implemented as multi-layer perceptrons, to process the input token [6, 13, 57]. These designs can surpass traditional dense models [8, 10, 22] in effectiveness while being more parameter-efficient and cost-effective during training and inference.\n2.2 LLM Inference\nLLMs are trained to predict the conditional probability distribution for the next token, $P(X_{n+1} | X_1,...,x_n)$, given a list of input tokens $(x_1, . . ., x_n)$. When deployed as a service, the LLM takes in a list of tokens from a user request and generates an output sequence $(X_{n+1}, ..., X_{n+T})$. The generation process involves sequentially evaluating the probability and sampling the token at each position for T iterations. The stage where the model generates the first token $x_{n+1}$ given the initial list of tokens $(x_1,...,x_n)$, is defined as the prefill stage. In the prefill stage, at each layer, the input hidden states to the attention block will be projected into the query, key, and value vectors. The key and value vectors will be stored in the KV cache. Following the prefill stage is the decode stage, where the model generates the remaining tokens $(X_{n+2}, ..., X_{n+T})$ sequentially. When generating token $x_{n+2}$, all the KV cache of the previous tokens $(x_1, . . ., x_{n+1})$ will be needed, and the token $x_{n+2}$'s key and value at each layer will be appended to the KV cache.\nThe auto-regressive nature of LLM generation, where tokens are generated sequentially, can lead to sub-optimal device utilization and decreased serving throughput [37]. Batching is a critical strategy for improving GPU utilization: [51] proposed continuous batching which increases the serving throughput by orders of magnitude. Numerous studies have developed methods to tackle associated challenges such as memory fragmentation [26] and the heavy memory pressure imposed by the KV cache [17, 24, 42]. The scenario of limited GPU memory introduces further challenges, especially for large MoE models, as it requires transferring large"}, {"title": "3 Performance Analysis", "content": "In this section, we introduce a Hierarchical Roofline Model (HRM) (\u00a73.2) extended from the classical Roofline Model [48], which we use to conduct a theoretical performance analysis for MoE inference (\u00a73.3). It also serves as the basics of our performance model used in scheduling policy search, which will be discussed in \u00a74.2. The Hierarchical Roofline Model extends the original Roofline Model for multicore architectures [48] to provide a stronger model of heterogeneous computing devices and memory bandwidth. We further identify two additional turning points that define settings where the computation is best done on CPU instead of GPU and where the application is GPU memory-bound or CPU memory-bound, providing explicit explanations for how LLM inference performance will be affected by different resource limits in the system.\n3.1 Roofline Model\nWe will start with the original Roofline Model [48], which provides a visual performance model to estimate the performance of a given application by showing inherent hardware limitations and potential opportunities for optimizations. It correlates a system's peak performance and memory bandwidth with the operational intensity of a given computation, where Operational Intensity (I) denotes the ratio of the number of operations in FLOPs performed to the number of bytes accessed from memory, expressed in FLOPs/Bytes. The fundamental representation in the Roofline Model is a performance graph, where the x-axis represents operational intensity I in FLOPs/byte and the y-axis represents performance P in FLOPs/sec. The model is graphically depicted by two main components:\nMemory Roof: It serves as the upper-performance limit indicated by memory bandwidth. It is determined by the product of the peak memory bandwidth ($B_{peak}$ in Bytes/sec) and the operational intensity (I). Intuitively, if the data needed for the computation is supplied slower than the computation itself, the processor will idly wait for data, making memory bandwidth the primary bottleneck. The memory-bound region (in blue) of the roofline is then represented by:\n$P \\leq B_{peak} \\times I$ (1)\nwhere P is the achievable performance.\nCompute Roof: This represents the maximum performance achievable limited by the machine's peak computational capability ($P_{peak}$). It is a horizontal line on the graph (top edge of the yellow region), independent of the operational intensity, indicating that when data transfer is not the bottleneck, the maximum achievable performance is determined by the processor's computation capability. The compute-bound part (yellow region) is then defined by:\n$P \\leq P_{peak}$ (2)\nThe turning point is the intersection of the compute and memory roofs, given by the equation:\n$I = \\frac{P_{peak}}{B_{peak}}$ (3)\ndefines the critical operational intensity \u012a. Applications with I > \u012a are typically compute-bound, while those with I < \u012a are memory-bound.\nIn practice, analyzing an application's placement on the roofline model helps identify the critical bottleneck for performance improvements. Recent works [52] analyze different computations (e.g., softmax and linear projection) in LLM using the Roofline Model.\n3.2 Hierarchical Roofline Model\nWhile the original Roofline Model demonstrates great power for application performance analysis, it is not enough for analyzing applications such as LLM inference that utilize diverse computing resources (e.g., CPU and GPU) and move data across multiple memory hierarchies (e.g., GPU HBM, CPU DRAM, and Disk storage).\nConsider a system with n levels of memory hierarchies. Each level i in this hierarchy is coupled with a computing processor. The peak bandwidth at which the processor at level i can access the memory at the same level is denoted by $B_{peak}^{i}$. Additionally, the peak performance of the processor is denoted by $P_{peak}^{i}$.\nDefinition 3.1 (General Operational Intensity). To consider different memory hierarchies, we define the general operational intensity I of the computation task x as the ratio of the number of operations in FLOPs performed by x to the number of bytes accessed from memory at level i.\nFor computation x executed at level i in the HRM, we can define its compute and memory roofs similarly as in the original Roofline Model:\n\u2022 Compute Roof at level i:\n$P \\leq P_{i}^{peak}$ (4)\nThis represents the maximum computational capability at level i, independent of the operational intensity.\n\u2022 Memory Roof at level i:\n$P_{i} \\leq B_{peak}^{i} \\times I$ (5)\nMore importantly, in HRM, there is also the memory bandwidth from level j to level i, denoted as $B_{peak}^{j,i}$, which will define another memory roof for computation x that is executed on level i and transfers data from level j:\n\u2022 Memory Roof from level j to i:\n$P \\leq B_{peak}^{j,i} \\times I$ (6)\nTherefore, if computation x is executed on level i, data from level j needs to be fetched, and the peak performance will be bounded by the three roofs listed above (Eqs. (4)-(6)):\n$P = min(P_{i}^{peak}, B_{peak}^{i} \\times I, B_{peak}^{j,i} \\times I)$ (7)\nIf operator x is executed on level i without fetching data from other levels, it reduces to the traditional roofline model and can achieve:\n$P = min(P_{i}^{peak}, B_{peak}^{i} \\times I)$ (8)\nTurning Points. Intuitively, our HRM introduces more memory roofs that consider cross-level memory bandwidth and compute roofs for diverse processors. This results in more \"turning points\" than in the original Roofline Model, which define various performance regions where different resources are the bottleneck. Analyzing these turning points is crucial for understanding the performance upper bound of an application under different hardware setups and computational characteristics.\nFor example, consider a computation task x that has data stored on level j, according to Eq. (6) and Eq. (8), when $P = min(P_{i}^{peak}, B_{peak}^{i} \\times I) > B_{peak}^{j,i} \\times I$, we have $P \\leq B_{peak}^{j,i} \\times I$. Therefore, the first turning point $P_1$ is at:\n$I = \\frac{min(P_{i}^{peak}, B_{peak}^{i} \\times I)}{B_{peak}^{j,i}}$ (9)\nThis gives the critical operational intensity \u012a, indicating the threshold below which it is not beneficial to transfer data from level j to i for computation for x.\nNow if we continue increasing I such that $P < B_{peak}^{j,i} \\times I \\leq min(P_{i}^{peak}, B_{peak}^{i} \\times I)$, then we obtain another turning point $P_2$:\n$I = \\frac{min(P_{i}^{peak}, B_{peak}^{i} \\times I)}{B_{peak}^{j,i}}$ (10)\nwhich denotes the critical operational intensity \u1f66 below which computation x is bounded by the memory bandwidth from memory at level j to memory at level i."}, {"title": "4 Method", "content": "In general, we adopt the zigzag computation order proposed in FlexGen [42]: loading the weights from CPU and performing the computation layer by layer. For the prefill stage, we perform all the computation on GPU and offload KV cache to CPU for all the micro-batches7. For the decode stage, within each layer, we propose a fine-grained GPU-CPU-I/O pipeline schedule (\u00a74.1) to increase the utilization of GPU, CPU, and I/O in decode stage. We also build a performance model (\u00a74.2) based on the HRM we extended from the Roofline Model to help search for the best hyper-parameters for the pipeline schedule, including the assignment of devices to perform different computations, the batch size, the micro-batch size and the ratio of weights to be placed on GPU statically. Note that for the memory-constrained scenarios we target in this paper, CPU attention is consistently better than GPU attention, according to our performance model. We also conduct an ablation study in \u00a76.3 to show how best policy changes under different hardware configurations.\n4.1 GPU-CPU-I/O Pipeline Schedule\nPipeline scheduling is a common approach to maximize compute and I/O resource utilization. Yet, the pipeline concerning GPU, CPU, and I/O is not trivial. In traditional pipeline parallelism for deep learning training [16, 18, 34], models are divided into stages which are assigned to different devices. Therefore, only output activations are transferred between stages, resulting in a single type of data transfer in each direction at a time. In our scenario, both weights and intermediate results need to be transferred between GPU and CPU. Intermediate results are required immediately after computation to avoid blocking subsequent operations, whereas weights for the next layer are needed only after all micro-batches for the current layer are processed. Additionally, weight transfers typically take significantly longer than intermediate results. Consequently, naive scheduling of I/O events can lead to low I/O utilization, which also hinders computation. CGOPIPE employs CPU attention as analyzed in \u00a73.3, alongside a weight paging scheme that interleaves the transfer of intermediate results for upcoming micro-batches with paged weight transfers to optimize computation and communication overlap. The GPU sequentially processes the post-attention tasks (primarily O projection and MoE FFN) for the current micro-batch, followed by the pre-attention tasks (mainly layer norm and QKV projection) for the next micro-batch. Concurrently, the CPU handles attention (specifically the softmax part) for the next batch, and a page of weights for the subsequent layer are transferred to the GPU.\nWeights Paging and Data Transfer Scheduling. To fully utilize the I/O, we propose a weights paging scheme to interleave the data transfer for different tasks, reducing bubbles in the I/O. There are mainly four kinds of data transfer:\n\u2022 $D_1$ (QKV DtoH): the intermediate results to be transferred from GPU to CPU after QKV projection.\n\u2022 $D_2$ (Hidden HtoD): the hidden states to be transferred from CPU to GPU after the CPU attention.\n\u2022 $D_3$ (Weights Transfer): the weights for the next layer to be transferred from CPU to GPU.\n\u2022 $D_4$ (KV cache Transfer): the KV cache for the next micro-batch to be transferred from CPU to GPU.\nDue to independent data paths, data transfers in opposite directions can happen simultaneously. Data transfer will be performed sequentially in the same direction. The challenge"}, {"title": "4.2 Search Space and Performance Model", "content": "Given a hardware configuration H, a model configuration M, and a workload configuration W, we search for the optimal policy P that minimizes per-layer latency T(M,H,W,P) for the pipeline schedule in \u00a74.1, without violating the CPU and GPU memory constraints, in order to reach the optimal balance point (Eq. (11)). Compared with FlexGen, we exclude disk-related variables from the search space and add two binaries to indicate whether to perform attention or MoE FFN on GPU.\nWe then build the performance model based on Eq. (7) and Eq. (8) in HRM to estimate per-layer decode latency T by:\nT(M,H,W,P) = max(Tcommcpu_to_gpu, Tcpu, Tgpu) (12)\nwhere $T_{commcpu_to_gpu}$ can be computed as the number of bytes needed to be transferred from CPU to GPU for a layer's computation divided by the CPU to GPU memory bandwidth $b_{cg}$. Here, for simplicity, we only consider the attention computation and the MoE FFN computation in a transformer block, and therefore we have:\nTgpu = Tattn+Tfn, Tcpu = Tattn +Tffn (13)\nTo estimate the time to perform a computation x on GPU or CPU, we can use $T_x = max(comm_x, comp_x)$ according to Eq. (8) in HRM, resulting in:\n$T_{ffn} = max(comm_{ffn}, comp_{ffn})$ (14)\nand similarly for $T_{attn}, T_{attn}$ and $T_{ffn}$\nFor a given computation x, we can calculate their theoretical FLOPS and data transfer based on M and then we have commx = bytesx/bg and comp = flopsx/pg (same for CPU). While there are discrepancies between the theoretical performance estimation and the kernel's real performance, such modeling can provide a reasonable estimation of the relative effectiveness of any two policies. In this paper, all the evaluation results of MOE-Lightning follow policies generated by a performance model with theoretically calculated computation flops and bytes with profiled peak performance and memory bandwidth for the hardware.\n4.3 Tensor Parallelism\nIn existing works [42], pipeline parallelism is used for scaling beyond a single GPU, which requires the number of devices to scale with the model depth instead of the layer size. However, according to our analysis for MoE models in \u00a73.3, Total GPU memory capacity can decide the upper bound throughput the system can achieve. Therefore, MOE-LIGHTNING implements tensor parallelism [35] within a single node to get a higher throughput upper bound. In this case, we have $tp_{size}$ times more GPU memory capacity and GPU memory bandwidth, we can then search for the policy similarly as for single GPU."}, {"title": "5 Evaluation", "content": "5.1 Setup\n5.2 End-to-end Results on Real Workloads\nWe evaluate the maximum generation throughput for all baseline systems on three workloads under S1, S2, S6, and S7 settings. As shown in Fig. 7 and Tab. 4, MoE-Lightning (p) outperforms all baselines in all settings, and MoE-Lightning achieves up to 10.3\u00d7 better throughput compared with the\n5.3 Tensor Parallelism\nThis section evaluates MoE-Lightning's ability to run on multiple GPUs with tensor parallelism. As shown in S1 and S2, due to our efficient resource utilization, MOE-LIGHTNING'S throughput is predominantly bounded by GPU memory capacity. This shows that increasing GPU memory can raise the system's throughput upper bound."}, {"title": "6 Ablation Study", "content": "6.1 Optimizer Policy\nIn this section, we compare MoE-Lightning (p), FlexGen with its policy and FlexGen with our policy. For this experiment, we do not turn on the CPU attention for FlexGen as it is consistently worse than FlexGen w/o CPU attention. We use the workload from MTBench on the S1 setting with a generation length of 128. The results are displayed in Tab. 5. By deploying our policy, we can see a 1.77\u00d7 improvement in FlexGen. We also increase the batch size to better amortize the weights transfer overhead and it gives a 2.17\u00d7 speedup. However, it still cannot match MoE-Lightning's throughput under the same policy, as KV cache swapping becomes the bottleneck for FlexGen in this case.\n6.2 CPU Attention vs. Experts FFN vs. KV Transfer\nIn this section, we study when CPU attention will become the bottleneck in the decode stage. For different batch sizes (from 32 to 256), we test the latency of the MoE FFN kernel on L4 GPU and compare it with the latency of the CPU attention kernel on a 24-core Intel(R) Xeon(R) CPU @ 2.20GHz with various context lengths (from 128 to 2048). Additionally, we also measure the latency for swapping the KV cache needed for the attention from CPU pinned memory to GPU to validate the efficiency of our CPU GQA kernel.\nAs shown in Fig. 9, our CPU attention kernel is 3-4x faster than KV cache transfer, which is close to the ratio of CPU memory bandwidth and the CPU to GPU memory bandwidth. The MoE FFN's latency doesn't change so much across different micro batch sizes, which is as expected since the kernel is memory-bound for the decode stage. As the micro-batch size and context length increase, the CPU attention will eventually become the bottleneck, which calls for higher CPU memory bandwidth.\n6.3 Case Study on Different Hardware Settings\nIn this section, we study how the best policy changes under different hardware settings. As we have shown in the previous ablation study, CPU attention can actually become the bottleneck for large batch size and context length, which means if we have more powerful GPUs, at some point, CPU attention may not be worth it. Moreover, if we have higher"}, {"title": "7 Related Work", "content": "Memory-constriant LLM Inference LLM inference requires substantial memory to store model parameters and computation outputs, making it typically memory capacity-bound. There is a line of research dedicated to memory-constraint LLM inference. This is particularly crucial for inference hardware such as desktop computers, or low-end cloud instances with limited computational power and memory capacity. To facilitate inference on such constrained systems, some work leverages sparsity or neuro activation patterns to intelligent offloading between CPU and GPU [15, 42, 43, 50]. Some approaches utilize not only DRAM but also flash memory to expand the available memory resources [3]. Additionally, since the CPU often remains underutilized during inference, it can be harnessed to perform complementary computations [25, 43, 49].\nLLM Inference Throughput Optimization To enhance inference throughput, some research focuses on maximizing the sharing of computations between sequences to minimize redundant processing of identical tokens [24, 56]. Another approach involves batching requests [51] to optimize hardware utilization. Additionally, some studies develop paged memory methods for managing the key-value (KV) cache to reduce memory waste, thereby increasing the effective batch size and further improving throughput [26]. FastDecode [17] proposes aggregating memory and computing power of CPUs across multiple nodes to process the attention part to boost GPU throughput. Compared with FastDecode, we are targeting the memory-constrained case where the model weights also need to be transferred between CPU and GPU, making the optimization and scheduling problem far more challenging.\nLLM Inference Latency Optimization To reduce LLM inference latency, some work addresses the inherent slowness caused by the autoregressive nature of LLM inference by developing fast decoding methods, such as speculative decoding [5, 28, 44] and parallel decoding [39], which generate multiple tokens simultaneously. Another approach aims to decrease inference latency by implementing efficient computational kernels [1, 11, 12] designed to minimize memory access and maximize GPU utilization."}, {"title": "8 Conclusion", "content": "We present MoE-Lightning, a high-throughput MoE inference system for GPU-constrained scenarios. MoE-Lightning can achieve up to 10.3\u00d7 (without request padding) and 3.5\u00d7 (with request padding) higher throughput over state-of-the-art systems on a single GPU and demonstrate super-linear scaling on multiple GPUs, enabled by CGOPIPE and HRM. CGOPIPE is a novel pipeline scheduling strategy to improve resource utilization, and HRM is a performance model based on a Hierarchical Roofline Model that we extend from the classical Roofline Model to find policies with higher throughput upper bound."}, {"title": "A System Implementation Details", "content": "In this section, we explain two system-level designs and their implementation details: 1. Appendix A.1 introduces how GPU and CPU memory are used and weights paging is implemented in MoE-Lightning, and 2. Appendix A.2 presents the batching algorithm employed in MoE-Lightning to support dynamic-length requests in a batch.\nA.1 Memory Management\nSince attention is performed on CPU, the KV cache for all micro-batches will be transferred to and stored on CPU after the corresponding computation completes. To enable CGOPIPE, we allocate a weight buffer with a size of 2 \u00d7 sizeof(WL), where W\u2081 denotes the size of the portion of a layer's weights stored in CPU memory. This buffer enables overlapping weight prefetching: as the current layer's weights are being used, the next layer's weights are simultaneously transferred to GPU memory.\nWeights are transferred in a paged manner. For example in Fig. 11, each expert in the MoE FFN kernel requires two pages, and the kernel accesses the appropriate pages using a page table. To accelerate transfers from CPU to GPU, weights are first moved from CPU memory to pinned memory, and then from pinned memory to GPU. These transfers are overlapped to hide latency. As illustrated in Fig. 11, while transferring Weights 2 for Layer 2 from pinned memory to GPU, Weights 4 for the same layer can be transferred concurrently from CPU to pinned memory.\nA.2 Request Batching\nFor a given workload, the optimizer introduced in \u00a74.2 takes the average prompt length to search for an optimal policy."}, {"title": "B Further Discussion", "content": "B.1 MoE v.s. Dense Models\nThe performance model and system optimizations proposed in this work are fully applicable to dense models. As discussed in \u00a73.3, MoE models present greater challenges with their higher memory-to-FLOPS ratio. This benefits them more from the system optimizations, which specifically aim to improve I/O efficiency and reduce pipeline bubbles. Dense models can benefit from these optimizations as well; however, they are more likely to be bottle-necked by CPU memory bandwidth during attention (depending on sequence length), where methods like sparse attention [9, 45, 54] and quantized KV cache may offer more gains.\nB.2 Optimizer Overhead\nIn \u00a74.2, we introduced the optimization target Eq. (12) and the search space. For a given workload, model and hardware specification, the optimal policy can be generated offline through mixed integer linear programming (MILP), which takes less than a minute.\nC Future Work\nAdvanced performance model. HRM presented in this work is limited to hardware within a single node and does not account for GPU-GPU communication or multi-node communication, both of which are critical for more comprehensive distributed performance modeling. Additionally, with recent advances in leveraging KV cache sparsity for long-context inference [45], it becomes essential to incorporate these optimizations into the performance model. For example, when CPU attention emerges as the bottleneck, the KV cache budget can be adjusted to better balance CPU and GPU computation, enhancing overall system efficiency.\nDisk and other hardware support. MoE-Lightning currently focuses on scenarios where GPU memory is limited but sufficient CPU memory is available to hold the model, highlighting the effectiveness of both CGOPIPE and HRM. However, when CPU memory is insufficient to hold the entire model, disk offloading becomes essential. Moreover, supporting hardware such as TPUs and other accelerators is essential for extending the versatility of MoE-Lightning to diverse computing environments."}]}