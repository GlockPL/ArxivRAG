{"title": "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models", "authors": ["Eitan Wagner", "Yuli Slavutsky", "Omri Abend"], "abstract": "Although language model scores are often treated as probabilities, their reliability as probability estimators has mainly been studied through calibration, overlooking other aspects. In particular, it is unclear whether language models produce the same value for different ways of assigning joint probabilities to word spans. Our work introduces a novel framework, ConTestS (Consistency Testing over Spans), involving statistical tests to assess score consistency across interchangeable completion and conditioning orders. We conduct experiments on post-release real and synthetic data to eliminate training effects. Our findings reveal that both Masked Language Models (MLMs) and autoregressive models exhibit inconsistent predictions, with autoregressive models showing larger discrepancies. Larger MLMs tend to produce more consistent predictions, while autoregressive models show the opposite trend. Moreover, for both model types, prediction entropies offer insights into the true word span likelihood and therefore can aid in selecting optimal decoding strategies. The inconsistencies revealed by our analysis, as their connection to prediction entropies and differences between model types, can serve as useful guides for future research on addressing these limitations.", "sections": [{"title": "1 Introduction", "content": "Pretrained Large Language Models (LLMs) emerged as high-performance predictors for diverse tasks (Brown et al., 2020). Tuning based on instructions and human feedback has further advanced their performance (Wei et al., 2022; Ouyang et al., 2022). In various applications, the model's success often depends solely on assigning high scores to the correct options. Yet, their interpretation as probabilities is beneficial in several applications, such as their treatment as confidence indicators for detecting hallucinations (Manakul et al., 2023; Ren et al., 2023) and robust ranking in sequence generation tasks (Zhao et al., 2022).\nIndeed, LLMs are commonly trained using the cross-entropy objective, which due to Gibbs' inequality is minimized by the true distribution. However, this global minima is hard to achieve (Chang and McCallum, 2022; Zhu et al., 2023), raising the question of whether the model's outputs can still be interpreted as estimated probabilities.\nWhile calibration of produced scores has been extensively studied (Zhao et al., 2022; Shen et al., 2024), multiple other aspects remain unexplored. Measuring calibration for string density estimation requires a ground-truth measure for the real distribution, which is challenging to obtain (see \u00a73). Therefore, alternative methods to validate the assumption that produced scores correspond to estimated probabilities, are needed.\nFor LLM scores to be interpreted as probabilities, consistency across estimation methods is essential. However, regardless of probabilistic interpretation, detecting and understanding inconsistencies among estimation methods is crucial, especially in the identification of preferable estimation methods. Considering completion of word spans, consistency implies that filling masks in different orders (first filling one word and then the other, or vice versa) produces the same joint probability (see Figure 1). In this work, we investigate whether this requirement is fulfilled.\nVarious factorizations of a joint distribution into conditional probabilities are possible, for instance by applying the chain rule from left-to-right or right-to-left. However, an unrestricted set of conditional probabilities does not guarantee a unique joint probability (see example in Appendix A.1). Although restricting the conditional probabilities, such as by disallowing cycles in Bayesian Networks, can ensure a unique joint probability, it is not necessary (see example in Appendix A.2).\nMasked language modeling (MLM) training lacks mechanisms to ensure that a set of conditional probabilities will form a unique joint distribution. However, since language modeling is based on the assumption that a distribution over strings generates the samples, estimated conditional distributions are expected to align with joint conditioning.\nWe investigate the consistency of both MLM and autoregressive models (which include decoder-only and encoder-decoder models), considering that MLM can function as a missing token classification task or a generative task with a specific instruction prompt (see \u00a74). When treating autoregressive models, we account for task comprehension as a contributing factor.\nWe introduce a novel framework that employs statistical tests \u2013 CONTESTS, for Consistency Testing over Spans, to analyze discrepancies between different estimation methods, and their behaviors across various model types.\nOur findings show that all examined LLMs fail to produce consistent probabilities for joint span modeling. However, we observe notable distinctions among model types and sizes: autoregressive models show increasing discrepancies as model parameters increase, while MLMs tend to provide more consistent predictions, with larger models offering further improvements. Additionally, we show that prediction entropies are indicative of the true likelihood for both model types, suggesting their usefulness in selecting optimal decoding strategies."}, {"title": "2 Preliminaries and Notation", "content": "The task of single-mask probabilistic masked language modeling is to estimate the probability of a masked location, given the rest of the sequence. For a model M and a sequence of tokens x = (X1, X2...Xn), we denote the estimation by\nPM(xi = Wi|X1 = W1, ..., Xi\u22121 = Wi\u22121, Xi+1 = Wi+1,..., Xn = Wn).\nWhen multiple masks are considered, that is, when in addition to i, the positions j1,.., jk are masked as well, the predicted distribution for the i-th position given all unmasked ones is given by\nPM(xi = wi|xj = wj\u2200j \u2209 {i, j1, ..., jk}),\nand the joint distribution of two masked positions i, j, given unmasked ones, is denoted by\nPM(xi = Wi, Xj = wj|xk = wk\u2200k \u2209 {i, j}).\nMasked language modeling can be performed with autoregressive models, predicting a span given an appropriate instruction prompt. These models predict PM(xi = s|x-i), where s is a span of arbitrary length (i.e., len(s) \u2265 1). The predicted length is determined by the prediction of an end-of-sentence (EOS) token. As autoregressive models are trained to estimate the probability of completions, the probability of the token followed by EOS should match the MLM probability.\nAlthough we define the masked language modeling task regardless of the prediction model, following common convention, we will use the simple term MLM for models that were pretrained with the MLM objective. We will explicitly mention the use of autoregressive models.\nFor simplicity, we focus on the estimation of joint probabilities of two tokens. To neutralize the effect of word distances, we analyze probabilities"}, {"title": "3 Previous Work", "content": "Consistency. Few works have directly addressed the assessment of consistency. Among those that have, many focused on testing whether outputs adhere to predefined constraints. Li et al. (2019) and Ribeiro et al. (2019) demonstrated violations of logical constraints in question-answering. Elazar et al. (2021) evaluated the internal knowledge consistency of language models (LMs) comparing outputs for paraphrases of semantically identical questions. Pezeshkpour and Hruschka (2023) demonstrated sensitivity to answer ordering in multiple-choice questions. Qiu et al. (2023) illustrated inconsistent temporal reasoning in LLMs across various time-related tasks.\nIn our work, we focus on the consistency of probabilities rather than outputs. While identical probabilities imply identical outputs, the reverse is not necessarily true in language modeling, making our approach more sensitive to inconsistencies.\nCalibration. A common approach to assessing the quality of predicted probabilities is through calibration, which evaluates how well predicted probability scores align with membership probabilities in some reference data. In fully calibrated multi-class classifiers, calibration is considered for every class in every prediction. However, evaluating calibration, even in binned probabilities, becomes challenging with a large number of classes, making meaningful binning for every class with representative data difficult. To address this, many studies opt for top-class calibration (Guo et al., 2017), which focuses solely on calibrating the predicted class.\nAlthough top-class calibration is sufficient to assess the confidence of the prediction, and therefore is frequently used (Jiang et al., 2012; Guo et al., 2017), full calibration is an essential requirement for a model to be used as a density estimator in multi-class classification, in structured predictions such as sequence predictions in autoregressive text generation, and complex probabilistic generative models with textual components.\nWhile measuring full calibration directly is challenging, our approach, which compares the consistency of assigned probabilities to the same expression using different methods, offers alternative means to identify uncalibrated models \u2013 inconsistent estimations across different methods imply that at least one of them is miscalibrated.\nMany works measured the calibration of neural models (Guo et al., 2017; Wang et al., 2021), generally finding that neural models are poorly calibrated (Chen et al., 2023). In LMs, most prior work on calibration has focused on downstream tasks, such as classification and question answering (Desai and Durrett, 2020; Dan and Roth, 2021). Studies that specifically addressed language modeling typically restricted their evaluations to top predictions (Zhu et al., 2023), top-prediction sets (Ravfogel et al., 2023), or aggregate measures like entropy rates (Braverman et al., 2019). These evaluations have primarily examined autoregressive models, consistently finding them to be miscalibrated. While some research on masked language models (MLMs) has suggested that they tend to be relatively well-calibrated (He et al., 2023), to the best of our knowledge, full-distribution calibration in language modeling was never addressed.\nLanguage Models as Density Estimators. Several studies have interpreted language models as density estimators and explored their probabilistic properties from a theoretical standpoint. Hahn (2020) proved that some cases cannot be efficiently modeled by Transformers. Du et al. (2023) showed that the requirement that infinite length strings will have zero probability might not be held for all models. However, they defined a theoretical notion of tightness that is satisfied by most common models and guarantees the requirement. Wang and Cho (2019) showed that MLMs can be interpreted as Markov Random Fields, thus providing probabilities for entire sentences. In contrast, Yang et al. (2018) showed theoretically that decoding based on softmax yields low-rank approximations that are inadequate for capturing the complexity of language distribution. Additionally, Chang and McCallum (2022) presented findings indicating that decoding based on a single embedding vector cannot generate arbitrarily rich distributions.\nResearch investigating sampling-based text generation includes the work of Zhang et al. (2021) that showed that sampling from an LM distribution results in low-quality text, and that the use of temperature scaling provides a tradeoff between quality and diversity. Similarly, Holtzman et al. (2020) proposed nucleus sampling, avoiding the tail of the distribution, and Meister et al. (2023) presented a sampling scheme based on the expected entropy of natural language.\nSeveral studies found conflicts between calibration and zero-shot capabilities. Zhu et al. (2023) showed that instruction tuning significantly hurts calibration. Kalai and Vempala (2023) proved that strict calibration, with respect to the training data, must lead to hallucinations. Lee et al. (2020) showed a discrepancy between the cross-entropy loss, used for language modeling, and task-specific losses.\nOther work disregarded the probabilistic nature altogether, filling masks with spans from a reference document (Min et al., 2023). This comes with the price of losing qualities of probabilistic estimation."}, {"title": "4 Desired Properties of a Consistency Testing Framework", "content": "Our goal is to evaluate whether LLMs maintain consistency across different estimation orders when calculating the joint probability of a word span. To ensure the robustness and reliability of this evaluation, it must meet the following requirements.\nVersatility. For the designed framework to apply to various models, it should address both MLMs and autoregressive models. It should account for task comprehension in autoregressive models, which are not specifically built for filling masks.\nSignificance of discrepancies. Minor variations in estimated probabilities may arise due to numerical issues. Additionally, discrepancies that are symmetrically distributed around zero, lacking a clear bias, may not have significant implications. Therefore, the analysis should prioritize statistically significant discrepancies.\nNullifying the impact of exposure in training. To prevent bias from analyzing data examples used in model training, evaluations should incorporate natural datasets that were not part of the model's training set.\nExplainability. To be effective in both identifying and addressing inconsistencies, the framework should offer insights into contributing factors of found inconsistencies, such as model types, sizes (in parameters), and training data sizes. It should be able to isolate the effect of each factor, keeping the contribution of others fixed."}, {"title": "5 Consistency Testing Framework", "content": "We present the CONTESTS framework, designed to meet the requirements outlined in \u00a74. Here, we detail how each requirement is addressed."}, {"title": "5.1 Task Comprehension in Autoregressive Models", "content": "Autoregressive language models are typically trained for next-token prediction and not directly for Masked language modeling. However, the MLM task was previously formulated as a conditional case of autoregressive language modeling the T5 model (Raffel et al., 2020) was pretrained to generate text spans when given sentinel tokens in the input, and Bavarian et al. (2022) proposed training decoder-only models with a prompt for text infilling. This formulation allows us to derive the distribution of the missing span by estimating the sequence of next-token probabilities.\nSince autoregressive masked language modeling depends on task comprehension, we examine whether autoregressive models rank the true word sequence similarly to MLMs. Additionally, since these models allow for predictions of multiple tokens even when asked to fill one only, we analyze the scores assigned to EOS as the second token. A high probability for EOS as the second token is a positive indicator for understanding the task."}, {"title": "5.2 Testing Discrepancy Significance", "content": "Since large language models usually provide small probabilities, and due to the connection between the expressions for the joint to the expression for PMI (see Equation 3), here we examine the consistency of a given model by the discrepancy between estimations of the two expressions in log scale\ndi,i+1(Xi, Xi+1) := log Pi,i+1(Xi, Xi+1)\n- log Pi+1,i(Xi, Xi+1).\nFor each text x(i) and a pair of consecutive tokens xi, xi+1 we computed di,i+1(xi, xi+1). As discrepancies d(i) are functions of the random variables x(i), they follow an unknown distribution f. For a perfectly calibrated model, d(i) = 0 for all j, indicating a singleton mass of f at 0. In practice, the distribution induced by a model M is unknown and requires non-parametric treatment. Therefore, to test for the consistency of M, we employ the paired two-sided Wilcoxon rank test (Wilcoxon, 1945) to assess the null hypothesis that f is symmetric around 0. That is,\nHo: f is symmetric around \u03bc = 0\nH1: f is symmetric around \u03bc \u2260 0.\nTo examine these hypotheses, the Wilcoxon test employs the test statistic T = \u2211j sgn(d(i))R(j) where R(i) is the rank of d(i) (i.e., position in the sorted array) and sgn(d(i)) = 1 if d(i) > 0, -1 if d(i) < 0 and 0 otherwise. The reliance of the Wilcoxon test on ranks carries the following implications: (1) it demonstrates robustness to extreme discrepancy values, and (2) the focus on ranks, rather than the discrepancy values themselves, increases the difficulty of rejecting the null hypothesis, rendering it a conservative test.\nIn our analysis, we apply the Wilcoxon test to assess the significance of discrepancy means for multiple models across various text datasets. Given that testing a model across multiple datasets, and testing multiple models on the same dataset, increases the risk of type I error (the mistaken rejection of a true null hypothesis), we conduct a correction for"}, {"title": "5.3 Data Gathering", "content": "To eliminate biases caused by exposure to data in training, in addition to benchmark and synthetic datasets, we constructed a new dataset by extracting news articles with topics \"WORLD\u201d, \u201cNATION\u201d, \"BUSINESS\u201d, \u201cTECHNOLOGY\u201d, \u201cENTERTAINMENT", "SCIENCE": "SPORTS"}, {"title": "5.4 Testing for Contributing Factors", "content": "To analyze differences between different model types, while isolating factors such as their different number of parameters or training volume, we conducted a linear regression analysis. This approach is chosen because, in linear regression, a coefficient represents the change in the dependent variable associated with a one-unit change in the independent variable, with all other variables held fixed.\nFor all considered models 1,..., K, let dk(i) d represent discrepancy value computed for the k-th model and a text x(i). for 1 \u2264 j \u2264 J. We set the model type Tk to 1 if the model is autoregressive, and set Tk = 0 if it is an MLM. We investigate the influence of the model type, parameter size Sk (in billions of parameters), and the size of the training set (in GB) Vk on the variance of discrepancy\nvk(d) := Var(dk(1),..., dk(J)).\nIn this analysis, we consider the variance vk as the dependent variable, while the other parameters serve as explanatory variables:\n\u00fb\u03ba =\u03b2\u03bf + \u03b2\u2081Sk + \u03b22Vk + \u03b23Tk + \u1e9e4SkTk\nWhether a model adequately captures the variability of the explained variable, is often measured by 0 < R\u00b2 < 1. When the model is indeed well fitted (R2 close to 1), large and statistically significant coefficients indicate a substantial effect of the corresponding explanatory variables (in our case the effect of model size within specific types).\nFor an autoregressive model, the estimated variance is\n\u00fbk = (\u03b2\u03bf + \u03b23) + \u03b22Vk + (\u03b2\u2081 + \u03b24)Sk,"}, {"title": "6 Experimental Design", "content": "MLMs. We conducted experiments with the following MLMs, each in multiple sizes (parameters): RoBERTa (Liu et al., 2019) base (125m parameters) and large (355m); XLM-ROBERTa (Conneau et al., 2020) base (280m) and large (550m); we also used the generator component from ELECTRA (Clark et al., 2020) small (14m parameters in the generator) base (110m), and large (335m).\nMLMs are typically trained with randomly masked tokens, including the possibility of adjacent masks (Devlin et al., 2019). Consequently, the probabilities used in our experiments align with the model's training structure. For two adjacent masks, the predicted probabilities should represent the marginal distributions \u2013 i.e., the likelihood of each token being correct, given the uncertainty of the adjacent token. When a token is masked independently of adjacent tokens, the prediction scores should reflect the probability conditioned on the surrounding context.\nAutoregressive models. We performed experiments using Flan-T5 (Chung et al., 2022) small, base, large, xl, and xxl; LLAMA 2 and LLAMA 2-CHAT (Touvron et al., 2023) with 7b, 13b, and 70b parameters each. We used 4-bit quantization for all the autoregressive models, with nested quantization for the LLAMA 2 ones. These models suit our experiments as they include multiple sizes for the same architecture and settings. We tested various prompts but did not notice notable differences. Prompts for the reported results are in Appendix B.\nWe note that a natural option to test decoding order is to use T5's special tokens, putting them by order or in reverse. This method proved problematic, as T5 seems highly biased towards by-order completion, the format it was trained on. Reversing the order lowered the likelihood for all word pairs, with a stronger impact on high-probability pairs (e.g., common phrases), thus introducing a confounding reason for the discrepancy."}, {"title": "6.2 Data", "content": "Natural Text. We tested consistency over two datasets with texts from a natural source. The first was Wikitext-2 dataset, where we ignored punctuation, stop-words, and tokens that were not whole words. We used the train section, consisting of \u224837K articles. Since Wikitext was used during the training of the models, we constructed a new dataset as described in \u00a75.3 for four dates: 2.7.2023, 6.7.2023, 4.9.2023 and 18.9.2023, all well after the models' data cutoff. The texts were pre-processed in the same manner as in Wikitext. Altogether, the News dataset consists of \u22482000 articles.\nThe exact set of word pairs evaluated in each dataset differed between models as different model types have different tokenizers. Model size (for a given type) does not affect tokenization. Additionally, due to computational limitations, a smaller set was used for the larger models. In summary, for ROBERTa and ELECTRA-generator we had >200K word pairs in Wikitext and >85K in News, for XLM-ROBERTa\u2248110K in Wikitext and \u224844K in News, and for Flan-T5 \u2248175K in Wikitext and \u224855K in News. For LLAMA 2 (chat and non-chat) in Wikitext, we had \u224885K word pairs for the 7b and 13b versions and \u224813K for the 70b version, and in News, we had \u224811K for all sizes.\nSynthetic Data. We performed all experiments on synthetic data as well, in which the context is fixed for all samples. See Appendix D.1 for details."}, {"title": "7 Main Results", "content": "Here we provide an analysis of the distribution of discrepancy values d(i) for each examined model on real datasets. Analysis of the synthetic dataset is available in Appendix D.2.\nThe results of the experiments, summarized in Figure 2, show that on both Wikitext (2b) and News (2a) datasets, the distribution of discrepancies in MLMs is characterized by medians close to 0 and high variance, while autoregressive models exhibit discrepancies with medians further away from 0, but often lower variances.\nThe Wilcoxon test results on real datasets show that, except for Llama 2-chat-7b (p-value = 0.5997)"}, {"title": "7.2 Explaining Inconsistencies", "content": "The results shown in figures 2a, 2b indicate that in real data, given a model type, MLMs exhibit smaller variance as the number of parameters grows. Autoregressive models show an opposite trend. We performed a linear regression analysis (see equation 9) to test the significance of these trends, with the results summarized in table 1.\nThe results on both datasets indicate with significance level \u03b1 < 0.001 that the variance increases with the growth of the number of parameters. The size of the training set does not have a significant effect on the variance on the Wikitext dataset but influences the variance observed on the News dataset with a significance level of \u03b1 = 0.036.\nBoth regression models capture the variability of vk values with R\u00b2 values of 0.775 and 0.794 for the models fitted on the Wikitext and News datasets, respectively.\nIn appendix C we provide a similar analysis, where instead of considering model types as autoregressive or MLMs, we consider their fine-grained model types (RoBERTa, ELECTRA, etc.)."}, {"title": "8 Task Comprehension in Autoregressive Models", "content": "Our results show that autoregressive models tend to be less consistent, prompting the question of how much this inconsistency can be attributed to the more challenging task setting. To investigate this, we compare token ranks between MLMs and autoregressive models, where lower ranks indicate higher scores. Ideally, both models should show similar rank distributions, indicating similar performance. Our analysis, shown in Figure 3, indicates that while MLMs assign lower ranks to the true first token, Flan-T5, and LLAMA 2 models often assign even lower ranks to the second token, suggesting improved predictions. However, LLAMA 2-CHAT models exhibit significantly higher ranks, indicating poorer performance.\nIn Appendix E, we provide a detailed analysis of probabilities assigned to an EOS token following the predicted missing word, as an additional measure of task comprehension. We find a positive correlation between models excelling in word rank predictions and those showing good task comprehension (indicated by lower EOS ranks). However, the poor performance of LLAMA 2-CHAT models cannot be solely attributed to a lack of task understanding, as they assign low ranks to EOS but high ranks to actual words. Across all model types, larger models in each category generally exhibit better task comprehension."}, {"title": "9 Are there Preferable Decoding Orders?", "content": "We showed that all examined models exhibit inconsistencies between different orders of estimation of joint probabilities of word spans. This raises the question of whether any of the examined completion orders yields higher scores for the true tokens.\nTo address this question, we analyze the correlation between di,i+1 and the entropies of the estimated probabilities involved in the joint probability estimations, which we denote with Hi, Hi+1i, Hi+1, Hi|i+1. A summary of the correlations is shown in Figure 4.\nThe analysis reveals that: (1) for entropies of predictions when two tokens are masked (i.e., Hi, Hi+1) the correlation with the discrepancy in the corresponding order (di,i+1, -di,i+1) is negative; (2) for one-mask entropies (Hi|i+1, Hi+1|i) the correlation is positive; (3) one-mask entropies exhibit stronger (in absolute value) correlations with discrepancy; and (4) correlations between entropies and discrepancy are stronger (in absolute terms) for MLMs compared to autoregressive models.\nThese results suggest that selecting the direction with higher entropy for one-mask prediction and lower entropy for two masks is likely to increase the likelihood of true tokens. In Appendix F we provide examples of the effect of decoding order."}, {"title": "10 Discussion", "content": "In this work, we investigated the probabilistic consistency of language models (LMs) and introduced a novel framework to quantify and explain discrepancies between equivalent estimation orders.\nOur findings indicate statistically significant inconsistencies among 16 out of 18 models on the News dataset and all 18 models on the Wikitext dataset. These results highlight significant differences between MLMs and autoregressive models, with the latter showing considerably larger inconsistencies and discrepancy variances.\nThe comparable discrepancy distributions between the News and Wikitext datasets suggest that exposure in training has little effect on consistency. In contrast, results from the synthetic dataset (see Appendix D.2) reveal a different pattern, with higher mean and variance in discrepancies, indicating reduced consistency on low likelihood data.\nAnalysis of the relationship between discrepancy variances and model size reveals significant trends only in Flan-T5 models, displaying a negative correlation. Across all model types, we found that larger models exhibit lower average prediction entropies. This suggests that artificially high consistency in autoregressive models may arise from high variance. However, this does not hold for MLMs, which show the opposite trend.\nA positive correlation between real data likelihood and overall entropy is termed overconfidence, while a negative correlation is called underconfidence (Ravfogel et al., 2023). Our experiments suggest that choosing a decoding direction with higher entropy for single-mask predictions and lower entropy for two masks is expected to yield higher estimated probabilities for the correct word pair. This suggests that overconfidence and underconfidence are influenced by the model type and the number of masked tokens, challenging the anticipated strong link between average likelihood and entropy. Consequently, this calls for cautious application of methods to address overconfidence and underconfidence, as models can exhibit both.\nIn conclusion, our investigation into the probabilistic consistency of language models has revealed significant inconsistencies across various model types and datasets. Our findings highlight the need for careful interpretation and application of the predicted scores.\nPrior research (Yang et al., 2018) argued that modeling the \u201cdistribution of language\" is a complex task. Our findings provide further support for this claim, showing that even high-performing models struggle to generate consistent estimations.\nRobust ranking of structured predictions and similar applications often require consistency in joint probability estimation. However, our research shows that achieving high performance does not necessarily depend on consistency, and vice versa: a model can be perfectly consistent but make inaccurate predictions. Therefore, a combination of consistency testing with other performance metrics is essential for thorough evaluation.\nOur analysis exposes inconsistencies and their links to prediction entropies and model-type disparities, offering valuable insights for future research to tackle these limitations effectively.\""}, {"title": "Limitations", "content": "Our framework is designed for comparing any two estimations of a joint distribution. However, our experiments specifically target a setting with two adjacent tokens. While effective in revealing inconsistencies, exploring additional estimation orders with longer word spans in future research could uncover additional ones.\nA limitation of the analysis of the dependency of discrepancy variances on model sizes lies in its treatment of each model as an observation, resulting in small sample sizes (7 MLMs, and 11 autoregressive models).\nWe also note that comparison between different model types is qualitative only, as they differ in their tokenization and other qualities (such as the scale of probabilities and entropies). In addition, due to the load of computation, in some cases, the sample sizes were relatively small."}, {"title": "Ethics Statement", "content": "Wikitext-2 is released under license CC BY-SA 4.0. The dataset of news articles was gathered from various new sites through Google News. The dataset was used for validation purposes only and will not be redistributed or used for training models. A list of URLs and access dates will be provided upon publication."}, {"title": "A Examples", "content": "A.1 Inconsistent Joint distribution from Conditionals\nConsider a (toy) example, where we have a two-word sentence X1,X2, over a yes-no alphabet {y, n}, and assume we have estimations P(x1 = y|x2 = y) = 0.9, P(x2 = y) = 0.9 and P(x2 = y|x1 = y) = 0.1, P(x1 = y) = 0.1. This leads to a contradiction, since\n0.81 = P(w\u2081 = y|w\u2082 = y) \u00b7 P(W2 = y)\n= P(w\u2081 = y, w2 = y)\n= P(w2 = y|W\u2081 = y) \u00b7 P(w\u2081 = y) 0.01.\nA.2 Consistent Joint distribution Without Structural Restrictions\nAs an example, consider a masked language model that is trained by maximum likelihood estimation with (2n + 1)-grams. Since all conditionals are determined by the counts of the (2n+1)-gram samples, any set of resulting conditionals must comply with the (2n + 1)-gram joint distribution."}, {"title": "B Prompts For Instruction Models", "content": "The prompt we used is:\nYou will be given a passage with one masked token that you should fill in. We denote this token by %. The passage might also contain corrupted tokens denoted by @. You are not expected to fill in corrupted tokens - fill only the masked one. Your answer should include the filled-in token only with no extra explanations or context.\nFor Flan-t5 and LLAMA 2 (non-chat version), the input format was:\nPassage: <passage with masks>\nAnswer:\nFor LLAMA 2-CHAT the format was:\n[INST] <SYS>\n<prompt>\n</SYS>\nPassage:"}, {"title": "C Explaining Inconsistencies With Model Types", "content": "In Table 2 we present the results of linear regression analysis with fine-grained model types. In this setting, we regard each model type (RoBERTa, ELECTRA, etc.) as a separate case:\n\u03bd\u03ba =\u03b2\u03bf + \u03b2\u2081Sk+\n+ \u03b221Tk=1 + \u00b7\u00b7\u00b7 + \u1e9et1Tk=t-1\n+ \u1e9et+1Sk\u00b7 1Tk=1 + \u00b7\u00b7\u00b7 + \u1e9e2tSk\u00b7 1Tk=t-1\nwhere Sk is the size (in billions of parameters) of model Mk of type Tk, and 1 is the indicator function.\nAs before, whether the model adequately captures the variability of the explained variable (in our case, the variability of vk), is measured by 0 \u2264 R2 \u2264 1, and large statistically significant coefficients indicate a substantial effect of the corresponding explanatory variables.\nTo avoid multicollinearity, the estimator of the variance includes t 1 model types. For a model of type 1, the estimated variance is\n\u00fbk = (\u03b2\u03bf + \u03b22) + (\u03b2\u2081 + \u1e9et+1)Sk,"}, {"title": "D Experiments with Synthetic Data", "content": "D.1 Data Generation\nIn addition to real datasets, we conducted tests on an automatically generated dataset, which allowed us the ability to control the context and manipulate the occurrence of lower probability tokens.\nWe used the template:\n[MASK1] [MASK2] is a thing\nand filled in the masks with predetermined spans. Noun phrases were extracted from fiction data using SpaCy, and filtered to those that consist of 2 words. Additionally, we used on phrases in which each word is a single token in all the models we tested with. The final dataset consists of 10K samples. Sentences in this dataset are not meaningful but are mostly grammatical and resemble real sentences.\nWhile we lack expectations about the specific probability values in synthetic data, which can be arbitrarily low, the model is expected to produce consistent probabilities."}, {"title": "E Additional Results on Task Comprehension in Autoregressive Models", "content": "In autoregressive models, even when instructed to predict a single word (assuming single-token words), the prediction necessarily consists of at least two tokens: the predicted word and the EOS token.. The EOS score provides valuable information, as a probability less than 1 indicates that some probability mass was allocated to spans longer than one token. Therefore, it can be used to assess how well the instruction was \u201cunderstood\" by the model, thereby aiding in distinguishing cases where poor predictions arise from a failure to comprehend the task \u2013 a challenge absent in classic MLMs where the infilling task is inherent in the architecture.\nPrevious work examines model consistencies through paraphrasing and logical dependencies (see\""}, {"title": "F Examples for The Effect of Decoding Order", "content": "To demonstrate the impact of the suggested decoding order on completion quality", "datasets": "a case where the decoding order notably affects the outcome", "Hz+1|i-Hili+1+Hi+1-H\u2081": "Large \u0394H values suggest that decoding in the suggested order will yield higher probabilities for true tokens", "observed": "nwith former master's students and co-first\n Pai\nThe true masked words are \"authors James\".\nThe top 10 completions ranked by joint probability", "are": "n1. with former master's students and co-first year student\nPai\n2. with former master's students and co-first year students\nPai\n3. with former master's students and co-first president\nKen Pai\n4. with former master's students and co-first president\nMichael Pai\n5. with former master's students and co-first chair Ken\nPai\n6. with former master's students and co-first lady Ken Pai\n7. with former master's students and co-first president\nPatrick Pai\n8. with former master"}]}