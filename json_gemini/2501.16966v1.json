{"title": "Heterogeneity-aware Personalized Federated Learning via Adaptive Dual-Agent Reinforcement Learning", "authors": ["Xi Chen", "Qin Li", "Haibin Cai", "Ting Wang"], "abstract": "Federated Learning (FL) empowers multiple clients to collaboratively train machine learning models without sharing local data, making it highly applicable in heterogeneous Internet of Things (IoT) environments. However, intrinsic heterogeneity in clients' model architectures and computing capabilities often results in model accuracy loss and the intractable straggler problem, which significantly impairs training effectiveness. To tackle these challenges, this paper proposes a novel Heterogeneity-aware Personalized Federated Learning method, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms. HAPFL optimizes the training process by incorporating three strategic components: 1) An RL-based heterogeneous model allocation mechanism. The parameter server employs a Proximal Policy Optimization (PPO)-based RL agent to adaptively allocate appropriately sized, differentiated models to clients based on their performance, effectively mitigating performance disparities. 2) An RL-based training intensity adjustment scheme. The parameter server leverages another PPO-based RL agent to dynamically fine-tune the training intensity for each client to further enhance training efficiency and reduce straggling latency. 3) A knowledge distillation-based mutual learning mechanism. Each client deploys both a heterogeneous local model and a homogeneous lightweight model named LiteModel, where these models undergo mutual learning through knowledge distillation. This uniform LiteModel plays a pivotal role in aggregating and sharing global knowledge, significantly enhancing the effectiveness of personalized local training. Experimental results across multiple benchmark datasets demonstrate that HAPFL not only achieves high accuracy but also substantially reduces the overall training time by 20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to existing solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "With the continuous advancement of Internet of Things (IoT) technology [1], the deployment of IoT devices is experiencing unprecedented growth. As of 2021, there are already over 10 billion active IoT devices worldwide, and this number is expected to exceed 25.4 billion by 2030. By 2025, an estimated 152,200 IoT devices will be connecting to the Internet every minute, contributing to a projected increase in data generation to 73.1 Zettabytes by that year a dramatic 422% rise from the 17.3 Zettabytes recorded in 2019. Traditionally, such vast amounts of data have been managed through centralized learning (CL) methods [2] [3], which aggregate data to train or fine-tune models. Although CL is effective for developing high-precision models and straightforward to implement, it relies heavily on centralized data collection. This dependency raises substantial privacy and security concerns, particularly as IoT devices often hold sensitive information, such as medical records containing patients' private health data or personal sensitive banking details. Data owners' reluctance to share such data compromises the feasibility of CL for practical applications. Moreover, the CL model, necessitating extensive data collection, incurs significant communication costs, which make it impractical for resource-constrained devices operating in bandwidth-limited edge wireless networks, thereby restricting the scalability and applicability of the CL approach in the rapidly evolving IoT environment.\nFederated Learning (FL) embodies a collaborative learning paradigm, specifically designed to address privacy concerns. This approach utilizes a distributed learning algorithm that allows clients to independently process local datasets without the necessity to exchange raw data [4]. In FL system, the server distributes a global model to each participating client, who then utilizes their unique local dataset to train a local model. Upon completion of training, clients transmit their model parameters to the server, which aggregates these contributions to refine the global model. This strategy ensures that the global model can effectively leverage the collective data from all participating clients without compromising individual data privacy, thus making FL highly practical and broadly applicable across various data-sensitive sectors.\nDespite its vast potential, FL encounters significant implementation challenges that hinder its widespread adoption. In addition to managing data privacy, one primary issue in FL arises from the computational and communication efficiency constraints that occur within networks of heterogeneous devices [5] [6] [7]. In synchronous FL systems, performance disparity-defined here as any factors that impact client training in a given round and contribute to training latency-can include specific elements such as computational resources, dataset size, and model size. This disparity often leads to \"straggler problems,\" where high-performance devices must wait for slower devices to complete their tasks before proceeding to aggregation. This delay can severely impact resource utilization and model efficiency, underscoring the need for methods that can dynamically adapt to the varied capacities of different clients.\nIn synchronous FL, the disparity in performance among heterogeneous devices often causes \"straggler problems\", where high-performance devices are delayed as they must wait for lower-performance devices to complete training and data upload before aggregation can proceed [8] [9]. One common strategy to mitigate this issue and reduce straggling latency involves allocating different training intensities based on client performance, where \u201ctraining intensity\" in this context refers to varying the number of training epochs assigned to each client during federated training rounds. However, while this approach can enhance the efficiency of high-performance devices, it also risks exacerbating disparities. Specifically, allocating too many epochs to high-performance devices and too few to lower-performance ones can lead to an imbalance, which might not only decrease the accuracy of the aggregated model due to under-representation of data from less capable devices but also result in suboptimal utilization of all client data across the network.\nWhile Semi-FL [10] and Asyn-FL [11] approaches have been proposed to address training latency in federated learning, each comes with notable limitations. Asyn-FL improves scalability and efficiency by allowing clients to update models asynchronously, yet it often struggles with stability in model convergence, especially in environments with numerous nodes or significant computational delays. Similarly, Semi-FL seeks a balance between global consistency and performance efficiency by introducing a delay window for client updates. However, its effectiveness depends heavily on carefully tuning this window, which complicates deployment in large-scale systems.\nTo address the challenges outlined above, this paper introduces a novel adaptive Heterogeneity-aware Personalized Federated Learning method, named HAPFL. HAPFL leverages two deep reinforcement learning (DRL) agents to dynamically allocate appropriately sized heterogeneous models and tailored training intensities to clients based on their performance capabilities, respectively. Furthermore, to tackle the obstacles posed by model heterogeneity during global model aggregation, each client is additionally equipped with a homogeneous lightweight model, named LiteModel. This setup allows each client to train its personalized local model under intensities specified by the reinforcement learning (RL) agent, while the LiteModel engages in mutual learning with the local model through knowledge distillation [12]. These well-learned homogeneous LiteModels are then used to perform global model aggregation. This dual-model approach not only facilitates efficient and effective global model aggregation but also enhances the local model with insights gained from the LiteModel, which accumulates and transfers knowledge across all clients. Moreover, to mitigate the adverse impacts of low-contributing clients on the accuracy and stability of the global model, this paper proposes a novel model aggregation method using information entropy and accuracy weighting. In summary, this paper makes the following three key contributions:\n1) We propose a novel heterogeneity-aware personalized federated learning framework, named HAPFL, that leverages two functional RL agents. These two agents are respectively designed to adaptively determine appropriately sized heterogeneous models for each client and dynamically adjust each client's training intensities based on their computing capabilities and performance, aiming to effectively mitigate the serious straggler problem and reduce the straggling latency.\n2) We introduce a lightweight homogeneous model called LiteModel, which is deployed on each client. This LiteModel and the corresponding local model on each client engage in continuous knowledge transfer through knowledge distillation-based mutual learning. The Lite-Model serves as a universally consistent model designed to aggregate and distribute global knowledge, thereby facilitating local training processes and effectively tackling the challenges associated with heterogeneous models.\n3) We develop a prototype of HAPFL and conduct extensive simulations to evaluate its performance on three well-known datasets: MNIST, CIFAR-10 and ImageNet-10. Experimental results demonstrate that our HAPFL approach significantly outperforms baseline methods, improving model accuracy by up to 7.3%, reducing overall training time by 20.9% to 40.4%, and decreasing straggling latency differences by 19.0% to 48.0%.\nThe remainder of this paper is organized as follows. Section II briefly reviews the related works. Section III presents the system modeling and problem formulation. Section IV elaborates on the design of our HAPFL approach. Section V provides experimental results. Section VI concludes this paper."}, {"title": "II. RELATED WORK", "content": "This section briefly reviews some related work in FL and discusses the limitations of these methods.\nThe most widely recognized algorithm in FL is the Federated Average (FedAvg) [13] proposed by Google. However, FedAvg is built upon assumptions of independent and identically distributed (IID) data, uniform model architectures, and reliable network connections. In practice, FedAvg struggles with non-IID data and client performance heterogeneity, which is common in real-world scenarios [14]. To address the issue of heterogeneous client performance, various methods have been proposed. Lei Yang et al. [15] utilized a clustering approach that groups clients based on performance, allocating the same model to each cluster and aggregating the different models on the server side. Ruixuan Liu et al. [16] introduced a hierarchical heterogeneous aggregation framework that assigns different-sized models to clients based on their computing capabilities, with a hierarchical aggregation of global models. Jun Xia et al. [17] adopted lightweight modellets and local models of varying sizes, training them through Deep Mutual Learning and aggregating all modellets on the server side for knowledge sharing. Yae Jee Cho et al. [13] tackled model heterogeneity by passing soft labels of local models, clustering soft labels on the server side, and training local models through knowledge distillation. Jianyu Wang et al. [18] proposed a method to address the objective inconsistency problem in heterogeneous federated optimization. By normalizing the model updates from clients, FedNova ensures comparability of updates from clients with diverse data distributions and computational capacities, enhancing convergence stability in federated learning settings with significant client heterogeneity. While these methods improve client performance utilization, they rely on pre-allocated fixed models and training intensities. In dynamic client groups with significant performance disparities, these approaches may still result in high straggler latency.\nPersonalized Federated Learning (PFL) aims to address client heterogeneity by tailoring models to individual clients. For example, Smith et al. [19] proposed Federated Multi-task Learning, where different tasks were learned across clients. Muhammad et al. [20] introduced Fedfast, a framework designed to accelerate federated recommender system training by allocating different training intensities to clients based on their capabilities, which reduces latency and improves efficiency. Li et al. [21] developed Ditto, which used personalized aggregation schemes to improve fairness and robustness in federated settings. Similarly, Fallah et al. [22] leveraged meta-learning techniques to personalize the model for each client. These efforts aim to account for individual differences across clients by customizing models based on local data and local conditions. However, despite the success of these methods in accommodating client heterogeneity, they do not directly address the issue of client latency and the associated straggler problem. In dynamic environments where clients experience varying latencies and computational capabilities, these personalized approaches may still suffer from inefficiencies, as clients with slower performance can cause delays in the overall training process, leading to high straggler latency.\nRecent advancements in PFL have proposed methods that go beyond basic personalization. For example, Deng et al. [23] introduced FedASA, which adapts model aggregation to account for heterogeneous mobile edge computing environments, addressing some aspects of latency and model convergence. Lee et al. [24] proposed FedL2P, which personalizes federated learning by utilizing model regularization techniques to handle data heterogeneity. Yang et al. [25] further extended these ideas by developing FedAS, a method that bridges inconsistencies in personalized federated learning. These methods, while advancing PFL, still do not fully tackle the latency issues faced by clients with significantly varying performance capabilities. In these scenarios, where high-performance clients finish training faster than low-performance ones, the straggler problem persists.\nMoreover, in FL, clients' training intensity is mostly uniform. However, in synchronous FL, high-performance clients completing training more quickly than lower-performance ones exacerbate the \"straggler problem,\" where faster clients must wait for slower ones before global aggregation can proceed. Although asynchronous FL [26] [27] [28] presents advantages in addressing the straggler problem, it also poses risks of model quality degradation and server crashes. Several approaches have been proposed to address this issue. Tianming Zang et al. [29] implemented a clustering method based on upload or communication time [30], ensuring that clients with similar time metrics are grouped together. In each training round, clients within the same group are selected for training, thereby reducing waiting time and mitigating the straggler problem. Yangguang Cui et al. [31] introduced a utility-driven and heterogeneity-aware heuristic user selection strategy. By considering client performance and heterogeneity, this approach optimizes client selection to minimize straggling latency. Peichun Li et al. [32] proposed on-demand learning, which adjusts the local model structure, gradient compression strategy, and computational frequency according to personalized latency and energy constraints.\nAlthough these methods effectively reduce straggling latency, they may face challenges in adapting to complex and dynamic environments. Further research is needed to develop robust and adaptable solutions for addressing the straggler problem in FL effectively. In recent years, DRL has emerged as a powerful tool for solving complex sequence decision optimization problems [33]. Given that FL can be conceptualized as a Markov decision process (MDP), DRL presents a promising approach for optimizing FL. Building on the success of DRL in FL optimization, several studies have applied DRL to address FL resource allocation challenges. To tackle the straggler problem in FL, Manying Zeng et al. [34] utilized a DRL model to dynamically adjust local training intensity. By adaptively modifying the training intensity based on local conditions, this approach effectively reduces straggling latency, enhancing overall FL performance. Similarly, Yi Jie Wong et al. [35] employed a DRL model for client selection and allocation of training intensity. By leveraging DRL, clients are intelligently selected and assigned varying levels of training intensity, thereby optimizing FL resource allocation and further mitigating the straggler problem. While these methods have shown effectiveness in reducing straggling latency, they face challenges when dealing with client groups exhibiting large performance disparities. Extreme resource allocation in such scenarios could potentially lead to performance degradation of the global model. When performance differences among clients are significant, allocating extensive training intensity to high-performance clients while providing minimal training intensity to lower-performance clients can create extreme imbalances in training. This imbalance may lead to the global model's performance degradation, as the limited training experienced by lower-performance clients can result in insufficient learning and poor model convergence. Thus, further research is essential to develop robust and adaptive DRL-based approaches that can effectively handle varying performance levels among client groups in FL settings.\nIn this context, in this paper, we propose a multi-level DRL-based personalized FL framework that incorporates two DRL agents to adaptively determine the client model sizes and training intensities for each client, respectively, and introduce a homogeneous lightweight LiteModel for global model aggregation enhanced by knowledge distillation-based mutual learning to address the model heterogeneity issues."}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "This section describes our HAPFL system model, mainly consisting of five phases, and the problem formulation that will be addressed in this paper."}, {"title": "A. System Model", "content": "Consider a FL system comprising K devices, each capable of participating in the learning process. In each communication round r, a subset of k clients is randomly selected from the total pool of K clients. Specifically, each round includes the following five phases:\nPerformance Assessment Training: Each client performs training with the LiteModel $w_{i}^{lite}$ to evaluate their current computational capabilities. The resulting data, which reflects each client's performance capability, is then sent back to the server. This step is crucial as it provides the server with each client's real-time processing power and readiness, enabling the server to make better decisions about allocations of model sizes and training intensities in subsequent phases.\n$P_{r,i}=f_{eval}(w_{i}^{lite};D_{i}),$ (1)\nwhere $P_{r,i}$ denotes the performance metric of client i at round r, $f_{eval}$ is the assessment function, and $D_{i}$ represents the local dataset of client i.\nAdaptive Training Adjustment: The server employs two distinct Proximal Policy Optimization (PPO)-based RL agents. Utilizing the performance assessment information Pr received from clients, these two agents generate two separate policies: $\\pi_{s}$ and $\\pi_{E}$. The first policy $\\pi_{s}$ determines the optimal size $s_{size}$ of each client's local model, while the second policy $\\pi_{E}$ adjusts the training intensities, specifically the number of training epochs $E_{r}$. These tailored configurations are then communicated back to the clients for implementation.\n$\\pi_{s}: P_{r} \\rightarrow s_{size};$\n$\\pi_{E}: P_{r} \\rightarrow E_{r}.$\n(2)\nThese training adjustments are specifically tailored based on the latest insights derived from RL agents, using the current performance metrics of clients. This ensures that all clients perform local training with models and training intensities optimally suited to their distinct capacities.\nModel Distribution: After making adaptive training adjustments with two RL agents in round r, the server dispatches the most recent global LiteModel $w^{lite}$, along with the corresponding aggregated differentiated local models $w_{i}^{local}$, to the participating clients based on the adjustment information. This ensures that all clients start their local training processes with the most updated models, maintaining consistency across the learning network.\nLocal Training and Mutual Learning: Clients initialize or update their LiteModels $w^{lite}$ and local models $w_{i}^{local}$, as well as the assigned number of training epochs $E_{r,i}$, according to the decisions issued by the PPO agents. Subsequently, both the LiteModel $w^{lite}$ and the local model $w_{i}^{local}$ engage in local training using local datasets $D_{i}$ while simultaneously participating in mutual learning through knowledge distillation. After the training phase concludes, clients upload their newly updated local model and LiteModel back to the server. The model is updated using the stochastic gradient descent algorithm, as follows:\n$w_{r+1,i}=w_{r,i} - \\eta \\nabla L_{i}(w_{r,i}, D_{i}),$ (3)\nwhere $\\eta$ is the learning rate and $L_{i}$ is the loss function.\nGlobal Model Aggregation: The server aggregates the LiteModels $\\{w_{i}^{lite}\\}_{i=1}^{k}$ received from all clients to update the global LiteModel:\n$w_{r+1}^{lite}=\\frac{1}{k}\\sum_{i=1}^{k}w_{r+1,i}^{lite},$ (4)\nAdditionally, it organizes the variously sized local models into several groups based on their sizes and aggregates the same-sized models within each group separately:\n$w_{r+1}^{(size)}=\\frac{1}{|G_{r+1}^{(size)}|}\\sum_{w_{r+1,i}^{local} \\in G_{r+1}^{(size)}}w_{r+1,i}^{local},$ (5)\nwhere $G_{r+1}^{(size)}$ is the group of clients with the same model size size. This dual model aggregation process prepares them for the next round of each client's model initialization, ensuring that updates are efficiently tailored to each client's diverse capabilities and configurations.\nThis improved FL system facilitates collaborative training of heterogeneity-aware personalized models across distributed heterogeneous devices while preserving data privacy. The adaptive training strategies ensure that each client contributes effectively to the collective learning process while realizing each client model's personalization. Notably, the incorporation of LiteModels facilitates a dual-learning mechanism that boosts not only personalized local model performance through knowledge distillation but also enhances global model accuracy through knowledge transfer and systematic aggregation."}, {"title": "B. Problem Formulation", "content": "In a typical FL system comprising a server and K clients, each client i \u2208 K maintains a local dataset $D_{i}$ of size $D_{i}$. The complete FL training process involves several key phases: model distribution, local training, and model aggregation. The total time required for these processes in each communication round r can be approximated as:\n$T_{r,i}=T_{b}^{r}+T_{d}^{r,i}+T_{u}^{r,i}+T_{a}^{r}$ (6)\nwhere $T_{b}^{r}$ represents the time needed for broadcasting the model to the clients, $T_{d}^{r,i}$ means the computation time at the client, $T_{u}^{r,i}$ indicates the time spent on model uploading, and $T_{a}^{r}$ denotes the model aggregation time at the server.\nHowever, the heterogeneity in client capabilities and dataset sizes often results in significant variations in local computation times among clients, leading to significant straggling latency in overall system performance. These straggling latencies are predominantly due to the local computation time, which often exceeds the combined time for model distribution, uploading, and aggregation.\nLocal computation comprises two components: performance assessment training and local training. Performance assessment training involves one epoch of local training with the client's LiteModel to evaluate the client's computational capacity, with training time denoted as $T_{d}^{a,i}$. Local training encompasses one round of training using both the LiteModel and local model on the client side, with training time denoted as $T_{d}^{l,i}$. The total time cost for client i in round r is given by:\n$T_{d}^{r,i}=T_{d}^{a,i}+T_{d}^{l,i}$ (7)\nIn FL, the server synchronizes model aggregation after all clients complete their training. Straggling latencies are exacerbated when faster clients must wait for the slower ones. Defining $S_{r}$ as the set of participating clients in the r-th training round, with j and j' denoting the slowest and fastest clients, respectively, the straggling latency in the r-th round can be expressed as:\n$\\Delta T^{r} = T_{d}^{r,j} - T_{d}^{r,j'}, j \\neq j', j \\in S_{r}, j' \\in S_{r},$\n$T_{d}^{r,i} = T_{d}^{a,i}+T_{d}^{l,i}$\n$T_{d}^{l,i} = f(D_{i}, w_{i}^{(size)}, \\psi_{i}),$ (10)\nwhere $T_{d}^{l,i}$ is the average time required for a single epoch of training on client i. This time depends on the client's hardware capabilities, dataset size $D_{i}$, and the size of the allocated model $w_{i}^{(size)}. f(.)$ is a function capturing the client's computational performance, and $\\psi_{i}$ represents the hardware and resource constraints of client i.\nSuch straggling latency primarily results from differences in client hardware performance and dataset sizes. To minimize these latencies, it is effective to allocate models of varying sizes and assign different training intensities based on client performance, which can significantly reduce discrepancies in training times. Let $T_{r,i}$ represent the assigned training intensity for client i in round r, with the objective to minimize $\\Delta T$:\n$\\min \\Delta T = \\min (T_{d}^{r,j}-T_{d}^{r,j'}),$ (11)\nSubject to\n$\\sum T_{r,i} = T_{r}$ (12)\n$\\epsilon \\in S_{r}$\n$T_{r,i}>0$ (13)\n$U(w_{r,i}^{(size)}) \\geq \\epsilon$ (14)\nEquation (12) ensures that the total training intensity allocated to each client in round r is equivalent to a predefined total $T_{r}$. Equation (13) guarantees that every participating client completes at least one local training iteration. Equation (14) indicates that the parameter count of the allocatable model exceeds the minimum threshold required for the current task, ensuring that the model can effectively fulfill the training task. Here, $U(.)$ represents the function for computing the model parameter count."}, {"title": "IV. HAPFL APPROACH DESIGN", "content": "In this section, we will elaborate on the design of the HAPFL's architecture and detail the specific algorithms employed within this framework."}, {"title": "A. Overview of HAPFL Architecture", "content": "The overall architecture of the HAPFL method is illustrated in Figure 1. In each round of federated training, clients first submit requests to participate in a round of federated training. After receiving these requests, the server selects appropriate clients. These selected clients conduct the performance assessment training based on the LiteModel and send the resulting data back to the server. Utilizing this assessment information, the server then performs adaptive training adjustments through an RL-based heterogeneous model allocation mechanism and an RL-based training intensity adjustment scheme. These adjustments are designed to optimally balance each client's unique capabilities and overall training efficiency. The server then sends the LiteModel, heterogeneous local models, and the training adjustment information back to the participating clients. After receiving local models with assigned training intensities, clients undertake local training using a knowledge distillation-based mutual learning mechanism to enhance learning efficiency. Upon completing their training, clients upload the training information to the server, including model parameters, the accuracy of both the local model and Lite-Model, the associated training time costs, and the information entropy of the client dataset. The server separately aggregates the collected LiteModels and heterogeneous models using a weighted aggregation method that considers both information entropy and model accuracy. Simultaneously, the server calculates RL rewards based on the collected training information and stores the (states, actions, rewards) tuples in a memory buffer. Once the memory buffer is full, it triggers the update of two RL models, continuously refining the training process. This cycle ensures that the HAPFL system dynamically adapts to the evolving needs and capabilities of its diverse clients."}, {"title": "B. Performance Assessment Training", "content": "In a real dynamic FL environment, client participation and its available computational resources can vary significantly from round to round. To effectively manage this variability, performance assessment training on each client is employed to provide the server-side RL agents with current computational capabilities of all involved clients. During the assessment training, clients conduct a brief training with their LiteModel for one epoch, facilitating rapid assessment without significant computational overhead while providing valuable insights into client performance. Upone completing this initial training, clients promptly upload the resulting assessment training time cost $T_{d}^{a,i}$ to the server. This information is crucial for server's RL agents to make informed decisions about model sizes and training intensities for each client in subsequent training rounds, ensuring that the system continuously adapts to the changing dynamics of the client pool and their capabilities."}, {"title": "C. Dual-Agent RL for FL Optimization", "content": "Allocating uniform training intensity to all clients with significant performance differences may suffer from the straggler problem, resulting in high straggling latency. However, relying solely on adjusting training intensities may lead to imbalanced and inadequate training for some clients, which often results in under-representation of data from lower-performance clients, thereby decreasing the overall accuracy of the model. To address these challenges, HAPFL employs a dual-agent DRL framework, which makes adaptive training adjustments by collaboratively leveraging an RL-based heterogeneous model allocation mechanism and an RL-based training intensity adjustment scheme. The former RL agent dynamically allocates different model sizes to clients based on their capabilities, aiming to mitigate performance discrepancies. The other RL agent further tailors the training intensities for each client, optimizing the balance between reducing straggling latency and ensuring sufficient training depth.\n1) RL-based Heterogeneous Model Allocation Mechanism: To mitigate the significant performance differences among clients, one common intuitive strategy is to allocate larger, more capable models to clients with superior performance. This enables them to achieve better performance, enhanced generalization capabilities, and the ability to handle more complex tasks. Building on this strategy, the HAPFL method incorporates an RL-based heterogeneous model allocation mechanism using the PPO model, abbreviated as PPO1. PPO1 utilizes a multi-head RL model architecture specifically designed for model allocation, enabling the mechanism to dynamically adjust model sizes based on each client's capabilities. The primary goal is to minimize performance disparities among clients and avoid the potential imbalances that may arise from varying training intensities alone.\nState space: In each round of federated training, the server collects the training time costs $T_{d}^{r,i}$ from the performance assessments of participating clients. This data, utilized as the input for the PPO1 model, can be expressed as follows:\n$T_{d}^{r} = \\{T_{d}^{a,i} \\in S_{r}\\} $ (15)\nTo effectively capture performance differences among clients, the collected assessment training time costs are normalized as:\n$T_{i}' = \\frac{T_{i}}{min(T_{d}^{r})}$ (16)\nThus, the state space $S_{r}$ of PPO1 in round r is defined as:\n$S_{r} = \\{T_{i}' \\in S_{r}\\} $ (17)\nAction space: The primary objective of PPO1 for model allocation is to balance performance differences among clients by dynamically assigning models of varying sizes. As such, the output of PPO1 determines the specific model category, reflecting different sizes, allocated to each participating client. The action space $A_{r}$ of PPO1 for this decision-making process in round r can be expressed as follows:\n$A_{r} = \\{a_{i} \\in S_{r}\\} $ (18)\n$\\alpha_{i} = \\{1, 2, ... , \\delta \\}, $ (19)\nwhere $a_{i}$ represents the model category assigned to the respective client i in the r-th round, and $\\delta$ represents the total number of model categories. Each category corresponds to a different model size, allowing PPO1 to facilitate heterogeneous training according to the individual capabilities and needs of each client, thereby promoting a more balanced and efficient learning process.\nReward function: Upon completion of the r-th round of federated training, the server collects the local training time costs $T_{l}$ from all participating clients i within the set S. This collection of training time costs can be formally expressed as:\n$T^{r} = \\{T_{i} | i \\in S_{r}\\} . $ (20)\nThe average training time $T_{l,avg}^{r,i}$ required for each participating client to complete one epoch is calculated based on the local training time $T_{d}^{r,i}$ and the number of iterations $T_{r,i}$:\n$T_{l,avg}^{r,i}=\\frac{T_{d}^{r,i}}{T_{r,i}}$ (21)\n$T_{l,avg}^{r} = \\{T_{l,avg}^{r,i} | i \\in S_{r}\\} . $ (22)\nThe reward function for the PPO1 model is then formulated to minimize the relative performance discrepancies among clients, encouraging a more balanced training:\n$R = M D\\frac{max(T_{l,avg}^{r})}{min(T_{l,avg}^{r})},$ (23)\nwhere MD represents the maximum acceptable multiple of performance differences between participating clients after balancing the training with differentiated model allocations. This function aims to narrow the gap between the fastest and slowest clients, thus enhancing overall training efficiency.\n2) RL-based training intensity adjustment scheme: Building on the achievements of PPO1 in balancing performance disparities among clients, the HAPFL method incorporates a second PPO-based decision-making scheme, referred to as PPO2. This scheme is designed to dynamically assign varying training intensities to clients according to their performance levels, aiming to further optimize overall straggling latency and achieve adaptive training adjustments. This dual-layered approach of PPO1 and PPO2 significantly boosts the efficiency and effectiveness of federated learning across heterogeneous client environments.\nState space: The state space for PPO2 is formulated by integrating the normalized assessment training time $T_{d}^{,i}$ with the outcomes $a_{r,i}$ of PPO1. This integration can be expressed as follows:\n$T_{m}^{r,i} = M(a_{r,i}) T_{d}^{,i}$ (24)\nwhere $T_{m}^{r,i}$ represents the modified training time for client i in round r, adjusted according to the model type allocated. M(.) represents the relative training time ratio for local training corresponding to different types of models. The collective state space $S_{r}^{'}$ for PPO2 in round r is then defined as:\n$S_{r}^{'} = \\{T_{m}^{r,i} | i \\in S_{r}\\} . $ (25)\nAction space: PPO2 aims to dynamically adjust training intensities according to the varying performance levels of clients to further optimize straggling latency. Thus, the action space $A_{2}^{'}$ of PPO2 encompasses the decision regarding an array of normalized training intensities $\\sigma_{i}^{r}$ , derived through a Softmax normalization process, which are assigned to k participating clients in round r:\n$A^{'} = [\\sigma_{i}^{r}]_{1xk}, $ (26)\n$0 \\leq \\sigma_{i}^{r} \\leq 1, \\sum \\sigma_{i}^{r} = 1, k=|S_{r}| .$\nFinally, the number of local training iterations $T_{r,i}$ assigned to client i in round r is calculated by multiplying the normalized array $\\sigma_{i}^{r}$ by the total training intensity $T_{r}$, as follows:\n$[T_{r,i}]_{1xk} = [\\sigma_{i}^{r}]_{1xk} * T_{r}.$ (27)\nReward function: The server receives the local training time, denoted as $T^{r} = \\{T_{d}^{l,i}\\} , i \\in S_{r}$, from each participating client and computes the straggling latency for the current round. The reward function for the RL process is defined as:\n$R = min(T) - max(T).$ (28)"}, {"title": "3) RL Learning Process:", "content": "The Proximal Policy Optimization [36", "as": "n$G_{r"}, "sum_{t=0}^{\\infty} \\gamma^{t} R_{r+t}$ (29)\nwhere $G_{r}$ indicates the cumulative discounted return at round r, $0 \\leq \\gamma \\leq 1$ is the discount factor that prioritizes immediate rewards over distant ones, and $R_{r+t}$ represents the reward at round r + t.\nActor Network: The actor network is designed to output the probability distribution of potential actions for a given state. It plays a crucial role in selecting actions based on the current state by implementing the parameterized policy function. During training, the primary objective of the actor network is to maximize the expected reward, which is achieved by adjusting the probabilities to favor actions that increase the expected returns under the current policy. The parameters of the actor network are updated using the policy gradient method to progressively converge the output action probabilities toward the optimal value. The loss function for the actor network is defined as follows:\n$L(\\theta) = E [min (p_{r}(\\theta) \\hat{A}_{r}, clip (p_{r}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{r})"], "clip\" refers to a clip function. The advantage function is given by": "n$\\hat{A}_{r} = G_{r} - V(S_{r}),$ (31)\nwhere $G_{r}$ represents the cumulative discounted return, and $V(S_{r})$ is the value estimate provided by the critic"}