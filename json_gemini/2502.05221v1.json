{"title": "Blackout DIFUSCO: Exploring Continuous-Time Dynamics in Discrete Optimization", "authors": ["Jun Pyo Seo"], "abstract": "This study explores the integration of Blackout Diffusion into the DIFUSCO framework for combinatorial optimization, specifically targeting the Traveling Salesman Problem (TSP). Inspired by the success of discrete-time diffusion models (D3PM) in maintaining structural integrity, we extend the paradigm to a continuous-time framework, leveraging the unique properties of Blackout Diffusion. Continuous-time modeling introduces smoother transitions and refined control, hypothesizing enhanced solution quality over traditional discrete methods. We propose three key improvements to enhance the diffusion process. First, we transition from a discrete-time-based model to a continuous-time framework, providing a more refined and flexible formulation. Second, we refine the observation time scheduling to ensure a smooth and linear transformation throughout the diffusion process, allowing for a more natural progression of states. Finally, building upon the second improvement, we further enhance the reverse process by introducing finer time slices in regions that are particularly challenging for the model, thereby improving accuracy and stability in the reconstruction phase. Although the experimental results did not exceed the baseline performance, they demonstrate the effectiveness of these methods in balancing simplicity and complexity, offering new insights into diffusion-based combinatorial optimization. This work represents the first application of Blackout Diffusion to combinatorial optimization, providing a foundation for further advancements in this domain.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization (CO) involves finding the optimal solution from a finite, discrete solution space. These NP-hard problems, such as the Traveling Salesman Problem (TSP), Knapsack Problem, and Job Scheduling, require significant computational resources to solve exactly, motivating the development of efficient approximation methods.\nAmong CO problems, TSP is a widely studied example. It entails finding the shortest route to visit all cities exactly once and return to the starting point. The problem's factorial complexity makes brute-force solutions impractical, especially for large instances, necessitating heuristic and learning-based approaches.\nTo address NP-hard CO problems, supervised learning (SL), reinforcement learning (RL), and semi-supervised learning (SSL) methods have been explored. SL approaches generate approximate solutions using historical optimal data, often producing stable results via heatmap-based decoding [2, 15, 16]. RL methods train agents to construct solutions sequentially or iteratively improve solutions through metaheuristics [1, 9, 10]. However, RL approaches can suffer from inefficiencies during training due to large initial Optimality Gaps. Hybrid methods that integrate SL into RL training have been proposed to address this challenge.\nRecently, DIFUSCO [15] introduced diffusion generative models for CO problems, achieving state-of-the-art performance on TSP. DIFUSCO generates heatmaps representing edge probabilities and decodes them into feasible solutions. Building on this, the T2T model [16] introduced objective-guided sampling, further improving performance and efficiency while retaining DIFUSCO's non-autoregressive structure. DIFUSCO and T2T operate in a discrete time-space framework, with categorical-state diffusion (D3PM) outperforming continuous-state diffusion (DDPM) in discrete optimization tasks.\nInspired by these advancements, we explore Blackout Diffusion [14], a framework that bridges discrete state spaces and continuous-time modeling, for CO problems. Continuous-time diffusion, as demonstrated in \"Score-Based Generative Modeling through Stochastic Differential Equations\" [11], has shown performance gains in generative tasks compared to discrete-time methods like DDPM [3]. We hypothesize that applying continuous-time discrete-state diffusion to CO tasks could improve solution quality by enabling smoother state transitions.\nThis work represents the first attempt to combine DIFUSCO with Blackout Diffusion. By incorporating insights from diffusion scheduling and optimal observation time design, we aim to refine sampling strategy and solution quality. Although the performance was not satisfactory, our study lays a foundation for future research into continuous-time diffusion models for combinatorial optimization.\nContributions\n\u2022 Integration of DIFUSCO and Blackout Diffusion for Combinatorial Optimization\n\u2022 Sampling Optimization for Efficient Blackout Diffusion\n\u2022 Design of Heuristic Diffusion Processes"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 TSP Formulation", "content": "In the case of the Traveling Salesman Problem (TSP), the goal is to compute the shortest Hamiltonian cycle that visits all N nodes in a Euclidean space exactly once and returns to the starting node. Here, S is the set of all feasible tours, and $x \\in S$ represents a specific tour. The objective function f(x) is the total length of the tour, defined as:\n$f(x) = \\sum_{(i,j) \\in x} d(i, j)$,\nwhere d(i, j) is the Euclidean distance between nodes i and j, and the summation is over all edges (i, j) in the tour x.\nIt is important to note that f(x) is only defined over the feasible set S, meaning x must satisfy the constraints of forming a valid Hamiltonian cycle. In TSP, this ensures that every node is visited exactly once and the tour is closed."}, {"title": "2.2 DIFUSCO: Graph-Based Diffusion Solvers for Combinatorial Optimization", "content": "Diffusion Framework and Heatmap Representation.\nDIFUSCO[15] leverages diffusion models to solve combinatorial optimization problems by progressively corrupting and reconstructing adjacency matrices. The ground truth adjacency matrix, denoted as xo, encodes the presence or absence of edges in a binary or relaxed format. Through a forward diffusion process, xo is gradually transformed into a fully corrupted state xf over T timesteps. The reverse diffusion process then reconstructs the structured adjacency matrix by iteratively refining the noisy intermediate states.\nAt any timestep t, the corrupted adjacency matrix xt represents probabilistic edge states. A discretized version, It, can be obtained by thresholding or rounding xt, ensuring interpretability and compatibility with downstream tasks. Both Xt and It encode the probabilities or binary states of edges, progressively transitioning from noise to structure during the reverse process.\nDIFUSCO employs two distinct diffusion paradigms:\n\u2022 Gaussian Diffusion (DDPM): Adds continuous Gaussian noise to xo, resulting in $x_t \\in R^{N \\times N}$.\n\u2022 Categorical Diffusion (D3PM): Uses a transition matrix to model probabilistic transitions between edge states, resulting in $x_t \\in R^{N \\times N \\times 2}$. This approach aligns more naturally with graph-based representations and has shown superior performance in maintaining structural integrity.\nNeural Network Architecture and Reverse Process.\nDIFUSCO utilizes attention mechanism to guide the reverse diffusion process. The network inputs include:\n\u2022 The corrupted adjacency matrix It,\n\u2022 The sinusoidal encoding of the timestep t,\n\u2022 Additional node positional information, such as coordinates or learned embeddings, seamlessly integrated with edge features through an attention mechanism that effectively captures interactions between nodes and edges.\nThe model outputs $\\hat{x_0} \\in [0,1]^{N \\times N \\times 2}$, which approximates the original adjacency matrix x0. During the reverse process, $\\hat{x_0}$ is iteratively refined and used to sample the next state xt-1. This iterative denoising aligns the adjacency matrix closer to the original structure xo, ensuring a gradual"}, {"title": "2.3 Diffusion Frameworks For DIFUSCO", "content": ""}, {"title": "2.3.1 Denoising Diffusion Probabilistic Models (DDPM)", "content": "Forward Process. DDPM [3] operates on continuous data distributions by introducing Gaussian noise in a forward process. The process starts with the original data xo, representing the final heatmap, and iteratively adds Gaussian noise at each timestep t until the data becomes random noise xT, approximated as $x_T \\sim N(0, I)$. The forward process is defined as:\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$,\nwhere xt represents the noisy data at timestep t, and $\\beta_t \\in (0, 1)$ is the variance controlling the noise level. Over multiple timesteps, the structure in xo is progressively corrupted, resulting in xT, which serves as the starting point for the reverse process.\nReverse Process. The reverse process reconstructs xo from xT by iteratively denoising through intermediate steps xT-1, xT-2,...,x0. At each timestep t, the reverse distribution is parameterized as:\n$p_\\theta(x_{t-1} | x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$,\nwhere $\\mu_\\theta$ is the predicted mean, $\\Sigma_\\theta = \\beta_t I$ is the fixed variance, and both are predicted by a neural network. The mean $\\mu_\\theta$ is computed as:\n$\\mu_\\theta (x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta (x_t, t))$\nwhere $\\alpha_t = \\prod_{i=1}^{t}(1 - \\beta_i)$ represents the cumulative noise schedule, and $\\epsilon_\\theta(x_t, t)$ is the predicted noise at timestep t. Starting from xT, the reverse process iteratively refines the noisy data, reducing the noise at each step to reconstruct x0.\nThe reverse process outputs either the predicted noise $\\epsilon_\\theta$ at each timestep, which is used to refine xt, or the reconstructed data x0 at the end of the process. This iterative denoising ensures that x0 is a high-quality approximation of the original data distribution, making DDPM an effective generative framework.\nTraining Objective. The training process optimizes the variational lower bound (VLB) to ensure that the reverse process accurately reconstructs the original data. The ob-"}, {"title": "jective is expressed as:", "content": "$\\mathcal{L} = E_{q(x_0)} \\Bigg[D_{KL}(q(x_T|x_0)||p(x_T))\n+ \\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t))\n- log p_\\theta (x_0|x_1)\\Bigg]$,\nwhere $D_{KL}$ is the Kullback-Leibler divergence, $q(x_T|x_0)$ represents the forward process's noise distribution, and $p_\\theta(x_{t-1}|x_t)$ parameterizes the reverse process.\nSimplified Loss. For practical implementation, the training objective is often simplified to directly predict the noise $\\epsilon$ added during the forward process, leading to the following reparameterized loss:\n$\\mathcal{L}_{simple} = E_{t,x_0,\\epsilon} [||\\epsilon - \\epsilon_\\theta(x, t)||^2]$,\nwhere $\\epsilon \\sim N(0, I)$ is the ground truth noise. This simplified loss allows efficient optimization of the neural network to predict noise accurately."}, {"title": "2.3.2 Discrete Denoising Diffusion Probabilistic Models (D3PM)", "content": "D3PM [7] generalizes diffusion models to discrete state spaces, making it particularly effective for tasks where data is inherently categorical or binary. In graph-based combinatorial optimization problems, such as those addressed by DIFUSCO [15], the data is represented as an $N \\times N \\times 2$ tensor, where each entry corresponds to the state of an edge between two nodes. The tensor encodes probabilities for the two discrete states: k 0 for no edge (1 - Pedge) and k 1 for an edge (Pedge).\nForward Process. The forward process progressively corrupts the original adjacency tensor xo by applying a transition matrix Qt \u2208 R2\u00d72 independently to each edge state at timestep t. The transition matrix Qt is defined as:\n$Q_t = \\begin{bmatrix}\n(1-\\beta_t) & \\beta_t \\\\\n\\beta_t & (1-\\beta_t)\n\\end{bmatrix}$,\nwhere $\\beta_t \\in (0, 1)$ governs the corruption level at timestep t:\n\u2022 (1-\u03b2t): Probability of maintaining the current state (edge or no edge),\n\u2022 \u03b2t: Probability of transitioning to the opposite state.\nAt each timestep t, the forward process is represented as:\n$q(x_t | x_{t-1}) = Cat(x_t; x_{t-1}Q_t)$,"}, {"title": "where $x_{t-1}Q_t$ computes the probabilities of transitioning to each state based on the prior state and the corruption level $\\beta_t$. The recursion is defined as:", "content": "$x_t = x_{t-1}Q_t$,\napplying Qt to all entries of xt\u22121. Over multiple timesteps, the adjacency tensor xt becomes increasingly corrupted, transitioning between edge and no-edge states with probabilities governed by \u03b2t. By the final timestep T, the corrupted state xT converges to a uniform or degenerate categorical distribution, effectively destroying all structural information in xo.\nThis formulation ensures systematic corruption where the degree of randomness introduced at each timestep is precisely controlled by \u03b2t, maintaining consistency across the $N \\times N \\times 2$ tensor structure.\nReverse Process. The reverse transition $q(x_{t-1}|x_t, x_0)$ is derived using Bayes' theorem and is defined as:\n$q(x_{t-1}|x_t, x_0) = \\frac{q(x_t|x_{t-1}, x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}$\nwhere $q(x_t|x_{t-1}, x_0)$ represents the forward corruption step from xt\u22121 to xt, $q(x_{t-1}|x_0)$ encodes the marginal transition from x0 to xt\u22121, and $q(x_t|x_0)$ normalizes the posterior over xt\u22121.\nThe forward transition across multiple timesteps is captured by the cumulative transition matrix $\\bar{Q}_t = Q_1Q_2...Q_t$, where each $Q_t \\in R^{2 \\times 2}$ represents the corruption probabilities at timestep t. The entries of $\\bar{Q}_t$ encode the overall probabilities of transitioning between the two states (edge or no edge) over the first t steps. This allows the marginal $q(x_t|x_0)$ to be expressed as:\n$q(x_t|x_0) = Cat (x_t; p = x_0\\bar{Q}_t)$.\nUsing this cumulative formulation, the posterior distribution $q(x_{t-1}|x_t, x_0)$ is given by:\n$q(x_{t-1} | x_t, x_0) = Cat \\Bigg(x_{t-1}; p = \\frac{x_0\\bar{Q}_{t-1} \\odot Q_t^T}{\\bar{Q}_t x_t}\\Bigg)$,\nwhere it is the discretized adjacency tensor at timestep t, $Q_t$ is the transpose of the transition matrix for timestep t and $\\odot$ denotes element-wise multiplication.\nThis formulation ensures that the reverse sampling process systematically reconstructs x0 while incorporating both the corrupted state xt and the clean prediction x0,\u03b8.\nThe neural network p\u03b8 is trained to predict $\\hat{x}_{0,\\theta}$ (discretized version of x0,\u03b8), the clean adjacency tensor, given the corrupted xt. The reverse process then substitutes the predicted $\\hat{x_0}$ into the posterior to compute:\n$p_\\theta(x_{t-1}|x_t) = \\sum_{x_0} q(x_{t-1}|x_t, x_0)p_\\theta(x_0|x_t)$."}, {"title": "By iteratively applying this reverse sampling from xT to xo, the process reconstructs the original adjacency tensor while maintaining consistency with the forward corruption model. The integration of the cumulative transition matrix $Q_t$ ensures that the overall corruption and reconstruction remain aligned across all timesteps.", "content": "Training Objective. The model is trained to minimize the binary cross-entropy loss between the predicted discretized tensor $\\hat{x_0}$ (output by the neural network) and the ground truth discretized tensor x0. The process involves uniformly sampling a timestep $t \\in \\{1,2,...,T\\}$, applying the forward corruption process to obtain xt, and using the neural network to predict $\\hat{x}_{0,\\theta}(x_t)$ from xt. The loss function is defined as:\n$\\mathcal{L} = E_{t,x_0} \\Bigg[ - \\sum_{i=1}^{NN} \\sum_{j=1}^{NN} \\sum_{k=0}^1 [x_0(i, j, k) log \\hat{x}_{0,\\theta}(i, j, k)\n+ (1 - x_0(i, j, k)) log(1 - \\hat{x}_{0,\\theta}(i, j, k))]\\Bigg]$,\nwhere x0(i, j, k) \u2208 {0,1} is the ground truth binary state for the edge between nodes i and j and $\\hat{x}_{0,\\theta}(i, j, k)$ is the predicted probability output by the neural network for the same edge state.\nThe training process ensures that the neural network learns to predict the ground truth discretized tensor x0 by minimizing the binary cross-entropy loss. By comparing $\\hat{x}_{0,\\theta}$ and x0 directly, the model aligns its predictions with the underlying structure of the original adjacency tensor, enabling accurate reconstruction during the reverse process."}, {"title": "3 Blackout Diffusco: Proposition", "content": ""}, {"title": "3.1 Motivation.", "content": "In DIFUSCO, the discrete diffusion model (D3PM) outperformed its continuous Gaussian counterpart (DDPM) due to its ability to naturally align with the discrete nature of adjacency matrix. Inspired by this success, we propose integrating Blackout Diffusion into DIFUSCO. Blackout Diffusion extends the discrete state-space paradigm to a continuous-time framework, combining the structural advantages of D3PM with the smoother transitions and finer control offered by continuous-time dynamics. This integration is anticipated to further enhance the model's ability to generate high-quality solutions for combinatorial optimization problems like the Traveling Salesman Problem (TSP)."}, {"title": "3.2 Blackout Diffusco", "content": "Forward Process. The forward process in Blackout Diffusion progressively corrupts the original adjacency matrix $x_0 \\in \\{0, 1\\}^{N \\times N}$, representing the binary edge connectivity, into a fully degenerate blackout state xT = 0. The dynamics are governed by a pure-death Markov process, where each edge state transitions from active (1, edge is in the tour) to inactive (0, edge is not in the tour) over a continuous time interval [0, T]. The forward process can be expressed as:\n$q(x_t | x_0) = Binomial(x_t; x_0, e^{-\\lambda t})$,\nwhere $e^{-\\lambda t}$ represents the decay rate of edge states over time. As t\u2192 T, the corrupted matrix xt converges to a black image, effectively destroying all structural information in x0. To ensure complete corruption, we set T to a sufficiently large value of 15.0, as done in this work.\nReverse Process. The reverse process reconstructs the adjacency matrix x0 by simulating a birth-only process, effectively reversing the pure-death dynamics of the forward process. Each element of the matrix, corresponding to an edge state (i, j), is processed independently. The reverse transition probability for edge state (i, j) is modeled as:\n$p(x_s(i, j)|x_t(i, j), x_0(i, j)) = \\binom{o-n}{m-n} r^{m-n}(1-r)^{o-m}$,\nwhere the variables are defined as follows:\n\u2022 n = xt(i, j): The corrupted edge state at timestep t,\n\u2022 m = xs(i, j): The intermediate reconstructed edge state at timestep s,\n\u2022 o = x0(i, j): The original binary edge state,\n\u2022 $r = e^{-\\lambda (t-s)}$: The binomial transition parameter.\nThis formulation, derived from the binomial bridge, ensures a smooth and probabilistically consistent transition from the corrupted state xt to the original state x0.\nTo approximate x0 from xt, a neural network NN\u03b8 is used to predict the change in state \u2206xt\u21920, representing the difference between $\\hat{x_t}$ and the original state x0, as:\n$\\Delta x_{t \\rightarrow 0} = NN_\\theta(\\hat{x_t}, t)$\nwhere:\n\u2022 NN\u03b8: A parameterized neural network,\n\u2022 Inputs: The corrupted adjacency matrix $\\hat{x_t}$ and the timestep t,\n\u2022 Outputs: The predicted change \u2206xt\u21920 between $\\hat{x_t}$ and x0.\nThe reconstruction of xs is then performed iteratively as:\n$\\hat{x_0} = \\hat{x_t} + \\Delta x_{t \\rightarrow 0}$.\nThe probability $p(x_s|\\hat{x_t}, x_0)$ follows a similar formulation to the binomial transition probability shown in Equation 3, ensuring consistency with the dynamics of the reverse process."}, {"title": "This iterative refinement ensures that the adjacency matrix $\\hat{x_0}$ is reconstructed accurately, element by element, while respecting the probabilistic structure modeled by the binomial bridge. Each edge state evolves independently during the reverse process, guided by the neural network's predictions and the binomial transition probabilities.", "content": "Training Objective. The training objective is designed to minimize the discrepancy between the predicted and true adjacency matrice, incorporating the temporal dynamics of the diffusion process. The loss function for each element i is formulated as:\n$l_i = (t_k - t_{k-1})e^{-\\lambda t_k} [\\gamma_i - ((\\hat{x_0} - x_{t_k})_i log \\gamma_i)]$,\nwhere:\n\u2022 tk and tk-1 represent successive timesteps,\n\u2022 \u03b3i is the difference between two states for the element i, which is \u2206xt\u21920,\n\u2022 x0 is the ground truth adjacency matrix,\n\u2022 $x_{t_k}$ is the corrupted adjacency matrix at timestep tk.\nThis loss function ensures that the model learns to predict transitions in the adjacency matrix while respecting the temporal dynamics introduced during the diffusion process. By incorporating the time-dependent weight $e^{-\\lambda t_k}$ and the logarithmic term log \u03b3i, the objective penalizes large deviations in the reconstructed probabilities and encourages convergence towards the true solution.\nSimplified Loss. The training objective can also be simplified to focus directly on the discrepancy between the predicted and true adjacency tensors, with less emphasis on the temporal dynamics. The simplified loss function for each element i is expressed as:\n$l_i = [\\gamma_i - ((\\hat{x_0} - x_{t_k})_i log \\gamma_i)]$,\nThis formulation simplifies the original loss by removing the explicit time-dependent weighting terms, allowing the focus to remain on accurately reconstructing the true adjacency tensor while still leveraging the diffusion dynamics for guidance."}, {"title": "3.3 Improved Blackout DIFUSCO", "content": "To enhance the performance of Blackout DIFUSCO, we propose a modification to the observation time scheduling inspired by the cosine scheduler from Improved DDPM [8]. Specifically, we analyze the standard deviation (std) of the corrupted heatmaps across the time axis and observe that, unlike Gaussian-based diffusion models, the variance in Blackout Diffusion is smallest at the final corruption stage (t = T) and largest at the midpoint.\nTo address this, we redesign the observation time scheduling such that the standard deviation increases linearly throughout the diffusion process, producing a sharper peak at intermediate timesteps. This adjustment ensures that the model allocates more computational focus during the middle stages, where the variance is highest, and less attention towards the extremes, where the variance is minimal. This \"peaked\" observation time distribution aligns with the unique characteristics of Blackout Diffusion, allowing the reverse process to reconstruct adjacency matrices more effectively."}, {"title": "3.4 More Improved Blackout DIFUSCO", "content": "Building upon the observation time scheduling introduced in Improved Blackout DIFUSCO, we further refine the design to focus on regions where the corrupted heatmaps exhibit the largest standard deviations (std), as these regions are more challenging for the network to reconstruct. To achieve this, we introduce a hyperparameter \u03b1 to control the sampling density along the time axis.\nThe new schedule is designed to allocate more sampling steps to regions with high std while maintaining symmetry around the point of maximum variance. Specifically:\n\u2022 The middle portion of the time axis, where the variance is highest, receives more samples.\n\u2022 The start and end segments (Ta) are linearly reduced to half the maximum variance, creating a smooth transition.\nFor our experiments, we set \u03b1 = 0.2, ensuring that 20% of the time axis on both ends tapers to half the maximum std. The middle portion transitions linearly from this midpoint back to the peak, forming a symmetric triangular schedule. This design enables the model to concentrate more sampling steps in regions with high reconstruction difficulty, further improving its ability to recover accurate adjacency matrices."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset Preparation", "content": "The models were trained on the TSP-50 dataset, consisting of 1,502,000 training samples, and evaluated on a test set of 1,280 samples. To assess the generalization capabilities, the trained models were further evaluated on TSP-100 and TSP-500 datasets, with test set sizes of 1,280 and 128 samples, respectively. All experiments were conducted on a system equipped with an NVIDIA GeForce RTX 3090 GPU."}, {"title": "4.2 Comparison with Other Models", "content": "Table 1 outlines the performance of the Blackout DIFUSCO variants and other greedy-based approaches alongside traditional solvers like Concorde and 2-OPT. The results highlight:\n\u2022 The Top-3 Greedy Performers: DIFUSCO (Greedy), T2T (Greedy), and B-DIFUSCO (more improved), which demonstrated the lowest gaps (%) compared to other methods."}, {"title": "4.3 Baseline Comparisons", "content": "Table 1 also demonstrates the progression in performance among Blackout DIFUSCO variants:\n\u2022 Original Blackout DIFUSCO achieved a gap of 0.23%, indicating a strong baseline.\n\u2022 Improved Blackout DIFUSCO further enhanced the performance but slightly increased the gap to 0.38% due to additional complexity.\n\u2022 More Improved Blackout DIFUSCO finalized with a gap of 0.20%, achieving near-optimal performance within the greedy-based category.\nThis incremental improvement underscores the effectiveness of targeted refinements in the diffusion model."}, {"title": "4.4 Result Analysis", "content": "Figure 3 provides a visual analysis of the optimality gaps across decoding methods (Greedy, Greedy + 2-OPT, Sampling, Sampling + 2-OPT) and datasets (TSP-50, TSP-100, TSP-500). Notable insights include:\n\u2022 Generalization Capability: Although trained exclusively on TSP-50, the Blackout DIFUSCO model effectively generalized to larger datasets (TSP-100 and TSP-500), maintaining competitive gaps.\n\u2022 Model Stability: The consistent performance of the diffusion-based methods across all datasets suggests robustness against instance complexity.\n\u2022 Greedy Efficiency: The results reaffirm the strength of greedy strategies when optimized diffusion techniques are applied.\nOverall, the results validate the proposed methods' capabilities and their potential for solving combinatorial optimization problems efficiently."}, {"title": "5 Conclusion", "content": "The integration of Blackout Diffusion into DIFUSCO introduces a novel approach to solving combinatorial optimization problems through continuous-time dynamics. By"}]}