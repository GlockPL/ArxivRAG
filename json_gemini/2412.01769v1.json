{"title": "COMMIT0: LIBRARY GENERATION FROM SCRATCH", "authors": ["Wenting Zhao", "Nan Jiang", "Celine Lee", "Justin T Chiu", "Claire Cardie", "Matthias Gall\u00e9", "Alexander M Rush"], "abstract": "With the goal of benchmarking generative systems beyond expert software development ability, we introduce CommIT0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library's API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, COMMITO is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. COMMITO also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use. We publicly release the benchmark\u00b9, the interactive environment\u00b2, and the leaderboard\u00b3.", "sections": [{"title": "INTRODUCTION", "content": "AI agents have been increasing rapidly in ability, particularly in domains such as problem-solving, math, and coding. Tasks related to software development have been particularly promising areas due to both their clarity of evaluation and economic value. This has motivated the release of several coding benchmarks in recent years (Hendrycks et al., 2021a; Chen et al., 2021; Zhuo et al., 2024). A notable example is SWE-bench (Jimenez et al., 2024), which assesses the ability of agents to generate patches to resolve real-world GitHub issues. While critical, these tasks generally remain within the skill set of an experienced software engineer. If LLM systems continue to improve at current rates, these tasks will be completely solvable.\nWe are interested in benchmarks that exist further beyond both the frontier of expert human ability as well as current model ability. Specifically, tasks that experts struggle to solve but can still be fully specified and reliably verified. Software engineering is an appealing domain for this, as the process of developing actual implementations of functions is very complex. Nevertheless, humans can fully specify the desired behavior of functions and validate them through unit testing.\nWith this goal in mind, we introduce COMMITO, a benchmark that tests an agent's ability to generate a software library from scratch. This task is especially challenging \u2013 large, real-world libraries are notoriously difficult to design, often requiring hundreds of engineers and years of development. Nonetheless, this task remains verifiable without requiring humans to solve it directly. Humans can provide specifications that outline the library's API and write unit tests to verify whether the API has been implemented correctly.\nCOMMITO extends beyond existing benchmarks in several ways. Central to COMMITO is interactive feedback. Due to the complexity of generating a library, it is improbable, or likely impossible, that an agent could generate a complete, working version in one shot. Instead, the benchmark is constructed"}, {"title": "RELATED WORK", "content": "Evaluation of LMs. Recent benchmarks for evaluating agents focus on knowledge-intense, exam-style questions in domains ranging from grade-school mathematics to quantum mechanics (Hendrycks et al., 2021c; Srivastava et al., 2023; Hendrycks et al., 2021b; Rein et al., 2023). While these questions are challenging, with some requiring PhD level knowledge, they are often short and easy to memorize, and they only require few steps of sequential reasoning. In contrast, Commit0 requires reasoning over a long horizon. Mastering the ability to develop full repositories requires considering many files, many unit tests, and complex static analysis feedback.\nSoftware engineering benchmarks Existing benchmarks for code generation focus on specific aspects of the software engineering pipeline. Program synthesis benchmarks evaluate code generation, for example, HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), BigCodeBench (Zhuo et al., 2024), CodeBenchGen (Xie et al., 2024), and Classeval (Du et al., 2023). Segmented benchmarks, such as DevBench (Li et al., 2024), separately evaluate different aspects such as code design, code generation, and unit test synthesis. R2E (Jain et al., 2024) introduces a more challenging task by requiring function generation that involves dependencies within and across files. SWE-bench (Jimenez et al., 2024) provides a more holistic evaluation of a model's ability to resolve pull requests, requiring the incorporation of repository-level context. However, the amount of context necessary to resolve a specific pull request varies greatly and is small on average. These previous benchmarks focus on generating one or a few functions and are thus manageable via static one-shot code generation. In contrast, COMMITO requires generating an entire codebase consisting of numerous interdependent functions, which necessitates a series of refinements based on execution results to pass all unit tests.\nSoftware engineering agents Recent work has made impressive progress in developing software engineering agents that operate on repositories (Yang et al., 2024; Zhang et al., 2023; Wang et al., 2024a). Commit0 proposes a software agent that not only operates at the repository-level but also self-corrects given test feedback. Our method extends prior work on self-correction (Madaan et al., 2023; Shinn et al., 2023) to the larger-scale problem of repository generation."}, {"title": "THE COMMITO BENCHMARK", "content": "COMMITO benchmarks an agent's ability to generate a functioning library from scratch. It consists of 54 Python libraries covering a wide range of topics, including machine learning, networking, databases, and data visualization. Given, (1) a specification document that contains both texts and images, (2) a starter repository with both unit tests and files to fill in, agents are tasked with completing the implementation of the API described in the specification document. Libraries are prepared by removing the core source code from their repo in a systematized manner.\nThe agent is provided with a specification in PDF format and a starter repository containing a source code directory and a test directory. The task is to make edits to the repository. In practice, the model generates modified versions of the source code files. We then replace the original files with the modified ones and perform a commit to save the repository state. The model is allowed to run unit tests interactively along with code generation. For evaluation, we clone this commit in a clear repository and run unit tests. The model's performance is measured only by the pass rate of these unit tests.\nTo prevent the model from copying source code, we restrict access to original GitHub repositories via web retrieval. However, the model is allowed to use the web for general knowledge lookup. For"}, {"title": "LIBRARY SELECTION", "content": "We focus on Python libraries due to their widespread use, abundant data resources, and strong ecosystem support. To select a set of high-quality libraries for models to implement, we design three sets of filtering criteria.\nLibrary requirements. We restrict libraries to be Python-only. Specifically, the library needs to contain over 95% Python code. The library also needs to have native Python implementations instead of using Python wrapping libraries in other languages. Finally, the library must support testing with pytest.\nSpecification requirements. We identify libraries that have comprehensive specifications. The specification must have its own webpage rather than a plain README page. The specification document must cover both a user guide which describes how the library is intended to be used and a comprehensive API reference that defines the input and the output of a function. The specification should both describe in natural language what are the inputs and outputs and specify the types of inputs and outputs.\nUnit test requirements. We include libraries with comprehensive unit tests to test the implementation of a library, while having understandable tests that are feasible to run in an interactive system. We limit the libraries to those with over 90% of code that can be covered by unit tests. We filter libraries whose unit tests take over than 30 minutes to run on a single CPU and the libraries where a significant number of unit tests can only be run on GPU.\nTo compile the list of libraries included in COMMIT0, we consider both generally popular Python libraries and PyPI packages with top download counts. We follow the annotation guidelines to filter out the libraries that satisfy the criteria described above.\nWe create two dataset splits: lite, which includes libraries with fewer functions to implement, and all, which contains all libraries. Lite has a total of 16 libraries. Due to the complexity of COMMITO and budget constraints, we focus most of our evaluation on CoMMIT0 lite."}, {"title": "BENCHMARK CONSTRUCTION", "content": "Ensuring Replicability. A key aspect of COMMITO is replicable running of unit tests across all the libraries, which depends on the correct setup of development environments. To achieve this, we annotate setup commands for each library. We begin by annotating a specific commit of the library repository, which is used to extract installation requirements and generate the starter repository. The installation requirements typically include a compatible Python version, necessary pip packages, and an installation command. Some libraries may also require system-level dependencies, such as clang. Finally, we annotate the pytest command, the directory containing the unit tests, and the source code directory.\nPreparing Libraries. We prepare a library for Commit0 by removing its core code in a systematic way. We assume that a library contains public functions which are accessible to users, and private functions which are not supposed to be called. This is often not enforced explicitly by Python but is upheld by convention. To determine if a function is a public function, we check if it has an associated docstring. To prepare COMMIT0, we replace the function body of all public functions to be empty (pass) and remove all private functions entirely. We perform these code modifications by first parsing each Python file into an abstract syntax tree, performing transforms on the syntax tree, and converting back to source code.\nPreparing Specifications. Specifications exist in different forms. Some libraries have pure text descriptions while others have extensive figures to demonstrate how the libraries work. For example, seaborn is a data visualization library; it uses figures to demonstrate the expected outcomes of API functions. To unify the format, we convert all specifications to the PDF format. Specifically, starting from the main documentation page, we crawl the webpage as well as all the internal links recursively and save them as a PDF."}, {"title": "INTERACTIVE ENVIRONMENT", "content": "A key feature of COMMITO is its interactivity. Generating an entire library in a single attempt is challenging for an agent; it may need to iteratively incorporate feedback to refine the implementations. COMMITO provides an interactive environment that allows agents multiple sources of feedback, including unit testing, static analysis, and coverage analysis.\nUnit Test Feedback Unit tests are crucial for validating the specified behavior of functions. The results from unit tests provide valuable information about implementation issues, including error types and execution traces. Our interactive environment allows agents to execute an arbitrary number of unit tests for any library in parallel. The primary challenge in this process is the need to set up environments for each library to run the unit tests. To address this, we create a Docker image for each library and execute the tests in these isolated environments. This setup allows agents to simultaneously develop and run unit tests across all libraries using the pre-built images.\nStatic Analysis Feedback Our interactive environment also offers comprehensive static analysis feedback, including linting and type checking, as an additional corrective signal. We apply a standardized linter and configuration file across all libraries to ensure consistency. Specifically, we use ruff as our linter. For reproducibility, we release both the Docker images and the linter configuration file alongside our benchmark.\nCoverage Feedback Coverage analysis serves as another valuable signal. For instance, if a unit test passes in one run but fails in another, the difference in coverage can help identify which lines of code are causing errors by comparing the differences in coverage. To provide this coverage information, we use pytest-cov and leverage the pre-built isolated environments described above to run the coverage analysis in a reproducible way."}, {"title": "THE SDE-I AGENT", "content": "Challenges involving complex multi-file dependencies and interactive feedback are challenging for current agentic systems (Yang et al., 2024). To test the difficulty level of COMMITO, we design a prototype interactive agent. SDE-I performs a basic software engineering loop. It writes functions, runs unit tests, and iteratively edits the code based on error messages. The agent operates in three stages as shown in Figure 3.\nStage 1: Draft initial implementations. SDE-I drafts an initial implementation for each function. The first challenge is determining the appropriate unit for generation. Implementing all functions at once is impractical, as concatenating them often exceeds the model's maximum context length. However, implementing each function in isolation loses the broader context of the other functions. To balance this, we choose to treat all functions within a single module as one generation unit.\nThe second challenge is managing complex dependencies between modules since implementing a module often requires understanding which other modules it depends on. SDE-I performs a topological sort on imports of source code modules. It constructs a directed acyclic graph (DAG) of the library, where each node represents a module. If a module imports others, the imported modules are set as its parent nodes. In the case of conditional import cycles, a random edge is removed to break the cycle. SDE-I then proceeds by filling in the modules in the order determined by the topological sort. When implementing a module, it also includes the content of all modules that the current module imports. In this step, SDE-I entirely ignores whether the generated code is executable or not.\nStage 2: Refine based on static analysis. SDE-I improves the initial implementations by running static analysis to detect and correct issues related to code style, syntax, and type errors. This static analysis helps enforce coding standards and catch potential problems before running more resource-intensive tests. SDE-I appends these results to the context and generates revised versions of where issues were located. This process is repeated until all errors are resolved or the maximum number of runs is reached.\nStage 3: Refine based on unit test results. SDE-I refines the implementation further by running unit tests to ensure functional correctness. A challenge similar to the first stage arises: running all unit tests simultaneously may generate error messages that exceed the model's maximum context length. However, unit tests are naturally grouped by functionality, with tests for related features typically organized within the same test module. We leverage this structure by executing unit tests one module at a time. The results are then incorporated into the context, allowing SDE-I to revise the code based on the error messages. This process is iterative, with SDE-I continually revising the code until all tests pass or a predefined limit on test runs is reached.\nImplementation The SDE-I is implemented to be modular for the underlying coding system and language model. For code generation, we default to the Aider framework. Aider's interface allows us to define a prompt, a lint command, and a test command. We construct a message that includes a prompt to fill in the missing function body, along with texts from specifications and any necessary import modules. For the LLM, we evaluate several model families known for their strong performance on coding benchmarks. Specifically, we consider GPT-40-mini (Hurst et al., 2024), 01-preview (OpenAI, 2024), Claude 3.5 Sonnet, DeepSeek-V2.5 (Guo et al., 2024), Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Llama-3.1-405B-Instruct (Dubey et al., 2024), and Codestral."}, {"title": "RESULTS", "content": "To assess the effectiveness of each stage in the SDE-I agent, we evaluate ablated versions of the method where we apply a fixed number of stages. We summarize the results on COMMITO lite in Table 1. (Note that we skip stage 2 for OpenAI 01-preview due to its high costs.) Among the three models, Claude 3.5 Sonnet consistently delivers the best performance across all three stages.\nSurprisingly progressing from Stage 1 to Stage 2 results in a decline in performance with the open-weights models. As discussed in the qualitative analysis in Section 6, although the static analysis feedback provides useful guidance for fixing bugs, the model struggles to apply it effectively, often introducing additional errors. This issue particularly affects the less capable models. However, moving from Stage 2 to Stage 3 consistently improves the average pass rate, demonstrating the value of utilizing unit test feedback. In the cost-constrained setting, Codestral in stage 1 has the best performance (6.34%) under $1, and Claude 3.5 Sonnet in stage 3 has the best performance (%29.3) under $100.\nResults on COMMITO all. We summarize the results in Table 3. Due to the high API cost of Claude 3.5 Sonnet, we only run it for the first stage of the SDE-I agent on COMMITO all. We note that running just the first stage with Claude 3.5 Sonnet already outperforms running all stages with GPT-40-mini or DeepSeek-V2.5. In the compute-constrained setting, GPT-40-mini in stage 1 has the best performance (2.87%) under $1, and Claude 3.5 Sonnet in stage 1 has the best performance (%6.12) under $100.\nResults on state-of-the-art software development agent: OPENHANDS. We test OPENHANDS (Wang et al., 2024b), the best-performing agent on SWE-bench. To have OPENHANDS take full advantage of the specifics of COMMITO, we share the list of unit test IDs, static analysis commands, and test commands with the agent. With this deep integration, OPENHANDS passes 42.95% of unit tests on COMMITO lite (a 13.65% improvement compared to SDE-I) and 15.25% on Commit) all (a 9% improvement). To understand this improvement, we analyze the TinyDB example, where OPENHANDS improved from 64 passed tests to 174. We attribute the success of OPENHANDS to its better debugging capabilities. Unlike SDE-I, which often repetitively generates the same fix for a bug, OPENHANDS is able to explore different solutions."}, {"title": "ANALYSIS", "content": "Ablations. We conduct ablation studies based on the first stage of SDE-I in Figure 4. First, we investigate whether including information from the specifications and tests can help LLM agents pass more unit tests. Since the length of the specification often exceeds the maximum context length of the LMs, we feed only the first 10,000 tokens from the specification. For test, we append the prompt to include test modules. Surprisingly, both additions reduce performance. We hypothesize that much of the specification and tests are irrelevant to implementing specific modules, which may distract the model. To better leverage the specification, the agent will likely need to first filter out only the relevant information. To verify this hypothesis, we perform retrieval to obtain 10,000 tokens. Specifically, we break the specification into chunks of 1,000 tokens and retrieve the top 10 chunks to include in the context. With the same number of tokens, using BM25-retrieved tokens yields a higher unit test pass rate, suggesting that agents can benefit from more relevant context.\nNext, we explore whether filling in functions following the order found by topological sort is helpful. Interestingly, we find that a random-ordering approach leads to more passed unit tests (22% compared to 17%). Upon further inspection, we found conditioning on incorrect implementations of previous modules was the issue. Relying on incorrect implementations appears to be more harmful than relying on empty ones. Lastly, we assess the impact of excluding relevant modules from the context. As expected, excluding these relevant imports results in fewer passed unit tests.\nTest-time Scaling. An interesting question is how unit test pass rates scale with more test-time compute. We address this question with two experiments. First, we sample a module 1, 3, and 10 times, picking the best implementation based on pass rates before proceeding to the next module. Additionally, we test whether continuous iterations on unit test feedback will eventually enable agents to pass all unit tests. We conducted an experiment where we applied unit test feedback over different numbers of iterations: 1, 3, and 10. We summarize the results on the left of Figure 5. We observed that, in both cases, unit test pass rates improve with more test-time compute.\nLibrary Memorization. COMMITO tests models on existing libraries, versions of which are may be part of their training data. This raises the possibility that LLMs may be simply recalling these libraries from memory, but not reasoning about the specifications and unit tests. (We not that a similar question exists for many other coding benchmarks.) To explore whether current models are primarily memorizing the libraries, we calculate the 10-gram overlap between the generated and reference libraries. We create a mapping from function IDs to their corresponding function bodies for public functions. We then compute the 10-gram overlap between the generated implementation and the reference implementation for each function. The overlaps are sorted by function length. The results, presented on the right of Figure 5, indicate that LMs either fully memorize the functions or produce implementations that are significantly different. Shorter functions are more frequently memorized. Given the low total test accuracy, the observed degree of memorization is not a major factor for the benchmark.\nQualitative Analysis. In the first example, we show how static analysis feedback might hurt performance. Presented in Table 4, the function implemented in stage 1 successfully passes the unit tests. However, the agent misinterpreted the static analysis feedback, which requested type annotations. In response, the agent adds Any as the return type and modifies the function to include yield None to match the type. Unfortunately, since a generator cannot return NoneType, this introduces a new error. In the second example, we show test feedback can improve performance. Presented in Table 5, the unit test feedback points out that the attribute '_callback' is missing. The agent thus revised the implementation by adding an if statement to check for relevant attributes, hence passing the unit tests."}, {"title": "CONCLUSION", "content": "We introduce COMMITO, a challenging task that requires LMs to generate libraries from scratch. Our task provides signals through unit test feedback, including both error messages and execution traces. It also offers comprehensive static analysis feedback including type checking. This task is meant to be beyond the level of most human experts, and currently seems beyond what state-of-the-art LLMs are capable of. We hope that it can serve as both as progress benchmark for AI development as well as spurring new agent architectures and methods."}, {"title": "LIMITATIONS", "content": "One limitation of COMMITO is that we exclusively include Python libraries in our evaluation, which may limit the generalizability of our findings to other programming languages. Additionally, our experimental environment requires a well-specified document and a full suite of unit tests for each library, which may not reflect the typical development process where documentation and tests are often incomplete or evolving. While these assumptions may not be entirely realistic in every real-world scenario, they are closely aligned with the principles of test-driven development (TDD), where rigorous testing and clear documentation are integral parts of the development process.\nFurthermore, our reliance on unit tests as the primary means of evaluating the correctness of the generated code introduces additional limitations. Unit tests may not cover all possible use cases or edge cases, potentially allowing erroneous or suboptimal code to pass unnoticed. This overreliance on testing may mask underlying issues that would be evident in a thorough code review or through integration testing. As a result, the agent's ability to generate code that passes all tests does not necessarily equate to producing robust, high-quality software.\nAnother concern is the risk associated with having agents generate code that humans may find difficult to read or maintain. Automatically generated code might be syntactically correct and functionally adequate to pass tests, but it may lack clarity, proper documentation, or adherence to coding standards and best practices. This can lead to maintainability issues, as future developers may struggle to understand or modify the codebase, increasing the risk of introducing bugs or vulnerabilities in subsequent updates. The opaqueness of machine-generated code also raises questions about accountability and traceability in software development."}, {"title": "ETHICS STATEMENT", "content": "COMMITO consists of forked public repositories whose licenses permit our use. Our study does not involve human participants, and does not rely on human task workers for data collection. We do not gather any data, including personal data, from GitHub.\nAdditionally, code generation without human oversight may raise potential ethical issues. For example, agents could introduce security vulnerabilities or violate licensing terms. Ensuring that the generated code is safe, compliant, and ethically sound may require human intervention, which our current benchmark does not account for.\nFinally, automating software engineering is a challenge that has both potential benefits and harms. Our release of COMMITO serves to measure progress towards this challenge."}, {"title": "REPRODUCIBILITY", "content": "We release the COMMITO benchmark in its entirety, along with all methods and results. We also provide the code for reproducing the dataset, so that it may be used for synthesizing data."}]}