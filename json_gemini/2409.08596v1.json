{"title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios with Versatile Instructions", "authors": ["Lingwei Meng", "Shujie Hu", "Jiawen Kang", "Zhaoqing Li", "Yuejiao Wang", "Wenxuan Wu", "Xixin Wu", "Xunying Liu", "Helen Meng"], "abstract": "Recent advancements in large language models (LLMs) have revolutionized various domains, bringing significant progress and new opportunities. Despite progress in speech-related tasks, LLMs have not been sufficiently explored in multi-talker scenarios. In this work, we present a pioneering effort to investigate the capability of LLMs in transcribing speech in multi-talker environments, following versatile instructions related to multi-talker automatic speech recognition (ASR), target talker ASR, and ASR based on specific talker attributes such as sex, occurrence order, language, and keyword spoken. Our approach utilizes WavLM and Whisper encoder to extract multi-faceted speech representations that are sensitive to speaker characteristics and semantic context. These representations are then fed into an LLM fine-tuned using LoRA, enabling the capabilities for speech comprehension and transcription. Comprehensive experiments reveal the promising performance of our proposed system, MT-LLM, in cocktail party scenarios, highlighting the potential of LLM to handle speech-related tasks based on user instructions in such complex settings\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have experienced rapid and significant advancements recently, achieving or even surpassing human-level proficiency in numerous natural language processing (NLP) tasks [1]\u2013[3]. These advancements have sparked interest in exploring the capabilities of LLM in multi-modal perception, including speech [4]\u2013[7], vision [1], [8], [9], and content generation [10], [11]. Several studies have investigated speech-related LLM, which typically involve a fine-tuned text LLM following speech-related instructions and pairing with auxiliary audio encoders [4]\u2013[7]. The audio encoder extracts acoustic representations and adapts them into the input feature space of LLM, enabling the LLM to perform various speech tasks such as automatic speech recognition (ASR), speech translation (ST), speaker verification (SV), and speech question answering (SQA), among others. However, despite the progress, the potential of speech LLMs in cocktail party scenarios-where multiple talkers speak simultaneously and overlapping occurs has not yet been sufficiently exploited.\nIn recent year, various end-to-end approaches have garnered interest and been developed to tackle multi-talker ASR task, which involves simultaneously transcribing speech from multiple talkers. These studies are based on Permutation Invariant Training (PIT) [12]\u2013[14], Heuristic Error Assignment Training (HEAT) [15], [16], or Serialized Output Training (SOT) [17]\u2013[20] to match predictions with corresponding target labels for loss calculation. However, these approaches typically transcribe speech from all talkers indiscriminately and fail to associate transcriptions with specific talkers, unless an additional external [21], [22] or internal [23]\u2013[25] model is employed to extract speaker information. Although several studies [26], [27] proposed handling multi-talker ASR in conjunction with other tasks within a single model, the addressed tasks remain constrained and lack the flexibility to address various user requirements specifying talker attributes such as please transcribe the talker who said the word \"strawberry\u201d.\nNevertheless, the rise of large language models illuminates new possibilities for tackling such problems with a unified model. In this work, we leverage the powerful comprehension and instruction-following capabilities of LLM to perform speech recognition based on various instructions in multi-talker scenarios. Specifically, we utilize Llama 2 [3] as our foundational LLM, coupled with the Whisper [28] encoder to extract semantic context, and WavLM [29] multi-layer features to capture acoustic information indicating speaker characteristics, referring to WavLLM [6] and SALMONN [5]. Corresponding adapters are designed to project audio embeddings into the LLM's input space. We denote the proposed model as MT-LLM (Multi-Talker LLM). Versatile instructions are used to prompt MT-LLM to perform tasks including (i) simultaneously transcribing the speech of multiple talkers into text, (ii) transcribing a target talker's speech given a reference audio clip, (iii) transcribing speech based on the talker's specific sex, (iv) transcribing the speech of a specified talker according to their occurrence order, (v) transcribing the speech of the talker where a given keyword appears, and (vi) transcribe the talker who speaks the specific language. The comprehensive experiments demonstrate that MT-LLM can effectively meet user's diverse requirements for transcribing multiple talkers based on instructions specifying talker attributes. Our major contributions are threefold:\n\u2022 We propose a pioneering effort to explore instruction-based speech recognition in multi-talker scenarios, leveraging the powerful comprehension and generation capabilities of LLM;\n\u2022 Beyond multi-talker ASR, MT-LLM can transcribe speech from specific talkers according to six versatile instructions, demonstrating promising performances;\n\u2022 We reveal that speech LLMs can support a more natural and effective human-computer interaction paradigm in complex speech environments, with parameter-efficient training."}, {"title": "II. METHODS", "content": "We propose empowering a text-based LLM to act as a versatile instruction follower for speech recognition in multi-talker speech scenarios. The proposed method consists of three major components: a large language model as the foundational model fine-tuned with LORA, dual speech encoders with corresponding adapters, and training data construction. We denote the proposed model as MT-LLM in the subsequent sections."}, {"title": "A. Problem Formulation", "content": "This study regards the speech recognition in multi-talker scenarios as the next-token-prediction language modeling task. Conditioned on the input speech waveform X and text instruction I, MT-LLM is optimized to autoregressively generate the target text output Y = [Yo, Y1, ..., YN-1,], by maximizing the following distribution:\n$\\displaystyle p(Y | X,I;\\theta) = \\prod_{t=0}^{T-1} P(Y_t | X, I, Y_{<t}; \\theta) $ \nwhere $\\theta$ represents the parameters of MT-LLM.\nAmong Y, The transcripts for multiple talkers require a permutation assignment to determine the talker order, thereby addressing the label ambiguity issue [30]. Drawing on previous experiences [17], [19], we employed the straightforward Serialized Output Training (SOT) method to address this issue. As illustrated in Fig. 1 (d), SOT arranges the transcripts based on the speaking order of the talkers, with plain text \"<sc>\" inserted between them to signify speaker changes."}, {"title": "B. Model Architecture", "content": "As shown in Fig. 2, the speech representations synthesized by the dual speech encoders are fused and subsequently projected into the feature space of the backbone LLM. The LLM then leverages both the speech input and text instructions to predict the target transcripts. The backbone LLM and speech encoders remain frozen, while the system is fine-tuned using parameter-efficient adapters, equipping it with the capability for speech processing and comprehension.\nDual Speech Encoders and Corresponding Adapters Referring to [5], [6], we utilize two pre-trained speech encoders, Whisper encoder and WavLM, to capture multi-faceted speech information. Whisper [28] is a speech recognition and translation model trained on web-scale weakly supervised data, with its encoder being sensitive to speech semantic context [31]. We exploit its last-layer output embedding to capture rich semantic information. In contrast, WavLM [29] is a self-supervised learning model trained using a masked speech prediction approach, leading to different layers encoding acoustic information sensitive to various downstream speech tasks. To better leverage the multi-scale acoustic information extracted by WavLM, we aggregate its multi-layer hidden states by summing them with learnable weights for each layer. The modality adapters for the two speech encoders, along with a linear projector, are designed to better align speech representations with the LLM feature space."}, {"title": "Backbone LLM and LoRA", "content": "We exploit Llama-2-7b-chat\u00b2, developed by Meta AI [3] on extensive and diverse training data, as our foundation backbone. Llama 2 excels across a broad spectrum of NLP tasks, showcasing superior capabilities in context understanding and text generation. To incorporate the speech modality into the LLM, we employ a parameter-efficient fine-tuning technique, LoRA [32]. LORA is applied to the key, query, value, and output weight matrices within Llama's attention modules, enabling the model's capability to process and comprehend speech modality inputs."}, {"title": "C. Task Descriptions", "content": "To validate the performance of MT-LLM in executing speech recognition based on instructions in multi-talker scenarios, we simulated multi-talker overlapped speech audios from single-talker speech corpora, and designed six tasks along with corresponding instructions and generated the respective target text label. As illustrated in Fig. 1 (b), the text instructions\u00b3 pertain to the following tasks:\nMulti-Talker (MT) ASR Simultaneously transcribing the speech of multiple talkers into text. This basic task challenges the model's ability to handle overlaps and distinguish between different talkers' voices within a single audio stream.\nTarget-Talker (TT) ASR A random talker is selected as the target, and a 3-second audio clip of the target talker, along with 3 seconds of silence, is concatenated with the input multi-talker speech. The model is instructed to transcribe only the target talker's speech from the overlapping audio. This task tests the model's capability to differentiate and isolate the speech of a specified individual given a reference audio clip as the clue.\nKeyword-Tracing (KT) ASR For each multi-talker sample, we first collect a set of unique words that appear only once across all talkers' speech content, each with a minimum length of six characters. From this set, we randomly select a keyword and instruct the model to transcribe the speech of the talker who said that word. This task assesses the model's proficiency in tracking specific lexical items and attributing them to the correct speaker.\nSex-Specific (SS) ASR We randomly instruct the model to transcribe all male or all female talkers from the multi-talker speech input. This task evaluates the model's ability to distinguish voices based on sex-related characteristics and filter the transcription accordingly.\nOrder-Specific (OS) ASR We randomly instruct the model to transcribe a talker based on their appearance order. This requires the model to keep track of the sequence of speakers and accurately extract the speech of a designated individual in the order they spoke.\nTarget-Lingual (TL) ASR We randomly instruct the model to transcribe the speech of talkers speaking either English or German. This task evaluates the model's ability to discriminate languages and transcribe spoken content based on the specified language."}, {"title": "III. EXPERIMENTAL SETUP", "content": "We simulated multi-talker speech audios from single-talker speech corpora to accommodate the versatile task instructions, following the protocol outlined in [17]. Specifically, the utterances are simulated primarily from the 960-hour LibriSpeech [33] training set, comprising mixtures of two or three talkers. The start time for each talker is randomly sampled, resulting in overlapped mixtures. Additionally, the 180-hour German subset of CoVoST 24 [34], along with its corresponding German target text, is employed to mix with LibriSpeech utterances to support the Target-Lingual ASR task.\nCombined with original single-talker corpora, the total training speech data amounts to ~6.3K hours, with ~10% containing German."}, {"title": "B. Evaluation and Metrics", "content": "We evaluate the performance of MT-LLM on versatile instruction-based tasks, using the official 2- and 3-speaker LibriSpeechMix test set and an additional home-made En-De-Mixed test set for Target-Lingual ASR task, ensuring that none of the speakers from evaluation sets are included in the training data. We also test single-talker ASR performance on LibriSpeech test-clean set and CoVOST 2 German (De) test set.\nThe word error rate (WER) is calculated between the predicted text and the target labels. For speech recognition tasks involving multiple talkers, permutations with minimum errors are used to compute WER, following previous studies [19], [35], [36]."}, {"title": "C. Model Settings and Training details", "content": "The proposed MT-LLM employs the encoder of Whisper-large-v2\u2075 and WavLM-base\u2076 to extract speech representations, with Llama-2-chat-7b as the backbone LLM. All parameters of above models are frozen. The LLM is fine-tuned using LoRA with a rank of 32. The adapters for both speech encoders consist of two convolution layers to down-sample and align the representations within the temporal domain, followed by a bottleneck adapter [37] and a linear layer. The outputs of both adapters have a time stride of 80 ms and a dimension of 2,048. The total number of parameters in MT-LLM is 7.55 billion, of which 1% (76.6 million) are trainable.\nMT-LLM is trained on 32 NVIDIA A100-40G GPUs with a batch size equivalent to 60 seconds per GPU for 150K updates. We optimize the model using AdamW optimizer, warming up the learning rate to a peak of 1e-4 over the first 10% updates, followed by a linear decay. All tasks are mixed together for training to develop a unified model."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We evaluate the single-talker ASR performance on the LibriSpeech and CoVOST 2 German test set, and multi-talker ASR performance on the 2- and 3-speaker LibriSpeechMix and the home-made En-De-Mixed test set. As shown in Table I, the proposed MT-LLM demonstrates promising results across all datasets, outperforming D2V-Sidecar-DB [26], a recent representative work employing PIT. SOT-Conformer [23] is a state-of-the-art model specifically designed and extensively trained for multi-talker ASR on the LibriSpeechMix dataset. On the 2-speaker LibriSpeechMix, MT-LLM achieves results comparable to SOT-Conformer, while lags behind on the 3-speaker set. Given that MT-LLM serves as an exploratory work aimed at exploring the potential of LLM for versatile instruction-based ASR in complex environments, rather than solely pushing the limits of multi-talker ASR, we argue that the gap is understandable. Despite being trained on both single-talker and multi-talker data, we note that SOT-Conformer falls short in single-talker scenarios, indicating its specialization in multi-talker ASR impairs its single-talker performance. In contrast, MT-LLM perform consistently good in single-talker ASR, demonstrating that its instruction-understanding capabilities help maintain high performance in simpler, single-talker settings. SALMONN [5] is a speech LLM trained on various speech-related tasks including overlapped speech recognition. However, using its official prompt, we observed significant inferior performance compared to our approach as shown in Table I.\nWe list wav2vec2-Large-XLSR-53-German model performance on De for reference. Given that the audio samples containing German"}, {"title": "B. Results on Versatile Tasks Based on Instructions", "content": "We investigate the performance of MT-LLM on various ASR tasks described in Section II-C. The evaluations are conducted using the 2-speaker and 3-speaker LibriSpeechMix and En-De-Mix test sets, with the results presented in Table II and Table III.\nTo validate MT-LLM's ability to accurately capture and transcribe a specified talker in multi-talker scenarios, we evaluate a best-matching result as a reference. Specifically, for each sample, we select the target text of the correct talker based on instructions specifying talker attributes (such as sex or keywords), and calculate WER against each ASR transcript of multiple talkers produced by MT-LLM in Section IV-A. The lowest WER obtained is reported as the best-matching result, eliminating the impact of speaker confusion. This result is compared with the model's performance when directly executing instruction-based ASR tasks, thereby reflecting MT-LLM's capability to follow instructions and correctly identify the intended talker.\nThe results in Table II and Table III highlight the effectiveness of the MT-LLM model across different instruction-based ASR tasks. In both the 2-speaker and 3-speaker mixed scenarios. MT-LLM achieves impressive performance for Target-Talker (TT), Keyword Tracing (KT), and Sex-Specific (SS) ASR tasks, which are within a reasonable range of the best-matching results. This suggests that MT-LLM is proficient at isolating and accurately transcribing speech from specified talkers based on instructions focusing on different speaker attributes. However, for Order-Specific (OS) ASR tasks, there is a gap compared to the best-matching result, indicating that MT-LLM slightly falls short in determining the order in which the talker appears. We anticipate significant improvements by using additional positional embedding for the speech part, employing larger speech modality adapters, or fully fine-tuning the speech encoder with heavier training as in [23]. Target-Lingual (TL) ASR task present a more pronounced challenge for the model, due to that the mixed audios containing German are relatively limited and noisier compared to English set. We observe that performance of the TL task in both English and German is close, resulting in similar WER to best-matching situation. This indicates room for improvement through language-specific embeddings or higher-quality multi-lingual data.\nFor the more complex 3-speaker scenarios, MT-LLM consistently manages to handle the added intricacies of increased overlapping of multiple talkers, despite a rise in WER compared to the 2-speaker cases. This discrepancy illustrates the challenge posed by denser overlapping in speech, yet MT-LLM still demonstrates a competent ability to follow instructions and transcribe the target speaker accordingly."}, {"title": "C. Ablation Study", "content": "Fist, we conduct an ablation study to examine the impact of using dual speech encoders. We train two models specifically on the multi-talker (MT) ASR task: one using dual speech encoders and the other using only the Whisper encoder. As shown in Table IV, incorporating the WavLM encoder into the architecture leads to notable enhancements in performance, underscoring the importance of multi-layer acoustic information captured by WavLM in improving speech recognition accuracy, particularly in three-talker scenarios.\nWe also investigate the effect of multi-task training on improving the basic MT ASR task. As illustrated in Table IV, training on a variety of tasks not only endows the system with the versatility to handle diverse ASR tasks but also enhances performance on the basic MT ASR task. This indicates that the different tasks are interdependent and complementary, helping to supervise the model and enhance its overall speech comprehension capabilities."}, {"title": "D. Limitations and Future Work", "content": "Despite MT-LLM's promising performance on various ASR tasks in multi-talker scenarios, we acknowledge several limitations. First, the primary intention of this study is to exploring the ability of LLMs to capture specific talkers according to instructions, and transcribe their speech in multi-talker scenarios. Therefore, MT-LLM is not designed to be a universal speech LLM for a broader spectrum of tasks. Second, the experiments are conducted on limited simulated datasets for insightful observation, rather than on real-world datasets, due to resource constraints. In the future, we anticipate the development of a more comprehensive speech LLM for cocktail party environments with meticulously crafted training data and scheme."}, {"title": "V. CONCLUSION", "content": "In this work, we present a pioneering exploration into the use of large language models (LLMs) for instruction-based speech recognition in multi-talker scenarios. We utilize the Whisper encoder to extract semantic context information and WavLM to capture multi-layer acoustic information indicating speaker characteristics, thereby enabling the foundational LLM to effectively handle speech modality input. With parameter-efficient fine-tuning, our proposed system, MT-LLM, demonstrates remarkable capabilities in comprehending and transcribing speech based on versatile instructions related to multi-talker ASR, target talker ASR, and ASR based on specific talker attributes such as sex, occurrence order, language, and keyword spoken. Comprehensive experiments reveal promising performance in complex multi-talker environments, highlighting the potential of LLMs to enhance speech-related tasks and improve human-computer interaction in challenging settings."}]}