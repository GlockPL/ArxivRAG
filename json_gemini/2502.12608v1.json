{"title": "Unveiling Mode Connectivity in Graph Neural Networks", "authors": ["Bingheng Li", "Zhikai Chen", "Haoyu Han", "Shenglai Zeng", "Jingzhe Liu", "Jiliang Tang"], "abstract": "A fundamental challenge in understanding graph neural networks (GNNs) lies in characterizing their optimization dynamics and loss landscape geometry, critical for improving interpretability and robustness. While mode connectivity-a lens for analyzing geometric properties of loss landscapes-has proven insightful for other deep learning architectures, its implications for GNNs remain unexplored. This work presents the first investigation of mode connectivity in GNNs. We uncover that GNNs exhibit distinct non-linear mode connectivity, diverging from patterns observed in fully-connected networks or CNNs. Crucially, we demonstrate that graph structure, rather than model architecture, dominates this behavior, with graph properties like homophily correlating with mode connectivity patterns. We further establish a link between mode connectivity and generalization, proposing a generalization bound based on loss barriers and revealing its utility as a diagnostic tool. Our findings further bridge theoretical insights with practical implications: they rationalize domain alignment strategies in graph learning and provide a foundation for refining GNN training paradigms.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have emerged as a dominant paradigm for processing graph-structured data, achieving state-of-the-art results across diverse applications, from social network analysis to bioinformatics and traffic prediction [34, 52]. Despite their empirical success, the fundamental understanding of GNN optimization dynamics and the intricate geometry of their loss landscapes remains limited, hindering progress in model interpretability and robustness [23]. Meanwhile, the concept of mode connectivity [11, 17], which refers to the relationships between disparate local minima obtained from varied training runs, offers a powerful lens for examining the loss landscape. Mode connectivity has been shown closely related to model robustness [60] and generalization capabilities [49]. Standing apart from other theoretical tools like Neural Tangent Kernels [56] or Mean Field Theory [3] that often rely on assumptions not readily applicable to real-world scenarios, mode connectivity allows for the study of practical models under realistic conditions.\nWhile mode connectivity has been extensively explored in fully-connected networks (FCNs), convolutional neural networks (CNNs), and transformers [11, 12, 17, 41], a systematic investigation into GNN mode connectivity is, to the best of our knowledge, absent. GNNs, operating on non-Euclidean graph data, introduce unique data-structure interactions that may fundamentally alter training dynamics compared to models processing independent and identically distributed (i.i.d.) data. Understanding whether GNN mode connectivity deviates from that of other architectures is crucial. Dissimilar behavior could unveil unique properties of the GNN loss landscape, while similarities would allow us to leverage existing theoretical frameworks from other domains to explain the GNN optimization. Therefore, exploring GNN mode connectivity is a critical step towards demystifying these powerful models, motivating our central inquiry: How does mode connectivity manifest in GNNs, and how is it influenced by the inherent structure of the input graph?\nThis work provides an initial, systematic answer to these questions through a comprehensive controlled study encompassing 12 diverse graphs from various domains. Our key observation is that, in contrast to FCNs and CNNs, GNNs exhibit a distinct non-linear mode connectivity, accurately characterized by a polynomial curve such as a quadratic B\u00e9zier curve. This phenomenon strongly suggests that the non-i.i.d. nature of graph"}, {"title": "2 Preliminaries", "content": "In this section, we introduce the graph neural network (GNN) model used for node classification (Section 2.1) and the Contextual Stochastic Block Model (CSBM) as the underlying data model for theoretical analysis (Section 2.2). We then formally define mode connectivity in GNNs (Section 2.3) and discuss different experimental evaluation methods used to study it."}, {"title": "2.1 Graph Neural Networks", "content": "In this work, we consider a graph neural network (GNN) trained for node classification task on a graph represented as G = (A, X, Y). The adjacency matrix A \u2208 Rn\u00d7n encodes the graph structure, where n is the number of nodes. The node feature matrix X \u2208 Rnxd contains d-dimensional features for each node. The label matrix Y \u2208 Rn\u00d7C represents the ground-truth labels for C classes.\nGNN Forward Propagation: In this work, we focus on Graph Convolutional Networks (GCNs) [28], the classic GNN architecture, which propagates node representations iteratively as: A GNN with L layers propagates node representations iteratively as:\nH(1) = \u03c3(AH(1-1)W(1)), l = 1, ..., L, (1)\nwhere H(1) \u2208 Rnxd\u0131 denotes node embeddings at layer l, with d\u2081 the embedding dimension of that layer. \u00c2 is the normalized adjacency matrix, W(1) is the trainable weight matrix, and o(\u00b7) is a non-linear activation function. The initial representation is H(0) = X.\nFinal Node Embeddings: After L layers of message passing, the final node representations are: Z = H(L)W(L), where Z \u2208 Rn\u00d7C represents the node embeddings before classification.\nOutput Layer (Softmax Probability Distribution): The predicted class probabilities are obtained using the softmax function:PZ ="}, {"title": "2.2 Contextual Stochastic Block Model (CSBM)", "content": "To theoretically understand the behavior of GNNs, we employ Contextual Stochastic Block Model (CSBM) [10] as the underlying data model for graphs, which has been widely adopted for node-level task [5, 6, 13, 33, 50, 51] due to its ability to simultaneously capture both node features and graph structures.\nDefinition 2.1 (Contextual Stochastic Block Model (CSBM)). A CSBM with two classes defines a graph G = (V, &, X, Y), where |V| = n denotes the number of nodes nodes and V belong to two disjoint classes C1 and C2, with labels Y \u2208 {C1, C2}. Each node v with label co has a feature vector xu ~ N(\u00b5i, \u03c3I), where \u00b5i \u2208 Rd is the class-wise mean, and o quantifies the separability of node features. Edges form independently with probability\nP((u, v) \u03b5 \u03b5) = { Pin, if Yu = Yo,\nPout, if Yu \u2260 Yo,\nwhere pin > Pout > 0 denote intra-class and inter-class connection probabilities, respectively."}, {"title": "2.3 Mode Connectivity", "content": "Mode connectivity [11, 17] studies the relationship between different parameter space modes in neural networks. Despite the abundant research about mode connectivity in FCN, CNN, and transformer [11, 12, 17, 41], there's yet no existing research on mode connectivity in GNN. So, we aim to extend existing definitions of mode connectivity to GNNs. Parameter Space Mode. A mode in the parameter space refers to a set of parameters @ that minimizes the loss function L for a given graph G. Formally, an optimal mode is defined as:\n\u03b8* = argmin\u03b8 L(f(G; \u03b8), Y), (6)\nwhere f(G;0) = PZ represents the probability distribution predicted by the GNN model, and Y is the ground-truth label matrix. For an L-layer GNN, the parameters are given by 0 = {W\u00b9} for l = 0,..., L. Different modes arise due to variations in training conditions such as random initialization and data sampling.\nGiven two independently trained GNN models under different training configurations Ma and M\u2081, we obtain two sets of parameters \u03b8\u03b1 and \u03b8\u03b7, each corresponding to a local minimum of L. A fundamental property of mode connectivity is the ability to connect these minima with a low-loss path [11, 17]. Two commonly considered paths are linear interpolation and B\u00e9zier curve interpolation, both of which are used to fit the minima of the trained model. A good fit suggests that the modes lie along a simple, low-loss manifold, providing further insights into the structure of the loss landscape.\nLinear Interpolation Path. Frankle et al. [14], Qin et al. [41] demonstrate that neural networks initialized similarly often exhibit a linear low-loss connection between minima. A linear interpolation path can be used to examine this property, which can be defined as continuous curve \u03c6(\u03b1) : [0, 1] \u2192 R|| connecting \u03b8\u03b1 and \u03b8\u266d:\n\u03c6(\u03b1) = (1 \u2013 \u03b1)\u03b8\u03b1 + \u03b1\u03b8\u03b9, \u03b1\u03b5 [0, 1]. (7)\nEven if a fully linear path between two minima results in a high loss, a low-loss non-linear path may still exist. A quadratic B\u00e9zier curve is often adopted [11, 17, 18] to examine such property.\nBezier Curve Interpolation Path. A bezier curve with two modes \u03b8\u03b1 and \u03b8 as endpoints can be given as\n\u03c6(\u03b1) = (1 \u2212 \u03b1)\u00b2\u03b8\u03b1 + 2\u03b1(1 \u2212 \u03b1)\u03b8 + \u03b1\u00b2\u03b8\u03b7, (8)\nwhere @ is a learnable parameter. Specifically, mode connectivity can be analyzed through (i) the smoothness of the loss and accuracy"}, {"title": "3 Main Analysis", "content": "In this section, we systematically study mode connectivity of GNNs. We begin by exploring the manifestation of mode connectivity in GNNs (Section 3.1). Considering the non-Euclidean nature of graphs and the intertwined roles of structure and features, we proceed to investigate how these graph properties impact the mode connectivity of GNNs (Section 3.2). Finally, we develop a theoretical framework to explain the empirically observed phenomena (Section 3.3) and provide further insights."}, {"title": "3.1 Does Mode Connectivity Exist in Graph Neural Networks?", "content": "While prior works have studied the mode connectivity for fully connected and convolutional neural network [11, 17], the graph-dependent message-passing mechanisms of GNNs may influence the loss landscape and thus mode connectivity, which motivates us to first study the the existence and characteristics of mode connectivity in GNNs.\nInvestigation protocol. We adopt node classification as our research subject and utilize one of the most commonly adopted GNNs, GCN [27], as the backbone models, while also studying the influence of different model architectures. In terms of data selection, we consider 12 graphs with different properties, including those exhibiting homophilic and heterophilic [33] characteristics. Following the settings in [32], we fix all hyperparameters except one, which we vary to generate different modes. Specifically, we investigate the impact of different initialization strategies, data orders (Appendix A), and model selections, as explored in Garipov et al. [17], Qin et al. [41]. After generating these modes, we follow Garipov et al. [17] to fit the corresponding performance data points using linear interpolation and B\u00e9zier curve interpolation, where train accuracy, test accuracy, train loss, and test loss are measured. To ensure statistical reliability, all experiments are repeated three times with different random seeds, and we report the average performance. Additional details and supplementary results are provided in Appendix A.\nObservation 1: The GNN minima are usually connected but not always linearly connected. After fitting the performance data points of different modes using linear interpolation, we observe a significant increase in both the training loss and the test loss (see Figure1). This indicates that although linear mode connectivity is a common phenomenon in fully connected networks (such as MLPs and CNNs) [14, 59], it does not always hold true for GNNs. First,"}, {"title": "3.2 How Do Graph Properties Influence Mode Connectivity?", "content": "In this section, we further study how relational dependencies inherent in graphs affect mode connectivity of GNNs through the lens of graph properties.\nStudy protocol. To come up with a controlled study in terms of graph properties, we first adopt a synthetic graph generative model CSBM. [add more experimental details here] We further consider the following key graph properties reflecting the graph structures and feature relationships\n\u2022 Density of the graph p = (pin + pout), which affects the connectivity of a graph. A denser graph implies a stronger structural relationship across different nodes.\n\u2022 Homophily h(G) = pin/(pin+pout), which refers to the tendency of nodes (vertices) with the same or similar labels to be more likely to connect to each other than to nodes with different labels.\n\u2022 Feature separability o reflects the difficulty of GNNs distinguishing the two different classes, and it can imply the influence of feature distribution on mode connectivity.\nAfter fitting different GNNs on generated graphs, we measure the correlation between mode connectivity and corresponding graph properties, where we use loss barriers as a metric for the mode connectivity. As shown in Figure 5, we observe:"}, {"title": "3.3 Why Does Mode Connectivity Occur? A General Bound for GNNS", "content": "While previous sections empirically investigated mode connectivity in GNNs, a rigorous theoretical understanding of why this phenomenon occurs remains elusive. In this section, we bridge this gap by establishing a theoretical framework that quantifies mode connectivity through a general bound on the loss barrier. Our analysis effectively validates previous experimental observations based on our understanding. We begin by formalizing the role of the graph structure in mode connectivity. One key factor is the spectral gap of the expected adjacency matrix, which controls the effective propagation of information in GNNs.\nDefinition 3.1 (Spectral Gap [9]). Let E[\u00c2] be the expected normalized adjacency matrix with eigenvalues\n\u03bb\u2081(\u0395[A]) \u2265 12(\u0395[A]) \u2265 \u00b7\u00b7 \u2265 \u03bb\u03b7(\u0395[A]).\nThe spectral gap is defined as:\n\u25b3 = 1 - maxi\u22652 |\u03bbi(\u0395[\u00c2])|.\nA larger spectral gap A implies faster information propagation across the graph and better separability of node representations, which is crucial for stable optimization [1, 9].\nTheorem 3.2 (General Bound of the Loss Barrier in L-Layer GNNs). Let \u03b8a and op be two sets of GNN parameters obtained under different initializations. Suppose the graph aggregation operator \u00c2 has an effective propagation factor deff (related to the spectral gap) and let:\nL\nN\u2081 = min{[ \u03a0j=1||W(j) ||}, \u03a0 j=l+1||W(j)||}.\nThen, the loss barrier satisfies:\n\u0392(\u03b8\u03b1, \u03b8) \u2264 max \u03bb\u03b5 [0,1] (1 \u2212 1)CL + A Le deff ||X|| \u03a3=0 Ni||W(1)-W(1)||(10)\nwhere CL captures higher-order curvature effects in the loss landscape, and Le is the Lipschitz constant of the loss function.(Proof in Appendix ??)\nRemark 3.3 (Implications for Mode Connectivity). Theorem 3.2 formally establishes that the loss barrier is influenced by three fundamental factors:"}, {"title": "4 Implications for the generalization of GNNs", "content": "After thoroughly investigating GNN mode connectivity, and with Observations 2 and 4 highlighting its relationship to GNN generalization behavior across both single and diverse graphs, we now leverage mode connectivity to derive a generalization bound for GNNs and, subsequently, to propose a mode connectivity-based metric for quantifying graph domain discrepancy."}, {"title": "4.1 How Does Mode Connectivity Reflect Generalization Performance?", "content": "Understanding generalization in GNNs remains a fundamental challenge, as their performance depends on both graph topology and node features. Specifically, while previous studies [25, 26, 46] in deep learning suggest that flatter minima lead to better generalization, it remains unclear whether this holds for GNNs due to their reliance on relational structures. To address this, we establish a formal connection between mode connectivity and generalization by analyzing the role of mode connectivity geometry in GNN training. The barrier can quantify the degree of sharpness between minima, where a higher barrier suggests a more isolated minimum with a steeper surrounding manifold.\nTheorem 4.1 (Generalization Bound via Loss Barrier). Let 01,be the model parameter obtained after T training iterations under different initialization, with m labeled and n m unlabeled data samples. Then, for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2013 8, the generalization gap satisfies\nAgen \u2264 \u039f (\u0392(\u03b8\u03b1, \u03b8\u03b5)\u00b7 (nm)3/2 (m+u)/2 log(c(T))TP log ( ))(12)\nDetailed proof is in Appendix ??. This bound formally establishes that the generalization gap is controlled by the mode connectivity barrier \u0392(\u03b8\u03b1, \u03b8\u03b7), along with dataset-dependent terms and the training dynamics. Several key insights are suggested from this result: 1. The bound directly links the loss barrier to generalization, indicating that a smaller barrier leads to stronger generalization. 2. The dependence on the labeling ratio (m + u)3/2/(mu) highlights the importance of labeled data in GNN generalization.\nTo validate the theorem, we measure the correlation between the barrier value and the generalization gap. Also, we compare the correlation between the training accuracy barrier and the generalization gap to that between the validation accuracy and the generalization gap, where the latter is one of the most commonly used metrics for measuring overfitting and selecting model checkpoints. As illustrated in Figure 6, where Each data point represents a trained model, we find that the training accuracy barrier demonstrates a stronger correlation with the generalization gap compared to the widely used validation accuracy. This suggests its potential utility as an indicator for selecting model checkpoints and mitigating overfitting. These findings collectively highlight that mode connectivity serves as a fundamental characteristic of GNN generalization, independent of specific architectures or hyperparameters."}, {"title": "4.2 Inter-Domain Graph Similarity via Mode Connectivity", "content": "Inspired from Observation 3, where the way trained minima are connected in the parameter space may implicitly capture domain-specific properties, we study how to come up with a mode connectivity-based metric to measure domain discrepancy. Measuring similarity between different graph domains is a fundamental problem in transfer learning, where an appropriate similarity metric helps guide domain adaptation. Prior works [19, 29] have proposed various structure-based and feature-based metrics, yet these are often heuristic and predefined, lacking validation from the model's training dynamics. In contrast, mode connectivity can reflect complex training dynamics and simultaneously consider both features and graph structures. We first introduce the metrics we design and then validate its effectiveness.\nMode Connectivity Distance. Consider two graphs, G\u00b9 and G2, trained under the same GNN, leading to minima 01, 01 for G\u00b9 and 02, 02 for G2. The mode connectivity curve between two minima is L(a) = L($(\u03b1)), where $(\u03b1) = (1 \u2212 \u03b1)\u03b8\u03b1 + \u03b1\u03b8\u03b7, \u03b1\u03b5 [0,1]. So we treat L(a) as a distribution over interpolation values a, capturing the smoothness of the loss landscape between minima. The discrepancy between two graphs can then be quantified by the Wasserstein-1 distance [42] between their respective loss distributions:\ndMC(G\u00b9, G\u00b2) = W\u2081 (LG\u00b9 (\u03b1), LG\u00b2 (a)). (13)\nTheoretical Justification: Upper Bounding Transferability Gap. We then theoretically show that dMC(G1, G2) provides an upper bound on the transferability performance gap between the two domains:\nAda \u2264 C.OdMC (G\u00b9, G\u00b2) (14)\nfor some constant C dependent on model complexity and domain properties. Crucially, this result holds independent of graph size, indicating that a smaller mode connectivity distance implies higher transferability. The proof is shown in Appendix ??.\nEmpirical Verification. To empirically validate our theoretical insights, we investigate unsupervised graph domain adaptation [54]. Specifically, we train a GNN model on a source graph and then"}, {"title": "5 Related Work", "content": "Graph Neural Networks (GNNs) have been extensively studied from various theoretical perspectives, including expressive power [36, 55], generalization properties [3, 48], and training stability [39]. While these works provide valuable insights, they mainly focus on spectral properties and convergence behavior, leaving open questions about the structure of solutions found by different training runs. A thorough survey for relevant literature is in Huang et al. [21], Jegelka [22]. Another key research direction involves analyzing how GNNs propagate and aggregate information across graph structures [30, 53]. However, a fundamental question remains unexplored: how do independently trained GNNs relate to each other in the parameter space? This study aims to bridge this gap.\nMode connectivity refers to the existence of low-loss paths between different trained neural networks, demonstrating that solutions found by independent runs of training are not necessarily isolated [11, 15, 17]. A particularly important variant, linear mode"}, {"title": "6 Conclusion", "content": "In this paper, we systematically study the mode connectivity behavior of GNNs and reveal that they exhibit distinct non-linear mode connectivity, primarily driven by graph structure rather than model architecture. This discovery deepens our understanding of GNN training dynamics and its link to improved generalization, paving the way for refined training methodologies and cross-domain adaptation techniques. Meanwhile, our research on mode connectivity has primarily focused on the node classification task. A promising future direction is to investigate the mode connectivity exhibited by GNNs on link prediction and graph-level tasks."}, {"title": "A More results about Mode connectivity in GNN", "content": "A.1 The performance of linear interpolations between two minima."}, {"title": "A.2 The performance of interpolations along quadratic B\u00e9zier curve connecting two minima", "content": ""}, {"title": "A.3 Effect of convolution mechanism on mode connectivity", "content": ""}, {"title": "A.4 Visualization of Loss basin and minimas", "content": ""}, {"title": "B Experimental Settings", "content": "We follow the model architecture design and hyperparameter settings of Luo et al. [32]. In order to accommodate the computational requirements for our extensive experiments, we harness a variety of high-capacity GPU resources. This includes: Tesla V100 32Gb, NVIDIA RTX A6000 48Gb, NVIDIA RTX A5000 24Gb, and Quadro RTX 8000 48Gb."}, {"title": "C Datasets", "content": "In this paper, we adopt 12 datasets from different domains.\n\u2022 [Citation network]. Cora, Citeseer, and Pubmed [57] are citation graphs where each node corresponds to a scientific paper. In these graphs, nodes are characterized by bag-of-words feature vectors, and each one is assigned a label that indicates its research field. It is important to note that all three datasets are examples of homophilous graphs.\n\u2022 [Amazon network]. Shchur et al. [43] In this network, products are nodes, and an edge signifies that two products are often bought together. Each product has associated reviews, which are treated as a bag of words. The task is to determine the product category for each item in the network.\n\u2022 [Coauthor network]. Shchur et al. [43] The network represents authors connected by co-authorship. Using the keywords from their published papers, we aim to classify each author according to their research field.\n\u2022 [Wikics network]. WikiCS [37] is a hyperlink network in the field of computer science on Wikipedia. The categories correspond to different research directions in computer science, such as artificial intelligence, computer vision, network security, etc.\n\u2022 [Wikipedia network]. Squirrel and Chameleon [40] represent two distinct portions of the Wikipedia web. The objective is to categorize each individual webpage (node) within these portions into one of five traffic-based classifications, determined by their respective average monthly page views.\n\u2022 [Heterophilous network]. These networks are from Platonov et al. [40] Amazon Ratings, A co-purchasing network of products with reviews used to predict product category. Minesweeper, a synthetic grid-based graph where nodes represent cells, and the task is to identify mines using information about neighboring mines. Roman Empire, a word dependency graph from a Wikipedia article, where nodes are words, and edges represent sequential or syntactic relationships, with the task of classifying words by their syntactic roles."}]}