{"title": "Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models", "authors": ["Chenrui Tie", "Shengxiang Sun", "Jinxuan Zhu", "Yiwei Liu", "Jingxiang Guo", "Yue Hu", "Haonan Chen", "Junting Chen", "Ruihai Wu", "Lin Shao"], "abstract": "Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans can learn manipulation skills from instructions in images or texts; for example, people can assemble IKEA furniture or LEGO models by following a manual's instructions. This ability enables humans to efficiently acquire long-horizon manipulation skills from sketched instructions. In contrast, robots typically learn such skills through imitation learning [59] or reinforcement learning [43], both of which require significantly more data and computation. Replicating the human ability to transfer abstract manuals to real-world actions remains a significant challenge for robots. Manuals are typically designed for human understanding, using simple schematic diagrams and symbols to convey manipulation processes. This abstraction makes it difficult for robots to comprehend such instructions and derive actionable manipulation strategies [32, 49, 48]. Developing a method for robots to effectively utilize human-designed manuals would greatly expand their capacity to tackle complex, long-horizon tasks while reducing the demand of collecting extensive demonstration data."}, {"title": "II. RELATED WORK", "content": "Manuals inherently encode the structural information of complex tasks. They decompose high-level goals into mid-level subgoals and capture task flow and dependencies, such as sequential steps or parallelizable subtasks. For example, furniture assembly manuals guide the preparation and combination of components and ensure that all steps follow the correct order [32]. Extracting this structure is crucial for robots to replicate human-like understanding and manage complex tasks effectively [19, 33]. After decomposing the task, robots need to infer the specific information for each step, such as the involved components and their spatial relationships. For example, in cooking tasks, the instruction images and texts may involve selecting ingredients, tools, and utensils and arranging them in a specific order [38]. Finally, robots need to generate a sequence of actions to complete the task, such as grasping, placing, and connecting components. Previous works have tried to leverage sketched pictures [42] or trajectories [15] to learn manipulation skills but are always limited to relatively simple tabletop tasks."}, {"title": "III. PROBLEM FORMULATION", "content": "Given a complete set of 3D assembly parts and its assembly manual, our goal is to generate a physically feasible sequence of robotic assembly actions for autonomous furniture assembly. Manuals typically use schematic diagrams and symbols designed to depict step-by-step instructions in an abstract format that is universally understandable. We define the manual pages as a set of N images. $I = {I_1, I_2,\u00b7\u00b7\u00b7, I_N}$, where each image $I_i$ illustrates a specific step in the assembly process, such as the merging of certain parts or subassemblies\nThe furniture consists of M individual parts $P = {P_1, P_2,\u2026, P_M}$. A part is an individual element in P that remains disconnected from other parts until assembly. A subassembly is any partially or fully assembled structure that forms a proper subset of P (for example, ${P_1, P_2}$). The term component encompasses both parts and subassemblies.\nGiven the manual and 3D parts, the system generates an assembly plan. Each step corresponds to a manual image and specifies the involved parts and sub-assemblies, their spatial 6D poses, and the assembly actions or motion trajectories required for execution."}, {"title": "IV. TECHNICAL APPROACH", "content": "Our approach automates furniture assembly by leveraging the VLM to interpret IKEA-style manuals and guide robotic execution. Given a visual manual and physical parts in a pre-assembly scene, a VLM generates a hierarchical assembly graph, defining which parts and subassemblies are involved in each step. Next, a per-step pose estimation model predicts 6D poses for each component using a manual image and the point clouds of involved components. Finally, for assembly execution, the estimated poses are transformed into the robot's world frame, and a motion planner generates a collision-free trajectory for part mating.\nThis paper shows an overview of our framework in Fig. 2. We describe the VLM-guided assembly hierarchical graph generation in Section IV-A, followed by per-step assembly pose estimation in Section IV-B and assembly action generation based on component relationships in Section IV-C."}, {"title": "A. VLM Guided Hierarchical Assembly Graph Generation", "content": "This section demonstrates how VLMs can interpret IKEA-styled manuals to generate high-level assembly plans. Given a manual and a real-world image of furniture parts (pre-assembly scene image), a VLM predicts a hierarchical assembly graph. We show one example in Fig. 2. In this graph, leaf nodes represent atomic parts, while non-leaf nodes denote subassemblies. We structure the graph in multiple layers, where each layer contains nodes representing parts or subassemblies involved in a single assembly step (corresponding to one manual image). The directed edges from the children to a parent node indicate that the system assembles the parent node from all its children nodes. Additionally, we add edges between equivalent parts, denoting these parts are identical(e.g. four legs of a chair). Representing the assembly process as a hierarchical graph can decomposes the assembly into sequential steps while specifying necessary parts and subassemblies. We give the formal definition of the hierarchical graph in Appendix J. We achieve this in two stages: Associating Manuals with Real Parts and Identifying Parts needed in Each Image.\n1) VLM Capabilities and General Prompt Structure: The task is inherently complex due to the diverse nature of input images. Manuals are typically abstract sketches, whereas pre-assembly scene images are high-resolution real-world images. Such diversity requires advanced visual recognition and spatial reasoning across varied image domains, which are strengths of VLMs due to their training on extensive, internet-scale datasets. We demonstrate the effectiveness of VLMs for this task in Section V-A and Appendix D.\nEvery VLM prompt consists of two components:\n\u2022 Image Set: This includes all manual pages and the real-world pre-assembly scene image. Unlike traditional VLM applications in robotics [23, 18], which process a single image, our method requires multi-image reasoning.\n\u2022 Text Instructions: These instructions provide a task-specific context, guiding the model in interpreting the image set. The instructions range from simple directives to Chain-of-Thought reasoning [51]. All instructions incorporate in-context learning examples, specifying the required output format-be it JSON, Python code, or natural language. This structure is essential to our multi-stage pipeline, ensuring well-structured, interpretable outputs that seamlessly integrate into subsequent stages.\n2) Stage I: Associating Real Parts with Manuals: Given the manual's cover sketch of the assembled furniture and the pre-assembly scene image, the VLM aims to associate physical parts with the manual. The VLM achieves this by predicting the roles of each physical part through semantically interpreting the manual's illustrations. This process involves analyzing spatial, contextual, and functional cues in the manual illustrations to enable a comprehensive understanding of each physical part. This design mimics human assembly cognition-people first map abstract manual images to physical parts before assembling. Our method follows CoT [51] and Least-to-Most [63] prompting, reducing cognitive load and improving accuracy. We considered pairwise matching of parts from manuals and scene images, but we found it impractical because the manuals not depict each part independently.\nTo enhance part identification, we employ Set of Marks [55] and GroundingDINO [31] to automatically label parts on the pre-assembly scene image with numerical indices. The labeled scene image and manual sketch form the Image Set. Text instructions consist of a brief context explanation for the association task of predicting the roles of each physical part, accompanied by in-context examples of the output structure:\n{name, label, role}\nFor example, in Figure 2 In Stage I Output, we describe the chair's seat as name: seat frame, label: [2], role: for people sitting on a chair, the seat offers essential support and comfort and is positioned centrally within the chair's frame.. Here, [2] indicates that this triplet corresponds to the physical part labeled with index 2 in the pre-assembly scene image. This triplet format enhances interpretability and ensures consistency by structuring all outputs into the same data format. We use the Image Set and Text Instructions as the input prompt for the VLM (specifically GPT-40 [1]) and query it once to generate real assignments for all physical parts. We then use these labels as leaf nodes in the hierarchical assembly graph.\nWe can obtain equivalent parts through these triplets. When two physical parts share the same geometric shapes, their triplets only differ by label. For example, in Figure 2 Stage I Output, {name: side frame, label: [0], role:...} and {name: side frame, label: [1], role:...}-these two parts are considered equivalent. Understanding equivalent part relationships is crucial for downstream modules, as demonstrated by our ablation experiments(see Appendix C).\n3) Stage II: Identify Involved Parts in Each Step: This stage focuses on identifying the particular parts and subassemblies involved in each manual page. The VLM achieves this by reasoning through the illustrated assembly steps, using the triplets and the labeled pre-assembly scene from the previous stage as supporting hints.\nIn practice, we observe that irrelevant elements in the"}, {"title": "B. Per-step Assembly Pose Estimation", "content": "Given an assembly order, we train a model to estimate the poses of components (parts or subassemblies) at each step of the assembly process. At each step, the model inputs the manual image and the point clouds of the involved components, predicting their target poses to ensure proper alignment. To support this task, we construct a dataset for sequential pose estimation. For a detailed description, see Appendix A.\nGiven each component's point cloud (obtained from real-world scans or our dataset), we first center it by translating its centroid to the origin. Next, we apply Principal Component Analysis (PCA) to identify the dominant object axes, which define a canonical coordinate frame. The most dominant axes serve as the reference frame, ensuring a shape-driven and consistent orientation that remains independent of arbitrary coordinate systems.\nThe dataset we create provides manual images, point clouds, and target poses for each component in the camera frame of the corresponding manual image(following [29]). For an assembly step depicted in the manual image $Z_i$, the inputs to our model include: (1) the manual image $I_i$; (2) the point clouds of all involved components. The output is the target pose $T \u2208 SE(3)$ for each component represented in the camera frame of $I_i$.\n1) Model Architecture: Note that the number of components at each step is not fixed, depending on the subassembly division of the furniture. Our pose estimation model consists of four parts: an image encoder $E_I$, a point cloud encoder $E_P$, a cross-modality fusion module $E_G$, and a pose regressor $R$.\nWe first feed the manual image $I$ into the image encoder to get an image feature map $F_I$.\n$F_I = E_I(I)$ (1)\nThen, we feed the point clouds into the point cloud encoder to get the point cloud feature for each component.\n${F_i} = E_P({P_i})$ (2)\nIn order to fuse the multi-modality information from the manual image and the point cloud features, we leverage a GNN [54] to update the information for each component. We consider the manual image feature and component-wise point cloud features as nodes in a complete graph, employing a GNN to update the information for each node.\n$F'_I, {F'_i} = E_G(F_I, {F_i})$ (3)\nwhere $F_I, {F_i}$ are updated image and point cloud features. Finally, we feed the updated point cloud features as input into the pose regressor to get the target pose for each component.\n$T_i = R(F'_i)$ (4)\n2) Loss Function: We adopt a loss function that jointly considers pose prediction accuracy and point cloud alignment, following [60, 30]. The first term penalizes errors in the predicted SE(3) transformation, while the second measures the distance between predicted and ground truth point clouds. To account for interchangeable components, we compute the loss across all possible permutations of equivalent parts and select the minimum loss as the final training objective. We provide further details on the loss formulation and training strategy in Appendix B."}, {"title": "C. Robot Assembly Action Generation", "content": "1) Align Predicted Poses with the World Frame: At each assembly step, the previous stage predicts each component's pose in the camera frame of the manual image. However, real-world robotic systems operate in their world frame, requiring a 6D transformation between these coordinates. Consider two components, A and B. The predicted target poses in the camera frame are denoted as ${^iT_A}$ and ${^iT_B}$. Meanwhile, our system can collect the current 6D pose of part A in the world frame, represented as ${^WT_A}$. To align ${^iT_A}$ to ${^WT_A}$, we compute the 6D transformation matrix ${^WT_i}$, which maps the camera frame to the world frame.\n${^WT_A} = {^WT_i}{^iT_A}$ (5)\nUsing the same transformation ${^WT_i}$, we compute the assembled target pose of part B (and all remaining components) in the world frame.\n${^WT_B} = {^WT_i}{^iT_B}$ (6)\nThis transformation accurately maps predicted poses from the manual image frame to the robot's world frame, ensuring precise assembly execution.\n2) Assembly Execution: Once our system determines the target poses of each component in the world frame for the current assembly step, it grasps each component and generates the required action sequences for assembly.\na) Part Grasping: After scanning each real-world part, we obtain the corresponding 3D meshes for each part. We employ FoundationPose [52], and the Segment Anything Model (SAM) [24] to obtain the initial poses of all parts in the scene.\nGiven the pose and shape of each part, we design heuristic grasping methods tailored to the geometry of individual components. While general grasping algorithms such as Grasp-Net [11] are viable, grasping is beyond the scope of this work. Instead, we employ heuristic grasping strategies specifically designed for structured components in assembly tasks. For stick-shaped components, we grasp the centroid of the object after identifying its longest axis for stability. For flat and thin-shaped components, we use fixtures or staging platforms to securely position the object, allowing the robot to grasp along the thin boundary for improved stability. We provide further details on these grasping methods in Appendix G.\nb) Part Assembly Trajectory: Once the robot arm grasps a component, it finds a feasible, collision-free path to predefined robot poses (anchor poses). At these poses, the 6D pose of the grasped component is recalculated in the world frame, leveraging the FoundationPose [52] and the Segment Anything Model (SAM)[24]. The system then plans a collision-free trajectory to the component's target pose. We use RRT-Connect [26] as our motion planning algorithm. All collision objects in the scene are represented as point clouds and fed into the planner. Once the planner finds a collision-free path, the robot moves along the planned trajectory.\nc) Assembly Insertion Policy: Once the robot arm moves a component near its target pose, the assembly insertion process begins. Assembly insertions are contact-rich tasks that require multi-modal sensing (e.g., force sensors and closed-loop control) to ensure precise alignment and secure connections. However, developing closed-loop assembly insertion skills is beyond the scope of this work and will be addressed in future research. In our current approach, human experts manually perform the insertion action."}, {"title": "V. EXPERIMENTS", "content": "In this section, we perform a series of experiments aimed at addressing the following questions.\n\u2022 Q1: Can our proposed hierarchical assembly graph generation module effectively extract structured information from manuals? (see Section V-A)\n\u2022 Q2: Can the per-step pose estimation be applicable to different categories of furniture and outperform previous settings? (see Section V-B)\n\u2022 Q3: How effective is the proposed framework in the assembly of furniture with manual guidance? (see Section V-C)\n\u2022 Q4: Can this pipeline be applied to real-world scenarios?(see Section V-D)\n\u2022 Q5: Can this pipeline be extended to other assembly tasks? (see Section V-E)\n\u2022 Q6: How should we determine and evaluate the key design choices of each module? (ablation experiments, see Appendices C and E)\nIn addition, we have included a comprehensive set of prompts utilized in the VLM-guided hierarchical graph generation process in Appendix K"}, {"title": "A. Hierarchical Assembly Graph Generation", "content": "In this section, we evaluate the performance of our VLM-guided hierarchical assembly graph generation approach. Specifically, we assess Stage II: Identifying Parts in Each Image using the IKEA-Manuals dataset [49]. We provide the rationale for excluding Stage I evaluation in Appendix H."}, {"title": "E. Generalization to Other Assembly Tasks", "content": "We design Manual2Skill as a generalizable framework capable of handling diverse assembly tasks with manual instructions. To assess its versatility, we evaluate the VLM-guided hierarchical graph generation method across three distinct assembly tasks, each varying in complexity and application domain. These include: (1) Assembling a Toy Car Axle (a low-complexity task with standardized components, representing consumer product assembly), (2) Assembling an Aircraft Model (a medium-complexity task, representing consumer product assembly), and (3) Assembling a Robotic Arm (a"}, {"title": "VI. LIMITATIONS", "content": "This paper explores the acquisition of complex manipulation skills from manuals and introduces a method for automated IKEA furniture assembly. Despite this progress, several limitations remain. First, our approach mainly identifies the objects that need assembly but overlooks other details, such as grasping position markings and precise connector locations (e.g., screws). Integrating a vision-language model (VLM) module to extract this information could significantly enhance robotic insertion capabilities. Second, the method does not cover the automated execution of fastening mechanisms, like screwing or insertion actions, which depend heavily on force and tactile sensing signals. We leave these challenges as directions for future work."}, {"title": "VII. CONCLUSION", "content": "In this paper, we address the issue of learning complex manipulation skills from manuals, which is essential for robots to execute such tasks based on human-designed instructions. We propose Manual2Skill, a novel framework that leverages VLM to understand manuals and learn robotic manipulation skills from manuals. We design a pipeline for assembling IKEA furniture and validate its effectiveness in real scenarios. We also demonstrate that our method extends beyond the task of furniture assembly. This work represents a significant step toward enabling robots to learn complex manipulation skills with human-like understanding. It could potentially unlock new avenues for robots to acquire diverse complex manipulation skills from human instructions."}, {"title": "APPENDIX", "content": "A. Per-step Assembly Pose Estimation Dataset\nWe build a dataset for our proposed manual guided per-step assembly pose estimation task. Each data piece is a tuple $(I_i, {P_i}_j, {T_i}_j, R_i)$, where $I_i$ is the manual image, ${P_i}_j$ is the point clouds of all the components involved in the assembly step, ${T_i}_j$ is the target poses for each component, and $R_i$ is the spatial and geometric relationship between components.\nInstruction manuals in the real world come in a wide variety. To cover as many scenarios as we might encounter in real-life situations, we considered three possible variations of instruction manuals when constructing the dataset, as shown in Figure 8. Our dataset encompasses a variety of furniture shapes. For each piece of furniture, we randomly selected some connected parts to form different subassemblies. Meanwhile, for each subassembly, there are multiple possible camera perspectives for taking manual photos. This definition enables our dataset to cover various manuals that we might encounter in real-world scenarios.\nFormally, for furniture consisting of M parts, we randomly select m connected parts to form a subassembly. Denoted as $P_{sub} = {P_1, P_2,\u2026\u2026, P_m }$, here each $P_i$ is a atomic part. Then, we randomly group the m atomic parts into n components while keeping all parts within the same group are connected, denoted as $P_{sub} = {{P_{11},\u2026\u2026 P_{1a_1}},\u2026\u2026{P_{n1},\u2026\u2026P_{na_n}}}$, where each $a_i$ represents the number of atomic parts in i-th component, and thus $\u03a3_{i=1}^n a_i = m$. We sample the point cloud for each component to consist of the point cloud of the data piece. We can also take photos of the subassembly from different perspectives.\nWe also provide annotations for equivalent parts in the auxiliary information. In this paper, we propose new techniques to leverage the auxiliary information for each assembly step, which significantly enhances the precision and robustness of our pose estimation model."}, {"title": "B. Pose Estimation Implementation", "content": "1) Loss Functions for Pose Estimation:\nRotation Geodesic Loss: In 3D pose prediction tasks, we commonly use the rotation geodesic loss to measure the distance between two rotations [53]. Formally, given the ground truth rotation matrix $R \u2208 SO(3)$ and the predicted rotation $\\hat{R} \u2208 SO(3)$, the rotation geodesic loss is defined as:\n$\\mathcal{L}_{rot} = \\arccos(\\frac{tr(\\hat{R}^T R) - 1}{2})$ (8)\nwhere $tr()$ denotes the trace of a matrix and $\\hat{R}^T$ is the transpose of $\\hat{R}$.\nTranslation MSE Loss: Following [29], we use the mean squared error (MSE) loss to measure the distance between the ground truth translation $t$ and the predicted translation $\\hat{t}$:\n$\\mathcal{L}_{trans} = ||t - \\hat{t}||^2$ (9)\nChamfer Distance Loss: This loss function minimizes the holistic distance between each point in the predicted and ground truth point clouds. Given the ground truth point cloud $S_1 = RP + t$ and the predicted point cloud $S_2 = \\hat{R}P + \\hat{t}$, it is defined as:\n$\\mathcal{L}_{cham} = \\frac{1}{|S_1|} \\sum_{x \\in S_1} \\min_{y \\in S_2} ||x - y||_2 + \\frac{1}{|S_2|} \\sum_{y \\in S_2} \\min_{x \\in S_1} ||y - x||_2$ (10)\nwhere $S_1$ is the point cloud after applying the ground truth 6D pose transformation, and $S_2$ is the point cloud after applying the predicted 6D pose transformation.\nPointcloud MSE Loss: We supervise the predicted rotation by applying it to the point of the component and use the MSE loss to measure the distance between the rotated point and the ground truth point:\n$\\mathcal{L}_{pc} = ||RP - \\hat{R}P||^2$ (11)\nEquivalent Parts: Given a set of components, we might encounter geometrically equivalent parts that we must assemble in different locations. Inspired by [60], we group these geometrically equivalent components and add an extra loss term to ensure we assemble them in different locations. For each group of equivalent components, we apply the predicted transformation to the point cloud of each component and then compute the Chamfer distance (CD) between the transformed point clouds. For all pairs $(j_1, j_2)$ within the same group, we compute the Chamfer distance between the transformed point clouds $P_{j_1}$ and $P_{j_2}$, encouraging the distance to be large:\n$\\mathcal{L}_{equiv} = - \\sum_{\\text{group }} \\sum_{(j_1, j_2)} CD(P_{j_1}, P_{j_2})$ (12)\nFinally, we define the overall loss function as a weighted sum of the above loss terms:\n$\\mathcal{L}_{total} = \\lambda_1 \\mathcal{L}_{rot} + \\lambda_2 \\mathcal{L}_{trans} + \\lambda_3 \\mathcal{L}_{cham} + \\lambda_4 \\mathcal{L}_{pc} + \\lambda_5 \\mathcal{L}_{equiv}$ (13)\nwhere $\u03bb_1 = 1$, $\u03bb_2 = 1$, $\u03bb_3 = 1$, $\u03bb_4 = 20$, $\u03bb_5 = 0.1$."}]}