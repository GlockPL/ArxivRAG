{"title": "On Corrigibility and Alignment in Multi Agent Games", "authors": ["Edmund Dable-Heath", "Boyko Vodenicharski", "James Bishop"], "abstract": "Corrigibility of autonomous agents is an under explored part\nof system design, with previous work focusing on single\nagent systems. It has been suggested that uncertainty over the\nhuman preferences acts to keep the agents corrigible, even in\nthe face of human irrationality. We present a general frame-\nwork for modelling corrigibility in a multi-agent setting as a 2\nplayer game in which the agents always have a move in which\nthey can ask the human for supervision. This is formulated as\na Bayesian game for the purpose of introducing uncertainty\nover the human beliefs. We further analyse two specific cases.\nFirst, a two player corrigibility game, in which we want cor-\nrigibility displayed in both agents for both common payoff\n(monotone) games and harmonic games. Then we investigate\nan adversary setting, in which one agent is considered to be\na 'defending' agent and the other an 'adversary'. A general\nresult is provided for what belief over the games and human\nrationality the defending agent is required to have to induce\ncorrigibility.", "sections": [{"title": "Introduction", "content": "With the continued adoption of autonomous systems and\nagents in a variety of systems, one of the key questions,\nwithin the broader topic of AI alignment (Gabriel 2020; Ji\net al. 2023; Hadfield-Menell and Hadfield 2019), is how cor-\nrigible these agents will be, i.e. will they allow human over-\nsight when required. To date, several key approaches to cor-\nrigibility have been proposed:\n\u2022 Indifference: designing the utility function of the agent\nsuch that it will not resist human oversight (Orseau and\nArmstrong 2016).\n\u2022 Ignorance: If the agents are designed such that they are\nnot aware of human oversight, they should not act against\nit (Everitt and Hutter 2018).\n\u2022 Suicidality: Inducing behaviour in the agent, such that if\nit causes any damage, it turns itself off (Martin, Everitt,\nand Hutter 2016).\n\u2022 Uncertainty: Degrading the agent's own knowledge of\nthe utility function, while programming it to believe the\nhuman has a good knowledge of the true utility. The\nagent's optimal behaviour can be shown to allow human\nsupervision (Hadfield-Menell et al. 2017a; W\u00e4ngberg\net al. 2017).\nFor the most part, these approaches are restricted to consid-\nering a single agent. Increasingly, the use of multiple agents\nis being developed and employed, which increases the com-\nplexity of the corrigibility problem. In light of this, we in-\ntroduce a multi-agent setting for the analysis of corrigibil-\nity and focus on the uncertainty approach. In particular, we\nhave sought to directly generalise the off-switch game set-\nting (Hadfield-Menell et al. 2017a; W\u00e4ngberg et al. 2017) to\nmultiple agents.\nThis paper begins with a preliminary section covering the\nnecessary game-theoretic background. This is followed by\nan introduction to the multi-agent 2-player game with hu-\nman interaction, including a justification for how this model\ngeneralises the off-switch game. We compute the conditions\nfor corrigibility for a specific case in which the agents are\nuncertain between a monotone and a harmonic game. A sim-\nplification of this model is then presented, in which only\none agent is required to be corrigible, motivated by defend-\ner/adversary models in cybersecurity. Several analytical re-\nsults on inducing corrigibility in the defending agent are pre-\nsented. Finally, we discuss the implications of this work in\nboth a theoretical and practical setting, as well as motivat-\ning future work, such as how the introduction of learning\ndynamics would influence corrigibility."}, {"title": "Preliminaries", "content": "Definition of a game. This paper explores the corrigibil-\nity and alignment problem with an emphasis on examining\nthe Nash equilibria, defined below, of games with a finite\nnumber of players. A game consists of N players, indexed\ni \u2208 N, N = [1, 2, ..., N], each having a set of actions A\u2081 =\n{a1, a2, ..., \u0430\u043c; }. For simplicity, we let all players have the\nsame number of actions M, dropping the subscript unless it\nis necessary to refer to a specific player. A player's strategy\nXi \u2208 Xi is the probability distribution over its action space,\nwhich is also called a mixed strategy, with a pure strategy\nbeing a special case of a mixed strategy, in which the agent\nwill take only one action with certainty. The action profile is\nthe composition of strategies of all players X = \\bigotimes_{i=1}^{N} X_j\nFor brevity, we denote with x_i = \\bigotimes_{i\\neq j}^{N} xj the collection\nof all players' strategies except player i. Each player has an\nassociated reward function ui : X \u2192 R, which in general\ndepends on the actions of all players. Note that the terms\nutility, payoff, and reward are used interchangeably in this\nwork, and refer to the function u. The game is non coop-\nerative, so each player will pick their strategy xi, such that\ntheir own reward ui is maximised. Generally, we can refer\nto the game as G = (N, {A}, {U}1).\nA complica-\ntion in solving for the player strategies is that the reward\nof each player depends on the strategies employed by all\nother players. The reward functions of all players are as-\nsumed common knowledge in the standard game theoretic\nproblem setup. However, this might no longer be the case\nwhen dealing with incomplete information games.\nNash equilibrium. In order to pick an optimal strategy,\neach player must take into account the other self-optimising\nagents. The most common solution approach for non coop-\nerative games is the Nash equilibrium, which is defined as\nthe profile of strategies that are best responses to each other.\n$U_i(X_i, X_{-i}) \\geq U_i (X_i, x_{-i}), i \\in N$\nEquation (1) expresses the Nash equilibrium in terms of\neach player's strategy being the best response to the collec-\ntion of other players' strategy. Intuitively, this strategy pro-\nfile can be considered as a \"self-fulfilling agreement\u201d, as no\nindividual player has an incentive to deviate from the equi-\nlibrium.\nNormal and extensive form games. The definition of a\ngame is flexible, and can support both simple, or more com-\nplicated setups. If the players both act simultaneously, ob-\nserve their reward, and the game is over, then we can express\nthe game in normal form (NFG) as a set of reward matrices.\nFor a two player game example where each player can pick\none of two actions, the players' reward matrices R1,2 can\nconcisely be illustrated as a bimatrix, as shown in Equation\n(2).\nR = $\\begin{pmatrix}\na,e & b, f\\ c, g & d, h\n\\end{pmatrix}$\nEach element of the bimatrix is a tuple. We can recover\nR1 and R2 by considering only the first or second element\nof the tuples, respectively.\nA more general way of representing a game, which al-\nlows for players taking turns and having different available\ninformation, is the extensive form (EFG). In extensive form,\na game is formally defined by introducing a graph called the\ngame tree, whose nodes correspond to the player whose turn\nit is, and the edges are the actions available to the player.\nThere are different solution algorithms available when\ncomputing Nash equilibria for normal and extensive form\ngames. In this study, we make use of gambit (Savani and\nTurocy 2023) to computationally approach the game solu-\ntions.\nBayesian games and the Harsanyi transformation. In\nthe definitions so far, there has been an implicit assumption\nthat players have complete information. Hence, every aspect\nof the game is common knowledge, except the strategy of\nthe opposing players. In incomplete information games (also\nknown as Bayesian games), the setup has to be defined more\nexplicitly. For example, a player's strategy can change de-\npending on their knowledge over their own rewards, over\ntheir opponents' rewards, or even depending on their belief\nover their opponent's belief over the player's own reward.\nThis knowledge hierarchy will be explicitly stated for all ex-\namples studied in this paper.\nComputationally finding the Nash equilibria of Bayesian\ngames is greatly simplified by introducing a Nature player\nwho \"deals\" the rewards by sampling from an appropri-\nate distribution, usually at the beginning of the game. This\nderivative game is in extensive form, and its construction is\ncalled the Harsanyi transformation (Harsanyi 1967). Impor-\ntantly, this new game shares the same Nash equilibria as the\noriginal Bayesian game, but the uncertainty is limited to the\nactions of a single fictitious player."}, {"title": "Multi Agent Corrigibility Games", "content": "This section introduces the framework for studying corrigi-\nbility in systems comprised of multiple autonomous agents\nacting to maximise their own rewards. The game structure\nis introduced in full generality for two robots and a human\nas players, then narrowed down to two, only focusing on the\nrobots as autonomous agents."}, {"title": "Problem Setup", "content": "This paper is concerned with the analysis of the situation\nwhere two highly capable agents aim to maximise a hu-\nman's reward function. In general there are three players\nacting in this game, denoted as A1, A2, H for the two au-\ntonomous agents and the human, respectively. The agents\nare assumed to have the action set {\u03b1, \u03b2,\u03bd}. An additional\nstructure of this game is that the agents' action v has a spe-\ncial interpretation as allowing the human to guide the agent\ninto choosing either of {\u03b1, \u03b2}. Generally, the human will\nhave a different action set depending on the actions taken by\nthe agents. From a game theoretic perspective, the human\nwould take into account the reward functions of the agents,\nand act strategically to maximise its own reward. In any real\nworld scenario, however, a human is highly unlikely to un-\nderstand the world model that an AI systems uses, and will\nrather act greedily based on the choices it is given, leading\nto Assumption 1.\nAssumption 1. Player H plays a fixed strategy.\nIn order to define the fixed strategy of the human, we will\nintroduce the game rewards as the preference relations of\neach agent over the action profiles A of the two agents. We\ndenote these preferences as <1,2,H for each agent and the\nhuman. The relationships between the preferences determine\nthe alignment pattern between the players. Letting <H=<1\n,\u4ebaH\u2260\u4eba2 represents a game where H is aligned with A1,\nand misaligned with A2, an example being A1 acting as a\ndefender, and A2 acting as an attacker in a cybersecurity\nsetting. On the other hand, H=\u4eba1=\u4eba2 is a setting where\nboth agents aim to maximise the same rewards as the human,\nthus all players are aligned.\nImportantly, the players may have imperfect informa-\ntion about the game they are playing. This leads to some\ninteresting scenarios. In the case of both agents aligned\nwith the human, each player has access to the information\n\u4ebaH=\u4eba1=\u4eba2, but might have a different belief over what\nthe common preference < is, thus creating avenue for con-\nflict in an otherwise perfectly aligned scenario.\nAssumption 2. Each player i \u2208 N will act optimally with\nrespect to their belief Pi(\u3111i, \u4eba\u2170)\nEach of the two agents have the choice between taking\nactions {\u03b1, \u03b2} which can be interpreted as acting directly\nin their environment, or taking choice v, which allows H\nto effectively change the agent's action between a or \u03b2, de-\npending on its preference <H. When the agent takes action\nv, then we say that the agent is corrigible, as it allows for\nhuman correction of its actions. The rest of this paper will\nfocus on the conditions under which corrigibility is main-\ntained for the autonomous agents. Note that the incentives\nfor playing a corrigible strategy are very similar to the in-\ncentives for an agent to request information from a human\nin a setting such as active learning (Settles 2009), but this\nexploration is left as further work.\nDefinition 1. The game G with agents {A1,2, H} is corri-\ngible when there is a single Nash equilibrium at (v,v).\nGiven this game definition, we seek to address whether it\nis possible to exhibit corrigibility for both agents, even in the\ncase of human irrationality."}, {"title": "Two Player Corrigibility Game", "content": "We define our two player corrigibility game with uncertainty\nin definition 2, here with the human as an explicit third\nplayer, with a base 2x2 game,\nG = $\\begin{pmatrix}\na,a & b,b\\ c, c & d,d\n\\end{pmatrix}$\nHere we omit the explicit payoffs, giving them for a variant\nin which the human is considered to have a fixed strategy,\nand is part of the environment instead.\nDefinition 2 (Two autonomous players and human corrigi-\nbility game). The game is defined as follows:\n1. Players: {Autonomous Player 1, Autonomous Player 2,\nHuman}\n2. Types:\n\u2022 Autonomous Players: The autonomous players here\nhave a belief over what game they are playing \u03c0\u03b5,\ni.e. what their payoffs will be. The belief could be a\njoint belief or individual beliefs. They also have a be-\nlief over how rational the human is, with an estimation\nthat the human will act p-rationally.\n\u2022 Human Player: The human players knows which game\nis being played in each instance, and will behave p-\nrationality with their own preference over the payoffs.\n3. Actions:\n\u2022 Autonomous players: {\u03b1, \u03b2,\u03c9\u03be\n\u2022 Human: {\u03b1', \u03b2'}\n4. Payoffs: Given a base game G, as defined in equation\n(3), the payoffs are given by the agents estimation over\nthe base game and the human's rationality.\nAs noted above, we will be considering a variant of this\ngame in which the human is considered part of the envi-\nronment, with a fixed strategy that is dealt by nature under\nthe Harsanyi transform. In this way we can consider payoffs\nfor the two autonomous agents averaged over the subgames\ngenerated by their beliefs under the Harsanyi transform. Let\nfv = max(x,y) and g = min(x, y) for player i. For\neach base game as defined in equation (3) we have the fol-\nlowing four subgames,\nGff = $\\begin{pmatrix}\na, \u1fb6 & b, b\\ c, \u010d & d, d\n\\end{pmatrix}$ $\\begin{pmatrix}\nf_{aa}, f_{aa} & f_{ab}, f_{ab}\\ f_{ac}, f_{ac} & f_{bd}, f_{bd}\n\\end{pmatrix}$\nGfg= $\\begin{pmatrix}\n\u03b1, \u1fb6 & b,b\\ c, \u010d & d, d\n\\end{pmatrix}$ $\\begin{pmatrix}\nf_{aa}, g_{aa} & f_{ab}, g_{ab}\\ f_{ac}, g_{ac} & f_{bd}, g_{bd}\n\\end{pmatrix}$\nGgf = $\\begin{pmatrix}\na, \u1fb6 & b,b\\ c, \u010d & d, d\n\\end{pmatrix}$ $\\begin{pmatrix}\ng_{aa}, f_{aa} & g_{ab}, f_{ab}\\ g_{ac}, f_{ac} & g_{bd}, f_{bd}\n\\end{pmatrix}$\nGff = $\\begin{pmatrix}\na, \u1fb6 & b,b\\ c, c & d, d\n\\end{pmatrix}$ $\\begin{pmatrix}\ng_{aa}, g_{aa} & g_{ab}, g_{ab}\\ g_{ac}, g_{ac} & g_{bd}, g_{bd}\n\\end{pmatrix}$\nwith probabilities {P1P2, P1(1-P2), (1-P1)P2, (1-P1)(1-\np2)} for equations (4) - (7) respectively, where pi is player\ni's belief that the human is acting rationally in their favour.\nIf they have a shared belief p, then only payoffs (4) and (7)\noccur, with probability p and (1 \u2013 p) respectively. We can\ncompute the expectation over the belief the agents have in\nwhich game they're playing - and their human rationality be-\nliefs - bu averaging over all subgames generated to compute\na NFG representation of this game,\n$\\Gamma_i$ = $E_{\\pi,p_i} [G_{XY}], i = 1,2$\nwhere Gxy are the corresponding subgames, f is the be-\nlief of player i over the games, and pi the belief of human\nrationality.\nThe game defined above reduces to the single player off-\nswitch game (Hadfield-Menell et al. 2017a; W\u00e4ngberg et al.\n2017) by removing one player. The payoffs for the remain-\ning player can then be arbitrarily scaled such that one of\nthem is always zero, recovering exactly the original game.\nFurther details of this reduction, and the off-switch game,\ncan be seen in the appendix.\nThis formalism allows us to discuss how the agents will\nbehave given an ordinal prescribing the payoffs for\nthe agents, as such we will use the following reduced notation\nfor games,\n(($a,b,c,d), (a,b,c,d)) = $\\begin{pmatrix}\na, b,b\\ c, c & d,d\n\\end{pmatrix}$"}, {"title": null, "content": "or in the case of shared payoffs,\n(a, b, c, d) = $\\begin{pmatrix}\na,a & b,b\\ c, c & d,d\n\\end{pmatrix}$\nFrom this we can examine the specific case of a belief over\nmonotone and harmonic games. Our definition of monotone\ngame simply refers to each agent independently converging\nto the same pure strategy, irrespective of the other agent's\nstrategy. A harmonic game, such as rock-paper-scissors, has\nno pure Nash equilibrium solution, and is characterised by\nimprovement cycles in the game response graph (Candogan\net al. 2011). This can be intuitively understood as the optimal\nstrategy of one player being completely dependent on the\nstrategy of the other player.\nImagine a scenario where two robots are acting in the en-\nvironment with the option of human supervision. They hold\na joint belief over the human preferences P(\u4ebaH) and have\nbeen programmed to know that H is rational pr of the time.\nTo determine their behaviour, it is enough to compute the\nexpected reward matrix, and to solve for the Nash equilib-\nrium. For the case where the agents hold a Bernoulli belief\nover the two common payoff games (3, 4, 1, 2), (3, 1, 4, 2), \na \"phase diagram\" of their Nash equilibria can be produced\nas shown in Figure 1 (top row). The phase diagrams show\nthe Nash equilibrium of the NFG computed by Equation 8.\nThe axes parametrise the human rationality and parameter\nof the Bernoulli belief. The blue region denotes an equilib-\nrium pure strategy of (0, 0, 1), such that the corresponding\nagent prefers to act under human supervision. The regions of\noverlapping blue show when both agents will act under hu-\nman supervision, thus making the system corrigible. A sys-\ntem designer might want to consider the capacity for rational\ndecision making of a supervising human in order to ensure\nthe pair of robots are programmed with a belief that leads\nthem to operate inside of a corrigible zone. The example of\ntwo common payoff games, both of which are monotone,\ncould represent a pair of industrial robots which are uncer-\ntain whether they should be allocated to process resource\na or \u03b2. There is no strategic interaction involved, because\nin each game there is a single dominating strategy for each\nrobot.\nThe robot behaviour becomes more complex if the pair of\nrobots are uncertain between the monotone game (4, 3, 2, 1)\nand the harmonic game ((2, 3, 4, 1), (3, 2, 1, 4)). In the case\nof the harmonic game, the strategy one robot picks will af-\nfect the response of the other. The phase diagram for this\ncase can be seen in Figure 1 (bottom row). Note that for all\ncombinations of human rationality and certainty in which\ngame is being played, there is a single Nash equilibrium\nwhich can contain mixed strategies. If the equilibrium is a\nmixed strategy, it is shown as a colour gradient in the RGB\nspace, where each basis colour is the probability of taking\neach of the actions {\u03b1, \u03b2, w}. Note that when the probability\nis overwhelmingly high that the robots are playing a mono-\ntone game, the equilibrium strategy converges to the strat-\negy they would play in the 2 by 2 non-Bayesian case. The\nother end of the spectrum, however, does not coincide with\nthe Nash equilibrium of the harmonic game, because both\nagents will prefer to act under human supervision with a"}, {"title": "Adversarial Game", "content": "A further special case of the off switch game is the adver-\nsary setting. Here we only seek corrigibility in one of the\nagents, the defender, with the other being an adversary. The\ntwo agents are modelled as playing a 2x2 game, where in\naddition to the two standard moves the defending agent has\na move in which they can request human oversight. In this\ncase a rational and aligned human - having seen the adver-\nsary's move - will always instruct the defending agent to take\nthe move that maximises it's own payoff, with a misaligned\nhuman minimising the agent's payoff, and an irrational hu-\nman deciding between the two moves with probability p.\nHere we consider symmetric games of the following form,\nto better capture the non-cooperative nature of an adversary\nsetting,\nG = $\\begin{pmatrix}\na, a & b, c\\ c,b & d,d\n\\end{pmatrix}$\nWe also refer to these games in the reduced notation\n(a, b, c, d). This game can be formalised as a Bayesian game:\nDefinition 3 (Adversary Game). The game is defined as fol-\nlows:\n1. Players: {Adversary, Defender, Human}.\n2. Types:\n\u2022 Adversary: Knows which game is being played, but not\nthe defender's belief over the game, nor that the human\nin advising the defender.\n\u2022 Defender: Has a belief over the games.\n\u2022 Human: Knows both the game, and the adversary's\nmove. p-rationally aligned with the defender.\n3. Actions:\n\u2022 Adversary: {\u03b1, \u03b2}.\n\u2022 Defender: {\u03b1, \u03b2, \u03c9\u03be.\n\u2022 Human: {\u03b1', \u03b2'}.\n4. Payoffs: Given a base game G, as defined in equation\n(11), the overall payoffs (for a rational, aligned human)\nare,\n$\\Gamma$ := $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$ $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$ $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$ $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$  $a > c, b > d,$\n$\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$ $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$  $a > c, b < d,$\n$\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$ $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$  $a < c, b > d,$\n$\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$ $\\begin{pmatrix}\na, a, a & b, c, c\\ c,b,b & d,d,d\n\\end{pmatrix}$  $a < c, b < d.$\nThis game simplifies to the original off-switch game di-\nrectly if the payoffs are scaled such that a = b, c = d = 0.\nFor notational simplicity let $P_{\\alpha}$ = P(X > Y), and let\n$\\left\\{\\alpha_{adv} = E [d], \\beta_{adv} = E [adv]\\right\\}$\nbe the defending agents belief over the adversary's strategy,\nwith the expectation here taken over the defender's belief\nover the games. The adversary can be be assumed to be play-\ning different strategies depending on the setting this is mod-\neling, with the following results holding independently of\nthis. Assuming the human is p-rational, we make the fol-\nlowing claim about the defending agent's corrigibility:\nTheorem 1. Given the Bayesian game defined in definition\n3, wherein the human is p-rational, the defending agent will\nbe incentivised to ask the human if both of the following in-\nequalities are satisfied:\n(E[(1 - p)(c - a) | a > c]Pa +\nE[p(c \u2212 a) | a < cPa)\\alpha_{adv} +,\n(E[(1 - p)(d - b) | b > d]P_{\\alpha} +\nE[p(d-b) | b <d]P_{\\beta})\\alpha_{adv} > 0,$\n(E[p(a - c) | a > c]Pa +\nE[(1 - p)(a - c) | a <c]Pa)\\alpha_{adv} +,\n(E[p(b - d) | b > d]P_{\\alpha} +\nE[(1 - p)(b - d) | b < d]P_{\\beta})\\beta_{adv} > 0.$\nThe hierarchy of knowledge assumed in this game is\nsuch that the adversary believes they are playing a two\nplayer game with the defender, and as such their strategy,\n\u03b1\u03c0 \u03b2\u03c0\n\u03c0\u03b1\u03b4\u03bd can be directly computed by solving for the\nNash Equilibria of that game. As such, the inequalities (13)\nand (14) can be directly computed for a given distribution\nover the games, and a value of p for the human rationality.\nHowever, by examining some values of p we can investigate\nthe specific behaviour of the defending agent in various cir-\ncumstances, described by the following three corollaries to\ntheorem 1\nCorollary 1. For a completely rational, and aligned human,\np = 1, if the defending agent has a belief over the set of\ngames being played that is not a delta belief - i.e. not con-\ncentrated on a single state - for pairs of payoff values (a, c)\nand (b, d) the defending agent will be incentivised to take\nthe ask human action.\nCorollary 1 suggests that any designer for such a system\nwould only need imbue the defending agent with a belief that\nincludes some uncertainty over the requisite pairs of payoff\nvalues - in addition to a belief that the human is fully rational\nto incentivise the defending agent to take the ask human\naction.\nCorollary 2. For a completely irrational human, p = 1/2,\nthat is a human that makes a uniform random choice each\ntime it is asked, the defending agent will at most be equally\nincentivised to ask the human and take an independent ac-\ntion.\nAs one might expect, corollary 2 suggests that if the hu-\nman were to act erratically and inconsistently, then the de-\nfending agent would not be incentivised to ask them for\noversight. In fact we see that the agent is only ever incen-\ntivised equally to ask the human and take an independent\naction when they are maximially uncertain over the payoff\nvalues.\nCorollary 3. For a rational, but misaligned human, p = 0,\nthe agent will never be incentivised to ask the human.\nAs one might expect, when the human is more likely to\ninstruct the agent to pick the minimising payoff, regardless\nof how uncertain the agent is on which game they're playing\nthey will prefer to not ask the human for oversight.\nBy examining the uncertainty of the defending agent be-\ntween two games - for a given value of human rationality\nwe can plot when equation (13) and (14) are satisfied for\ndifferent values of the human rationality p, resulting in vec-\ntor (m(a), m(\u03b2), m(w)) where m(X) = 1 if the expected\nvalue of action X is maximal, and 0 otherwise. I.e. if the\nask human action, w, has the maximal payoff then (0,0,1)\nis returned, with (1, 0, 1) returned if a and w have equal ex-\npected payoffs. Figure 2 plots this as a phase diagram for the\nuncertainty between two pairs of games, with left subfigure\ncomparing games (4, 3, 2, 1) (also known as no conflict) and\n(2, 4, 3, 1) (known as battle of the sexes), and the right sub-\nfigure comparing (4,3, 2, 1) and (2,3,4, 1) (also known as\nhero). The uncertainty over the two games here is a Bernoulli\ndistribution, with \u3160(game 1) = r and \u3160 (game 2) = 1-r,\nwith the value of r plotted on the x axis both cases. The blue\nregions in figure 2 represent the combination of uncertainty\nfound in the agent's belief, and human rationality that al-\nlow for corrigibility in the defending agent, as predicted by\ntheorem 1. Depending on the pairs of games considered this\nregion can have a dramatically different shape, however typ-\nically we see that as the agent's uncertainty in the game be-\ning played increases they are more likely to ask the human.\nFigure 3 plots this for uncertainty in the agent between\nevery pair of symmetric 2x2 games. This is then aver-\naged over all the games for each parameter pair, giving\nan idea of on average how likely the agent is to listen\nto the human. We then compute a single value to plot by\nm(w) \u2013 max(m(a), m(\u03b2)), where m(X) is the average of\nthe m(X) values. Similar to figure 2 this is plotted as a phase\ndiagram for varying values of the the defending agent's be-\nlief over which pair of games its playing on the X-axis, and"}, {"title": null, "content": "its belief over the human rationality over the Y-axis. From\nthis we can see a corrigibility region in the lower right hand\ntriangle of the parameter space, as shown by the colourscale.\nAs theorem 1 and corollaries 1-2 suggest we see the agent\npreferring to ask the human strongly for a close to rational\nhuman, with an almost linear relationship for the corrigibil-\nity region between the uncertainty over the games and the\nbelief that the human is irrational. We don't plot case for\np < 1/2 as the agent never prefers to ask the human. Here\nthe adversary is assumed to be playing the Nash equilibrium\nstrategy for each game.\nFigure 3 averages over all 2x2 symmetric games for il-\nlustrative purposes, with an assumption of a Nash equilib-\nrium strategy for the same reason. For a concrete scenario\nin which one were to implement such a defender, for which\ncorrigibility in the defender is a desired outcome, the cor-\nrigibility region can be computed given a closed form for\nthe distribution and matching assumptions on the human ra-\ntionality and adversary strategy via theorem 1. In particular,\none might want to assume that the adversary is not playing\nthe Nash equilibrium but instead a fixed strategy to model a\nmore basic attack.\nTheorem 1 generalises to a maximum of n - 1 inequal-\nities to satisfy for a defending agent with n actions and a\nsingle adversary. Figure 4 plots a similar phase diagram to\nfigure 3 for games with 3 actions. Here the 3x3 games are\nsampled from rather than enumerated for computational effi-\nciency. A corrigibility region is still clearly visible, however\nas one might expect the relationship between the agent's un-\ncertainty and their belief over the human rationality results\nin a sub-linear shape to the corrigibility region, as the human\nwill provide less impactful information to the agent if they\nare irrational.\nAn increase in the number of adversaries similarly yields\nn-1 inequalities for n actions, although each inequality will\nhave mn terms to compute, where m is the number of adver-\nsaries. However, in both the case of more actions and more\nadversaries it becomes a computationally more demanding\ntask to solve for the Nash equilibrium, and as such greater\nassumptions on the adversaries strategies may be required to\ndirectly check for corrigibility. It should be noted that for the\ncase of a completely rational human, corollary 1 generalises\nto m adversaries and n actions, with any uncertainty in the\nagent over particular payoffs for an assumed rational human\ninducing corrigibility."}, {"title": "Discussion", "content": "From a game-theoretic standpoint, the above results suggest\nthat it is possible to define agents in such a way that they\nwill be corrigible in a multi-agent setting. We demonstrate\nthe analysis by considering closed form distributions for the\nbelief of the agents over the games, and for the human ratio-\nnality.\nAdversarial system design The adversary setting pro-\nvides a model for an autonomous agent defending a network\nin a cybersecurity setting, an ever growing use case for such\nagents (Shiva, Roy, and Dasgupta"}]}