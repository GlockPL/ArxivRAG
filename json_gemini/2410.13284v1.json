{"title": "Learning to Route with Confidence Tokens", "authors": ["Yu-Neng Chuang", "Helen Zhou", "Prathusha Sarma", "Parikshit Gopalan", "John Boccio", "Sara Bolouki", "Xia Hu"], "abstract": "Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.", "sections": [{"title": "1 Introduction", "content": "Recent years have seen an explosive growth in the deployment of Language Models (LLMs), in forms ranging from helpful conversational agents [1, 47, 53], to frameworks for automating software workflows [15, 50], to tools tailored to domain-specific tasks. LLMs have been used to summarize doctor-patient interactions [29, 45, 3], answer questions from students [43, 51], and provide customer support [28]. As LLMs are given more agency in settings of increasing consequence, it becomes crucial to know when an output is reliable [52]. Given knowledge of the trustworthiness of an output, systems can proactively prevent faulty behavior by seeking a second opinion (e.g. by routing to a more costly LLM), or choosing to abstain from answering (e.g. instead defaulting to a safe behavior).\nHowever, the state-of-the-art in LLMs face challenges with providing an accurate estimate of confidence in whether a prediction is correct. Several works identify that token probabilities (derived from a softmax over logits) are not well-aligned, or calibrated, with the actual probabilities of correctness [22, 11, 49]. Others have found that modifying the prompt by asking the LLM to verbalize its confidence yields slightly better results [46, 48, 35]; however, these results may be subject to the dataset and prompt engineering, often leading to unstable or unreliable results. Since LLMs are typically trained using a cross-entropy loss, they can overfit on accuracy rather than calibration, often leading to overconfidence and misalignment with real-world distributions. Thus, a natural question arises: how can we train LLMs to accurately estimate their own confidence levels, while preserving their performance on tasks of interest?\nTo answer this question, we introduce Self-Reflection (Self-REF), a lightweight framework for fine- tuning LLMs to accurately assess confidence in their predictions while preserving their performance"}, {"title": "2 Related Work", "content": "In this section, we briefly introduce related studies on uncertainty quantification, LLM routing, and LLM rejection learning. More discussion is provided in Appendix B."}, {"title": "2.1 Uncertainty Quantification in LLMS", "content": "Recent studies on calibrating LLMs have explored leveraging internal features like logits and attention scores to estimate uncertainty without additional training [56, 35, 48]. Post-processing methods such as temperature scaling [14], prompting LLMs to verbalize confidence scores [46], and instruction learning [55, 40, 32, 34] have also shown improvements in calibration. However, uncertainty scores derived from LLMs may exhibit a weak correlation with actual prediction correctness [22]. Thus, while calibration is important, our work goes beyond it and focuses on the downstream correctness and model confidence scores for the purpose of LLM routing and rejection learning."}, {"title": "2.2 LLM Routing", "content": "In confidence-based query-routing scenarios, one line of research trains additional classifiers to route queries to language models (LLMs) based on specific performance metrics and routing data, forming routing pipelines with mixture-of-experts (MoE) systems to boost performance [12, 41, 44, 24]. Another approach leverages routers to optimize utility-efficiency trade-offs by reconstructing model architectures, aiming to reduce inference costs while maintaining performance [8, 54]. In contrast to existing work that focuses on training query routers based on performance metrics, our approach routes queries based on the model confidence of each LLM's output from the LLMs themselves. The confidence estimations in our method are predicted in autoregressive decoding, making the model's confidence more closely aligned with the correctness of the output answers. This dynamic approach allows for more accurate and adaptive routing decisions based on real-time confidence assessments."}, {"title": "2.3 LLM Rejection Learning", "content": "Learning to reject [10] was originally introduced to equip the Bayes classification model with a rejection option for predictions. Despite advancements in LLMs, their predictions can still be prone to uncertainty due to inherent knowledge limitations. This highlights the need for LLMs to have the ability to refuse to answer, especially in situations where it is infeasible to fallback upon more advanced models.\nLLMs can be trained for the rejection task through rejection knowledge injection [7], an additional rejection LLM agent [36, 37], and rejection-oriented loss adaptations [33, 38]. However, such approaches involving extra knowledge injection or loss adaptation may degrade the base performance of the LLM. In this work, we aim to equip LLMs with the capability for rejection learning based on their confidence levels, without the need for designing new loss functions, thereby preventing any potential degradation in the performance of downstream tasks."}, {"title": "3 Learning Confidence Tokens", "content": ""}, {"title": "3.1 Problem Setup and Notation", "content": "Consider any local large language model $M : \\mathcal{X} \\rightarrow \\mathcal{Y}$ with input query domain $\\mathcal{X}$ and output prediction domain $\\mathcal{Y}$. Here, local refers to a model that one has the ability to fine-tune (as opposed to API access). Let $\\mathcal{D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$ denote a dataset, where $x^{(i)}$ are queries and $y^{(i)}$ are ground truth answers to the queries. Let $\\mathcal{D}_{train}$ denote the training split of the dataset, $\\mathcal{D}_{val}$ the validation split, and $\\mathcal{D}_{test}$ the test split, where $\\mathcal{D} = \\mathcal{D}_{train} \\cup \\mathcal{D}_{val} \\cup \\mathcal{D}_{test}$. The goal in learning confidence tokens is to fine-tune $M$ on $\\mathcal{D}_{train}$ to use confidence tokens, two special tokens with trainable embeddings: as assessed on $\\mathcal{D}_{test}$, the confident token $\\text{<CN>}$ should be generated when a model is confident that its prediction is correct (i.e., $\\hat{y}^{(i)} = y^{(i)}$), and the unconfident token $\\text{<UN>}$ should be generated when a model is not confident that its prediction is correct (i.e., $\\hat{y}^{(i)} \\neq y^{(i)}$)."}, {"title": "3.2 Self-REF", "content": "The Self-REF framework fine-tunes a base model $M$ to use confidence tokens and also learn their embeddings. It involves (i) confidence token annotation, where data are augmented with confidence tokens, and (ii) Self-REF fine-tuning on the augmented data, and (iii) confidence score extraction.\nConfidence Token Annotation To incorporate model feedback into our training process, we first use the base model $M$ to generate predictions $\\hat{y}^{(i)}$ for each sample $i = 1, ..., N_{train}$ in the dataset, where $\\hat{y}^{(i)} \\in \\mathcal{Y}$. For instances where $y^{(i)} \\neq \\hat{y}^{(i)}$ (the prediction is incorrect), we construct an augmented dataset $\\mathcal{D'}_{train,\\text{ <UN>}} = \\{(x^{(i)}, \\hat{y}^{(i)} \\text{ <UN>})\\}$, where $\\hat{y}^{(i)} \\text{ <UN>}$ denotes the concatenation of the incorrect prediction $\\hat{y}^{(i)}$ with an unconfident token $\\text{<UN>}$. Conversely, for instances where the prediction is correct ($y^{(i)} = y^{(i)}$), we create the set $\\mathcal{D'}_{train,\\text{ <CN>}} = \\{(x^{(i)}, \\hat{y}^{(i)}\\text{<CN>})\\}$, with $\\hat{y}^{(i)} \\text{ <CN>}$ representing the correct prediction concatenated with a confident token $\\text{<CN>}$. The augmented training dataset is then formed by combining these two datasets. This augmentation strategy enriches the training data by explicitly encoding the correctness of the model's predictions, thereby providing additional supervisory signals that can guide the model to better distinguish between correct and incorrect responses. The detailed steps of confidence token annotation are shown in Algorithm 1.\nSelf-REF Fine-tuning Next, the base model $M$ is fine-tuned using $\\mathcal{D'}_{train}$. The confidence tokens $\\text{<UN>}$ and $\\text{<CN>}$ are special tokens during training, initialized as the average of other existing token embeddings. Gradients from the incorrect answers in the unconfident samples are masked out, to avoid increasing the probability of an incorrect answer given a query $P(\\hat{y}^{(i)} \\neq y^{(i)} | x^{(i)})$. Instead, the model learns to increase the probabilities of confident tokens given correct answers $P(\\text{<CN>} | \\hat{y}^{(i)} = y^{(i)}, x^{(i)})$, probabilities of unconfident tokens given incorrect answers $P(\\text{<UN>} | \\hat{y}^{(i)} \\neq y^{(i)}, x^{(i)})$, and probabilities of correct answers given the query $P(y^{(i)} = y^{(i)} | x^{(i)})$.\nNote that the unconfident and confident tokens are generated end-to-end conditioned on both the prefix query and the answer that the LLM itself generated, in contrast to prior work [41, 12, 44] which typically routes based on the query alone with an extra router. Furthermore, by fine-tuning with the LLM's own predictions on the training set, Self-REF customizes the notions of uncertainty to the LLM itself, rather than to the inherent uncertainty of a query.\nConfidence Score Extraction After Self-REF training, one can extract a continuous confidence score $C_M(x^{(i)}, \\hat{y}^{(i)}) \\in [0, 1]$ by getting the token probability of $\\text{<CN>}$, normalized over the sum of the $\\text{<UN>}$ and $\\text{<CN>}$ token probabilities: $C_M(x^{(i)}, \\hat{y}^{(i)}) = \\frac{P(\\text{<CN>})}{P(\\text{<UN>}) + P(\\text{<CN>})}$. This continuous score gives us some control over navigating different tradeoffs in downstream settings."}, {"title": "3.3 Learning to Route and Reject", "content": "Confidence-based Routing to a Larger Model Often, constraints on computing resources and data may prevent researchers from fully fine-tuning very large LLMs. However, suppose one has the ability to fine-tune a smaller local LLM, as well as API access to a very large LLM. Given the limited capabilities of smaller LLMs and the high cost of very large LLMs, can we effectively route queries to the larger model based on the confidence level of the smaller model? How does one trade-off between cost and accuracy?\nIn confidence-based routing, answers where the smaller fine-tuned local LLM is uncertain, i.e., $C_M(x^{(i)}, \\hat{y}^{(i)}) < t$ for some chosen threshold $t \\in [0,1]$, will be routed to a more powerful LLM $M_{Large}(\\cdot)$. For answers with $c_M(x^{(i)}, \\hat{y}^{(i)}) \\geq t$ (i.e., certain), the answers from the local LLM $M(\\cdot)$ will be directly returned to the user.\nConfidence-based Rejection A larger LLM may not be accessible in every situation. If one does not have access to a larger more capable model, one may still want to utilize measures of uncertainty in order to decide whether to rely on the model's prediction. This experimental setup studies the following scenario: \u201cSometimes, none of the provided options are good. How good am I at saying that I am not confident in any of the answers?\"\nTo study this question, we create an evaluation set where half of the samples do not contain the ground truth, i.e., we remove all ground-truth information from $x^{(i)}$, and replace the label with \u201cnone of the above,\" $y^{(i)} = \\{\\emptyset\\}$. Ideally, the model's confidence in any of the remaining answers should be low, and we can evaluate how good a policy of choosing to abstain from answering the question does against this dataset. Thus, if $M(\\cdot)$ has a low confidence, i.e., $c_M(x^{(i)}, \\hat{y}^{(i)}) < t$ for a chosen threshold $t \\in [0, 1]$, we treat it as if the model were to abstain from answering, i.e., $\\hat{y}^{(i)} = \\{\\emptyset\\}$."}, {"title": "4 Experiment Setup", "content": ""}, {"title": "4.1 Datasets and Baselines", "content": "Dataset All experiments are conducted on four public datasets: Massive Multitask Language Understanding (MMLU) [17, 18], OpenbookQA [9], GSM8K [9], and MedQA [25] datasets. The MMLU dataset consists of multiple-choice questions covering a wide range of knowledge domains with 57 distinct tasks. The OpenBookQA is a question-answering dataset designed to test deep understanding through multi-step reasoning, common knowledge, and text comprehension, similar to open-book exams. The GSM8K is a dataset containing graduate school math questions, where each question is collected from the Math World Problem Repository [42]. The MedQA is a multiple-choice open domain question answering dataset for solving medical problems collected from professional medical board exams. More details can be found in Appendix C.\nBaselines We compare the Self-REF framework to four state-of-the-art baselines for measuring model confidence, following the settings from existing work. Details about the prompts and hyper- parameters utilized in these baselines are given in Appendix E. We list the baselines as follows:\n\u2022 Verbalizing Uncertainty [46]: Prompt the LLM with in-context learning to yield confidence scores between 0 and 1. This encourages the LLM to generate its response while simultaneously estimating the confidence level of the response.\n\u2022 Verbalizing Yes/No Tokens [46]: Prompt the LLM with a question of whether the model is confident in its answers. The confidence score is calculated by normalizing the probabilities of the \u201cYes\u201d and \"No\" tokens by their sum."}, {"title": "4.2 Experimental Settings", "content": "Confidence-based Routing Task In the routing task, we aim to accurately route instances that the smaller LLMs (Llama3-8B-Instruct and Mistral-7B-Instruct) struggle with to the larger LLMs (Llama3- 70B-Instruct) for improved performance. All experiments utilize Llama3-70B-Instruct with only its strong in-context learning capabilities during instance routing. The routing decisions are determined by the probabilities of the predicted confidence token $\\text{<CN>}$ and $\\text{<UN>}$ generated from Self-REF. To characterize the trade-off between cost and performance, we analyze performance along several possible routing thresholds $t$. In particular, for a set of confidence scores $C_{M, \\mathcal{D}} = \\{C_M(x^{(i)}, \\hat{y}^{(i)})\\}_{i=1}^N$, we set $t$ at quantiles, where $t = Q_p (C_{M, \\mathcal{D}})$ for $p \\in \\{0, 1, 2, ..., 20\\}$. Instances are routed to the larger LLM when $C_M(x^{(i)}, \\hat{y}^{(i)}) < t$, so these thresholds correspond to routing approximately 0%, 5%, ..., 100% of total queries to the larger LLM.\nConfidence-based Rejection Learning Task The rejection learning task is evaluated on the MMLU and OpenbookQA multiple-choice datasets. Self-REF does not explicitly train for the rejection learning task, as no additional samples are added where \"None of the above\" is a choice. This experiment tests whether the notion of confidence encoded by $\\text{<UN>}$ and $\\text{<CN>}$ may nevertheless be useful for determining whether to reject all provided options.\nTo assess performance on the rejection learning task, we construct a specialized evaluation set where 50% of the time, the correct answer choice is removed from the choice list in the input question. Ideally, for any question where the ground truth choice was removed, the Self-REF fine-tuned LLM should append the $\\text{<UN>}$ token to any selected answer, since it would be incorrect. In order to examine the trade-off between rejecting too many samples vs. providing too many incorrect answers, we again utilize the $c_M(x^{(i)}, \\hat{y}^{(i)})$ confidence score with several possible thresholds $t \\in [0, 1]$.\nImplementation Details We select Llama-3-8B-Instruct [47] and Mistral-7B-v0.3-Instruct [23] models as the pre-trained local base LLMs for Self-REF. Llama-3 70B model [47] is the larger, more powerful LLM to which unconfident queries are routed. Parameter-efficient fine-tuning is performed using LoRA adapters with ranks 2, 4, and 8 following the observations from [16]. Under at most eight epochs of training, all other hyperparameters on model training are decided by grid search with the perplexity loss on validation sets. More details of hyper-parameters are in Appendix E."}, {"title": "5 Experiment Results", "content": "We conduct experiments to evaluate the performance of Self-REF, centered around the following three research questions: RQ1: Compared with state-of-the-art baselines, how does Self-REF perform on confidence-based routing? RQ2: How reliable are confidence scores from Self-REF for the rejection learning task? RQ3: How well-aligned are confidence-token-based scores of Self-REF and the actual probabilities of correctness?"}, {"title": "5.1 Routing Performance (RQ1)", "content": "We analyze the routing performance from both a system-level accuracy and efficiency perspective.\nOverall Accuracy Confidence-based routing using Self-REF consistently achieves the best accuracy vs. routing rate trade-off for all four datasets (MMLU, OpenbookQA, GSM8K, MedQA) and both local LLMs (Llama3-8B-Instruct model and Mistral-7B-Instruct). Using Self-REF on Llama3-8B-Instruct for MMLU, confidence-based routing of 39% of queries can achieve comparable performance to routing to the Llama3-70B-Instruct model alone. Similar observations hold for the OpenbookQA, GSM8K, and MedQA datasets, where routing the Llama3-8B-Instruct model with rates 49%, 65%, and 40%, respectively, can yield comparable performance to Llama3-70B- Instruct alone. For Self-REF on Mistral-7B-Instruct, the minimum routing rates to achieve parity with Llama3-70B-Instruct are 70%, 50%, 75%, and 70% for MMLU, OpenBookQA, GSM8K, and MedQA, respectively.\nEfficiency Gains Smaller local models often have lower latency (sec) during inference as well as lower monetary cost. Thus, routing only a fraction of queries to larger, more powerful models can significantly improve the efficiency of the overall system compared to relying entirely on the more powerful models. For each small local LLM (Llama3-8B-Instruct and Mistral-7B-Instruct), we select the best routing rate as the minimum routing rate that achieves comparable performance to Llama3-70B-Instruct. Without sacrificing performance compared to Llama3-70B-Instruct, we observe that Self-REF achieves as much as a 2.03\u00d7 latency improvement for Llama3-8B-Instruct on MMLU and a 2.00\u00d7 latency improvement for Mistral-7B-Instruct on OpenbookQA."}, {"title": "5.2 Rejection Learning Performance (RQ2)", "content": "In this section, we analyze the utility of confidence scores from Self-REF for the purpose of learning to abstain from answering. Without incorporating additional knowledge of the rejection task during training, Self-REF and baselines rely solely on thresholding the confidence scores to determine whether to reject a prediction. In safety-critical applications where one may hope to detect cases should bed abstained from answering (i.e., high recall on the reject option) while retaining performance on the task at hand (i.e., low false positive rate), it is useful to examine the ROC curve, which makes this trade-off. Across all thresholds, we observe that on both MMLU and OpenbookQA dataset, Self-REF outperforms all baselines on the rejection learning task. Because the confidence tokens are trained and conditioned on the correctness of the predicted answers, Self-REF has a higher likelihood of being aware of the correctness of its generated responses using these tokens. In this manner, Self-REF may better distinguish when there is no correct answer to choose, resulting in better rejection learning capabilities."}, {"title": "5.3 Calibration with Utility (RQ3)", "content": "Here, we analyze the correlation between calibration, or how well predicted probabilities align with actual probabilities, and utility for downstream routing. Following the settings from B\u0142asiok et al. [4], Tian et al. [46], three calibration metrics are used to assess the calibration: (1) expected calibration error, ECE [14]; (2) Brier Score, BS [5]; and (3) the cross-entropy score, CE. Alongside these calibration metrics, we also show the rates of routing required for each technique in order to attain comparable performance to Llama3-70B-Instruct. As shown in Table 2, although Self-REF often achieves the best or second-best calibration scores, better-calibrated methods do not guarantee optimal results for routing. This aligns with observations from prior work [22], which found that well-calibrated confidence scores do not necessarily imply a strong correlation with correctness of the predictions. Overall, Self-REF achieves superior routing performance across all datasets and local LLMs, even though it only outperforms baseline methods under most of the calibration assessments."}, {"title": "5.4 Ablation Study on Gradient Masking", "content": "To understand the impact of gradient masking on downstream fine-tuning performance, we perform an ablation study of Self-REF with and without gradient masking on the OpenbookQA dataset. For both Llama3-8B-Instruct and Mistral-7B-Instruct, we observe that gradient masking improves downstream routing task accuracy over not having gradient masking. This aligns with intuition, since the purpose of gradient masking is to prevent the LLM from learning to make wrong predictions when trained on samples with <UN> tokens."}, {"title": "6 Discussion", "content": "Self-REF is a lightweight approach that requires only LoRA fine-tuning of 8B and 7B LLMs, yet it delivers comparable or even superior system-level routing performance compared to the non-finetuned 70B model. Note that a non-finetuned 70B model is used to mimic the common setting where practitioners may not have sufficient resources to fine-tune such a large model, but can instead fine-tune a smaller model on the task of interest. Overall, the Self-REF technique outperforms all baselines, with fine-tuned logits achieving the next best performance. In this section, we dive into the following questions: (1) Why can Self-REF after confidence-based routing achieve better performance than the non-finetuned 70B model? and (2) How can one navigate the tradeoffs in routing and rejection?\nFor question (1), we believe that one possible reason behind this phenomenon is that fine-tuning with Self-REF does not degrade the model's performance and effectively introduces correct knowledge, such as niche astronomy knowledge or complicated moral disputes, that the model does not inherently possess. Additionally, since the error distributions of two independent models are unlikely to overlap perfectly, the ability to route can serve as a form of ensembling of models. In the Appendix, we provide"}, {"title": "7 Conclusion", "content": "In this work, we introduce Self-REF, a lightweight fine-tuning strategy that incorporates confidence tokens, enabling LLMs to better express their confidence levels in a way that closely correlates with prediction correctness. These confidence scores are valuable for confidence-based routing and rejection learning, which, as we show, can improve system performance, efficiency, and output reliability.\nOur findings suggest that integrating confidence tokens into LLMs is a promising step toward improving their reliability and safety, particularly in real-world applications where errors carry signifi- cant costs. Future research will focus on integrating explicit rejection mechanisms into the Self-REF fine-tuning process, rather than relying solely on confidence tokens for rejection learning."}, {"title": "Appendix", "content": ""}, {"title": "A Limitations and Potential Risks", "content": "Our proposed Self-REF is designed for self-routing purposes to achieve better downstream task performance. We primarily focus on the cost of GPU hours and latency in different sizes of the LLMs during inference, where we ignore the signal transmission time between different models on different devices. The quality of signal transmission between each device highly depends on the environment and congestion control of the bandwidth. Therefore, it is hard and complicated to consider the transmission time in the routing task. One of the potential risks of routing to larger LLMs is the possibility of data leakage during transmission. Moreover, routing user queries to different LLMs can raise privacy concerns, especially when data is transmitted to third-party or unauthorized models for inference. Further research into securing routing paths and designing network protocols for communication between devices on private clouds and servers is highly recommended."}, {"title": "B Related Work", "content": ""}, {"title": "B.1 Uncertainty Quantification in LLMS", "content": "There is a growing body of recent works examining the calibration of LLMs. One line of research [56, 35, 48] leverages the internal features of LLMs (e.g., logits, attention scores) to assess and estimate uncertainty without requiring additional training or fine-tuning. Improvements in calibration have also been observed from post-processing approaches such as temperature scaling or Platt scaling [14], as well as prompting approaches asking the LLM to verbalize its predicted confidence scores Tian et al. [46]. Another line of work [55, 40, 32, 34] focuses on RLHF or instruction learning for achieving better prediction calibration.\nIn addition to model-centric analysis, another school of approaches [19, 30] focuses on examining the semantics and ambiguity in the given input questions to estimate uncertainty in LLMs. On the other hand, [22] have found that uncertainty scores derived from logits and verbalized scores exhibit a weak correlation with the correctness of LLM predictions across several models and datasets. Thus, while calibration may be a desirable characteristic of uncertainty estimates, we note that this work goes beyond calibration as a goal, focusing more on the utility of model confidence scores downstream, for tasks such as instance routing and rejection learning."}, {"title": "B.2 LLM Routing", "content": "LLM routing, where all or part of a query is directed from one LLMs to different LLMs, has been studied in the literature in a few capacities: (1) routing entire queries to different LLMs, (2) retrieval- augmented generation (RAG) systems that route part of the query to an external retrieval system, and (3) speculative decoding which utilizes outputs from models of varying complexity in the decoding step.\n(1) In the query-routing scenario, one line of work trains additional classification models to route queries to other LLMs for answers based on designated performance metrics [12, 20, 21] and routing data [41, 44], which eventually form a routing pipeline with mixture-of-experts (MoE) systems [26, 24] to boost the performance. Another line of work leverages routers to measure utility-efficiency trade- offs, aiming to reconstruct model architectures to reduce inference overhead in terms of cost and execution time while maintaining utility [8, 54]. (2) In RAG systems, routing provides effective and efficient navigation to specific data sources, offering efficient and effective indicators for RAG systems to retrieve. A notable approach is Self-RAG [2], which fine-tunes an LLM retriever using augmented data with predefined routing indicators to optimize retrieval performance. This approach improves RAG systems by enabling more efficient retrieval through dynamic routing decisions of each given"}, {"title": "B.3 LLM Rejection Learning", "content": "Learning to reject [10] has been initially proposed to offer the Bayes classification model a rejection option as the final prediction. However, with the rapid advancement of LLMs, the issue of uncertainty in LLMs' predictions may stem from their limited inherent knowledge, necessitating an option or mechanism for LLMs to refuse to answer, especially when no advanced LLMs are helpers for two-step verification or answering in action. The rejection decisions made by LLMs can enhance the reliability of their generated predictions. One group of work offers LLMs a rejection option while they generate the responses by finetuning or instruction learning [7] with newly designed rejection loss. Another group of work trains the additional rejection LLM agents to issue a rejection instruction after the predictor LLMs have made their prediction [27, 39, 36, 37]. Unlike the training paradigm of traditional machine learning models, establishing a new rejection loss with new knowledge in fine-tuning LLMs may result in sub-optimal performance and potentially degrade the abilities of pre-trained LLMs in next-token prediction. A more effective approach is to adjust LLMs using the original training data and loss functions [33, 38] while incorporating the capability for rejection learning without altering the foundational training dynamics. Moreover, training additional agents to handle rejection learning can introduce significant latency and may be overfitted, making the approach impractical for real-world applications. In this work, we aim to equip predictive LLMs with the capability for rejection learning by enabling them to reveal their confidence levels, without the need for designing new loss functions, thereby preventing any potential degradation in the performance of downstream tasks."}, {"title": "C Details about Baseline Methods", "content": ""}, {"title": "C.1 Verbalizing Confidence Prompt Usage", "content": "We provide a listing of the evaluation prompts in Table 3 utilized in assessing the performance of the baselines among all four datasets. The first sections reveal the evaluation prompt for \"Verbalizing Uncertainty;\u201d the second sections demonstrate the evaluation prompt for \u201cVerbalizing Yes/No Tokens;\" and third batches demonstrate the evaluation prompt for \u201cZero-shot/Fine-tuned Logits.\""}, {"title": "C.2 Finetuned Logits Training", "content": "The hyperparameter settings used for LoRA training of the \"Finetuned Logits\" baseline are as follows. The LoRA dimension are set from either 2, 4, and 8, and the reported performance of \"Finetuned Logits\" is the one with highest downstream task accuracy."}, {"title": "D Additional Experiment Results of Transferability", "content": "In this section, we further explore the transferability of Self-REF across unseen datasets (i.e., not included in the training data). Using Self-REF fine-tuned LoRA weights from OpenbookQA, we test on MMLU datasets for Llama3-8B-Instruct and Mistral-7B-Instruct. The results show that Self-REF outperforms the baselines with good routing performance on MMLU dataset even when using the transferred LoRA weights of OpenbookQA dataset. With the LoRA weights finetuned on OpenbookQA, in order to achieve parity with the 70B model, we observe that 75% and 70% of routing rate are required on MMLU datasets under Mistral-7B-Instruct and Llama3- 8B-Instruct, respectively. Furthermore, compared to the results obtained by fine-tuning directly on MMLU (i.e., a 70% routing rate on Mistral-7B-Instruct and a 40% routing rate on Llama3-8B-Instruct shown in Figure 2), transferred Self-REF achieves competitive routing performance with only a slight degradation in routing rates. This demonstrates the potential of Self-REF to be trained for general, multi-task purposes due to its transferability."}, {"title": "E Experiment and Model Training Details", "content": ""}, {"title": "E.1 Hyper-parameter Settings", "content": "Our experiments on the dataset are conducted under Self-REF. We focus on the confidence-based routing and rejection learning tasks, following the pipeline of Base Model Training and Confidence Token Inference. Each step is shown as follows:\nBase Model Training Self-REF is fine-tuned on local LLMs (i.e., Llama3-8B-Instruct and Mistral- 7B-Instruct) under the following hyper-parameters. We apply LoRA adapters to every query, key, and value (Q-K-V) layers, the token embedding layers, and the final linear layers in the local LLMs with the batch size of 4 and learning rate of 1e-4. For ratio of training dataset, we only adjust the OpenbookQA and MedQA dataset in Mistral-7B-Instruct model to prevent the overfitting situation. We retain the original dataset settings with the correctness and incorrectness directly inferred by local LLMs."}, {"title": "E.2 Computation Infrastructure", "content": "The experiments are conducted based on the following physical computing infrastructure in Table 6. This setup provided the necessary computational resources to efficiently train and evaluate our models."}, {"title": "F Some Case studies of Self-REF", "content": "We now perform a case study of Self-REF on the MMLU dataset. Recall from the results that Self-REF can effectively learn confidence tokens without degrading performance. We here showcase two types of examples: (1) cases that Self-REF finetuning on Llama3-8B-Instruct got correct but Llama3-70B- Instruct got incorrect , and (2) cases that Llama3-8B-Instruct initially got incorrect but were corrected by routing to Llama3-70B-Instruct using Self-REF confidence scores."}]}