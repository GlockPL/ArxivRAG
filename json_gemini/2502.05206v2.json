{"title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "authors": ["Xingjun Ma", "Yifeng Gao", "Yixu Wang", "Ruofan Wang", "Xin Wang", "Ye Sun", "Yifan Ding", "Hengyuan Xu", "Yunhao Chen", "Yunhan Zhao", "Hanxun Huang", "Yige Li", "Jiaming Zhang", "Xiang Zheng", "Yang Bai", "Zuxuan Wu", "Xipeng Qiu", "Jingfeng Zhang", "Yiming Li", "Jun Sun", "Cong Wang", "Jindong Gu", "Baoyuan Wu", "Siheng Chen", "Tianwei Zhang", "Yang Liu", "Mingming Gong", "Tongliang Liu", "Shirui Pan", "Cihang Xie", "Tianyu Pang", "Yinpeng Dong", "Ruoxi Jia", "Yang Zhang", "Shiqing Ma", "Xiangyu Zhang", "Neil Gong", "Chaowei Xiao", "Sarah Erfani", "Bo Li", "Masashi Sugiyama", "Dacheng Tao", "James Bailey", "Yu-Gang Jiang"], "abstract": "The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational Al, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard Al models. GitHub: https://github.com/xingjunm/Awesome-Large-Model-Safety.", "sections": [{"title": "INTRODUCTION", "content": "Rtificial Intelligence (AI) has entered the era of large models, exemplified by Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-Training (VLP) models, Vision-Language Models (VLMs), and image/video gen- eration diffusion models (DMs). Through large-scale pre-training on massive datasets, these models have demonstrated unprece- dented capabilities in tasks ranging from language understanding and image generation to complex problem-solving and decision- making. Their ability to understand and generate human-like content (e.g., texts, images, audios, and videos) has enabled applications in customer service, content creation, healthcare, education, and more, highlighting their transformative potential in both commercial and societal domains. However, the deployment of large models comes with sig- nificant challenges and risks. As these models become more integrated into critical applications, concerns regarding their vul- nerabilities to adversarial, jailbreak, and backdoor attacks, data privacy breaches, and the generation of harmful or misleading content have intensified. These issues pose substantial threats, including unintended system behaviors, privacy leakage, and the dissemination of harmful information. Ensuring the safety of these models is paramount to prevent such unintended consequences, maintain public trust, and promote responsible AI usage. The"}, {"title": "VISION FOUNDATION MODEL SAFETY", "content": "This section surveys safety research on two types of VFMs: per-trained Vision Transformers (ViTs) [407] and the Segment Anything Model (SAM) [408]. We focus on ViTs and SAM because they are among the most widely deployed VFMs and have garnered significant attention in recent safety research."}, {"title": "Attacks and Defenses for ViTs", "content": "Pre-trained ViTs are widely employed as backbones for various downstream tasks, frequently achieving state-of-the-art perfor- mance through efficient adaptation and fine-tuning. Unlike tra- ditional CNNs, ViTs process images as sequences of tokenized patches, allowing them to better capture spatial dependencies. However, this patch-based mechanism also brings unique safety concerns and robustness challenges. This section explores these issues by reviewing ViT-related safety research, including adver- sarial attacks, backdoor & poisoning attacks, and their correspond- ing defense strategies. Table 2 provides a summary of the surveyed attacks and defenses, along with the commonly used datasets."}, {"title": "Adversarial Attacks", "content": "Adversarial attacks on ViTs can be classified into white-box attacks and black-box attacks based on whether the attacker has full access to the victim model. Based on the attack strategy, white-box attacks can be further divided into 1) patch attacks, 2) position embedding attacks and 3) attention attacks, while black-box attacks can be summarized into 1) transfer-based attacks and 2) query-based attacks."}, {"title": "White-box Attacks", "content": "Patch Attacks exploit the modular structure of ViTs, aiming to manipulate their inference processes by introducing targeted per- turbations in specific patches of the input data. Joshi et al. [17] pro- posed an adversarial token attack method leveraging block sparsity to assess the vulnerability of ViTs to token-level perturbations. Expanding on this, Patch-Fool [1] introduces an adversarial attack framework that targets the self-attention modules by perturbing individual image patches, thereby manipulating attention scores. Different from existing methods, SlowFormer [2] introduces a universal adversarial patch can be applied to any image to increases computational and energy costs while preserving model accuracy. Position Embedding Attacks aim to attack the spatial or sequential position of tokens in transformers. For example, PE- Attack [3] explores the common vulnerability of positional em- beddings to adversarial perturbations by disrupting their ability to encode positional information through periodicity manipulation, linearity distortion, and optimized embedding distortion. Attention Attacks target vulnerabilities in the self-attention modules of ViTs. Attention-Fool [4] manipulates dot-product similarities to redirect queries to adversarial key tokens, exposing the model's sensitivity to adversarial patches. Similarly, AAS [5] mitigates gradient masking in ViTs by optimizing the pre-softmax output scaling factors, enhancing the effectiveness of attacks."}, {"title": "Black-box Attacks", "content": "Transfer-based Attacks first generate adversarial examples using fully accessible surrogate models, which are then transferred to attack black-box victim ViTs. In this context, we first review attacks specifically designed for the ViT architecture. SE-TR [6] enhances adversarial transferability by optimizing perturbations on an ensemble of models. ATA [7] strategically activates uncertain attention and perturbs sensitive embeddings within ViTs. LPM [9] mitigates the overfitting to model-specific discriminative regions through a patch-wise optimized binary mask. Chen et al. [18] introduced an Inductive Bias Attack (IBA) to suppress unique biases in ViTs and target shared inductive biases. TGR [11] re- duces the variance of the backpropagated gradient within internal blocks. VDC [12] employs virtual dense connections between deeper attention maps and MLP blocks to facilitate gradient backpropagation. FDAP [13] exploits feature collapse by reducing high-frequency components in feature space. CRFA [16] disrupts only the most crucial image regions using approximate attention maps. SASD-WS [15] flattens the loss landscape of the source model through sharpness-aware self-distillation and approximates an ensemble of pruned models using weight scaling to improve target adversarial transferability. Other strategies are applicable to both ViTs and CNNs, en- suring broader applicability in black-box settings. Wei et al. [8], [19] proposed a dual attack framework to improve transferability between ViTs and CNNs: 1) a Pay No Attention (PNA) attack, which skips the gradients of attention during backpropagation, and 2) a PatchOut attack, which randomly perturbs subsets of image patches at each iteration. MIG [10] uses integrated gradients and momentum-based updates to precisely target model- agnostic critical regions, improving transferability between ViTs and CNNs. Query-based Attacks generate adversarial examples by querying the black-box model and levering the model responses to estimate the adversarial gradients. The goal is to achieve success- ful attack with a minimal number of queries. Based on the type of model response, query-based attacks can be further divided into score-based attacks, where the model returns a probability vector, and decision-based attacks, where the model provides only the top-k classes. Decision-based attacks typically start from a large random noise (to achieve misclassification first) and then gradually find smaller noise while maintaining misclassification. To improve the efficiency of the adversarial noise searching process in ViTs, PAR [14] introduces a coarse-to-fine patch searching method, guided by noise magnitude and sensitivity masks to account for the structural characteristics of ViTs and mitigate the negative impact of non-overlapping patches."}, {"title": "Adversarial Defenses", "content": "Adversarial defenses for ViTs follow four major approaches: 1) adversarial training, which trains ViTs on adversarial examples via min-max optimization to improve its robustness; 2) adversar- ial detection, which identifies and mitigates adversarial attacks by detecting abnormal or malicious patterns in the inputs; 3) robust architecture, which modifies and optimizes the architecture (e.g., self-attention module) of ViTs to improve their resilience against adversarial attacks; and 4) adversarial purification, which pre-processes the input (e.g., noise injection, denoising, or other trans- formations) to remove potential adversarial perturbations before inference. Adversarial Training is widely regarded as the most effec- tive approach to adversarial defense; however, it comes with a high computational cost. To address this on ViTs, AGAT [20] introduces a dynamic attention-guided dropping strategy, which accelerates the training process by selectively removing certain patch embeddings at each layer. This reduces computational over- head while maintaining robustness, especially on large datasets such as ImageNet. Due to its high computational cost, research on adversarial training for ViTs has been relatively limited. ARD- PRM [28] improves adversarial robustness by randomly dropping gradients in attention blocks and masking patch perturbations during training. Adversarial Detection methods for ViTs primarily leverage two key features, i.e., patch-based inference and activation charac- teristics, to detect and mitigate adversarial examples. Li et al. [21] proposed the concept of Patch Vestiges, abnormalities arising from adversarial examples during patch division in ViTs. They used statistical metrics on step changes between adjacent pixels across patches and developed a binary regression classifier to de- tect adversaries. Alternatively, ARMOR [23] identifies adversarial patches by scanning for unusually high column scores in specific layers and masking them with average images to reduce their impact. ViTGuard [22], on the other hand, employs a masked autoencoder to detect patch attacks by analyzing attention maps and CLS token representations. As more attacks are developed, there is a growing need for a unified detection framework capable of handling all types of adversarial examples. Robust Architecture methods focus on designing more ad- versarially resilient attention modules for ViTs. For example, Smoothed Attention [27] employs temperature scaling in the softmax function to prevent any single patch from dominating the attention, thereby balancing focus across patches. ReiT [32] integrates adversarial training with randomization through the II- ReSA module, optimizing randomly entangled tokens to reduce adversarial similarity and enhance robustness. TAP [29] addresses token overfocusing by implementing token-aware average pooling and an attention diversification loss, which incorporate local neighborhood information and reduce cosine similarity among attention vectors. FViTs [31] strengthen explanation faithfulness by stabilizing top-k indices in self-attention and robustify predic- tions using denoised diffusion smoothing combined with Gaussian noise. RSPC [30] tackles vulnerabilities by corrupting the most sensitive patches and aligning intermediate features between clean and corrupted inputs to stabilize the attention mechanism. Col- lectively, these advancements underscore the pivotal role of the attention mechanism in improving the adversarial robustness of ViTs. Adversarial Purification refers to a model-agnostic input- processing technique that is broadly applicable across various"}, {"title": "Backdoor Attacks", "content": "Backdoors can be injected into the victim model via data poi- soning, training manipulation, or parameter editing, with most existing attacks on ViTs being data poisoning-based. We classify these attacks into four categories: 1) patch-level attacks, 2) token-level attacks, and 3) multi-trigger attacks, which exploit ViT-specific data processing characteristics, as well as 4) data- free attacks, which exploit the inherent mechanisms of ViTs. Patch-level Attacks primarily exploit the ViT's characteristic of processing images as discrete patches by implanting triggers at the patch level. For example, BadViT [39] introduces a universal patch-wise trigger that requires only a small amount of data to redirect the model's focus from classification-relevant patches to adversarial triggers. TrojViT [40] improves this approach by uti- lizing patch salience ranking, an attention-targeted loss function, and parameter distillation to minimize the bit flips necessary to embed the backdoor. Token-level Attacks target the tokenization layer of ViTs. SWARM [41] introduces a switchable backdoor mechanism fea- turing a \"switch token\" that dynamically toggles between benign and adversarial behaviors, ensuring high attack success rates while maintaining functionality in clean environments. Multi-trigger Attacks employ multiple backdoor triggers in parallel, sequential, or hybrid configurations to poison the victim dataset. MTBAS [43] utilize these multiple triggers to induce coexistence, overwriting, and cross-activation effects, significantly diminishing the effectiveness of existing defense mechanisms. Data-free Attacks eliminate the need for original training datasets. Using substitute datasets, DBIA [42] generates universal triggers that maximize attention within ViTs. These triggers are fine-tuned with minimal parameter adjustments using PGD [409], enabling efficient and resource-light backdoor injection."}, {"title": "Backdoor Defenses", "content": "Backdoor defenses for ViTs aim to identify and break (or remove) the correlation between trigger patterns and target classes while preserving model accuracy. Two representative defense strategies are: 1) patch processing, which disrupts the integrity of image patches to prevent trigger activation, and 2) image blocking,"}, {"title": "Datasets", "content": "Datasets are crucial for developing and evaluating attack and de- fense methods. Table 2 summarizes the datasets used in adversarial and backdoor research. Datasets for Adversarial Research As shown in Table 2, adversarial researches were primarily conducted on ImageNet. While attacks were tested across various datasets like CIFAR- 10/100, Food-101, and GLUE, defenses were mainly limited to ImageNet and CIFAR-10/100. This imbalance reveals one key issue in adversarial research: attacks are more versatile, while defenses struggle to generalize across different datasets. Datasets for Backdoor Research Backdoor researches were also conducted mainly on ImageNet and CIFAR-10/100 datasets. Some attacks, such as DBIA and SWARM, extend to domain- specific datasets like GTSRB and VGGFace, while defenses, including PatchDrop, were often limited to a few benchmarks. This narrow focus reduces their real-world applicability. Although backdoor defenses are shifting towards robust inference tech- niques, they typically target specific attack patterns, limiting their generalizability. To address this, adaptive defense strategies need to be tested across a broader range of datasets to effectively counter the evolving nature of backdoor threats."}, {"title": "Attacks and Defenses for SAM", "content": "SAM is a foundational model for image segmentation, comprising three primary components: a ViT-based image encoder, a prompt encoder, and a mask decoder. The image encoder transforms high- resolution images into embeddings, while the prompt encoder converts various input modalities into token embeddings. The mask decoder combines these embeddings to generate segmen- tation masks using a two-layer Transformer architecture. Due to its complex structure, attacks and defenses targeting SAM differ significantly from those developed for CNNs. These unique challenges stem from SAM's modular and interconnected design, where vulnerabilities in one component can propagate to others, necessitating specialized strategies for both attack and defense. This section systematically reviews SAM-related adversarial at- tacks, backdoor & poisoning attacks, and adversarial defense strategies, as summarized in Table 2."}, {"title": "Adversarial Attacks", "content": "Adversarial attacks on SAM can be categorized into: (1) white- box attacks, exemplified by prompt-agnostic attacks, and (2) black-box attacks, which can be further divided into universal attacks and transfer-based attacks. Each category employs distinct strategies to compromise segmentation performance."}, {"title": "White-box Attacks", "content": "Prompt-Agnostic Attacks are white-box attacks that disrupt SAM's segmentation without relying on specific prompts, using either prompt-level or feature-level perturbations for generality across inputs. For prompt-level attacks, Shen et al. [47] proposed a grid-based strategy to generate adversarial perturbations that disrupt segmentation regardless of click location. For feature- level attacks, Croce et al. [48] perturbed features from the image encoder to distort spatial embeddings, undermining SAM's seg- mentation integrity."}, {"title": "Black-box Attacks", "content": "Universal Attacks generate UAPs [411] that can consistently dis- rupt SAM across arbitrary prompts. Han et al. [53] exploited con- trastive learning to optimize the UAPs, achieving better attack per- formance by exacerbating feature misalignment. DarkSAM [54], on the other hand, introduces a hybrid spatial-frequency frame- work that combines semantic decoupling and texture distortion to generate universal perturbations. Transfer-based Attacks exploit transferable representations in SAM to generate perturbations that remain adversarial across different models and tasks. PATA++ [50] improves transferability by using a regularization loss to highlight key features in the image encoder, reducing reliance on prompt-specific data. Attack-SAM [49] employs ClipMSE loss to focus on mask removal, optimiz- ing for spatial and semantic consistency to improve cross-task transferability. UMI-GRAT [52] follows a two-step process: it first generates a generalizable perturbation with a surrogate model and then applies gradient robust loss to improve across-model transferability. Apart from designing new loss functions, opti- mization over transformation techniques can also be exploited to improve transferability. This includes T-RA [47], which improves cross-model transferability by applying spectrum transformations to generate adversarial perturbations that degrade segmentation in SAM variants, and UAD [51], which generates adversarial examples by deforming images in a two-stage process and aligning features with the deformed targets."}, {"title": "Adversarial Defenses", "content": "Adversarial defenses for SAM are currently limited, with exist- ing approaches focusing primarily on adversarial tuning, which integrates adversarial training into the prompt tuning process of SAM. For example, ASAM [55] utilizes a stable diffusion model to generate realistic adversarial samples on a low-dimensional manifold through diffusion model-based tuning. ControlNet [412] is then employed to guide the re-projection process, ensuring that the generated samples align with the original mask annotations. Finally, SAM is fine-tuned using these adversarial examples."}, {"title": "Backdoor & Poisoning Attacks", "content": "Backdoor and poisoning attacks on SAM remain underexplored. Here, we review one backdoor attack that leverages perceptible visual triggers to compromise SAM, and one poisoning attack"}, {"title": "Datasets", "content": "As shown in Table 2, the datasets used in safety research on SAM slightly differ from those typically used in general segmentation tasks [415], [416]. For attack research, the SA-1B dataset and its subsets [408] are the most commonly used for evaluating adversar- ial attacks [47]\u2013[51], [53]. Additionally, DarkSAM was evaluated on datasets such as Cityscapes [417], COCO [418], and ADE2k [419], while UMI-GRAT, which targets downstream tasks related to SAM, was tested on medical datasets like CT-Scans and ISTD, as well as camouflage datasets, including COD10K, CAMO, and CHAME. For backdoor attacks, BadSAM was assessed using the CAMO dataset [420]. In the context of data poisoning, UnSeg [57] was evaluated across 10 datasets, including COCO, Cityscapes, ADE20k, WHU, and medical datasets like Lung and Kvasir-seg. For defense research, ASAM [55] is currently the only defense method applied to SAM. It was evaluated on a range of datasets with more diverse image distributions than SA-1B, including ADE20k, LVIS, COCO, and others, with mean Intersection over Union (mIoU) used as the evaluation metric."}, {"title": "LARGE LANGUAGE MODEL SAFETY", "content": "LLMs are powerful language models that excel at generating human-like text, translating languages, producing creative content, and answering a diverse array of questions [421], [422]. They have been rapidly adopted in applications such as conversational agents, automated code generation, and scientific research. Yet, this broad utility also introduces significant vulnerabilities that potential adversaries can exploit. This section surveys the current landscape of LLM safety research. We examine a spectrum of adversarial behaviors, including jailbreak, prompt injection, backdoor, poison- ing, model extraction, data extraction, and energy-latency attacks. Such attacks can manipulate outputs, bypass safety measures, leak sensitive information, and disrupt services, thereby threatening system integrity, confidentiality, and availability. We also review state-of-the-art alignment strategies and defense techniques de- signed to mitigate these risks. Tables 3 and 4 summarize the details of these works."}, {"title": "Adversarial Attacks", "content": "Adversarial attacks on LLMs aim to manipulate a model's re- sponse by subtly altering input text. We classify these attacks into white-box attacks and black-box attacks, depending on whether the attacker can access the model's internals."}, {"title": "White-box Attacks", "content": "White-box attacks assume the attacker has full knowledge of the LLM's architecture, parameters, and gradients. This enables the construction of highly effective adversarial examples by directly optimizing against the model's predictions. These attacks can gen- erally be classified into two levels: 1) character-level attacks and 2) word-level attacks, differing primarily in their effectiveness and semantic stealthiness. Character-level Attacks introduce subtle modifications at the character level, such as misspellings, typographical errors, and the insertion of visually similar or invisible characters (e.g., homoglyphs [58]). These attacks exploit the model's sensitivity to minor character variations, which are often unnoticeable to hu- mans, allowing for a high degree of stealthiness while potentially preserving the original meaning. Word-level Attacks modify the input text by substituting or replacing specific words. For example, TextFooler [59] and BERT-Attack [60] employ synonym substitution to generate ad- versarial examples while preserving semantic similarity. Other methods, such as GBDA [61] and GRADOBSTINATE [63], leverage gradient information to identify semantically similar word substitutions that maximize the likelihood of a successful attack. Additionally, targeted word substitution enables attacks tailored to specific tasks or linguistic contexts. For instance, [62] explores targeted attacks on named entity recognition, while [64] adapts word substitution attacks for the Chinese language."}, {"title": "Black-box Attacks", "content": "Black-box attacks assume that the attacker has limited or no knowledge of the target LLM's parameters and interacts with the model solely through API queries. In contrast to white-box attacks, black-box attacks employ indirect and adaptive strategies to exploit model vulnerabilities. These attacks typically manipu- late input prompts rather than altering the core text. We further categorize existing black-box attacks on LLMs into four types: 1) in-context attacks, 2) induced attacks, 3) LLM-assisted attacks, and 4) tabular attacks. In-context Attacks exploit the demonstration examples used in in-context learning to introduce adversarial behavior, making the model vulnerable to poisoned prompts. AdvICL [65] and Transferable-advICL manipulate these demonstration examples to expose this vulnerability, highlighting the model's susceptibility to poisoned in-context data. Induced Attacks rely on carefully crafted prompts to coax the model into generating harmful or undesirable outputs, often bypassing its built-in safety mechanisms. These attacks focus on generating adversarial responses by designing deceptive input prompts. For example, Liu et al. [66] analyzed how such prompts can lead the model to produce dangerous outputs, effectively circumventing safeguards designed to prevent such behavior. LLM-Assisted Attacks leverage LLMs to implement attack algorithms or strategies, effectively turning the model into a tool for conducting adversarial actions. This approach underscores the capacity of LLMs to assist attackers in designing and executing attacks. For instance, Carlini [423] demonstrated that GPT-4 can be prompted step-by-step to design attack algorithms, highlighting the potential for using LLMs as research assistants to automate adversarial processes. Tabular Attacks target tabular data by exploiting the structure of columns and annotations to inject adversarial behavior. Koleva"}, {"title": "Adversarial Defenses", "content": "Adversarial defenses are crucial for ensuring the safety, reliability, and trustworthiness of LLMs in real-world applications. Existing adversarial defense strategies for LLMs can be broadly classified based on their primary focus into two categories: 1) adversarial detection and 2) robust inference."}, {"title": "Adversarial Detection", "content": "Adversarial detection methods aim to identify and flag potential adversarial inputs before they can affect the model's output. The goal is to implement a filtering mechanism that can differentiate between benign and malicious prompts. Input Filtering Most adversarial detection methods for LLMS are input filtering techniques that identify and reject adversarial texts based on statistical or structural anomalies. For example, Jain et al. [68] use perplexity to detect adversarial prompts, as these typically show higher perplexity when evaluated by a well- calibrated language model, indicating a deviation from natural language patterns. By setting a perplexity threshold, such inputs can be filtered out. Another approach, Erase-and-Check [69], ensures robustness by iteratively erasing parts of the input and checking for output consistency. Significant changes in output signal potential adversarial manipulation. Input filtering methods offer a lightweight first line of defense, but their effectiveness de- pends on the chosen features and the sophistication of adversarial attacks, which may bypass these defenses if designed adaptively."}, {"title": "Robust Inference", "content": "Robust inference methods aim to make the model inherently resis- tant to adversarial attacks by modifying its internal mechanisms or training. One approach, Circuit Breaking [70], targets specific activation patterns during inference, neutralizing harmful outputs without retraining. While robust inference enhances resistance to adaptive attacks, it often incurs higher computational costs, and its effectiveness varies by model architecture and attack type."}, {"title": "Jailbreak Attacks", "content": "Unlike adversarial attacks that modify the prompt with character- or word-level perturbations, jailbreak attacks trick LLMs into gen- erating harmful content via hand-crafted or automated jailbreak prompts. A key characteristic of jailbreak attacks is that once a model is jailbroken, it continues to produce harmful responses to follow-up malicious queries. Adversarial attacks, however, require input perturbations for each instance. Most jailbreak research tar- gets the LLM-as-a-Service scenario, following a black-box threat model where the attacker cannot access the model's internals."}, {"title": "Hand-crafted Attacks", "content": "Hand-crafted attacks involve designing adversarial prompts to exploit specific vulnerabilities in the target LLM. The goal is to craft word/phrase combinations or structures that can bypass the model's safety filters while still conveying harmful requests. Scenario-based Camouflage hides malicious queries within complex scenarios, such as role-playing or puzzle-solving, to obscure their harmful intent. For instance, Li et al. [74] instruct the LLM to adopt a persona likely to generate harmful con- tent, while SMEA [76] places the LLM in a subordinate role under an authority figure. Easyjailbreak [75] frames harmful queries in hypothetical contexts, and Puzzler [80] embeds them in puzzles whose solutions correspond to harmful outputs. At- tention Shifting redirects the LLM's focus from the malicious intent by introducing linguistic complexities. Jailbroken [73] employs code-switching and unusual sentence structures, Tastle [77] manipulates tone, and StructuralSleight [78] alters sentence structure to disrupt understanding. In addition, Shen et al. [90] collected real-world jailbreak prompts shared by users on social media, such as Reddit and Discord, and studied their effectiveness against LLMs. Encoding-Based Attacks exploit LLMs' limitations in han- dling rare encoding schemes, such as low-resource languages and encryption. These attacks encode malicious queries in formats like Base64 [73] or low-resource languages [71], or use custom encryption methods like ciphers [72] and CodeChameleon [79] to obfuscate harmful content."}, {"title": "Automated Attacks", "content": "Unlike hand-crafted attacks, which rely on expert knowledge, au- tomated attacks aim to discover jailbreak prompts autonomously. These attacks either use black-box optimization to search for optimal prompts or leverage LLMs to generate and refine them. Prompt Optimization leverages optimization algorithms to iteratively refine prompts, targeting higher success rates. For black-box methods, AutoDAN [81] employs a genetic algorithm, GPTFuzzer [82] utilizes mutation- and generation-based fuzzing techniques, and FuzzLLM [86] generates semantically coherent prompts within an automated fuzzing framework. I-FSJ [93] injects special tokens into few-shot demonstrations and uses demo-level random search to optimize the prompt, achieving high attack success rates against aligned models and their defenses. For white-box methods, the most notable is GCG [91], which introduces a greedy coordinate gradient algorithm to search for adversarial suffixes, effectively compromising aligned LLMs. I- GCG [92] further improves GCG with diverse target templates and an automatic multi-coordinate updating strategy, achieving near-perfect attack success rates. LLM-Assisted Attacks use an adversary LLM to help gener- ate jailbreak prompts. Perez et al. [88] explored model-based red teaming, finding that an LLM fine-tuned via RL can generate more effective adversarial prompts, though with limited diversity. CRT [89] improves prompt diversity by minimizing SelfBLEU scores and cosine similarity. PAIR [83] employs multi-turn queries with an attacker LLM to refine jailbreak prompts iteratively. Based on PAIR, Robey et al. [424] introduced ROBOPAIR, which targets LLM-controlled robots, causing harmful physical actions. Similarly, ECLIPSE [95] leverages an attacker LLM to identify adversarial suffixes analogous to GCG, thereby automating the prompt optimization process. To enhance prompt transferability, Masterkey [84] trains adversary LLMs to attack multiple models. Additionally, Weak-to-Strong Jailbreaking [94] proposes a novel attack where a weaker, unsafe model guides a stronger, aligned model to generate harmful content, achieving high success rates with minimal computational cost."}, {"title": "Jailbreak Defenses", "content": "We now introduce the corresponding defense mechanisms for black-box LLMs against jailbreak attacks. Based on the inter- vention stage, we classify existing defenses into three categories: input defense, output defense, and ensemble defense."}, {"title": "Input Defenses", "content": "Input defense methods focus on preprocessing the input prompt to reduce its harmful content. Current techniques include rephrasing and translation. Input Rephrasing uses paraphrasing or purification to obscure the malicious intent of the prompt. For example, SmoothLLM [96] applies random sampling to perturb the prompt, while Se- manticSmooth [97] finds semantically similar, safe alternatives. Beyond prompt-level changes, SelfDefend [98] performs token- level perturbations by removing adversarial tokens with high perplexity. IBProtector, on the other hand, [99] perturbs the encoded input using the information bottleneck principle. Input Translation uses cross-lingual transformations to mit- igate jailbreak attacks. For example, Wang et al. [100] proposed refusing to respond if the target LLM rejects the back-translated version of the original prompt, based on the hypothesis that back- translation reveals the underlying intent of the prompt."}, {"title": "Output Defenses", "content": "Output defense methods monitor the LLM's generated output to identify harmful content, triggering a refusal mechanism when unsafe output is detected. Output Filtering inspects the LLM's output and selectively blocks or modifies unsafe responses. This process relies on either judge scores from pre-trained classifiers or internal signals (e.g., the loss landscape) from the LLM itself. For instance, APS [101] and DPP [102] use safety classifiers to identify unsafe outputs, while Gradient Cuff [103] analyzes the LLM's internal refusal loss function to distinguish between benign and malicious queries. Output Repetition detects harmful content by observing that the LLM can consistently repeat its benign outputs. PARDEN [105] identifies inconsistencies by prompting the LLM to repeat its output. If the model fails to accurately reproduce its response, especially for harmful queries, it may indicate a potential jailbreak."}, {"title": "Ensemble Defenses", "content": "Ensemble defense combines multiple models or defense mecha- nisms to enhance performance and robustness. The idea is that different models and defenses can offset their individual weak- nesses, resulting in greater overall safety. Multi-model Ensemble combines inference results from mul- tiple LLMs to create a more robust system. For example, MTD [104] improves LLM safety by dynamically utilizing a pool of diverse LLMs. Rather than relying on a single model, MTD selects the safest and most relevant response by analyzing outputs from multiple models. Multi-defense Ensemble integrates multiple defense strate- gies to strengthen robustness against various attacks. For instance, AutoDefense [106] introduces an ensemble framework combining input and output defenses for enhanced effectiveness. MoGU [107] uses a dynamic routing mechanism to balance contributions from a safe LLM and a usable LLM, based on the input query, effectively combining rephrasing and filtering."}, {"title": "Prompt Injection Attacks", "content": "Prompt injection attacks manipulate LLMs into producing unin- tended outputs by injecting a malicious instruction into an oth- erwise benign prompt. As in Section 3.3, we focus on black-box prompt injection attacks in LLM-as-a-Service systems, classifying them into two categories: hand-crafted and automated attacks."}, {"title": "Hand-crafted Attacks", "content": "Hand-crafted attacks require expert knowledge to design injec- tion prompts that exploit vulnerabilities in LLMs. These attacks rely heavily on human intuition. PROMPTINJECT [108", "109": "show how attackers can manipulate LLMs by ap- pending malicious commands or using context-ignoring prompts to leak sensitive information. Greshake et al. [110", "112": "formalized prompt injection attacks and defenses, introducing a combined attack method and establishing a"}]}