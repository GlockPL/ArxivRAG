{"title": "GINET: Integrating Sequential and Context-Aware Learning for Battery Capacity Prediction", "authors": ["Sara Sameer", "Wei Zhang", "Xin Lou", "Qingyu Yan", "Terence Goh", "Yulin Gao"], "abstract": "The surging demand for batteries requires advanced battery management systems, where battery capacity modelling is a key functionality. In this paper, we aim to achieve accurate battery capacity prediction by learning from historical measurements of battery dynamics. We propose GINET, a gated recurrent units enhanced Informer network, for predicting battery's capacity. The novelty and competitiveness of GINET lies in its capability of capturing sequential and contextual information from raw battery data and reflecting the battery's complex behaviors with both temporal dynamics and long-term dependencies. We conducted an experimental study based on a publicly available dataset to showcase GINET's strength of gaining a holistic understanding of battery behavior and predicting battery capacity accurately. GINET achieves 0.11 mean absolute error for predicting the battery capacity in a sequence of future time slots without knowing the historical battery capacity. It also outperforms the latest algorithms significantly with 27% error reduction on average compared to Informer. The promising results highlight the importance of customized and optimized integration of algorithm and battery knowledge and shed light on other industry applications as well.", "sections": [{"title": "I. INTRODUCTION", "content": "Batteries power devices and systems for different industry sectors, from consumer electronics to mobility. They are often considered the cornerstone of modern technology and society. The demand for Li-ion batteries alone is expected to grow by over 30% annually to reach ~4.7 TWh by 2030. Batteries for mobility, e.g., electric vehicles (EVs), account for about 90% of the demand [1]. In line with the surging demand, the revenue of the battery value chain is expected to surpass 400 billion dollars by 2030. The huge demand and broad adoption require batteries to be safe, efficient, and sustainable, typically ensured by the critical battery management system (BMS). A BMS shall actively monitor battery dynamics, e.g., current, voltage, and temperature, and guard against overcharging, deep discharge, thermal runaway, etc., that can result in hazards like fires and explosions [2]. Often, BMS makes decisions based on several important indicators. One of them is state of charge (SoC), which tells the remaining capacity of a battery as the ratio of its total capacity at any given time [3]. SoC is highly important for battery management, especially in applications like EVs and smart grids.\nAccurate measurement of SoC has been a challenging task. Traditional methods include open-circuit voltage [4], Coulomb-counting [5], etc. These methods are simple and widely used, though they face challenges with limited accuracy and the inability to adapt to dynamic battery conditions. For example, [4] requires batteries to be at rest for accurate measurements and is impractical for applications during operation time. Data-driven approaches based on machine learning (ML) have gained more significance in recent years due to their high accuracy and robustness. Battery measurements are often sequential and naturally time-series ML algorithms, e.g., recurrent neural networks (RNNs), are used for SoC estimation and prediction. Long short-term memory (LSTM) is perhaps the most popular type of RNN and serves as a baseline in many SoC studies. Newer RNNs have been investigated recently to improve SoC prediction performance and a comparison study is available in [6]. The latest Transformer-based ML is also capable of handling sequential data but in a different way than RNNs, e.g., with attention mechanisms. The Transformer models have demonstrated superior performance to traditional RNNs for many applications [7]. In a recent study [8], a temporal transformer-based network is used to model the relationship between battery measurements and SoC. A common feature of most ML-based SoC studies is that each uses a single ML algorithm. Battery behavior is known to be complex, diverse, and multi-faceted, and single-model-based SoC prediction is often insufficient to understand battery behavior well and deliver optimal performance."}, {"title": "II. GINET METHODOLOGY", "content": "We present the overall architecture and main components of GINET in this section. An illustration of the GINET architecture is in Fig. 1.\nA. Model Overview and Technology Recap\nGINET uses the battery monitoring data for battery capacity prediction. The collected raw data undergoes data pre-processing to generate input vectors ready for GINET prediction. The input of GINET is first processed by a GRU module to extract sequential features and understand time-series patterns from the battery data. The GRU features are fused with the original feature to enhance the sequential features with contextual information on battery dynamics. The fused features serve as the input of the Informer's embedding and Informer analyzes the features with its attention-based encoder and decoder. Finally, GINET maps the output of the Informer module to the battery capacity forecast horizon and produces the SoC prediction results accordingly.\nBefore presenting GINET's technology details, we provide a brief recap of its supporting ML technologies, GRU and Informer, as below.\n1) GRU: GRU is one type of RNN. Compared to the popular RNN-based LSTM, GRU's structure is simple. GRU has one less gate than LSTM. It combines LSTM's forget and input gates into an update gate to control the amount of historical information to be retained and new information to be used. It also has a reset gate to control the historical information to be deleted and GRU's unique gate mechanism minimizes gradient vanishing issues that are common for other RNNs. Finally, the gate output is used to calculate the final hidden state, which is the GRU-based representation of the input data, e.g., GRU-based battery temporal dependencies. GRU details such as gates and hidden state calculations are available in [10].\n2) Informer: Transformers have been widely used for time-series forecasting applications and many of them are based on full attention as,\n$\\text{Attention}(Q, K, V) = \\text{softmax} (QK^T/\\sqrt{d}) V$ (1)\nwhere $Q$, $K$, and $V$ are query, key, and value matrices, respectively, and $d$ is the input dimension. A key challenge of Eq. (1) is that the attention weights of all pairs of queries and keys are computed with a high time complexity of $O(n^2)$ with an input length of $n$. Informer aims to process long sequence data effectively. Its key innovation is to reduce computation complexity with a new attention mechanism called ProbSparse self-attention as,\n$\\text{ProbSparse}(Q, K, V) = \\text{softmax} (QK^T/\\sqrt{d}) V$ (2)\nwhere $Q$ is a sparse matrix which contains top-u queries with dominant attention only and $u$ is expected to be much smaller than $n$. The dominance of each query key pair is quantified by a query sparsity measurement function $M(q, K)$ for a query $q$. The new attention allows the Informer to achieve a time complexity of $O(n \\log n)$. Furthermore, Informer uses distillation operations to produce shortened representations of long sequences. Overall, Informer with its attention-based encoder and decoder processes data input and performs linear transformations for forecast generation. Informer details such as sparsity measurements can be found in [11]."}, {"title": "B. Building Blocks", "content": "Based on the model overview, we decompose GINET architecture into several building blocks and present the technical details below.\n1) Data Pre-processing: Most ML algorithms favor big data for both quantity and diversity to achieve optimal performance. However, big data may not be practical for real-world applications like battery capacity prediction, as it requires increased costs for data collection, storage, processing, etc. In this paper, we aim to develop a practical solution for batteries and we consider common battery measurements including voltage, current, and temperature that are easy to monitor for GINET processing. Nevertheless, we would like to mention that GINET is not restricted to specific features, and its input can be customized to accommodate different battery systems. Specifically, let $x_t = (x_t^I, x_t^V, x_t^T)$ be the battery data at time slot $t$ where $I$, $V$, and $T$ represent current, voltage, and temperature, respectively. GINET is capable of processing the data of a sequence of time slots, i.e., time-series, and we apply over-lapping windows [8] to prepare the input data of GINET. Let $T_{in}$ be the length of the input window and the input of GINET is $X_t = (x_{t-T_{in}}, x_{t-T_{in}+1},..., x_{t-1})$ at time slot $t$. GINET can estimate the battery capacity of the current time slot as well as future time slots, referred to as forecast horizon. Let $T_{out}$ be the length of the forecast horizon and $\\hat{y}_t$ be GINET's estimated SoC for time $t$. With the input of $T_{in}$ time slots data, GINET produces $\\hat{Y} = (\\hat{y}_t, \\hat{y}_{t+1},..., \\hat{y}_{t+T_{out}-1})$ for $T_{out}$ time slots at time $t$. For simplicity, let $\\hat{y}_{soc} = \\sum_{i=t}^{t+T_{out}-1} y_i / T_{out}$ be the average of the $T_{out}$ estimations and our reported results in this paper are based on $\\hat{y}_{soc}$ by default. Battery capacity datasets often provide the ground-truth SoC as $Y_t = (y_t, y_{t+1},..., y_{t+T_{out}-1})$ for evaluating the performance of battery capacity models. However, the ground truth cannot be used during model training and is unavailable in many practical settings. Furthermore, we employ min-max normalization for input feature scaling and keep the output non-normalized to align with real-world SoC prediction.\n2) GRU-enhanced Features and Feature Fusion: At each time slot, we extract features from the GINET input and expect the features to be better correlated with battery capacity than the raw input. We aim to capture both temporal dependencies and contextual features. In GINET, we employ GRU to extract temporal dependencies from the sequential battery data input. Specifically, GRU processes the sequential input over a sequence of time slots and generates high-dimensional (1024 in this paper) hidden state representation at each time slot. We configure two layers for hierarchical feature extraction and representation where the output of the first layer serves as the input of the second layer and a dropout layer is applied after the first layer to prevent overfitting. The representations effectively summarize the temporal dependencies from the battery time series and encode complex interactions between battery dynamics over time. Finally, the representations are passed through a linear transformation to map the representations to feature space that matches the original input and let GRU's generated feature be $H_t = GRU(X_t)$. Such mapping ensures compatibility for the following feature fusion.\n3) Feature Fusion: We apply an element-wise addition between the GRU-generated features and the GINET's original input, and the fused feature is $F_t = H_t + X_t$ for time $t$. We expect the fused features to retain contextual features from the original input while embedding temporal features extracted by GRU, and serve as the input of the Informer-based data modeling detailed below.\n4) Informer-based SoC Prediction: The fused feature $F_t$ is passed to the Informer's embedding layer first. $F_t$ is converted into a high-dimensional space suitable for Informer's attention mechanism. We use three different embedding techniques, including positional, value, and temporal, to enrich the time-series battery data. The first one helps capture the position of each data point of the input and value embedding uses a 1-dimensional convolutional layer to expand the features into the desired embedding dimension. The last one encodes time-related features, e.g., seconds, to capture the potential periodic nature of batteries. Finally, the input from all three embedding techniques is unified into a single data embedding layer.\nThe following stage of Informer processing is encoding. The key component of encoding is the multi-head self-attention which incorporates ProbSparse attention for processing"}, {"title": "III. EXPERIMENTAL STUDY", "content": "We present our experimental study in this section for experiment setup, results presentation, and discussions.\nA. Data\nA battery capacity model shall be trained and optimized with a real-world battery dataset. For our experimental study, we use an open dataset [12]. The dataset is based on Panasonic 18650PF Li-ion batteries and tests were conducted across a wide temperature range, i.e., -20 to 25 \u00b0C, to analyze battery performance variations under different ambient conditions. The key tests involved in this dataset include charging and discharging batteries at a slow and steady rate (C/20 charge-discharge cycles), measuring battery power and energy capabilities (hybrid pulse power characterization), and analyzing the internal resistances of the cells (electrochemical impedance spectroscopy). The tests also simulated the battery performance of a Ford F150 electric truck under different driving conditions, e.g., aggressive urban driving (US06), highway driving (HWFET), typical urban driving (UDDS), and tailored driving profiles generated by a neural network. The dataset provides high-resolution (10 measurements per second) battery dynamics, including voltage, current, power, amp-hours, watt-hours, and battery temperature. Overall, this data offers a ground for our experimental study with diverse operational conditions for model development and validation.\nWith the dataset, we parsed, concatenated, normalized, and split the data into training, validation, and test datasets, with a ratio of 10:2:5. The training data is composed of mixed cycles and driving profiles at four different ambient temperatures (i.e., -10, 0, 10, and 25 \u00b0C). Two cycles are reserved for testing to evaluate model performance with unseen data.\nB. Model Configuration and Implementation\nGINET involves several parameters. We have optimized the parameters and chosen one of the optimal configurations as the default setting. The batch size is 32, and the learning rate is 1e-4. A scheduler for learning rate decrease, zero-padding, and distillation are set to true. We have 20 epochs and early stopping can be triggered when the model stops improving. Battery capacity modelling is relatively a small-scale application, and we configure 2 encoder layers and 1 decoder layer in GINET. We implement GINET in the PyTorch library. All experiments are conducted in our workstation with an AMD Ryzen 9 5950X processor and NVIDIA GTX 3080 GPU.\nWe present experiment results based on statistical performance. Among different performance evaluation metrics, we choose two mainstream ones including MAE and root mean squared error (RMSE). Relatively, RMSE is more susceptible to outliers than MAE because the mistakes are first squared. Let $\\hat{y}^{SoC} = {\\hat{y}_1^{SoC},..., \\hat{y}_n^{SoC}}$ be a list of n SoC predictions and let $y^{SoC} = {y_1^{SoC}, . . ., y_n^{SoC}}$ be the corresponding ground-truth SoC. MAE is given by,\n$MAE = \\frac{1}{n} \\sum_{k=1}^{n} |y_k^{SoC} - \\hat{y}_k^{SoC}|$ (3)\nand RMSE can be calculated as,\n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{k=1}^{n} (y_k^{SoC} - \\hat{y}_k^{SoC})^2}$ (4)\nIn the following parts, we show that our proposed GINET achieves minimal MAE and RMSE for SoC prediction.\nC. Comparison Study\nWe first demonstrate GINET's performance competitiveness by comparing GINET with three comparison algorithms, including LSTM, GRU, and Informer. We have implemented comparison algorithms with parameters optimized and we consider different settings for a fair comparison. Specifically, we investigate the algorithms' performance with an input window of 10, 100, and 200 and a forecast horizon of 10 and 25. We report the performance in terms of MAE and RMSE and the results are shown in Table I. We have the following observations.\n1) Overall Results: First, GINET performs the best. In nearly all tested configurations of input window and forecast horizon, as well as for different metrics, GINET outperforms the comparison algorithms. GRU and Informer are two building blocks of GINET, and we notice that applying either GRU or Informer alone is insufficient for achieving optimal performance. This highlights GINET's concept of integrating sequential and context information, handled by GRU and Informer, respectively, for battery capacity prediction. Relatively, Informer alone performs better than GRU, and this demonstrates the superior modelling capability of the latest Transformer architecture. For sequential data analysis, LSTM is one of the most popular algorithms. Both LSTM and GRU are RNNs where GRU's architecture is simpler with fewer gates and parameters. We can see that LSTM incurs higher errors than GRU and the reason should be that GRU's simple architecture aligns well with our application, which is relatively small-scale with over-fitting risks. Let LSTM be the baseline algorithm. We calculate the error reduction improvement for the rest algorithms with input window 200 and forecast horizon 10. We can see that GRU and Informer outperform LSTM with 8% and 35% lower MAE, respectively. GINET has the best performance and the improvement is 58%, which is 46% and 28% better than its building blocks GRU and Informer, respectively. RMSE results support our observations as well.\n2) Input Window: A large input window helps improve performance. Many ML models favor more data to establish an accurate mapping from the input data to the output. For battery capacity prediction, the prediction output is correlated with not only short-term historical battery operation data but also long-term data. GINET employs sequential learning and context-aware learning for short-term and long-term data, respectively. Given a large input window, our tested algorithms have more information to process and can achieve improved performance if the information is processed well. For GINET with forecast horizon 10, the achieved MAE is 0.15, 0.14, and 0.11 for input window 10, 100, and 200, respectively, with a significant MAE improvement of 27% from window 10 to 200. LSTM also shows a 21% improvement from window 10 to 200 for MAE. Interestingly, we observe that GRU or Informer alone is much less sensitive to the input window size compared to GINET, where the difference of MAE is around 0.01 only for forecast horizon 10. This implies the importance of dealing with both short-term and long-term data with different technologies to well explore the potential of large window input data.\n3) Forecast Horizon: Besides, predicting for a long horizon is challenging. Compared to horizon 10, both MAE and RMSE results of the tested algorithms for horizon 25 are generally higher. For GINET with input window 200, its MAE is 18% higher with the forecast increased from 10 to 25. Intuitively, the historical battery data is more correlated to the near future battery dynamics and the long-term changes depend on the updated battery operation information which is not yet available. Given that the prediction accuracy drops for long horizons, it is important to convey the algorithm strength and limitation to users with comprehensive performance measurements, e.g., by quantifying the prediction uncertainty [13].\nD. Sensitivity Analysis\nGINET has several parameters that shall be optimized for optimal performance. In this part, we conduct sensitivity analysis to understand GINET's performance variation with different parameter settings. Specifically, we investigate GINET'S ProbSparse attention mechanism, distillation, and the number of encoder and decoder layers.\n1) Attention: GINET uses ProbSparse attention which addresses the inefficiencies of full attention used in standard Transformer models by focusing on the most critical dependencies within the input sequence. In full attention, all data points in a sequence are treated equally and all pairwise interactions are calculated. ProbSparse is different by selecting informative interactions only and enables the model to use memory efficiently and focus on the most relevant battery patterns for effective learning. Seen from Table II, ProbSparse helps reduce the prediction error compared to full attention and improve MAE by 6.3% and 5.3% for the implementation with and without distillation, respectively.\n2) Distillation: Distillation complements ProbSparse for efficient and effective processing of long battery sequences. While ProbSparse focuses on optimizing attention, distillation aims to reduce the length of the data input. It applies convolutional layers to shorten the data sequence in the encoder without sacrificing temporal and contextual information. With reduced redundancy, the distilled data allows GINET to focus on high-level features for improved generalization and performance on new battery dynamics. Table II shows that distillation helps improve prediction accuracy for both ProbSparse-based and full attention, where the MAE improvement is 6.3% and 11.1%, respectively.\n3) Encoder Decoder Layers: Encoder and decoder are key components of GINET (refer to Fig. 1). Inside each encoder, there can be multiple layers, each with attention, feedforward layer, distillation, etc., and the multiple layers can be stacked together in the encoder. This is true for the decoder also, except for some differences such as the decoder's specialized attention. In this part, we test the performance of GINET with different numbers of encoder layers and decoder layers. As battery capacity modelling is relatively a small-scale application, we configure the GINET models with minimal encoder and decoder layers. Specifically, we test three configurations, including one encoder layer and one decoder layer, two encoder layers, and two decoder layers, and two encoder layers and one decoder layer, denoted as E1D1, E2D2, and E2D1, respectively, as shown in Fig. 2. We can see that one encoder layer is insufficient to achieve optimal performance. Compared to the best configuration, i.e., E2D1 with two encoder layers and one decoder layer, the MAE is on average 8.3% higher for one encoder layer only. This suggests that our battery data embedding is not simple enough to be well modeled with one encoder layer. An encoder shall well process GINET's embedding that contains a comprehensive representation of both original battery data and the GRU-generated temporal dependencies. For the decoder, we notice that one layer supports optimal performance, and increasing the number of decoder layers does not enhance GINET'S prediction, with 10.9% higher MAE on average. A potential reason is that the decoder's role in GINET is to interpret encoder-generated representations and produce battery capacity predictions and one layer meets the application requirements well. When more layers are configured for the decoder, there could be unnecessary model complexity and risks of overfitting."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose GINET, a GRU-enhanced Informer network for battery SoC prediction. GINET is designed and developed to capture both sequential and contextual information of battery dynamics where battery capacity changes are correlated with both information. Among the GINET components, GRU models sequential dependencies and Informer captures contextual information by analyzing long sequence battery data with its attention mechanism. GINET inherits and customizes Informer's ProbSparse attention and distillation operation for efficient and effective battery data processing. An experimental study is conducted based on a public dataset with Panasonic 18650PF battery cells. We demonstrate that GINET yields a low error rate and stable prediction across various input window and forecast horizon configurations. The minimal achieved MAE for GINET is 0.11 and GINET outperforms comparison algorithms, including LSTM, GRU, and Informer, significantly. The average error reduction compared to the second best performed Informer is 27%. And RMSE results suggest similar insights.\nIn the future, we would like to enhance the extraction of sequential and contextual information with further optimized GRU and Informer or the latest ML algorithms. Feature fusion plays a vital role in ML [14] and we plan to upgrade GINET'S feature fusion module to further enhance the performance. Finally, we would like to improve GINET's generalizability of processing other battery datasets and customize GINET for other battery applications such as battery health monitoring."}]}